COMPUTINGPRACTICES
Edgar H. Sibley
Panel Editor

Mean response time and availability as optimization criteria for checkpoint
placement are better replaced by workable formulas that calculate the ratio
between the marginal gain accrued to users who experience system failure
and the (presumably slight) loss suffered on average by all users.

OPTIMIZATION CRITERIA FOR
CHECKPOINT PLACEMENT

C. M. KRISHNA, KANG G. SHIN, and YANN-HANG LEE

Checkpointing is becoming increasingly popular in
real-time and database systems as a means of mitigating
the consequences of failure. Checkpointing involves
storing authenticated process state information that can
then be used in the event of failure to restart the affected computation from the latest checkpoint, instead
of from the beginning. There are a number of articles in
the literature dealing with the optimal number and
placement of checkpoints [1-6, 9, 10]. Our purpose here
is to address an important point that has been overlooked; that is, although the analyses presented are often ingenious and elegant, their practical validity is
thrown into doubt by an inappropriate choice of optimization criteria. We propose alternative criteria that
we believe to be of greater practical relevance.
In every published analysis of optimal checkpointing
that we have seen, the only optimization criteria used
are mean response time and/or availability. No explicit
justification for this is ever given. A carelessness in
choosing optimization criteria can result in inadequate
and misleading performance analyses, for, in a real
sense, optimization criteria are relative, not absolute.
The simple act of choosing a particular criterion imposes a bias on the results that follow. By definition,
This work has been supported in part by NASA under Grant NAG 1-296. Any
opinions here expressed are those of the authors and do not necessarily
represent the views of NASA. All correspondence regarding this paper should
he sent to Kang G. Shin.
©1984ACM0001-0782/84/lOO0-1008 75¢

1008

Communicationsof the ACM

optimization criteria specify the commodity that is of
importance and therefore to be optimized. Optimization
criteria have a very subtle influence on the way systems are viewed. In a sense, they are languages through
which we seek to convey system performance. We
know from experience with natural languages that
these affect not only the way in which ideas are expressed but also the very ideas themselves. Optimization criteria are no exception. They manipulate our
view of system behavior, contorting it to fit a prefabricated mold. For this reason, the choice of optimization
criteria determines the practical usefulness of the results that are then derived.
Optimization criteria are performance measures. To
choose them correctly, it is important to determine
what it is we wish to express. In this case, where we
wish to measure the advantages that accrue from
checkpointing, it is best to trade the benefits derived
from them against the overhead they impose: in other
words, to carry out a value analysis. First, we shall show
why mean response time and availability do not do so
effectively and are therefore inadequate performance
measures for checkpoints.
Mean response time has long been a favorite measure
of computer performance among queuing analysts. It is
much easier to compute than the higher moments of
response time. That is, it is sometimes easy to obtain
the mean response time even when the response time
distribution is very difficult or impossible to calculate.

October 1984 Volume 27 Number 10

Computing Practices
Also, in many cases, the data that are supposed to be
representative of the arrival or service time distributions are not known to a sufficient degree of accuracy
to warrant obtaining the higher moments of the response time distribution. Again, the mean response
time is often quite adequate for most day-to-day purposes.
Availability, which is defined as the percentage of
time for which the system is operational, is another
well-known measure. It is also basically a first-moment
measure.
Our argument is that neither measure is suitable for
checkpointing models. Although checkpoints are useful
auxiliary means to enhance reliability, it must be assumed that the reliability of the system is already considerable without them. If the Mean Time Between Failures (MTBF} for a system were to be much less than,
say, 50-100 hours, checkpointing would be only a minor concern; the designers would have much more
pressing woes.
Nonetheless, when checkpointing models are studied
using mean response time or availability as criteria,
developers are frequently driven to the extremes of
considering MTBF values of between 2 and 10 hours in
their numerical examples. For systems that fail much
less frequently than that (e.g., the systems that one
comes across in practice), the improvements in both
mean response time and availability due to checkpoints
are too small to be numerically significant: Indeed, in
many cases, checkpointing actually increases mean response time.
The reason for this is that both mean response time
and availability express performance from the system
point of view, and are therefore insensitive when it
comes to probing the consequences of failure. Their
chief use lies in characterizing the vast majority of
tasks or transactions that do not experience system failure. They would be entirely appropriate for checkpoints if they were only devices to enhance normal (i.e.,
failure-free) operation. However, since checkpoints are
meant only to improve the handling of failure, and indeed add overhead to the execution time of nonfailing
transactions or tasks, we need to seek out more appropriate performance measures. In so doing, we assume
that the failure rate is very low: less than 10-3 per
hour. In order to be practically useful, any performance
measures for checkpointing should have the following
two basic features: They should express the resulting
improvement in handling failures, and they should
consider the impact on transactions or tasks that suffer
no failure. Yet, at the same time, they should also not
make unrealistic demands on system data. We shall
consider the application of these principles first to
special-purpose (i.e., real-time) systems and then to
general-purpose systems.
REAL-TIME APPLICATIONS
Computers used in the control of critical systems whose
malfunction may endanger life, public safety, or property have stringent reliability requirements. The chief

October 1984 Volume 27 Number 10

distinction between the requirements for control computers as opposed to general-purpose computers is that,
in the former, an outage of more than a very short
duration may have catastrophic consequences. This is
because real-time computers have hard deadlines, which
when missed cause the controlled system to fail.
The function of checkpoints in real-time applications
is to increase the probability of the system's recovering
from failure quickly enough to meet hard deadlines.
The overhead they impose is a slight increase in the
response time of processes that do not suffer failure.
This increase can degrade the quality of control provided to the controlled system and decrease system efficiency. The extent of this decline in efficiency can be
used as a measure of the overhead imposed by the
introduction of the checkpoints. In terms of a value
analysis, the benefit that accrues from checkpoints is a
reduction in the probability of missing hard deadlines
(in [7] we call this the probability of dynamic failure),
whereas the price that has to be paid is the decline of
efficiency of the controlled system. (The decline in efficiency is expressed through a cost function [7] whose
domain is the computer response time and whose range
is the performance index--in units of energy, time,
etc.--of the controlled system.) In connection with this
decline in efficiency, we define mean cost as follows: Let
f(t} be the density function of the response time distribution for a given control task, and g(t) the cost function associated with a response time of t for that task.
Then, the mean cost accrued for every execution of the
given task is equal to
MC =

f(t)g(t) dt

Both the probability of missing a hard deadline and
the overhead due to increased response time as reflected in the performance of the controlled system are
of direct physical relevance insofar as they link the
behavior of the controlling computer with that of the
controlled system. By using these quantities, we are
explicitly carrying out an analysis of the impact of the
computer on the application. This was not the case with
mean response time or availability, where the impact
on the application is not made evident. ~ For this reason,
the proposed measures have much greater physical
meaning than do mean response time and availability.
Our value analysis is therefore a trade-off between
the decline in the probability of missing a hard deadline and the possible increase in the average overhead
incurred by the controlled system due to the added
delay owing to checkpointing. We illustrate this by the
following example.

Numerical Example
Let us examine the benefits that might accrue when
checkpointing is used in the controlling computer of a
computer-controlled aircraft in the final stages of descent just prior to landing. (Such aircraft are expected
t In a n y case, mean response time and availability are very poor yardsticks in
the real-time domain.

Communications of the ACM " 1000

Computing Practices

to be operational early in the next century.) A mathematical analysis can be found in [8]; here, we restrict
ourselves to a concise description.
The computer task to be considered is the deflection
control of the aircraft's elevator. The four state variables of interest are the altitude, descent rate, pitch
angle, and pitch angle rate of the aircraft. Constraints
are prescribed for the value of each of these quantities
at touchdown. The region over which touchdown is to
occur is also specified, and the optimal trajectory is
given for all four state variables. The task of the controlling computer is to estimate the value of the state
variables periodically (every 60 milliseconds} and to
compute the optimal deflection of the elevator. Owing
to the nonzero response time of the computer, the control provided is only suboptimal. The performance index that is appropriate in this case is a weighted sum of
the squares of the deviation of each of the state variables from the optimal trajectory. The greater the response time of the computer, the greater the deviation
of the state variables from the optimal trajectory and,
therefore, the greater the overhead imposed by the
computer on the controlled aircraft. The hard deadline
over the final part of the descent is found in [8] to be 60
milliseconds. The cost function {i.e., the weighted sum
of the squares of the deviations of the state variables as
a function of the controller response time} was also
derived in [8] and is reproduced here as Figure 1. Our
goal here is to use these data in computing the optimal
number of checkpoints.
Let the MTBF be 10,000 hours; let the occurrence of
error be a Poisson process with rate X = 1/MTBF; and
let to, t~, and tov be the time needed to set up rollback,
restart, and one checkpoint. Let us assume also that the

saved state may be contaminated with probability ps,
which means the system can be recovered using rollback with probability pb = 1 - ps and has to restart
with probability ps. Clearly, pb = 0 when the number of
checkpoints, n, is zero. Let the nominal execution time
be denoted by ~. Then, the total execution time ~t is
given by
~t = ~ + ntov +trec

where n is the number of checkpoints inserted and t,ec
is the time overhead used in recovery, trec is a random
variable that depends on the probability of failure, pb,
and p~:
0
tb + tron
trec=
ts + tstart

where troll and tstart are the computation undone because of rollback and restart, respectively. Since the
ratio of the execution time of any single task to the
MTBF is of the order of 107, we may assume that the
probability of a second failure occurring to the same
task is negligible. Let the checkpoints be placed at equidistant intervals and let tiny = ~/(n + 1) be the interval
between successive checkpoints. The density function
of trol~ and t~tart is given by froH(t) = Xe-Xt/(1 -- e -xt"v) for
t e [0, ti,v] and f~tort(t} = ~e-X'/(1 - e -x~} for t ~ [0, ~],
respectively. The density function of the total response
time ~t can be easily obtained from the above equations. Three cases are considered for the nominal execution time of the deflection task: 20 ms, 30 ms, and 40
ms. For each, the probability of dynamic failure and the
mean cost that ensues with checkpoints is computed.
To express the marginal benefit accrued (in terms of
the reduction probability of dynamic failure pay,}
against the price paid (in terms of the increased mean
cost MC, i.e., operating overhead}, we use the following
trade-off ratio:
pd~. with (n - 1) checkpoints
- Par. with n checkpoints

O3

Trade-off ratio(n) =

0

M C with n checkpoints
- M C with (n - 1) checkpoints

Od

0

0
0

0

q
0

,I

0.00

10.00

I

'1

I

20.00 30.00 40.00
Delay (ms)

'1

"

Communications of the ACM

I

50.00 60.00

FIGURE 1. The Cost Function for a Real-Time System

1010

if no error occurs
if error occurs and
the version is recovered
by rollback
if error occurs and
the task has to restart

The results are presented in Table I. When the nominal
execution time is 20 milliseconds, all the checkpoints
do is increase the overhead, that is, the mean cost. No
discernible drop is noticed in the probability of dynamic failure when checkpoints are added. The marginal gain in reliability on adding checkpoints is therefore zero.
However, as nominal execution time increases,
checkpointing begins to cause a noticeable decrease in
the probability of dynamic failure. This is expressed
through a positive trade-off ratio: n = 1 for a nominal
execution time of 30 ms; n = 1, 2 for 40 ms; and n --- 1,
2, 3, 4, 5 for 50 ms. These ratios show that a tangible

October 1984

Volume 27 Number 10

Computing Practices

TABLE I. Checkpoints in Real-Time Applications
(Pb = 0.9, MTBF = 104 hours, t=, = 0.1 ms, tb = 2.0 ms, ts = 2.0 ms.)

0
1
2
3
4
5

0.12848
0.12909
0.12971
0.13033
0.13095
0.13157

0.3086E-15
0.3086E-15
0.3086E-15
0.3086E-15
0.3086E-15
0.3086E-15

0.0
0.0
0.0
0.0
0.0

(a) Nominalexecu,ontime:2Oms.

Mean~.
0
1
2
3
4
5

0.26156
0.26431
0.26709
0.26991
0.27272
0.27567

! i !(rra~ff~tio) X lO!i
0.37037E-07
0.37037E-08
0.37037E-08
0.37037E-08
0.37037E-08
0.37037E-08

121.5
0.0
0.0
0.0
0.0

(b) Nominalexecutiontime: 30 ms.

ciated with executing tasks. The basic idea is to consider separately the impact of checkpoints on processes
that do not experience failure and processes that do. If
we are willing to countenance a performance vector in
place of a scalar, the following might suffice:
[M~o]
p -- L M L ]

where MTo = mean execution time for processes not
experiencing failure, and MTj = mean execution time
for processes experiencing failures. This would be a
much finer measure than unconditioned mean execution time or availability, while retaining all the advantages of only requiring the computation of the first moment of response time distribution. Of course, to compare two performance vectors, one would need a metric
such as the following trade-off ratio:
MTf with (n - 1) checkpoints
- MT! with n checkpoints
Trade-off ratio(n) = MTo with n checkpoints
- MTo with (n - 1 ) checkpoints

0
1
2
3
4
5

0.55352
0.55472
0.55586
0.55694
0.55795
0.55891

0.30555E-06
0.43055E-07
0.30555E-07
0.30555E-07
0.30555E-07
0.30555E-07

2177.0
109.8
0.0
0.0
0.0

(c) Nominalexecutiontime: 40 ms.

0
1
2
3
4
5

0.89694
0.90848
0.92025
0.93231
0.94466
0.95730

0.46666E-06
0.35666E-06
0.26166E-06
0.16666E-06
0.71666E-07
0.46666E-07

95.6
80.6
78.7
76.9
19.8

(d) Nominalexecutiontime: 50 ms.

gain in reliability has been made for the indicated
number of checkpoints (i.e., the probability of dynamic
failure is reduced by a factor of 10 in each case). Of
course, this has been achieved at the price of a certain
increase in the mean cost, which is also reflected in the
trade-off ratio.
Had mean response time and availability been used
for the MTBF indicated, the results would have led to
the recommendation that there be no checkpoints at
all. The gain in reliability indicated above would have
been masked by the high proportion of jobs that do not
suffer failure. Mean response time is therefore a blunt
instrument when it comes to probing the consequences
of failure. A similar argument can be made for availability as a criterion.
GENERAL-PURPOSE SYSTEMS
The schema described above can be extended to encompass systems that do not have hard deadlines asso-

October 1984 Volume 27 Number 10

The trade-off ratio computes the ratio of the marginal
gain made to the mean execution time of jobs that
experience failure to the marginal loss made to the mean
execution time of jobs that are flee from failure. Unlike
the measure of mean response time, this measure allows
for the fact that the execution of failure-free jobs may
actually be degraded by the introduction of checkpoints.
The above trade-off ratio is used when we are interested in the role of checkpoints in reducing the execution
time for those processes that undergo one or more failures, and not in what happens to processes that suffer n
failures for some n. In other words, we average in the
above trade-off ratio all fail-and-recover tasks regardless
of the number of failures; the n u m b e r of failures is
generally no more than one. If, for some reason, one
wished to consider separately the handling of processes
according to the number of failures they suffered, an
expanded performance vector P = [MTo, MT1 . . . . ]T resuits, and it is not obvious what the metric for P would
be. 2

A second useful measure is the effect of checkpoints
on various percentiles of execution time. This is analogous to the pdy,, computation for real-time systems that
we referred to earlier.
We now come to the second condition, namely, that
the performance measures should not make unrealistic
demands on the data that go into calculating them. In
computing response time distribution, which would normally be arduous and sometimes impossible, the assumption of low failure rate comes to our rescue. Since
the failure rate is assumed to be small, the distribution
of failure between two adjacent checkpoints can quite
accurately be taken to be uniform.
To convince the reader that these measures are indeed
computationally feasible and practical, a numerical example is presented below.
2We need a metricfor comparingtwo vectors.

Communications of the ACM

1011

Computing Practices

N u m e r i c a l Results
S o m e n u m e r i c a l results for g e n e r a l - p u r p o s e s y s t e m s are
p r e s e n t e d in T a b l e II. W e see t h a t a l t h o u g h u n c o n d i t i o n e d m e a n e x e c u t i o n t i m e i n c r e a s e s in all cases, t h e
m e a n e x e c u t i o n t i m e t a k e n o v e r all jobs t h a t e x p e r i e n c e failure is r e d u c e d w i t h t h e i n t r o d u c t i o n of c h e c k p o i n t s u n t i l m o r e t h a n six c h e c k p o i n t s h a v e b e e n introd u c e d in t h e first case (with n o m i n a l e x e c u t i o n t i m e 70
ms) a n d m o r e t h a n eight c h e c k p o i n t s in t h e s e c o n d
(with n o m i n a l e x e c u t i o n t i m e 90 ms). T h e r e a s o n w h y
m o r e c h e c k p o i n t s yield a b e n e f i t in t h e s e c o n d (Table
IIb) is t h a t t h e g r e a t e r n o m i n a l e x e c u t i o n t i m e i n c r e a s e s
t h e p e n a l t y i n c u r r e d o n a restart. H e r e again, if m e a n
e x e c u t i o n t i m e h a d b e e n c h o s e n as a c r i t e r i o n , t h e optim a l n u m b e r of c h e c k p o i n t s r e c o m m e n d e d w o u l d h a v e
b e e n zero. Of course, a d d i n g c h e c k p o i n t s i n c r e a s e s t h e
m e a n e x e c u t i o n t i m e t a k e n o v e r all jobs in t h e s y s t e m
but, at t h e s a m e time, m a r k e d l y r e d u c e s t h e e x e c u t i o n
t i m e for jobs t h a t suffer s o m e failure. M e a n r e s p o n s e
t i m e fails e n t i r e l y to i n d i c a t e w h a t t h e t r a d e - o f f ratio
does; t h a t is, it fails to s h o w e x p l i c i t l y w h a t is g a i n e d as
o p p o s e d to w h a t is lost.
As in t h e r e a l - t i m e case, m e a n r e s p o n s e t i m e w o u l d
h a v e m a s k e d t h e r e d u c t i o n in r e s p o n s e t i m e for jobs
t h a t suffer failure, Again, t h i s p a r t i c u l a r l i m i t a t i o n of
m e a n r e s p o n s e t i m e c a n b e c a r r i e d o v e r to availability.
TABLE II. Checkpoints in General-Purpose Systems
Checkpoint establishment overhead = 0.5 ms;
MTBF -- 104 hours and no hard deadline.

Number of
Checkpoints
0
1
2
3
4
5
6
7
8
9
10

MTf

MTo

105.8
92.3
88.1
86.3
85.4
84.9
84.8
84.8
84.9
85.1
85.3

70.0
70.5
71.0
71.5
72.0
72.5
73.0
73.5
74.0
74.5
75.0

Trade.offRatio
26.99
8.33
3.67
1.80
0.86
0.33
-0.01
-0.22
-0.37
-0.49

(a) Nominal execution time: 70 ms.

1012

MT~

MTo

0
1
2

135.8
118.3
112.8

90.0
90.5
91.0

34.99
11.00

3
4
5
6
7
8
9
10

110.3
108.9
108.3
107.9
107.8
107.8
107.9
108.1

91.5
92.0
92.5
93.0
93.5
94.0
94.5
95.0

5.00
2.60
1.40
0.71
0.28
0.00
-0.20
-0.35

Communications of the ACM

REFERENCES
1. Baccelli, F, Analysis of a service facility with periodic checkpointing. Acta Inf. 15, 1 (1981), 67-81.
2. Brodetskiy, G.L. Periodic dumping of intermediate results in systems
with storage-destructive failures. Eng. Cybern. 15, 5 (Sept.-Oct. 1979),
685-689.
3. Chandy. K.M.. Browne. J.C., Dissly. C.W., and Uhrig, W.R. Analytic
models for rollback and recovery strategies in data base systems.
IEEE Trans. Softw. Eng, SE-1.1 (Mar. 1975), 100-110.
4. Chandy, K.M., and Ramamoorthy. C.V. Rollback ~ihd recovery strategies for computer programs. IEEE Trans. Comput. C-21, 6 (June
1972), 546-556.
5. Gelenbe, E. On the optimum checkpoint interval. J. ACM 26, 2 (Apr.
1979), 259-270.
6. Gelenbe, E., and Derochette, D. Performance of rollback recovery
systems under intermittent failures. Commun. ACM 21, 6 (June
1978), 493-499.
7. Krishna, C.M., and Shin. K.G. Performance measures for real-time
controllers. In Performance 83, A, Agrawala and S.K. Trlpathi, Eds.
North-Holland, Amsterdam, 1983, pp. 229-250.
8. Shin, K.G., Krishna, C.M.. and Lee, Y.-H. Unified methods for evaluating real-time controllers: A case study. Computing Research Laboratory Rep. CRL-TR-23, The Univ. of Michigan, Ann Arbor, June
1983.
9. Tantawi, A.N., and Ruschitzka. M. Performance analysis of checkpointing strategies. In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems (Minneapolis,
Minn., Aug. 29-31). ACM, New York, 1983, p. 129.
10. Young, J,W. A first order approximation to the optimum checkpoint
interval. Commun. ACM 17, 9 (Sept. 1974), 530-531.
CR Categories and Subject Descriptors: D.4.5 [Operating Systems]:

Number of
Checkpoints

(b) Nominal execution time: 90 ms.

SUMMARY
O u r object in this p a p e r h a s b e e n to s h o w t h a t t h e
m e a s u r e s of m e a n r e s p o n s e t i m e a n d a v a i l a b i l i t y t h a t
h a v e c o n v e n t i o n a l l y b e e n u s e d to i n d i c a t e t h e b e n e f i t s
t h a t a c c r u e from c h e c k p o i n t i n g are i n a d e q u a t e i n t h a t
t h e y are not s u f f i c i e n t l y sensitive. To r e m e d y t h i s p r o b lem, w e h a v e p r o p o s e d w o r k a b l e t r a d e - o f f ratio f o r m u las t h a t c a l c u l a t e t h e price p a i d for c h e c k p o i n t i n g in
t e r m s of t h e d e c l i n e in efficiency of t h e s y s t e m as a
whole. Our performance measure has been relatively
simple: t h e ratio b e t w e e n t h e m a r g i n a l gain a c c r u e d to
u s e r s w h o suffer s y s t e m failure a n d t h e ( p r e s u m a b l y
slight) loss s u f f e r e d o n a v e r a g e b y all users.
In this paper, no effort h a s b e e n m a d e to a d d r e s s t h e
issue of u s e r p e r c e p t i o n . At t h i s stage, t h e r e is o n l y a
i n t u i t i v e link b e t w e e n t h e a b o v e p e r f o r m a n c e m e a s u r e
ratio a n d t h e r e s p o n s e t i m e as p e r c e i v e d b y t h e user.
M o r e r e s e a r c h r e m a i n s to b e d o n e into t h e n a t u r e of
u s e r - p e r c e i v e d delays,

Trade.offRatio

Reliability--checkpoint~restart, fault-tolerance; C.3 [Special-Purpose and
Applications-Based Systems]: real-time systems; C.4 [Performance of
Systems]: performance attributes; reliability, availability, and serviceability
General Terms: Performance, Reliability

Received 8/83: revised 12/83: accepted 3/84
Authors' Present Address: C.M. Krishna, Kang G. Shin, and Yann-Hang
Lee, Computer Research Laboratory, Dept. of Electrical Engineering and
Computer Science, The University of Michigan. Ann Arbor, MI 48109.
Permission to copy without fee all or part of this material is granted
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication
and its date appear, and notice is given that copying is by permission of
the Association for Computing Machinery. To copy otherwise, or to
republish, requires a fee and/or specific permission.

October 1984

Volume 27

Number 10

Computer Communications 20 (1997) 586±598

Virtual cell in mobile computer communications
Kyungshik Lim a,*, Young-Hwan Lim a, Yann-Hang Lee b
a

Computer Communications Section, Multimedia Technology Department, Electronics and Telecommunications Research Institute,
161 Gajong-Dong, Taejon 305-350, South Korea
b
Computer and Information Sciences Department, University of Florida, Gainesville, FL 32611, USA
Received 17 August 1995; accepted 15 March 1996

Abstract
In this paper, we design and develop a virtual cell approach for the transmission of IP datagrams in mobile computer communications. A
virtual cell consists of a group of physical cells whose base stations are implemented by remote bridges and interconnected via high-speed
datagram packet-switched networks. Host mobility is supported at the data link layer by virtual cell protocol (VCP) using the distributed
hierarchical location information of mobile hosts.
As far as the IP layer is concerned, it appears within a virtual cell as if the communication between two mobile hosts in different physical
cells were taking place directly as in the same physical cell. Between virtual cells, a one-hop forwarding pointer from a mobile host’s native
virtual cell to its current virtual cell is maintained at the VCP layer. This means that the combination of the existing routing protocols of the IP
layer and the forwarding pointer maintained at the VCP layer between virtual cells gives the same effect of having a fully duplicated location
information among virtual cells for the global mobile network.
Thus, the virtual cell approach eliminates the necessity of IP-level mobile host protocols that may interfere with the conventional IP
protocol in a practical sense. In addition, given mobility and communication patterns among physical cells, it achieves a logically flexible
coverage area of a virtual cell so as to minimize the total communication cost for the global mobile network where inter-virtual cell
communication is more expensive than intra-virtual cell communication (K. Lim and Y.-H. Lee, IEEE Personal, Indoor and Mobile
Radio Communications PIMRC’94, pp. 1237–1241, Sep. 1994; K. Lim, Y.-H. Lim and Y.-H. Lee, IEEE ICC’95, pp. 1839–1843, June
1995). To demonstrate the correct operations of VCP and measure data which are used as parameters to a performance model for the virtual
cell system, we implement VCP on a simulated environment. The performance analysis of the virtual cell system is out of scope in this paper
and one is referred to K. Lim, Y.-H. Lim and Y.-H. Lee, Proc. 10th Int. Conf. Information Networking ICOIN-10, pp. 551–560, Jan. 1996.
q 1997 Elsevier Science B.V.
Keywords: Virtual cell; Mobile IP; Mobile computer communications

1. Introduction
As computers become more powerful and portable
with the appearance of high-speed wireless interfaces,
there is an increasing demand on the provision of mobile
computer communications in TCP/IP environments. A fixed
host in internets is always assigned an IP address which
not only serves a unique identifier used by the higher
layer protocols but also represents the current location of
it. An inherent problem from the transmission of IP datagrams in mobile computer communications is that when a
mobile host (MH) migrates, its IP address is only valid as
the identification information but not able to represent the
current location information. To solve this problem, much
research and development has focused on how to integrate
* Corresponding author. E-mail: kslim@com.etri.re.kr

0140-3664/97/$17.00 q 1997 Elsevier Science B.V. All rights reserved
PII S 01 40 - 36 6 4( 9 7) 0 00 4 8- 0

the functionality of host mobility into the IP layer, preserving the compatibility with the conventional IP protocol
[1–6].
Although these mobile host protocols vary on how to
represent and maintain location information for efficient
tracking of MHs, the techniques of using the IP options
and IP encapsulation have been considered. Typical examples of the first technique are virtual internet protocol (VIP)
[2,3] and the mobile host protocol using the IP loose source
routing option [4]. IP-within-IP (IPIP) [1] and internet
packet forwarding protocol [6] use the second technique.
Even though the details of these mobile host protocols are
different, we consider one representative mobile host protocol for each technique, VIP and IPIP, to illustrate common
features and problems of integrating host mobility into the
IP layer.
In VIP, the network layer is divided into two layers; the

K. Lim et al./Computer Communications 20 (1997) 586–598

VIP layer resides on top of the normal IP layer. The VIP
packet header is implemented as an option of the IP packet
header. An MH keeps its permanent IP address used at the
VIP layer for its identification and acquires a transient IP
address used at the IP layer for its current location information when it migrates. If the MH is in its native network,
both addresses are the same. After acquiring a transient IP
address, the migrating MH sends its native network a
notification packet whose header contains the permanent
and transient IP addresses. As the packet propagates to the
native network, every network entity including MHs,
mobile support gateways (MSGs), and even intermediate
gateways on the path snoops the header information and
stores address conversion information to a cache. In the
same way, the header information of all data packets in
transit is also used by the network entities to maintain
their caches. If the source has the cache entry for the
destination, the source executes address conversion before
sending a packet. The existing routing protocols of the IP
layer can then correctly deliver this packet to the destination. Otherwise, the source assumes that the destination
resides in its native network and sends the packet accordingly. As the packet traverses to the native network of the
destination, if an intermediate gateway has the cache entry
for the destination the gateway executes address conversion
and forwards the packet to the current location of the
destination.
Unlike VIP, intermediate gateways in IPIP are not
involved in the support of host mobility but merely in transport service. The source MSG encapsulates a network
control packet for mobility management or an IP datagram
from an MH into another IP datagram whose source and
destination addresses specify the communicating MSGs,
and transmits it over an internet. The existing routing
protocols of the IP layer then correctly deliver it to the
destination MSG. Thus, MSGs consider internets as a full
mesh of logical point-to-point links to interconnect them. A
mobile network consists of a number of MSGs, each of
which maintains the location information of its own MHs.
Every MH in the mobile network is assigned a unique IP
address but the network part of the IP address is the same.
When the MH migrates, a forwarding pointer is set from the
previous MSG to the new MSG for location tracking. If the
MSG has no location information for a specific MH, it
broadcasts an inquiry message to the other MSGs in the
same mobile network asking who has the MH. However,
IPIP also requires a transient IP address when the MH visits
a foreign mobile network.
Regardless of which technique is used, the integration of
host mobility into the IP layer reveals several implications.
First, underlying networks differ widely in their network
size, bandwidth, protocol, and packet size so that they or
some of them may not meet performance needs for the
support of host mobility such as rapid migration and
tracking of MHs. Moreover, because they are usually
under different administrative controls, efficient network

587

management and optimization for the global mobile
network may not be easy tasks.
Second, intermediate gateways may cause some performance problems no matter they are involved in the
support of host mobility or not. If they are involved, as in
VIP, they must be modified or replaced to understand VIP so
that the benefit from using existing internets as transport
networks is diminished. Furthermore, because intermediate
gateways have to snoop every packet in transit to maintain
location information, and examine every data packet in
transit to try to perform address conversion, the protocol
processing and memory loads at intermediate gateways
may severely affect overall performance. Even in case
that they are not involved, as in IPIP, because they usually
implement packet switching operations in software, protocol processing time coupled with possible network delay
between multiple hops of IP gateways may greatly affect
cell switching latency.
Third, each physical cell administered by an MSG is in
principle assigned an IP network address because every MH
and intermediate gateway is a network entity in TCP/IP
environments. It means that locating the MH in a different
physical cell necessitates a different network address to
represent its current location. Some undesirable features
of IP-level mobile host protocols essentially comes from
this fact. VIP requires a large amount of the transient IP
address space which becomes a very scarce network
resource as internets are rapidly growing. IPIP uses one
permanent IP address for each MH but relies on the broadcast inquiry among MSGs when the location information is
not available by a forwarding pointer, restricting the
scalability of IPIP to a local area.
Fourth, although IP-level mobile host protocols based on
options keep the compatibility with the conventional IP
protocol in their specifications, in a practical sense they
may interfere with it because most existing fixed hosts
and intermediate gateways do not implement the IP options
and their implementations to support the IP options may not
be feasible in the near future. In addition, the current efforts
to provide IP multicasting protocols and connectionoriented protocols require IP-level mobile host protocols
to be compatible.
As high-speed, connectionless, packet-switched networks
are emerging to extend LAN-like performance across a wide
area, we believe that they can greatly affect the support of
host mobility in mobile computer communications.
Examples are ATM networks [7,8] and switched multimegabit data services (SMDS) networks [9–12] which are
subnetworks providing an MAC service across a wide area
in a large interconnected network. Considering the requirements of base station networks from two different viewpoints, application and mobility management, the virtual
cell system takes advantage of high-speed datagram
packet-switched networks with the multicast ability for
base station networking. The base station networks
interconnect remote bridge BSs, so as to preserve the

588

K. Lim et al./Computer Communications 20 (1997) 586–598

interconnection level of physical cells at the data link layer.
The current location of MHs is identified by the base station
network address. It represents which BS can communicate
with a particular MH. For efficient location tracking of
MHs, the virtual cell system constructs a distributed hierarchical location information structure. Based on the distributed hierarchical location information, the virtual cell
protocol for handoff, address resolution, and data transfer
is designed. The handoff procedure can utilize the multicast
ability of the base station network to achieve the consistency
of the distributed location information. Since the IP network
address of a migrating MH represents at least the near location information of the MH in the virtual cell system, the
distributed location information coupled with the existing
routing protocols of the IP layer give the same effect of
maintaining a conceptually centralized server for the
whole mobile network.
Given mobility and communication patterns among
physical cells, suppose that an optimal partition of a cover
of disjoint clusters of physical cells is obtained and the
virtual cell system is deployed so that each cluster corresponds to a virtual cell. To evaluate the performance of the
virtual cell system, we apply an open multiple class queueing network model. There are three types of messages
entering or leaving the virtual cell system via BSs: the handoff message, the data message, and the address resolution
message. The handoff messages are generated due to
mobility patterns, and the data and address resolution
messages are due to communication patterns. The mobility
and communication patterns in conjunction with the topology of the deployed virtual cell system are used to determine service transition probabilities for each type of
message in the queueing network model. By solving the
traffic equations of the queueing network model, we obtain
various performance measures such as the network response
time for each type of message and the utilization of the base
station networks and the backbone network of the virtual
cell system.

2. Virtual cell concept
Consider the transmission of IP datagrams between two
MHs within a physical cell. Because radio links provide the
broadcast ability, the source can deliver IP datagrams to the
destination using the normal address resolution protocol
(ARP). Thus, the broadcast ability of radio links itself
eliminates the necessity of locating MHs, resulting in no
mobility management protocols at the IP layer and no
modifications to the normal ARP. To apply the similar
rationale to MHs crossing physical cell boundary, we propose the virtual cell concept, as depicted in Fig. 1.
A virtual cell is a logically extended cell of physical cells
whose BSs are implemented by remote bridges and interconnected via a base station network. Because remote
bridge BSs enable one to preserve the interconnection
level of physical cells at the data link layer, the whole virtual
cell is assigned an IP network address. In order to make it
possible for MHs in a virtual cell to directly deliver IP
datagrams using the conventional IP and ARP protocols, a
mobility management protocol, called virtual cell protocol
(VCP), is implemented at the data link layer of BSs. Each
virtual cell is also allocated an ARP/location server (ALS)
which implements VCP and supports the normal gateway
function with other virtual cells or fixed networks. VCP
supports handover, address resolution, and location tracking
for datagram delivery, based on the distributed hierarchical
location information of BSs and the ALS, as described in
detail in Section 4.
It should be noted that the identification and location
information of an MH is represented by two different data
link layer addresses in the virtual cell, the radio network
MAC (RMAC) address and the base station network
MAC (BMAC) address, respectively. Note that the BMAC
address is not only used as the location information of the
MH but also as the identification information of a virtual
cell. For example, the SMDS addresses can be formatted
with a similar structure used for the North-American

Fig. 1. The virtual cell concept.

589

K. Lim et al./Computer Communications 20 (1997) 586–598

Fig. 2. The data path between virtual cells.

numbering plan to represent the geographic semantics. This
separation eliminates the necessity of acquiring a temporary
address to represent the MH’s current location as it
migrates. In addition, unlike IP-level mobile host protocols,
the IP network address also represents the correct location
information of the MH while it moves to a different physical
cell in the virtual cell. This is possible because the whole
virtual cell is assigned an IP network address and host
mobility is shielded from the IP layer.
Even when the MH migrates between virtual cells, the IP
network address is still valid as the near-location information in the virtual cell environment. Fig. 2 shows the data
path from a fixed host or an MH in a different network to
MH 1 which migrates from its native virtual cell A to virtual
cell B to virtual cell C. Whenever MH 1 crosses the boundary
of virtual cells, a one-hop forwarding pointer is maintained
from ALS A in its native virtual cell to the ALS of its current
virtual cell, ALS B or ALS C, by the handover procedure of
VCP. Note that the forwarding pointer is not maintained at
the IP layer but at the VCP layer. When a fixed host or an
MH in a different network wants to send an IP datagram to
MH 1 in virtual cell B or C, the existing routing protocols of the
IP layer correctly deliver it to ALS A. Then ALS A redirects it at
the VCP layer to the corresponding ALS, ALS B or ALS C, which
keeps track of the exact location of MH 1 in its virtual cell.
Thus, every MH resides in its native virtual cell from the
IP layer’s viewpoint but practically may reside in an
adjacent virtual cell because of a one-hop forwarding
pointer from the native virtual cell to the current virtual
cell maintained at the VCP layer. It means that the combination of the existing routing protocols of the IP layer and
the forwarding pointer between virtual cells at the VCP
layer gives the same effect that we have a fully duplicated
location information among virtual cells for the global
mobile network.
To configure neighboring physical cells into virtual cells,
the underlying transport network should provide high-speed
datagram packet-switched services and the group addressing capability at the data link layer. The flexible configuration allows network managers and designers to customize

the logical coverage area of the virtual cell to their requirements according to host mobility and data traffic patterns
among physical cells.
Consider an environment that a number of physical cells
are deployed in a metropolitan area. The cell size in the
urban area is usually smaller than that in the suburban
area in order to accommodate a larger population of mobile
users. Even though user mobility is inherently unpredictable, there could be a high possibility that the daily routine
of mobile users is usually confined to several physical cells
in the urban area. Because of a relatively small cell and large
population of mobile users in the urban area, it becomes
very important to achieve the fast location tracking of
mobile users and the small cell switching latency between
physical cells, mitigating the effect of a large volume of
mobility management information. Thus, the physical
cells covering the urban area could be combined into one
or a few number of virtual cell(s) for efficient mobility
management and data delivery.
3. Virtual cell architecture
Because the virtual cell is a logical concept, the same
transport network may support several virtual cells simultaneously. As shown in Fig. 3, there are two roles of the
transport network: base station network and backbone network. The base station network is used to interconnect a
number of BSs and an ALS to build a virtual cell, and the
backbone network is used to interconnect among virtual
cells and fixed networks. Note that the base station network
utilizes both point-to-point and multicast communications,
while the backbone network does only point-to-point
communication.
3.1. Base station network
The base station network should meet some requirements
from two different viewpoints:
•

Application’s

viewpoint.

IP

operates

under

the

590

K. Lim et al./Computer Communications 20 (1997) 586–598

Fig. 3. The virtual cell architecture.

•

assumption that packet arrivals are independent and
unpredictable. If IP is coupled with the connectionoriented base station network, one connection establishment and release may be needed for each individual
packet over a fixed link between a pair of BSs. Although
a bundle of packets could be transmitted over one
connection by some multiplexing techniques, when
communicating MHs migrate to different physical
cells with the connection, the problem becomes complex
and even intractable. In addition, because location tracking has inherently connectionless properties, the base
station network should support the datagram delivery.
The current ubiquitous coverage of TCP/IP networks
and applications also requires that the base station network be able to cover a wide area.
Mobility management’s viewpoint. Combined with the
distributed location information of MHs among BSs,
host mobility is a main cause of the inconsistency. In
order to maintain the consistency of the distributed location information efficiently, the base station network
should support the multicast ability. Furthermore,
because the network control information for mobility
management is expected to be greatly increased as the
cell size becomes smaller, the base station network
should have high-bandwidth not so as to affect the
normal data traffic greatly.

We consider SMDS as an example with these characteristics. Both individual and group addressing capabilities
combined with a set of addressing-related service features
(e.g. source address validation, source and destination
address screening) enable one to create a number of logical
networks over SMDS. Every BS and ALS is attached to a
subscriber network interface (SNI) and individually
addressed. At the same time, a group address identifies all
BSs in a virtual cell, ensuring that each group address identifies uniquely only one set of individual addresses. The
number of SNIs to be supported by a switching system is
at least 256 SNIs and the future number is up to 4096 SNIs
within a local access transport area. Considering that the
range of a physical cell is usually 3–5 km in the urban

area or 10–15 km in the suburban area, the capacity of a
very few number of switching systems may be enough to
support base station networking within a metropolitan area.
3.2. Base station
The communication architecture in a BS is shown in
Fig. 4. The internal port connected to a physical cell implements an RMAC entity, while the external port connected to
the base station network implements a BMAC entity. The
MAC relay entity translates the information format between
the physical cell and the base station network. The protocol
identifier field of both the RMAC and BMAC frame headers
is set for LLC type 1 unnumbered information format. For
example, the higher layer protocol identifier field of the
SMDS interface protocol L3_PDU used as BMAC frames
is set to 1. The LLC service access point of the RMAC
frame is set for subnetwork access protocol (SNAP), while
that of the BMAC frame is set for VCP.

Fig. 4. The base station architecture.

K. Lim et al./Computer Communications 20 (1997) 586–598

The VCP header has three fields: VCP type, VCP length,
and RMAC address. The VCP type is set to one of
ADDRESS, DATA and HANDOVER to specify the address
resolution, data transfer and handover modules of VCP,
respectively. Depending on the VCP type, the information
field of the VCP frame has a different format. The DATA
type indicates that the information field has an IP datagram,
and the information fields for the HANDOVER and
ADDRESS types are given in the next section. The VCP
length indicates the length of the VCP header. Note that
the RMAC address is only used with the DATA type.
Each BS maintains a partial location information of the
virtual cell in its internal membership table (IMT) and
external membership table (EMT). IMT keeps track of all
MHs currently roaming in its physical cell. The IMT entry
for an MH has two fields, the IP and RMAC addresses, each
of which is used as an identifier but has a different role. The
IP address is used as an identifier by the higher network
layers as usual, but the network part of the IP address
represents the MH’s native virtual cell to which it initially
belongs. This IP address obtained by the handover procedure is only needed for fast address resolution, as
described in the next section. On the other hand, the
RMAC address is the other identifier used by VCP for the
support of host mobility at the data link layer. EMT has a
relatively small number of entries for MHs roaming in
different physical cells of the same virtual cell. The EMT
entry for an MH has three fields, the IP, RMAC and BMAC
addresses, respectively, of which the BMAC address
represents the MH’s current network address.
If the source MH wants to send an IP packet to the destination MH whose native virtual cell is the same, the source
performs ARP and sends the IP packet using the destination
RMAC address. On the other hand, if the source wants to
send an IP packet to the destination whose native virtual cell
is different, the source does not perform ARP and sends the
IP packet using the RMAC address of its current BS, which
is obtained by a beacon message. When the BS receives an
SNAP/LLC/RMAC frame from the internal port, it
checks the protocol identification (PID) field of the
SNAP header. If the PID field is set for ARP, the ARP
packet is sent to the address resolution module which
performs virtual cell address resolution protocol
(VCARP). VCARP achieves the IP-to-RMAC address
binding for the destination and also distributes the location
information of the source and destination. If the PID field
is set for IP, the IP packet is sent to the data transfer
module with the destination RMAC address. If the destination RMAC address is the BS’s RMAC address, the
data transfer module sets the RMAC address field of the
VCP header to the BS’s RMAC address and relays a
VCP/LLC/BMAC frame to the ALS without searching
IMT and EMT. Otherwise, the data transfer module keeps
track of the destination MH’s current location using IMT
and EMT. If the corresponding entry is not found, the data
transfer module transmits to the ALS a VCP/LLC/BMAC

591

frame whose VCP header contains the destination RMAC
address.
3.3. ARP/location server
Each ALS maintains a global membership table (GMT)
for MHs roaming in its virtual cell, as shown in Fig. 5. The
entry format of GMT is the same as that of EMT. There are
two types of frames from the backbone network for data
transfer. One is the VCP/LLC/BMAC frame redirected
from other virtual cells that maintain one-hop forwarding
pointers at the VCP layer, and the other is the SNAP/LLC/
BMAC frame from fixed networks or other virtual cells. The
first type of frame is sent to the data transfer module which
identifies the destination BS by searching GMT with the
RMAC address of the VCP header, and transmits it to the
base station network. The second type of frame is, however,
directly sent to the routing module which extracts the IP
packet. If the network part of the destination IP address
indicates the same virtual cell, the packet is sent to the
data transfer module of VCP where the IP-to-RMAC
address binding occurs and a VCP/LLC/BMAC frame is
transmitted to the base station network or to the backbone
network, depending on whether the destination MH resides
in this native virtual cell or moved to another virtual cell,
respectively. If the IP packet is in transit, it is transmitted to
the next-hop router.
The VCP/LLC/BMAC frame from the base station network is always sent to the VCP module. If the frame carries
a VCARP packet, the address resolution module performs
the IP-to-RMAC address binding with GMT and sends it
back to the base station network. If the frame carries an IP
packet with a BS’s RMAC address of the virtual cell in the
VCP header, the data transfer module dose not search GMT
and directly pass the IP packet to the routing module where
the next-hop router is determined. Otherwise, the data
transfer module searches GMT with the destination
RMAC address and then transmits a VCP/LLC/BMAC
frame back to the base station network or to an adjacent
virtual cell following a forwarding pointer. At the same
time, if the destination MH resides in this virtual cell, the
ALS sends to the source BS a VCARP reply which conveys

Fig. 5. The communication architecture in an ARP/location server.

592

K. Lim et al./Computer Communications 20 (1997) 586–598

Fig. 6. The distributed location information in a virtual cell.

the destination BMAC address so that the subsequent data
transfer can directly go to the destination BS.

4. Virtual cell protocol
4.1. Distributed location information
There are generally three different ways to distribute
location information: centralized, partitioned, and duplicated. Depending on which way is used to treat location
information, there is a tradeoff between location registration
and paging. In a centralized system, a large volume of
location updates at one physical site may degrade the network performance significantly. However, the consistency
of location information is obtained and simple paging is
achieved. A typical example is the first generation of mobile
cellular telephone systems. In a partitioned system, the
different partitions of location information are held at
different sites. An example is IPIP where every BS maintains its local location information and paging is basically
required when two remote physical cells are involved in
communication unless a forwarding pointer is found.
Thus, this approach can alleviate the problem of location
registration in the centralized system, but may need frequent
paging. In a duplicated system, on the other hand, the same
location information is held at the different sites. VIP is an
example where the location information of an MH is held on
several intermediate gateways. Although it can give an
optimal routing path in the normal case, when a communicating MH migrates, the inconsistent location information
may be spread over several intermediate gateways.
In order to support mobility management in a distributed
manner, the virtual cell takes advantage of the distributed
hierarchical location information which involves a combination of partitioning, duplication, and centralization, as
shown in Fig. 6. GMT is partitioned between virtual cells.
On the other hand, IMT is partitioned with other IMTs and
duplicated with GMT in a virtual cell. EMT partially duplicates a part of GMT for MHs in different physical cells in
the same virtual cell so that address resolution and location
tracking for data transfer are first tried to accomplish among

BSs independent of the ALS. Thus, GMT is only referred in
case that they cannot be solved among BSs.
It should be noted that there is another conceptual hierarchy above the GMT level. When MH 1 migrates from its
native virtual cell A to virtual cell B, forwarding pointers are
maintained from ALS A to ALS B to the current BS at the VCP
layer. From the prospective of the IP layer, however, MH 1 is
regarded as if it were in its native virtual cell. Thus, when a
remote fixed host or an MH in a third network wants to send
a packet to MH 1, the existing routing protocols of the IP
layer correctly deliver the packet to ALS A. Then ALS A traces
MH 1 using the forwarding pointers. Therefore, the IP network address of a migrating MH represents at least the nearlocation information of the MH, and when coupled with the
existing routing protocols of the IP layer it can give the same
effect that we maintain a conceptually centralized server for
the whole mobile network. If MH 1 migrates within a virtual
cell, the IP network address of MH 1 gives the exact location
information.
4.2. Impact of mobility
The location registration due to the handover procedure is
a main source of inconsistent location information. To
achieve the consistency of distributed information is not
trivial at all and many researchers have been working on
this problem in fixed network environments [13–15]. In the
virtual cell environment, however, host mobility is integrated with fixed networks, which makes the problem
even more complex. To solve this problem, the handover
procedure should utilize the multicasting ability of the base
station network.

Fig. 7. An example of the inconsistency problem.

K. Lim et al./Computer Communications 20 (1997) 586–598

593

Fig. 8. A communication model for the handover procedure.

Assuming that the distributed location information is
initially consistent, consider that MH 1 moves from BS 4 to
BS 1 which periodically broadcasts its identity, as shown in
Fig. 7. When MH 1 receives a beacon packet, it sends a
greeting message containing its identity to BS 1. BS 1 detects
the entry of MH 1 to its local cell, deletes its corresponding
EMT entry if exists, and inserts a new IMT entry. Then, BS 1
multicasts a location registration message to every BS so
that it maintains the consistent location information for
MH 1. However, some BSs may fail to receive the message
due to communication errors and buffer overflows. The long
propagation delay also has a similar effect as the message
loss at a point in time due to mobility. To explore a practical
solution, we should begin with the assumption that the base
station network supports the atomic multicasting by using
an existing protocol such as the Trans protocol [15]. The
basic idea of the Trans protocol is that acknowledgements
for multicast messages are piggybacked on messages that
are themselves multicast, using a combination of positive
and negative acknowledgement strategies. This piggyback
feature is suitable for the handover procedure because in the
steady state, if some MHs move out then there will be a high
possibility that other MHs move in.
Note that although the atomic multicast is supported, the
message loss can still occur if BS 1 multicasts the location
registration message. Assume that BS 3 failed to receive it
and BS 4 received it with a long message latency. Then, the
following four cases can happen when other MHs transmit a
message to MH 1 during the process of the atomic multicast.
1. If BS 4 has the old location information for MH 1, the
message transmitted from MH 4 to MH 1 will be lost.
2. If BS 4 has the new location information for MH 1, the
message transmitted from MH 4 to MH 1 will be received.
3. Because BS 3 has the old location information for MH 1,
the message transmitted from MH 3 to MH 1 will be lost or
received, depending on what value BS 4 has. If it has the
old value, the message will be lost. Otherwise, the
message will be redirected to BS 1 by BS 4.
4. Because BS 2 has the new location information for MH 1,

the message transmitted from MH 2 to MH 1 will be
received.
From the above observation, we can see that at least the
previous base station BS 4 must have the new location
information for MH 1 as soon as possible in order to avoid
a possible message loss or to mitigate the effect of long
message latency.
4.3. Handover procedure
The new BS informs the previous BS of an MH’s migration using point-to-point communication immediately after
detecting the MH’s identity so that all messages received at
the previous BS during the handover procedure can be
correctly redirected to the new BS. Next, the previous BS
is responsible for maintaining the consistency of the MH’s
location information at all BSs using an atomic multicasting. Note that the ALS is excluded from the multicasting
group. If the ALS is involved, every movement of the MH
will need an access to the ALS and there is no difference
from the centralized system. However, it may cause another
inconsistency problem between the ALS and BSs because
the ALS may have old location information for some MHs.
Thus, the previous BS periodically backups the location
information updated during a predefined time interval to
the ALS using point-to-point communication. When an
MH continues to move to different physical cells, there
can be a redirection chain with multiple hops from the
first BS to the current BS. However, the chain is eliminated
when the newest location information is received through an
atomic multicasting.
Fig. 8 shows a communication model for the handover
procedure. For convenient description, we define the following notations where the VCP header information is omitted.
•

A ⇒ B,..., N: {frame} implies that A multicasts {frame}
to B,..., N over the base station network, where {frame}
consists of {source address, destination address l frame
data}.

594

•
•
•
•
•

K. Lim et al./Computer Communications 20 (1997) 586–598

A → B: {frame} implies that A sends {frame} to B via a
point-to-point link.
A w B: {frame} implies that A broadcasts {frame}
whose destination is B via radio links.
IP(A) implies the IP address of an MH A.
RMAC(A) implies the radio MAC address of A, where A
may be a BS or an MH.
BMAC(A) implies the address of A in the base station
network (BMAC), where A may be a BS or an ALS.

4.3.1. Within a virtual cell
Consider that MH a moves from BS 2x to BS nx. In addition to its own addresses, IP(MH a) and RMAC(MH a),
MH a initially keeps the current base station addresses,
RMAC(BS 2x) and BMAC(BS 2x).
(1) Bs nx w MH a: {RMAC(BS nx), broadcast address l
BMAC(BS nx)}
MH a receives a beacon including BMAC(BS nx) from
BS nx, decides to switch from BS 2x to BS nx, and updates
its base station address from RMAC(BS 2x) and
BMAC(BS 2x) to RMAC(BS nx) and BMAC(BS nx),
respectively.
(2) MHa w BSnx : {RMACðMHa Þ, RMAC(BS nx) l
IP(MH a), BMAC(BS 2x)}
BS nx receiving a greeting message from MH a processes
its IMT and EMT using the source address
RMAC(MH a) as a key. If the EMT entry for MH a is
found, the entry is deleted and then the IMT entry for
MH a is added. IP(MH a) in the EMT entry is used for
fast address resolution, as described in the VCARP
section later.
{BMAC(BS nx),
BMAC(BS 2x)
(3)
BSnx → BS2x :
lIP(MH a), RMAC(MH a), BMAC(BS nx)}
BS 2x receiving the notification of MH a’s migration
sends an acknowledgement to BS nx and then processes
IMT and EMT. The IMT entry for MH a is deleted and
the new EMT entry for MH a is added.
(4) BS 2x ⇒ BS 1x,..., Bs nx: {BMAC(BS 2x), multicast
address l IP(MH a), RMAC(MH a), BMAC(BS nx)}
Every BS receiving the notification of MH a’s migration
except BS nx processes its EMT. If the EMT entry for MH a
already exist, the entry is updated.
4.3.2. Between virtual cells
Consider that MH a moves from BS nx to BS 1y. In addition
to its own addresses, IP(MH a) and RMAC(MH a), MH a keeps
the current base station addresses, RMAC(BS nx) and
BMAC(BS nx).
(1) BS1y w MHa : {RMAC(BS 1y), broadcast address l
BMAC(BS 1y)}
(2) MHa w BS1y : RMACðMHa Þ, RMAC(BS 1y) l
IP(MH a), BMAC(BS nx)}
BS 1y receiving a greeting message from MH a detects

from the previous base station address BMAC(BS nx)
that MH a was in another virtual cell. BS 1y creates the
IMT entry for MH a.
(3) BS1y → ALSY : {BMACðBS1y Þ, BMACðALSY Þ
lIP(MH a), RMACðMHa Þ, BMACðBS1y Þ, BMACðBSnx Þ}
ALS Y receiving a location registration message creates
the GMT entry for MH a using IP(MH a), RMAC(MH a),
and BMAC(BS 1y). Then, ALS Y knows that BS nx is a base
station in virtual cell X, using BMAC(BS nx).
(4) ALSY → ALSX : {BMAC(ALS Y), BMAC(ALS X)
lIPðMHa Þ, RMACðMHa Þ, BMACðALSY Þ, BM ACðBSnx Þ}
ALS X receiving a location registration message updates
the GMT entry for MH a using IP(MH a), RMAC(MH a),
and BMAC(ALS Y). This entry serves as a forwarding
pointer at the VCP layer if ALS X is the ALS of MH a’s
native virtual cell. If it is not, ALS X informs the ALS of
MH a’s native virtual cell of MH a’s migration to virtual
cell Y so that a forwarding pointer is maintained from
MH a’s native virtual cell to virtual cell Y.
(5) ALSX → BSnx : BMACðALSX Þ, BMACðBSnx Þ
lIP(MH a), RMAC(MH a), BMAC(ALS X)}
BS nx receiving the notification of MH a’s migration
sends an acknowledgement to BS 1y in the reverse direction and then processes IMT and EMT. The existing
IMT entry is deleted and an EMT entry using
IP(MH a), RMAC(MH a), and BMAC(ALS X) is added.
(6) BSnx ⇒ BS1x , BS2x ; :::; BSðn ¹ 1Þx : {BMAC(BS nx),
multicast address l IP(MH a), RMAC(MH a),
BMAC (ALS X)}
Every BS receiving the notification of MH a’s migration
except BS nx manipulates its EMT. If the EMT entry exists, it
is updated.
4.4. Address resolution
VCARP is designed to get the physical address of the MH
in a remote physical cell of the virtual cell and should satisfy
at least the following three requirements. First, in order to
achieve the compatibility with ARP, VCARP must be
shielded from MHs as if they were in a physical cell.
Second, because the VCARP packet latency can directly
affect the scalability of a virtual cell, a distributed address
resolution mechanism should be considered rather than
broadcasting or a centralized solution. Third, when an MH
moves to an adjacent physical cell immediately after sending an ARP request and further moves to another physical
cell, the ARP reply may be lost and then the ARP request
may be repeatedly generated. Hence, VCARP must also
support host mobility.
Fig. 9 shows the ARP/VCARP packet format. The ARP
packet follows exactly the same format as the existing ARP
standard and has longer fields SENDER HA and TARGET
HA in order to make it possible to accommodate the use of
hierarchical 64-bit E.164 network addresses in radio
networks. The VCARP packet format has an additional

K. Lim et al./Computer Communications 20 (1997) 586–598

595

request. As soon as the destination MH receives the ARP
request, it responds with an ARP reply that includes the
requested physical address of the destination. At the same
time, the BS that received the ARP request searches IMT
and knows that the destination MH is in the local physical
cell so it discards the ARP request.

Fig. 9. The ARP/VCARP packet format.

64-bit field BASE HA which is used to convey the location
information which is expected to be used for data transfer in
the very near future. The source MH performs ARP only
when it resides in its native virtual cell and the destination
MH belongs to the same native virtual cell. Otherwise, it
directly transmits IP packets without performing ARP.
4.4.1. Within a physical cell
The procedure is the same as in the normal ARP. The
same ARP request/reply packet formats are used where an
8-bit hardware address length field allows one to accommodate arbitrary radio network addresses. The IP protocol
of the source MH checks the destination IP address with its
address resolution table. If the address binding is successful,
the source MH uses the RMAC address to transfers datagrams. Otherwise, the source MH broadcasts an ARP

4.4.2. Within a virtual cell
Fig. 10(a) shows the flow of messages when MH 1 broadcasts an ARP request to get the physical address of MH 2 and
it is resolved at BS 1. BS 1, which received the ARP request,
searches its EMT. The EMT entry for MH 2 already contains
its physical address and BS 1 directly sends an ARP reply
within the physical cell. At the same time, BS 1 sends a
VCARP reply to BS 2. Then, BS 2 transforms the format of
the VCARP reply to that of the ARP reply by stripping out
the field BASE HA and broadcasts the transformed ARP
reply in its local cell. This VCARP reply also conveys the
location information of MH 1 to BS 2 through the field BASE
HA which contains the network address of BS 1. Then, BS 2
creates an EMT entry so that MH 2 can directly transmit data
to MH 1 without any additional address resolution in the near
future communication. It is based on the observation that
computer communication is usually bidirectional; if MH 1
has some reason to talk to MH 2, then MH 2 will probably
have some reason to talk to MH 1.
Fig. 10(b) shows the flow of messages when the address
resolution is accomplished at the ALS. BS 1 sends a VCARP
request to the ALS because it has no entry for MH 2. Then,
the ALS sends a VCARP reply to BS 1 after resolving MH 2’s

Fig. 10. The VCARP flow diagram.

596

K. Lim et al./Computer Communications 20 (1997) 586–598

address binding using GMT. At the same time, the ALS also
sends BS 2 a VCARP reply with the same reason as in the
above case. The location information of MH 1 and MH 2 is
also conveyed via both VCARP replys to establish a logical
bidirectional link between them. Now we describe the
support of host mobility in VCARP. Consider the case
that MH 1 requested an address resolution to send datagrams
to MH 2 and then moved to a third BS n before the VCARP
reply is arrived. By the handover procedure, BS 1 knows that
MH 1 currently belongs to BS n. As soon as BS 1 receives the
VCARP reply, it redirects the packet to BS n.
4.5. Data transfer
For the transmission of packets between virtual cells and
fixed networks, we have already described in the previous
sections. This section deals with the packet transmission
within a physical cell and within a virtual cell. Let BS ij
denote the ith BS of virtual cell j in Fig. 6, where i ¼
1,...,n when j ¼ A and i ¼ 1,..., m when j ¼ B. We assume
that MH ij belongs to BS ij and M x initially belongs to BS 1A.
We also assume that the address binding is achieved by
VCARP.
4.5.1. Within a physical cell
Consider the case that MH 1A wants to send a packet to
MH x. By the address resolution protocol, MH 1A knows the
RMAC address of MH x and directly broadcasts the packet to
it. BS 1A knows that MH x resides in its physical cell by checking IMT using the RMAC address of MH x as a key, and
discards the received packet.
4.5.2. Within a virtual cell
Consider the case that MH x moves from BS 1A to BS 2A and
each of MH 1A, MH 2A, and MH nA wants to send a packet to
MH x. By the handover procedure, a forwarding pointer is set
from BS 1A to BS 2A at the EMT entry of BS 1A and the packet
from MH 1A is correctly delivered to MH x by the forwarding
pointer. The packet from MH 2A is directly delivered to MH x
and BS 2A discards it because the IMT entry for MH x is
found. The delivery of the packet from MH nA has three
cases. If the EMT entry for MH x is found before the completion of BS 1A’s multicasting operation, the packet will be
delivered to BS 2A via BS 1A. If the EMT entry is found after
the completion of its multicasting operation, the packet will
be directly delivered to BS 2A. If the EMT entry is not found,
the first packet from MH nA will be delivered to BS 2A through
the ALS. At the same time, the ALS generates a VCARP
reply in order to have BS nA learn that MH x is currently
belongs to BS 2A. It makes it possible to directly transfer
the subsequent packets following the first one to BS 2A without going through the ALS repeatedly.
4.6. VCP implementation
The virtual cell system is implemented as a distributed

application that uses TCP/IP as a transport mechanism.
There are five types of distributed network component processes, each of which corresponds to a network component
in the virtual cell system: mobile host process, physical cell
process, base station process, base station network process,
and ARP/location server process. The physical cell process
simulates the broadcast ability of a physical cell while the
base station network process simulates a full mesh of logical
point-to-point links with the multicast ability. Both are
implemented as servers and the other processes are implemented as clients. The mobility and data traffic are generated at the mobile host process. When a client process is
initiated, the user defines a simulated IP address, the
destination machine and protocol port number of the corresponding server process(es). Because the socket address
structure contains an internet address and a protocol port
number to identify a communication endpoint of a TCP
connection, the , IP address, port number . pair is
used as an RMAC or BMAC address of a client process.
The purpose of this implementation is to demonstrate the
correct operations of VCP and measure data including the
protocol processing time of the VCP layer at a BS and an
ALS, which is used as parameters to a performance model
for the virtual cell system. A performance analysis of the
virtual cell system is presented in Ref. [16].

5. Discussion and conclusion
Considering the ubiquitous coverage of TCP/IP applications and the availability and flexibility of SMDS and ATM
networks for base station networking to support future
multimedia and mobile communications, the virtual cell
protocol has several advantages against IP-level mobile
host protocols. The use of high-speed datagram packetswitched networks for base station networking in the virtual
cell system resolves the difficulties arising from the use of
diverse underlying networks in internets in terms of the
efficiency of mobility management function and the complexity of network management function.
Moreover, because a virtual cell is a group of physical
cells with an arbitrary topology properly engineered so that
the intra-virtual cell traffic is as much larger than the intervirtual cell traffic as possible and the bridge function of base
stations is usually much faster than the gateway function, a
significant increase in the performance of the data transfer
and mobility management functions can be achieved. Given
mobility and data traffic patterns among physical cells, the
optimal deployment of virtual cells is described in Refs.
[17,18]. The objective is to minimize the total communication cost of mobility and data traffic for the global mobile
network where inter-virtual cell communication is more
expensive than intra-virtual cell communication. For highway cellular systems, we design an efficient optimal
partitioning algorithm of O(mn 2) by dynamic programming,
where m is the number of virtual cells and n is the number of

K. Lim et al./Computer Communications 20 (1997) 586–598

physical cells in the global mobile network [17]. For hexagonal cellular systems, we develop several heuristics for
multiway partitioning, based on the techniques of moving
or interchanging boundary physical cells between adjacent
virtual cells. These heuristics produce optimal partitions
with respect to the initial partitions obtained randomly or
by centering. The heuristics are compared and shown to
behave quite well through experimental testing and analysis
[18].
Preserving the interconnection level of base stations at the
data link layer also makes it possible to shield host mobility
from the IP layer, and using the base station network address
as a contact point of a mobile host makes it possible to avoid
acquiring a temporary address by some kinds of local
utilities whenever the mobile host migrates. Thus, the problems of interfering with the conventional IP layer and of
reserving a large amount of IP address space for location
information can be resolved.
There are two major issues related to the performance of
the virtual cell protocol. One issue is the load of the ARP/
location server. Within a virtual cell, a base station first tries
to perform address resolution and data transfer locally without the intervention of the ARP/location server, using its
two membership tables, IMT and EMT.
Thus, the hit ratio of the EMT search directly affects the
degree of referring to the ARP/location server when remote
mobile hosts in different physical cells are involved in communication. This issue directly addresses the size and
update scheme of EMT and is also closely related to the
IP traffic and host mobility pattern. A number of research
results on the internet protocol traffic analysis in a fixed
network environment reveals that certain hosts communicate more with one another than with other hosts [19–21].
Even in the virtual cell environment, the locality of destination hosts can give an important implication for the address
resolution and data transfer functions. A small ARP cache at
each mobile host can greatly reduce VCARP traffic within a
virtual cell and then address resolution should not severely
burden the size of EMT. If we do not consider host mobility,
the locality property also means that data transfer does not
severely burden the size of EMT. When considering host
mobility, the update scheme of EMT could be rather
important if we can properly deploy virtual cells based on
mobility and communication patterns among physical cells.
The other issue is the amount of multicast traffic among
base stations, which is generated only by the handover function of the virtual cell protocol. Because the multicasting
ability is supported by switches in the base station network,
each base station is not related to the load of generating the
multicast traffic, but the bandwidth usage of the base station
network is affected by it. The relatively short length of the
handoff message should not severely affect the highbandwidth base station network and the handoff message
response time is slightly increased in a reasonable range.
This is addressed quantitatively based on an analytical performance model of the virtual cell system in Ref. [21],

597

adopting an open multiple class queueing network model.
A computer simulation work will be conducted to compare
with the analytical results in the future research.

Appendix A List of Acronyms
ALS
ARP
ATM
BMAC
BS
CPS
EMT
GMT
IMT
IP
IPIP
LAN
LLC
L3_PDU
MAC
MH
MSG
PID
RMAC
SMDS
SNAP
SNI
TCP
VCARP
VCP
VIP

address resolution protocol/location server
address resolution protocol
asynchronous transfer mode
base station network medium access control
base station
cellular packet switch
external membership table
global membership table
internal membership table
internet protocol
internet protocol within internet protocol
local area network
logical link control
level 3 protocol data unit
medium access control
mobile host
mobile support gateway
protocol identification
radio network medium access control
switched multimegabit data service
sub-network access protocol
subscriber network interface
transmission control protocol
virtual cell address resolution protocol
virtual cell protocol
virtual internet protocol

References
[1] J. Ioannidis, D. Duchamp, G.Q. Maguire Jr., IP-based protocols for
mobile internetworking, ACM SIGCOMM’91, 1991, pp. 235–245.
[2] F. Teraoka, Y. Yokote, M. Tokoro, A network architecture providing
host migration transparency, ACM SIGCOMM’91, 1991, pp. 209–
220.
[3] F. Teraoka, K. Claffy, M. Tokoro, Design, implementation, and
evaluation of virtual internet protocol, IEEE DCS’92, 1992, pp.
170–177.
[4] C. Perkins, Y. Rekhter, Short-cut routing for mobile hosts, IETF draft
RFC, T.J. Watson Research Center, IBM Corp., July 1992.
[5] C. Perkins, Y. Rekhter, Support for mobility with connectionless
network layer protocols, IETF draft RFC, T.J. Watson Research
center, IBM Corp., January 1993.
[6] H. Wada, T. Ohnishi, B. Marsh, Packet forwarding for mobile hosts,
IETF draft RFC, Matsushita Corp., November 1992.
[7] M. Kawarasaki, B. Jabbari, B-ISDN architecture and protocol, IEEE J.
Select. Areas Commun. 9(9) (1991) 1405–1415.
[8] J.-Y. Le Boudec, The asynchronous transfer mode: a tutorial, Comp.
Networks ISDN Systems 24 (1992) 279–309.
[9] D. Piscitello, J. Lawrence, The transmission of IP datagrams over the
SMDS service, IETF RFC 1209, 1991.
[10] G.H. Clapp, LAN interconnection across SMDS, IEEE Network Mag.
5 (5) (1991) 25–32.
[11] D.M. Piscitello, M. Kramer, Internetworking using switched multimegabit data service in TCP/IP environments, Comp. Networks ISDN
Systems (1991) 62–71.
[12] F.R. Dix, M. Kelly, R.W. Klessig, Access to a public switched

598

[13]
[14]
[15]

[16]

[17]

[18]
[19]

[20]

[21]

K. Lim et al./Computer Communications 20 (1997) 586–598
multi-megabit data service offering, Comp. Networks ISDN Systems (1991) 46–61.
S.B. Davidson, Consistency in partitioned networks, ACM Computing
Surveys 17(3) (1985) 341–370.
J.-M. Chang, N.F. Maxemchuk, Reliable broadcast protocol, ACM
Trans. Computer Systems 2(3) (1984) 251–273.
M. Melliar-Smith, L.E. Moser, V. Agrawala, Broadcast protocols for
distributed systems, IEEE Trans. Parallel Distributed Systems 1(1)
(1990) 17–25.
K. Lim, Y.-H. Lim, Y.-H. Lee, A performance analysis of the virtual
cell system for mobile computer communications, Proc. 10th Int.
Conf. on Information Networking ICOIN-10, January 1996, pp.
551–560.
K. Lim, Y.-H. Lee, Optimal partitioning of heterogeneous traffic
sources in highway cellular systems, IEEE Personal, Indoor and
Mobile Radio Communications PIMRC’94, September 1994, pp.
1237–1241.
K. Lim, Y.-H. Lim, Y.-H. Lee, Heuristics for multiway partitioning in
hexagonal cellular systems, IEEE ICC’95, June 1995 pp. 1839–1843.
A. Schmidt, R. Campbell, Internet protocol traffic analysis with
applications for ATM switch design, ACM SIGCOMM’92, 1992,
pp. 39–52.
R. Caceres, P.B. Danzig, S. Jamin, D.J. Mitzel, Characteristics of
wide-area TCP/IP conversations, ACM SIGCOMM’91, 1991, pp.
101–112.
M. Khalil, J.C. Hand, M. Mariswamy, Analysis and traffic characterization of a wide area network, IEEE ICC’93, 1993, pp. 1829–1835.

Kyungshik Lim received his M.S degree in
Computer Science from the Korea Advanced
Institute of Science and Technology, Seoul,
South Korea, in 1985 and his PhD. degree in
Computer and Information Sciences from the
University of Florida, Gainesville, FL, in
1994. Since February 1985, he has been a
principal member of the engineering staff
and the head of the Computer Communications Section of the Electronics and Telecommunications Research Institute, Taejon,
Korea. His research interests include mobile
computing, wireless networks, high-speed communications networks,
and parallel and distributed systems.

Young-Hwan Lim received a B.S. degree in
Mathematics from the Kyungpook National
University, Taegu, South Korea, in 1977 and
an MS degree in Computer Science from the
Korea Advanced Institute of Science and
Technology, Seoul, South Korea, in 1979.
He received his Ph.D. degree from the
Northwestern University in 1985. Since
January 1979, he has joined the Electronics
and Telecommunications Research Institute
as a director of the research staff. He
received a Ph.D. His research interests
include multimedia systems, high-speed information processing and
networking software, and artificial intelligence.

Yann-Hang Lee received his Ph.D. degree in
Computer, Information, and Control Engineering from the University of Michigan, Ann
Arbor, MI, in 1984. From December 1984 to
August 1988, he was a research staff member
at the Architecture Analysis and Design
Group, IBM T.J. Watson Reserach Center,
Yorktown Heights, NY. Since August 1988,
he has been an associate professor with the
Computer and Information Sciences Department, University of Florida, Gainesville, FL.
His current research interests include realtime systems, communication networks, computer architecture, and
performance evaluation.

An Efficient Scheduling Discipline for Packet
Switching Networks Using Earliest Deadline First
Round Robin*
Deming Liu and Yann-Hang Lee
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287, USA
{dmliu, yhlee}@asu.edu
sufficient for the traffic and allow no bursty traffic to disrupt
the service performance temporarily. On the other hand,
service discipline is the most significant part to ensure the
guaranteed performances of packet-based switches.

Abstract—In this paper we propose a frame-oriented
scheduling discipline, EDF-RR (earliest-deadline-first roundrobin), for OQ (output-queued) switch architecture and data
traffic consisting of fixed-length cells. Bandwidth reservation for
an active session is performed by holding a number of cell slots
for the session in frames. Each cell that is going to be transferred
in a frame is assigned a virtual release time and a virtual
deadline according to the bandwidth reservation scheme. The
transmitting order of the cells in frames is thus determined by
non-preemptive non-idling EDF algorithm so that cells of a
backlogged session in frames are distributed as uniformly as
possible. Through the analysis applying real-time scheduling
theory and network calculus as well as network simulation, EDFRR takes the advantage of low computational complexity, and
possesses tight delay bounds and lenient buffer requirements.
The proposed scheduling discipline is appropriate for distributed
real-time systems as we show that sessions can be configured
based on message traffic models and deadline requirements.
Also, a modified version of EDF-RR, called EDF-DRR, can be
applied as traffic regulator when jitter requirements exist among
active sessions.

Typical QoS-guaranteed service disciplines are based on
rate allocation implemented by packet scheduling algorithms.
GPS (generalized processor sharing) is the ideal rate allocating
discipline with absolute fairness and, however, cannot be
implemented in practice since it is based on pure flow model
with the assumption of infinite divisibility of data traffic.
However, it is often used as the benchmark to evaluate the
characteristics of practical service disciplines. Basing on OQ
(output-queued) switches, a number of service disciplines [1]
have been proposed aiming at providing deterministic
communication performance. All these disciplines can be
regarded as PGPS (packet based GPS) [6] in a wide sense. The
design objectives must address the issues of computational
complexity and bound tightness. Low computational
complexity is preferred since a switch scheduler must work at
line speed. At the same time, real-time traffic may have
relatively stringent performance requirements desiring tight
QoS bounds. Generally there are two kinds of service
disciplines, RR-based (round-robin) and deadline-based. RRbased disciplines such as Weighted Round-Robin (WRR) [10],
Deficit Round-Robin [4] and Elastic Round-Robin [5] have
the low computational complexity as O(1) whereas they
cannot provide satisfying worst-case performance bounds.
Deadline-based service disciplines such virtual clock [11],
WFQ (weighted fair queuing) [3], WF2Q (worst case fair
weighted fair queuing) [8], WF2Q+ [15] and delay-EDD
(earliest due date) [9] can provide relatively tighter QoS
bounds and, however, require higher computational
complexity.

I. INTRODUCTION
Distributed real-time systems are experiencing tremendous
advancement driven by new technologies such as powerful
computing devices, microelectronics, and high speed networks
etc. In addition, transporting more data traffic with hard realtime requirements becomes a necessity for many critical
applications.
QoS can be generally specified as demands in throughput,
delay, jitter and loss rate. Currently packet switching network
is the dominating approach to provide performanceguaranteed services. In packet switching networks, data traffic
is transmitted along connections established between source
and destination nodes rather than routing as does in IP
networks. The strategies to guarantee communication
performances include bandwidth reservation, traffic
regulation, and packet service discipline with deterministic
performances. The first two establish a service capability

Although most schedulers support variable-length packets,
switching networks supporting only fixed-length packets excel
at transferring performance-guaranteed traffic. Service
disciplines supporting fixed-length transmission units, in
which a long packet is partitioned into short fixed-length cells
and cells are the basic transmission unit as is done in ATM
networks, take the advantage of fairness in sharing bandwidth.
In fact, dividing long packets into short fixed-length cells

* This work was sponsored in part by the Federal Aviation Administration
(FAA) via grant DTFA03-01-C-00042. Findings contained herein are not
necessarily those of the FAA.

0-7803-7945-4/03/$17.00 (C) 2003 IEEE

5

cell slot in a frame belongs to one of the K sessions, obviously
the following equation is true.
K
(1)
m / n =1

avoids long waiting times for urgent backlogged sessions and
thus enhances fairness and latency performance.

∑

In this paper we propose an approach combining RR
(round robin) and EDF (earliest deadline first) scheduling in
order to trade off computational complexity and tightness of
performance bounds for OQ switches. The proposed service
discipline, called EDF-RR (earliest-deadline-first roundrobin), adopts a round robin scheduling for repetitive frames
and uses EDF to determine the transmission order of fixedlength cells within a frame. The scheduling order and frame
structure are set up when sessions are initiated. When a
session has no backlog, the assigned cell slots can be skipped
in aid of maximizing bandwidth utilization. Our analysis will
show that the approach possesses a low computational
complexity and a tight delay bound.

i

i =1

With a simple transform we have
K

∑1 / p

=1

i

(2)

i =1

where we define session period of session i as pi = n / mi . We
try to transfer a session i cell in every pi . Note that pi is not
necessary to be an integer and 1/pi is the bandwidth reserved
for session i (the fraction of a cell slot that is allocated to
session i in average).
In the next step we will determine the transmitting order of
the n cells belonging to the K sessions in a frame by nonemptive non-idling EDF (earliest-deadline-first) algorithm.
EDF is widely used in scheduling multiple tasks in a
processor-sharing environment. It is a dynamic-priority
scheduling algorithm in that priorities are assigned to tasks
according to the deadlines of their current instances. A task is
assigned to the highest priority if its deadline is the earliest
among all pending tasks. A task, whose virtual requests occur
periodically and whose executions are required to complete in
a regular interval after their requests, is called periodic task.
The regular interval is called relative deadline for the
corresponding task. A task is preemptive if an executing
instance of a task is interrupted by a task request with higher
priority and resumes running later. It is non-preemptive
otherwise. A scheduling algorithm is non-idling if the
processor cannot be idle as long as there exists some pending
task.

The rest of this paper is organized as follows. Section II
gives a description of the proposed EDR-RR discipline and its
essential characteristics. In Section III, we discuss the delay
bounds and buffer requirements of EDF-RR for both singlenode and multiple-node cases. Section IV presents some
simulation results. In Section V, several application-related
issues are addressed. Finally, Section VI concludes the paper.
II. EARLIEST DEADLINE FIRST ROUND ROBIN SCHEDULING
We expect the communication mechanism in distributed
real-time systems to be composed of many end nodes and
switch nodes. End-to-end sessions are established prior to
packet transmission. When a packet from a source node is
ready to be transmitted following the path of an established
session, it is partitioned into fixed-length cells that are sent to
the network respecting a traffic regulation mechanism, such as
a leaky-bucket model. According to the traffic volume and
deadline requirement, each session must reserve a minimal
amount of bandwidth along its path. Thus, transmission slots
will be assigned to backlogged sessions at each switch node.
Arriving cells, which may be queued if the corresponding
buffer is backlogged, are delivered to the corresponding
output links subject to the scheduling discipline. For the
convenience of the following analysis, we assume a cell
arriving at the very beginning of a cell slot in the form of
pulse.

The schedulability of K periodic tasks with preemptive
EDF algorithm is described by the well-known result as
follows [2][12]. For a given set of K periodic tasks whose
relative deadlines are equal to their periods, they can be
scheduled if and only if
K

∑e / p
i

i

≤1

(3)

i =1

where pi and ei are the period and execution time, respectively,
of task i.
If preemption is not allowed in scheduling a set of tasks,
the conclusion holding for the EDF schedulability is not true
any more since non-preemption may cause priority inversion.
Priority inversion means that a task with a higher priority is
waiting for execution while a lower-priority task is running.
The priority inversion delay introduced by non-preemption
can be analyzed by the following lemma.

EDF-RR (earliest deadline first round robin) proposed in
this paper is a frame-based RR scheduling discipline for fixedlength cell traffic. Similar to WRR and WFQ, EDF-RR is a
weight-based service discipline. A frame is composed of a
number of cell slots. A session reserves a portion of
bandwidth by occupying a number of cell slots in a frame.
Instead of transferring in an arbitrary order the cells reserved
by all active sessions in frames, EDF-RR tries to select an
order such that the cells associated with an active session are
distributed in frames uniformly.

Lemma 1. If a set of periodic tasks with relative deadlines
equal to their periods are schedulable by preemptive EDF
algorithm, then any task instance can finish its execution
within (p + emax) after it is released under a non-preemptive
non-idling EDF algorithm, where p is the task’s period and
emax is the maximum execution time of all tasks.

We define that a frame consists of n fixed-length cells. A
cell has, for convenience, the length of 1 in terms of the time it
takes to transmit a cell from a session buffer to the
corresponding output port within an OQ switch. In other
words, the length of a cell slot is 1 unit time. Let K be the total
number of active sessions and mi be the number of cells shared
by session i in a frame, where 1 ≤ i ≤ K. Assuming that each

Letting ei equal to 1 (1 ≤ i ≤ K) in (3) and then comparing
(2) with (3), we may schedule the transmission of the cells in
every frame as if we schedule synchronous periodic tasks by
preemptive EDF. Thus we can conclude that exactly one cell

6

theorem gives the minimal
guaranteed for each session.

(it may be broken with preemption allowed) of session i can
be transferred in each of session i periods with the first session
period starting from the beginning of a frame. However, since
a cell is an indivisible transmission unit that the scheduler
must deal with atomically, preemption is impracticable in
scheduling session cells. Therefore, in our EDF-RR algorithm,
we use non-preemptive non-idling EDF scheduling the
transmitting order of cells in a frame. This may lead to
additional transmission latency from priority inversion. But
the finish time of delivering a cell can still be bounded. Below
we first describe the EDF-RR discipline and then give
conclusions addressing its characteristics.

If all K sessions are backlogged, the frame is
transferred repeatedly such that in every frame the cells
follow the order given in (a).

(c)

If there is no backlog for a session during the current
frame, their cell slots in a frame are skipped. The
remaining backlogged sessions are transferred in the
same order as in (a).

S i (0, t 2 ) − S i (0, t1 ) > (mi / n)(t 2 − t1 ) − 2 ,

where the busy interval of session i begins at time 0 and, t1
and t2 are any two time instants in the busy interval with t2 ≥ t1.
DELAY BOUND AND BUFFER REQUIREMENT ANALYSIS
OF EDF-RR
In this section, we will use network calculus, a mathematic
analysis tool for networks, to obtain delay bounds and buffer
requirements of EDF-RR for both single-node and multiplenode cases. Also an intuitive explanation is given for the
analysis results.
III.

To analyze delay bounds and buffer requirements, traffic
models must be established to specify session traffic
characteristics such as average rate and burstiness. (σ, ρ)
model is one of them [1]. A session traffic flow is said to
satisfy (σ, ρ) model if there are at most σ+ρt units of traffic
during any interval t. σ and ρ denote the burstiness and
average rate of the traffic respectively. For example, traffic
flow coming from the traffic regulator of leaky bucket
satisfies (σ, ρ) model. According to the definition of arrival
curve, the statement that a traffic flow satisfies (σ, ρ) model
has the same meaning that the traffic flow is constrained by
arrival curve σ+ρt. In this paper we assume session traffic has
arrival curve σ+ρt and traffic unit is fixed-length cell. In the
following text we do not distinguish server and switch by the
convention of network calculus literature. The results in this
section are obtained by applying Theorem 1 and network
calculus. Please refer to [7][13] for the concepts and results of
network calculus used here.

Table I shows an example that gives the cell transmission
order of a 10-slot frame in which, sessions 1, 2 and 3 shares 5,
3 and 2 cell slots respectively. The numbers in the table denote
the relevant sessions.

Lemma 5. If session i occupies mi cell slots in an n-cell frame
passing through an EDF-RR server, the EDF-RR server offers
the service curve (mi / n)t − 2 for session i.

TABLE I. SCHEDULING ORDER IN A FRAME
1

2

1

3

1

2

1

2

1

capacity

Theorem 1. In any busy interval of session i scheduled with
EDF-RR, we always have

EDF-RR DISCIPLINE
(a) An n-cell frame is partitioned among K active sessions
(all unused bandwidth can be considered as one
dummy session) such that session i (1 ≤ i ≤ K) transfers
mi cells in the frame. Session i is assumed to have cell
transmission requests at time jpi (suppose a frame
starts from time 0) with corresponding deadlines at
(j+1)pi, where pi = n / mi and j = 0, 1, 2, … mi -1
(well will call those requests and deadlines as virtual
requests and virtual deadlines respectively). The
transmission order of the n cells in the frame is
determined by non-preemptive non-idling EDF
algorithm.
(b)

communication

In the single-node case in which a session traffic flow goes
through an EDF-RR server, we have Theorems 2 and 3
addressing delay bounds and buffer requirements respectively.

3

Lemma 2. There is no idle time in a frame if all K active
sessions are backlogged with EDF-RR.

Theorem 2. If session i traffic flow is constrained by arrival
curve σ i + ( m i / n )t , the delay it experiences passing through

A work-conserving algorithm means that there is no idle
time as long as there are some backlogged sessions. From
EDF-RR discipline and Lemma 2, obviously EDF-RR is a
work-conserving discipline.

an EDF-RR server is not more than (σ i + 2)(n / mi ) cell slots.
Theorem 3. If session i traffic flow constrained by arrival
curve σ i + (mi / n)t passes through an EDF-RR server without
buffer overflow, the buffer size that the server needs for
session i is not more than σi +2 cells

Lemma 3. With EDF-RR, for any backlogged session i, 1 ≤ i
≤ K, mi cells can be completely transferred within an n-cell
frame.

Now we consider the multiple-node case in which a
session traffic flow traverses multiple EDF-RR servers. We
define bandwidth share of session i in an EDF-RR server as
mi / n if session i passing through the EDF-RR server occupies
mi cell slots in an n-cell frame. A minimum bandwidth share
server for session i is the EDF-RR server who has the

Lemma 4. With EDF-RR, if a session has a virtual request at
time t for a backlogged session, a cell from the session will be
transmitted within [t, t+p+1], where p is the session’s session
period.
Let Si (a, b) be the number of cells transferred in the
interval [a, b] for session i under EDF-RR. The following

7

sequence of packets is generated at the source node and
destined to the destination node for each of the 4 sessions.
End-to-end delay from source to destination and queue length
at link C1C2 for session S1-D1 are observed and compared for
EDF-RR, WRR and WFQ.

minimum value of bandwidth share among the sequence of
EDF-RR servers that session i traverses.
Lemma 6, Theorems 4 and 5 as follows are based on the
assumption that session i passes through a number of EDF-RR
servers without any buffer overflow, the minimum bandwidth
share server for session i among the first k EDF-RR servers
allocates mi* cell slots in its n*-cell frame to session i, and the
session i traffic flow is constrained by arrival curve
(mi* / n* )t + σ i .

S1
10 M

S2

Lemma 6. The output flow from the kth sever for session i is
constrained by arrival curve (mi* / n*)t +σi + 2k .

C2

24 M

C3

2 4M

C4

1 5M

S3

C5

1 5M

C6

1 0M

D4

S4

10M

D2

10M

10 M

D3

Figure 1. The network topology for the simulation (the two numbers on
each link are channel bandwidth and the number of channels).

server for

TABLE II. WEIGHT ALLOCATION OF SESSIONS
FOR MULTIPLEX LINKS

Apparently if we ignore the computation for judging
whether a session buffer is empty or not, EDF-RR has the
computational complexity of O(1). On the other hand, it is
necessary to have empty buffer checking taken into account
from the viewpoint of implementation. In the worst case that
only one of K active sessions is backlogged, K-1 times of
empty buffer checking are needed during the transmission of a
frame according to the work-conserving property of EDF-RR.
Thus the computation complexity is O(K) instead of O(1) in
this scenario. The simple way to avoid this overhead is to
prevent the empty slots of the nonbacklogged sessions from
being applied by other backlogged sessions. But this strategy
would change the service discipline into nonwork-conserving,
thus wasting bandwidth. In the more efficient method, those
empty slots can be used to transfer traffic from best-effort
sessions. If there do not exist best-effort sessions, a set of
sessions can be assumed to be nonempty by some heuristic
approach. If some of the found sessions are really nonempty,
they will use those empty slots. Otherwise these empty slots
will be discarded. No mater the empty slots are employed or
not, the worst-case performance of real-time sessions will not
be impaired.

S1-D1
S2-D2
S3-D3
S4-D4
Frame size1

C1C2
25
34
15
46
120

C2C3
25
34
15
46
120

C3C4
25
34
15
46
120

C4C5
25
34
15
-74

C5C6
25
34
15
-74

In Scenario 1, we assume all packets along the 4 sessions
have the identical length, i.e. the cell length (100 bytes). The
same simulation loading with the same traffic is performed 3
times so that each time one of EDF-RR, WFQ and WRR is
applied to all multiplex links. The observed packet delay and
queue length of session S1-D1 for EDF-RR, WFQ and WRR
are shown in Figure 2(a) and Figure 2(b) respectively. In the
two figures we may pay attention to the fact that the curves of
EDF-RR and WFQ are almost completely overlapped. Thus
EDF-RR and WFQ have fairly close packet latency and buffer
requirement if they deal with fixed-length packets. On the
other hand, WRR exhibits larger fluctuation of packet latency
and queue length than EDF-RR and WFQ, thus inferrior to
both EDF-RR and WFQ.
5

2000
EDF-RR
WFQ
WRR

4.5

E DF-RR
W FQ
W RR

1800

4

1600

3.5

Packet delay (ms)

IV. SIMULATION RESULTS
In this section, we will compare EDF-RR, by simulation,
with other two weight-based service disciplines, WRR
(weighted round robin) [10] and WFQ (weighted fair queuing)
[3]. The network topology for the simulation is shown in
Figure 1. In the following simulation, there are 4 sessions
established in the network, denoted by source-destination
pairs, S1-D1, S2-D2, S3-D3 and S4-D4.

2 4M

1400

Queue length (bytes)

Theorem 5. The buffer size needed by the k
session i is not more than (σi+2k) cells.

C1

10M

Theorem 4. The delay that the session i traffic flow
experiences from the source to the kth server is not more than
(n* / mi* )(σ i + 2k ) cell slots.
th

D1
1 0M

10 M

3
2.5
2

1.5

1200
1000
800
600

1

400

0.5

200
0
130

From Figure 1, links CiCi+1 (i = 1, 2…5) are multiplexed
by more than one session. The weights allocated to multiple
sessions through the multiplex links are given in Table II.
These multiplex links can be scheduled according to EDF-RR,
WFQ or WRR. In addition to weights, frame sizes are required
to be specified when EDF-RR is applied to the multiplex
links. The frame sizes of EDF-RR for each multiplex link are
also given in Table II. In the 3 scenarios of simulation as
follows, exponential distributed traffic consisting or a

135

140

145
150
Sim ulation time (ms)

155

160

0
130

135

140

145
150
S im ulation tim e (m s )

155

(a) Packet delay curves
(b) Queue length curves
Figure 2. Packet delay and queue length curves for Scenario 1.

As the performance difference between WRR and EDFRR is apparent, comparing EDF-RR and WFQ is of our
interest in the next step. In Scenario 2, all packets along the 4

1

8

This parameter is only effective for EDF-RR.

160

share of the session (it does not matter if updates take place
very fast). Thus, when a new session starts up, a portion of the
bandwidth kept in the idle session can be assigned for it.
Similarly, when a session terminates, its bandwidth can be
merged into the idle session. Then, a new frame schedule is
computed and is started at a frame boundary. For the existing
sessions, their bandwidth shares are not changes and the delay
bounds are assured.

sessions are assumed to have the same length of 3 cells (300
bytes). For EDF-RR, packets are broken into cells before
entering the network and reconstructed by assembling cells at
destinations. For WFQ, packets are treated atomically. We are
concerned about the delays of packets instead of cells. The
packet delay curves of session S1-D1 are displayed in Figure
3(a). Figure 3(b) gives the queue length curves of session S1D1 on link C1C2. By comparing packet delay curves and queue
length curves, it is obvious that EDF-RR has better
performance than WFQ in terms of both packet latency and
buffer requirement in this scenario. As long packets are
“chopped” into small chunks, i.e. cells, EDF-RR provides
more fairness than WFQ.
4

Rescheduling transmission order is needed only when
there are sessions to be established, cancelled or updated,
which happen infrequently from the perspective of users. The
associated overhead can be ignored since a new transmission
order can be computed in parallel to the current transmission,
and is swapped at the next frame boundary.

1200
EDF-RR
W FQ

EDF-RR
WFQ

It is worthy to consider the application of EDF-RR in the
message communications of a distributed real-time system.
Consecutive messages are sent from a source application to a
corresponding destination application. Assume that each
message consisting of P cells is with a deadline requirement,
and will be routed through k switches to reach its destination.
To utilize the proposed EDF-RR scheme, a session of proper
bandwidth share and the required buffers must be established
to facilitate the message communication.

3.5
1000

Queue length (bytes)

Packet delay (ms)

3
2.5
2
1.5

800

600

400

1
200

0.5
0
300

310

320

330
340
350
Simulat ion time (ms)

360

370

0
340

380

342

344

346

348
350
352
Simulation time (ms )

354

356

358

360

(a) Packet delay curves
(b) Queue length curves
Figure 3. Packet delay and queue length curves for Scenario 2.

Let us keep the same assumptions as in Theorems 4 and 5.
We define the session delay for a packet as the interval
between the packet head entering the source node and the
packet completely leaving the destination. If a packet of
session i can be broken into P cells, the packet’s delay bound
along the k EDF-RR servers that session i travels through can
be expressed as (σ i + P −1 + 2k)(n* / mi* ) . As a result, the session
must be constrained by the following rules.

WFQ supports variable length packets. In Scenario 3, all
assumptions in Scenario 2 are kept except that session S1-D1
has packet length of 7 cells (700 bytes) and all other three
sessions, 6 cells (600 bytes). Packet delay and queue length
curves for session S1-D1 are shown in Figure 4(a) and Figure
4(b) respectively. From the curves in Figure 8, the properties
we observed in Scenario 2 still hold in Scenario 3.
Furthermore, since we increase packet length in Scenario 3,
the packet delay and queue length differences between EDFRR and WFQ are much larger than those of Scenario 2. We
may conclude that EDF-RR exhibits much better performance
than WFQ as packet size increases.
1400
5

EDF-RR
W FQ

E DF-RR
W FQ

4.5

3
2.5
2

800

600

400

1.5

200
1

400
Simulat ion time (ms)

2.

Session i is
σ i + (mi* / n * )t .

constrained

by

arrival

curve

In other words, bandwidth reservation for session i in a
server is performed by reserving mi cells of the n-cell frame.
The criteria of how to determine the frame size can be based
on the facts that (i) the frame size of a server should not be too
smaller, otherwise we cannot guarantee that sufficient
granularity for allocating bandwidth; and (ii) the computation
of rescheduling a frame increases as frame size increases.
Thus large frame size may introduce long session setting-up or
updating time.

1000
Queue length (bytes)

Packet delay (ms)

4

350

(σ i + P −1 + 2k)(n* / mi* ) ≤ D , where D is the message
deadline of session i.

1200

3.5

0.5
300

1.

450

0
360

365

370

375
380
385
Simulation time (ms)

390

395

400

(a) Packet delay curves
(b) Queue length curves
Figure 4. Packet delay and queue length curves for Scenario 3.

Besides meeting deadline requirement, the other concern
in real-time systems is message jitter. A session may require
that the message delivery jitter be minimized. This can be
accomplished if the message cells only use the scheduled slots
regardless any empty slots in a frame. Now let’s consider a
scenario of existing mixed types of sessions, in which some
sessions only need to meet their deadline requirements and
others have additional jitter constraints. Then the question
comes up whether we can let a set of backlogged sessions
share the empty cell slots in a frame and the other set of
backlogged sessions keep their original scheduled slots such

V. APPLICATIONS OF EDF-RR
EDF-RR is a service discipline for OQ switches applying
for connection-oriented network environment where fixedlength cell is the basic traffic unit. From the RR property of
this policy, its computational complexity is O(1). We must
note that determining a schedule in a frame involves deadline
comparisons in EDF-RR. Thus the results of Section III for
sure are valid only if a session is in steady state. We say a
session is in steady state if during an interval at each server the
session passes through there is no update to the bandwidth

9

possible. In other words, EDF-RR tries to simulate GPS with
the constraint of the fixed cell transmission order. Through the
analysis and simulation of the delay bound and buffer
requirement of EDF-RR for both single-node and multiplenode cases, we show that EDF-RR reveals high-performance
characteristics in terms of requirements of real-time
communications. Even though EDF-RR is a work-conserving
discipline, a modified version, called EDF-DRR, can support
additional jitter requirements for message sessions.

that all sessions’ delay bound and buffer requirement cannot
be deteriorated. We introduce an EDF-DRR (earliest-deadlinefirst dual round-robin) discipline that is intended to address
this requirement.
EDF-DRR DISCIPLINE
(a) Schedule a frame exactly the same as EDF-RR when
all active sessions are backlogged. We call this frame
as big frame. In addition to keep a big frame, EDFDRR maintains a dynamic small frame in which the
cell slots are composed of all the cell slots of currently
backlogged non-jitter-constrained sessions in the big
frame in the same order.

(b)

REFERENCES

The server iterates to transfer the cells according to the
schedule in the big frame. When a cell slot in the big
frame is empty, rather than skipping them to transfer
the next available cells in the big frame as does in
EDF-RR, EDF-DRR selects the next cell according to
the schedule in the small frame. If the size of the small
frame is zero, no cell is transmitted during the empty
slots.

The jitter property of EDF-DRR is given by Lemma 7.
Lemma 7. In any busy interval of jitter-constrained session i
with EDF-DRR, we always have
(t 2 − t 1 ) / p i − 2 < S i (0, t 2 ) − S i (0, t1 ) < (t 2 − t 1 ) / p i + 2 ,

[1]

Hui Zhang, “Service disciplines for guaranteed performance service in
packet-switching networks,” Proceedings of the IEEE, Vol. 83, No. 10,
Oct. 1995, pp. 1374-1396.

[2]

C. L. Liu and James W. Layland, “Scheduling algorithms for
multiprogramming in a hard-real-time environment,” Journal of the
Association for Computing Machinery, Vol. 20, No. 1, Jan. 1973, pp.
46-61.

[3]

Abhay K. Parekh and Robbert G. Gallager, “A generalized processor
sharing approach to flow control in integrated services networks: the
single-node case,” IEEE/ACM Transactions on Networking, Vol. 1, No.
3, June 1993, pp. 344-357.

[4]

M. Shreedhar and George Varghese, “Efficient fair queuing using deficit
round-robin,” IEEE/ACM Transactions on Networking, Vol. 4, No. 3,
June 1996, pp. 375-385.

[5]

Salil S. Kanhere, Harish Sethu and Alpa B. Parekh, “Fair and efficient
packet scheduling using elastic round robin,” IEEE Transactions on
Parallel and Distributed Systems, Vol. 13, No. 3, March 2002, pp. 324336.

[6]

A. Demeres, S. Keshav, and S. Shenker, “Analysis and simulation of a
fair queueing algorithm,” J. Internetworking Res. And Experience, Oct.
1990, pp. 3-26.

[7]

Jean-Yves Le Boudec, “Application of network calculus to guaranteed
service networks,” IEEE Transactions on Information Theory, Vol. 44,
No. 3, May 1998, pp. 1087-1096.

[8]

J. C. R. Bennet and H. Zhang, “WF2Q: worst-case fair weighted fair
queuing,” Proceedings of IEEE INFOCOM’96, CA, March 1996, pp.
120-128.

[9]

D. Ferrari and D. Verma, “A scheme for real-time channelestablishment in wide area networks,” IEEE Journal on Selected Areas
in Communication, April 1990, pp. 368-379.

where the busy interval of session i begins at time 0 and, t1
and t2 are any two time instants in the busy interval with t2 ≥ t1.
EDF-DRR is a hybrid of work-conserving and non-workconserving policies. Since the cells of jitter-constrained
sessions in the big frame are always transferred relatively at
the same positions, the performances of these sessions are kept
unchanged no matter how many idle slots there are in big
frames. On the other hand, delay bound and buffer
requirement of non-jitter-constrained sessions are still
guaranteed when some sessions are idle. This is due to the fact
that the cell slots allocated to backlogged non-jitterconstrained sessions in a big frame are occupied by the same
sessions no matter there are any idle sessions or not. If there
are some empty cell slots in a big frame in the case of existing
idle sessions, more cells may be transferred for non-jitterconstrained sessions in these cell slots. Obviously if there are
some backlogged non-jitter-constrained sessions, the EDFDRR server works as work-conserving mode and if no
backlogged session is non-jitter-constrained, the EDF-DRR
server may work in non-work-conserving mode.

[10] M. Katavenis, et al., “Weighted round-robin cell multiplexing in a
general-purpose ATM switch chip,” IEEE Journal on Selected Areas in
Communication, Vol. 9, No. 8, 1991, pp 1265-1279.
[11] Lixia Zhang, “VirtualClock: a new traffic control algorithm for packet
switching networks,” Proc. ACM SIGCOMM’90, Philadelphia, PA, Setp.
1990, pp. 19-29.
[12] John A. Stankovic, Marco Spuri, et al., Deadline Scheduling for RealTime Systems: EDF and Related Algorithms, Kluwer Academic
Publishers, 1998.

VI. CONCLUSIONS
In this paper, we proposed an applicable packet-scheduling
algorithm for distributed real-time systems. The algorithm is a
round-robin based discipline for OQ switches in packet
switching networks with fixed-length cells being the basic
transmission and scheduling unit. Bandwidth reservation for a
session at a switch is carried out through the assignment of a
number of cell slots in each frame for the session. The
transferring order of cells in a frame is determined using nonpreemptive non-idling EDF algorithm so that cells of a
backlogged session in a frame are distributed as uniformly as

[13] Jean-Yves Le Boudec and Patrick Thiran, Network Calculus: A Theory
of Deterministic Queuing Systems for the Internet, Springer, 2001.
[14] S. Jamaloddin Golestani, “A self-clocked fair queueing scheme for
broadband applications,” Proc. IEEE INFOCOM’94, Toronto, CA, June
1994, pp. 636-646.
[15] J. C. R. Bennet and H. Zhang, “Hierarchical packet fair queueing
algorithms,” IEEE/ACM Trans. Netw., vol. 5, no. 5, Oct. 1997, pp. 675689.

10

IEEE TRANSACTIONS ON COMPUTERS, VOL. 43, NO. 5 , MAY 1994

548

A Nonblocking Algorithm for Shared
Queues Using Compare-and-Swap
Sundeep Prakash, Yann Hang Lee, and Theodore Johnson

Abstract-Nonblocking algorithms for concurrent objects guarantee that an object is always accessible, in contrast to blocking
algorithms in which a slow or halted process can render part
or all of the data structure inaccessible to other processes. A
number of algorithms have been proposed for shared FIFO
queues, but nonblocking implementations are few and either
limit the concurrency or provide inefficient solutions. In this
paper we present a simple and efficient nonblocking shared FIFO
queue algorithm with O(n) system latency, no additional memory
requirements, and enqueuing and dequeuing times independent
of the size of the queue. We use the comparekswap operation
as the basic synchronization primitive. We model our algorithm
analytically and with a simulation, and compare its performance
with that of a blocking FIFO queue. We find that the nonblocking
queue has better performance if processors are occasionally slow,
but worse performance if some processors are always slower than
others.

I. INTRODUCTION

A

LGORITHMS for concurrent access to data structures
fall in two broad categories: blocking and nonblocking.
Blocking algorithms are those in which a process trying to
read or modify the data structure isolates or locks part or
all of the data structure to prevent interference from other
processes. The problem with the blocking approach is that
in an asynchronous system with processes having different
speeds, a slower process might prevent faster processes from
accessing the data structure. Non-blocking algorithms, on the
other hand, ensure that the data structure is always accessible to all processes and a process that’s inactive (whether
temporarily or permanently) cannot render the data structure
inaccessible. Such an algorithm guarantees that some active
process will be able to complete an operation in a finite
number of steps [3], making the algorithm robust with respect
to process failures.
Shared queues are required in a number of multiprocessing
applications. A nonblocking implementation is desirable due
to its robustness and consistent performance in the presence
of processes with varying speeds. A number of shared queue
implementations exist. Lamport [7] gives a wait free implementation but restricts the concurrency to a single enqueuer
and a single dequeuer (an algorithm is wait-free if every
process can complete an operation in a finite number of
steps [3]). Gottlieb et al. 121 present a blocking algorithm
Manuscript received July 15, 1992; revised January 15, 1993. This work
was supported in part by a grant from Florida High Technology and Industry
Council.
The authors are with the Department of Computer and Information Sciences,
University of Florida, Gainesville, FL 3261 I .
IEEE Log Number 9214076.

for enqueuing and dequeuing using the replace-add and
swap operations for synchronization. This implementation
allows a for high degree of parallelism, limited only by the
predefined maximum queue size. However, it is possible for an
enqueuer or dequeuer to block other dequeuers and enqueuers.
Stone [ 111 proposes a “nondelaying” implementation using the
compare&swap operation for synchronization, which allows
an arbitrary number of enqueuers and dequeuers. However,
a faulty or slow enqueuer can block all the dequeuers. Herlihy and Wing [4] give a nonblocking algorithm using the
compare&swup operation. This also permits an arbitrary number of enqueuers and dequeuers but is impractical, as it requires
an infinite array size for continued operation. Treiber 1131
gives a nonblocking algorithm for concurrent FIFO access to
a shared queue. The enqueue operation requires only one step
but the time taken for the dequeue operation is proportional
to the number of objects in the queue, so the algorithm is
inefficient for large queue lengths and many simultaneous
dequeue attempts.
Herlihy [3] presents a general methodology for automatically transforming any sequential data structure implementation into a concurrent nonblocking or wait-free implementation, but the memory requirements for each concurrent process
grow in proportion to the total number of concurrent processes.
Prakash [9] and Turek, Shasha, and Prakash [15] propose
a transformation for creating nonblocking algorithms from
locking algorithms, and Turek [ 141 proposes a transformation that creates wait-free algorithms. These transformations
require only O(1) space overhead per processor. The algorithm
presented here is considerable simpler and more efficient than
those provided in [9], [ 141, [ 1.51, and in fact a previous version
of this work is used in 1141.
In this paper, we present an efficient nonblocking algorithm for a shared FIFO queue. This implementation uses
the compare&swap operation for synchronization. As proved
elsewhere 151, it is impossible to design nonblocking or waitfree implementations of many simple data structures using
other well-known synchronization primitives (i.e., read, write,
test&set, fetch&add and swap). However, the compare&swap
operation in its simple form has a standard difficulty (the
A-B-A problem [16]). The solution is to use the modified
compare&swup operation (also described in [ 16]), which is
what we have done. The nonblocking property is achieved by
processes cooperating to complete an operation started by one
of the processes. Both the enqueue and dequeue operations are
independent of the size of the queue.

0018-9340/94$04.00 0 1994 IEEE

549

PRAKASH et al.: A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

The rest of the paper is organized as follows: In Section I1
we briefly discuss the compare&swap operation and the A-BA problem. Section 111 describes the data structures we use for
implementing the queue and Section IV contains the algorithm,
the proof of its correctness and its analysis. We model the
performance of the algorithm in Section V. In Section VI, we
conclude by summing up the results.

We use the implementation of the compare&swap operation
found in the IBM/370 architecture (also used in the Cedar
supercomputer at the University of Illinois [17], the BBN
[ I ] , and the Motorola 68000 family [8]). It is a three-operand
atomic instruction of the form CS(A, B, C). A, B and and C
are one-word variables.' The instruction does the following:
If A equals C
then put B into C, return condition code 0
else put C into A, return condition code-1.
The compare-and-swap instruction is used in the following
manner: C is a shared variable and A is the private copy of it
made sometime earlier by a process. B is the value which the
operation is attempting to put in C. The operation is allowed to
do so only if C has not been modified by some other process
since this process made a copy of it. If the attempt fails, the
current value of the shared variable is returned. We do not use
this last feature in our algorithm.
The A-B-A problem occurs when C is the same as A
even though C has been modified a number of times since
the process made a copy of it. A compare&swap operation
performed by the process now will succeed, which can cause
errors to occur in many implementations of concurrent objects.
To prevent this error a counter is appended to C and is
always incremented when a modification is made to C. This
leads to the compare&swap double operation (implemented
in the Motorola 68000 family [SI), a five-operand atomic
instruction of the form CSDBL(A1, A2, B1, B2, C) that does
the following:
If A1 1 1 2 A2 equals C
then put B1 1) B2 into C, return condition code 0
else put C into A1 )I A2, return condition code-I.
Now, the shared variable C is twice the size of the other
variables since one half of C is used as a counter (we shall
assume it to be the second half). A process first reads the value
of C and puts it in A1 and A2. It puts the desired new value
of C into B1, assigns to B2 the value A2 + 1 , and then executes
the CSDBL instruction. Although the A-B-A problem can still
occur, the probability is much lower [ 111 and is acceptable for
a large class of applications.
THE

Poinkr-toabject
Inkpr

Structureobject
data
nutobject

Structure-of-data
Poinkr

Shared V u i a b l w
Sb.redhd
Shued-td

Poinkr
Pointer

Private V u i n b b
Privrtehil
Privrkhebed

11. THE A-B-A PROBLEM

111. DATA STRUCTURES FOR

StNCtUrEPointer
Ptr
aunt

QUEUE

The data structure we use for the shared FIFO queue is a
singly linked list. An object in the list consists of a pointer to
the next object and whatever other structure may be needed for
the particular application. The pointer is a double-size variable,
that is, one half is the actual pointer and the other half is the
'We do not give the exact location of the variables at the time of instruction
execution as it is not important here.

Nut
objectptr

Pointer
Pointer
Pointer
Pointertoobject

Fig. 1. Data structure of the queue.

counter (to be used in the compare&swap double operation).
One bit of the counter, however, is used to indicate whether
the object is going to be dequeued. We will discuss the use
of this bit in Section 4. When the counter is used in this way
this bit shall be referred to as mark and the remaining bits
as counter. The head and tail of the queue are pointed to by
shared pointers of the same type as those in objects of the list.
The counter need not be subdivided further for these variables.
The data structures and variables are shown in Fig. 1. An
important assumption for the shared queue is that objects cannot be destroyed. A dequeued object must be either returned
to a shared pool or kept for later use in enqueuing (the reason
for this shall become evident in Section IV). A shared pool,
if used, can be maintained as a stack. The implementation
of a nonblocking concurrent stack using the compare&swap
operation has been discussed elsewhere [ 131.
I v . T H E ALGORITHM
Objects are dequeued at the head and enqueued at the tail
of the linked list. For the algorithm to be nonblocking it is
necessary that
either the enqueue and dequeue operations are atomic,
or when they comprise more than a single step, the
subsequent steps can be performed by any process.
Our algorithm successfully implements the second option.
When the enqueue or dequeue operation consists of two or
more steps, the queue goes into a unique intermediate state
after the first step of the enqueue or dequeue operation. Any
process can now identify and correct the state of the queue
thus completing the previously incomplete enqueue or dequeue
operation and enabling it to proceed with its own operation.
A. Reading the State of the Queue

The queue can be in one of the eight states shown in Fig. 2.
These states are generated on the basis of steps taken in the
enqueue and dequeue operations which are discussed in the
following subsections. The state of the queue can be uniquely
identified by getting a simultaneous picture of the variables
Sharedhead, Shared-tail and the nextobject pointer of the
object pointed to by Shared-tail, storing these values in
Privatehead, Private-tail and Next. This can be obtained by
the Snapshot procedure shown in Fig. 3. When the procedure
succeeds, it gives the simultaneous value of Sharedhead,
Shared-tail and the nextobject pointer at the instant the

IEEE TRANSACTIONS ON COMPUTERS, VOL. 43, NO. 5, MAY 1994

550

&--a&....-.......

7.

6.

Fig. 2.

8.

States of the Queue.

nextobject pointer is last read. This procedure returns to the
caller (in the caller’s own private variables) the value in each
of these variables.
B. The Enqueue Procedure

An enqueuer can proceed with its operation from only three
states, namely states 1, 3, and 7 (the states are shown in Fig.
2). If the queue is in some other state, the enqueuer must
first bring the queue to one of these states. Two cases can
occur when a process enqueues an object starting from one of
these “correct” states. The steps taken in each of these cases
are described below and the state transitions they cause are
shown in Fig. 4.
When the queue has one or more elements (state 1 or 3)
the operations are the following.
a)

b)

(Operation 2E(m):1 in Fig. 4) Append the object
to the end of the linked list. This operation will
not succeed if a dequeuer has already marked it
for deletion, by setting mark to DEQ (this case
will only occur if there is only a single object
in the queue, since only then may enqueuers and
dequeuers be attempting to make changes to the
same object) or, if another enqueuer has already
appended its own object to the end of the linked
list.
(2E(m):2) Shift the tail to point to the new end of
the list.

When the queue is empty (state 7) the operations are:
a)
b)

(2E(O):1) Shift the tail to point to the new object;
(2E(O):2) Shift the head to point to it.

If after a process completes the first step another enqueuing
or dequeuing process accesses the queue, the second process
may do the second operation itself (since it detects an intermediate state). Nonetheless, the first process will try to do

ProcedureSnap.hot(Privatehed, Printct.il, N u t )
repeat

Read Shuedhead into finthud
repeat

R u d She.rcdf.il inb finttul
if finttul.ptr # NULL then
Read finttad.ptr->nu(object i n b N u t
Rud Shued-tail ink P r i n k h i l
until Privatefad = f i n t W
Rud Shuedhud into P r i n k - h d
until P r i v a k A e d = finthud
end procedure Sn+

Fig. 3. The Snapshot procedure.

the second operation and will be unsuccessful. This is due to
the fact that we use the compure&swup instruction to do the
operation, so once the tail (or head, in the second case) has
been changed, no process which read the tail (head) before
the change can succeed in modifying it. The remaining part
of the enqueue procedure is devoted to detecting intermediate
states of the queue and correcting them by completing the
unfinished enqueue and dequeue operations. The complete
enqueue procedure is shown in Fig. 5.

C. The Dequeue Procedure
A dequeuer can proceed with its operation only from states
1,2, 3, and 7. Just as with enqueuers, if a dequeuer encounters
the queue in some other state, it must first bring the queue to
one of these states. Two cases arise when an object is to be
dequeued.
If there are two or more objects in the queue (state 1
or 2), dequeuing consists of a single step: shifting the
head of the queue to the next object in the list (Operation
lD(m):l in Fig. 4).
If there is only a single object in the queue (state
3), it must be first marked, to prevent enqueuers from
appending objects to this object and also, to tell other
dequeuers that the object has already been claimed. The
object is marked for deletion by changing mark of

PRAKASH et al.: A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

55 1

IB(C):d
.
:Total nllmba of steps

B E -Enqueue
D -Dequeue
C: N U ” afelsmsnt~
in the queue when
apsntim.tartd

m -many

lor dequeuers
>0f
a enqusum)

( >1

d

0 -Zero
1 -one
step being performed

Paliopl, OPZ):
indicates P simultaneous
change due ta Opl and Op2

Fig. 4.

State Transition Diagram.

the nextobject pointer from ENQ to DEQ. If the mark
operation succeeds, then the object can be dequeued by
shifting the tail and head to NULL (Operations 3D( 1): 1,
2, and 3, respectively).
In the second case, if other enqueuers and dequeuers access
the queue after the first step is complete they will cooperate
in completing the dequeue operation before proceeding to do
their own operations. If the queue is in state 7, the procedure
simply reports an empty queue. Just as in the enqueuing
procedure, part of the dequeuing procedure is devoted to
correcting the state of the queue.
The complete dequeue procedure is shown in Fig. 6.
D. Proof of Correctness

We show that this algorithm is correct by proving it satisfies
certain safety and liveness properties. The safety properties
are the following.
If the head and the tail of the queue are different, serial
changes may be made simultaneously at the head and at
the tail. If head and tail are the same, all changes occur
serially.
Additions to the queue are done only at the tail of the
queue and deletions only at the head.
and the liveness property is:
if there are enqueue and dequeue attempts being made on
the queue, some attempt will succeed in a finite amount
of time.
The first safety property ensures that each enqueue (dequeue)
operation must be completed before the next enqueue (dequeue) operation is started. Enqueuing and dequeuing may
proceed in parallel, however, except for the case in which the
head is the same as the tail. Here, the enqueue and dequeue
operations are serialized. Since we require enqueuing to be

done only at the tail and dequeuing only at the head, it follows
that the queue will be accessed in FIFO order.
The liveness property ensures that some enqueuer or dequeuer must succeed in performing its operation in a finite
amount of time regardless of the failure or inactivity of
other enqueuers and dequeuers. Since the liveness property
is unconditional, this algorithm is nonblocking.
The proof of the safety and liveness properties follow from
the state transition diagram (Fig. 4). The safety properties are
obeyed as follows.
When the head and the tail of the queue are different,
enqueue attempts are serialized by a single decisive
operation such as the successful appending of an object to
the end of the linked list. (Concurrent actions are decisive
operation serializable [ 121 if they are serializable [ 121
and there is an operation dec(a) in each of the actions
such that if d e c ( a i ) occurs before d e c ( a j ) then ai comes
before aj in the serial order.) Due to the nature of
the compare&swap operation, only one enqueuer can
succeed in its decisive operation. All other enqueuers then
cooperate to complete the enqueue operation. They do
so when they detect an intermediate state of the queue
which implies an incomplete enqueue operation. Many
enqueuers may attempt to correct the state, but only one
can succeed. Dequeue attempts are also serialized by a
single decisive operation, the shifting of the head to the
object after the one being pointed to by the head. Again,
it is the compare&swap operation which allows only one
dequeuer to succeed, because once the head has been
shifted, no process which read the head before the change
can modify (shift) it.
When the head and the tail are the same and the queue is
not empty, all operations are serialized by the attempt to
claim the nextobject pointer of the object pointed to by

552

IEEE TRANSACTIONS ON COMPUTERS, VOL. 43, NO. 5, MAY 1994
Procedure Eaqueue (object&)
h i t i d i r e object, l u r e cord rntorchd e z u p t for

CSDBL (Private-tail.ptr, PrivatehLcount,
NULL, Privakfail.count+l,

marlimg IAe object to k crgrerd

Shared-tail)

objeetptr->natobject.plr = NULL
objectp(r->aexbbject.count.mut = ENQ

Shifl the head t o NULL
CSDBL (Privakhead.ptr, Privaklead.count,
NULL, Privatehead.count+l,

Sbared-head)

repeat

6 Complete Icquertnq the object
Snaphol(Privaklud, Privltctsil, N u t )

CSDBL (Privakhead.ptr, Private-head.count,
NULL, Privakhead.count+l,

Determine date ofqueue and usign to S t a k

Shdhead)
7 No elements in the queue, do an emptj queue
enqrere 6yfirst

cc = CSDBL (Nut.ptr, Nut.wunt, objutptr,

cc = CSDBL (Private-tdptr. Privakfsil.count,

ENQ 11 Nut.count.counkr+l,
Pnnkfd.ptr->wtohject)
ifcc

= 0 then

&fling tail to the object

objectptr, Privatefail.count+l,
Sharedfail)
ifcc

= 0 then

Attempt we# rrccersfd, ro ahifl the tail to tAe

The object has been added,

object jut dld

to p t n t

CSDBL ( P d n k f d . p t r , Privak+ail.count,

CSDBL (Privakhead.ptr, Privatehead.count,

to

so sAtfl

the Aead

rt

objutptr, Privakfail.count+l,

objectptr, Privatehead.count+l,

Sharedfail)

Sharedhead)
aucccn = TRUE

auccen = TRUE
2 3 : FamuA incomplete enqrcre opcntwn

CSDBL (Private-tail.ptr, Private-tail.count.

8: F i r i d incomplete engrrrc opemtior
CSDBL (Privatehead.ptr, Privakhead.eount,

Next.ptr, Privakfdcount +I,

Prinkhil.ptr, Privakheul.couat+l,

Sharedfail)

Sbuedhd)

4: Coopnte in Jcqrertng tAc object

and case

UtaN
Shifl the tail to NULL

C

W

and procedure enqueue

Fig. 5 . The Enqueue Procedure.

the head and the tail (dequeuers do this by changing mark
and enqueuers by changing the pointer). Again, only one
attempt can be successful and all processes cooperate to
complete that operation. This happens when the processes
detect an intermediate state of the queue which implies
an incomplete enqueue or dequeue operation.
When the queue is empty, enqueue attempts are serialized
by the successful shifting of the tail to a process’s own
object. Again, only one process can be successful in
shifting the tail and all processes now attempt to complete
the operation with only the first one making the attempt
being successful.
The proof of the liveness property follows from the observation of two facts. First, there is no queue state from which
an enqueuer or dequeuer cannot take an action. Second, the
only way enqueuers and dequeuers can be kept from taking

an action is if the state of the queue changes while they
are reading or trying to modify the state of the queue. By
contradiction, if no one succeeds then the state of the queue
cannot change, so some process must succeed. Together, these
parts imply that some action must be completed in a finite
amount of time.
In this proof we have not taken into consideration the AB-A problem, which we have assumed to have a very low
probability of occurrence. Using one bit of the count in the
nextobject pointer of an object to mark objects will not
significantly affect the chances of occurrence of the A-B-A
problem for the queue as a whole. This is because the A-BA problem is more likely to occur at the head or tail, since
a compare&swap double operation will be performed on the
nextobject pointer of an object far less frequently than the
head or the tail (as there are many objects in use).

PRAKASH

el

553

al.: A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

Procedrue Dequeudobjatptr)
succcI

Shifl Pe tsil to NULL

= FALSE

CSDBL (Printe-td.ptr, PrinteAul.count,

repeat

NULL, Printefail.count+l,
Shdfail)

Snaphot (Privatehead, Private-tail, N u t )
Determine state of queue and assign it to State

Now rhifl Be heed to NULL
w e State

of:

CSDBL ( Printehead.ptr. Printehead.count.
NULL, P r i n k h d . c o u n t + 1,

1, 2: Do s single step deqrerc i.e shifl h a d to tAe

Shuedhead)

red abject

5: Complete rnfinished cnqrere

cc = CSDBL(Privatehead.ptr, Privatehead.count,

oprration

Privatehead.ptr->nextobject.ptr,

CSDBL (PrivateJail.ptr, Privatefail.count,

Privatehead.count+l, Sbaredhead)

Privakful.ptr->nutohject.ptr,

Privatefail.count+l, Shared-tail)

ifcc = 0 then
objectptr = Privatehead
sucm

6 : Complete deqrering the object

= TRUE

CSDBL (Pdvakhead.ptr, Privatehead.count,

3: Single objeci in qrcrc, so do a thne step deqrere

NULL, Privatehed.count+l,
Shuedhead)

Mark the object t o k deqrcrcd

cc = CSDBL( Nut.ptr, Nutxount, Next.ptr,
DEQ

7: Report empty qrcrr

11 Nut.count.counkr+l,
objectptr

Private.tail.ptr->netobject)

ifcc = 0 then

s u m

= NULL

= TRUE

~ucce(ll=TRUE
objectptr = Privatehead

8: FinW

incomplete cnqrere operation

CSDBL (P?intehed.ptr. Printchead.count,

Attempt was srcccssfil, so make the tail NULL

PrintehiLptr. Privakhead.count+l,

Sharedhead)

CSDBL (Private-tail.ptr, Private-tail.count,
NULL, Privatefail.count+l,

and c u e

Shared-tail)

until s u m
Now

difl ihe h u d

to NULL

Delint d e p e r d objrct Jrom /id

CSDBL (Priv.Lehead.ptr, Privakhead.count,
ifobjatptrfNULL then

NULL, Prinkhead.count+l,

ohjectptr->nextobject.ptr = NULL

Sharedhead)

and p d u m dequeue

4: Coopair in deqrerii, the object

Fig. 6. The Dequeue Procedure.

It may seem that the snapshot isn’t needed, since all
operations are protected by the compare&swap double operation. However, an error can result if the snapshot isn’t
used. Consider the following example: The queue has a single
element in it, and it is marked for dequeuing. An enqueuer
reads the head and tail of the queue, then blocks. The element
is removed, and at some point the DEQ mark is reset. The
enqueuer unblocks, reads the element. The queue appears to
be in state 3, so the enqueuer attempts to append its element

to the dequeued element, and might succeed in appending its
element to one not in the queue.

E. AnaLYsis
The algorithm has an O ( n ) system latency, where n is
the number of processes in the system (system latency is
defined in [3] as the maximum number of steps a system can
take without completing an operation). Assuming n enqueuers

IEEE TRANSACTIONS ON COMPUTERS, VOL. 43, NO. 5 . MAY 1994

554

and dequeuers, a single modification of Sharedhead or
Shared-tail by one of them could cause all others to fail
in reading the state of the queue and require them to start
reading the state all over again. The number of modifications
a process can make is a constant, so this sequence of events
could be repeated only a constant number of times until a full
enqueue or dequeue operation is complete, after which the
next operation must succeed.
The number of steps taken for the enqueue and dequeue
operations is independent of the queue size, as is evident from
the state transition diagram. There are no additional memory
requirements for the algorithm (except for some temporary
private variables). The only requirement is that objects should
not be freed but returned to a shared pool because even after
dequeuing there may be some processes reading or attempting
to write to the object (the write will fail). As mentioned
earlier, this pool can be maintained as a stack. In the event
of the failure of an enqueuing or dequeuing process, all that
is (possibly) lost is the single object in the process of being
enqueued or had been dequeued. The queue is thus faulttolerant.
As we discuss in the introduction, several authors have
written lock-free queue algorithms, but most are not directly
comparable because they limit concurrency [7], block processes [2], [ l 13, require infinite space for continued operation
[4], or require O ( n ) time to perform a dequeue, where
there are n items in the queue [13]. Several authors have
presented transformations with similar complexities. Herlihy’s
transformation [3] required O ( p 2 ) space overhead for correct execution. Our algorithm requires O ( p ) space overhead.
Prakash and Turek et al. [9], [14], [15] present methods
for transforming locking algorithms into asymptotically efficient non-blocking algorithms. While these transformations
are asymptotically as efficient as our algorithm and are more
general, our algorithm is practically implementable.
V. PERFORMANCE
Our nonblocking queue has some definite advantages over
a blocking queue in terms of fault tolerance. The question
remains as to whether the nonblocking queue has better
performance than a blocking queue. The nonblocking queue
lets fast processors perform work for slow processors, but
requires processors to restart if the queue is modified. In order
to compare the algorithms, we simulated them and developed
an analytical model.
We wrote a hypothetical locking algorithm to compare
against our nonblocking algorithm. The locking algorithm has
the same concurrency as the nonblocking algorithm. There are
three locks in the system: an enqueue lock, a dequeue lock,
and a common lock. When an enqueuer comes into the system
it tests to see if the enqueue lock is set. If so, another enqueuer
is active and the blocked enqueuer goes to sleep in a queue
at the enqueue lock. If not, it sets the enqueue lock. It then
tests to see if there is only one element in the queue (from
which elements are to be dequeued and enqueued). If not,
it proceeds to do an enqueue. When the enqueuer leaves, it
resets the enqueue lock only if there are no enqueuers waiting

at the lock, else it wakes the first waiting enqueuer. If there
is only a single element in the queue after the enqueuer sets
the enqueue lock, it checks to see the common lock and if
the lock is not set, it sets the lock and continues’enqueuing as
before. If the lock is set, then a dequeuer wants that element,
so the enqueuer goes to sleep at the enqueue lock, after setting
an indicator that all enqueuers are asleep. When the dequeuer
leaves, it should wake one of the waiting enqueuers. Whenever
an enqueuer leaves, it too checks to see if the indicator that all
dequeuers are asleep is set, so that it can wake one as it goes.
A dequeuer proceeds in very much the same way. It first
sets the dequeue lock, and then checks to see if there is
only a single element in the queue. If so, it checks to see
if the enqueue lock is set. If the lock is, enqueuers are in
the system, and they get preference, so the dequeuer goes
to sleep after setting the all dequeuer asleep indicator. If the
enqueue lock was not set, the dequeuer sets the common lock
and proceeds to do a dequeue. When it leaves it wakes one
of the waiting dequeuers (if there are any), else it resets the
dequeue lock. If there was more than one element in the system
after the dequeuer set the dequeue lock, it proceeds to do a
dequeue without doing any additional checking. If there were
no elements in the queue, the dequeuer leaves, and also sends
away all dequeuers waiting at the dequeue lock, saying that
the queue is empty.
This algorithm also has two streams, an enqueuing stream
and a dequeuing stream, which combine to a single stream
when there is only a single element in the queue.
A. The Simulator
We wrote a discrete event simulation of the locking and
nonblocking algorithms. Enqueue and dequeue operations in
both the locking and the nonblocking algorithms required
about the same number of instructions. Operations arrived in
a Poisson process, and we assumed that operations of a given
class always required the same amount of time to complete. We
measured the number of times snapshots were missed, and the
number of times a process caught the queue in an intermediate
state and corrected it (this tells us how much “cooperation”
there is among processes). We ran the simulations until 5000
enqueue and dequeue operations completed, both being equally
probable.

B. Simulation Results
We found that the locking and nonblocking algorithms
show very similar behavior when all processes have the same
speed. We next used a mix of processes of two speeds. Here,
some processors ran ten times fasters than the others, and
90% of the processors were of the faster variety. Processes
take 17 instructions for enqueuing and 13 for dequeuing and
one instruction takes a single time unit for fast processes.
Since a processor always works at the same rate, we call
this model the constant speed model. We found that in this
model, the blocking queue has lower response times than
the nonblocking queue (see Fig. 7), especially as the arrival
rate increases. While fast processes have a lower response
time in the nonblocking queue (see Fig. 8), this advantage is

PRAKASH

et

01.: A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

Comparison of response times
of slow processes

Comparison of Average
Response Times
Responae tirm

160

-

140

-

Response time

1000

800

120 100 -

80

555

600

-

0.005

0.015

0.01

0.02

1

4
1

/
,

0 4
0

0.025

0.01

0.02

Arrival rate

-

Blockingqueue

0.03

0.04

0.05

Arrival rate

-

+ Non-blockingqueue

Fig. 7. Simulation results for the constant speed model.

Blaking queue

+ Non-blockingqueue

Fig. 9. Simulation results for the constant speed model.

Comparison of response times
of fast processes

Missed Snapshots and
Cooperation Instances

Responsetune

thousands
10

50 1

7

30 20
10

i
~

i

+ I + t t t i : : : ' ''

t
'

0.005

0

0
0

0.005

0.01

0.015

0.025

0.02

-

Arrival rate

-

Blaking queue

+ Non-blaking

queue

0.01

0.015

0.02

0.025

Arrival rate

0.03
M

W w p s (Iklw)

+ Mlsoed-(IOIBI)

-c cmper.nlXllnrSno

Fig. 8. Simulation results for the constant speed model.

Fig. 10. Simulation results for the constant speed model

more than offset by the higher response times of the slow
processes (see Fig. 9). We find that the snapshots missed
by the slow processes increases rapidly (see Fig. 10) with
the increasing arrival rate, indicating starvation. The number
of times processes cooperate also goes up with arrival rate.
This increase is not enough to compensate for the missed
snapshots, since the penalty for a missed snapshot is greater
than the advantage gained in a single cooperation instance in
this algorithm. On the whole, slow processes do not get the
chance to commit their operations very often, even after getting
valid snapshots, as is indicated by their slow response times. It
is this inability of slow processes to complete their work that
results in the overall poorer performance of the nonblocking
algorithm.
The constant speed model addresses differences in processors, but doesn't account for the possibility that a processor
might temporarily become slow due to memory, network, or
CPU contention. We also considered the varying speed process
model, in which we again assume that there are two process
speeds, the fast speed being ten times the slow speed. A
process is ten times more likely to start out as fast than slow.
The amount of time spent at the fast speed is exponentially
distributed with the mean as 300 time units. The amount of
time spent at the slow speed is exponentially distributed with
the mean as 30 time units. As before, one instruction takes

a single time unit at the fast speed. In the varying speed
model, the nonblocking queue has better performance than
the blocking queue (see Fig. 11). The performance of the
nonblocking algorithm is better in this situation because of
the following.
When a process is slow, it is very probable that the
process will not be able to either get a valid snapshot.
Even if the process does get a snapshot, it may not
be quick enough to take a decisive action. Thus, the
slow process gets delayed, but does not delay others. In
the blocking algorithm, since there is no discrimination
against slow processes (or processes while they are slow),
a process in its slow phase may get a lock and delay
all the waiting processes. In the non-blocking algorithms,
operations are performed on the queue at the pace of the
fast processors.
Since a process does eventually become fast, it will
ultimately be able to perform its operation. This situation
is in contrast to the constant speed model, in which slow
processes were starved by the fast processes, which led
to very poor response times.
We thus see that the performance of the nonblocking algorithm depends on the situation, and is better than a blocking
algorithm when we have a system of processes running on

IEEE TRANSACTIONS ON COMPUTERS, VOL. 43, NO. 5. MAY 1994

556

system is calculated by taking the expectation of the per-class
expected residence times. The system efficiency is calculated
by

Comparison of average
response time
Response time
100,

\
\c=1

*O

1

04

0

0.01

0.02

0.03

0.05

0.04

0.06

0.07

In order to calculate the per-class efficiencies, we need to
calculate the probability that a transaction aborts due to a data
conflict. If CP, is the probability that a transaction conflicts with
a class c transaction when it commits, then other transactions
conflict with a class c transaction at rate

Arrival rate

Fig. 11.

blocking queue

--t

non-blocking queue

Simulation results for the varying speed model

processors which occasionally get slow. It is not better in the
case of a mixture of slow and fast processors, because the
algorithm favors the faster processes.
C. Analytical Model

To better understand the phenomena that the simulations
revealed, we analytically modeled both the blocking and the
nonblocking queues. We make the simplifying assumption
that the queue never empties, so that the enqueue and the
dequeue operations are always independent. In this case,
the blocking queue can be modeled as a standard M/G/I
queue. Our nonblocking queue needs different analytical tools,
since there is no queuing. The nonblocking algorithm uses
methods similar to Optimistic concurrency control [6], since an
operation reads the control data, computes a modification, then
commits the change only if no other operation has modified
the data. We make use of the analytical framework provided
by Ryu and Thomasian [lo].
Constant Speed Model Ryu and Thomasian model a closed
transaction processing system in which V transactions each
execute one of C transaction types. When a new transaction
enters the system, it is a class c transaction with probability
f,. If a transaction is aborted, it restarts as the same class
transaction, so that when a transaction commits, it is a class
transaction with probability f c , because of conservation of
flow. Thus, we can apply Ryu and Thomasian's techniques to
the constant speed model.
A class c transaction is assumed to have an execution time
of P(V)b,(z), where b,(x) is the probability density function
of the execution time of a class c transaction, and p(V) is
the increase in execution time due to resource contention.
The expected residence time (time required to execute the
transaction if there is no abort) is denoted by R;(V) z
;?(V)bc,where b, is the expected value of b,. A transaction
might be required to restart several times due to data conflicts.
The expected time that a transaction spends executing aborted
attempts is denoted by @ ( V ) ,and the total residence time of a
class c transaction is R'( V) = R; ( V ) R;(V ) .The efficiency
of a class is the proportion of its expected residence time spent
executing a transaction that commits: U, = R\(V)/(Ri(V)+
R i ( V ) ) .The expected residence time a transaction in the

+

Yc

=

(V - 1)@r
b

where b is the expected execution time of all transactions. If the
execution time of a class c transaction is fixed, its residence
time has a geometric distribution, based on the number of
restarts until it can commit. Ryu and Thomasian assume that
the process by which other transactions conflict with a class c
transaction can be accurately modeled as a Poisson process.
In our model, a queue operation reads the state variables,
computes the necessary changes, then performs the decisive
operation to commit the changes. When an operation attempts
to commit its change, it fails if another operation has already
changed the data structure. We call the interval between the
time that the operation first reads a control variable to the time
that it attempts to commit its change the execution time of the
operation. We assume that the queue never empties, and that
we are modeling only the enqueuers (or only the dequeuers),
so that every operation conflicts with every other and = 1.
The probability of conflict between any two operations is the
same, so their conflict rates are the same, and we will denote
the conflict rate by y.
In the simulations, we modeled fast and slow operations.
Correspondingly, we use fast and slow transaction types in
our analytic models. We define f s to be the probability that a
transaction is a slow transaction (and 1 - f s is the probability
that a transaction is fast).
In [IO], if a transaction executes for t seconds, then aborts, it
will execute for t seconds when it restarts. Our system is better
described by assuming that the time required to execute the
restarted operation is independent of the time to execute the
first transaction, and is a sample from the same distribution.
If an operation requires t seconds, the probability that it
will be commit is e-5t, since we assume that conflicts form a
Poisson process. Therefore, the unconditional probability that
the operation commits is:

lz0
C X

Pc =

e-Ytbc(t)dt.

(2)

The number of times that the operation executes has a geometric distribution, so an operation will execute l / p , times.
The first l/yc - 1 times the operation executes, it will be
unsuccessful. Knowing that the operation is unsuccessful tells
us that it probably required somewhat longer than average to
execute, since slow operations are more likely to be aborted.
Similarly, successful operations are likely to be faster. The

557

PRAKASH et ai.:A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

distributions of the execution times of the successful and
unsuccessful operations is given by

Since f: and f j = 1 - f s are the proportion of fast and
slow operations, we need to redefine the conflict rate to be

h z ( t ) = K,e-Ytb,(t)
b , f ( t ) = K ~ (-I e+)b,(t).
where K , and K f are normalizing constants computed by

Given a conflict rate 7,we calculate pc using formulae ( 2 ) .
If b, has an exponential distribution, then we can calculate p ,
to be p,/(y
p c ) . We have two equations in two variables,
which we can solve to find that

+

If b: and b: are the expected values of b z ( t ) and b , f ( t ) , the
(1/ p expected time to complete a class c operation is b:
1)b;f. Therefore, we find that

+

u, =

bC

(b:

+(l/P

-

1)bCf).

(3)

Equation 1 for the system efficiency depends on the perclass efficiencies, and equation 3 for the per-class efficiency
depends on the system efficiency. Ryu and Thomasian show
that this system can be rapidly solved by iteration.
If we assume that the operations have an exponentially
distributed execution time, b,(t) = p r e - p c t , then the system
is simple enough to solve algebraically. If the rate at which a
fast transaction commits is pf and the rate at which a slow
transaction commits is h,, then:

Uf

=

(1 - UsPL,)ffPsUs
Pf(PsUsfs -

f s

-

USV

and p , is the solution in [O. 11 of

+ Us).

where Us is the solution in [0,1] of

If the operations have deterministic execution times (the
case in the simulation models), the assumption of Poisson
conflict arrivals is a poor approximation to the actual conflict
arrival process. We use a different model to handle deterministic execution times.
Variable Speed Model The simulations showed that the
nonblocking queue has markedly different performance depending on whether slow processors stay slow, or whether they
are occasionally slow, then become fast. Ryu and Thomasian’s
model can be directly applied to the case in which there is a
mixture of fast and slow processors in the system, since a
transaction stays in its class until it commits. However, we
need another model for the case when a processor is usually
fast, but occasionally becomes slow. We will assume that the
processor executing an operation is either fast or slow, but it
can change its speed after trying to commit the operation. In
this model, f i is the probability that any given transaction is
slow (and 1 - f i is the probability that any given transaction
is fast). In Ryu and Thomasian’s model, every transaction
eventually commits as a transaction of its initial class. Now, it
isn’t necessarily the case that every slow transaction commits
as a slow transaction. We can calculate f i to be f,b,/b. In
this model, we will solve for the probability that an operation
commits, p S and p f .

b f b - b,b

+ (h,

-

-

2bf)ps

Vb: f f
b

+ bf

-

,

Vb b,
f

)Pi
(5)

= 0.

Deterministic Execution Times The Ryu-Thomasian model
can’t be applied to the case of deterministic execution times,
because the conflict process becomes significantly different
than a Poisson process. We provide here a simple model for
deterministic execution times.
Under the constant speed execution model, a fast processor
will always commit before the slow processors. As a result,
slow processors tend to accumulate, so the processor population is either all slow processors, or one fast and all but
one slow processor. If a processor commits and is replaced
by another fast processor, no time is lost. If a fast processor
commits and is replaced by a slow processor, then the first
processor to abort will be the one that commits. The time
between the commit of the fast processor and the first abort
of a slow processor is the wasted time. If we model the
commit of the fast processor as occurring uniformly randomly
within the slow processor’s execution time, then the wasted
time is the minimum of N - 1 samples of a uniform [O,b,]
random variable, which has mean b,/N. So the throughput of
nonblocking queue under the constant speed processor model
with deterministic execution times is:
1
In the varying-speed processor model, a fast processor (almost)
commits, so the throughput is

N

D. Comparison
We can compare the performance of the blocking and the
nonblocking queues by comparing their throughput in closed
systems with varying numbers of concurrent operations. The
throughput of the blocking queue is l/(fS/ps f f / p f ) ,
independent of the number of concurrent operations.
Fig. 12 shows a comparison between throughput of the
blocking and the nonblocking queue when the execution times

+

IEEE TRANSACTIONS ON COMPUTERS. VOL. 41. NO. 5 , MAY 1994

558

Comparison of Throughput

Comparison of Throughput
throughput

Throughput

’1

0.6

4 /

:

:

0l 5

O

h

2

1

3

Blocking queue

+ Constant speed

5

e
6
7
8

Concurrent operations

Concurrent operations

-

4

+ Varying speed

Fig. 12. Analytical results for deterministic execution times.

are deterministic. The throughput of both the constant and
the varying speed models is shown. The input parameters
are similar to those used in the simulation experiments:
Fast operations required an expected 1 second to finish, and
constituted 90% of the operations. Slow operations required 10
seconds to finish. As expected, the constant speed nonblocking
queue has a lower throughput than the blocking queue, which
has a lower throughput than the varying-speed nonblocking
queue.
We also performed calculations assuming exponentially
distributed execution times. Fig. 13 shows a comparison of
the throughput of the locking queue and of the nonblocking queue under both processor modeIs. Under the constant
speed model, the throughput falls below that of the locking
queue for two and three concurrent operations. This drop in
throughput reflects the time wasted by invalidated operations.
However, throughput increases with increasing concurrency.
This is due to the preference shown to fast operations. The
average execution time of the successful operations for both
nonblocking queue processor models is shown in Fig. 14.
Under the varying speed model, the nonblocking queue has
a higher throughput than the blocking queue. Again, this
difference reflects the preference shown to fast operations by
the nonblocking queue.
VI. CONCLUSION

The algorithm we have presented is a simple nonblocking
concurrent implementation of a FIFO queue. Modifications
to the queue are not always made atomically, as might be
expected in a nonblocking implementation. The cooperation
between the processes compensates for the non-atomicity and
ensures the nonblocking property.
We model the nonblocking queue both analytically and with
simulations. Our results show that if a processor takes a varying amount of time to complete an operation (due, perhaps, to
network or memory contention), then the nonblocking queue
will have better performance because the fastest operations are
favored. If, on the other hand, some processors are always fast
and some always slow, then the slow processors are starved
and the result is a less efficient data structure. If the underlying

Blocking queue

--t Constant speed

l
i Varying speed

Fig. 13. Analytical results for exponential execution times

Comparison of Execution Time
For Sucessful Operations
Execution time

2

1

o
c

-

1

2

1

3

4

5

6

7

8

Concurrent operations

-

Blocking queue

+ Constant speed

+ Varying

speed

Fig. 14. Analytical results for exponential execution times.

architecture provides uniform memory access (UMA), then the
nonblocking queue will provide better performance than the
blocking queue. If the underlying architecture provides nonuniform memory access (NUMA), then processors that are
distant from the queue will be starved and the nonblocking
queue will be worse than the blocking queue. The alternative
in a NUMA architecture is a wait-free implementation [14],
but wait-free data structures require significant overhead.

REFERENCES
“TC2000 Programming Handbook,” BBN Advanced Computers, Inc.
A. Gottlieh, B. D. Lubachevsky, and L. Rudolph, “Basic techniques
for the efficient coordination of very large numbers of cooperating
sequential processors,” ACM Trans. Programming Languages Syst., vol.
5 , no. 2, pp. 16&189, Apr. 1993.
M. Herlihy, “A methodology for implementing highly concurrent data
structures,” in Proc. 2nd ACM SICPLAN on Principles and Practice of
Parallel Programming, Mar. 1989, pp. 197-206.
M. Herlihy and I. Wing, “Axioms for concurrent objects,” in 14th ACM
Symp. Principles of Progrumming Languages, Jan. 1987, pp. 13-26.
M. P. Herlihy, “Impossibility and universality results for wait-free
synchronization,” in Seventh ACM SIGACT-SIGOPS Symp. Principles
of Distributed Computing, Aug. 1988, pp. 276-290.
H. T. Kung and J. T. Rohinson, “On optimistic methods for concurrency
control,” ACM Trans. Database Sysr., vol. 6, no. 2, pp. 213-226, 1981.
L. Lamport, “Specifying concurrent program modules,” ACM Trans.
Programming Languages Syst., vol. 5 , no. 2, pp, 190-222, Apr. 1983.
“M68000 Family Programmer’s Reference Manual,” Motorola.

PRAKASH et al.: A NONBLOCKING ALGORITHM FOR SHARED QUEUES USING COMPARE-AND-SWAP

S. Prakash, “Non-blocking algorithms for concurrent data structures,”
Masters thesis, Univ. of Florida, Dept. of Computer Sci., 1991. Available
at anonymous ftp site cis.ufl.edu:cis/tech-reports/tr9l/tr9 1-002.ps.Z.
I. K. Ryu and A. Thomasian, “Performance analysis of centralized
databased with optimistic concurrency control,” Performance Evaluation
7, pp. 195-211, 1987.
J. M. Stone, “A simple and correct shared-queue algorithm using
compare-and-swap,” in Proc. IEEE Comput. Society and ACM SIGARCH
Supercomputing ‘90 Cont, Nov. 1990, pp. 495-505.
D. Shasha and N. Goodman, “Concurrent search structure algorithms,”
ACM Trans. Database Syst., vol. 13, no. 1, pp. 53-90, Mar, 1988.
R. Kent Treiber, “Systems programming: Coping with parallelism,” RJ
51 18, IBM Almaden Res. Ctr., Apr. 1986).
J. Turek, “Resilient computation in the presence of slowdowns,” Ph.D.
Thesis, Dept. of Comput. Sci., NYU, 1991.
J. Turek, D. Shasha, and S. Prakash, “Locking without blocking: Making
lock based concurrent data structure algorithms nonblocking,” in ACM
Symp. Principles of Database Syst., 1992, pp. 2 12-222.
IBM T. J. Watson Res. Ctr., Systeml37O Principles of Operations, 1983,
pp. 7.13, 14.
C.-Q. Zhu and P.-C. Yew, “A synchronization scheme and its applications for large multiprocessor systems,” in Proc. 4th Int. Con& Distrib.
Computing Syst., May 1984, pp. 48-93,

Sundeep Prakash received the B.Tech degree in electrical engineering from
the Indian Institute of Technology, Delhi, India, in 1989, and the M.S. degree
in computer science from the University of Florida, Gainesville, in 1991.
Since September 1991, he has been working towards the Ph.D. degree in
computer science at UCLA. His research interests lie in the area of parallel
and distributed computing.

559

Yann-Hang Lee received the B.S. degree in engineering science and the M.S. degree in electrical
engineering from the National Cheng Jung University, in 1973 adn 1978, respectively, and the
Ph.D. degree in computer, information, and control
engineering from the University of Michigan, Ann
Arbor, MI in 1984.
From December 1984 to August 1988, he was a
Research Staff Member at the Architecture Analysis
and DesiEn Group, IBM T. J. Watson Research
Center, Yorktown Heights, NY Since August 1988,
he has been an Associate Professor with the Computer and Information
Sciences Department, University of Florida, Gainesville His current research
interests include distributed computing and parallel processing, real-time
systems, computer architecture, performance evaluationm, and fault-tolerant
system
Dr Lee is a member of the Association for Computing Machinery and the
IEEE Computer Society

Theodore Johnson received the degree in mathematics from the Johns Hopkins University in 1986,
and the Ph.D. degree in computer science from
the Courant Institute of New York University in
1990.
Since 1990, he has been an Assistant Professor at
the University of Florida, Gainesville. His research
interests include the study of parallel, distributed,
and concurrent processing, performance modeling,
random data structures, and real-time systems.
Dr. Johnson is a member of the Association for
Computing Machinery and the IEEE Computer Society.

Real-Time Syst (2007) 36: 47–74
DOI 10.1007/s11241-007-9016-3

Schedulable garbage collection in CLI virtual execution
system
Okehee Goh · Yann-Hang Lee · Ziad Kaakani ·
Elliott Rachlin

Published online: 6 April 2007
© Springer Science+Business Media, LLC 2007

Abstract Virtual software execution environment, known as Virtual Machine (VM),
has been gaining popularity through Java Virtual Machine (JVM) and Common Language Infrastructure (CLI). Given their advantages in portability, productivity, and
safety, etc., applying VM to real-time embedded systems can leverage production
cost, fast time-to-market, and software integrity. However, this approach can only become practical once the VM operations and application tasks are made schedulable
jointly.
In this paper, we present a schedulable garbage collection algorithm applicable on
real-time applications in CLI virtual machine environment. To facilitate the scheduling of real-time applications and garbage collection operations, we make the pause
time due to garbage collection controllable, and the invocation of garbage collection
predictable. To demonstrate the approach, a prototype for a schedulable garbage collection has been implemented in CLI execution environment. The garbage collection
is carried out by a concurrent thread while meeting a targeted pause time and satisfying the memory requests of applications. A cost model of garbage collection is
established based on measured WCET such that the execution time and overhead of
garbage collection operations can be predicted. Finally, we illustrate a joint scheduling algorithm to meet the time and memory constraints of real-time systems.
O. Goh () · Y.-H. Lee
CSE, Arizona State University, Tempe, AZ, USA
e-mail: ogoh@asu.edu
Y.-H. Lee
e-mail: yhlee@asu.edu
Z. Kaakani · E. Rachlin
Honeywell International Inc., Phoenix, AZ, USA
Z. Kaakani
e-mail: Ziad.Kaakani@honeywell.com
E. Rachlin
e-mail: Elliott.Rachlin@honeywell.com

48

Real-Time Syst (2007) 36: 47–74

Keywords Real-time garbage collection · Virtual machine · Garbage-collection
scheduling · Garbage collection cost model

1 Introduction
Real-time embedded systems have been changing rapidly these days as the application areas for the systems get proliferated from industrial controls to home automation, communication equipment, consumer gadgets, medical devices, defense systems, etc. The apparent trends of the systems are shortly summarized as “increased
software portions to support complex features”, and “connectivity and heterogeneity” (Sun Microsystems Inc. 2004). The emerging features provided in the systems
are often implemented by software programs rather than hardware as a cost-effective
approach to realize sophisticated functionality. Furthermore, heterogeneous embedded systems get connected to allow information fusion and to provide coherent services in distributed environments.
This software centric development of real-time embedded systems demands a robust execution environment and programming language support. Currently, most applications in real-time embedded systems are developed in C and C++ languages.
While C and C++ programs can be optimized to attain good execution performance,
the approach is short in supporting software productivity and portability. For instance,
memory leak problems, which can crash the systems, are not easily observed and
fixed. The dependency on platforms (processors, compilers, operating systems, etc.)
certainly hinders the portability of C and C++-based programs. Hence, there is a need
to establish an abstraction of real-time embedded system platforms to improve software development process and to enable a safe execution environment.
One of the abstract software execution environment is Virtual Machine (VM)
which has been gaining popularity through Java Virtual Machine (JVM) and Common Language Infrastructure (CLI). The features of VM, such as portability, productivity, and safety can make VM an attractive environment for real-time embedded
systems to lower production cost, enable fast time-to-market, assure high integrity,
etc. On the other hand, to support timely and concurrent operations, and bounded
resource usage required in real-time embedded systems, VM operations must be deterministic and schedulable. However, both JVM and CLI, which are not designed
for real-time applications, include operations that cause extended and unpredictable
blocking delays.
One of notable unpredictable VM operations is garbage collection. Many virtual
machines utilize garbage collection (GC) (Jones 1999) which automatically reclaims
not-anymore referenced memory to refill available free memory. It helps relieve application developers from mishandling memory references as well as prevent memory
leak. However, despite the benefits of GC, GC’s unpredictability in terms of its response time and invocation is an obstacle of meeting timing constraints and memory
availability for real-time embedded applications.
To make GC suitable for real-time embedded systems, we can take the following
approach. Firstly, to avoid unpredictable blocking delay, GC’s operation should be
preemptible and, if a portion of the operation is not preemptible, the length of pause
times should be bounded and set with a fine granularity. Thus, real-time applications

Real-Time Syst (2007) 36: 47–74

49

Fig. 1 A scheduling model in the schedulable garbage collection

with timeliness requirements will not suffer long blocking delays. Secondly, available
memory resources for applications must be predictable. Finally, the GC operations
may run concurrently with applications and there is a need to have an integrated
scheduling for both GC operations and application tasks.
In this paper, we present a complete approach to design, implement, and analyze
schedulable garbage collection for embedded applications in CLI virtual machine.
The schedulable garbage collection (S-GC) is made of preemptible activities and
the pause time of GC operations is controllable with a fine granularity. The operation model of the S-GC can be depicted in Fig. 1 where GC activation cycle (or GC
cycle in short) refers to the period from the moment that the GC is triggered until
all garbage memory is reclaimed. Once a GC cycle is triggered, GC work is carried
out in multiple GC increments which have a bounded execution time and are invoked
periodically. Thus, GC operations, conducted by a separate task, are interleaved with
the execution of mutators. To show the applicability of the design, we develop a prototype for the schedulable garbage collection on a CLI platform, and its performance
is experimented. A cost model of the garbage collection is derived from the prototype
implementation and is used to predict the WCET (Worst Case Execution Time) of GC
for scheduling. Using a periodic task set, a scheduling algorithm is examined through
an experiment on the prototype. This paper is extended to embrace a thorough cost
model of garbage collection from our previous papers, (Goh et al. 2005) in which
the design of the schedulable garbage collection in CLI environment is proposed,
and (Goh et al. 2006) in which an integrated scheduling model of garbage collection
with applications is discussed.
The paper is organized as follows. Section 2 presents the background of CLI and
garbage collection. Section 3 introduces the schedulable GC’s design and prototype
implementation. The performance results are analyzed in Section 4, and the cost
model based on the proposed design of the schedulable GC is formulated in Section 5. In Section 6, we present an integrated scheduling algorithm for real-time periodic tasks and GC, and the experimental results of the scheduling algorithm in CLI.
At last, Section 7 discusses related works, and Section 8 draws a conclusion.
2 Background
In this section, we firstly survey the status of embracing VM in real-time applications,
and introduce CLI. Then we briefly discuss the background of garbage collection,

50

Real-Time Syst (2007) 36: 47–74

and Boehm–Demers–Weiser garbage collector (BDW GC) v6.1 (Boehm and Weiser
1988), which is the base garbage collector of our prototype.
2.1 VM for real-time applications
Although VM’s features such as portability, multithreading, highly secured sandbox
model, and programming productivity are attractive to real-time embedded systems,
its nondeterministic behavior hinders VM’s applicability to real-time embedded systems. The nondeterministic behavior comes from garbage collection’s unbounded response time, dynamic class loading, Just-In-Time (JIT) compilation, unbounded priority inversion due to access to critical section, inappropriate thread scheduling, etc.
In addition to these limitations, VM shows a lack of certain features necessary to low
level programming such as event handling and physical memory access. These issues
have motivated research to make JVM suitable for real-time embedded systems: realtime garbage collection (Bacon et al. 2003c; Henriksson 1997; Kim and Shin 2003;
Cheng and Blelloch 2001), optimized JIT, adopting Initialization Time Compilation
or Ahead-of-Time compilation (AOT) and so on. JVM aimed for embedded applications by partially addressing these issues includes Metronome project (Bacon et
al. 2005), JamaicaVM (AICAS GmbH 2006), KVM (Sun Microsystems Inc. 2000),
JBed (Esmertec Inc. 1999), etc. In addition, coordinated research efforts to extend
JVM for real-time embedded systems resulted in Real-Time Specification for Java
(RTSJ) (Bollella et al. 2000), which is a standard specification of JVM to meet the requirements of real-time embedded systems. PERC (Aonix North America Inc. 2006)
and Sun Real-Time Java System (Sun Microsystems Inc. 2005) are available commercial implementations of RTSJ.
2.2 CLI
CLI (ECMA 2002)—a ECMA (European Computer Manufacturers Association)
specified standard for a core technology of Microsoft .NET framework—is aimed
to make it easy to write components and applications with multiple languages and for
multiple platforms. Applications of multiple languages are translated into intermediate language code (Common Intermediate Language or CIL in short) which is executed in a virtual execution system (VES). A rich set of types defined in CLI permits
the interoperations among components written with CLI-compliant languages. In addition, CLI defines metadata, which allows each component to carry self-describing
information so that languages can be extensible without introducing new keywords.
The VES in CLI, similar to JVM, is an abstract stack-based machine featuring
loader, verifier, JIT compiler, garbage collector, security system, multiple threading,
exception handling mechanism, etc. The ECMA standard for CLI does not confine
a specific garbage collection algorithm for VES. However, if the implementation of
a garbage collector in CLI moves memory objects, the garbage collector should be
aware of a unique instruction ‘pinned’: while a method with a pinned variable is executing, a garbage collector should not relocate the object referenced by the variable.
This is to ensure memory objects not to be relocated if the objects are accessed by
unmanaged pointers, which are not traced by a garbage collector. The unmanaged
pointers in CLI are limited to access primitive types. But, an unmanaged pointer can

Real-Time Syst (2007) 36: 47–74

51

access to a value type field of an instance of a reference type. If a garbage collector
moves the instance (a memory object), it may cause a problem. Users should embrace
the pointer’s access with a keyword, ‘fixed’ so that a compiler can generate a ‘pinned’
construct.
Microsoft .NET (Microsoft Corp. 2006) and WinCE .NET are commercial products of CLI. Microsoft’s SSCLI (Microsoft 2006), Ximian/Novell’s MONO (Ximian
2005), and DotGNU Portable .NET are available as open-source implementations
of CLI. C#, Managed C++, Visual Basic .NET, Visual J# .NET, and JavaScript are
available CLI-compliant languages. To the best of our knowledge, there is no implementation of CLI aiming to support time-constrained embedded applications.
MONO (Ximian 2005), which is the prototype platform of our schedulable
garbage collection, is an open source development platform of Microsoft .NET
framework and runs on most of Linux platforms of various architectures. Following ECMA standard, MONO provides C# compiler, VES, and standard class libraries. The VES in MONO consists of well-optimized JIT and AOT, and the Boehm–
Demers–Weiser (BDW) garbage collector (Boehm and Weiser 1988), etc.
2.3 Garbage collection
Mark and sweep garbage collection (Jones 1999), on which our schedulable GC is
based, is a simple and efficient trace-based garbage collection algorithm. It determines the memory objects’ liveness through reachability. The mark-sweep garbage
collection operates primarily in two phases: a mark phase and a sweep phase. The operations in a mark phase traverse and mark all objects directly or indirectly reached
from a root-set that includes processor registers, program stacks, and global variables. The objects marked (reached) in the mark phase are considered alive. Then, in
a sweep phase, the objects not marked (not reached) in the mark phase are considered
as garbage objects and the garbage objects are reclaimed to fill up free memory.
One of issues in real-time garbage collection is to bound the pause time due to
garbage collection. Incremental garbage collection distributes its operations into incremental steps and relinquishes CPU after each step, instead of performing an entire
garbage collection cycle continuously. That is, incremental garbage collection allows
applications (mutators) to interleave with a running garbage collector. The memory
freed during the cycle is not available to mutators until the cycle completes. To satisfy memory allocation requests by mutators while a garbage collection cycle is in
progress, a proper amount of free memory must be reserved when the garbage collection cycle starts. As well, to prevent mutators from running out of memory before
the garbage collection cycle completes, the progress of incremental garbage collection should be able to pace memory consumption rates. A typical example of this
algorithm is to collect certain amounts of garbage memory whenever a memory allocation request of application arrives (Wilson 1992). However, associating garbage
collection operations to the memory allocation requests can lead to a burst of pauses
when a burst of memory allocation requests arrives. Then, it may result in depriving the execution opportunity for the mutators. As a metric for mutator’s sufficient
progress during incremental garbage collection, Cheng et al. (Cheng and Blelloch
2001) introduced a new metric, Minimum Mutator Utilization (MMU), i.e., a minimum fraction of time which mutators execute at any time window. The metric can
characterize the length and placement of the GC pauses.

52

Real-Time Syst (2007) 36: 47–74

In incremental garbage collection, the interleaved executions of applications (mutators) and a garbage collector can cause a memory inconsistency problem. That
is, the reachability status of memory objects known to the garbage collector can
be changed by the mutators while they are accessed by the GC. Dijkstra’s tricolor marking algorithm (Dijkstra et al. 1978), and barrier algorithms (Yuasa 1990;
Jones 1999) present a synchronization mechanism between a garbage collector and
mutators to reveal any changes of the connectivity among memory objects.
RTSJ introduced an extended memory model, immortal memory and scoped memory to have real-time code avoid the latency of garbage collection: a hard real-time
thread (realized as NoHeapRealTimeThread) is not allowed to use or access a heap
memory which is garbage collected, but restricted to use immortal memory and/or
scoped memory. Although RTSJ can provide expressive power on programming realtime applications and also the new memory model can help meet the requirements of
extremely low latency for certain hard real-time tasks, the complex memory model
of RTSJ imposes several shortcomings (Bacon et al. 2003b): it limits data sharing
between threads using a heap memory and threads not using the heap memory; it is
hard to determine that which memory objects must be placed in immortal or scoped
memory for NoHeapRealTimeThread (threads not using a heap memory); standard
class libraries designed to use a heap memory are not compatible with NoHeapRealTimeThread so that they must be rewritten.
2.4 BDW garbage collector
The Boehm–Demers–Weiser garbage collector (BDW GC) (Boehm and Weiser 1988)
is a variant of conservative mark-sweep garbage collection designed for C and C++
languages. It adopts a two-level hierarchical memory allocation mechanism which
handles the allocations for large and small memory objects separately. A heap memory is divided into large blocks and small blocks: a large block consists of multiple
memory pages, and a small block is only one memory page. For a large object, whose
size is larger than a half of a page, its request is rounded up to a memory space of
multiple pages. Small blocks are subdivided such that segregated lists of free objects
of fixed sizes are maintained. For a small object, its request is rounded up to a proper
size of the maintained free lists. Allocating a small object is to take a first element
from the free list of the proper size. This two-level memory structure helps provide
better memory fragmentation characteristic, and also a constant memory allocation
time in normal cases. In addition, each block’s mark bitmap, which keeps marking
status of objects within the block, is separately placed from the block in a heap memory.
The BDW GC with some compilers, such as GCJ (GNU Compiler for Java) runtime (GNU 2005) and MONO, looks for pointer type information from a compiler
or a runtime environment. GCJ and MONO generate a reference bitmap to indicate fields of a reference type in each class. That is, for objects in a heap memory,
the BDW GC with GCJ runtime and MONO can utilize the reference bitmap to identify pointers in a mark phase. In addition, for variables in a stack memory, the BDW
GC adopts various techniques to minimize the number of false references (e.g., considering an integer as a pointer to a heap object).

Real-Time Syst (2007) 36: 47–74

53

To reduce pause time caused by GC operations, the BDW GC has evolved to
support ‘mostly parallel GC’ for uniprocessor systems (Boehm et al. 1991). The
algorithm to support incremental steps in ‘mostly parallel GC’ is as follows. During a mark phase, marking is done in a small number of objects at every memory
allocation request. In a sweep phase, all memory blocks’ mark bitmaps are examined to check their marking state. The sweep phase immediately reclaims memory
blocks which become garbage entirely, i.e. the blocks are full of garbage object(s).
For the blocks which contain both live and garbage objects, the sweeping operation
for the blocks is made only when a memory allocation request for the same size of
the objects cannot be satisfied. Even though this on-demand memory reclamation
helps distribute the work of a GC cycle, the expense for the reclamation is placed
into memory allocation requests and it becomes difficult to predict the WCET of
the applications. To detect the changes of object connectivity due to concurrent execution with mutators, a write barrier mechanism is implemented in the BDW GC as
follows. Initially, all heap memory pages are protected as read-only by using the virtual memory protection mechanism of operating system (via mprotect system call).
The memory pages updated by mutators are flagged as dirty, and the dirty pages are
re-scanned in a termination step of a mark phase during which all other threads are
suspended. The advantages of this approach include that it does not require a compiler to emit code for write barriers, and it places relatively low overhead on mutators
because only the first update operation on a memory page involves a signal handler
to flag the page.
Although the BDW GC supports efficient memory allocation and partial incremental garbage collection operations, the following limitations make the algorithm hard
to be schedulable. Firstly, the GC operations get started on allocation requests facing
memory shortage and are performed by the application thread that makes the requests. Triggering garbage collection on a basis of allocation makes the collection
work dependent on allocation patterns of applications. Secondly, using virtual memory protection mechanism for a write barrier can lead to a long pause time to protect
all heap pages during the mark phase’s initial step, and to re-scan both dirty pages
and a root-set during the termination step. In addition, using virtual memory protection, which is a system dependent feature, can limit the portability of the GC. Finally,
the GC works incremental partially so that certain phases of GC result in a long pause
time and the duration of the pause time is not feasible.

3 Design and implementation of schedulable garbage collection
To realize the schedulable GC (S-GC), our design assumes a single garbage collector
task in charge of garbage collection (GC task) in uniprocessor systems. The GC task
runs concurrently with application tasks and is subjected to thread scheduling. That
is, a GC cycle can be triggered by specific events and the GC operations are scheduled
to run at certain time intervals. To avoid causing long blocking delays to application
tasks, the GC operations are carried out incrementally during all phases constituting
a GC cycle. Thus, according to applications’ time constraints, the pause times of GC
can be controlled by adjusting the work size of a GC increment before yielding CPU.

54

Real-Time Syst (2007) 36: 47–74

Meanwhile, applying a write barrier at each updated heap object instead of an updated
memory page achieves a fine granularity and thus incurs a minimal latency.
We have implemented a prototype of the schedulable garbage collection by modifying the BDW garbage collector v6.1 (Boehm and Weiser 1988) which is employed
in MONO v0.25, and MONO JIT (Just-In-Time Compiler). In the following, we describe our approaches and several optimization techniques embraced on the development of the prototype.
3.1 Incremental garbage collection
The co-scheduling of GC and real-time application tasks implies that context switches
between the GC task and application tasks are required. Ideally, the GC task should
be immediately preempted whenever high-priority mutators need to run. However,
as the GC task manipulates the data structures for a heap memory, it is required for
the GC task to reach to stable points to safely relinquish CPU and to leave a heap
memory accessible to application tasks. The size of a basic work unit of GC determines the time taken to reach to the stable points and thus the minimal pause time of
a GC increment. Since the pause time can be viewed as a preemption delay, it should
be bounded to meet mutators’ time-constraints.
A GC cycle in S-GC is composed of the following five phases:
• Initialization: The GC task initializes a new GC cycle.
• Root-Scan: The root-set is scanned and the heap references reachable directly from
the root-set are marked.
• Mark: The GC task scans a heap memory and marks all accessible references starting from marked references in a root-scan phase.
• Sweep: The GC task scans a heap memory to reclaim garbage objects.
• Reset: The mark bits of all heap memory blocks are reset.
In our design, the GC operations are incremental during all phases except an initialization phase. The time taken in the initialization phase is a trivial constant. In
a root-scan phase, we focus on two issues. The first one is to reduce the size of a rootset. The second one is to make the root-scan phase incremental. If GC suspends all
application threads at a time to scan the stacks of the threads, the pause time in a
root-scan phase can be extensive and vary according to the number of threads in the
application. To avoid this long delay, we make the root-scan operation preemptible after scanning each thread stack and at the same time, protect the mutators’ changes on
the threads’ stacks that have not yet been scanned. Additional discussion on the operations during a root-scan phase is described in detail in Sect. 3.3.
In other three phases, the basic units of GC work are defined separately as follows.
The basic unit of work in a mark phase is specified as a size of memory to be scanned.
For example, if the unit is set to 4 KBytes (KB), the GC task in the mark phase scans
up to approximately 4 KB heap memory to find reachable references, and marks
the reachable objects. In a sweep phase, the basic unit of work is to examine a single
memory block (4 KB) for garbage reclamation. If the block is an unmarked large
object or the block’s all objects become garbage objects, the block is reclaimed (i.e.,
returned to a large object free list). For the blocks which contain some reachable
objects, all the garbage objects in the block are reclaimed (i.e., returned to the small

Real-Time Syst (2007) 36: 47–74

55

Table 1 Runtime parameters of the schedulable GC
Parameters

Descriptions

GC_TRIGGER_THRESHOLD

Threshold of free heap memory to trigger a GC cycle

GC_INC_PERIOD

A period of a GC Increment

GC_INC_PAUSE

Pause time of a GC Increment

GC_MAX_SCAN_THREADS

Number of threads scanned at a time in root-scan phase

GC_MAX_BLOCKS

Number of memory blocks examined at a time in a sweep phase
and a reset phase

Table 2 CIL reference update
instructions that require a write
barrier

CIL

Descriptions

Stind.ref

Store an object reference at a specified address

Stfld

Store a value in the field of an object

Stsfld

Store a value in the static field of class

Stelem.ref

Store a value in a vector element

object free lists of their corresponding sizes). Finally, in a reset phase, the basic unit
of work is to reset the mark bits of a single memory block. Based on the work units,
the size of work in each phase can be adjustable through runtime parameters listed in
Table 1.
Table 1 also includes the parameters for the scheduling of the GC task. The task
starts a GC cycle either at the instant that the free heap memory goes below a free
memory threshold or periodically. Once a GC cycle starts, GC increments are invoked
periodically and the execution time of each increment is bounded to a targeted pause
time by regulating the amount of GC work. Having the GC task enables an integrated
scheduling algorithm to address the deadlines of applications and the progress of GC
operations.
3.2 Write barriers in a mark phase
To have a consistent view of the objects in a heap memory during a mark phase of
a concurrent and preemptible GC cycle, we use Yuasa’s snapshot-at-the-beginning algorithm (Yuasa 1990) which traps mutators’ updates on heap pointers. In the Yuasa’s
algorithm, the objects which have become unreachable at the time that a GC cycle
is initiated are treated as garbage objects. On the other hand, live objects at the time
that the GC cycle starts are treated as live objects even if they become unreachable
during the cycle. To do this, the algorithm preserves at least one original path to live
objects: when an object is updated to store a new reference by mutators, the overwritten reference must be preserved (that is, it preserves the original path to the old
reference) if it is not yet marked (white according to Dijkstra’s tri-color algorithm). In
this case, the write barrier triggers an operation to mark the old reference and pushes
it into a mark stack (gray). Table 2 lists the Common Intermediate Language (CIL)’s
instructions that require a write barrier due to their update operations to object refer-

56

Real-Time Syst (2007) 36: 47–74

Fig. 2 Indirect internal
functions for CIL reference
update instructions

ences. The objects newly allocated during a GC cycle are marked live (black) to ease
a termination condition in a mark phase.
CLI includes a JIT compiler that translates CIL code into native code. For the CIL
reference update instructions listed in Table 2, MONO JIT is extended to emit function calls to internal procedures for the corresponding update operations. That is,
the execution of the reference update instructions is made by calling the corresponding internal procedures defined in MONO runtime system. To embed write barriers
when a GC cycle is in progress, the emitted call is an indirect call via a function
pointer. The function pointer is altered by the GC task such that, as illustrated in
Fig. 2, the reference update procedure with a write barrier is invoked when a GC cycle is under way, otherwise the one without a write barrier is invoked. This indirect
function call approach eliminates the overhead of checking whether GC is in progress
or not. Once a write barrier is triggered, two operations are performed: the test operation (write barrier test operation) checks the marking status of the old reference, and
if the reference is neither NULL nor marked, an actual guard operation (write barrier mark&scan operation) makes the old reference marked and pushed into a mark
stack.
3.3 Write barriers and optimization in a root-scan phase
We set a single thread stack as a basic unit of work to scan in a root-scan phase.
Hence, mutators can interleave their execution and make changes to their stacks after or before each thread stack is scanned during a root-scan phase. In this incremental root-scan phase, the Yuasa’s algorithm of graying the old references (overwritten reference) of objects being updated cannot provide a consistent snapshot because the algorithm cannot handle changes incurred on thread stacks. The example
in Fig. 3 shows that the algorithm cannot preserve the original paths to the old references which are stored on a thread stack. Let us assume that the stack of a thread
TH 1 is scanned but the stack of a thread TH 2 is not yet scanned at Fig. 3(a). When
TH 1 executes, a reference to an object O2 is stored to one of fields in an object O1
at (b). At (c), the thread TH 1 returns the reference to an object O2 to a local variable
of a stack frame B of TH 1 , and eliminates the reference from the object O1 . And
at the same time, thread TH 2 ’s stack frame C which has the reference to the object
O2 pops up (assume that all the paths to the object O2 is removed). This causes that
the object O2 is left unmarked although it is reachable through a path from the stack
frame B of TH 1 .
This problem is solved by extending the Yuasa’s algorithm. The extended algorithm grays a new reference (a target operand) as well as an old reference of objects

Real-Time Syst (2007) 36: 47–74

57

Fig. 3 A possible problem caused if a new value in an incremental root scan is not grayed

when the objects are updated by the CIL reference update instructions listed in Table 2. The algorithm prevents connectivity loss due to updates on stacks in the incremental root-scan phase. This algorithm is similar to the write barrier suggested
in (Doligez and Leroy 1993) where the write barrier is used to synchronize the mutators’ operation of graying their own stacks and GC task to start marking at the same
time.
If a thread stack is deeply nested, our per-stack approach may still cause a long latency while scanning the whole stack. The approach can be extended, if necessary, to
apply a write barrier to trap updates on thread stacks. The CIL instructions performing stack manipulations must be protected through a write barrier. They include starg
and stloc.index: starg is an instruction that stores the value of a evaluation stack
into an argument of method, and stloc.index is an instruction that stores the value
of a evaluation stack into local variable numbered with index. However, this is very
expensive approach because these stack update operations are much more frequent
compared to update instructions on objects.
Incremental root-scan algorithms to bound scanning a deeply nested stack are suggested in (Yuasa et al. 2000) and (Siebert 2001). To scan a stack frame at a time,
Yuasa et al. (2000) suggested a return barrier that keeps a current active frame always
scanned. It is based on the observation that changes on a stack always occur in a current active frame. The return barrier works when a callee returns to a caller: a special
return routine inserted to a stack frame scans a new active frame (caller’s stack frame)
in advance if the frame is not yet scanned. Siebert et al. (2001) suggested a technique
for constant-time root-scan such that all references of thread stacks are tracked in
shadow stacks placed on a heap memory, and the shadow stacks are scanned incrementally during a GC cycle. This requires a compiler to instrument applications to
construct shadow stacks for references on a stack. However, both approaches still
pose a substantial overhead at runtime.
In a root-scan phase, GC scans the whole root-set which is formed of static data,
processor registers, and thread stacks. If GC does not have any information about
which global variables in the static data actually refer to heap objects, the entire data
segment of a process is treated as static data of a root set. To reduce the size of the
root-set, we only trace global variables that refer to heap objects; to identify the variables, the variables are registered when references are established. Our experiment
shows that the size of static data scanned in a root-scan phase decreases from about

58

Real-Time Syst (2007) 36: 47–74

340 KB (the size of data segment of MONO) to about 5 KB, and the time taken to
scan the reduced static data area becomes negligible.
The other issue in a root-scan phase is how to identify the range of active stack
frames. This is an implementation issue specific to MONO in Linux, in which each
CLI thread is mapped to a pthread with a default size of stack (2 MBytes for threads
in Linux). Mostly, the active range (used as stack frames) within a thread stack is
much less than the default stack size. Thus, to avoid extra overhead, only the active
range should be scanned instead of the entire stack area given by default. To identify
the active range of a thread stack, the current stack-pointer must be retrieved. The
approach used by the BDW GC is to put a signal handler for each thread such that,
when a corresponding signal is received, the signal handler posts a current stackpointer of the thread. This is likely to result in an unbounded delay as a garbage
collector must wait until all threads respond to the signal. Instead of using a signal
handler, we refer a /proc/ pseudo file to get a current status of a process in Linux.
This technique is available in Linux where a thread is implemented using a process.
3.4 Limitation
The BDW GC is mark-sweep garbage collection that does not employ memory compaction. Hence, it cannot completely avoid memory fragmentation even though its
two-level hierarchical memory allocation mechanism helps reducing the rate of fragmentation. Memory fragmentation is one of sources that hinder the precise prediction
of the size of free memory.
Bacon et al. (2003a, 2003c) suggested a defragmentation technique which relocates live objects in less crowded pages to more crowded pages. Their experiment, however, showed that the cost for defragmentation is still expensive. They also
showed that, by applying a memory allocation strategy of allocating objects to newly
reclaimed free space (objects recently died), objects’ time to die gets along with their
neighbors and the rate requiring defragmentation occurs sparsely. Their defragmentation technique and memory allocation strategy can be applied to the BDW GC because both have similar memory allocation structures, i.e., heap is divided into blocks
and a block is divided into fixed sized objects. As we focus on minimizing a workunit at each phase of GC so that the GC increments can have a bounded pause-time,
the defragmentation issue is not addressed in our work.

4 Experiments
We present the experimental results of the prototype of the schedulable GC in this
section. The experiment focuses on showing the performance of S-GC, including
the flexibility of bounding the pause times of GC increments, and the execution overhead of running S-GC in a preemptible mode.
Experiments on the prototype are conducted in a PC workstation with a 1.5 GHz
Pentium IV processor and 256 MB memory. To have a high resolution timer and preemptive kernel, TimeSys’ Linux/Real-Time (v4.1.147) (TimeSys Corporation 2004)
is used.

Real-Time Syst (2007) 36: 47–74

59

Table 3 GC’s benchmark applications
Benchmarks

Descriptions

DirectedGraphs

Constructing and restructuring a directed weighted graph with 200 nodes

ThreadGraphs

10 threads constructing a directed graph with 50 nodes; each thread repeats
the job 50 times

AHC

A utility compressing and uncompressing data of an input file with Adaptive Huffman Compression

MultiStructures

Running 20 threads conducting the creation and modification of objects
with diverse data types and data structures including array, linked list, hash
table, etc.

Periodics

A single task running periodically by generating objects including array of
reference type, linked list; the life time of objects are adjusted on the basis
of the period of the task

In Table 3, the synthetic benchmark applications used in our experiment are listed.
DirectedGraphs, and ThreadGraphs were adopted from stress test applications of
garbage collection in SSCLI v1.0 (Microsoft 2006). AHC was introduced by Microsoft Research Center (Microsoft Corp. 2005). MultiStructures and Periodics were
the applications to explore diverse data structures with multithreads, and to simulate
periodic activities, respectively. To the best of our knowledge, no time-constrained
benchmark applications for CLI are available. The selected benchmark applications
in Table 3 do not completely fit into that domain either. However, they are still sufficient to measure the performance of GC and its pause times, as well as to derive
a cost model for S-GC.
4.1 Controllable pause time of schedulable garbage collection
The experiment firstly examines whether the proposed schedulable GC is able to
bound the pause times of GC increments. In addition, we compare the performance
of the S-GC with that of a stop-the-world GC of the BDW GC (STW-GC). In STWGC, once a GC cycle starts, it runs until the whole GC operation completes. The
experiment of the mostly parallel BDW GC is not presented here because, in MONO,
the incremental feature of the BDW GC cannot be enabled; the write barrier using
virtual memory protection mechanism cannot be established.
For the STW-GC, the size of a heap memory for the experiment was determined
with a minimal size with which the benchmarks run without suffering from out-ofmemory problem. For the S-GC, a heap memory is set to 1.5 times larger than that of
the STW-GC because incremental GC requires reserving free memory before a GC
cycle starts. The 50% space overhead was set aside to guarantee the availability of
memory for the applications while a GC cycle is in progress. The space overhead can
be examined by considering application’s memory related parameters, and the schedule of GC such as maximum live memory, maximum amount of memory allocation,
and GC task’s deadline. We will discuss this issue in detail in Section 6. Accordingly, the threshold of free heap memory triggering a GC invocation was set to 33%
of the heap memory. The pause time and the period of GC increment in S-GC were
arbitrarily set to 1 ms and 5 ms, respectively.

60

Real-Time Syst (2007) 36: 47–74

Table 4 A profile of the STW-GC
Benchmarks

Heap
(KB)

GC
cycle
count

Avg.
GC
pause
(µs)

Max.
GC
pause
(µs)

Total
GC
time
(ms)

App.
Exec.
time
(ms)

DirectedGraphs

1280

8

7101

10 014

57

6215

ThreadGraphs

5120

93

3099

3705

288

11 645

AHC

26 624

4

8234

8850

33

4694

MultiStructures

2048

11

4050

7008

45

775

Periodics

1580

17

2895

3754

49

2839

GC
cycle
count

Avg.
GC
pause
(µs)

Max.
GC
pause
(µs)

Table 5 A profile of the S-GC
Benchmarks

Heap
(KB)

Total
GC
time
(ms)

App.
Exec.
time
(ms)

DirectedGraphs

1920

7

868

964

49

6166

ThreadGraphs

7680

85

835

984

273

11 821
4737

AHC

39 936

4

880

961

26

MultiStructures

3072

13

862

1028

66

814

Periodics

2370

15

788

967

46

2843

The garbage collection profiles for the STW-GC and the S-GC are presented in
Table 4 and Table 5, respectively. The data are collected after 20 experimental executions of each application. The result in Table 5 shows that the maximum pause time
can meet the targeted pause time in the given experimental environment. The pause
time for MultiStructures is slightly higher than the targeted pause time. However, it
can also be met by adjusting the runtime parameters of S-GC described in Table 1.
The total execution time of benchmark applications with the S-GC is in general
greater than that with the STW-GC. The S-GC incurs additional overhead attributed
by the GC scheduling mechanism and the synchronization operations between interleaving garbage collation and mutators. However, the result also shows that the execution time of the S-GC for DirectedGraphs benchmark is smaller than that of the STWGC. The comparison of the times spent on garbage collection between the STW-GC
and the S-GC is not trivial with following reasons. Firstly, in the STW-GC, garbage
collection is carried out in the context of application threads that encounter a shortage
of free memory. In the S-GC, garbage collection is conducted by a separate thread
that runs with a higher priority than that of application threads. Secondly, the S-GC
task figures out the top stack-pointers of all other mutators by examining the /proc
file whereas the STW-GC uses a signaling mechanism to obtain the information of
the allocated stack memory of each thread. These differences may lead to a shorter
garbage collection time in the S-GC for some benchmarks.

Real-Time Syst (2007) 36: 47–74

61

Table 6 The overhead of write barriers
Benchmarks

Pause time = 1 ms
GC
(ms)

Directed-

Exec.
(ms)

Pause time = 2 ms
WB
Test

WB

GC
(ms)

Exec.
(ms)

WB
Test

WB

49

6166

2466

17

47

6188

790

0

273

11 821

36 118

550

245

11 753

10 069

0

AHC

26

4737

6598

3

25

4724

723

0

Multi-

66

814

499 410

141

76

814

218 751

26

46

2843

668 313

99

47

2843

265 491

0

Graphs
ThreadGraphs

Structures
Periodics

4.2 Overhead of schedulable garbage collection
Running S-GC in a preemptible mode incurs various overhead especially due to write
barrier operations. It is not easy to measure the overhead of write barriers because
the overhead is distributed throughout the execution of mutators. Because of that,
we try to understand the overhead by examining the number of write barrier invocations. The experiment was conducted under two different pause times for GC increments. However, both used the same period for GC increments. The expectation is
that a longer pause time may cause less number of write barrier invocations comparing to the case of a shorter pause time. The target pause times were set to 1 ms, and
2 ms in the experiments. The period of the GC increment was set to 5 ms in both.
The result in Table 6 shows that application execution time is slightly reduced for
the pause time of 2 ms. In fact, the longer the pause time is, the shorter a GC cycle is because the interval of interleaved execution with mutators is reduced. Hence,
the execution duration that requires write barrier operations is less. Table 6 also shows
the number of write barrier test operations (WB Test) and the number of write barrier
mark&scan operations (WB) conducted in both cases. The differences clearly indicate that the CIL reference update instructions that require actual write barriers at
runtime are much less than the instructions generated at compile time. In addition,
the difference of the counts with 1 ms and 2 ms pause times explains the reduction
of application execution time resulted from the elimination of write barrier test operations and write barrier mark&scan operations.

5 A cost model of garbage collection
5.1 A cost model
The integrated scheduling of real-time applications and GC operations requires precise prediction of the cost and overhead of GC operations. In this section, we establish a cost model to derive the WCET of the proposed algorithm for the schedulable

62

Real-Time Syst (2007) 36: 47–74

Table 7 Cost equations of GC
cycle’s all phases

GC phases

Cost equations

Initialization

CI = Constant

Root-Scan

CR = Πr1 R1 + Πr2 R2 + Πr3 R3

Mark

CM = Πm1 M1 + Πm2 M2

Sweep

CS = Πs1 S1 + Πs2 S2 + Πs3 S3

Reset

CC = Πc1 S1

WriteBarrier

CW = Πw1 W1 + Πw2 W2

Table 8 Memory and application parameters used in cost equations
Parameters

Descriptions

R1

Number of objects marked in a root-scan phase

R2

Size of memory scanned in a root-scan phase (KB)

R3

Number of thread stacks scanned in a root-scan phase

M1

Number of objects marked in a mark phase

M2

Size of a heap memory scanned in a mark phase (KB)

S1

Number of memory blocks examined in a sweep phase

S2

Number of heap memory blocks completely full of garbage objects and reclaimed in a sweep phase

S3

Number of heap memory blocks containing garbage objects partially and reclaimed in a sweep phase

W1

Number of CIL reference update instructions that conduct a write barrier test
operation

W2

Number of CIL reference update instructions that conduct a write barrier
mark&scan operation

garbage collection. Hereafter, we use GC to indicate the schedulable GC unless otherwise noted.
EGC = CI + CR + CM + CS + CC .

(1)

In (1), the execution time of a single GC cycle is defined as the sum of costs
during the phases constituting a GC cycle, as specified in Sect. 3.1. The cost of each
phase depends upon the operations during the phase and is illustrated in Table 7. The
definitions of memory and application parameters referred in the cost equations are
given in Table 8. Table 9 gives the measured worst case execution time (WCET), in
terms of number of CPU cycles and time, for each single step of the operations. In
addition to the cost of GC operations, application tasks may suffer a delay due to
write barrier operations. Thus, for schedulability analysis, this cost must be weighed
on mutators. The cost of write barrier operations, Cw can be expressed as a function of
the number of write barrier test operations and the number of write barrier mark&scan
operations as shown in Table 7.
The worst case execution times given in Table 9 are measured as follows. To enable a consistent measurement environment, we firstly locked a fixed size of a heap
memory and then conducted experiments in a single user mode. To obtain the num-

Real-Time Syst (2007) 36: 47–74

63

Table 9 Cost coefficients in cost equations
Cost

WCET

WCET

coefficients

(cycle)

(µs)

Operations

Πr1

144

9.65E−02

Mark an object in a root-scan phase.

Πr2

3496

2.34E+00

Scan static data and stacks to find heap memory references in a root-scan phase (µs/KB).

Πr3

114 004

7.64E+01

Get a top stack-pointer of a thread in a root-scan
phase.

Πm1

196

1.31−01

Mark an object in a heap memory (µs/KB) in a mark
phase.

Πm2

12 042

8.07E+00

Scan a heap memory to find references in a mark
phase (µs/KB).

Πs1

88

5.90E−02

Scan a memory block in a sweep phase.

Πs2

1380

9.25E−01

Reclaim a heap memory block completely full of
garbage objects in a sweep phase.

Πs3

8528

5.71E+00

Reclaim a heap memory block partially containing
garbage objects in a sweep phase.

Πc1

420

2.81E−01

Reset mark bits from a heap memory block in a reset
phase.

Πw1

292

1.96E−01

Conduct a write barrier test operation.

Πw2

696

4.66E−01

Conduct a write barrier mark&scan operation.

ber of cycles to perform each operation, the rdtsc instruction provided in Intel x86
was used to read a time stamp counter before and after the operation. As a way to
get each operation’s WCET which results from executing the worst-execution path
to complete the operation, the largest value was chosen from multiple measures of
the operation over the execution of a benchmarking application, DirectedGraph described in Table 3.
The cost model derived is based on the GC algorithm and its implementation.
For instance, the execution time in a sweep phase is dominated by the operation of
sweeping memory blocks that contain garbage objects. The cost of handling each
memory block varies depending on the status of the block, i.e., whether the block
is full of garbage or not. If the block is an unmarked large object (a garbage) or
a small block with all unmarked objects, the entire block is reclaimed to a large free
object list. Otherwise, the garbage objects in the block (which has some reachable
objects) are reclaimed individually to a segregated free object list of a corresponding
size. Thus, the cost equation for CS is a function of the total number of memory
blocks swept, as well as the number of memory blocks of different types. The derived
cost model shows some similarity with the one in (Bacon et al. 2003c; Kim et al.
1999) because of the comparable mark and sweep operations, and also dissimilarity
in the way of the implementation.
The estimated cost derived from the cost model is compared with the measured
cost of the benchmarking applications in Fig. 4 for each phase constituting a GC
cycle. The measured cost of each phase is the average cost across all GC cycles
during the application run that results in the longest execution time among 20 experiments. Some benchmarks (i.e., MultiStructures) show a much larger estimated

64

Real-Time Syst (2007) 36: 47–74

Fig. 4 The comparison of a measured cost and an estimated cost of GC

cost than the measured cost in a mark phase. The difference comes from the fact that
the worst case execution time for each GC operation used in the cost model is derived
from the worst case path among the corresponding operations, but the measured cost
indicates an average cost whose operations may not always hit the worst case paths.
This indicates that the computation of the WCET can be refined by considering additional factors, such as the distribution of pointers as well as the size or the number of
live objects as taken in (Bacon et al. 2003a). However, it is probably more practical
to have a simple and conservative estimate rather than a complex and accurate one.

6 Integrated scheduling for garbage collection and real-time tasks
Similar to real-time tasks which must meet their deadlines, the GC task, once triggered, should complete a GC cycle before the free memory is used up. It is apparent
that the GC task and real-time tasks must be co-scheduled and the availability of
heap must be addressed in the scheduling algorithm. In this section, we firstly suggest a scheduling algorithm for real-time periodic tasks and the GC task, and then
present an experiment result of the scheduling algorithm in a CLI runtime system.
6.1 Garbage collection scheduling algorithm and analysis
We set up a scheduling approach for the GC task and application tasks using a fixed
priority scheduling algorithm. The application tasks, Ti where i = 1, 2, . . . , N , are
assumed to run periodically with their deadlines equal to their periods Pi . Each task’s
worst case execution time is known a priori and its priority is determined based on

Real-Time Syst (2007) 36: 47–74

65

a rate monotonic scheduling algorithm (Liu and Layland 1973) except the GC task,
which is assigned with the highest priority. The assumption of the highest priority for
the GC task is to ensure the progress of GC operations while, by controlling its pause
time, application tasks would not be blocked for the whole GC cycle and meet their
deadlines.
During the execution of application tasks, memory allocation requests must be satisfied. To simplify the memory request model, we assume that the real-time systems
conduct a phased execution consisting of Initialization phase and Steady phase. In an
initialization phase, the system is configured to load required class libraries, initialize
application tasks, construct static objects, etc. In a steady phase, task instances arrive periodically and get executed. Each task instance may construct its objects, and
the maximum amount of memory allocated by an instance of a task Ti is denoted as
Ai . It is assumed that the memory objects created in an instance of a task are not
referred by its successive task instances. That is, the memory objects allocated by an
instance of a task in the steady phase becomes collectable once the instance finishes.
Although we assume a restrictive memory usage to simplify the live memory analysis, this approach of not using dynamically created objects across task instances is
a practical way to build modular software for real-time applications. The computation results generated during a task period usually represent the updated system status
and can be saved in static objects for reference. In order to relax the restrictive memory usage assumption, the techniques to predict a tight upper bound of live memory
can be employed. For example, Persson (Persson 1999) proposed an analysis technique to predict the upper bound of live memory for object-oriented languages by
finding references of activation records of all call chains and applying annotation for
recursive data structures, e.g., linked list or tree.
Our scheduling approach for GC task is depicted in Fig. 5 and controlled by
the following parameters:
• The priority of the GC task, i.e., the highest priority.
• The minimum triggering interval of GC cycles (PGC ), and the maximal allocated
memory (HAllocated ) which should be bounded to the given size of a heap memory
(H ).
• Once a GC cycle is triggered, a fixed execution time quantum for each GC increment (EGCI ) is assigned. The GC increments are invoked periodically with a fixed
period (PGCI ) until the GC cycle completes. The deadline of a GC cycle for the GC
task is denoted with DGC .
Predicting the upper bound of live memory (i.e. the memory used for live objects)
is essential because the size of live memory determines the execution cost of marksweep garbage collection algorithms and the required memory size. The memory
objects survived from the initialization phase in the system are likely to last during
the system’s lifetime. Thus, the maximum live memory consists of live memory allocated in the initialization phase (LC ), and live memory allocated in the steady phase
by all active task instances (LD ). To get the upper bound of LD , we consider that
all tasks are active at the moment that a GC cycle is triggered, and that all memory
objects allocated for each active instance are alive. Thus, with active instances for N

66

Real-Time Syst (2007) 36: 47–74

Fig. 5 GC scheduling and memory allocation

tasks at the same moment, we have the maximal live memory equal to
Lmax = LC + LD = LC +

N


(2)

Ai .

i=1

In terms of the size of a heap memory, we can obtain a bound on the allocated
memory following the curve depicted in Fig. 5. Right before the GC task returns
the reclaimed objects back to the heap memory at the end of a GC cycle, the allocated
heap reaches the maximum and is used for three types of objects: (i) the long live
objects allocated in the initialization phase and the live objects allocated by active
task instances (live objects), (ii) the objects allocated by the task instances which
end before the current GC cycle starts (garbage objects), and (iii) the objects that are
freed since the current GC cycle starts (floating objects). The maximal allocated heap
memory, which must be less than the total heap size H, can be bounded as

HAllocated ≤ Lmax +


N 

PGC + DGC
i=1

Pi





− 1 × Ai ≤ H.

(3)

Note that the allocated heap memory in the above equation consists of the live
objects created during the initialization phase and the active task instances, and
the garbage objects created by the instances since the beginning of the previous
GC cycle. The worst-case execution time for a GC cycle, EGC , can be computed
via the cost model of garbage collection given in (1) of Section 5 by considering
the maximal live memory, total heap memory size, maximal garbage memory, etc.

Real-Time Syst (2007) 36: 47–74

67

Since the GC runs at the highest priority, we have the following equation for the deadline of each GC cycle, DGC , which depends upon the parameters EGCI and PGCI , and
the estimated EGC :




EGC
EGC
(4)
DGC =
× PGCI + EGC −
× EGCI .
EGCI
EGCI
The threshold of free heap memory (HTH ) to trigger a GC cycle must be greater
than the amount of memory allocated during a GC cycle and can be computed using
the equation:


N 

DGC
× Ai .
(5)
HTH =
Pi
i=1

To determine whether application tasks can meet their deadlines, schedulability
analysis can be done through either Utilization Bound Analysis (Liu and Layland
1973) or Response Time Analysis (Joseph and Pandya 1986). While checking the total
utilization of a task set, EGCI and PGCI are used to compute the utilization of TGC . In
the response time analysis, the worst-case response time of a task instance occurred at
a critical instant is compared with the task’s deadline. With this analysis, preemption
delays caused by the GC task must be included. Using (Joseph and Pandya 1986)’s
response time calculation equation, we get the worst-case response time Ri for a task
Ti ,

  R n 
i
× Ej + P_GCDelayi (Rin )
Rin+1 = Ei +
(6)
Pj
∀j ∈hp(i)

where Ei is the execution time of Ti and hp(i) is a set of application tasks that have
the higher priority than that of Ti . The preemption delay caused by the GC task,
Rin
Rin
EGCI if Rin is less than or equal to DGC , or  PGC
EGC
P_GCDelayi (Rin ) is  PGCI
otherwise. Starting with Ri0 = Ei , (6) iterates until Rin is converged or becomes larger
than Pi .
In summary, to verify whether the system is schedulable, we need to check a response time Ri for a task Ti to be less than its deadline Pi and the allocated heap
memory to be less than the given heap size H. To find a feasible set of controlling
parameters for the GC task, including EGCI , PGCI , PGC , and DGC , a search algorithm
can be employed such that the maximal allocated heap memory is bounded according to (3) and task response times from (6) are bounded to their deadlines. Or, we can
assume that DGC is a fixed fraction of PGC and HAllocated = H , and then compute
• PGC : based on (3);
• EGC : based on (1) and the parameters about the sizes of live memory Lmax and
the allocated memory HAllocated ;
• EGCI and PGCI : based on (4) and subject to Ri ≤ Pi for all task Ti .
6.2 Example of garbage collection scheduling
An experiment of the scheduling algorithm with a periodic task set is conducted in
MONO with the S-GC. The experiment is designed as a feasibility study for the sug-

68

Real-Time Syst (2007) 36: 47–74

Table 10 A task set example

Pi
(ms)

Ei
(ms)

Ai
(KB)

1

27

T1

50

200

T2

50

250

760

T3

100

400

616

TGC

800

Table 11 GC scheduling parameters for a task set
EGCI
(ms)

PGCI
(ms)

EGC
(ms)

PGC & DGC
(ms)

Lc
(KB)

HAllocated
(KB)

HTH
(KB)

1

27

5

135

172

6500

2476

gested garbage collection scheduling algorithm on an actual runtime environment.
Besides that, the heap allocation distribution according to the behavior of GC is examined.
The task set given in the experiment consists of three periodic tasks, and each
task’s execution time, period, and worst case memory allocation per an instance are
presented in Table 10. The tasks are scheduled through Rate Monotonic scheduling
algorithm (Liu and Layland 1973) such that the task with a shorter period has a higher
priority.
The GC task, TGC , is assigned with the highest priority and its execution time and
period of GC increments (EGCI and PGCI ) are chosen to make the task set schedulable
according to Rate Monotonic schedulability analysis. Table 11 summarizes the TGC ’s
scheduling parameters as well as the lower bound of the size of heap memory and
the free heap memory threshold to trigger a GC cycle. To derive EGC and DGC ,
(1) and (4) are used respectively. The lower bound of a heap memory is set with
the maximum heap allocated HAllocated derived from (2) where PGC is equal to DGC .
In the experiment, garbage collection is triggered in two different ways: at free
heap memory threshold HTH , and at every GC period PGC (hereafter, the former is
called “GC at free memory threshold” and the latter is “GC at periods”). The system
environment for the experiment is similar to the one given in Section 4.
The invocation behavior of GC cycles and the distribution of heap memory allocation are presented in Fig. 6 and Fig. 7. The bars on the top of the graph show the moments of GC cycle invocations: each bar indicates a single GC cycle, and the width
of the bar indicates the duration of a GC cycle. In Fig. 7, the width of the bars indicates the small amount of work during each GC cycle which is resulted from frequent
invocations of GC cycles.
In the same experiment duration, the GC at periods has more frequent invocations
of GC cycles; it has 44 cycles compared to 15 cycles under the GC at free memory threshold. However, while the GC at free memory threshold shows consistent
memory reclamation rates (from 2966 KB up to 3672 KB) at each GC cycle, there is
a large variation on memory reclamation under the GC at periods (from 0 KB up to
4556 KB). The overhead of invoking GC cycles frequently without gaining much free
memory can result in a waste of CPU resource even though the task set is schedulable.

Real-Time Syst (2007) 36: 47–74

69

Fig. 6 Allocated heap distribution under the GC at free memory threshold

Fig. 7 Allocated heap distribution under the GC at periods

With the GC at free memory threshold, the interval between two consecutive GC
cycles PGC varies from 209 ms up to 400 ms. The reasons of the gap between the observed PGC and the derived PGC in Table 11 are as follows. Firstly, the memory al-

70

Real-Time Syst (2007) 36: 47–74

location is not evenly distributed and is related to task arrivals. Secondly, the derived
PGC is the minimum interval for the worst case that guarantees the schedulability of
the task set and the availability of a heap memory.

7 Related works
Scheduling approaches to meet real-time requirements of application tasks in
the presence of garbage collection have been discussed in (Henriksson 1997;
Kim et al. 1999; Bacon et al. 2003c; Robertz and Henriksson 2003). The approaches
were motivated to address the problems that in incremental garbage collection triggered by memory allocation requests, it can neither have a bounded pause time of
garbage collection increment nor guarantee the response time of application tasks.
The scheduling algorithms also identify the memory requirement accordingly, and
they all are based on a separate garbage collection task. Besides (Henriksson 1997;
Kim et al. 1999; Bacon et al. 2003c; Robertz and Henriksson 2003), a garbage collector introduced in (Pfeffer et al. 2004) was dedicated to a hardware thread in a platform
of a multithread Java microcontroller (Komodo) (Pfeffer et al. 2004). As Komodo,
supporting up to four hardware threads, maintains a separate stack set per a thread,
a garbage collector on the hardware thread can exploit a fast context switch; a context switch between hardware threads does not require saving/restoring contexts of
the threads. The garbage collection also presented a computable (but conservative)
algorithm to locate gray objects in bounded time while applying a tri-color marking
algorithm.
Henriksson (1997) suggested a semi-concurrent scheduling algorithm on the implementation of a Brook-style copying algorithm (Brooks 1984). The scheduling algorithm is designed to prevent time-critical tasks from being delayed due to garbage
collection operations. With the suggested scheduling algorithm, tasks are divided
into high priority tasks of a critical deadline and low priority tasks of a soft deadline. Whenever there are no high priority tasks running, a garbage collection task
(a background task) runs to reclaim garbage objects that are allocated by the high
priority tasks. The garbage collection for low priority tasks are triggered when memory allocation requests are made by the low priority tasks. A schedulability analysis
(Joseph and Pandya 1986) was employed to compute the worst-case response time of
the garbage collection task which can be used to determine a suitable size of reserved
heap memory for the high priority tasks.
Even though a garbage collection task scheduled in a background can ensure
a minimal disturbance to high priority tasks, there are issues to be considered in applying the scheduling algorithm into a practical runtime environment. Firstly, if high
priority tasks overrun, the background task cannot progress sufficiently and high priority tasks may suffer from a delay due to a shortage of a heap memory. Secondly,
the background task may have a longer response time than any high priority tasks.
Consequently, it requires to reserve a large amount of memory before a garbage collection cycle starts. Thirdly, the separation of garbage collection work according to
associated tasks is not simple in most of virtual machines which have a single heap
and shared memory objects among tasks.

Real-Time Syst (2007) 36: 47–74

71

Kim et al. (1999) emphasized on a scheduling approach that minimizes the memory requirement as well as guarantees the timeliness of real-time applications. By
considering the invocation of garbage collection as aperiodic jobs, a sporadic server
with the highest priority is employed to conduct garbage collection using a copying garbage collection algorithm. The shortest period is assigned to the sporadic
server compared to periods of mutators so that the server can hold the highest priority according to a rate-monotonic scheduling algorithm. The simulation shows that
the memory requirement with the algorithm is lower than that of the background
server approach in (Henriksson 1997). However, placing the server with the highest
priority can restrict the flexibility for the size of an execution budget and the execution period of the server because of the rationale of a rate monotonic scheduling
algorithm (a task with a shorter period is associated with a higher priority). Their
extended algorithm in (Kim and Shin 2003) employed dual servers responsible for
garbage collection. One server with the highest priority conducts an initialization of
a GC cycle such as flipping a from-space memory and a to-space memory, which can
complete in a short time, and the other server with a low priority conducts the main
garbage collection work such as scanning a heap memory and evacuating live objects.
Bacon et al. (2003a, 2003c) designed and implemented a real-time garbage collector (called Metronome) based on Yuasa’s snapshot-at-the-beginning algorithm on Jike
RVM. The Metronome includes features essential to real-time garbage collection, including memory compaction and ArrayLet. The memory compaction that helps minimizing memory defragmentation is performed at a memory page level according to
the fragmentation rate of a memory page. ArrayLet that helps avoiding unbounded
delay for compacting or scanning a large array object, allows a large array to be
organized in a two-level structure consisting of a sequence of arraylet. In addition,
a cost model is established to derive the execution time of garbage collection based
on the employed design. The scheduling algorithm is aimed to have a uniform MMU
(Minimum Mutator Utilization). Hence, its garbage collection is interleaved with mutators and is assigned with a fixed time quantum periodically.
The experiment showed that the Metronome successfully achieved the targeted
MMU. However, because in the experiment the time quantum for garbage collection
was set with 10 ms, which is a timer resolution in AIX, the experiment did not show
the flexibility of a granularity of a garbage collection increment. In addition, the paper
described that incremental stack scanning is not yet supported. The atomic root-scan
phase for applications with multiple threads can cause a long latency.
Robertz and Henriksson (2003) considered a deadline of a garbage collection cycle as the only parameter necessary to schedule garbage collection; once the deadline
is determined, a process scheduler treats a garbage collection task as a real-time task
in accordance with real-time scheduling algorithms. The approach is based on the assumption that garbage collection is eligible for quick preemption, i.e. higher priority mutators can always preempt a garbage collection task quickly. However, even
fine-grained incremental garbage collection algorithms take a certain time to finish
a minimum amount of a garbage collection increment (i.e. a preemption latency) so
that the following mutators as well as the succeeding garbage collection increment
can resume in a stable state. The proposed scheduling algorithm does not indicate
how the preemption latency of a garbage collection task affects the higher priority
mutators.

72

Real-Time Syst (2007) 36: 47–74

Our work presented in the paper embraces the full aspects necessary to utilize
garbage collection for real-time applications in a virtual machine. It includes the design of fine-grained incremental garbage collection, the implementation on a real
execution environment, a cost model to derive the WCET of garbage collection,
the scheduling algorithm to guarantee the timeliness of applications, and the experiment of heap memory allocation by the scheduling algorithm on the CLI implementation. This work is quite similar to Metronome (Bacon et al. 2003a, 2003c) in a different virtual execution system. The Metronome project designed incremental garbage
collection, implemented that on JVM, and established a cost model on the basis of
the design. Additionally, a scheduling model, placing a fixed quantum for a mutator
and a garbage collector periodically, focused on the estimation of excess memory.
However, our scheduling approach is an integrated scheduling model with real-time
application tasks by considering their timing constraints such as deadline and WCET.
In addition, our cost model and scheduling model are studied with practical experiments so that they can be further elaborated based on the investigated aspects of
garbage collection.

8 Conclusion
In this paper, we describe a complete approach to design, implement, and analyze
a schedulable garbage collection for embedded applications in CLI virtual machine.
In the proposed schedulable GC, garbage collection operations are done by a separate task that runs concurrently with application tasks. In the garbage collection task,
once it is triggered by free heap threshold, the GC operations are carried out incrementally and their pause time is controlled in a small work unit. Using the measured
execution times of each GC operation in a prototype implementation, a cost model
is established to provide a reasonable prediction for the WCET of GC. Finally, we
experiment a suggested scheduling algorithm which integrates the cost model of GC
operations and the execution characteristics of a periodic task set on the prototype.
The results show that the timeliness of real-time applications can be guaranteed in
VM environment along with garbage collection.

References
AICAS GmbH (2006) JamaicaVM. http://www.aicas.com/
Aonix North America, Inc. (2006) PERC. http://www.aonix.com/perc.html
Bacon DF, Cheng P, Rajan VT (2003a) Controlling fragmentation and space consumption in
the metronome, a real-time garbage collector for Java. In: LCTES, pp 81–92
Bacon DF, Cheng P, Rajan VT (2003b) The metronome: a simpler approach to garbage collection in realtime systems. In: OTM workshops, pp 466–478
Bacon DF, Cheng P, Rajan VT (2003c) A real-time garbage collector with low overhead and consistent
utilization. ACM SIGPLAN Not 38(1):285–298
Bacon DF, Cheng P, Grove D, Hind M, Rajan VT, Yahav E, Hauswirth M, Kirsch CM, Spoonhower
D, Vechev MT (2005) High-level real-time programming in Java. In: Proceedings of the fifth ACM
international conference on embedded software, Jersey City, New Jersey (invited paper)
Boehm H-J, Weiser M (1988) Garbage collection in an uncooperative environment. Softw Pract Exp
18(9):807–820

Real-Time Syst (2007) 36: 47–74

73

Boehm H-J, Demers AJ, Shenker S (1991) Mostly parallel garbage collection. ACM SIGPLAN Not
26(6):157–164
Bollella G, Gosling J, Brosgol B, Dibble P, Furr S, Turnbull M (2000) The real-time specification for Java.
Addison–Wesley, Reading
Brooks RA (1984) Trading data space for reduced time and code space in real-time garbage collection on
stock hardware. In: LISP and functional programming, pp 256–262
Cheng P, Blelloch GE (2001) A parallel real-time garbage collector. In: PLDI ’01: proceedings of the ACM
SIGPLAN 2001 conference on programming language design and implementation. ACM, New York,
pp 125–136
Dijkstra EW, Lamport L, Martin AJ, Scholten CS, Steffens EFM (1978) On-the-fly garbage collection: an
exercise in cooperation. Commun ACM 21(11):966–975
Doligez D, Leroy X (1993) A concurrent, generational garbage collector for a multithreaded implementation of ml. In: POPL, pp 113–123
ECMA (2002) Ecma-335 common language infrastructure
Esmertec Inc. (1999) Jbed VM. http://www.esmertec.com/
GNU (2005) GNU compiler for Java. http://gcc.gnu.org/java/index.html
Goh O, Lee Y-H, Kaakani Z, Rachlin E (2005) A schedulable garbage collection for embedded applications
in CLI. In: RTCSA, pp 189–192
Goh O, Lee Y-H, Kaakani Z, Rachlin E (2006) Integrated scheduling with garbage collection for real-time
embedded applications in CLI. In: ISORC, pp 101–108
Henriksson R (1997) Predictable automatic memory management for embedded systems. In: OOPSLA’97
workshop on garbage collection and memory management
Jones RE (1999) Garbage collection: algorithms for automatic dynamic memory management. Wiley, New
York
Joseph M, Pandya PK (1986) Finding response times in a real-time system. Comput J 29(5):390–395
Kim T, Shin H (2003) Scheduling-aware real-time garbage collection using dual aperiodic servers. In:
RTCSA, pp 1–17
Kim T, Chang N, Kim N, Shin H (1999) Scheduling garbage collector for embedded real-time systems. In:
Proceedings of the ACM SIGPLAN 1999 workshop on languages, compilers, and tools for embedded
systems (LCTES’99), pp 55–64
Liu CL, Layland JW (1973) Scheduling algorithms for multiprogramming in a hard-real-time environment.
JACM 20(1):46–61
Microsoft (2006) Microsoft SSCLI. http://msdn.microsoft.com/net/sscli
Microsoft Corp. (2005) CLI benchmarks. http://research.microsoft.com/~zorn/benchmarks
Microsoft Corp. (2006) Microsoft .NET. http://msdn.microsoft.com/net
Persson P (1999) Live memory analysis for garbage collection in embedded systems. In: LCTES ’99:
proceedings of the ACM SIGPLAN 1999 workshop on languages, compilers, and tools for embedded
systems. ACM, New York, pp 45–54
Pfeffer M, Ungerer T, Fuhrmann S, Kreuzinger J, Brinkschulte U (2004) Real-time garbage collection for
a multithreaded java microcontroller. Real-Time Syst 26(1):89–106
Robertz SG, Henriksson R (2003) Time-triggered garbage collection: robust and adaptive real-time gc
scheduling for embedded systems. In: LCTES, pp 93–102
Siebert F (2001) Constant-time root scanning for deterministic garbage collection. In: CC, pp 304–318
Sun Microsystems Inc. (2000) CLDC and the K virtual machine. http://java.sun.com/products/cldc/
Sun Microsystems Inc. (2004) The real-time java platform
Sun Microsystems Inc. (2005) Sun real-time Java system. http://java.sun.com/j2se/realtime
TimeSys Corporation (2004) Timesys linux/real-time user’s guide, version 2.0
Wilson PR (1992) Uniprocessor garbage collection techniques. In: IWMM, pp 1–42
Ximian (2005). MONO. http://www.go-mono.com
Yuasa T (1990) Real-time garbage collection on general-purpose machines. J Softw Syst 11(3):181–198
Yuasa T, Nakagawa Y, Komiya T, Yasugi M (2000) Return barrier. J Inf Process 41(9):87–99

74

Real-Time Syst (2007) 36: 47–74
Okehee Goh received her Ph.D. degree from Arizona State University in 2006.
Since then, she is working as a software engineer in Motorola in Austin, TX.
Her research interest includes real-time systems, virtual machines for resource
constraint systems, and mobile java platforms.

Yann-Hang Lee received his Ph.D. degree in Computer, Information, and Control Engineering from the University of Michigan, Ann Arbor, MI, in 1984. From
December 1984 to August 1988, he was a research staff member at the Architecture Analysis and Design Group, IBM Thomas J. Watson Research Center, Yorktown Heights, NY. He joined Computer and Information Sciences Department,
University of Florida, Gainesville, FL, in 1988, and is currently a professor in the
Department of Computer Science and Engineering, Arizona State University.
Dr. Lee’s research interests are in the areas of real-time systems, software engineering, distributed systems, fault tolerant computing, and performance evaluation. His current research projects are focused on various aspects of real-time
systems and have been funded by NASA, FAA, DARPA, and NSF. Through the
collaboration with Honeywell International, United Technology Research Center, Boeing, and Motorola
Labs, he has participated in the research of many practical real-time application systems. He has published
many technical papers and co-edited two special issues of IEEE Proceedings in the subject of real-time
systems.

Ziad Kaakani received his B.Sc. and M.Sc. in Electrical Engineering from the
University of Minnesota in 1987. Since then he held various positions at Honeywell and Intel working on data communication, wireless technologies, embedded
systems, and deterministic managed runtime environments. Currently he is a Software System Architect at Honeywell focusing on system security and distributed
open platforms for process control.

Elliott Rachlin received his BSCS from Michigan State University. He is an
Engineering Fellow at Honeywell where he has worked for the past 28 years on
areas including operating systems, artificial intelligence, process control, communications and modeling. Currently he is participating in the design of software for
System Management and Abort Decision Logic in the Orion Space program.

Wireless Netw (2015) 21:1713–1732
DOI 10.1007/s11276-014-0878-8

Routing algorithm of minimizing maximum link congestion
on grid networks
Jun Xu • Jianfeng Yang • Chengcheng Guo
Yann-Hang Lee • Duo Lu

•

Published online: 20 December 2014
 Springer Science+Business Media New York 2014

Abstract As regular topology networks, grid networks
are widely adopted in network deployment. Link congestion and routing path length are two critical factors that
affect the delay and throughput of a network. In this paper,
we study the routing problem in grid networks concerning
these two factors. The main objective of our routing
algorithms is to minimize the maximum link congestion.
The shortest path minimum maximum (SPMM) link congestion and non-shortest path minimum maximum
(NSPMM) link congestion routing problems are studied.
The two problems are first formulated as integer optimization problems. Then, corresponding routing algorithms
(SPMM and NSPMM routing algorithms) are proposed.
For SPMM routing algorithm, the path length is optimal,
while for NSPMM routing algorithm, the path is limited in
a submesh. Thus, the path length can be bounded. At last,
we compare the proposed routing algorithms under different scenarios with other popular routing algorithms
(RowColumn, ZigZag, Greedy, Random routing algorithms). The performances are evaluated through different
J. Xu  J. Yang (&)  C. Guo
School of Electronic Information, Wuhan University, Wuhan,
Hubei, China
e-mail: yjf@whu.edu.cn
J. Xu
e-mail: eisxujun@whu.edu.cn
C. Guo
e-mail: netccg@whu.edu.cn
Y.-H. Lee  D. Lu
Computer Science Department, Arizona State University,
Tempe, AZ, USA
e-mail: yhlee@asu.edu
D. Lu
e-mail: duolv@asu.edu

metrics including link congestion, path length, path congestion, path congestion to path length ratio, delay and
throughput.
Keywords Grid networks  Link congestion  Routing
algorithm  Scheduling

1 Introduction
Network congestion occurs when some nodes or links carry
too much traffic. It derogates network performances in
several aspects, such as increasing the end-to-end delay,
packet loss, and decreasing throughput. Reducing maximum link/node congestion and balancing network traffic
are fundamental problems in network management. For
example, one of the main objectives of Traffic Engineering
(TE) [1] is load balancing. The load balancing problem has
been widely studied and some researchers surveyed the
load balancing problem on Internet [2], IP/MPLS [3],
WiMAX [4], Wireless Sensor Networks [5, 6], P2P networks [7] and MANET [8, 9]. Related schemes on network
congestion include rate control [10], queue management
[11], topology control [12] and joint congestion control and
medium access control [13]. Besides, designing a high
performance routing algorithm is also an efficient method,
such as multipath routing [14, 15]. However, the multipath
routing method induces packet reordering problem [16].
In this paper, we design routing algorithms aiming to
minimize the network congestion. The networks we study
are the 2-dimensional grid topology networks. As studied
in paper [17], in wireless mesh networks, grid topology
networks are much more suitable for large scale mesh
deployment compared to random topology networks. When
formulating and analyzing wireless communication

123

1714

Wireless Netw (2015) 21:1713–1732

problems, the 2-dimensional grid network provides a
framework. In other communication networks, grid networks can be the testing topologies for routing and
scheduling algorithms [18].
In the study [19], Leighton, Maggs, and Rao proved that
there exist packet scheduling algorithms completing the
transmissions of all the packets within duration O(c ? d),
where c is the maximum link congestion of the network
and d is the largest path length. Motivated by this result,
many researchers tried to find effective routing algorithms
aiming to minimize the maximum link congestion by
adopting routing paths as short as possible.
Finding routing algorithms to minimize the maximum link
congestion has been studied in game theory area as the bottleneck routing problem [18, 20–22]. The bottleneck node/
link of a routing path is the most congested node/link on the
path. In the routing games studied in [18, 20, 22], the objective
of each player is to find a path with the minimum bottleneck
link congestion. In [18], it allows the path to take some bends
which implies that the routing paths adopted are not always the
shortest paths. The price of anarchy (PoA) is proportional to
the number of bends. In [22], the PoA is O(l ? log(n)) where
l is the length of the longest path in the player’s strategy sets,
and n is the size of the network. In [20], the splittable and unsplittable bottleneck games are studied. In [21], each user aims
to minimize the sum of the maximum link congestion and
service cost of the chosen path. In [23], the bottleneck congestion games are discussed, the aim is to minimize the
maximum resource congestion of the network. In the routing
game studied in [24], any player i tries to minimize its ci ? di
value which represents the sum of the congestion and path
length. Although the studies mentioned above tried to minimize the maximum link congestion of a network, all of them
considered only the maximum link congestion of its routing
path, while ignored the path congestion (which is the summation of all the link congestions along the path). However,
this may result in a higher maximum link congestion of the
network. An example is shown in Fig. 1. The numbers on the
links are the current link congestions. Let flow fi make a

decision before flow fj. If each flow only considers the bottleneck link congestion of the chosen path, then for flow fi, the
bottleneck link congestion of all the routing paths are 2. It
randomly chooses the path pi = {si, v3, v2, di}. Now the
congestions on links (si, v3), (v3, v2), and (v2, di) are increased
to 3. The maximum link congestion of the network is 3. Now,
consider flow fj, no matter which path it chooses, it will
increase the maximum link congestion of the network from 3
to 4. However, if the path congestion is considered after the
bottleneck link congestion, then flow fi should choose the path
pi = {si, v1, v2, di} which has the minimum path congestion.
Now, for flow fj, it will choose path pj = {sj, si, v3, dj} which
has the minimum bottleneck path congestion. In this situation,
the maximum link congestion of the network is only 3, which
is smaller than 4.
Motivated by the above observation, we consider not
only the maximum link congestion but also the path congestion when selecting path for a flow in a grid network to
minimize the maximum link congestion of the network.
In this paper, we propose two routing algorithms which
are denoted by SPMM and NSPMM routing algorithms,
corresponding to the two routing problems.
The evaluations of the proposed routing algorithms are
executed under three different traffic patterns: Partial Permutation Traffic Pattern, General Traffic Pattern and the
Specific Traffic Pattern. For the Partial Permutation Traffic
Pattern, no two flows in F share the same source or destination node, that is, for any two flows fi and fj, we have
that si 6¼ sj , si 6¼ dj , di 6¼ sj and di 6¼ dj . For the General
Traffic Pattern, the source and destination nodes for any
flow can be randomly selected. And more than one flow
can share the same source or destination node. The construction of the Specific Traffic Pattern will be introduced
in Sect. 5.
Our contributions include:
•

•
X
v1

1

v3

2

v4

1

Y

Fig. 1 An example of grid networks

123

dj
1

1
3

di

2

2

2
sj

2

2

1
si

v2

v5

Submesh Gi
of flow fi

•

The mathematical formulations of the SPMM and
NSPMM problems. In both of the problems, some
constraints such as the flow conservation constraints
should be met.
The SPMM and NSPMM routing algorithms. The
SPMM routing algorithm tries to find the shortest path.
However by NSPMM routing algorithm, the paths are
not always the shortest ones. The aim of the two routing
algorithms is the same.
Performance evaluation. The SPMM and NSPMM
routing algorithms are compared with other popular
routing algorithms designed for grid networks, namely
the RowColumn, ZigZag, Random and Greedy routing
algorithms. The performance metrics we evaluate
include link congestion, path length, path congestion,
path congestion to path length ratio, delay and
throughput.

Wireless Netw (2015) 21:1713–1732

The paper is organized as follows. In Sect. 2, we
introduce some related works of the routing algorithms on
grid networks. In Sect. 3, we present the network model
and formalize the SPMM and NSPMM routing problems.
The routing algorithms are described in Sect. 4. In Sect. 5,
we evaluate the performances of the SPMM and NSPMM
routing algorithms by comparing them with some existing
routing algorithms designed for grid networks. At last, in
Sect. 6, we conclude this paper.

2 Related work
Routing algorithms have been studied widely in different
kind of networks, such as the disruption tolerant networks
[25], delay tolerant networks [26], cognitive radio networks
[27], wireless vehicular sensor networks [28], data centers
[29] and multicast routing protocol [30]. Routing is an
essential management mechanism for supporting Qos [31].
For grid networks, some routing algorithms with different objectives have been studied [18, 32–36].
The authors in [18] have studied the routing games in
grid networks, each player aims to minimize the maximum
link congestion of its routing path. The ultimate objective
is to minimize the maximum link congestion of the network which is the same as that in this paper.
In [32], to minimize the makespan of a set of packets,
the authors proposed the RowColumn routing algorithm
combining the longest path first scheduling algorithm for
the partial permutation traffic pattern. The optimal makespan can be achieved when all the packets are starting to
transmit simultaneously. However, in applications like
industry control, each flow generates packets with a specified period, thus it’s hard to guarantee that the packets are
started to transmit simultaneously unless the flows are
simple periodic [37]. Adopting the RowColumn routing
algorithm, the maximum link congestion can be as high as
max{n1, n2} for the Partial Permutation Traffic Pattern. For
the General Traffic Pattern, the maximum link congestion
can be as high as nmax, where nmax represents the maximum
times of the nodes in a row or column being source or
destination nodes among all the rows and columns.
In [33] and [35], the objective is to maximize the reliability of packet transmissions with the assumption that all
the links in a grid network have the same packet delivery
probability. As shown in [33], the ZigZag shortest path
routing algorithm is optimal in 2-dimensional grid networks. However, it is not optimal in the torus [34]. In [35],
the MSP (Maximum-Shortest-Path) routing algorithm has
been proposed to achieve the same goal. By MSP, a packet
is always forwarded to the neighbor with the maximum
number of shortest paths to the destination. Unlike the

1715

ZigZag routing algorithm, MSP is optimal not only in 2-D,
3-D meshes, but also in N 9 N torus with N being an even
number greater than 4.
A local-information-based routing algorithm denoted by
ZigZag has been presented in [36]. By this routing algorithm, any given path between two nodes can be transformed into a shortest path. This strategy is useful in
scenarios where nodes can move around.
Another type of routing algorithms is Random routing
strategy [33, 38]. By the random routing algorithm discussed in [33], a packet is forwarded randomly to a
neighbor which is nearer to the destination node. In [38], a
packet is forwarded randomly to a neighbor which is not
necessarily nearer to the destination node. With the purpose
of transmitting all the packets to their destinations as
quickly as possible, the author in [39] has studied the
random algorithms for packet routing on grid networks.
These algorithms consider packet routing as well as packet
scheduling. In paper [40], the authors have tried to control
C (the maximum link congestion) and D (path length,
which is called dilation in the paper) simultaneously. The
network congestion (maximum link congestion) achieved
by the algorithm in this paper is O(dC*log(n)) and the
stretch (ratio of path length to the shortest path length) is
O(d2), where C* is the optimal congestion and d is the
dimension of the grid (mesh as used in the paper). Randomization is used in the algorithm.
Compared to the above studies, our work aims to minimize the maximum link congestion of the network. One of
our routing algorithms achieves the optimal path length, in
other words, the stretch (the ratio of the path length to the
shortest path length [40] ) of the selected path of a flow is 1
and it also keeps the maximum link congestion low. The
other routing algorithm has a higher stretch. However, in
some instances, it can keep the maximum link congestion
lower than the routing algorithms with the optimal stretch.

3 Network model and problem formulation
3.1 Network model
A grid network is represented by an undirected graph
G = (V, E), where V is the set of nodes and E is the set of
edges. For any node v 2 V, its coordinate is represented by
(vx, vy). By convention, the grid networks studied are
2-dimensional topology networks.
For a n1 9 n2 grid network, it means that the numbers of
nodes in any row and any column are n1 and n2, respectively. The flows in the network are represented by
F = {f1,…, fn}. For any flow fi 2 F, it is represented by a
pair of source and destination nodes {si,di} where si 6¼ di .

123

1716

For any grid network, the origin point [with coordinates
(0, 0)] is in the left-up corner. The X-axis is the horizontal
line while the Y-axis is the vertical line. The X-coordinate
of a node is the horizontal distance (hops) from the node to
the origin point. Similarly, the Y-coordinate of a node is
the vertical distance (hops) to the origin point. Thus, for a
flow fi whose source node is si with coordinate ðsxi ; syi Þ and
destination node is di with coordinateðdix ; diy Þ, the horizontal distance between the two nodes is hi;x ¼ jdix  sxi j
and the vertical distance is hi;y ¼ jdiy  syi j. In Fig. 1, we
show an example of grid networks. The coordinates of
node v1 is (0,0). Thus, pick a node, say v3, its coordinates
are (1,1). The horizontal distance from v3 to v1 is 1, so is
the vertical distance.
Considering that by sacrificing path length, a routing
algorithm may achieve a lower maximum link congestion
compared to routing algorithms adopting shorter paths,
thus we study two routing problems, which are shortest
path minimum maximum link congestion (SPMM) and
non-shortest path minimum maximum link congestion
(NSPMM) routing problems.
In the SPMM routing problem, only the shortest paths
will be adopted. However, in the NSPMM problem, the
routing path of any flow fi is restricted within its submesh
Gi. The submesh Gi is defined in the following.
Definition 1 (The submesh Gi) For the flow fi with source
node si and destination node di, the submesh Gi includes all
the nodes and links in the rectangle with si and di as two
opposite diagonal nodes.
In Fig. 1, the submesh of fi is the one in the red (dashed)
rectangle, it includes nodes Vi = {v1, v2, v3, si, di, dj} and
links Ei = {(v1, v2), (v2, di), (v1, si), (v2, v3), (di, dj), (si, v3),
(v3, dj)}. Path p ¼ fsi ; v1 ; v2 ; v3 ; dj ; di g is a path restricted
within the submesh Gi, while path p0 = {si, sj, v4, v3, v2, di}
is a path beyond the submesh Gi.
The formal definitions of the two routing problems are
given as follows.
Definition 2 (SPMM Routing Problem) Given a grid
network G = (V, E) and a set of flows F = {f1,…, fn}. For
each flow fi 2 F, find a shortest path pi for it such that the
maximum link congestion of the network is minimized.
Definition 3 (NSPMM Routing Problem) Given a grid
network G = (V, E) and a set of flows F = {f1,…, fn}. For
each flow fi 2 F, in the submesh Gi, find a path pi for it
such that the maximum link congestion of the network is
minimized.
Now, we give the following definitions which will be
used frequently when describing the routing algorithms.

123

Wireless Netw (2015) 21:1713–1732

Definition 4 (Shortest Path) For any flow fi, a shortest
path pi between its source node si and destination node di
includes exactly jsxi  dix j þ jsyi  diy j links.
Definition 5 (Link Congestion) The congestion l(e) on
any bidirectional link e is the total number of paths
P
including link e, that is, lðeÞ ¼ fi 2F ri;e where ri;e is 1
when pi includes link e, otherwise, it is 0.
Definition 6 (Bottleneck Path Congestion) The bottleneck path congestion b(p) of path p is defined as the
maximum link congestion among all the links on p, that is,
bðpÞ ¼ maxe2p lðeÞ.
Definition 7 (Path Congestion) The path congestion of
path p is the summation of congestions of all the links
P
included by p, that is, wðpÞ ¼ e2p lðeÞ.

3.2 Problem formulation
3.2.1 The SPMM routing problem
The minimizing maximum link congestion routing problem can be formulated as an integer optimization programming. In the problem formulation, the undirected
graph can be viewed as a directed graph by transforming
each edge into two directed edges, let E0 represent these
edges.
Note that, for ðu; vÞ 2 E0 and ðv; uÞ 2 E0 , they are the
same edge in E, thus the congestion of a link in E is the
summation of congestions on two links in E0 .

Now let Cþ
v and Cv be the incoming and outgoing links
adjacent to node v 2 V, respectively. Let e0 be any directed
link in E0 . The problem then can be formulated as follows
and the variables we need to obtain are
ri;e0 ði ¼ 1;    ; n; e0 2 E0 Þ.
X
X
Min: Max:
ri;ðu;vÞ þ
ri;ðv;uÞ
ð1Þ
ðu;vÞ2E0

Subject to:
X
e0 2Cþ
v

X

ri;e0 

X
e0 2C
v

ri;e0

ðv;uÞ2E0

8
if
< 1
¼ 1
if
:
0 otherwise

v ¼ si
v ¼ di ; fi 2 F
ð2Þ

ri;e0 ¼ hxi þ hyi ; fi 2 F

ð3Þ

e0 2pi

ri;e0 2 f0; 1g;

i ¼ 1; . . .; n; e0 2 E0

ð4Þ

Constraints (2) ensure flow conservation for each flow
on each node. Constraints (3) ensure that each flow can

Wireless Netw (2015) 21:1713–1732

1717

only adopt the shortest path. Constraints (4) ensure that
each variable is a 0–1 integer. The above integer optimization programming problem is a variation of the multicommodity flow problem. For the original multi-commodity flow problem, price-directed decomposition,
resource-directed decomposition and partition methods are
introduced in [40]. We refer the interested readers to [41]
for a much more detailed discussion.

Compared to the SPMM routing problem, instead of
choosing a shortest path for each flow, a path restricted
within the submesh is considered in the NSPMM routing
problem. It is ensured by constraints (7) where each node
v included by the path pi is in the submesh Gi.

3.2.2 The NSPMM routing problem

4.1 Routing algorithm: SPMM

The motivation to consider the NSPMM routing problem is
that, with the sacrificing of the path length, we may achieve
smaller maximum link congestion. An example is shown in
Fig. 2. In Fig. 2(a), the path of flow f3 is not a shortest path
and the network congestion is 1 after all the three flows
choosing their paths. While in Fig. 2(b), the path is a
shortest path, the network congestion is 2. The tradeoff for
a shortest path is the increasing of the network congestion.
The NSPMM routing problem is formalized as follows.
X
X
Min: Max:
ri;ðu;vÞ þ
ri;ðv;uÞ
ð5Þ

In this section, we design a shortest path routing algorithm
(SPMM) for grid networks. The ultimate goal is to minimize the maximum link congestion of the network.
Let Pi denote the set of shortest paths of flow fi. The
SPMM routing algorithm will select a path pi 2 Pi for it.
When selecting the path, the following two conditions
should be satisfied:

ðu;vÞ2E0

Subject to:
X

ri;e0 

X

ri;e0

e0 2C
v

e0 2Cþ
v

ðv;uÞ2E0

8
if
< 1
¼ 1
if
:
0 otherwise

v¼s
v ¼ di ; fi 2 F

v 2 Gi ; for v 2 pi

ð7Þ

i ¼ 1; . . .; n; e0 2 E0

ð8Þ

d1

0

1

d2

0

d3

jux  sxi j þ juy  syi j\jvx  sxi j þ jvy  syi j; u 2 Piv and u 2 Gi :

0

s1

1

0

(b)
0

1

d2

0

Fig. 2 NSPMM routing versus SPMM routing

d3

0

s2

0

0

0

0

0

0
s3

d1

The SPMM routing algorithm is a dynamic programming algorithm. Before discussing it in details, we need to
introduce the concept of effective parent nodes and effective child nodes.

0

s2

1

0

0

s1
0

0

0

s3

0

The bottleneck path congestion of pi is the minimum
among all the routing paths in Pi, that is, bðpi Þ ¼
minfbðpÞjp 2 Pi g.
Let P0i represent the set of paths in Pi whose bottleneck
path congestions are equal to b(pi), that is
P0i ¼ fpjbðpÞ ¼ bðpi Þ; p 2 Pi g. The path congestion of
pi is less than or equal to the minimum path congestion
among all the paths in P0i , that is, wðpi Þ 
minfwðpÞjp 2 P0i g.

Definition 8 (Effective Parent Nodes) When considering
the routing for flow fi, the set of effective parent nodes Piv
of node v are the nodes which locate in the submesh Gi and
are nearer to the source node si. The following conditions
are satisfied.

(a)
0

•

•

ð6Þ

ri;e0 2 f0; 1g;

4 Routing algorithms

Definition 9 (Effective Child Nodes) When considering
the routing for flow fi, the effective child nodes Cvi of node
v are the nodes which locate in the submesh Gi are nearer to
its destination node di. The following conditions are
satisfied:
jux  dix j þ juy  diy j\jvx  dix j þ jvy  diy j; u 2 Cvi and u
2 Gi :

0

The pseudocode of the SPMM routing algorithm is
shown in Algorithm 1. For different flows, the number of

123

1718

shortest paths may be different. The number of shortest
!
hi;x þ hi;y
paths of flow fi is
. For any flow, the more the
hi;x
choices are, the more likely it can avoid the congested
links. It implies that the sequence of flows selecting paths
is important to minimize the maximum link congestion. In
algorithm 1, I(u) represents the id of node u. Each node in
the network has a specified id. We can label the nodes row
by row with increasing ids.
In line 1, we first sort the flows with the increasing
number of shortest paths.
For each flow fi 2 F, we need to do the process from
line 3 to line 28. Since a shortest path of flow fi is
within its submesh Gi, in line 3, we first get the submesh. The set S records the current visited nodes. The
set T includes the nodes to be visited in the next round.
It records the effective child nodes of the nodes in set S.
The set Y records the unvisited nodes in Vi. Let bðpsi ;v Þ
represent the bottleneck congestion of the selected path
from node si to node v, wðpsi ;v Þ represent the path congestion of the selected path from node si to node v. In
fact, path psi ;v can be viewed as a subpath of the final
selected path pi.
From line 4 to line 7, we first initialize all the sets, and
the values of bðpsi ;si Þ and wðpsi ;si Þ. Considering that in the
submesh Gi, the destination node di is the farthest node
from node si, so it’s the last node to be visited. Thus, if set
Y is not empty (which means node di hasn’t been visited),
we will keep doing the process from line 9 to line 19. In
this process, for each node in set T, from line 10 to line 14,
a precedent node pre(v) is selected for it from its effective
parent nodes. First, in line 10, we get the effective parent
nodes for node v. In line 11, we set the bottleneck path
congestion for the subpath from source node si to node v,
P0v;i records the parent nodes from which node v can obtain
a path to si with the minimum bottleneck path congestion.
In line 12, the algorithm updates the path congestion as the
minimum path congestion of the subpath to si and selecting
from P0v;i the nodes from which node v can obtain a path
whose path congestion is the minimum, P00v;i records these
parent nodes. If there’re more than one nodes in set P00v;i ,
select the one with the smaller node id as the precedent of
node v, as shown in line 13 and line 14. From line 16 to line
18, the algorithm updates the corresponding sets. From line
20 to line 25, we can simply construct the path from the
source node si to the destination node di by tracking the
precedents from node di until we reach the source node.
From line 26 to line 28, we increase the congestion of the
links in path pi by one.

123

Wireless Netw (2015) 21:1713–1732

4.2 Routing algorithm: NSPMM
In this section, we present the NSPMM routing algorithm, the
basic idea is the same as that of the SPMM routing algorithm.
The biggest difference between the two routing algorithms is
that the path for a flow found by NSPMM routing algorithm is
not always the shortest one. In order to control the path length,
the path found by NSPMM routing algorithm is restricted
within the submesh of each flow. The main process of the
NSPMM routing algorithm is described as follows.
•

We first sort the flows according to the areas of the
submeshes. The area Ai of the submesh of flow fi is
calculated by Ai ¼ ðdix  sxi Þ  ðdiy  syi Þ.

Wireless Netw (2015) 21:1713–1732

•

•

Then we find the paths with the minimum bottleneck
path congestion. For flow fi, the paths are a subset of P^i ,
which are represented by P^0i .
Finally we find the path in P^0i with the minimum total
path congestion, which is the one we need.

Unlike the SPMM routing algorithm, it’s hard to find the
efficient parent nodes and efficient child nodes in the
NSPMM routing problem. Thus, it’s not practical to design
a dynamic programming routing algorithm. However, by
introducing the concept of critical congestion for each flow,
we can design an efficient NSPMM routing algorithm. The
definition of the critical congestion is given as follows.

1719

Now, the only problem we need to solve is how to get
the critical congestion for each flow. The authors in [43]
proposed the method solving the problem of finding the
maximum minimum bandwidth path. By slightly modifying the algorithm, we can use it to find the minimum
maximum link congestion of a submesh. This link congestion will be the critical congestion of the corresponding
flow.

Definition 10 (Critical Congestion) For flow fi 2 F, at the
time for it to find a routing path, for each link e 2 E, the link
congestion is l(e). Let li be a link congestion value.
Removing all the links with link congestions greater than li ,
there exists at least one path within the submesh Gi connecting node si and di. However, removing all the links with
link congestion equal to or greater than li , we can’t find any
path within the submesh Gi connecting node si and di.
The pseudocode of the NSPMM routing algorithm is
shown in Algorithm 2.
First, in line 1, all flows are sorted. For each flow fi 2 F,
we need to execute the process from line 3 to line 11. For
flow fi, the algorithm first get its submesh Gi (line 3), then
find its critical congestion in Gi (line 4). From line 5 to line
9, we remove the links in Gi with link congestion greater
than the critical congestion. Now, in the residual network,
all the links have link congestion equal to or less than the
critical congestion. In line 10, using Dijkstra’s algorithm
[42], we can easily get the path with the minimum total
path congestion. Note that, when running the Dijkstra’s
algorithm, we always pick the node with the smaller node
id when there are more than one nodes that have the
minimum path congestions in the paths from itself to the
source node. In line 11, the congestions of the links in path
pi are increased by one.

In order to make this paper self-contained and consider
that our objective is different from [43], we give the
pseudocode of how to get the critical congestion for a flow,
as shown in Algorithm 3. For the detailed description of the
algorithm, we refer the readers to paper [43]. As proved in
the paper, the time complexity of algorithm CriticalCongestion(Gi) is O(|Ei |).
4.3 Algorithm performance
Theorem 1 For a n1  n2 2-dimensional grid network
with n flows, the time complexity of Algorithm 1 is
O(nn1n2).
Proof Sorting flows will take O(nlog(n)) steps. In Algorithm 1, for any flow fi, getting the efficient parent nodes of
any node v can be accomplished with time complexity O(1)
(using the coordinates of nodes). From line 8 to line 19, for
flow fi, each node in the submesh is visited exactly once,
thus the time complexity is O(|Vi|). The constructing of the
routing path (line 20 to line 25) takes O(hi) steps, where hi

Table 1 initial configuration
Network
size

Number
of nodes

Number
of links

Original
number of
flows

Initial link
congestion

10 9 10
20 9 20

100
400

180
760

50
200

0
0

123

1720

Wireless Netw (2015) 21:1713–1732
0

1

2

3

4

5

0

s9

s10

s11

s12

s13

s14

1

s15

s5

s3

s4

s8

s16

2

s17

s1

s6

s7

s2

s18

X

3

d18

d2

d8

d5

d1

d17

4

d16

d7

d4

d3

d6

d15

5

d14

d13

d12

d11

d10

d9

Theorem 2 For a n1 9 n2 2-dimensional grid network,
the time complexity of Algorithm 2 is O(nlog(n) ? nn1n2log(n1n2)) where n is the total number of flows.

Y
Fig. 3 An example of flow construction with network size 6 9 6

(a)

Proof First, sorting the n flows could be completed with
time complexity O(nlog(n)). As proved in [43] (please refer
to Theorem 1 in [43] ), the time complexity of finding the
critical congestion (Algorithm 3) for flow fi is O(|Ei|).
Subtracting the links in Ei with congestions greater than li
takes |Ei| steps. The time complexity of the Dijkstra’s
algorithm of finding the path with minimum path congestion takes O(|Ei| ? |Vi|log(|Vi|)) when implemented by

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

25

20

15

Average Maximum Link Congestion

Maximum Maximum Link Congestion

is the shortest path length for flow fi. From line 26 to line
28, we update the congestions of links in path pi, the time
complexity is O(|Ei|). The time complexity for flow fi to get
its routing path is O(|Vi| ? hi ?|Ei|). For a n1 9 n2 grid, for
n1 n2
2
any flow fi, we have that, jVi j  n1 n
2 ; hi 
2 ; and
jEi j  ðn1  1Þn2 þ n1 ðn2  1Þ.Thus, we have that jVi jþ
hi þ jEi j  Oðn1 n2 Þ. In total, there are n flows, thus the time
complexity of the SPMM routing algorithm is O(nn1n2).h

10

5

0

10 * 10

20 * 20

25
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

20

15

10

5

0

10 * 10

20 * 20

grid size

grid size

123

(a)
8000
NSPMM
SPMM

7000
6000
5000
4000
3000
2000
1000
0

1.4

1.6

1.8

2

grid size: 10*10

2.2

Distribution of the Average Link Congestion

Fig. 5 Distribution of the
average link congestion.
a Average link congestion for
the 10 9 10 grid. b Average
link congestion for the 20 9 20
grid

Distribution of the Average Link Congestion

Fig. 4 Maximum link congestion: partial permutation flows. a Maximum maximum link congestion. b Average maximum link congestion

(b)
8000
NSPMM
SPMM

7000
6000
5000
4000
3000
2000
1000
0

3

3.2

3.4

3.6

3.8

grid size: 20*20

4

4.2

Wireless Netw (2015) 21:1713–1732

1721

60
50
40

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Maximum Path Length

Maximum Maximum Path Length

(a) 70

30
20
10
0

10 * 10

20 * 20

70
60
50
40

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30
20
10
0

10 * 10

grid size

14
12
10

(d)

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Average Path Length

Maximum Average Path Length

(c) 16

8
6
4
2
0

10 * 10

20 * 20

grid size
14
12
10
8
6
4
2
0

20 * 20

grid size

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10 * 10

20 * 20

grid size

Fig. 6 Path length: partial permutation flows. a Maximum maximum path length. b Average maximum path length. c Maximum average path
length. d Average average path length

priority queue [44]. Thus, for flow fi, the time complexity
of finding a path is,

5.1 Scenarios and metrics

OðjEi jÞþjEi jþOðjEi jþjVi jlogðjVi jÞÞ;

We compare the SPMM and NSPMM routing algorithms with other four popular routing algorithms in
grid networks, which are Row Column, ZigZag, Random and Greedy routing algorithms. Although we have
introduced these four routing algorithms in Sect. 2, we
still briefly describe them for a better understanding
here.

which is also O(|Ei| ? |Vi|log(|Vi|)). For a n1 9 n2 2dimensional grid, we have that jEi j  n1  n2 and
jVi j  n1  n2 . There are in total n flows which implies that
the time complexity of the routing algorithm is
O(nlog(n) ? nn1n2log(n1n2)).
h

•
5 Performance evaluation
We evaluate the routing algorithms in grid networks in this
section. All the algorithms are implemented with JAVA
language in Eclipse. The programs are running on a laptop
equipped with an Intle(R) Core(TM) i5-2410 M CPU
whose frequency is 2.30 GHz. And the size of the RAM is
2.0 GB. The operation system is Windows 7. We test the
algorithms in networks with size 10 9 10 and 20 9 20, the
initial configurations of the networks are shown in Table 1.

•

•

In the RowColumn routing algorithm, each packet will
be first transmitted along the row to the node in the
same column of the destination node, and then along
the column to the destination node.
In the ZigZag routing algorithm, packets will be first
transmitted along the row or column to the diagonal
node of the destination node, and then transmitted to
the destination node using a ZigZag shape path.
In the Random routing algorithm, each node will select
randomly a next-hop node from the legible neighbors
which is nearer to the destination nodes.

123

1722

Wireless Netw (2015) 21:1713–1732

20

15

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Maximum Ratio

Maximum Maximum Ratio

(a) 25

10

5

0

10 * 10

25

20

15

10

5

0

20 * 20

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10 * 10

grid size

(d) 25

15

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Average Ratio

Maximum Average Ratio

(c) 25
20

10

5

0

20 * 20

grid size

10 * 10

20 * 20

grid size

20

15

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10

5

0

10 * 10

20 * 20

grid size

Fig. 7 Ratio: partial permutation flows. a Maximum maximum ratio. b Average maximum ratio. c Maximum average ratio. d Average average
ratio

•

In the Greedy routing algorithm, each node will send
the packet to the next-hop node which has the smallest
congestion.

Besides the maximum link congestion and average link
congestion, we are also interested in the following
metrics.
•

•
•

Path Length For a single path p, the path length hðpÞ ¼
jfeje 2 pgj which is the number of links included by the
path.
Path Congestion It has been introduced in Sect. 3.
Path Congestion to Path Length Ratio For a single path
p, the path congestion to path length ratio is expressed
as rðpÞ ¼ wðpÞ
hðpÞ .

•
•

Network delay The average delay of all the flows is also
important.
Network throughput We estimate the network throughput by calculate the number of packets received within
a time duration.

123

For each setting, we run the simulation 20,000 times. Note
that, in the expression ‘‘Maximum Maximum’’, the first
‘‘Maximum’’ means the maximum value among the 20,000
runs, and the second ‘‘Maximum’’ means the maximum value
among flows generated in a single instance. Other expressions,
such as ‘‘Maximum Average’’, ‘‘Average Maximum’’ and
‘‘Average Maximum’’ follow the similar explanations.
All the above metrics are evaluated under three traffic
patterns. The Partial Permutation Traffic Pattern and
General Traffic Pattern have been introduced in the previous section. Here, we introduce the Specific Traffic
Pattern (W) which we believe will cause very high maximum link congestion of a network. The following construction of the traffic pattern Wn1 ;n2 is only suitable for
grids with size n1 9 n2 where n1 C 4 and n2 C 4. The
concept of constructing the traffic patterns W can be
extended to other situations easily.
The coordinates of the nodes in a grid network have
been introduced in Sect. 3. The construction of the specific

Wireless Netw (2015) 21:1713–1732

1723

(b)
350
300
250
200

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0

10 * 10

20 * 20

Average Maximum Path Congestion

Maximum Maximum Path Congestion

(a)

350
300
250
200

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0

10 * 10

grid size

(c)

(d)
100

80

60

Average Average Path Congestion

Maximum Average Path Congestion

20 * 20

grid size

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

40

20

0

10 * 10

20 * 20

grid size

100

80

60

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

40

20

0

10 * 10

20 * 20

grid size

Fig. 8 Path congestion: partial permutation flows. a Maximum maximum path congestion. b Average maximum path congestion. c Maximum
average path congestion. d Average average path congestion

traffic pattern composes two steps. In step1, we need to
construct the four special flows. And in step2, for the
remaining nodes, we just construct a symmetric traffic
pattern.
1.

Step1: construct the special flows.
 
 
Now let x ¼ n21 and y ¼ n22 . As shown in Fig. 3,
this is the node d5 with coordinates (3, 3).

•

•

•

•

For node ðx ; y Þ, the coordinate of the corresponding
source node is ðx  2; y  2Þ(which is node s5 in
Fig. 3);
For node ðx ; y  1Þ(which is node s7 in Fig. 3), the
coordinate of the corresponding destination node is
ðx  2; y þ 1Þ(which is node d7 in Fig. 3);
For node ðx  1; y Þ(which is node d8 in Fig. 3), the
coordinate of the corresponding source node is
ðx þ 1; y  2Þ(which is node s8 in Fig. 3);
For node ðx  1; y  1Þ(which is node s6 in Fig. 3),
the coordinate of the corresponding source node is
ðx þ 1; y þ 1Þ(which is node d6 in Fig. 3).

2.

Step2: construct the symmetric traffic pattern.

For the other nodes, the source and destination nodes are
paired according to the following rules.
•

•

•

When n1 and n2 are both even. If a node with
coordinate ðsxi ; syi Þðsyi  y Þ is set to be a source/destination node, then the destination/source node is set to
be the one with coordinate ðn1  1  sxi ; n2  1  syi Þ.
When one of the values of n1 and n2 is odd. Without
loss of generality, let n1 be odd. If a node with
coordinate ðsxi ; syi Þðsxi  x Þ is set to be a source/destination node, then the destination/source node is set to
be the one with coordinateðn1  2  sxi ; n2  1  syi Þ.
After that, the remaining nodes can be arbitrarily
paired.
When both the values of n1 and n2 are odd. If a node
with coordinate ðsxi ; syi Þ (sxi  x and syi  n2  1) is set
to be a source/destination node, then the destination/
source node is set to be the one with coordinate
ðn1  2  sxi ; n2  2  syi Þ. After that, the remaining

123

1724

(b)
Average Maximum Link Congestion

(a)
Maximum Maximum Link Congestion

Fig. 9 Maximum link
congestion: general flows.
a Maximum maximum link
congestion: 10 9 10. b Average
maximum link congestion:
10 9 10 c Maximum maximum
link congestion: 20 9 20.
d Average maximum link
congestion: 20 9 20

Wireless Netw (2015) 21:1713–1732

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0

50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10
150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

nodes are the ones on the bottom row and right-most
column. For a source node among this has the
coordinate ðsxi ; syi Þ, then the destination node coordinate
is ðsyi ; sxi Þ.
Note that, for a grid n1 9 n2 with n1 \ 4 or n2 \ 4, the
construction of a special traffic pattern is a little bit different from the above process. In Fig. 3, the Specific
Traffic Pattern of a grid network with size 6 9 6 is given.
As shown in Fig. 3, the 4 9 4 center grid is shown in the
dashed square. And for all grids, the construction of the
most central 4 9 4 grid is the same.
5.2 Performance evaluation under the partial
permutation traffic pattern
Figure 4 shows the maximum maximum link congestion
and average maximum link congestion of six routing
algorithms with network sizes 10 9 10 and 20 9 20. The
SPMM and NSPMM routing algorithms show the identical
results. The maximum and average maximum link congestions of these two routing algorithms are much smaller
than those of the other four routing algorithms. For the

123

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10

(d)
Average Maximum Link Congestion

Maximum Maximum Link Congestion

(c)

150

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

SPMM and NSPMM routing algorithms, the improvement
ratios of the maximum maximum link congestions are
around 50, 65, 45, and 62 % compared to RowColumn,
ZigZag, Greedy, Random routing algorithms, respectively.
The average link congestions of all the shortest path
routing algorithms under a certain traffic pattern should be
the same. Thus, we only need to compare the average link
congestions of the non-shortest path routing algorithm with
the shortest path routing algorithms. In Fig. 5, it shows the
distribution of the average link congestions in grids with
size 10 9 10 and 20 9 20. The frequencies of SPMM
routing algorithm are higher than those of NSPMM routing
algorithm for small link congestions. However, for large
link congestions, the frequencies of SPMM routing algorithm are smaller than those of NSPMM routing algorithm.
It’s because of the longer path length generated by
NSPMM. The long path lengths may not lead to a larger
maximum link congestion (which can be seen from Fig. 4),
however they do increase the average link congestion.
Considering that the SPMM, RowColumn, ZigZag,
Random and Greedy routing algorithms are all shortest
path routing algorithms, the maximum/average path length
of each flow under these five routing algorithms are the

(a) 40

(b) 17.5
Average Maximum Path Length

Fig. 10 Maximum path length:
general flows. a Maximum
maximum path length: 10 9 10.
b Average maximum path
length: 10 9 10 c Maximum
maximum path length: 20 9 20.
d Average maximum path
length: 20 9 20

1725

Maximum Maximum Path Length

Wireless Netw (2015) 21:1713–1732

35
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30

25

20

15
50

100

150

200

250

300

350

400

17
16.5
16
15.5
15
14.5
50

Number of Flows with Network Size 10*10

150

200

250

300

350

400

(d) 38

90
80
70

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

60
50
40
30
200

100

Number of Flows with Network Size 10*10

Average Maximum Path Length

Maximum Maximum Path Length

(c)

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

same, which can be seen in Fig. 6. The differences of path
lengths between the non-shortest path routing algorithm
and the shortest path routing algorithms can be seen clearly
in Fig. 6. Under both the 10 9 10 and 20 9 20 grids, the
maximum maximum path lengths, average maximum path
lengths, maximum average path lengths and average
average path lengths of the NSPMM routing algorithm are
larger than those of the five shortest path routing
algorithms.
In Fig. 7, we show the path congestion to path length
ratios. It’s obvious that the maximum maximum, average
maximum, maximum average, and average average ratios
of the NSPMM and SPMM routing algorithms are smaller
than all the other four routing algorithms. The ratio reflects
the average link congestion for a path. The smaller the ratio
of a path is, the lighter the average link congestion the path
has. Still, the differences of the ratios between NSPMM
and SPMM routing algorithms are very small. However,
they do achieve different performances for some other
metrics. This can be seen in Fig. 8(a), the maximum
maximum path congestions of the NSPMM routing algorithm under grid networks with sizes 10 9 10 and 20 9 20
are very high. For SPMM routing algorithm, the values in

37
36
35
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

34
33
32
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

all the four subfigures are the smallest. Recall the maximum maximum path lengths in Fig. 6(a). The main reason
is that the long path lengths lead to the higher path congestions. In a network, let the minimum path congestion be
lm and the maximum path congestion be lM. By increasing a
path length by 1, the increasing congestion is at least lm ?1,
and in some cases, the path congestion may be increased by
lM ?1. Thus, the path length and network link congestion
play important parts in order to achieve smaller path
congestion.
5.3 Performance evaluation under the general traffic
pattern
For the scenarios with general traffic patterns, we run the
simulation for 20,000 times with varying the number of
flows. For the 10 9 10 grid, we vary the number of flows
from 50 to 400 with an increment of 50. For the 20 9 20
grid, we vary the number of flows from 200 to 1,600 with
an increment of 200.
In Fig. 9, we show the maximum maximum link congestions, average maximum link congestions for the grids
with sizes 10 9 10 and 20 9 20. The values increase with

123

1726

(a) 150

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

Average Maximum Ratio

Maximum Maximum Ratio

Fig. 11 Maximum ratio:
general flows. a Maximum
maximum ratio: 10 9 10.
b Average maximum ratio:
10 9 10 c Maximum maximum
ratio: 20 9 20. d Average
maximum ratio: 20 9 20

Wireless Netw (2015) 21:1713–1732

50

0
50

100

150

200

250

300

350

Number of Flows with Network Size 10*10

Average Maximum Ratio

Maximum Maximum Ratio

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

the increasing of the number of flows. The NSPMM and
SPMM routing algorithms achieve the smallest maximum
maximum and average maximum link congestions. Like
the performances under the partial permutation traffic
patterns, the NSPMM and SPMM routing algorithms
achieve the identical maximum and average maximum link
congestions. This phenomenon motivates us to observe the
path lengths under these two routing algorithms. As shown
in Fig. 10, in all the four sub-figures, the path lengths of the
NSPMM routing algorithm are larger than those of the
other five shortest path routing algorithms. And the path
lengths of all the shortest path routing algorithms are the
same.
For the path congestion to path length ratios shown in
Fig. 11, the best performance routing algorithms are the
NSPMM and SPMM routing algorithms. Also, differences
between the two routing algorithms are not obvious under
this metric. However, in Fig. 12, the differences are
obvious. The maximum path congestions of the NSPMM
routing algorithm become very high. However, the
SPMM routing algorithm is still the best under this
metric.

123

50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10

(d)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

0
50

400

(c) 150

150

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

5.4 Performance evaluation under the specific traffic
pattern
Now, given the evaluation of the performances of the
routing algorithms under the partial permutation and general traffic patterns, we’d like to predict that the SPMM and
NSPMM routing algorithms will still achieve the best
performances of the maximum link congestions and the
path congestion to path length ratios under the Specific
Traffic Pattern. The path congestions of the NSPMM
routing algorithm should be bad.
The prediction of the maximum link congestions and
path congestion to path length ratios are correct, as shown
in Figs. 13 and 14. However, for this traffic pattern, the
path congestions of the NSPMM are not the worst, as
shown in Fig. 15(a) and (b).
Besides the previous observations, there is something
interesting about the maximum link congestion which is
the main motivation for us to study this traffic pattern. For
the 10 9 10 and 20 9 20 grids, the maximum link congestions under this traffic pattern is 6 and 11, respectively.
When constructing 20,000 instances of partial permutation

Wireless Netw (2015) 21:1713–1732

(b)

Maximum Maximum Path Congestion

(a)
600
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

500
400
300
200
100
0
50

100

150

200

250

300

350

400

Average Maximum Path Congestion

Fig. 12 Maximum path
congestion: general flows.
a Maximum maximum path
congestion: 10 9 10. b Average
maximum path congestion:
10 9 10 c Maximum maximum
path congestion: 20 9 20.
d Average maximum path
congestion: 20 9 20

1727

Number of Flows with Network Size 10*10

(d)

2500

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

2000

1500

1000

500

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

Maximum Link Congestion

30
25
20

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

15
10
5
0
10 * 10

400
300
200
100
0
50

100

150

200

250

300

350

400

2500
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

2000

1500

1000

500

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

smaller than the corresponding values under the Specific
Traffic Pattern.
Although we can’t prove it, we strongly believe that the
specific traffic pattern is a kind of worst-case traffic pattern
for our routing algorithms, which are NSPMM and SPMM.
And for a n1 9 n2 grid network, the maximum link con
 

gestion under this traffic pattern is maxf n12þ1 ; n22þ1 g. If

 

the guessing is true, maxf n12þ1 ; n22þ1 g will be the upper
bound of the maximum link congestion under the NSPMM
and SPMM routing algorithms for the Partial Permutation
Traffic Patterns.

40
35

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

500

Number of Flows with Network Size 10*10
Average Maximum Path Congestion

Maximum Maximum Path Congestion

(c)

600

20 * 20

5.5 Delay and throughput

grid size
Fig. 13 Link congestion: specific traffic pattern

traffic patterns randomly, the maximum link congestions
[refer to Fig. 4(a)] among the 20,000 instances is 5 and 7
for the 10 9 10 and 20 9 20 grids, respectively. Both are

Besides the above congestion related performance metrics,
delay and throughput are also importance metrics to estimate the network performance. However, the delay and
throughput are related to the path length, link congestion of
the flow path, and also the scheduling algorithm adopted
when transmitting packets.
Here, we choose the simple FCFS (First Come First
Serve) scheduling algorithm and run 20,000 instances for

123

1728

Wireless Netw (2015) 21:1713–1732

(a) 40

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30
25
20

30

15

25
20
15

10

10

5

5

0

10 * 10

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

35

Average Ratio

Maximum Ratio

35

40

0

20 * 20

10 * 10

grid size

20 * 20

grid size

Fig. 14 Ratio: specific traffic pattern. a Maximum ratio. b Average ratio

400

300

(b) 500
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Path Congestion

Maximum Path Congestion

(a) 500

200

100

0

10 * 10

20 * 20

400

300

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

200

100

0

10 * 10

grid size

20 * 20

grid size

Fig. 15 Path congestion: specific traffic pattern. a Maximum path congestion. b Average path congestion

each setting. In this section we only show the results of the
general traffic pattern considering that the partial permutation and the specific traffic pattern are special cases of the
general traffic pattern.
We show the average delay of the network among all the
flows by varying the number of flows in Fig. 16. Also, we
choose the network size to be 10 9 10 and 20 9 20. From
the figures, we can see that for the average average delay,
the NSPMM and SPMM routing algorithms achieve the
shortest delays. And for the average maximum delay, the
SPMM routing algorithm achieves the lowest delay, and
the NSPMM routing algorithm achieves the second lowest
delays.
The throughputs of different routing algorithms are
shown in Fig. 17. For the 10 9 10 grid, we choose the
delay bound to be 10 slots and 20 slots. For the 20 9 20
grid, the delay bounds are chosen to be 20 slots and 40
slots. When increasing the number of packets, the SPMM

123

and NSPMM routing algorithms achieve the most number
of packets successfully received within the specific delay
bound compared to other routing algorithms, except in
Fig. 17(c) when the number of flows is 1,600. The reason
for this is that the network is in a congested state.
5.6 Discussion
5.6.1 Maximum link congestion
Under the Partial Permutation Traffic Pattern, a guessing of
the upper bound of the maximum link congestion achieved
by the NSPMM and SPMM routing algorithms is

 

maxf n12þ1 ; n22þ1 g in a n1 9 n2 grid network. If the
specific traffic pattern is a kind of worst-case traffic pattern,
the upper bound will be true.
Through the above numerical results, we know that the
NSPMM routing algorithm sacrifices path length to achieve

(a) 18
Average Average delay

Fig. 16 Delay performance.
a Average average delay:
10 9 10. b Average maximum
delay: 10 9 10. c Average
average delay: 20 9 20.
d Average maximum delay:
20 9 20

1729

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

16
14
12
10
8
6

50

100

150

200

250

300

350

(b)

45

Average Maximum delay

Wireless Netw (2015) 21:1713–1732

40

Number of Flows with Network Size 10*10

25
20

100

150

200

250

300

350

400

(d) 90
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30

25

20

15

10
200

30

Number of Flows with Network Size 10*10

Average Maximum delay

Average Average delay

(c) 35

35

15
50

400

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

smaller maximum link congestion. However, this disadvantage can be avoided by the SPMM routing algorithm
since it achieves the smallest maximum link congestion
and uses only the shortest paths.

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

80
70
60
50
40
30
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

notice from the flow with priority i - 1, it then can start
the path seeking process.

6 Conclusions
5.6.2 Algorithm implementation
The implementation of the SPMM routing algorithm is
much easier than the NSPMM routing algorithm, it can be
implemented distributed. The only information that should
be informed to the source of each flow is when it should
start to seek the routing path. This can be achieved if all the
flows first broadcast a packet telling others the number of
its shortest paths. According to this information, each flow
knows its priority which determines when it can seek the
routing path. When a flow finishes the path seeking process, it then broadcasts a packet to inform other flows. If a
flow with priority i receives the routing path completion

In this paper, we have proposed the NSPMM and SPMM
routing algorithms aiming to minimize the maximum link
congestion in grid networks. By evaluating these two
routing algorithms under the Partial Permutation Traffic
Pattern, General Traffic Pattern and the Specific Traffic
Pattern, we know that these two routing algorithms can
achieve smaller link congestions compared to RowColumn,
ZigZag, Random and Greedy routing algorithms. However,
the NSPMM routing algorithm has sacrificed the path
length and path congestion. And the delay and throughput
performance of the SPMM and NSPMM routing algorithms are better than the other four routing algorithms.

123

1730

(b)
140
120
100
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

80
60
40
20
50

100

150

200

250

300

350

400

Average Average Number of Packets

(a)
Average Average Number of Packets

Fig. 17 Throughput
performance. a Average average
number of packets: 10 9 10, 10
slots. b Average average
number of packets: 10 9 10, 20
slots. c Average number of
packets: 20 9 20, 20 slots.
d Average average number of
packets: 20 9 20, 40 slots

Wireless Netw (2015) 21:1713–1732

Number of Flows with Network Size 10*10

250
200
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0
50

100

150

200

250

300

350

400

(d)
500
450
400
350
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

300
250
200
150
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

Overall, the SPMM routing algorithm is the most efficient
routing algorithm.
Acknowledgments This work was supported by the National Hightech R&D Program of China (863 Program) (Grant No.
2012AA010904), and the Scientific Research Fund of Sichuan
Province, China (Grant Nos. 2013GZ0016, 13ZA0296). And our
thanks to the China Scholarship Council (CSC) for the support for the
Joint Ph.D. Program.

References
1. Awduche, D., Chiu, A., Elwalid, A., Widjaja, I., & Xiao, X.
(2002). Overview and principles of internet traffic engineering.
Tech. rep., RFC 3272.
2. Siripongwutikorn, P., Banerjee, S. & Tipper, D. (2002). Traffic
engineering in the internet: A survey of load balanced routing.
White paper.
3. Ravindra Kumar Singh, N. S. C., & Saxena, K. (2012). Load
balancing in ip/mpls networks: A survey. Computer Science and
Communications, 4(2), 151–156.
4. Askarian, C., & Beigy, H. (2012). A survey for load balancing in
mobile wimax networks. Advanced Computing: An International
Journal, 3(2), 119–137.
5. Wajgi, D., & Thakur, N. V. (2012). Load balancing algorithms in
wireless sensor network: A survey. International Journal of
Computer Networks and Wireless Communications (IJCNWC), 2,
456–460.

123

300

Number of Flows with Network Size 10*10
Average Average Number of Packets

Average Average Number of Packets

(c)

350

1400
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

1200
1000
800
600
400
200
0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

6. Yao, Y., Cao, Q., & Vasilakos, A. V. (2013). EDAL: An energyefficient, delay-aware, and lifetime-balancing data collection
protocol for wireless sensor networks. In: MASS (pp. 182–190).
7. Shen, Zhijie, et al. (2011). Peer-to-peer media streaming: Insights and
new developments. Proceedings of the IEEE, 99(12), 2089–2109.
8. Suri, P. K., & Kaur, S. (2012). A survey of load balancing
algorithms in manet. Engineering Science and Technology: An
International Journal, 2(3), 495–504.
9. Maheshwari, D., & Nedunchezhian, R. (2012). Load balancing in
mobile ad hoc networks: A survey. International Journal of
Computer Applications, 59(16), 44–49.
10. Kelly, F. P., Maulloo, A. K., & Tan, D. K. (1998). Rate control
for communication networks: Shadow prices, proportional fairness and stability. Journal of the Operational Research Society,
49(3), 237–252.
11. Firoiu, V. & Borden, M. (2000). A study of active queue management for congestion control. In INFOCOM 2000. Nineteenth
Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE (Vol. 3, pp. 1435–1444).
12. Li, Mo, et al. (2013). A survey on topology control in wireless
sensor networks: Taxonomy, comparative study, and open issues.
Proceedings of the IEEE, 101(12), 2538–2557.
13. Chen, L., Low, S. H. & Doyle, J. C. (2005). Joint congestion
control and media access control design for ad hoc wireless
networks. In INFOCOM 2005. 24th Annual Joint Conference of
the IEEE Computer and Communications Societies. Proceedings
IEEE (Vol. 3, pp. 2212–2222).
14. Kewei Sha, J. G., & Greve, J. G. (2013). Multipath routing
techniques in wireless sensor networks: A survey. Wireless Personal Communications, 70(2), 807–829.

Wireless Netw (2015) 21:1713–1732
15. Radi, M., Dezfouli, B., Bakar, K. A., & Lee, M. (2012). Multipath routing in wireless sensor networks: Survey and research
challenges. Sensors, 12(1), 650–685.
16. Piratla, N. M. & Jayasumana, A. P. (2006). Reordering of packets due
to multipath forwarding-an analysis. In ICC’06 (Vol. 2, pp. 829–834).
17. Robinson, J. & Knightly, E. W. (2007). A performance study of
deployment factors in wireless mesh networks. In INFOCOM
2007. 26th IEEE International Conference on Computer Communications. IEEE (pp. 2054–2062).
18. Busch, C., Kannan, R., & Samman, A. (2012). Bottleneck routing
games on grids. In Game Theory for Networks (Vol. 75 LNICST,
pp. 294–307).
19. Leighton, F. T., Maggs, B. M., & Rao, S. B. (1994). Packet
routing and job-shop scheduling in O (congestion ? dilation)
steps. Combinatorica, 14(2), 167–186.
20. Banner, R., & Orda, A. (2007). Bottleneck routing games in
communication networks. IEEE Journal on Selected Areas in
Communications, 25(6), 1173–1179.
21. Busch, C., Kannan, R., & Vasilakos, A. V. (2008) Quality of
routing congestion games in wireless sensor networks. In Proceedings of the 4th Annual International Conference on Wireless
Internet. no. p. 71.
22. Busch, C., & Magdon-Ismail, M. (2009). Atomic routing games
on maximum congestion. Theoretical Computer Science,
410(36), 3337–3347.
23. Rajgopal Kannan,et al. ‘‘Optimal Price of Anarchy of Polynomial
and Super-Polynomial Bottleneck Congestion Games.’’ GAMENETS. pp. 308-320, 2011.
24. Busch, Costas, et al. (2012). Approximating congestion ? dilation in networks via ‘quality of routing’ games. IEEE Transactions on Computers, 61(9), 1270–1283.
25. Spyropoulos, T., et al. (2010). Routing for disruption tolerant networks: Taxonomy and design. Wireless Networks, 16(8), 2349–2370.
26. Zeng, Y., et al. (2013). Directional routing and scheduling for
green vehicular delay tolerant networks. Wireless Networks,
19(2), 161–173.
27. Youssef, M., et al. (2014). Routing metrics of cognitive radio
networks: A survey. IEEE Communications Surveys and Tutorials, 16(1), 92–109.
28. Liu, Y., et al. (2010). Multi-layer clustering routing algorithm for
wireless vehicular sensor networks. IET Communications, 4(7),
810–816.
29. Chen, Kai, et al. (2011). Survey on routing in data centers:
Insights and future directions. IEEE Network, 25(4), 6–10.
30. Li, P., Guo, S., Yu, S., & Vasilakos, A. V. (2012). CodePipe: An
opportunistic feeding and routing protocol for reliable multicast
with pipelined network coding. In INFOCOM (pp. 100–108).
31. Demestichas, Panagiotis, et al. (2004). Service configuration and
traffic distribution in composite radio environments. IEEE Transactions on Systems, Man, and Cybernetics, Part C, 34(1), 69–81.
32. Britta, P., Martin, S., & Andreas, W. (2010). Packet routing on
the grid. In Theoretical Informatics—9th Latin American Symposium, Proceedings (Vol. 6034 LNCS, pp. 120–130).
33. Badr, H. G., & Podar, S. (1989). An optimal shortest-path routing
policy for network computers with regular mesh-connected topologies. IEEE Transactions on Computers, 38(10), 1362–1371.
34. Weller, T., & Hajek, B. (1994). Comments on’’an optimal
shortest-path routing policy for network computers with regular
mesh-connected topologies. IEEE Transactions on Computers,
43(7), 862–863.
35. Wu, J. (1999). Maximum-shortest-path (msp): An optimal routing
policy for mesh-connected multicomputers. IEEE Transactions
on Reliability, 48(3), 247–255.

1731
36. Takatsu, S., Ooshita, F., Kakugawa, H., & Masuzawa, T. (2013).
Zigzag: Local-information-based self-optimizing routing in virtual grid networks. In International Conference on Distributed
Computing Systems, 33rd IEEE (pp. 357–368).
37. Liu, J. W. (2000). Real-time systems (pp.115–189). Upper Saddle
River, NJ: Prentice Hall PTR.
38. Dhanapala, D. C., Jayasumana, A. P., & Han, Q. (2009). Performance of random routing on grid-based sensor networks.
CCNC, 2009, 1–5.
39. Rajasekaran, S. (1991). Randomized algorithms for packet routing on the mesh. Technical Reports (CIS). Paper 328. http://
repository.upenn.edu/cis_reports/328.
40. Busch, C., Magdon-lsmail, M., & Xi, J. (2008). Optimal oblivious
path selection on the mesh. IEEE Transactions on Computers,
57(5), 660–671.
41. Ahuja, R. K., Magnanti, T. L., & Orlin, J. B. (1993). Network
flows: Theory, algorithms, and applications (pp. 649–684). Upper
Saddle River, NJ: Prentice Hall PTR.
42. Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001).
Introduction to algorithms (Vol. 2). Cambridge: MIT press.
43. Kaibel, V., & Peinhardt, M. (2006). On the bottleneck shortest
path problem. Technical Reports.
44. Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps and
their uses in improved network optimization algorithms. Journal
of the ACM (JACM), 34(3), 596–615.

industry control
communications.

networks,

Jun Xu received her Bachelor’s
degree in the School of Electronic Information, Wuhan
University, China, in 2009. She
is currently a Ph.D. candidate in
the School of Electronic Information, Wuhan University,
China. And she is also a visiting
scholar in the School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, USA,
from Sep. 2012 to Mar. 2014.
Her research interests are in the
area of wireless mesh networks,
and real-time and reliability

Jianfeng Yang received his
Bachelor’s degree in Electronic
Information in 1998, Master’s
degree in Signal and Information System in 2003 and Ph.D.
degree in Communication and
Information System in 2009, all
from the school of Electronic
Information, Wuhan University,
China. He is Currently an
Associate Professor of Wuhan
University, his research interests
are in the areas of Embedded
System, Wireless Mesh Networks, Parallel Computing and
real-time and Reliability Communications. Also, he severed as the
committee member of China Computer Federation Technical Committee of Embedded System from 2010.

123

1732

Wireless Netw (2015) 21:1713–1732

Chengcheng Guo received his
Ph.D. degree in the School of
Electronic and Information,
Wuhan University, China. He
received his Bachelor and Master degree in the Computer
School of Wuhan University,
China. He is currently a professor and a Ph.D. supervisor in the
School of Electronic and Information, Wuhan University,
China. His research interests are
Internet and Communication
technology, wireless mesh networks, industry control networks, and real-time and reliability communications.
Yann-Hang Lee received his
Ph.D. degree in Computer,
Information, and Control Engineering from the University of
Michigan, Ann Arbor, MI, in
1984. He is currently a professor
in the School of Computing,
Informatics, and Decision Systems Engineering, Arizona State
University, USA. Since 2008,
he has served as the Program
Chair of the Computer Science
and Engineering Program in the
School. Dr. Lee’s research
interests are in the areas of real-

123

time computing, embedded systems, software engineering, distributed
systems, and performance evaluation.
Duo Lu received his Bachelor’s
degree in Shanghai Jiao Tong
University, China, in 2009. He
is currently a Ph.D. student in
the School of Computing,
Informatics, and Decision Systems Engineering, Arizona State
University, USA. His research
interests are in the area of realtime embedded system and realtime wireless network.

1328

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 11, NOVEMBER 1987

Optimal Checkpointing of Real-Time Tasks
KANG G. SHIN, SENIOR MEMBER, IEEE, TEIN-HSIANG LIN, STUDENT MEMBER, IEEE, AND YANN-HANG LEE, MEMBER, IEEE

Abstract-Analytical models for the design and evaluation of
checkpointing of real-time tasks are developed. First, the execution of a real-time task is modeled under a common assumption
> P of> on-line
. detection
. mechanisms
s .
. is.
of perfect coverage
(which
termed a basic model). Then, the model is generalized (to an
extended model) to include more realistic cases, i.e., imperfect
coverages of on-line detection mechanisms and acceptance tests.
Finally, we determine an optimal placement of checkpoints to
minimize the mean task execution time while the probability of an
. >..~~~
\a specified
..
unrellable result (or lack of confidence) is .,
kept. below
level.
level.
In the basic model, it is shown that equidistant intercheckpoint
intervals are optimal, whereas this is not necessarily true in the
extended model. An algorithm for calculating the optimal

1.

number of checkpoints and intercheckpoint intervals is presented
some numerical examples for the extended mlodel.

witb

Index Terms-Checkpointing, failure coverages, mean task
execution time, on-line detection mechanisms and acceptance
tests, optimal placement of checkpoints, probability of an
unreliable result, rollback and restart failure recovery.

greatly reduce the mean time of running a long program on an
unreliable computing system. In their discussion, equal
inre ckpo intervals
tintervalsmaeIusedef
inr s checkont in
are used for inserting checkpoints in
1~~~~~~~ntercheckpoint
the program. However, the degree of confidence in execution
results has not been addressed. The purpose of this paper is to
consider checkpointing as a viable method to satisfy both of
the above requirements (i.e., fast and correct execution) for
real-time tasks under more realistic assumptions. Mathematimodels will be developed first and the optimal solutions
~~~~~~~~~~~~~cal
will then be derived. Our discussion begins with a brief review

of the checkpointing techniques used in database systems.
Since for database applications the system is unavailable to
users during error recovery, an obvious objective of checkpointing database systems is to maximize the portion of the
time the system is available to users, i.e., system availability.
Another useful objective is the mean response time, which is
the average time a user has to wait until the system completes
his transaction request. Availability and mean response time
have been the primary criteria for evaluating the performance
of checkpointing in database systems. The variables com-

I. INTRODUCTION
(1HECKPOINTING a database system is defined as the monly used in such studies are 1) checkpointing time which is
time. required to save a checkpoint, 2) recovery time which
operation
of saving
sain.teurenvrso
of
the current version of the database the
.
_operation
is
the
ttime needed to reload a checkpoint and reprocess the
(called a checkpoint) on a separate secure device (such as a
backup tapes) and also saving the before-image and the after'
image of all transactions made between two successive successive checkpoints. The only controllable variable is the
checkpoints (called the audit trail). When an error is detected, intercheckpoint interval. The checkpointing time is systemthe system will stop normal operation and start a procedure for ddependent aand uusually assumed to be constant within a system.
The recovery time depends on the length of the audit trail
rollback recovery, which restores the system to the
is often assumed to be proportional to the sthen
'
number of
~~~~~~~~~~which
recent checkpoint and then reprocesses the transactions re- tasctios ofn tasumd tr an hence d
transactions
on the audit trail and hence depends upon the
corded on the audit trail. Since checkpointing is an effective
load and the intercheckpoint intervals. Consequently,
and economic
mtofrisystem
and conmicmetod or mprvingrelabiityof
ataase
most of earlier works [3]-[8] have been to determine the
systems (compared to the hardware redundancy technique) it
ost of terlierkworks [3]-[8] haveebenetodetemine the
o
i
i
t e
m
t
has been widely used and studied by many researchers.
availability or minimize the mean response time.'t A
system
s
Although most previous works have dealt with the perform- common
assumption in these works iS that errors are detected
ance evaluation of checkpointing in database systems (or other
iimmediately upon their occurrences.
transaction-oriented systems), the same basic concept can be
Young [3] made
approximation to the optimal
i a first-order
w
applied to real-time tasks. Note that a real-time task has i
min
under the
that eckointing
stringent requirements for fast and correct execution. The beteenkrors
between errors under the assumption that 1) checkpointing
studes n [1 an [2 hav shwn tat hecpoiningcan
time and intercheckpoint intervals are fixed, 2) errors do not
occur during error recovery, and 3) error occurrence is a

mostm

Manuscript received July 22, 1985; revised January 13, 1986 and March 6,
1987. This work was supported in part by NASA under Grants NAG-1-296
and NAG-1-492. An opinions, findings, and conclusions or recommendations

Poisson process Chandy et al [4 proposed three models A

B, and C. All three models assume high system availability,
fixed checkpointing time, and a Poisson error occurrence
a

expressed in this paper are those of the authors and do not necessarily reflect
the views
of
NASA.prcs.MdlAfrhrasmstaerosantocu
K. G. Shin and T.-H. Lin are with the Real-Time Computing Laboratory,

rcs.MdlAfrhrasmsta

roscno

cu

Department of Electrical Engineering and Computer Science, The University

during error recovery and the system load is constant. Model

Y.-H. Lee is with the IBM Thomas J. Watson Research Center, Yorktown
Heights, NY 10598.
IEEE Log Number 8715434.

'The probability distribution of response time is not usually available in a
manageable form.

of Michigan, Ann Arbor, MI 48109.

B allows errors to occur during error recovery. Model C

0018-9340/87/1100- 1328$Ol.OO © 1987 IEEE

SHIN et al.: CHECKPOINTING OF REAL-TIME TASKS

1329

assumes that the arrival rate of transaction requests varies
widely with time in a cyclic fashion. Optimal intercheckpoint
intervals are determined for models A and B to achieve a
maximum availability, while for model C the objective is to
minimize the number of transactions arriving during checkpointing and error recovery. Gelenbe [5] studied this problem
with a queueing model and assumed that intercheckpoint
intervals, checkpointing time, and intererror intervals are all
independent and exponentially distributed. Errors are not
allowed to occur during checkpointing and error recovery. He
then showed that the optimal average intercheckpoint interval
which maximizes the system availability is a function of the
system load. With a similar model, Gelenbe and Derochette
[6] derived an expression for the optimal average intercheckpoint interval which minimizes the mean response time. They
also found that the optimal intercheckpoint interval which
minimizes the mean response time is usually smaller than that
maximizing the system availability. Using theory in Markov
renewal process and semi-regenerative process, Baccelli [7]
obtained an analytic expression of the mean response time for
the same model as in [6].
Recently, Tantawi and Ruschitzka [8] proposed a general
model where an arbitrary distribution of the intererror interval
is considered. In their model, errors may occur during
checkpointing and error recovery, and checkpointing intervals
are allowed to depend on the audit trail reprocessing time and
error distribution. A general expression for the system
availability was derived. An equicost strategy for selecting the
intercheckpoint interval was also proposed and shown to be
superior to the equidistant strategy. (Other works typically use
the equidistant strategy with little justification.)
Checkpointing in a real-time system is quite different from
that in a database system. A real-time system usually includes
many tasks which do not communicate during their execution,
although these tasks can exchange information through shared
system memory, e.g., FTMP [9], [10] and the SIFT computer
[11]. Because a real-time task is usually executed periodically,
access to the system memory by any task is allowed only at
the beginning (for input) and the end (for output) of execution.
This restriction can be justified by the fact that real-time
applications are usually well-defined to be decomposed in such
a fashion and do not have luxury to allow for a long delay in
accessing shared resources such as system memory or bus.
Thus, a real-time task needs no audit trail since all the data are
local once the task is initiated. When a failure occurs, the
task has to redo all the computation after the last checkpoint or
start all over again from the beginning.
Fast and correct execution of tasks is of the utmost
importance to real-time systems. Thus, a real-time computer
system generally will have hardware redundancies (e.g.,
multiple processors, memories, and buses). As in most
multiprocessor systems, e.g., Cm *, we do not allow multiprogramming on each processor of the system. When a task is

should be considered as a unit on which checkpointing is
applied, i.e., a task-oriented view.
Taking the task-oriented view, we shall derive two mathematical models to describe the behavior of task execution and
occurrence of and recovery from errors. These models will
then be used to determine optimal intercheckpoint intervals
and an optimal number of checkpoints for a task by minimizing the mean task completion time subject to the required
confidence in execution results.
The paper is organized as follows. In the following section,
we introduce the general concept, design issues, and related
terminology in the checkpointing of real-time tasks. Section III
formally states the problem and presents the assumptions to be
used. Section IV derives the optimal checkpointing strategy
for the basic model. Section V considers the extended model
where a numerical algorithm is developed for calculating an
approximate solution. Section VI summarizes our results.

assigned to a processing unit in the system, it will run on that

backup on some other memory device or if the program itself may change

unit until it finishes (as long as the unit does not fail). So, a
failure affects only the task which is running on the failed unit.
Henc, itisn
loner pproriat to hinkof he ceckpintig
technique from a system-oriented view, but rather each task

II. CHECKPOINTING REAL-TIME TASKS
Checkpointing a real-time task means occassionally saving
the state of the task on other safe devices such as tapes, disks,
or even other (redundant) memory modules. The state of a task
includes values of data variables and contents of the internal
registers.2 The saved states of a task are called checkpoints3
or recovery points (RP's) [12], [13]. To ensure the correctness of the saved checkpoint, an acceptance test must be
applied to the checkpoint before saving it [14]. There are also
on-line detection mechanisms to detect fault manifestations
during task execution [13], [15], [16]. When a module fails
and the failure is detected either by the acceptance test or the
on-line detection mechanism, the most recently saved checkpoint for the task running on this module will be loaded to a
good module, and the task then resumes execution from that
checkpoint
A hardware fault is defined as an incorrect state caused by
the physical change in a component, whereas an error is
defined to be the erroneous information/data resulting from
the manifestation of a fault. As we classified in [15], there are
two important classes of detection mechanisms: one is termed

the signal-level detection mechanisms, and the other is
termed the function-level detection mechanisms. At the

signal level, the manifestation of a fault is captured by built-in
on-line detection mechanisms before the fault generates an
error in a program. Undetected faults may generate errors
which may then be captured by the function-level detection
mechanisms. The acceptance test is one of the function-level
detection mechanisms.
Consider the assumption that errors are detected immediately upon their occurrences. Extensive efforts have been
made to design various "failure4" detection mechanisms, yet
no detection mechanism can proclaim to cover all possible
2 The task state may even include the program code if it does not have a

during execution.
3In this paper we shall use the terms "checkpoint" and "state"
4We shall use the term "failure" to represent either fault or error,
depending on the context.

1330

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. I1, NOVEMBER 1987

failures. Even if all failures are covered, there may be some
latency between occurrence and detection of a failure under
any existing detection mechanism. If checkpointing is performed between a failure occurrence and its detection, the
checkpoint saved could be incorrect and, thus, the subsequent
rollback recovery following the detection of this failure may
become unsuccessful. In such a case, the task has to be
restarted from the beginning (i.e., restart recovery) if only
one checkpoint is saved. This reinforces the fact that an
acceptance test is needed to assure the correctness of the
checkpoint to be saved. If the acceptance test detects an error
or abnormality, then an appropriate error recovery should be
initiated. The error coverage of an acceptance test is again less
than 100 percent, i.e., imperfect coverage. The imperfect
detection coverages of the on-line detection mechanism and
the acceptance test imply the existence of a nonzero probability of a task ending with latent errors. The task may or may not
produce correct results under latent errors; however, we no
longer have any confidence in the results. These results are
hence called "unreliable." We quantify this fact by the
probability of anl unreliable result, F, as a measure of lack
of confidence.
In our models, the computation time of a task is an
estimate of the time for computing the task under a fault-free
situation, and the execution time of a task is the time needed
to complete the task under the occurrences of faults. The
computation time of a task can be determined a priori by
averaging over repeated tests and validations of the task, and

time between the ith and (i + l)th checkpoints, i.e.,

hence is a constant. The computation time has no direct
connection with the hard deadline associated with the task.
Tasks should be triggered by some mechanism (e.g., real-time
clock) in such a way that their hard deadlines will be placed
well after worst-case computation times under a fault-free
condition. Note, however, that the execution time may vary
due to the random occurrences of failure and will thus be
treated as a random variable.
There are other aspects of checkpointing (e.g., sensor data
and some integrated quantities such as time, velocity, or
position) that are not addressed here.5 These will usually
impose an upper limit on the intercheckpoint interval. Further,
the placement of checkpoints is dictated by functional and
programming considerations as well as the optimization
aspects to be discussed in this paper. For the latter, we will
consider two parameters of checkpointing: i.e., 1) the duration
of checkpointing and 2) the correctness of checkpoints.
III. PROBLEM STATEMENT

"=+1 - T - t

fl
(3.1)
O
where t, is the checkpointing time, and for consistency, we let
T - 0 and Tn+ = T. Using the above definitions, our
problem can be stated formally as
a task with respect to n and
Problem P. Minimize
0 c i c n, subject to F c
where Espec is the desired
level of confidence in execution results.
Note that P is a realistic problem which arises frequently in
the system design process. The constraint on F, Fspec, can be
viewed as a requirement that must be satisfied in order to
achieve the desired level of confidence in the execution result.
Also, note that real-time constraints such as hard deadlines,
correctness, memory sizes, etc., are included implicitly in P,
since 1) the optimal criterion is related to the task execution
time, and 2) Espec is derived from both correctness and
execution time.
Other variables in our models are defined below. Let t, be
the checkpointing time which is assumed to be fixed and
consists of two parts: the time for an acceptance test and the
time for saving the current state. An acceptance test is
automatically performed at the end of a task, but state saving is
not needed following this last acceptance test. The last
acceptance test is, however, assumed to take the same time tc
as the regular checkpointing, because the time for the
acceptance test makes up the major portion of tc if a high
detection coverage is desired. However, the last checkpoint is
notcountedinthenumberofcheckpoints.Failuresareallowed
to occurduring normal executionaswellas during checkpointing. Whenever a failure is detected during checkpointing, that
checkpoint is considered to be invalid and, therefore, not
saved. Due to the storage overhead, only the most recent
checkpoint is assumed to be saved, i.e., at any time the state

Wfofr
Eapec,

hed,

savint

is assume o be
pi ntime
Define
saving device can store only one checkpoint.
T I + tc,

0:

i n.

(3.2)

Henceforth, intercheckpoint intervals are used to mean either
Tr or Ii depending on the context. Fig. 1 shows a timing
diagram of task execution.
On-line detection mechanisms can detect a failure upon its
occurrence with the probability d E (0, 1] or cannot detect the
failure at all (i.e., the failure is not covered by the mechanism)
with the probability 1 - d. Those failures undetectable by the
on-line detection mechanism can only possibly be detected by
the next level of detection, i.e., acceptance tests. If the system

contains some latent errors, they will be detected by the
Consider a task with a fault-free computation time T and the ccetance tes t arpositieprbili c et(0, 1] tha
acceptance test,
.
ioa
c, i the codt
total execution time w. Define W as the mean execution time the coveragye
of an acceptance test. c, iS the conditional
tt
and F as the probability of producing an unreliable result at the prbblt ofdtcigteicrecns.ftets
that
of that
task
furhe
end o thetaskSuppoe futherthat
(ye unkown)given
there are latent errors in the system.
checkpoints will be inserted at T,, 1 . i < n, during task
Suppose a failure occurs and is detected within zi' of its
execution. The ith checkpoint is established when the task
curneadthsyemcnrcorfomheaireb
execution has successfully progressed up to T. Define the ith either rollback or restart recovery. For rollback recovery, we
intercheckpoint interval, I_, 0 < i < n, as the computation first restore the system to the most recently saved checkpoint
and then resume the task from that point. Let r be a constant

end~
~
~ ~ ~ ~ ~ ~ ~~~~~~orbblt
thecn

This point was brought to the author's attention by an anonymous referee.

Supposens

nas
(yetuknwn

representing rollback setup time. The rollback setup time is

1331

SHIN et al.: CHECKPOINTING OF REAL-TIME TASKS
C

T + (n +l)t,

4

- rO -

Fig. 1.

T,

--4r I

T2

+

failure

-

Timing diagram for checkpointing in a real-time task.

the time measured from failure occurrence to the restoration of
the most recent checkpoint. In case of unsuccessful rollback(s), the system will attempt a restart recovery. Let s be the
restart setup time which is the mean time from failure
occurrence to the restart of the task [including the time spent
on unsuccessful rollback(s)]. The assumption of constant tC
and r is made mainly for two reasons: reasonableness and
mathematical tractability. In most cases, these quantities do
not vary much because the routines for establishing a
checkpoint and setting up rollback differ only in the number of
variables being saved/checked and this number is rather
stabilized in most practical cases.
Rollback recovery may fail due to 1) failure of the
communication link to the checkpoint saving device, and 2)
failure internal to the checkpoint saving device, thereby
making the saved checkpoints inaccessible or incorrect. In
these cases, the task has to be recovered by restart.6 Let p and
q, where p + q = 1, be the probabilities of recovering a task
an restart, under the condition that no latent
rolbc and
byby rollback
'
failures (to be explained later) exist when the last
ing is done. If this condition is not satisfied, we will take a
more pessimistic view and assume that restart recovery is
inevitable.
Throughout the paper, we assume that arrival of failures is a
the system
Poisson process with rate X. We also assume X
task.'
a
it
starts
when
failures
latent
contains no
The major difference between the basic and the extended
model is in the assumptions of coverages of the acceptance test
and the on-line detection mechanism. In the basic model it is
assumed that d = 1, i.e., detection of a failure coincides with
its occurrence. However, in the extended model, no assumption on d and c is made. Note when d = 1, the value of c
becomes irrelevant, that is, acceptance tests are not needed at
all (since all failures can be detected solely by the on-line
detection mechanism). The perfect coverage of the on-line
detection mechanism implies that 1) there are no latent failures
in the system, 2) the task execution results are always correct
(i.e., E = 0), and 3) each checkpoint is always correct.
Hence, in the basic model, the problem P is reduced to finding
the solution that minimizes W without any constraint on E.
However, it is practically impossible to design a signal-level
detection mechanism with perfect coverage. In some cases, we
cannot even accurately determine failure coverage. Thus, we
have to consider imperfect failure coverages for the design and
analysis of a real system.
Consideration of both imperfect coverages and the probability of an unreliable result is more realistic and natural, and is
thus a significant departure from previous works in the

Ti

Ti

v

w

w

*1

.
Fig. 2. Graphical explanation for W,,
!

V,, and Wi+1.

aforementioned references. Another assumption which is
different from existing models in the literature is that the
checkpoint saving device is subject to failures so that the saved
checkpoints may be destroyed or inaccessible. Therefore,
there is a nonzero probability of restart recovery even in the
basic model.
Let

w_

1

'

IV. BASIC MODEL
.
< i
n + 1, be the execution time from the

of the task to the first completion of the ith
hbeginning
c
the to,an eteW ti£ (i)e the
ntI.e.,
W
the texection te derive
+ae
execution time of the task, respectively. We shallandethe
mean execu1ian

retr,udrtecniinta.oltn
hw innFg
xrsinfrWi
Fig.
for Win terms offW.A
J'i. As shown
expression
.~~~hekoit a recursive
for 0 i n let v represent the task execution time in the
checkpont-2,
interval [T., Ti+ 1], and zi be the computation time completed
within Ti, given that a failure occurs during Ti. If Y represents
then the density
the interval between two successive

6 As pointed out earlier, previous works r3]-r8] assume that the checkpoint
saving device and its link would never fail, and hence rollback recovery is
always successful, i.e., no restart recovery is needed.
7Relaxation of this assumption for this work is not difficult.

o

m

em

a reusv

function of Y is fy(.y)
o
f

=

Xe

y

failures,

0.

The probability of

~~~failure occurring during -ri, Fi(ri), becomes8
for Ocicn. (4.1)
F1(T-) = Prob [ YCT-] = 1 - e-xui
By definition, z; represents the computation done within T1
under the condition that some failures do occur during Ti. The
density function of zi can be expressed as

fy(t)

fzi(t) = f(

Xe-'
Xe

t<zz.

F (Ti) I - e -

77j

Zi=

0tT
i

tfz1(t) dt=

Tje

i

(4.2)

(4

(4.3)

In the interval To, detection of a failure always leads to a
restart, and after a task is restarted the process is renewed
probabilistically for the variable w1. Thus,

wI

l

To

Zo + s + w1

-.

with probability 1- Fo(ro) (4.4)
with probability Fo(To).

Th roesialornwdfrw,1.i<n

an/rrsatrcvr.Hne
i1=W

+u

ftrolbc

fo1<<n(45

x Although the subscript for F is not necessary in the case of Poisson failure
process, it is adopted for clarity and extension to a general failure process.

1332

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 11, NOVEMBER 1987

where

(Ti
zi+ r+ vi

n
n
aw
J
hY0U1
u1+ky
f1I
aT=
j=2
i=2

with probability I - F1 (r)
with probability Fi(T1)p
(46)

with probability Fi(Ti)q.

zi + s + wi+ I

From (4.4), (4.5), and (4.6), the following recursive expressions are derived for 1 . i < n.
F0 (To)

WITO+ I -Fo(') (Z0±s)
F'(Ti)

-pF (r)

1 -Fj(rT)

1 -Fj(T)

aw aw
7To

)

(48)

i=

Wi+ I= (qeXT7i±p)W/V1(eX1Ti-1)

(4.10)

Applying (4.10) recursively n times, we can get W = Wn 1
W= hYoJJtuj±+kyiJ7Ju1±+ * +kyn- lUn +kyn (4.1 1)

where

j

j

(hy'u1-hyou'-ky')=O.

Again,
T)

qI
4e
l(+p oge (kh
x

(4.16)

This proves the first part of the equality in the theorem. For 1
< n -1
-=V,hyo.
j+
j=I

uikfl

I
j=2
j=i+l

+ u/- hyo J

aw

The problem now is to minimize Wwith respect to n and Ti,
i c n, subject to

aT7i+ I

<

+Tn= T+(n + It, - T'.

+ky***
uj

u kyifI
l'

u1

,

juu

n

n

j=i+2

j=i+2

3W
(kyf n~---0=
u+1kyiu1'+1ky/f
aTi

(AI±+(ui'ui+,-uiu/'+1) (hyo j*i,i+1=1

- 7)]=°

n

+ ky1 ]7
j=2

_W

awO awT

T

ui +

j:#i,i+ I

I

n

+ ky11 H u1)
j=i+2 /

Substituting the expressions for ui, yi, ui', and y1', in the
above equation produces

which is equivalent to
dW

jJ u1

j-i+2

11

(4.14)

Proof: Let 0 be a Lagrangian multiplier. The optimal
solution will then satisfy the differential equations

V[WW+(T T-To-T1-

u
n

ji+1

(4.13)

= T2
= 7n
l±X r\ =T1=T2~~~=Tn.

n

11

j*i+l

We shall approach this problem in two steps: first assume n is
given and minimize W with respect to Ti for 0 c i c n, then
use the above expressions to minimize W with respect to n.
-W
The following theorem provides a solution to the first step.
c9T+l
Theorem 1: For a given n, the minimum W is attained
whenn

logn (=
AOge

H

+ukkyi
(4.12)

70

(4.17)

1pxI+Xs

j=i+

ui=qexu+p, k=-+Pr+qs
yi=eXTi-ln.

ro +T +

XeXTi and ui'

=

)=T++-log (1+Xr)

a\

i2

(4.15)

substitute (4.12) for h and k in (4.16) to get

1
1
h=- +s, k-u-+pr+qsI

h=- +s,

0

I(hyO ul-hy0ul'-kyl')=O.

hpeXro - (k - hq)eXTI -0.

(4 9)

.+sq

j

Substitute (4.12) for ui and yj and then use yi'
Xqexri to get

Substituting F,(Ti) and z; into (4.7) and (4.8) produces
( + sYexo- 1)

7T1

Since H>2 U1 = fi
'I'"=2 (qeXri + p) * 0, it must be true that

(4.7)

(

n

= n
j=2

=

6p(er

nn

/

(
Xi

hyo

u1±+ky1

Then,1
aw

aT

ot

j+**+Y

±±k1
j=

IUj

U
/=+

kker-Xil

j=i+2

j

4 u1+k<

1333

SHIN et al.: CHECKPOINTING OF REAL-TIME TASKS

Since p > 0
/

by assumption,
n

j=1

n

u1+ky,

Xpq hyo J7

C

J uj

O

j=2

j*i,i+I

j*i,i+ I

\

n

+ ***+kyi_

n

CO
-4

+ kp rI u > O

j=i+2

j=i+2

It follows that eXri - eXTi+ I = o andTi = ri+ 1, 1 c i n *
1. This then completes the proof.
Theorem I shows that the optimal intercheckpoint intervals
in the basic model are equidistant except the first one.
However, this result will only be true under the assumptions of .
perfect coverage of on-line detection mechanisms and Poisson
fault occurrence process. (Theorem 1 "cannot" apply to the
more general case to be discussed in the next section.)
To minimize W with respect to n, we express Ti, 0 c i <
n, as functions of n
CR1
+

Tro

0.

rT*+ 1.i.n

{

(4.18)

-.oo

u=~~~og0~

+Xr)

1+Xs);

1

T-b
.*=

From (4.12) and (4.18), we have, for l

n+lI
<

Fig. 3.

+ tc.

5

a

where

b X log,

3.0

4.00

Wversus n when

12.01
8.00 of checkpoints
Number

16.00

20.00

=0.2, s

0.5, p

O0.8, T

X=0.01, r

=

00.

i < n,

yo = eXbeXr* -1

yi=y=eXT* -1
ui=u=qeX* + p.

(4.19)

Hence, (4. 1 1) becomes
n

W=hyo fJu+ky
j=l

=hyo0+ky

n

J

j=2

n-I

E

I

u+*+kyu+ky

u

(4.20) u

m=0

If q = 0, u

=

p

=

1, while u > 1 if q > 0. Therefore,

hyo + nky if q=O
I
ql±hyo)
-kq-I cmif
if q>O.
Untun(kq-+ykqkq

co

~~~~~~~~~~~~~~~~~~~~

x_
(4-21) |i

Note that u, y, and yo are all functions of n, an integer to be
determined. Although it is not possible to derive an explicit
expression for the optimal n, the optimal value can be obtained
by solving the equation d W/dn = 0, and comparing the value
of W to the two nearest integers of this solution.
Some numerical examples are shown in Figs. 3-5, where W
is plotted as a function of n using (4.21). The unit of timerelated variables is hour or per hour. Fig. 3 compares the
curves for different values of the checkpointing time tc. It is
observed that a smaller tc usually requires more checkpoints to
attain the minimum W. The shape of the curve changes
dramatically when tc. is changed. The curve for (c = 0.5

=
8
0 o

4.00

8.00

12.00

Number of checkpolnts
Fig. 4. W versus n when r = 0.1, s = 0.3, t.

16.00

i.001
20.00

1.5, p = 0.9, T = 100.

almost flattens out when n > 5, while the curve for tc = 2.0
rises sharply beyond the minimum point. This shows that if it
takes more time for checkpointing, the mean execution time
becomes more sensitive to the number of checkpoints.
In Fig. 4, X is changed from 0.001 to 0.2 which translates
into changing from one failure every 10 tasks to 20 failures per

1334

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 11, NOVEMBER 1987

p

In (5.1), the first term represents the case when failures occur
in To but detected neither by the on-line detection mechanism
nor by the acceptance test; the second term represents the case
when a failure occurs and is detected within r0, triggering a
restart recovery. The process of task execution renews after
each restart recovery. Define the combinedfailure coverage
D d + (I - d)c. Then, (5.1) becomes

06

,,,+ P=o.7
S
/

d)(l c)
El-Fo(I1l-FoD
-F0D
-

;e.\

/

/
P

X3 X
g-~

w
CD

-

0.8

~~~~~~~~~
52

The derivation of Ej for j > 1 is similar to but more
,gscomplicated than that of El. Let Gj be the conditional
f
,/
,>
of an unreliable result at the end of rj, given that
,>s.probability
the system is free of failure at the beginning of rj. Following
sf
,
y
.ef
\i~ z X pX .f
arguments similar to those for El, we can get

v

/

Gj = Fj(1 - d)(1 - c) +FjD(pGj+ qEj+1)
where the process of task execution renews after a rollback or
restart recovery. Simplifying the above expression yields
to

-o.oo

Fig. 5.

4.00

8.00
12.00
Number
of checkpolnts

Wversus n when X

=

0.01, r

=

16.00

Gj+ ,,Fj(11 -pFjD
d )(I

20.00

0.1, s = 0.3, t, = 2.0, T = 100.

task (when T = 100 is assumed for all tasks). When the failure
rate is very low (such as X = 0.001) the overhead of
checkpointing will offset the saved execution time. But if
failures occur more often (e.g., X = 0.2), having one or more
checkpoints can greatly reduce the execution time. In Fig. 5,
the curves for different p are shown. It is clear that a lower
probability of rollback requires fewer checkpoints.

c)

V. EXTENDED MODEL
The extended model takes into consideration more realistic
coverages of both the on-line detection mechanism and the
acceptance test. The constraint E . Epec in problem P now
plays an important role, since, unlike in the basic model, E *
0. Again, closed-form expressions for the optimal intercheckpoint intervals and the number of checkpoints cannot be
obtained because of their inherent complexity. However, a
computation algorithm will be derived to determine approximate optimal solutions.
Let F1, 0 < i < n, denote the probability of an unreliable
result when the task has progressed to the beginning of the
interval r1. Let FE-En ±1 and F1 F1(77), 0 ' j '~n. Then,
it is obvious that Fo = 0 and

F1 =F0(l-d)(l-c)±+F0[d± (1-d)c]E1 .

(5.1)

qFj D
Eq+
-pFjD

(5.3)

Ej+1 can be calculated as the probability of sum of two
events: 1) no latent failure exists at the beginning of ij and 2)
some latent failures exist at the beginning of -rj. Thus,
Ej1+ = (1 -Ej)Gj+Ej[(1 -Fj) +Fj(l - d)](1 - c)
+ Ej(I

-Fj)cEj l +EjFjDEj+ l.

Replace Gj with (5.3) to get

EjI Fj(I -d)(I -c)+(I -Fj)(1 -c)Ej-(1-c)(l -Fjd)pFjDEj
1 - FjD - c(1 - Fj)Ej - (I - c)(1 - Fj§d)pFjDEj
Generally, more checkpoints are required to achieve minimum W when the failure rate X is high, the checkpointing time
t, is small, and the rollback probability p is high.

1

(54)

where 0 c j c n.
We now derive an expression for the mean execution time
W. A task has to be restarted with probability one whenever
failures are detected in the interval T0, rollback is the same as
restart in this case. For subsequent intervals rj, j > 0, the task
may be free of failures with probability 1 - Ej or may have
latent failures with probability Ej at the beginning of 7r. For
the former case, the last checkpoint must be correct at the time
of its establishment, so the task can roll back or restart with
probability p and q, respectively.
For the latter case, the last checkpoint may be correct or
incorrect depending on whether or not the latent faults have
induced error(s). In such a case, we assume that the last
checkpoint is incorrect. (This is to err on the safe side.) If the
system does detect a latent error9 within T1 either by the on-line
detection mechanism or the acceptance test, it will roll back to
the last checkpoint, resume execution, and then detect the
9Of course, at this point the system does not know whether it is latent or

not.

1335

SHIN el al.: CHECKPOINTING OF REAL-TIME TASKS

same error again. The system may roll back several more
times to see if the same occurs. Then, the last checkpoint can
be declared to be incorrect and a restart recovery follows.
Determining the number of rollbacks before restart is an issue
of its own. As mentioned earlier, the time spent on unsuccessful rollback(s) is included in the restart setup time s. The
expression for the mean execution time W can be expressed as
follows:

Substituting (5.9) into (5.5) and (5.6) gives

w

-_

d

- - qj
1pj

+

pJ+
+j

For 1 < j c n,

Wi± I =W + V

Fj)(1 - c) + Fj (I - d )(I - c)] f;j.

I (To

E

variables. This can, of course, be easily generalized to the
dependency between (n + 1) variables by a simple induction.
Definition: Define an operator V,,, 0 c i < j < n, on
En +l and Wn + 1 as follows:
V( ) i

X (T0,

60O

,rT+ 6,

7,T- 6,

Tn (TO,,
X,
TO

TO)

whereXisE,+I or W +.
Then, Vij(En+1) can be calculated by

After simplification, we get
inO+

Wj. (5.1 1)

we will restrict ourselves to the dependency between any two

Vj=(1 -Ej)[(I -Fj)+Fj(l -d)(1 -c)]j
+ (1 - Ej)Fjd [p (r +Zj + Vj) + q(s + Zj + Wj+ 1)]
+ (I - Ej)Fj (I - d)c [p (r + -rj + Vj) + q (s + -rj + Wj+ l)]
+EjFjdd(s+ Z+ Wj+>)
+ Ej [Fj(l -d) +(I- Fj)] c(s + -j + Wj+ 1)

Vi(

-p
-p1

++

For a fixed n, it is clear from (5.4) and (5.11) that both E +
and Wn+ 1 are functions of the (n + 1) variables, ri, 0 ' i c
n. However, because of the constraint (4.13), one of the
variables is dependent on the other variables. The choice of the
dependent variable is arbitrary. In the discussion that follows,

=(1 -Fod)-ro±Fodzo+FoD(s± W1).

-

qq

l~~-pj-qj I-pj-q

+Fo(1 -d)c(s+ ro+ W) +Fo(1 -d)(1 - c)ro

(5.10)

FJd 1
1 - p1 - qj )

Wl=(l-Fo)To+Fod(s+zO+Wl)

+ Ej [(I

FoD
Fod 1
1 - FoD X 1 - FoD

1-d
1 - FoD

7i

6s

lmEn + I (70TO'*

,Tj+6),

Tn) -En + 1 (To0,

Tn)-En + 1(T
7

X'',

7j-i6, '',Tn)

7 n)

i-

0+

aE1+ I aEi+ 1
aTi
arj
WI

1 -Fod
Fod
T +1

Z +

(5.12)

FoD

S

(5.5)

Fj1d

1 -1F7d

ZI
WJ+ I=1 p q + ,
I-pJ-qj I-pj-qj

J

j

+

pjr+q1s

+

1 -Pi

(
Wj (5.6)

where pj and qj are defined as

(5.7)

pj=p(l Ei)DI%

q1=q(1 -E)DF + E,[c +(1l-c)Fjd ].

~X=T _

(C2) d(1 +p) > 1.
Proof: From the recursive formula of (5.4), En +1 can be
viewed as a function of Tn and Es and En as a function of Rn- I

(5.8) and En 1, and so on. Applying the chain rule in (5 .12), we get

From (4.3), z; can be expressed as

X1-re

Some important properties about E 1+I and Wn+I will be
stated and proved below in Theorems 2 and 3. Based on these
theorems, a numerical algorithm will be derived to obtain an
approximate solution to the problem P.
Theorem 2: For any pair of integers i and j, 0 c i < j c n,
the inequality Vij(E,+ 1) > 0 holds if the following conditions
are true.

j(+X)=**X
r.

(5.9)

aEn+ 1 dEn

L

_____

1±1 aE

1j

(5.13)

1336

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. I1, NOVEMBER 1987

For 0 c k c n, the following expressions can be derived.

Since F,, Fj < 1 by C1, the value of the last factor in (5.17) is
approximately 1, and hence (5.17) is dominated by the other

OFk+l
aEk

aEk+1

aTk

(1 -c)(1 -Fk)(l -Fkd)(1 -pFkD)
[1-FkD-c(l-Fk)Ek-(l-c)(1-Fkd)pFkDEk]2

remaining factors. It is easy to show that

[(1

-

pFkD)(1

-

Fkd)(I - Fk)]/(1 - FkD)2] < 1 for allO c k c n + 1 if
(5.14) d(1 + p) > 1. Hence, the fact (1 - c)ij- < I makes the

X(1 -Fk)(l - c)(1 - d) [1 -k(l - cFj)]
[1 -FkD- c(1 -Fk)Ek -(1 - c)(1 -Fkd)pFkDEk]2
pDFk(1 - 2Fkd)(1 - Fk)(1 - Ek)
[1 -FkD-c(l -Fk)Ek- ( - c)(l -Fkd)pFkDEk]2

(5.15)
(5.15

Note that the fact aFk/l'Tk = Xe-Xk - X(I - Fk) has been
used in deriving (5.14) and (5.15). In (5.14), aFk+±/Ek > 0
for all k, 0 c k c n, since c, d, D, p, and Fk are all less than
1. Thus, from (5.13) the theorem will have been proved if

OEj+F I-+ Ej,a
I E1+2 aEi+I
* * ___ __ >0.
OEi + Tri
Orj aOEj

(5.16)

I

Condition C 1 is used to approximate the denominators of both
(5.14) and (5.15) as (1 - FkD)2. Further approximation on
the numerator of (5.15) then leads to
OEk+ I X(l - Fk)(l - c)(l - d)
OTk
(1 -FkD)2
This results in

aEj+

aEi+ 2 aEi+ I

aEj+ 1

Or

OE,1 Ol

OF

X(1 -Fj)(1 -c)(1 -d) X(1 -Fi)(1 -c)(1 -d)
(1 - F1D)2
(1 - FjD)2
1

decreased by stretching earlier intervals against later intervals.
When two adjacent intervals are considered, stretching the
earlier interval means delaying the establishment of the
checkpoint in between. That is, En+± can be reduced by
moving any checkpoints to the right on the time axis in Fig. 1.
Theorem 2 also verifies the fact that if all the checkpoints are
inserted near the end of the task, the execution result will
become very reliable, since the task has to pass all the
acceptance tests near the end of the task.
, aTheorem 3: For any pair of integers i and j, 0 < i <
nq, V2jWn)
V+j(Vij(Wn +1)) > 0 if Cl and the following
condition hold: (C3) Dp > 0.5.
Proof: First simplify the recursive formula of (5.11) by
using Cl to get

(1 -c)(l -Fk)(l -Fkd)(I -pFkD)W

(1-FkD~~)2

{
171

X (1

U
whole product less than 1.
Usually the values of Ek must be very small for any realtime system. This is the rationale behind the conditions C1 and
C2 of Theorem 2. To produce a low probability of having an
unreliable result, the system requires a reasonably high fault
coverage and a high probability of rollback when failures
occur (i.e., d(l + p) > 1), and a low failure occurrence rate
(thus, Fk < 1). If the system cannot meet Cl and C2, the
probability of an unreliable result will be high. In that case,
cherkpointing is not a useful technique at all to improve the
system's reliability, and it would be better to employ other
schemes such as triplicated voting.
Theorem 2 states that if the length of an interval rj is
increased at the expense of decreasing a preceding interval ri, i
< j, En+ I will always increase. Consequently, En+ can be

-Fj)(l -.c)(l - d)
{
(1- FjD)2
( - c)(I - Fk)(l

I

(1 -Fjd)(l

-pFjD)
(1 Fid)(I pFiD)
-

+

-

Fkd)(I -pFkD)

prFD+qsFD
I FjD

= ( - d)

(Il-FkD)23.1-&

k=i

___

(ldr

F

1 -pFD
+

FjD Wi

+ (dX- I +prD + qsD + qDWj)

1-FjD

The inequality (5.16) holds if

(1 -1Fjd)(l -pFjD)
(1 -Fid)(1 -pFiD)

j-jj (I c)(1 -Fk)(I -Fkd)(I -pFkD)

(5.18)

In (5.18), Wj+ I can be viewed as a function of Tj and
hence for all j,

W,j, and

-

k

(1 -FkD)2

OW1 1-pF,D
OW 1 l-I

the left-hand side of which can also be expressed as

i -1rJ (1 -pFkD)(l -Fkd)(l -Fk)3
(1 - pFjD)(1 -F,d )(l - F1)

OTJ1
(5)

-FiD)(5.17)

(1

where

A1 -j) BI-F1 )+ C(1 )rDe

1=+±XprD+ )qsD+ )qDJ'j

(5.19

(5.20)

(5.21)

SHIN el al.: CHECKPOINTING OF REAL-TIME TASKS

1337

Bj= d+ (1 -d)D+ XprD + XqsD+ XqDWj (5.22) brackets of (5.27), we get
1-Fj a
C=(I -d)DX.
(5.23) 1 -pFjD 2W,±
'
+
-2XqD
a -2
(I -FjD)2 aT,
I1-FjD
Now W, + I is a function of fn and Wn, W, is a function of Tr -1
JD 1
and W, 1, and so on. Using the chain rule and (5.12) as in the
proof of Theorem 2, we get
a wn+l

aWn

aWn aWn-I
w
rawj+1 dW+l

[H

W+l

1-FjD

afr

{(1 -pFjD)(I -FjD)(I -Fj)[2DAj- (1 +FjD)Bi

a Wj+2
a wj1I

a-wj

Fawj

(5.29)

(1 - FiD) 3(1 - FjD)2

Wi+2 a

Wi+

+(2D-I-FjD)TjC]-2qD(I-FjD)(1 -Fj)
[Ai-FfBj+(1-F,)TjC]}

1

aWi+ I a1i
H2

i

ari

(1 FD) 3(I FjD)2 [(1 -pFD)(1 -FD)

(5.24)

-

-

(2D-1-F,D)

-2qD(1 - F,D)(1 - Fj)]

where
H=I!1

(i FkD

)

(5.25)

(I -FjD)3(1 -FD)

2

if 2D- 1 -2qD>O.
Notethat2D - 1 - 2qD > 0and 2D - 1 - FjD > 0, if C3
holds. Hence, from (5.27), (5.28), and (5.29), Vij(Vij(Wn+0))
U
>0.
are functions independent of r, and rj. It is easy to see that HI
Again, to make any checkpointing meaningful, the system
> 0 and H2 > 0, since p, Fk, and D are all less than 1.
must have D > 0.7 and p > 0.7, thereby satisfying the
be
derived
from
that
It can
(5.24)
condition C3. 10
By applying Theorem 3 to every pair of intervals, it is
a j
a2(W
l)Hldwi+t_± H >\.qD 1-F)j
possible (although very time consuming) to find the global
H2
XqD
=HI2HI
n+I
minimum of Wn+ 1. However, even if the minimum of W,+I is
aTj2
(1 -FjD)2
aT,
found, the interval combination which yields this mninimum
-pFjD
H1
may not lead to the probability of an unreliable result
H2 +WI+
1 -FjD
aT 2
satisfying the constraint En+ 1 < Espec. Therefore, the following algorithm is proposed to solve problem P.
rI1 -pFjD a2Wi+l
a2Wj+I
Algorithm A:
HIf a2 + HI,H2 L 1-FjD aTi
aT~~~
L 1-F)D
~~~~~~Al. Set n: 1.

H2=fJ (1-PFkD
k
k=i+l 1

(5.26)

2XqD(1 Fj) a Wi+
(1-FjD)2 a j

(5.27)

-

Note that Aj > Bj [both defined in (5.21) and (5.22)] for 0 c j
< n. So from (5.20), we get

a2Wj±

J7-

X(l-F1)

3 [2DAj -(I +
(I
;irj
(1-FjD)
+ (2D- 1 -FjD)r,C]

>

FjD)Bj..7KEnI

rKI

X(l-Fj)J3[2DAJ -(1 + FjD)Aj

A4. Find W*+ ((T', * *

+ (2D- 1-F1D)T1C]

Tn)
If W*+1 <

X(l-F1)
-F

)3 (A1+ C)(2D- 1 -FjD) >0

(1 -FjD)
if 2D- 1-F1D>0.

Wo(T + tc)
else set Wrnin to a large value.
A2. Construct a finite sequence of interval vectors (Tk, * *,
Ik), k = 1, * , K by a systematic way such that
, iTt1) for 1 c k
I.k) > En+1(To,
En+ I (ro
c K. Such a sequence is called a search path.
A3. Find K such that
*I(,T0
STrKI)
*,r)Espec>E+(oS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~En+
If no such K can be found then set n = n + I and go
to A2.

(1 -F1D)3

=(1

If EI(T + tc) c Espec then set Wmin

(5.28)

Plugging (5.28) into (5.27) and examining the terms inside the

~~~~~~~A5.

)

minK k

W.+

(Tr,k

Wmjn then set Wmin: =W*+1, n : =

+

1, and go to A2 else stop.

~~~~~~~~Algorithm A starts by calculating £ and W for the case of
no-checkpointing, i.e., E1(T + tc) and W0(T + ta). If F c
" Otherwise, checkpointing should not be used as mentioned earlier.

1338

Espec, no-checkpointing is a legitimate candidate, so Wmin is set
to Wo(T + tc). If E > Espec, no-checkpointing cannot be a

solution of problem P, and W,in is set to an arbitrary large
value to indicate that no solution has been found yet. Steps
A2-A4 will find a solution of problem P if it exists for each
given n, the number of checkpoints. If no solution exists for a
given n, the algorithm repeats those steps after incrementing
n. Since the task execution result becomes increasingly
trustable as the number of checkpoints increases, for any Espec
there is always an integer m such that En + 1 < Espec for all n >
m. This guarantees the existence of K in step A2 for some n.
The algorithm terminates in step A5 when there is an n such
that W*+ > Wmin. This terminating condition must hold
eventually, since the checkpointing overhead increases linearly with n while the saving of execution time from rollback
recoVery is limited.
The s'earch path needed in step A2 can be determined by
Theorem 2. We can use any search path that has the effect of
moving one or more checkpoints to the right in the time axis
while increasing k, since Theorem 2 has shown that En, I on
such a path is decreasing. There are many ways of construct-,
ing such a search path. In practice, the choice of a search path
depends heavily on the convenience of checkpoint implementation and/or the physical limitation in a particular system.
Two simple approaches of constructing search paths are
conceivable: the common ration approach and the common
difference approach. The common ratio approach considers
intervals with the relation rjT 1 = pTj, 0 c j < n, where p is a
constant ratio between two adjacent intervals. The search path
is obtained by decreasing p in discrete steps within some given
range, e.g., 0.8 . p ' 1.2. Similarly, the common difference
approach considers intervals with the relation rj+ I = rj - 6, 0
< j < n, where 6 is the common difference. The search path
is obtained by decreasing 6 in discrete steps within some given
range, e.g., -4 c 6 c 4. The range of the common ratio or
the common difference is determined primarily by the physical
limitation of the system.
.
. is.
In step A4, W*n+1 is obtained once a local minimum
found, since Theorem 3 has shown that W, +I is concave with
respect to the operator Vij. If no local minimum is found, the
minimum would occur either at k - K or k = K.
Some examples are shown in Figs. 6-8 using the common
ratio approach. The solid line represents the curve for W,+1
and the dashed line represents the curve for E,+ . It is
observed that if the failure coverages are high (Fig. 6), the
minimum W occurs around p = 1.0, while if the failure
coverages are low (Fig. 7), the minimum W occurs at p >
1.0. This result is expected since for high failure coverages,
the extended model will be close to the basic model where the
optimal intercheckpoint interval is equidistant, and for low
failure coverages, the probability of restart is high so that more
frequent checkpointing at the beginning of the task and less
frequent checkpointing near the end of the task are required to
reduce the time wasted in restart recovery. The following
procedure is derived immediately from Algorithm A to get the
optimal solution to P using the common ratio approach once
the graphs similar tothose in Figs. 6-8 have been obtained.

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. l1, NOVEMBER 1987

'
Tc

CM\

g\+

CD

X

- i
\

.\-

t \
g T
e
\ \

-

,

C0 -

-

,

.- -A
., -

/
\

-/ \

D

c

CY)

0.0
0.80

0.90

Common

1.00

Ratio

P

1

1.20

1.10

1.3

13F

Fig. 6. Diagram of Wand E versus common ratio when c = 0.8, d = 0.9,
p

=

0.8, X = 0.01, r = 0.4, s = 0.7, t,

=

1.5, T = 100.

Draw a horizontal line En+l = Espec on the graph.
The intersecting points of this line with the dashed lines
maximum aallowable
give
b cscommon ratios for different themaximu
ent values of n.
S3. For each n, find the minimum W,,+I in the region of the
~~allowable common 'ratios.
S4. Choose the value of n which yields the minimum

S1.
S2.

W

l-

Two examples below illustrate the use of this procedure.
Example 1 uses Fig. 6, while using Fig. 7 for Example 2.
Example 1: Let T = 100 h, X = 0.01 per hour, c = 0.8, d
0.9,r = 0.4h,s= 0.7h,p = 0.8,q = 0.2, andtc = 1.5
h. Three cases are considered, i.e., Espec = 0.002, 0.003,
0.005 for 1 c n c 11. The results are given in Tables I-III.
From Table I, the minimum W with Espec = 0.002 occurs
when n = 7 and p = 0.83. Similarly, the smallest W with
Espec = 0.003 occurs when n = 6 and p = 0.89, and the
smallest W with E£pec = 0.005 occurs when n = 5 and p =
1.02. We can see that a lower Espec will produce a solution
with more checkpoints and a smaller ratio.
Example 2: Let T = 100 h, X =0.01 per hour, c = 0.6, d
= 0.7, r =0.4 h, s = 0.7 h, p = 0.8, q = 0.2, and tc = 1.5
h. Two possible values of FSe are considered, i.e., 0.02, 0.04
for 1 c n . 11. The results are given in Tables IV and V.
The smallest Wwith £Se - 0.02 occurs when n = 7 and p

1339

SHIN et al.: CHECKPOINTING OF REAL-TIME TASKS

Co

C3

~

~

~

~
~

~

~

~

~

~

~

~~~~~~~I

70

0

C3

~

~

0
47

7

c

CO.--

>(

7

7013
7

o

o

C8

3

Co~~~~~~~~~~~~4

&

0

77

7

0

7
7~omo
Li)
Rati
0aklng a 7ask-orlented 7lew, we have developed the baslc

-

1.0~~~~~

~~

.,d=07
~~~~~~~~~~~

7

0

~

~

7

7.8

L~~~~~~~~~~~~~~~~~~~~~~~C
TC

7

oo

CD~~~~~
=

~ 6
~~~~~~~~~~~~~~~~

= 5an p
= Ratio
when ~~n
with
Espec
0.04 occursoptimal
= 0.89, but that
ersnt
with
moeaecvrg.Ti
Cleetuoioccurronce.
checkpointing~~~~~~~~~~~~~~~I
produceam
willcheckpointagain
that a lower Espec
= 1.08. One tim
can see

3

chckpointingtime
practcal
case tan the oneofndlong
more
-7
asmaler
atio Ths
ithmorechekpoits
soluion
7l
eapsismdtolenth80.7
in
th-ai,t/,
analysis
implies that a more reiable system requiresmore checkpoints
1.00 0.30
0.70 0.80

1.10

1.20

end of the task.

138.2B7cm
0.77
0. 1
17..81

--40794
0.4

0.01

0.004-4
0.0037

4.114 0018
loo09
1.0l0.
142.07634 0.00198
ll o.s TALEn
EXAMLE I WITH tApe.
OPTIUM SOLUTIONS FOR 73.599

0.1~~~~~~~~~~~~~~~-'~~~~~7 08 3.67z1 0.01

1.30

9

08
3~~~~~~~~~~~~~~~~~~~~~~~~~07 0.42

. ...,,.rv ..20.26~ ~ ~ ~ ~ ~.-4~D l-0629002

0.010 1306

14.0805

0.012
0.001047
0023
0016
B9575

I

rangeo
1usperfrmchenttitigorrpeent

hr
the siutono
euetytoad
checkpointing time with,N moeaecvrg.Ti2ersnsa1
onoflg
morepatiatae
1F
checkpointing
1.20, t-i
any eae
falr
1.10nsmtimeotecvrg
0.90in n 1.00tio
covrag0 o 0.80tha the

esscoverage.wl
withetnc high

eur

ogrt

0.00o

11

0

Fing.7 Diratgra ofdeWands vesutinscomownrtiowhen cqiitn fo0.6 d.C

0.05

06

3.922

0.8
14.588
.0_18870

14.3150.7omon

00104

I

0.001708
0.001081
.013
.

0012

ato

n 0a77 minimum39W
of checkpointing ofreal-time tasks. The performance criterion ~~~~~~~2 0.26 1380.68232
3
0.55
143.658065
0.710 138.4670176
usned in tIs pperceis the realr(mean) texectuationtim Wsforta
tasckpsubjetint tthe speifimoedroabilityvofane unisreliberesultsas 0.82 136.5171351
0.80 136.213365
6fln hcpinigtm
£maoteprcomtion
cs ofa thetask

0.24
80.075

.9 1

130n60127

140.5717164
142.087324

0.03

revalatinbtweenthempeformance
Andhoextendedmodesto designand

Tfhecbasicnodtingofra-ies onksTheasuptiformnof ariperfect2

0.002

XMPE1WIHE.
OPTMU SOUTON
04 017
0.11 FO 175.0831

5 0.69m13n.40828
t=.
oFi.7acepancem tets highEerucoverag waill reuien longer6
d
:0.1
0.77 s138.2673n
san reltio btween.5Them0.
Alhog we.,did0ot4ssum
foraourE
Fi.8
iarmo
3.677
ma)eeuintmeWfra407
pae0stera
7n
thi
~
~but that ~
~
~
~
~
~
~~~~~
0.04 occu7r0.83si38en0742= 0.00107
with Espec
0.89,
18
8
wlieinodthe8a.7
analysis the ratio se/Tagin tall eamploesri madet
=

7002

7IT ES

-OREXML 1
7OUIN
~
~ ~~~~~~~~~~~~PIU

7~~~

=

.,T= 10

Ln

hem for ur
Although e did notassume ay relatio between
L~~~~~~~~~~~~~~~~~~~.~~~~~~~

ZD

al

7
.,s=07

0.01, r

-,

--

- --

7
05

J

-

ocur whe n= ad
= 0.89 7u tha wit E-e = 7.0
.an se giLht ioe)Se wl rdc
=1-08 7n
7

,7

BD C
_

C"J~~~~~~~~~~~~~~~~~~~~~~-77C LU

~~~~~7

of Wadvru-omnrtowe
Fig.
~
7. Diara
~

used

C

160.605475
1437.606622

E14
0.002924
0.002906

0.002903

0.002085
0.002081

0.002024
0.002083

.001

0
2

08,d

09
0.099

1340

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO.

TABLE III

OPTIMUM SOLUTIONS FOR EXAMPLE 1 WITH

n

ratio

2

3

4
5
6
7
8

minimum W

0.004020

0.93

1.02
1.01

135.336271

135.009280
105.540305

0.004929

136.429802

137.562550

1.01
1.01

138.866307

140.295496

141.831054

1.00

3

0.004989

0.004230
0.003747

0.003376

q
W

0.003080

0.002840
0.002524

= 0.02.

E

4

163.700223
151.669977

0.019825
0.019767

7

0.830000
0.890000

145.891900
145.592264

0.019480
0.019553

10
11

0.960000
0.990000
1.010000

8
9

0.750000

147.325267

148.122613

0.930000

147.085528

148.241936
149.694418

.1

0.019755

0.019403
0.019294

0.019731
0.019847

Vi
Zi

Computation time between a failure occurrence and the

n

ratio

ininimum W

0.810000

1545.760283

2

0 .470000

4
5

0.990000

3
6

7
88
0
10
11

1.080000
1.130000

1.120000
1.110000
1.10000
1.100000
1.000000
1.090000

E

l

1 57.7 830o88

0.039g808].

141.816575

0.039909

143.086989

0.038819
0.035350

141.383623
141.990891

144.41490
144.414490
145.00904B
147.53002-14

149.236187

0.039520

checkpoint.
ithstribution
unction of Zi

Ej

Failure occurrence rate.
Probability of an unreliable result just before the jth

d

Coverage of the on-line detection mechanism.

c

OPTIMUM SOLUTIONS FOR EXAMPLE 2 WITH Ep = 0.04.

checkpoints.

X

D
TABLE V

.
faliure

Probability of restart recovery upon detecting a failure.
Mean execution time for a task.
Mean execution time after establishing the ith checkpoint.
Mean execution time between ith and (i + 1)th

Wi

0.400000
0.620000

5
6

Total number of checkpoints for a task.
Sum of Is and tc, except for i = n.
Time for setting up a rollback recovery.
Time for setting up a restart recovery.
Probability of rollback recovery upon detecting a

ri

TABLE IV

ratio [minimum W

Time for establishing a checkpoint (checkpointing

time).

r
S
p

OPTIMUM SOLUTIONS FOR EXAMPLE 2 WITH

n

tc
n

137.3853603 0.00490,28

0.77

147.3855.74

1.01

10

E

0.49

1.01

9

Epc= 0.005.

11, NOVEMBER 1987

checkpoint.

Coverage of an acceptance test.
Combined failure coverage, i.e. D

-

d + (1 - d)c.

ACKNOWLEDGMENT
The authors would like to thank the anonymous referees for

useful comments on the first draft of this paper.

0.039456

REFERENCES

0.032350
0.032446

[1] programs
C. H. C. in
Leung
and Q.computing
H. Choo, systems,"
"On the execution
of large
batch
unreliable
IEEE Trans.
Software
Eng., vol. SE-10, pp. 444-450, July 1984.

0.020926
0.027675

0.0268173

any given number of

[2] A. Duda, "The effects of checkpointing on program execution time,"
Inform. Proc. Lett., vol. 16, pp. 221-229, June 1983.
[3] J. W. Young, "A first order approximation to the optimum checkpoint
interval," Commun. ACM, vol. 17, pp. 530-531, Sept. 1974.
[4] K. M. Chandy, J. C. Browne, C. W. Dissly, and W. R. Uhrig,
"Analytic models for rollback and recovery strategies in data base

checkpoints. The extended model
includes imperfect coverages of both the on-line detection
systems," IEEE Trans. Software Eng., vol. SE-1, pp. 100-110, Mar.
mechanism and the acceptance test. We have shown that under
1975.
E. Gelenbe, "On the optimum checkpoint interval," J. Ass. Comput.
imperfect coverages, E can always be reduced by moving [5] Mach.,
vol. 26, pp. 259-270, Apr. 1979.
checkpoints to the right on the time axis. Using this property, [6] E. Gelenbe and D. Derochette, "Performance of rollback recovery
an algorithm isalgorithm
derived which, assuming that the checkpoint
systems under intermittent failures," Commun. ACM, vol. 21, pp.
June 1978.
intervals must maintain derivedwhich,assumingthathec493-499,
a common ratio, determines the
[7] F. Baccelli, "Analysis of a service facility with periodic checkpointminimum Wsubject to E c Espec. The significant finding from
ing," Acta Informatica, vol. 15, pp. 67-81, Jan. 1981.
the algorithm is in that if a task requires a high probability of [8] A. N. Tantawi and M. Ruschitzka, "Performance analysis of checkpointing strategies," ACM Trans. Comput. Syst., vol. 2, pp. 123correct execution results, we must do checkpointing more
144, May 1984.
frequently towards the end of the task. This result is a [9] T. B. Smith and J. H. Lala, "Development and evaluation of a faulttolerant multiprocessor (FTMP) computer Vol. 1: FTMP principles of
departure from the conventional assumption that checkpoints
NASA Contr. Rep. 166071, May 1983.
operation,"
shoud
istibutd
he
ask.Thebasic
beeqully
thougout
basic
should be equally distributed throughout the task. The
A. L. Hopkins et al., "FTMP-A highly reliable fault tolerant
[o0]
reason of using unequal checkpoint intervals is that the
multiprocessor for aircraft," Proc. IEEE, vol. 66, pp. 1221-1239,
requirements on program's correctness outweigh those on
Oct. 1978.
execution
~~~~~~~~[1
1] J. H. Wensley
et al.,control,"
"SIFT: Design
and analysis
a fault-tolerant
program's excto
pe.computer
for aircraft
Proc. IEEE,
vol. 66,ofpp.
1240-1255,
an

is

speed.

LIST OF VARIABLES

T Totalfaut-free comptation timefor a
T1

Ii

Oct. 1978.
[12] B. Randell, "System structure for software fault tolerance," IEEE

Software Eng., vol. SE-1, pp. 220-232, June 1975.
task.Trafns.
[13] Y.-H.

Computation time at which the ith checkpoint is

Lee and K. G. Shin, "Design and evaluation of a fault-tolerant
multiprocessor using hardware recovery blocks," IEEE Trans. Corn-

placed.

put., vol. C-33, pp. 113-124, Feb.
[14] H. Hecht and M. Hecht, "Use of fault1984.
trees for the design of recovery

checkpoints.

[15] K. G. Shin and Y.-H. Lee, "Error detection process-Model, design,

Computation time between the ith and (i + I)th

blocks,' in Dig. Papers, FTCS-12, 1982, pp. 134-139.

1341

SHIN et al.: CHECKPOINTING OF REAL-TIME TASKS

116]

and its impact on computer performance," IEEE Trans. Comput..
vol. C-33. pp. 529-540, June 1984.
A. Avizienis, "The four-universe information system model for the
study of fault-tolerance," in Dig. Papers, FTCS-12, 1982, pp. 6-13.

Kang G. Shin (S'75-M'78-SM'83) received the
B.S. degree in electronics engineering from Seoul
National University, Seoul, Korea in 1970, and the
M.S. and Ph.D. degrees in electrical engineering
from Cornell University, Ithaca, NY, in 1976 and
1978, respectively.
He is a Professor in the Department of Electrical
Engineering and Computer Science, The University
of Michigan, Ann Arbor, which he joined in 1982.
He has been very active and authored/coauthored
over 120 technical papers in the areas of fault-

tolerant real-time computing, computer architecture, and robotics and
automation. In 1986, he founded the Real-Time Computing Laboratory,
where he and his students are currently building a 19-node hexagonal mesh
multiprocessor to validate various architectures and analytic results in the area
of distributed real-time computing. From 1970 to 1972 he served in the
Korean Army as an ROTC officer and from 1972 to 1974 he was on the
Research Staff of the Korea Institute of Science and Technology, Seoul,
Korea, working on the design of VHF/UHF communication systems. From
1978 to 1982 he was an Assistant Professor at Rensselaer Polytechnic
Institute, Troy, NY. He was also a Visiting Scientist at the U.S. Airforce
Flight Dynamics Laboratory in Summer 1979 and at Bell Laboratories,
Holmdel, NJ, in Summer 1980. He was a Guest Editor of the 1987 August
special issue of IEEE TRANSACTIONS ON COMPUTERS on Real-Time
Systems.
Dr. Shin was the Program Chairman of the 1986 IEEE Real-Time Systems
Symposium (RTSS), the General Chairman of the 1987 RTSS. He is a
member ACM, Sigma Xi, and Phi Kappa Phi.

Tein-Hsiang Lin (S'83) received the B.S. degree
from the National Taiwan University. Taipei, Taiwan, in 1980, and the M.S. degree from the Iowa
State University, Ames, in 1984, both in electrical
engineering.

a

Currently, he is working towards the Ph.D.

X

OWN,

degree in computer, information, and control engineering at the University of Michigan, Ann Arbor.

t

His research interests include multiprocessor and
distributed system performance evaluation and
fault-tolerant computing.

Yann-hang Lee (S'81-M'84) received the B.S.
degree in engineering science and the M.S. degree
in electrical engineering from National Cheng Kung
University, in 1973 and 1978, respectively, and the
Ph.D. degree in computer, information, and control
engineering from the University of Michigan, Ann
Arbor, in 1984.
Since December 1984, he has been with IBM
Thomas J. Watson Research Center, Yorktown
Heights, NY. His current research interests include
distributed computer systems, database processing,
fault-tolerant computing, performance evaluation, and stochastic decision
theory.
f

976

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO.

8, AUGUST

1987

Progressive Transaction Recovery in Distributed
DBIDC Systems
YANN-HANG LEE, MEMBER, IEEE, PHILIP S. YU, SENIOR MEMBER, IEEE, AND BALAKRISHNA R. IYER

Abstract-The demand for on-line transaction processing has bility, and b) the potential for isolating failures and performing
grown rapidly in recent years. To meet the transaction demand, recovery actions independently on different subsystems.
several DB (database management) and DC (data communication
In a distributed DB/DC environment, it is important to

management) subsystems can be coupled together to form a
distributed DB/DC system. A key problem is to provide these

distributed systems with effective means to recover transactions
upon failure while paying little performance penalty during
normal processing. Also, there should be minimal interference of
fault-free components, during
recovery of a failed
compo. . the
. transacnent. By decentralizing recovery
management, and using
tion level structural information to eliminate costly lower level
handshaking protocols, proposed progressive transaction recovery protocols seek to solve the problem. A queueing model for
evaluating the transaction response time during normal process-

.

t

contain the effect of a failure and the disruption due to the
resultant recovery operations. First, the transaction messages
that are lost due to the failure must be identified. Transactions
may be affected because they were in execution on the failed
at the time of failure, or in communications to or
~~~~~~~~~subsystem
from the failed entity. The messages of these transactions are
needed during recovery. Hence, the "whereabouts" of every
in-flight transaction must be tracked. As a consequence, the
recovery protocol, in a distributed DB/DC environment is

ing for the progressive and pessimistic protocols is developed and more complex than the recovery protocol for a centralized DB/
solved, via simulation. The progressive recovery protocols are
shown to reduce normal processing overhead and lead to DC environment like for IBM's IMS system [13], [19]. In a
centralized environment, DB and DC services are integrated,
performance improvement over the pessimistic protocol.

executed under the same processor, and hence fail together.
Index Terms-Distributed database systems, performance The centralized logging of before and after images of updated
evaluation, transaction recovery,
database records and commit records is sufficient for recovery.
I. INTRODUCTION
In a distributed environment, the pessimistic approach can
HE demand for on-line transaction processing has grown be used for transaction recovery, i.e., to secure processing of
l rapidly in recent years. On-line transaction processing each process. The result of processing is encapsulated in a
systems have found a wide range of applications, e.g., the message and saved in a backup process or nonvolatile storage
control of ATM's, inventory control, point of sale, airline before starting the next process. For instance, transaction
reservation, etc. An on-line transaction processing system messages will be sent to the next process only after what has
must provide both small response time and high availability. been executed so far is committed. Also, execution by the next
Rapid recovery from failure is becoming an increasingly process will start only after the communication between the
important design goal. A survey of recent work in highly two processes has been committed. The distributed DB/DC
available systems may be found in [8], [17], [18], [21].
system tracks the exact status of transaction flow and can
Multiple DB (database management) and DC (data com- recover exactly those transactions lost, in the case of failure.
munication management) subsystems, usually running on Although transactions can be recovered quickly, substantial
different processors, can be coupled together to provide for the overhead is incurred during normal transaction processing and
increased transaction volume. This is referred to as a the transaction response time is adversely affected.
An optimistic approach to distributed system recovery is
distributed DB/DC configuration. A transaction is initiated by
a transaction message entered from a terminal to a DC proposed by Strom and Yemini [20]. It is based on checkpointsubsystem. The DC, which provides data communication and ing system states, logging input messages, and tracking
message formatting services, processes the message. Then it is message dependencies. The term optimistic is used since the
routed to a DB subsystem, where an application program is approach allows computations, communications, and checkinvoked to access the database. After all database activities of pointing to proceed fully asynchronously. The underlining
the transaction are completed, an output message is sent back idea of this approach is that a process may be restored to the
to the terminal via a DC subsystem. This distributed DB/DC state immediately before the failure by replaying all input
configuration provides a) flexible growth in processing capa- messages to the checkpointed states. Thus, it is necessary to
have deterministic computation at each component so that the
same results can be obtained by the replay of messages. The
Manuscript received January 13, 1987; revised March 25, 1987.cocpoftastinntaedbrtsntsuotd.FthYorktowncoepoftastinntaedbrtsntsuotd.Fth-

The authors are with IBM Thomas J. Watson Research Center,

Heights, NY 10598.
IEEE Log Number 8715066.

more, the potential cost of checkpointing system state and
logging messages to track communications and state dependen-

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS

cies cannot be overlooked. As pointed out by Strom and
[20], for transaction applications on distributed systems,
transaction-based recovery is likely to be more effective than
the optimistic approach.
Here we describe a progressive approach to transaction
recovery for distributed DB/DC systems. The approach
considers transaction processing within a subsystem as a unit
of recovery and tracks the progress of a transaction through
subsystems. The emphasis is on a) eliminating the need for
synchronous communication of messages, b) decentralizing
recovery management mechanisms. In the following section,
we describe distributed DB/DC environment and clarify how
we differentiate transaction recovery from database recovery.
In Section III, the principle of our approach for transaction
recovery is presented. In Section IV, we discuss issues arising
from the application of progressive recovery to three different
system structures: systems a) with logging devices, b) with
shared queues, and c) with backup subsystems. Modeling and
performance analyses are described in Section V. The response time improvement of the progressive approach over the
pessimistic approach is demonstrated. Concluding remarks
appear in Section VI.

977

transaction have been backed out. In addition to making the
components operational, database recovery and transaction
recovery may need to be performed.
Database recovery aims at maintaining database integrity
and deals with redundant database information in stable
storage. It is usually implemented in the form of before image
and after image records. If either a transaction aborts on its
own volition or is forced to abort because of failure of the
subsystem executing the transaction, the updates made by the
transaction on the database must be undone as long as the
transaction has not gone beyond what is called commit point
[3], [9]. If the transaction had proceeded beyond commit point
then the system ensures that the updates due to the transaction
are reflected in the database. Tracking individual transaction
status is also essential since a transaction has to be backed out
when its application process fails.
Transaction recovery recognizes that a transaction goes
through several processing stages and divides transaction
processing into several consecutive units of recovery [7] (or
units of work [12]). After transaction processing in a unit of
recovery is completed, the results will be "secured," if all
components involved commit the execution. Securing means
checkpointing into another entity, e.g., disk, to provide
redundancy for recovery. Otherwise, the transaction processing at the unit of recovery is aborted and any results generated
in that unit of recovery are discarded. During recovery the
transaction processing can be restarted provided that the
transaction message is available from the previous unit of
recovery. An important issue in transaction recovery is to
identify the transactions affected by failure. Hence, it is
essential to track whereabouts of all transactions during
normal processing.
In both database recovery and transaction recovery, it is
necessary to relate components in which processing is done
and the units of recovery of a transaction. This relationship
determines which transactions need to be recovered when a
component fails. Certainly, we can consider the processing in
a component as a unit of recovery and record the component's
states whenever a transaction message is received or delivered. Also, the transmission of transaction messages between
components can also be treated as a unit of recovery and with
sender and receiver using a synchronous communication
protocol. This approach, typical of a pessimistic strategy,
minimizes the recovery time and the number of components
involved in recovery of a single component failure. However,
performance during normal processing can be noticeably
degraded as will be shown in Section V.

II. DISTRIBUTED DB/DC ENVIRONMENT
In a distributed DB/DC environment, the multiple data
communication (DC) and database (DB) management subsystems generally reside on different processors as shown in Fig.
1. Each subsystem may consist of multiple components or
processes [23].1 The DB may include the transaction manager,
database manager, application programs, etc. The application
program typically requires to read data from the database,
performs functions based on the information read, and may
update some data in the database. The database manager does
the access of data from the database, the update to the
database, and handles database concurrency and buffer coherency control [22], etc. The transaction manager interfaces
with the DC and schedules an application program according
to the transaction message. The DC may consist of components like network manager, message formatter, etc. In the
input DC, the message formatter analyzes the screen and
generates a transaction message which is then sent to one of
the DB's. In the output DC, the formatter generates a response
screen to the user based on the message from the DB.
Although the input and output DC's are usually on the same
processor for a transaction, we present our approach in terms
of transactions with a message flow of DC to DB to DC. The
presentation can be easily generalized to conversational type
transactions which can be viewed as an iteration of the above
III. THE PRINCIPLE OF PROGRESSIVE RECOVERY
model [4], [8].
Generally speaking, most system recovery methods involve
When a system comprises of subsystems with multiple
components, any failure may cease the operations of one or either pessimistic or optimistic mechanisms, or a combination
more components. For instance, when a processor executing a of the two. The pessimistic mechanisms force every operation
DC fails, all its components are affected and have to be to be secured before starting the next operation. Thus,
restarted. If only an application program, that is executing a recovery is simply a retry of the failed operation. However,
transaction, terminates abnormally, it alone may be restarted the optimistic approach assumes that operations are going to
with a fresh copy after all database updates of the interrupted be successful and may delay the securing of intermediate
hl ytm h
'The terms process and component will be used interchangeably in the reuls Inti ae eoeymyafc
time during normal
of
response
basis
the
on
Evaluated
sequel.

978

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 8, AUGUST 1987

DC,

...

monoge1~~~
~ ~

DC

DBi
App

~~~~~maae

Network

I

process!

DB2

TX out

Fig. 1. Block diagram of distributed DB/DC systems.

processing and recovery time needed after a failure, these
mechanisms have opposite effects. There are other important
issues that also have to be considered.
1) Failure Pattern: Since a failure may halt the operations
of several components (component failures may be correlated), treating each such component as a different unit of
recovery may not be cost effective. Also, if independent
failures of components are rare, it may not be worthwhile to
incur large overhead during normal operation for a fine
granularity of recovery.
2) Recovery Interference: In a distributed environment,
ideally, the recovery of a subsystem should be confined to the
failed subsystem. Transaction recovery triggered by a failure
in a subsystem should contain propagation of recovery effects
to, at most, interfaces of that subsystem.
3) User Friendliness: All recovery-related activities should
be user-transparent. The probability that a user, who submits a
transaction request, has to reenter input should be made as
small as possible. Once user's input is acknowledged,
the result of transaction processing must be guaranteed. The
system should provide recovery facilities for transaction
initiated abort and automatic transaction backout on application program failure.
Consider the distributed DB/DC system described in the
previous section. Transaction execution can be naturally
partitioned into several stages of processing at different
subsystems. The message flow between subsystems represents
the progress of a transaction. To provide for recovery, a
simple and systematic approach, called the progressive
recovery approach, is proposed. It takes advantage of the
inherent partition in a distributed DB/DC system and saves
transaction executions progressively at subsystem interfaces,
The approach recognizes that a transaction executes at
several subsystems in its lifetime, i.e., DC-DB-DC. In some
subsystems, e.g., the DB, transaction processing changes the
database state and in others, e.g., the DC, simply transforms
the format of messages. In the latter type of subsystems, even
if it cannot be determined whether a transaction execution has

been lost due to failure, reexecuting the transactions and
catching duplications at the input of the subsequent subsystems
can be used for transaction recovery. As a consequence,
tracking transaction status within such subsystems can be
simplified. However, subsystems in which transaction processing may change global states should be well protected from
haphazard reexecution.
In the progressive recovery approach, we consider transaction processing within a subsystem as a unit of recovery. It is
natural to define DB and DC subsystems as units of recovery
because of the following reasons: a) at DB and DC's
interfaces, message routing decisions have to be made and
these decisions need to be secured, b) the DC-DB boundary
represents a boundary of processing between components for
which transaction execution is repeatable (no global state
changes) and those that are not, c) DB and DC subsystems
could be executed on different processors and their failures
made uncorrelated.
The progressive transaction recovery retains the notion of a
transaction [11], uses two-phase commit for database updates,
but does not require commit for message communications
between subsystems. Conceptually, it is based on the following three principles:
1) Store-and-forward operations during normal process-

ing.

A store-and-forward operation is performed between subsystems to save transaction messages which provide the
information not only for identifying transaction progress, but
also fortransactionreexecution. Transaction messages may be
secured through logging, checkpointing to backup processes
[2], etc. On a failure, affected transactions are recovered by
replaying the last message secured for each of these transactions.
2) Asynchronous transaction tracking during normal
processing.
The number of transactions executed between failures can
be substantial. It would be infeasible to examine every
message secured during recovery. Tracking of all transactions

979

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS

within a subsystem during normal processing is helpful for quent subsystem until the logging is completed. Let us assume
identifying transactions affected by failure. Data structures that a transaction Tx flows through DCi, DBj, and DCk. The
(illustrated in the next section) containing messages of transaction message log records, Tx.Log,, n
1, 2,
transactions executing within a subsystem and those in the will be written into the input and output log data sets of DCi,
process of transmission between subsystems may be used to DBj, and DCk, respectively. These log records can be
facilitate transaction tracking. It is necessary to examine expressed as the following.
secured messages, only of affected or in-doubt transactions. Tx.Log_ (Tx, Tx.MS0)LDS(DC'n)
Note that the tracking is not at the component level (but at thel
subsystem level) and need not be synchronous with the Tx.Log2 (DBj, Tx, Tx.MS1 )LDS(DC?ut)
transaction progress. This is the optimistic component of the
progressive approach.
3) Selectively replaying transaction messages during re- Tx.Log4 (DCk, Tx, Tx.MS2)LDS(DBout)
covery with duplicate detection.
In case of failure, transactions in the affected subsystems Tx.Log5 (DBj, Tx, Tx.MS2)-+LDS(DCk)
will be identified by the analysis of transaction tracking
information. Previously secured transaction messages of such Tx.Log6 (Tx, Tx.MS3)kLDS(DC% )
transactions are resubmitted. To avoid possible replications, where Tx.MSm is the transaction message of Tx after
duplicate checking is performed at receipt of a transaction processing by m subsystems. For example, the transaction log
message.
written into LDS(DB,n), TX.Log3, is defined as (DCi, Tx,
=

IV. APPLICATIONS OF PROGRESSIVE RECOVERY
In this section, applications of progressive recovery to three
different system structures are considered. In the first structure, reliable logging devices, e.g., disks, are used to save
transaction messages. Detailed algorithms are presented for
tracking logged messages and for recovering transaction
messages. The second system structure eliminates logging
through the use of reliable shared queues for storing and
forwarding messages. Management of stored messages and
recovery of transactions are both simplified. Finally, the
system structure with backup subsystems for each DB and DC
is considered, where checkpointing of transaction messages to
the backup subsystem is used to save transaction progress.

Tx.MS1)

The logs described above are called transaction progression logs since they not only have the transaction messages but
also represent the progression of transaction processing. For
example, the differences between the input log and output log
of a subsystem indicate the transactions currently being
processed by this subsystem. Also, we can find all transactions
which are in transmission between two subsystems by examining the sender's output log and the receiver's input log.
Transaction processing is recoverable through the logging
mechanism.
1) Recovery Protocol: When a failure occurs in a subsystem (due to component failure or processor failure), the
workload on the affected subsystem is assumed to be taken
over by a recovering subsystem running either on the original
A. Progressive Recovery with Logging Devices
processor or another, depending on the cause of failure. The
Like traditional database logging, we assume that each recovering subsystem is assumed to have access to the log data
subsystem owns a log data set for saving messages on a sets of the failed subsystem. This can be achieved by
logging device, e.g., disk. The logging device may be connecting logging disk through a dual-ported controller to
duplexed for protection from media failures. A log action is two subsystems.
deemed to be complete only after the write operation to the
If the failed component is one that changes the global state
device
is
known
to
be
The
of
the system, then the first recovery action is the restoration
logging
successful.
log data sets of
subsystem, SS, are denoted as LDS(SS n) and LDS(SSOUt) for of a consistent state. For example, after DBj fails, recovery at
input and output transaction messages, respectively. For each DBj restores committed state changes. All committed transactransaction, a unique transaction identification number, de- tions will have their messages logged into LDS(DByUt). All
noted by Tx, is generated and used during the transaction's uncommitted transactions will be backed out. If the failure is
lifetime. When a subsystem receives an input transaction in the DC subsystem, all ongoing processing is assumed to be
message, Tx.MS, a log record which is composed of Tx, the lost. Transaction messages whose processing was disrupted by
message and the name of the precedent subsystem is written the failure are reissued and processed again. Transaction
into LDS(SSh")before transaction processing is started. After messages that have been logged at the output log data set of the
the input message is processed by the subsystem, a log record failed subsystem but not received at the destinations are simply
containing the transaction's identification number, the mes- resubmitted. Transaction messages which are destined to the
sage to be sent out, and the name of destination is written into recovering subsystem may also need to be retransmitted from
LDS(SSoUL). Note that in the DB, by the end of transaction their sources.
processing, before and/or after images for database updates
We focus on recovery of a single transaction Tx going
need to be logged. A commit record ends the processing of the through DC1, DB1, and DCk. Suppose DB1 fails during the
transaction on the DB. Logging of the commit and the course of executing transaction Tx. By comparing LDS(DBpn)
transaction message into LDS(SSOUt) can be combined. The and LDS(DBy.Ut), it can be determined that Tx must be
processed transaction message cannot be sent to the subse- reexecuted. If Tx was lost in transmission to DB1, it would be

980

logged at LDS(DCOUt)
and not appear at LDS(DBiJ ). The
i
transaction message would be retransmitted. Similarly, the
transaction messages logged at LDS(DBOUt) will be compared
to the logs at LDS(DCin) and the missing messages will be
resent to account for the loss of Tx after logging. When DCi
fails, the messages logged in LDS(DC, ) but not in
LDS(DC,Ut) must be reexecuted in DCi. Those messages
destined to DBj, if logged in LDS(DC7u') but not in
LDS(DB,n), must be retransmitted to DBj. When DCk fails,
the messages destined to DCk, if logged in LDS(DBOUI) but
not in LDS(DC'k), must be retransmitted to DCk after DCk
has been recovered. Also, transactions logged in LDS(DCi)
but not in LDS(DC%1k) have to be reexecuted by DCk. When
failures occur in multiple subsystems, the actions stated above
for each recovering subsystem must be performed separately.
Reissued transaction messages may be duplicated if we do
not quiesce the recovering subsystems. To avoid this situation,
the sender should submit a list of transaction messages which
are going to be reissued to the destined subsystems. The
sender will require an acknowledgement. If the list is lost, it
will be resubmitted. Duplicate checking is performed for these
messages before the logging of transaction messages at the
receiving subsystems. All duplicated transaction messages will
be thrown away.
2) Log Management and Comparison: Log data sets
may grow indefintely with the accumulation of transaction
messages. During recovery, when we need to identify affected
transactions, the logs of completed transactions are superfluous. Reading and comparing the entire data sets is time
consuming. A good log management algorithm incurs small
overheads during normal logging activity, but cuts down the
time to identify affected transactions during recovery.
Such an algorithm is developed based on the following
observation. When a new transaction progression log record
for a specific transaction is written into a log data set, all the
previous progression log records of that transaction are no
longer needed for transaction resubmission. To signal a
logging event, when Tx.Log, ±1 is established, an acknowledgement is sent back to the subsystem where Tx.Log, is
logged. The log data set containing Tx.Log, is denoted as
P.LDS(Tx.Log +0). From the structure of transaction progression logs, P.LDS( Tx.Log + 1) must be associated with
the input phase of the same subsystem if Tx.Log, I1 is in an
output log data set, or the output phase of the precedent
subsystem if Tx.Logn I1 is in in input log data set.
To identify unacknowledged transaction messages, two lists
are associated with each log data set. The first one, receive
list, denoted by R.List(LDS), contains transaction names of
the received transaction messages which have not yet been
acknowledged back to the precedent log data set. The second
one, pending transaction list, denoted by P.List(LDS),
contains unacknowledged transaction messages. The log data
sets and these two lists in each subsystem are shown in Fig. 2.
At each log data set, the following steps are performed.
1) When Tx.Log has been logged and the transaction
messages are ready to be sent forward to the next subsystem or
for processing, a tuple (Tx, P.LDS( Tx.Log)) is appended to
R.List(LDS) and Tx.Log is added to P.List(LDS).

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 8, AUGUST 1987
Input

Tx.MS

Ack

-

Subsystern
*(DB
or

0,Output

DC)

I I Lijt

R.List

II

Ack

Lo

Tx.MS

v
Lo

Ack

Fig. 2. Transaction progression log.

2) Periodically (perhaps, depending on the length of
R.List(LDS) or the elapsed time from the last dispatch of
acknowledgement), an acknowledgement which consists of all
Tx's in R.List(LDS) is sent back to P.LDS(Tx.Log). Then,
R.List(LDS) is reset to empty.
3) When an acknowledgement is received, all Tx.Log
acknowledged are removed from the pending transaction list,
PList(LDS)
4) Periodically, a checkpoint log containing all Tx.Log of
P.List(LDS) is written into the log data set.
Note that P.List(LDS) contains all unacknowledged messages at LDS which need to be considered for replay. During
recovery, if P.List(LDS) is available, messages for replay
may be identified via P.List(LDS). If P.List(LDS) is lost,
we will use a superset of the P.List(LDS) to identify
messages for replay. The superset, denoted by SP(LDS),
consists of the most recently checkpointed log of P.List
(LDS) and all other messages logged after that checkpoint log
in the log data set. The checkpoint log contains all unacknowledged messages at the time the checkpoint was taken.
Messages that arrived subsequently, both acknowledged and
unacknowledged, are contained in SP(LDS). Hence, at all
times, P.List(LDS) c SP(LDS).
LDSO1 is called a predecessor to LDS, if a transaction
message logged at LDSm will arrive at LDS, without being
logged at any other intermediate log data set. An efficient
mechanism to identify the transactions destined for LDS, that
have log records in the predecessor LDS,, but not in LDS, is
proposed below. Transaction messages that have to be
reissued at failure will be regarded as the log difference.
Case 1. Both R.List(LDS0) and P.List(LDSm) are accessible.
The log difference will be those transactions that have
entries in P.List(LDSm) but not in R.List(LDS,).
Case 2: P.List(LDS,,) is accessible and R.List(LDS0) is
not.

Assume that message acknowledgements are sent in the
order that messages are logged. We denote LDS7m as the
subset of LDS0 to consists of transaction log records for which
the previous log record of the transaction was written at
LDSm. An entry in LDSm that does not appear in P.List(LDSm) must have been acknowledged. Any transaction log
record in LDSn' prior to this entry must also have been
acknowledged and cannot appear in P.Ljst(LDSm). To find
the log difference between LDSm and LDS0 we only need to
compare P.Lj5t(LDSm) and LDSn . We scan LDS', from the
most recent log record written until a log record that does not

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS

981

appear in P.List(LDSm) is found. The records scanned so far
DC(k)
DB(j)
will be removed from P.List(LDSm). The remaining entries
in P.List(LDSm) that are destined to have their next progression log records written in LDS, constitute the log difference
between LDSm and LDS,. They will be reissued.
LD
Case 3: P.List(LDSm) is not accessible.
Since P.List(LDSm) is not accessible, we use SP(LDSm),
Fig. 3. Simplified transaction logging arrangement.
which is a superset of P.List(LDSJm), to carry out the same
procedure described in Case 2 to determine the log difference. before and after it is transferred between two subsystems. If
Note that this procedure does not require access to R.Lis- we could merge the output log data set, communication
t(LDSn)medium, and the input log data set, a natural improvement to
3) Optimization and Variations of Log-Based Progres- the previous distributed DB/DC structure would ensue. The
sive Recovery: Intuitively, there are many logging operations use of shared queues between the DC subsystems and the DB
involved during a transaction's lifetime. Transactions will be subsystems results in such an optimization. Shared queues can
delayed by the log writes. However, some logs are desirable be implemented through an independent queue server and can
and necessary. For instance, the input log of the user's request be fault tolerant by duplexing memory, sparing and pairing the
is used to avoid reentering input messages. There are some server, etc. [15].
optimizations and variations on the proposed log-based proIn Fig. 4, three subsystems (SS1, SS2, and SS3) are
gressive recovery.
shown, with shared queues SQ1 and SQ2. Focus on the
1) Since every communication incurs a setup overhead, shared queue SQ1 which receives messages from SS1 and
batching messages may reduce the workload of the system. delivers messages to SS2. In the general usage of a queue, a
Logging of messages may be postponed until just before the message is deleted from a queue at the moment it is dequeued.
communication batch is sent out, and all the messages in the This is not appropriate here, because the shared queue is also
communication batch may be logged together. By synchroniz- intended to provide the function of a logging device. If,
ing batch logging and batch communication, messages will subsequent to dequeueing a message, SS2 fails before it is able
incur the batching delay only once. By batch logging, the to process the message and enqueue the processed message
number of write operations to stable storage used for logging into SQ2, a transaction would be lost. Hence, a message must
is also reduced.
be retained in SQ 1 (even after it is dequeued by SS2) until the
2) Until now we have been thinking of stable storage as processed message is enqueued in SQ2. If SS2 fails, we can
being equivalent to disk. Nonvolatile semiconductor memories replay all dequeued messages in SQ 1 that do not appear in
are becoming popular and may be used as storage for logging, SQ2. This poses still another subtle problem. Consider the
speeding up the process.
case where a message enqueued in SQ2 is quickly dequeued
3) There is a tradeoff between the number of log operations and processed by SS3 and enqueued in SS3's output queue.
and the time to recover a transaction affected by failure. As an When the message is deleted from SQ2, due to the asynchronillustration, consider the case where routing of transactions ous nature of message deleting, the corresponding transaction
through DC, DB, and DC is deterministic. As shown in Fig. message may still appear in SQ1. If SS2 fails, the original
3, the output log of DC, and the input log of DCk may be message in SQ I will be replayed to SS2, constituting a double
eliminated. If a transaction is lost in DCi or before it is logged execution of the transaction. One way to prevent this is not to
into LDS(DB7n), it can be detected by comparing LDS(DCi) remove a message from SQ2 until its corresponding message
and LDS(DBDn), the transaction will be reissued at the input of is removed from SQ 1. A more practical way, described
DC, and be reprocessed in its entirety by DCi. Similar below, uses an acknowledgement to SQ 1 when the message
considerations lead to the elimination of the input log for DCk. has been enqueued at SQ2, indicating that this message should
4) One would like to be able to send out the transaction no longer be considered for retransmission from SQ 1.
message to the next subsystem simultaneously with the logging
As in Section IV-A, we shall use acknowledgements to
of the message. That is to operate more optimistically. signal the event of enqueueing and dequeueing a message from
However, to make the transaction whereabouts traceable, an a shared queue. A subsystem will acknowledge the shared
additional message which indicates the completion of logging queue from where it obtained a message, after the subsystem
is needed to trigger the scheduling of the transaction execution processes and enqueues the message into a subsequent shared
at the receiving subsystem [14]. This allows more parallel queue. This acknowledgement, which we call the receiver
operations of logging and communication at the cost of extra acknowledgement, signals the fact that the message need no
messages to contain the increase recovery complexity. The longer be held in the precedent queue for replay purposes.
two effects tend to cancel each other and whether any gain in After this acknowledgement, the subsystem can signal the
performance can be achieved depends on the system parame- subsequent shared queue that the message will not be
ters [14].
retransmitted upon failure. We call this the "sender acknowl-

LE LE

LIE

B. Progrssive
Reovery wth Share QueuesFor each message in a shared queue, two flags, ra and sa
It can be observed, in Section IV-A, a transaction message can be attached. These flags indicate if the acknowledgements
is logged into an output log data set and an input log data set from the receiver and the sender have arrived, respectively.

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 8, AUGUST 1987

982

SSI

LXs

SETS

DEQ

ENQ

ra

f

d

Shared

SS3

O.cdal.l III

cib a

Queue SQ1

DEQ

ENQ

SS2

SETR

SETS

sa

ra

t

I FIs
I

Shared

Queue SQ2

II
SETR

Fig. 4. Transaction progression through shared queues.

Initially, we reset these two flags when a message is enqueued.
If both flags are set for a message, we are assured that the
message has been picked up and secured in a subsequent
shared queue and will never be retransmitted. Hence, messages with both flags set can be removed from the shared
queues. The shared queues, queued messages, and flags are
illustrated in Fig. 4.
There are four basic reliable atomic operations on the
shared queues. DEQ(SQ) retrieves a transaction message
from shared queue SQ. ENQ(SQ, TX.MS) puts the message,
TX.MS, to the shared queue SQ. SETR(SQ,TX.MS) sets
the ra flag of message TX.MS at shared queue SQ and
SETS(SQ, TX.MS) sets the sa flag of message TX.MS at the
shared queue SQ. Each subsystem follows the next sequence
in transaction processing, as illustrated for subsystem SS2 in
Fig. 4.
1) DEQ(SQ 1) to obtain message TX.MS from SQ1
2) process TX.MS
3) ENQ(SQ2, TX.MS)
4) SETR(SQl, TX.MS)
5) SETS(SQ2, TX.MS).
Notice that the first three steps are normal operations for any
system with shared queues. The last two are required for
progressive recovery. For performance purpose, they may be
batched as batching of the acknowledgement of R.List in the
log-based approach. The order of SETR and SETS guarantees
that a transaction messages at the output shared queue will not
be deleted before the input shared queue is acknowledged and,
for every transaction, at least one message is kept in the shared
queues.
When a subsystem fails, all its input shared queues and
output shared queues are examined. Table I presents all
possible conditions for transaction messages and the associated
flags between the subsystem's input and output shared queues.
The recovery action is quite simple. After the flags are
modified according to Table I and all deletable transaction
messages are removed, and the transaction message in the
input shared queue with its ra flag not set will be replayed.
Conceptually, transaction messages in transit are retained in
the shared queues. The status of transactions is identified
through the examination of the sa and ra flags of messages in
shared queues. Thus, shared queues not only provide a reliable
communication medium, but also reduce recovery interference. All subsystems have their normal operations as usual
and will have local recovery in case of failure. Meanwhile, by
means of reliable shared queues, transaction processing
becomes recoverable without the logging operations of the
system structure in Section IV-A.

C. Progressive Recovery with Paired Subsystems

Next we consider the case where checkpointing is done in
the system through paired subsystems. Tandem [1], [6] first
commercially explored the concept of process pairing. Auragen [5] refined it to reduce the number of checkpoint
messages. Under the process pair environment, critical processes are backed up by identical backup processes in another
processor. The backup process takes over from the primary
process in case of failure of the primary. The primary process
sends checkpoint messages to the backup process to keep the
backup informed of its current state. The effect of checkpointing into a log is quite similar to checkpointing into a backup
process. The difference is in recovery time and overhead.
When backup processes are employed, recovery time is
reduced and the normal overhead is determined by the
overhead for communication between processes and the
additional bookkeeping to keep backups in synchronization.
To examine the applicability of backup checkpointing to
progressive recovery, consider a backup subsystem to be
provided for each subsystem in a distributed DB/DC system.
Each primary subsystem checkpoints transaction messages
into its backup subsystem at its input and output interfaces.
The primary subsystem can begin the processing and forwarding of transaction messages after it receives the acknowledgement of checkpointing from its backup subsystem. The flow of
information is shown in Fig. 5. Meanwhile, P.List and R.List
are maintained and updated for checkpointed and acknowledged messages in the primary as described in the Section IVA. These two lists are also checkpointed into the backup
subsystem periodically at synchronization points. A backup
subsystem can be created by checkpointing both P.List and
R .List from the primary subsystem. Thus, a backup subsystem will have the same P.List and R.List as its primary when
it is created. When a checkpointed transaction message is
received at a backup subsystem, it is appended to the P.List
and R.List at the backup. As a consequence, in the backup
subsystem, an updated P.List is equivalent to the superset
SP(LDS) defined in Section IV-A, and an updated R.List
becomes a superset of R .List of the primary. At every
synchronization point, the P.List and R .List in the backup
subsystem will be made the same as the P.List and R.List in
the primary, respectively.
When a backup subsystem fails, a new copy of the backup
subsystem will be created by synchronization (checkpointing)
of P.List and R.List. If a primary subsystem fails, transaction
progression information retained at the backup suffices to
identify transaction messages needed for replaying. The

983

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS
TABLE I

STATES OF TRANSACTION MESSAGE BETWEEN SHARED QUEUES
SQ I

SQ2

Action

Description

Tx.MS

TxMS
Tx.MS S
TxMS S

don't care
Set S = 1
don't care

Tx.MS has been deleted in SQl
Tx.MS has been deleted in SQ l

Tx.MS k

NWMS

replay Tx.MS

TxMS is in-flight transaction and needs to be resubmitted.

Tx.MS R
Tx.MS R
Tx.MS R

Tx.MS.S
Tx.MS S

Set R = S
Set S = I

Tx.MS S

don't care

Subsystem fails after ENQ(SQ2,Tx.MS)
Subsystem fails after SETh (SQl, Tx.MS)
Subsystem fails after SETS(SQ2, Tx.MS)

=

I

SQ I, SQ2: input and output shared queues.
Tx.MS(TxMS) a transaction message exists (does not exist) in a shared queue.
R, S: the ra, sa flags associated with the transaction message.

Primary

3-

x

IA.

Primary y

-6

.2 7
T | .|]

1

t

s |

Backup x

Backup y

1,4: the clheckpoints of transaction messages.

2,5: thie acknowledgements of I and 4.
3: the forwarding transaction message.
6: the batched acknowledgemient based on R.List.
7: tlle pserioclic clieckpoint of P.List andl R.List.
Fig. 5. The message flow for paired subsystems.

P.List and R'.List at the backup will be used, as the Case 1 of
Section IV-A-2, to identify the transaction messages for
replay.
To limit the storage for saving checkpointed messages at a
backup subsystem, a proper interval between the checkpointing P.List and R.List must be found. A possible solution is to
define a threshold parameter N and to checkpoint both lists to
the backup and acknowledge receipt of messages to the
precedent subsystem after every N transaction messages. This
also has the effect of limiting the number of messages which
need to be reissued or resubmitted during recovery.
V. PERFORMANCE ANALYSIS
As the progressive approach eliminates the need for
synchronous communication protocols between subsystems
and components, its response time during normal processing is
expected to be improved over the pessimistic approach. A
queueing network model is constructed and simulated to
quantify the response time improvement. A system structure
with logging devices is assumed. Similar analysis can be
applied to other system structures.
A. Model
In the queueing network model, each subsystem has several
service stations which represent the transaction executions at

the subsystem. Jobs at service stations of a subsystem are
served by a processor according to the processor sharing
discipline. The transaction arrival process is assumed to be
|Poisson. We assume that there is a single log disk at each
subsystem. Log information is written into the log disk
sequentially as done in the write-ahead log protocol [19], i.e.,
no seek time for each log write is required. Also, all log
information queued can be batched and written in one disk
operation whose service time is assumed independent of the
amount of log information.

To illustrate the difference in transaction response times
-between the progressive and pessimistic approaches, the
system is assumed to consist of equally loaded DC and DB
processor. From the analysis of IMS traces, it is found that the
instruction path lengths at DC (includes input and output DC)
and DB subsystems are in the ratio 1:2. Thus, we model the
system which is configured with two DB's and one DC
(contains input and output subsystems). From analysis of IMS
traces reported in [22], the following workload parameters are
used to drive our model: average transaction path length of
approximately 450K instructions, 15 lock requests, and 11
database I/O's per transaction. A total of 8K instructions for
sending and receiving communications (for a transaction
message) is assumed. The processor speed is assumed to be 14
MIPS. The DB and DC subsystems are assumed to be running
on the processors that are geographically close, so that the
physical time delay to send signals between processors is
negligible.
Our model for a DC subsystem is illustrated in Fig. 6 for the
progressive approach. The processor on which the DC runs
incurs the overheads for communications, I/O, and processing. New transactions go through several services, i.e.,
message receiving, input logging, message formatting, output
logging, and then are sent to the DB subsystem. The output
DC has the same model as in Fig. 6, in which transaction
messages come from DB, get formatted, and are then sent to
output devices.
The DB subsystem under the progressive approach is
modeled in Fig. 7. Besides the communications, logging, and
transaction execution operations, the DB also handles database
access. Database I/O delay is modeled by an infinite server.
(This is different from the disk server for logging entity.) The

984

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 8, AUGUST 1987

DC

subsystem

user

input
(from DBS

:

receive
nsg.

initiote
input securing
minput
..

formot

nsg

send msg.

iritiote out-:
secgput securing

d;sk

oD
t subsystem

(output

rnsg.)

disk

write (>write (

---------se.curi_ng _entity

.....

Fig. 6. Model of DC subsystem with progression recovery protocol.

DB subsystem
lock
contention

de

ay server
dotobase
serve,

fromn

to

output DC

input DC
receive
msg

initiote

input securing

applicotion
processes

initiote

output securing

disk

send msg.

disk
write

write

securing entity
Fig. 7. Model of DB subsystem with progression recovery protocol.

increase in overheads on the DB due to the recovery protocol
not only increases the transaction response time directly, but
also indirectly increases waiting time during lock contention.
ENcution of the transaction in the DB is interspersed with
lock requests needed to serialize database access. Our model
assumes that when a lock request is issued, the requesting
transaction may be blocked by a concurrent transaction for a
time proportional to the execution time of the blocking
transaction (to simplify the model, a proportionality constant
of 1/3 is used, which is obtained from assuming that locks are
acquired uniformly over the lifetime of the transaction as
described in [22]). The overheads for various system activities
such as locking/unlocking, I/O, task switching, block invalidation, etc., are all captured and combined with the application processes.
The contention probability varies with respect to the
transaction rate and transaction execution time at DB. With the
asymptotic results in [11], [16], it is observed that the
contention probability grows as the product of the transaction
rate and response time. To take into account the increase in
lock contention and lock delay with transaction rate and the
feedback effect of lock contention increasing with response
time, the simulations of the models are done iteratively,
Initially, the contention probability is first estimated based on
the contention probability observed in trace driven simulation

and the assumed transaction rate. The average transaction
execution time at DB is first obtained through simulation for
the initial estimate on contention probability. Then the
resulting execution time at the DB is used to compute a new
contention probability by assuming that the contention probability grows as the product of the transaction rate and response
time. The lock delay is also adjusted proportionally. The
simulation model is then run again with the new contention
probability and lock delay. The iteration is repeated until
convergence is obtained. Only a few iterations are required in
the contention range considered.
In Figs. 8 and 9, the models for input DC and DB
subsystems under the pessimistic approach are illustrated. The
main difference between these and the models in Figs. 6 and 7,
is in the use here of two-phase commit protocol for coordinating logging and communication. Note that the join operation
of the prepare message and- the reply "yes" message from the
receiver acts as a synchronization point between phase 1 and
phase 2. In contrast to the progressive approach, the messages
for commit preparation cause two additional logging delays at
DC and DB, adding to transaction response time. The message
passed from receiver (DB or output DC) to acknowledge the
receipt of the commit message and the disk write, signaling
"end of commit" are omitted since they can be done
asynchronously with respect to transaction execution. Other

985

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS

input DC subsystem
'tx msg

user

to

fork

receive

msg.

initiote
input securing

/oin\

. ..

.

59

.

receive

*e~ ~ ~ ~ ~ ~ ~ ~ _rply_

yes'

prepore conmmit
msg.

disk

write

write

c

-e

prepor-

or commi tt

.. .

disk

DQ

send rns

formot msg.

msg.

securing entity
Fig. 8. Model of DC subsysem with pessimistic recovery protocol.

DB
e

subsystem
corntent ion

ysre

dotobose

:ommit

server

msg:

tx.
rnsgo

input DC

tx.

eceive

\op

send reply

S

sen msg.

~~~~~~~~~iniliot'e

(0) securing

initiote

or commitr

is

n

msg tox

i

s

q.

prear

receive

output DC

securing entity
Fig. 9.

Model of DB subsystem with pessimistic recovery protocol.

fulnction of transaction path length and report it inFig. 11. The
transaction rate was chosen so that the transaction load
presented to the system maintained a 70 percent utilization (the
transaction load is the produlct of the transaction path length
and the transaction rate, excluding all the logging and
communication overheads). The pessimistic approach is uniformnly inferior and the first to break down. However, both
approaches break down for small transactions because the
communication and logging overheads increase linearly with
the number of transactions and saturate the systems.
B. Results
Communication costs, measured in terms of the total
In Fig. 10 we plot the transaction response time against the number of instructions needed to send and receive a message,
transaction rate for the two approaches and include the for systems vary, depending on the communication protocols
transaction response time if no recovery protocol were and mechanisms implemented. In Fig. 12 we study the effects
employed. The progressive approach not only provides a of changing the communication overhead. The number of
better response time but also can support a greater transaction instructions required to communicate a message is varied from
rate before breaking down.
2 to 20K instructions per message. A transaction load of 70
Since different application environments may exhibit trans- percent is maintained on the processors. Again progressive
actions of different path lengths we undertake a sensitivity approach exhibits a smaller increase in response time than
analysis of the performance of the two approaches as a pessimistic approach as communication overhead increases.
operations, such as message formatting at the DC and
application processing at the DB, are similar to that described
above.
Note that for the pessimistic approach the overhead for
component level tracking in main memory is not modeled.
Hence, the response time curves given in Section V-B for the
pessimistic recovery protocol should be viewed as a lower
bound for the actual response time for a system using the
pessimistic approach.

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-36, NO. 8, AUGUST 1987

986

-^---- pessimistic
* *

*

0

recovery

progressive recovery
no recovery

E

CL
0.

0

0

I-

x
o

~ ~

~~~c

0
c

6~~~0 ~40

60

50

Transaction Arrival Rote (tx/sec)

Fig. 10. Sensitivity of transaction response time to transaction rate.

_ _|

-.=PESSIMISTIC
-.-PROGRESSrVERECOVERY
RECOVERY

e

AlA_

pessimistic recovery

* * progressive
---norcvy

u\

recovery

EI

o~~~~~~~~~~~~~~~~~~~~~n recoveryom
-----NO RECOVERY

E

CD

st

Fig. 11.

~

I

°200

~

~

600
tRANSACTION PATH LEN9GTH
400

~

~

~

800

i
~~~~~I
~

100

00

oo

1
01

I

l
20

(incKilo-instructions)oCommunication OverheCd in Klo- instructions
Fig. 12. Sensitivity of transaction response time to communication cost.
Sensitivity of transaction response time to transaction path-length.

VI. CONCLUSION
Transaction recovery in a distributed transaction processing
environment can be achieved through a store-and-forward
scheme which saves information progressively at subsystem
interfaces. This progressive recovery approach eliminates
costly synchronous communication protocols between subsystems and components. It avoids any centralized recovery
management mechanism and restricts recovery actions to no
more than immediate antecedent or precedent subsystems.
We have applied the principle of progressive recovery to
three different system structures. In the first, logging is

employed as the main basis for saving work done on a

betwen subsstems
o messags beween
Loging
transacion.
ubsysems aLoggngofmessaes
transation
counts for the overhead cost during normal processing.

Second, progressive recovery is made to exploit reliable
shared queue facilities where store-and-forward operations are
combined. Finally, backup subsystems may be employed to
replace logging without changing the basic technique of
progressive recovery. Based on the system structure with
logging devices, a queueing model is developed and solved by
simulation and the performance during normal processing is
analyzed. It is found that the progressive approach can reduce
normal processing overhead and lead to noticeable improvement of response time over the pessimistic approach.
REFERENCES

[1] J. F. Bartlett, "A nonstop operating system,"' in Proc. 11th Hawaii
Syst. Sci., 1978.in Proc. Eighth ACM Symp. Oper. Syst.
"A nonstop
___,Conf.

[2Int.

Principles, 1981.

kernel,"'

LEE et al.: PROGRESSIVE TRANSACTION RECOVERY IN DB/DC SYSTEMS

131 L. A. Bjork, "Recovery scenario for a DB/DC system," in Proc.
ACMNat. Conf., 1973, pp. 142-146.
[4] M. W. Blasgen et al., "System R: An architectural overview," IBM
Syst. J., vol. 20, no. 1, pp. 41-62, 1981.
[5] A. Borg, J. Baumbach, and S. Glazer, "A message system supporting
fault tolerance," in Proc. 9th ACM Symp. Oper. Syst. Principles,
1983, pp. 90-99.
[61 A. Borr, "Transaction monitoring in ENCOMPASS: Reliable distributed transaction processing," in Proc. 7th Int. Conf. Very Large
Databases, 1981, pp. 155-165.

171 R. A. Crus, "Data recovery in IBM database 2," IBM Syst. J., vol.

23, no. 2, pp. 178-188, 1984.
C. J. Date, An Introduction to Database System, Vols. I and 2.
Reading, MA: Addison-Wesley, 1981 and 1982.
I9] C. T. Davies, "Recovery semantics for a DB/DC system,'" in Proc.
A CM Na. Conf., 1973, pp. 136-141.
[10] J. Gray, "Notes on database operating systems," Lecture Notes
Comput. Sci.: Operating Syst. New York: Springer-Verlag, 1978,

181

1111
[121

1131

[141
115]
116]

[17]
118]

[191
[201

1211

[221

pp. 393-481.
J. Gray, P. Obermarck, and H. Korth, "A straw man analysis of
probability of waiting and deadlock," IBM Res. Rep. RJ-3066, San
Jose, CA, 1981.
IBM Corp., "Customer information control system (CICS/VS): Recovery and restart guide," Publ.
June 1983.
IBM Corp., "IMS/VS version I recovery/restart," Pub]. GG24-1515,
IBM World Trade Syst. Centers. San Jose, CA, Oct. 1979.
B. R. Iyer, P. S. Yu, and Y. H. Lee, "Analysis of recovery protocols
in distributed on-line transaction processing systems," in Proc. Realtime Syst. Symp., 1986.
B. R. Iyer, P. S. Yu, and L. Donatiello, "Analysis of fault tolerant
multiprocessor architectures for lock engine design," in Proc. Comput. Syst. Sci. Eng., vol. 2, no. 2, pp. 59-75, 1987.
S. Lavenberg, "A simple analysis of exclusive and shared lock
contention in a database system." Perform. Eval. Rev.. vol. 12, no. 3,
pp. 143-148.
W. Kim, "Highly available systems for database applications,
Comput. Surv., vol. 16, pp. 71-98, Mar. 1984.
0. Serlin, "Fault-tolerant systems in commercial applications,"
Computer, vol. 17, pp. 19-30, Aug. 1984.
J. P. Strickland, P. P. Uhrowczik. and V. L. Watts, "IMS/VS: An
evolving system," IBM Syst. J.. vol. 21, no. 4, pp. 490-510, 1982.
R. Strom and S. Yemini, "Optimistic recovery in distributed systems,"
ACM Trans. Comput. Syst., vol. 3, no. 3, pp. 204-226, 1985.
1. L. Traiger, "Trends in system aspects of database management,"
IBM Res. Rep., RJ-3845, 1983.
P. S. Yu, D. M. Dias, J. T. Robinson, B. R. lyer, and D. W. Cornell,
"Modelling of centralized concurrency control in a multi-system
environment," Perform. Eval. Rev., vol. 13, no. 2, pp. 183-191,
1985.

SC33-0135-l,

987

[231 P. S. Yu, B. R. Iyer, and Y. H. Lee, "Transaction recovery in

distributed DB/DC systems: A progressive approach," in Proc. 5th
Symp. Reliability Distributed Software Database Syst., pp. 207214, Jan. 1986.

diYann-Hang Lee (S'81-M'84) received the B.S.
di egree in engineering science and the M.S. degree
in electrical engineering from National Cheng Kung
University, in 1973 and 1978, respectively, and the
Ph.D. degree in computer, information, and control
engineering from the University of Michigan, Ann
Arbor, in 1984.
Since December 1984, he has been with IBM
Thomas J. Watson Research Center, Yorktown
dHeights, NY. His current research interests include
distributed computer systems, database processing
systems, fault-tolerant computing, VLSI testing, performance evaluation, and
stochastic decision theory.

Philip S. Yu (S'76-M'78-SM'87) received the
B.S. degree in electrical engineering from National
Taiwan University, Taipei, Taiwan, Republic of
China, in 1972, the M.S. and Ph.D. degrees in
electrical engineering from Stanford University,
Stanford, CA, in 1976 and 1978, respectively, and
th M.B.A. degree from New York University,
New York, NY, in 1982.
Since 1978 he has been with IBM Thomas J.
Watson Research Center, Yorktown Heights, NY.
Currently he is the Manager of the Architecture
Analysis and Design Group. His current research interests include computer
architecture, database management systems, performance modeling, workload analysis, computer networking, and VLSI testing.
Dr. Yu is a member of the Association for Computing Machinery.

Balakrishna R. Iyer received the B.Tech. degree from the Indian Institute of
Technology, Bombay, and the M.S. and Ph.D. degrees from Rice University,
Houston, TX.
He has previously worked for Bell Telephone Laboratories, Murray Hill,
NJ. He is currently with the IBM Thomas J. Watson Research Center,
Yorktown Heights, NY. His research interests include computer architecture,
database systems, fault-tolerant computing, voice-data integration, computer
networks, performance and reliability analysis, and videotex systems.

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-3 1, NO. 1 1, NOVEMBER 1982

Design

of HM2p

Multimicroprocessor

1045

Hierarchical
for General-Purpose
A

Applications

KANG G. SHIN, MEMBER, IEEE, YANN-HANG LEE, STUDENT MEMBER, IEEE, AND J. SASIDHAR,
MEMBER, IEEE

Abstract-This paper presents a tree-structured multiprocessor
called the hierarchical multimicroprocessor (HM2p), each node of
which is composed of a cluster of processor modules (PM's), common
memory, DMA interface, switches, communication lines, and a data
processor associated with it. The HM2p consists of two different
hierarchies, one for data processing and the other for data distribution,
which provide clean, structured separation between processing components and user interface components.
There are two levels of interprocessor communications in the HM2p,
an implementation of which is developed with the monitor concept. By
examining the access pattern of shared hardware resources, we have
modeled the performance of the HM2p as a multichain closed queueing
network. Using this queueing model, the performance falloffs due to
shared hardware (e.g., processors, memory, and I/O devices) are also
analyzed, and the optimum number of processors in each cluster is then
determined.
Index Terms-Hierarchical multiprocessor, monitor, performance
falloff, processing/data distribution hierarchy, queueing model, synchronization.

I. INTRODUCTION
C ONTINUING advances in VLSI technology have made
it attractive to interconnect many inexpensive microprocessors and memories to build a powerful, cost-effective
computer, namely, a multimicroprocessor (M2p). Potential
benefits to be -gained from an M2p include improved cost
performance resulting from the exploitation of parallelism in
most algorithms and many inexpensive but powerful microprocessors and memories, enhanced fault tolerance by using
many available processors in the M2p as redundant spares, and
a high degree of modularity which permits the M2p to grow
or shrink by addition or removal of modular components.
However, the main question that still remains to be answered
satisfactorily is whether the microprocessor can be utilized as
a building block for large general-purpose computer systems,
thereby achieving a higher performance/cost ratio as compared to traditional uniprocessors. Excellent surveys of existing
Manuscript received January 7, 1982; revised June 17, 1982. This work
was supported in part by the National Institute of Justice, U.S. Department
of Justice, under Contract J-LEAA-014-78.
K. G. Shin and Y.-H. Lee were with the Electrical, Computer, and Systems
Engineering Department, Rensselaer Polytechnic Institute, Troy, NY. They
are now with the Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Ml 48109.
J. Sasidhar was with the Electrical, Computer, and Systems Engineering
Department, Rensselaer Polytechnic Institute, Troy, NY. He is now with
Softech, Waltham, MA 02154.

multiprocessors can be found in [1] and [2]. The unsolved
issues associated with multiprocessors are well discussed in
[3].
Computational tasks in many applications such as engineering, socioeconomics, biology, medicine, etc., can be partitioned into smaller cooperating tasks (which we shall call
processes). The cooperating processes are then clustered into
a number of tightly coupled process groups that are hierarchically related to each other. Also, typical applications of
computer systems-especially those related to information
retrieval (such as information handling related to medical, law
enforcement, social services, etc.)-impose a time-varying load
that calls for a robust system. Tree-structured multiprocessors
appear to have the potential to exploit the hierarchical nature
of most computations [4]-[6] and are structurally flexible.
This paper deals with one such tree-structured multiprocessor
called the hierarchical multimicroprocessor (HM2p). The
HM2p forms a tree, each node of which consists of a cluster
of processor modules (PM's), shared memory, switches, DMA
interface, and serial communication links. The HM2p has two
distinct hierarchies; one is for data processing (called the Phierarchy) and the other is for data distribution (called the
D-hierarchy). The necessity of including these two hierarchies
becomes apparent when we review the Cm* architecture developed at Carnegie-Mellon University and the Hierarchical
Multicomputer Organization at the State University of New
York, Stony Brook.
The Cm* consists of a two-level hierarchy, its lower level
is composed of computer modules (Cm's) which are grouped
into a cluster via a time-shared bus (map bus), and its upper
level comprises homogeneous clusters which are interconnected
via intercluster buses [6]-[8J. The central idea for the Cm*
operating system is the concept of task forces, i.e., large collections of executing processes that cooperate to accomplish
a single purpose [8], [9]. The main drawback in Cm*, however,
is the integration of the I/O units into the system. The I/O
units are made dependent on individual computer modules,
and this results in an unstructured operating system and gives
rise to reliability and utilization problems. This, to some extent,
has been solved by the Hierarchical Multicomputer Organization [10], [1 1] where the idea of separating the control and
data moving functions has been proposed. In the HM2p, this
idea has been extended to include clean, structured data processing as well as I/O interface to the system.

001 8-9340/82/1100-1045$00.75

©) 1982 IEEE

1046

IEEE TRANSACTIONS ON COMPUTERS, VOL.

c-31, NO. 11, NOVEMBER 1982

structure of the kernels necessary to implement monitor
primitives [14], [15] for synchronization purposes. Finally,
Section IV analyzes the performance falloffs due to shared
hardware resources, and the conclusion then follows in Section
V.
II. ORGANIZATION OF HM2p

The HM2p is structured to accommodate the hierarchical
nature of most computations, to exploit the parallelism existing
within an algorithm, and to provide a systematic I/O interface
to the system. The HM2p consists of two distinct hierarchies:
one for data processing (P-hierarchy) and the other for data
distribution (D-hierarchy). The former is responsible for executing cooperating processes, and the latter is to distribute
data necessary for the P-hierarchy. Both hierarchies consist
of tree-structured clusters which are formed by a number of
processors. To differentiate.the processors in the P-hierarchy
from those in the D-hierarchy, the former are referred to as
the P-processors and the latter as the D-processors. The two
hierarchies of the HM2p are merged at the top by a root cluster
(see Fig. 1 for an overall system organization).

C - Cluster
DP- D- Processor
SM- Secondary Memory

Fig. 1. System organization.
To Parent P Processor
-

A. Processing Hierarchy
The extent of exploitable parallelism with a multiprocessor
depends on the overhead involved in communicating across

P

S
A

Mc

ML

-

-

-

DP SCI -

To Child Cluster
Processor

To Child Cluster

Local Switch
Arbiter
Common Memory
Local MemorY
D Processor
Serial Communication

[D]
P
To Child

D-Processor

-

Interfoce

Fig. 2. Block diagram of a cluster with its associated D-processor in the
HM2p.

The clustering of processor modules in the HM2p primarily
is intended to provide hardware facility to execute a group of
tightly coupled cooperating processes. This facility allows us
to utilize the high degree of access locality [3] within the group.
Saturation can, however, occur very quickly as the cluster
expands. Indeed, it is a well-known fact that after a point,
contentions for shared resources cause the performance of a
cluster actually to decline with the addition of extra processors
[12], [13]. It is therefore important to estimate the performance falloffs due to these contentions and then determine the
optimum number of processor modules in each cluster. In this
paper, we have studied this problem for the HM2p using
queueing network models.
For structural flexibility, the HM2p has been designed to
simplify the interconnection structure as far as possible. It uses
only a few types of functional units as building blocks for the
system. The aim of the design has been to create a generalpurpose multiprocessor with no restriction on the types of algorithms which it can exploit.
This paper is organized as follows. Section II introduces the
HM2p architecture in some detail. Section III describes the

process boundaries. The hardware interconnection scheme
which has the lowest associated communication overhead is
the shared memory method. The main drawback in using
shared memory is that the communication overhead tends to
increase rapidly with the number of processors in the multiprocessor. To alleviate this problem, the P-hierarchy is designed
to include two levels of communication. At the first level, the
communication time is kept to a minimum and is independent
of the total number of processors in the HM2p. Note that at
this level, a restriction is imposed on the number of interconnected processors which are grouped into a cluster via a
time-shared common bus and a shared common memory. At
the second level, the communication speed is sacrificed for
expansibility and hardware interconnection costs. At this level,
the clusters are connected in a tree-structured fashion via serial
links.
The significance of this approach becomes clearer when we
consider process locality; interaction within a defined group
of processes is generally frequent, and interaction between
different groups is infrequent. If processes are assigned to
processors such that the processes of the same task reside in
a single cluster, then the communication overhead would
correspond to that of a closely coupled system. This relates to
the well-known task-assignment problem [16] in multiprocessors.
A cluster consists of processing modules (PM's) which have
a sibling relationship to each other, and these PM's share a
common memory by means of time-shared common bus (Fig.
2). Conflicts of access to the common bus are resolved by the
bus arbiter. The handshaking required for gaining control of
the bus is handled by the switch, which is a component of each
PM. Each PM consists of a microprocessor, local memory, a

1047

SHIN et al.: DESIGN OF HM2p

switch, a DMA interface, and serial links to its child and parent
PM's.
When the microprocessor issues a memory access, the switch
in the concerned PM routes this access to its local memory or
to the common memory. The common memory is used to store
the system monitor parameters and the user-declared common
segments that are shared by interacting processes (system
synchronization will be discussed in Section III). As in the
Cm*, processor-generated common memory addresses are
translated by a window register [6] in the switch such that the
user process is transparent to the physical allocation of shared
segments. To perform a common memory access, the switch
has to gain control of the common bus by handshaking with
the arbiter. The switch has been given the capability of buffering a single data word which has to be read from or written
into the common memory.
The arbiter has to be moderately complex since it must be
able to grant control of the common bus at two separate levels
(one for synchronization primitives and the other for common
memory access), and there are certain rules to follow in order
to preserve the integrity of the interprocess synchronization
primitives (on this, more will be discussed later). The arbiter
provides a round-robin service -to requesting processors to ensure that all requests will be honored in due time. The switch
in a PM has two request lines to the arbiter for requesting
control of bus at the two levels, and correspondingly there are
two grant lines to each switch. The switch includes a status to
indicate whether its request is for synchronization or for access
to the common memory. This status is explicitly set by the
processor and is alterable only by the processor. Thus, the bus
arbiter functions as either a coordinator or a global lock within
a single cluster, depending on the nature of the currently
honored request.
The DMA interface transfers a block of code/data to or
from the local memory of the D-processor associated with the
cluster and the local memories of the PM's. When the Dprocessor receives the block transfer order from the parent
P-processor of the cluster, the D-processor then sets the address register and the word count register of the DMA interface appropriately and then initiates the transfer. Upon completing the transfer, the DMA interface notifies the P-processor which, in turn, notifies the parent P-processor of the
cluster.
Besides the lateral interconnection paths, each P-processor
has several serial links to its parent P-processor and its child
cluster. With these serial links, clusters form a tree-structured
hierarchy in which the number of branches at each node is
equal to the number of P-processors in a single cluster. These
links are used to transfer the control and status information
between adjacent levels of the tree. They are also used to exchange messages between interacting processes that are located in different clusters. At both ends of the link, we need
additional processing for buffering a message, generating interrupts, and setting up flags at the completion of a message
transfer. A parent processor can interrupt its child processor
through the serial link at two levels: one level is maskable and
the other is nonmaskable. An interrupt at either of the two
levels will cause the child to execute a message-receiving

routine which is a part of the kernel software. In normal operation, a parent interrupts its child at the maskable level. This
implies that if the child is inside the kernel, the interrupt will
remain pending until the child exits from the kernel. But if the
parent has reason to believe that a malfunction has occurred,
it interrupts at the nonmaskable level. The child, on the other
hand, can interrupt its parent through the serial control link
only at the maskable level. This ensures that the parent can still
function with a faulty child processor.
B. Data Distinction Hierarchy
For each cluster in the processing hierarchy, there is an
associated D-processor which handles the transfer of code/
data into or out of the cluster via the DMA channel. An additional link between the D-processor and the parent P-processor is provided to handle DMA commands and requests for
process creation. The D-processors are interconnected to form
the data distribution hierarchy as in Fig. 1. The secondary
storage units as well as I/O devices are attached to the Dhierarchy. Since most of the processing is done at the bottom
level of the P-hierarchy, most of the file transfers in the system
will be handled by the associated leaf D-processors. Thus, we
need high capacity data links between the secondary storage
units and the bottom level D-processors of the D-hierarchy.
To perform the file management functions of the system, the
D-processors need to exchange short control messages among
themselves. The D-processors are interconnected hierarchically
by means of serial links, and since at times there will be file
transfers on these links, a packet switching communication
system has to be implemented.
All the user interfaces to the system are connected to the
D-processors, and so it acts as the source of all tasks which need
processing power from the processing hierarchy. New processes enter the P-hierarchy via the serial control links interconnecting the two hierarchies, and the computation results
enter the D-hierarchy through the DMA channel. The Dprocessors act as command message interpreters in the same
sense as the "shell" of the UNIX system [17] (called the
shell-like process) and create processes which execute the
command message in the processing hierarchy. Using the
D-hierarchy and the links among D-processors, a new process
can enter the P-hierarchy at any level. Although the cost of
communication with I/O units may depend on their locations
in the D-hierarchy, processor modules in the P-hierarchy are
made logically independent so as to enhance the reliability of
the overall system and to balance the work load in the P-hi-

erarchy.

III. SYNCHRONIZATION AND COMMUNICATION

The objective of the operating system in the HM2p is to
allow maximum concurrency and transparency. Since the
P-hierarchy includes the two-level structure-the closely
coupled clusters and the tree network of clusters-we adopt
two monitor systems associated with them. The processes are
distributed in the system and communicate with each other
via monitors and proper addressing mechanisms. Interprocess
communication is based on the monitors and the kernels for

1048

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-3 1, NO. 11, NOVEMBER

the processes residing in a cluster and the monitor procedures
in the tree network.
A. Kernels of the HM2p Operating System

From a software point of view, monitors [8] are an ideal
mechanism with which one can implement synchronization
and interprocess communication. Monitors consist of shared
data and procedures to operate on the shared data. A process
can operate on the shared data only through the monitor procedure. The operations on the shared data are kept mutually
exclusive. There are four primitives needed to support monitors, namely, entering a monitor, exiting a monitor, signaling
a condition, and waiting for a condition. Since there are two
levels of interprocess communication in the HM2p, the kernels
are designed to provide the execution environment at each
level.
1) Kernels at the Cluster Level: For each P-processor, we
define the processor kernel (called the P-kernel) which works
as the supervisor of a processor to manage the process residing
in that processor. The P-kernel has certain procedures to
handle the execution of the user task. The task running in the
P-processor of the cluster is in the user mode, and is trapped
to the P-kernel when one of the following cases occurs: the
invocation of a synchronization primitive, an addressing fault
during a global access, an interrupt from the parent P-processor, or an exception. Through the P-kernel, the process can
enter a monitor at the cluster level or the network level to
perform synchronization and interprocess communication or
request to declare a new space in the common memory. When
the process enters the P-kernel, the interrupt is disabled to
avoid a race condition.
The cluster kernel (called the C-kernel) handles the monitor
of all processes residing in that cluster and is located in the
common memory of that cluster. The mutual exclusion of the
C-kernel is secured by the bus arbiter by means of the following
steps.
a) If the process wants to enter the C-kernel, it sets a status
bit in the switch. The switch then asserts the C-request line.
b) The bus arbiter asserts the C-grant line if the C-kernel
is not in use. The switch then sets a flag indicating to the processor that it can now proceed to use the C-kernel.
c) Once the processor exits from the C-kernel, it resets the
status bit in the switch which causes the switch to deassert the
C-kernel request line.
The C-kernel provides mutual exclusion of the monitors by
associating with each monitor a flag which records whether
or not the monitor is busy. Thus, the C-kernel provides a means
of having more than one monitor busy at the same time. The
C-kernel maintains the queues for processes waiting to enter
monitors and queues for each condition. There are four monitor primitives in the C-kernel as discussed above. When a
process awaits a condition or a monitor, the P-processor executing this process becomes idle. When the process releases
a monitor or signals a condition, it sends the identification of
the waiting process to the parent P-processor. The parent Pprocessor then interrupts the processor which has that process
in the wait queue. Thus, we have a "positive wakeup of a process" [18]. The C-kernel contains both the identification of

1982

the process and the physical processor in which it is residing,
whereas the P-kernel keeps the full status of the process necessary to restart the process.
2) Kernel at the System Level: To implement the synchronization primitives at the system level, the monitor concept
can still be used. This approach provides transparency and
makes it easier for the system programmer to implement the
system software. Since the clusters are arranged as a tree, we
can always find a common ancestor cluster to interacting
processes that are located in different clusters. The monitor
procedure is allocated in the common ancestor cluster. The
execution of a monitor procedure can be implemented via
messages. The mutual exclusion is easily maintained since only
one process is allowed to be in the monitor procedure of that
ancestor cluster at one time.
B. Capability-Based Addressing
The objects of the system consist of resources and processes.
The resources are passive elements which include files, I/O
interfaces, processors, and memories. Resources are used for
the execution of a process. In the HM2p, each object is associated with its capability list which describes the environment
of the execution and the relationship with other objects. A
capability consists of a description of the concerned object, the
associated operation, and the access right. Thus, the access of
an object can be handled through this capability list in the
execution space. The important advantages of the capability
are to decentralize the overhead of address mapping [ 19] and
object protection [20], and to treat the objects uniformly from
the user's standpoint [ 1 7 ].
In the HM2p, the addressing of the common memory in a
cluster is translated by the switch. The parent P-processor
partitions the common memory into several segments and
provides the physical address to the concerned child P-processor. In order to address the I/O devices and the fi'le system,
the P-kernel maps the logical names to the physical locations,
and also checks the access right on the basis of the capability
of the object. At the destination, the condition of the object is
checked to ensure the read/write consistency. Most passive
objects reside in the data distribution hierarchy, and the Dprocessor has several I/O routines to provide the operations
to access these objects.
When an I/O device is active, the shell-like process generates a local space (defined by the capability list), depending
on the characteristics of the device and the user identity. All
requests or commands are handled in the basic local space and
its extension. When a process is invoked, a new space is generated including the capabilities associated with the process
itself and the arguments. The legality of the process invocation
is checked first in the basic space; then this invocation and the
capability list are sent to the parent P-processor (the invoker
may be the shell-like process in the D-processor or the process
residing in the cluster). Either the loading of the process to an
available processor or just migrating of the argument into an
existing process would take place. If there is no available
processor in the cluster, this invocation would be sent up to the
grandparent P-processor, and a free processor would then be
acquired from a neighboring cluster.

SHIN et al.: DESIGN OF HM2p

IV. PERFORMANCE ANALYSIS

The performance falloffs in a multiprocessor are generally
introduced due to contention for use of shared hardware and
software resources. Since the HM2p consists of two levels (i.e.,
clusters and tree-structured entire system), and since most of
the data processing and file management are carried out at the
leaf levels of the data processing and distribution hierarchies,
we first determine the performance of a single cluster, treating
it as a stand-alone unit. Then we calculate the performance
of the entire system by multiplying the number of leaf clusters
by the performance of a cluster that is obtained from the single
cluster analysis.
In the present work, the performance refers only to the
throughput of the system, and is analyzed on the basis of the
following two assumptions.
Assumption 1: Each cluster consists of a finite number of
processors whose mechanisms of accessing the shared resources
are statistically identical. All the clusters at the same level of
the hierarchy thus have the same statistical property.
Assumption 2: No single processor in a cluster has the
multiprogramming capability, while the cluster does have this
capability. Thus, a new process would be assigned to a processor when the processor completes the previous assignment
and becomes idle. The processor has to be suspended every time
it awaits the availability of a shared resource.
A. Performance of a Single Cluster
A cluster in the HM2p has a number of processors which are
interconnected via the time-shared bus and communicate with
each other via the common bus and common memory. This
interconnection structure is very simple and flexible for expansion and reduction, but the system performance saturates
quickly as the number of processors in a cluster increases
mainly due to interprocessor communications. It is therefore
important to analyze the performance of a single cluster which
enables us to determine the optimum number of processors in
a cluster and to calculate the performance falloffs due entirely
to the shared hardware resources. In this paper, we do not
consider the performance falloffs due to software precedence
relationships which determine the final figure of performance.
The hardware resources shared by the P-processors of a
cluster are the time-shared common bus-common memory
pair, the parent P-processor, and the D-processor. Interference
in sharing these resources results in a decrease in the performance of each processor. Taking this interference into consideration, we have developed a two-part queueing model for
evaluating the performance of a single cluster. The first part
expresses the performance falloffs due to common memory
interference, and the second part models those due to the
parent processor and D-processor of the cluster.
1) Common Memory Interference: Let the common
memory access time (tma) be the time taken to read or write
a single word into the common memory once the switch has
mastership of the time-shared common bus in a cluster. Let
the access request interval (tar) denote the time interval between two consecutive access requests to memory by a pro-

1049

cessor. The access requests can be either for code or data, and
may also be made to either processor's local memory or the
common memory of the cluster. As will be explained la-ter; a
processor may have to access the common memory at several
consecutive times, for example, when it is granted to execute
the C-kernel code. This fact eliminates the need of distinguishing the access request interval for the common memory
from that for processor's local memory. Although there is a
variation in the access request interval times, it is assumed for
simplicity to be a constant as in [21 ].
Let us further denote the integer value Ltar/tmaJ by m where
lx] is the largest integer not exceeding x. Then one can observe
that the greater the value of m, the less the interference due
to the shared resource, and thus the greater is the performance
of the processors in the cluster. With the current technologies
of microprocessor and memory, the value of m is usually in the
range 3-10 [22], and this can be used as a design parameter.
To analyze the interference in accessing the common
memory, we should have an understanding of the nature of the
stochastic process which describes the accesses to common
memory by each processor. Reviewing the use of common
memory we find that the common memory is used only for
monitor procedures and their associated data and control
mechanisms. When a processor starts executing a monitor
procedure, all memory accesses will be to the common memory
since both code and data reside in the common memory. Thus,
successive accesses to common memory by the same processor
cannot be modeled as independent random variables.
If a process executes any of the monitor primitives, it begins
executing the code of the C-kernel, and then, depending upon
the type of monitor primitives desired and the state of the desired monitor, one of the following actions takes place.
a) The processor starts to execute the monitor procedure.
b) It wakes up a process residing in another processor to
execute the monitor procedure.
c) It waits for another process to signal it, and at that time
it continues to execute the monitor procedure.
d) It does not execute the monitor procedure nor does it
wake up another process to execute the monitor procedure.
In the first two cases, the monitor procedure is executed
either by the same processor or by another immediately following the execution of the C-kernel. In the last two cases, the
monitor procedures are not executed, and the next time the
processor accesses the common memory, it would execute the
C-kernel. Once a monitor procedure is being executed, the
processor has to execute one of the monitor primitives to exit
from the monitor. The above four cases can be condensed to
the following case: the processor first executes the C-kernel,
and then a monitor procedure, and then the C-kernel again.
Finally, it returns to the local process, as shown in Fig. 3.
We can model the memory contention problem as a closed
queueing network with appropriate service times and scheduling policies. The service time it provides can be measured in
terms of the number of common memory accesses. The number
of common memory accesses needed to execute a portion of
a monitor procedure sandwiched between two consecutive

1050

IEEE TRANSACTIONS ON COMPUTERS, VOL.

Case l: No. of Requesting Processors < m.

Monitor
Procedure

C- kernel

C-31, NO. 11, NOVEMBER 1982

tar

0-

_h
Common

Execution of
Local Code

Fig. 3. Common memory reference pattern.
-

nodel

node 3
node 2
C- kernel

Queue

Access to C-kernel by Processor 1.
Monitors by Processors 2, 3.

2, 3 - Access to

Case 2: No. of Requesting Processors > m.

tar

Monitor

Queue

23

Cluster P-Processor

time0

Memory

Not Used for
Two Cycles

Virtual Servers

Fig. 4. Queueing model for common memory interference.

a

415

1623 41 I5 62

3

time
- Access to C- kernel by Processor 1.
2- 6 - Access to Monitors by Processors 2 to 6.

Fig. 5. Individual accesses to common memory.

monitor primitives can be treated as a random variable with
an exponential distribution. The number of local memory accesses between two monitor calls can also be treated as a random variable with an exponential distribution. The number
of common memory accesses needed to execute the monitor
primitive by means of the C-kernel is assumed to have an exponential distribution.
The queueing network consists of three nodes as shown in
Fig. 4. The first node consists of n servers where n is the
number of child P-processors in the cluster. The service time
for these servers corresponds to the distribution of the number
of local memory accesses between two consecutive monitor
calls. Node 1 is of type-D [23] since the customers are delayed
independently of other customers at this service station.
The common memory can be treated as m virtual parallel
servers since effectively there can be m common memory accesses in time period tar. Also note that we cannot give more
than one common memory access to a processor in a given time
period t,r (Fig. 5). Of the m virtual servers of the common
memory, one server serves the C-kernel queue which is node
2 of the queueing network. The rest of the m- i virtual servers
serve the monitor queue and form node 3 of the queueing
network.
In the actual system, however, the server serving the Ckernel queue would serve customers in the monitor queue if
there is no customer in the C-kernel queue. Therefore, the
performance characteristics obtained by this queueing network
model gives a lower bound of the actual performance. The
upper bound of the performance can be easily obtained by
having an additional parallel server at node 3.
The scheduling policy used for nodes 2 and 3 of the queueing
network is first-come-first-served (FCFS). In the actual system, the type of scheduling used to service the monitor queue
is round robin. As we are only interested in the mean values
of the waiting time and the mean queue lengths, we can assume
an FCFS service mechanism. As long as the scheduling is independent of the service requirements of a customer, the mean
values do not change [241.
The analytical solution of the queueing network was carried
out by the recursive algorithm in [231. The results shown in

0

5

10
15
No. of P - Processors

20

25

Fig. 6. The performance with common memory interference.

Fig. 6 correspond to mean service times indicated below (values
normalized by the mean number of local memory accesses
needed to execute a block of local code sandwiched between
two consecutive monitor calls).
1) Mean percentage of common memory accesses needed
for executing the C-kernel: case 1: 2.5 percent; case 2: 5 percent.
2) Mean percentage of common memory accesses needed
for executing monitor code sandwiched between two monitor
primitives: case 1: 10 percent; case 2: 20 percent.
These cases represent the access localities 89 and 80 percent
respectively, which correspond to the experimental result obtained from Cm* [3]. The results give the lower and upper
bounds of the performance of the cluster with common
memory interference for m=3 and m=4.
2) Parent P-Processor and D-Processor Interference: To
evaluate the interference in the parent P-processor and Dprocessor, the functions in both processors should be considered. When the process residing in a child P-processor requests
the service from global resources (e.g., parent P-processor and
data hierarchy), the process sends a message to the parent
P-processor and waits for the reply. In the parent P-processor,
the message handler accepts the request and puts the request

1051

SHIN et al.: DESIGN OF HM2p

on a ready queue for service. Those services requested by the
child P-processors are classified as one class with three types
which are described as follows.
The first type is the synchronization request between processes residing in the same cluster. Since the processing time
of this service is short and any delay will seriously affect the
performance of the cluster, this request is given higher priority
and relayed to the child P-processor immediately.
The second type of request is for execution of monitor procedures residing in the parent P-processor or communication
with a processor in a different cluster. After recognizing this
request, a new process is created and inserted into the ready
queue of the parent P-processor. The processing time is longer,
but the frequency of occurrence of this request is small.
The last type of request in this service class is the data
transfer between the child P-processor and I/O devices or file
system. The parent P-processor relays the request to the Dprocessor and then to the file system or I/O device. Upon the
completion of data transfer, the D-processor informs the
concerned parent P-processor, and then the blocked child
P-processor resumes execution.
There are three other classes of services issued, respectively,
by the grandparent P-processor, the I/0 devices, and other
D-processors in the data distribution hierarchy. The job assigned by the grandparent P-processor to the parent P-processor may be a user task that needs to be allocated to one of
its offsprings, or a system supervisor task, or a communication
message. In the D-processor, besides the data transfer, it
provides the shell-like process for each I/O device and communication with other D-processors.
Considering interference at the shared resources, a multichain closed queueing network is developed to model the performance of an HM2p cluster (Fig. 7). There are four chains
with four different types of customers corresponding to the four
service classes discussed above.
In the first chain, the requests issued by the child P-processor travel around the queueing network. Note that the
model of common memory interference is also included in this
network. Let Pijr denote the branch probability from node i to
node j following the service at node i in chain r. The process
in the child P-processor (node 1) may request the use of either
common memory with probability P121 or parent processor
. In the former case, the request
(node 4) with probability
goes to node 2 (C-kernel) and node 3 (monitor) and then returns to node 1. In the latter case, the parent P-processor services the request, which then follows one of the three paths to
the next node. The path that directly returns to the child Pprocessor represents the synchronization service. The request
is sometimes fed back to the parent P-processor in the second
path in order to acquire further service, for example, the execution of the procedure or the communication with other
clusters. The third path routes the request to the D-processor
(node 5) or file system (node 6) because this request involves
the data transfer via the DMA path to or from the I/O devices
or file system. Corresponding to these paths, we define the
three branch probabilities p411, p441, and P451 . Furthermore,
after receiving the service from the D-processor, the request

Fig. 7. Queueing model for performance of the H M2p with interference
in entire hardware resource.

branch to the file system (node 6) or to the I/O devices
for the data transfer between a child P-processor and an I/O
device, (two branch probabilities P541, P561 represent these two
paths, respectively).
In the second chain, the parent P-processor accpets the request from the grandparent P-processor, and then returns the
reply to the originator when the service is completed. The
grandparent does not send another request until the previous
service is completed. The third chain represents the shell-like
process in the D-processor. The shell-like process would become active if an I/O device (node 8) issues a request. The
fourth chain describes another function of the D-processor,
namely, the communication in the data distribution hierarchy
(in which the parent D-processor is represented as node 9).
This implies that the interference due to the congestion in the
D-hierarchy is also included in this model.
To solve this queueing problem, let us assume that node i
9) has an exponentially distributed service or
(i = l, 2,
request time with mean /I,i, for a customer in chain r (r =
1, 2, 3, 4). This service/request time may be a "think" time to
make requests, an execution time, etc. For example, in node
1, the service/request time is the time interval between two
external accesses; in node 4 it becomes the process execution
time in the parent P-processor. From the branch probabilities
Pijr and the mean service/request rate azrg the relative traffic
density Pir can be obtained where Ai,r is normalized such that
Ml1= 1. By assuming the service time in each node and the
branch probabilities following each service, we can evaluate
the total processing power for various cases. For instance, a
small "think" time in the parent D-processor (node 9) represents a higher demand for file access from the D-hierarchy.
A higher branch probability P141 and P441 will indicate heavier
intercluster communications. Using the mean value algorithm
in [23], the throughput of each chain is calculated (denoted
as /Xr) The number of equivalent processors is then equal
may

,

to

3

The number of equivalent
i
(Lpp)/X
.

processors is

calculated and then plotted in Fig. 8 for the following two

cases:

case

1: ol

=

[1.0, 0.025, 0.1, 0.017, 0.027, 0.027]

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-3 1, NO. 11, NOVEMBER 1982

1052

ideal

o

6

16

a.

case

z

6

1

6

21

16

26

No. of P-Processors

Fig. 8. The performance with interference in entire hardware resource.
1.2

Case I
-----Case 2

8
to

1.0

deal case

o

a.

0.8

-

V. CONCLUSION AND DISCUSSION

a-

E

41

0.6

2
E

0.4

z

0.21*1

6

1

16
No. of P-Processors

21

26

Fig. 9. Performance degradation versus the number of P-processors.
case

2:

Xl

=

[1.0, 0.05, 0.2, 0.033, 0.053, 0.053].

For both cases
1k2 =

and m

=

Assuming that we desire at least 90 percent of the ideal
performance (i.e., when there is no interference) and the value
of m is equal to 4, we come up with the figure of 14 processors
from the performance curves in Fig. 8 (case 1), which is the
optimum number of P-processors for a single cluster. In this
case, the net cluster performance is equivalent to that of 12.6
processors.
Most of the actual processing is assumed to take place in the
leaf clusters, and the P-processors in the higher levels are thus
busy synchronizing and performing other communication
tasks. The total number of the leaf P-processors in the system
is n k -i where n is the number of P-processors in a cluster and
k is the number of levels in the entire tree-structured system.
Note that the total number of the P-processors in the system
is (nk -1 )/(n - 1). If the number of child P-processors in a
cluster is 13, then the equivalent number of the P-processors
that are busy with the computation task only is 90 percent*( 13) k; this is almost 83 percent of the entire P-processors
when k is large. On the other hand, the system provides
(nk-I - 1)/(n - 1) P-processors to perform the communication task, task allocation, and system management. These
constitute about 17 percent of the entire P-hierarchy.

[0.05, 0.5, 0.1, 5.0, 0.5, 0.6, 5.0]

4 where
I

=

[P1 1, P21, P31, P41, P51, P61]

02 = [P42, P72, P53, P83, P54, P64, P941.
The process localities used in cases 1 and 2 are the same as
those in Fig. 6. Both cases are assumed to have the same interferences from the parent P-processor and D-processor, but
the service times by shared resources are different (case 2 has
service times twice those of case 1). The normalized performances of the child P-processor for both cases are plotted in
Fig. 9.
B. The System Performance
From the above analysis, we have to arrive at the figure for
the optimum number of processors in each cluster. Since m,
the figure of merit of the common memory, can be varied
within a reasonable range, one can design the HM2p such that
the parent P-processor and D-processor become the critical
shared resources of a cluster.

We have here presented the architecture of the HM2p, the
synchronization and communication primitives, and finally
the performance of the system based on these primitives. The
HM2p is developed and analyzed based on a common feature
of most computational tasks; namely, the tasks can be decomposed into cooperating processes which are then classified
into closely coupled clusters, and these clusters hold a hierarchical relationship with each other. However, our simulation
results for the performance analysis have indicated that the
effectiveness of a multiprocessor heavily depends on the access
locality. Thus, a key issue to any successful design of multiprocessors remains to be the development of an automatic
method of decomposing a given task into interacting processes,
and then assigning them to processor modules so that a high
degree of access locality may be achieved.
As pointed out by one of the referees, it should be noted that
the size of the local memory and the common memory will also
affect both the access locality and the process allocation. (So
does the size of cluster!) The access locality is therefore an
important design parameter which is a function of the size of
cluster, local memory, and common memory.
The processing power calculation for the HM2p is useful not
only for the leaf P-processors and their associated D-processors, but for the P-processors in the upper levels of the HM2p.
Since the P-processors in the upper levels are used for the
communication and system supervision, the service times and
the branch probabilities for these P-processors are different
from those used for the leaf level. These processor modules in
the upper levels may be regarded as a tree network used to
exchange messages among cooperating processes. The communication overhead is implicitly included in the queueing
model with the branch probability P441, but needs further study
for a more refined analysis. However, the branching factor for

1053

SHIN et al.: DESIGN OF HM2p

the upper levels may differ from that for the leaf level. The
optimal values should be decided by examining the amount of
communication (which is task-dependent). The higher level
processors may become a bottleneck in the system if the
amount of communication is heavy. It is also related to the
number of the intercluster messages, which further depends
on the structure of the operating system.
The effects of software precedence have to be introduced
into our queueing models for determining the actual performance falloffs. This will be useful in determining the effects
of both software and hardware constraints in the system. We
know by intuition that the figure for the optimum number of
processors in each cluster will increase when these effects are
taken into account.
ACKNOWLEDGMENT
The authors are grateful to C. M. Krishna and the referees
for the comments and criticisms on this paper.

[15] R. C. Holt et al., Structured Concurrent Programming. Reading, MA:
Addison-Wesley, 1978.
[16] W. W. Chu, L. J. Holloway, M. T. Lan, and K. Efe, "Task allocation
in distributed data processing," Computer, vol. 13, pp. 57-69, Nov.
1980.

[171 D. M. Ritchie and K. Thompson, "The UNIX time-sharing system,"
Commun. Ass. Comput. Mach., vol. 17, pp. 365-375, July 1974.
[18] H. K. Reghbati and V. C. Hamacher, "Hardware support for concurrent

programming in loosely coupled multiprocessors," in Proc. 5th Annu.
Symp. Comput. Architecture, Apr. 1978.
[19] R. S. Fabry, "Capability-based addressing," Commun. Ass. Comput.
Mach., vol. 17, pp. 403-412, July 1974.
[201 E. Cohen and D. Jefferson, "Protection in the Hydra operating system,"
in Proc. 5th Symp. Operating Syst. Principles, Austin, TX, Nov.

1975.

[211 D. P. Bhandarker, "Analysis of memory interference in multiprocessors,"
IEEE Trans. Comput., vol. C-24, pp. 897-908, Sept. 1975.

[221 B. Parasuraman, "High-performance microprocessor architectures,"

Proc. IEEE, vol. 64, pp. 851-859, June 1976.
[23] M. Reiser and S. S. Lavenberg, "Mean value analysis of closed multichain queueing networks," J. Ass. Comput. Mach., vol. 27, pp. 313-332,
Apr. 1980.
[24] L. Kleinrock, QueueingSystem, Vol. I and 2. New York: Wiley, 1975,
1976.

REFERENCES
[1] P. H. Enslow, "Multiprocessor organization-A survey," Comput.
Surveys, vol. 9, pp. 103-129, Mar. 1977.
[2] M. Satyanarayanan, Multiprocessors: A Comparative Study. En-

giewood Cliffs, NJ: Prentice-Hall, 1980.
[3] S. H. Fuller, J. K. Ousterhout, L. Raskin, P. L. Rubinfeld, P. J. Sindhu,
and R. J. Swan, "Multi-microprocessors: An overview and working
example," Proc. IEEE, vol. 66, pp. 2f6-228, Feb. 1978.

[41 J. R. Goodman and C. H. Sequin, "Hypertree: A multiprocessor in[5]
[61
[7]

[81

[91
[10]

[II]

[12]
[131
[141

,

Kang G. Shin (S'75-M'78) was born in Choongbuk, Korea, on October 20, 1946. He received the
B.S. degree in electronics engineering from Seoul
National University, Seoul, Korea, in 1970, and

the M.S. and Ph.D. degrees in electrical engineering from Cornell University, Ithaca, NY, in 1976
and 1978, respectively.
From 1972 to 1974 he was on the Research
Staff of the Korea Institute of Science and Technology, Seoul. From September 1978 to August
1982 he was an Assistant Professor in Computer
Engineering, Department of Electrical, Computer, and Systems Engineering; Rensselaer Polytechnic Institute, Troy, NY. He has also taught short
courses in the area of computer architecture for the Computer Science Series
of the I BM Kingston Experienced Technical Education, Kingston, NY. Since
September 1982 he has been a Faculty Member in the Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor. He
is currently engaged in research and teaching in the areas of computer architecture, distributed and fault-tolerant computing, robotics and automation,
and the application of control theory to biomedical systems.
Dr. Shin is a member of Sigma Xi and Phi Kappa Phi.

terconnection topology," IEEE Trans. Comput., vol. C-30, pp. 923-933,
Dec. 1981.
S. A. Browning, "Computations on a tree of processors," in Proc. VLSI
Conf., California Inst. Technol., Pasadena, Jan. 1979.
R. J. Swan, S. H. Fuller, and D. P. Siewiork, "Cm*-A modular
multi-microprocessor," in Proc. AFIPS Conf., Nat. Comput. Conf., vol.
46, 1977, pp. 637-644.
R. J. Swan, A. Bechtolsheim, K. Lai and J. K. Ousterhout, "The implementation of Cm* multi-microprocessor," in Proc. AFIPS Conf.,
Nat. Comput. Conf., vol. 46, 1977, pp. 645-655.
A. K. Jones, R. J. Chansler, I. Durham, P. P. Feiler, and K. Schwars,
"Software management of Cm*-A distributed multiprocessor," in
Proc. AFIPS Conf., Nat. Comput. Conf., vol. 46, 1977, pp. 657-663.
J. K. Ousterhout, D. A. Scelza, and P. S. Sindhu, "Medusa: An experiment in distributed operating system structure," in Proc. 7th Annu.
. g_1
Yann-Hang Lee (S'81) received the B.S. degree in
Symp. Operating Syst. Principles, Nov. 1979, pp. 115-126.
engineering science and the M.S. degree in electriJ. A. Harris and D. R. Smith, "Hierarchical multiprocessororganizacal engineering from National Cheng Kung Unition," in Proc. 4th Annu. Symp. Comput. Architecture, Mar. 1977, pp.
versity, Taiwan, R.O.C., in 1973 and 1978, respec41-48.
R. B. Kieburtz, "A hierarchical multicomputer for problem solving by
Currently he is working towards the Ph.D. dedecomposition," in Proc. Ist Int. Conf. Distributed Comput. Syst., Oct.
gree in electrical and computer engineering at the
1979, pp. 63-71.
University of Michigan, Ann Arbor. His research
W. W. Chu, D. Lee, and B. Iffla, "A distributed system for naval data
interests include multiprocessor, performance
communication networks," in Proc. AFIPS Conf., Nat. Comput. Conf.,
evaluation, and fault-tolerant computing.
vol. 47, 1978, pp. 783-793.
C. J. Jenny, "Process partitioning in distributed systems," in NTC Dig..
Papers, 1977, pp. 31:1-31:1O.
C. A. R. Hoare, "Monitors: An operating system structuring concept," J. Sasidhar (S'80-M'81), photograph and biography not available at the time
Commun. Ass. Comput. Mach., vol. 17, no. 10, pp. 549-557, 1974.
of publication.

Optimal Scheduling of Signature Analysis for VLSI Testing
Y.-H. Lee
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598

Abstract

Signature analysis has become a popular way of
testing VLSI circuits. We present a simple algorithm
to optimally schedule the signature analyses. The
objective is to minimize the mean testing time per
VLSI circuit.

1. Introduction
VLSI circuits are tested by applying inputs to them,
and comparing the circuit output against that of a perfect
circuit. Even moderately sized circuits can require an enormous number of test inputs to ensure sufficient coverage
(i.e., the probability that a faulty unit is caught by the testing), and so it is impractical to store the entire test output
for later comparison with the desired output. For this reason, compression techniques are frequently used in practice
[2,3]. Tests are applied to the circuit in quick succession,
and the circuit output stored in compressed form. Once
the test is over, the compressed output can be examined
for signs that the circuit is malfunctioning, by looking for
the “signature” of the faults that can occur. The process
has therefore got two phases: a test-application phase and
a signature-analysis phase.
If the total number of tests is large, it makes sense to
break down the application of the test to parts, checking
the compressed output at the end of each part for circuit
malfunction. If the circuit is found to be malfunctioning,
one can dispense with the rest of the testing, thus potentially saving some test time. In this paper, we present a
simple algorithm do this, i.e., to optimally interleave testapplication and result-checking phases. The objective is to
minimize the mean testing time per VLSI circuit. An agreeable byproduct of interleaving the testing and the signature
analysis is that the probability of a fault being missed due
to aliasing is reduced. Aliasing occurs when the action of

C.M. Krishna
Electrical and Computer Engg.
University of Massachusetts
Amherst, MA 01003

compression of the test output data destroys the signature
of a fault. Clearly, the longer the test stream whose output is compressed, the greater the probability of aliasing.
By breaking up the test stream into portions and searching
for fault-signatures in the output of each of these portions
therefore tends to reduce the probability of aliasing.
In the next section, we present a simple algorithm which
shows how to optimally schedule the test-application and
the signature-analysis phases. The section is largely selfcontained so that readers who are not interested in the
mathematical underpinnings of the algorithm, but simply
want to consider using it, can stop there. In Section 3, we
present the mathematical proofs associated with the algorithm.

2. The Algorithm
We begin with specifying some notation. Let T be the
total time needed to exercise the chip with a given set of
test patterns and F ( t ) be the coverage after applying test
pattern for period t . That is, F ( t ) is the probability that
application o f t seconds’ worth of the test stream results in a
faulty chip being uncovered as faulty. Note that 0 5 F ( t ) 5

1 and F ( t ) is non-decreasing. The chips to be tested are
assumed to be independent, identical, and with a constant
probability of being faulty which is denoted by w , where
0 < w < 1. For simplicity, we denote w F ( t ) by @ ( t ) .
We assume that F ( t )is a concave and an increasing function o f t . This is not a limiting assumption. The concavity
of F ( t ) expresses the fact that the rate at which faults are
uncovered per unit time of testing is a non-increasing function of how long the system has already been under test.
This is realistic because the initial tests can catch the easyto-detect faultp.
Exponential, or a mixture of exponential, distribution

1988 International Test Conference
CH2610-4/88/0000/0443$01.OO 0 1988 IEEE

Paper 23.3
443

functions are frequently used to model the coverage function. For example, in [4], an argument is made for the use of
the hyperexponential distribution, which is a weighted sum
of exponential distributions. Mixtures of exponential distributions are both monotonically increasing and concave.
Suppose that an analysis scheme consists of N signature
analyses and each analysis has a constant overhead 7,. This
overhead consists of the time it takes to scan the compressed
output, checking it to see if it contains the signature of a
fault, and initiating the next test pattern (if any) if no fault
has been discovered yet in that circuit. The maximum possible value of N that can be used, called ,Pm,,.,is generally
determined by how much memory is available: after each
signature analysis, the compressed output has to be compared with the output of a good circuit for that sequence of
tests, and the latter has to be stored in memory.

analysis before the entire test set has been applied, there is
no need to continue to apply further tests: the circuit has
been found to be faulty already.
The objective is to minimize S, the mean test time per
circuit.
The algorithm is the following. Readers interested in
the mathematical proof of correctness will find it in Section
3. We only present in this section the case that 7, # 0:
7,= 0 corresponds to the case where the signature-analysis
time is zero. For completeness, we do consider this case in
the mathematical derivations in Section 3, and present the
algorithm there.
The Algorithm

1. Set L = N,,,,,,

Denote the test-application time between the the (i 1)'th and the i'th signature analyses as A;, where i = 1,2, ..,N
and

xzlA; = T. In addition, we define t; = E;,,

wi-1)

Aj.

Define the total test process time to be the sum of the
time for applying test patterns and the time spent for analysis. The test process will be ended either by a fault being
discovered, or the input test stream being completely applied without any fault being discovered. Thus, the total
test process time is a random variable and depends upon
when, if ever, an erroneous signature occurs and is analyzed.
With a given N and t;, the average test time can be
formulated as

tL-l

= T. Compute

iteratively (tL-2 2 tL-3 1
equation

F

-

'

,...,tl

tL--Z,tL-3

2 tl) using the implicit

= F(ti) - f(t;)[t;+1 - ti

+

7.1.

Stop at i = 1;' for which the equation yields F(tL.-l)
0. Set K' = L - L
' - 1.

2. Define

JA

5

= m u 2 0 _ < t l ~ . . . t ~ - l ~ T { J ~*( t '9l , tN-1)).

To obtain .TI;, we need to determine the arguments
t l , . . - , t ~ that
- ~ maximize JN. This is done by ex- ~functions of tN-1, and adjusting
pressing tl, . . - , t ~ as
tN-1 so that t o = 0. Use the equation
t;-1

=

+ 7,)] if i > 1

F t; - f(t;)[t;+l - t;
{ FF-l
- (( F ( t ; )- f(t;)[t;+l - t i ] )

if i = 1

In other words, start with tN = tN-l = T and keep
decreasing tN-l and calculating tl, . .,t N - 2 using the
above formula. Keep decreasing tN-l until to as given
by the above expression is zero, i.e., until

F(h)- f(tl)(tZ - tl} = 0.
These values oft;, 1 5: i 5 N, are those for which JN is
maximized. Apply a standard line-search algorithm,
e.g., Dichotomous or Fibonacci search [l],over the

The derivation is quite simple, and is as follows. T + NT,
is the time it would take to apply all the tests to the circuit,
given that there are N signature analyses interleaving the
test stream. Subtracting from this the term J N ( t 1 , . tN-1)
corrects for the fact that if a fault is detected by a signature

.

a,

Paper 23.3
444

range N E (1, ...,K*},to obtain the optimal N, called
N', for which JI; is maximized, i.e., 51;. = maz(J1; I
0<N

5 K*}.

Some numerical results are presented in Figures 1 - 3 and
Table 1. We use a hyperexponential coverage function F ( t) =

3. Mathematical Details

1 - 0.7e-' - 0.3e-'I3. As we have remarked above, such hyperexponential functions have been suggested by other authors in the past (e.g., [4]). The intuitive justification for
such functions is that faults can be classified according to
how difficult they are to detect. Each fault class will have a
different expected detection time. For example, in our example, we assume that the faults fall into two classes: one
which has a mean detection time of 1time unit, and another
with a mean detection time of 3 time units.

To minimize S, we need to determine the optimal dimension, N, and the corresponding t; values. In the following,
we shall divide the optimization problem into two parts:

Figure 1 shows how the optimal number of analyses depends on the defect probability, w , and the signature analysis time, 7.. For obvious reasons, the optimal number is a
non-decreasing function of the defect probability, and a nonincreasing function of the signature analysis time. Figure 2
shows the minimized average testing time against the number of analyses. As the number of analyses, N, increases, so
does the signature-analysis overhead. But the probability
that a fault is caught before all the tests have been applied
tends to increase with N . This would cause the test process
for a given chip to be terminated early. We therefore have
a tradeoff here, and Figure 2 illustrates it. As the number
of analyses increases from 1, the resulting increased probability of an early test termination initially more than makes
up for the increased signature-analysis time. However, after a certain point, the increase in the signature-analysis
time is greater than the expected decrease due to an early
test termination, and any further increase in the number of
analyses increases the expected testing time.

0

0

Define J N ( t 1 , ...,t N ) = ~ g ;@(t;)(t;+l-t;+r,).
"
With
a given N, solve the non-linear programming problem
Pl which is to find:

S( N ) = T

+ N r , - 51;. Solve the non-linear program-

ming problem Pa which is to find:

The above two non-linear programming problems are difficult to solve for an arbitrary coverage function. However,
the process of solving them is simplified if the coverage function is increasing and concave with respect to the test period. As we have explained in Section 2 above, this is a
realistic assumption.
Let q~ = {(tl,tz,...,t N - 1 ) I O 5 tl 5 tt.... 5 t N - 1 5
T} be the set of the feasible solutions of problem PI. Also,
let i n t ( q N ) be the interior of * N . It is clear that QN is a
non-empty convex set in EN-1. In what follows, we denote
the vector {tl,...,t l } by The dimension of this vector will
be obvious from the context.
For any vector < E q ~we, have the following theorem.

Table 1 provides an example of the placements of the
signature analysis epochs. We can see from this that the
inter-analysis intervals are very different in size: the earlier intervals are shorter than the later ones. This can be
explained from the fact that, under our assumptions, the
coverage function has a non-increasing hazard rate. Figure
3 compares the average test time afforded by the optimal
method with that test time resulting from making the interanalysis intervals equal in length. At N = 1, the average
test times are equal by definition. As N + 00 also, the
test times are equal. The reason is that when a fixed interval is divided into a very large number of subintervals, the
difference in subinterval length between the optimal placement and the equidistant-placement strategies goes to zero.
However, the optimal placement method promises - in this
example - an reduction of about 20% in the average testing
time for the optimal number of analyses, N'.

Theorem 1. If F ( t )is increasing and concavein 0 5 t

5 T,

then J N ( ~is) concave at t'E i d ( * N ) .

Proof: Consider the function @(t;)(t;+l-t;+r,). Its Hessian
matrix is negative semidefinite for any 0 5 t; 5 t;+i 5 T
when F ( t ) is increasing and concave. This implies that

+

@(t;)(t;+l
- t; r,) is concave at any t' E q ~ The
. theorem follows since the summation of concave functions is
concave. Q.E.D.
The next corollaries follow naturally due to the concavity of J N .

Corollary 1. If

t'; E i n t ( q N ) and

vJ~(fi) = 0, then 6

is the global optimal solution of the problem PI.

Paper 23.3
445

Corollary 2. If no interior point of
0 , then there exists an integer 0 < k

satisfies V J N ( ~ ' ) =
< N which defines the

t', such that
(t;,t;,...,ti-,) satisfies

global optimal solution of the problem PI,

tr = T for all k 5 i
VJk(t' ) = 0.

Proof: If

< N - 1 and

no interior point of *N satisfies

)

VJN(~'

= 0,

ing, it is easy to see that

JN(t1,

...,t;,t;+l,t;+l,....,t ~ - , <
)

....,tN-,,T).

...,t;,

Thus, there exists an integer 0 < k < N such that tf is equal to T for all k i

< <

N - 1. The second result of the Corollary follows because
- tf 7.). Q.E.D.
of the concavity of E;=;' @(t)(t;+,

+

The condition that VJ,(<) = 0 can be rewritten as a
set of equations.

F(k-1) = F(t;)-f(t;)(ti+,-ti+7,)

for 1 5 i

< N-1
(2)

where f ( t ) is the first derivative of F ( t ) . The following
Lemma is needed to prove the existence and uniqueness of

Lemma 1. If ti-, satisfies F(ti-1) = F ( t i ) - f ( t ; ) ( t ; + , ti 7;)given t; t;+l, then
is decreased and A; is in-

<

creased as t; is reduced provided that F ( t ) is increasing and
concave.
Proof Taking the derivative of F(t;-1) with respect to ti,
we have

f(t;-,)-

at,-,

ati

= 2 f ( t ; ) - f'(t;)(t;+,

- ti

+

2

5 i 5 N - 1. One possible t' is ( T ,T , ...,T ) . By decreasing
and solving F(t:-,) = F(t;)-f(t:)(t:+,-t:) iteratively,

< <

for 2 i N - 1, a new @ can be found in which ti is decreasing and ta - ti is increasing (from Lemma l). A unique
intersection of F ( t i ) / f ( t i )and
decreasing

th-,.

ti - ti exists as the result of

Thus, VJN(~?)= 0 has a unique solution,

t' E int

*N, which maximizes J N ( < ) . The second part of
the Theorem follows from the fact that 51; is increasing of

N . Q.E.D.
The intuitive explanation for this is clear: when it costs
nothing to do the signature analysis (which is what the
7,= 0

case represents), it is optimal to do it the maximum

number of times that the memory constraint (expressed
through N,.,) will allow. To obtain the optimal times
t l , t a , ...,tN,,,,,, simply use the the equation resulting from
V J N ( < ) = 0, namely
setting tN,,, = T .
7
.

ma~l<kg&,,..{kI

#

f(t;-1)=

F(t;) - f(ti)[ti+l - ti

+

7.1,

0, then there exist a finite K', K' =

fi E @ k

and
) = 0). Also, S(N)
is convex for 1 5 N 5 K' and there is an integer N',
1 5 N'
K', for which S ( N ' ) is minimized.
Proof: The existence of K is shown by the following: With
L = N,,, and tL-, = T , t; can be calculated iteratively
based on the equation F(t;-,) = F(t;)- f(t;)(t;+l-t; 7.).
The iterative calculation of t; should stop at i = L' where

<

+

(3)

F(t;)-f(t;+l)(t;+l-t;+r,)
0 or L' = 1. From Lemma 1,
it can be shown that for k 2 L - L', no interior point of *k
satisfies

<

VJk(<) =

0. On the other hand, for k

which is always negative since F ( t ) is increasing and concave. Thus, A; gets greater when t; is reduced. Q.E.D.

Theorem 2. If r, = 0, then V J N ( ~ ?
=)0 has a unique

f(t;)(t;+l- t;

+ T ~ ) ,for L - k + 1 5 i < L - 2 such that

F(tL-k) -f(t&k)(tL-k+l-

t L - k +T,) = 0 .

- L'

K' as defined as

- 1.

Next, we shall show that N' which minimizes S ( N ) is
less than or equal to K'. Note that, for all N > K', JI; =

J;.

+ (N - K * ) @ ( T ) T ,Since
.
@ ( T )= w F ( T ) < 1, S ( N ) is

increasing for all N

> K'.

Thus, N'

so-

lution, t' E int(iPN), which maximizes J N ( ~ ? )Also,
.
S(N)
is minimized at N = N,., .

< L - L',

a solution of on(<)
= 0 can be found by continuously
decreasing tL-, and iteratively solving F(t;-,) = F(t;) -

in the Theorem is equal to L

446

E @N

<

7,)

where f ' ( t ) is, of course, the second derivative of F ( t ) . The
right hand side is greater than zero when t; ti+, and F ( t )
is increasing and concave. Thus, ti-, gets smaller when t;
is reduced. In addition, with A; = t; - ti-,, we have

Paper 23.3

@ = (ti,t;, ...,tk-,)

and satisfy the set of F(t;-,) = F(t;) - f(t;)(ti+,-ti), for

Theorem 3. If

the solution of V J N ( < )= 0.

+

that F ( t l ) / f ( t l )is increasing of tl when F ( t )

is increasing and concave. Let

tk-,

the optimal solution must be in the boundary. It means
that at least one inequality of the constraints in PI is active. Let t;+, be one such inequality. Since F ( t ) is increasJN(t1,

Proof: Note

To prove that S ( N ) is convex for 1

5 K'.

< N 5 K', let us de-

fine G ( k ) = J;+(K*-k)@(T)r,.Note that G ( k ) = JK.(G
)

whereti; E g~.,
(uk,l,ukJ,...,uk,k-l) is the optimal solution

= T for all k 5 i < K*. Since JK.(~) is
concave on g p , J p ( t ) is concave on the convex hull of
{U:, zii, ...,U&}
Thus, G(k) is concave which implies S ( N )
is convex for 1 5 N 5 K’.Q.E.D.
of Jk(i?) and

References

Uk,i

From Lemma 1, Theorem 2 and Theorem 3, we obtain
the algorithm which minimizes the mean testing time: this
has been described in Section 2.

4. Discussion
We have presented a simple algorithm which minimizes
the mean testing time for VLSI circuits. By breaking up
the testing process into subintervals, and analyzing the signature at the end of each subinterval, we can abort future
tests if the circuit is found to be faulty, thus saving test
time. Subdivision of the test process also reduces the probability of aliasing, thus increasing the effective coverage of
the signature analysis process. Also, if the process is sufficiently subdivided, it may be possible to use the test results
not only to determine if the circuit is faulty or not, but to
diagnose the fault [ 6 ] .

[l] M. S. Bazaraa, C. M. Shetty, Non-Linear Programming: Theory and Algorithms, New York: John Wiley,
1979.
[2] N. Benowitz et al., “An Advanced Fault Isolation System for Digital Logic,” IEEE Trans. on Computers,
Vol. C-24, No. 5, May 1975, pp. 489-497.

[3]R. David, “Feedback Shift Register Testing,” Dig. Papers, 8th Annual Int ’1 Conf. Fault Tolerant Computing,
1978.
[4]E.J. McCluskey, S. Makar, S. Mourad, and K.D. Wagner, “Probability Models for Pseudorandom Test Sequences,” Proc. IEEE Int’l Test Conference, 1987, pp.
471-479.
[5]J. E. Smith, “Measures of the Effectiveness of Fault
Signature Analysis,” IEEE Trans. on Computers, Vol.
C-29, No. 6, June 1980, pp. 510-514.
[6] J.A. Waicukauski, V.P. Gupta, and S.T. Patel, “Diagnosis of BIST Failures by PPSFP Simulation,” Int? Test
Conference, 1987,pp. 480-485.

Paper 23.3
447

N= I
N= 2
N= 3

10.0
2.43
1.82

(a).

10.0
3.77

10.0

[Ti)= 1.0 - 0.7e-' -0.3e-"',

7'= 10.0, T,

= 0.01

1,

1

2

3

4

5

6

7

8

(b). ['(I)= I.0-0.7~-'-0.8~-"', 7'= 10.0, 7,=0.1

Table I . The optimal schedules of signature analyses

Paper 23.3
448

9

10

T-10.0
F(t )-1-0.7e-t-0.3e-t/3
.N,

0

I

0

-40

I

0.2

I

I

I

I

0.4

0.6

I

I

0.8

I

I
1.o

defect probability o

FIGURE1, OPTIMAL NUBER

OF

ANALYSES

Paper 23.3
449

0-0.45
T-10.0
F(t )-1-0.7e-t-0.3e-t’3
N
,,
-40

._
E
f.

0

10

20

number of analyses

50

(NI

FIGURE2, MINIMIZEDAVERAGE TESTING TIME

Paper 23.3
450

40

-

0-0.45
T-10.0
F(t 1- 1-0 .7e4 -0.Je-V3
“ e x -40

I

I

10

I

I

: equal intervals
: optimal placement

I

20

I

I

30

I
40

number of analyses ( N I

FI GURE 3, COMPAR I SON OF CHECKPOINT PLACEMENT STRATEGI ES

Paper 23.3
451

Real-Time Systems, 24, 303±317, 2003
# 2003 Kluwer Academic Publishers. Manufactured in The Netherlands.

Voltage-Clock Scaling for Low Energy Consumption
in Fixed-Priority Real-Time Systems
YANN-HANG LEE
yhlee@asu.edu
Computer Science and Engineering Department, P.O. Box 875406, Arizona State University, Tempe,
AZ 85287-5406
C. M. KRISHNA
krisha@ecs.umass.edu
Electrical and Computer Engineering Department, University of Massachusetts, Amherst, MA, 01003
Abstract. Power and energy constraints are becoming increasingly prevalent in real-time embedded systems.
Voltage-scaling is a promising technique to reduce energy and power consumption: clock speed tends to
decrease linearly with supply voltage while power consumption goes down quadratically. We therefore have a
tradeoff between the energy consumption of a task and the speed with which it can be completed. The timing
constraints associated with real-time tasks can be used to resolve this tradeoff.
In this paper, we present two algorithms for voltage-scaling. Assuming that a processor can operate in one of
two modes: high voltage and low voltage, we show how to schedule the voltage settings so that deadlines are
met while reducing the total energy consumed. We show that signi®cant reductions can be made in energy
consumption.
Keywords: voltage-scaling, scheduling, low-power designs, real-time embedded systems

1.

Introduction

Power and energy constraints are more pervasive today than ever before in real-time
embedded systems. Promising power reduction techniques exist at the physical, circuit,
architecture, and algorithm levels (Yeap, 1998; Raghunathan et al., 1998). In this paper,
we present voltage-scaling techniques at the system level, to reduce energy consumption
while still meeting all task deadlines.
Voltage-scaling has attracted increasing attention over the past several years (Chang
and Pedram, 1997; Weister et al., 1994). The basis for energy savings from voltagescaling is that the power consumption in CMOS is a quadratic function of the voltage, in
particular,
2
Pdynamic  CL NSW VDD
f

where CL is the CMOS circuit output load capacitance, NSW is the average number of
switching activities per clock cycle, VDD is the supply voltage, and f is the clock
frequency (Yeap, 1998; Raghunathan et al., 1998). The side effects of voltage reduction

304

LEE AND KRISHNA

include reduced noise immunity, additional level converters, and most importantly,
increased delay in circuit elements according to the expression:
delay  K

VDD
VDD

2

VT 

where K is a constant and VT is the threshold voltage of transistors.
Since the power consumption goes up quadratically with the supply voltage and the
delay increases only linearly, it makes sense to run processors at the lowest voltage at
which the tasks meet their deadlines and the noise margins are adequate.
To make the discussion concrete, consider the ARM7D processor. It can run at 33 MHz
(30.6 MIPS) at 5 V and at 20 MHz (19.1 MIPS) at 3.3 V (ARM Documentation, 1995).
The power consumptions at these voltages are 185 MIPS/W and 579 MIPS/W,
respectively. Another example is the Motorola PowerPC 860 processor (MPC860,
1998). It can be run at 50 MHz on 3.3 V, or at 25 MHz on 2.4 V.
The problem of voltage-scaling for power reduction has been studied in workstations,
notebook computers, and PDA devices (Weister et al., 1994; Pering et al., 1998; Govil et
al., 1995). A voltage-scaling decision for the next execution interval can be based on the
processor utilization over preceding intervals or anticipated execution demands. In digital
signal processing applications where computational loads are predictable, data-driven
approaches (e.g., by considering the amount of data in input buffers) can be used to pick
the appropriate supply voltage (Amirtharajah and Chandrakasan, 1997).
For some real-time systems, such as spaceborne platforms, power consumption and
heat dissipation can be a crucial issue. For these systems, voltage-scaling must be applied
subject to the constraint that no hard deadlines of critical tasks are missed. Some
preliminary work has been done in this area. For example, Ishihara and Yasuura
considered the requirement of completing a set of tasks within a ®xed interval and the
amount of switching activities for each task (Ishihara and Yasuura, 1998). They used an
integer programming approach to schedule voltage levels and showed that, if only a small
number of voltage levels are available, two voltage levels are suf®cient to minimize the
energy consumption under any given timing constraints. Yao et al. have devised an
earliest deadline ®rst (EDF)-based static voltage-switching algorithm for independent
tasks with arbitrary arrival times and deadlines (Yao et al., 1995). For non-preemptive
systems and time-based scheduling, a branch-and-bound heuristic was proposed in (Hong
et al., 1998) to determine resource allocation and task assignment for power
minimization.
In this paper, we study the voltage-clock scheduling problem in real-time systems with
®xed-priority scheduling. We assume that the processor can run in one of two operational
modes: high voltage and fast clock (HV mode) and low voltage and slow clock (LV
mode). Such a two-level system may be easier to implement than mechanisms requiring
continuous variability in voltage.
We propose two scheduling approaches in this paper. The ®rst statically assigns each
task to either HV or LV mode. This is the default mode in which that task will be run.
The second approach dynamically switches operational modes based on the state of the
system. For example, if a task ®nishes before its worst-case execution time, the

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

305

reduced workload that results may permit more of the workload to be executed in LV
mode.
The rest of this paper is organized as follows. In Section 2, we present the system
model and list the assumptions we make. In Section 3, we describe our static and dynamic
approaches. Numerical results are provided in Section 4, which show that our algorithms
make possible signi®cant savings in energy consumption. The paper concludes with a
brief discussion in Section 5.

2.

System Model

We consider the scheduling of tasks running on a single processor. This processor may be
part of a multiprocessor, or may stand alone. In the former case, a higher-scope algorithm
would have carried out task assignments to the various processors, and the voltagescaling is done separately on each processor, based on the tasks assigned to it.
There are two voltage levels at which the processor can be operated: HV and LV (as
described in the previous section). The clock rate at HV is a times the rate at LV, where
a  1.
When the processor switches voltage level, there will be a switching interval, ts , for
the voltage regulator and the PLL clock generator to complete the mode change.
During this interval, the processor cannot execute its workload. The switching interval
is of the order of microseconds since the frequency of digital controllers for pulsewidth modulation dc±dc converters can reach 500 KHz (Arshak and Almukhtar, 1999;
Mangat et al., 1998). Although ts is much smaller than task execution times, a voltagescaling algorithm must account for this type of switching overhead. In the following,
we shall devise switching algorithms ®rst and then incorporate the overhead ts in the
algorithms.
The task-scheduling algorithm is assumed to be rate-monotonic (RM) or deadlinemonotonic (DM) (Liu and Layland, 1973; Audsley et al., 1991; Krishna and Shin, 1997).
There are n tasks in all, t1 ; t2 ; . . . ; and tn , which are numbered in decreasing priority
order. That is, ti has greater priority than tj if i 5 j. Tasks may be either periodic or
sporadic. Let Ti denote the minimum inter-arrival time between two consecutive
instances of ti , and Ci be the worst-case execution time of ti when the processor is
running in LV-mode. The worst-case processor utilization under LV-mode is therefore
given by r  Sni 1 Ci =Ti. Each task, ti , has a deadline, Di relative to its release time. We
assume Di , where Ci  Di  Ti .
To reduce energy consumption while ensuring that no deadlines are missed, the system
must control the voltage level appropriately. Energy consumption is minimized by
maximizing the time for which the processor is in LV mode.
We will, in our energy model, assume that the processor consumes negligible power
when it is idle. In many modern processors, there are sleep, doze, or power-down modes,
which reduce the power consumption of idling processors to very low levels (MPC860,
1998).

306
3.

LEE AND KRISHNA

Voltage-Clock Scaling Algorithms

We begin by presenting a static algorithm which assigns a voltage mode to each task: that
task will always be run at that mode, regardless of the system state. Then, we present a
more complex and better algorithm, which adjusts voltage levels dynamically to account
for time that may have been released by tasks which complete before consuming their
worst-case execution time. This dynamic approach can also eliminate unnecessary
execution of HV-mode under various task arriving phases.

3.1.

Static Scheduling

Let H and L denote the tasks that are run in HV- and LV-mode, respectively. First, we
consider task sets composed exclusively of tasks where Di  Ti . To reduce the energy
consumed, we must ®nd task sets H and L such that:
*

H [ L  ft1 ; t2 ; . . . ; tn g

*

H\L1

*

Si [ H Ci =Ti is minimized

subject to the well-known suf®cient constraint for the schedulability of periodic tasks
under rate-monotonic scheduling, i.e.,
1 X Ci X Ci

 n21=n 1
a i [ H Ti i [ L Ti
Given that r  Sni 1 Cj =Tj , the problem can be formulated as the selection of a subset
H(ft1 ; t2 ; . . . ; tn g such that Si [ H Cj =Tj is minimized and is at least
ar n2I=n 1=a 1. This optimization problem can be treated equivalently as
the decision problem of the subset sum which is NP-complete. Consequently, ef®cient
search techniques should be employed to ®nd a solution if n is large.
If Di  Ti , we assume that tasks are scheduled according to the deadline-monotonic
scheduling algorithm (Lehoczky et al., 1989; Lehoczky, 1990). De®ne
X
X
Wi t 
C
dt=T
e

1=a
C dt=Tj e
j
j
j  i; j [ L
j  i; j [ H j
which is the worst-case cumulative execution demand made over the interval 0; t by
tasks with priority at least equal to that of ti . Then, task ti is schedulable if there exists
t [ Fi  flTj j j  1; 2; . . . ; i; l  1; 2; . . . ; Di =Tj g [ fDi g
such that
Wi t  t

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

307

Hence, we partition the sets into H and L such that the utilization in HV-mode is
minimized subject to the constraint Wi t  t. Note that the complexity of the
schedulability test based on time-demand analysis is On2 Tmax =Tmin  for a set of n
tasks, where Tmax and Tmin are the maximal and the minimal task periods, respectively.
This suggests that the partition of the task set into H and L subsets based on the timedemand analysis is also NP-complete.
We have a simple branch-and-bound algorithm to assign tasks to the sets H and L. The
basic philosophy is to start with everything in L, and move tasks to H if the current
assignment does not meet all deadlines, and if the utilization is less than the existing
bound of a schedulable assignment. If the new assignment comes up with a lower
utilization with all tasks being schedulable, this bound is updated. If the utilization is
greater than the existing bound, then at least one task assigned to H must be moved to L
and a branch into a new assignment path initiated.
It is easy to install this static approach in a real-time operating system. Once the
operational mode (HV or LV) for each task is determined and attached as one of the task
attributes in the task control block, at each context switch, the dispatch mechanism of the
real-time operating system can set the voltage appropriately. Thus, the change of the
voltage-clock setting becomes a part of the context switching, and the switching interval,
ts can be included in the overhead of the context switch.
3.2.

Dynamic Scheduling

Task execution times may be signi®cantly less than their worst-case estimates. When a
task which was scheduled to be run at low voltage completes execution well before its
worst-case time, there is some slack time which can potentially be used by running part or
all of an H-task at low voltage, without endangering any deadlines. In addition, several
higher priority tasks must be executed in HV-mode such that lower priority tasks can
meet their deadlines under the worst arriving phases, such as at a critical instance. This
HV-mode execution would not be always required for other dissimilar arriving phases.
These facts are exploited in our dynamic scheduling algorithm.
We begin our description of that algorithm by introducing some notation.
1.

O is the set of all tasks which would miss their deadlines if all tasks ran to their
worst-case execution times in LV-mode.

2.

BCi denotes the interval over which the processor is continuously busy, and over
which only tasks with priority equal to, or greater than, ti , execute. BCi is the level-i
busy cycle.

3.

BC0i denotes a BCi which ends with the completion of task ti .

4.

H and L are the set of tasks slated to run in HV- and LV-mode, respectively.

5.

C0i is the LV-mode execution time of ti if ti [ L and 0 otherwise.

308

LEE AND KRISHNA

We say that BCi or BC0i is active at any time t if t [ BCi or t [ BC0i , respectively. We use a
greedy approach which seeks to run the processor in LV-mode whenever this can be done
while still guaranteeing that all hard deadlines can be met.
The algorithm is shown in Figure 1. To begin with, we compute O based on the
necessary and suf®cient condition for schedulability (assuming that tasks run to their
worst-case times). Then, L and H are obtained through a search technique, such as the
branch-and-bound algorithm. All this is done of¯ine.
During execution, the algorithm maintains Ri t, which is the LV-mode time associated
with BCi . If BCi is inactive at time t, Ri t  0. Otherwise, we follow these rules:
1.

If BCi becomes active at time t, Ri t  C0i .

2.

If task tj arrives at t and j 5 i, then Ri t  Ri t  C0j .

3.

The time that the processor runs in LV-mode is continually subtracted from Ri t for
all active BCi .

Figure 1. Dynamic scheduling algorithm for voltage-clock scaling.

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

309

To understand how this algorithm works, consider a simple example. Suppose we have
a two-task system, consisting of t1 and t2 . Suppose task t1 arrives at time t1 and that task
t2 was in the system immediately prior to t1 . Suppose t1 [ L and t2 [ H. Clearly, BC1
will be activated at time t1 . Assume that R2 t1   0. At t1 , the algorithm will set
R1 t1   R2 t1   C1 . t1 will preempt t2 and execute in LV-mode. Suppose t1 ®nishes in
C1 x time, i.e., it takes x seconds less than its worst case. When t1 departs at time
t0  t1  Ci x, we will have R2 t0   C1 C1 x  x. For this remaining x seconds
of time, t2 can be run in LV-mode, although t2 [ H.
THEOREM The dynamic algorithm will not cause any deadlines to be missed.
Proof: It is obviously suf®cient to consider just tasks in O, since all other tasks are
schedulable even if the entire task set runs at low voltage. Consider a given version of
task ti [ O and the processing time during a level-i busy cycle BC0i that begins at t0 and
ends with the completion of ti at time t0 . The total increment to Ri t will be the sum of
the WCET Cj of arriving tasks tj in BC0i where j [ L and j 5 i, and C0i . Since the processor
is continuously busy throughout the interval spanned by BC0i, the completion time of ti , t0 ,
must be the smallest t > t0 which satis®es the equation
!
X
1 X
t t0
mk Ck 
m C 0
a k[H k k
k[L
where mk is the number of task tk completions in BC0i . Consider now the execution
sequence according to a task-based voltage-clock scaling with partitions L and H starting
from t0 . If tk is completed at time t00 and meets its deadline, then we must have
!
X
X
1
m00k Ck 
m00 C  e
t00 t0
a k[H k k
k[L
where m00k is the number of task tk completed in t0 ; t00  and e is the processor idle time
during t0 ; t00 . This implies that t0  t00 e under the dynamic algorithm, and hence the
version of ti meets its deadline.
j
In Figure 2, we show an example of dynamic voltage-clock scaling. Consider an example
system that can speed up by a factor a  1.5 if running in HV-mode. There are three realtime tasks, t1 , t2 , and t3 . The WCET's of the tasks in LV-mode are 3, 6, and 12,
respectively. Also, all tasks arrive periodically with periods 9, 20, and 32, and have their
deadlines equal to the periods. If all tasks are running in LV-mode, then t3 cannot meet its
deadline in a critical instance, i.e., t3 [ O. A task-based static scheduling, as described in
the above subsection, determines that H  ft2 g and L  ft1 ; t3 g. Thus, t3 becomes
schedulable and the system consumes the least amount of energy.
In Figure 2(b), we plot the function R3 t with the assumption that task t3 has an arrival
phase of 15. According to the algorithm, we add t3 's LV-mode execution duration of 12 at
the beginning of every BCi, i.e., at the arrival instances of task t1 , t2 , or t3 , while the
processor is idle. If task t3 does arrive during this busy cycle, its LV-mode execution time

310

LEE AND KRISHNA

Figure 2. The execution sequence when task t3 has an arrival phase of 15 (a) HV-mode execution delayed (b)
R3 t.

has already been included in R3 t. On the other hand, if a BCi ends without a completion
of task t3 , no deadline can be missed since both tasks t1 and t2 are schedulable in LVmode. As a result, the processor switches voltage-clock modes based on the remaining
LV-mode execution duration as shown in Figure 2(a). The ®gure discloses that, when a
busy cycle is short, we can save possible HV-mode executions by borrowing the LV-mode
execution duration at the beginning of the busy cycle.
Since the number of voltage switches per task under the dynamic algorithm is bounded
above by 2, the total voltage-switching overhead associated with that task is bounded by
2ts . If ts is not negligible, this time must be taken into account when carrying out the
worst-case schedulability analysis (by simply adding 2ts to the execution time of each
task iteration).

4.

Numerical Results

In this section, we show that the dynamic voltage/clock-scaling algorithm is effective in
greatly reducing energy consumption.
To create a test case, we ®rst generate 10 random task periods in the range of 100 to
1000, and assign the task deadlines equal to their periods. The WCET's of the tasks are
chosen in order to have a ®xed utilization r which is set between 0.9 and 0.7  a, where
a  2.0 and 1.5. The real task execution time is then selected randomly such that the mean
execution time is in the range of 0.6  WCET to 1.0  WCET. r is therefore the worst-case
utilization at low voltage, and the actual utilizations will be lower when the processor is
running at HV-mode. A test case may be dropped if the time-demand based
schedulability test shows that it is scheduable in LV-mode or unschedulable in HVmode. Otherwise, we compute the partitions of L0 and H0 using a branch and bound search
algorithm, and perform a simulation for the dynamic voltage-clock scheduling.

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

311

Figure 3. The percentage of execution time in HV-mode for the simulated cases (solid lines: static task-based
scheduling; dashed lines: dynamic scheduling) (a) a  1.5 (b) a  2.0.

In Figure 3, we show the evaluation results for variant utilization when a  2.0 and 1.5.
The curves with solid lines are the percentage measures from the static assignment
method, while the dashed lines are based on the dynamic voltage-clock scheduling. If we
run all tasks in HV-mode and keep the processor in idle mode whenever it is not busy,
then the fraction of the time in HV-mode is in the range of 0.27 (i.e., 0.45  0.6) to 0.7
when a  2.0. This is much higher than the measures obtained from the two proposed
methods. For instance, when r  1.0 and the mean task execution time is 60% of the
WCET, the utilization in HV-mode is 0.3 if all tasks are done in HV-mode. With the static

312

LEE AND KRISHNA

method, the processor only spends 8% of the time in HV-mode. Remarkably, the dynamic
method can further reduce this percentage to 1.8%.
The curves in Figures 3(a) and (b) show many interesting phenomena. With the static
mode assignment, the measured percentage in HV-mode grows linearly with utilization.
The dynamic method is very effective when the utilization is low. This is due to the fact
that many short busy cycles may end during the borrowed LV-mode execution duration
under the greedy approach. Conversely, when the utilization is high and all tasks take the
execution time equal to their WCET's, the dynamic approach cannot gain much. In some
cases, it is slightly worse than the static approach since the selected partitions L and H are
subject to additional constraints and may not be optimal for energy reduction. In addition,
we should notice the difference between static assignment and dynamic scheduling when
the utilization equals 0.7  a. The difference is quite substantial for a  1.5, but is much
smaller for a  2.0. This change is caused by the relatively inef®cient processing speed in
LV-mode. In other words, if a  1.5, we have the processing speed in LV-mode equivalent
to 66% of the speed in HV-mode. When the ratio is reduced to 50% for a  2.0, the
execution in LV-mode at the early stage of each busy cycle cannot be effective in
completing the tasks which arrived at the beginning of the cycle. Thus, the dynamic
approach does not save much when the utilization is high.
In Figure 4, we plot the percentage reduction of the time in HV-mode under the
dynamic voltage-clock scheduling over the static mode assignment. For instance, with
a  1.5 and r  0.99, the percentages of the time in HV-mode under the static and
dynamic scheduling methods are 10.6% and 3.9%, respectively. These percentages
indicate that the reduction of the time in HV-mode reaches (0.106±0.39)/0.106  63%.
The curves again con®rm the advantage of dynamic voltage-clock scheduling when task

Figure 4. The percentage reduction in the time in HV-mode under dynamic voltage-clock scheduling.

313

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

Table 1. Estimated power consumption for the simulated task sets in ARM7D processor.
ARM7D
20 MHz, 3.3 V, 19.1 MIPS, 0.033 W
33 MHz, 5 V, 30.6 MIPS, 0.165 W
1.5 ( & 30.6 MIPS/19.1 MIPS)

LV-mode
HV-mode
a
Worst-case util. at HV-mode
Worst-case util. at LV-mode

0.62
0.93

0.66
0.99

Scheduling method

None

Static

Dynamic

None

Static

Dynamic

HV-mode execution
LV-mode execution
Idle state

49.6%
0
50.4%

11%
57.9%
31.1%

3.1%
69.8%
27.1%

52.8%
0
47.2%

20.3%
48.8%
31.9%

9.8%
64.5%
25.7%

Av. power consumption
% of the reduction

0.082 W
Ð

0.037 W
54.5%

0.028 W
65.6%

0.087 W
Ð

0.050 W
43.1%

0.037 W
57%

execution time is much smaller than the WCET and when a is small. Notice that the gains
drop quickly at high utilization. This is due to the fact that busy cycles get much longer if
the utilization is high.
We can use the electrical characteristics data of ARM7D and PowerPC 860 (ARM
Documentation, 1995; MPC860 PowerPC Hardware Speci®cation, 1998) to illustrate the
reduction of power consumption under the proposed methods. Using the simulated task
sets, the estimated power consumption are shown in Tables 1 and 2 where the mean
execution time is 0.8  WCET and the values of a are assumed to be 1.5 and 2.0, for
ARM7D and 860 processors, respectively. In addition to the average power consumption
for the proposed methods, we also show the result of running all the tasks in HV-mode.
When the processor is not busy, it is put in the idle state and consumes no power at all.
The resultant power consumption estimations attest to the signi®cant reduction of the
Table 2. Estimated power consumption for the simulated task sets in PowerPC 860 processor.
PowerPC 860
25 MHz, 2.4 V (internal), 0.241 W
50 MHz, 3.3 V, 1.3 W
2.0 (  50 MHz/25 MHz)

LV-mode
HV-mode
a
Worst-case util. at HV-mode
Worst-case util. at LV-mode

0.5
1.0

0.65
1.3

Scheduling method

None

Static

Dynamic

None

Static

Dynamic

HV-mode execution
LV-mode execution
Idle state

40%
0
60%

10.6%
58.8%
30.6%

3.9%
72.2%
23.9%

52%
0
48%

34.9%
34.2%
30.9%

31.8%
40.4%
27.8%

Av. power consumption
% of the reduction

0.52 W
Ð

0.279 W
46.2%

0.224 W
56.8%

0.676 W
Ð

0.536 W
20.6%

0.510 W
24.4%

314

LEE AND KRISHNA

proposed two scaling methods.1 For instance, when a  1.5 and r  0.93, the average
power consumption can be reduced by a factor of 65% (i.e. from 0.086 to 0.028 W).
Concerning the number of mode switches under the static and dynamic voltage-clock
scheduling algorithms, we show the average number of switches per arriving task in
Figure 5 for the simulation cases of AMD7D and PowerPC 860. The curves, none_total,

Figure 5. Average numbers of mode switches under none-, static- and dynamic-scheduling algorithms.
(a) AMD7D with a  1.5 (b) PowerPC 860 with a  2.0.

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

315

in the ®gures indicate the average number of switches between idle mode and execution
mode when all tasks are running in HV-mode. As the utilization increases, the processor
will likely be kept in busy state. This leads to a decrease in the number of mode switches.
For dynamic and static scheduling algorithms, the numbers of switches between idle
mode and execution mode (either HV- or LV-mode) are depicted in the curves,
dynamic_idle and static_idle, respectively. As the dynamic scheme uses LV-mode as
much as possible, the processor would not enter idle mode as often as the case under the
static scheme. Adding the switches between HV-mode and LV-mode, the total numbers of
switches under the two scheduling algorithms are drawn in the curves, dynamic_total and
static_total. The difference further illustrates that the dynamic approach eliminates many
unnecessary switches from LV-mode to HV-mode. In most simulation cases, the dynamic
approach has the minimal number of switches among all three schemes. When the
utilization is high and the LV-mode execution is slow (in Figure 5(b), the clock speed in
LV-mode is one-half of that in HV-mode), the dynamic approach cannot bene®t from the
temporary use of the LV-mode execution duration at the beginning of the busy cycle and
degenerates into the same characteristics of the static approach.

5.

Conclusion

In this paper, we investigate the voltage-clock scaling problem in real-time embedded
systems. Power and energy consumption has always been an important constraint in many
real-time applications. However, when the energy consumption is reduced by lowering
the operating voltage and the clock rate, task executions will be delayed and may not be
able to meet their deadlines. This power-delay tradeoff can be considered to an
optimization problem of minimizing energy consumption subject to schedulability
constraints.
We are currently extending the proposed methods to address different task and system
models. For instance, task dependency may exist due to resource con¯ict, synchronization, and data communication. The system can consist of multiple CPU and DSP
processors for task execution. For disk-based real-time systems, a different power
consumption model for the disk drive must be incorporated that considers the spindle
motor, arm movement, and various mechanical delay components. The scheduling
approaches devised in this paper can be used to de®ne a promising base for more
complicated task and architecture models.

Acknowledgment
This work was presented in part at RTCSA'99, Hong Kong, December 1999 and a brief
version of this work was published in the conference proceedings of RTCSA'99 by the
IEEE Computer Society Press. The work reported in this paper is supported in part by
NSF under Grants EIA-0102539 and EIA-0102696.

316

LEE AND KRISHNA

Notes
1. It is our intention to show the reduction of power consumption with real electrical characteristics data. By no
means is the comparison done between the two processors. In fact, PowerPC860 is a chip integrated with a
PowerPC core, a communication processor, on-chip caches, memory controller, and many interfaces for
networking and peripherals.

References
Amirtharajah, R., and Chandrakasan, A. 1997. Self-powered low power signal processing. IEEE Symposium on
VLSI Circuits, pp. 25±26.
Arshak, K. I., and Almukhtar, B. 1999. The design and development of a novel ¯yback planar transformer for
high frequency switch mode DC-DC converter applications-Part 1. 22nd IEEE International Conference on
Microelectronics, pp. 505±508.
Audsley, N., Burns, A., Richardson, M., and Wellings, A. 1991. Hard real-time scheduling: the deadlinemonotonic approach. In Eighth IEEE Workshop on Real-Time Operating Systems and Software, pp. 133±137.
Chang, J.-M., and Pedram, M. 1997. Energy minimization using multiple supply voltages. IEEE Transactions on
VLSI Systems 5(4): 436±443.
Govil, K., Chan, E., and Wasserman, H. 1995. Comparing algorithms for dynamic speed-setting of a low-power
CPU. In Proceedings of the 1st International Conference on Mobile Computing and Networking, MOBICOM
1995, November, pp. 13±25.
Hong, I., Kirovski, D., Qu, G., Potkonjak, M., and Srivastava, M. B. 1998. Power optimization of variable
voltage core-based systems. In ProceedingsÐDesign Automation Conference, pp. 176±181.
Ishihara, T., and Yasuura, H. 1998. Voltage scheduling problem for dynamically variable voltage processors.
International Symposium on Low Power Electronics and Design, August, pp. 197±202.
Krishna, C. M., and Shin, K. G. 1997. Real-time Systems. New York: McGraw-Hill.
Lehoczky, J., Sha, L., and Ding, Y. 1989. The rate-monotonic scheduling algorithm: exact characteristics and
average case behavior. In Proceedings of the IEEE Real-Time Systems Symposium, December, pp. 166±171.
Lehoczky, J. 1990. Fixed priority scheduling for periodic task sets with arbitrary deadlines. In Proceedings of the
IEEE Real-Time Systems Symposium, December, pp. 201±209.
Liu, C. L., and Layland, J. W. 1973. Scheduling algorithms for multiprogramming in hard real time environment.
Journal of the Association for Computing Machinery 20(1): 46±61.
Mangat, S., Xi, Y., Jain, P. K., and Liu, Y. F. 1998. A modi®ed asymmetrical pulse-width-modulated resonant
DC/DC converter topology. 29th Annual IEEE Power Electronics Specialists Conference, pp. 662±668.
MPC860 PowerPC Hardware Speci®cation 1998. MPC860EC/D, Motorola, December.
Pering, T., Burd, T., and Brodersen, R. 1998. The simulation and evaluation of dynamic voltage scaling
algorithms. In International Symposium on Low Power Electronics and Design, August, pp. 76±81.
Raghunathan, A., Jha, N., and Dey, S. 1998. High-level Power Analysis and Optimization. Boston, MA: Kluwer
Academic Publishers.
Weiser, M., Wilch, B., Demers, A., and Shenker, S. 1994. Scheduling for reduced CPU energy. In Proceedings
of the 1st USENIX A Static Task-Orient Scheduling Symposium on Operating Systems Design and
Implementation, November, pp. 13±23.
Yeap, G. K. 1998. Practical Low Power Digital VLSI Design. Boston, MA: Kluwer Academic Publishers.
Yao, F., Demers, A., and Shenker, A. 1995. A scheduling model for reduced CPU energy. IEEE Foundations of
Computer Science, pp. 374±382.
Introduction to Thumb, Version 2.0, March 1995, ARM DVI-0001A, Advanced RISC Machines Ltd.

VOLTAGE-CLOCK SCALING FOR LOW ENERGY CONSUMPTION

317

Yann-Hang Lee received his Ph.D. degree in
Computer, Information, and Control Engineering
from the University of Michigan, Ann Arbor, MI,
in 1984, and is currently a professor in the Computer
Science and Engineering Department, Arizona State
University, Tempe, AZ. His research interests include
real-time systems, communication networks, computer architecture, and performance evaluation.

C. M. Krishna received his Ph.D. from the
University of Michigan in 1984. Since then, he has
been on the faculty of the Department of Electrical
Engineering, University of Massachusetts, Amherst,
MA. He has research interests in real-time systems,
fault-tolerance, and distributed computing.

Teaching Microprocessor Systems Design Using a SoC and Embedded Linux
Platform 1
Yann-Hang Lee and Aung Oo
Department of Computer Science & Engineering
Arizona State University
yhlee@asu.edu, aung.oo@asu.edu
Abstract
In traditional microprocessor systems design courses,
students learn to develop assembly language programs to
control peripherals, handle interrupts, and perform I/O
operations. We adopt a 32-bit StrongARM architecture on
the Motorola MX1ADS board with Embedded Linux to
present a modern microprocessor system design course.
With this new platform, we use a high-level language to
develop projects that accelerate the students’ learning
curve. Embedded Linux also provides the necessary
flexibility and tool set required for students to debug their
own projects. Our students' responded very positively to
this change. They were excited about the renewed course
structure, the updated learning environment, and the
challenging projects.

1. Introduction
Embedded systems are designed for dedicated
applications running in control systems. The unique
feature of such systems is the capability to perform timely
and predictable operations in response to concurrent
requests arriving from the external environment. To create
an effective embedded system one must properly employ
the appropriate system architecture, hardware/software
interfaces, peripheral devices, and software components.
Currently, embedded systems companies are facing with a
shortage of engineers having the appropriate skills to
respond to market opportunities [8]. Therefore, embedded
software engineering has emerged as a key element for
curriculums in Computer Science, Computer Engineering,
and Electrical Engineering at universities throughout the
world.
To teach the subject of software/hardware integration
and I/O interfaces, undergraduate computer science and
engineering programs incorporate a microprocessor
1

This course development project is supported in part by NSF
Educational Innovation Grant EIA-0122600, the Consortium for
Embedded and Inter-Networking Technologies (CEINT), and
Motorola University Program.

Copyright held by author

system and applications course. In the course, students
develop assembly language programs to control
peripherals, handle interrupts, and perform I/O operations.
Then students perform experiments with a target singleboard microprocessor system integrated with typical
interface circuits such as programmable timers, serial
ports and parallel ports. Unfortunately, this approach fails
to keep pace with industry technology. This lag is
prompted by the advent of rapid prototyping development
of microelectronic systems that includes:
a. SoC-based platforms for embedded applications:
The system-on-a-chip (SoC) devices have made great
progress along with the ever-growing number of
transistors that can be integrated on a chip.
b. Abundant I/O interfaces: Besides programmable
timers, serial ports, and parallel ports, there are several
new I/O standards designed for human interfaces,
multimedia,
networking,
and
inter-IC/device
communication.
c. I/O programming with high-level languages: For
software portability, modularity, and readability, highlevel programming languages have been used in all levels
of software development. An appropriate use of
programming languages and software structures often
leads to reusable embedded software.
Our traditional computer engineering curriculum also
taught relatively outdated techniques in the subjects of
software/hardware integration and interface. The
“Microprocessor System Design” course emphasizes
assembly language programming and exercises only a
limited number of I/O interfaces. The course falls short in
addressing state-of-the-art interfacing technology and
emerging applications.
In our curriculum development project sponsored by
the NSF EIA program, we redesigned the microprocessor
system design class. Our goals were to provide a learning
environment which aligned with emerging technology and
improved the effectiveness of instruction. We also
developed a laboratory environment which incorporated
cutting-edge programming approaches to manage
hardware components in SoC platforms. This renewed
course goes beyond the inclusion of various interfaces and
devices. The course focuses on the appropriate software

structures using a mixture of high-level and assembly
language programming, I/O operations in modern
operating systems, and reusable software components.
In this paper, we will explore the challenges and
successes we encountered in implementing this new
microprocessor system design class. The course serves as
the first of three embedded system courses in our
curriculum. Section 2 presents background information on
the embedded system curriculum at Arizona State
University (ASU). In Section 3, we will present the new
course design followed by the course objectives, the
course material and the setup of the laboratory
environment for programming projects. Section 4 will
cover some of our lessons learned and feedback from our
students. In Section 5 we conclude our discussion.

2. Background
ASU, Motorola, and Intel formed a not-for-profit
Consortium for Embedded and Inter-Networking
Technologies (CEINT) in 2001 [3]. CEINT developed an
infrastructure to support a strong curriculum in embedded
systems. The end product was a concentrated path in
Computer Systems Engineering, which consisted of an
Embedded Systems Development, Embedded Systems
Engineering, and Embedded Systems Capstone course
[1].
We wanted to provide students with the opportunity to
learn practical development techniques using the
Embedded Systems Development course. To accomplish
this goal, we chose Motorola MX1ADS boards using
MontaVista’s HardHat Linux Toolkit. Although we
discussed both assembly level and high level
programming development, C was the main language
used for developing projects. This particular combination
of programming language, development environment, and
microcontroller architecture is rare for an introductory
level embedded systems class.
At the same time, the students were challenged to get
quickly up to speed on the fundamentals required to use
the new development environment and tools. Most of the
students did not have strong backgrounds in developing
software for Linux. To lessen this steep learning curve,
we provided laboratory demonstrations and walked
through simple development projects in small groups.
We also provided online tutorials, sample Linux drivers,
and low level C code examples for students to study.
In this course, we introduced students to memory
devices, memory controllers, buses, handling interrupts,
DMA, timers, counters, UART, SPI, I2C, parallel I/O,
keypad, LCD, touch panels, and A/D - D/A converters.
The students also developed device drivers for timers,
PWM, UART, gpio, and SPI eeprom as class projects.
Other available features such as watchdog timer, blue

tooth technology, USB, and CMOS sensors were left for
more advanced courses in the sequence.
Assembly language teaches the students about the
detailed architecture of the hardware. This gives students
an appreciation for high level constructs implemented in
assembly language [2]. However, implementing all
software programs in assembly language neither practical
nor desired. In fact, assembly-language programming is
no longer the best choice for developing embedded
systems, due to the availability of excellent compilers and
the rising complexity of software projects [6][9].

3. Course Design
3.1. Course Objectives
The objectives of the course are to familiarize the
students with hardware-software interfaces, hardware
designs of microprocessor systems and peripheral devices
and their communication protocols. Students work at
acquiring technical knowledge and applying this
knowledge to the development of programs for
controlling peripheral devices and interfaces. Thus, the
students learn to analyze and synthesize suitable solutions
for building integrated hardware/software systems capable
of interacting with external world.

3.2. Course Content
The revamped course places emphasis on
software/hardware integration and I/O programming, the
incorporation of the state-of-the-art SoC platforms, and
emerging embedded system development tools. Our plan
is to gear the integration of hardware modules to construct
embedded systems and the programming models and
characteristics of various I/O interfaces and peripherals.
The course syllabus is established as follows:
Course Syllabus: Microprocessor System Design
Course Goals:
• Develop an understanding for using a CPU core as a
component in system-level design.
• Develop the ability to integrate the CPU core with
various interface units in embedded systems.
• Gain the necessary skills for programming and
debugging I/O operations to manage peripherals for
embedded applications.
Major topics covered:
• Introduction and review of instruction set and
assembly language programming, instruction
execution cycle and timing (4 lectures)
• C programming for embedded systems (2 lectures)
• Interrupts and I/O multiplexing (2 lectures)
• Parallel I/O interface and signal handshaking (1
lecture)

• Timers and counters (2 lectures)
• Serial communication: UART, SPI, and I2C (4
lectures)
• Keypad and LCD interfaces (3 lectures)
• Transducers and sensors, touch panels, A/D-D/A
converters (3 lectures)
• Memory devices, SRAM, DRAM, flash memory,
and SDRAM controller (3 lectures)
• Buses, access arbitration, timing, and bus protocols
(2 lectures)
Laboratory projects:
• Introduction project on understanding the
programming environment on a target development
board.
• 3-4 small (1-2 weeks) assignments on programming
and interfacing with various peripheral units.
• 2 medium (3-4 weeks) sized projects to build
applications integrating multiple devices.

As shown in the syllabus, the course started with an
introduction to the ARM architecture and instruction sets.
We then discussed C programming for embedded systems
which included accessing I/O registers, bit manipulation,
C calling convention, and in-line assembly. The students
used the ARM Software Development Toolkit (ARM
SDT 2.02u) to develop and debug their assembly/C
programs in an ARM instruction set simulator called an
ARMULATOR.
Following the introduction to ARM architecture and
programming, we presented the overall architecture of
MX1 processor and the connection to peripheral
interfaces. For the I/O interfaces and interrupt signals, we
started the discussion with the general-purpose
input/output (GPIO) and handshaking signals. Since most
I/O functions and peripheral interfaces are multiplexed at
the I/O pads, the lectures focused on the programming
techniques for configuring I/O pins and functions.
Similarly, interrupt multiplexing and configuration
techniques were discussed, followed by interrupt vectors
and ISR operations. This allowed us to look into each
peripheral interface in subsequent lectures.
The peripheral interfaces covered in the class included
a timer, pulse-width modulator, UART, SPI, I2C, LCD
controller, and touch panel controller. The lectures
addressed the basic design principles, the internal register
configuration of the peripheral interfaces, and interrupt
mechanisms. The timing diagrams of the signal
waveforms at I/O pins were discussed to illustrate the
interaction of programming model and device operations.
In addition, the schematics of the MX1ADS development
board were used to show the connections of MX1

processor with external interface circuits and devices.
While discussing LCD and touch panel controllers, the
lectures also encompassed general raster display devices
and A/D converters.
After discussing the selected peripheral interfaces and
the programming techniques, the lectures focused on the
memory structure of microprocessor systems. Both the
abstract model and physical memory architecture of the
SRAM and DRAM were explored. We paid special
attention to synchronous DRAM, their timing
characteristics, and access modes. We used the Micron
MT48LC32M8A2 as an example of SDRAM.
The interconnection mechanism of microprocessor
systems is also an important subject of the course. We
focused on the bus architecture and the protocols of PC’s
XT, AT, ISA, and PCI buses. The general bus designs,
including synchronous/asynchronous, bus arbitration, and
block transfer were also covered. The final topic covered
optimization techniques of bus performance such as
pipelined transfers and split transactions.

3.3. Hardware Platform for Lab Projects
Although our goal was to teach the general principles
of the microcontroller architecture and system design, we
desired to have a target platform available to students to
use for experimentation. We decided to use a 32-bit RISC
platform instead of a traditional 8-bit architecture such as
the Intel 8051 and Motorola 6811. There were three
motivating factors in choosing a 32-bit RISC architecture
over an 8-bit architecture. First, we wanted to use a
current technology so that students would be well
prepared for a career in the embedded systems industry.
Second, we wanted to introduce multiple peripheral
devices and bus technologies that were only available on
32-bit architectures. And finally, we had received a large
endowment from industry partners to provide equipment
and classroom support for the 32-bit architecture.
The target hardware platform had to include a high
performance SoC microprocessor for which popular
interfaces were available and configurable. To acquire
additional support to build the experimental environment,
we contacted the Motorola’s Dragonball University
Program, sponsored by Motorola SPS in 2003. The
University Program considered our approach for
software/hardware integration as an effective instructional
method for embedded systems software development, and
donated thirty Dragonball MX1 development boards
(MX1ADS) for our lab. Motorola also agreed to provide
all necessary technical support to expedite the installation
of lab equipment.
To facilitate various projects, the SoC-based
development boards are accompanied with a peripheral
board on which various devices are installed. Figure 1

depicts a typical development system that enables
programming development for different I/O projects.
SDRAM

Flash memory
(for booting)

Mouse
(serial interface)
(USB
interface)

LCD

Keyboard
(PS2 serial
interface)

Dragonball
MX1

GPIO to
digital/analog
converter

Development board

Serial EPROM
(SPI interface)

Touch panel

CMOS video
sensor

Serial EPROM
(I2C interface)

Figure 1. The target development system for lab
assignment

over $60.0 million in 2003. This number is projected to
reach over $115 million in 2006 [4].
In the target environment, students test their software
components to manage peripheral devices. Since the I/O
addresses are a part of the kernel address space and are
protected, software components are developed as loadable
device drivers modules. User applications use the drivers
through standard file operations such as open, close, read,
write, and ioctl. Interrupt service routines can also be
registered as the modules are installed. This approach is
quite attractive since the software for hardware interfaces
are modular and embedded as a part of the operating
system to support user applications. For students who
have not taken any operating system courses, it may be
challenging to comprehend the software structure and
kernel APIs, and to develop kernel modules.

3.4. Software Platform for Lab Projects
Embedded Linux was chosen as the software platform
on the MX1ADS boards. The fine modularity of Linux
components allowed us to customize the Linux kernel for
the course. Only the device drivers required to boot the
target board were kept in the Embedded Linux build. This
enabled students to load their drivers as modules.
Additionally, Linux provided a rich set of freely available
debugging tools and environments, such as printk, strace,
gdb, ksymops, and klogd. With MontaVista’s Linux, we
established the software development environment shown
in Figure 2.

Register signal
handler with task
structure
Device driver
module with
registered IRQ

Application
code

Register user
process for
signaling and
read/write to
IO’s

….
sig
handler
sig mask
sig
pending
….

buffers

Task structure
for the user
process

memorymapped
IO’s

Core Kernel

Signal
Handler(s)

User Space

Asynchronou
s signals

IRQs

interrupts
Kernel Space

Influence from industrial trends also played a
Host PC workstation
ARM elf
gcc crosscompiler

T arget MX1 ADS

Figure 3. A pseudo driver for exercising kernel I/O
address space and interrupts

GDB
debugger

Applications
GDB Server

MontaVista IDE
cygwin

Embedded Linux
DB MX1 ADS board
support package

Windows

Figure 2. The target software development environment
for MX1 ADS

significant role in our decision to use Linux. Currently,
Linux is one of the preferred choices in the embedded
system industry due to the availability of kernel source
code without loyalties. This has lead toward recent trends
of Linux becoming a dominant platform in embedded
controllers. According to a survey conducted by the
Venture Development Corporation, the estimated
worldwide shipments of embedded Linux operating
systems, add-on components, and related services reached

To assist students with Linux specific driver
development, we provided several example driver
modules to illustrate the interactions between user
applications and device drivers. One example is a pseudo
driver, shown in Figure 3, which allows a user application
to access memory locations in the I/O address space.
When read or write functions are called, a command
structure consisting of an I/O address and a data field is
passed from the user application to the driver. The driver
then reads from or writes to the I/O address. Hence, the
student’s application program can manipulate and access
various control and status registers of peripheral
controllers. To illustrate interrupt-driven data transfer, we
added a ring buffer in the pseudo driver with which I/O
data can be saved for subsequent read calls. Blocked
driver function calls and the interaction with ISRs are
demonstrated using a wait queue, interruptible_sleep_on,
and wake_up_interruptible kernel functions. In addition,
the pseudo driver makes use of asynchronous notification
to emulate interrupts to user application programs. An
ISR can invoke kill_fasync to signal a user application

handler once it is registered. The signal handler can then
take an action or pass the status changes to the main
program. This pseudo driver also provides a great
example to build character device drivers for some
peripheral devices.

3.5. Sample Projects
To reduce the learning curve on Linux device driver
development models and Linux kernel application
programming interfaces (API), we provided a driver
framework for each assignment. This allowed the students
to concentrate on writing the hardware/software interface
code rather than worrying about Linux’s internal device
driver interface. For example, the following segment of
code is part of the driver framework we provided to
students to develop a timer driver.
int init_module()
{
int result;
/* register our character device */
result = register_chrdev(IO_major, driverName, &IOBridge_fops);
if (result < 0) {
printk("<1>%s: Can't get major %d\n", driverName, IO_major);
return result;
}
if (IO_major == 0)
{
IO_major = result;
}
// initialize hardware timer
timer_init();
// Register timer interrupt from the kernel.
if (request_irq(TIMER_IRQ, timerISR, 0, "Timer2", NULL)) {
printk("<1> Unable to get IRQ for Timer 2\n");
unregister_chrdev(IO_major, driverName);
return -EBUSY;
}
return 0;
}
void cleanup_module() /* This function is called when we do rmmod. */
{
printk("<1>Freed %s\n", driverName);
free_irq(TIMER_IRQ, NULL);
unregister_chrdev(IO_major, driverName);
}
void timer_init() {
}
void timerISR(int irq, void *dev_id, struct pt_regs *reg) {
}

In terms of projects, the platform enabled many
development assignments with peripheral device
controllers and hardware configurations. The following
lists some sample projects given in the Fall of 2004.

1. Measurement of execution of the CRC-32 procedure
with a hardware timer. The measurement was done in
the eLinux environment on MX1ADS target board
using MontaVista’s DevRocket IDE on a Windows PC
or Linux workstation.
2. Development of an interrupt-driven mouse driver for a
serial mouse. The project employed a Microsoft 2button serial mouse (Version 2.0A) attached to UART
serial port. The driver compiles three mouse
movement data packages and then reports any
movement to the user applications.
3. Development of a driver for an external memory
device. A Microchip 25LC640 EEPROM which
consisted of 256 32-byte pages (or blocks) was used.
The EEPROM contained an SPI interface. Hence, all
commands and data transfer operations are done via a
SPI bus controller. The project introduced students to
the important concept of timing in device driver
programming.
For the first project, we provided a Linux character
driver capable of writing and reading registers on the
target board. The students were tasked with developing
an application to measure the execution time of a given
program by using the hardware timer. This assignment
introduced students to the Linux device driver model and
software-hardware interface.
Next, the serial mouse driver project allowed students
to apply their theoretical understanding of UART to
develop an interrupt driven mouse driver. The driver uses
an asynchronous I/O signal to communicate between the
application and device driver in the kernel. We provided a
framework for asynchronous I/O implementation in the
Linux device driver.
The overall goal of the assignments was to reinforce
classroom learning by providing the students with
interesting projects. This gave them a greater
understanding of theoretical concepts and a feeling of
satisfaction upon completion of the projects [2].

4. Outcome and Evaluation
At the end of the semester, we surveyed the students
about their learning experience. Twenty-eight out of
forty-four students responded to the survey (64%). The
survey questions are grouped into five categories: C
programming, the Linux development environment,
system architecture and system-level design, peripherals
and projects, and overall satisfaction.
According to the survey, over 80% of the students
agreed their understanding of C programming language
has increased and that they were comfortable with
developing device drivers using C. Even though the
students were not familiar with the tools and development
platform we used in class, we found that they were able to

learn them quickly. About 73% of the students suggested
that they were able to use the tools effectively at the end
of semester.

6. References
[1]

The most challenging issue was the lack of proficiency
in C programming and Linux development environments.
We are planning to integrate a Linux environment in some
prerequisite classes and add more emphasis on C in basic
programming courses in the future.

[2]

5. Conclusion

[3]

Similar to many computer engineering curriculums,
the microprocessor system design course at ASU has
focused on teaching hardware/software interfacing and
the management of peripheral devices. The previous
approach
of
using
assembly
language
and
microcontroller-based platforms had been in place for
more than a decade. It allowed the students to appreciate
machine level processor operations and hand optimization
to achieve the efficiency of assembly programs. However,
with the advent of modern software development tools
and the wide-spread use of embedded systems
applications, a change in course material becomes
inevitable.
There are a few important initiatives used in our
approach for the microprocessor system design course.
First, the use of assembly language for software
development to control peripheral interfaces should be
minimized. Students must be able to assess the cases
where the use of assembly code can be justified. This
would include encapsulating assembly code in welldefined interfaces and incorporating the code in software
components as required. Second, the use of a broad set of
peripheral interfaces including serial buses, LCD
controller, touch panel, and data acquisition should be
introduced. Finally, a practical software development and
execution environment should be utilized so that students
can gain familiarity with modern tools to build structured
software components for embedded applications.
With these initiatives, the microprocessor system
design course was transformed and introduced in the Fall
of 2004. It was anticipated that knowledge gaps would
exist in some of the prerequisite courses. Hence, we
assumed that students may encounter difficulty with the
required learning curve. However, we were surprised and
satisfied with students’ reception to the course. In general,
students were excited about the new course structure, the
updated learning environment, and the challenging
projects, although complaints over the large amount of
manuals and data sheets still existed. Overall, we believe
this course was successful and we look forward to the
development of the more advanced courses in the
Embedded Systems curriculum.

[4]
[5]

[6]

[7]

[8]

[9]

Gerald C. Gannod, et. al., “A Consortium-based Model for
the Development of a Concentration Track in Embedded
Systems”, Proceeding of the 2002 American Society for
Engineering Education Annual Conference & Exposition.
Chris Hudson, “Teaching Microcontroller Technology –
learning through play”, IEEE International Symposium on
Engineering Education: Innovation in Teaching, Learning
and Assessment, Volume: Day 1, 4 January 2001.
David C. Pheanis, “CEINT Internship Program”, 33rd
ASEE/IEEE Frontiers in Education Conference, November
2003.
Chris Lanfear, Steve Balacco, “The Embedded Software
Strategic Market Intelligence Program, 2004”, Venture
Development Corporation, July 2004.
Seongsoo Hong, “Embedded Linux Outlook in the PostPC
Industry”, Proceeding of the Sixth IEEE International
Symposium on Object-Oriented Real-Time Distributed
Computing, 2003.
Frank Vahid, “Embedded System Design: UCR's
Undergraduate Three-Course Sequence”, Proceedings of
the
2003 IEEE
International
Conference
on
Microelectronic Systems Education, 2003
Naehyuck Chang and Ikhwan Lee, “Embedded System
Hardware Design Course Track for CS Studnets”,
Proceeding of the 2003 IEEE International Conference on
Microelectronic Systems Education, 2003.
Shlomo Pri-Tal, John Robertson, Ben Huey, “An Arizona
Ecosystem for embedded Systems”, IEEE International
Conference
on
Performance,
Computing,
and
Communications, 4-6 April 2001.
Konstantin Boldyshev, “Linux Assembly HOWTO,”
http://www.linuxselfhelp.com/HOWTO/AssemblyHOWTO/index.html.

Constrained Energy Allocation for
Mixed Hard and Soft Real-Time Tasks*
2

Yoonmee Doh1, Daeyoung Kim , Yann-Hang Lee3, and C.M. Krishna4
1

CISE Department, University of Florida,
Gainesville, FL 32611-6120, USA
ydoh@cise.ufl.edu
2 Information and Communications University, Munji-dong, Yusong-gu,
Daejon, 305-714, Korea
kimd@icu.ac.kr
3 Dept. of Computer Science and Engineering, Arizona State University
Tempe, AZ 85287-5406, USA
yhlee@asu.edu
4
Electrical & Computer Engineering Dept., University of Massachusetts
Amherst, MA 01003, USA
krishna@ecs.umass.edu

Abstract. Voltage-Clock Scaling (VCS) is an effective approach to reducing
total energy consumption in low power microprocessor systems. To provide
real-time guarantees, the delay penalty in VCS needs to be carefully considered
in real-time scheduling. In addition to real-time requirements, the systems may
contain non-real-time tasks whose response time should be minimized. Thus, a
combination of optimization objectives should be addressed when we establish
a scheduling policy under a power consumption constraint. In this paper, we
propose a VCS approach which leads to proper allocations of energy budgets
for mixed hard and soft real-time tasks. Based on the schedulability of VCSEDF, we investigate the characteristics of energy demand of hard periodic and
soft aperiodic tasks. Using simulation and subject to a given energy budget,
proper voltage settings can be chosen to attain an improved performance for
aperiodic tasks while meeting the deadline requirements of periodic tasks.

1 Introduction
Mobile computing and communication devices such as laptop computers, cellular
phones, and personal digital assistants (PDA’s) have become commonplace; the demands for embedded applications on those devices are increasing. However, processors are also becoming increasingly power-hungry. For this reason, the field of
power-aware computing has gained increasing attention over the past decade.

*

The work reported in this paper is supported in part by NSF under Grants EIA-0102539 and
EIA-0102696.

J. Chen and S. Hong (Eds.): RTCSA 2003, LNCS 2968, pp. 371–388, 2004.
© Springer-Verlag Berlin Heidelberg 2004

372

Y. Doh et al.

Simple techniques, such as turning off (or dimming) the screen while a system is
idle and shutting down hard disks while it is not accessed is now commonly adopted
in most portable device designs [1]. However, in many cases, re-activation of hardware can take some time, and affect response time. Also, deciding when and which
device should be shut down and woken up are often far from trivial [3].
Another effective approach to power reduction is a technique called Voltage-Clock
Scaling or Dynamic-Voltage-Scaling in CMOS circuit technology. The power con2
sumed per every cycle in a digital circuit is given by Pcmos = C L N swV DD
f , where
CL is the output capacitance, Nsw the number of switches per clock, and f the clock
frequency. Due to the quadratic relationship between the supply voltage (VDD) and the
clock frequency, a small reduction in voltage can produce a significant reduction in
power consumption. However, lowering VDD increases the circuit delay following the
2
equation t d = k VDD (VDD − VT ) , where k is a constant depending on the output gate
size and the output capacitance, and VT is the threshold voltage. This implies that the
clock frequency must be reduced and the execution time is extended [11]. Obviously,
the longer execution time may lead to performance degradation in application response time and a failure to meet real-time deadlines.
Most of today’s processor cores have been designed to operate at different voltage
ranges to achieve different levels of energy efficiency, as shown in Table 1. For instance an ARM7D processor can run at 33MHz and 5V as well as at 20MHz and
3.3V. The energy-performance measures at these two modes of operation are 185
MIPS/WATT and 579 MIPS/WATT, and the MIPS measures are 30.6 and 19.1, respectively [7]. From these figures, if we switch from 33MHz and 5V to 20MHz and
3.3V, there will be around (579-185)/579=68% reduction in energy consumption at
an expense of (30.6-19.1)/19.1=60% increase in processing time. Kuroda et al. use
voltage scaling in the design of a processor core as shown in [4], in which they can
adjust internal supply voltages to the minimum automatically according to its operating frequency.

Table 1. Microprocessors that allow the core operate at different voltages and frequencies

Processors
StrongARM SA-2
[5]
Pentium-III
[6]
Crusoe (TM5400)
[9][10]
ARM7D
[7]
PowerPC860
[8]

Voltage
1.30
0.75
1.60
1.35
1.65
1.10
5.0
3.3
3.3
2.4

Power
Speed
Consumption Features
(MHz)
(Watt)
600
0.45
12-fold energy reduction
150
0.04
650
22
SpeedStep Technology
500
9
- 2 modes
700
2
16 levels
200
1
in steps of 33MHz
33
0.165
185 MIPS/W
20
0.033
579 MIPS/W
50
1.3
2 modes
25
0.241

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

373

If low energy consumption is a desirable feature in real-time embedded systems,
voltage-clock scaling must cooperate with the task scheduling algorithms since the
power-delay tradeoff property in low power design affects meeting the strict timeconstraints of real-time systems. The execution of a high-priority at a low voltage and
slow clock rate may cause a low-priority task to miss the deadline due to the additional delay from the execution of the high priority task.
The concept of real-time scheduling has been applied to dynamic speed setting in
[12] by Pering et al. Regarding the impact on energy of the number of available
distinct voltage levels, Ishihara and Yasuura pointed out that at most two voltage
levels are usually enough to minimize energy consumption [13]. A minimum-energy
scheduler based on the EDF scheduling policy was proposed in [14], where an offline algorithm assigned the optimal processor speed setting to a critical interval that
requires maximum processing. Similar to the approach for EDF scheduling in [14],
Hong et al. considered a low energy heuristic for non-preemptive scheduling in [15]
and the optimal voltage setting for fixed-priority scheduling is studied in [16]. These
approaches require that the task release times must be known a priori. Using twomode voltage scaling under EDF scheduling, dynamic resource reclaiming was proposed in [17], which is useful when task arrival instances or phases are not known a
priori and an extension of [18]. For the periodic task model and rate-monotonic
scheduling, two on-line voltage-scaling methods [19] were proposed, which change
voltage levels at the execution stage from the initially assigned levels as such changes
become necessary.
While VCS has been a well-populated research area, power-aware system design
has generally focused on minimizing total power consumption. For systems consisting of soft aperiodic tasks, the objective of minimizing power consumption will result
in slow execution. On the other hand, in many cases, the battery capacity can be replenished or there is a finite mission lifetime. Minimizing power consumption that
doesn’t utilize all available energy may not lead to optimal system performance. A
better power control strategy in such cases is to minimize the response times of soft
real-time tasks, providing that the deadlines of hard real-time tasks are met and the
average power consumption is bounded.
In this paper, we target battery-driven real-time systems, jointly scheduling hard
periodic tasks and soft aperiodic tasks, whose battery capacity is bounded in the feasible range given by a set of tasks. The scheduling should guarantee meeting the task
deadlines of hard real-time periodic tasks and achieve average response time of aperiodic tasks that are as low as possible. Under the constraints of a bounded energy
budget, finding an optimal schedule for a task set should aim to satisfy both optimal
power consumption and strict timing constraints simultaneously.
We first investigate the characteristics of energy demands of periodic and aperiodic tasks focusing an EDF scheduling exploiting the feature of VCS. Based on the
energy requirement of mixed real-time tasks, we also propose a static scheduling for
energy budget allocation, which determines the optimal two-level voltage settings of
all tasks under bounded energy consumption, while guaranteeing that no deadline of
any periodic task is missed and that the average response time of aperiodic tasks is
minimized. The algorithm selects the voltage settings that have the minimum average

374

Y. Doh et al.

response time among the schedulable ones within a given energy consumption. To
schedule aperiodic tasks, we adopt the Total Bandwidth Server, which was proposed
by Spuri and Buttazzo and handles aperiodic tasks like periodic tasks within the reserved bandwidth such that it outperforms other mechanisms in responsiveness [21].
The paper is organized as follows. In Section 2, we outline the preliminary system
model having several assumptions. Then, we discuss the characteristics of energy
demand and processor utilization under bounded energy budget in Section 3. Considering on the characteristics described in Section 3, energy allocation methods and an
algorithm of voltage assignment are described in Section 4. To illustrate the effectiveness of the proposed algorithm, we evaluate its performance in Section 5 through
simulation studies. In Section 6, a short conclusion is provided.

2 System Model
For the targeted real-time systems, tasks may arrive periodically and have individual
deadlines that must be met. Or they can be aperiodic and can accrue computation
values, which are inversely proportion to their response times. Under a given bound
on energy consumption, we build a system model and make several assumptions as
follows.
2.1 Schedule for Periodic Tasks
For Earliest Deadline First (EDF) scheduling, a periodic task τi is modeled as a cyclic
computational activity characterized by two parameters, Ti and Ci, where Ti is the
minimum inter-arrival time between two consecutive computation instances and Ci
the worst-case execution time (WCET) of task τi. The EDF scheduling algorithm
always serves a task that has the earliest deadline among all ready tasks. The following assumptions are analogous to assumptions made in real-time scheduling theory
[20].
• Tasks are independent: no task depends on the output of any other task.
• The deadline for task τi is equal to Di, which is less than Ti.
• The worst-case execution demand of each task τi, i.e. Ci, is known. The actual execution demand is not known a priori and may vary from one arrival
instance to the other.
• The overhead of the scheduling algorithm is negligible when compared to the
execution time of the application.

2.2 Schedule for Aperiodic Tasks
An infinite number of soft aperiodic tasks {Ji | i=0,1,2,…} are modeled as aperiodic
computation activities represented by two parameters, λ and µ, where λ is the average

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

375

inter-arrival time between two consecutive aperiodic instances and µ the average
worst-case execution time of all aperiodic tasks.
Aperiodic tasks are scheduled by Total Bandwidth Server (TBS) algorithm that
makes fictitious but feasible deadline assignment based on the available processor
utilization guaranteed by the isolation of bandwidth between periodic and aperiodic
tasks. In the TBS algorithm, the k-th aperiodic request arriving at time t = rk, a task
deadline

d k = max( rk , d k −1 ) +

Ck
UA

(1)

is assigned, where Ck is the execution time of the request and UA the allocated processor utilization for aperiodic tasks. By definition d0=0. The request is then inserted into
the ready queue of the system and scheduled by the EDF algorithm, as are any other
periodic instances or aperiodic requests already present in the system.
Note that the assignment of deadlines is such that in each interval of time, the
processor utilization of the aperiodic tasks is at most UA. Hence, a set of periodic tasks
with utilization factor U P =

∑

n

i =1

C i Ti and a TBS with a bandwidth UA is schedul-

able by EDF if and only if UA + UP ≤ 1. The definition and the formal analysis of this
algorithm are proved in [21]. Comparing to other scheduling algorithms for aperiodic
tasks, the TBS algorithm has a very simple implementation complexity and shows
very good performance in average response time.
2.3 Voltage Clock Scaling

• Voltage Switching
We assume voltage switching consumes a negligible overhead. This is also analogous to the assumption made in classical real-time scheduling theory that preemption
costs are negligible [20]. Voltage switching typically takes a few microseconds. In
fact, a bound of the total overhead can be calculated by simply counting the number
of task arrivals and departures since voltage switches are only done at task –dispatching instances.
• Two Voltage Levels
The system operates at two different voltage levels. Ideally, a variable voltage
processor that has continuous voltage and clock setting in the operational range is
available as explained in Table 1. We assume a simple setting arrangement that the
processor in a real-time system can be dynamically configured in one of two modes:
low-voltage (L) -mode and high-voltage (H)-mode. In L-mode, the processor is supplied with a low voltage (VL) and runs at a slow clock rate. Thus, task execution may
be prolonged but the processor consumes less energy. On the other hand, the processor can be set in H-mode, i.e. be supplied with a high voltage (VH) and run at a fast
clock rate, in order to complete tasks sooner at the expense of more energy consumption. The operating speeds at L-mode and H-mode are denoted as αL and αH, respec-

376

Y. Doh et al.

tively, in terms of some unit of computational work. Depending on the voltage setting
for task τi, the worst-case execution time is Ci/αL or Ci/αH.
2.4 Bounded Energy Consumption
In battery-powered embedded systems, it is often equally important to control power
consumption to extend the battery lifetime and to enhance system performance. Given
that the battery can be replenished or the mission lifetime is limited, we may assume
that the available capacity can safely be consumed during a predefined interval of
operation. Thus, an average power consumption rate or energy budget can be set to
the ratio of available capacity to the target operation interval. Also, it is possible to
communicate with the battery such that the system and its scheduler can know the
current status of the battery capacity. One of the mechanisms for doing this is the
Smart Battery System (SBS), which has been now actively standardized and introduced to battery-driven systems [2]. In the paper, we assume the embedded system,
whose processor is the major factor of the energy consumption

3 Energy Budget Allocation in Real-Time Embedded Systems
For all real-time tasks, the available energy consumption is confined to a given energy budget called EC, which has to be shared among periodic and aperiodic tasks. Let
EP and EA are the energy budget allocated to periodic tasks and aperiodic tasks, respectively. The voltage-clock scaling problem is to find voltage settings for both
periodic and aperiodic tasks such that

•
•

•

all periodic tasks are completed before their the deadlines and have an energy
consumption less than EP.
all aperiodic tasks can attain the minimal response times while consuming an
energy less that EA.
EP + EA ≤ EC

3.1 Periodic Tasks
Assume that, for periodic task τi, mi is the voltage setting determined between the two
possible modes, i.e. L-mode and H-mode and αi (mi) is the speed of task τi at mode mi.
Given mi for all of periodic task τi, the energy demand for periodic task of Ep is

E P ( mi ) = ∑

1 Ci
p( mi )
α i ( mi ) Ti

(2)

where p(mi) is the power consumption at mode mi, Ci the average execution time of
task τi In addition, the worst-case utilization is given by

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

U P ( mi ) = ∑

377

(3)

1 Ci
α i ( mi ) Ti

If mi is H-mode for all periodic tasks, the processor runs at a fast clock rate all the
time, thereby minimizing the utilization. The maximum energy demand for the tasks
is represented as

max E P = ∑

1 Ci
p
α H Ti H

(4)

1 Ci

(5)

and its utilization becomes
minU P = ∑

α H Ti

On the contrary, if mi is L-mode for every periodic task τi, the processor runs at a
slow clock rate all the time such that the utilization is maximized, but consumes the
minimum possible energy. For the sake of schedulability, the tasks should be
scheduled in such a way that the utilization is less than unity Therefore, we define
min EP as an energy demand when there exists a set of {mi} so that the worst-case
utilization

U P ( mi ) = ∑

1 Ci
1 Ci
≤ 1 and E P ( mi ) = ∑
p( mi ) is minimized.
α ( mi ) Ti
α ( mi ) Ti

(6)

In Fig. 1, we describe the relationship between energy consumption and utilization
for a set of periodic tasks. The maxima and minima are denoted as max EP and min EP
for the energy and max UP and min UP for the utilization, respectively. Again, min EP
follows equation (6). Regarding the feasibility of the energy constraint and the worstcase utilization, EC must be greater than min EP and min UP should be no greater than
1. By its definition, if min UP is greater than unity with all H-mode executions, it is
impossible to find voltage settings to ensure that all tasks meet their deadlines. If max
UP is less than 1, the tasks are schedulable with all L-mode assignments and energy
consumption can never be less than min EP. In the case, max UP becomes
1 Ci
1 Ci
and min EP does E P (L ) = ∑
p .
U P (L ) = ∑
α L Ti
α L Ti L
available

is the
If energy budget EP is given in the range from min EP to max EP, UP
available utilization corresponding to the allocated energy budget EP. And, by
searching a set of voltage settings meeting the given energy budget and schedulability, energy demand and utilization for periodic tasks are determined as EP (mi) ≤ EP
available
and UP (mi) ≥ UP
, respectively.

378

Y. Doh et al.
min EP

0

EP

maxEP
Energy

Schedulable
Utilization

min UP

0

UPavailable

1

max UP

Fig. 1. The relationship between power consumption and utilization for a set of periodic tasks

3.2 Aperiodic Tasks
Denote by mA the voltage setting determined between the two possible modes for
aperiodic tasks, which have the average inter-arrival time of λ and the average worstcase execution time of µ. If all of them are assigned in mode mA and the power consumption at mode mA is p (mA), the energy consumption and utilization of them are

E A ( mA ) =

1 µ
p( m A )
α( mA ) λ

(7)

1 µ
α ( mA ) λ

(8)

U A ( mA ) =

respectively.
Also, if all of them are assigned in L-mode or H-mode, they demand minimum energy min EA or max EA given by the following equations

min E A =

1 µ

αL λ

p L and max E A =

1 µ

αH λ

pH

(9)

having the utilization

min U A =

1 µ

αL λ

and max U A =

1 µ

(10)

αH λ

3.3 Energy Budget Allocations and Utilization
While the constraint EP + EA ≤ EC must be satisfied, we can decide how processor
utilization, task scheduling, and task response time are affected. From the viewpoint
of utilization, the more utilization is available for aperiodic tasks, the shorter the
deadlines that are assigned to them by the deadline assignment of equation (1). This
assigns higher priorities to them in EDF scheduling such that they can get a faster

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

379

response. To give more utilization to aperiodic tasks, the utilization of periodic tasks
must be shrunk and it can be done by assigning more tasks to H-mode, but requires
more energy consumption. Since the total energy budget is bounded, the energy
budget left to aperiodic tasks will be reduced. As a result, the aperiodic tasks must be
run in a low voltage mode and their response times will be extended.
Likewise, from the viewpoint of the energy budget, the portion assigned in Hmode for aperiodic tasks should be maximized within an assigned energy budget EA to
get faster responsiveness. But, as before, if the energy demand of aperiodic tasks is
increased, the energy available for periodic tasks will be decreased. In consequence,
the available utilization for aperiodic tasks will be decreased due to the increased
execution time of periodic tasks, which may result in degradation in responsiveness.
Eventually, to get both schedulability and fast responsiveness under a bounded energy budget, an effective scheduling and energy allocation scheme is needed for
jointly scheduling hard periodic and soft aperiodic tasks. The scheduling should address the concern of the trade-off between utilization and energy consumption as
shown in Fig. 1.

4 Constrained Energy Allocation Using VCS-EDF Scheduling
In this section, we describe an energy allocation scheme, which allocates bounded
energy budget to periodic and aperiodic tasks based on VCS-EDF scheduling, meeting the requirements of real-time tasks, i.e. to meet deadlines of periodic tasks and to
get faster average response time for aperiodic tasks. Given an energy budget EC, considering the feasible range of energy demand determined by tasks, it finds voltage
settings for periodic tasks and the execution portion in H-mode and L-mode to the
worst-case execution time for aperiodic tasks under a bounded energy budget.
4.1 Energy Allocation Factors
Suppose that EP and EA can be allocated in the range of [min EP, max EP] and [min EA,
max EA], respectively, Emax = max EP + max EA, and Emin = min EP + min EA. If the
bounded energy consumption budget is given as EC, EC must fall into the range Emin ≤
EC ≤ Emax where min EP, min EA, max EP, and max EA are as defined in equations (4),
(5), and (9) to a given set of tasks. Then, voltage settings must be determined such
that the energy consumption satisfies the constraint of EP + EA ≤ EC, while guaranteeing the schedulability of periodic tasks and minimizing average response time for
aperiodic tasks. For ease of explanation, we define Ediff = Emax − Emin.
Let β and γ be energy allocation factors of aperiodic and periodic tasks, given by 0
≤ β ≤ 1 and 0 ≤ γ ≤ 1, respectively. Then, the energy budgets EP and EA allocated to
them are represented as
EP = min EP + β(max EP − min EP),

(11)

380

Y. Doh et al.

EA = min EA + γ (max EA − min EA),

(12)

respectively.
Suppose ∆a = (max EA − min EA), ∆p = (max EP − min EP), and ∆c = EC − (min EA
+ min EP), respectively, then the inequality (EP + EA ≤ EC) becomes
Hence, β and γ are determined by

0≤β =

γ∆a + β∆p ≤ ∆c.

∆c − ∆p
∆c
∆c − γ∆a
≤ 1 and
≤γ ≤
∆p
∆a
∆a

(13)

respectively. The choice of γ determines β, and vice versa, and also determines EA and
EP by equations (11) and (12).
If γ=0 and γ=1, energy min EA and max EA are assigned to aperiodic tasks, i.e. assigning all aperiodic tasks in L-mode and H-mode, respectively. If γ is 0.6, energy
assigned for aperiodic tasks becomes (min EA + 0.6∆a). Unlike voltage settings for
periodic tasks, which are decided on the basis of a task, the running mode for aperiodic tasks are determined by the fraction in H-mode and L-mode. If the fraction assigned to H-mode is xH, then that assigned to L-mode becomes (1-xH). The energy
consumption needs to be bounded by the budget, and so

xH µ

αH λ

pH +

(1 − x ) µ
H

αL

λ

pL ≤ E A

(14)

Similarly, the execution time of an aperiodic task is determined according to the
voltage modes and the deadline assigned in Equation (1) is adjusted. As for responsiveness, the greater the fraction of the processor utilization that is given to aperiodic
tasks, the better is the responsiveness expected under the TBS algorithm, because
shorter deadlines are assigned to them. Under energy budgets of EC and EP, the utilization for aperiodic tasks will be increased if the voltage settings are determined to
allocate more H-mode to periodic tasks within the energy budget such that it can
available
minimize the utilization UP (mi) and make an increase in UA
. We therefore have a
constrained optimization problem to determine the optimal voltage settings, maximizing H-mode execution, within the constraint of budget EP and guaranteeing that no
deadline of any periodic task is missed.
The optimization problem to find voltage settings for periodic tasks can be stated
as follows: Pick the task subsets H and L for voltage settings of H-mode and L-mode
such that
• H∪ L = {τ1,τ2, ..., τn}
• H∩ L = ∅
1 Ci
• U P ( mi ) = ∑
is minimized
α ( mi ) Ti

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

381

subject to the well-known sufficient condition1 for the schedulability of periodic tasks
under EDF, i.e.,

1

αH

Ci

(15)

Ci

1

∑ min( T , D ) + α ∑ min( T , D ) ≤ 1
i∈H

i

i

L

i∈L

i

i

and the energy consumption constraint of

pH

αH

Ci

Ci

pL

∑ min( T , D ) + α ∑ min( T , D ) ≤ E
i∈H

i

i

L

i∈L

i

P

i

This optimization problem can be treated equivalently to the decision problem of the
subset sum, which is NP-complete. Consequently, efficient search heuristics, e.g.,
branch-and-bound algorithms, should be employed to find a solution if n is large.
4.2 Algorithm for Energy Budget Allocation
We describe here the algorithm for the dynamic allocation explained in the previous
section. The algorithm outputs the energy allocation factors,β and γ, voltage settings for
periodic tasks, {mi}, and the percentage of H-mode assignment for aperiodic tasks, xH.
The Algorithm of VCS-EDF Scheduling Under Bounded Energy Consumption EC
1. Compute min EP, max EP, min EA, max EA, Emax, and Emin.
2. If EC is less than Emin = (min EP + min EA), there is
not enough energy to execute the workload.
3. If EC is in the range of Emin ≤ EC ≤ Emax, compute the
range of γ and β, EA and EP, accordingly.
4. For each γ in the range of 0 ≤ γ ≤ 1, execute the
following steps
(4a) Compute β, EA and EP,
(4b) Find {mi}, which satisfies

Ci

∑T

i

that U P (mi ) = ∑ Ci ⋅

1
Ti α ( mi )

⋅

1

α ( mi )

⋅ p( mi ) ≤ E P

and

is minimized, where mi is

voltage setting either in H-mode or L-mode for
periodic task τi, using simple search or branchand-bound algorithms.
1

The condition is also necessary if Di≥Ti for all i.

382

Y. Doh et al.

(4c) Compute UAavailable = 1 – UP (mi).
(4d) Given EA, find xH, the fraction of execution in
H-mode for aperiodic tasks, and (1–xH) the fraction in L-mode.
(4e) Applying the TBS algorithm for the deadline assignment UP (mi) and UAavailable computed in step
(4b) and (4c), respectively, run VCS-EDF scheduling in voltage settings {mi} for periodic
tasks and xH and (1–xH) for aperiodic tasks.
5. Find γ having the minimum average response time from
the result of the scheduling in step 4.
6. The value of γ determined in step 5 is selected for
energy allocation, which gives the best performance
for aperiodic tasks, xH for running the aperiodic
tasks in H-mode and {mi} for voltage settings of the
periodic tasks are determined accordingly.

5 Simulation Evaluation
We analyze here the properties of sharing the bounded energy budgets between periodic and aperiodic tasks based on VCS approach and evaluate the VCS-EDF scheme
to schedule mixed real-time tasks. For the power consumption and speed settings,
Motorola’s PowerPC 860 processor is used for our simulation, which can be operated
in a high-performance mode at 50MHz and with a supply voltage of 3.3V, or a lowpower mode at 25MHz and with an internal voltage of 2.4V[8] such that VH and VL
are fixed to VH=3.3 and VL=2.4. The power consumption in the high-performance
mode is 1.3 Watts (pH), as compared to 241mW (pL) in the low-power mode. The
clock rate at high voltage is 100% greater than at low voltage: αH=2.0 and αL=1.0.
A simulation study is performed to address the improvement of task execution time
with extra available energy. In other word, the system is assumed to possess enough
energy to complete the tasks and meet the deadline requirements. In addition, there is
extra energy that can be allocated to improve the response time of aperiodic tasks. Our
immediate objective of the simulation study is to see how the response time can be reduced through a proper voltage setting. Furthermore, this extra energy can be allocated
to periodic tasks such that the processor utilization reserved for periodic tasks is reduced. This leads to a reduction of deadline assignment in the total-bandwidth scheduling scheme. On the other hand, the extra energy can be consumed by aperiodic tasks that
can result in a first-order effect in the reduction of response time.
In our simulation, we first generate 10 random task periods in the range of 100 to
1000 and set the task deadlines equal to their respective periods. The worst-case exe-

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

383

cution demands of the tasks are randomly chosen such that, for each simulation case,
no deadlines need be missed and the resultant utilization is set to Up (L)=0.8, 1.0, or
1.2, respectively. For aperiodic tasks, we adopt the exponentially distributed execution time with an average µ equal to 45. Then we let the inter-arrival time be exponentially distributed with mean of between 450 (10% workload, i.e. UA (L)=0.1) and
112.5 (40%, i.e. UA (L)=0.4). The energy budget EC is set at each of several energy
levels in the range from (Emin+0.6Ediff) to (Emin+ Ediff).
To get fast responsiveness, how much energy budget can be allowed to periodic
and aperiodic tasks, respectively? Over various γ’s and constraint energy budgets, we
obtain the average response times of aperiodic tasks from the simulation and plot
them in Fig. 2. Regardless of increase in the energy budget, Fig. 2 reveals a trend of
reduction in average response time of aperiodic tasks as γ increases. The average
response time does not show always a monotonic decrease with an increase in γ. In
some regions, it has an abrupt increase or is flat over increasing γ. This occurs especially when EC =(Emin+0.6Ediff) or EC =(Emin+0.7Ediff).
Av erage Resp onse Tim e to Gam m a
100

100
0.6 E d iff
0.7
0.8
0.9
1.0

80
70

90

Average RspTime

Average RspTime

90

60
50
40
30
20

80
70
60
50
40
30

0

0.2

0.4

0.6

0.8

20

1

0

0.2

Gam m a
(a) Up=0 .8, Ua=0.3

0.4

0.6

0.8

1

Gam m a
(b) Up=1.0, Ua=0 .3

200

1200

Average RspTime

Average RspTime

1000
150

100

50
20

800
600
400
200

0

0.2

0.4

0.6

Gam m a
(c ) Up=1 .2, Ua=0.3

0.8

1

20
0

0.2

0.4

0.6

0.8

1

Gam m a
(d ) Up=1.4, Ua=0.3

Fig. 2. Responsiveness to the energy allocation of aperiodic tasks

Note that when we increase γ, aperiodic tasks are invoked more in high-voltage
high-speed execution. This results in a reduced CPU utilization, i.e. the utilization
required by aperiodic tasks under the voltage setting. On the other hand, as β is reduced, the energy allocated to the periodic tasks decreases which leads to an increase

384

Y. Doh et al.
available

in UP (mi) and a decrease in UA
. The two reductions, one on the demand to complete aperiodic tasks and the other one on the available utilization for aperiodic tasks,
can have a profound impact on the response times. Let the CPU utilization required
real
available
real
be denoted as UA and we show the ratio of UA
to UA in Fig. 3. For instance,
with EC =(Emin+0.6Ediff) and γ=1.0 in Fig. 3(a), there still exists extra energy to be assigned to periodic tasks (β >0) and an optimal voltage setting is obtained which leads
available
real
to UP (mi)=0.55 and UA
= 0.45. On the other hand, UA is reduced to 0.15 as we
increase γ to1.0. A ratio of 3 is then obtained and plotted in the Figure.
It is interesting to observe that, whenever the ratio is flat in Fig. 3, the average reavailable
sponse times have uneven decreases in Fig. 3. In fact, as long as the ratio of UA
to
real
UA continues to increase, the processor possesses greater capacity to complete
aperiodic tasks and the response time drops. In contrast, there would be a monotonic
decrease in response time if the ratio were flat as we increase γ.

4

3.5

3.5

Utilization Ratio

Utilization Ratio

The R atio o f Av ailable Utilizatio n to the Min. Utilizatio n fo r Aperio dic Tasks
4

3
2.5
2

0.6E d iff
0.7
0.8
0.9
1.0

1.5
1

0

0.2

0.4

0.6

0.8

3
2.5
2
1.5
1

1

0

0.2

4

3.5

3.5

Utilization Ratio

Utilization Ratio

4

3
2.5
2
1.5
1

0.4

0.6

0.8

1

0.8

1

Gam m a
(b) Up=1.0 Ua=0.3

Gam m a
(a) Up=0.8 Ua=0.3

3
2.5
2
1.5

0

0.2

0.4

0.6

Gam m a
(c) Up=1.2 Ua=0.3

0.8

1

1

0

0.2

0.4

0.6

Gam m a
(d) Up=1.4 Ua=0.3

Fig. 3. The ratios of available utilization to the required utilization for aperiodic tasks

The other interesting observation in Fig. 3 is that utilization ratios are not available
for all of γ values. It indicates that the possible choices of γ only exist in the range where
the plots are shown. This is also evidenced in Equation (14) and is originated from the
definition of γ, in which the minimum value of γ means the percentage of energy available for aperiodic tasks after periodic tasks take energy budget as much as they can.

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

385

From these results, to get fast responsiveness of aperiodic tasks, the a greater portion
of the energy budget should be allocated to aperiodic tasks, and then voltage settings of
periodic tasks need to be determined within the energy budget remaining for them. Note
that the way we formulate the minimal energy budget is based on the schedulability for
periodic tasks and ensuring no CPU starvation for aperiodic tasks. If the energy budget
is below this minimum, aperiodic tasks will incur much longer response times.
To reveal the causes that lead to the flat regions in Fig. 3, we now investigate how the
energy budget is allocated to periodic and aperiodic tasks, respectively. In Fig. 4, we
show the energy sharing as percentages of allocated energy EP for periodic and EA for
aperiodic tasks to the maximum energy demand, Emax, that is the maximal energy consumption by a given task set. The plots in Fig. 4 (a)~(c) cover the case when EC is
bounded to (Emin+0.6Ediff). But in Fig. 4 (d), we plot the energy percentages under EC =
(Emin+0.7Ediff) unlike the ones for other periodic workloads. The reason is the energy
budget (Emin+0.6Ediff) is too low to select proper voltage settings making the given set of
tasks schedulable under the periodic workload of UP =1.4.
The Percentage of Allocated Energy to Emax
En 1
er
gy 0.8
All
oc
ati 0.6
on
Pe 0.4
rce
nta
0.2
ge

En 1
er
gy 0.8
All
oc
ati 0.6
on
Pe 0.4
rce
nta
0.2
ge
0

0

0.2

0.4

0.6

0.8

0

1

0

Gamma (Ec=Emin+0.6Ediff)
(a) Up=0.8, Ua=0.3
En 1
er
gy 0.8
All
oc
ati 0.6
on
Pe 0.4
rce
nta
0.2
ge
0

En 1
er
gy 0.8
All
oc
ati 0.6
on
Pe 0.4
rce
nta
0.2
ge
0

0.2

0.4

0.6

0.8

Gamma (Ec=Emin+0.6Ediff)
(c) Up=1.2, Ua=0.3

0.2

0.4

0.6

0.8

1

Gamma (Ec=Emin+0.6Ediff)
(b) Up=1.0, Ua=0.3

1

0

Ec
Ep
Ep(mi)
Ea

0

0.2

0.4

0.6

0.8

1

Gamma (Ec=Emin+0.7Ediff)
(d) Up=1.4, Ua=0.3

Fig. 4. Energy allocation percentage to the maximum energy demand

When a set of periodic tasks can make the most of the given energy budget EP, i.e. EP
(mi) ≤ EP, EP (mi) is determined by the chosen set of voltage settings, mi, in the VCS-EDF

386

Y. Doh et al.

algorithm subject to requirements imposed by the need to maintain schedulability. Thus,
there is a small discrepancy in energy consumption between EP and EP (mi).
Over several regions of γ, EP (mi) is kept at the same level even if EP is decreasing,
while being less than EP. In other words, the same voltage settings are selected for different EP’s. For all the possible combinations of voltage settings, if we sort them in descending/ascending order according to energy demands, a discontinuity in energy demands exists between any two sets of voltage settings adjacent in the sorted list. Let the
discontinuity in energy demand be an energy gap. Then, even if there is a small amount
of change in energy budget EP, it cannot change voltage settings unless it jumps
up/down any energy gap between adjacent energy levels. However, if the number of
periodic tasks is getting bigger, the flatness in Fig. 4 will be reduced because of the fine
energy gap between adjacent energy levels of discontinuous voltage settings.
It should be noticed the big drops in the response times of aperiodic tasks occur when
the voltage settings of periodic tasks result in a energy allocation EP (mi) that is very
close to the available budget EP. For instance, at γ’=0.1 and 0.5 of Fig. 4 (c), the settings
available
real
lead to a little reduction of UA
which, combining with the decrease of UA , bring
about a considerable decrease in the task response time of Fig. 2 (c).
We now consider how much improvement we can obtain from an increased energy
budget. In Fig. 5, we show the evaluation results for the minimum average response
time to the constraint energy ranging from 0.6 to Ediff. The responsiveness of aperiodic
tasks for UA=0.1 and 0.2 is not much affected by the periodic tasks’ workload UP and the
constraint energy budget EC. Since every aperiodic task is assigned to H-mode (i.e.
γ’=1.0 to ensure minimal response time) and is allocated with the maximal energy
budget, the available energy budget for periodic tasks decreases as UA increases. As a
consequence, the increased workload in periodic tasks increases the average response
available
time for the case of UA=0.3 and 0.4 as UA
is limited and the deadlines assigned to the
aperiodic tasks are extended.

6 Conclusion
In this paper, we have presented an algorithm to carry out voltage clock scaling in
workloads consisting of periodic hard and soft real-time tasks. The aim is to keep
within a predefined energy budget. The objective of the scheduling scheme is to minimize the response time of aperiodic tasks while all deadlines of periodic tasks are met
and the total energy consumption is bounded by the energy budget. As we apply total
bandwidth scheduling for aperiodic tasks, we notice two conflicting factors in energy
budget allocation. When extra budget is assigned to aperiodic tasks, their execution can
be done in high-voltage and high-speed mode. This leads to a reduced response time. On
the other hand, the extra energy budget allocated to periodic tasks can result in a lowering of the CPU utilization reserved for periodic tasks. This, in turn, leaves more available CPU utilization for aperiodic tasks and cause shorter deadlines as defined in the
total bandwidth scheduling scheme.
Our simulation study assumes that there the energy budget is enough to meet the hard
real-time periodic tasks and to complete the aperiodic tasks. In addition, here is extra

Constrained Energy Allocation for Mixed Hard and Soft Real-Time Tasks

387

energy that can be allocated to either periodic or aperiodic tasks. Our results demonstrate
that the VCS-EDF scheduling gets the fastest responsiveness when the extra energy
budget is allocated to aperiodic tasks at their maximum energy demand such that all of
them can be run in H-mode. Given the requirement of responsiveness and any energy
budget, the proposed scheduling method can decide the voltage settings for periodic
tasks so that real-time tasks can share the bounded energy budget effectively. Therefore,
the work provides the battery-driven embedded real-time system designer with a general
view, which allows scheduling real-time tasks considering their general characteristics
of energy demands and processor utilization, given a constraint of bounded energy
availability.
Min. Av erage R esponse Time to Bounded Energy C onsum ption
40
U a=0.1
U a=0.2
U a=0.3
U a=0.4

38
36

38

Min. AverageRspTime

Min. AverageRspTime

40

34
32
30
28
26
24

36
34
32
30
28
26
24

22
1

0.9

0.8

0.7

22
1

0.6

0.9

0.8

0.7

0.6

The % of Energy to Ediff
(b) Up=1.0

40

40

38

38

Min. AverageRspTime

Min. AverageRspTime

The % of Energy to Ediff
(a) Up=0.8

36
34
32
30
28
26
24

36
34
32
30
28
26
24

22
1

0.9

0.8

0.7

The % of Energy to Ediff
(c) Up=1.2

0.6

22
1

0.9

0.8

0.7

0.6

The % of Energy to Ediff
(d) Up=1.4

Fig. 5. Average response time with respect to the bounded energy budget

References
1.

2.

Compaq, Intel, Microsoft, Phoenix, and Toshiba, “Advanced Configuration and Power
Interface specification,” available at http://www.intel.com/ial/powermgm/specs.html
(1996)
K.Lahiri, A.Raghunathan, S.Dey, D.Panigrahi, "Battery-Driven System Design: A New
Frontier in Low Power Design", International Conference on VLSI Design /ASP-DAC,
pp.261-267, Jan. 2002.

388
3.

Y. Doh et al.

L. Benini, A. Bogliolo, and G. De Micheli, “A Survey of Design Techniques for SystemLevel Dynamic Power Management,” IEEE Trans. on Very Large Scale Integration Systems, Vol.8, No.3, (2000) 299 –316
4. T. Kuroda et. al., "Variable supply-voltage scheme for low-power high-speed CMOS
digital design," IEEE Journal of Solid State Circuits, Vol. 33, No. 3, (1998) 454-462
5. Jay Heeb, “The next generation of StrongArm,” Embedded Processor Forum, MDR
(1999)
6. Intel Corporation, “Mobile Pentium III Processor in BGA2 and micro-PGA2 packages,”
Datasheet Order #245302-00 (2000)
7. “Introduction to Thumb,” ARM Documentation, Advanced RISC Machines Ltd.
8. MPC860 PowerPC Hardware Specification, MPC860EC/D, Motorola (1998)
9. Transmeta Corporation, “TN5400Processor Specification,” available at
http://www.transmeta.com (2000)
10. J. Pouwelse, K. Langendoen, H. Sips, “Dynamic Voltage Scaling on a Low-Power Microprocessor,” International Symposium on Mobile Multimedia Systems & Applications
(MMSA’2000) 157-164
11. A. P. Chandrakasan, S. Sheng, and R. W. Brodersen, “Low-power CMOS digital design,”
IEEE Journal of Solid-State Circuits, 27(4), (1992) 473-484
12. T. Pering and R. Brodersen, “Energy Efficient Voltage Scheduling for Real-Time Operating Systems," The 4th IEEE Real-Time Technology and Applications Symposium, Works
In Progress Session (1998)
13. T. Ishihara and H. Yasuura, “Voltage scheduling problem for dynamically variable voltage processors,” Proceedings of International Symposium on Low power Electronics and
Design (ISLED’98) 197-202
14. F. Yao, A. Demers, and S. Shenker, “A Scheduling Model for Reduced CPU Energy,”
IEEE Foundations of Computer Science (1995) 374-382
15. I. Hong, D. Kirovski, G. Qu, M.P otkonjak and M. B. Srivastava, “Power Optimization of
Variable Voltage Core-based Systems,” Proceedings of the 35th annual conference on
Design Automation Conference (DAC’98) 176-181
16. Gang Quan and Xiaobo (Sharon) Hu “Energy Efficient Fixed-Priority Scheduling for
th
Real-Time Systems on Variable Voltage Processors,” Proceedings of 38 Design Automation Conference (2001)
17. Y. H. Lee, Y. Doh, and C. M. Krishna, “EDF Scheduling Using Two-mode Voltage-ClockScaling for Hard Real-Time Systems,” Proceedings of Compilers, Architectures, and
Synthesis for Embedded Systems (CASES 2001)
18. C. M. Krishna and Y. H. Lee, “Voltage-Clock-Scaling Adaptive Scheduling Techniques
for Low Power in Hard Real-Time Systems,” IEEE Proceedings of Real Time Technology
and Applications Symposium (RTAS 2000)
19. Y. H. Lee and C. M. Krishna, “Voltage-Clock Scaling for Low Energy Consumption in
Real-Time Embedded Systems,” Real-Time Computing Systems and Applications
(RTCSA’99)
20. C.L.Liu and J. Layland, “Scheduling Algorithms for Multiprogramming in a Hard Realtime Environment,” Journal of the ACM, Vol.20, (1973) 46-61
21. M. Spuri and G. Buttazzo, "Scheduling Aperiodic Tasks in Dynamic Priority Systems",
The Journal of Real-Time Systems, Vol. 10, No. 2, (1996) 179-210

2009 International
International Conferences
Conference on
2009
on Embedded
Embedded Software
Software and
and Systems
Systems

A Genetic Algorithm Based Approach for Event Synchronization Analysis in
Real-time Embedded Systems
Yan Chen1, Yann-Hang Lee2*
1

Xiaofeng Xu3, W.Eric Wong4, Donghui Guo1
3

The School of Information Science and Technology,
Xiamen University, China.
2
Computer Science and Engineering Department
Arizona State University, U.S.A.

Concurrent tasks in real-time embedded systems
usually communicate through message queues or shared
variables. Enforced by RTOS, the operations on message
queues are atomic. Similarly the accesses to shared
variables should be guarded by semaphores. However,
multiple accesses to message queues or shared variables
may be interleaved in arbitrary orders. This leads to the
so called message races [3] and semaphore races [4].
Figure 1. shows an example of a semaphore race where
tasks 1 and 2 can take the semaphore in distinct orders in
different execution scenarios.
In the past two decades, many static and dynamic
approaches [3], [4], [5], [6] are proposed to detect race
conditions in concurrent programs. However, the
problem of detecting all feasible race conditions is NPhard in general cases [2]. Due to the scheduling and
timing characteristics in real-time embedded systems,
many race conditions detected by those approaches may
be infeasible in practice. For instance, in the example of
Figure 1. , if we assume the system uses priority-based
preemption scheduling algorithm, the priority of Task 1
is higher than Task 2 and they are released at the same
instant, then the execution sequence (1) would always
happen, while the execution sequence (2) would never
happen. So, in order to ensure the correctness of realtime embedded systems, it is necessary to know exactly
whether a race condition is feasible or not, i.e., whether a
specific case of synchronization order of some potential
events can happen in an execution. If so, it is necessary
to find a corresponding execution sequence which
satisfies the case for understanding, analysis and testing.
To find the possible execution sequences, we can
consider a parametric analysis about the timing instants
that tasks are released and external events occur. Such an
analysis is not only time consuming, but also have to
face the exponential increase of state spaces in the
execution model. In this paper, a new method, based on
a genetic algorithm, is presented to analyze event
synchronization in real-time embedded systems. There
are several contributions of this method:
1. Timed event automata (TEA) are presented to describe
the timing behavior of a target real-time system, and
each state in the TEA is a synchronization event.
2. A race condition graph (RCG) [17] is used to specify
the synchronization order of events which have races.
3. A genetic algorithm (GA) working with a simulationbased approach [14], [15] is used to verify whether the

Abstract
In real-time embedded systems, due to race conditions,
synchronization order between events may be different
from one execution to another. This behavior is
permissible as in concurrent systems, but should be
fully analyzed to ensure the correctness of the system.
In this paper, a new intelligent method is presented to
analyze event synchronization sequence in embedded
systems. Our goal is to identify the feasible sequence,
and to determine timing parameters that lead to these
sequences. Our approach adopts timed event automata
(TEA) to model the targeted embedded system and use a
race condition graph (RCG) to specify event
synchronization sequence (SYN-Spec). A genetic
algorithm working with simulation is used to analyze
the timing parameters in the target model and to verify
whether a defined SYN-Spec is satisfied or not. A case
study shows that the method proposed is able to find
potential execution sequences according to the event
synchronization orders.
Keywords – embedded system, event, synchronization
sequence, timed event model, race condition, genetic
algorithm.

1 Introduction
Real-time embedded systems are computing systems
that must react within precise time constraints to current
events in the application environment. A reaction that
occurs late could not only be useless but also
catastrophic. A real-time system must have some notion
of time. The time base can be absolute, corresponding to
a physical clock, or relative, based on specific events. A
synchronization primitive can be implemented to
establish and maintain ordered execution between
computational tasks [1]. However, because of race
conditions [2], which are caused due to the nondeterminism in the inter-tasks communication and
synchronization mechanisms, the order that the
synchronization operations take place may be different
from on execution to another. This can result in different
order of concurrent operations which may lead to
incorrect behavior and output. Even if the orders of
concurrent operations are permissible, they should be
analyzed thoroughly to ensure the correctness of the
systems [1].
978-0-7695-3678-1/09 $25.00 © 2009 IEEE
DOI 10.1109/ICESS.2009.48

The Department of Physics, Xiamen University, China.
4
The School of Engineering and Computer Science,
University of Texas at Dallas, U.S.A.
*
Corresponding Author: yhlee@asu.edu

201
199

events synchronization order specified (SYN-Spec) is
satisfied or not by the target TEA; if it is satisfied, a
corresponding execution sequence is given.

cannot possibly result in a data race because of schedule
restrictions enforced by event posts and waits. Fumihiko
[13] presented a new parallel computational model, the
LogGPS model, which was useful to analyze
synchronization costs of parallel programs that used
message passing. All these approaches do not consider
scheduling and timing, so they cannot be used for
checking the feasibility of a race in real-time system.
Johan and Anders [14], [15] presented a simulationbased approach to do the impact analysis of real-time
system. In their approach, a simulation model of a target
system, which is described by ART-ML, is extracted
from both static and dynamic analysis. Though it was
only used for timing analysis, the simulation-based
approach can be used to do much further analysis of realtime systems.
In our previous work, we have proposed a
technology based on model checking to verify race
conditions in real-time embedded systems [16], and a
race condition graph to analyze the concurrent program
behavior is presented in [17]. In this paper, we propose a
new method to analyze the event synchronization order
based on simulation-based technology according to race
conditions in the target systems.

Binary Semaphore s = available;
Task 1
……
semTake(s);
//Access the critical resource
semGive(s);
……

Task 2
……
semTake(s);
//Access the critical resource
semGive(s);
……

(a) A program with two tasks
T1 Semaphore T2

T1 Semaphore T2
t1

t1

g1

g1
t2
g2

t2
g2
(2)

(1)

(b) Two execution sequences with different event
synchronization orders
Figure 1.

3 Embedded System Modeling And Simulation

An example of semaphore race

The analysis approach for event synchronization
order is shown in Figure 2. . There are two inputs to the
analysis: a target system which is represented as TEA
and a specific synchronization order SYN-Spec which is
described by RCG. Then, an execution algorithm is used
to simulate the target TEA based on a priority-based
preemptive
scheduling
algorithm,
inter-task
synchronization and communication constructs and
virtual clock. At last, GA working with simulation is
used to analyze the timing parameters in the TEA to
verify whether the SYN-Spec is satisfied or not. If the
SYN-Spec is satisfied, a corresponding execution
sequence is obtained.

The rest of the paper is organized as follows. In
Section 2, some related works are given. Section 3
presents the real-time system modeling and simulation
with TEA. Section 4 describes how to use RCG to
specify the events synchronization order. In Section 5, a
GA is presented to working with simulation approach to
analyze the timing parameters. Then, a case study of
real-time dining philosopher program is shown up,
followed by a conclusion and the future work.

2 Related Works
A well-known approach which can be used to
analyze and verify the real-time embedded systems is
model checking technology. In this approach, a system is
represented by a kind of formal model based on finite
state machine, such as timed automata [7], timed I/O
automata [8], and linear hybrid automata [9] and so on.
Properties required are specified by a kind of temporal
logic language, such as LTL, CTL, and TCTL [10], or
even by another formal model. The algorithms for model
checking are typically based on an exhaustive state space
search of the model of the target system: for each state of
the model it is checked whether it behaves correctly, that
is, whether the state satisfies the desired specification. In
its most simple form, this technique is known as
reachability analysis. The disadvantage of model
checking technology is the state explosion problem. So,
its capacity is restricted by the huge program state
spaces.
Perry [11] described a system that automatically
detects races in a parallel program. In this approach, the
dynamic execution trace of the program was used to
build a task graph and logged points of event style
synchronization. David [12] developed a static analysis
method for determining which dependences in a program

Target System
(TEA)

SYN-Spec
(RCG)

Genetic Algorithm
Simulation
Execution Algorithm
Scheduling
Algorithm

Synchronization &
Communication
Constructs

Virtual Clock

No

Figure 2.

Yes

Architecture of the method

3.1 Syntax and Semantics of TEA
A real-time system is composed of multiple
concurrent tasks with a scheduling algorithm to control

202
200

the transition a is triggered, and then γ (a) is
executed.
The
syntax
of
γ
(a)
is:

CPU resource. These tasks synchronize and
communicate with each other by using message passing,
shared variables, etc. In this paper, timed event
automata (TEA) are used to describe the timing
behaviors of a real-time system. It includes
synchronization events (SYN-Event), such as sending
message (MS), receiving message (MR), taking
semaphore (ST), giving semaphore (SG) and task delay
(TD) and so on.
Definition 1: SYN-Event. Each SYN-Event e in a realtime embedded system is defined as a 5tuple e =< p, λ , o, x, i > , where p is the calling
task; λ maps e into one of the following types: MS, MR,
ST, SG, TD; o is the operating object of e, which may be
a message queue, a semaphore or even null; x is the
related data of e, which may be a string, an integer, a
parameter or even null; i is an unique identifier of e. The
relation of λ , o and x is shown in TABLE I. .
TABLE I.
Event type ( λ )

γ (a) ::= L := R | (γ 1 ∨ γ 2 )

MS

Message Queue

Data Sent

Message Queue

Data Received

SG

Semaphore

Null

ST

Semaphore

Null

TD

Null

Delay Time

L ::= x

and

3.2 Simulating the TEA
The simulator engine is based on four parts: scheduler,
synchronization & communication constructs, virtual
clock
and
execution
algorithm.
Let
S =< P, X , E , A,τ , μ , γ > be a TEA, ∀ p ∈ P has three
states: ready, run, wait. A priority-based preemptive
scheduling algorithm [20] is used to schedule the tasks
and control the states transitions of tasks. With the
synchronization & communication constructs which
include message-passing, semaphore and task delay,
∀ e ∈ E is processed according to the characteristic of
each type of SYN-Events during an execution.
For timing-accurate simulation, TEA uses timing
predicate τ to describe the amount of CPU time required
by a task to execute from one SYN-Event to the next, i.e.
the execution time of the code between these model
events. In this simulation approach, a virtual clock C is
used to calculate the execution time of TEA. When an
execution is started, C is increased steadily until the
execution is ended. Let ai(ei , ei+1) ∈ A be a transition, and
τ (a i ) = [te(ai)][min(ai), max(ai)] be a timing predicate of
ai, then the algorithm for time consume from ei to ei+1 is
shown in Figure 3. . In this algorithm, the function of
getGlobalClockValue() is used to read the value of
virtual clock C.

Related Variable (x)

MR

and

R ::= n | x | ( x ~ y ) | ( x ~ n) , where x and y are
variables, n ∈ R+; ~ ∈ { +, − , *, /}.

Synchronization Events

Operating Object (o)

,

In this paper, we focus on the synchronization and
communication constructs with FIFO queues of
asynchronous message passing, binary semaphore and
counting semaphore, but other synchronization events
can also be added according to the requirement of cases.
Definition 2: TEA. A real-time embedded system is
described
as
a
TEA
which
is
a
7tuple: S =< P, X , E , A, τ , μ , γ > with the following
restrictions. P is a set of tasks; ∀ p ∈ P is a periodic or
an aperiodic task, including task name, task ID, task
priority, and entry and exit nodes. X is a set of variables
which may be integer variables (int), timing parameters
(para), message queues (mq), binary semaphores (bs) or
counting semaphores (cs). E is a set of SYN-Events.
A ⊆ E × E is a set of transitions. ∀ a(es, et) ∈ A, such
that

Process timeConsume (Transition a)
t = te(ai);
cc = getGlobalClockValue();
pc = cc;
while (pc − cc) < t, do
pc = getGlobalClockValue();
end while
End process
Figure 3.

(1)
(2)
(3)
(4)
(5)
(6)
(8)
(9)

An algorithm for time consume

In a simulation, all tasks in TEA are concurrent tasks
and the execution algorithm of each task is shown in
Figure 4. . In this algorithm, statement (2) means a task
executes from a “start” node; statement (5) searches for
the next transition a according to es and the
corresponding local predicates; statement (6) calls
timeConsume() function to consume execution time from
current event to the next event; statement (7) is used to
do a set of assignments; statement (8) gets the next event
and statement (9) uses executeEvent() to execute the
event according to the event type. Note that an executing
task may be preempted at any time by the other ready
task with higher priority.

♦ τ (a) is a timing predicate describing the effective
execution time and timing constraint from a source
event es to a target event et of a, and τ (a) ::= [x] |
[x] [y, z]
| [x] [y, +∞ ], where
x ∈ R+ ∧ y ∈ R+ ∧ z ∈ R+ ∧ z ≥ y, [x] is the effective
execution time, and [y, z] is a timing constraint.
♦ μ (a) is a local predicate describing the triggering
condition of a, and μ (a)::= true | (x ~ n) | (x ~ y) |
¬μ | ( μ 1 ∧ μ 2), where x and y are variables;
n ∈ R+; ~ ∈ { ≤ , < , = , !=, > , ≥ }; ¬ and ∧ are
Boolean negation and disjunction respectively. If
μ (a)::= true, it is usually omitted.

Process taskExecution
es = “start”;
et = null;
while not et = “end”, do
a = getNextTransition(es);

♦ γ (a) is a set of assignments. If τ (a) ∧ μ (a) = true,

203
201

(1)
(2)
(3)
(4)
(5)

timeConsume(a);
executeAssignment( γ (a));
et = getNextEvent(a);
executeEvent(et);
es = et;
end while
End process
Figure 4.

Figure 5.

(6)
(7)
(8)
(9)
(10)
(11)
(12)

Assume the timing parameters x and y equal to 4 and
1, Figure 5. (c) is an execution of the TEA. First, T1 is
selected to run by the scheduler as it has a highest
priority in all ready tasks; e1 of T1 is a task delay event
which makes it to sleep for 4 time units, then the
scheduler picks up T2 to run. The event e1 of T2 is also a
task delay event which makes it to sleep for 1 time units,
and T2 goes on executing after waking up. Because the
execution time from e1 to e2 in T2 is 1 time unit, the total
time spent on the first transition of T2 is 2 time units.
During the transition from e2 to e3 in T2, T1 wakes up,
so T2 is preempted by T1 which has a higher priority.
When T1 is going to simulate e2, which is a taking
semaphore event, it is blocked because the semaphore
has been taken by T2, so context switches to T2 again.
After T2 gives the semaphore by e3, it is preempted by
T1 because the semaphore is ready now. Finally, T1
finishes firstly and T2 finishes later. In this simulation,
the time spent on the transition from e1 to e2 in T1 is to =
4 + 1 + 1 = 6, while the time spent on the transition from
e2 to e3 in T2 is to = 1 + 1 + 1 = 3, so the execution
satisfies the timing constraints of the TEA, i.e., the
execution is feasible. In Section 5, we will use a GA to
generate the values of timing parameters according to the
SYN-Spec.

An execution algorithm of each task in TEA

Let to(ai) be the total time spent on the transition ai,
we have to(ai) = te(ai) + td(ai) + tb(ai), such that:
♦ te(ai) is the effective execution time of ai. It is the
time spent executing run-time or system services on
its behalf, without being pended or delayed.
♦ td(ai) is the sleeping time of ai. It is caused by a task
delay event TD.
♦ tb(ai) is the time spent by ai blocked due to the
unavailability of a resource, such CPU, semaphore,
or message.
Therefore, a feasible execution of TEA should
satisfy the following inequality: ∀ ai ∈ A,
min(ai) ≤ to(ai) ≤ max(ai).
Figure 5. (a) and (b) show a simple example of TEA.
It includes two tasks T1 and T2, and the priority of T1 is
higher than T2. In these two tasks, there are timing
constraints between e1 and e2, x and y are two timing
parameters.
Priority(T1)=1;
Priority(T2)=2;
Binary semaphore: s1;
Timing Parameter: x, y;

T1

4 Specification Of Event Synchronization
Order

to=1
T2

T1

T2

An execution of TEA exercises a sequence of
synchronization events. Let S =< P, X , E , A,τ , μ , γ > be
TEA, an execution sequence generated by a simulation
is denoted as: Q = {<t, e> | t ∈ R+ is the happened time
of e, e ∈ E}. There are two characteristics of Q:
♦ Q satisfies happened-before relation, denoted as
HB
“ ⎯⎯→
⎯ ”, which is a partial order over the SYNEvents and shows the sequence of events that
potentially affect one another [19].
♦ Q has a synchronization order, which is a total order
over all of the SYN-Events of an execution.
With a postmortem approach [3], an execution
sequence is analyzed to detect race conditions. In this
section, RCG is used to specify the synchronization
order of SYN-Events which have races.

TD(4)
to=1

[1]

[1]

e1:
TD(x)

e1:
TD(y)

TD(1)

[1][1, 10]

td=4

to=2

[1][1, 10]

e2:
ST(s1)

e2:
ST(s1)

[2]

[2]

e3:
SG(s1)

e3:
SG(s1)

[1]

[1]

ST(s1)
te=1
te=1
te=1
ST(s1)

SG(s1)

4.1 Race Set
Let Q be an execution sequence. A race set of Q is
HB
given as a triple RS = < ER , ⎯⎯→
⎯ , ⎯RD
⎯→ >, where ER
HB
is a finite set of events and ER ⊆ E and ⎯⎯→
⎯ and

to=2
SG(s1)

(a)

Examples of a timed event model and an potential
execution sequence

⎯RD
⎯→ are relations defined over ER.

(b)
to=1

The race related events in the set of ER, are the events
that have direct relationship with race conditions in Q.
The race-dependence relation, denoted by ⎯RD
⎯→ ,
shows the relative order in which events execute and the
race dependence with each other. In this paper, we focus
on message races and semaphore races. Assume there
are 3 events a, b, c ∈ E, i.e., a, b and c are events in the

te=1

(c)

204
202

SYN-sequence Q:
♦ Assume a and b are sending message events, c is the
receive message event, and there is a message race
between events a, b with respect to c, i.e., c may
receive the message from a first in one execution or
even receive the message from b first in another
execution. If a comes before b in the Q, then
a ⎯RD
⎯→ b, otherwise, b ⎯RD
⎯→ a.
♦ Assume a and b are taking semaphore events, and
there is a semaphore race between events a and b,
i.e., event a may take the semaphore firstly in one
execution or b may take the semaphore firstly in
another execution. If a comes before b in the Q, then
a ⎯RD
⎯→ b, otherwise, b ⎯RD
⎯→ a.

ST(T3, 2, s3) ⎯RD
⎯→ ST(T4, 4, s3);
ST(T4, 2, s4) ⎯RD
⎯→ ST(T5, 4, s4);
ST(T1, 4, s5) ⎯RD
⎯→ ST(T5, 2, s5)};
Figure 7.

Let G=<V, L> be an RCG with respect to an
execution sequence Q, and G’=<V’, L’> be a SYN-Spec.
If every vertex v ∈ V’ also belongs to V and every edge
l ∈ L’ also belongs to L, i.e., G’ belongs to G, denoted by
G’ ⊆ G, then we say that Q satisfies the SYN-Spec G’,
denoted by Q |= G’.
Proposition. A real-time system S described by TEA
satisfies a SYN-Spec G described by RCG, if and only if
there is at least one execution sequence Q obtained by
simulating TEA and Q |= G.

4.2 Race Condition Graph

5 Genetic Algorithm Based Approach for
Parametric Analysis

HB
Let RS = < ER , ⎯⎯→
⎯ , ⎯RD
⎯→ > be a race set of an
execution sequence Q. An RCG is a graph G = <V, L>,
where V = ER is the set of vertices of the graph G and
L ⊆ V2 is the set of relations of V. There are two kinds
of L in an RCG:
1) If events a ∈ ER, b ∈ ER and a ⎯HB
⎯→ b, a solid
arrow “ ⎯
⎯→ ” is used to show their happenedbefore relation in the RS.
2) If events a ∈ ER, b ∈ ER and a ⎯RD
⎯→ b, a dashed
arrow “- - ->” is used to show their racedependence relation in the RS.

Because of the non-determinacy of real-time
embedded systems, there usually have some parameters
unknown which influence the timing of the execution. In
this paper, a GA based approach is used to search
heuristically these timing parameters in a TEA. The goal
of the analysis is to identify a set of timing parameters
that result in a specific SYN-Spec. If no such timing
parameters can be found, we can conclude that the SYNSpec is not a valid one.
5.1 Chromosome Encoding
Assume there are n timing parameters in the TEA. In
the GA approach, each timing parameter xi (i ≤ n) is
represented as a binary string with the length of Li ,
which is decided by the range and precision of xi . A
chromosome is composed of a set of binary strings,
which is shown in Figure 8. .

The SYN-Spec, i.e., the synchronization order of
SYN-Events, can be described by an RCG. For example,
Figure 6. shows a SYN-Spec of the dining philosopher
case (which will be introduced in Section 6).
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e4:
ST(s4)

x2

x1
b11

Figure 6.

The corresponding race set of the example

b12

…

L1

Figure 8.

An example of SYN-Spec

b21

b22
L2

xn
…

…

bn1

bn2

…

Ln

A chromosome encoding

5.2 Fitness Function
The fitness function gives a quantitative measure of
the suitability of the generated timing parameters for the
purpose of satisfying the SYN-Spec. Let G’=<V’, L’>
be a SYN-Spec. According to the proposition mentioned
in Section 4.2, if the TEA satisfies G’, then there is a
new RCG G=<V, L> derived from a new execution
sequence obtained by simulating the TEA, and G’ ⊆ G.
Therefore, in this paper the fitness function is used to
measure the similarity of G’ and G, i.e., the similarity of
two RCGs.
In order to know the similarity of two RCGs, it needs
to calculate the difference between them. Figure 9.
shows an algorithm to calculate the difference between
a SYN-Spec G’ and an RCG G derived from a new
execution sequence obtained by simulating the target

In this SYN-Spec, there are 5 tasks (T1, T2, …, T5),
5 binary semaphores (s1, s2, …, s5), and 10 ST events.
Assume the notation λ (p, i, o) denotes the ith SYN-event
in the task p operating on the object o, the
corresponding race set of this example is shown in
Figure 7. .
RS = { ST(T1, 2, s1) ⎯HB
⎯→ ST(T1, 4, s5);
ST(T2, 2, s2) ⎯HB
⎯→ ST(T2, 4, s1);
ST(T3, 2, s3) ⎯HB
⎯→ ST(T3, 4, s2);
ST(T4, 2, s4) ⎯HB
⎯→ ST(T4, 4, s3);
ST(T5, 2, s5) ⎯HB
⎯→ ST(T5, 4, s4);
ST(T1, 2, s1) ⎯RD
⎯→ ST(T2, 4, s1);
ST(T2, 2, s2) ⎯RD
⎯→ ST(T3, 4, s2);

205
203

TEA. The statement (1) initializes a variable named
difference. For each edge l’=<v’, u'> from G’, the
statement (3) aims to find the same vertices v and u in G
according to the SYN-event information described by v’
and u'. If it fails to find v or u in G, then we say that the
difference between G and G’ is infinity. The statement
(4) and (5) show that if the synchronization relation
between v and u isn’t the same as v’ and u’, then the
difference is increased.
Process calculateDifference (G’, G)
// G’ = <V’, L’ > is a SYN-Spec of the TEA;
// G = <V, L > is an RCG derived from an new
// execution sequence of the TEA;
set the difference = 0 initially;
for each edge l’=<v’, u' > ∈ L’
get the vertices v = v’ and u = u’ from V;
if the synchronization relation between v and u
isn’t the same as l’, then
difference = difference + 1;
end if
loop
return difference;
End process
Figure 9.

else
calculate the difference between G’ and G’;
end if
else
continue;
end if
loop
Step 3: Genetic operations.
Select one or two chromosome(s) from the population with
a probability based on fitness to participate in genetic
operations: selection, crossover, and mutation. Renew
individual chromosome(s) with specified probabilities based
on the fitness.

(1)
(2)
(3)
(4)

Step 4: Termination.
If the number of generations exceeds the upper limit
specified, then return fail; else go to step 2.

(5)
(6)
(7)
(8)

Specially, if a chromosome cause deadlock during a
simulation, the algorithm will mutate the chromosome
and repeat the simulation again and again until the
system runs smoothly.

Figure 10. Analysis process of the GA

6 A Case Study

An algorithm of calculating the difference
between two RCGs.

As a case study, a real-time dining philosopher
program, which is implemented on the VxWorks RTOS,
is used for the case study. There are 5 dining
philosophers with the same priorities: T1, T2, T3, T4
and T5. 5 chopsticks are represented by 5 binary
semaphores s1, s2, s3, s4, and s5. To eat their dinner, T1
takes s1 and s5, T2 takes s2 and s1, T3 takes s3 and s2,
T4 takes s4 and s3, T5 takes s5 and s4. Thus, there have
5 semaphore races in the program. In addition, to
represent a possible thinking time before picking up a
chopstick, there is a task delay event for random time
before each semaphore taking operation. So, there are
10 timing parameters in the system: x1, x2,…,x10. In
the study, we assume that for each timing parameter x1,
there has 1s ≤ x1 ≤ 8s; the timing constraint between
each task delay event and semaphore taking event is [1,
+∞].
Figure 11. shows the T1 described by TEA, the
effective execution time of each transition is calculated
from traces of original executions on the VxWorks OS
and a target processor board. The time unit is second.
Other tasks are described by TEA similarly. The static
data of the case is shown in TABLE II. .

Therefore, in the GA, the fitness of a chromosome
(i.e., a set of timing parameters) is dominated by the
difference value calculated using the algorithm shown
in Figure 9. : if the difference value between the SYNSpec G’ and a new RCG G becomes smaller, then G’
becomes more similar with G, i.e., the corresponding
chromosome has higher contribution and suitability; if
the difference value equals to 0, then G’ ⊆ G, i.e., the
SYN-Spec is satisfied by the target TEA.
5.3 Termination Condition
The genetic algorithm is terminated at the following
conditions: (1) it has successfully found a group of
timing parameters with which an execution sequence
generated by simulating the target TEA satisfies with the
SYN-Spec; (2) the number of generations exceeds the
upper limit and the genetic algorithm is terminated, at
the situation we consider that the target model does not
satisfy the SYN-Spec.
5.4 Analysis Process
Let S =< P, X , E , A,τ , μ , γ > be TEA and M is a set
of timing parameters. The process of analysis is
described in Figure 10. .
Step 1: Initialization.
Generate a set of initial chromosomes randomly.
Step 2: Calculate fitness.
for each chromosome, do
decode it and get a set of values X;
substitute X for M in S;
simulate S and get a new execution sequence Q;
if all timing constraints is satisfied in Q, then
generate a new RCG G from Q’;
if G’ ⊆ G, then
output the Q and X;
return success;

Figure 11. T1 described by TEA

206
204

TABLE II.

In this case study, the crossover rate of GA based
approach is 0.7, the mutation rate of GA is 0.001, the
revision rate is 0.5 and the number of initial population
of chromosomes is 6. After simulating the target TEA,
the result is shown in the TABLE III. . The result shows
that SYN-Spec 1 and 2 are satisfied by the TEA but SYNSpec 3 and 4 are not. SYN-Spec 3 is infeasible because a
global circle is included in SYN-Spec 3 [17], while the
SYN-Spec 4 causes a deadlock and the event e4 (take the
2nd chopstick) can never succeed.

Static Data of the case study

Items
Processes
Events
Transitions
Variables
Parameters

Number
5
35
40
15
10

Figure 12. shows 4 SYN-Specs described by RCGs,
which mean the different orders of taking chopsticks of
5 dining philosophers. For example, in SYN-Spec 1, T1
takes s1 before T2, T2 takes s2 before T3, T3 takes s3
before T4, while T5 takes s4 before T4 and T1 takes s5
before T5. The purpose of the case study is to verify
whether these event synchronization orders are possible
or not.
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

TABLE III.

e 4:
ST(s4)

Result of the case study

SYNSpec No.

Result

Generations
of GA

1

Yes

61

2

Yes

578

3
4

No
No

Inf
Inf

Values of Timing
Parameters
x1=1, x2=1, x3=5,
x4=8, x5=2, x6=5,
x7=7, x8=1, x9=3,
x10=3
x1=1, x2=1, x3=7,
x4=7, x5=8, x6=1,
x7=6, x8=1, x9=3,
x10=2
None
None

7 Conclusion

(1) SYN-Spec 1
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

In this paper, a simulation-based analysis with GA
based search is presented to analyze event
synchronization in real-time embedded systems. The
TEA presented is composed of SYN-Events but not
system states. Instead of using reachability analysis, the
target TEA is simulated to generate execution sequences.
Via analyzing execution sequences with a postmortem
method, it is able to check whether a SYN-Spec
described by RCG is satisfied or not. In the mean time, a
GA based approach is used to analyze timing parameters
in the target TEA. Experiments show that the method
proposed can be used to find execution sequences
according to the event synchronization order defined by
user. As a consequence, the execution sequence defined
by SYN-Spec can be indentified. Also, test sequences
can be generated to verify the system operations.

(2) SYN-Spec 2

References
[1] N. Suri, M.M. Hugue, C.J. Walter. Synchronization
issues in real-time systems. Proceedings of the IEEE.
Vol.82(1), pp: 41-54. Jan 1994.
[2] R.H.B. Netzer and B.P. Miller. What are Race
Conditions? Some Issues and Formalizations, ACM
Letters on Programming Languages and Systems, Vol.
1(1), pp. 74-88. 1992.
[3] K.C. Tai, Race Analysis of Traces of Asynchronous
Message-Passing Programs, Proc. of ICDCS’97.
Baltimore, Maryland, USA. pp.261-268, May 1997.
[4] P.N. Klein, H.I. Lu and R.H.B. Netzer. Detecting Race
Conditions in Parallel Programs that Use Semaphores,
Algorithmica, Springer-Verlag New York Inc. Vol.35,
pp. 321-345. 2003.
[5] S. Savage, M. Burrows and G. Nelson. Eraser: A
Dynamic Data Race Detector for Multithreaded

(3) SYN-Spec 3
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

(4) SYN-Spec 4
Figure 12. Specifications of events synchronization

207
205

[6]

[7]
[8]

[9]

[10]

[11]

[12]

programs, ACM Transactions on Computer Systems,
Vol. 15(4), pp. 391-411, 1997.
E. Pozniansky and A. Schuster. Efficient On-the-Fly Data
Race Detection in Multithreaded C++ Programs,
PPoPP’03, San Diego, California, USA. pp. 179-190. Jun
2003.
R. Alur, C. Courcoubetis, D.L. Dill. Model Checking for
Real-Time Systems, IEEE LICS, 1990.
DK Kaynar, N Lynch, R Segala, F Vaandrager. Timed
I/O automata: A mathematical framework for modeling
and analyzing real-time systems. In: Kaynar DK, Lynch
N, Segala R, Vaandrager F, eds. Proc. of the 24th IEEE
Int’l Real-Time Systems Symp. Washington: IEEE
Computer Society, pp. 166-177, 2003.
F. Wang. Symbolic Parametric Safety Analysis of Linear
Hybrid Systems with BDD-like Data-Structures. IEEE
Transactions on Software Engineering, IEEE Computer
Society. Vol.31(1), pp. 38-51. Jan 2005.
F. Wang, G.-D. Huang, F. Yu. TCTL Inevitability
Analysis of Dense-Time Systems: From Theory to
Engineering. IEEE Transactions on Software Engineering,
IEEE Computer Society, Vol. 32(7), July 2006.
P.A. Emrath, S. Chosh, D.A. Padua. Event
synchronization analysis for debugging parallel
programs. In Proceedings of the 1989 ACM/IEEE
conference on Supercomputing. Reno, Nevada, United
States. pp: 580-588. 1989.
D. Callahan, K. Kennedy and J. Subhlok, Analysis of
event synchronization in a parallel programming tool, in
Proceedings of the second ACM SIGPLAN symposium
on Principles & practice of parallel programming, Seattle,
Washington, United States, pp: 21-30. 1990.

[13] F. Ino, N. Fujimoto and K. Hagihara, LogGPS: a parallel
computational model for synchronization analysis, in
Proceedings of the second ACM SIGPLAN symposium
on Principles & practice of parallel programming,
Snowbird, Utah, United States, pp: 133-142. 2001.
[14] J. Andersson, J. Huselius, C. Norström and Anders Wall,
Extracting Simulation Models from Complex Embedded
Real-Time Systems. In Proc. of the International
Conference on Software Engineering Advances. Tahiti,
French Polynesia. pp.7-17. Oct. 29. 2006.
[15] J. Andersson, A. Wall, and C. Norström. A framework
for analysis of timing and resource utilization targeting
complex embedded systems. In ARTES - A Network for
Real-Time research and graduate Education in Sweden,
Editor: Hans Hansson, pp. 297–329. Uppsala University,
2006.
[16] Y.H. Lee, G. Gannod, K.S. Chatha, and W.E.Wong,
Timing and Race Condition Verification of Real-time
Systems, Progress Report, 2003.
[17] Y. Chen, Y.H. Lee, W.E. Wong and D.H. Guo, A Race
Condition Graph for Concurrent Program Behavior. Proc.
of ISKE’08, Xiamen, Fujian, China. pp: 662-667. Nov.
2008.
[18] Y. Lei and R.H. Carver, Reachability Testing of
Concurrent Programs, IEEE Transactions on Software
Engineering, Vol.32(6), pp. 382-403, 2006.
[19] L. Lamport. Time, Clocks, and the Ordering of Events in
a Distributed System. Communications of the ACM,
Vol.21(7), pp. 558-565, 1978.
[20] C.L. Liu, J.W. Layland. Scheduling Algorithms for
Multiprogramming in a Hard Real-time Environment(J).
Journal of ACM, Vol.20(1), pp. 46-61. 1973.

208
206

Telecommunication Systems 28:3,4, 453–474, 2005
 2005 Springer Science + Business Media, Inc. Manufactured in The Netherlands.

An Efficient Scheduling Discipline for Packet Switching
Networks Using Earliest Deadline First Round Robin ∗
DEMING LIU

deming@kiyon.com

Kiyon Inc., 4225 Executive Square, Suite 290, La Jolla, CA 92037, USA
YANN-HANG LEE
yhlee@asu.edu
Department of Computer Science and Engineering, Arizona State Univeristy, Tempe, AZ 85287, USA

Abstract. This paper addresses a frame-oriented scheduling discipline, EDF-RR (earliest deadline first
round robin), for OQ (output-queued) switch architecture and data traffic consisting of fixed-length cells.
Bandwidth reservation for an active session is performed by holding a number of cell slots for the session
in a repeatedly-transferred frame. Each cell that is going to be transferred in the frame is assigned a virtual release time and a virtual deadline according to the bandwidth reservation scheme. The transmitting
order of the cells in the frame is determined by non-preemptive non-idling EDF algorithm so that cells of
a backlogged session in the frame are distributed as uniformly as possible. Through the analysis applying
real-time scheduling theory and network calculus as well as network simulation, EDF-RR takes the advantage of O(1) computational complexity, and possesses tight delay bounds and lenient buffer requirements.
The proposed scheduling discipline is appropriate for distributed real-time systems as we show that sessions
can be configured based on message traffic models and deadline requirements. Also, a modified version of
EDF-RR, called EDF-DRR, can be applied as traffic regulator when jitter requirements exist among active
sessions.
Keywords: packet switching networks, round robin scheduling

1.

Introduction

Distributed real-time systems are experiencing tremendous advancement driven by new
technologies such as powerful computing devices, microelectronics, and high speed networks etc. In addition, transporting more data traffic with hard real-time requirements
becomes a necessity for many critical applications. One example is the future avionics systems. Flight instruments distributed in the entire aircraft will be integrated to
share information. They can also cooperate with each other to process an unprecedented
amount of information pertaining to performance, safety and efficiency. It is inevitable
that a QoS-guaranteed and versatile network infrastructure will be required in the next
generation of aircrafts [4], such as Airbus A380.
QoS can be generally specified as demands in throughput, delay, jitter and loss rate.
Currently packet switching network is the dominating approach to provide performance∗ This work was sponsored in part by the Federal Aviation Administration (FAA) via grant DTFA03-01-

C-00042. Findings contained herein are not necessarily those of the FAA.

454

LIU AND LEE

guaranteed services. In packet switching networks, data traffic is transmitted along connections established between source and destination nodes rather than routing as does
in IP networks. The strategies to guarantee communication performances include bandwidth reservation, traffic regulation, and deterministic service discipline. The first two
establish a service capability sufficient for the traffic and allow no bursty traffic to disrupt
the service performance temporarily. On the other hand, service discipline is the most
significant part to ensure the guaranteed performance of packet-based switches.
Typical QoS-guaranteed service disciplines are based on rate allocation implemented by packet scheduling algorithms. GPS (generalized processor sharing) is the
ideal rate allocating discipline with absolute fairness and, however, cannot be implemented in practice since it is based on pure flow model with the assumption of infinite
divisibility of data traffic. However, it is often used as the benchmark to evaluate the
characteristics of practical service disciplines. Basing on OQ (output-queued) switches,
a number of service disciplines [Zhang, 16] have been proposed aiming at providing deterministic communication performance. All these disciplines can be regarded as PGPS
(packet based GPS) [Demeres et al., 3] in a wide sense. An efficient service discipline must address both computational complexity and bound tightness. Low computational complexity is preferred since a switch scheduler must work at line speed. At the
same time, real-time traffic may have relatively stringent performance requirements desiring tight QoS bounds. Generally there are two types of service disciplines, RR-based
(round-robin) and deadline-based. RR-based disciplines such as Weighted Round-Robin
(WRR) [Katavenis et al., 8], Deficit Round-Robin [Shreedhar and Varghese, 13] and
Elastic Round-Robin [Kanhere et al., 7] have the low computational complexity as O(1)
whereas they cannot provide satisfactory worst-case performance bounds. Deadlinebased service disciplines such virtual clock [Zhang, 15], WFQ (weighted fair queuing)
[Parekh and Gallager, 12], WF2 Q (worst case fair weighted fair queuing) [Bennet and
Zhang, 1], WF2 Q+ [Bennet and Zhang, 2] and delay-EDD (earliest due date) [Ferrari
and Verma, 5] can provide relatively tighter QoS bounds and, however, require higher
computational complexity.
Besides scheduling disciplines, a transmission unit can be of variable or fixed
lengths. Allowing variable length units may improve bandwidth utilization. However, it
can impair performance in terms of latency since latency is closely related to the maximum packet size for most service disciplines [Zhang, 16]. Even though most RR algorithms are easy to implement, they are subject to a few problems, for instances, unfair
bandwidth partition and possible long worst-case latency, when the transmission units
are of variable length. On the other hand, deadline-based FQ (fair queuing) algorithms
can provide relatively better fairness and latency guarantees. They, however, have higher
computational complexity, e.g., O(N) for WFQ and WF2 Q, and O(log N) for WF2 Q+.
Although most schedulers support variable-length packets, switching networks
supporting only fixed-length packets excel at transferring performance-guaranteed traffic. Service disciplines supporting fixed-length transmission units, in which a long
packet is partitioned into short fixed-length cells and cells are the basic transmission
unit as that is done in ATM networks, take the advantage of fairness in sharing band-

EFFICIENT SCHEDULING DISCIPLINE

455

width. This perspective can be understood by considering task scheduling in real-time
operating systems (RTOS). Almost all RTOSes support task preemption, such that an
urgent task does not have to be blocked by a running task with less urgency, even though
preemptions incur context-switching overhead. In the same way, dividing long packets
into short fixed-length cells avoids long waiting times for urgent backlogged sessions
and thus enhances fairness and latency performance.
In this paper we propose an approach combining RR (round robin) and EDF (earliest deadline first) scheduling in order to compromise computational complexity and
performance bounds. The proposed service discipline, called EDF-RR (earliest deadline
first round robin), adopts a round robin scheduling for repetitive frames and uses EDF
to determine the transmission order of fixed-length cells within a frame. The scheduling
order and frame structure are established when sessions are initiated. When a session
has no backlog, the assigned cell slots can be skipped in aid of maximizing bandwidth
utilization. Our analysis shows that the approach possesses computational complexity
of O(1) and tight delay bounds.
The rest of this paper is organized as follows. Section 2 gives a description of the
proposed EDR-RR discipline and its essential characteristics. In section 3, we discuss
the delay bounds and buffer requirements of EDF-RR for both single-node and multiplenode cases. Section 4 presents some simulation results. In section 5, several applicationrelated issues are addressed. Finally, conclusions are given in section 6.
2.

Earliest deadline first round robin scheduling

We expect the communication mechanism in distributed real-time systems to be composed of many end nodes and switch nodes. End-to-end sessions are established prior
to packet transmission. When a packet from a source node is ready to be transmitted
following the path of an established session, it is partitioned into fixed-length cells that
are sent to the network respecting a traffic regulation mechanism, such as a leaky-bucket
model. According to the traffic volume and deadline requirement, each session must
reserve a minimum amount of bandwidth along its path. Thus transmission slots will
be assigned to backlogged sessions at each switch node. Arriving cells, which may be
queued if the corresponding buffer is backlogged, are delivered to the corresponding
output links subject to the scheduling discipline. For the convenience of the following
analysis, we assume a cell arriving at the very beginning of a cell slot in the form of
pulse.
EDF-RR (earliest deadline first round robin) proposed in this paper is a framebased RR scheduling discipline for fixed-length cell traffic. Similar to WRR and WFQ,
EDF-RR is a weight-based service discipline. A frame is composed of a number of cell
slots. A session reserves a portion of bandwidth by occupying a number of cell slots
in a frame. Instead of transferring in an arbitrary order the cells reserved by all active
sessions in a frame, EDF-RR tries to select an order such that the cells from an active
session are distributed in the frame uniformly.

456

LIU AND LEE

We define that a frame consists of n fixed-length cells. A cell has, for convenience,
the length of 1 in terms of the time it takes to transmit a cell from a buffer to the output
port. In other words, the length of a cell slot is 1 unit time. Let K be the total number
of active sessions and mi be the number of cells shared by session i in the frame, where
1  i  K. Assuming that each cell slot in the frame belongs to one of the K sessions,
obviously the following equation is true:
K

mi

= 1.

(1)

K

1
= 1,
p
i=1 i

(2)

i=1

n

With a simple transform we have

where session period of session i is defined as pi = n/mi . The scheduler tries to serve
a session i cell in every pi . Note that pi is not necessary to be an integer and 1/pi is the
bandwidth reserved for session i (the fraction of a cell slot that is allocated to session i
in average).
In the next step we will determine the transmitting order of the n cells belonging
to the K sessions in the n-cell frame by non-emptive non-idling EDF (earliest-deadlinefirst) algorithm. EDF is widely used in scheduling multiple tasks in a processor-sharing
environment. It is a dynamic-priority scheduling algorithm in that priorities are assigned
to tasks according to the deadlines of their current instances. A task is assigned to the
highest priority if its deadline is the earliest among all pending tasks. A task, whose
requests occur periodically and whose executions are required to complete in a regular
interval after their requests, is called periodic task. The regular interval is called relative
deadline for the corresponding task. A task is preemptive if an executing instance of a
task is interrupted by a task request with higher priority and resumes running later. It is
non-preemptive otherwise. A scheduling algorithm is non-idling if the processor cannot
be idle as long as there exists some pending task.
The schedulability of K periodic tasks under preemptive EDF algorithm is described by the well-known result as follows [Liu and Layland, 11; Stankovic et al., 14].
For a given set of K periodic tasks whose relative deadlines are equal to their periods,
they can be scheduled if and only if
K

ei
 1,
p
i=1 i

(3)

where pi and ei are the period and execution time, respectively, of task i.
If preemption is not allowed in scheduling a set of tasks, the conclusion of EDF
schedulability is not true any more since non-preemption may cause priority inversion,
i.e., a task with a higher priority is waiting for execution while a lower-priority task is

EFFICIENT SCHEDULING DISCIPLINE

457

running. The priority inversion delay introduced by non-preemption can be analyzed by
the following lemma.
Lemma 1. If a set of synchronous periodic tasks with relative deadlines equal to their
periods are schedulable by preemptive EDF algorithm, then any task instance can finish
its execution within p + emax after it is released under a non-preemptive non-idling EDF
algorithm, where p is the task’s period and emax is the maximum execution time of all
tasks.
Proof. At first we prove that priority inversion from non-preemption can happen at
most once for any task instance.
Suppose that t is an instant at which a deadline missing occurs with nonpreemption non-idling EDF. Then there must exist some task instance with deadline
greater than t 
getting execution before t. Otherwise, since deadline missing happens
at t, we have K
i=1 t/pi ei > t (note that there is no idle time before t), leading to
K
i=1 ei /pi > 1. This contradicts (3). Let [ts , te ] be the last interval before t during
which a task instance with deadline after t gets executed. Then there must be at least one
priority inversion occurring in (ts , te ). If not so, only instances releasing in [te , t] and
having deadlines
K deadline missing happens
 in [te , t] can get executed in [te , t]. Since
(t
−
t
)/p
e
>
t
−
t
,
leading
to
at t, we have K
e
i i
e
i=1
i=1 ei /pi > 1. This contradicts (3). Therefore there is some priority inversion occurring in (ts , te ). Once the lower
priority instance completes at te , however, only instances that request after ts and with
deadline before or at t can execute in [te , t]. In the scheduling of these instances, there
may be more priority inversions, but only one execution in [ts , te ] can has an impact on
the deadline missing at t. Since at most one other task instance can cause a task instance
to miss its deadline and any task instance’s execution time is less than or equal to emax , a
task instance with period p can finish its execution within (p + emax ) after its release. 
Letting ei equal to 1 (1  i  K) in (3) and then comparing (2) with (3), we
may schedule the transmission of the cells in the frame as if we schedule synchronous
periodic tasks by preemptive EDF. Thus we can conclude that exactly one cell (it may be
broken with preemption allowed) of session i can be transferred in each session i period
with the first session period starting from the beginning of the frame. However, since a
cell is an indivisible transmission unit that the scheduler must deal with atomically, preemption is impracticable in scheduling session cells. Therefore, in EDF-RR algorithm,
we use non-preemptive non-idling EDF scheduling the transmitting order of cells in the
frame. This may lead to additional transmission latency from priority inversion. But the
finish time of delivering a cell can still be bounded. Below we first describe the EDF-RR
discipline and then give results addressing its characteristics.
EDF-RR discipline.
(a) An n-cell frame is partitioned among K active sessions (all unused bandwidth can
be considered as one dummy session) such that session i (1  i  K) transfers mi

458

LIU AND LEE

Table 1
Scheduling order in a frame.
1

2

1

3

1

2

1

2

1

3

cells in the frame. Session i is assumed to have cell transmission requests at time
jpi (suppose a frame starts from time 0) with corresponding deadlines at (j + 1)pi ,
where pi = n/mi and j = 0, 1, . . . , mi −1 (we will call those requests and deadlines
as virtual requests and virtual deadlines, respectively). The transmission order of the
n cells in the frame is determined by non-preemptive non-idling EDF algorithm.
(b) If all K sessions are backlogged, the frame is transferred repeatedly such that in
every frame the cells follow the order given in (a).
(c) If a backlogged session turns to be idle during the frame being transmitted, the rest
cell slots of the session in the frame are skipped. The remaining backlogged sessions
are transferred in the same order as in (a).
Table 1 shows an example giving the cell transmission order of a 10-cell frame in
which, sessions 1, 2 and 3 shares 5, 3 and 2 cell slots, respectively. The numbers in table
denote the relevant sessions.
Lemma 2. There is no idle time in a frame if all K active sessions are backlogged with
EDF-RR.
Proof. Since a cell has one unit length, thetotal number of cells that are scheduled for
transmission in an interval [0, t] is at most K
i=1 t/pi  according to EDF-RR. Suppose
t is the beginning of the first idle slot in a frame (if there are virtual requests in (t − 1, t],
they cannot be scheduled in the slot [t − 1, t] according to EDF-RR; actually virtual
requests in (t − 1, t] do not exist; otherwise, the slot beginning with t cannot be idle),
therefore we have

K 

t
< t + δ,
(4)
pi
i=1
K

where δ is any small positive number. Since K
i=1 t/pi 
i=1 t/pi , the following
holds by combining with (4):
K

δ
1
<1+ .
p
t
i=1 i

Since δ is arbitrarily small, we have K
i=1 1/pi < 1, which conflicts with (2).

(5)


A work-conserving algorithm means that there is no idle time as long as there are
some backlogged sessions. From EDF-RR discipline and lemma 2, obviously EDF-RR
is a work-conserving discipline.

EFFICIENT SCHEDULING DISCIPLINE

459

Lemma 3. With EDF-RR, for any backlogged session i, 1  i  K, mi cells can be
completely transferred within an n-cell frame.
Proof. It is sufficient to consider only the case 
that all K active sessions are backlogged. According to EDF-RR, there are totally K
i=1 mi (= n) virtual requests in an
n-cell frame (there are mi session periods for session i, 1  i  K). Since there are no
idle
cell slot in a frame by lemma 2, the n slots in the frame is enough to accommodate

the K
i=1 mi cells from the K active sessions.
Lemma 4. With EDF-RR, if a session has a virtual request at time t for a backlogged
session, a cell from the session will be transmitted within [t, t + p + 1], where p is the
session period.
Proof. With non-preemptive non-idling EDF as EDF-RR has, according to lemma 1, a
session’s virtual request at t with session period p can finish its execution before (t +
p + emax ), where emax is the maximum execution time of all session instances. Also
considering that all session instances have transmission demand of a cell with length 1

(emax = 1), lemma 4 follows.
Let Si (a, b) be the number of cells transferred in the interval [a, b] for session i
under EDF-RR. The following theorem gives the minimum communication capacity
guaranteed for each session.
Theorem 1. In any busy interval of session i scheduled by EDF-RR,
Si (0, t2 ) − Si (0, t1 ) >

mi
(t2 − t1 ) − 2,
n

where the busy interval of session i begins at time 0 and, t1 and t2 are any two time
instants in the busy interval with t2  t1 .
Proof. Firstly we consider the case 1 that all active sessions are backlogged. Referring
to figure 1, a virtual request of session i occurs at tr with virtual deadline td . [tr , td ] is a
session period of session i. With lemma 4, the execution of the virtual request at tr can
be completed as early as shown in figure 1(a) or as late as shown in figure 1(b).

(a)

(b)

Figure 1. Earliest and latest executions of a session instance.

460

LIU AND LEE

Figure 2. Reference diagram of case 1 for theorem 1.

Figure 3. Reference diagram of mapping case 2 to case 1 for theorem 1.

Letting t ∈ [tr , td ), by figure 1(a), we have
Si (0, t)  Nr + 1,

(6)

where Nr is total number of session i periods in [0, tr ]. With figure 1(b), we have
Si (0, t) > Nd + 1,

(7)

where Nd is the total number of session i periods in [0, td ]. Applying (6) and (7) for t1
and t2 , as shown in figure 2, respectively, gives
Si (0, t1 )  Nr1 + 1,
Si (0, t2 ) > Nd2 − 1.

(8)
(9)

Si (0, t2 ) − Si (0, t1 ) > (Nd2 − Nr1 ) − 2.

(10)

Subtracting (8) from (9) gives

According to the definitions of Nd2 and Nr1 , it is obvious that
(Nd2 − Nr1 )pi = td2 − tr1 .

(11)

Since td2 − tr1 > t2 − t1 , we have
(Nd2 − Nr1 )pi > t2 − t1 .

(12)

Hence, Nd2 − Nr1 > (t2 − t1 )/pi . Combining with (10) gives
Si (0, t2 ) − Si (0, t1 ) >

t2 − t1
mi
(t2 − t1 ) − 2.
−2=
pi
n

(13)

Now we extend the result to the case 2 that there are some idle sessions during the
busy interval of session i. We do the mapping from case 2 to case 1 such that an instant
t at which a cell is being transferred in case 2 is mapped into the instant τ at which the

EFFICIENT SCHEDULING DISCIPLINE

461

same cell is being transferred in case 1. Case 1 is a hypothesis for analyzing case 2.
Please refer to figure 3. for the mapping of two time instants, t1 and t2 .
Since in case 1 there are some empty time slots in [τ1 , τ2 ] whereas the empty time
slots are skipped in [t1 , t2 ] in case 2, we know τ2 − τ1  t2 − t1 . Hence,
mi
mi
(τ2 − τ1 ) − 2 
(t2 − t1 ) − 2.
Si (0, t2 ) − Si (0, t1 ) = Si (0, τ2 ) − Si (0, τ1 ) >
n
n
Combining case 1 and case 2, theorem 1 holds.

3.

Delay bound and buffer requirement analysis of EDF-RR

In this section, we will use network calculus, a mathematic analysis tool for networks,
to obtain delay bounds and buffer requirements of EDF-RR for both single-node and
multiple-node cases. Also an intuitive explanation is given for the analysis results.
To analyze delay bounds and buffer requirements, traffic models must be established to specify session traffic characteristics such as average rate and burstiness. (σ, ρ)
model is one of them [Zhang, 16]. A session traffic flow is said to satisfy (σ, ρ) model
if there are at most σ + ρt units of traffic during any interval t, σ and ρ denote the
burstiness and average rate of the traffic, respectively. For example, traffic flow coming
from the traffic regulator of leaky bucket satisfies (σ, ρ) model. According to the definition of arrival curve, the statement that a traffic flow satisfies (σ, ρ) model has the same
meaning that the traffic flow is constrained by arrival curve σ + ρt. In this paper we
assume session traffic has arrival curve σ + ρt and traffic unit is fixed-length cell. In
the following text we do not distinguish server and switch by the convention of network
calculus literature. Please refer to appendix A [Le Boudec, 9, 10] for the concepts and
results of network calculus that are used in this paper.
Lemma 5. If session i occupies mi cell slots in an n-cell frame passing through
an EDF-RR server, the EDF-RR server offers the service curve (mi /n)t − 2 for
session i.
Proof. By the definition of strict service curve and theorem 1 it is trivial to know that
an EDF-RR server offers a strict service curve (mi /n)t − 2 to session i. Proposition A.1
states if a server offers β(t) as strict service curve to a flow, then it also offers β(t) as
service curve to the flow. Therefore lemma 5 holds.

In the single-node case in which a session traffic flow goes through an EDF-RR
server, we have theorems 2 and 3 addressing delay bounds and buffer requirements,
respectively.
Theorem 2. If session i traffic flow is constrained by arrival curve σi + (mi /n)t, the
delay it experiences passing through an EDF-RR server is not more than (σi + 2)(n/mi )
cell slots.

462

LIU AND LEE

Proof. The session i traffic flow is constrained by arrival curve αi (t) = σi + (mi /n)t.
From lemma 5 the server offers the service curve βi (t) = (mi /n)t − 2 for session i. By
theorem A.2 the maximum delay session i traffic flow experiences is


sup δi (s) = sup inf τ  0: αi (s)  βi (s + τ )
s0

s0



mi
n
= sup inf τ  0: σi 
τ − 2 = (σi + 2) .
n
mi
s0



Theorem 3. If session i traffic flow constrained by arrival curve σi + (mi /n)t passes
through an EDF-RR server without buffer overflow, the buffer size that the server needs
for session i is not more than σi + 2 cells.
Proof. The session i traffic flow is constrained by arrival curve αi (t) = σi + (mi /n)t.
From lemma 5 the server offers the service curve βi (t) = (mi /n)t − 2 for session i. By
theorem A.1, the maximum backlog is given by



sup αi (s)−βi (s +τ ) = sup{σi +2} = σi +2.
s0

s0

Now we consider the multiple-node case in which a session traffic flow traverses
multiple EDF-RR servers. We define weight of session i in an EDF-RR server as mi /n
if session i passing through the EDF-RR server occupies mi cell slots in an n-cell frame.
A minimum weight server for session i is the EDF-RR server who has the minimum
value of weight among the sequence of EDF-RR servers that session i traverses.
Lemma 6, theorems 4 and 5 as follows are based on the assumption that session i
passes through a number of EDF-RR servers without any buffer overflow, the minimum
weight server for session i among the first k EDF-RR servers allocates m∗i cell slots in
its n∗ -cell frame to session i, and the session i traffic flow is constrained by arrival curve
(m∗i /n∗ )t + σi .
Lemma 6. The output flow from the kth sever for session i is constrained by arrival
curve (m∗i /n∗ )t + σi + 2.
Proof. Since m∗i /n∗ is the minimum weight among the first k EDF-RR servers, it is
obvious that any one of the first k servers offers βi (t) = (m∗i /n∗ )t − 2 as service curve
for session i. According to the concatenation property by theorem A.4, the first k servers
totally offer a service curve for session i as follows:

k
k 
 ∗
	
	
mi
βi (t) =
t −2 .
βik (t) =
n∗
j =1
j =1
According to the definition of min–plus convolution operator ⊗, we have the total
service curve βik (t) = (m∗i /n∗ )t − 2k after a simple computation.

EFFICIENT SCHEDULING DISCIPLINE

463

Since the source flow of session i is constrained by arrival curve αi (t) = σi +
(m∗i /n∗ )t, the arrival curve constraining the traffic flow after the kth EDF-RR server is
given as follows by theorem A.3:
 m∗

αi (t +u)−βik (u) = ∗i t +σi +2k.
n
0ut

αi (t)βik (t) = sup



Theorem 4. The delay that the session i traffic flow experiences from the source to the
kth server is not more than (n∗ /m∗i )(σi + 2k) cell slots.
Proof. The session i traffic flow after the (k − 1)th sever is constrained by arrival curve
(m∗i /n∗ )t + σi + 2(k − 1) from lemma 6. Obviously the kth server offers as service
curve (m∗i /n∗ )t − 2 for session i. Applying theorem 2 gives the upper delay bound of
the source flow of session i passing through the first k servers, i.e., (n∗ /m∗i )(σi + 2k). 
Theorem 5. The buffer size needed by the kth server for session i is not more than
(σi + 2k) cells.
Proof. The session i traffic flow after the (k − 1)th sever is constrained by arrival curve
(m∗i /n∗ )t + σi + 2(k − 1) from lemma 6. Obviously the kth server offers (m∗i /n∗ )t − 2
as service curve for session i. Applying theorem 3 gives the buffer requirement of the

kth server, i.e., σi + 2k.
We can give some intuitive explanations to the above analysis. According to EDFRR in a server, after transferring a cell for session i the server will wait for at most two
session periods before transferring the next cell for session i. It is easy to understand this
point by referencing figure 4. The effect is that an EDF-RR can introduce the worst-case
delay of 2 session periods for a session flow. The maximum delay, (n∗ /m∗i )(σi + 2k),
which session i flow experiences when traversing k EDF-RR servers, can be portioned to
two portions. The first portion results from source traffic flow’s burstiness and the second
one comes from the k EDF-RR servers. This idea can be understood by referencing
figure 4. A GPS flow with rate m∗i /n∗ passes through k + 1 buffers in a chain in which
the amounts of data buffered are uncertain. The first buffer with size σi corresponds to
the original flow’s burstiness. Each of rest k buffers with size 2 corresponds to an EDFRR server. A cell traversing the chain will experience the worst-case delay if the buffer
is full when the cell enters anyone of the k + 1 buffers.

Figure 4. Intuitive explanation of performance of EDF-RR servers.

464

LIU AND LEE

Considering different bandwidth shares for session i in multiple servers along session i, we may have tighter delay bound than that given in theorem 4. The upper delay
bound of the session i flow constrained by passing through k EDF-RR servers can be as
low as
k

nj
σi n∗
+2
,
m∗i
m
ij
j =1

where nj is the frame size at server j and mij is the number of cell slots reserved for
session i in a frame at server j . Please note that we assume that any server transfers a
cell per cell slot and the transmission of a cell is done at the beginning of a cell slot in
form of pulse.
Apparently if we ignore the computation for judging whether a session buffer is
empty or not, EDF-RR has the computational complexity of O(1). On the other hand,
it is necessary to have empty-buffer checking taken into account from the point of view
of implementation. In the worst case that only one of K active sessions is backlogged,
K − 1 times of empty-buffer checking are needed during the transmission of a frame
according to the work-conserving property of EDF-RR. Thus the computation complexity is O(K) instead of O(1) in this scenario. The simple way to avoid this overhead is
to prevent the empty slots of the nonbacklogged sessions from being applied by other
backlogged sessions. But this strategy would change the service discipline into nonworkconserving, thus wasting bandwidth. In the more efficient method, those empty slots can
Table 2
Comparison of several work-conserving disciplines.
Service
discipline

Traffic
model

Multi-node
delay
k


Multi-node
jitter
k


Buffer
requirement
k


Comp
complexity

D-EDD

bi (·)

VC

(σi , ρi )

k Lm

σi + kLm
+
ri
j =1 Cj

σi + kLm
ri

σi + kLm

O(logN)

WFQ& WF2 Q

(σi , ρi )

k Lm

σi + kLm
+
ri
j =1 Cj

σi + kLm
ri

σi + kLm

O(N)

WF2 Q+

(σi , ρi )

σi + kLm
ri

σi + kLm

O(logN)

SCFQ

(σi , ρi )

k Lm

σi + kLm
+
ri
j =1 Cj
σi + kLm
+
ri

σi + kLm
+
ri

σi + kLm

O(logN)

k


k


σi + 2kLc

O(1)

j =1

j =1

EDF-RR

(σi , ρi )

dij

Kj

j =1

Lm
Cj

k Lc

σi + 2kLc
+
ri
j =1 Cj

j =1

dij

(Kj − 1)

σi + 2kLc
ri

bi

j =1

dij

O(N)

Lm
Cj

EFFICIENT SCHEDULING DISCIPLINE

465

be used to transfer traffic from best-effort sessions. If there do not exist best-effort sessions, a set of sessions can be assumed to be nonempty by some heuristic approach. If
some of the found sessions are really nonempty, they will use those empty slots. Otherwise these empty slots will be discarded. No mater the empty slots are employed or not,
the worst-case performance of real-time sessions will not be impaired.
To compare the performance of DEF-RR, we list the measures of multiple-node
delay, buffer space requirement, delay jitter, and computation complexity in table 2. The
measures for D-EDD, VC, WFQ, WF2 Q, and SCFQ are partially from [Zhang, 16]:
– D-EDD: delay earliest due date [Ferrari and Verma, 5];
– VC: virtual clock [Zhang, 15];
– WFQ&WF2Q: weighted fair queuing [Parekh and Gallager, 12] and worst-case fair
weighted queuing [Bennet and Zhang, 1];
– WF2 Q+: enhanced version of WF2 Q [Bennet and Zhang, 2];
– SCFQ: self-clocked fair queuing [Jamaloddin Golestani, 6].
In the previous discussion of EDF-RR discipline, we assume the transfer operation
of a cell is completed in very short time at the beginning of a cell slot and thus the real
transmitting time of a cell is thus ignored. For interpreting table 2, we assume that the
delay for a packet or cell is the time interval from the head of the packet or cell entering
into the source node to its tail leaving the destination node. Therefore for conveniently
comparing with other disciplines we consider the transmitting time of cells in table 2.
Cj is the link speed of the j th server of k servers that session i traverses. Kj is the total
number of active sessions for the j th server. ri is the reserved rate for session i such that
ρi  ri . Lm is the maximum packet length and Lc the cell length. dij is the delay for
session i at node j .
4.

Simulation results

In this section, we will compare EDF-RR, by simulation, with other two weightbased service disciplines, WRR (weighted round robin) [Katavenis et al., 8] and WFQ
(weighted fair queuing) [Parekh and Gallager, 12]. The network topology for the simulation is shown in figure 5. In the following simulation, there are 4 sessions established
in the network, denoted by source–destination pairs S1 –D1 , S2 –D2 , S3 –D3 and S4 –D4 .
From figure 5, links Ci Ci+1 (i = 1, . . . , 5) are multiplexed by more than one session. The weights allocated to multiple sessions through the multiplex links are given in
table 3. These multiplex links can be scheduled according to EDF-RR, WFQ or WRR.
In addition to weights, frame sizes are required to be specified when EDF-RR is applied
to the multiplex links. The frame sizes of EDF-RR for each multiplex link are also given
in table 3. In the 3 scenarios of simulation as follows, exponential distributed traffic
consisting of a sequence of packets is generated at the source node and destined to the
destination node for each of the 4 sessions. End-to-end delay from source to destina-

466

LIU AND LEE

Figure 5. The network topology for the simulation (the numbers on links are bandwidths).

(a)

(b)
Figure 6. Packet delay and queue length curves for scenario 1: (a) packet delay curves; (b) queue length
curves.

EFFICIENT SCHEDULING DISCIPLINE

467

Table 3
Weight allocation of sessions for multiplex links.

S1 –D1
S2 –D2
S3 –D3
S4 –D4
Frame size1

C1 C2

C2 C3

C3 C4

C4 C5

C5 C6

25
34
15
46
120

25
34
15
46
120

25
34
15
46
120

25
34
15
–
74

25
34
15
–
74

1 This parameter is only effective for EDF-RR.

(a)

(b)
Figure 7. Packet delay and queue length curves for scenario 2: (a) packet delay curves; (b) queue length
curves.

468

LIU AND LEE

(a)

(b)
Figure 8. Packet delay and queue length curves for scenario 3: (a) packet delay curves; (b) queue length
curves.

tion and queue length at link C1 C2 for session S1 –D1 are observed and compared for
EDF-RR, WRR and WFQ.
In scenario 1, we assume all packets along the 4 sessions have the identical length,
i.e., the cell length (100 bytes). The same simulation loading with the same traffic is
performed 3 times so that each time one of EDF-RR, WFQ and WRR is applied to
all multiplex links. The observed packet delay and queue length of session S1 –D1 for
EDF-RR, WFQ and WRR are shown in figures 6(a) and (b), respectively. In the two
figures we may pay attention to the fact that the curves of EDF-RR and WFQ are almost completely overlapped. Thus EDF-RR and WFQ have fairly close packet latency
and queue length when they deal with fixed-length packets. On the other hand, WRR
exhibits larger fluctuation of packet latency and queue length than EDF-RR and WFQ,
thus inferrior to both EDF-RR and WFQ.

EFFICIENT SCHEDULING DISCIPLINE

469

As the performance difference between WRR and EDF-RR is apparent, comparing
EDF-RR and WFQ is of interest in the next step. In scenario 2, all packets along the
4 sessions are assumed to have the same length of 3 cells (300 bytes). For EDF-RR,
packets are broken into cells before entering the network and reconstructed by assembling cells at destinations. For WFQ, packets are treated atomically. We are concerned
about the delays of packets instead of cells. The packet delay curves of session S1 –D1
are displayed in figure 7(a). Figure 7(b) gives the queue length curves of session S1 –D1
on link C1 C2 . By comparing packet delay curves and queue length curves, it is obvious that EDF-RR has better performance than WFQ in terms of both packet latency and
buffer requirement in this scenario. As long packets are “chopped” into small chunks,
i.e., cells, EDF-RR provides more fairness than WFQ.
WFQ supports variable length packets. In scenario 3, all assumptions in scenario 2
are kept except that session S1 –D1 has packet length of 7 cells (700 bytes) and all other
three sessions, 6 cells (600 bytes). Packet delay and queue length curves for session
S1 –D1 are shown in figures 8(a) and (b), respectively. From the curves in figure 8,
the properties we observed in scenario 2 still hold in scenario 3. Furthermore, since
we increase packet length in scenario 3, the packet delay and queue length differences
between EDF-RR and WFQ are much larger than those of scenario 2. We may conclude
that EDF-RR exhibits much better performance than WFQ as packet size increases.
5.

Applications of EDF-RR

EDF-RR is a service discipline for OQ switches applying for connection-oriented network environment where fixed-length cell is the basic traffic unit. From the RR property
of this policy, its computational complexity is O(1). We must note that determining a
schedule in a frame involves deadline comparisons in EDF-RR. Thus the results of section 3 for sure are valid only if a session is in steady state. We say a session is in steady
state if during an interval at each server the session passes through there is no update
to the weight of the session (it does not matter if updates take place very fast). Thus,
when a new session starts up, a portion of the bandwidth kept in the idle session can
be assigned for it. Similarly, when a session terminates, its bandwidth can be merged
into the idle session. Then, a new frame schedule is computed and is started at a frame
boundary. For the existing sessions, their weights are not changed and the delay bounds
are assured.
Rescheduling transmission order is needed only when there are sessions to be established, cancelled or updated, which happen infrequently from the perspective of users.
The associated overhead can be ignored since a new transmission order can be computed
in parallel to the current transmission, and is swapped at the next frame boundary.
It is worthy to consider the application of EDF-RR in the message communications
of a distributed real-time system. Consecutive messages are sent from a source application to a corresponding destination application. Assume that each message consisting of
P cells is with a deadline requirement, and will be routed through k switches to reach its

470

LIU AND LEE

destination. To utilize the proposed EDF-RR scheme, a session of proper weight and the
required buffers must be established to facilitate the message communication.
Let us keep the same assumptions as in theorems 4 and 5. We define the session
delay for a packet as the interval between the packet head entering the source node and
the packet completely leaving the destination. If a packet of session i can be broken
into P cells, the packet’s delay bound along the k EDF-RR servers that session i travels
through can be expressed as (σi + P − 1 + 2k)(n∗ /m∗i ). As a result, the session must be
constrained by the following rules:
(1) (σi + P − 1 + 2k)(n∗ /m∗i )  D, where D is the message deadline of session i.
(2) Session i is constrained by arrival curve σi + (m∗i /n∗ )t.
In other words, bandwidth reservation for session i in a server is performed by
reserving mi cells of the n-cell frame. The criteria of how to determine the frame size
can be based on the facts that (i) the frame size of a server should not be too smaller,
otherwise we cannot guarantee that sufficient granularity for allocating bandwidth, and
(ii) the computation of rescheduling a frame increases as frame size increases. Thus
large frame size may introduce long session setting-up or updating time.
Besides meeting deadline requirement, the other concern in real-time systems is
message jitter. A session may require that the message delivery jitter be minimized.
This can be accomplished if the message cells only use the scheduled slots regardless
any empty slots in a frame. Now let us consider a scenario of existing mixed types of sessions, in which some sessions only need to meet their deadline requirements and others
have additional jitter constraints. Then the question comes up whether we can let a set of
backlogged sessions share the empty cell slots in a frame and the other set of backlogged
sessions keep their original scheduled slots such that all sessions’ delay bound and buffer
requirement cannot be deteriorated. We introduce an EDF-DRR (earliest-deadline-first
dual round-robin) discipline that is intended to address this requirement.
EDF-DRR discipline.
(a) Schedule a frame exactly the same as EDF-RR when all active sessions are backlogged. We call this frame as big frame. In addition to keep a big frame, EDF-DRR
maintains a dynamic small frame in which the cell slots are composed of all the cell
slots of currently backlogged non-jitter-constrained sessions in the big frame in the
same order.
(b) The server iterates to transfer the cells according to the schedule in the big frame.
When a cell slot in the big frame is empty, rather than skipping them to transfer the
next available cells in the big frame as does in EDF-RR, EDF-DRR selects the next
cell according to the schedule in the small frame. If the size of the small frame is
zero, no cell is transmitted during the empty slots.
The jitter property of EDF-DRR is given by lemma 7.

EFFICIENT SCHEDULING DISCIPLINE

471

Lemma 7. In any busy interval of jitter-constrained session i with EDF-DRR, we always have
t2 − t1
t2 − t1
− 2 < Si (0, t2 ) − S1 (0, t1 ) <
+ 2,
pi
pi
where the busy interval of session i begins at time 0 and, t1 and t2 are any two time
instants in the busy interval with t2  t1 .
Proof. The proof is similar to that of theorem 1. Please refer to figures 1 and 2.
S2 (0, t2 ) − S1 (0, t1 ) > (t2 − t1 )/pi − 2 is directly obtained for theorem 1. Applying
(6) and (7) for t2 and t1 , respectively, as shown in figure 2, respectively gives
Si (0, t2 )  Nr2 + 1,
Si (0, t1 ) > Nd1 − 1.

(14)
(15)

Subtracting (15) from (14) gives
Si (0, t2 ) − Si (0, t1 ) < (Nr2 − Nd1 ) + 2.
Because session i period is only scheduled in the big frame,
tr2 − td1
t2 − t1

.
(Nr2 − Nd1 ) =
pi
pi
Hence, Si (0, t2 ) − S1 (0, t1 ) < (t2 − t1 )/pi + 2 holds.



EDF-DRR is a hybrid of work-conserving and non-work-conserving policies.
Since the cells of jitter-constrained sessions in the big frame are always transferred relatively at the same positions, the performances of these sessions are kept unchanged no
matter how many idle slots there are in big frames. On the other hand, delay bound and
buffer requirement of non-jitter-constrained sessions are still guaranteed when some sessions are idle. This is due to the fact that the cell slots allocated to backlogged non-jitterconstrained sessions in a big frame are occupied by the same sessions no matter there are
any idle sessions or not. If there are some empty cell slots in a big frame in the case of existing idle sessions, more cells may be transferred for non-jitter-constrained sessions in
these cell slots. Obviously if there are some backlogged non-jitter-constrained sessions,
the EDF-DRR server works in work-conserving mode and if no backlogged session is
non-jitter-constrained, the EDF-DRR server may work in non-work-conserving mode.
6.

Conclusions

In this paper, we proposed an applicable packet-scheduling algorithm for distributed
real-time systems. The algorithm is a round-robin based discipline for OQ switches
in packet switching networks with fixed-length cells being the basic transmission and
scheduling unit. Bandwidth reservation for a session at a switch is carried out through
the assignment of a number of cell slots in a frame. The transferring order of cells in a

472

LIU AND LEE

frame is determined using non-preemptive non-idling EDF algorithm so that cells of a
backlogged session in the frame are distributed as uniformly as possible. In other words,
EDF-RR tries to simulate GPS with the constraint of the fixed cell transmission order.
Through the analysis and simulation of delay bound and buffer requirement for both
single-node and multiple-node cases, we show that EDF-RR reveals high-performance
characteristics in terms of requirements of real-time communications.
We also discuss some aspects related to EDF-RR’s applications in this paper. The
criteria of defining session bandwidth requirement can be extracted from message’s traffic model and deadline requirement. Selecting a suitable frame size is a trade-off among
a number of factors depending on practical applications. Even though EDF-RR is a
work-conserving discipline, a modified version, called EDF-DRR, can support additional jitter requirements for message sessions.
Appendix A. Basics of network calculus
This appendix gives an introduction to the main concepts and results of network calculus
that are used to estimate performance of network nodes [Le Boudec, 9, 10].
Model for data flows. Data flows are conveniently described by means of the cumulative function R(t), defined as the number of bits or traffic units seen on the flow in time
interval [0, t].
Min–plus convolution. Let f and g be two functions or sequences. The min–plus convolution of f and g is the function


(f ⊗ g)(t) = inf f (t − s) + g(s) .
0st

Min–plus deconvolution. Let f and g be two functions or sequences. The min–plus
devonvolution of f by g is the function


(f  g)(t) = sup f (t + u) − g(u) .
0ut

Arrival curve. Given a wide-sense increasing function α defined for t  0, we say that
a flow R is constrained by α if and only if for all s  t, R(t) − R(s)  α(t − s). We say
that R has an arrival curve α, or also that R is α-smooth.
Service curve. Consider a system S and a flow through S with input and output functions R and R ∗ . We say that S offers to the flow a service curve β if and only if β is
wide-sense increasing and R ∗ (t)  R(t) ⊗ β(t).
Strict service curve. We say the system S offers a strict service curve β to a flow if,
during any backlogged period of duration u, the output of the flow is at least equal
to β(u).

EFFICIENT SCHEDULING DISCIPLINE

473

Proposition A.1. If a node offers β as a strict service curve to a flow, then it also offers β
as service curve to the flow.
Theorem A.1 (Backlog bound). Assume a flow, constrained by arrival curve α, traverses a system that offers a service curve β. The backlog R(t) − R ∗ (t) for all t satisfies


R(t) − R ∗ (t)  sup α(s) − β(s) .
s0

Theorem A.2 (Delay bound). Assume a flow, constrained by arrival curve α, traverses
a system that offers a service curve of β. The virtual delay d(t) for all t, satisfies d(t) 
h(α, β). h(a, b) is the maximum of all values of δ(s), where δ(s) is defined as


δ(s) = inf τ  0: α(s)  β(s + τ ) .
Theorem A.3 (Output flow). Assume a flow constrained by arrival curve α, traverses
a system that offers a service curve of β. The output flow is constrained by the arrival
curve α ∗ = α  β.
Theorem A.4 (Concatenation). Assume a flow traverses systems S1 and S2 in sequence.
Assume that Si offers a service curve of βi (i = 1, 2) to the flow. Then the concatenation
of the two systems offers a sevice curve of β1 ⊗ β2 to the flow.
References
[1] J.C.R. Bennet and H. Zhang, WF2 Q: Worst-case fair weighted fair queuing, in: Proc. of IEEE INFOCOM’96, CA (March 1996) pp. 120–128.
[2] J.C.R. Bennet and H. Zhang, Hierarchical packet fair queueing algorithms, IEEE/ACM Transactions
on Networks 5(5) (1997) 675–689.
[3] A. Demeres, S. Keshav and S. Shenker, Analysis and simulation of a fair queueing algorithm, Journal
of Internetworking Research and Experience (October 1990) 3–26.
[4] Draft of project paper 664 aircraft data network, Parts 1 to 8, ARINC specification 664, Aeronautical
Radio, Annapolis, MD (2002).
[5] D. Ferrari and D. Verma, A scheme for real-time channel-establishment in wide area networks, IEEE
Journal on Selected Areas in Communications (April 1990) 368–379.
[6] S. Jamaloddin Golestani, A self-clocked fair queueing scheme for broadband applications, in: Proc.
of IEEE INFOCOM’94, Toronto, Canada (June 1994) pp. 636–646.
[7] S.S. Kanhere, H. Sethu and A.B. Parekh, Fair and efficient packet scheduling using elastic round
robin, IEEE Transactions on Parallel and Distributed Systems 13(3) (2002) 324–336.
[8] M. Katavenis et al., Weighted round-robin cell multiplexing in a general-purpose ATM switch chip,
IEEE Journal on Selected Areas in Communications 9(8) (1991) 1265–1279.
[9] J.-Y. Le Boudec, Application of network calculus to guaranteed service networks, IEEE Transactions
on Information Theory 44(3) (1998) 1087–1096.
[10] J.-Y. Le Boudec and P. Thiran, Network Calculus: A Theory of Deterministic Queuing Systems for the
Internet (Springer, Berlin, 2001).
[11] C.L. Liu and J.W. Layland, Scheduling algorithms for multiprogramming in a hard-real-time environment, Journal of the Association for Computing Machinery 20(1) (1973) 46–61.

474

LIU AND LEE

[12] A.K. Parekh and R.G. Gallager, A generalized processor sharing approach to flow control in integrated
services networks: The single-node case, IEEE/ACM Transactions on Networking 1(3) (1993) 344–
357.
[13] M. Shreedhar and G. Varghese, Efficient fair queuing using deficit round-robin, IEEE/ACM Transactions on Networking 4(3) (1996) 375–385.
[14] J.A. Stankovic et al., Deadline Scheduling for Real-Time Systems: EDF and Related Algorithms
(Kluwer Academic, Dordrecht, 1998).
[15] L. Zhang, VirtualClock: A new traffic control algorithm for packet switching networks, in: Proc. of
ACM SIGCOMM’90, Philadelphia, PA (September 1990) pp. 19–29.
[16] H. Zhang, Service disciplines for guaranteed performance service in packet-switching networks, Proceedings of the IEEE 83(10) (1995) 1374–1396.

Schedulable Persistence System for Real-Time
Applications in Virtual Machine
Okehee Goh

Yann-Hang Lee

Ziad Kaakani

CSE, Arizona State University
Tempe, AZ

CSE, Arizona State University
Tempe, AZ

Honeywell International Inc.
Phoenix, AZ

ogoh@asu.edu

yhlee@asu.edu

ziad.kaakani@honeywell.com

ABSTRACT

1. INTRODUCTION

Persistence in applications saves a computation state that
can be used to facilitate system recovery upon failures. As
we begin to adopt virtual execution environments (VMs)
for mission-critical real-time embedded applications, persistence service will become an essential part of VM to ensure
high availability of the systems.
In this paper, we focus in a schedulable persistence system in VMs and show a prototype persistence system constructed on CLI’s open source platform, MONO. By employing object serialization, the system enables concurrent and
preemptible persistence operation, i.e., the task in charge of
persistence service runs concurrently with application tasks
and is a target of real-time scheduling. Thus, the execution
of application tasks can be interleaved with the operations
of persistence service, and the task timeliness can be guaranteed as the pause time caused by persistence service is
bounded. The experiment output on the prototyped system
illustrates that persistence service is appropriate for realtime applications because of its controllable pause time and
its optimized overhead.

Virtual Machines (hereafter, VMs), such as JVM[17] and
CLI (Common Language Infrastructure)[9], enable an abstract computing environment for program execution. Application programs are compiled into intermediate codes (bytecodes) to permit portability of ”write once, run everywhere.”
Besides that, applications written in Java[13] or CLI-compatible
languages are claimed secure due to type-safety and security
sandbox model [27]. The ensured safety, portability, and
reusability of OO languages make VMs attractive to embedded systems and have led to the introduction of many
VMs designed for embedded applications. One example is
Real-Time Speciﬁcation for Java (RTSJ)[4] which has been
established as a standard speciﬁcation of JVM to meet the
requirements of real-time embedded applications.
Along with the interests of employing VM in real-time
embedded systems, persistence of applications is becoming
a necessity as an approach to ensure high availability of
long-running embedded applications. Persistence in applications preserves a computation state of applications beyond
its lifetime. If a failure occurs, the systems can be recovered
by using the preserved computation state while minimizing any loss of computation. Without a support of persistence, system failures may force the systems to restart from
the beginning (cold start) or result in an inconsistent state
with which computation cannot advance. Despite of the advantages of persistence, there is a concern whether making
applications persistent may introduce an unpredictable latency that can impair the timeliness of real-time embedded
systems. For example, Monga et al. [19] shows in the experiment of persistence service using standard object serialization of .NET framework that serialization for 100 instances
of System.Int32 type takes about 100ms, which is intolerable to most real-time applications in the aspects of both
performance and pause time.
Before proceeding further, we ﬁrstly deﬁne some terminologies, frequently used in the rest of this paper. Persistence System and Persistence Service refer to a subsystem
in charge of making applications persistent, and an activity
of making applications persistent, respectively. Additionally, Persistent data and Persisted data indicate the data
that needs to be persistent among runtime data in applications, and the one that is saved through persistent service,
respectively.
Due to the time constraints that real-time applications
have, persistence service integrated to real-time systems should
not cause unpredictable blocking delays to application tasks.
To prevent unpredictable blocking delay, the service should

Categories and Subject Descriptors
C.3 [Special-purpose and application-based systems]:
Realtime and embedded systems; D.4.5 [Operating systems]: Reliability —Checkpoint/restart; D.4.7 [Operating
systems]: Organization and Design—Real-time systems and
embedded systems

General Terms
Reliability Performance

Keywords
schedulable persistence system, checkpoint/recovery, realtime applications, virtual machine, CLI

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
EMSOFT’06, October 22–25, 2006, Seoul Korea
Copyright 2006 ACM 1-59593-542-8/06/0010 ...$5.00.

195

Period of Persistence Increment

2. BACKGROUND AND RELATED WORKS

Pause of Persistence Increment

2.1 Checkpointing & Object Serialization in
VMs

Persistence Cycle

Figure 1: Scheduling Model of Schedulable Persistence System

be a schedulable object by allowing preemptivity and bounded
pause time. Thus, a Schedulable Persistence System can be
interleaved with real-time applications and both the timeliness and persistence of the real-time applications can be
guaranteed.
In this paper, we depict an approach of schedulable persistence service in VM environment. The persistence service
is taken by a separate task that runs concurrently with realtime application tasks. The service is divided into small portions (persistence increment, hereafter) to bound the pause
time caused by the service. As illustrated in Figure 1, the
scheduling model for the proposed approach is basically to
allocate CPU cycles to consecutive persistence increments
periodically. Hence, the whole persistence service becomes
preemptible after each persistence increment and its execution can be treated as a periodic task invoked in a persistence
cycle.
One of the issues raised while supporting preemptivity of
persistence service is to guarantee the consistency of persisted data. We deﬁne that a consistent persisted data is
a snapshot of all persistent data when the persistence service is triggered. With concurrent and preemptible persistence service, mutators (i.e. application tasks) are executed
concurrently with the persistence service and may update
the persistent data before the data becomes persisted completely. We address this issue by using write barrier in a
cost eﬀective manner.
The proposed approach of making persistence service schedulable along with real-time applications in VM environment
can be viewed as a part of eﬀorts to make VMs suitable
for real-time embedded systems. However, even if we consider the emerging RTSJ’s commercial products, PERC [1]
and Sun Real-Time Java System [25], there is no existing
VM that enables persistence of applications while considering the timeliness of real-time applications simultaneously.
Beyond VMs, a few previous works [16, 8] aimed at concurrent checkpointing for general real-time systems. These
approaches are to save a process memory content based on
architectural support and/or by using coarse grained write
barriers. Their limitations include non-portability of checkpoint data (process image) and/or the lack of semantics of
preserved data for alternate recovery processing.
In the following, we give a brief background on object serialization and a discussion of related works in Section 2. The
design approach for a schedulable persistence system is introduced in Section 3. In Section 4 and Section 5, we present
the details of the prototyped schedulable persistence system
and the experimental results. Finally, Section 6 draws a
simple conclusion.

196

Checkpoint based roll-back recovery is a fault-tolerant
technique to minimize the lost computation due to failures.
A checkpoint can be taken during failure-free execution and
a state of computation is saved. Upon a failure, the recovery operation uses the checkpoint to restore the computation state up to the moment that the checkpoint was taken.
Then, the computation can proceed.
Checkpoint can be done in the level of VM to save the
state of live objects which represent the computation state of
applications. Also, as suggested in [5] and [23], the applications’ execution state can be save as portable checkpoints of
thread objects. The saved state of data and thread objects
can be serialized by following VM’s standard data format
and then become readable by a VM in any architecture.
Object Serialization in Java and CLI-compatible languages
[22, 18] is a signiﬁcant functionality used for lightweight object persistence, and data marshaling/demarshaling in RMI
(Remote Method Invocation) or .NET Remoting programming. Object serialization transforms the state of objects
in memory into a sequence of bytes or other representations
suitable for transmission to ﬁle systems or communication
media. It can be a simple approach to support object persistence. According to object reachability, Persistent Objects
or Persistent Data include not only a persistent root object,
but also all reachable objects from the root object. During serialization all reachable objects from the root object
must be traced to generate persisted data. Then, deserialization restores objects from the persisted data. However,
the current approach for object serialization suﬀers several
drawbacks [10]:
• The procedure for serialization or deserialization runs
as a sequential operation of the invoking threads. Hence,
the normal thread operation can only proceed after
the serialization operation and may suﬀer a long pause
(the serialization operation cannot suspend in the middle of the procedure).
• Object serialization does not consider consistency of
persisted data. For example, if a thread serializes an
object graph (all reachable objects from a persistent
root object) while other threads mutate any objects in
the graph, the consistency of the persisted data in the
graph is not guaranteed.
Apart from the aforementioned drawbacks, the performance of the existing serialization operation in terms of time
and space is also a concern. The object serialization in both
JVM and CLI heavily uses Reﬂection mechanism of VM,
which allows managed code to retrieve information of ﬁelds,
methods, constructors for objects, classes etc. Reﬂection is
basically an interpreted operation on object meta data and
may incur a signiﬁcant overhead and performance penalty.

2.2 Efficient Object Serialization
There were several attempts to improve the performance
of Java Object Serialization in order to enhance RMI mechanisms. Haumacher et al. [20] suggested a serialization function in pure Java code. To remove the overhead in a wire
protocol, it minimizes the size of metadata in the serialized

updated in software, rather than depending on a speciﬁc
hardware. To reduce the cost of write barriers, the mirror
copy is mapped to separate physical memory pages while
checkpoint is underway. Hence, write barrier operations do
not need to be changed according to the presence of checkpoint. Employing a mirror copy can be a solution of checkpoint while concern the timeliness of applications. However,
the cost having extra memory blocks may not be tolerable
to resource constraint embedded systems, and the approach
is also limited to checkpoint memory content in OS level.
There have been a lot of research work focusing in the
scheduling issues for fault-tolerant real-time applications with
time-redundancy schemes [15, 12, 11], and [21]. Because the
time-redundancy schemes require extra time for fault detection and fault recovery, scheduling fault-tolerant real-time
applications are resorted to ﬁnding the WCET (Worst Case
Execution Time) of the applications with failure detection
and recovery, conducting schedulability study, and establishing eﬃcient scheduling algorithms in the presence of faults.

data by omitting data ﬁelds’ type information, name, etc.
To avoid any costly invocation of reﬂection mechanism in a
generic serialization operation, it supports only user-deﬁned
serialization, which requires programmers to specify methods to serialize/deserialize objects of each class. Breg et al.
[6] implemented a serialization function in native code by
using JNI. Their study shows that the overhead from Reﬂection and JNI is still high although the implementation in
native codes improves the performance to a certain extent.
In addition, a type-cache was employed in their approach
to reduce the overhead caused by repetitive invocations of
Reﬂection functions.

2.3 Orthogonal Persistent Object Systems
Making data persistent beyond the lifetime of applications may require extra programming eﬀort to save and restore data using speciﬁc programming constructs, for example, DB languages to access DBMS. Orthogonal Persistent System [2] aims at a transparent programming such
that no signiﬁcant programming eﬀort is required to handle persistent data in a database. In addition, it adopts
Lazy pointer swizzling and Incremental updates techniques
to reduce the latency due to restoring and saving the persistent objects. Lazy pointer swizzling is to update the loaded
object’s address (from persistent storage address to memory address) on demand. Furthermore, with incremental
updates, only updates, instead of the whole data objects,
are saved to a persistent storage since the latest persistence
service. Through the techniques, the orthogonal persistent
object system provides scalability on enterprise applications
that handle massive data. However, its unpredictable latency still makes the system inappropriate for real-time applications.

3. DESIGN APPROACHES
The design of the proposed persistence system is focused
on achieving three goals: to enable concurrent and preemptible persistence service with adjustable pause time while
ensuring consistency of persisted data, to make persistence
service eﬃcient for resource constrained systems, and to provide essential features necessary for fault recovery as well as
data recovery. We will limit the scope of the persistence
service to data persistence but not thread persistence for
applications. However, we claim that the characteristics of
real-time embedded applications, and inevitable initialization for external data during recovery make data persistence
the foremost concern for failure recovery.
We design a schedulable persistence system by employing
object serialization and extending the functions of VM environment. We use MONO, an open source development platform of .NET framework running on Linux platforms [28],
as an example platform to illustrate the details of our design. The beneﬁt of employing object serialization is portability because object serialization generates persisted data
with VM-aware standard format. Thus, the persisted data
can be used in both HW and time redundant fault tolerant
architectures. Serialization and Deserialization are implemented in native codes as a subsystem of CLI. This decision
is drawn to overcome the limitations of serialization provided as managed code including poor performance and the
inadequacy to be extended for preemptible and concurrent
serialization. Since we employ object serialization to achieve
persistence, we will refer serialization and serialized data interchangeably with persistence service and persisted data,
respectively.

2.4 Checkpointing for Real-Time Applications
There are few investigations on checkpointing memory
content of real-time application processes in OS level. Li
et al.[16] present a concurrent checkpointing algorithm in
which mutators are allowed to interleave with a checkpointing thread. The algorithm incurs a reduced latency by
checkpointing memory pages one at a time rather than checkpointing the whole memory space of mutators at once. The
consistency of checkpointed data is addressed by employing
a copy-on-write mechanism. The mechanism places writeprotection on memory pages when checkpoint starts. When
mutators try to update a memory page, the page is checkpointed before the update is applied. Although this approach can reduce the latency modestly, two limitations follow. Firstly, the granularity in a level of a memory page
is too coarse to satisfy the timely requirements of real-time
applications. Secondly, there is a signiﬁcant initialization
delay to enable MMU (Memory Management Unit) write
protection on the whole memory space of the application.
Cunei et al. [8] propose a concurrent checkpoint mechanism by employing a mirror copy of memory blocks. The
mirror copy works as follow: the checkpoint mechanism
maintains an auxiliary memory block as a mirror copy of
main memory. In order to have the mirror copy constantly
updated, update operations of mutators apply on both the
main memory and the mirror copy. When a checkpoint
starts, the updates on a mirror copy are suspended, and
then the data on the mirror copy is saved to a persistent
storage. Write barriers can be used to allow the mirror copy

3.1 Efficient Serialization Algorithms
The serialization operation consists of three steps, ﬁrstly
to obtain the state of objects, secondly to transform the state
into binary sequences, and then to write the serialized data
to memory, or ﬁles (persistent storage) [14]. The factors
aﬀecting the performance of serialization are:
• Reﬂection allows managed code to retrieve information of persistent objects’ ﬁelds. It is only way to obtain the state of objects in serialization implemented
in managed code, but it is very costly.

197

• The serialized data consist of metadata as well as states
of persistent objects. The metadata include type information, headers necessary to parse the states for deserialization. The excessive metadata increases IO overhead as well as the size of serialized data. The benchmark on object serialization implemented for .NET
Compact Framework [6] indicates that reducing the
size of metadata overhead helps improving the performance.

Serialized data includes not only the states of persistent
objects but also object header, which describe class information, ﬁelds’ types, ﬁelds’ names, and other header information, etc. We design a Serialization Protocol to deﬁne
the format of serialized data. The protocol minimizes the
amount of serialized data by reducing the amount of the
metadata to be saved with persisted data. This is done by,
ﬁrstly, not carrying a verbose format of metadata to specify
names and types of its ﬁelds in serialized data. Those information can be retrieved through persistent class map, which
is also generated during deserialization. Secondly, persistent classes are distinguished with unique class identiﬁcation (Class ID) and are maintained in a Persistent Class
Cache with detail information such as assembly version etc.
Serialized data of persistent objects carry only Class ID representing its class instead of verbose format of its class information. The cache is also serialized so that deserialization
can generate the same type of persistent class cache. The
serialized data is converted into binary stream, which is independent of the underlying hardware platforms.
To invoke serialization/deserialization operations, we employ a simple API of three static methods of class SPC.
AddRoot registers a persistent root object and returns a
persisted data ID. When the data needs to be persisted,
the second method SendPData is called. For deserialization, RecvPData is invoked with the ID of a persistent root
object which is used to locate the corresponding persisted
data.

• The execution performance in managed code is much
slower than native codes. To improve the performance,
Breg et al. [6] implemented serialization in native
codes using JNI. However, the overhead associated
with JNI is not negligible.
An eﬃcient serialization should avoid any reference to reﬂection mechanism and minimize the meta data information
saved in persistence storage. To further optimize the performance, we can adopt a native code implementation of
serialization service as internal functions of VM. To begin
the design, persistence class and ﬁelds should be declared.
Hence, programmers can apply their knowledge of the applications to select the class and ﬁeld of data that must be
made persistent. In C#, this can be done with additional
attributes, such as [SPersistent] and [SPersistentField]. As
shown in Listing 1 of a C# class declaration of a persistent
class with two persistent ﬁelds. Attribute in C# is associated with managed code in a form of Class Metadata, but
does not alter the managed code’s semantics. When managed code is loaded, the associated class metadata is placed
as in-memory data structure. In Java, similar to Serializable
interface, a persistence interface can be deﬁned to indicate
a class whose instances may be persistent.

Foo oKey , oValue ;
i n t nRootID1 , nRootID2 ;
// I n i t i a l i z a t i o n Phase f o r c o l d s t a r t
i f ( IsWarmRestart ()== f a l s e )
{
oKey
= new Foo ( ) ;
oValue = new Foo ( ) ;

[ SPersistent ]
c l a s s Foo
{
[ SPersistentField ]
public int i ;

// R e g i s t e r p e r s i s t e n t r o o t o b j e c t s
nRootID1 = SPC . AddRoot ( oKey ) ;
nRootID2 = SPC . AddRoot ( oValue ) ;

}
// I n i t i a l i z a t i o n Phase f o r warm r e s t a r t
else
{
oKey
= SPC . RecvPData ( nRootID1 ) ;
oValue = SPC . RecvPData ( nRootID2 ) ;
}

publ i c double d ;
[ SPersistentField ]
p u b l i c Bar o ;
}

Listing 1: Example declaration of a persistent class

// P e r i o d i c O p er a ti o n Phase
while ()
{
// Main o p e r a t i o n s
...
// S e r i a l i z e a l l p e r s i s t e n t o b j e c t g r a p h s
SPC . SendPData ( ) ;
}

To implement serialization/deserialization as internal functions in VMs, we will not be able to use reﬂection mechanism to retrieve underlying information of persistent classes/ﬁelds. Our approach is to maintain a Persistent Class
Map internally for persistent classes. The map facilitates
the introspection function of a reﬂection mechanism and
contains persistent ﬁelds’ type information and oﬀset. For
example, the map for Foo class in Listing 1 lists two entries for a ﬁeld x and a ﬁeld o. The map helps eﬃciently
locating persistent ﬁelds from persistent objects during serialization and deserialization. A persistent class and its
persistent ﬁelds are identiﬁed by accessing the persistence
attribute information in class metadata. The map can be
built when the class is loaded or when the class’ persistent
objects are serialized for the ﬁrst time.

Listing 2: An example program of using persistence
service
Listing 2 is an example of a persistent application to support warm start using the API. In many real-time control
applications, function blocks are invoked repetitively using
sensor inputs and to control actuators. The applications are
typically structured with Initialization Phase and then Periodic Operation Phase. In the initialization phase, resource

198

c l a s s Foo {
[ SPersistentField ]
public int x ;

required to execute the function blocks are initialized. Then,
in the operation phase, function blocks are invoked periodically. Once applications’ data is persisted at the end of
each period, failure recovery can be done by restarting the
applications at a new period with the latest persisted data.

[ SPersistentField ]
p u b l i c I x V e r t e x PNodes ;

3.2 Concurrent Persistence Mechanism

p u b l i c i n t XP {
get {
return x ;
}

The existing serialization in JVM or .NET is an atomic
procedure, i.e., the applications have to wait until the last
object of persistent object graph gets serialized. This may
result in a long pause on applications. To make persistence service schedulable for real-time embedded applications, we assign a separate task responsible for the service
(hereafter, SP task ) and make the task preemptible by dividing the service into small increments. As a consequence,
real-time scheduling algorithms can be applied to dispatch
urgent tasks once the SP task performs a bounded persistence service per increment and relinquishes CPU cycles to
other tasks.
We deﬁne that a consistent persisted data is a snapshot of
all persistent data when the persistence service is triggered.
With concurrent and preemptible persistence service, the
mutators may modify the persistent data before it is completely serialized. We address this issue by using a write
barrier. The write barrier traps the update operations of
mutators on persistent objects. If the persistent object yet
gets serialized, the write barrier performs serialization of the
object before the update is eﬀective on the object.
Ideally, a write barrier should be only applied to the update operations on the persistent data when persistence service is in progress. However, unless the object graph starting
from the root object is scanned, there is no straightforward
approach that can identify whether an object of persistent
class needs to be serialized. To avoid the application of write
barrier to all objects, we may take either of the following two
approaches: the ﬁrst approach is to apply write barrier on
methods that update persistent ﬁelds of a potential persistent object, and the second approach is to mark persistent
objects in advance.

}

[ SPersistentUpdate ]
set {
x = value ;
}

p u b l i c v o i d Bar ( ) {
...
XP=10;
PNodes [ i ]=Baz ;
...

}
}
public c l a s s IxVertex {
[ SPersistentField ]
p u b l i c V er tex [ ] nodes ;
p u b l i c I x V e r t e x ( V er tex [ ] param1 ) {
nodes = param1 ;
}
p u b l i c V er tex t h i s [ i n t i n d e x ] {
get {
r e t u r n nodes [ i n d e x ] ;
}

}

}

[ SPersistentUpdate ]
set {
nodes [ i n d e x ] = v a l u e ;
}

Listing 3: An example persistent class with method
annotations

3.2.1 Annotating Methods that Update Persistent
Objects

However, if the methods speciﬁed with the [SPersistentUpdate] attribute conduct update operations on nonpersistent ﬁelds as well as persistent ﬁelds, the write barrier
is unnecessarily applied to the update operations on nonpersistent ﬁelds. The solution is to use a wrapper method
designated for updating a persistent ﬁeld, and to make other
program codes call the method to update the ﬁeld. To realize that solution, we recommend to deﬁne Property for each
persistent ﬁeld, and access the ﬁeld through its corresponding property. Property in C# is an interface to provide
a direct access of a class ﬁeld through two methods: Get
method to return the value of the ﬁeld, and Set method to
write a value to the ﬁeld. Thus, specifying [SPersistentUpdate] to the property’s set method can conﬁne write barrier
to the persistent ﬁeld’s update. For example, in Listing 3, an
integer type ﬁeld x is a persistent ﬁeld and XP is a property
to the ﬁeld x. The attribute [SPersistentUpdate] on a set
method of the property XP allows to activate write barrier
on the set method while a persistence service is in progress.
If a persistent ﬁeld is array (actually a reference to an array object in CLI), the update operation on each element of
the array should trigger the write barrier. However, the set

Under this scheme, the methods that update persistent
objects are annotated (hereafter ”Annotating SP”) such that
a write barrier becomes eﬀective when the methods are invoked during a persistence service interval. Then, the write
barrier performs two operations on the object encountered:
a test operation for the serialization status of the object and
a serialization operation if it is not yet serialized. The annotation can be done by the programmer or by compiler when
a deﬁnition of persistent ﬁelds is detected. When it is done
by the programmer, the approach relies on users’ descriptions to eﬀectively restrict the range of write barrier to likely
update operations on persistent ﬁelds of persistent objects.
Because not all ﬁelds in persistent classes are persistent, applying write barrier on methods that update persistent ﬁelds
can narrow down the eﬀective range. As shown in Listing
3, the attribute [SPersistentUpdate] is used to specify the
annotated method set. This scheme can be also applied in
JVM given that annotation is supported since JDK 1.5 as
a result of JSR 175, ”a metadata facility for the Java programming language” [24].
[ SPersistent ]

199

method of the property corresponding to an array reference
does not invoke a write barrier on access to the element of
the array. A solution is to deﬁne an Indexer class for an
array in C#. This approach allows the instances of a class
or struct being indexed in the same way as arrays. Indexer
consists of get/set methods similar to properties except that
their accessors take the indices to the elements such that
the elements are guarded. In Listing 3, for example, a class
IxVertex is an Indexer class deﬁned to provide get/set methods to an access to array’s elements.
The limitation of the Annotating SP is that it may consider nonpersistent objects as persistent objects. For example, a nonpersistent object whose class happens to be a persistent class but it is not reachable from the persistent root
objects can be still protected by the write barrier. Although
it can cause unnecessary overhead, it does not violate the
consistency or integrity of persisted data because the nonpersistent objects mistakenly serialized will be deserialized
as dead objects that are unreachable via any live objects
and are the subject of garbage collection. The another limitation of this scheme is that any updates to persistent ﬁelds
must be done through properties or indexers although in
C#, properties and indexers are recommended to provide
encapsulation. However, we can also rely on compiler techniques to insert write barrier on updates to persistent ﬁelds
or deﬁne properties or indexers as shown in Listing 3.

SP Task
Serializing
Buf

Nonvolatile
RAM

WriterTask
Writing
Buf
Filesystem

Persistent Objects

PersistedData
Buffer

Persistent
Storage

Figure 2: Structure of the proposed Schedulable
Persistence System

3.3 Structure of Data Persistence
The architecture for schedulable persistence system is devised to aim at a general persistence system that can be
applied regardless the types of persistent storage. The data
persistence system consists of two tasks, SP task, and Writer
task in Figure 2. SP task traces persistent objects, serializes the state of the objects, and then writes the persisted
data into a buﬀer. On the other hand, Writer task moves
the data from the buﬀer to a persistent storage. The system
maintains two buﬀers: one buﬀer can be the SerializingBuf
to where SP task writes persisted data; and the other buﬀer
will be the WritingBuf read by Writer task to move persisted data to persistent storage. The roles of the buﬀers
are switched back and forth such that the write and read
operations are performed concurrently. Hence, a smooth
data transfer of persisted data to storage devices can be accomplished.

3.2.2 Marking Persistent Objects In Advance
To identify precisely the persistent objects, the persistence
service can mark persistent objects in advance before serializing the objects. The approach (hereafter, ”Marking SP”)
takes two phase: Marking Phase and Serialization Phase.
In marking phase, SP task traces and marks all persistent
objects in the object graph. In serialization phase, all the
marked objects get serialized. Both phases are preemptible
and are protected by a write barrier. The operations of the
write barrier is diﬀerent according to the phases. In marking
phase, the write barrier tests the serialization status of the
object encountered, and then in serialization phase, write
barrier tests the mark status of the object, and serialize the
object if it is marked and is not yet serialized. The write
barrier in making phase does not check the mark status because marking on objects is underway. It may generate some
false-serialized objects, but they may not be many because
the marking phase usually takes a short amount of time
comparing to the serialization phase.
This scheme can conﬁne the write barrier on any update
operations to persistent objects. However, the overhead is
that all update operations in marking phase conducts test
operation for the object encountered. That can be implemented with a fast path like inline codes. The study on cost
of write barrier using a fast path [3] shows that the cost
is tolerable. Unlike Annotating SP, this does not give programming burden to limit access to persistent ﬁelds through
properties or indexers.
In summary, either Annotating SP or Marking SP can
be used to distinguish persistent objects from nonpersistent
objects and to eﬃciently narrow down the eﬀective range
that the guard operation by a write barrier is applied. The
choice among two approaches might depend on the amount
of checking operations and the annotations provided by programmers.

4. IMPLEMENTATION
We implemented a prototype of schedulable persistence
system in CLI’s MONO platform version 1.1. The major effort in the implementation includes the construction of the
SP task designated for persistence service and write barrier
schemes. The SP task starts to serialize objects when SendPData API gets called as stated in Listing 2. As illustrated
in Figure 1, persistence service is performed by conducting
small increments of the service periodically. Runtime parameters are set with the targeted pause time of each persistence increment and the period of persistence increment.
During each persistence increment, the minimum work
unit of serialization is a single object; that is, the pause
time of a persistence increment can be as small as the duration taken to serialize a single persistent object. Persistent
object graphs are traced through a breadth-ﬁrst search, and
any object types including strings and array of strings are
treated as single persistent object to keep a minimum work
unit of a single object. An internal queue is employed to
buﬀer the persistent objects encountered when the object
graph is traced. During each persistence increment, depending upon the target pause time, a number of objects from
the queue are traced and serialized. To access the ﬁelds of
a persistence object, persistent class map is used to quickly
locate persistent ﬁelds within persistent objects. The map
is organized as a linked list of all persistent ﬁelds speciﬁed

200

with attribute [SPersistentField]. In our prototype implementation, the map is built while loading a persistent class
in order to avoid any delay during serialization.
To serialize persistent objects, the state of objects as well
as their reference relationship must be preserved. Each reference points to a corresponding object using an actual memory address (MID). In persistence storage, each MID must
be converted to a unique logical address (LID). We maintain the pairs of MID and LID for all persisted objects in
a hash table MID2LID. Whenever an object is encountered
during the breadth-ﬁrst search of persistent object graph,
MID2LID is looked up such that references to shared objects
can get converted by the same LID. In addition, MID2LID
table keeps the status of each object indicating whether it
has been serialized or not. The status is encoded by using MSB (most signiﬁcant bit) of LID in the table to save
memory space.
To invoke write barrier when a serialization gets started,
we consider the instructions deﬁned in Common Language
Instruction (CIL). Among the 250 instructions, only stﬂd
and stelem.{i,i1,i2,i4,i8,r4,r8,ref} are used to store a new
value in a ﬁeld of an object and in a vector element, respectively1 . To protect any update operations through write
barriers, MONO’s JIT compiler is modiﬁed such that the
two instructions are translated into CLI’s internal functions.
Each of these functions calls indirectly to a update function
through a pointer which is altered according to the presence
of concurrent persistence service. In other words, the update function with write barrier is invoked when concurrent
persistence service is underway, otherwise the one with no
write barrier is called. This approach is applied to both Annotating SP and Marking SP. However, the former modiﬁes
the implementation of stﬂd and stelem instructions when
they are a part of the methods annotated with [SPersistentUpdate]. The translated code is not altered if the stﬂd and
stelem instructions are not done by an annotated method.
On the other hand, in the latter scheme, the translated codes
for all stﬂd and stelem instructions are modiﬁed. Then, object mark is tested to trigger a write barrier.
The operation of de-serialization is completed in two phases.
In construction phase, objects are re-constructed based on
serialized data and a hash table with pairs of LID and MID
is established. In the following ﬁx-up phase, the LIDs are
ﬁxed up with corresponding memory address (MID) to recover the reference relationship between objects. Only the
objects with reference member ﬁelds are enqueued into a ﬁxup cache in construction phase so that the ﬁx-up operation
is limited to the objects with reference ﬁelds.
Unlike serialization, in our current implementation, deserialization does not work in a concurrent mode; that is,
when deserialization for recovery is requested due to an application’s failure, the recovered application’s execution can
resume once deserialization over the entire serialized data
completes. If the recovery, due to large amount of serialized
data, places a long latency beyond the application’s timely
requirements, the timeliness of the application cannot be
guaranteed even with persistence service. One of solutions
over this problem might be to resume the execution of a recovered application with a partial recovery once the partial
1
CIL deﬁnes additional store instructions such as stsﬂd,
stind.type, starg.length, and stloc. However, these store
instruction do not require a write barrier because their store
operation is not conducted on instance objects.

201

recovery initializes the application enough to resume. To allow a partial recovery, we can apply on-demand object deserialization and lazy pointer swizzling [2]: on-demand object
deserialization deserializes objects when they are referred,
and lazy pointer swizzling converts LID to a correspondent
MID when the object represented with the LID is actually
accessed. Other technique to be considered is In-Place Object Deserialization which is suggested as a method of serialization/deserialization for Java RMI (Remote Method Invocation) in [7]. The in-place object deserialization conducts
deserialization without allocation and copying of objects by
reusing a buﬀer allotted to serialized data for deserialized
object. We leave considering a partial recovery as a future
work.

5. EXPERIMENTS
Experiments in the prototype persistence system are conducted to examine the performance of the proposed design approaches. The experiment is done in a PC workstation with 1.5GHz Pentium IV processor and 256MB memory. To have a high resolution timer and preemptive kernel,
TimeSys’ Linux/Real-Time(v4.1.147)[26] is used.
The experiments basically follow a comparative study among
the performance measures of serialization and deserialization operations. MONO’s serialization library and the proposed schedulable persistence (SP) service with the Annotating SP and Marking SP write barrier schemes are tested.
We collect both the maximal and average measures after
running the experiments 40 times. If a speciﬁc measure is
common to Annotating SP and Marking SP schemes, we
simply denote it as a SP measure for the proposed schedulable persistence system. Whenever SP service and applications run concurrently in Linux threads, the SP thread is
assigned with a higher priority than application threads.

5.1 Performance of Serialization and
Deserialization
The performance of serialization and deserialization operations using MONO’s serialization library, Annotating SP,
and Marking SP, are collected by running in the stop-theworld mode. Since the time needed to complete the operations depends upon the object types as well as the number
of persisted objects, the serializations of a red-black tree of
composite objects (consisting of multiple primitive ﬁelds),
and an array of a primitive type (integer) are benchmarked.
The average response times of serialization and deserialization for the red-black tree and the array of a primitive type
are shown in Figure 3, Figure 4, Figure 5, and Figure 6
respectively.
The results in the ﬁgures clearly indicate that, in terms of
the response time of the serialization and deserialization operations, the proposed schedulable persistence system (with
either Annotating SP and Marking SP) outperforms the
MONO’s serialization library signiﬁcantly. The performance
improvement is derived from two main factors. First, the SP
serialization is implemented as a subsystem of CLI by deﬁning native functions which is much more eﬃcient than the
serialization library of managed code. The second factor is
the use of persistence class map which allows us to avoid
costly reﬂection mechanism.
Given that the experiments were run in the stop-of-theworld mode, the eﬀect of write barrier schemes is not presented in the results. However, comparing the Annotating

600

130
Serialization Library
Annotating SP
Marking SP

120

Serialization Library
Annotating SP
Marking SP

500
110

Avg. Serialization Time (ms)

Avg. Serialization Time (ms)

100
400

300

200

90

80

70

60

50
100
40

0
5000

10000

15000
Number of Persistent Objects

20000

30
5000

25000

Figure 3: Serialization of Tree

10000

15000
Array Size

20000

25000

Figure 5: Serialization of Array of Primitive type
90

600
Serialization Library
SP

Serialization Library
SP

80
500

Avg. Deserialization Time (ms)

Avg. Deserialization Time (ms)

70
400

300

200

60

50

40

30
100

20

0
5000

10000

15000
Number of Persistent Objects

20000

10
5000

25000

Figure 4: Deserialization of Tree

10000

15000
Array Size

20000

25000

Figure 6: Deserialization of Array of Primitive type
The benchmarked application mainly constructs a directed
weight graph, which contains 1326 persistent objects. The
total amount of serialized data is about 43KBytes. The application works as follows. First, the application constructs
a directed weight graph of nodes. After the construction is
complete, it triggers a persistence service. Then, the application starts to make changes on the connections of nodes
and then delete all the nodes of the graph one by one. In
other words, in preemptible mode, the SP task serializing
the graph runs concurrently with a mutator that updates
the persistent objects.
The experiment results are shown in Table 1. It indicates
that the schedulable persistence system by using both write
barrier schemes is able to control the pause time of each
persistence increment to meet the targeted bound of 500µs.
The average execution time of serialization in preemptible
mode is slightly higher that that in the stop-the-world mode.
This diﬀerence is somehow expected given additional overhead caused by context switches and controlling the pause
time of each increment.
The total execution times of the application of the Marking SP in both stop-the-world mode and preemptible mode
are higher in than that of the Annotating SP. In both write
barrier schemes, the CIL update instructions, stﬂd and stelem,
are translated into internal functions during JIT (runtime
compilation). Then, depending upon whether a serialization is in progress or not, the internal functions call actual
update functions with either a write barrier or without a

SP and Marking SP schemes, we found that Marking SP has
a 30% more overhead than Annotating SP especially for applications with many reference objects. This is mainly due
to the diﬀerence of the invocation numbers of internal functions translated from stﬂd, and stelem. More detail is given
in 5.2. Furthermore, the extra marking phase before conducting serialization in Marking SP scheme also contributes
to that. On the other hand, in this experiment, the size of
the serialized binary data in SP is not much diﬀerent from
that of serialization library because in each of the benchmarked applications there is only one persistent class and
the metadata preserved is only a small portion of serialized
data.

5.2 Controlling the Pause Time of Persistence
Increments
The aforementioned experiments illustrate the execution
time of the schedulable persistence system. To support realtime applications, we need to examine whether the proposed
schedulable persistence system has a behavior following the
scheduling model of Figure 1. The experiments conducted
are to apply the two write barrier schemes in two diﬀerent execution modes, stop-the-world mode (STW) and preemptible modes. For the preemptible mode, the targeted
pause time of each persistence increment and the period
of persistence increment are set to 500µs and 3ms, respectively.2
2
This experiment focuses on measuring the ﬂexibility of persistence increment’s granularity and the overhead due to

persistence service’s preemptivity so that these numbers,
500µs and 3ms, are arbitrarily chosen in that sense.

202

Properties

Annotating SP
STW
500µs

Marking SP
STW
500µs

Marking SP(v2)
STW
500µs

Max. pause time per increment(µs)

5171

453

6162

478

6344

478

Avg. pause time per increment(µs)

5056

401

6084

388

6257

398

Avg. persistence cycle (ms)

NA

39

NA

48

NA

48

Max. serialization time(µs)

5171

6056

6162

6758

6344

7367

Avg. serialization time(µs)

5056

5632

6084

6612

6257

6859

Max. application exec. time(ms)

300

301

374

390

302

309

Avg. application exec. time(ms)

279

282

351

366

272

284

Avg. No. of Increments

1

14

1

17

1

18

Avg. No. of WB invocations

0

1284

0

185762

0

16284

Avg. No. of WB Serializations

0

24

0

41

0

50

Table 1: Pause time and overhead of schedulable persistence system
write barrier, accordingly. The main diﬀerence between the
two schemes is the number of CIL update instructions translated into the internal functions. Annotating SP limits the
instructions to those in the methods annotated with [SPersistentUpdate] whereas all stﬂd and stelem instructions, including the ones in standard class libraries, are converted
into the internal functions in Marking SP. The overhead incurred in the execution of the internal functions results in a
longer execution time of the application in Marking SP. Furthermore, that Marking SP has a marking phase in advance
which also aﬀects the execution time for serialization.
To ﬁnd out a compromised solution for the added cost of
internal functions for write barrier, we experiment a modiﬁed Marking SP scheme in which only the stﬂd and stelem
instructions in the methods of persistence classes are translated into the internal functions. The performance is shown
in the Marking SP(v2) column of Table 5.2. This modiﬁcation reduces the number of invocations of the internal
functions signiﬁcantly and leads to a total execution time
of the applications similar to that of Annotating SP. It indicates that even though the execution time in Marking SP
is high, it can be adjusted by deﬁning the scope of the update operations to persistent ﬁelds. If persistent ﬁelds are
updated through the methods of persistent class, a write
barrier on the methods of persistent class is able to provide
a suﬃcient guard without an excessive overhead.
Besides execution times, the numbers of increments to
complete a persistence cycle and the length of a persistence
cycle are presented in Table 1. In addition, the numbers of
invocations of write barrier operations, and the numbers of
objects serialized during write barrier operations are given.
The distinct characteristics revealed on the two measures is
discussed in the following subsection 5.3.

STW

Annotating
SP

Marking
SP

Avg.No. of persistent objs.

1326

1326

1326

Avg.No. of serialized objs.

1326

1677

1326

Avg.No. of WB invocations

0

2643

240912

Avg.No. of WB serialization

0

339

13

Table 2: Checking and serialization operations of
the two write barrier schemes
persistent root object before deleting any nodes. The second
one has no persistent objects but using the same persistent
class as the ﬁrst thread. In the third thread, a diﬀerent class
is used to instantiate the objects in the graph and no persistence service is invoked. Among three threads, only the
ﬁrst one triggers a persistent service after the construction
of the graph. The application has 1326 persistent objects
out of a total of 3978 objects.
The results of the experiment is shown in Table 2. The
data indicates that Marking SP is able to precisely pinpoint
persistent objects. On the other hand, extra 351 nonpersistent objects accessed by the 2nd thread are serialized in Annotating SP. These extra objects happen to be the same class
of the persistent objects and are updated by the annotated
methods when the 1st thread requests a persistence service.
However, because Annotating SP limits the write barrier
to the annotated methods, the number of invocation to the
write barrier operations is much less than that in Marking
SP. Note that, in Annotating SP, the average number of
objects serialized by the write barrier operations, i.e. 339,
is less than the average number of the extra nonpersistent
objects serialized by the scheme. This should not happen
if nonpersistent objects can only be serialized (mistakenly)
by the write barrier operations. In fact, an object serialized
by the write barrier operations may have references to other
objects. These referenced objects must also be serialized
and are placed in the internal queue such that the SP task
can trace the object graph through the breadth-ﬁrst search.
Thus, extra nonpersistent objects may be preserved by the
SP task during the subsequent persistence increments.

5.3 Objects Serialized by Annotating SP and
Marking SP
In this experiments, we look into how the two schemes,
Annotating SP and Marking SP, eﬃciently distinguish persistent objects from nonpersistent objects. To have mixed
persistent and nonpersistent objects in heap memory, the
benchmark application consists of three threads and each
of which ﬁrstly constructs a directed graph of nodes and
then deletes all the nodes one by one. Each graph is diﬀerent in the perspective of persistence. The ﬁrst one makes
all objects in the graph persistent by making its root node a

6. CONCLUSION & FUTURE WORKS
In this paper, we aim for a schedulable persistence sys-

203

[10] H. Evans. Why Object Serialization is Inappropriate
for Providing Persistence in Java. Technical report,
Department of Computing Science, University of
Glasgow, Glasgow, 2000.
[11] S. Ghosh, R. G. Melhem, D. Mosse, and J. S. Sarma.
Fault-tolerant rate-monotonic scheduling. Real-Time
Systems, 15(2):149–181, 1998.
[12] S. Ghosh, R. Mellhem, and D. Mosse. Enhancing
real-time schedules to tolerate transient faults. In
IEEE Real-Time Systems Symposium, pages 120–129,
1995.
[13] J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java
Language Speciﬁcation. Addison-Wesley, 2nd edition,
2000.
[14] M. Hericko, M. B. Juric, I. Rozman, and A. Zivkovic.
Object serialization analysis and comparison in Java
and .NET. ACM SIGPLAN Notices, 38(8):291–312,
2003.
[15] H. Lee, H. Shin, and S.-L. Min. Worst case timing
requirement of real-time tasks with time redundancy.
rtcsa, 00:410, 1999.
[16] K. Li, J. F. Naughton, and J. S. Plank. Real-time,
concurrent checkpoint for parallel programs. In Second
ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming (2nd PPOPP’90),
SIGPLAN Notices, pages 79–88, Mar. 1990.
[17] T. Lindholm and F. Yellin. The Java Virtual Machine
Speciﬁcation. Addison-Wesley, 2nd edition, 1999.
[18] Microsoft Corp. Microsoft .NET framework
binaryformatter serialization format, 2002.
[19] M. Monga and A. Scotto. A generic serializer for
mobile devices. In Proceedings of the 20th annual
ACM symposium on applied computing, Santa Fe, New
Mexico, USA, 2005.
[20] M. Philippsen and B. Haumacher. More eﬃcient
object serialization. In IPPS/SPDP Workshops, pages
718–732, 1999.
[21] S. Punnekkat, A. Burns, and R. Davis. Analysis of
checkpointing for real-time systems. Real-Time
Systems, 20(1):83–102, 2001.
[22] R. Riggs, J. Waldo, A. Wollrath, and K. Bharat.
Pickling state in the Java(TM) system. USENIX,
Computing Systems, 9(4):291–312, 1996.
[23] T. Suezawa. Persistent execution state of a java
virtual machine. In Java Grande, pages 160–167, 2000.
[24] Sun Microsystems, Inc. JSR175 : a metadata facility
for the java programming language, 2005.
http://www.jcp.org/en/jsr/detail?id=175.
[25] Sun Microsystems Inc. Sun Real-Time Java System,
2005. http://java.sun.com/j2se/realtime.
[26] TimeSys Corporation. Timesys linux/real-time user’s
guide, version 2.0, 2004.
[27] B. Venners. Inside the Java Virtual Machine.
McGraw-Hill, 1999.
[28] Ximian. MONO. http://www.go-mono.com.

tem to support object persistence for real-time applications
in VMs. The system is designed with a concurrent persistence service task to avoid any long pause delay caused
by sequential operation of serialization. To guarantee the
consistency of persistent objects due to the concurrent operations of persistence service task and application tasks,
write barrier schemes are devised. A prototype system, involving the modiﬁcation of JIT and rewriting the serialization operation, is constructed in CLI’s open source platform,
MONO. The experiments show a signiﬁcant performance
gain resulted from the native function implementation for
serialization operation and a replacement of reﬂection mechanism for class introspection. In particular, we are able to
demonstrate the bounded pause time for each persistence increment in the prototype system and the modest overhead
incurred in write barrier operations.
Based on the insight gained from this study, our future
work is to establish a cost model for the persistence service. The model is to estimate the worst case execution
time of persistence service based on number of persistence
objects, the class information of the objects, and their reference dependency. The model can then be used to schedule
persistence cycle and increments, as well as to design a suitable scheduling algorithm for application tasks subject to
recovery constraints.

7.

ADDITIONAL AUTHORS

Additional authors: Elliott Rachlin (Honeywell International Inc., email: elliott.rachlin@honeywell.com).

8.

REFERENCES

[1] Aonix North America, Inc. PERC, 2006.
http://www.aonix.com/perc.html.
[2] M. Atkinson and M. Jordan. A review of the rationale
and architectures of PJama: a durable, ﬂexible,
evolvable and scalable orthogonally persistent
programming platform. Technical Report TR-2000-90,
Sun Microsystems Laboratories and Dept. Computing
Science, Univ. Glasgow, UK, 2000.
[3] S. M. Blackburn and A. L. Hosking. Barriers: friend
or foe? In ISMM, pages 143–151, 2004.
[4] G. Bollella, J. Gosling, B. Brosgol, P. Dibble, S. Furr,
and M. Turnbull. The Real-Time Speciﬁcation for
Java. Addison-Wesley, 2000.
[5] S. Bouchenak, D. Hagimont, S. Krakowiak, N. D.
Palma, and F. Boyer. Experiences implementing
eﬃcient java thread serialization, mobility and
persistence. Softw., Pract. Exper., 34(4):355–393,
2004.
[6] F. Breg and C. D. Polychronopoulos. Java virtual
machine support for object serialization. In Java
Grande, pages 173–180, 2001.
[7] C.-C. Chang. Safe and Eﬃcient Cluster
Communication with Explicit Memory Management.
PhD thesis, Cornell University, 1999.
[8] A. Cunei and J. Vitek. A new approach to real-time
checkpointing. In Proceedings of the 2nd ACM Virtual
Machine and Execution Environments Conference
(VEE 2006), June 14-16, 2006 Ottawa, Canada.
ACM, 2006. (to appear).
[9] ECMA. Ecma-335 common language infrastructure,
2002.

204

Sharing I/O in Strongly Partitioned Real-Time Systems
Ravi Shah1, Yann-Hang Lee1, and Daeyoung Kim2
1Computer Science and Engineering Dept.
Arizona State University, Tempe, AZ 85287
{ravi.shah, yhlee}@asu.edu
2School of Engineering
Information and Communications University, Daejeon, South Korea
kimd@icu.ac.kr

Abstract. Strongly partitioned real-time systems have been adopted to provide
an integrated run time environment for applications with varied criticalities.
This is achieved by guaranteeing spatial as well as temporal partitioning for the
applications. To enable accesses to shared I/O devices in such an environment,
this paper provides an effective model that the co-existence of any application
does not hinder the execution of IO operations or spatial and temporal requirements of other applications. The model utilizes a microkernel-based approach
to regulate the work of drive drivers and thus integrate the device-sharing
model with the microkernel.

1

Introduction

Strongly partitioned real-time system (SP-RTS) supports co-existence of multiple
execution partitions for various mission critical and time critical applications. All
these integrated applications are guaranteed to meet their timing and space requirements while sharing resources with other applications [1, 2]
Partitioning the system is one of the common ways to provide such a guarantee.
Partitioning aims at controlling the additional hazard created when a function shares
its processor and resources with other functions. Traditionally, partitioning has meant
separating the memory available in a system such that one partition cannot change or
affect the state of other partitions. But real-time applications have their own timing
requirements, which require broadening this common definition of partitioning to
include time as a critical component of a partition and to have two different components [3, 4]:
z Temporal partitioning: to ensure that the service received from shared resources
by the software in one partition is not affected by the ones in other partitions.
z Spatial partitioning: to ensure that software in one partition cannot change the
software or private data of another partition nor command the private devices of
other partitions.
Existing real-time OS such as WindRiver’s vxWorks AE653 and Green Hill’s Integrity-178B provide the platform to support partitioning for safety critical ARINC
Z. Wu et al. (Eds.): ICESS 2004, LNCS 3605, pp. 502-507, 2005.
 Springer-Verlag Berlin Heidelberg 2005

Sharing I/O in Strongly Partitioned Real-Time Systems

503

653 applications [5]. For instance, as shown in Figure 1, vxWorks AE653 includes a
core OS based on vxWorks AE kernel which interacts directly with the computing
platform (core module) and a partition OS based on vxWorks 5.5 Wind kernel. Partitioning support is provided inside the vxWorks AE kernel and a special private message-passing interface is available between the vxWorks 5.5 kernel (running in the
partition) and the vxWorks AE kernel (running as the Core OS).
I/O Partition
Application
1

Application
1

Application
1

Application
Interface

Partition
OS

Partition
OS

Partition
OS

I/O Layer

Application
Tasks

User Level
Device Driver

Core OS

Processor

Application Partition

Hardware
Devices

Supervisor Level
Driver Routines

Core OS

Fig. 1. vxWorks AE653 OS and Device Driver Models
This architecture involves a two level scheduling policy. A special partition layer
is provided to serve the APEX (application executive) calls which requires interaction
with the Core OS. Also, a fixed number of kernel threads are implemented to limit the
number of blocking kernel calls by a partition at any given point of time [5].
vxWorks AE653 model implements a special I/O partition to accommodate various
input/output devices. This partition is isolated from the application partitions and
implements a user level device driver. Any application partition that wants to use an
I/O device communicates to this partition using the ARINC ports. But the communication between the I/O partition the Core OS is done only in supervisor mode. This
prevents any uncertified code from impacting the Core OS or application partitions,
thus guaranteeing the spatial and temporal partitioning requirements of SP-RTS [5].
The multiple-layered kernel approach in [4, 5] is one of the most common approaches towards partitioning. It is preferred because of its clean design in terms of
satisfying spatial and temporal partitioning requirements. This seemingly simple and
elegant model seems to suffer some serious problems the moment we employ a partition to operate a shared hardware which is open to the outside world for interruption
and that requires mediation from the CPU for its proper functioning. For instance, an
invocation of IO operation can only be processed when the IO partition is scheduled.
This results in a longer latency and limits IO bandwidth. Or on the other hand, if an
IO partition is scheduled frequently, the context switch overhead may become significant.
In this paper, we propose a device driver model that can be implemented for SPRTS models to allow the application partitions to share and access the devices. After
presenting the design objectives for shared I/O devices in Section 2, the architecture

504

Ravi Shah, Yann-Hang Lee, and Daeyoung Kim

design and components are illustrated in Section 3. In Section 4, we show an experiment result in a simulated implementation. A short conclusion follows in Section 5.

2

Shared I/O Design Objectives

While dealing with shared I/O devices, the following issues need to be considered
and solved:
1.

2.

3.
4.

5.

3

Authorized Access: Not all partitions in the system might require access to the
input/output devices. In this case, it is imperative that only the partitions that
need to share particular devices are allowed to share those devices.
Independent Functionality: Despite the presence of shared devices on the system,
the partitions should remain unaware of this fact, and should behave and perform
as if no devices were shared.
Spatial Partitioning: Shared devices should not become a source of breaking the
strong spatial partitioning of the system.
Temporal Partitioning: Any kind of time stealing of a partition from any other
device being shared by other partition should be taken care for. That is, proper
accounting of the time used by the shared devices should be done.
Predictable timing constraints for I/O: Despite the I/O paradigm (synchronous or
asynchronous), it is imperative that proper timing guarantees are given to the partitions for finishing I/O operations [6]. Also, proper mechanisms should be in
place so that the partitions can query device status.

Design of Shared I/O Architecture

The design of the system is based on the architecture of multiple-layer kernel such as
SPIRIT [4] and vxWork AE653 [5]. The design tries to address all the issues that are
introduced in SP-RTS due to the use of shared I/O devices in the system. Note that at
microkernel level, partitions are scheduled by a cyclic scheduler. The partitions can
have their own scheduling policy to schedule the tasks that run inside it. Following
are the major components of the architecture as shown in Figure 2.
3.1

Publish-Subscribe Architecture

The microkernel supports a “publish-subscribe architecture” similar to the one used in
[4] to help in controlling the access to the device by authorized partitions. The microkernel constructs a “publish table”, where it makes an entry for all the devices and
device names that are registered for the device. On receiving a request from the partition to subscribe to a device, a memory device is created inside the partition, and an
entry is made for this device in the subscription table.

Sharing I/O in Strongly Partitioned Real-Time Systems

3.2

505

Pseudo-device Driver

Pseudo device driver is a layer of software inside the partitions to support reading/writing or any normal device driver function into memory space rather than an
actual physical device.
One memory device is added inside the partition for every shared device that the
partition has subscribed to using the “Publish-Subscribe Architecture”. The partition
is aware of only the memory devices, i.e., it assumes the memory device to be the
physical device and performs all the operations on it as if it were being done on the
physical device.
The microkernel also uses this device driver layer to read the data that partition
wants to transfer to the device and then perform the actual transfer of that data to the
device. It is also used to provide the partitions with the data that is coming from the
device for the partition. The pseudo device maintains multiple pointers inside it so as
to perform all its tasks. Two circular queues are adopted to support the communication with the microkernel. The circular queues do not allow any overwrite of data in
order to simulate the FIFO property generally present inside the devices.

Partition OS

Partition OS

pseudo
device

pseudo
device

pseudo
device

pseudo
device

Partition OS

pseudo
device

pseudo
device

user space
kernel space
device queue
device schedule

device driver
IS

device 2

device 1

device 3

Fig. 2. Shared I/O Architecture in SP-RTS
3.3

Device Queues

Inside the microkernel, device queues (one per physical device) are maintained to act
as a buffer for storing the data that is received from the device. These queues are
assumed to be large enough to accommodate all the data that can arrive from the
device before being transferred to the partition. The physical devices are configured
in interrupt driven mode in the system. Once receiving any data, the device sends an
interrupt to the CPU that is served through the device specific ISR.
Both these responsibilities are properly performed by the system using the device
queues and low overhead ISRs. ISR simply transfers data from the physical device to
the corresponding device queue. The time that it takes for the ISR to transfer the data
from device to the device queue is accounted in the time of all the partitions that are

506

Ravi Shah, Yann-Hang Lee, and Daeyoung Kim

sharing this device. Though this mechanism introduces some overhead in terms of
extra copying of the data, but it can minimize the time the ISR takes to transfer data.
3.4

Device Scheduler

It is a layer inside the microkernel that is responsible for transferring data from a
pseudo device to its corresponding physical device, and from the device queues to the
pseudo devices inside the partitions. The device scheduler is given its own time slices
on the basis of IO bandwidth and scheduling policy that is defined at the system design time. The goal of this layer is to serve the application partitions and the devices
in such a way that the partitions always get the requested IO bandwidth.
There are multiple ways in which the device scheduler can be scheduled. If the device scheduler is scheduled in partition context switch instants, then this type of
scheduling is called “interleaved scheduling”. It is desirable that the time for which
the device scheduler is scheduling is directly proportional to the total buffer size of all
the pseudo devices (corresponding to the shared physical devices) of preceding and
succeeding partitions. This will give the device scheduler enough time to transfer all
the data from the pseudo devices to the physical device, and also to transfer all the
data from the device queues to the corresponding pseudo devices.

4

Experiment

The experiment is based on vxWorks RTOS that acts as supplement for the microkernel present in an actual SP-RTS. The prototype implementation therefore is a simulation of device sharing in SP-RTS. A cyclic scheduler is implemented as a task running on vxWorks, which schedules various partitions of the system. Also, device
scheduler and device queues are implemented on vxWorks.
Along with the pseudo device drivers, the drivers for the physical device are also
implemented. The reference implementation uses two device drivers - an IEEE 1284
parallel port device driver and a PCI based joystick device driver. The ISR routines of
the device driver are written into the device queues as mentioned in section for device
queues. While the parallel port can be configured in both read and write mode, the
joystick can be configured only in read mode.
Readings are taken by varying different parameters like the number of partitions,
number of devices, device bandwidth (i.e. the maximum amount of data that the device can transfer per unit of time). Figure 3 shows that given a fixed device scheduler
time slice (i.e. total device bandwidth of all the shared devices), per device bandwidth
decreases non-linearly with increasing number of shared devices in the system.

5

Conclusion

The proposed middleware based approach to share input/output devices can be a
practical and efficient solution in SP-RTS architecture. The system design addresses

Sharing I/O in Strongly Partitioned Real-Time Systems

507

the two most important goals of SP-RTS while sharing the I/O devices. Spatial partitioning is achieved by introducing new components at pseudo-device layer and device
scheduler layer. Temporal partitioning is accomplished by properly accounting the
time taken for serving the I/O devices to the responsible partition. By considering the
total bandwidth of all the pseudo devices that are created inside the partitions, time
slice for device scheduling can be determined.
Fixed device scheduler time

1600
1400
1200
1000
800
600
400
200

0
1

2

3
Number of devices

4

5

Fig. 3. Sharing of device bandwidth with multiple devices

References
1. Y. H. Lee, D. Kim, M. Younis, J. Zhou and J. McElroy, Resource scheduling in dependable
integrated modular avionics, Proceedings of IEEE International Conference on Dependable
Systems and Networks, June 2000.
2. Y. H. Lee, D. Kim, M. Younis and J. Zhou, Scheduling tool and algorithms for integrated
modular avionics systems, Proceedings of IEEE/AIAA Digital Avionics Systems Conference,
October 2000.
3. John Rushby, Partitioning in avionics architectures: requirements, mechanisms, and assurance, NASA Contractor Report CR-1999-209347, June 1999.
4. Y. H. Lee, D. Kim and M. Younis, SPIRIT-microkernel for strongly partitioned real-time
systems, Proceedings of the 7th IEEE Conference on Real-Time Computing Systems and
Applications, Pages: 73 - 80, November 2000.
5. Paul Parkinson, Safety-critical software development for integrated modular avionics, WindRiver White Paper, 2003.
6. Mark H. Klein and Thomas Ralya, An analysis of input/output paradigms for real-time
systems, Technical Report, CMU/SEI-90-TR-19, July 1990.

The Journal of Systems and Software 65 (2003) 71–86
www.elsevier.com/locate/jss

Software architecture supporting integrated real-time systems
Daeyoung Kim
b
c

a,b,*

, Yann-Hang Lee b, Mohamed Younis

c

a
Department of Computer Science and Engineering, Information and Communications University, Taejon, South Korea
Department of Computer Science and Engineering, Arizona State University, P.O. Box 875406, Tempe, AZ 85287-5406, USA
Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, MD, USA

Received 17 April 2001; received in revised form 15 August 2001; accepted 24 October 2001

Abstract
To achieve reliability, reusability, and cost reduction, a signiﬁcant trend in building large complex real-time systems is to integrate separate application modules of diﬀerent criticalities in a common hardware platform. An essential requirement of integrated
real-time systems is to guarantee spatial and temporal partitioning among applications in order to ensure an exclusive access of
physical and temporal resources to the applications. In this paper we propose software architecture, implemented as SPIRITlKernel, for strongly partitioned integrated real-time systems. The SPIRIT-lKernel has been designed and implemented based on a
two-level hierarchical scheduling methodology such that the real-time constraints of each application can be guaranteed. To
demonstrate the feasibility of the SPIRIT-lKernel, we have ported two real-time operating systems (RTOS), WindRiverÕs VxWorks
and CygnusÕs eCos, on the top of the microkernel. Thus, diﬀerent RTOS can be applied in various partitions to provide required
features for each application. Based on the measured performance results, the SPIRIT-lKernel architecture is practical and appealing due to its low overheads of kernel services and the support for dependable integration of real-time applications via
scheduling algorithm.
Ó 2002 Elsevier Science Inc. All rights reserved.
Keywords: Real-time; Scheduling; Operating system; Strong partitioning; Integrated modular avionics

1. Introduction
Recently the integrated real-time systems have been
the subject of signiﬁcant research in both industry and
academia. In an integrated real-time system, applications of diverse levels of temporal and mission criticality
are supposed to share the same computing resources
while maintaining their own functional and temporal
behaviors. To protect applications from potential interference, the integrated system must provide spatial
separation such that the memory and I/O space of an
application are protected from illegal accesses attempted
by other applications. In addition, the system must
support temporal separation such that the execution
time reserved to one application would not be changed
either by overrun or by hazardous events of other applications. These spatial and temporal separations are
*

Corresponding author. Tel.: +82-42-866-6704; fax: +82-42-8666110.
E-mail address: kimd@icu.ac.kr (D. Kim).

the two key components of the strong partitioning
concept. We refer to such kind of systems as strongly
partitioned integrated real-time systems (SPIRIT).
The concept of integrated modular avionics (IMA)
(ARINC, 1991; ARINC, 1997) is a good example of
SPIRIT architecture. With the strong partitioning support in IMA systems, the same processor can run crucial
applications such as control navigation systems and a
non-ﬂight-critical function such as cabinÕs temperature
control. Even if the software that controls the cabin
temperature is not certiﬁed to the highest level and has a
probability of not working correctly, the navigation
system would not be aﬀected. By integrating applications from federated computer boxes into smaller
number of high performance sharable computing resources, they can reduce the interconnection network,
weight, power supplies, and physical volumes of computing resources. This also achieves the reliability,
maintainability, and dramatic cost reduction. Also a
commercial-oﬀ-the-shelf (COTS) approach contributes
a lot in the development and maintenance of the system

0164-1212/03/$ - see front matter Ó 2002 Elsevier Science Inc. All rights reserved.
doi:10.1016/S0164-1212(02)00028-6

72

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

considering long-life time of such systems. Even though
IMA concept was introduced in the early 90Õs (ARINC,
1991), there has not been signiﬁcant progress in the
areas of task scheduling and software architecture,
which are main concerns of this paper.
In this paper, we propose the scheduling theory
and the SPIRIT-lKernel that is a key technology in
implementing SPIRIT systems. In designing the SPIRIT-lKernel, we followed the design concepts of the
second-generation microkernel architecture because it
provides good reference model to achieve both ﬂexibility
and eﬃciency. First, to achieve better ﬂexibility, it builds
the microkernel from scratch to avoid any negative inheritance from monolithic kernels. Second, to achieve
better performance, it implements hardware dependent
kernel maximizing the support from the hardware.
The goals of the SPIRIT-lKernel are to provide dependable integration of real-time applications, ﬂexibility
in migrating operating system personalities from kernel to user applications including transparent support
of heterogeneous COTS real-time operating systems
(RTOS) on top of the kernel, high performance, and
real-time feasibility. To support integration of real-time
applications that have diﬀerent criticality, we have implemented strong partitioning concept using a protected
memory (resource) manager and a partition (application) scheduler. We also developed a generic RTOS port
interface (RPI) for easy porting of heterogeneous COTS
RTOS on top of the kernel in user-mode. The kernel
provides minimum set of functions such as address space
management, interrupt/exception dispatcher, inter-partition communication, and partition scheduler, etc. A
variety of operating system personalities such as task
scheduling policy, exception-handling policy, inter-task
communication can be implemented within the partition
according to individual requirements of partition RTOS.
To demonstrate this concept, we have ported two different application level RTOS, WindRiverÕs VxWorks
5.3 and CygnusÕs eCos 1.2, on top of the SPIRITlKernel.
The scheduling policy of the SPIRIT-lKernel is
based on the two-level hierarchical scheduling algorithm
that guarantees the individual timing constraints of all
partitions in an integrated system. At the lower microkernel level, a distance-constrained cyclic partition
scheduler arbitrates partitions according to an oﬀ-line
scheduled timetable. On the other hand, at the higher
COTS RTOS level, each local task scheduler of a partition schedules own tasks based on a ﬁxed-priority
driven scheduling. Though they are not presented in this
paper due to the space limit, the scheduling theories
include other features; such as scheduling algorithms for
easy upgrade and evolution, fault tolerance, and aperiodic tasks, etc.; to meet the requirements of large complex real-time systems (Kim et al., 2000; Kim, 2001; Lee
et al., 1998; Lee et al., 2000).

We can ﬁnd the application areas of the SPIRITlKernel in the following two cases. First, it can be used
in integration of existing real-time applications that run
on diﬀerent COTS RTOS. Second, it can be used in
integration of real-time applications that require diﬀerent operating system personalities.
The rest of the paper is structured as follows. We
discuss SPRIRIT model and design concepts of the
SPIRIT-lKernel in Section 2. Fundamental two-level
scheduling theory is presented in Section 3. We describe
the kernel architecture and generic RTOS port interface
in Sections 4 and 5, respectively. In Section 6, we explain
the prototype implementation and performance evaluation. A short conclusion is then followed in Section 7.

2. SPIRIT model and design concepts
Strongly partitioned integrated real-time system is a
system architecture which supports the integration of
multiple, diﬀerent levels of temporal and mission criticality for real-time applications in sharable computing
environment. These integrated applications must be
guaranteed to meet their own timing constraints while
sharing resources like processors, communication bandwidth with other applications. To guarantee timing
constraints and dependability of each application, it is
required to realize the concepts of strong partitioning,
spatial and temporal partitioning.
In this section, we describe a strongly partitioned
integrated real-time system model for which we build the
SPIRIT-lKernel and then discuss the design concepts of
the SPIRIT-lKernel.
2.1. Strongly partitioned integrated real-time system
model
The SPIRIT is composed of multiple communicating
partitions in which there are also multiple interacting
tasks, as shown in Fig. 1. The SPIRIT uses the two-level
hierarchical scheduling policy in which partitions are

Fig. 1. Strongly partitioned integrated real-time system model.

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

scheduled by the SPIRIT-lKernelÕs cyclic partition
scheduler, and the tasks of a partition are scheduled by
the ﬁxed-priority driven local task scheduler of each
partition.
In order to meet the deadlines of periodic tasks within
each partition, the processing capacity dedicated to a
partition is then dependent on how frequently it is served
in the cyclic schedule. We have investigated the constraints on the allocated capacity (a) and the invocation
period (g) and have been able to construct a monotonic
function between a and g for any set of periodic tasks,
under either rate- or deadline–monotonic scheduling algorithms. Then, similar to pinwheel scheduling (Chan
and Chin, 1992), we can compose a distance-constrained
cyclic schedule for all partitions in the SPIRIT system.
Following this approach, for a system of n ordered
partitions P1;...;n , each partition server, Pi , will be given a
processor capacity of P
ai in the period of gi to process its
periodic tasks, where ni¼1 ai 6 1 and gj is multiples of gi
for i < j. By the characteristics of distance-constrained
cyclic scheduling, the independence between the lowerlevel partition scheduler and the higher-level local task
scheduler is achieved. The details of the scheduling work
will be discussed later.
2.2. Design concepts of SPIRIT-lKernel
As a key implementation vehicle of the strongly
partitioned integrated real-time systems, the SPIRITlKernel has the following design concepts at its core.
2.2.1. Ensuring temporal partitioning
Enforcing temporal separation should be implemented eﬃciently by avoiding excessive overheads, and
feasibly by meeting applicationÕs timing constraints. The
key to achieving temporal partitioning is the scheduling
strategy. One of the most diﬃcult jobs is to build a
feasible schedule for the SPIRIT. Since the SPIRIT is
composed of multiple partitions that might have diﬀerent local scheduling policies, the kernel must provide a
feasible two-level hierarchical scheduling strategy. In
this paper, we have devised a feasible two-level hierarchical scheduling algorithm that solves a pair of ﬁxedpriority and cyclic scheduling. In the ﬁrst prototype of
the kernel, we adopt this pair of scheduling policies.
2.2.2. Ensuring spatial partitioning
Resources allocated to a partition must be protected
from unauthorized access by other partitions. Also the
kernelÕs system resources must be protected from the
partitions. In addition to protection, eﬃciency and deterministic applications execution are important features
that should not be sacriﬁced for spatial partitioning.
Usually, spatial protection is implemented using the
memory management unit of a processor. The important factors to be concerned with in achieving eﬃciency

73

and deterministic execution are a kernel-user-mode
switch, a partition address space switch, a table lookaside buﬀer (TLB) overhead, etc.
2.2.3. Supporting applications based on heterogeneous
COTS RTOS on top of the kernel
Our emphasis is also on the ﬂexibility of accommodating COTS RTOS on top of the kernel. For example,
in IMA, there is a need to integrate multiple applications
that are originally developed in diﬀerent RTOS. If the
kernel and scheduling algorithms are certiﬁable, it can
reduce integration eﬀorts and costs by a tremendous
amount.
2.2.4. Restricting COTS RTOS to user-mode
In the SPIRIT, all codes including the COTS RTOS
kernel of the partition must be run in user mode. This is
a challenge because we need to develop a secure interaction method between a kernel and partitions and to
modify COTS RTOS in order to enforce such a policy.
The problem becomes even harder when both COTS
RTOS and their applications run in supervisor mode for
eﬃciency purposes, as is common in COTS RTOS.
2.2.5. Capturing second-generation microkernel architectural concepts
Since the SPIRIT supports diﬀerent kinds of application environments in a shared computing platform, it
is better to follow a microkernel architecture than a
monolithic kernel. The second-generation microkernel
architecture provides better ﬂexibility and high performance.
3. Scheduling integrated real-time applications
In this section, we discuss the scheduling algorithm
for the two-level hierarchical SPIRIT scheduling model,
which uses distance-constrained cyclic scheduler and
ﬁxed-priority driven scheduler in lower- and higherlevel, respectively. The scheduling algorithm is an essential theoretical background of the proposed the
SPIRT-lKernel. The SPIRIT-lKernel uses the result
of the scheduling analysis to dispatch integrated applications. In practice, a system integrator can assign a
ﬁxed-priority to a task within a partition and allocate a
processor capacity to each partition according to the
result of scheduling analysis.
We show the notations used in the model of SPIRIT
in Table 1.
Table 1
Index notations used in the SPIRIT model
Notation

Description

nðP Þ
nðsi Þ, 1 6 i 6 nðP Þ

Number of partitions
Number of tasks in the partition i

74

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

Table 2
Notations used in the SPIRIT model
Notation

Description

Pi
Ai or Ak
si;j or sk
Sj or Sk
ai or ak
gi or gk
qi or qk
Ti;j or Tk
Ci;j or Ck
Di;j or Dk

Partition i
Application i which is implemented in partition i
Task j of the partition i
Periodic partition server for Pi
Partition capacity of partition server Si
Partition cycle of partition server Si
Processor utilization of partition server Si
Period of task si;j
Worst case execution time of task si;j
Deadline of task si;j

For the parameters shown in Table 2, we use the index such that 1 6 i 6 nðP Þ, and 1 6 j 6 nðsi Þ. For most of
the notations, we also provide a simpliﬁed form such as
Ak for Ai;j .
We consider the scheduling requirements for a partition server, Sk that executes the tasks of an application
partition Ak according to a ﬁxed-priority preemptive
scheduling algorithm and shares the processing capacity
with other partition servers in the operating system level.
Let application Ak consist of s1 ; s2 ; . . . ; sn tasks. Each
task si is invoked periodically with a period Ti and
takes a worst-case execution time Ci . Thus, the total
processor
P utilization demanded by the application is
qk ¼ ni¼1 ðCi =Ti Þ. Also, upon each invocation, the task
si must be completed before its deadline Di , where
C i 6 D i 6 Ti .
As modeled in SPIRIT, the tasks of each partition are
running under ﬁxed-priority preemptive scheduling.
Suppose that there are n tasks in Ak listed in priority
ordering s1 < s2 <    < sn , where s1 has the highest
priority and sn has the lowest. To evaluate the schedulability of the partition server Sk , letÕs consider the case
that Ak is executed at a dedicated processor of speed ak ,
normalized with respect to the processing speed of Sk .
Based on the necessary and suﬃcient condition of
schedulability in (Lehoczky, 1990; Lehoczky et al.,
1989), task si is schedulable if there exists a t 2 Hi ¼
flTj jj ¼ 1; 2; . . . i; l ¼ 1; 2; . . . ; bDi =Tj cg [ fDi g, such
that
 
i
X
Cj t
Wi ðak ; tÞ ¼
6 t:
ak T j
j¼1
The expression Wi ðak ; tÞ shows the worst cumulative
execution demand made on the processor by the
tasks with a priority higher than or equal to si during
the interval [0, t]. We now deﬁne Bi ðak Þ ¼ maxt2Hi
ft  Wi ðak ; tÞg and B0 ðak Þ ¼ mini¼1;2;::;n Bi ðak Þ. Note that,
when si is schedulable, Bi ðak Þ represents the total period
in the interval [0, Di ] that the processor is not running
any tasks with a priority higher than or equal to that of
si . It is equivalent to the level-i inactivity period in the
interval [0, Di ] (Lehoczky and Ramos-Thuel, 1992).

By comparing the task executions at server Sk and at
a dedicated processor of speed ak , we can obtain the
following theorem:
Theorem 1. The application Ak is schedulable at server Sk
that has a partition cycle gk and a partition capacity ak ; if
(a) Ak is schedulable at a dedicated processor of speed ak ;
and
(b) gk 6 B0 ðak Þ=ð1  ak Þ.
Proof. The task execution at server Sk can be modeled by
tasks s1 ; s2 ; . . . ; sn of Ak and an extra task s0 that is invoked every period gk and has an execution time
C0 ¼ ð1  ak Þgk . The extra task s0 is assigned with the
highest priority and can preempt other tasks. We need
to show that, given the two conditions, any task si of Ak
can meet its deadline even if there are preemptions
caused by the invocations of task s0 . According to the
schedulability analysis in (Lehoczky, 1990; Lehoczky
et al., 1989), task si is schedulable at server Sk if there is a
t 2 Hi [ Gi , such that
 
 
i
X
t
t
Cj
þ C0
6 t;
Tj
gk
j¼1
where Gi ¼ flgk jl ¼ 1; 2; . . . ; bDi =gk cg.
If si is schedulable on a processor of speed ak , there
exists a ti 2 Hi such that Bi ðak Þ ¼ ti  Wi ðak ; ti Þ P
B0 ðak Þ P 0 for all i ¼ 1; 2; . . . ; n. Note that Wi ðak ; ti ) is a
non-decreasing function of ti . Assume that ti ¼ mgk þ d
where d < gk . If d P B0 ðak Þ,
i
X
j¼1


Cj

ti
Tj




þ C0

ti
gk



¼ ak Wi ðak ; ti Þ þ ðm þ 1ÞC0


6 ak ti  B0 ðak Þ þ ðm þ 1ÞC0


¼ ak ti  B0 ðak Þ þ ðm þ 1Þð1  ak Þgk


6 ak ti  B0 ðak Þ þ ð1  ak Þðti  dÞ
þ B0 ðak Þ
¼ ti þ ð1  ak ÞðB0 ðak Þ  dÞ
6 ti :

The above inequality implies that all tasks si are
schedulable at server Sk .
On the other hand, if d < B0 ðak Þ, then, at
ti0 ¼ mgk < ti , we have
 0
 0
i
X


t
t
Cj i þ C0 i 6 ak ti  B0 ðak Þ þ mC0
Tj
gk
j¼1


6 ak ti  d þ mð1  ak Þgk
¼ ak ti0 þ ð1  ak Þti0
¼ ti0 :
Since ti0 2 Gi , the application Ak is schedulable at server
Sk .

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

75

When we compare the execution sequences at server
Sk and at the dedicated processor, we can observe that,
at the end of each partition cycle, Sk has put the same
amount of processing capacity to run the application
tasks as the dedicated processor. However, if the tasks
are running at the dedicated processor, they are not
blocked and can be completed earlier within each partition cycle. Thus, we need an additional constraint to
bound the delay of task completion at server Sk . This
bound is set by the second condition of the theorem and
is equal to the minimum inactivity period before each
taskÕs deadline. 
Theorem 1 provides a solution to determine how
frequently a partition server must be scheduled at the O/
S level and how much processor capacity it should use
during its partition cycle. It is easy to see that B0 ðak Þ and
gk are increasing functions of ak This implies that if
more processor capacity is assigned to a partition during
its partition cycle, the tasks can still meet their deadlines
even if the partition cycle increases. To illustrate the
result of Theorem 1, we consider an example in Table 3
where four application partitions are allocated in a
processor. Each partition consists of several periodic
tasks and the corresponding parameters of (Ci , Ti ) are
listed in the Table. Tasks are set to have deadlines equal
to their periods and are scheduled within each partition
according to a rate–monotonic algorithm (Liu and
Layland, 1973). The processor utilizations demanded by
the four partitions, qk , are 0.25, 0.15, 0.27, and 0.03,
respectively.
In Fig. 2, the maximum partition cycles are depicted
with respect to the assigned capacity ak . If the points
below the curve are chosen to set up cyclic scheduling
parameters for each partition at the O/S level, then the
tasks in the partition are guaranteed to meet their
deadlines. For instance, the curve for partition 2 indicates that, if the partition receives 28% of processor
capacity, then its tasks are schedulable as long as its
partition cycle is less than or equal to 59 time units.
Note that the maximum partition cycles increase as we
assign more capacity to each partition. This increase is
governed by the accumulation of inactivity period when
ak is small. Then, the growth follows by a factor of
1=ð1  ak Þ for a larger ak .

Fig. 2. The maximum partition cycles for diﬀerent processor capacity
assignments.

Fig. 2 suggests possible selections of (ak , gk ) for the
four partitions subject to a total assignment of processor
capacity not greater than 1. From the ﬁgure, feasible
assignments for (ak , gk ) are (0.32, 36), (0.28, 59), (0.34,
28), and (0.06, 57), respectively. In the following subsection, we shall discuss the approaches of using the
feasible pairs of (ak , gk ) to construct cyclic schedules.
Given the schedulability requirement of (ak , gk ) for
each partition server Sk , a cyclic schedule must be constructed at the lower-level scheduler. Notice that the pair
of parameters (ak , gk ) indicates that the partition must
receive an ak amount of processor capacity at least every
gk time units. The execution period allocated to the
partition needs to be not continuous, or to be restricted
at any speciﬁc instance of a scheduling cycle. This
property makes the construction of the cyclic schedule
extremely ﬂexible.
In distance-constrained scheduling, all partitions
must be allocated an exact capacity at any instance of
partition cycle period as shown in Fig. 3. So the tasks of
a partition, which arrive arbitrarily, can be allocated
suﬃcient capacity with which they can be scheduled.
Let a feasible set of partition capacities and cycles be
ða1 ; g1 Þ; ða2 ; g2 Þ; . . . ; ðan ; gn Þ and the set be sorted in the
non-decreasing order of gk . The set cannot be directly
used in a cyclic schedule that guarantees the distance
constraint of assigning ak processor capacity for every gk

Table 3
Task parameters for the example partitions

Tasks (Ci ; Ti )

Partition 1
(utilization ¼ 0:25)

Partition 2
(utilization ¼ 0:15)

Partition 3
(utilization ¼ 0:27)

Partition 4
(utilization ¼ 0:03)

(4, 100)
(9, 120)
(7, 150)
(15, 250)
(10, 320)

(2,
(1,
(8,
(4,

(7, 80)
(9, 100)
(16, 170)

(1, 80)
(2, 120)

50)
70)
110)
150)

76

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

Fig. 3. Distance-constrained scheduling model.

period in a partition. To satisfy the distance constraint
between any two consecutive invocations, we can adopt
the pinwheel scheduling approach and transfer fgk g into
a harmonic set through a specialization operation (Chan
and Chin, 1992; Han et al., 1996). Note that, in (Han
et al., 1996), a ﬁxed amount of processing time is allocated to each task and would not be reduced even if we
invoke the task more frequently. This can lead to a
lower utilization after the specialization operations. For
our partition-scheduling problem, we allocate a certain
percentage of processor capacity to each partition.
When the set of partition cycles fgk g is transformed into
a harmonic set fhk g, this percentage doesnÕt change.
Thus, we can schedule any feasible sets of (ak , gk ) as long
as the total sum of ak is less than 1.
A simple solution for a harmonic set fhk g is to assign
hk ¼ g1 for all k. However, since it chooses a minimal
invocation period for every partition, a substantial
number of context switches between partitions could
occur. A practical approach to avoiding excessive context switches is to use HanÕs SX specialization algorithm
with a base of 2 (Han et al., 1996). Given a base partition
cycle g, the algorithm ﬁnds a hi for each gi that satisﬁes:
hi ¼ g2j 6 gi < g2jþ1 ¼ 2hi :
To ﬁnd the optimal base g in the sense of processor
utilization, we can test all candidates g, in the
of
Prange
h
(g1 =2; g1 ] and compute the total capacity
a
.
To
k k
obtain the total capacity, the set of gk is transferred to
the set of hk based on corresponding g and then the least
capacity requirement, ahk , for partition cycle hk is obtained from Theorem 1. The optimal g is selected in
order to minimize the total capacity.
In Fig. 4, we show a ﬁxed-cyclic processor scheduling
example that guarantees distance constraints for the set

of partition capacities and cycles, A(0.1, 12), B(0.2, 1 4),
C(0.1, 2 1), D(0.2, 2 5), E(0.1, 48), and F(0.3, 50). We use
the base of 10 to convert the partition cycles to 10, 10,
20, 20, 40, and 40, respectively.

4. SPIRIT-Kernel architecture
The SPIRIT-lKernel provides a strongly partitioned
operating environment to the partitions that can accommodate application speciﬁc operating system policies and specialties. A partition can have the ﬂexibility
of choosing its own operating system personalities, such
as task scheduling policy, interrupt and exception-handling policy, and inter-task communication, etc. The
SPIRIT-lKernel provides only the minimal necessary
functions, such as partition management and scheduling, memory management and protection, interrupt/
exception dispatcher, timer/clock services, inter-partition communication, device driver support, and system
conﬁguration management, as shown in Fig. 5. Following the design concepts of the second-generation
microkernel architecture, we have built the kernel from
scratch and ﬁnely tuned it, utilizing the features of underling processor architecture in order to achieve ﬂexibility and eﬃciency. The current implementation of the
SPIRIT-lKernel takes advantage of hardware support
from the PowerPC processor architecture. We believe
that the kernel can be ported to diﬀerent hardware
platforms easily, because the kernel requires only a
MMU and timer interrupt support. In this section, we
discuss only the fundamental features of the kernel. The
subject related to RTOS port interface will be discussed
in the following section.

Fig. 4. Example of distance constraint cyclic schedule.

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

Fig. 5. The architecture of SPIRIT-lKernel.

4.1. Memory management
To guarantee strong spatial partitioning, we use a
ﬁxed two-level hierarchy of address spaces––kernel and
partition address spaces. The kernel and partitions have
their own protected address space in which their codes,
data, and stacks are located. Protection of address space
is implemented by a hardware protection mechanism
like the memory management unit of a processor. We
believe that an optimization of address space management to the underlying processor architecture is the best
solution to achieve both eﬃciency and deterministic
access. A kernel address space is protected from illegal
accesses of partitions, and a partition address space is
also protected from potential interference of other partitions. Since the kernel is dependable and needs to be
certiﬁed in safety critical systems, we can allow the
kernel to access the address spaces of all partitions.
In the SPIRIT-lKernel environment, address space
allocation and access policy are determined at system
integration time and saved in a secure kernel area. Like
most RTOS, we excluded the use of virtual memory
concept in which each partition is allocated independent
virtual address space. While conﬁguring an integrated
system, we divide total physical memory space into nonoverlapped regions that satisfy the memory requirement
of each partition. The RTOS on top of the SPIRITlKernel is informed only its own allocated address
space. The SPIRIT-lKernel is not responsible to allocate memory space to real-time tasks because it is the
job of the local memory manager of the RTOS. Instead

77

of using the full features of the memory management
unit, we use an address translation from virtual (logical)
address to physical address for protection purposes
only. If we allocated a separate page table per partition
for supporting a full virtual memory system, we would
face poor deterministic behavior and performance, and
increased page table size. In the SPIRIT-lKernel, since
we use the same address for both virtual address and
physical address, the page table size is only dependent
on the size of physical memory.
One of the advantages of microkernel architecture is
that the user application is allowed to have its own
memory management scheme. To support this capability, the microkernel oﬀers kernel primitives that can be
used in dynamic conﬁguration of the address space. The
SPIRIT-lKernel also provides a limited capability of a
dynamic memory management primitive for a partition.
It provides a kernel primitive to allow a partition to
change its page attributes dynamically. For example,
using this primitive, a partition can change access attribute of its code segment to read-only. When the
primitive is issued by a partition, its validity is checked
at kernel level before modifying the actual page attributes by the kernel. However, it is always enforced that
the ownership of particular page, which is determined at
system integration time, is not changed at run time.
To achieve high performance as well as protection,
we optimized the memory management scheme of the
SPIRIT-lKernel according to the PowerPC architecture. Both MMU block address translation (BAT) and
segment/page functions of the PowerPC are used to
implement spatial partitioning. The BAT and segment/
page table mechanisms are used to protect the kernelÕs
address space and partitionÕs address space respectively.
Combined use of the two memory management techniques of the PowerPC enables us to achieve a very
low overhead in kernel–partition switching and task
switching within the partition address space. For example, there are expected to be many invocations of
kernel primitives and interrupt/exception handlers. All
these invocations potentially require context switching
between the partition (user-mode) and the kernel (supervisor mode). Using BAT and segment/page table
methods for the kernel and partitions, respectively,
we can signiﬁcantly reduce kernel–partition switching
overhead because the PowerPC can concurrently perform address translations and protection checks in both
BAT and segment/page table mechanisms. In the case of
partition switching, only one additional segment register
loading would be suﬃcient.
4.2. Partition management
An application partition is the basic scheduling entity
of the SPIRIT-lKernel. A partition represents a welldeﬁned and protected (both temporally and spatially)

78

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

entity in which diﬀerent kinds of RTOS and their
applications can be implemented. Diﬀerent operating
system personalities, such as local task scheduler, communication facilities, synchronization method, etc. can
be implemented within the boundary of a partition. Due
to the two-level static scheduling policy used in our
SPIRIT, temporal and spatial resources are allocated to
the partitions at system integration time. Therefore, all
attributes of the partitions, including time slices, memory allocations, etc. are stored in the partitionÕs conﬁguration area in the kernel space. Table 4 shows the
important conﬁguration parameters of a partition
stored in the kernel.
Basically, we do not allow the shared libraries among
the partitions because our goal is to protect against
partition failure, even caused by the COTS kernel itself.
So all partition code, including partitionÕs local kernel,
are executed in user-mode. If we allow partitions to
share the COTS RTOS kernel code and its libraries, a
corruption or bug of the shared code would aﬀect all the
partitions that share the code. However, we allow
shared libraries among the non-critical partitions to save
memory space.
As described earlier, we use distance-constrained cyclic partition scheduler to dispatch partitions. There are
two possible methods in implementing a cyclic scheduler.
One is to use a SPIRIT-lKernel time tick implemented
by a periodic time tick interrupt with a ﬁxed rate. In this
case, the granularity of the execution time allocation to
the partitions and local time tick slice of a partition are
limited by the resolution of the kernel tick. Although it
may decrease the schedulable utilization bound of the
integrated system, it enables building a simpler and more
predictable cyclic scheduler. The other approach is to use
a reference clock and a timer of ﬁne resolution. It has the
advantages of accurate time allocation and avoiding a
spurious kernel tick overhead, however it increases the
complexity of managing time both in the kernel and
partitions. In the prototype implementation, we use the
ﬁrst approach, as used in MITÕs exokernel.
4.3. Timers/clock services
Timers/clock services provided by COTS RTOS
generally rely on the time tick interrupt of the kernel,

which has a relatively low resolution, such as 10 or 16
ms. The sleep system call is a typical example of this
service category. The other way of providing high-resolution timers/clock service is by implementing a special
timer device and its device driver. In our environment,
the latter can be implemented by the device driver
model, which will be described later.
A partition maintains a local time tick that is synchronized to the global reference clock, the value of
kernel tick. The local time tick interval of a partition
must be multiples of kernel tick interval. Since we use a
cyclic scheduling approach, it is possible that the user
timer service event could occur in the deactivation time
of a partition. The solution to this is that every time a
partition is resumed, it goes to a local event server ﬁrst.
The event server checks the events that have occurred
during the deactivation time of the partition, and delivers events to the local kernel if needed. The event
server will be discussed later in the paper.
4.4. Exception-handling
We distinguish the interrupt and exception in designing the kernel. An interrupt is triggered from the
external environment of the core processor, while an
exception is triggered internally during the execution.
For example, a time tick event is considered as an interrupt, while an illegal instruction violation event is an
exception. The distinction is necessary as exceptions are
usually processed in the context of their own partitions.
For example, if an exception occurs in the middle of a
task, the exception-handling routine of the corresponding partition should handle the exception as an event
related to the task. We will discuss detailed exceptionhandling using the event delivery object and the event
server in Section 5. In this sub-section, we only describe,
on a higher-level, exception-handling issues related to
the dependability of the integrated system.
Since the SPIRIT-lKernel has two-level hierarchical
schedulers implementing strong partitioning concepts, a
well-deﬁned exception-handling mechanism is essential
to achieve the dependability of the integrated system.
For example, when a fatal exception occurs while executing the partitionÕs local kernel, it causes the crash of
the partition. But, if an exception occurs while executing

Table 4
Partition conﬁguration information
Class

Data

Description

Spatial partitioning
Temporal partitioning

Address range
Cyclic schedule table
Local time tick slice
Event server
Event delivery object
Initialization address
Interrupt Flag

Memory space allocated to a partition
Distance constraint guaranteed cyclic time allocation table
Local time tick resolution of a partition
Address of event server of a partition
Address of event delivery object
Start address of a partition
Address of interrupt ﬂag to emulate user-mode interrupt enable/disable

Miscellaneous

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

a task, the handling may be diﬀerent. In the ﬁrst case, we
need to restart the crashed partition, or disable the
partition and execute idle partition server instead. In the
latter case, we just suspend the failed task or restart
the task.
4.5. Inter-partition communication
In complex and evolving real-time systems like
SPIRIT, it is very important to provide scalable and
robust communication facilities for communicating partitions. The legacy inter-process communication method
used in traditional microkernel architecture is not suﬃcient to support all the requirements of SPIRIT, which
needs special features like fault tolerance, one(many)to-many partitions communications, supporting system
evolution, and strong partitioning.
Therefore, we have selected a publish-subscribe model
as the basic inter-partition communication architecture
of SPIRIT-lKernel. The proposed publish-subscribe
communication model was designed to achieve the following goals.
4.5.1. Supporting a variety of communication models
Due to the complexity of SPIRIT, it needs a variety
of communication models, such as point-to-point,
multicast and broadcast to serve various requirements of
integrated partitions.
4.5.2. Fault tolerance
In SPIRIT, it is not uncommon to replicate applications for fault tolerance. With the enhanced publishsubscribe model we can implement a fault tolerant
communication model such as a master/shadow model.
4.5.3. Evolvability/upgradability
Considering the relatively long-life-time of complex
real-time systems such as avionics, it is anticipated that
the system will face modiﬁcations for system evolution
and upgrade. A basic publish-subscribe model is a good
candidate for this purpose.
4.5.4. Strong partitioning
Basically, all communication requirements are
scheduled statically at system integration time. So the
correctness and compliance of messages exchanged
among partitions are to be checked at the kernel to
protect other partitions from being interfered with by
non-compliant messages.
In the inter-partition communication mechanism of
the SPIRIT-lKernel, it is not recommended to use acknowledged message communication among partitions.
This is because we use a cyclic scheduler for the partitions. When a sender sends a message and waits for the
response from a receiver, it has to wait until the receiver
resumes at the next assigned time interval, and the

79

senderÕs own partition gets dispatched after that. More
detailed information about inter-partition communication is beyond the scope of the paper.
4.6. Device driver model
The SPIRIT-lKernel provides two device driver
models for exclusive and shared devices. The exclusive
device driver model can be used for the devices that are
safety-critical or exclusively used by a partition. To allow a partition to access exclusive devices, the device I/O
address must be exclusively mapped to the address space
of the corresponding partition. In safety-critical systems
like avionics systems, it is preferred to use a polling
method rather than an interrupt method, because a
polling method is more predictable and reliable than
an interrupt method. The kernel also provides a secure
interrupt registration method for the exclusive device
driver model as well as the polling method. The shared
device driver model is used for devices that are potentially shared by multiple partitions using multiplexing/
demultiplexing methods. More detailed information
about device driver model is beyond the scope of the
paper.

5. Generic RTOS port interface (RPI)
The SPIRIT-lKernel provides a generic RTOS port
interface (RPI) that can be used to port diﬀerent kinds
of COTS RTOS on top of the kernel. With the help of
the RPI, the SPIRIT-lKernel itself can be built independently without considering the COTS RTOS. It also
helps developers to port new RTOS easily. In this section, we describe the selected features of RPI, such as
event delivery object (EDO), event server, kernel context
switch request (KCSR) primitive, user-mode interrupt
enable/disable emulation, and miscellaneous considerations. To give an overview of the RPI mechanism, we
depict an example of partition switching using RPI in
Fig. 6.
In Fig. 6, when a partition is to be preempted, the
interrupt/exception dispatcher saves the context of the
current partition and loads the context of the next
partition using its control block (1). The dispatcher
prepares the EDO, including the new loaded context,
and delivers it to the event server of the next partition
(2). The event server does housekeeping jobs for the next
partition and also invokes the scheduler of the partition
if needed (3). The updated partition context, to which
CPU control goes, is delivered to the SPIRIT-lKernel
via the KCSR primitive (4). Finally, the kernel gives
CPU control to the correct location given by the KCSR
primitive (5). The detailed explanation will be given in
subsequent sections.

80

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

Fig. 7. Structure of the EDO.

Fig. 6. EDO, event server, and kernel-context switch request primitive
interaction.

5.1. Event delivery object
A partition, which might be based on a COTS RTOS,
needs to be informed of hardware events, such as time
tick expiration, processor exceptions, and interrupts.
Since these event classes are caught by the SPIRITlKernel in supervisor mode, secure and eﬃcient delivery
of an event from the kernel to a partition is necessary to
be processed in the local event handling routine of a
partition. In SPIRIT, it is not allowed for the kernel to
directly execute RTOS-speciﬁc local event handling
routines for two reasons. Firstly, if an event handler in
RTOSÕs address space is called in the context of the
kernel (supervisor mode), it may violate the strong
partitioning requirements of the system due to the
possible corruption or overrun of the event handler.
Secondly, it increases the complexity of the kernelÕs interrupt/exception-handling codes and data structures,
because the kernel must have detailed knowledge of the
RTOS design, which is vendor speciﬁc in COTS RTOS.
So we have devised a generic event delivery method that
can be used to solve the problem. The event server and
kernel context switching request primitive, introduced in
the next two sub-sections, are used together with the
event delivery method.
As mentioned above, to deliver an event securely and
eﬃciently from the kernel to a partition, we have devised
the EDO. The EDO is physically located in the address
space of a partition, so it can be accessed from both the
kernel and its owner partition. We show the abstract
structure of the EDO that is used in our PowerPC
prototype in Fig. 7.

We describe the usage of the EDO with an example of
task switching caused by partition time tick expiration
delivery. There are two diﬀerent task context-saving
approaches in implementing RTOS. One is to save the
task context that is caught at the instance of the time
tick interrupt arrival. The other is to save the context in
the middle of the time tick service routine of the RTOS.
For instance, the ﬁrst approach is used in VxWorks and
the latter in eCos. In the SPIRIT-lKernel environment,
the kernel must transfer the saved task context to a
partition in the ﬁrst case, but not in the second case.
However, for the generality of the SPIRIT-lKernel, it
also delivers the saved context in the second case. Based
on the EDO mechanism, we have developed the generic
interrupt and exception-handling routine of the SPIRITlKernel shown in Fig. 8.
Responding to local time tick expiration, the local
task scheduler may switch tasks based on its local
scheduling policy and the status of tasks. In Fig. 6, upon
the arrival of the time tick expiration interrupt, the
kernel saves the context in the EDO information area of
the corresponding partition, and sets the EDO type as
time tick expiration (7) and (8). After preparing an
EDO, the kernel sets the interrupt return address as the
entry address of the partitionÕs event server (9).
After return from the interrupt, now in user-mode,
the local event server checks the EDO and dispatches
the proper local event handling routine. In the VxWorks
case, receiving a time tick expiration, the local time tick
interrupt handler checks the status of the VxWorks
kernel to determine whether task switching is needed or
not. When task switching is needed, it saves the context
in the EDO information area in the task control block
of the preempted task. Then the event server issues a
KCSR to the kernel providing the context of the newly
dispatched task. While interrupt and exception-handling
routines are dispatched by the kernel in supervisor mode
in the original COTS RTOS, the partitionÕs local handling routines are dispatched by the event server in usermode in the SPIRIT-lKernel. Since all local event
handling routines are executed in user-mode and in the
local partitionÕs address space, we can protect the kernel
and other partitions from the faults of a partition. The

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

81

Fig. 8. SPIRIT-lKernelÕs generic interrupt/exception-handling procedure.

overall procedure of the event server can be found in the
Fig. 9.
5.2. Event server
The event server of a partition is an essential part of
the SPIRIT-lKernel. It has the role of a mediator between the kernel and partitions by executing partitionÕs
local interrupt and exception-handling routines, and
performing housekeeping jobs such as delivering local
kernel events that occurred during the deactivation time
of a partition. Fig. 9 shows the generic event server
procedure of a partition. The implementation of an
event server depends on the corresponding RTOS. We
explain the handling of the partition switching event as
an example.
In Fig. 9, when an event server is user-mode-called
from the SPIRIT-lKernel, it checks the type of EDO
and process the EDO properly. If the event is a partition
switching, it executes the housekeeping jobs of a parti-

tion (3). Since there may be asynchronous local kernel
events that arrived in the deactivation time of the newly
dispatched partition, they must be delivered to the local
RTOS kernel before dispatching the task of the partition. If rescheduling is needed in the local RTOS kernel
and the current resume pointer is within the application
task, not within a local kernel, the old task context that
is in the EDO will be saved in the task control block,
and the new context will be transferred to the kernel
using the KCSR primitive (4) and (5). It should be noted
that the SPIRIT-lKernel would not allow any potentially harmful modiﬁcations to privileged registers based
on security rules.
5.3. Kernel context switch request primitive
In most processor architectures, the context of a task
is composed of both user-mode and supervisor-mode
accessible registers. When a partitionÕs local task scheduler tries to restore the context of the next dispatched

82

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

Fig. 9. SPIRIT-lKernelÕs generic event server procedure.

task in the SPIRIT-lKernel system, a privilege violation
exception may occur, because the context loader runs in
user mode. To solve this problem, we have devised a
KCSR primitive, which delegates the kernel to perform
a context-loading job. The kernel is provided with the
restoring context information with which the kernel
loads and resumes a new task. The primitive has two
types; one is for loading privilege registers only, and the
other for loading all registers. Since a large number of
registers are user-mode accessible, it is better to use the
ﬁrst primitive in most cases for better performance.
However, in some cases, these primitives are not needed
because the context of a task is solely composed of usermode accessible registers.
5.4. Interrupt enable/disable emulation
Most COTS RTOS use interrupt- enable and disable
functions to guard critical sections in implementing
system calls and the core of their kernels. But, in the
SPIRIT-lKernel environment, we cannot allow a partition to execute interrupt control functions, because
these functions must be run in supervisor mode. Dis-

abling interrupts may block the SPIRIT-lKernel for a
signiﬁcant amount of time. To solve this problem, it is
required to re-write the original interrupt- enable and
disable functions, while guaranteeing its original intention of protecting critical sections from interrupts.
We use atomic set and reset instructions to implement
replacements of the original interrupt enable and disable
functions. In the PowerPC architecture, atomic set and
reset logical functions are implemented by a reservationbased instruction set. In a similar way to implementing a
binary semaphore, disabling interrupts is emulated by
an atomic set instruction for a variable, interrupt ﬂag,
which is located in the partition address space and
shared by the kernel and the owner partition. While the
interrupt ﬂag is set, the kernel delays the delivery of an
interrupt until the interrupt is enabled. The interrupt
enable function is implemented by the atomic reset instruction, which clears the ﬂag.
5.5. Miscellaneous interface
There are other interfaces to be considered while
porting new COTS RTOS on top of the SPIRIT-lker-

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

nel. An example of these miscellaneous interfaces is related to hardware initialization. Most RTOS provide a
board support package that is used to initialize and
conﬁgure the hardware. The board support package
includes hardware dependent activities such as cache
enabling and disabling, MMU manipulation, etc. Since
only the SPIRIT-lKernel initializes and conﬁgures
shareable hardware resources to ensure dependable operation, hardware dependent functions in a board support package must be removed.

6. Performance evaluation
In order to prove and demonstrate the feasibility of
the proposed SPIRIT-lKernel, we have implemented a
prototype on a DY4-SVME171 commercial-oﬀ-the-shelf
embedded CPU board. The board houses an 80MHz
Motorola PowerPC 603e and 32MB of main memory.
The PowerPC 603e provides a memory management
unit that is essential for implementing spatial partitioning. The board provides a programmable periodic
timer interrupt for kernel tick implementation. These
two hardware components are the minimum requirements of the SPIRIT-lKernel. On top of the hardware
platform, we have implemented the SPIRIT-lKernel
that cyclically schedules heterogeneous COTS RTOS
partitions, WindriverÕs VxWorks and CygnusÕs eCos.
The reason we chose VxWorks and eCos for COTS
RTOS partitions is that they are extreme cases to be
considered in designing the kernel. While the VxWorks
provides binary kernel and modiﬁable board support
packages written in C, eCos provides full Cþþ kernel
sources. It takes more eﬀorts to port vxWorks than
eCos. However, using RTOS port interface (RPI), the
porting eﬀorts were signiﬁcantly reduced. The size of the
SPIRIT-lKernel code is 36 Kbytes.
In evaluating the prototype we integrate and run four
diﬀerent partitions, two VxWorks-based and two eCosbased applications. We measure the execution time using the PowerPC time base register, which has a 125 ns
resolution. To evaluate the performance of the kernel,
we have measured and analyzed the overheads for kernel tick, partition switching, kernel-user switch, and
TLB miss handling.

83

Table 5
Measured kernel overheads
kt_overhead1

kt_overhead2

Avg (ls)

Min (ls)

Max (ls)

Avg (ls)

Min (ls)

Max (ls)

0.837

0.750

3.875

9.383

9.000

18.375

expiration. kt_overhead2 is the time taken to deliver the
EDO type of Time_Tick_Expiration to the event server
of the local partition. We show the result in Table 5.
To help ﬁnd the feasible kernel tick resolution, we
calculate r_kt_overhead and p_kt_overhead that are the
percentages of the redundant kernel tick overhead and
total kernel tick overhead for a partitionÕs local task
scheduling, respectively. kt_period and pt_period represent the length of kernel time tick and partition time
tick, respectively.
kt overhead1
;
kt period



pt period
p kt overhead ¼ kt overhead1
1
kt period
	
pt period:
þ kt overhead2
r kt overhead ¼

In Fig. 10, we depict average and worst case kernel
tick overheads that are obtained by varying the kernel
tick period, 100 ls, 500 ls, 1 ms, 2 ms, 5 ms, with a ﬁxed
partition local tick period of 10 ms.
As illustrated in the Fig. 10, while the kernel tick
period is greater than 500 ls, each of all four overheads
is less than 1% of the total CPU capacity. These practically acceptable overheads were possible due to the
eﬃcient design of the kernel tick interrupt handler and
low latency interrupt handling support from the power
PC processor. In the instance of the redundant kernel
tick event, the kernel tick interrupt handler increments

6.1. Kernel tick overhead
Since the kernel and partitionÕs local RTOS are synchronized to and scheduled based on the kernel tick, the
resolution of the kernel tick is an important factor that
aﬀects the systemÕs performance and schedulability. To
evaluate the overhead of the kernel tick, we obtained
two basic components, kt_overhead1 and kt_overhead2.
kt_overhead1 is measured when a kernel tick is used for
neither partition switching nor local partition time tick

Fig. 10. Kernel tick overheads.

84

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

the kernel tick value and compares it with the precomputed kernel tick value of the next events of either
partition switching or local time tick expiration, and
returns from the interrupt.

Table 7
Kernel-context switch request primitive performance
Avg (ls)

Min (ls)

Max (ls)

1.343

1.250

7.500

6.2. Partition switch overhead
Based on the oﬀ-line scheduled timetable, the kernel scheduler dispatches partitions. When a partition
switching event occurs, the kernel scheduler saves the
context of the current partition and reloads the saved
context of the next partition. Then CPU control goes to
the event server of the newly dispatched partition. Since
the kernel has a generic partition switching handler regardless of the types of RTOS that are saved or restored,
we expect deterministic partition switch overheads. The
Table 6 shows the partition switch overheads.
6.3. Kernel-user switch overhead
As in a conventional operating system, the kerneluser-mode switch overhead is an important factor for
evaluating the performance of the system. To reduce the
overhead, we use the BAT and segment/page table
schemes for the kernel and a partition, respectively.
Since both schemes are executed concurrently during the
address translation and a result is chosen according to
the current processor mode, the only overhead for the
kernel-user switch is the time needed to set or reset the
processor mode bit in the machine state register. So we
can claim that the pure overhead due to the kernel-user
switch, while distinguishing the processor modes, supervisor and user-mode, can be ignored.
Instead of measuring the pure kernel-user switch
overhead, we measured the kernel-context switch request primitive, which is an essential method of CPU
control transfer, which requires supervisor-mode privileges, in user-mode. The execution time of the primitive
is measured and listed in Table 7.
6.4. TLB miss handling overheads
In many real-time applications, virtual memory is not
supported and supervisor and user-modes of execuTable 6
Partition switch overhead
Avg (ls)

Min (ls)

Max (ls)

10.491

10.000

21.500

tion are not distinguished. However, it is an essential
principle to distinguish supervisor and user-modes
to guarantee the spatial partitioning concept. The
SPIRIT-lKernel uses the PowerPC MMUÕs segment/
page scheme to protect the address space of partitions.
Since this scheme relies on virtual to physical address
translation, the performance of the TLB (translation
look-aside buﬀer) scheme of the PowerPC is very important. Considering the fairly uniform distribution of
memory access in the SP-RTS environment due to
partitioning, we should pay much attention to the performance of the TLB. In the prototype, it requires 8K
page table entries for 32Mbytes of physical memory.
However, the PowerPC provides only 64 page table
entries for ITLB and DTLB, respectively. We measured the TLB miss handling overhead and showed in
Table 8.

7. Related works
A diﬀerent two-level hierarchical scheduling scheme
has been proposed by Deng and Liu (1997). The scheme
allows real-time applications to share resources in an
open environment. The scheduling structure has an
earliest-deadline-ﬁrst scheduling at the operating system
level. The second level scheduling within each application can be either time-driven or priority driven. However their model does not support one of our target
applications, the IMA architecture. In higher-level point
of view, their model does not support strong partitioning concept because their OS level scheduler should have
knowledge of tasks which resides in higher-level application.
There have been a growing number of contemporary
RTOS both in the academia and in the commercial
market place. Systems such as MARUTI (Mosse et al.,
1990) and MARS (Engler et al., 1995) represent a timetriggered approach in the scheduling and dispatching of
safety-critical real-time applications, including avionics.
However these systems, either provide no partitioning
scheme, as in the case of MARUTI, or they rely com-

Table 8
TLB Miss handling overheads
Instruction TLB

Data TLB load

Data TLB store

Avg (ls)

Min (ls)

Max (ls)

Avg (ls)

Min (ls)

Max (ls)

Avg (ls)

Min (ls)

Max (ls)

1.405

0.875

3.500

2.433

1.625

4.250

2.217

1.125

4.875

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

pletely on proprietary hardware, as in the case of
MARS.
Commercial RTOS such as OSE, VRTX, and Neutrino provide varying levels of memory isolation for
applications. However, applications written for any of
them are RTOS speciﬁc. In addition, it is not possible
for applications, which are written for one RTOS to
coexist on the same CPU with a diﬀerent RTOS without
a considerable eﬀort of re-coding and re-testing. The
approach that we presented in this paper overcomes
those drawbacks by ensuring both spatial and temporal
partitioning while allowing the integration of application developed using a contemporary RTOS.
The Airplane Information Management System
(AIMS) on board the Boeing 777 commercial airplane is
among the few examples of IMA based systems (Johnson, 1993). Although, the AIMS and other currently
used modular avionics setup oﬀer strong partitioning,
they lack the ability of integrating legacy and third
partyÕs applications. In our approach, a partition is an
operating system encompassing its application. All
previous architectures have the application task(s) as the
unit of partitioning.
The SPIRIT-lKernel cyclic scheduler can be viewed
as virtual machine monitor that exports the underlining
hardware architecture to the application partitions,
similar to the IBM VM/370 (IBM, 1973). The concept of
virtual machines has been used for variety of reasons
such as cross-platform development (Goldberg, 1974),
fault tolerance (Bressoud and Schneider, 1995), and for
hardware-independent software development (Gosling
and McGilton, 1996). Although some of these virtual
machines restrict memory access to maintain partitioning, we are not aware of any that enforces temporal
partitioning or supports RTOS. Needless to say that, it
does not guarantee any hard real-time constraints any
better that the host operating system does.
Decomposing the operating system services into a lkernel augmented with multiple user-level modules has
been the subject of extensive research, such as SPIN
(Bershad et al., 1995; Ford et al., 1996), VINO (Small
and Seltzer, 1995), Exokernel (Engler et al., 1995), and
L4 (Liedtke, 1995; Liedtke, 1996; Hartig et al., 1997).
The main objective of these l-kernel-based systems is to
eﬃciently handle domain-speciﬁc applications by oﬀering ﬂexibility, modularity, and extendibility. However,
none of these systems is suitable for hard real-time applications.
RT-Mach and CVOE are among the few l-kernel
based RTOS that supports spatial and temporal partitioning. Temporal guarantees in RT-Mach are provided
through an operating system abstraction called processor reserve (Mercer et al., 1994). Processor reservation is
accepted through an admission control mechanism employing a deﬁned policy. CVOE is an extendable RTOS
that facilitates integration through the use a callback

85

mechanism to invoke application tasks (TI, 1997). Yet
tasks from diﬀerent applications will be mixed and thus
legacy application cannot smoothly be integrated without substantial modiﬁcation and revalidation.

8. Conclusion
The SPIRIT-lKernel is designed to provide a
software platform for strongly partitioned integrated
real-time systems such as IMA systems that have been
signiﬁcantly studied in both academia and industries.
Using the kernel, we can achieve better reliability, reusability, COTS beneﬁts, and cost reduction in building
complex real-time systems. We also propose the scheduling algorithm with which multiple hard real-time applications can be scheduled to run on the SPIRITlKernel platform.
For further study, we are planning to enhance the
kernel in three directions. First, we will build our own
local kernel for a partition instead of using COTS
RTOS. It will make a full suite of two-level operating
systems platform. Second, we will extend current uniprocessor architecture to multiprocessor and distributed
real-time computing environment. Third, we will develop other two-layer scheduling strategies instead of
cyclic/priority driven scheduler pair while guaranteeing
strong partitioning requirement.

Acknowledgements
We would like to thank Dr. Mohamed Aboutabl, and
Mr. James Mcelroy for their valuable discussion and
suggestions.

References
ARINC, 1991. Design Guide for Integrated Modular Avionics.
ARNIC Report 651, Aeronautical Radio Inc.
ARINC, 1997. Avionics Application Software Standard Interface.
ARNIC Report 653, Aeronautical Radio Inc.
Bershad, B. et al., 1995. Extensibility safety and performance in the
SPIN operating system. In: Proceedings of the ACM Symposium
on Operating System Principles, pp. 267–284.
Bressoud, T.C., Schneider, F.P., 1995. Hypervisor-based fault-tolerance. In: Proceedings of the ACM Symposium on Operating
Systems Principles, pp. 1–11.
Chan, M.Y., Chin, F.Y.L., 1992. General schedulers for the pinwheel
problem based on double-integer reduction. IEEE Trans. Comput.
41, 755–768.
Deng, Z., Liu, J., 1997. Scheduling real-time applications in an open
environment. In: Proceedings of IEEE Real-Time Systems Symposium, pp. 308–319.
Engler, D., Kaashoek, M., OÕToole Jr., J., 1995. Exokernel: an
operating system architecture for application-level resource

86

D. Kim et al. / The Journal of Systems and Software 65 (2003) 71–86

management. In: Proceedings of ACM Symposium on Operating
Systems Principles, pp. 251–266.
Ford, B. et al., 1996. Microkernels meet recursive virtual machines. In:
Proceedings of the Second Symposium on Operating Systems
Design and Implementation, pp. 137–151.
Goldberg, E.P., 1974. Survey of virtual machine research. IEEE
Comput., 34–45.
Gosling, J., McGilton, H., 1996. The Java Language Environment: A
White Paper. Technical Report, Sun Microsystems Computer
Company.
Han, C.C., Lin, K.J., Hou, C.J., 1996. Distance-constrained scheduling
and its applications to real-time systems. IEEE Trans. Comput. 45
(7), 814–826.
Hartig, H., Hohmuth, M., Liedtke, J., Schonberg, S., Wolter, J., 1997.
The performance of l-kernel-based systems. In: Proceedings of
ACM Symposium on Operating Systems Principles, pp. 66–77.
IBM, 1973. IBM Virtual Machine Facility /370: Release 2 Planning
Guide. Technical Report GC20-1814-0, IBM Corporation.
Johnson, M., 1993. Boeing 777 airplane information management
system––philosophy and displays. Proceedings of the Royal Aeronautical SocietyÕs Advanced Avionics Conference on Aq330/A340
and the Boeing 777 Aircraft.
Kim, D., 2001. Strongly Partitioned System Architecture for Integration of Real-Time Applications. Ph.D Dissertation, University of
Florida.
Kim, D., Lee, Y.H., Younis, M., 2000. SPIRIT-lKernel for strongly
partitioned real-time systems. In: Proceedings of IEEE Real-Time
Computing Systems and Applications, pp. 73–80.
Lee, Y.H., Kim, D., Younis, M., Zhou, J., 1998. Partition scheduling
in APEX runtime environment for embedded avionics software. In:
Proceedings of IEEE Real-Time Computing Systems and Applications, pp. 103–109.
Lee, Y.H., Kim, D., Younis, M., Zhou, J., McElroy, J., 2000. Resource
scheduling in dependable integrated modular avionics. In: Proceedings of IEEE International Conference on Dependable Systems and Networks, pp. 14–23.
Lehoczky, J., 1990. Fixed priority scheduling for periodic task sets
with arbitrary deadlines. In: Proceedings of IEEE Real-time
Systems Symposium, pp. 201–209.
Lehoczky, J., Ramos-Thuel, S., 1992. An optimal algorithm for
scheduling soft-aperiodic tasks in ﬁxed-priority preemptive systems. In: Proceedings of IEEE Real-Time Systems Symposium, pp.
110–123.
Lehoczky, J., Sha, L., Ding, Y., 1989. The rate–monotonic scheduling
algorithm: exact characteristics and average case behavior. In:
Proceedings of IEEE Real-Time Systems Symposium, pp. 166–
171.
Liedtke, J., 1995. On l-kernel construction. In: Proceedings of ACM
Symposium on Operating Systems Principles, pp. 237–250.
Liedtke, J., 1996. Toward real microkernels. Commun. ACM 39 (9),
70–77.
Liu, C.L., Layland, J.W., 1973. Scheduling algorithms for multiprogramming in a hard real-time environment. J. ACM 20 (1), 46–61.

Mercer, C., Rajkumar, R., Zelenka, J., 1994. Temporal protection in
real-time operating systems. In: Proceedings of the IEEE Workshop on Real-Time Operating Systems and Software, pp. 79–83.
Mosse, D., Agrawala, A., Tripathi, S., 1990. Maruti a hard real-time
operating system. In: IEEE Workshop on Experimental Distributed Systems, pp. 29–34.
Small, C., Seltzer, M., 1995. Structuring the kernel as a toolkit of
extensible, reusable components. In: Proceedings of the 1995
International Workshop on Object Orientation in Operating
Systems, pp. 134–137.
TI, 1997. Architecture Document for the Combat Vehicle Operating
Environment. Technical Report, Texas Instruments, Defense
Systems and Electronics Group.
Daeyoung Kim received the BS and MS degrees in computer science
from the Pusan National University, Korea, in 1990 and 1992, respectively, and PhD degree in computer engineering from the University of Florida in August 2001. Since February 2002, he has been an
assistant professor with the Department of Computer Science and
Engineering, the Information and Communications University, Korea.
From September 2001 to January 2002, he was a research assistant
professor at the Arizona State University. Dr. Kim worked as a research staﬀ for Electronics and Telecommunications Research Institute
(ETRI), Korea, from January 1992 to August 1997. His research interests include real-time systems (scheduling, communication, and
operating system), wireless and mobile networks, JAVA technology for
real-time applications, embedded system architecture, and distributed
and fault tolerant system. He is a member of the IEEE, the IEEE
Computer Society, the ACM, and the AIAA.
Yann-Hang Lee received the BS degree in engineering science and the
MS degree in electrical engineering from National Cheng Kung University in 1973 and 1978, respectively, and the Ph.D degree in computer, information, and control engineering from the University of
Michigan, Ann Arbor, in 1984. From December 1984 to August 1988,
he was a research staﬀ member of the Architecture Analysis and Design Group, IBM Thomas J. Watson Research Center, Yorktown
Heights, NY. From August 1988 to July 2000, he was an Professor
with the Computer and Information Science and Engineering Department, University of Florida, Gainesville. Since August 2000, he
has been a Professor with the Computer Science and Engineering
Department, Arizona State University, Tempe. His current research
interests include distributed computing and parallel processing, realtime systems, computer architecture, performance evaluation, and
fault-tolerant systems.
Mohamed YounisÕs technical interest includes embedded systems, fault
tolerant computing, system integration, distributed real-time systems,
resource allocation and scheduling and compiler-based analysis. Before joining the department of computer science and electrical engineering at UMBC, he was with the Advanced Systems Technology
Group, an Aerospace Electronic Systems R&D organization of Honeywell International Inc. While at Honeywell he led multiple projects
for building integrated fault tolerant avionics, in which a novel architecture and an operating system were developed. Prior to Honeywell, Dr. Younis worked on developing software design tools for
Siemens Corporate Research in Princeton, New Jersey and for IBM
Watson Research Center in Hawthorne, New York. Dr. Younis has
two granted and ﬁve pending patents. He served on multiple technical
committees and published in many technical conferences and journals.

Real-Time Support of Flash Memory File System for Embedded Applications
Sudeep Jain

Yann-Hang Lee

Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287

Abstract
NAND flash memory has emerged as an attractive
proposition for embedded storage applications due to
its properties of robustness, low price, a high number
of write-erase cycles, and large capacities. To make it
suitable for real-time embedded applications, this
paper presents the design of a log structured file
system that utilizes data redundancy and parallel
operations to guarantee bounded read-write access
latencies to real-time tasks, in the presence of requests
from non real-time tasks. The proposed scheme
requires minimal support from the underlying
operating system. In addition, the paper proposes a
storage scheme to reduce RAM footprint for the log
structured file system while preserving reliability and
minimal read/write overhead.

1 Introduction
Flash memory storage solutions have the properties
of shock resistance, low power consumption and
persistent storage that make them especially attractive
for storage in mobile and embedded systems. As flash
memory prices fall further it is plausible that in future,
large scale storage systems may be implemented using
this technology. For instance, in order to overcome the
limited bandwidth and storage provided by a single
flash chip, several flash chips may be stacked together
and managed by a hardware controller. Using this
approach, products that offer as high as 90GB of solid
state storage, accessible via an IDE or SCSI interface
[1][2] are available.
A NAND flash memory chip is organized into
banks, further subdivided into blocks, subdivided into
pages. The basic unit for a read or write operation is a
page and there is no overwrite operation. Once data is
written to a page, it - along with the entire block that it
belongs to - has to be erased by a garbage collection
operation before data can be written again on that page
[3]. Further, the read, write and erase operations have
different latencies, i.e., an erase operation takes an
order of magnitude more time than a write operation,
which takes substantially more time than a read
operation.

The use of flash memory in real-time embedded
systems is attractive due to the size and robustness of
flash devices and also the constant time response for
each access. However, a constant time read-write
access to flash memory cannot be guaranteed due to
garbage collection operations. Also, due to the lack of
in-place update capability, log structured file systems
will be a better choice for flash memory in which an
accurate state of the data was maintained by storing a
continuous “log of changes” on the media [4]. Like the
existing JFFS2 [5] and YAFFS [6], a log structured file
system suffers from the problem of having a very high
RAM footprint. This is because both the file systems
stores the “direct map” of logical to physical addresses
on the flash in RAM.
In this paper, we present an effective real-time flash
memory file system design to provide bounded delays
on read and write accesses and to reduce RAM
footrpint. The proposed scheme uses data redundancy
and parallelism over multiple flash memory banks in
order to guarantee real-time access. To reduce RAM
footprint requirement, our approach follows typical
hierarchical designs for storage mapping by moving
the per inode data structures (a table or linked list in
case of JFFS2, and a tree in case of YAFFS) onto the
flash. The data structures and algorithms required to
move the per inode data onto the flash without
sacrificing reliability, read-write cycles, and without
imposing higher latencies, are described.
The remainder of the paper is organized as follows.
Section 2 provides an overview of the existing
approaches towards managing storage on NAND flash
memories. Section 3 introduces the proposed garbage
collection algorithm. In Section 4, the data structures
for file system design are presented in which we aim at
a significant reduction of RAM footprint. In Section 5,
a concise conclusion is presented.

2 Background and Related Work
2.1 Garbage Collection on Flash Memories
Garbage collection is of crucial importance on flash
memories due to their page write, block erase
properties. When new data is to be overwritten on
existing data on the flash, the earlier copy is marked as
dead, and the new data is written over a completely

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

IEEE

different set of pages. As the file system remains in
operation, the number of dead pages increase, and
garbage collection has to be performed in order to
reclaim more free space. The operation of a garbage
collector over a single erase cycle may be broken into
the following steps.
• Choose a block to perform garbage collection from.
• Copy all the live data in the block to a new block.
• Erase the block from which garbage collection is
being performed.
Out of the three steps, the erase operation is nonpreemptible and takes much longer time than read or
write operations (as shown in Table 1).
Read
time
125μs

Write
time
250μs

Erase Page
time size
2000μs 2KB

Block
size
64KB

Bank
size
128KB

Table 1. Typical Flash Memory Characteristics
Most research on flash memory storage systems has
been focused on improving garbage collection for the
block device emulation approach. For instance, the
garbage collector suggested in [7] takes into account
not only the number of dead pages on a block but also
the time when a page was last marked dirty on that
block. The clustering of hot and cold data have been
proposed [8][9] to reduce the overhead of reads and
writes of live data, per block erasure. In general,
garbage collection duration is unpredictable and naive
schemes of garbage collection may cause a long
blocking delay [10].

2.2 Real-time Garbage Collection
There hasn't been a lot of work on real-time storage
on NAND flash memories. In [8], a scheme to provide
a real-time periodic garbage collector for a NAND
flash memory system is presented. The presented
solution is implemented over the Flash Translation
Layer (FTL) [11] and relies on operating system
support for real-time tasks. Real-time access is
provided by real-time tasks declaring their resource
requirements for flash (bandwidth required, period,
amount of garbage generated etc.) beforehand. A realtime garbage collector task is initiated for each realtime task. To guarantee a bounded access time, it
requires support from the underlying platform in the
form of a real-time scheduler and tasks, i.e. the
schedules for task and flash memory operation are
intertwined, and real-time data is still inaccessible
while the non-preemptible erase operation is in
progress.

2.3 High RAM Footprint of Log Structured
Flash File System
All flash file systems intended for data storage over
flash implement an out of place update mechanism.
This means that a data page identified by a {Inode
Number, offset} tuple or Logical Block Address is put
on a different physical address on the flash every time
an update to that page happens. Thus, a “map” from
logical addresses to physical addresses is maintained
by the file system. For larger capacity flash systems,
this map can be prohibitively large to store in the RAM
[12]. In YAFFS and JFFS, the two common log
structured file systems for flash memories, the basic
approach taken is to treat the blocks on the flash
memory as a logical log, and make updates to the data
as updates to a log of changes. Both JFFS2 and
YAFFS suffer the problem of having a very high RAM
footprint. This is because both the file systems store
the “direct map” of logical to physical addresses on the
flash in RAM.

3 Real-time Media Access Operations and
Garbage Collection
A basic requirement of the real-time file system is
that read accesses need to be real-time. In other words,
the time taken by the mechanism to read pages should
not be delayed by write or erase operations.
To implement the real-time read operations, we
need a minimum of two flash memory banks. All the
banks in the storage systems are paired into bank sets,
each of size two, as shown in Figure 1. Any bank in the
system may be in one of two states: Read or Write.
While one of the two banks in a particular set is in
Read state, the other is in Write state. A bank in the
Write state can perform physical writes to the media
that may or may not be interleaved with garbage
collection, while a bank in the Read state is reserved to
perform reads from the media.
Bank 0

Bank 1

Bank 2

Bank 3

State = Read

State = Write

State = Read

State = Write

Set 0

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

IEEE

Set 1

Figure 1. Flash organization of banks where two copies
of data stored in a bank set
Garbage collection is performed on a maximum of
one bank at a time, with the other bank left free to
honor read requests. A bank is scheduled for garbage
collection once the number of dirty pages on it crosses
a threshold Nth. Note that, following the log file
structure, there are three types of blocks in a bank. The

first type consists of the blocks that are all dead or live
pages. The second type is the current block that forms
a part of the log and contains dead, live, and free
pages. The last type is the blocks with only free pages.
The blocks under the consideration for garbage
collection should be restricted to the type-1 blocks. A
simple greedy garbage collector can choose a block
that has the greatest number of dead pages. This value
Nth is not an independent variable, but is fixed by the
maximum allowable utilization of the flash.
It is assumed that a write buffer can be attached to
each flash bank such that data in the buffer is ready for
accesses without any operations to the media.

3.1 Real-Time Read
The real-time read algorithm for reading one page
of data is as follows.
• If present in the write buffer, read the data.
• Otherwise, select the physical bank in the read state
for honoring this request. Compose a physical read
request - represented as {(bank, block, page), offset,
size} - using the logical read request and address
mapping present in the RAM.
• Send the read request to the selected bank.
An interarrival time constraint amounting to the
time it takes to complete one read request on one bank
is imposed on the read requests.

3.2 Real-Time Write
The real-time write operation aims at making two
copies of the data in two memory banks. A real-time
write is a two stage operation, as the physical write
operations are serialized across the two memory banks
in the bank state.
A write request causes the system to choose a bank
set that can honor the request. Each set is chosen one
after the other in a round robin manner. Once a bank
set is chosen, the first stage of the write creates a copy
of the data in the memory bank in the Write state. In
the Write state, the bank may also be performing a
garbage collection operation along with the write. In
this stage, the request is queued up in the write buffer
on the bank, say (bank W), and the request can be
interleaved with the garbage collector requests. The
second copy of the data is to be made on the memory
bank in the Read state. The request is also queued up in
its write buffer. Note that consecutive writes on
buffered page can be done in the buffer and don’t
introduce multiple writes to the media.
The second stage of the write flushes the pending
write operations on the second bank, say (bank R), into
the media. The requests are kept pending till the state
of this bank changes to Write and will be flushed to the
media as a non-real-time write. Note that this second

stage does not block any new reads, because (bank W),
having performed the write operation and/or garbage
collection is now free to handle any new read requests.
An inter-arrival time amounting to the time it takes
to complete one write request on one bank is imposed
on the write requests, i.e. the arrival times of two write
requests are separated by at-least Twrite seconds. The
bandwidth of write operation will be limited by the free
pages that can be reclaimed by garbage collection
operation and will be analyzed later in the section.

3.3 Non-Real-Time Access
The non real-time read and write algorithms are the
same as the real-time read and write algorithms, except
that only a single copy of data is made for non-realtime write requests. Also, all non-real-time requests are
queued in the non-real-time queue of the bank selected
to service the request.

3.4 Garbage Collection
After every write operation, the file system checks
the number of dead pages on the media and if the
memory banks in the Write state needs garbage
collection. If they do, the garbage collector starts
operation on the block with the highest number of dirty
pages on the bank. The garbage collection algorithm
itself is simple. For all the live pages in that block, the
garbage collector reads the live pages into memory by
issuing a blocking read request. Then, using the file
system meta data, a physical write request for the
pages is issued. Finally, the file system meta data
associated with the copied page is modified. Once all
the pages to be copied are in the write buffer, the
garbage collector issues an erase operation for the
block being collected. Note that the copy operation of
live pages should be done as non-real-time writes.

3.5 Bandwidth Analysis
To reach a steady state in the flash file system, the
amount of free space reclaimed and time spent in
garbage collection may be bounded by employing a
greedy garbage collection scheme along with a
threshold on the number of dirty pages on the flash. To
derive specific limits on the garbage collection
proposed above, we use the symbols listed in Table 2
for the following analyses.
NLive
NDirty
NP
Nb
Nreclaim
Nth
tread

number of live pages on a bank.
number of dirty pages on a bank.
number of pages per block on a bank.
number of blocks on a flash memory bank.
number of pages reclaimed in each garbage
collection.
The threshold on number of dirty pages to
initiate a garbage collection.
Read time for a page.

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

IEEE

Write time for a page.
Erase time for a block.
time to complete a garbage collection.
interval between two garbage collections.

Table 2. Notations for the Analysis of Garbage
Collection
1.

Real-Time Read Bandwidth and Latency
Throughout the operation of the system, one
memory bank is dedicated towards honoring real-time
read requests. Therefore, the latency and the
interarrival time constraints for real-time reads are the
same as they would be on system that had one memory
bank dedicated towards honoring real-time read
requests.
2.

Real-Time Write Bandwidth
Let there be one garbage collection on one memory
bank every TGC seconds and the lower bound on the
number of pages in a block that are reclaimed in a
garbage collection operation be Nreclaim. Thus, in time
period TGC, one garbage collection operation is
performed on each bank of a pair. For a system to be in
steady state, the number of physical writes (as opposed
to logical writes) to a flash memory bank, over a period
of time TGC must be equal to the minimum number of
pages reclaimed from the bank by way of garbage
collection, over the same period of time. If garbage
collection and writes are interleaved, this time period
TGC must be sufficient to permit both the writes and the
garbage collection to happen.
This time period TGC should be large enough to
allow both Nreclaim pages to be garbage collected, and
Nreclaim pages to be written on a memory bank by the
file system. If tGC is the time taken for garbage
collection and twrite is the time taken for a write, then
we get the following equation for TGC.
TGC = 2( N reclaim t write + tGC )

Note that, during a garbage collection, all live
pages must be read and then written to other blocks.
This implies tGC=(Np – Nreclaim)(tread+twrite) + terase and
TGC = 2( N p t write + ( N p − N reclaim )t read + terase )

This time period TGC is also the period after which
the state of all banks in the system is toggled. As the
real-time write operation makes two copies of its data,
and these two physical write operations are serialized
across the two banks in a bank set, in a time period,
TGC, we can have at most Nreclaim logical writes. This
gives the following maximum bandwidth possible.
Bandwidthwrite =

N reclaim
2( N p t write + ( N p − N reclaim )tread + terase )

The write bandwidth is plotted in Figure 2 as a
function of Nreclim. Note that, the maximal write
bandwidth can reach 8000 pages per second on 2 flash
memory banks if there is no read or erase operations.
This maximal bandwidth is reduced as a bank is
devoted to read operations and is lowered again as each
erase operation takes 2 ms and live pages must be
rewritten.
1800
1600

Write bandwidth (pages/sec)

twrite
terase
tGC
TGC

1400
1200
1000
800
600
400
200
0
0

5

10

15

IEEE

25

30

35

Figure 2. Write bandwidth as a function of reclaimed
pages
A low bound on Nreclaim can be found when all dirty
pages are uniformly distributed among the type-1
blocks. Hence, we have
⎤
⎡
N Dirty
N Dirty
N Dirty
N reclaim ≥ ⎢ N p
≥
⎥≥Np
N
N
N
+
N
Nb
+
Live
Dirty ⎥
Live
Dirty
⎢

The above equation implies that when the number
of dirty pages reaches a threshold, a write bandwidth
can be guaranteed. It is important to note here that the
garbage collection operation itself is not nonpreemptible. In other words, file system writes can be
given with a higher priority than the copy operations
performed during garbage collections.

4 Reducing Memory Footprint of Flash
File System
A general approach to reduce the RAM footprint
for log structured file systems on flash has two parts.
One is the data structures stored on the media, while
the second is the data structures stored in the RAM. If
the RAM footprint of reliable flash file systems is to be
reduced, the easiest way to do this is to move the per
inode data structures onto the flash. This implies a
hierarchical storage scheme, where only the top tier of
data is stored in RAM, and the lower layers are loaded
into RAM when required. To reduce read/write
overhead, a caching mechanism shall be required.

4.1 Data Structures on the Media
The key to the proposed storage scheme is a page
table that is stored across physically discontinuous

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

20

No. of page reclaimed in each erase operation

pages of flash memory, and each of the fragments (one
physical page in length) has enough information to
enable a reconstruction of the page table at boot time.
Each of the page table fragments can be considered
to have two parts. First is the data stored in the Out of
Band (OOB) area1 of the flash page that fragment is
stored on, followed by a list of the physical page
addresses that belong to the inode. The data in the
OOB area stores the metadata associated with the page
table fragment.
Page Table Fragment
When a page table fragment is stored on a physical
page on the media, the information is stored in both the
out of band (OOB) area, and the data area associated
with that physical page. The OOB area of a page table
fragment is shown in Table 3.
Field

No. of
bits

Node type

3

inode
number
Fragment
index
Dirty

Remark
To identify that the page in question is a
page table fragment.

20

The index gives the relative position in
the list of page table fragments.
1 To indicate the page is dead or alive.
To indicates whether the file system
Change
1 image of the page table differs from that
on the flash.
Serial
The serial number helps in providing
2
number
crash resistance.
Table 3. Out of Band Area of a Page Table Fragment
16

The serial number bits are used to aid in crash
recoveries, in case there is power loss during a rewrite
operation - after a new version of the page has been
written out, but the earlier version has not been marked
as dirty. The changed bit is used to indicate whether
the state of the page table fragment on the flash differs
from the state in the RAM. This may happen if some
pages pointed to by the fragment are rewritten and the
update to that page table fragment is delayed for
efficiency reasons.
Following the OOB area, there is a list of page
table entries that contain pointers to physical pages
belonging to the inode. As each page table entry is 4
Bytes long, if the page size is 512 bytes, then each of
the page table fragments shall contain 128 pointers to
pages 512 bytes in size. This gives a maximum file
16
size of 2 ×128×512 bytes = 4GB.
Data Page
1

Each page of size 512 bytes on the flash has an OOB area
of size 16 bytes. This is meant to store the error correction
codes for data in that page, and the file system metadata.

As in the page table structure, each data page
maybe divided into data stored in the OOB area, and
data stored in the data area. The OOB area of a data
page is similar to the one for page table fragment,
except that an additional field is included to index the
data page within the fragment. This offset, when combined
with the offset of the page table, gives the actual offset of the
data. Thus each page is a self descriptive structure, that

contains information about what inode and which page
table fragment it belongs to and at what offset. As the
data page contains only a “logical” index for the page
table fragment it belongs to, if a page table fragment is
updated, an update to the data pages that belong to that
table is not required.

4.2 Data Structures in RAM
There are two main data structures required in the
RAM for this scheme. These are as follows.
For each inode on the storage media, a list of pages
that store the page table for that inode is stored in
RAM. Using this scheme, the amount of storage space
required in RAM increases at a much slower rate with
the growth in the size of the files. As each page
fragment stores upto 128 page table entries, this
represents a substantial saving over the traditional log
structured file systems approach, of having a mapping
from logical to physical addresses present in the RAM.
In addition, the file system also implements a page
table fragment cache. Each read and write of a page
table fragment goes to the media via the page table
fragment cache. A Read operation in the proposed
scheme shall require two reads, one of the appropriate
page table fragment, and next the read of the required
data page. Similarly, a write request shall also require
two operations, one to update the physical page, and
the second to update the corresponding page table
fragment.

4.3 File System Operation
The OOB area of the entire flash bank is scanned at
boot time to build up a consistent state of the file
system in RAM. This state consists of a list of page
table fragments for each inode present on the flash, and
for each page table fragment, a list of page table entries
it contains that are no longer valid.
As an example, the operations for a page write
request first locates the physical address of the required
page table fragment and read the page table fragment
(possibly from the page table fragment cache) into the
RAM. Then the page table entry corresponding to the
required page is obtained. This entry contains the
physical address of the data page on the media, if it
exists. The page table fragment on the media is marked
as changed. After writing the new data page on the

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

IEEE

media, the earlier copy of the page is marked as
obsolete if it exists. Finally, the RAM copy of the page
table fragment is updated and put in page table
fragment cache.
The last step in the above algorithm may cause the
flushing of a page table fragment from the cache based
on a LRU scheme. This flushing operation marks the
earlier copy of the page table fragment as obsolete, and
makes a new copy of the fragment on the media. For
real-time read operation, page table fragments may
need to be locked in the cache, such that only one read
operation to the media is required.

4.4 Reliability and Crash Recovery
The serial number bits in the OOB area of the flash
pages – as shown in table 3 - are used to aid in crash
recoveries. When any page is obsolete by new data, or
it is rewritten by the garbage collector, there is a short
duration of time before the earlier copy of the page can
be marked as discarded, when there are two copies of
the fragment on the media with OOB area flags that
indicate that both are valid. While either copy of may
be chosen for a page rewrite during garbage collection
as the data within them is the same, we will have two
pages with the same OOB area, but different data on
them in the case of new data being written. To resolve
this ambiguity, the serial number is incremented
(modulo 2) each time a page is rewritten. If there is a
loss of power before the earlier copy of the page is
marked as discarded, the newer copy of the data can
still be distinguished by using the 2 bit serial number,
as the two bits are sufficient to find the latest copy of
the page. This scheme is similar to the one used in [4].
For reliable update of page fragment, note that the
changed bit in the OOB area of a page table fragment
is used to indicate whether the state of the page table
fragment on the flash differs from the state in the
RAM. The first update to the page table fragment in
RAM causes the changed bit to be set in the copy of
that page table fragment on flash. This update of the
changed bit is not cached. An update to the cache
resident copy of the page table fragment may happen if
some pages pointed to by the fragment are rewritten
and the physical update to that page table fragment on
flash is delayed in the cache for efficiency reasons.

5 Conclusion
In this paper, we aim at solving the major problems
associated with the long blocking delay of garbage
collection over flash memories and the large RAM
footprint of log structured flash file systems. Instead of
incorporating flash memory operation with application
task scheduling, we propose to use replication copies to
enable concurrent read and garbage collection
operations. Hence, application tasks can obtain a

minimal response time on read operations given that
the interarrival time of requests is constrained. For the
mapping from logical inode to physical pages, we
propose a hierarchical structure to effectively utilize
out-of-band data area of each page to provide a reliable
storage scheme.

References
[1] Dan, R., Williams, J., “A TrueFFS and FLite technical
overview of M-Systems' flash file systems,” MSystems, Tech. Rep. 80-SR-002-00-6L Rev. 1.30, Mar
1997. Available: http://www.m-sys.com/tech1.htm.
[2] M-Systems Inc., “IDE/SCSI Solid-State Flash Disk
Product
Family,”
http://www.msys.com/Content/Products/FFDFamily.asp.
[3] Inoue, A., and Wong, D., “NAND Flash Applications
Design
guide,”
http://www.semicon.toshiba.co.jp/
eng/prd/memory/doc/pdf/nand\_applicationguide\_e.pdf
[4] Rosenblum, M., and Ousterhout, J.,”The design and
implementation of a log-structured file system,” In
ACM Transactions on Computer Systems, Vol. 10, No.
1, 1992, pp. 26 - 52.
[5] Woodhouse, D., “JFFS2: The Journaling Flash File
System,” http://sources.redhat.com/jffs2/jffs2.pdf.
[6] Yet Another Flash Filing System (YAFFS).
http://www.aleph1.co.uk/yaffs.
[7] Kawaguchi, A., Nishioka, S., and Motoda, H., “A Flash
Memory Based File System,” In Proceedings of the
Usenix Technical Conference on Unix and Advanced
Computing Systems, 1995.
[8] Chiang, M., Lee, P., Chang, R., “Using data clustering
to improve cleaning performance for flash memory,”
Software-Practice and Experience, vol. 29, no. 3, 1999
[9] Chiang, M., Chang, R., “Cleaning policies in mobile
computers using flash memory,” The Journal of
Systems and Software, vol. 48, no. 3, pp. 213-231, 1999
[10] Chang, L., Kuo, T., and Lo, S., “Real-Time Garbage
Collection for Flash-Memory Storage Systems of RealTime Embedded Systems,” In ACM Trans. on
Embedded Computing Systems, November 2004, pp.
837-863.
[11] Intel Corporation, “Understanding the Flash Translation
Layer (FTL) Specification,” Application Note 648,
1998.
http://developer.intel.com/design/
flcomp/applnots/297816.htm.
[12] Gal, E., and Toledo, S., “Mapping structures for flash
memories: techniques and open problems,” In
Proceedings of the IEEE International Conference on
Software-Science, Technology and Engineering, SwSTE
05, Feb. 22 - 23, 2005.

Proceedings of the Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems and
Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA’06)
0-7695-2560-1/06 $20.00 © 2006

IEEE

2010 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing

Replay Debugging for Multi-threaded Embedded Software
Yann-Hang Lee1, Young Wn Song1, Rohit Girme1, Sagar Zaveri1, Yan Chen2
1

2

Computer Science and Engineering Department
Arizona State University, U.S.A.

The School of Information Science and Technology,
Xiamen University, P.R. China

{yhlee, ywsong, rgirme, szaveri}@asu.edu, yanchen@xmu.edu.cn
Abstract

It may be claimed that the existing debugging tools, such
as GDB [2], TotalView [3], and IDB [4], can be used to
debug multi-threaded programs. Although the tools include
the functions of setting up breakpoint, single stepping and
monitoring at thread level, they don’t ensure the
reproduction of execution behavior that is required in cyclic
debugging. For instance, when a thread reaches a breakpoint
while other threads are running, we may choose the “stop the
world/all threads run” process control model in Intel’s IDB
or the "non-stop" mode in the release of GDB 7.1 in which
only a subset of threads are suspended. However, there is no
support to coordinate the arrivals of external events or the
execution progress of running threads. It is the developers’
responsibility to feed external signals synchronously with
thread execution during the debugging process. This may
become extremely troublesome, if not intractable, when
dealing with real-time control signals, video/audio streaming
data, and internet packets.
Due to the impediment of using cyclic debugging for
multi-threaded embedded software, engineers are forced to
look into the trace data from a failed run and to understand
thread dependency. To pinpoint any specific problems, they
manually create execution sequences to mimic the behavior
of the failed run. This tactic is certainly problematical and
time consuming. To deal with the inherent complexity of
debugging such applications, approaches have been
developed and published in last few years [5]. Out of those
mechanisms, one of the most well known approaches is
deterministic replay or trace based replay [8], [9], [12]-[28].
It is based on recording information that can be played again
in a deterministic way during replay phase. However so far
none of them have talked about the capabilities and
functionalities that replay phase can provide to users.
In this paper, a Replay Debugger based on Lamport
clocks [6] is designed for multi-threaded embedded software.
The debugger considers a multithreaded execution as a
partially ordered sequence of interactions and reproduces the
sequence in cyclic debugging process. Hence, the developers
can inspect the execution steps of each thread as well as the
interactions among threads and external world. They will not
be distracted by context switches or the physical time
instants at which events and interactions occur. As a
consequence, the debugging process can be focused in any
single thread while the execution of other threads is
coordinated precisely to reproduce identical software
behavior.
The Replay Debugger is implemented as a record/replay
framework layered between multithreaded applications and
operating system kernel. The design criteria is to minimize
any instrumentation overhead (i.e., probe effect) in record

The non-deterministic behavior of multi-threaded embedded
software makes cyclic debugging difficult. Even with the same
input data, consecutive runs may result in different executions and
reproducing the same bug is itself a challenge. Despite the fact
that several approaches have been proposed for deterministic
replay, none of them attends to the capabilities and functionalities
that replay can comprise for better debugging. This paper
introduces a practical replay mechanism for multi-threaded
embedded software. The Replay Debugger, based on Lamport
clock, offers a user controlled debugging environment in which the
program execution follows the identical partially ordered
happened-before dependency among threads and IO events as that
of the recorded run. With the order of thread synchronizations
assured, users can focus their debugging effort in the program
behavior of any threads while having a comprehension of threadlevel concurrency. Using a set of benchmark programs, experiment
results of a prototyped implementation show that, in average, the
software based approach incurs a small probe effect of 3.3% in its
record stage.
Keywords – embedded system, replay debugging, multithread, partial order, Lamport clock.

1 Introduction
Debugging has been an imperative step to ensure the
correctness of software programs. However, one of the
survey data, in a 2002 NIST report [1], suggests that an
average bug found in post-product release takes 15.3 hours
to fix. This is quite costly in terms of engineer’s time as
well as the impact to the software products and users.
Often the very first step in debugging is to reproduce
the failed execution. In the cyclic debugging process,
breakpoints are set in the program and the application is rerun. Thus, the cause of the failure can be observed. For
sequential programs the process is pretty straightforward –
as long as each execution is deterministic and repeatable,
the cause of the observed failure can be reproduced and
identified. On the contrary, it is challenging to debug
embedded software which is often structured with multiple
threads to process concurrent IO events and application
tasks. There are significant interactions between these
threads as they need to pass control signals, application
status, intermediate computation results, and to share
resources. This reproduction of the identical execution
behavior under this software structure and with thread
interaction becomes extremely difficult since
 Different ordering of thread interactions and timing
dependency may introduce non-determinism in the
execution of concurrent threads, and
 The system interacts with, and is dependent of, an
external real-time context.
978-0-7695-4322-2/10 $26.00 © 2010 IEEE
DOI 10.1109/EUC.2010.13

15

stage and to maximize observability and analyzability in
replay stage. The record framework is a wrapper for IO and
IPC (inter-process communication) library calls. During
recording, the framework intercepts any IPC and device IO
function calls and traces the returned code, data, and error
code into a log before returning to the application. Then, the
happened-before relationships among the execution of
multiple threads and IO events can be constructed. During
replaying, the concurrent execution can be repeated and
thread execution can be controlled individually following the
partial order of thread interactions. Moreover, to make the
Replay Debugger more practical, the proposed replay
mechanism has been incorporated into GDB with an
extended debugging interface for thread control and
observation. It provides a GUI to user that shows the threads
and various events that happened during the record phase.
The users are allowed to use all GDB commands such as
single stepping, breakpoints, tracepoints, and status views.
They can also step over the individual thread interaction
events and control the execution order of concurrent threads.
There are three contributions of our Replay Debugger:
1. Both IPC and IO events are considered during the
record and replay.
2. It provides two different debugging modes, named
Replay Minima and Replay Maxima, for users to
control over a debugging process with the
deterministic order of execution.
3. It is practical and has been incorporated into GDB
with an extended debugging interface for thread
control and observation.
The rest of the paper is organized as follows. In Section 2,
a concise survey of related works is described. Section 3
presents the design of our Replay Debugger for multithreaded embedded software. The implementation details are
explained in Section 4. In Section 5, performance evaluation
is presented, followed by a conclusion and the future work.

On software based replay mechanisms, Carver et al.
provided an approach to reproduce the rendezvous of Ada
tasks [13]. The approach can only replay concurrent program
execution events like rendezvous, but not real-time specific
events like scheduled preemptions, asynchronous interrupts
or mutual exclusion operations [14]. LeBlanc and MellorCrummey proposed a method for recording information
during run-time and using this information to reproduce the
exact behavior of the execution off-line [15]. This method,
called Instant Replay, allows cyclic debugging techniques to
be used for non-deterministic systems. It is targeted at coarse
grained operations and traces all these operations. It does not
use any technique to either reduce the size of the trace files
or limit the perturbation introduced. Replay mechanism for
message passing systems is discussed in ROLT [16]. For
shared memory computers, Netzer [17] introduced an
optimization technique based on vector clocks. As the order
of all memory accesses is traced, both synchronization and
data races will be replayed. However, no implementation
was ever proposed (of course, the overhead would be huge if
all memory accesses are traced). A more elaborate discussion
on debugging parallel programs can be found in [18].
Montesinos et al. introduced a software-hardware
interface for deterministic replay of multiprocessors [19].
This approach, called Capo, separates the responsibilities of
the hardware and software components of the replay system.
During the recording phase, the software logs all replay
sphere inputs into a Sphere Input Log, while the hardware
records the interleaving of the replay sphere threads into an
Interleaving Log. During the replay phase, the hardware
enforces the execution interleaving encoded in the
Interleaving Log, while the software provides the entries in
the Sphere Input Log back into the replay sphere and
squashes all outputs from the replay sphere. The approach
combines the performance of hardware-only schemes with
the flexibility of the software-only ones.
Park et al. [20] proposed a probabilistic replay via
execution sketching to help reproduce concurrency bugs on
multi-processors. This approach recorded only partial
execution information during the production run, and relied
on an intelligent replayer during diagnosis time to
systematically explore the unrecorded non-deterministic
space and reproduce the bug. With only partial information,
the approach has low recording overhead, but it may require
more than one coordinated replay run to reproduce a bug.
Moreover, as the approach exhaustively search the
unrecorded non-determinism space to find a bug-producing
combination, it might be difficult to reproduce an occurred
bug within an acceptable time limit.
The concept of deterministic replay was applied for
debugging real-time systems. In [21], a general-purpose
processor is dedicated to monitoring on each multiprocessor.
The monitor can observe the target processors via shared
memory. The target systems software is instrumented with
monitoring routine, by means of modifying system service
calls and interrupt service routines. Their basic assumption
about having a distributed system consisting of
multiprocessor nodes makes their approach less general.
A similar technique of deterministic replay to debug
distributed real-time systems is discussed in [22]. The
method starts by identifying and recording significant events,

2 Related Works
With respect to related work in the field of replay
debugging of concurrent and real-time systems, the main
issue with record/replay approaches is what information
should be traced during the initial record phase? On the one
hand, enough information about the execution has to be
generated for a faithful re-execution of the program. On the
other hand, the amount of information traced and the
computational overhead should be limited as much as
possible in time and in space in order to avoid any probe
effect [7].
On general topic of replay, some researches have been
relying on special hardware to reproduce interrupts and task
switches [8][9]. The software instruction counter approach
[10] records the backward branches and has been used to
identify the exact location at which an event of interest (e.g.
interrupt) occurs. The approach was adopted for log-based
recovery to force the replay of asynchronous events at the
same execution points [11]. In addition, Hill et al. present
their Flight Data Recorder (FDR) [12] as an intrusive
hardware that facilitates debugging of data races in
multiprocessor systems.

16

recorded information is then used to restore the processor
state during reverse execution.
In conclusion, although there are many related works on
the record/replay of concurrent and real-time systems, there
still has no practical and convenient mechanism or tool for
users to debug the multi-threaded embedded software. In
this paper, we propose a practical Replay Debugger which
offers a user controlled debugging environment. With the
Replay Debugger, users can focus their debugging effort in
the program behavior of any threads while having a
comprehension of thread-level concurrency.

together with the time at which they occur, in the execution
of the sequential program. To reproduce the run-time
behavior during debugging, all inputs and outputs are
replaced with recorded values. All transfers of control,
accesses to critical regions, releases of higher priority tasks,
and preemptions by interrupts are dictated by the recorded
timestamps. A similar replay debugging using Time
machines is discussed in [23]. As the authors claim, it is the
first method for deterministic replay of single tasking and
multitasking real-time systems using standard off-the shelf
debuggers and real-time operating systems and without
instruction counters support for replay thus making it
compiler and RTOS independent.
Probably the most similar approach to the replay
mechanism of the proposed debugger is RecPlay [24][25]
which replays the synchronization operations, while
detecting data races. It is also based on Lamport’s happensbefore relation [6]. By checking the ordering of all events
and monitoring all memory accesses, data races can be
detected for one particular program execution. Some prior
work based on Lamport clock, i.e., ordering based replay
method for shared memory programs is discussed in [26].
This mechanism lists the advantages of using Lamport clock
to generate partial ordering. It produces small trace files and
is less intrusive. Moreover, the method allows one to use a
simple compression scheme [27] which can further reduce
the trace files. Different from the RecPlay, our Replay
Debugger focuses on debugging techniques for multithreaded embedded software based on the record/replay
framework. It is integrated with GDB and supplies
debugging functionalities for users to control over a
debugging process with the deterministic order of execution.
Another approach based on Lamport clock is taken by a
race detector Eraser [28]. It goes slightly beyond the works
based on the happened-before relation. Eraser checks that a
locking discipline is used to access shared variables: for each
variable it keeps a list of locks that were hold while
accessing the variable. Each time a variable is accessed, the
list attached to the variable is intersected with the list of
locks currently held and the intersection is attached to the
variable. If this list becomes empty, the locking discipline is
violated, meaning that a data race occurred. The most
important problem with Eraser is that its practical
applicability is limited in that it can only process mutex
synchronization operations and in that the tool fails when
other synchronization primitives are built on top of these
lock operations.
The record/replay approaches have become acceptable for
debugging in practice. Recently, VMware Workstation 6.5
includes the feature of replay debugging for C/C++
developers using Microsoft Visual Studio. The tool records
program execution via VMWare’s virtualization layer and
the replay is guaranteed to have instruction-by-instruction
identical behavior. It also includes a feature of simulating
reverse execution of the program, making it easier to pin
point the origin of a bug.
GDB record patch, available at [29], enables a reversible
debugging function in GDB. By disassembling the
instruction that will be executed, the record patch saves the
memory and register content before they are modified. The

3 Design of the Replay Debugger
To facilitate execution replay, we will need to record the
execution paths and input events of the original run. Hence,
instrumentation must take place in a record phase and must
have a minimal probe effect. To replay single threaded
programs, we will need to supply identical input values read
from device drivers and system calls (e.g. timer function).
The execution can then be reproduced given the sequential
nature of the programs. On the other hand, for multithreaded programs, threads can interact with each other by
invoking inter-process communication operations. To
reproduce the execution, these invocations must take place
in the identical order as the original run. In this paper, we
assume these interaction and input events are invoked via
system calls, including inter-process communication
primitives and device drivers. Shared resources are
protected by synchronization primitives such as locks or
semaphores and there is no data race condition. If a shared
resource is not properly protected and causes a race
condition during record phase, deterministic replaying the
program will reveal the cause of errors.
3.1 Definitions
As mentioned above, for a reproducible replay, we need
to record thread interaction and input read events (or simply
“thread interaction events”) during the original thread
execution. In the following, several related definitions are
given.
Definition 1. A thread interaction event is a 4-tuple:
E = <type, Tid, Oid, t>
(1)
where type is an event type of E, e.g., read(), sem_wait(),
sem_post(), etc.; Tid is the thread invoking event E; Oid,
identifies the synchronization and communication object
which E operates on, e.g., semaphore, message queue,
mutex lock, IO file, etc.; and t is the Lamport clock
timestamp [6] of E.
Definition 2. The thread interaction events operated on the
same synchronization and communication object are
collected into an event set. The set is called object event set
and is denoted as OESid for object Oid.
Definition 3. The thread interaction events in the same
thread are collected into an event set. The set is called
thread event set and is denoted as TESid for thread Tid.
We use the happened-before relation [6] to maintain a
partial order between thread interaction events. The
happened-before relation between events a and b is denoted
by “  ”, i.e., if a happened before b, it is represented as

17

a  b. The Lamport clock [6] is used to identify events in
reconstructing a partial order graph in the replay phase. A
Lamport clock is maintained for each Oid or Tid and is equal
to the Lamport clock of the most recent event happened on
Oid or invoked by Tid. LC(a) is a function that returns a
Lamport clock timestamp taking a parameter as Ei, Oid or Tid.
In addition to the happened-before relation we define an
immediate happened-before relation, denoted by “  ”, for
two successive thread interaction events as follows:
Definition 4. For two thread interaction events Eb and Ea,
there is an immediate happened-before relation Eb  Ea, if:
1) Eb, Ea  Setid where Setid is OESid or TESid, and
2)  Ei  Setid (i ≠ b) that satisfies
LC(Ea) – LC(Ei) > LC(Ea) – LC(Eb) > 0

1: CalculateE()
2: For each event Ei =<type, Tid, Oid, t> in V,
3:
Find Ei1 such that Ei1  TEStid and Ei1  Ei
4:
Find Ei2 such that Ei2  OESoid and Ei2  Ei
5:
L = L  (Ei1, Ei)
6:
L = L  (Ei2, Ei)
Algorithm of Edge Calculation

Figure 1

1: int shared = 1;
2: void thread2(int *arg){
3:
sem_wait(&sem);
4:
shared = shared + (*arg);
5:
sem_post(&sem);
6: }
7: void thread3(int *arg){
8:
sem_wait(&sem);
9:
shared = shared * (*arg);
10: sem_post(&sem);
11: }
12: int main(void){
13:
pthread_t t2, t3;
14:
char buf[10];
15:
int arg;
16:
17:
sem_init(&sem, 0, 0);
18:
read(0, buf, 10);
19:
arg = atoi(buf);
20:
21:
pthread_create(&t2, NULL, (void *)thread2, &arg);
22:
pthread_create(&t3, NULL, (void *)thread3, &arg);
23:
sem_post(&sem);
24:
25:
pthread_join(t2, NULL);
26:
pthread_join(t3, NULL);
27:
return 0;
28: }

(2)

According to the expression (2), Eb is the last event
before Ea in the Setid.
In the subsequence discussion, a timestamp for a thread
interaction event is denoted as an instant of Lamport clock
when the thread interaction event occurs and an event is
denoted as a thread interaction event.
3.2 Record
In the record phase of Replay Debugger, the happenedbefore relations between events are captured by chains of
immediate happened-before relations. The program
execution is reproduced by following the execution orders
of events represented in the immediate happened-before
relations during the replay.
A wrapper function is provided for each event type, and
recoding of an event Ei is invoked a thread Tid and operates
on an object Oid, the computation of their timestampts
follows the Lamport clock algorithm [6] as
LC(Ei) = max(LC(Ttid), LC(Ooid)) + 1
LC(Ttid) = LC(Ei)
(3)
LC(Ooid) = LC(Ei)
Then, the 4-tuple <type, Tid, Oid, t>, as the unique
representation of Ei, is saved into a log file. If Ei is an input
read event, the input data is saved too.
Note that LC(Ooid) is initialized to 0 when the object is
created, e.g., open() for files, sem_open() for semaphores,
and mq_open() for message queues, in Linux. Also, when a
new thread is created, we add a thread creation event Ec as
the first event to the new thread and the timestamp of the
new thread is initialized to LC(Ec). If Ei is a thread exit
event of a joinable thread, Ei is added as the last event in Ttid
and LC(Tid) is kept until the join action is invoked.
After the recording stage, a partial ordering of events is
constructed using the 4-tuple event information collected.
The partial ordering is represented by a graph G = (V, L),
where V is a set of events recorded, L is a set of edges, and
each l=(E1, E2)  L denotes the immediate happened-before
relation between E1 and E2. The algorithm to calculate L is
shown in Figure 1. Note that the steps 3 and 4 can be done
easily once the events are sorted according to their
timestamps.
Figure 2 shows an example program, and the program
has few possible execution paths. Figure 3 shows one
possible program execution with a partial order graph
calculated from timestamps.

Figure 2
main()

LC: 1
LC: 2

thread2

An example program
thread3

SI1
RD1
TC

LC: 3
LC: 4
LC: 5

TC
SP1

TCX
SW1

LC: 6

SP1

LC: 7

EX

LC: 8
LC: 9

TCX

SW1
SP1

JN
EX

LC: 10
JN

LC: 11

Figure 3

LC: Lamport Clock timestamp
RD: read
SI : semaphore init
TC: thread creating
TCX: thread created
SP: semaphore post
SW: semaphore wait
JN: thread join
EX: thread exit
* Subscript represents identity of
corresponding event

A partial order graph for one possible execution path

3.3 Replay
The deterministic replay is achieved by following an
order represented in the partial order graph G. To execute an
event, we should execute all events that are happened before
it. This is done by following edges of the partial order graph
backward and executing all the events happened before the
target event.
For an event Ei  TEStid, let ProceedToEvent(Ei) be a
schedule algorithm to run the thread Tid from its current

18

program counter to the completion of an event Ei. The
algorithm can be realized by recursive calls as depicted in
Figure 4, where the Lamport clock for threads and events
are managed following the same steps in Eq. (3).

implemented by differentiating event statements that we
record from other normal statements. The corresponding
replay algorithm of the current debugging mode is
performed at the event statements. The ProceedToEvent
algorithm, shown in Figure 4, implements the Repaly
Minima mode. Other program statements that we do not
record are preceded just as normal debugging statements
without applying the algorithm.
The external IO data saved during record phase is also
replayed. This guarantees that the program execution during
replaying receives the same input data as in the recording
stage. Note that in user’s point of view all the details about
deterministic replay are hided and he/she can concentrate on
debugging without worrying about switching threads and
feeding IO data.

1: ProceedToEvent(Ei)
2: // assume that Ei  TEStid, i.e. to be invoked by Tid
3:
4: while LC(Tid)<LC(Ei) do
5:
run thread Tid until the next event Ek
6:
find the event Ej where Ej  Ek
7:
and EiTEStid
8:
ProceedToEvent(Ej)
9:
execute Ek
10: return
Figure 4

Algorithm of event execution

4 Implemenation

Given a partially ordered happened-before graph, there
are many different execution orders. In addition to user
controlled execution ordering, Replay Debugger provides
two debugging modes: Replay Minima and Replay Maxima.
In Replay Minima mode, the events of the selected thread
will proceed first. The events of other threads will be carried
on only if they are happened before the executing event of
the selected thread. On the other hand, for Replay Maxima,
the events of all other thread can proceed first until there is
happened-before dependency upon the current execution of
the selected thread. In this case, the events of the selected
thread will be advanced in order. The two replay modes are
useful to illustrate the dependency between threads. Let’s
assume a breakpoint BRi is set along a thread Ti ’s execution
path. When the thread execution is suspended after reaching
the breakpoint under the replay modes:
1) Replay Minima: Only a limited subset of events in all
other threads is executed. This subset represents the
minimal amount of execution that must be done for Ti to
reach BRi. Any causes that may result in an incorrect
state status at BRi would have been triggered by the
execution of the events in the subset.
2) Replay Maxima: A maximal set of events has been
carried out as thread Ti is suspended at BRi. This set
represents how far the impact of Ti’s current execution
can stretch to if there is no further execution in Ti. So, if
thread Ti’s execution is bug-free up to BRi and no bugs in
other threads, then the execution of this maximal set of
events should be error-free.
Thread 1

Thread 2

Thread 3

Thread 1

Thread 2

E4
LC: 1

Multi-Threads Applications

Record/Replay
Library

IPC

IO

Figure 6

Thread 1

E1

Thread 2

E6

E2

LC: 3

E5

LC: 5

E6

E1
E7

E3
E5

E4

LC: 5
E8

LC: 6

E8

LC: 6
Replay Minima at E1:
E4, E3, E2, E1 are executed
in order

Figure 5

GDB

Scheduler

The architecture of Replay Debugger

Thread 3
E1

E2

LC: 4
E7

Linux Kernel

Replay

E3
LC: 2

LC: 4

Log
Files

4.1 Replay Scheduling Using GDB

Thread 3

LC: 1

E2

Record

E4

E3
LC: 2
LC: 3

The current implementation of Replay Debugger is built
on POSIX APIs in Linux environment. The architecture of
Replay Debugger is shown in Figure 6. It includes a library
of wrapper functions. The actual system calls are substituted
with the wrapper functions during compile time. In the
record phase, the wrapper functions log information needed
in building a partial order graph including Lamport clocks
and save external input contents into files. The wrapper
functions in the replay phase read external input contents
from the files saved in the record phase. In the current
implementation, it is the GDB thread module that performs
a deterministic replay using the partial order graph built in
the record phase. A selected subset of system calls are
implemented at this stage. Further extension will include
mmap device, signal handling, time function, etc.

Replay Maxima at E1:
E4, E3, E2, E6, E5, E1 are executed
in order

Figure 7








Thread 1 stops at E4 (bkpt)
Switch to thread 2 & resume (GDB)
Thread 2 stops at E2
Switch to thread 3 & resume (GDB)
Thread 3 stops at E1 & Single step on E1
Switch to thread 2 & single step on E2 &
resume
 Thread 2 single steps on E3
 Switch to thread 1
 Single step on E4

A scheduling example using GDB

Replay mode example

The GDB thread module is modified to schedule thread
executions according recorded partial order. On start of a
GDB session, Linux is prevented from scheduling threads

Two different debugging scenarios by Replay Minima
and Replay Maxima are shown in Figure 5. The replay is

19

clock. Thread Information displays a table similar to the
Replay Graph in the Eclipse Console. This table shows the
different events for corresponding threads which have not
yet been executed. To call the commands added to CDT, we
contributed menu/buttons to the workbench using
commands in Eclipse, which is shown in Figure 9. A
command in Eclipse is a declarative description of a
component. The behavior of a command is defined via
handlers. Where and how should the command be included
in the UI is defined using a location Uniform Resource
Identifier.

using the GDB command “set scheduler-locking on” [33]
and a breakpoint is set at each event for checking happenedbefore relations. The GDB command “thread thread-num”
[33] is used for switching to a thread numbered “threadnum”. When GDB stops at an event E1, threads are
scheduled as follows,
 If there is no immediate happened-before event before
E1, then single step on E1 and resume;
 Else find E2 such that E2  E1, switch to the thread
that E2 belongs to and resume the thread.
Figure 7 shows an example of thread scheduling.
4.2 GUI Support
We have also implemented a graphical environment for
our Replay Debugger in the form of pluggable components
called Eclipse [30] plug-ins. The current implementation is
based on Eclipse for Linux. Eclipse plug-ins are components
that provide certain types of services within the context of
the Eclipse workbench. They add to the C/C++
Development Tools (CDT) that provides a fully functional
C and C++ Integrated Development Environment (IDE) for
the Eclipse platform.
Support for our Replay Debugger functionality in
Eclipse can be categorized into three parts.
4.2.1 Support for the Record
To enable recording, applications are built using the
record library so that we can record its behavior. Our plugin provides a new project type “RecordReplay” which is
shown in Figure 8. It has two new configurations “Record”
and “Replay”. The user can avail of the Record and Replay
services in GDB by selecting this project type.
Since our plug-in is a part of Eclipse, the new project
wizard includes the project-type and configuration
definitions from our manifest files along with other manifest
files to populate the list of choices presented to the user.
Contents of the build property page for a project are created
by examining the tool-chains, tools, option categories, and
options defined for the current configuration. Therefore, if
the user wants to use the Replay Debugger functionality, the
“RecordReplay” type of project should be created from the
list of project types. After selecting the project type, the user
then selects “Record” and “Replay” configurations for his
new project.
4.2.2 Support for the Replay

Figure 8

A new project type “RecordReplay” in the plug-in

Figure 9

Menu of replay commands

4.2.3 Support for displaying threads and events
Displaying the different threads and events of an
application being debugged gives the user a clear picture of
what is going on. We have used Swing and Abstract
Window Toolkit provided by Java for our purposes.

In Eclipse framework, the C/C++ Debugger Interface
(CDI) was created by Eclipse/CDT developers, so that CDT
can access external debuggers. Similarly, the Machine
Interface (MI) [30] was created by GDB developers, so that
external applications/software can access the GDB.
For our purpose, we added MI commands to the
GDB/MI interface which are used by our code in CDT to
call any particular replay commands. So far replay
commands of Start Replay, Thread Information, Set Minima
Mode, Set Maxima Mode and Set LC Breakpoint have been
added to the CDT. Start Replay initializes the replay module
in GDB; Set Minima Mode sets minima mode for execution
of events in GDB; similarly Set Maxima Mode configures
the maxima mode for event execution in GDB. Set LC
Breakpoint initiates a breakpoint at the thread clock entered
by user. Thus, debugging can follow the order of Lamport

Figure 10 Displaying threads and events

20

TABLE II Static data of benchmark programs

The memory log obtained after recording the application
contains the information about the threads and events of the
program. Information from this log is used to draw the
display.
Figure 10 shows an example of the diagram displaying
threads and events. The display provides us with
information like number of threads, events occurred along
with their types (thread created, semaphore/mutex taken or
released etc), thread clock at which the event occurred and
so on. An additional functionality provided is each event on
the display maps to a particular line in the application
program.

Program
multi_con_pro
multi_send_rev_1
multi_send_rev_2
sem_philosopher
sem_readerwriter
sem_sleepingbarber
read_send_rev
pthread_mutex_unlock

Thread
Number
500
80
200
5
200
101
4
60

Total Event
Number
200200
3242
9486
610
1002
1208
12011
3720

TABLE III The overhead during record for the benchmark programs

5 Performace Evaluation
Program

The performance of Replay Debugger is measured on a
Linux 2.6.28 system with Intel(R) Core 2 Duo CPU T6600
@ 2.20GHz and 3GB RAM. We use PAPI Library [31] to
measure the instruction counts of actually system calls and
the wrapper calls. The results are shown in TABLE I.
“mq_send” and “mq_receive” events are measured with a
message length of 40 bytes; “read” and “fread” events are
measured with reading 1024 bytes data. The instruction
counts of wrapper calls are larger than the original calls, as
each wrapper call should do additional work: update the
Lamport clock, write log in current memory buffer, and
switch buffer and signal the “dump thread” if current buffer
is full. Moreover, the wrapper calls for “read” and “fread”
events should also create log files to save the data read, so
the instruction counts of these two wrapper calls are larger
than other wrapper functions.

multi_con_pro
multi_send_rev_1
multi_send_rev_2
sem_philosopher
sem_readerwriter
sem_sleepingbarber
read_send_rev
pthread_mutex_unlock
Average

Original
18
16
81
82
40
42
949
94
575

Record
Runtime (s)
81.460
2.831
6.392
52.414
2.008
5.503
9.939
30.142

Overhead
8.405%
9.264%
3.414%
1.162%
0.050%
0.019%
4.008%
0.040%
3.295%

6 Conclusion
In this paper we have presented a practical approach to
replay-based debugger for multithreaded embedded
software. The proposed debugger uses the Lamport’s
happened-before relation to establish a partial order between
thread interaction and IO events. The partial order is then
applied to guide the thread execution during debugging. The
enhanced replay functionalities like Replay Minima and
Replay Maxima on events provide simple control over a
debugging process with the deterministic order of execution.
Moreover, it has been successfully integrated with GDB,
and we have also wrapped our tools for the debugger in the
form of pluggable components in Eclipse.
As a further step, we would like to improve our record
and replay library to support most kinds of thread
interaction events, asynchronous transfer (e.g. signal and
watchdog timer), and mmap devices. In addition, we would
like to enhance the functions of our GUI and the Eclipse
plug-in based on user feedbacks. The current
implementation of the proposed tool is available at
http://rts.lab.asu.edu/ReplayDebugger.

TABLE I Instruction counts of actually system calls and the wrapper calls
System Call
sem_wait
sem_post
mq_send (40 bytes)
mq_receive (40 bytes)
pthread_mutex_lock
pthread_mutex_unlock
pthread_create
read (1024 bytes)
fread (1024 bytes)

Normal
Runtime (s)
75.144
2.591
6.181
51.812
2.007
5.502
9.556
30.130

Wrapper
514
515
537
535
574
780
1688
3940
4446

TABLE II and TABLE III give an idea of the overhead
caused by Replay Debugger during record phase for
programs from the LTP benchmark suite [32]. TABLE II
shows static information of benchmark programs, including
the number of threads and the total number of interaction
events during executions with the same input data. TABLE
III shows that the overhead during the record phase for the
benchmark programs ranges from 0.019% to 9.264% with
the average of 3.295%. The reason of the low overhead is
that Replay Debugger effectively succeeds in greatly
reducing the number of synchronization operations that has
to be stored on the log file. Apparently, optimizations can
also be done to further reduce the overhead of wrapper
function and logging operation. For instance, the code to
prevent preemption during the wrapper function can be
eliminated if the wrapper is moved to the kernel.

7 Acknowledgments
This work was supported partially by a grant from the
NSF Industry/University Cooperative Research Center
(I/UCRC) on Embedded Systems at Arizona State
University.

References
NIST report, “The economic impacts of inadequate
infrastructure for software testing,” Technical report, U.S.
Department of Commerce, May 2002.
[2] R. Stallman, R. Pesch, S. Shebs, et al., “Debugging with GDB
(ninth edition, for GDB version 7.1),” Free Software
Foundation, 2010.
[1]

21

[3]
[4]
[5]
[6]

[7]
[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

TotalView Technologies, “TotalView graphic user interface
reference guide: version 8.8,” 2010.
R. M. Albrecht, “White Paper: Intel Debugger for Linux*,”
Intel Corporation, 2009.
J. Engblom, “Debugging real-time multiprocessor systems,”
Embedded Systems Conference, Silicon Valley, 2007.
L. Lamport, “Time, clocks, and the ordering of events in a
distributed system,” Communications of the ACM, Vol.21(7),
pp: 558-565, 1978.
J. Gait, “A probe effect in concurrent programs,” Software
Practice and Experience, Vol.16(3), pp: 225-233, 1986.
H.-Y. Chen, J.P.P Tsai, K-Y. Fang, and D. Bi, “A
noninterference monitoring and replay mechanism for realtime software testing and debugging,” IEEE Transactions on
Software Engineering, Vol.16(8), pp: 897-916, 1990.
D. R. Hower, P. Montesinos, L. Ceze, M. D. Hill, and J.
Torrellas, “Two hardware-based approaches for deterministic
multiprocessor replay,” Commnications of the ACM,
Vol.52(6), pp: 93-100, 2009.
J. Mellor-Crummey and T. LeBlanc, “A software instruction
counter,” Proceedings of the Third International Conference
on Architectural Support for Programming Languages and
Operating Systems, pp:78-86, Boston, Massachusetts, Unite
State, April, 1989.
H. Slye and E.N. Elnozahy, “Supporting nondeterministic
execution in fault-tolerant systems,” Proceeding of
International Symposium on Fault-Tolerant Computing, pp:
250 - 259, Sendai, Japan, June, 1996.
M. D. Hill, M. Xu, and R. Bodik. “A ‘flight data recorder’ for
enabling full-system multiprocessor deterministic replay,”
Proceedings of the 30th Annual International Symposium on
Computer Architecture, pp: 122-133, San Diego, California,
Unite States, June, 2003.
R. H. Carver, K. C Tai, and E. E. Obaid, “Debugging
concurrent ada programs by deterministic execution,” IEEE
Transactions on Software Engineering. Vol.17(1), pp: 45-63,
1991.
F. Zambonelli and R. Netzer, “An efficient logging algorithm
for incremental replay of message-passing applications,”
Proceedings of the 13th International and 10th Symposium on
Parallel and Distributed Processing, pp: 392 - 398, San Juan,
Puerto Rico, Unite States, April, 1999.
T. J. LeBlanc and J. M. Mellor-Crummey, “Debugging
parallel programs with instant replay,” IEEE Transactions on
Computers, Vol.36(4), pp: 471-482, 1987.
D. Kranzlmller and M. Ronsse, “Rolt(mp) - replay of Lamport
timestamps for message passing systems,” Proceedings of the
Sixth Euromicro Workshop on Parallel and Distributed
Processing, pp: 87-93, Madrid, Spain, January, 1998.
R. H. Netzer, “Optimal tracing and replay for debugging
shared memory parallel programs,” Proceedings of the
ACM/ONR Workshop on Parallel and Distributed Debugging,
pp: 1-11, San Diego, California, Unite States, May, 1993.

[18] J. Huselius, “Debugging parallel systems: a state of the art

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]
[30]
[31]
[32]
[33]

22

report,” In MRTC Technical Report 63, Mlardalen University,
Department of Computer Science and Engineering,
September, 2002.
P. Montesinos, M. Hicks, S. T. King, and J. Torrellas, “Capo:
a software-hardware interface for practical deterministic
multiprocessor replay”, Proceedings of Fourteenth
International Conference on Architectural Support for
Programming Languages and Operating Systems, pp: 73-84,
Washington, DC, Unite States, March, 2009.
S. Park, Y. Zhou, W. Xiong, Z. Yin, et al., “PRES:
probabilistic replay with execution sketching on
multiprocessors”, Proceedings of SOSP’09, pp: 177-191, Big
Sky, Montana, Unite States, October, 2009.
P. Dodd and C. V. Ravishankar, “Monitoring and debugging
distributed real-time programs,” Software Practice and
Experience, Vol.22(10), pp: 863-877, 1992.
H. Hansson and Henrik Thane, “Using deterministic replay
for debugging of distributed real-time systems,” Proceedings
of 12th Euromicro Conference on Real-Time Systems, pp:
265-272, Stockholm, Sweden, June, 2000.
J. Huselius, A. Pettersson, H. Thane, and D. Sundmark,
“Replay debugging of real-time systems using time
machines,” Proceedings of the 17th International Symposium
on Parallel and Distributed Processing, Nice, France, April,
2003.
K. D. Bosschere, M. Ronsse, and M. Christiaens, “Debugging
shared memory parallel programs using record/replay,” ACM
Future Generation Computer Systems, Vol.19(5), pp: 679687, 2003.
M. Ronsse, M. Christiaens, and K.D. Bosschere, “Cyclic
debugging using execution replay”, Proceedings of the
International Conference on Computational Science, pp: 851860, San Francisco, California, Unite States, May, 2001.
K. Audenaert, L. Levrouw, and J. Van Campenhout, “A new
trace and replay system for shared memory programs based
on Lamport clocks,” Proceedings of the Second Euromicro
Workshop on Parallel and Distributed Processing, pp: 471478, Malaga, Spain, January, 1994.
L. Levrouw, M. Ronsse, and K. Bastiaens, “Efficient coding
of execution-traces of parallel programs,” Proceedings of the
ProRISC / IEEE Benelux Workshop on Circuits, Systems and
Signal Processing, pp: 251-258, Utrecht, Holland, March,
1995.
G. Nelson, P. Sobalvarro, T. Anderson, S. Savage, and M.
Burrows, “Eraser: a dynamic data race detector for
multithreaded programs,” ACM Trans. Comput. Syst. Vol.
15(4), pp: 391-411, 1997.
GDB record patch, http://sourceforge.net/projects/record/.
Eclipse, http://www.eclipse.org/.
PAPI, http://icl.cs.utk.edu/papi/.
Linux Test Project, http://ltp.sourceforge.net/.
GDB Manual,
http://sourceware.org/gdb/current/onlinedocs/gdb/.

Optimal Resource Control in Periodic Real-Time Environments

Kang G. Shin*, C. M. Krishna**, and Yann-Hang Lee***

'Real-Time Computing Laboratory
Dept. of EECS
The University of Michigan
A n n Arbor, MI 45 109

**Dept. of ECE
University of Massachusetts
Amherst, MA 01003

aches its peak from 9 to 5 and diminishes at night. This kind of
periodic load distribution occurs also in other real-time systems,
for example those used in air traffic control, as also in telecommunication and computer networks. To achieve the greatest re-

Abstract
Three factors determine the optimum configuration of a
multiprocessor at any epoch: the workload, the reward structure,
and the state of the computer system. We present an algorithm
for the optimal (more realistically, quasi-optimal) configuration
of such systems used in real-time applications with periodic reward rates and workloads. ms algorithm is based on Markov
decision theory.

wards from a given computer system, it is important to
reconfigure it in such a way as to optimally match (a) the current
state of the hardware (e.g., the number of processors which are
up) and (b) the current demands of the operating environment.

Reconfiguration in most real-time systems is currently limited to reacting to individual component failure. We suggest here
a change in the workload or the reward structure be as DOWerfd a motivation for reconfiguration as component failure. Such
changes occur naturally over the course of operation: we present
as an example an on-line transaction processing system with a
workload and reward structure that has a period of a day.

Two parameters characterize the operating environment:
the reward srnrcitrre, and the imposed load. The former needs
some elaboration. The operating environment imposes a vu/m
on each of the many services it receives from the computer.
Putting it more formally, there is a reward (which could be negative) which accrues from each job execution, and this reward is a
function of the needs of the application. When the imposed load
changes, such a change can be quantified by the change in the
reward structure or failure rates that this causes. For example, the
reward accruing from a transaction-handling machine in a bank
is different at peak banking hours than it is at, say, midnight. Or,
a penalty may be introduced if job processing delays become excessive under heavy loading. Naturally, it would be useful to be
able to optimally configure or service the system as a function of
the prevailing application needs.

1.0 Introduction
Because multiprocessor systems can function (albeit sometimes in a degraded condition) in a very wide variety of configurations and environments, managing such systems is much more
difficult than managing a conventional uniprocessor machine. In
this paper, we deal with the problem of optimally controlling the
resources that make up such systems.

Resocrce control decisions also have to be made when the
computer changes due to component failures. When, for instance,
is it appropriate to summon a repairman? Which (degraded)
configuration should the system switch to, prior to repair? Or,
consider the problem of allocating channel bandwidth optimally
to members of a set of token-ring networks. Each network has a
set of users each of which pays a certain amount for a given
quality of service (e.g. waiting time). Additionally, the system
response time is a function of the load imposed on the system.
How does One
bandwidth amongst the various networks
so as to maximize reward (i.e., customer payment)? When systerns are simple, such decisions can be made on the basis of intuition alone. When they get complex, unsupported intuition is

In real-time applications, both component failure and
workload variation can lead to significant changes in the ability
of the computer to meet the demands of the operating environment. It is quite easy to identify several cases where job arrival
and workload follow certain periodic laws. For instance, the
loading of on-line transaction processing systems typically reThe work was performed when Yann-Hang Lee was with IBM T. J. Watson
Research Center, Yorktown Heights. NY. This work was supported in part by
NASA under grant NAG-1-296. and by the National Science Foundation under
grant NSFDMC-8504971. Any opinions. findings. and conclusions or recommendations expressed in this publication are those of the authors, and do not
necessarily express the view of the funding agencies.

33

CH2618-7/88/0000/0033$01.000 1988 IEEE

***Computer Science Dept.
University of Florida
Gainesville, FL 3261 1

theory. In Section 4 we provide a numerical example, and the
paper concludes with Section 5 .

insufficient, and must be supplemented by algorithms enabling a
more precise control. This is especially true when the performance of the system is an intricate function of parameters which
may act at cross purposes to one another.

2.0

The measure of performance used here is based on Meyer’s
perforntability [l]. It is one of the most powerful applicationsensitive metrics available today, and incorporates both the traditional measures of performance (e.g. throughput) and of
reliability. Performability formally accounts for the requirements
of the application by defining ucconrplishnient levels. The vector
of probabilities of meeting the accomplishment levels is
performability. Suppose we identify a reword with each accomplishment level. Then, the expected reward rate can be obtained
from the perfomability of the system. This reward rate is also
defined as reward structure by Furchtgott and Meyer [2], and as
rewardficnction by Donatiello and Iyer [3,4]. It is this reward rate
that we use to characterize the performance of the system. The
average reward received over infinite time horizon is used as an
optimization criterion for resource control in a distributed realtime computer system.

Problem Formulation

Consider a system which may exist in one of several states.
A state is a compact description of everything about the system
that it is relevant to know. At predefined instants of time, an
action or input is applied to it. The system response to that action
is characterized by the matrix of transition rates p,,(a), Le., the
rate of a direct transition from state i to state j under action U .
Assume that there is a reward that the system generates for its
owners per unit time; a reward which is clearly a function of the
system state. Assume also that taking an action Costs something
(zero is a permissible cost).
U

Maximizing the net reward per unit time by suitably
choosing the actions U is one of the most important problems of
decision theory. We refer the reader to [6,7,8] for an excellent
introduction to the subject. In the remainder of this section, we
formalize and elaborate on what we have said above.

The expected total reward accumulated during a mission
lifetime as an optimization criterion has been studied in [5] for
configuring degradable systems. The results suggest that the
system should perform not only passive reconfigurution to respond
the occurrence of a failure, but also uctiw reconfigriration to better
adapt itself to the current needs of the operating environment.
The problem is formulated as a dynamic programming problem
with a finite horizon. It can be simplified by identifying the relationships between configurations and switch times (the time
that the system performs active reconfiguration.)

Suppose our computer system has n units of some resource.
A unit of resource is defined to be the smallest part of the whole
system which may fail to operate, and which can be repaired, reloaded, or replaced. Examples are processors, memory modules,
1/0 channels, shared tables, f i e units, and data sets. A unit can
provide useful services when it is fault-free and may become unavailable or invalid in the event of failure or loss of controL
The system state space is the aggregate of all states, denoted
by @. @ is a finite set. The system state at time I can be defied
as a stochastic process, S(t).

The solution provided in [5] is limited to applications with
non-repairable system and with a finite mission lifetime. Its
underlying system model is represented by an acyclic Markov
process. To examine the optimal system operation in a periodic
real-time environment, additional considerations must be kept in
mind: (1) time dependence due to the periodic nature of the
system, (2) cyclic Markov process for system states if the system
is repairable, and (3) average reward criterion. A solution methodology is suggested in the paper which consists of two steps: in
the first step, an embedded Markov chain and decision process
are established to model the system between successive periods,
and in the second step, recursive computations are conducted to
search for an improved strategy and to calculate state transition
probabilities of the embedded Markov chain. We shall show that
the suggested approach converges to the optimal solution.

For a state i E @, the system may be in one of various operational modes or may take certain actions. For instance, the
system might choose to reconfigure itself. Let A,, be the set of all
available actions or operational modes at time t when S(t) = i,
and let the system choose an action U from A,,. It is easy to see
that transitions between system states depend upon the current
state and the current action or operational mode. When the causes
for changing the availability/functionality of units are uncorrelated and the Occurrences of failure in each unit are Markovian,
state transitions will be conditional independent of the past states
and the past actions, given the present. Thus, the system’s behavior at time t can be fully specified by S(r), u(S(t),t) and the
associated p,,(u(S(t),t)) ,where S(t) E @ and u(S(t),t) E AS(,,,is
the action chosen at time t with system state S(t).

This paper is organized as follows. In Section 2, the optimal
system operation problem in periodic real-time environment is
stated formally. In Section 3, we construct a solution methodology based on the strutegv miprowmerit procedure from decision

Let p ( i , a f ) be a reward rate at time t associated with the
system state i and the action a E A,,. This reward rate represents
what the system can achieve, or may lose, per unit time, with
(i,u,t). In addition, we assume that there exists a cost, c(i,of) ,

34

3.0

with the action a taken when the system enters state i at time 1.
The cost c(i,a,t) represents the instantaneous cost (if any) of
taking the action a . Certainly, both the reward rate and the cost
of the system should be defined based on the job arrivals or load
condition. For instance, a reward rate in a telecommunication
system could be the number of calls connected within a unit time.
Also, the cost of carrying on subsystem maintenance during the
heavy load period is much higher than that during the light load
period.

Optimization Approach

In this section, we propose an optimization approach to
solve the problem formulated in Section 2. The solution is divided
into two steps: in the first step, we show that an embedded
Markov chain can be used to model the system behavior between
successive periods, and then in the second step, an algorithm
based on Chapman-Kolmogorov equations is developed to search
for the optimal strategy.

If there is a periodicity in the reward, state transition,
available actions, or cost structures, then the system is periodic.
If. as sometimes happens, their periods are different, the least
common multiple of these becomes the system period. Let T be
the system period. The period, T, is obtained from practical
considerations: it may be a day, a week, or any other period natural to the application. We shall represent the functions p(i,u,r)
, c(i,a,t), p,,(a,t) , and A,, in terms of the relative time within a
period. For instance, at time f, the reward rate associated with
state i and the action a is p(i,a, T), where T = t - [t/TJTand 1x1
is the greatest integer less than or equal to x.

3.1

Embedded Markov Chain and Decision Process

When the reward structure is periodic, it is convenient to
partition a strategy into separate sub-strategies.
At time
t E [mT,(nt + 1)Tj, an action U ( T , i) of the m-th sub-strategy n,
is applied where nz = Lt/Tj and 7 = t mT. In other words, each
sub-strategy consists of actions for a period T and is expressed
as a, = { a ( T , i ) I i E a, 7 E [ O , T j ] . Similarly, the optimal strategy a* is given by a set of a,* where ni = 0,1,2
,....

-

Given a set of sub-strategies, the system state transitions
are determined by the current state and the current action which,
in turn, specify the transition rate p,,(a,t). Thus, the system can
be viewed as a time-varying Markov process. Let us examine the
system at times ntT, nt = 0,1,2,...,and let the system state at time
ntT be s,,,. When a sub-strategy nmis applied, the system will
transfer into state s,
at time (nt + l ) T with probability
q,,(n,,,) = Prob{s,,,, = j I s,,, = i) . The system can then be modelled by an embedded Markov chain specified by the sequence

With the system described above, we need to choose an
action at time /, given that the system is in state S(f). The action
chosen not only controls the system's behavior (or system state
transitions), but also determines the reward or cost the system
receives during its operation. A strategy a is specified by a set of
actions, {a(i,t) 1 o(i,t) E A,,, i er a] . Thus, given a strategy n ,
the reward accumulated during [O,r) can be expressed as

is,]. Also, let w(s,, a,) denote the performability accumulated
between niT and (nt + l ) T , which is determined by the state s,,,
and the sub-strategy n,. The original optimization problem defined in Section 2 is then, transformed into a discrete Markov
decision process [6,7,8] where actions are equivalent to the substrategies in our periodic system and the objective function bel
comes 1 ; ~ ,~,w(s,,,a,,,).

M

where S(0)= i, v- = $$v
- c) and I ( i j ) = 1 if i # j, or 0 otherwise. Note that, in the above equation, the cost of switching
an action is represented by the second term.

Because we are interested in maximizing the asymptotic
reward rate, the transient states have no effect on our objective
and are therefore not considered. Also, we limit ourselves to the

The resource control problem arising from this model is to
determine a strategy n' such that the average expected reward
which is defined as

case where there is only one recurrent class in this Markov process. (Recurrence follows from the fact that repair is allowed).
This is not a limiting factor: even if there are multiple classes,
each of them can be considered separately.
There are several existing results of Markov decision process which we shall apply to solve our optimization problem. First,
since the system behavior is periodic and the state space is finite, the optimal sub-strategies must be stationary [10,11]. That
is, rk* = n,* for all k,l. This implies that an optimal sub-strategy
be used periodically. From Theorem 7.6 of IS], an oplimal strategy exists if there exists a bounded function h(i) for i @ , and
a constant g such that

is maximized with respect to all acceptable strategies. This is a
common problem: for instance, if a two-dyad system were to
suffer a processor failure, does it reconfigure into a one-triad
system, or into a one-dyad, one-simplex system? Another example is choosing the recovery action to be taken when a file becomes inaccessible.

35

where q,,(k) is the conditional probability that the system is in
state j at time K6 given it is in state i at time k6 and w(i, k ) is the
reward accumulated during [k6, K6). In the right hand side of
Eq. (61, the first term indicates the reward received during the
k-th duration. The second term is the possible cost due to a new
action or transition from the k-th duration to the k+l-th. The
final term gives the accumulated reward starting from the k+ 1-th
duration.

where g is the maximal performability.
Notice that the condition in this theorem will be met automatically when w(i, n
), is bounded for all i and sub-strategy n
,,
and all stationary strategies give rise to a finite and irreducible
state space. The above equation can be solved either by linear
programming or by using the Howard’s strategy improvement
procedure [61. For each possible n,,,, we need to determine both
qi,(n,) and w(i, qJ, which, in turn, depend upon the sub-strategy
n, applied.

Similarly, to find an improvement in a,, we shall search for
actions a^(i,k) ,k = K 1 to 0 and i e CP , which maximize the
following recursive equation:

-

3.2 Optimal Sub-strategy

To apply the policy improvement procedure in our decision
process, we need to solve the following linear equation first:

The initiaAcondition of the above equations should be set to

h (i,K) = h (i, n), which is the solution of Eq. (4).

and then to search for an improved strategy.

In our algorithm (to be presented), the period is digitized
to make it suitable for digital implementation. Let K be a large
natural number, 6 = T / K , and let o(i, k ) be the action taken when
the system is in state i and 7 e [k6,(k + 1)6) . We want to determine the optimal sub-strategy

We summarize the computation suggested above in the following algorithm which determines the optimal sub-strategy n,,,*.

Algorithmfor Optimal Sub-Strategy:
nn,*(K)

= b * ( i , k )Ii

CP, and k = 0, 1, ...,K

- 11

Select an arbitrary strategy
nnl =

Also, it is convenient here to regard pi,@, k) as the probability
that the state transition i + j occurs in the k-th time period of
duration 6. This probability can be easily calculated based on the
transition rate k,,(u,t). WithpJo, k), p(i, U , k) and &,a, k) as the
state transition probability, reward rate, and cost of action U at
the k-th duration under an action U , the Chapman-Kolmogorov
backward equations can be used to calculate the state transition
probability and accumulated reward during each period. These
are represented by the following equations:

{u(i,k)I i e Q, k = 0,1, ..., K

- 1, o(i,k) e Aika].

For each i = 1,2,.... n, calculate the state transition probability and reward between periods niT and (nt + 1)T. This
can be done by solving Eqns. ( 5 ) and (6) iteratively, with the
initial conditions q,(K) = 0 if j # i, 1 otherwise, and
w(i,K) = 0. Then, qi,(am)and w(i, n,,,) are set to qi,(0) and
w(i, 0), respectively.
We must solve the set of linear equations (4), which describes the embedded Markov chain between successive periods. Note that, since [q,,(a,)]is a transition matrix, the set
of equations (4) is dependent, and thus, h (i, n), cannot be
uniquely determined. This is not a limiting factor in practice
since it is easy to $ow that only the relative, and not the
absolute, values of / I (i, n,,,) affect-our search for the optimal
actions. We can set one of the h (i, a,) to some value, and
solve for the rest.
h

-9,

With h (iX)= h a,“),
we search for those actions a^(i,k)
which maximize h(i,k) of Eq. (7) for k = K 1 to 0, and
i e 9 .

36

-

5.

-

Let a,' = ;u(i,k) I i E a, k = 0, 1,...,K 11. If a i = a,,
then stop the algorithm with am* = a i . Otherwise, set
gm = a
,', and go to step 2.

and then rises to its peak value by 8:OO. This value is maintained,
with a break of an hour at noon, until 5:OO in the evening, after
which it declines.
The system consists of ten processors which can be grouped
into several operation units which are either dyads or triads. Thus,
a single processor failure can be detected if dyads are used or
masked immediately if triads are used. We further assume that
the system must be configured entirely in triads or entirely in
dyads. The reward rate the system receives is assumed to be the
product of the arrival rate and the number of operation units.
Given there are i processors available, the reward rate is
A(t)Li/2] or A(t)Li/3J at time t when the system is configured
in dyads or triads. respectively.

For finite K, this algorithm yields nearly-optimizing, but not
necessarily optimal, actions, since the actions are taken at specific
epochs in the operating interval (namely, at multiples of 6). It is
easy to show, from the fundamentals of dynamic programming,

-.

that the algorithm tends to be optimal as K m [ 10,111. It is
trivial to show (by contradiction) that if there are two numbers
k, and k2,with X; = nik, for some natural number m > 1, then the
algorithm with K = & yields a policy which is at least as good as
that with K = k,. This nearly-optimal issue is less troublesome
from a practical standpoint than it first appears, since, for example, reconfiguration of a computer system is too expensive (in
terms of time overhead) to be performed frequently. 6 can then
be chosen appropriately. For instance, in the numerical example
that follows, we use 6 = 3 minutes. This means that reconfiguration can take place up to 20 times an hour: something that
should be perfectly adequate for the example in question.

When a processor in a dyad fails, there is a penalty incurred
since the job that is interrupted must be restarted. We express
this through a penalty multiplier which sets the penalty for failure
equal to the arrival rate at the failure moment times the multiplier.
However, if a processor in a triad fails, errors can be masked by
voting, and no job recovery is necessary. The penalty for
processor failure is zero in this configuration.
There is a repairman on call. We have the choice of
summoning him if he is away, and of keeping him or sending him
away if he is on-site. The cost rate of keeping a repairman per
unit time is also assumed to be periodic with a period of 24 hours.
The rate is shown in Figure 2. As one might imagine, this rate is
greatly magnified when a repairman is called in after normal
business hours.

Theorem: The above algorithm converges.
Proof: We exploit the well-known fact that the strategyimprovement procedure converges for homogeneous systems.
For every periodic system S, with finife action set A,, transition
functions &P( ), and a reward structure, we can construct a
discrete-time (with time-period T ) homogeneous system S, with
finite action set A,, transition functions @( ), and a reward
structure. such that

The state of the system expresses two things: whether the
repairman is present or absent, and how many processors are
functional.
That
is,
The
= {(i, {) I i E {01,2,3,...,101, { E (present, absent 11.
action taken by the system as it enters each state is to decide what
to do with the repairman (whether to call, keep, or send him
away), and how to configure the system (into dyads or triads).
There is a cost associated with summoning the repairman and a
cost per unit time for keeping him. As explained above, there is
a penalty incurred when a dyad fails. The system receives a reward per unit time equal to the number of groups (dyads ortriads)
functioning minus any costs incurred due to the action at that
state.

for every policy a, in the periodic system, there is a corresponding action U, E A, such that #(a,) = @(a.) ,
the expected reward per time Taccruing as a result of taking
action U, with S, in state 11 is equal to the reward per period
(of length T'J due to policy a, with S, in the corresponding
statep at the beginning of the period.
Clearly, running the strategy-improvement procedure for

S,is equivalent to running it for S,. Convergence is thus established.

0

4.0

Processors fail, and are repaired (if the repairman is present), according to an exponential law with mean p / l and p;1, respectively. More specifically, the following symbols will be used
for this example.

Optimal Operation of an On-line Transaction
Processing System

p,: repair rate.

p,: failure rate.

Consider a transaction processing system which has a periodic job arrival rate A(,) with a period of 24 hours. The job amval
rate is shown in Figure 1: it is low until about 7:OO in the morning,

r,: cost per hour of keeping the repairman.
r,: cost per hour of summoning the repairman.
37

1

......................
?Zz
ckow

t

0

I2

8

4

u

,

lo

I6

24

h d *

Figure 1. Reward Function

With repoirmon presevt

..3

Wilh repoirmon obsenl

r,

= IO00

.........................................

IO

..................................

, ..................

*

0

4

@

I2

I

,

I&

,

,

20

,

24

Figure 2. Cos1of Keeping the Repairman
We have set K = 480, i.e., the day is divided down into 480
3-minute segments. We also assume that if the repairman is called
at time k6, he arrives at time ( k + 1)6.
Numerical results are presented in Figures 3 to 7. Figure 3
deals with the effect of r,, i.e., the cost of having the repairman
on-site, on the optimal action. As expected, the system tends to
use him less when he is more expensive. As r, increases, it becomes better to send the repairman away during particularly expensive periods (e.g., during the lunch hour or in the evening),
and to accept the additional cost, r,, of summoning him later. A
sample trajectory is plotted in Figure 3b. The system starts the
day (0:OO hours) with eight processors functional, and the
repairman away. At 4:OO AM, a processor fails (point b). Still,
the repairman is not called until 6:OO AM, when the cost of
keeping him is sufficiently low. With the repainnan present, the
system is brought up to eight functional processors by about 9:OO
AM, (point e), to nine processors (point f, by about 1O:OO AM,
and fully functional (point g ) around 11:OO AM. At this point,
the repairman is sent away. The sample path for the second half

With repoirmon present

With repoirmon obsenl

k.r,=ZS000

Fi”
u)ooo, p,

38

3. Ella3 d changing r,
0.01. C. = 1.0, pnrlty muliipliir=300

I

of the day can be interpreted in the same way: at night, for example, with the reward rate down and the repairman expensive,
even a bad succession of failures does not prompt the summoning
of the repairman: not unless there are fewer than two processors
functional is the repairman summoned.
Figure 4 considers the effect of the penalty incurred on
dyad/triad failure: as the penalty increases, it shows that the
system configures itself more and more into triads. When the
penalty is 200,the only triad formed is when there are only three
processors functional: this is obvious since with just that many
processors available, there will be as many triads as there can be
dyads. As the penalty for failure increases, triads are preferred
more and more, despite the reduction in throughput that results.
When the penalty is 300, the system now configures into triads
when there are 9 functioning processors as well. As the penalty
rises to 400, this is the case for 6 and 7 functional processors in
addition to those mentioned above. Finally, when the penalty is
600, the system is always configured into triads except when there
are not sufficient processors to make up even one triad.

no. of processors

71Y

w:11

I

”

4

5

6

7

B

9

of day ”12

$0

no. of processors funclional

l?fl

Figure 4. Effect d Failure Penalty on Configuration

r,=1000.

17%

r,-W000.p,-0.01.p,=1.0

The change in penalty of failure also affects how the
repairman is handled. In Figure 5 , we plot the repairman curves
for two penalty multipliers: 200 and 500. As one might expect,
when the failure penalty is large, the repairman is called sooner
and retained longer.

Hpre 6. Effect d Time d I h y on Cmligumtion with a Constant
Puulty d 1oooO.

In Figure 6, we consider the case when the penalty is constant and not a function of time, and show how the changing reward rate affects the optimum configuration. Below three
functional processors, there is no decision to be taken: the system
has to work in a dyad. When, for instance, there are nine
processors functioning, the system is configured into triads from
midnight to 7:12 AM, when it switches to dyads. It switches back
to triads at 12:12 PM, and back again to dyads at 1:12 PM.
Finally, at 5:OO PM, when the reward rate begins to drop, the
system goes back to operating in triads.

39

1

Figure 7 considers the effect of increasing the cost of
summoning the repairman. The repairman is now kept for a
greater number of states when the cost of summoning him becomes very great: it is better to pay the cost of keeping the
repairmen under such circumstances. When r, = 106 , the
repairman is sent away only when all processors are functioning,
and it is eight o’clock in the evening (the time when the reward
rate k very small and the cost of keeping the repairman is especially large).
While all these trends are intuitively clear and don’t need a
sophisticated algorithm to determine, the exact epochs at which
the repairman should be called or sent away, and the system
configured into triads or dyads, cannot be obtained through intuition alone, and do require an algorithm such as this one.

With repoirmon present

7a. r,

5.0

-

With rewirmon obsenl

5ooo

Discussion

In this paper, we have studied the problem of optimally
controlling resources and system operations in periodic real-time
environments. The problem is of great practical siflicance because gracefullydegrading systems are being used more and more
in such commercial fields as bankiig, communicationand process
control. Such a environment often has a periodic job arrival or
workload. It is also possible to estimate more or less accurately,
Nnning costs, the benefits from having a certain throughput at
various times, repair costs, etc. We have proposed an algorithm
which uses an embedded Markov decision process to model system behavior over a period. Optimalcontrol of the kind described
here is the key to achieving good performance and availability.
In this paper, we discussed on-line transaction processing
systems. However, this algorithm should be useful wherever a
multiprocessor system has to cope with a job load that has a
roughly periodic structure (no job loading is ever exactly periodic). Examples are real-time embedded systems, local-area
networks, etc.

With repoirmon present

7b. r,

-

With repoirmon obsent

50000

Acknowledgement
The authors would like to thank the referees for many helpful
comments that improve the paper.

With repoirmon present

With rcpoirmon obsent

7c. r, = 1OOOOO

- - -

7. mfcct d Summoning-Caston R c p i m n
1.0, penalty multiplier = MO
r,
3ooo. p, 0.01.

I

References
J. F. Meyer, "Closed-form Solutions of Performability,"
IEEE Trans. Conipirfers,Vol. C-31,NO. 7, pp. 648-657,
July 1982.
D. G. Furchtgott and J. F. Meyer, "A Performability Solution Method for Degradable Nonrepairable Systems,"
IEEE Trans. Coniputers, Vol. C-33,No. 6, pp. 550-554,
June 1984.
L. Donatiello and B. R. Iyer, "Analysis of a Composite
Performance Reliability Measure for Fault Tolerant Systems," Research Reporf RC-10325 IBM Thomas J. Watson
Research Center, Yorktown Heights, NY, Jan. 1984: (to
appear in Journal of the ACM 1.
B. R. Iyer, L. Donatiello, and P. Heidelberger, "Analysis
of Performability for Stochastic Models of Fault-Tolerant
Systems," Research Report RC-10719 IBM Thomas J.
Watson Research Center, Yorktown Heights, NY, Sep.
1984,(to appear in IEEE Trans. Conipicters). ).
Y.-H. Lee and K. G. Shin, "Optimal Reconfiguration
Strategy for a Degradable Multi-Mudule computing System," Journal of ACM Vol. 34, No. 2, April 1987, pp.

326-348.
R. A. Howard, Dynanric Probabilirtic Systenu, Vol. II:
Senti-Markov and Decision Processes. New York: Wiley,

1971.
C. Derman, Finite Sfate Markovian Decision Processes, Academic Press, 1970.
S. M. ROSS,Applied Probability Models with Optiniizafion
Applications, Holden-Day, 1970.
E. B. Dynkin and A. A. Yushkevich, Controlled Markov
Processes, Springer-Verlag, 1979.
[lo] A. Federgruen and H. C. Tijms, "The Optimality Equation
in Average Cost Denumerable State Semi-Markov Decision
Problems, Recurrency Conditions and Algorithms," J. Applied Probabiliw, Vol. 15, 1978,pp. 356-373.
[ll] R. Bellman, "Functional Equations in the Theory of Dynamic Programming IV,A Direct Convergence Proof,"
Annals Math. Vol. 65, pp. 215-223, March 1957.

--

41

STUBcast - Efficient Support for Concurrency Control in Broadcast-based
Asymmetric Communication Environment
Yan Huang

Yann-Hang Lee

Computer & Inf. Sci. & Eng. Dept.
University of Florida
P. O. Box 116120,
Gainesville, FL 32611, USA

Computer Sci. & Eng. Dept.
Arizona State University
P. O. Box 875406,
Tempe, AZ 85287, USA

Abstract- Observing that it is impractical to use traditional
methods to control concurrency in broadcast-based asymmetric
communication environment, we introduce a concurrency
control protocol designed for broadcast-based transaction
processing called STUBcast (Server Timestamp and Update
Broadcast Supported Concurrency). STUBcast supports two
new correctness criteria proposed in this paper - Single
Serializability and Local Serializability. These criteria are
weaker than global serializability but are practical and easier to
achieve in broadcast environment. This article also shows some
simulation results. These results suggest that STUBcast could be
very efficient in realistic application environment.

Multi-Version and Serialization Graph could be substantial in
the process of backing-up/broadcasting/computing old
version data items or serialization graphs. Among the
solutions supporting both read-only and update transactions,
F-Matrix broadcasts an N×N control information (F-Matrix)
each cycle. Certification Reports approach needs to
synchronize server and clients’ time clocks, send both readonly and update transactions to server, and backup server
transaction history for verification.

Keywords- data broadcast, asymmetric communication,
concurrency control, transaction processing.
I. INTRODUCTION
A new communication model called broadcast-based
asymmetric communication [1] has become a subject of
attention in the past few years. In this model, a server
broadcasts information to a huge number of clients on a high
bandwidth channel and clients access the information by
listening to the channel. This model can eliminate read
requests sent from clients to the server and reuse broadcasted
information. Therefore, the demand for network bandwidth
and the server service burden could be greatly alleviated,
which makes it an appropriate model for transaction
processing in many wireless and Internet applications.

The data broadcast model suggests the uplink
communication from any client to the server should be
avoided. This implies that the traditional schemes such as 2phase locking, time stamp ordering, and optimistic
concurrency control [2, 3] cannot be applied to control
concurrency in broadcast-based transaction processing
because all these methods require extensive communication
among clients and the server. In order to fully take advantage
of this model and still achieve consistency, a suitable
concurrency control protocol is needed.
There already exist several methods for broadcast-based
concurrency control in literature, such as Update First
Ordering (UFO) [4], Multi-Version Broadcast [5, 6],
Serialization Graph [5, 6], Broadcast Concurrency Control
with Time Stamp Interval (BCC-TI) [7], F-Matrix [8], and
Certification Reports [9]. All of these methods have some
limitations. For instance, UFO, Multi-Version, Serialization
Graph, and BCC-TI only support client read-only
transactions. On the other hand, the processing overhead in

0-7803-7128-3/01/$10.00 (C) 2001

262

Moreover, protocol inefficiency is introduced by some
solutions because of the support of strict global serializability
[2, 3] as their correctness criterion. For example, UFO, MultiVersion, BCC-TI and Certification Reports are designed to
support global serializability, which is very difficult to
achieve in distributed broadcast environment because it
requires all existing transactions on any clients and the server
to be serializable [2, 3, 8]. Without direct communication
among clients and the server in broadcast environment, when
the information on transaction execution is not sufficient or
when certain optimizations must be carried out, some of these
solutions chose to reject transactions that could be
serializable in order to guarantee global serializability.
As an example, Fig. 1 shows how BCC-TI [7] rejects a
serializable transaction. BCC-TI assigns each read-only
transaction a time interval (LB,UB) with initial value (0, +∞).
Whenever a transaction reads an item d , it changes LB to the
maximal of current LB and WTS(d) (time that d is last
updated). At the beginning of each cycle, a transaction’s read
set in last cycle is compared with the write set of each
committed transaction U in last cycle, denoted as WS(U). An
overlap will change UB to the minimal value of current UB
and TS(U) (time that U commits). A transaction is aborted if
and only if LB becomes larger than UB. In Fig. 1, T’s LB is
U2
U1
WS(U2)=B
WS(U1)=A
TS(U2)=3
TS(U1)=2
A
B
WTS (A) = 2 WTS (B) = 3

WS(U1)=A
TS(U1)=2
WS(U2)=B
TS(U2)=3

U1
T

A
T
(LB, UB)
=(0,+∞)

WTS (A) = 1
Read A
(1, +∞)

U2

B
WTS (B) = 3

Last cycle overlaps
with WS(U1)
(1, 2)
time

Read B
(3, 2), 2<3
ABORT

Fig. 1. How BCC-TI rejects a serializable transaction.

set to 1 since it reads A (WTS(A)=1) in the first cycle, within
which U1 updates A at time 2 and U2 updates B at time 3.
T’s UB is later set to 2 since its read set overlaps with
WS(U1) in the first cycle. When T reads B (WTS(B)=3) in
the second cycle, T’s LB is updated to 3. Consequently, T
aborts because LB > UB. However, the abort is unnecessary
because the conflict graph [2, 3] of these transactions is
acyclic.
BCC-TI aborts serializable transactions because global
serializability is very difficult to achieve while it is lack of
sufficient information of transaction execution in broadcast
environment. Similarly, Certification Reports may also reject
serializable transactions [9, 10] by the same reason.
Rejecting transactions that are actually serializable
reduces the efficiency of the transaction processing systems.
This inefficiency is caused since a pessimistic view of
transaction conflict graph is applied to achieve global
serializability and it is problematic to establish a precise
conflict dependency in such a distributed system.
Nevertheless, the strong global serializability may not be
necessary and relaxed correctness criteria could still be
practical in real applications. For instance, the F-Matrix
approach ensures the consistency and serialization among all
update transactions [8].
While the update consistency is essential to guarantee
data integrity, we argue that weaker criteria than global
consistency are practical in many distributed applications. For
instance, the reports of two independent stock price updates
do not need to be consistent if they are taken at two different
sites of the systems. That is, one informs the old price of
stock A and the newly updated stock B in New York,
whereas the other presents the new price of stock A but the
old version of stock B in Chicago. In other word, the updates
are concurrent and the viewing operations do not need to (or
cannot) be atomic as they occur in separate locations and
instances.
The relaxed criteria motivate us to look into two types
of serializability: single serializability (SS) and local
serializability (LS). Single serializability insists on that all
update transactions and any single read-only transaction are
serializable [2, 3]. Local serializability, conversely, requires
all update transactions in the system and all read-only
transactions at one client site are serializable. SS and LS are
obviously weaker than global serializability and easier to
achieve, but they guarantee the consistency and correctness in
server’s database because they require all update transactions
to be serializable. SS is practical since most read-only
transactions under broadcast need not worry about how other
read-only transactions view the updates. Even if several
views of updates are required in decision-making, it is often
that the transactions at the same client site are involved,
where LS can be applied. Therefore, SS and LS are practical
correctness criteria in broadcast environment and easier to
achieve than global serializability.

263

Motivated by the limitations of existing solutions and the
applicability of the relaxed serialization criteria, we report in
this paper, the design and evaluation of a new broadcastbased concurrency control protocol called STUBcast (Server
Timestamp and Update Broadcast Supported Concurrency).
STUBcast supports single serializability and local
serializability and aims at overcoming existing solutions
limitations. In the subsequent section, STUBcast's system
model is introduced. The protocol of STUBcast is presented
in Section 3. Then we show simulation results of STUBcast
in Section 4. Section 5 briefly concludes the advantages of
STUBcast and discusses our current and future studies related
to STUBcast.
II. SYSTEM MODEL
The system model of STUBcast allows both read-only
transactions (reading from air without being known by the
server) and update transactions (modifying data by sending
requests through a low bandwidth channel after completion at
client). All operations are executed in order. We have no
assumption of cache [11], index [12] and scheduling [1],
while how to integrate them with STUBcast is presented in
[10].
STUBcast divides broadcast operations into primary
broadcast (Pcast) and update broadcasting (Ucast). Pcast
disseminates data items with current values and ID by a
scheduled sequence using any broadcast algorithms. A Ucast
is inserted into the on-going Pcast once an update transaction
commits at the server, with the updated data items’ new value
and ID. Each Ucast stage begins with an update broadcast
beginning (UBB) tag and ends with a update broadcast ending
(UBE) tag (Fig. 2). The IDs of the data items read by the
committed update transaction are also attached with UBE tag.
III. THE PROTOCOL
STUBcast has three main components: client side readonly serialization protocol (RSP), client side update tracking
and verification protocol (UTVP), and server side server
verification protocol (SVP). RSP has two versions: single
serializability supported RSP (RSPSS) and local serializability
supported RSP (RSPLS). RSPSS (or RSPLS) accepts a read-only
transaction if and only if it conforms to SS (or it conforms to
LS). UTVP and SVP only accept part of the serializable
update transactions. Together, RSP, UTVP and SVP guarantee
SS or LS in all accepted transactions. The correctness of these
claims is proved in [13].
Transaction T span

Ucast

Ucast
Pcast
Data in
Pcast

Ucast
Pcast

Pcast

Pcast
Data in
Ucast

UBB

UBE with read
data ID

Fig. 2. Broadcast operations in STUBcast protocol

A. Protocol Essentials and RSPSS
Protocol components of STUBcast achieve concurrency
control by inserting Ucast into server’s Pcast and maintaining
some essential data structures. This section presents how
these essential structures are used through the introduction of
RSPSS. We assume all update transactions committed at
server are serializable.
In STUBcast, a Server Timestamp (time that the last update
of a data is committed, denoted as ST) is always saved and
broadcasted with each data. Each transaction has a
Timestamp Array (TSA), where it records the server
timestamp attached with a data when it reads that data from
air for the first time. Since a Ucast on the broadcast channel
implies an update transaction committed on the server, by
comparing the timestamp attached with each updated data in
that Ucast and the corresponding timestamp in the timestamp
array, STUBcast can decide whether a client transaction reads
any data before it is changed by that update transaction. This
basically identifies the conflict relationship from a client
transaction to a committed update transaction.
Given that Ucast operations are sequenced in the commit
order of updated transactions, it can be asserted that update
transactions committed outside a client read-only
transaction’s spanning time cannot cause any cycle in the
conflict graph of that read-only transaction. This is illustrated
in Fig. 3, where no cycle can include U1 or U4. Any potential
cycles can only be caused by the update transactions that
have a Ucast inside a read-only transaction’s spanning time.
We enumerate all possible cycles in Fig. 4.
Fig. 4 (a) and (b) indicate the cases that a read-only
transaction T reads data 1 from the Ucast of update
transaction U, and has ever read a version of data 2 earlier
than U updates it (we call this as a read ahead). A cycle is
formed between U and T due to these two read operations.
STUBcast uses a Read Ahead Flag (RAF) to represent
whether a read ahead status is detected within a Ucast stage.
For example, in (a), when data 2 is broadcasted in U’s Ucast,
it must have a timestamp larger than the one recorded for data
2 in T’s time stamp array. STUBcast sets T’s read ahead flag
to true whenever such a situation is detected. Note that the
only difference between (a) and (b) is that the read ahead flag
is set after or before T reads data from the Ucast. Suppose
that in case (a) T reads data 1 and commits, then the read
ahead status would never be detected. To avoid this, a NoCommit Flag (NCF) is set until the end of a Ucast to prevent
U1

U2

U3

U2

U3

2

123

U

R1

T

2

U

U2

123

34

R2
(a) T

R4

R2
(d) T

U
U

2

U
U

123

U2
T
T R2
(b)

U

4

U2

T

4

4
R4

R2
(e) T

U

U2

123

45

R2,3
U2

U

3

T
T R2
(c)

R3

U

U
U2
T
(f)
U

U2

U2

123

T

R3

2

2

R4,5

2

123

2

123

4

4
R4

T R2
R2,5
U2

R6
U3

R6
U4

4

5

57

7

U3
U4
T

(g)

T R2

R7

Fig. 4. Cycles in the conflict graphs including read-only transaction

T from committing if T ever reads any data within an ongoing Ucast. STUBcast aborts a client read-only transaction
if its no-commit and read ahead flags are both set, which is
also the first policy of RSPSS.
Suppose T reads ahead of an update by U and does not
read data from U within its Ucast, a cycle can still exist if T
reads any data U updates from a later Pcast (Fig. 4 (c)). So in
STUBcast, once the read ahead flag is set for T in U’s Ucast,
the data U updates are recorded in T’s Conflict Array (CFA)
where each unit has two fields: update (CFA[unit].u) and
read (CFA[unit].r) fields. All update fields mapping to U’s
Ucast items are set to true if a read ahead is detected for T
within this Ucast. So later once T reads any data whose
update field in T's conflict array is true, a cycle is implied and
T will be aborted. This is also the second policy of RSPSS.
Fig. 4 (d), (e), (f), and (g) indicate the cases that T reads
ahead of U but never reads anything U updates. A cycle can
still happen because of the indirect conflicts among update
transactions. Fig. 4 (d) shows that U and U2 have a writewrite conflict and then T reads from U2. Fig. 4 (e) shows that
U and U2 have a read-write conflict and then T reads from
U2. Fig. 4 (f) shows U and U2 have a write-read conflict and
then T reads from U2. We call these indirect conflicts a
Conflict Chain. At last, Fig. (g) shows T only reads from U4
but there is a more complicated conflict chain among U, U2,
U3 and U4. The conflict array is again used to solve these
cases.
Here conflict array is based on the idea of conflict chain,
which is constructed by the commit sequence of the update
transactions:
• An update transaction of which a client transaction T
reads ahead is added to T's conflict chain.
• An update transaction of which T does not read ahead is
added to T's conflict chain if the update transaction has a
write-write, read-write or write-read conflict with the
transactions in T’s existing conflict chain.

U4

T

U1

U
U

U4

T

Fig. 3. All possible conflict dependencies with a read-only
transaction T

264

A conflict array uses both update and read fields to record
the data that transactions in T’s conflict chain have written or
read and to detect whether a new transaction should be put in
the chain. If an update transaction is in the chain, the update
fields mapping to the data in Ucast and the read fields
mapping to the data with the read IDs attached with UBE are
set to true. An update transaction of the current Ucast is put
in the chain if T reads ahead of it. Otherwise, if any update or
read field in the existing conflict array mapping to any data in
its Ucast is true (write-write/read-write conflicts) or if any
update field mapping to any data in its read IDs is true (writeread conflicts), an update transaction is also put in the chain.
Consequently, as a policy of RSPSS, whenever T reads any
data whose update field in T's conflict array is true, a cycle is
implied and T will be aborted.
As a conclusion, this section has described the policies of
RSPSS while introducing some important data structures used
by RSPSS and other components of STUBcast. Additionally,
STUBcast uses a RECarray to submit update transactions to
server, which is a list of records that keeps the tracks of client
update transaction operations for server verification and
database update.

B. RSPLS
The major difference of RSPLS from RSPSS is that it needs to
detect any non-serialization status caused by combining all
local read-only transactions to all update ones. Fig. 5 shows
all possible conflict cycles caused by multiple read-only
transactions. To detect these cycles, RSSLS keeps the history of
timestamp array and conflict array of all executing or
executed read-only transactions on the same client. We call
these histories TSAList and CFAlist. Moreover, each conflict
array unit extends to have a timestamp field
CFA[unit].timestamp. When a data's update field in the array
is first set as true, the timestamp attached to the data in the
corresponding Ucast is put in its timestamp field. The
additional steps used for RSPLS on a transaction T is as
following:
Condition: T reads a data k successfully and at least one
update field of T's CFA is true (corresponding to transaction
R2 in Fig. 5, so there is UmÅTÅUi/Uk or UnÅTÅUi /Uk).
Step 1: Search in CFAlist, check if there is any CFA's
(other than T's CFA) CFA[k].u is true. (check whether there
is a read-only transaction R that RÅUmÅT or
RÅUm…ÅUnÅT). Continue transaction if no true value is
detected (no such R exists).
Step 2: Otherwise (such R exists), for each CFA that has a
true CFA[k].u field, locate its owner transaction's TSA in
TSAlist (here name it as TSA1). Then for each true update
field CFA[j].u of T's CFA, compare if CFA[j].timestamp of T
≥ TSA1[j]. If any such compare results as true (TÅUkÅR or
TÅUi …ÅUkÅR exists, so R corresponds to R1), abort T.
C. UTVP
An update transaction executes at client and all write
operations are executed locally. Protocol UTVP tracks local
written values and other important operations in a RECarray

265

Ui

Uk

Um

Un

Ui

Uk

Uk

R2

Um

Un

Ui

Uk

R1
R2

Un

Um

Un

R1

R1
Ui

Um

R2

R1
time

R2

Fig. 5. Possible conflict cycles caused by multiple read-only transactions

until the transaction completes and submits it to server for
commit and database update. UTVP also verifies and detects
possible non-serialization status of a transaction at client. The
transaction waits for the server's reply of whether it is
accepted after the submission of RECarray.
A write on one data item in an update transaction can be
the first write or non-first write. Only the latest written value
of each data updated by the transaction is recorded and
submitted to the server. A read on a data item can be local
read (read that data's locally written value), first non-local
read (1st time read on that data from air) and non-first nonlocal read (read that data from air again). Table 1 gives
UTVP’s tracking policies in constructing RECarray based on
these operations, while actions for detecting possible nonserialization status are:
•
•
•
•
•
•

If non-local read in Ucast, set NCF = true.
If first non-local read at m, set TSA[m]=ST(m).
When finished and NCF is set, wait until NCF is reset.
When finished and NCF is not set, submit RECarray.
When data n is in Ucast and TSA[n]< ST(n), Abort.
When UBE is on air, reset NCF.

D. SVP
SVP sequentially verifies whether each received update
transaction is serializable to all other update transactions
committed earlier using the transaction’s RECarray and
following steps:
Step 1: Check the record of each first non-local read
operation (m, timestamp of m, “R”). If the timestamp has a
value less than the one currently saved with the data in the
database, send an "Abort" reply to client.
Step 2: If step 1 is finished without aborting, update the
server database using the value in each (m, value, “W”)
record. Attach the current server timestamp (same for all
operations) to each data item. Send a "Commit" reply to
client.
TABLE I. TRACK POLICIES OF UTVP
Operation
Policy
on data m
First Write
Add (m, value, “W”) to RECarray
Non-First Write
Update (m, value, “W”) in RECarray to (m,
newvalue, “W”)
Local Read
Do nothing
First Non-Local Read Add (m, timestamp of m, “R”) to RECarray
Non-First Non-Local
Do nothing
Read

TABLE II. SIMULATION CONFIGURATIONS
DEFINITION
The time used to broadcast a data (fixed size)
The time used to broadcast UBB
The time used to broadcast UBE
Read-only to update transactions ratio
Read to write operation ratio in update transactions
Non-local to local read ratio in update transactions
Primary schedule scheme
Downlink bandwidth to uplink bandwidth ratio
Number of transactions simulated
Number of items in the database
Max transaction length
Mean transaction inter-arrival time (Poisson)
Mean operation inter-arrival time (Poisson)
Distribution mode of data accesses

simulation run uses only one client and large number of
clients is emulated by using a small average transaction interarrival time on one client. As one of the two data access
distribution mode, NON_UNIFORM divides database to n
parts and simulates the situation that some parts of data are
accessed more often than others. To see how the protocols
influence the performance, for each configuration, the
average response time is also recorded without any
concurrency control and compared with the value obtained
under STUBcast.

Step 3: Insert a Ucast based on the write tracks into Ucast,
attach the IDs of all first non-local reads to UBE.
IV. SIMULATION RESULTS
This section presents our simulation results of STUBcast
with RSPSS, UVP and SVP. Average response time of
transactions is the major performance measurements. Table 2
shows the important configuration parameters and their
definitions. Read-only and update transactions are simulated
based on the ratios defined in the table. Each transaction in
the simulation consists of several data access operations and
computation operations before or after each data access. A
transaction’s length is the number of access operations in it.
MAX_TRAN_LEN represents the maximal transaction
length in the simulated transactions and the transaction length
is uniformly distributed. Therefore, the average length of all
transactions is MAX_TRAN_LEN/2. The computation time
between two access operations is the operation inter-arrival
time. Time between the starts of two consequent transactions
on one client is denoted as transaction inter-arrival time. Each

2000
0

6000
4000
2000

(a) TR_INTARR_TIME 50

32

24

8

16

Max T ransaction Length
NonCon, D100, T r50
NonCon, D1000, T r50
Con, D100, T r50
Con, D1000, T r50

6000
4000
2000
0

0

32

24

16

8

0

8000

Max T ransaction Length
NonCon, D100, Tr100
NonCon, D1000, Tr100
Con, D100, Tr100
Con, D1000, Tr100
Fig. 6. UNIFORM data access
(b) TR_INTARR_TIME 100

266

32

4000

8000

10000

24

6000

10000

12000

16

8000

12000

0

10000

14000
Avg. Response Time
(Number of Broadcast-Units)

Avg. Response Time
(Number of Broadcast-Units)

12000

0

Several simulations are run under different combination of
DB_SIZE, ACCESS_DISTR, TR_INTARR_TIME, and
MAX_TRAN_LEN. Fig. 6 and Fig. 7 show that STUBcast
has almost the same average response time performance as
using no concurrency control when the maximum transaction
length is less than or equal to 12 in all cases. STUBcast also
performs very well when the maximum transaction length is
less than or equal to 20. Moreover, when transactions’
density is relatively low, like when inter-arrival mean time is

14000

14000
Avg. Response Time
(Number of Broadcast-Unit)

VALUE
20
1
10
1:1
2:1
4:1
Flat
8:1
5000
100, 1000 (D)
4,8,12,16,20,24,28,32
50, 100 or 500 (Tr)
1
UNIFORM or NON_UNIFORM
[ n==10, Pi=1/2i (1≤i≤n-1), Pi=1/2n-1 (i==n)]

8

PARAMETER NAME
Broadcast-unit
UBBunit
UBEunit
READ_TO_UPDATE
READ_TO_WRITE
NONLOC_TO_LOC
PCAST_SCHEME
DOWN_TO_UP
NUM_OF_TRANS
DB_SIZE
MAX_TRAN_LEN
TR_INTARR_TIME
OP_INTARR_TIME
ACCESS_DISTR

Max T ransaction Length
NonCon, D100, T r500
NonCon, D1000, T r500
Con, D100, T r500
Con, D1000, T r500
(c) TR_INTARR_TIME 500

5000

(a) TR_INTARR_TIME 50

32

24

8

16

Max T ransaction Length
NonCon, D100, Tr50
NonCon, D1000, Tr50
Con, D100, Tr50
Con, D1000, Tr50

10000
5000
0

0

32

24

16

0

8

0

0

10000

15000

Max T ransaction Length
NonCon, D100, T r100
NonCon, D1000, T r100
Con, D100, T r100
Con, D1000, T r100
Fig. 7. NONUNIFORM data access
(b) TR_INTARR_TIME 100

500 broadcast-units in the simulations, STUBcast performs
excellently. As a conclusion, STUBcast could work very
efficiently in realistic environment since having transactions
with length less than or equal to 12 (or 20) or having 500
broadcast-units inter-arrival mean time among transactions is
very applicable in real applications.
V. CONCLUSION
In this paper, we introduced the STUBcast protocol, which
supports single serializability and local serializability in
broadcast-based asymmetric communication. Using these two
criteria that are weaker than global serializability, STUBcast
can overcome many drawbacks in other solutions and has
some good features such as independence of broadcast
schedule and supporting both read-only and update
transactions. We expect that it can lead to low broadcasting
overhead since only timestamps are used as additional
information during data or update broadcasts. Only inserting
Ucast into primary schedule other than other processed
control information introduces the simplicity. Additionally, it
only aborts [13] read-only transactions that cannot be
serialized with the committed update transactions, which is
expected to achieve more efficiency than other concurrency
control protocols that may abort serializable read-only
transactions. Simulation results also show that STUBcast
could be very efficient in real applications.
We have also conducted some prior and recent works
related to STUBcast. We have proved the correctness of
STUBcast in [13]. Cache replacement policies that we think
fitting into some special features of STUBcast [10] have been
designed and evaluated. We also designed an index strategy
[10] that adapts to STUBcast’s solution model, to avoid
clients always tuning to the channel and save power in
wireless equipment. Moreover, we have found solutions to
insert Ucast into evenly spaced broadcasting algorithms to
guarantee optimal and predictable overall average data access
time [10]. We consider how to apply STUBcast in real-time

267

32

5000

15000

20000

24

10000

20000

25000

8

15000

25000

16

20000

30000

0

25000

30000

Avg. Response Time
(Number of Broadcast-Units)

Avg. Response Time
(Number of Broadcast-Units)

Avg. Response Time
(Number of Broadcast-Units)

30000

Max T ransaction Length
NonCon, D100, Tr500
NonCon, D1000, Tr500
Con, D100, Tr500
Con, D1000, Tr500
(c) TR_INTARR_TIME 500

and error-prone environment as important topics for future
research. Eventually, real application models supporting
concurrency control, high performance scheduling, cache and
index, fault tolerance and real time requirements can be built
based on these research results.
REFERENCES
[1] Acharya, S., Alonso, R., Franklin, M., and Zdonik, S., “Broadcast disks:
data management for asymmetric communication environments,” Proc.
of the ACM SIGMOD Conference, pp.199-210, 1995.
[2] Bernstein, P. A., Hadzilacos, V., and Goodman, N., Concurrency control
and recovery in database systems, Addison-Wesley Publishing
Company, 1987.
[3] Ozsu, M. T., and Valduriez, P., Principles of distributed database
systems, Prentice Hall, 1991.
[4] Lam, K. Y., Au, M. W., and Chan, E., “Broadcast of consistent data to
read-only transactions from mobile clients,” Proc. of the Second IEEE
Workshop on Mobile Computing Systems and Applications, 1999.
[5] Pitoura, E., “Supporting read-only transactions in wireless broadcasting,”
Proc. of the DEXA98 International Workshop on Mobility in Databases
and Distributed Systems, pp. 428-422, 1998.
[6] Pitoura, E., and Chrysanthis, P. K., “Scalable processing of read-only
transactions in broadcast push,” Proc. Of the 19th IEEE International
Conference on Distributed Computing System, 1999.
[7] Lee, V. C. S., Lam, K. W., and Son, S. H., “Maintaining data
consistency using timestamp ordering in real-time broadcast
environments,” Proc. of the 6th IEEE International Conference in Realtime Computer Systems and Applications, pp. 29-36, 1999
[8] Shanmugasundaram, J., Nithrakashyap, A., Sivasankaran, R., and
Ramamritham, K., “Efficient concurrency control for broadcast
environments,” ACM SIGMOD International Conference on
Management of Data, 1999.
[9] Barbara, D., “Certification reports: supporting transactions in wireless
systems,” Proc. of the IEEE International Conference on Distributed
Computing Systems, 1997.
[10]Huang, Y., “Efficient transaction processing in broadcast-based
asymmetric communication environment,” Ph.D. Dissertation Proposal,
2001.
[11]Zdonik, S., Franklin, M., Alonso, R., Acharya, S., “Are ‘disks in the air’
just pie in the sky?” Proc. of the IEEE Workshop on Mobile Computing
Systems and Applications, 1994.
[12]Imielinski, T., and Viswanathan, S., and Badrimath, B., “Energy
efficient indexing on air,” Proc. of the ACM SIGMOD Conference, 1994.
[13]Huang, Y., and Lee, Y. H., “Concurrency control protocol for broadcastbased transaction processing and correctness proof," ISCTA PDCS 2001,
in press, August 2001.

On Robust Transaction
and Load Sharing
PHILIP S YU and AVRAHAM
IBM Thomas

J. Watson

Routing

LEFF

Research

Center

and

YANN-HANG
University

In this

LEE

of Florida

paper

we examine

environment

where

processing
class.

systems

A

response
of the load

depend

on workload
the

which
the

tries

parameters

an incoming

is somewhat
trying

between

the

actual

Categories

three

–dmtrzbuted

tems—modelzng
systems;

D, 4.8

Descriptors:

K. 6.4

addresses:

P.S.

NY

A. Leff,

10027;

10598;
Y.-H.

Gainesville,

FL

Lee,

Yu,

a

factor
to the

based

on

accuracy

of

estimation.

[Computer-Communication

Performance—

IBM

Thomas

Department

Computer

Networks]:

modelmg

Distributed

Performance

of Sys -

Design—distributed

and prediction,

queueing

theory,

and

or distributed

publication

Research

Center,

Science,

Columbia

Information

and

specific

permission,

01991

ACM

fee all or part

for direct
its

for Computing

date

appear,

Machinery.

on Database

of this

commercial

0362-5915/91/0900-0476

Transactions

J. Watson
of Computer

Science

Department,

P. O. Box

704, Yorktown

University,

New

University

York,

of Florida,

32611

to copy without

Association

ACM

time

employes
the discrepancy

a correction
respect

more

strategy,

/ decentz-alzzation

Heights,

of the

response

third

It momtors

with

of just

a strategy

by being

The

[Computer Systems Organization]:
[Operating Systems]: organization
and

Systems]:

Authors’

not made

C.2.4

introduces

to be robust

pursue

H.2.4

cen trali~atzon

Permission

and

alternative

Instead

transactions,

estimates.

ignore

[Database Management]: Systems—dzstrzbuted
systems, transaction
processlManagement of Computing and Information
Systems]: System Management
—

.wmulation;

NY

times

are shown

Three

load

to the

simply

is achieved
the

to

a strategy

is sensitive
that

C.4

D.4.7

[Operating

response

that

adaptive

incommg

time

find

strategies

This

for balancing

of individual

in the

two

estimates

it 1s important

performance

optimization

response

used

database;

techniques;

time

We

and

first

indiscriminate
time

strategies,

worse

the

candidate

future

strategies

parameters

and Subject

Systems

ing;

estimated

All

time,

global

response
adjust

even

certain

transaction

response

transactions

naive

database
that

between

avadable,

accuracy

incriminatory,

response
or the

Since

not be readily

imply

for a given

a balance

hand,

have

others

affimty.

of incoming

other

threshold,
towards

the

and

and system

declslons,

condition

to adaptively

analysis

workload

oriented

the

may

time

On the

transaction’s

to minimize

process

regression

routing

strike

distributed

locality

than

to information

response

examined:

more

on either

feedback

the

suitable

can

that

in a locally

as reference

on transaction

decisions

values.

in making

optimizing

while

of routing

therefore

strategy
only

routing

such
more

parameters

parameter

are

restrictive

routing

to minimize

of certain

transaction

as being

based

and system

strategies
that

based

and routing

robustness

strictly

accuracy

of robust
characteristics

can be identified
time

sharing
examine

the issue

transaction

Systems,

and

material

advantage,
notice

is given

To copy otherwise,

is granted
the ACM
that

provided
copyright

copying

or to republish,

that

the copies

notice

is by permission
requires

$01.50
Vol.

16, No

3, September

1991, Pages 476-512.

are

and the title
of the

a fee and/or

On Robust Transaction Routing and Load Slharing
General

Terms:

Additional

Key

transaction

Algorithm,
Words

Design,

Management,

and Phrases:

.

477

Performance

Distributed

database,

load balancing,

performance

analysis,

routing

1. INTRODUCTION
The demand
for high
transaction
processing
rates has grown
rapidly
in
recent years. This trend
motivates
the development
of a multiprocessor
or
locally
distributed
system. 1 The locally
distributed
database
environment
that we discuss is one in which the database is partitioned
among the various
processing
systems; incoming
transactions
are routed to one of the processing
systems
by a common
front-end
system.
The environment
is depicted
in
Figure
1, and is also referred
to as “shared
nothing
architecture”
[23]. This
approach
and

of coupling

academia;

processing
Tandem’s

e.g.,

systems

has been

Non-Stop

system

followed

in both

[3, 4], Teradata

industry
[24],

and

Gamma
[11] database
machines.
If a transaction
issues a database
request
that references
a nonlocal
database
partition,
the request,
referred
to as a
remote request,
must be shipped to the system owning
the referenced
partition

for processing.

transaction,
that

is, the

Routing
system”
mote

Therefore,

we can often
system

to

with

identify
which

regard

to database

a specific

the

system

transaction

requests

issued

as its preferred

sends

most

of its

by a

system:
requests.

decisions
can thus be classified
as being either
“local”
/“preferred
routing
or “remote”/”
nonpreferred
system”
routing.
The local/redistinction

routed

refers

to its preferred

of its requests
In studying

to the
system

fact
will

that,
make

reference
data already
the performance
of

on average,
fewer

a transaction

remote

requests

reside in the preferred
transaction
processing

database
environments,
the reference
locality
age of database
calls issued by a transaction

that

because

is

most

system.
in distributed

distribution,
i.e., the percent to each database
partition,
has

to be considered.
An important
effect of the reference
locality
distribution
is
that
each transaction
class is faced with
a set of heterogeneous
servers
among which one is better
requests
increase
execution
system
systems,
danger
ignoring

utilization.
the

If

amount

suited than the other servers.
time (due to communication)

incoming

transactions

of communication

are

Remote database
as well as overall

routed

overhead

can

to their

preferred

be minimized.

in exclusively
routing
transactions
to their preferred
the advantages
offered by load sharing
in a distributed

The

system lies in
complex.
On

the other hand, because of the additional
overhead
associated
with remote
database
requests,
routing
an incoming
transaction
to the system with the
lightest
load may provide
worse performance
than routing
the transaction
to
its “preferred”

1 By locally
the entire

system.

distributed
system

Thus

we mean

is located

in the
ACM

transaction

so close together
same

machine

Transactions

routing

that
room

strategies

communication
or the

on Database

need to strike

delay

a

is negligible,

e.g,

16, No. 3, September

1991.

same facility.

Systems,

Vol

478

P. S. ‘tu et al,

.

Front-end
System
a

0

I

Interconnection Network

a

9
Processing
System 1

.

.

.

.

I

.

balance

The configuration

between

sharing

remote requests.
It is important

to

Partitioned Dotaboses

DBN

of a distributed

robust

a routing

scheme
and

cannot

rely

system

systems

routing

certain
degree of inaccurate
information
ance. From a practical
standpoint,
this
transaction

transaction

the load among

seek

...
(rrl
?

Dal

1,

Processing
System N

1

. . .

Fig.

I

.

systems.

and reducing

mechanisms

the number

that

can

supplied
In

with

perfect

addition

to

of

tolerate

and still optimize
system
issue is extremely
important

on being

characteristics.

processing

knowledge

reference

a

performbecause
of

locality,

other information
that needs to be considered
in making
routing
decisions
include
the average
number
of database
calls per transaction,
its variance,
communication
overhead,
and the average
amount
of (CPU or IO) work
incurred
by a database
call. This information
may be given by experienced
system

programmers

and

database

administrators,

or they

may

be collected

through
on-line
measurement.
Given the complexity
of the information,
the
cost of collecting
it, and the dynamic
nature
of transaction
environments,
the
parameinformation
that is provided
can be far from accurate.
For example,
ters such as the database
reference
locality
distribution
may vary over time
as the transaction

mix

or the content

parameters
such as communication
practice,
although
rough estimates

of the database

is changed,

overhead
may be workload
can be supplied
by system

and system
dependent.
In
programmers

or transaction
developers
in production
environments,
a practical
routing
scheme cannot be overly sensitive
to the accuracy of the estimates.
In this paper we examine
the robustness
of a set of dynamic
transaction
routing
strategies
that
are designed
for a locally
distributed
database
ACM

‘rransact,ons

on Database

Systems,

Vol

16, No. 3, Septembsr

1991.

On Robust Transaction Routing and Load Sharing
environment.

The

of the routing
system

time

system

load
that

The

as an
and

measure

factor

issue

more

and

system

not

be accurate.
the

characteristics

effect

Three

dynamic

CPU

rough

the

of both
decisions,

and

is the

response

each incoming
strategy

always

guaranteed
domly
that

transaction

response

time
routes

each

to minimize

routes

to the processing
based on steady
transaction

on the

that

its

preferred

overhead.

routed

minimize
and

strategy

systems,

its

system

system,

The random

to each processing

of
with

[29], a
routes

The preferred

to one of the processing

of transactions

would

to

quality

and compared

analysis.

to

communication

each transaction

the number

system

state

time

analysis

the minimum
response
time (MRT)
strategy
proposed
by Yu et al.
preferred
system strategy,
and a random
strategy.
The MRT strategy
estimated

very

of’ transac-

regression

estimation

and

transaction

estimates

are proposed

quality

impact

calculated

and

time

strategies

the

routing

utilization,

simulation

response

the

making

if only

available,

both

of inaccurate

decisions.

than

are

use

reflects

479

transaction

use (estimated)

When

However,

critically

concerning

that

affinity.

general

We

is how

strategies

measure
site

we need to optimize.

will

routing

integrated

is much

here

on assumptions

dynamic

transaction

tion

identify

investigated

depends

characteristics.

response
this

main

decisions

.

is

ran-

and ensures

system

is balanced.

We find that, although
the MRT strategy
performs
better
than the random
and preferred
system strategies,
because it pursues
an individual
optimization strategy
it tends to overstate
the benefits
of nonpreferred
system routing. Moreover,
MRT
is sensitive
to both the assumed
reference
locality
distribution
and the assumed
communication
overhead
of remote
database
requests.

In

The

strategy

first

contrast,

improvement
strategy
to

when

applies

a nonpreferred

The third

advantage
actual
routing

routing

to

they

decision

time.

attempts
It tracks

making

of the readily
control

response
decision

to directly

decision;

inaccuracies

the

available

information

MRT
about

first

time
conforms
to the response
process, and then systematically

Chow and Kohler
at any processing

common

in making

about

either

in the estimation
estimation

strategy
actual

response

the

that

does not take
degree

time.

A

to which

time estimate
used in the
compensates
for deviations

from the estimate.
All three strategies
greatly
enhance
routing
strategy
with respect to inaccurate
information
load and system parameters.
Most previous
and Morris
[26],

time
second

This heuristic
optimization.

time

analyzes

A

the risk

selective

of the response

the

load.

system routing.
than individual,

model

is used that

more

problems.

response

long transactions

the

seek to reduce

a routing

mechanism

routing

share

by being

the accuracy

these

is considered;

against

attempting

have

on estimated

routing

discriminates

the two strategies:

system

was used when
feedback

system

when

do not

criterion

or the candidate
for nonpreferred
a concern to pursue social, rather

strategy

of response

strategies

a threshold

that

system

underlies

the condition
also reflects

proposed

nonpreferred

a policy

nonpreferred

heuristic

the

imposes

the robustness
about critical

studies of dynamic
load sharing
approaches,
Eager et al. [12], Agrawala
et al. [1], Carey

of the
work-

such as Wang
et al. [5], and

[7], assume that incoming
tasks can be completely
serviced
system. This implies
either that requested
resources,
such
ACM

Transactions

on Database

Systems,

Vol.

16, No. 3, September

1991,

P. S. Yu et al

480

0

as the

database

systems,
ing

or

or that

model,

files,

all tasks

of

course,

are

replicated

are purely
these

or

shared

computational.

assumptions

among

Under

are

all

processing

the data partition-

invalid.

Carey

and

Lu

[6]

considered
an inte~ated
approach
to query processing
and load balancing
for
locally distributed
databases.
A heuristic
algorithm
is presented
that tries to
minimize

an “unbalance

factor”

defined

of CPU-bound

number

of I/O-bound

locality

distribution

or

transaction

site

schemes

to minimize

the

estimated

response

demonstrated
system

that,

optimal

edge

Zhou

and

determining
queue

the

length

that,

in

That

is,

directly

the

they

number

based
are

affinity

into

The

Section

4,

we
of

compare

the

Section

5 three
are
We

that

the

try

the

actual

the

are

that

system,

through
the

in

al.

[29]

does

not

does

not

database

regardless

of where

transaction

a

locally

3, the
is

make

site

are

distributed

MRT

studied
MRT

strategy

is

detail.

In

in

sensitive

analysis

estimated

and

Section

for

system.

make

take

in

to be

strategies.

compared

is

response

designed

or random

simulation

results

system

Regression
the

proposed

et

length

given

Section

that

with

Yu

preferred

not

of

index

to MRT.

in

values.

samples

to its

inaccuracy

time

preferred

do

[14]
mean-

of sampling

by

queue

transactions

causes

take

approach

a

on

a load

shown

distribution,

2. Then,

parameter

response

evaluated

of

[32]

which
Zhou

based

transactions

that

information

identify

MRT,

at

inferior

model

strategies

the

queued

locality

Section
to

to

respect

Strategies

the

model’s

with

because

and

calculate
is

on knowl-

approach

index

sampled

load

performance

in

summarize

2. MODEL
For

have

robustness

accuracy

either

system’s
reference

presents

Its

transactions

queued.

environment

described.

ances

their

account

paper

database

gies.

of

currently

environment,

performance

depend

load

It

It

to the

Ferrari

This

approach.

sharing

information,

static

Zhou

commands.

MRT

load

state

[251.

to

of the

of reference

transactions.

not

the

by

length

heterogeneity

to the
on

performed

database

does

oriented

queue

the

class

as compared

to

of the

variance

designing

dynamic

MRT

variance

issue

of incoming

optimization
time

the

the

in

time

contrast

of UNIX

from

correspond

requests

for

smoothed

placement

differs

transaction

in

experiments

a distributed

capture

rates

use

affinity

Also,

rates,

of the
plus

considered

improved

a response

The

length

[29]

sum

sites

available

strategy.

arrival

equations.

queue

et al.

be greatly

routing

proposed

distributed

readily

arrival

the

[32]

Yu

using

can

static

upon

value

than

by

of transaction

depends

to the

queries.

performance

of the

and

queries

to be the

number

the

the

time.
more

Their

to

to
used

robust
perform-

other

strate-

6.

DESCRIPTION

environment

facilitates
the

data.

In

these

considerations

tion

facility

[10],

or a file.

ACM

TransactIons

depicted

the
case

of

[81. A remote

on

can

Database

Figure

1, a mechanism

of transactions

IBM’s
led

CICS

in

execution

Customer

to

the

Information

introduction

request
determine
Systems,

which

can

Vol

16, No

its

access

table

a DL/1,

1991

to

System

Intersystem

definitions

3, September

to be provided

access

Control

of an

either

from

needs
need

nonlocal
(CICS),

Communicaa DB2
that

the

database
database

to
In

On Robust Transaction Routing and Load Sharing
or file

being

accessed

other

CICS

for

CICS,

a CICS-provided

receive

the

is located

execution.

and

handled

in

the

Variations

to

horizontal

from

respond
same
this

throughput
environments.

access

to

The

all

values
locally

to the

system

by

consists

Figure

1.

DB,

where

DB,

are

speed

is

tions

of each

distribution
of

processing
assumed

for

processing

by

there

be

are

demands

each

accessed

1/0

with

access

completed,
partition

The matrix

is

dk.

and

Hence,

specific

between

two

here

imply

are

different

studied

here

has

following.

The

i = 1,2, . . . . N,
as shown

of

11111,

database

requests

P,. The

system

in

DB~,

...,

to

processor

the

(1)

(3)
of

the

service
and

and

end

issuing

of database

number

processing

at

of

a database

of

de-

reference

1/0

each

service

application
request

transaction

number

transac-

processing

(2) the

front-end

a transaction

a number

the

the

the

incoming

system,

with

of terminating
the

through

to

processing

database

calls

issued

a

are
by

a

distributed.
classes
For

in the

the

process

k-th

with

Both

ak and

probability

by

probability

[ pk, /(1 – pko)] shows

and

denote

arrive

The

mean

and

database

segments

a database

p~,, or may

requests.

device

service

time

processing
with

of database

ACM Transactions on Database Systems, Vol

txh

of

by measuring

database

request

terminate

distribution

service

requests

an 1/0

the mean

a class

according

processing

and
that

of an application

the

txk

bk can be estimated

and that

issue

let

transactions

txk we assume

p~”,

execution

txh may

&.

segment

processing

issued
the

system

class,
rate

processing

request

DBZ with

The
high

some

the

consists

processing

Also,

probabilities

When

for

considered

system

by

and

fixed

1, , . . . K.

transactions

[41.

support

shipping–provid-

P,. All

segment,

partition

a fixed

is

region.

Borr

interconnect

is interleaved

request.

application

database

[15]

to route

assigned

requests,

are ah and bk, respectively.
of application
the pathlengths
For

the

characterized

Poisson

of the

the
that

transaction

to a time-invariant

CICS

to

P, where

that

enter

processing

class.

K

and

in

speed

is employed

At

database

k =

[3]

system

systems

a high

the

users

strategy

is geometrically

k transaction,

the

p,.

database

each

by

database

process

database

transaction
Let

by

segment,

particular

execute

trade-off

summarized

system,

application

each

is

processing

systems.

of

demands

the

simply

processing
and

by

Transactions

mand

[30]

processing

an application

requests.

to

transaction

in

model

to the

a routing

to processing

and

and

mirror

request

variations

be handled

submitted

where

invokes

al.

connected

P, is denoted

of

vs IO

the

a partitioned

to

Transactions
system

et

is attached

assumed

the

scheme

transaction

Yu

system,

There

Chou

remote

model.

of N transaction

a front-end

and

to the

the

is invoked

structure

analyzed

shipping

The

The

Bartlett

Horst

request
by

interpret

of system

[30]

variations:

distributed

described

by

et al.
call

these

by

the

transaction,

transaction

type

studied

data.

of

received

CICS,

presented

Yu

remote

to

parameter

and

is

ships

is

system.

other

of this

approaches—database

applicable

been

any

are

potential

application
ing

as

and

a mirror

requesting

way

transaction

called
requesting

the

scheme

growth

different

the

to

CICS

a request

transaction,

request

request,

on another

Whenever

481

.

to

will

be

of each

segment
the

probability
calls

is

database
issued

p~o.
by

16, No. 3, September 1991

482
txk

P S Yu et al.

.
and

is

referred

transaction
is

to

as

txk, we call

the

maximum

the

MRT

can

of

be

routing,
routing
With

to

the

time

above

system,

break

S,

is

we

to

communication

achieved

the

equal

to

with

the

all

number

measure

to

of the

us

processing

to

by
of the

processor.

and

thus

executed

at

request

input

2 Note

that

referred

the

canned
trace

analysis.

transaction.
tion

class,

accesses
class.
issued

Similarly,

from

the

DL/I

number

on the

database
system

for DB2,

a class
(For

the

on Database

requests.

IIowuse

arrival
routing

this

rates.

strategies

is associated

utilization

the

which

therefore

different

and

affect

utilization

We

be
speed

system,

to remote

based

with

the

on process-

more

‘kpk’bk

-f
k=l

the

characteristics
to

or DB2,

behavior

of each

trace

can

provide

thus

3 performance
details

transaction

class

we
trace

can
the

on each

then

obtain

can provide

Systems,

Vol

has

request

an identifier,
to invoke,

can be obtained
DL/I

can be obtained

preferred

database

the

program

the

system
information

call

3, September

1991

For

through

issued

by a

for each transac
average

for each

number

of

transaction

on each

see Yu et al, [28] and Yu et. al. [31] for lMS

16, No

is

being

the

After

message

application

information

identify

i # j,
PI.

to

transaction

which

and

a transaction

where

P,

to each database

partitioning,
and

DBJ,

each

Indicates

transactions

When

system

class, that

of accesses

of

decisions.

request

IMS

(2.2)

pkO

)

Transactions

CPU

of transaction

of

processing

like

processing

by a transaction

respectively
ACM

Based

the

would

with

CPU

S, a portion

routing

a database

execution

average

a set

denote

= ~

upon

systems

the

In IMS,

to each

by

S is a lower

strategies

total
due

performance

transaction

as the transaction

The

of
whole

P, as follows:

be shipped

transactions,

the

that

distributed

strategy.

given

We

utilization,

utilization

Routing

incurred

utilization

requests.

depends

in database

to here

in
divided

uniprocessor

the

routing

load

Of this

P, issues

must

at

(2.1)

original

affect

of the

at

of the

one

single

W,
only

during
time

‘pm)bk).

in this

speeds

compare

requests

S,b

ph,

utilization

requests
demands

the

GPU

the

of database

independent

if

selection

processor

service

to the

s:

Again,

given

maximum,

response

database

included

overhead

S value.

database

system

average

and

represents

replaced

requests,

indicate

a given

see the

is not

is independent

allows

under

unique

estimated

incoming

and

communication

S itself

no

a

system2

P,

were

attached

the

ever,

ing

sum

of remote

includes

This

system

disks

is

preferred

,$+ja’+(1

utilization,

the

can

total

overhead

actual
if

on

there

minimum

For

preferred

power:

z

on

(If

processes

s=+

Since

tie

distribution.

P, its

)

processing

bound

, N.

the

application

equal

locality
system

.

the
with

definitions,

transaction

aggregate

reference

processing

J“ = 1,2,

system

is chosen.

the

serving

Pkl,

used

i.e.,

the

SQL

call

and DB2,

On Robust Transaction Routing and Load Sharing
0: opplicotion
1: local

requests

0’.’: Communication

PI

overheod

service

remote

483

process

dotobose

c receiving

.

-DBI

ond

dotobose

requests

Front-end

system

DC

>

w
P3-

3

0

~...-------------

Tco-r
..............
submodel
for
dotobose
requests

rrv

{i & r)
Fig.

gets processed,
call

in which

The

total

both

of

the

service
the
c.

service

the

system

infinite

server
of

the

assumption

is not

aggregate

length,

and

except

for

routing

the

3. RESPONSE
We
first

now

estimates

[16,

to

this

Figure

calls

as

database

receiving
the

services.

and

results

at

communication

additional
2.

processor
of each

at

to

communication

widely
as

Note

that

front-end

aggregate
we

and

the

model

the

simulation

mean

completely

processing
can

that

all

exponential

successfully

time,
is

On
as an

representa-

assume

validated

response

to the

system,

or

system.

is modeled

Although

mean

processor

processing

simplicity,

used

such

17].

single-server

system

a global

For
distributed.

been

A

each

processing

of transactions

the

system.
be

in
queue

defined
Given

performed

a
to

performance.

the
the

a remote

and

remote

referred

subsystem.

it has

TIME BASED

examine

in
the

measures

system

is

exponentially

exact,

in

is called

sending

receiving

corresponds

1/0

assignment

strategy

measure

This

utilization
the

This

system,

subsystem

are

estimating

and

illustrated

to model

complex

demands

back.

c/2.

is

1/0

model

to perform

processing

queue.

a more

service

be sent

systems

each

is used

hand,

System

of sending

is thus

model

queue

other

tion

For

2.

Pj have

communicating

overhead,
The

will

demand

processing

sharing

result

P, and

both

I

MRT

average

DYNAMIC
strategy
queue

ROUTING
proposed

length

by

of each

STRATEGY
YU

et al.

processing

[29].

The

system

strategy

P,. Then,

ACM Transactions on Database Systems, Vol. 16, No. 3, September 1991.

484

P S Yu et al.

.

the

expected

response

processing
that

provides

the

transaction.

MRT

times

P,, for

system
the

the

and

incoming

1, . . . . N,

minimum
In

strategy

of an

i. =

expected

following,

then

its

if it

estimated.

response

we

examine

transaction,

are

The

time

describe

is

the

sensitivity

were

routed

processing
chosen

to

execute

implementation

to

the

to

system

of

assumed

the

transaction

characteristics.

Description

3.1
Under

the

actions
Each

MRT

are

column

new

arrival
entry
to

of the
and
in the

the

the

table

the

the

at each

expected

approximated

be

time

of the

to

the

system.
time

a

the

in

sampling

of

no

is required.
expected

response

depends

arrivals.

on the

For

is

upon

efficient

applied

to
and

mean-value

transaction

by

overhead

characteristics

Based

for

the

is decremented

analysis

transaction

the

is completed,

systems

future

and

to reflect

transaction

a steady-state

processing

one

that

estimate

table.

row

negligible

and

incoming

and

k-th

table

is

processing

used

using

response

of the

there

the

system

time

in the
by

trans-

history

a transaction

system,

of an

front-end,

response

column

front-end

can

entry

when

executing

a routing

is incremented

that

from

response

at
mean

the

currently
in

P,, the

and

Note

information

behavior

length

[21],

at

history

implementation

queue

table

to
table

row

for

system

Furthermore,

departure.

expected

transient

history

routing.

state

routing

decisions

front-end

corresponding

the

The

mate

routing

the

instantaneous
The

routing

the

txk is routed

its

reflect

maintaining

time.

the
by

a transaction

i-th

one

strategy,

maintained

time

the

of The MRT Strategy

routed

estimean

equations

P,

to

can

be

as

(3.1)

L,

where

is

the

transaction,
ters

appear

in

parameters

pkj)

and
known
one

3.2
In

(c

are

variable

in

order

queue

above
v,),

or

formula,
from

provided
the

referred

length

. . . . L~).

by

above

at
Note
they

transaction
the

at

although

are

is

residence

the

quite

derived

from

characteristics

time

of

parame-

either

profile.

L. In the

arrival
a few

( ak,

staticatransaction

equation

to as the

P, observed
that

Appendix,

approach

we

by

system

bh,
The

Yu

the

db,

p~”,

only

un-

summarize

et al.

[29],

to

L,.

Robustness

simulation

the
and

that

approach,

estimate

ryean

L = ( Ll,

and

of the MRT Strategy

to compare
for

the

the
model

effects

of different

illustrated

in

routing
Figure

strategies,
2.

In

the

ACM Transactions on Database Systems, Vol. 16, No 3, September 1991.

we
simulation

developed
studies

a

On Robust Transaction Routing and Load Sharing
Table

I.

The Reference

Locality

Distribution

.

485

Usedin

Simulations

Database

reported

here,

transaction
into

three

This

choice

enables

gies

us

that

for

holds

and

load

robustness

to information

rations

containing

degree

of

were

is the

difference

of no dynamic
of

load

optimal

We

transactions

having

transaction
in

this

class,
class

load

not

identical
load

can

communication

sharing).

decided

be

issues

can

routed

overhead.

sharing

reference

sharing

in

to

the

any

In

other

which

the

system

the

factor

of

with

all

efficient

routing

locality

case

to the

case

larger

as

K

class

of

With

since

such

a

transactions

penalty

problem

server

to the

another

systems.

no

configua similar

of

achieved

with

words,

of refer-

with

becomes

to

easily

good

homogeneous

addition

locality

be more

load

a

a strategy’s

(corresponding

difference

consider

less

the

the

that

(corresponding

system

This

to

that

of
in

systems

share

cases

found

strate-

configura-

locality

examined

problems

a M/M/K

to

This
confi-

various

us to study

we

analogy

M/M/l

and

load

also

the
An

K

sharing),

dynamic

increases.

locality,

dramatized.

between

we

of systems,

good

point

between

enables

preferred
[291.

system

the

balance

When

number

of the

A three

three

grouped

et al.

obtain

systems

also

inaccuracy.

fact

formulation

a proper

of

are

Yu

to

demonstrate

configuration

system

in

to

consist

a different
by

needed

(nonpreferred)

strike

a larger

preferred

strategies
case

to

has

of systems.

suffices

This

class

the

to

transactions

considered

Certainly,

alternative

needs

assumed

time

number

0.14
0.11
0.83

the

transaction

simulation

any

class,

sharing.

is

= 3) where

configuration

the

for

two

strategy

ence

Each

keep

transaction

routing

(N

to the

to

provides

each

= 3).

manageable.

considered

tion

(K

23
0.11
0.82
0.06

environment

systems

is similar

intervals

class 1
class 2
class 3

distributed

classes

system.

dence

the

processing

1
0.75
0.07
0.11

Partition

Transaction
Transaction
Transaction

of

additional

reduces

to

of

reference

on the

work

simpler
does

not

appear.
The

choice

Cornell

et

on-line

of a sample
al.

[9],

planning

to determine

Cornell

et al.’s

allocation

subject

to

partitions,
that
The

[91 paper

the

Table

results
of

L

we

30].

and

a parts

are

to minimize
routing
show
calls

the

the

and
the
is

papers,

database

local

denoted

of remote

are

distribution

In

partitions.
calls,

For

three

system

deterministic
where

an

analyzed

database

of a preferred

as [ q~,l,

by

from

databases.

constraints.

under

locality

system

to different

sharing

reported

traces

to physical

number

existence
be

reference

study. The distribution
corresponds
derived
from the trace under
the

these

allocated

load

will

distributed
ShOW

In

patterns

databases

clearly

is based

[28,

reference

these

database

locality

locality

al.

system

is optimized

reference
ln

et

transaction

deterministic

65%-85%

~kO).

Yu

database

in order
The

reference

and

such

routing.

q~i = Pk, /(1
USed

for

–

this

to the results of Cornell
et al. [91 that are
optimal
database
partitioning
described

above.
ACM

Transactions

on Database

Systems,

Vol.

16, No. 3, September

1991.

486

P. S Yu et al

.
Table

II

The Response

Times

Under

Various

S=
Geometric

Distr.

Constant
Bernoulli

Based

Distr.

on

database

the

p~o = 0.0625,
9K

incurred

due

represent

overhead,

access

time,

database

dk,

load
cated
the

and

following

s~’s

are

specified

be obtained
now

model

described

have

MRT
based
for

ones

mean
different
mined

based

3 With

the
from

operating

on

addition
0.80

points

Transactions

40

linear

run

and

database

calls
Table

database

load.

S =

or

.071

on Database

The
and

on the
strategy

Systems,

Vol.

12.5

transaction
time

S$ = S:

value

the
c

= S$.

These

is of significance
16, No, 3, September

The

average
utdizations
only
1991

when

the

is not

of database

calls

case,

a Bernoulli

have

a short

run

percentage

of

so that

is

MRT
rates

of

for

15,

percent,

systems

resulting

number

but

class

arrival

the

transactions

The

under

processing

transaction

overhead,

depending

as the load sharing

II.

0.813

of communication
to O 94,

each

of response
The

either
calls.

is

the

is

second

can

ones

to

simulations

number

the

A,s

transaction

actual

calls

64 database

of long

for

the
In

class

with

that

results
in

15.

the

that

conducted

case,

to

of

= s:),

relative
In

of database

one

as indiratios

(2.2).

However,

We

a

processing
are

= s:

strategy

assumed

a transaction

or a long

and

IO

classes,

relative
s!

information.

number

The
during

total

the

to

communi-

disk

calls

case,

MRT

relative

transaction
the

and

(2.1)

the

implicitly

In

all

balanced

distribution.

equal

for

so that

the

15K

the
low

a

of database

system
is

for

and

overhead.

(S!)

of

mean

which

0.7

21K

overhead

3K

0.05

have

bk, are

denote

accessing

equations

distribution.
in

of

S is given

for

be

adjusted

loads

it

to
We

of
i.e.,

processors

communication

v equals

and

are

the

communication

ms

number
classes,

ak and

chosen

p~”,

is a constant

performance

same

range

ACM

the

distributions
the

high

and

percent,

of

q. Thus

When

2,

is

~sK

average

that

overheads.

distribution.

which

is 87.5

the

c,

by

a geometric

calls

number

call,

robustness

Section

assume

additional

A,s,

workload

the

transactions

pathlengths,

for

be

the

the

is used

we

all

The

instance,

solving

transaction

present
have

in

8 database

short

to

that

for

the

probability,

a nongeometric

distribution
with

0.25

rates,

(for

on a geometric

each

a + b)),

the

follows

strategy

15

that

processing

in

assume

is

examples.

by

calls

we

and

equals

about

database
may

c/(2(

examine

assumptions

281,

communication

arrival

the

We

MIPS

individual

in

can

0.972

assumed

The

(S)

0.993

0.709

database

and

are

respectively.

0.708

0.711

high

and

call,

0.711

15K

1,2,3. In addition,

cost,

cation

0.617
0.619

remote

and

communication

0.986

respectively.

to

low

c=

0.700

[9,

Calls

S=0.81

0.707

0.618

of 7.5

instructions,

of Database

c=3K

c=

transaction

k =

speed

on Number

O.71

c=3K

trace

per

for

an identical
and

same

requests

Distributions

still
for

15.
the

assumed

are

then

processor
represent
utilization

We

three

are
results

the

to

deter-

show

that

utilizations
interesting
is high

[29],

On Robwst Transaction Routing and Load Sharing
Table

III.

The False

Reference

Locality

case 2

1

EE

i

Ill

8

!:1!

3

1
060
0:15
0.19

3
7

i~i%

487

Distributions

case 1

Database Partltlon

.

1:9!

2
0:19
0.67

018
0:66
0.14

N

-’

(wzLLtY)

0

-’

a
o

Q

o

I

I

I
04

I

I

1

the

MRT

strategy

database
Next

due to tronsoction

we

is not

out

performs

examine

the

exact
for

partition

DBZ.

tion

is much

class

estimates

the

completely

measure,

equally

well

(S)

processing

of MRTstrategy

in

same

preferred
1

has

preferred
based

case

locality

that

However,

to locality

with

distribution.

these

of

75

assumed

three

distributions

of

the

Simulations

are

locality

distributions,
decisions.

system

as the

actual

preferred

locality.
actual
ACM

However,
locality

calls,

preferred

of

two

on Database

distributo figure

to

calls

in

give

for

with

III,

Systems,

in
Vol

transac-

inaccurate
are

used

by

maintain
different

case

continue
shown

database

each
two

Table

while

a precise

access

distributions
but

locality

locality
difficult

in which

shown
The

distribution

i.e.,
system

performed

transactions

Transactions

be quite

database

distribution

system

reference

it may

database

percent

routing

stronger
on the

the
practice,

identifying

easier.

making

In

distribution

instance,

system

where

accurate.

of reference

router

case
calls

Sensitivity

J

0.8

calls.

tion

the

3.

1

t

!
0.7

0.6

Utilization
Fig,

t

I

0.5

to

Table
16, No

2

the

intensity:
has

weaker

issue

database

1. The

response

3, September

1991.

488

.

times

P. S

are

plotted

is assumed

load

a strong

locality

routings

are

assumed

distribution

response

time

q.
are

high

In

alleviate

this

Section

a weak

is

required

fewer

This

to

the

estimation.

MRT

is similarly

such

as

q

assumed

load

as

is

similar

behavior
5, we

is

develop

the

and

locality

the

case

examine

that

to assump-

router,

performance

routings

than

of assuming

various

the

distribution

communication

nonpreferred

to

With
system

sensitive

the

high

done.

shows

relative

by

more

for

are

nonpreferred

improved.

sensitive

distribution

degradation

than

assumed,

time
is

locality

performance

parameters
smaller

This

1)

response

transaction

done.

locality.

a

When

routings

strategy

system

When

for

15K.
notice

(case

the

MRT

the

overhead,
degrades

and

c =

2), we

nonpreferred

the

about

3 for
(case

as more

of
in

Figure

router

done

performance

quired

in

by the

transaction

tions

Yu et al.

re-

a weaker

enhancements

that

problem.

4. ACCURACY

OF RESPONSE

In the previous
section,
some model parameters.

TIME ESTIMATE

AND ITS EFFECT

we observed that MRT is sensitive
to the accuracy of
We now take a closer look at the response
time

estimation
to understand
the underlying
reasons for this behavior.
This will
provide
guidance
for enhancements
to the basic strategy,
that will be discussed in Section
5. The fundamental
assumption
of the MRT
that, over the course of a transaction,
the system is in a steady
state is estimated
by viewing
the number
of running
transactions,
by the routing
history
table,
as a fixed
population
in
network.
This allows the router
to calculate
an estimated
routing
the transaction
to each processor;
the front-end
processor

with

minimum

analysis

of the

system

correct.

The

Because
routing

system

load

of database
decision

estimated
suggests
does

this

locality

than

time.

over
and

a local

a closed queueing
response time for
then routes to the

As we shall

critical

fluctuate

reference

costs more

response

that

assumption
the

course

see, detailed
is not entirely

of a transaction.

communication

decision,

strategy
is
state. This
indicated

cost,

a remote

and the presumed

response

time improvement
implied
by minimizing
estimated
response time may be an
overestimate
if the nonpreferred
system subsequently
receives more transactions to process. This is a problem
of individual
optimization—the
optimal
decision for the newly arrived
transaction
can compromise
the optimality
of
the assignment
for transactions
already
in the system. We first compare the
estimated
response time with actual response time in Section 4.1 and then
examine
the effect of its accuracy
on load sharing
in Section 4.2.

4.1

Regression

In the
response
For

our

response

actual
response

MRT

Analysis

strategy,

times

the

given
we

time:

namely,

time

time
whose

shall

Response

estimated

that

system,

response

of Actual

the

response

transaction

hypothesize

Time
times

are

is routed

a general

model

R~’ = a k + (3~R~’ + fl~ Xk’
of the
calculation

i-th

class

to

in

the

Section

ACM Transactions on Database Systems, Vol. 16, No 3, September 1991.

as the

specified

to describe

where

k transaction,

is described

regarded

actual
system.

the

actual

R: L represents
R$ 2 is its

3.1,

and

the

estimated
Xk L =

1 if a

On Robust Transaction Routing and Load Sharing
Table IV.

Regression

Output

for Unbalanced

Coefficients

.

489

Case

of the New Model

A

~k

S=

O.71c=3K

S=0.71

S=

C= 15K

O.81C=3K

S=

O.81C=15K

Confidence

intends

*

hypothesis

-- MRT

* * -. Cofidence

nonpreferred
wise).
that

is,

the

departures
ter,

first

decision.

The
f?~=land

We

have

times,
tion
are

shown

normalized

0.48 f

0.05 * 0.04

0.16

0.67 + 0.16

Txl

0.27 f

0.07

Tx2

0.33 * 0.09

0.65 f

0.02 + 0.04 *
0.11 ~ 0.06

0.07

0.58 + 0.09

0.03 * 0.04 *

0.23 f

0.72 ~ 0.10

0.02 + 0.04 *

Tx 1

0.34 * 0.12

0.60 f

0.12

0.02 ~ 0.05 *

Tx2

0.25 f

0.07

0.68 + 0.07

0.02 * 0.03 *

Tx3

0.28 f

0.08

0.66 f

0.02 f

Txl

0.10 f

0.04

0.82 ~ 0.05

0.14 * 0.07

Tx2

0.15 * 0.05

0.82 t

0.04

0.01 * 0.03 *

Tx3

0.10 + 0.05

0.86 + 0.04

0.00 + 0.03 *

0.10

0.08

0.03 ‘

otherwise

is taken

decision

a k, is adopted
the

over

MRT

the

degree

times.

for that
capture

to

steady-state

of a transaction.

of relative

correspondence

third

utilization
under

the

parameter,
due

to

MRT

transaction

the

(O other-

system

fluctuation;

approximation

course

The

system

~~,

due

The

second

between

actual

is intended

a nonpreferred

strategy

is

to

to

special

and

show

system

a

the

paramethe

routing
case

with

f3j=Oforallk.

performed

regression

with
The

0.15

Tx3

from

the

values

in Tables
&k and

IV
fi~

= 0.5:1:1.

analysis

MRT-based
confidence

and

V.

Instead

are

of actual

routing

and

which

The unbalanced
case
database
requests
at
S!: Sj:Sj

0.15

0.26 f

hypothesis

together
results.

0.43 f

Tx3

routing

the

of increased

O!k =0,

Tx2

arrivals

response

impact

0.04 + 0.06 *

on ~~ cannot be rejected at the gs~. level
interval is at 95 ‘/o level

variation

(3I, represents

estimated

0.47 * 0.25

parameter,

and

~~

0.46 ~ 0.25

are at 99% level unless indicated

system

The

,M

Txl

the

intervals

of

u k and

a k and

p;

level,4

response

from

regression

is one in which
the utilization
at the other
PI is half of that

At the 99% confidence

estimated
derived

of the

of presenting
ratios

and

decisions

simula-

coefficients

/3~, we

show

the

R ~‘.
based on processing
processors.
That
is,
to the

average

in all cases we find

that

the

4 Response time of individual

transactions
are not, of course, independently
distributed.
The
estimates for the coefficients are unbiased. Because of the serial correlation the true confidence
intervals will be somewhat larger than those shown in Tables IV and V. However, we can ignore
this for practical purposes because more then 40,000 transactions
were used in the regressions.
Even grouping the dependent transactions
together leaves the confidence intervals unchanged.
ACM

Transactions

on Database

Systems,

Vol.

16, No. 3, September

1991.

490

.

P. S. YU et al

Table

V,

Regression

Output

for Balanced

Coefficients

of the New

1’
S=

O.71C=3K

S=

O.71 c=

S=

15K

*

hypothesis
coefficients

0.52 f

0.14

0.55 * 0.14

0.04 ~ 0.03

Tx 1

0.31 f

0.08

0.61 + 0.08

0.07 f

0.13

0.30 + 0.08

0.64 f

0.09

0.05 + 0.04

0.27 f

0.66 f

0.09

0.07 * 0.04

Tx 1

0.14 + 0.07

0.09

0.78 + 0.07

0.04 f

0.03
0.03 *

Tx2

0.20 ~ 0.08

0.73 * 0.07

0.02 f

Tx3

0.10 ~ 0.08

0.84 f

0.03 * 0.02 **

0.08

0.08 t O04

0.87 ~ 0.04

0.05 * 0.04

Tx2

0.17 f

0.05

0.78 i

0.05

0.05 + 0.04

Tx3

0.09 f

0.06

0.84 + 0.06

0.08 + 0.05

1

are at 997. level unless

on

transaction

unbalance.

response

time

This
is

0.04

Tx2

indicated

othenvise

at the 95!/.

level

be rejected.
a k – () and (3; = 1 should
response time,
(3;, vary
between

of

0.02 +*

Tx3

We

on estimated

depending
system

0.03 f

0.41 + 0.13

-- MRT
hypothesis
on ~j cannot
be rejected
-- Contldence
interval
is at 95 Y. level

**

0.01 + 0.03 *

0.37 f

Tx

intervals

il

0.53 ~ 0.13

Tx3

O.81C=15K

Confidence

0.13

Tx2

O.81C=3K

S=

Model

Pf

0.40 f

Tx]

Case

class,

processing

indicates

that

always

equal

to

~~,

is less

load,
MRT’s

actual

that

note

47

87 percent

that

estimated

communication

cost,

assumption

response

time

the

and

is

only

a

and
rough

approximation.
The

role

response
and

a

nonpreferred

modeled

communication

incurs
system;

this

was

smaller

a 99

percent

hypothesis

that

In

analyzing

system

load

Conversely,

steady

state

the

predicted
In

in

time

when

MRT.

routed
Some

high

we

load

class

1

to a nonpreferred

cases

have

statistically

cases

actual

the

transaction

nevertheless

other

affecting

as in

situation,

by
are

arrivals
at

and

system

with

few

routing

table

contains

time

results

correlation

increases,

nature

system,

Transactions

response

level.

departures

cannot

on Database

many
not

(3; coeffisignificant

reject

the

entries),

Systems,

of

Vol

then

following.

MRT

a k decreases.
of

change

actual

the

the

to the
load

a single

16, No, 3, September

are
system

state

relatively

arrival
1991

or

effect

state

active

that

as

as well.

This

system

extent

is

First,

R ~‘ increases

transactions

decision,
When

the

R ~’ and

magnitude

do

routing

transactions.
few

observe

approximation
When

of the

we
between

the

of MRT’s

approximation.
the

decision
Sometimes,

unbalanced

that

regression

existing

ACM

not

routing
clear.

= O.

the

the

in

confidence
@

as load

by

the

magnitude,

increases

caused

of

increase

increase

with

system

by
case

a substantial

cients
at

of

time,

in
from

they
low

departure

is

by
that

affect
(and

a

the
a
the

has

a

On Robust Transaction Routing and Load Sharing
magnified
table

effect

and

this

the

on

variation;

valid.

~~

Therefore,

improves

increasing

system

state

captures

when

with

the

over
the

the

estimation

is

cient

is due

MRT

can

quently
after

modeled
to

the

overestimate
receives

incurring

system

the

a few
the

potential

gain.

routing

decision,
system

routing.

as communication

cost

increases.

preferred

system

routing

Let

P, be the

a

unbalance.

nonpreferred

system

munication

costs

P ~cC,~,.. arrive

the

actual

high

and

system
many

of

actual

response

P ~=C,~iO. becomes
then

the

modeled

effect

high

class

when

4.2

course

larger

and

which

P~

the behavior
particularly

routed

to

nonpreferred
MRT

database
sider

the

is

of low

communication

percent,

We

Naturally,
communication

we

system

if

(1)

preferred

system

than

predicted

decision

not

is

accurately

presented

database

in

utilization

routed
remain

apply,

Table

IV,

costs

are

P1.

of

PI

to

as

do

communication

3 is

com-

system

a nonpreferred

case

1 will

the

ratio.
where

same

then

In

MRT’s

underutilized

at

the

examine

expect

Simulations

a higher

results

overhead,

routing

is the

percentage

is

Table

over

utilization

system,

i.e.,

overhead

where

c = 15 K.

A

very

becomes
unbalanced

S:

high,

the
case

NPR

as

NPR,

the

case

under

the

= S:

to processing
= S;.

and

We

cases

NPR,

con-

of high

close

to

is

low.

Even

when

is

still

close

to

high

where

two

of transac-

due

c = 3K

overhead

strategy,

to

a balanced

processor

each

referred

VI,

communication

an

a given

first

This

In
the

where

when
overhead

next

under
The

systems.

routing

overhead

is observed

to serve.

of the

communication

communication

percent.

P~,C,~ZO. be the

The underutilization
of
temporary;
the transac-

routing

system

meaningful,

is examined

requests

cases

the
2 or

system

is valid.

nonpreferred

system

strategy

twice

of a nonof

Clearly,

conditions

when

class

that

let

that

increase

of Load Sharing

statistics
tions

of

cost

then

unbalanced

coefficients

have

predicts

In analyzing
are

@

transaction

Effectiveness

system

In the

than

extent

with

these

nonpreferred

the

that

greater

of

then,

a

to

tx i, and

substantially

subse-

tends

transactions

either

a nonpreferred

1 has

of the

be
If

a transaction

approximation
the

will

utilized.

estimation.

Pz

because

contrast,

of

by MRT’s

transaction

time

more

system
utilized,

be better

the

coeffi-

decision,

not

is routed.

of the transaction,

this

~$

on

will
involve
a substantial
penalty.
routing
decision
table is only
P ~,C,~LO~that is recorded by the routing
tion’s

in

observe

depends

transaction

(2)

over the course

we

also

the

in

more

may

by MRT’s

routing

nonpreferred

reason,

However,

preferred

variation

becomes

time

this

approximation

captured

not

inherent

response
For

decision

to which

are

If the

communication

preferred

of

the

is

~f coefficients.

a nonpreferred
and

history

a k captures
approximation

loaded,

observed

transactions

491

routing

MRT

decision

of the
making

additional

the

and

a k

the

transaction.

heavily

the

when

more

which

routing

13;. Some

that,

to

in

system

by

fact

of the

is more

change

The cost of a nonpreferred

between

course

degree

system

a consequent

variation

the

.

S~: S;: S;

50
20

= 0.5:1:1.

NPR because
PI has fewer
database requests
In the case of low
are shown in Table VII.
since

MRT

already

encourages

nonpreferred

ACM Transactions on Database Systems, Vol. 16, No. 3, September 1991.

492

P. S. Yu et al.

.

Table

VI.

The MRT

Performance

S=

for Balanced

Case

S=0.81

O.71

c=3K

C= 15K

c=3K

C= 15K

RT

0.619

0.707

0.700

0.986

IVPR

46.6%

19.4%

46.3!40

17.9~o

SH’R

78.2%

37.6’%

85.0%

43.4%

RT -- mean res nse time
NPR -- ratio o ? non-preferred
system routing
S WR -- ratio of swapping phenomena

Table

VII,

The MRT

Performance

s=

c=3K

RT

0.622

0.744

NPR

49.4%

25.9%

0.718
AQ
10/..
,. .-.

S WR

63.5~o

18.OVO

routing

balanced

even

case,

processing

Although

the

about

the

with

referred

of

response
is

routed

unbalanced
high
than

to

the

case is

communicathat
of the
underutilized

swapping

of

that
in

that
routing

to

l’ransact,ons

count
that

decision

is

count

routing

history

preferred

forces

about

load,

it

remote
It

with
the

optimal

calls.

can

also

the

does

not

More

create

a

PI

a preferred

system

next

of transac-

arrival

Systems,

by
Vol.

16, No

txl

to

been

Table

3, September

we
1991.

include

front-end

Whenever

a

PI

system

see whether
to

the

simularouting

preferred

routed
VI,

been

the

in the
occur.

with

is checked

have
in

of esti-

of process-

a nonpreferred

phenomena

Pz has
In

would

be maintained

e.g.

terms

period

observed

whenever

swapping

one.

be

(in

entire

routing

easily

made,

the

can

table
can

decisions

over

system

table

system

is incremented
on Database

the

processors.

i.e.,

history

A swapping
of times

the

something

us

share

additional

txl,

phenomenon

routing
with

the

“locally”

number

Pz,

transaction

the

conditions

preferred

the

the

on

“globally”-

checking

track

by

to

Pz, to be routed to P1. This vicious cycle is
Thus,
we are concerned
with
the
phenomenon.

swapping

nonpreferred

tells

attempt

transaction

load

The

to

ACM

in the

system

decision.
is made.

swapping

are

routing

its

placed

a sequence
time)

decision

routed

in

in

caused

after

change

suboptimal

by

does

is

that

to as the

correct

transactions

effects

a preferred

phenomenon
ing—

side

such

Pz, the

to
txz,

mated

NPR

case,

of nonpreferred
NIRT

overhead

cycle

routed
tion

balanced

I
I

69.5%

that in the balanced
case. With
is about
30-40
percent
higher
more

degree
that

communication
vicious

the

because
PI.

of work

us

tion

in

than
NPR

I

C= 15K
1 14Q

I

system

amount
tell

S=0.81

C= 15K

slightly
higher
overhead
the

Case

0.71

c=3K

system

only
tion

for Unbalanced

any

P1. If
the

true,

active
the

swapping

On Robust Transaction Routing and Load Sharing
ratios,

S WR, defined

observe

a swapping

whether

the MRT

SWRS in Table
tion

The

is making

VI

overhead.

case.

In Table

but

are

large,

VII

are

of nonpreferred

This

too many

are quite

SWRS

overhead,

as the percentage
phenomenon.

statistic

quite

high

significantly

in

lower

in

in which

nonpreferred

the

ratios

case

the

case

493

an indication

we
as to

routings.

for cases with

we show the swapping

still

routings

provides

suboptimal

especially

.

The

low communica-

for the unbalanced

of low

communication

of high

communication

overhead.
Section 4.1 shows that the response time analysis
tends to exaggerate the benefit
of nonpreferred
routing
and thus make unnecessary
nonpre ferred

routings.

balanced

This

5. PROPOSED
In this
old,

ROBUST

section,

we

4 indicate

that

routing

and
MRT

decision.

improvements
balanced

routing

the

system

for

estimates.
routing

subsequent
the

to

In
only

It

is

especially

to

that

routing

and

decisions.

for

robustness

issue

seek

possible

routings,

even

if this

the

the

in

system

of potential

class

the

first

Section

the

In

into

look

at the

cost

words,

system

a

as the

computes

an

of response
of nonpreferred

account

three

less

candidates
other

strategy

additional
cost

in

nonpreferred

approximation

this

system

is considerably

preferred

adaptive
in

the

to take

a

preferred

The

Section

performance

the

recognizes

bias

threshin

results

strategy,

transactions.

quantify

strategy:

discussed

of a nonpreferred

short

the

We’ll

cost

threshold

decision.

to

MRT

MRT

results

therefore

strategy,

limited

attempts

under

the

when

to eliminate

when

strategies

making
and

then

5.4.

Strategy

Threshold
approaches
sharing
in distributed
motely.

nonpreferred

is taken

the

routing

Threshold

threshold

SWR,

to the
The
the

strategies

discriminatory

approach

compensation

alternatives

systems.

routing

on-line

5.1

high

strategies.

two

reducing

default

examine

in

to underestimate

is considered
In

conservative

system

three

first

among

nonpreferred

time

result

STRATEGIES

adaptive

tends

The
by

load

overloaded.
for

to

ROUTING

consider

discriminatory,

system

is likely

case.

have been considered
by Eager
systems with
identical
nodes.

determine

Specifically,

whether

a task

a node is selected

should

at random

et al. [12, 13] for load
Such a policy uses a

be processed
and probed

locally

or re-

to see whether

the transfer
of a task to that node would place the node’s queue length above
a given threshold.
It has been shown that, even with a small threshold,
this
policy can improve
can also be applied

system performance.
to the MRT algorithm

We demonstrate
that a threshold
in the environment
studied
here.

The new approach
not only reduces the response time but, most significantly,
reduces the sensitivity
to accuracy
in the workload
and system parameters
used in the analysis.
Typically,
the threshold
values used in threshold
policies are highly
dependent on system characteristics,
and require
information
concerning
system
load or arrival
rates. Threshold
values therefore
usually
need fine-tuning
to
ACM

Transactions

on Database

Systems,

Vol

16, No

3, September

1991.

494

P

.
Table

S

Yu et al.

VIII.

Performance

Comparisons

of the Threshold

in the Balanced

and Other

Strategies

Case
Random

Preferred

Threshold
1.(MRT)

1.15

1.2

1.3

RT
NPR
S WR

0.618
46.60/.
78.2%

0.616
13.670
43.3%

0.616
7.O~o
8.770

0.627
3.6%
3.3?/0

0.661
0%

0.692
66.7?40

RT
NPR

0707
19.4$6

0.705
8.2%

0.701
4.2V0

0.753
0%

66.7!/’0

S14’R

37.6%

17.5°h

4.1 0/0

0.716
2.1 YO
l.OO/O

c=3K

C=15K

(a)

1.15

0.700
46.3!4.
85.O!L

0.694
17.7%
49.070

0.986
17 9%
43 4%J
z

0.933
11.070
24.3%

SJ4’R

0.71

=

)ld

Thresh

1.(MRT)

S

1.707

I Preferred

1.2

I 1.3

I Random

I

1

c= 15K

L-;.;R

S WR

0.916
6.9%
8.4°A

0.941
4.4!40
2.7%

(b)

achieve

good

thresholdin local
[27].

and

Our
is

into

account
to

MRT

can

whether

be

response

on

fraction

that

‘as

changes

server’s

this

policy

idle

type

time

of informa-

automatically

because

time

updating

such

of each

acquiring

threshold

response

adaptively

of quantities

characteristics

the

threshold

ratio
that

make

the

approximation

the

Simulation

ACM Transactions

NPR,

when

response
into

takes

threshold
used

with

out

and

the

is

in

the

in

times

making
of
are

S WR

on Database Systems, Vol

does

routing

between

that

routing

provide

first

We
testing

preferred

time

Otherwise,

is less
the

decision.

the

threshold

strategy

in

VIII,

presented.

not

of the

is done.

routing

response

response

shown

are

by

be

in

desirable.

algorithm

estimated

the

should

reduction

benefit

MRT

preferred

performance
results

time

minimum

If it is, then

a transaction

system

response

system

that

a sizeable

nonprefemed

of estimated
of the

is carried

time,

a

it is clear

only

concept

threshold.

examine

section,

Marginal

to

algorithm

We

our

system

system

achieved.

a given

thresholds.

done

is such

previous

reward

and

been

[201, or the

estimated

in the

the

system
MRT

the

0.81

estimation

However,

dynamic

S =

XxXx
66.7!40

algorithm.

incorporate

than

rates

to a nonpreferred

sufficient

has

ongoing

environment
task.

weight

Work
on

arrival

some

indicated

routed
time

remote

a difficult

original
As

based

transaction

tion
used

performance.

values

1.346
070

Table
Four

16, No. 3, September 1991

under
where

cases

are

different
the

overall

considered:

On Robust Transaction Routing and Load Sharing
high

and

tively,

low

and

communication

high

respectively.

The

threshold
than

equal

those

to

improvement

head,

the

trend

can

time

is more

Because

reflects

works

for

a range

system

routings

response

time.

variations

in

routing

decisions.
access

5 Nonpreferred
overhead.

Robustness
We

also

compare

enhanced,
a case

nonpreferred
high

ance

transaction

to their

with

is presented.

a threshold

is trivially

algorithm.
system

Looking
strategy

(low

communication

In
of

contrast

site

to the
and

strategy

ignores

attempts

to balance

VIII

present

we

5 The routing
negative

effect

decision

the

In

these
the

the

balanced

case,

worse

the

(high

strategy

incurred

the response

transactions

already

Transactions

the
derived

by

we

on Database

degrades

in the

perform-

the

than

The
this

preferred

load

considers

to each

than

calls
site.

the

sharing.
the

the

random

transaction.

and

20 percent

cost)

database

the

of this

strategies,

more

only

of

always

a threshold

under

loadsharing,

of the incoming
in the

that

to

time

defined

of dynamic

results

the

analytically.

see

we

to those
routing

to

communication

routed

When

strategy

threshold

remote

also
VIII.

performance

is not

for

the

of increasing

this

VIII

which

This

through

response

time

of transactions
derived

effect

are

importance

it:

Table

VIII.

is equivalent

the

to

response

routing

pronounced

response

opportunity

analytically

ACM

than

it has

the

number

Table
statistic

35 percent

overhead

it

with
its
in

Table

Because

numbers

system

ignores

is to minimize

more

due

subsequent

routing.

load

the

SWR

load

and

preferred

to some other

MRT),

system,

demonstrates

the
the

the

preferred

at

cost)

affinity

see the

strategy.

of high

This

we
share

is still

of

5.4.

in

to

routing

performs

conditions

strategy.

trend

1.3

1.2,

of the

and

first
always

under

and

that

Section

of nonpreferred

amount

effect

as indicated

the

of infinity.

Note

O percent,

that

MRT

This

see in

is as

estimated

realized

system

SWR

the

be

improve

of

strength

system

strategy
NPR

(the
loads.

strategy

shall

opportunities

preferred

transactions

the

overall

nonpreferred

associated

can

a threshold

of 1.15

ignoring
systems

of the

routes

thresholds

By

as we

and

a cost

strategy

of nonpreferred

with

times

the

time

and

in

in

value

The reason

of the
not

and

has

threshold
reducing

is also

response

pattern

fraction

consider

from

threshold.

large

may

routing

The

of substantially

the

resulting

for

this

effect

arrival

response

improvement

improvement

this

threshold

overhead

fraction

over-

communica-

estimated

significant.

significant

obtain

decrease

high

a single

is very

with
times

degree,

the

and

communication

a marginal

marginal

subsequent

by avoiding
the

a

load

weight

case

we

a lesser

although

information,

NPR

of

only

to

the

response

communication
To

1.2,

respec-

S = 0.71,

Although

high

of high

used

which

that

provide
the

is

is

and

to

examined.

495

c = 3 K,

smaller

10 percent.

case

state

in

shows

then

the

reduction

This

remote
time

cases

The

Simulation

in

equivalent

and

threshold

system

of

vary.

load

threshold

dynamic

load

follows.

the

cases

and

S = 0.81

slightly

all

the

marked

is

of high
less

when

with

of 1.15,

in

case

is still

observed

cost.

has

the

c = 15 K

loads

strategy

obtained

in

be

which

system

MRT
a threshold

are

with

transaction

original

improvement

response

overheads

low

1. For

of MRT

more

tion

and

.

effect

random
and
In

only
Table

strategy’s

This

may

have

a

system.
Systems,

Vol.

16, No. 3, September

1991.

P. S. YU et al

>0
.---

D

MRT

:
f
no
:0
c
~
3
:
‘~

.

0

!

0

0

15

10

5

20

meosure of syslem mbolonce
Fig,4.

performance.
statistic

Note
is also

does

even

ignores

reference

the

preferred

(The

extra

it

for

percent,
the

case

the

unbalance

greatly

system

increases

overhead

that

cannot

has

and
the

random

processor

policy

fact

load

handle

the

SWR

the

in

high

incurred

the

case

and

of both

of ‘ XXXX’. ) Because

communication

and

balanced

strategy,

the

that

entry

locality,

In

system

1.0

fact

an

system

66.7

strategy.

than

by

ofsampled

is trivially

this

the

greater

is indicated

the
of

NPR
for

overhead.

load

probabditles

the

than

utilization

communication

result

that

undefined

worse

processor
action

Occurrence

trans-

strategy

utilization

by

a

high

remote

as a
database

calls.

NPR

As
we

expect

probability

under

during
the

in

unbalance
is

strategy
Since
with

strate~

the

the

the

is

the

and

in

in
time

the

compared

The
the

defined

tail

queue

is

threshold

due

than

more

allowing
to

remote

call

than

that

and

is,

is

of 1.2
the

MRT.
the

the
the

mean
Also,

threshold
MRT

does.

system

will

unbalance—coupled

overhead—leads

ACM Transactions on Database Systems, Vol. 16, No 3, September 1991,

P,,

unbalance

of the

unbalanced

approach.

the

a threshold

That

a small

MRT,

11, lZ, 13} –

of

note

that

unbalanced

as to how

max{

system

with
We

distributions.

of a transaction,
utilization

15 K.

higher

of the

to become

c =

of the

examining

length
of

strategy

and

by
as

distributions

threshold

S’ = 0.81

to that

is verified

instantaneous

of uncertainty

of the

low
This

unbalance,

strategy

system

course

reduction

response

system

threshold

is a window

over

of

4 for

a difference
allows

is very

balanced.

simulations.

Figure

of the

there

remain

1,

the

MRT

plotted

there

strategy

to be less

11, lZ, 13} where

rein{

better

system

distribution

sampled
are

threshold

of the
the

to

the

On Robust Transaction Routing and Load Sharing
Table IX.

Performance

Comparisons

of the Threshold

in the Unbalanced

.

497

and Other Strategies

Case
Preferred

Threshold

Random

MRT

TS 1

TS 2

0.622
49.4%
63.5?40

0.630
14.6%
2.0°41

0.615
28.3%
16.070

0.941
0%

0.707
66.7%

0.744
25.97.
18.0%

0.764
11.7%
0.60/0

0.735
20.3%
4.6%

1.555
o%

5.038
66.7%

~=3~
RT
NPR
S WR
C= 15K
RT
NPR
S WR
TSI -- Threshold

strategy
strategy

TS2--Threshold

with
with

sasne weight (1.2) used for all classes
system speeific threshold:
1.2/1.05/1.05
(a)

S

0.71

=

Threshold
MRT

TS 1

TS 2

RT
NPR
S WR

0.718
49.3?0
69.5%

0.720
19.5yo
3.5%

0.693
30.6%
21.47.

RT
NPR
S WR

1.149
25.3%
14.6%

1.226
17.9%
().4%

1.134
22.2?0
4.3~o

c=3K

c=15K

(b)

Next

we examine

already

an

discussed

an

unbalanced

tered

in

that

different
the

weight

where

used
time

we

different
times

in

compare

the

in

to
the

be

The

threshold

MRT,

and

in

case,

high
In
we

to

The

this
When

against
that

balanced

must

to

the
is

ACM Transactions

can

be

work

greatly

we

find

in

Table

IX,

under

two

estimates

system
system

performance
that

that

response
MRT

system

approach,

performance

with

seen

nonpreferred

thresholds

in
strategy

is unbalanced,

1) therefore

how

case,

nonpreferred
(TS

have

is encoun-

strategy

MRT

system

threshold

better

specific

not

threshold

threshold
that

the

case

achieve

choose

difficulty

unbalanced
This

case

communication

the

We

namely,

to underutilized

simple

suggests

order

the

MRT.

transactions

biased
of the

the

6 percent.

case.
state

for

= 0.5:1:1.

algorithm,
does

apply

with

state.

more

always

sense.

unbalanced

reason

routes

system

of

strategy

system

balanced

case

66.7?40

case a similar

if we

that

MRT
The

and

the

designed

than

0?+0

S:: S:: S!

threshold

example,

balanced

on present

of this,

though

than

the

higher

thresholds.

where

1.008
66.7%

a threshold

successful
For

is

based

aware
does

a previously

0.81

=

In the unbalanced

environments.

response

case

of operating

issue

to choose the threshold.

S

a
070

is

1 than

it

however,

is

routing—even
routing
does

degrades
than
explicitly

makes

worse
MRT

than

by

more
in

the

take

the

on Database Systems, Vol. 16, No. 3, September 1991.

498

.

P. S. Yu et al

different

utilizations

into

threshold

approach

is still

The

preferred

strategy,
(S:)

system

of course

case

the

strategy

the

highly

random

but

at

least

overhead.
the

does

When

the

penalty

for

increases,

the

a

naive

Discriminatory

Another
system

at

the

than

than

the

worse

the

case

of high
is high

worse

is

load

of

load

sharing

fails

when

the strategy

the

sharing

case

transaction

lower

in

the

IX

communication

preferred

load

strategy
the

is low
because

penalty

When

for

transaction
the

survives

see that

strategies,

transaction

increases:

random

in

the

communication

policy

case

of

low

cost is also high.

Strategy

way to minimize
the risk incurred
when making
a nonpreferred
routing
decision
is to try to share the load with a short transaction

because even if it overestimates
the gain from nonpreferred
can take a new measure
of system state relatively
soon.
basis

of a refinement

short and long
discriminatory
tion.

not
load

that involves
only a few remote calls rather than with a long transaction
involves
many remote
calls. The system does not commit
itself too

the

the
the

balanced

we

threshold

than

load

although

Table
low

of high

P~ exceed

extra

that,

and

the

system.

do

version

load

higher

utilized

In

of the

and

than

calls

lightly

the

load

in order

Pz and

strategy

case.
any

sharing

in the

Note

preferred

threshold

database

under

MRT.

than

cost

with

Even

unbalanced

the

to

cope

systems.
worse

performs

need

cost;

fails

utilizations

database

of

communication

5.2

in

remote

then
does

fail

to do load

it actually

the

much

strategy

advantage

which

in

the

systems
cannot

extreme

where

processor

much

worse

does
not

thresholds,

an

situations
is unable

simply

communication

random

taking

strategy

does

changes

strategy

class-specific

representing
in

utilized

performs

situation

the

then

policy

strategy

the

the
The

at

system

random

badly

= O. 81).

that

arriving

preferred

(S

these

strategy,

very

of underutilized

load

1.0, showing
load

routing

Because

advantage

transaction

With

valid.

performs

is unbalanced.

to take

account.

of the

MRT

strategy

transactions
when making
strategy.
In the following,

Each transaction

class consists

that

that
much,

system routing
This observation

distinguishes

it
is

between

routing
decisions.
We call this the
we consider
a hypothetical
situa-

of two subclasses:

one long

and one short

which have 64 and 8 database calls, respectively,
as in the Bernoulli
distribution of Section 3.3. Assume that we can distinguish
the subclasses from the
input parameters
associated
with each transaction.
We discriminate
against
long transactions
by automatically
doing preferred
system routing.
We only
when
short
transactions
arrive
—i, e., we attempt
to
adopt the MRT strategy
minimize
the

the

risk
By

response

automatically

doing

discriminatory
share

the

strategy
which

times

in

situations

where,

even

if we

make

a mistake,

long

transactions,

is low.
strategy
load

among

avoids
can

making

result

the

case

for

ACM

TransactIons

both

in

preferred
gives

the

on Database

some

systems.

nonpreferred

a large

high

system

up

and

number
low

Systems,

routing

On

the

routing
of remote

that

other
calls.

16, No. 3, September

MRT

hand,

decisions

communication
Vol.

for

opportunities

the
for

In

overhead
1991

would

discriminatory

long

Table

the
use

X,

under

transactions
we

examine

a balanced

to

On Robust Transaction Routing and Load Sharing
Table

X.

The Performances

of the Discriminatory

(balanced

S=

database

andthe

499

MRTStrategies

load)

O.71

S=0.81

IX crirninatory

MRT

.

MRT

Di scriminatory

c=3K
RT (short)

0.334

0.328

0.384

0.372

RT (long)

1.963

1.917

2.244

2.152

RT (overall)

0.619

0.606

0.709

0.684

NPR

46.6%

40.9~o

46.3%

40.9’%

S WR

78.2°h

38.070

85’%

44.9yo

C= 15K

database

RT (short)

0.384

0.373

0.529

0.486

RT (long)

2.249

2.152

3.062

2.741

RT (overall)
hlPR

0.711

0.684

0.972

0.885

19.5%

19.5~o

18.2%

19.5%

SWR

37.6%

8.0%

43.4~o

9.3!%

load.

We find

that,

over

all transaction-length

categories,

and over

all cases, the discriminatory
algorithm
has lower response time than that of
the MRT. However,
the reductions
are less than 5 percent except for the case
of high
communication
overhead
transaction
improve
response time
percent;

and overall

transactions
ferred

system

response

by the

fact

The larger

communication

the improvement
shown on Table

overhead,

time

that

and it is the short

the load balanced.
the

system

benefit

with
S = 0.81. In this case, the
by 8 percent;
the long transactions
is improved

they

always

transactions

that

the transaction
the

larger

by 9 percent.
get

executed

get moved

processing

the

risk

for

short
by 11

The long
at the

around

pre-

to keep

load and the higher
nonpreferred

routing:

of the discriminatory
strategy
is thus more apparent.
Also
X are the NPR and SWR. Notice, that with the same degree

of NPR, there are substantial
reductions
in the SWR for all of the cases. This
shows that the risk of nonpreferred
system routing
is minimized.
In Table

XI, we show the performance

unbalanced

database

criminatory

strategy

ment

in response

time

loads,
still

i.e.,

of the discriminatory

S:: S;: S; = 0.5:1:1.

does better

is not as great

than

We

the MRT.

as in the balanced

reduced
percent.

by

8 percent;

4.1

indicates

5.3

Adaptive

The

regression

mizing
is

6 percent;

transaction

dis-

In the case

case where there is a
response time is only
and

overall

by

7

Strategy
model

response

a good

short

the

the improve-

situation.

(the only
transaction

under

that

However,

of high load and communication
overhead
larger than 5 percent
improvement),
long
by

strategy
find

indication

time

specified

in

is concerned,
of

actual

ACM

Section
the

response

Transactions

estimated
time—

on Database

that

response
except

Systems,

that
Vol.

as far
time
it

as mini-

under

MRT

systematically

16, No. 3, September

1991.

500

.

P. S. Yu et al.

Table

XI.

The Performances

of the Discriminatory

(unbalanced

database

and the MRT

strategies

load)

—
S=

E=

MRT

O.71
Dis criminatory

MRT

-1

7

~=3K

RT (short)

0.335

0.330

0.386.

.RT (Iorw)
,-. —=,

1.971

1.961

2.259

1

v.d

,“

I

2.232

RT (overall)

0.620

0.615

0.714

0.701

NPR

49.5%

49.2!/.

49.3%

49.9?4.

S WR

63.5%

18.4%

69.5%

21.5%

c=15K
RT (short)

0.410

0.393

0.628

0.576

RT (long)

2.399

2.398

3.619

3.385

RT (overall)

0.741

0.727

1.152

1.067

NPR

26.0%

32.8’%

25.3’%

35.7?0

SWR

18.0%

1.5~o

14.6%

1.3%

ignores

some

routing

decision.

additional

nication

costs,

opment

of an

additional
cost

As
and

associated

have

system

seen,

when

varies

These

system

making

making

cost

strategy,

of nonpreferred

account

with

this

unbalance.
routing

“adaptive”

cost

into

cost
we

a nonpreferred
with

system

observations
which

routing

subsequent

motivate

attempts
under

system

load,

commuthe

devel-

to (1) quantify

MRT

and

are processed

actual response
how well actual

time of the transactions.
response time conforms

adaptively
that

adjusts

future

are used to weight

and leave

estimates
factors

this

decisions.

5.3.1 The Strategy.
The adaptive
strategy
consists of three steps.
first, the system estimates
the response time of incoming
transactions.
as transactions

the

(2) take

the system,

the front-end

This enables
to the estimate.

by periodically

in the response

records

the

the system to monitor
Finally,
the front-end

recalculating

time

In the
Then,

coefficients

estimation.

More specifically,
upon transaction
arrival
the front-end
uses the MRT
calculation
(equation
(3. 1)) to make an initial
estimate
of the transaction’s
expected response time. In a previous
processing
period the system performed
in
R:’ = ak + fi~R~L + ~~Xk’
as explained
a regression
of the equation
classes. The
4.1. This regression
is done for each of the k transaction
front-end
uses the 13~ and D$ coefficients
to adjust the MRT estimate
of the
transaction’s
expected response time at each of the processing
systems. Note
that
ak is irrelevant
in making
routing
decisions
because
it adjusts
the
in the
same
way.
Nonpreferred
response
time
to each candidate
system

Section

routing

is done

is still

less

When

to the

6 Because

ACM

only
that

a transaction

added

history

than

of the

running

inherent

of an observation
Transactions

when

the

of the
leaves
totals

serial
period

on Database

modified

estimate

local

system.

the

system,

accumulated

correlation

its

actual

at the

among

for

the

performance

front-end.6

response

time

16, No. 3, September

1991

in the regression.

Systems,

Vol.

nonpreferred

In

values,

system

statistics
performing

we

use

the

are
the

entire

On Robust Transaction Routing and Load Sharing
regression,

the

transaction

front-end

class

number

of class

k to

solves

the

obtain

the

k transactions

following
f?: and

that

two

linear

equations

~~ coefficients.

arrived

during

.

the

Let

501

for

each

m~ be the

observation

period.

where

(5.3)

(5.4)

Equations
(5. 1) and (5.2) are derived
from the orthogonality
principle
of
linear
mean-square
error. They are easily rewritten
so that the parameters
can be calculated
with running
totals
of each transaction’s
statistics:
R ~‘,
Xk’, Yh’, Rh~Xh~
Rki, Yki, Xki YkL and (R$’)2.
Such accumulation
imposes
negligible
and

o~erhe~d

actual

remote,

as the information,

response

is readily

times

available

and

which

whether

consists

the

solely

routing

of the estimated

decision

was

local

or

in the front-end.

Periodically,
the system recalculates
the coefficients,
6? and 6.$, using the
equation
~~+ ~ = e(7~+ ~) + (1 – ~)-y~. Here -y.+ ~ represents
either coefficient,
6? or (3~, that the system
will
use in
transaction
processing;
-y; + ~ represents
actually
yields in the n-th period.
the previous
period’s
coefficient
regression
of the current
period
calculation

introduces

the

adjusts

system

transaction
modification.

in

5.3.2
the

the
the

processing

in which

in the
the

e, which

coefficients,
previous

processed
that the

for (1) e, which
coefficients,
and

determines

and

period
of
regression

how rapidly

w, the window

coefficients

Choice of Parameters.
The potential
problem
adaptive
strategy
must
be addressed.
The

self-tuning
except
recently
calculated

(n + l)th
that the

That is, the system uses a weighted
sum of
and the 13~ and 13~ determined
by the
to yield the next period’s
coefficient.
This

two free parameters:
to changes

next, i.e.,
coefficient

are used

size of
without

of the free parameters
strategy
is completely

weights
the contribution
(2) w, the window
size

of the most
of transactions

before a new calculation
of the coefficients.
In Figure
5 we show
adaptive
strategy
is robust
with
respect to the weighting
of the

coefficients.
In this figure
the window
size is held constant
(the
are recalculated
whenever
1000
transactions
of any
class
ACM

Transactions

on Database

Systems,

Vol

coefficients
have
been

16, No. 3, September

1991.

502

P S. Yu et al

.
on-

80-

GD-

4Q-

2Q-

0 co-

-I

~=jl(,

s=

71

c=15K,

4 cases
Fig

5.

processed;
0.45.

Performance

then

with

in

window
which

71

strategy

starts
we set

c=l~l,,~=

s= 81

c=3i’,,

balanced

of adaptive

a new

Simulations

S=

dotabase
w~th different

from

c (labelled

scratch),

e to 0.25

~,1

Iood

and

provide

by ((1 – c), (c))

~ varies
slightly

As observed
in the
simulation,
overall
than
other
c values.
of nonpreferred
system
routings
strategy
reduces the percentage
Note,
however,
that the response
times under
as 15 percent.

strategy
in
performance

better
Figure
5 are only slightly
of the adaptive
strategy
strength

to inaccurate
In Figure

values.
parameter
6 we examine
what

than

at 0.25,
cients is held constant
processing,
w, varies.
Although

is discussed
the

0.05 to
results

adaptive
by as much
the adaptive

the

under
MRT.
The real
robustness
with respect

is its

This robustness
happens
when

from
better

weight,

in Section 5.4.
E, for the coeffi-

and the size of the window
of transaction
w varies from 100 to 10,000, the adaptive

by the change
in this parameter.
When
strategy
appears almost unaffected
in which 1000 transactions
of any class have
we set w to specify an interval
been processed,
response
time
values
are slightly
better
than those
with
sizes. Note
that Figures
5 and 6 are for the balanced
case.
different
window

Similar
results
hold
strategy’s
robustness

for

to be confident

although

that,

and

w to

5.4

Robustness

The

major

following.

1000

parameters.
ACM

Transactions

unbalanced
to the

respect

transactions,

in
the

to Information

advantage
MRT

the

with

is

of the

on Database

simulations
are

not,

we
in

fact,

of the

adaptive

it is reasonable

always

set

e to

0.25

brittle.

Accuracy
three

sensitive

In practice,

other
results

case [181. Because
two free parameters,

these
Systems,

to

enhancements,
precise

may
Vol.

knowledge

be hard

as compared
of

to determine

16, No. 3, September

1991

system

to MRT,
and

accurately,

is the

workload
and

they

On Robust Transaction Routing and Load Sharing

.

503

I

() 00

1
c=3K,

S= 71

c=15K,

4 cases
Fig.6.

may

vary

with

respect

over

Consider

the

estimating
for

cases

router
ing

case

where

for
to

the

the

lengths

and

reference

[q;,
x,

qk ,(1

–

set

/(1

–

a ratio

indicates

to

(i.e.,

the

same

Table

of the
of the

locality

system

q;,
for

in

calls

the

accordIn

1. The

these

estimated

a percentage
and

for

= O for

of
i # k,

all

preferred
nonpreferred

each

in

conducted

assumed

given

= (1 + x)q~k

localities

preferred

is

database

following:

qj~

distributions

execution.

in

q~k = 1 and

otherwise
the

the

the

whereas

proportionally),

[ q~z 1 still

defined

robust
is not.

were

[ qj,1
issue

[ qh, 1 during

distribution

is

performance

locality

distribution

< 1, then

qkk))

x

strategies

Simulations

Transactions

according

(1 + x)q~k

‘qkk

with

decrease

1, is

if

inaccurate

distribution

to the

w.

MRT’s

times.

locality

decisions.

[ qh J is set

distribution,

uses

response

!31

different

proposed

parameters;

router

inaccurate

routing

with

of the

system

C=15K,S=

load

strategy

performance
and

S= 81

database

ofadaptive

in which

actual

inaccuracy
creases

The

an

making

simulations,

q~, =

time.

queue

with balanced

Performance

to workload

c=3K,

S=,71

system
insystems
i # k.

all

transaction

Note

class

as

[qk,].
The
gies

response
are

When

tion,

time
the

percent.
tions

swapping

are

when
times

when

an

the

inaccurate

80

these

percent

under

when

reference

the

locality

range

much

distribution

ACM Transactions on Database Systems, Vol

c = 15.

are

the

the
at

degrades

strategies
are

in

MRT

MRT

x is in the

strate-

and

decisions,

observed

of the

enhancements

adaptive

S = 0.81

routing
As

enhancement

small

and

with

making

performance

three

quite

that

in

case

substantially.

reach
the

are

discriminatory

balanced

assumed

can
why

It is clear

the

degrade

ratio

However,

threshold,

7 for

can

explains

in response
MRT

MRT,

localities

This

case.

for

Figure

of MRT

to 20 percent.
the

in

weaker

sponse

this

times

plotted

re-

simulax =

– 30

badly
used,

in

varia-

of – 30 percent
more

robust
is

than

assumed,

16, No 3, September 1991.

504

P S, Yu et al

.

1.250

1

1.200-

‘ ..,,
,..,
‘..,
‘...,

:
:1,150m
c

gl.loo~

‘e,.....

c 1 J-J50_
Q
%
u
g 1.000- -——+.

.,...-.
~ .............m..
‘....,

‘.....
=-..........

0
k
g o.950-

%..
,..,, .,

‘b.
(+ .-

:

-.&

0.900-

.--+

----*

----0

-- --------

*----*

o.=o~
–30

–25

-20
-15
Percentage

a.

Fig,

7.

Performance

MRT

with

-*

–10
change

-5
0
In locality

Discrirnindoy

inaccurate

information

10
( Y,)

15

20

-u- Threshold

Adaptive

+-

locality

5
accumcy

(Balanced

case with

S = 0,81

and

C = 15 K),

Under the threshold
and discriminatory
strategies,
we see that the conservative approach
of either restricting
the conditions
for nonpreferred
routing
or
being
more selective
about
candidates
for nonpreferred
routing
pays off.
Under the adaptive
strategy,
the router is able to get an understanding
about
the true
cost or benefit
of routing
to nonpreferred
systems
through
the
feedback.
This can be observed through
the value of (3; which indicates
how
the

router

compensates

and represents,

for

the

basic

for each transaction

dynamic

estimation

class, the degree

ferred system routing.
In Figure
8 we show the average
fl$ values
has inaccurate
reference
locality
information.
weaker

locality

system

routing

has

accurate

increases

expected.
Note
actually
ACM

actually

an

attempt

to

information.
the

increasingly
realizes

than
in

In

(i?; value

as the

negative.
that
The
that
exists

Transactions

result
when

to
is
the

(encouraging
on Database

a

then
sharing

load

Figure

8

of

the

less

router

Systems,

Vol

preferred
16, No

in

does
the

stronger
system

3, September

the

routing
reference
routing),
1991

router
strategy

accuracy

a

becomes
the

higher
is

router

cost

than

done.
locality

Figure

-

the router
assumes a

nonpreferred
when

coefficient

involves

time,
nonpre

adaptive

locality
the

system
a

more

it

that

in

system

assumes
more

see

change

nonpreferred

does

than

change

nonpreferred

that

for the case where
When the router
MRT

we

percentage

Because

routing

exists,
do

of response

of bias against

than
7 shows

On Robust Transaction Routing and Load Sharing

.

505

2.501

1

2.00-

1.504 I
t

\

.

\

-

Tx CIOSS 3

+-

TX class

2

“

Tx class 1

-.
H

1.00-

9

0.50‘“—~

k.<.

,a
\

0.00

/

Fig.

8.

that

-20

13z with

router

is similar

further
supports
MRT does a lot
assumes

information

actually
has

to that

ferred

(Balanced

1
15

reference

case with

1
20

of the

other

locality,

S = 0.81

somewhat

information.

the discussion
in Section
of marginal
nonpreferred
stronger

I
10

I

5

improves

accurate

to be more conservative
than
Under the adaptive
strategy,
increases
decreases
is shown

I
0

I

-5

change In Iocollty accuracy

locality

performance
the

performance

router

-15
Porcontago

inaccurate

MRT’s

case where

I
–lo

I

I

–25

and

c = 15K)

as compared

In

this

enhancement

to the

situation,

MRT’s

strategies.

4, in which we claimed
that
system routing.
Thus, when
and is thereby

mistakenly

This
the
the

“forced”

it normally
is, response time actually
as the percentage
change in locality

improves.
accuracy

(thus
discouraging
nonpreferred
system
routing),
the (3: value
from the value when accurate
information
is available.
The trend
in Figure
8. This tells the router
that a greater
degree of nonpre -

system

routing

sharing
is achieved.
The response time
where
communication
overhead
is low, MRT

can

be done

estimate

than

expected:

in (3.1) depends

overhead
can afford

as a result

on various

system

more

load

parameters,

is very important.
When
communication
to do more load sharing,
and thus reduces

overall response time. Consider
the situation
where the router does not have
accurate
information
about communication
cost. This may happen
because
system pathlength
information
may not be readily
available
or easily determined. Simulations
were run for various
inaccurate
assumptions;
in Figure
9
we graph mean response time against
inaccurate
q values (defined in Section
3.2) awmrned by the router
for the balanced
case with
S = 0.81. The true
value
router

of q is 0.25: whereas the assumed q varies from 0.05 to 0.45. When the
underestimates
the communication
cost (low q), MRT
does much
ACM

Transactions

on Database

Systems,

Vol.

16, No. 3, September

1991.

506

P. S. Yu et al

-

1.80

1
I

“’,
.,.

:1 .60=

.40-

“’”

\
(k

c

:1 .oo-

‘.

0.80

=---

~

I

.050

Fig.

9

and

v = O 25 (true

Performance

strategy
working
that

the

a-----

i

-*.

-

1

-- *--*

inaccurate

-----+

-+- RT (Adaptive)

communication

overhead.

-+

-.*

1
.400

I
.350

assumed by front-end

avwhoad

RT (Discriminotbry)

with

I
.300

I

.250

.200

.. -.--+

1
.450

routor
+

RT (Threshold)

(Balanced

case with

S = O 81

value))

system

routing

that

really

it

not

should

do. Because

the

MRT

has no way to determine
that
its load sharing
efforts
are not
as they should,
response time (in the worst case) is almost
double

of the

tion,

-----

.100
.150
Communlcatlon

Q m (MRT)

nonpreferred

---*-

case of accurate
swapping

ratio

information.
(in

the

As a matter

worst

case)

of fact,

approaches

in the

simula-

100 percent.

The

three enhancement
strategies
all do quite well. The discriminatory
strategy
has the best performance
as it takes advantage
of an additional
piece of
information,
namely
whether
an incoming
transaction
is long or short.
We now take a closer look at the adaptive
strategy.
The adaptive
in fact, gets feedback that the nonpreferred
system costs more than

strategy,
predicted.

Even though
the adaptive
strategy
does not known why nonpreferred
system
routing
decisions
turn out badly,
it simply
imposes
a higher
,6$ to restrict
nonpreferred
routings.
In Figure
10 we show average
6$ values against
the
assumed

q

values.

These

information

which

bias

against

nonpreferred

tion

class.

The

information,
MRT

strategy

preferred
ACM

of
system

Transactions

does

does.

overestimates

indicate
the

system

router

and

values

underestimates

is
less

On

the

routing

therefore

other

on Database

to

the

overhead
At

Systems,

an

extreme,

Vol.

16, No

router

than
strategy

reducing
the

3, September

/5’j

for
for

routing

the
value

1991

is

given

overhead,

increases

adaptive
by

the

compensate

system

hand,

when

communication

steadily

able

nonpreferred

communication
routing.

that
true

each
the

the

the
transac-

inaccurate
nonfeedback

compensates
bias
actually

against

for
non-

becomes

On Robust Transaction Routing and Load Sharing

.

507

3.00

2.50L

2,00-

b, \

1.50-

““‘+
“\.
\:.,.

,,
I

1.00-

~,,
,...
“...
..,. km,
,...... ~

0.50-

.......
\

“-..

\\

\

o.oo-

-0.50 /

I

.050

.100

I

I

.150

.200

Communlcotion
Fig.
(true

10.

& with

inaccurate

I

I

\

I

.250
.300
.350
ovortmod ossumod by fmnt-snd

communication

overhead

(Balanced

I

I

.400
mutsr

.450

case with

S = 0.81 and q = 0.25

value)).

negative.

As

a result

nonpreferred

the

system

adaptive

routing

in

5.5 Some Remarks

on the Three

We have presented

and studied

strategy
an

effort

encourages
to share

the

the

router

to do more

load.

Strategies
three

robust

routing

strategies.

Now

we point

out some similarities
and differences
among the strategies.
We first discuss
the “threshold”
concept
in a general
sense. Choosing
a good threshold
requires
trial
and error because there is no analytic
formula
to derive the
optimal

threshold.

change.

The

the

threshold

can further
would

strategy.
distinguish

of infinity

has the effect
ratio

threshold
strategy

If for
apply

used

system’s

long

short

which

in the

transaction

strategy.

transactions

threshold

strategy,

same preferred

we can

system

also

as different

treat

long

and

subclasses

short

P; /(3~

in contrast

to

advantage

of

In other
(Note that

transactions

and obtain

we
we

and a

strategy,

It has the

being
dynamically
derived
from the regression
analysis.
the “threshold”
does not have to be specified
explicitly.
adaptive

class,

subclasses,

In the adaptive

is an additive

threshold

case of

transaction

of 1 to the short

transactions.

as the workloads

as a degenerate

preferred

and

a threshold

to the long

of a “threshold,”

threshold

can in fact change

can be viewed

each

between

be able to simply

threshold
the

The optimal

discriminatory

different

words,
in the

with

the

regression

coefficients
for each subclass. As a matter
of fact, any number
of subclasses
can be kept for each preferred
system.
There may be a practical
limitation
because of the increased
cost of estimating
regression
coefficients
as the
ACM Transactions on Database

Systems,

Vol.

16, No. 3, September

1991.

508

P, S Yu et al.

.

number
of classes/subclasses
becomes large. Furthermore,
w, used to estimate
the regression
coefficients
also needs
the

number

of classes/subclasses

increases.

the window
size,
to be increased
as

) Furthermore,

as pointed

out in

Section
5.3.2,
the adaptive
strategy
is relatively
insensitive
to the free
parameters
specified
in its algorithm.
Next we consider the optimization
objective.
Given the fact that we do not
have exact knowledge
about future
arrivals,
both threshold
and discriminatory

strategies

try

optimization
mizes

the response

tries

in some

instead

to pursue

the

success

The

correction

sense

of just
time

to pursue

pushing

for

of incoming

some

transactions.
but

it incorporates

of individual

optimization

into

a correction

6$ /(3~, tends

to be smaller

Hence, with accurate
the performance
under

slightly
inferior
to that
of the threshold
strategies
lead to substantial
improvement

The

the adaptive
we consider

scheduling

strategies
in the

imposed

on the front-end

couple
only

maintain
R ~‘, Xh’,

recalculate

of additions
done

back-end

the

and

periodically.
as the

by

MRT,

straightforward:

table,
(2) making
response
Section 3. (In the simulation,
front-end
to reach a routing

periodically

term

In

table

threshold

the running
Yhl, R~’Xk’,
regression
any
can

which

front-end.

threshold

and

The basic

operations
the

totals
of seven
R~’Yk’,
XkLYk’

discriminatory
involved

routing

history

coefficients.
The

case,
simply

systems

this

The

second
all

first
may

statistics

is too large

is the

sum

of all

the

only

is more

calculation

ship

statistics
for each
and (R$’)2),
(2) to

local

involves

a

involved

but

be done

at the

to the

to be handled

front-end,
multiple
front-ends
can be employed
to perform
algorithm.
Each front-end
system now needs to maintain
routing
history
table reflecting
its own routing
decisions
history

or threshold.

optimal

time
estimates
based on the formula
given in
it only takes a few hundred
instructions
in the
decision.)
For the adaptive
strategy,
in addition,

perform
the calculation.
If the number
of back-end

routing

about

knowledge
about workthe adaptive
strategy
is

(1) updating

multiplications.

front-end

still

lesson

strategy.
Nevertheless,
all the
in robustness.
From a practical

are all comparable.

are quite

we need to (1)
transaction
(i.e.,

past

the

mini-

strategy

strategy
may be the most promising.
the scheduling
overhead
incurred
at the

overheads

front-end

than

or global
that

The adaptive

optimization

term,

of social

optimization

individual

under the threshold
strategy.
load and system information,

viewpoint,
Finally,

degree

individual

back-end

is
to

by a single

the same routing
not only a local
but also a global
routing

tables

and

represents
an estimate
of the overall
effect of all front-ends.
The multiple
front-ends
would
then need to exchange
information
about the changes
in
their
local routing
history
tables,
so that each of them can derive a global
routing
history
table.

6. CONCLUSION
Distributed
systems have the potential
of greatly
increasing
the computing
power available
to users. To take advantage
of this power,
a distributed
computing
environment
must
be able to shift
load from
overutilized
to
underutilized
systems.
Load sharing
mechanisms
are often conceived
as a
ACM

Transactions

on Database

Systems,

Vol

16, No. 3, September

1991.

On Robust Transaction Routing and Load Sharing
trade-off

between

the

communication/transfer

delay

incurred

.

509

by transferring

jobs to a remote system, and the benefit
achieved
by reducing
response time
at the remote system. However,
even when transfer
costs are minimal,
in the
case where

the

systems

case of heterogeneous
the job still
through

are not
servers),

remains.

Our

probabilistic

equally
the

approach

knowledge

capable

problem

of serving

of selecting

is to explicitly
of the

data

a given

the

factor

best

job

(the

system

for

in the heterogeneity

reference

locality.

This

allows

good estimation
of expected
response
time.
Naturally,
this concept can be
applied to other types of heterogeneity
as well.
The MRT
algorithm
provides
good performance
over a wide range
of
parameter
values,
even though
it uses a steady state formula
to estimate
queue length
and response
time.
It is shown to be far superior
to naive
strategies
such as preferred
system
and random
routing
strategies.
The
strategy
successfully
balances
the need for load sharing
against
the extra
overhead
assumes

incurred
prefect

improves

performance.

information
study

by

needed

shows

remote

that

This

tendency

for the response
the

MRT

strategy

time

is not reached
then preferred
the risk of doing nonpreferred

transactions
not

than
attempt

transactions

is to be achieved

for short
to

are candidates

for load

automatically
routed
to their
respect to doing nonpreferred

on

feedback

behavior

of actual

of the system

its expectation

sharing

accurate

Our

accuracy

of the

such as com-

and

system

adaptive,

are

routing.

If the

is done by default.
is greater
for long
approach

transactions.

attempts:

long

simply

Only

short

transactions

are

By being conservative
both of these strategies

response

times.

then

The

strategy

its expectations

the strategy

with
im-

information

about

monitors

system

the

actual

are in fact realized.

compensates

degree
of remote
routing
that
it does in trying
demonstrated
that, due to the adaptive
strategy’s
need for very

accurate

but also robustness
to information
accuracy.
to improve
the response time estimate
based

to see whether

is not realized,

the

routing
routing

long

preferred
system.
system routings,

prove not only response
time
The adaptive
strategy
attempts

when

be obtained.

the discriminatory
with

strategy

marginally

accuracy.
The threshold
estimated
improvement
in

system
system

sharing

MRT
only

parameters

by a nonpreferred

transactions,

do load

to

discriminatory,

that
improve
robustness
to information
imposes
a threshold
criterion
on the
that

cannot

and system

response

that

a problem

sensitive

threshold,

the

sharing

estimation
is

distribution
the

However,

load

can become
time

threshold
Because
does

calls.

it does much

assumed reference
locality
munication
overhead.
Three strategies,
called
proposed
approach

database

information;

If

by changing

the

to share the load.
feedback
mechanism,

We
the

and workload

parameters

is

greatly
reduced so that better robustness
is achieved.
The system is capable
of detecting
that its attempts
at load sharing
are not succeeding
as they
should, and takes this into account in subsequent
decisions.
APPENDIX:
Different
applying

QUEUE
ways
some

LENGTH

of estimating
steady
state
ACM

ESTIMATION
Li have been
approximations

Transactions

on Database

considered
by Yu et al. [291 by
based on the routing
history
Systems,

Vol.

16, No. 3, September

1991.

510

P. S. Yu et al

.

table.
In this paper, we focus on the MRT strategy
that
residence
time calculation.
This approach
provides
better
the other

approaches

considered

by Yu et al. [29].

is based upon the
performance
than

It regards

the numbers

of

running
transactions
indicated
by the routing
history
table as fixed populations in a closed-queueing
network.
Naturally,
it is possible to calculate
the
exact mean queue lengths
based on the mean-value
algorithm
[21]. However,
this approach
is impractical
given the complexity
of the mean value analysis.
Instead,

the MRT

strategy

uses an approximation

algorithm

[2, 22] to calculate

processing

system.

Then,

the residence

the utilization

based on Bard-Schweitzer’s

time

of each transaction

and queue

length

system are computed.
The approach
is summarized
below.
h~, be the number
of active
class k transactions
Let
processing

system

P,. Let

probh(

i, j)

denote

transaction
processing

assigned
to processing
system
service P]. Hence, we can express

the

at each

of each processing
assigned

probability

that

to

the

a class

k

P, is waiting
for or receiving
the expected queue length
L, of

P] as
(Al)
forj=

l,...,

residence
system

N. Note

time
PJ. Bard

PJ of a class

that

(including

time

probk(i,
and

suggested

that
j)

is its total

Sh( i, j)

(c/2)E~
prob~(i,

probability

and Schweitzer

k transaction
Tk(i,

where

the
service

that

is initially

= Sk(i,
service

j)

~

Tk( i, j)

be written

+ l)/pJ

otherwise.

Tk(i,
~

=

to the

processing

time

to Pl can

in
as

(A.2)

at PI and is equal

if i = J, or (b~ + C/2)pk,,
1. 1* 1 Pkl),
j) can be expressed
as

j)

at the

the residence

-pk(i,

demands

is proportional

time)

assigned

j)(LJ

prob~(i,

j)

waiting

to (ah + bk ph, +

Thus,

the

probability

j)
(A.3)

T,(i,l)

“

1:1
Hence,

given

hk, from

the

routing

history

table,

the

mean

queue

lengths

L, are computed
by iterating
(A.2), (A. 3) and (A. 1), starting
with zero values
for queue lengths
LJ and probabilities
prob~( i, j), for k = 1, . . . . K, i, j =
routed to P, can be
1, . . . . N. Then, the response time of a class k transaction
obtained

from

(3. 1)

REFERENCES
1. AGRAWALA, A.
waiting

time

2. BARD, Y.

K.,

TRIPATHI, S. K.,

technique.

A model

IEEE

of shared

Trans.
DASD

AND

RICART,

G.

Adaptive

Softw

Eng.

SE-8

1,(Jan.

and

multipathing.

Commzm.

routing
1982),
ACM

using

a virtual

76-81
23,

10 (Oct.

1980),

564-572.
3. BARTLETT,
Principles

J.

4. BORR, A.J.
ACM

A nonStop

(Pacific

Transactions

Grove,

Transaction
on Database

kernel.

In

Calif.,

Dec.

monitoring
Systems,

Proceedings
1981),

in encompass:
Vol

of the 8th

Symposium

on Operating

System

pp. 22-29.
Reliable

16, No. 3, September

distributed
1991

transaction

process-

On Robust Transaction Routing and Load Sharing
ing.

In

France,

Proceedings

of the 7th International

Sept.

pp, 155-165.

5. CAREY,

M.

system.

In

1981),

J., LIVNY,

tems(Denver,
6. CAREY,

M.,

Proceedings
Colo.,

M.

of

H.

of the 5th

May1985),

the

Dynamic

on Very

task

International

allocation

Conference

8.

processor

Customer
cilities

systems.

Information

Guide,

International

9, CORNELL,

D.

shipping.

IEEE

Control

SC33-0133,
W.,

IEEE

10. DATE, C.J.

D.

Trans.

System

IBM

DIAS,
Trans.

Data

Bases

(Cannes,

in a distributed

on Distributed

Load balancing in a locally distributed

AG’M-SIGMOD

Conference

(Washington,
D. C., June 1986), pp. 108-119.
Models
for dynamic
7. CHOW, Y-C. AND KOHLER, W. H.
multiple

Large

511

database

Computing

Sys-

pp. 282-291.

Lu, H.

J. AND

Proceedings

AND Lu,

Conference

.

Comput.

/Virtual

load

Management

balancing

C-28, 5 (May

Storage

database system. In

on

in

1979),

(CICS/VS):

of

Data

a heterogeneous

354-361.

Intercom

munlcation

Fa -

Corporation.

AND Yu,

M,,

SoftvJ.

Eng.

An Introduction

P. S.

SE-12,

to Database

Analysis

of multi-system

function

request

1986), 1006-1017.

10 (Oct.
Systems,

Vol.

l.

Addison-Wesley,

Reading,

Mass,,

1981.
11, DEWIT,

D. J.,

ISHNA, M.
12th

GERBER, R. H.,

GAMMA-A

International

12. EAGER, D.

L., LAZOWSKA,

13. EAGER, D.

D.,

dataflow

Trans.

AND ZHOU,

S.

Nov.

Softw.

A load

1986),

1985),

S. S.

10, l(Oct,

18. LEFF,

sharing

in homogeneous

1986),662-675.

load

forhigh

balancing.

volume

on Computing

M.

In

Proceedings

of

transaction

processing

Architecture

(Boston,

Mass.,

In
June

classes.

PULIDAS,

S., TOWSELY,
Systems

networks.

of the

Holland,

Amsterdam,

STONEBRAKER,
Teradata:

Queueing

performance.

K.

C.

Network

M.

H.

Adaptive

transaction

Perform.

Quantitatzue

Models.

routing

Yorktown

K.

Optimal

load

balancing

in

SE-11,

5 (May

Trans
D,,
In

(San

Softw.

Eng.

AND STANKOVIC,
Proceedings
S.

J. A.

of the

Jose, Calif.,

June

S.

1988),

on

a multiple
1985),

System

Prentice

Hall,

a heterogeneous
N. Y.,

processor

1988.
system

with

491-496.

gradient

International

estimators

Conference

in

load

on Dzstrzbuted

pp. 482-490.

Mean-value

Conference

in

Heights,

Imbedding

8th

analysis

1980), 313-322.
analysis of multiclass

International

of closed

queueing

Stochastic

multichain

networks

Control

and

queueing

of queues. In ProOptz mization.

North

1979.
The

DBC/1012

Y

RC14114,

Approximate

ceedings

24.

G. S. AND SEVCIK,
using

Rep.

27, 2 (Apr.

J. ACM

22. SCHWEITZER, P.

GRAHAM,
Analysis

Research

AND LAVENBERG,

23.

of computer

IBM

IEEE

algorithms.
M.

J.,

AND LEE,

AND HWANG,

Computing

models

N. J., 1984.

manyjob

REISER,

on queueing

System

environment,

balancing

25.

ZAHORJAN,

Yu, P. S.,

A.,

L.

D.,

Cliffs,

database

ment

Symposium

A perspective

Computer

Englewood

21.

dynamic

and

53-68.

1989),53-76,

E.

Performance:

20.

for

1986),

load

5(May

of the

PP. 228-237.

of receiver-initiated

6, 1 (March

Adaptive

Proceedings

1986),

pp. 240-245.

17, LAZOWSKA,

19. NI,

J.

B. AND MURALIKR-

In

Aug.

A comparison

SE-12,

K.

machine.

(Kyoto,

Eval.

Anarchitecture

of the International

16. LAVENBERG,
Eual.

index

KUMAR,

pp. 684-690.

15. HORST, W, R. AND CHOU, C. K.
Proceedings

J.

Eng.

L.,

Bases

Perform.

sharing.

M.

database

Data

E. D. AND ZAHORJAN,

IEEE

Tex.,

on Very Large

load

LAZOWSKA,

systems.

FJCC(Dallas,

G., HEYTENS,

E. D. AND ZAHORJAN,

adaptive

L.,

distributed

GRA~FE,
performance

Con ference

sender-initiated

14. FERRARI,

high

case for shared

Data

Base

nothing.

Computer

IEEE

Concepts

Database
and

Eng.

Facilities.

9, 1, (1986).
Teradata

Corp.

Docu-

C02-0001-00.

TANTAWI,
systems.

A.
J.

N.

26. WANG, Y.-T.,
Comput.
27. WEINRIB,
geneous

AND TOWSLEY,

C-34,

D.

Optimal

static

load

1985), 445-465.
AND MORRIS, R. J. T. Load sharing

ACM

balancing

in distributed

computer

32, 2 (Apr.

3 (March

1985),

in distributed

systems.

IEEE

Trans.

load sharing
in large
Orleans,
La.,
Mar.

hetero1988),

204-217.

A. AND SHENKER, S.
Greed is not enough:
Adaptive
systems.
In
Proceedings
of the INFOCOM
(New

pp. 986-994.
ACM

Transactions

on Database

Systems,

Vol.

16, No. 3. September

1991

512
28

P. S. YU et al

.

On coupling
P. S , DI.AS, D. M., ROBINSON, J T., IYER, B, R., AND CORNELL, D. W.
IEEE Proceedings
75, 5 (May 1987), 573-587.
multi-systems
through data sharing
Yu, P. S , BALSAMO, S., AND LEE, Y.-H.
Dynamic
transaction
routing
in distributed
database
systems.
IEEE Tram
Softw. Eng SE-14, 9 (Sept. 1988), 1307-1318.
Yu, P S,, GORNELL, D. W , DIAS, D. M., AND THOMASIAN, A. Performance
comparison
of IO

Yu,

29
30

shipping
form.
31

(Orlando,

call

environment.
Fla

ZHOU, S.
Eng

SE-14,

Received

August

ACM

database
10, 1 (Oct.

shipping:

1989),

Schemes

, Dec

In

A trace-driven

Transactions

9 (Sept.

1989;

Proceedings

1990),

on Database

multisystem

partitioned

databases.

Per-

M.

of the 16th

Workload

characterization

Computer

Measurement

of relational
Group

Conference

pp. 235-244,
simulation

1988),

revised

in

15-33.

Yu, P. S , HEISS, H -U,, LEE, S , AND CHEN,
database

32

and

Eval.

study

of dynamic

1990;

accepted

load

balancing.

1327-1341,

February

Systems,

Vol

16, No

September

3, September

1991.

1990

IEEE

Trans.

Soflw.

Voltage-Clock-Scaling Adaptive Scheduling Techniques for Low Po wer in Hard
Real-Time Systems

C. M. Krishna
Electrical and Computer Engineering
University of Massach usetts
Amherst, MA 01003

Y ann-Hang Lee
Computer and Information Science
University of Florida
Gainesville, FL 32608

krishna@ecs.umass.ed u

yhlee@cise.ufl.edu

Abstract

Many embedde d systemsoperate under severe power
and energy constraints. V oltageclock scaling is one
mechanism by which energy consumption may be reduced: it is base d on the fact that power consumption
is a quadratic function of the voltage, while the speed
is a linear function. In this pap er, we show how voltage scaling can be sche dule dto reduc eenergy usage
while still meeting real-time deadlines.
1. Introduction

Many applications impose sev ere pow erand/or energy
constraints on embedded systems. Examples include
battery-pow ered devices and spacecraft relying on solar
or nuclear pow er.
V oltagecon trol isa pow erfulmechanism for reducing the energy consumption: the pow er consumption declines as the square ofthe v oltage, while circuit delays
increase linearly . Since the cloc k frequency is proportional to the inverse of the circuit dela y,w e have an
obvious tradeo betw eenthe power consumed and the
speed of the circuit.
The idea of exploiting this tradeo has attracted increasing atten tion since the rst paper was published
in 1994 [6]. In [1], a circuit may choose from among
multiple voltage lev els to reduce pow er consumption
while satisfying latency constraints. In [8], the Dhrystone 1.1 benchmarks were run on an arm7d processor
at two voltage-frequency combinations: (5.0V, 33 MHz)
and (3.3V, 20 MHz) yielding 185 MIPS/watt and 579
MIPS/watt, respectively. Y aoet al. deriv ed a voltagecontrol heuristic to reduce energy consumption, assuming that the pow er usage is a convex function of the clock
rate [7]. A benchmark suite and simulation environment
for voltage scaling are presented in [5].
In this paper, w e focus on hard real-time systems,
where meeting critical task deadlines is of paramount
importance [4]. Suc h systems are to be found in, for example, 
y-by-wire aircraft and spacecraft. We show how
to schedule voltage settings so that energy consumption
is reduced, while still guaranteeing that all task deadlines
1
0-7695-0713-1/00 $10.00 ã 2000 IEEE

are met. Our algorithms consist of an oine phase, in
which voltage settings are picked to reduce energy consumption assuming that tasks run to their worst-case execution times (wcet). How ev er, many tasks nish well
before their wcet, and we have an online phase which
adjusts the voltage settings on-the-
y to reclaim any resources released b y suc h tasks. Our n umerical results
indicate that substantial energy savings are attained.
The paper is organized as follows. In Section 2, w e
outline our system model. This is follo w edin Section
3 b y a scheduling algorithm which works for the case
where the tasks have a common period. In Section 4, arbitrary task periods are allowed, and a somewhat more
complex algorithm is used. We also show how this algorithm handles task sets whose phasings are not known
until run time.The paper concludes with a brief discussion in Section 5.
2. System Model

Most real-time systems used in critical embedded applications use periodic workloads. That is, each task, Ti,
has a period, Pi, and an iteration of Ti is released each Pi
time units. The deadline of a task is equal to the period.
That is, a task iteration must be done by the time the
next iteration of that task is released. The worst-case
execution time of each task is assumed to be known.
There is a huge literature on the problem of allocation and scheduling of tasks in real-time systems; for
a surv ey,see [4]. The typical approach is to carry out
an allocation of tasks to processors and then to run a
uniprocessor scheduling algorithm on eac h of the processors to decide when each task will execute.
In this paper, w e focus on the problem of uniprocessor scheduling. The task-sc hedulingalgorithms are
Cyclic and Earliest Deadline First (edf). Under a cyclic
schedule, a subset of tasks will be selected for execution in a minor frame while a set of minor frames iterate
periodically in a major frame [2]. As the term implies,
edf pic ks the task to run whose deadline is the earliest
among all the ready tasks. Ties are broken arbitrarily.
The edf algorithm is preemptive, and it is assumed that

preemption costs are negligible compared to the task run
times. It can be sho wn that under such assumptions,
and for task sets whose deadlines equal their respective
periods, edf is an optimal uniprocessor scheduling algorithm. That is, if edf cannot feasibly schedule some
task set, no other algorithm can, either. A periodic task
set with the deadline of each task equal to its period is
edf-schedulable i the task set utilization does not exceed 1; this is a ligh tw eigh
t schedulability test for the
edf algorithm. Note that, for a cyclic schedule, it is feasible if the task set utilization during every minor frame
does not exceed 1.
Our other assumptions are as follows:
A1 V oltage switching consumes negligible overhead.
A2 There is a time-of-day clock available to the system,
with suÆcient precision to time-stamp the completion of tasks and other signicant events.
A3 T asks are independent: no task depends on the output of any other task.
A4 The worst-case execution time of each task Ti, !i ,
is known. The actual execution time is not known,
ho wever, and may vary from one iteration to the
next: it is a random variable with distribution
Gi().
A5 The overhead of the sc heduling algorithm is negligible when compared to the execution time of the
application workload.
We now present tw o algorithms. The rst deals with
the tasks scheduled in a minor cycle under a cyclic algorithm. The second algorithm focuses in edf algorithms.
3. Algorithm 1: Cyclic Scheduling

F or eac h minor frame, all sc
heduled tasks are released at
the beginning of the frame and must nish by the end of
that frame. We assume that the tasks have a predened
order of execution.

3.1 Algorithm Description

used over a task period, subject to the need to meet all
deadlines. We record, for these labellings (i.e., voltage
settings), the time at which each task executes. This
information, together with the lv and hv labellings, allow us at any time to compute the total unnished work
remaining in the system at any time, t. Denote this
unnished w ork b y offline unf(t). Similarly, w e can
obtain the voltage setting at time t: denote this b y
offline setting(t).
Let us now consider the online phase of the algorithm, i.e., the scheduling algorithm that is used during actual execution. The system keeps track of the
w orst-case unnished work remaining in the system (i.e.,
total unnished w ork assuming eac h unnished task
takes its w orst-case execution time). Denote this b y
online unf(t).
The online scheduling algorithm is as follows.
 The task to be executed is chosen based on the
pre-dened order.
 At any time t, the processor is set at low voltage,
unless each of the following conditions is satised:
{ online unf(t) = offline unf(t).
{ The voltage setting of the processor in the of
ine phase at time t is high. (This does not
have to be stored separately: an examination
of the slope of the offline unf(t) line at t
pro vides this information).
Note that the algorithm does not need to keep track
of whether the online unf(t) < offline unf(t) condition is satised for every cloc k cycle (that w ouldbe
impossible). Instead, when a processor takes up a task,
it checks this condition. If the condition is true, and
the low-voltage setting is used, the system computes the
time  at which online unf(t) = offline unf(t). This
can be done easily , since w e know the rate of execution at high and low voltages, as also the rate at which
offline unf(t) declines with t. If the task is still executing at , and the voltage setting at that instant in the
oine phase is high, the processor is switc hedat that
epoch, to high voltage.
It is not diÆcult to show that the scheduling algorithm does not miss any deadlines if the original task set
is feasible: the proof of the following statements is left
to the reader.

The algorithm consists of tw o phases. In the oine (or
pre-processing) phase, so-called because it is executed
before the system is actually used, we simulate the task
execution, using the w orst-case execution times. The
purpose of the oine phase is to come up with a labelling of eac h task as either a high-voltage (hv) or a
Lemma 1 online unf(t)  offline unf(t), for all t.
low-voltage (lv) task. A task labelled hv (lv) will have
all of its iterations executed at high voltage (low voltage). The oine phase nds, b y a standard search al- Theorem 1 No deadlines are missed by the online
gorithm, the labellings that minimize the total energy algorithm.
2
0-7695-0713-1/00 $10.00 ã 2000 IEEE

3.1.1 Example

We illustrate this algorithm b y w alking through a single simulation. The task set consists of three tasks,
T0; T1; T2, all with period 10, and with worst-case highvoltage execution times 1.933, 3.678, and 1.888, respectively. If all tasks are run at high voltage, the processor utilization would be 0.75. Let the actual task highvoltage execution times be uniformly distributed over
the respective in terv als: [wcet/2, wcet], and assume
that the system works 50% slow er at low voltage.
The oine algorithm determines that the best oine
voltage assignment is low for tasks T0 and T2, and high
for T1. If tasks take their worst-case execution times, the
processor utilization is 0.941. The reader should note
how close this is to 1, which assures us that this simple
algorithm would be close to optimal if all tasks consume
their worst-case execution times.
By simulating this algorithm, it is easy to obtain
offline unf(t) (see Figure 1). offline unf(t) consists
of a set of straigh t-linesegments: b y storing the endpoints of these segments, the value of the function at
any t can quickly be computed. The oine part of the
algorithm is now over.
Consider now the operation of the online part. Suppose, in an execution, the actual high-voltage execution times of T0; T1; T2 were 1:53; 2:57; 1:87, respectively.
The corresponding low-voltage times are 2:30; 3:86; 2:80,
respectively. Since the processor is not an oracle, it
cannot know these execution times un tilafter the respective tasks have completed execution. (This means
that online unf(t) has potential downw ard jumps at
the epochs of task completion). T ask T0 starts executing, at low voltage, and completes at time 2:30. A t
this time, the algorithm knows that online unf(t) <
offline unf(t) for t = 2:30. As a result, it can
execute T1 at low voltage up to time , at which
offline unf() = offline unf(). It is easy to see that
 = 4:10. A t that instant, the system switches from low
to high voltage, and completes T1 at time 5:47. At this
time, T2 can be run at low voltage to its own completion, at time 8:27. The execution trajectory is shown in
Figure 1.

3.2 Perfo rmanceModel

In this section, we derive models to compute the energy
savings for our algorithm. First, we present a simple 
uid approximation that provides us with a low er bound
on the energy consumed. Then, we presen t a more exact analysis for a system consisting of a nite number of
3
0-7695-0713-1/00 $10.00 ã 2000 IEEE

tasks.
We start b y dening some notation. Note that all w orkloads are dened b y the time tak en to execute
them at high voltage.
off
V oltage setting at timet, specied
A (t)
b y the oine phase, for task set A.
worst(A) T otal execution time for task setA,
Toff
under the schedule developed by the
oine algorithm, if all tasks run
to their worst-case execution times.
actual(A) T otal execution time for task setA,
Ton
under the online algorithm.
worst
W
(A) Total workload due to task set A
if all tasks run to their
w orst-case times.
W actual(A) Actual total workload due to
task set A.
worst
Eactual
(
A
)
;
E
off
off (A) Actual and worst-case
energy consumed, respectively, if
the tasks in set A run to the settings
prescribed by the oine algorithm.
Eactual
on (A); Eworst
on (A) Actual and worst-case
energy consumed by tasks in set A.
Uworst
H (A) w orst-case processor utilization if all
tasks in task set A are run at high
voltage.
P
Common period of all the tasks.




Power consumption at low voltage
Power
consumption
at high voltage
Clock rate
at high voltage
Clock rate at low voltage

Where the task set is ob vious fromthe context, no
argument is provided to functions. For example, if w e
are talking about just one task set, Uworst
H would be the
w orst-case processor utilization for that task set.
Throughout, w e assume that = < 1; otherwise,
there would be no point in running anything at low voltage!

3.2.1 Fluid Approximation

In this model, we assume that the workload consists of
tasks whose execution times are independent and identically distributed, with w orst-caseexecution time (at
high voltage), . The n umber of tasks, ntasks ! 1
and  ! 0 in such a way that the total worst-case workload, ntasks  = W , a constant. Uworst
H = W=P, where
Uworst
is
the
worst-case
processor
utilization
if all tasks
H
actual
are run at high voltage. Let W
be the actual total
workload.
If all tasks run to their w orst-caseexecution times

The oine scheduling algorithm picks the voltage
and we use the schedule and voltage settings generated
b y the oine phase of the algorithm, the total busy time settings per task to minimize the energy consumed, under the constraint that the entire task has to be exefor the processor over the period is given b y
cuted
at the prescribed voltage setting (e.g., one cannot
worst = minfW wor st; Pg:
Toff
execute half a task at high and the other half at low
settings).
Since we have an innite number of tasks, with probaworst(A) > Toff
worst(B), then Eworst
If Toff
off (A) <
bilit y 1, the online algorithm will consume
worst
wor
st
Eworst
(
B
)
(since
W
(
A
)
=
W
(
B
)
).
off
actual = minfW actual; Toff
worstg:
Ton
Now, suppose we run task set B using setting A (t)
at time t. We can do this because the tasks in B are inWe now have tw o cases.
nitely short, and so the voltage setting can be switched
ac
tual
worst
at
any time by the oine algorithm.
Case 1: W
  Toff : In this case, with probaThis will result in task set B taking exactly the same
bilit y 1, the online algorithm will keep the entire w orkw
orst-case
execution time as task setA. In such a case,
load at low voltage, and so the energy consumed will be
B
will
use
less
energy than it did under the B (t) oine
E = 100% of that at high voltage.
voltage setting. This contradicts the fact that the oine
wo rst: Some of the workload will algorithm picks v oltage settings to minimize the energy
Case 2: W ac tual > Toff
have to be run at high voltage. Let th and t` be the time consumed. QED
over which the processor is run at high and low voltsets A and B are as dened
wo rst= P when Theorem 2 Suppose task
age, respectively. Clearly, th + t` = Toff
actual
in
Lemma
2.
Then,
E
on (A)  Eactual
on (B).
worst.
W actual > Toff
ac tua (lA) 
The total workload is W actual seconds at high volt- Proof:
F rom the algorithm, Ton
actual
worst
actual(B) =
age. Since the processor runs  times slower at low minfW
(A); Toff (A)g and Ton
worst(A)g. Since W ac tual(A) =
voltage, we must have
minfW actual(B); Toff
actual
worst
worst(B) (from Lemma
W
(B) and Toff
(A)  Toff
W actual = t`= + th
actual
actual
3), Eon (A)  Eon
(B). QED
) t` =  (P - W actual) since th + t` = P

-1

3.2.2. Relaxing the Innite-Task Assumption

The energy consumed, as a percentage of the all-high- Relaxing the innite-task assumption complicates the
voltage setting, is therefore given b y
analysis. We would then ha vea model with a nite
n umber of tasks, each with a certain execution time dist` + th  100
E
=
tribution.
One can construct a model which can then be
actu
al
W
solv ed n umerically
.
( - 1)P + (1 - )W act ual
 100
=
The
most
practical
approach is to construct, given
-1
the
worst-case
execution
times, the oine schedule. This
This analysis provides us with a lower bound to the energy consumed under this algorithm for a total actual yields us a plot of the unnished work over time that
w orkloadof W ac tua .l We now pro vethat this is the is used as a template b y the online algorithm. Then,
one conditions on the actual execution times of the oncase.
line tasks: giv en thesetimes, a numerical evaluation is
Lemma 2 L et A and B be two task sets such carried out to determine the voltage settings over time
that W worst(A) = W worst(B) and W ac tual(A) = for the online phase, and thus compute the energy conW actual(B). A has a nite number of tasks; B follows sumed. That is, if zi is the actual execution time of
the innite-task
mo del. Then,
task Ti, w ewill obtain E(z1 ;    ; zntasks ); the energy
consumed under these conditions.
worst(A)  Toff
worst(B):
Toff
Then, we uncondition on the actual execution times,
obtaining the overall energy consumed as:

Z 
1 Z 
ntasks
E(z1 ; ; zntasks )dGntasks (zntasks )
z1 =0 zntasks=0
dG1 (z1 )

Proof: We proceed by contradiction. Suppose the lemma is false, and there do exist task sets A and B which
constitute a counter-example.





4
0-7695-0713-1/00 $10.00 ã 2000 IEEE



This integral can be evaluated either through numerical
means or simulation.
It only remains for us to sho w ho w to deriv e
E(z1 ;    ; zntasks ).
We will need some further notation for this.

(i) slac k time available at the end of the task Ti
execution, i = 1; ; ntasks . That is,
if task Ti ends at time i , the slack is the
time remaining to when Ti completes

Solving these equations yields:

i-1  - 1
1 = 
i - i--11
We will run Ti for up to 0 at low voltage: if it still has0

=

n't nished, we will run it to completion at high voltage.
We have two subcases:



Case 2a. zi  0 =: In this case, the entire execution
of Ti can be done at low voltage. We can therefore write:

in the oine schedule,
i.e., i = 
1 +    + 
i - i .
For convenience, dene 0 = 0.
low P ow er consumption at lo
w voltage
high P ow er consumption at high oltage
v
1
if oine voltage setting is low for Ti
i
 otherwise
i Finishing time of Ti in the online schedule,
i = 1;    ; ntasks . F or convenience,
dene 0 = 0.
"i Energy consumed by Ti in the online schedule.

i = i-1 + zi 
"i = zi low
i = 
1 + + 
i - i
Case 2b. zi > 0 =: In suc h a case, we rst execute
Ti for 0 seconds at low voltage, and then switch to high


voltage for the rest of the execution. We therefore have:

i

=
=

"i
i

We have tw o cases.

=
=

i-1 + 0 + zi - 0 =
i-1 + 0  - 1 + zi
0 low + (zi - 0 =)high

1 + + 
i - i


Case 1: Oine setting of task Ti is low: In suc h a case, The total energy consumed is then given b y
task Ti will also be executed at low voltage in the online
E(z1 ;    ; zntasks ) = "1 +    "ntasks
schedule. Then we can immediately write:

"i
i
i

=
=
=

zilow
i-1 + zi
i
X
i=1

3.3 Simulation Results

We present here results of a simulation written from rst
principles. In our experiments, we assumed that at high
voltage, the pow er consumption was 0.165 watts and at
low voltage, it was 0.033 watts. The clock rate at high
voltage is 50% higher than at low voltage. All execution
times are specied in terms of the high-voltage setting.
We assumed that the actual execution time of task Ti
varies uniformly in the interval [a!i ; !i ], where a is a
constant and !i is the worst-case execution time of Ti.
The common period was set to 10.
Given the processor utilization at high-voltage (i.e.,
the utilization if all the workload was executed at high
voltage), the task execution times were generated randomly to meet this requirement.
Figure 2 shows the energy consumption for an 8-task
system for various processor utilizations at high voltage,
UH . By \percentage online consumption" we mean the
energy consumed by the online algorithm as a percentage of the consumption of the processor if ev erything
were run at high voltage.


i i - i

Case 2: Oine setting of task Ti is high: The total
time available to execute Ti is 
i + i-1 . Since the
task may in volve up to 
i units of high-voltage work,
w e ha veto compute ho w muc h of the task can safely
be done at low-voltage and still leave enough time for it
to be completed, even if it runs to its worst-case time.
Let 0 be the maximum time that it can be run at low
voltage without being in danger of missing its deadline.
Now, dene 1 to satisfy the following equations:

0 = + 1
0 + 1

=
=


i

i + i-1

F rom these equations, the reader can easily recognise
that maxf1 ; 0g is the time available for high-voltage execution, should that prove necessary.
5
0-7695-0713-1/00 $10.00 ã 2000 IEEE

T able 1 shows some experimental results on the average processor utilization, U, that would result from using
the voltage settings generated by theoine algorithm,
if eac h task ran to its worst-case time. Except when all
tasks can be run at low voltage, U is extremely close to
1 even for small task sets. U is a measure of how close
the oine algorithm is to optimal: in the optimal case,
we would have the processor utilized 100% at worst-case
execution times.
Figure 3a is a plot of the energy consumption of the
online schedule as a percentage of that obtained by using just the settings of the oine phase of the algorithm.
It indicates the gains that are possible when the scheduler reclaims resources after a task has completed before
its w orst-case execution time would predict. When the
utilization of the task set is small, everything can be executed at low voltage, and there is nothing to be gained
from the online phase. As the utilization increases beyond this region, the savings of the online phase steadily
increase. Resource reclaiming is greatest when a = 0,
and decreases as a increases. Clearly, when a = 1, there
is no resource reclaiming possible and the online energy
consumption is the same as that using just the oine
settings.
We next consider the impact of the size of the task
set. As the number of tasks increases, tw o things happen. First, the oine algorithm has more 
exibility in
making its pow ersettings, and consequently is able to
get the w orst-caseprocessor utilization with its power
settings closer to 1. We have alrady seen this in Table 1.
Also, the resource reclaiming opportunities increase with
the n umber of tasks. (T otake an extreme example, if
the entire task set consists of just one task, there can be
no reclaiming. If it consists of tw o tasks, the reclaimed
time from just one task can be used.) As a result, the
online energy consumption as a percentage of the corresponding oine energy consumption decreases with the
n umber of tasks. This is shown in Figure 3(b).
4. Algorithm 2: EDF Scheduling

In this section, w ediscuss voltage-clock scheduling for
the edf algorithm. In addition to Assumptions A1 to
A5, we have:

If each of these conditions is true, the voltage setting is
high at time t.

4.1 Proof of Correctness

Denote the online executing task at time t b y
online task(t). Dene iteri (t) = bt=Pic, where Pi is
the period of task Ti. Dene Ti;m as the m iteration of
task Ti.
0

0

Lemma 3 If online task(t) 6= offline task(t),
then the online schedule has already completed the iterofflinet ask(t)(t) th execution of task
offline task(t).
0

Proof: Suppose this lemma is not true. We have,
from the denition of the model, that online task(0) =
offline task(0), so if the lemma is un true, there exists some t > 0 which the earliest time at which
online task(t) 6= offline task(t) but the online schedule has not yet nished the iterofflinet ask(t) (t) th execution of task offline task(t).
Let offline task(t) = Tj;n and online task(t) =
Ti;m. By denition of t, Tj;n is not yet done in the online
schedule at time t. Since Ti;m is being executed instead
b y the online schedule at t, w emust ha veTi;m  Tj;n
(A  B means that A has higher priority than B).
0

A6 T ask phasings are known in advance.
This extra assumption can be relaxed as we show at the
end of the section.
Algorithm 2 is very similar to Algorithm 1, except
in the data that are collected. It consists of oine and
online parts.

0

0

0

0

0

6
0-7695-0713-1/00 $10.00 ã 2000 IEEE

The oine part consists of selecting the voltage settings that will minimize the total energy used over
the LCM of the periods, while still maintaining EDFschedulability. Following this, the schedule, using the
EDF algorithm and the w orst-case execution times,
is generated, and the functions offline unf(i; t) are
computed. offline unf(i; t) denotes the unnished
w ork under the oine schedule of task i at time t.
offline unf(i; t) consists of straigh t-linesegments for
each task i, and so only the end-points of these segments must be stored. Also stored is offline task(t),
which is the task which is executing at time t. When
these functions have been obtained up to the LCM of
the task periods, the oine phase ends.
The online part also uses the EDF scheduling algorithm. At any time t, the voltage setting is at low unless
each of the following conditions is satised (i is the online executing task):
 i = offline task(t).
 The unnished work of the executing task (based
on the worst-case execution times) at time t is equal to that of offline unf(i; t).
 offline setting(i) = high.

0

By denition of t,  < t, if online task() =
offline task(), then the online schedule has already
nished offline task() b y time .
Note that w e cannot ha ve online task(x) =
offline task(x) x < t. If this w ereto happen, then
8

mediately from this and the voltage-setting rule of the
algorithm. QED
F rom Lemmas 3 and 4, we have the following theorem:

6

8

the oine and online schedules w ould both be exactly parallel until time t. In particular, task Ti;m w ould
execute at precisely the same interv als in both the of
ine and online schedules prior to t. But, since Ti;m is
done by the oine schedule before t, it follows from the
voltage-selection rule in Algorithm 2 that it would also
be done in the oine schedule before t, which con tradicts the assumption that online task(t) = Ti;m.
The assumption that the lemma is false therefore
requires that there must be some time y < t suc h
that online task(y) 6= offline task(y). But, from
the denition of t, w e must have for every z < t,
online task(z) = Ti;m whenever offline task(z) =
Ti;m: otherwise, by the denition of t, Ti;m w ould have
been completed before t in the online schedule. Let us
now consider tw o cases:
Case 1. The oine voltage setting of Ti is low.
In this case, since the oine schedule nishes executing
Ti;m b y time t, so must the online schedule, since the
oine schedule assumes worst-case execution times. So,
Case 1 cannot happen.
Case 2. The oine voltage setting of Ti is high.
During times when both the oine and online schedules are executing Ti;m, the online schedule will only use a low-voltage setting at some time u when
online unf(i; u) < offline unf(i; u). From this, and
the fact that Ti;m is executed in the online schedule
whenever it is executing in the oine schedule, it follows that Ti;m must have nished in the online schedule
before t.
We therefore have a contradiction: no such t exists,
and so the proof is complete. QED
0

0

0

0

0

0

0

0

0

0

Lemma 4 Every iteration is completed in the online
schedule no later than when it is completed in the
oine schedule.

Proof: Suppose this is not true, i.e., that there exists
some iteration Ti;m which completes in the oine schedule before it has completed in the oine schedule.
Let t be the time at which Ti;m completes in the
oine schedule. By the preceding Lemma, Ti;m will
execute in the online schedule whenever it does so in
the oine schedule (since otherwise it would be done in
the online schedule ahead of t). The result follows im0

0

0

7
0-7695-0713-1/00 $10.00 ã 2000 IEEE

Theorem 3 A ll task deadlines are met by the online
algorithm.

4.2 Analysis

An analysis of Algorithm 2 can be done along the same
lines as for Algorithm 1. How ever, since task periods
can be dierent, the number of special cases that have
to be considered is very large. Analysis is only useful
when it either produces a compact expression that oers
insight into performance, or when it allows for faster performance ev aluation than simulation. The analysis for
Algorithm 2 would be so complex that it w ouldlikely
satisfy neither requirement. Accordingly, w e ha verestricted ourselves to simulation for studying the performance of Algorithm 2.

4.3 Numerical Results

The experimental setup for these runs has been brie
y
described earlier. The only dierence is that the task
periods are chosen randomly to be integers betw een1
and 11. Figure 4 shows the energy consumption for an
8-task system for various processor utilizations at high
voltage, UH .
Figure 5 mirrors Figure 3 of the previous section, and
has similar characteristics.

4.4 Relaxing Assumption A6

Let us no w relax A6, and assume that task phasings
are not known in advance. As before, we can compute
the oine voltage settings, since these
depend only on
the need to keep worst-case execution times so that the
task set utilization does not exceed 1. How ever, we cannot precompute the oine schedule. Instead, the oine
schedule must be generated on-the-
y, as tasks arrive.
In other words, the system builds up the oine schedule
as tasks arrive, assuming that the oine voltage settings
are used and that each task runs to its worst-case time.
As the oine schedule is generated, the system can follow Algorithm 2 to pick the appropriate voltage setting.
T o combine the simulation of an oine on-the-
y
schedule and the voltage-clock schedule, w ecan adopt
a slack-time queue (ST-Queue) to track the slack times
resulting from early task completions. Note that a task
can execute during the slack time of a nished task or
during the period assigned to it in the oine schedule.
In the normal edf task queue (TK-Queue), w euse t-

exploit the fact that pow er consumption tends to drop
quadratically with voltage, while circuit delays (and thus
the clock period) increase only linearly. Our algorithms
have oine and online components. The oine component assumes that the tasks run to their worst-case
execution times, and computes the voltage settings to
minimize energy consumption. The online component starts with the oine voltage settings as a base, and
then reclaims any time resources that are released b y
tasks which nish ahead of their predicted worst-case execution times, thus making for a further round of energy
savings. Our results indicate that signicant energy savS1 When task Ti arriv es, it is inserted into TK-Queue. ings are made possible, while guaranteeing that all tasks
will contin ue to meet their deadlines.
The variables cst and ct are set to 0.
wo variables to keep track of the computation times for
each task. The rst one, cti , species the computation
time that task Ti has consumed during its scheduled period of the oine schedule. This allows us to compute
a task's slac k time when it nishes. The second variable, csti , indicates how long a hv-mode task can stay
in lv-mode execution after it steals slack time. As in the
TK-Queue, the slac k times of the completed tasks are
ordered according to a task's deadlines in the slack-time
queue.
The steps to perform voltage-clock scheduling are as
follows:

i

i

S2 When task Ti completes, a slack time sti = !i - cti
is inserted into ST-Queue if the dierence is
greater than 0.

References

[1] J.-M. Chang and M. P edram, \Energy minimization using multiple supply voltages," IEEE Trans.
VLSI Systems, V ol. 5, No. 4, December 1997, pp.
436{443.
[2] C. D. Locke, \Software Architecture for Hard Realtime Applications: Cyclic Executives vs. Fixed Priority Executives," Journal of Real-Time Systems,
Vol. 4, 1992, pp. 37-53.
[3] M. M. Khellah and M. I. Elmasry, \P ow erminimization of high-performance submicron cmos
circuits using a dual-Vdd dual-Vth (DVDV) approach, Proc. 1999 International Symp. LowPower Electronics and Design, pp. 106{108, 1998.
[4] C. M. Krishna and K. G. Shin, R eal-Time Systems, New York: McGraw-Hill, 1997.
[5] T. Pering, T. Burd, and R. Brodersen, \The simulation and evaluation of dynamic voltage scaling algorithms," Proc. 1998 International Symp. LowPower Electronics and Design, pp. 76{81, 1998.
[6] M. Weiser, B. Welch, A. Demers, and S. Shenker,
\Sc heduling for reduced CPU energy," Pr oc.
USENIX Symp. Operating Systems Design and
Implementation, pp. 13{23, 1994.
[7] F. Yao, A. Demers, and S. Shenker, \A scheduling
model for reduced CPU energy," Proc. 36th IEEE
Symp. F oundations of Computer Science, 1995,
pp. 374{382.

S3 When the processor is idle (i.e. TK-Queue is empty), the slack time at the head of ST-Queue decreases ev ery unit of time. Once it reac hes zero,
the slac k time is deleted fromST-Queue.
S4 When a task Ti is dispatched (under edf), it can
consume slack time stj at the head of ST-Queue,
if task Ti has a deadline greater than task Tj. If
offline setting(i) = high, we can switch the setting to low for an additional period stj -1 (to be
accumulated in csti ).

S5 When a task Ti cannot nd any available slack time
for its execution, it is executed at the voltage-clock
mode offline setting(i) if csti = 0 or at lv-mode
if csti > 0. Also, the time used in its computation
is then accumulated in cti .

It can be shown that, at step S5, i = offline task(t)
when a task Ti cannot nd any available slack time for its
execution. Thus, the slack time due to an early completion can be computed correctly by sti = !i - cti . Also,
if csti = 0 at time t and offline setting(i) = high,the
unnished work of the executing task Ti (based on the
w orst-case execution times) at timet is equal to that of
offline unf(i; t).
5. Conclusion

In this paper, w eha ve described simple algorithms for [8] Intr oduction to Thumb, ARM Documentation,
voltage scaling in real-time systems. These algorithms
Advanced RISC Machines, Ltd.

8
0-7695-0713-1/00 $10.00 ã 2000 IEEE

Percentage Online Consumption

8

6

4

onl

offl

ine

2

ine

_un

f(t)

_un
f

(t)

0
0

1

2

3

4

5

6

7

8

9

10

90

8-task system

80
70

50

a = 0.00

40
30
20
0.6

0.7

0.8

1.0

Task T1
completes

Figure 2. P ercen tage Energy Consumption
Online Percentage of Offline Energy

Online Percentage of Offline Energy

0.9

Worst-Case Task Set Utilization

Figure 1. Oine and Online Trajectories

8-task system

100

a = 0.85

90
80

a = 0.75

70

a = 0.50

60

a = 0.25

50

a = 0.0
0

40
0.6

0.7

0.50
a=
5
a = 0.2

60

time
Task T0
completes

5

a=

0.7

0.8

0.9

1.0

Worst-Case Task Set Utilization

100

Worst-Case Task Set Utilization = 0.90
90

2 tasks
80
70

4 tasks
6 tasks

60

8 tasks

50

10 tasks

40
0.0

0.2

0.4

0.6

(b) Eect of Number of Tasks

Online Percentage of OfflineEnergy =

Energy Consumption of Online Algorithm  100
Energy Consumption of Offline Algorithm

Percentage Online Consumption

Figure 3: Online as a Percen tage of Oine Energy Consumption
80

8-task system

70

5

a=

60

0.7

0

a=

50
40

0.5

a=0.25

0

a = 0.0

30
0.7

0.8

0.9

1.0

Worst-Case Task Set Utilization

Figure 4: P ercen tage Energy Consumption

9
0-7695-0713-1/00 $10.00 ã 2000 IEEE

1.0

a

(a) Eect of Reclaiming

0.6

0.8

Online Percentage of Offline Energy

Online Percentage of Offline Energy

8-task system

100

a=0.85
5
a=0.7

90
80
70

0.50

a=

60

a=0.25

50
40
0.6

a=0.00
0.7

0.8

0.9

1.0

Worst-Case Task-Set Utilization

100

Worst-Case Task-Set Utilization = 0.9
90
80
70
60

2 tasks

4 tasks
6 tasks

50
40
0.0

(a) Eect of Reclaiming

8 tasks

10 tasks
0.2

0.4

2
0.900
0.975
0.893
0.892
0.924
0.898
0.916
0.953

Energy Consumption of Online Algorithm  100
Energy Consumption of Offline Algorithm

No of Tasks
4
6
0.900 0.900
0.975 0.975
0.975 0.992
0.979 0.995
0.982 0.996
0.974 0.994
0.968 0.991
0.967 0.983

8
0.900
0.975
0.997
0.999
0.999
0.999
0.998
0.993

Note: All utilizations are for worst-case task run times.
T able 1.Average Processor Utilization with Oine Settings

10
0-7695-0713-1/00 $10.00 ã 2000 IEEE

1.0

a

Figure 5: Online as a Percen tage of Oine Energy Consumption

0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95

0.8

(b) Eect of Number of Tasks

Online Percentage of OfflineEnergy =

UH

0.6

496

IEEE TRANSACTIONS ON COMPUTERS,

Errata

C.M. Krishna, Senior Member, IEEE, and
Yann-Hang Lee, Member, IEEE

æ
IN Section 4 (page 1591, column 1) of this paper [2], some sentences
describing the updating of the online unfinished work were
inadvertently omitted from the materials submitted for publication. These sentences (which should be inserted after “If both these
conditions are satisfied, the processor runs at high voltage”) are as
follows:
The management of the online unfinished work must be done
according to slack times and task deadlines. In [1], we showed a
way in which, when a task finishes, the online unfinished work can
be updated and the released time reclaimed. In this paper, we
adopt the following strategy:
When a task finishes without using its worst-case execution
time, the slack time can be released to other tasks which have
deadlines greater than that of the finishing task. Assume that
task TA is scheduled to run at time t. the difference between
offline_task(t) and online_task(t) can be simply represented as the sum of two terms. The first term is the slack time that
can be released to the task TA at time t. The second term is the
difference between the unfinished work of TA in the offline
schedule (with inflated execution time) and the unfinished work of
the real task TA .

REFERENCES

[2]

NO. 4,

APRIL 2004

_________________________________________________________________________________________________________________________

Addendum to
“Voltage-Clock-Scaling Adaptive
Scheduling Techniques for Low Power
in Hard Real-Time Systems”

[1]

VOL. 53,

Y.H. Lee, Y. Doh, and C.M. Krishna, “EDF Scheduling Using Two-Mode
Voltage Clock Scaling for Hard Real-Time Systems,” Proc. Int’l Conf.
Computers, Architecture, and Synthesis for Embedded Systems (CASES), 2001.
C.M. Krishna and Y.-H. Lee, “Voltage-Clock-Scaling Adaptive Schedule
Techniques for Low Power in Hard Real-Time Systems,” IEEE Trans.
Computers, vol. 52, no. 12, pp. 1586-1593, Dec. 2003.

. C.M. Krishna is with the Electrical and Computer Engineering Department, University of Massachusetts, Amherst, MA 01003.
E-mail: krishna@ecs.umass.edu.
. Y.-H. Lee is with the Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287. E-mail: yhlee@asu.edu.
For information on obtaining reprints of this article, please send e-mail to:
tc@computer.org, and reference IEEECS Log Number 112768.
0018-9340/04/$20.00 ß 2004 IEEE

Published by the IEEE Computer Society

2009 IEEE Conference on Commerce and Enterprise Computing

Ontology-based Service Composition Framework for Syndicating Building Intelligence
Wei-Tek Tsai, Yann-Hang Lee, Avi Wiezel*, Xin Sun, Wu Li
Computer Science and Engineering Department
School of Construction*
Arizona State University
Tempe, AZ 85287-8809, USA
{Wei-Tek.Tsai, yhlee, avi.wiezel, xin.sun, wu.lu}@asu.edu
Abstract

1.1

Cost savings in building construction and operation
stages have always been a primary concern in the AEC
(Architecture, Engineering, and Construction) industry.
One of the approaches for cost saving is to adopt a
single tool and information source that all building
project teams can share and collaborate. Starting from
a 2-D drawing, BIM (Building Information Model) has
evolved into a single, object-oriented database for
storing the information of all objects.

Building Information Model (BIM) encompasses
building geometry, spatial relationships, geographic
information, and quantities and properties of building
components[1]; however, it does not include any
information related to the functions that a building
might require. On the other side, Service–Oriented
computing emerged as a promising technology to
enable fast, efficient software development. Even
though features such as standard protocols, loosely
coupled components, ontology-enabled dynamic
discovery and composition of SOA seems a perfect fit
for the building construction domain, the potential
gain of applying SOA techniques has not really been
discovered. In the paper, an ontology-based service
composition framework for construction domain is
proposed, and BIM is extended to include function
requirements and service specifications. An example is
shown to demonstrate the advantages over traditional
techniques also.

1.

The core data structure in BIM is comprised of
Industry Foundation Classes (IFC). IFC represent the
building in a way that allows the following [3]: 1. Data
exchange among software applications within the AEC
and facility management industry sectors; 2. Modelbased descriptions of spatial elements, building
elements, MEP (Mechanical, Electrical and Plumbing)
elements and other components that constitute a
building or facility; 3. Shape representations of such
components (visualization); 4. Relationships of such
components between each other and to the spatial and
system structure; 5. Attachment of properties,
classifications, external library access, etc. to such
components.

Introduction

To many people’s surprise, the cost of operating and
maintaining a building throughout their lifetime, also
known as running cost, is much more expensive than
the cost of constructing it. The performance of a
building
(such
as
energy
efficiency)
is mainly influenced by original building design,
however, budget-limitations, planned-cost-deferring,
and developers’ lack of initiatives of constructing highperformance building [2] often result in less than ideal
building design, which in return leaves the tenants with
only one option to reduce running costs: using
intelligent facility management techniques. Intelligent
management can reduce operating costs by as much as
50% for old buildings and 20% for newer buildings [3].

978-0-7695-3755-9/09 $25.00 © 2009 IEEE
DOI 10.1109/CEC.2009.89

Building Information Model

Unfortunately, as BIM was driven by the needs of the
designers and builders, it does not provide descriptions
of the functions of the building. These functions, called
building services, are left to the latitude of the tenant or
the building manager. Examples of building services
are heating and cooling patterns, elevator movement,
signaling of best available evacuation path, tenant
tracking in high-security building, etc. As stated above,
optimization of these services, by employing
intelligent building control systems, can reduce
operating costs by 20% to 50% [3].

445

1.2

day-to-day operations, as well as unexpected events
such as random safety hazards, security attacks, and
paramedical emergencies. To attain the goal, our
approach will be focused on the following two
aspects:

Ontology

Ontology has received significant attention in recent
years with the emergence of semantic web. It is an
explicit specification of conceptualizations which
organizes the semantic information as the knowledge
base of the specific application domains [4, 5].
Furthermore, its use is extended to Service-Oriented
Computing (SOC) to facilitate more intelligent service
discovery and composition. Ontology has been
established for context-aware and smart home
applications [6]. For instance, in [7], a Getting up
scenario is described to show whether the ontologybased model is valuable for description of context
information in a home domain. In [8], an ontologybased model of the Tele-health Smart Home to
initialize the Bayesian networks to recognize patient
activity. Furthermore, ontology is used to model the
relations between devices and services in a home
environment and to manage software configurations
[9].

2.

1. An ontology-based knowledge representation
for the semantic content and process model of
intelligent building applications: The knowledge
representation is to provide a conceptual description of
services in buildings that can be inquired and searched.
The ontology will be an extension of BIM IFCs to
include service classification, description and
reasoning mechanism, as well as the function aspects
and relations of construction products. This building
service ontology will deal with the specification,
analysis, evaluation, and ranking of building functions,
and the related techniques and tools such as description
language and wiki-style repository.
2. An integrated composition, execution and
policy enforcement framework for building
services: The framework will use service flow model
and the knowledge in building service ontology to
construct execution plans based on the available
devices and occupant’s preference in specific
buildings. It will include a service composition engine,
service deployment and execution environment,
simulation and verification facilities, and policy-based
monitoring and enforcement mechanisms.

Objectives

It is foreseeable that a modern building will emerge
into a physical space full of embedded appliances and
devices. Rather than developing specific software for
some required services with a given set of embedded
devices, it is tempting to create a fertile ground on
which innovative building services can be discovered,
composed, and published. Existing services can be
imported and tailored based on the building equipment
and occupant’s preferences, and work collaboratively
with the services/information/knowledge available on
the Internet.

3.

Ontology-based Service Composition
Framework

An overall framework is shown in Figure 1 and has a
layered architecture with four layers. This overall
framework is designed so that it is relatively easy to
change, add, and delete information.

The proposed framework is to devise an infrastructure
on which intelligent building services can be created,
discovered, composed, verified, and deployed. The
services are to manage building facilities for static or

Figure 1 Overall Framework

446

good for software services only at this time. All of
these ontology systems can be represented in XML.

Layer 1 Ontology, Service Repository, BIM, User
Interface, and Wiki Layer: This layer hosts the
information that a user can browse, contribute, share,
evaluate, and rank all the reusable assets regarding
intelligent building facility management.

PSML-S classifies each element into six distinct
groups: Actors, Conditions, Data, Action, Timing, and
Events, as well as elements that have various
relationships such as used-by, owned-by, changed-by,
Composedof, equivalent, disjoint, overlapping,
subClassOf,
superClassOf,
complement,
and
inverseOf, . By labeling elements into distinct groups,
various kinds of relational analyses can be performed,
such as transitive closure relationships. A significant
advantage of PSML-O is that it is compatible with
PSML-S (for specifying services) and PSML-P (for
specifying policies); all three use the same element
model. Thus, it is possible to have only one engine that
can automatically generate application code and
simulate applications.

Layer 2 Composition, Policy Editing, and
Simulation Layer: This layer consists of three key
services:
•

Facility Service Composition Editor (FSCE),

•

Facility Policy Editor (FPE)

•

Facility
Management/Policy
Engine (FMPS).

Simulation

A user can use FSCE to develop facility management
applications based on the requirements, and BIM
information, and other information provided in Layer
1. The FPE is an editor that allows regulators, facility
managers, and users to input their policies. Policies can
be global (applies to many devices), local (applies to
single devices), and temporal (depending on time and
events). The FMPS allows a user to simulate facility
applications before deployment. All these components
take input from the repositories at Layer 1.

BIM adds numerous dimensions to the ontology
information that is contained in WSDL, OWL-S and
PSML-O, particularly related to topology, shape,
architecture, and material information. These will add
new kinds of relationships, such as located-at,
connection-at, structural compatibility, stress-support,
construction order, heat-transfer-rate, and triggeringevents. These additions will greatly enrich the
representations of building facility information to the
existing ontology techniques and expand the scope of
analyses including architectural analysis, cooling
effect, and elevator load, that are not possible in
traditional ontology systems.

Layer 3 Execution and Monitoring Layer: This
hosts the service execution and monitoring engine.
This will issue commands to appliances or devices in
buildings to facilitate various activities. Key status and
activities will be monitored, and various policies
including security and safety policies will be enforced
regardless of the commands issued.

Previous research from authors such as completeness
and consistency algorithms, relational analysis, and
temporal analysis developed in can be applied to
verify the correctness and consistency of the ontology.
Furthermore, if a building facility also has a wellknown problem, such as a pipe or duct collision, this
can be analyzed using ontology information.

Layer 4 Communication Network Layer: This will
use the industrial standard BACnet [10] [11] together
with the Internet as the communication infrastructure.
BACnet has been adopted by numerous construction
firms and it provides the communication
interoperability among appliances in building facilities.

3.1

3.2

Facility Service Ontology (FSO)

Facility Service Ontology (FSO) is the foundation core
of this framework. The existing BIM ontology consists
mainly of classes and their attributes only; it does not
include any operational services. An application
engineer needs to develop application code, such as
class methods, to implement the functionality. The
reuse level is low for intelligent facility management.
Several important service specification standards are
available, notably WSDL (Web Service Description
Language) [12], OWL-S (Web Ontology Language for
Service) [13], and PSML-O (Process Specification and
Modeling Language for Ontology), but they are mainly

Consumer-Centric SOA and UserCentric SOA with Various Repositories

In traditional service-oriented architecture (SOA),
services are published by service providers, which an
application engineer needs to search and discover.
Thus, the support of SOA is mainly for service
providers. Our framework mainly aims to support
facility users rather than service providers, a new kind
of SOA will be needed. Several new SOA variants
have been proposed in the literature including CCSOA
(Consumer-Centric SOA where users can publish their
needs and let service providers discover requirements,
and publication of workflows and application
templates), UCSOA (User-Centric SOA where a group

447

of users interested in a domain will create a set of
domain-specific services useful in that domain), and
UISOA (User Interface SOA where user interface can
be composed like application services) [14]. In this
proposed framework, a UCSOA will be adopted as
users can now publish, share, evaluate, and rank
sample applications, as well as policies.

extinguishers need to be triggered and an escape route
needs be to calculated and returned to people at
different locations at the moment within a very short
time, therefore, real time computing is a necessity in
IDEE. The IDEE is fully compatible with the real-time
SOA architecture proposed in [19] . The Service
discovery, composition, deployment, execution and
management can all be restricted with real timing
constraints.

These new features will change the existing SOA
publishing,
matching,
discovery,
composition
processes and algorithms. In general, the publishing
will be more involved as verification will be performed
before an item can be published, and relationships with
other published items must be updated, and thus a
publication will be more involved. However, matching
and discovery processes will be easier as various
relationships among published items, such as an
application template consisting of a specific workflow
with a certain number of services, will be known, and
thus discovery and composition processes will be
much easier than the traditional SOA. To support
these, various repositories are needed to store various
assets including application templates and policies.
Figure 2 illustrates the updated SOA infrastructure.

3.3

3.4

Integrated Execution and Simulation
Environment with Policy Enforcement

The proposed framework also includes the following
Integrated Development and Execution Environment
(IDEE) for policy-based computing framework, as
shown in Figure 2. The components in the bottom layer
provide the domain knowledge representation and
system configuration (i.e., device registration and
proximity description) used to construct services and
policies. The components in the middle part of the
figure focus on developing and analyzing policies and
services of the target system. The components in the
upper part are responsible for execution, policy
enforcement, and validation.

Real Time Support

Some of the building functions are mission critical. For
example, in case of a fire accident, the ceiling fire

Figure 2 IDEE Overview
information related to natural environments, buildings,
In IDEE, ontology is introduced to support the
and users. When physical devices in a building
semantic discovery of devices, services, policies, and
complex are registered, and the facility information is
proximity description. Device ontology describes the
populated, services and policies in the complex can be
concepts related to devices and equipment in a facility
specified by instantiating the referred objects in the
complex. Function ontology, as the core of the whole
function templates and policy metadata.
knowledge base, describes the concepts related to
IDEE integrates functional specification, policy
functions and templates attached to each composable
specification,
verification,
code
generation,
function. Policy metadata and specification describe
deployment, simulation, execution, validation,
the basic system constraints and preference rules
evaluation and maintenance, as shown in Figure 2.
regarding the use of devices and functions.
When developing a system, verification and validation
Environment ontology describes the concept of facility

448

(V&V) need to be performed in each step of the
development. IDEE supports both traditional
verification (model checking and C&C) and dynamic
validation (policy enforcement). In the first step of the
development, the requirements are translated into
policy specification in PSML-P (Process Specification
and Modeling Language for Policies) and functional
specification in PSML-S (Process Specification and
Modeling Language for Service)[15, 16]. The
specifications are verified statically by model checking
[17] and C&C analysis [18], respectively. After several
iterations of V&V, the final specifications are obtained.
Test cases are automatically generated from the
specifications. Additionally, an automated code
generation tool generates the executable for simulation.
During the execution, a policy engine dynamically
loads the policies from the policy database, interprets
them, and enforces them at runtime. Since the policies
are stored as data and are separated from the
executables, it allows policies to be easily changed
(added, removed, updated, and enabled / disenabled)
on-the-fly at any time.

b.

c.

d.

3.5

changed, all the analyses can be repeated
automatically.
Code generation from the PSML model
including execution code and policy
enforcement code;
Policy specification languages to allow
various policies and system constraints
including security policies (e.g., BLP, RBAC,
and Chinese Wall) to be specified.
Allows policy to be analyzed - including
C&C, model checking, and simulation enforced, and modified at runtime.

Automated Code Generation with Device
Ontology

The IDEE supports automated ontology-based code
generation based on the following code generation
process as shown in Figure 3. The design is divided
into two parts: model translations preprocess and code
generation. The translation preprocess is to integrate
and convert PSML ACDATER model into XML files.
Code generation process is to parses pre-process files
one by one to generate the code. The output of the code
generation includes configuration files and the code:
• Configuration file is a project file for
compilation and it manages all the generated
code and template files.
• For each PSML ACDATER elements, the
code generation part will generate a
corresponding class.

The advantage of using simulation to support our
research on policy-enabled trustworthy computing is
that simulation can run extensive cases to ensure
coverage. Thus, one can perform both static and
dynamic analyses and measure their coverage, e.g.,
identification of most active policies and most
frequently violated policies.
The IDEE has the following features:
a. Single Model Multiple Analyses (SMMA)
Approach [15] -- All the analyses are based
on the same PSML model combined with
BIM information. When the model is

Figure 3 Example of Code Generated and its XML Representation

449

3) Harnessing Collective Intelligence: Instead of

Figure 3 shows the code generation based on device
ontology, function ontology, and device registration.
Thus, a new or foreign device can register its existence
and control software, and then be automatically
generated based on the ontology information
published. The device registration also contains device
status and device attributes such as location and
network interface definitions. Figure 3 shows the code
generated in two different formats, one in XML, the
other in Java, and it can generate 1) class definitions,
2) object initialization, 3) function flow, 4) execution
steps, and 5) task scheduling for a complete control.

depending on the project investigators and graduate
students to develop needed services and infrastructure,
this proposed framework uses the Web 2.0 (wiki, open,
users can be active contributor, long tail of
applications) principles to encourage users to
contribute.

4) Web-Based Education Outreach: By taking
the Web 2.0 approach, in addition to collective
intelligence, the framework will educate the
community of the total building supply chain including
tenants, owners, constructors, appliance and device
manufactures, caterers, security, event planners and
organizers, and architects, allowing them to study and
develop a better understanding of the possibility of
collaborative facility services that were not possible
before.

An important feature of the proposed approach is that
code generation is fully integrated with ontology. Each
device has its own ontology specified using
ACDATER, and its action is represented as a subclass
of executableElement, which is a part of generated
code. This feature allows complete traceability from
ontology to code for management, debugging, testing,
and monitoring purposes.
3.6

5) Incorporating Research into Formal
Education and Sustainability: The materials
developed and accumulated by this framework will be
a part of the new curriculum in Arizona State
University's Facility Management concentration of the
Master of Science in Construction program. The
materials will become a formal part of Advanced
Facility Management to be taught in the Del E. Webb
School of Construction.

Wiki-based Portal for Syndication

Numerous wiki software are available today, and an
example of a reasonable list of wiki software is listed
through Wikipedia at [20]. The proposed framework
chose MediaWiki [21] as its Wiki-based portal because
of its popularity. The ontology is ported to the web and
its correctness and consistency is ensured by running
C&C analysis.

6) CPS Theme: This framework mainly addresses
tools and techniques of the CPS theme, but also covers
other aspects:

1) Cross-Discipline Collaborative Project: This

Tools: This framework will develop tools to integrate
facility ontology for intelligent building management.
The tools developed project include design and
planning, service and code generation tools,
simulations tools, monitoring tools, policy enforcement
tools, and web sharing and collaborative tools. These
tools enable a new system of integration by
composition, not available in the current approach
using BIM and BACnet. This new approach enables
rapid model-driven system integration, development,
simulation, evaluation, testing, and runtime
enforcement using ontology information.

framework brings together experts in construction,
architecture, emergency planning, building material,
web technology, service publication, simulation, policy
specification and analysis, Web 2.0 to develop an
integrated project to reduce energy consumption,
enhance building facility comforts, security, operation,
and maintenance.

2) Advancement of Key Technology: While
currently used in numerous building facilities in the
world including federal buildings, hospitals, and
stadiums, currently BIM, specifically IFC, mainly
hosts class information. Thus, BIM does not enable
computation services needed for intelligent facility
management. Currently, these services must be
manually developed by individual companies or
organizations for their specific applications and
buildings only. This proposed framework advances
BIM to include service knowledge such as ontology,
specification, composition, execution, monitoring, and
simulation, allowing many of these steps to be
automated and shared.

Runtime infrastructure and platform: One of the
features in this project is to provide and experiment an
execution platform on which composed services are
carried out in real-time threads/processes and drives
embedded devices for control functions in a networked
environment. CPU and network bandwidth will be the
managed resources to meet scheduling and
performance constraints.
Foundation: This framework develops ontology
specification techniques including service models and

450

abstractions for facility management, algorithms to
facilitate service composition, execution, simulation,
and monitoring including automated code generation
algorithms, and algorithms to evaluate service
specification to generate the key monitoring
parameters such as the critical path and dominant path
automatically from an ontology system[2].

4.

the devices are connected via BACnet and can
communicate with each other.

Example

In this section, a case study is provided to demonstrate
the advantage of our innovative approach over the
traditional approach. A construction company has a
plan to develop a small residential community. The
floor plan has 1 living room, 4 bedrooms, and 2
bathrooms. Here we take the living room as an
example. Aside from allowing its future tenants to
have a say about the floor plan, the construction
company also provide them with a list of functions
extracted from the function ontology. If the tenants
choose to have some certain functions to be
implemented, they will be informed with what kinds of
appliances or devices they need to install accordingly.
Suppose two available functions for the living room
are Temperature Control and Media Control, and
the tenant chooses Temperature Control to be
implemented with the house. He will be informed that
there are two function plans available for this function:
One needs air-conditioning with cooling and heating
functionalities; the other needs a thermometer, a heater
and cooling fan. Suppose, the tenant is really sensitive
about temperature change, he chooses both plans to be
implemented in his living room in case of hardware
failure.

Figure 5 PSML model for "Temperature Control"
After the house was delivered and the required
electronics and appliances were installed, all the
functions required by the tenants will start running. In
this Temperature Control case, the plan implemented
by A/C is the first choice. The temperature is
maintained within a span required by the tenant.
However, in case of A/C failure, the proposed IDEE
framework will search for other function templates
available that also implement the Temperature
Control function. In this case, a template modeled by
PSML that uses a thermostat, a heater, and a cooling
fan is found, shown in Figure 5, the system will
dynamically generate the executable code for the
specific devices (shown in Figure 3), and deploy it to
the execution environment. If at a later time, the tenant
purchases a new heater from a different manufacturer,
it will be registered in device ontology. The heater can
be discovered the next time the framework performs
code generation. In the case that there are multiple
devices available for the same function, the framework
will use predefined optimization rules to choose one of
them. Thus, the tenants can always enjoy a living room
with pleasant temperature seamlessly.

5.
Figure 4 Floor Plan for Living Room

Conclusion and Future Work

In this paper, an ontology-based service composition
and execution framework is proposed to enrich current
BIM models with knowledge of functions that a
building might require. The framework supports V&V,
C&C, code generation, code deployment, policy
enforcement to make facility management more
efficient. The wiki-based portal brings the “power of
mob”, enabling the proposed framework to be a

The design of his living room is shown in Figure 4.
The figure was created using the popular modeling
software, Google Sketchup. Its data format can be
transferred from and to BIM. As you can see in the
figure, the living room is designed to be equipped with
an A/C, a thermometer, a cooling fan and a heater. All

451

[15]

platform where everyone can share and contribute. We
have completed most of work in IDEE, and our
immediate future work includes implementation of the
wiki-based portal, so individuals can start contributing
as soon as possible.

6.
[1]
[2]

Conference on Software Engineering and
Applications (SEA), Phoenix, 2005, pp.

References
[16]

Wikipedia. "Building Information Model,"
http://en.wikipedia.org/wiki/Building_Inform
ation_Modeling.
Bashford, Walsh, and e. al, “Drivers for

Energy Efficiency Decisions in a
Competitive Residential Construction
Market,” Cost Engineering, vol. 44, no. 4,
[3]
[4]
[5]

[17]

pp. 22, April 2002, 2002.
Iai-tech. http://www.iai-tech.org/.
D. Fensel, Ontologies: Silver Bullet for
Knowledge Management and Electronic
Commerce, Berlin: Springer-Verlag, 2001.
KSL. "What is an ontology," http://www-

[7]

[8]

[9]

[10]
[11]
[12]
[13]
[14]

160-167.
W.-T. Tsai, X. Liu, Y. Chen et al., “Dynamic
Simulation Verification and Validation by
Policy Enforcement,” in 38th Annual
Simulation Symposium (ANSS), San
Diego, CA, 2005, pp. 91-98.
H. Huang, W.-T. Tsai, R. Paul et al.,
“Automated Model Checking and Testing
for Composite Web Services,” in 8th

IEEE International Symposium on
Object-oriented Real-time distributed
Computing (ISORC), Seatte, 2005, pp. 300[18]

307.
W.-T. Tsai, X. Wei, and Y. Chen, “A Robust

Testing Framework for Verifying Web
Services
by
Completeness
and
in
IEEE
Consistency
Analysis,”
International Workshop on ServiceOriented System Engineering (SOSE),

ksl.stanford.edu/kst/what-is-anontology.html.
[6]

W.-T. Tsai, R. Paul, B. Xiao et al., “PSML-S:
A Process Specification and Modeling
Language for Service Oriented Computing,”
in the 9th IASTED International

T. Xiao, X. Wang, H.-K. Pung et al., "An
Ontology-based Context Model in Intelligent
Environments."
E. Kim, and J. Choi, “An Ontology-Based
Context Model in a Smart Home",” in
Ubiquitous Web Systems and Intelligence,
UWSI, 2006, pp. 11-20.
F. Latfi, B. Lefebvre, and C. Descheneaux,
“Ontology-Based Management of the
Telehealth Smart Home, Dedicated to Elderly
in Lost of Cognitive Autonomy,” in Third
International Workshop, OWL Experience
and Directions, 2007.
E. Meshkova, J. Riihijarvi, P. Mahonen et al.,
“Modeling the Home Environment Using
Ontology with Applications in Software
Configuration Management,” in International
Conference on Telecommunications, ICT,
2008, pp. 1-6.
Bacnet. http://www.bacnet.org/.
S. T. Bushby, and H. M. Newman, “BACnet:
A Technical Update,” ASHRAE, vol. 36, no.
1, January 1994, 1994.
W3C. "WSDL,"
April
30th, 2009;
http://www.w3.org/TR/wsdl.
W3C. "OWL-S," April 30th, 2009;
http://www.w3.org/Submission/OWL-S/.
Y. Chen, and W.-T. Tsai, Distributed
Service-Oriented Software Development:
Kendall/Hunt Publishing, 2008.

[19]

[20]

[21]

452

Beijing, 2005, pp. 151-158.
W.-T. Tsai, Y.-H. Lee, Z. Cao et al.,
“RTSOA:
Real-Time
Service-Oriented
Architecture,” in Service-Oriented System
Engineering, 2006, pp. 49-56.
Wikipedia. "wiki software," March 28th,
2009;
http://en.wikipedia.org/wiki/List_of_wiki_soft
ware.
MediaWiki.
March
25th,
2009;
http://www.mediawiki.org/wiki/MediaWiki.

VLSI Circuit Testing Using An Adaptive Optimization

Philip S. Yu*

C. M. Krishna**

*IBM Thomas J. Watson Research Center
P. 0. Box 218
Yorktown Heights, NY 10598

Abstract
The purpose of testing is to determine the correctness of
the unit under test in come optimal way. One difficulty in
meeting the optimality
requirement is that the stochastic
properties of the unit are usually unknown a priori. For instance, one might not hnow exactly the yield of a VLSI production line before one tests the chips made as a result.
Given the probability of unit failure and the coverage of
a test, the optimal test period is easy to obtain. However, the
probability of failure is not usually known a priori. We therefore develop an optimal sequential testing strategy which estimates the production yield based on ongoing test results, and
then use it to determine the optimal test period.

1.0

Introduction

In this paper, we attack the problem of determining optimal test lengths when the production yield (e.g. failure probability) of VLSI circuits under test is not exactly known in
advance. We present algorithms which adjust the test period
adaptivelly for improving test performance as experiences are
accumulated from testing.
As the density of VLSI products increases, their testing
becomes costlier and more difficult.
The generation of test
patterns has shifted from a deterministic approach, in which a
testing pattern is generated automatically using a fault model
and an algorithm, to a random selection of test signals [1,2,3,4].

Model

Yann-Hang Lee*

**Dept. of Electrical and Computer Engineering
University of Massachusetts
Amherst, MA 01003

This is motivated by the fact that applying all possible inputs
to the chip under test is impractical for any but the simplest
circuits. However, no matter whether deterministic or random
generation of testing patterns are used, the testing pattern applied to the VLSI chips can no longer cover all possible defects.
Therefore, the question regarding to sufficient testing needs to
be explored in detail.
Consider the test process for VLSI chips as shown in
Figure 1. There are two costs associated with the test process:
the cost of testing and the cost of passing as good an imperfect
chip. The first cost is a function of the time spent on testing
or, equivalently, the number of test patterns applied to the chip.
This cost will add to the cost of the chips thcmselvcs. The
second cost represents the fact that, when a defective chip has
been passed as good, it may fail very expensively after being
embedded in its application. An optimal testing strategy should
trade off both costs and determine an adequate test length (in
terms of testing period or number of test patterns).
Apart
considcrcd
production
functionally
If the yield
most chips

from the cost of testing, two factors need to be
when determining the test lengths. The first is the
yield, which is the probability that a product is
correct at the end of the manufacturing process.
is high, we may not need to test extensively since
to be tested will be “good”, and vice versa. The

(cost of pass;ng

The work of the second author was supported in part by
the
National
Science Foundation
under
grant
no.
DMC-850497 1.
Permissionto copy without fee all or part of this material is granted
provided that the copies are not madeor distributed for direct commercial
advantage,the ACM copyright notice and the title of the publication and
its date appear, and notice is given that copying is by permission of the
Association for Computing Machinery. To copy otherwise, or to
republish, requires a fee and/or specific permission.
24th ACM/IEEE

0 1987 ACM 0738-100X/87~0600-0399$00.75

Design Automation

testing patterns
(Number

of potterns)

uesting cod)

Figure 1. Testing Process for VLSI chips
Conference
Paper 24.3
399

other factor that must be considered is the coverage function
of the test process. The coverage function is defined as the
probability of dctccting a defective chip given that it has been
tested for a particular duration or a given number of test pat‘terns. If WCassume that all possible defects can be detected by
the test process, the coverage function of the test process can
be regarded as a probability distribution function. Thus, by
investigating the density function or probability mass function,
we should be abIc to calculate the marginal gain in fault detection if the test continues.
Given the production yield and the coverage function, the
relationship between the number of test patterns applied and
the defect level, which is defined as the probability that a “bad”
chip is passed by the test process, has been studied by Williams
and l3rown [4,5]. The number of test patterns required to attain
a specified defect 1cvcI can then be determined. The coverage
function of a test process can be obtained through theoretical
analysis as suggested in [5.6,7,8], or experiments on simulated
fault models [9,10,11,12]. However, the production yield of
every run must be estimated using test results and an acceptable
defect level must be defined based on the total testing cost.
The determination of the optimal testing period or test
length which minimizes the total cost of testing the current
chip is the central issue of this paper. We first obtain the optimal test period under the assumption that the production yield
is known. Next, we examine the case in which the production
yield is unknown. After a chip is tested, the test results will be
fed back to estimate the production yield. The estimated yield,
then, is used to determine the optimal testing period for testing
the subsequent chips. The problems we have to solve, namely
the estimation of the yield and the determination of the test
period, are similar to the decision problems that occur in sequential experiments [ 131. However, the decisions on the optimal test period are correlated and the information obtained
from testing may be “censored”, i.e., a faulty chip may not be
captured by the test process.
In the rest of this paper, we will assume that chips are
under test to determine whether they are failed or good. This
terminology is meant for concrctencss only and must not be
construed as limiting the applicability of the work. With very
minor modifications, the results of this paper can be used for
optimal testing with respect to any paramctcr, not just failed
or good.
In the following discussion, we use a continuous test pcriod, instead of the number of test patterns applied, to describe
the test process. A test with period I means that a sequence of
test patterns is applied for I units of time to the chip under test.
This is analogous to the number of test patterns applied to the
chip. Meanwhile, we assume that the coverage function of the

Paper 24.3

400

test process has a non-increasing hazard rate. This implies that
the marginal utility of testing decreases as the test continues.
The order of test patterns applied should be arranged so that
the coverage function has a non-increasing hazard rate. For a
random test, the exponentially distributed function, which has
a constant hazard rate, is commonly used as the coverage
function [5,6,7,8].
In the next section, the testing strategy for a given yield
is studied. The optimal test period and the properties of the
optimal test strategy are identified. Section 3 introduces a sequential test procedure in which both the optimal test period
and the yield are determined simultaneously.
In section 4, we
extend this to test chips in batches. We conclude with section
5.

2.0

Optimal Testing Policy Assmning Yield i?

Known
Let the coverage function of a testing process with rcspcct
to a testing period I, be F,(f,), which is the probability of detecting a fault given khe existence of the fault after testing for
a period I,. We assume that F,(f) is continuous with derivative
/;(!) . We also assume that defects are mutually independent
from chip to chip, and that the probability of a chip being defective is w. Then, the production yield, which is the probability that a product is fault-free, becomes 1 - o. If testing is
applied for a period I, the probability of detecting a fault is
wF,(r) and the probability that the test fails to detect an existing
fault is WC([) , where E(i) = 1 - F,(I).
Let the cost of testing be c, per unit time and the cost of
failing to detect a faulty chip be c,. When the test is pcrformcd
until either a fault is detected (the chip is rejected) or the period I, has elapsed (the chip is passed the test), the mean total
cost becomes

To obtain the optimal testing period. which minimizes the total
testing cost C(r,, w), we use Theorem 1 below.
Theorem 1. If r,(l) has non-increasing hazard rate /z,(f) , then
the optimal testing peri:!, I*(W), which minimizes C(f,, O) is
equal to 0 when f.(O) < WC/ or satisfies the following equation,
otherwise:

Proof: From Eq. (l), we obtain the dcrivativcs

of C(& w)

c.-1 .o
ratio-q/c,
coverage

function:

exponential
with meon

dis.
1

Since r,(f) has a non-increasing hazard rate, f;(r) is $ecreasing
and /l,(f) 5 h,(O) = f,(O).
When /,(O) <oc/,
C’(O, w) > 0
and
I&) <G
which
implies
that

44

< $ and c)(f, w) > 0 for all f. Thus, C(f, w) is in1 - W”‘f’
creasing unction of f and the optimal testing period is 0.
It can be observed that the right hand side of Eq. (2) is a
non-negative and non-increasing
function of f, and that
f,(m) = 0. When J,(O) > &,
thcrc must bc at lcast one solution which satisfies Eq. (21. Let f*(w) satisfy Eq. (2). It can

PROB. OF HAVING FAULT (w)

Figure 2. Optimal testing periods for exponential

40

function

is nonbe easily shown that C”(f*(w), w) > 0 and
1 -.oF(r)
increasing with respect to t. Thus, f*(o) is umquC and minimizes C(f, w).
c,-1 .o

0

ratio-C./c,

coverage

It follows from Theorem 1 that when the test has been
performed for a period f*(o). the cost saved from detection by
continuing testing is equal to or less than the cost of continuing
to test. Thus. testing should then stop immediately. The minimized mean total cost becomes
c*(w) E C(r,*(w), 0) = cpf,*~(f$“)

+ cpF$(f$*) + cp

J

converge

function:

exponential
with mean

dis.
1

c*
fdF,(d
0

(3)

The optimal testing period, r,*(o), is nonCorollary
1.
dccrcasing with respect to w, and the minimized mean total
cost, C*(w), is a concave function of 0.
Proof: The proof is ommitted here and can be found in [14].

Two examples arc presented in Figures 2 to 5, where the
coverage functions are assumed to be exponential and Wcibull
with a unit mean, respectively. Pigurcs 2 and 4 show the optimal testing periods with respect to w , and the minimized expected costs are illustrated in Figures 3 and 5 respectively, for
these two coverage functions. It will be observed that the optimal testing period and the minimized expected cost are greater
in the case of the Weibull distribution: this is because of the
long tail of the Weibull distribution. Also, when o approaches
one, the testing will be ended mostly with a fault being detected. Thus, the minimized expected cost will converge to I.
It is interesting to notice that, when o approaches tither 0 or
1, we have a small testing cost. This is due to the fact that the
uncertainty regarding the existence of a defect at the tested chip

I

0

0

I

#

0.2

I

0.4

I

I

,

0.6

I

0.8

I

1.0

PROB. OF HAVING FAULT (2)

Figure 3. Minimized

mean costs for exponential
function

coverage

is low. On the other hand, when w is close to the middle of
[O,l], the cost of testing becomes high. Furthermore, the value
of wlowis quite small in the above cases, especially, Q. = 0 when
the coverage function is the Wcibull distribution.

Paper 24.3
401

G-1 .o
ratio-C./c
coverage

0

I unction:

Wcibull dis.
with meon 1
sharp parameter

0.2

0.4

number of chips will be sequentially tested. Before any chip is
tested, prior information about the yield will bc used to determine the optimal testing policy. Then, the results of the tests
will be used to refine the estimate of the yield which will be
applied to design the testing policy of the chip under test and
all subsequent chips. Figure 6 illustrates the whole process
where the measurement of yield, the control of the testing period and the testing process are integrated.

A

0.75

0.6

0.6

1 .o

PROB. OF HAVlNG FAULT (w)

Figure 4. Optimal testing periods for Wcibull coverage function

2

Let the prior probabiIity of having a fault before testing
the i -th chip be described by a random variable w,. w, is dependent on the prior information on yield and the results of
previous tests. WC express the uncertainty of o, in terms of a
probability distribution defined on [O,l]. Let E,(w) be the generalized probabiiity density function [13] of w,, i.e. 6, is a probability density function if o, is continuously distributed, or a
probability function if the distribution of W, is discrete. The
mean and the variance of w, arc denoted by W,and af, respectively.
With an initial density function E,(U) of wl, where
uf # 0, the sequential testing procedure can be given as follows.
For testing i-th chip,

C.-l .o
rotio-q/c,

coverage

I-

W&bull dis.
with meon 1
shorp parameter
0.75

I

I
0.2

I

I
0.4

1,2 .. .. .

1.

Calculate the optimal testing period, I,*(&), based on the
current information of wi which is given by t,(o).

2.

Test the i-th chip until either

3.

01
0

i =

function:

I
0.6

I
0.6

I
1.0

a.

No fault is detected at the end of testing period t,*(<,)
: The chip is then passed as good.

b.

A fault is detected before the end of testing period
I,*(&) : testing then stops and the chip is rejected.

Based on the test result and .$,(w), calculate the posterior
density function of w, which will be used as the prior density function of w,+~in the subsequent test.

In what follows, we show how to integrate the process of
estimating the failure probability and simultaneously adapting
the test process to achieve optimal results.

PROB. OF HAVING FAULT (0)

Figure 5. Minimized

mean costs for Wcibull coverage function
units

under
test

testing

D

Testing

result

Process

0

testingperiod

3.0

Optimal Testing Policy and Mcasurcmcnt of
Yield
v

Observation:

Decision:

The previous section presents the optimal testing policy
when the production yield is known. Now, the measurement
of yield should be based on the results of testing: in many cases,
it will not be realistic to assume that the production yield is
known before testing. In this section, we shall discuss the optimal testing policy for uncertain yield. We assume that a large

Paper 24.3
402

Optimal

I

testing

’

I

Estimate

I

yield

I

Figure 6. Flow diagram of sequential testing and estimation

Let the events observed from a test be &(I) and ,?,(I),
when a fault is detected and no fault is detected after testing
has been applied for f 5 I,*((), respectively.
Based on which
event is observed, information about w, can be updated and used
to estimate w,+,. Bayesian statistical methods [13] are adopted
here to update prior information.
When the event E,,(f) or
E,(I) occurs, the posterior density function of w,, becomes

(4)

I,(w

1 E.fj,

=

EAw)(l

1 -

wl;,(f))

(5)

OiFs(f)

It follows that the mean values of the posterior w, conditioned
on the occurrence of Ed(f) or E,(r) are

=

‘d&(w 1Ed(f))&

= G{ + $

Next, we prove that the f,*(E) which satisfies Eq. (8) is
equal to zero if and only if I,*(W,) = 0 _ When t,*(G,) = 0 ,
I,*G,(S,(f))) = 0 for all f . Thus, f,*(E) = 0 is the only solution
of Eq. (8).
On the other
hand, if I,*(.&) = 0,
f,*(W,) = f,*(iS,(E,;(O))) = 0 . This also implies that I,*([,) > 0 if
and only if r,*(O) > 0 .
Suppose that we have been applying testing for a period
0 5 f < r,*(&) and have not detected a fault yet. The updated
mean of o, becomes 0,(,?,(f)) . Testing should be continued
since
qqo))fsw

2
i&!&(f))

At first, we show that I,*([,) which satisfies Eq. (8) exists. Since G,(S,(t)) is non-increasing
and f,*(w) is nondecreasing I *(S;,(.!?,(I))) is non-increasing.
Thus, there must
exist one and only one non-negative t which satisfies Eq. (8).
Proof:

(6)

1 - Gj(Ej(f))Ff(f)

>
-

qE,o,*(tj)))f,w
l - W,(EjJf$*(ti)))F$(f)

>
-

1

i&y’))

1
6+(w

E

I E&t))dd

= zi -

s 0

4&(f)

1 - GiF$(f)

(7)

Let C(f,, t,) be defined as the expected cost of testing when
the testing period is f, given the density function E,(w) of w. If
E,(o) is fixed during the testing of the i-th chip, we have
C(r,, 5,) = C(r,, 5,) for any given testing period t,, since C(r,, w)
is a linear function of w from Eq. (1). From Theorem 1, this
implies that r,*(G) is the optimal testing period. However,
when the event E,(f) is observed during the current testing period, the prior information should be updated. The updated
distribution should be used to modify the current testing period. The optimal testing period is dependent on not only the
prior distribution of w but also on what happens during testing.
Thus, to determine the optimal testing period in this case, the
current testing result has to taken into account. To obtain the
optimal testing period, we need the following lemma.
1: For all I 2 0. W,(E,(f)) is decreasing with respect to I
and 0 5 O,(E,(f)) 5 G, 5 G,(&(f)) 5 1.
Proof: The lemma follows immediately from Eq. (6) and (7).
0

Lemma

2: With a non-increasing hazard rate coverage function r;,(t) and a prior density function E,(w), the optimal testing
period which minimizes C(f,, t,), f,*([,) > 0 , satisfies

Theorem

f,*(q(EJf)))

= I

(8)

Also, if f > f,*(E) 2 0, we have

wi(qo)f,(4

<

1 - S,(E&f))l’,(f)

-

i;j;(E&f))f~(f$*(5i)) <
1 - qEf(f))Fs(fs*(~J)

-

The above inequality implies that the saving from detecting a
fault is less that the cost of testing at f > z,*(l). Testing should
not continue if we have not detected a fault after testing for
period I,*([).
(Note that the last inequality in the above
equation occurs when f,*([,) = 0).
0
Eq. (8) can be interpreted as follows. When the testing
begins, WChave the testing period r,*(G,) which is optimal for a
fixed t,(o). If no fault is detected after a testing period f, t,(o)
will be updated and then we have a new optimal testing period,
i.e. f,*(G,(,(E,(f))). When no fault is detected after the testing
period f,*([,) , the optimal testing period is equal to the elapsed
testing time. Testing should not then continue. In Figure 7,
we present solutions of Bq. (8) for a Weibull-distributed
coverage function. The intersections of ~,*&(E,(I))) and the diagonal line are the respective optimal testing periods. Notice
that, when UT increases, G,(E,(z)) decreases quickly, i.e., the
optimal testing period becomes short.
There is an important question that arises at this point.
Suppose one has a long run of good units. Prom what we have
said above, the testing period will be shortened greatly. This
raises the question of whether this period will ever go to zero

Paper 24.3

403

following a sufficiently long run of good units. For if it does,
no further updates of the failure probability are possible, and
the system stops learning. We have shown in [14] that this is
impossible. That is, no matter how long the sequence of good
units may be, the estimated probability of failure will never
Sink so low as to make the optimal testing time go to zero. This
is a result of both theoretical and practical significance.
We have also been able to show [14] that as the number
ot tests approaches infinity, the estimated probability of failure
approaches the true probability of failure so long as [,(w,) # 0
to begin with. The proof involves martingale convergence thcory and is omitted here.
In Figure 8, we show the expected density function of w
after testing n chips (i.e. E[&(w)]) under the optimal testing
strategy. The coverage function is assumed to be with exponential distribution.
For all three cases, wT = 0.05,0.33, and
0.80, the estimated w converges to the true defect probability
wr . Notice that, when either w is close to 0 or 1, the uncertainty
of having defects is small. Thus, the estimates in these cases
converge rapidly.

4.0

ratio-0.0001
coverage function:

Weibull dis.
with mea” 1

parameler

sharp

0.75

avarhb0
vor(w;bO
varh;bO.
varlwi k-0
varh; )-0.05

.Q5
10

//

/

Cd,-0.2

v0r(w; b-0. 10

Figure 7. Optimal test periods with estimated yield

Sequential Hatch Testing Strategy

We now extend the above results of sequential testing
procedure to cover sequential batch testing in this section.
Assume that instead of testing chips one at a time, we do so in
batches of size Neach. We further assume that the tester treats
all functioning chips in a batch identically: i.e. testing stops for
all the functioning chips in a batch at the same time. If the
production yield is known, every chip should be tested at most
with the testing period defined at the Theorem 1. This is independent of the batch testing or the batch size. In contrast, when
the production yield is unknown, all testing results of the chips
in one batch should interfere with the estimation of the yield
and, then, affect the optimal testing period of the currently
tested batch and subsequent batches. Thus, the policy for sequential batch testing with unknown production yield should
differ from the policy studied in the previous section where the
batch size can be regarded as 1. The other difference is that,
when a defective chip is detected and rejected, the testing of the
remaining batch should be continued whereas, if the batch size
is one, the testing will be performed on the subsequent chip.

Defect

Prababtitty

o

Figure 8. Estimated density function of w under the optimal
testing strategy

With a prior probability density function t,(w) of o,, before the
testing, the posterior density function of u,, becomes

Let the events observed from the testing of a batch be
&(r) and E;(I), if at least a fault and no fault is detected after
The
the testing has been applied for a period t, respectively.
probability of having the events E?(I) and @(I), given the production yield is w, are (1 - ~FJI))~ and 1 - (1 - wF,(t))” .

Paper 24.3
404

EiC”
IEy(r))
= (1-1(1-~~,(0)“)Ei(4
s
(12)

l-

(1 - wF,W&Mw

0

Theorem 4. With a non-increasing hazard rate coverage function F,(I) and a prior density function E,(W), the optimal testing
policy, for a batch of N chips. is to stop testing if no fault is
detected after testing for the period I,*(&, N) which satisfies

if the event E;(r) or @‘(I) occurs, respectively.
Let o,(E$t))
and w,(Er(f))
represent the random variables with density
functions .&(w ] E?(t)) and &(w ] ET(l)) , and means G,(Ey(t)) and
W,(&(t)) , respectively. The following lemma and theorem for
batch testing are analogous to Lemma 1 and Theorem 2 in the
previous section. We only present the proof of the lemma. The
proof of the theorem is omitted since it is similar to that of
Theorem 2.

fs”(Oi(E~(f)))

= f

if no fault is detected during the period.

From our assumption that all functional chips in a batch
are treated identically by the tester, it follows that the equation
governing stopping times for the batch case is identical to that
which yielded I,* for the sequential case, except that instead of
iS,(E,(r)), we have O,(ET(l)). Lemma 2 below then allows us to
carry our sequential results to the batch processing case in determining the optimal stopping time for the test, c*.
Lemma 2: For all f 2 0, G,(Ey(l)) is decreasing with respect to
f and 0 5 Z,(Ey(f))
5 G, 5 W,(Ef(f))
5 1
Proof: Since, given w, < w2 and o,( Ed),

Theorem 4 indicates that testing of a batch of N should
not be continued if no fault is detected after the testing period
r,*({,, N) . If there is a detection at t, where f < I,*(&, iV) , the
defective chip will be rejected and the testing of the remaining
should
N-l
chips
be continued
for
at
least
f,*(&(w
] Ed(l)), N - 1) - I as explained below. This is similar
to the situation that, with the initial prior density function
t,(w), the defective chip is first tested and a fault is detected
after the test period 1. Then, the testing of a batch of N - 1 is
started. The optimal testing period for the batch of N - 1. if
no fault is detected, is the optimal testing period with the prior
density function &(w ] Ed([)) , i.e., r,*(&(o ] Ed(f)), N - 1) .
With an initial density function E,(o) of w, where 0: # 0,
we shall summarize the sequential batch testing procedure,
which minimizes the cost of testing of every chip, as follows.

we can have that

For testing the i-th batch of Nchips,

4itw*I EyO1))= C1- OIF$(fl)lNA(f*)<
(1 - o,qgm4)
tif”l
I EfN(t2))
(1

-

W2r;‘,(4))%2)

(1

-

o24.,(~2e4(4)

=

lib2

I @%I))

tiCw2

I E/(f*))

where A(I) = J;(l - wl:,(f))N[,(w)dw.
Thus o,(E,N(Q) is greater
than w,(Ey(~,)) in the sense of likelihood ratio [15]. This implies
that w(ET(r,)) is stochastically
greater than w(Ey(Q). i.e.
Prob(w(E~(f,))
1 c) 2 Prob(o(E~(f,))
1 c) for any constant c.
Thus, G,(E’;(r)) is decreasing with respect to I.
Based on the fact that w, is stochastically larger than
a,(.!?#)) and (1 - wF,(l))N is a decreasing function of w , we
have the following inequalities which lead to the second part
of the Lemma.**
1
s 0

(1

-

4W)

$

(1

-

1,2....,

1.

(Initialize the working parameters) Let K = N which is the
size of batch under test and 0(o) = t,(w) to describe the
information on the production yield. Also, define t to be
the elapsed testing time with initial value 0.

2.

Calculate f,*(f), K) which is the optimal testing period of
the batch or the remaining batch.

3.

Perform testing on the batch or the remaining batch until
that
a.

No defective chip is detected before the elapsed testing
time reaches r,*(t), K) . Then, all chips in the batch or
remaining hatch passes the testing. The posterior density function of w,, which will bc used as the prior
density of o~+~,is calculated using Eq. (11) based on
O(w), K , and the elapsed testing time.

b.

A chip is found to be defective at the time 7, where
40)
where
I < z,*(O, K). Then, O(w) is updated to -,
T3
W is the mean based on density function O(o) , and set
K = K - 1. If K = 0, then set E,+,(o) to the updated

1

N d,(w)
-----Ad
q

i =

wF~(f))N5i(w)dw

s 0

1

S

(1

0

-

(1

-

wF,(r))N)*do

2
1
1

S

(1 -

(1 -

~F,(f))%*(w)dw

0

0

** The Theorem that, if a random variable X is stochastically
greater
than the another
random
variable
Y, then
Ewx)] 2 EM Y)] for all non-decreasing functions /[lS], is applied here.

Paper24.3
405

6(o) and begin to test the next batch. Otherwise, the
testing of the remaining batch of K - 1 is continued
from step 2 above. (Note that the elapsed testing time
7 wili not be reset until the testing of a new batch
starts.)

5.0

T. W. Williams, “Sufficient Testing in a Self-Testing EnProc. of the Id.
Test Conf., 1984, pp.
vironment,”
167-172.

[6]

I. J. Shedletsky and E. J. McCluskey, “The Error Latency
of a Fault in a Combinational Digital Circuit,” Proc. ofthe
Fault-Tolerant
Computing Symp., 1975, pp. 210-214.

[7]

I. I. Shedletsky, “Random Testing: Practicality vs. Verified Effectiveness,” Proc. of the Fault-Toleranf
Conyrrting
Synp., 1977, pp. 175179.

[8]

J. E. Smith, “Measures of the Effectiveness of Fault Signature Analysis,” IEEE Trans. on Computers, June 1980,
Vol. C-29, No. 6, pp. 510-514.

[9]

T. W. Williams and E. B. Eichelberger, “Random Patterns
within a Structured Sequential Logic Design,” Proc. of the
Int’i. Test Conf, 1977, pp. 19-26.

[lo]

C. Timoc, F. Stott, K. Wickman, and L. Hess, “Adaptive
Self-Test for Microprocessor,”
Proc. of he Int’l. Test
ConJ, 1983, pp. 701-703.

Discussion

In this paper, we have presented VLSI chip testing using
a adaptive optimization model. The design goal of VLSI testing
procedures is to minimize the total cost of testing which includes both the cost of testing process and the effects of incomplete testing. When the production yield is unknown, the
sequential testing procedures take the accumulated information
in the process of testing into account for modifying the testing
process. So the production yield can be estimated and the cost
of testing is minimized based on the best knowledge of the
manufacturing process. The results, in practice, will be of use
whether the yield is a constant or varies from one manufacturing run to another. Theoretically, the approach shows a way
to integrate the processes of information collection and adaptive optimization,
There are many interesting and useful extensions to this
work. One of these is is to obtain the optimal test length if the
total number of chips to bc tested is known. This problem differs from the present one in that not only do we have to take
into account the influence of past test results (as we do here)
but also the effect of refining our estimate by testing the (current) batch on the expected cost of testing the remaining chips.
One way to solve this problem would be dynamic programming.

References

[I]

D. P. Siewiorek and L. K-W. Lai, “Testing of Digital
Systems,” Proc. of the IEEE, Vol. 69, No. 10, Oct. 1981,
pp. 1321-1333.

[2]

R. A. Rasmussen, “Automated Testing of LSI,” Computer,
March 1982, Vol. 15, No. 3, pp. 69-78.

[3]

‘I?. W. Williams, “VLSI Testing,”
Vol. 17, No. 10, pp. 126-136.

[4]

T. W. Williams and N. C. Brown, “Defect Level as a
Functiori of Fault Coverage,” IEEE Trans. 011 Computers,
Dec. 1981, Vol. C-30, No. 12, pp.987-988.

Paper 24.3
406

[S]

Computer,

Oct. 1984,

[l 1] V. D. Agrawal and P. Agrawal, “An Automatic Test
Generation System for Illiac IV Logic Boards,” IEEE
Trans. on Conzpu/ers, Sep. 1972, Vol. C-21, No. 9, pp.
10151017.
[12] V. D. Agrawal, “An Information Theoretic Approach to
Digital Fault Testing,” IEEE Trans. on Computers, Vol.
C-30, No. 8, Aug. 1981, pp. 582-587.
[13] M.
H.
DeGroot,
Optimal
McGraw-Hill
Book Co., 1970.

Stafistical

Decisions,

[14] P. S. Yu, C. M. Krishna, and Y.-H. Lee, “Optimal Design
and Sequential Analysis of VLSI Testing Strategy,” IEEE
Trans. on Computers, to appear.
[15] S. M. Ross, Stochastic
1983.

Processes, John Wiley

and Sons,

A Schedulable Garbage Collection for Embedded Applications in CLI
Okehee Goh and Yann-Hang Lee
CSE, Arizona State University
Tempe, AZ 85287
{ogoh, yhlee}@asu.edu

Ziad Kaakani and Elliott Rachlin
Honeywell International Inc.
Phoenix, AZ
{Ziad.Kaakani,Elliott.Rachlin}@honeywell.com

Abstract
Common Language Infrastructure (CLI) has been introduced as a core technology of Microsoft .NET. It enables
”writing in multiple languages, running in multiple platforms” by providing Virtual Execution System (VES), Common Intermediate Language, and Common Type System etc.
The advantages of using CLI, including portability, compactness, and interoperability, could beneﬁt the productivity of application software development and deployment.
However, for embedded real-time systems, the applications’
time-constraints cannot be satisﬁed easily due to several
features of CLI runtime environment, such as thread priority, thread scheduling, garbage collection etc.
In this paper, we aim to have a garbage collection mechanism applicable on real-time applications in CLI and other
virtual machine environments. We achieve the goal by making the pause time of garbage collection operations predictable, and the invocation of garbage collection and applications schedulable. A cost model based on measured
WCET is established to predict the execution time and overhead of garbage collection operations.

1. Introduction
Given the advantages of portability, compactness, efﬁciency, and interoperability, the ideas of virtual execution
system, intermediate languages, and language independent
execution platforms have been attractive to system development for a long time. Besides JAVA and its virtual machine JVM, Common Language Infrastructure (CLI)[6] has
been introduced as a core technology of Microsoft .NET [8]
and standardized by the international standardization organization ECMA. CLI provides a virtual execution system
(VES) that can support multiple-languages as well as independence of platforms. The machine-independent bytecodes that ”write once, and run everywhere,” contributes to
expedite the evolution of this virtual machine (VM) technology by minimizing cost and time-to-market of application

development.
A real-time embedded CLI environment can enormously
expand the applicability of CLI, not only for consumer electronics devices but also the real-time embedded systems for
home appliances, telecommunication, industry automation
etc. However, CLI, like a JVM, was not designed to target real-time embedded applications whose timeliness is as
important as correctness of computation results and where
resource constraints must be considered. To make CLI applicable for the real-time embedded systems, several issues
of VES, such as thread priority, thread scheduling, exception handling, garbage collection etc., must be reconsidered.
Among them, we focus on garbage collection of VES in this
paper.
Garbage collection (GC, hereafter)[7] automatically reclaims not-anymore referenced memory to reﬁll available
free memory. It helps application developers relieve from
errors on mishandling memory references as well as helps
preventing memory leak. However, GC’s unpredictability
in terms of response time and invocation is an obstacle
of meeting timing constraints and memory availability for
real-time applications.
To meet time constraints in real-time systems, system
and application operations must be scheduled, extended
blocking delays must be eliminated, and resources must
be available for timely computation. Thus, GC operations must be controlled to ensure applications meeting
their deadlines and memory requests. To achieve this goal,
scheduling GC operations primarily requires two conditions. Firstly, the activity of GC must be controllable as
a schedulable unit and the pause time of GC operations
should be short enough not to delay the execution of realtime applications extensively. Secondly, the behavior of
GC, especially execution time and resource usage, must be
predictable.
Based on Boehm-Demers-Weiser (BDW) garbage collector in MONO CLI environment, we have designed a
schedulable garbage collection (S-GC, hereafter) such that
its activity is controllable and the pause time is bounded.
The S-GC is carried out by a concurrently running garbage

Proceedings of the 11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA’05)
1533-2306/05 $20.00 © 2005 IEEE

collector thread so that it can be co-scheduled with real-time
application tasks. The cost model to derive the execution
time and overhead of S-GC is established based on measured WCET (worst-case execution time) of the operations
of GC.
The paper is organized with Section 2 presenting an
overview of S-GC’s platform, Section 3 introducing the design of S-GC, Section 4 presenting the performance results
and cost model of the S-GC, and Section 5 drawing a conclusion.

2. CLI, MONO, and BDW GC
We have implemented S-GC by extending BDW garbage
collector v6.1 [3] employed in MONO v0.25[9], which is
an open-source implementation of CLI. Here, we give an
overview of CLI, MONO, and BDW garbage collector.
CLI[6] is aimed to make it easy to write components and
applications with multiple languages and for multiple platforms. It deﬁnes a rich set of types to allow CLI-aware
languages to interoperate and allows each component to
carry self-describing information. Applications of multiple
languages are translated into intermediate language codes
which are executed in a virtual execution system (VES).
The VES in CLI, similar to JVM, is an abstract stack-based
machine featuring loader, veriﬁer, JIT compiler, garbage
collector, security system, multiple threading, exception
handling mechanism etc. The ECMA standard for CLI does
not conﬁne a speciﬁc garbage collection algorithm for VES.
MONO[9] is an open source development platform
based on .NET framework and runs on Linux platform of
various architectures. Following ECMA standard, MONO
provides C# compiler, VES, and class libraries. In addition, MONO provides .NET compatible libraries such as
ADO.NET, ASP.NET etc. and also gtk+, GNOME libraries,
IDE, Debugger etc. The VES in MONO is composed of
well-optimized JIT and AOT, and a BDW garbage collector
etc.
BDW Garbage Collector[3] (BDW GC, hereafter) was
designed to support languages such as C, C++ etc. It is
a conservative mark-sweep GC that works without cooperation from a compiler or runtime library. To reduce a
pause time caused by GC operations, BDW GC has evolved
to support ’mostly parallel GC’ (partly incremental) for
uniprocessor systems, and parallel markers (not incremental) for multiprocessor systems.

3. Design of Schedulable Garbage Collection
The mostly parallel GC in BDW GC[2] imposes limitations not allowing the GC to be schedulable. Firstly,
GC is triggered on allocation requests encountering memory shortage and performed by the application thread that

makes the requests. Triggering garbage collection on a basis of allocation makes collection work dependent on allocation patterns of applications. Secondly, the virtual memory protection for write barrier is a system dependent feature so that it limits portability of CLI. Additionally, using
virtual memory protection leads to a long pause time during
an initial step to protect all heap pages, and a termination
step to re-scan both dirty pages and a root-set. Thirdly, the
GC works partially incremental so that certain phases of GC
result in a long pause time so that controlling the duration
of pause time is not ﬂexible.
We extended BDW GC and JIT of MONO to address the
problems mentioned above. Firstly, garbage collection is
performed by a concurrent task so that garbage collection
can be applicable for thread scheduling. Thus, the GC can
be scheduled at certain time intervals. Secondly, the GC
operations are implemented such that all phases constituting
garbage collection are carried out incrementally and their
pause time is controlled using a small work unit. Thirdly, a
write barrier is implemented to protect per an object, rather
than a block. We run single garbage collector task targeting
uniprocessor system.

3.1. Concurrent Garbage Collector
In BDW GC, once GC cycle is triggered, GC Increment
is performed at every memory allocation until the cycle
completes. The problem with this approach is that because
allocation requests are bursty in most applications, this easily causes either inconsistent progress of GC or too much
interruption on the execution of mutators. We designate a
concurrent task to carry out the GC operations. The task
(GC task, hereafter) is triggered at certain time interval,
and decides whether to start a GC cycle. Once the cycle
starts, the execution of GC Increment is limited to a targeted
pause time. Having the GC task enables real-time scheduling algorithms to address the applications’ deadlines and the
progress of GC operations.

3.2. Incremental Garbage Collection
GC cycle in mark-and-sweep garbage collection is composed of initialization phase, root-scan phase, mark phase,
sweep phase, and clear phase. We have designed all the
phases constituting the GC cycle incremental and made the
pause duration of an increment of each phase be controlled
with the extent of basic work unit of a corresponding phase.
The basic unit of work indicates a minimum work of GC
that must be done before GC task relinquishes CPU to give
mutators a chance to run.
In root-scan phase, the root-set, including static data,
registers, and thread stacks, is scanned. For static data, the
BDW GC in Linux scans the entire data segment because

Proceedings of the 11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA’05)
1533-2306/05 $20.00 © 2005 IEEE

it does not have information about which global variables
actually refer to heap objects. To reduce the size of the
root-set to be scanned, we register actual global variables
referring to heap objects (a registered root-set) when they
are created. In root-scan phase, the registered root-set is
scanned rather than entire data segment. Our experiment
shows that the size of static data scanned in root-scan phase
is decreased from about 340KB (the size of data segment of
MONO) to about 5KB, and the time taken for this is very
negligible. Subsequently, scanning thread stacks becomes a
major parameter that determines the cost of root-scan phase.
This can be further divided such that a basic work unit of
the incremental activity is a scan to each thread stack. The
changes in stacks of threads not yet scanned in incremental
root-scan phase are protected using an extended write barrier that will be explained in Section 3.3.
The basic unit of work in mark phase is to mark a heap
memory of approximately 4KB; GC task scans about 4KB
heap to ﬁnd reachable heap references, and marks the reachable objects. This is adjustable based on the target pause
time. In sweep phase, the basic unit of work is to sweep a
single memory block. Finally, in clear phase, the unit work
is to reset the mark bits of alive objects in a single block.

3.3. Write Barrier
We use Yuasa’s snapshot-at-the-beginning algorithm[10]
to trap mutators’ updates on heap pointers during mark
phase of a GC cycle. MONO includes a JIT compiler that
translates Common Intermediate Language codes(CIL) into
native codes. Among about 220 CIL instructions, the ones
that require a write barrier due to their update operations to
object references are Stind.ref, Stﬂd, Stsﬂd, Stelem.ref etc.
To apply a write barrier for these CIL instructions, we modiﬁed the JIT to generate internal calls including write barrier code as well as corresponding update operations. As
they are implemented as MONO internal calls in MONO
runtime system, the calls are altered using indirect function pointers such that the function pointer points to internal
calls with write barrier while GC cycle is in progress, and
otherwise points to internal calls without write barrier. This
helps avoiding overhead incurred for checking whether GC
is in progress or not to determine the necessity of write barrier.
In our incremental root-scan, scanning a stack of thread
is conducted thread by thread; mutators will not stop all
together until GC task completes scanning whole stacks of
all threads in root-scan phase, but mutators can interleave
after GC task ﬁnishes scanning one stack of a thread. In
this phase, Yuasa’s algorithm, making gray the old reference
of object being updated, is not complete because mutators
can interleave and make changes on stacks before GC task
ﬁnishes obtaining all root set.

In order to address this problem, we extended Yuasa’s algorithm to be applied in incremental root-scan phase where
threads are scanned one by one. The extended write barrier
prevents loss due to updates on stacks in root-scan phase.
The extended write barrier grays a new reference (a target
operand) as well as an old reference of object being updated
on update instructions. This algorithm is similar to write
barrier suggested in [5] where the write barrier is used to
synchronize the mutators graying their own stacks and GC
task starting marking at the same time.

4. Experiment
4.1. Experiment Environment
With our knowledge, no benchmark applications designed with time-constraints are available in CLI. Our
benchmark applications–DirectedGraphs, Multithreads,
Periodics, ThreadGraphs, and AHC–are not designed with
that requirement either. However, they are still good enough
to measure the pause time, the performance, and the overhead of GC in S-GC. With S-GC, we also measure a stopthe-world GC of BDW GC (Stop-GC, hereafter) which runs
until it ﬁnishes whole GC cycle.
The experiment is conducted in a PC workstation with
a 1.5GHz Pentium IV processor and 256MB memory.
To have a high resolution timer and preemptive kernel,
TimeSys’ Linux/Real-Time(v4.1.147)[4] is used. To minimize inﬂuence from kernel’s activity, the experiment is carried out in single user mode of Linux. Also, a ﬁxed sized
heap memory allocated at an initialization of CLI is locked
in physical memory using mlock() system call to prevent
paging.

4.2. Controllable GC’s Pause Time

Table 1. Proﬁle of Stop-GC & S-GC
Stop-GC
S-GC
Max.
Total Max.
Total
Benchmark
GC
App.
GC
App.
Pause
Time Pause
Time
(µs)
(ms)
(µs)
(ms)
DirectedGraphs 10014
6215
964
6166
MultiThreads
7008
775
1028
814
Periodics
3754
2839
967
2843
ThreadGraphs
3705 11645
984 11821
AHC
8850
4694
961
4737
Table 1 presents the garbage collection proﬁles for StopGC and S-GC. We choose a worst case by running them 20
times. For S-GC, we setup 1.5 times larger heap than that

Proceedings of the 11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA’05)
1533-2306/05 $20.00 © 2005 IEEE

4.3. Overhead of Schedulable GC

4.4. Cost Model of Garbage Collection
Scheduling real-time applications requires a precise prediction of cost and overhead due to GC. As a GC cycle in SGC is composed of Initialization, Root-Scan, Mark, Sweep,
Clear phases, we deﬁne the execution time of a single GC
cycle with equation (1). We derive a cost equation for each
phase by identifying cost parameters, and measure cost coefﬁcients as the worst case execution time (WCET) of the
basic operations in the experiment. The detail for that is
skipped due to the limitation of pages.
= Initialization + RootScan
+M ark + Sweep + Clear

(1)

Figure 1 compares GC cycle’s measured cost (the plots
named with (M)), and estimated cost (the plots named with
(E)) using the cost model. Some benchmarks (i.e., MultiThreads) in Mark phase shows that the estimated cost is
larger than the measured cost. The difference comes from
the fact that the cost coefﬁcient used in the cost model is
derived from the worst case path among associated operations, but the measured cost indicates an average cost whose
operations may not always hit the worst case path. This indicates that deriving practical WCET of GC cost needs to be
reﬁned by considering the distribution of pointers as well as
the size or the number of live objects as taken in [1].

5

DirectedGraphs(M)
MultiThreads(M)
Peridics(M)
ThreadGraphs(M)
AHC(M)
DirectedGraphs(E)
MultiThreads(E)
Peridics(E)
ThreadGraphs(E)
AHC(E)

7000

6000

5000

4000

3000

2000

The total execution time of benchmark applications in SGC is in general greater than that in Stop-GC. The S-GC
incurs additional overhead attributed by the GC scheduling mechanism and the synchronization operations for interleaving garbage collation and mutators. In this experiment, total GC time is not comparable between Stop-GC
and S-GC due to the difference of heap size applied.

EGC

8000

Average Time (us)

of Stop-GC because incremental GC requires reserving free
memory before starting GC cycle. The target of maximum
pause time due to GC increment is set to 1ms.
The experiment shows that the maximum pause time for
all benchmarks but Multithreads does not exceed the speciﬁed pause time target in a given environment. The pause
time for Multithreads also can be met within the pause time
target by adjusting work units of GC phases.

Conclusion

We have designed and implemented a schedulable
garbage collection for embedded applications in CLI such
that all phases constituting a garbage collection cycle are

1000

0
Initialization

Root−Scan

Mark
GC Phases

Sweep

Clear

Figure 1. Comparison of GC Cost
carried out incrementally and their pause time is controlled
in a small work unit. The cost model established using measured cost coefﬁcients of each GC phase and characteristic
parameters of applications can be used to predict WCET of
GC.

References
[1] D. F. Bacon, P. Cheng, and V. T. Rajan. Controlling fragmentation and space consumption in the Metronome, a realtime garbage collector for Java. In ACM SIGPLAN 2003
Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES’2003), 2003.
[2] H.-J. Boehm, A. J. Demers, and S. Shenker. Mostly parallel
garbage collection. ACM SIGPLAN Notices, 26(6):157–164,
June 1991.
[3] H.-J. Boehm and M. Weiser. Garbage collection in an uncooperative environment. Software Practice and Experience,
18(9):807–820, 1988.
[4] D. D. and L. X. A concurrent, generational garbage collector for a multithreaded implementation of ml. In Twentieth
Annual ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, New York, NY, 1993. ACM
Press.
[5] ECMA. Ecma-335 common language infrastructure, 2002.
[6] R. E. Jones. Garbage Collection: Algorithms for Automatic Dynamic Memory Management. Wiley, Chichester,
July 1999.
[7] Microsoft.
Microsoft .NET.
Available:
http://www.microsoft.com/net/.
[8] TimeSys Corporation. Timesys Linux/Real-Time User’s
Guide, version 2.0, 2004.
[9] Ximian. MONO. Available: http://www.go-mono.com.
[10] T. Yuasa. Real-time garbage collection on general-purpose
machines. Journal of Software and Systems, 11(3):181–198,
1990.

Proceedings of the 11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA’05)
1533-2306/05 $20.00 © 2005 IEEE

Home Network Semantic Modeling and Reasoning A Case Study
Topi Pulkkinen, Mikko Sallinen

Jiyeon Son, Jun-Hee Park

Yann-Hang Lee

Networked Intelligence
VTT Technical Research Centre of
Finland
Oulu, Finland
topi.pulkkinen@vtt.fi

Green Computing Department
Electronics and Telecommunications
Research Institute (ETRI)
Daejeon, Republic of Korea
jyson@etri.re.kr

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University (ASU)
Tempe, Arizona, USA
yhlee@asu.edu

apartment complex. Additionally, a semantic model of user and
environment information enables us to create totally new type
of services by integrating several services of individual
appliances together. This aspect is critical when we wish to
optimize home services beyond the limited functions of
appliance. For instance, we may have two thermometers in
different rooms monitoring home heating and cooling process.
If the energy consumption from the heating and cooling device
doesn’t match with the pattern of temperature measurements,
there are some possible scenarios such as “a window is open in
one of the rooms” or “the heating and cooling device is
broken”. These abnormal cases can be detected by data
clustering and reasoning after which intelligent actions can be
taken.

Abstract— To use smart home services efficiently, semantic
modeling of different layers of home network resources as well as
environment data and user's personal data is required. This
suggests that a smart home system must be able to support data
fusion and derive the context where services are operating in
order to classify the new information provided by network agents
correctly. Following the recent home network standardization
efforts, the paper presents a feasible approach of smart home
services and acts as a proof-of-concept for a dynamic home
network management and diagnostics system.
Keywords-component; home network, service agent, semantic
reasoning, home network standards

I.

INTRODUCTION

The rest of the paper is organized as follows. Section 2
introduces the standardization goals and efforts done within
ISO/IEC for home network interoperability. Section 3 presents
the research work for context information modeling that is
relevant for our work and describes the new model based on
the standard proposal. Section 4 contains the proof-of-concept
case, where a person’s heater is controlled based on the context
information fused from many information sources. Section 4
also depicts the general software architecture that was
implemented for the test-case and the results of the functional
tests. Finally in Section 5 we present the conclusions of our
results.

Home networks, emerging smart appliances and ever
increasing computing power of mobile devices and sensors
have created a smart home ecosystem that has huge business
potential [1]. A smart home aggregates various information
sources intelligently to optimize and enhance available services
according to user’s preference. The core function in this type of
operation is to understand the context of different situations so
the environment can be fully comprehended [2].
One method to define the context is to utilize semantic
modeling and reasoning to map the relationships between the
information sources and to select the suitable actions for the
services. Because a great portion of the information relates to
the life patterns of the user, fusion of time series sensor data is
mandatory [3]. To understand how powerful context data can
be, we can envision a smart washing machine which may be
controlled to minimize the cost of electricity usage. Add-on
services can consider user’s preferences, e.g. when the clothes
should be dry, or safety issues, for example if the user doesn’t
want to turn on the machine when no one is at home. These
services can be realized once the machine can interact with the
environment via a semantic modeling of user’s priorities, day
and time, usage policies, and various dynamic factors including
neighbors’ living patterns and optimal electricity usage in

II.

STANDARDIZATION ACTIVITIES OF HOME NETWORK
RESOURCE MANAGEMENT

Authors are currently working on one of the home network
standardization issues, which is HNRM (Home Network
Resource Management). HNRM is being developed in
ISO/IEC JTC1/SC25 which is responsible for HES (Home
Electronic System) standards. The HES standard collection
focuses on the interoperability of home network
communication protocols and home network terminology, but
also on system and application integration e.g. safety, security,

338

management because resource objects may be related to other
objects of the same domain. This helps to detect problems
such as power outage, network fault or software crash, which
are normally difficult to analyze remotely.

system management and building automation. Given the
existing communication protocols, including IGRIS, Echonet,
LonWorks, KNX, UPnP, etc., the interoperability in logical and
semantic level can be seen as an enabler technology for
different players, e.g. building constructors, device
manufacturers, and service providers, to develop personalized
and cost-effective home service infrastructure for the end-users.

The data structure of a resource object is presented in Fig. 2. A
resource object is constructed of two parts: common
information and domain specific information. The common
information defines the object’s identifier, type and name. The
domain specific object information contains the resource’s
domain specific data.

Specifically HNRM tries to solve configuration problems
between network entities as well as remote diagnostic and
management problems [3]. It considers a conceptual resource
model of a home network as illustrated in Fig. 1, which can be
adapted into a semantic model discussed in introduction. Here
the service-, device-, network- and floor plan information
layers are coupled to provide simple management architecture
for home environment and resources [4]. The example in the
figure shows an Entertainment Management System (EMS)
connected with devices that have relation on the network layer
links. Two of the devices are located in the Living room.

Fig. 2 Resource object structure.

In TABLE I and TABLE II resource object information is listed
in detail. The information structure in TABLE I is identical for
all resource objects.
TABLE I. COMMON INFORMATION OF A RESOURCE OBJECT
resource_id
Constructs of a domain identifier and an object
identifier. Domain ID is the identifier of the domain
the resource object belongs to. Object ID is the
unique identifier for the object in the domain.

Fig. 1 Conceptual smart home resource model for defining relationships
between the home network entities.

Resource type is combination of the class and the
sub-class of a resource as shown in Fig. 3. It is
hexadecimal number.

resource_name

The resource object name. Character string.

Because the domain specific information varies, TABLE II
shows the structure of a device domain resource object. The
other domain specific resource objects that are specified in
home resource model are: physical space domain resource
object, network domain resource object and service domain
resource object.

A. Home Resource Model
The HNRM standard proposal specifies the Home
Resource Model as an abstract formal representation of objects
in home that include their properties, relationships and the
operations which can be performed on the objects [4]. The
home resource model is divided into three hierarchy levels that
includes resource object, resource class and resource domain.
The hierarchies are illustrated in Fig. 3.

TABLE II. DOMAIN SPECIFIC INFORMATION OF A DEVICE DOMAIN OBJECT
device_id
Device identifier. Alpha-numeric string

Resource object represents a managed object in home network
and it has a one-to-one correspondence with a real-world
component. Depending on the resource object domain, there is
a variable amount of descriptive data associated with each
resource object. For example an air-conditioner is a resource
object of device domain and appliance class.
Resource class categorizes resource objects based on their
functionality. This classification is useful for example when
setting policies concerning a group of devices or managing
services based on usage patterns.
Resource domain categorizes the resource objects by their
resource type. Domain is important for remote fault

This work is supported by the IT R&D program MKE/KIAT. [2010-TD300404-001, Home Information Remote Aggregation and Context Inference
Prediction Technology Development]

resource_type

339

device_name

The name of device. Alpha-numeric string

physical address

The physical address of the device

status

The current status of device

version

Device version

manufacturer

Device manufacturer name

device_desription

Device description with user-defined format

function_list

The list of functions device supports

object_info

Intra-device-domain relation, the list of intradevice-relation relation objects and related
parameters

Fig. 3 Hierarchy of the resource objects in home resource model.

III.

contains attributes (owl:DatatypeProperty) and relations
(owl:ObjectProperty). Adding more entities is possible by using
the built-in OWL property (owl:subClassOf), which enables
ontology extension if additional service specific concepts are
required. CONON ontology was later extended to contain subdomain ontologies like “office domain” and “vehicle domain”
and OWL-Time temporal ontology for more complex services.
CONON was utilized with OSGi (Open Services Gateway
initiative) based Service Oriented Context Aware Middleware
(SOCAM) architecture to produce rule-based reasoning results
from context information and invoke smart home services [7].

MODELING OF THE CONTEXT INFORMATION

The standardization work presented in Section 2 provides the
basis to interoperate various home network technologies for
integrated services. It successfully introduces domain specific
resource model which can be utilized for monitoring intradomain relationships, such as to detect a network problem with
remote fault-diagnosis [5]. However, it does not yet support
techniques that a service integrator could use while creating
intelligent home applications that handle multi-domain
relationships effectively. Also, the standard work does not
include models for context representation and awareness, e.g.
user life patterns and preferences. Thus, to have a broader
information model for smart home applications, the specified
home resource model has to be extended.

Another very successful context information modeling is the
one adapted by two software architectures namely Context
Broker Architecture (CoBrA) and peer-to-peer data management
architecture MoGATU [8]. CoBra ontology was originally
designed for modeling knowledge in smart meeting rooms and
MoGATU to model beliefs, desires and intentions (BDI) of
software agents and human users. Both of them utilize OWL
format for context modeling and reasoning purposes.
Additionally, upper ontologies from Standard Ontology for
Ubiquitous Pervasive Applications (SOUPA) are employed to
enhance the application development process of the service
integrator. This means that SOUPA contains various general
vocabularies such as “time”, “space”, “location”, “action”,
“events”, etc. which are imported to the to the domain ontology
(e.g. Cobra-Ont) that is utilized by the architecture [9].

A. Related work
Here we describe the related research on context information
modeling. The description will focus only on the existing
pervasive computing system information modeling, which are
closely related to smart home environments. We also leave out
application specific models because interoperability and
reusability are key issues in home network environments. In
addition, it is reasonable to suggest that there should be a
centralized context information management component (for
storing personal preferences, history data etc.) instead of a fully
distributed system. Fully distributed systems could be better
suited for public smart spaces where many services rely on the
processing capacity of the smartphone.

The most recent ontology related reasoning architecture for
intelligent control of domotic devices is IntelliDomo [10,11].
IntelliDomo’s approach is to maintain a database that has the
identical structure and data values with a device ontology stored
in OWL format. The OWL device model also contains rules,
written in semantic web rule language (SWRL), to derive
intelligent services.
When a device changes its state, the data is first stored into
the database and then the ontology instance is updated in the

CONON is one of the few existing context information
models that are originally designed for smart home domain [6].
Its idea is simply to represent home conceptual entities with
Ontology Web Language (OWL) format. The ontology
constructs of entities that are either physical or conceptual
objects (e.g. “person”, “location” or “activity” and
“computational entity”). Each entity in CONON ontology

340

OWL model. If a derived answer from the rule-engine updates
the database, a software daemon will send out an update
message using the home network bus. IntelliDomo also
maintains OWL model for personal preferences so user’s
requirements can be transferred into facts and considered in
inference operation. It can be understood that, other than the
home environment modeling, the most important aspect of
IntelliDomo is to use the semantic web rule language to produce
intelligent services in pre-meditated environment.

management if some totally new device type or concept emerges
to home network markets.

In addition to the context information models mentioned
before there are other models with their own supporting
frameworks such as Context Managing Framework [12] and
Context Aware Sub-Structure (CASS) [13]. The typical feature
for these context information models is simplicity so various
applications can utilize the information structure easily. This is
why it seems that their main focus was in their context
framework operations rather than in context information
modeling. For instance, Context Managing Framework utilizes
data-centric blackboard architecture with simple abstraction
model to share context data between the entities.

Fig. 4 Core ontology model (left) where each class represent a mapping to a subontology and object properties of Device class (right).

The relationship structure of the core ontology is further
described in in Fig. 5 where each element represents a subontology and a mapping class in core ontology respectively. The
Device class plays a central part of the model as it has many
relationships but also User class is important in perspective of
service integrator building context aware services. The subontologies, which are marked with stripes, are based on the
Home Resource Model domains: Service, Network, Device and
Physical.

The main difference with our approach and the existing
models is the layered smart home model where each domain
(service, network, device and physical) will have their own
individual sub-ontology instead of merging everything together.
This layered structure provides an abstract separation among the
components of smart home applications and allows the
interoperability of these components. SOUPA is probably the
closest related context information model as it can also be used
to map concepts from different domains together.
B. Ontology structure based on Home Resource Model
In general the different approaches of context information
modeling depend on the objectives of the modeling. The
example objectives may consist of (i) the ease of capturing the
real-world concepts to a digital form, (ii) the effective
representation of context information model, (iii) the context
reasoning support, and (iv) the easiness and scalability of
context information management [14].
Based on the standard HES work, our approach is to
construct a smart-home domain ontology utilizing several subontologies e.g. the home domain information, the user-specific
context data and the application specific data. This method
supports context reasoning and provides means to expand the
context information model easily.
The core ontology is shown in Fig. 4 where each class
represents a mapping class to sub-ontology. The idea is to
provide a common interface for different applications so they
can utilize the functions provided by devices and also to be able
to include additional ontologies, such as user, temporal, and
location so context aware reasoning is possible. Each of the subontologies can have their own versioning to support information

Fig. 5 The smart-home domain ontology structure including the ontologies based
on home resource model standard proposal.

The arrows in Fig. 5 illustrate object properties between the
mapping classes. For example an instance in the User class
hasService relationship with an instance in Service class. Also

341

user will arrive at home, 2) when to turn on the heating device, 3)
how to react to the context information changes.

instance in the Service class hasUser relationship to instance of
the User class. Similarly there is one way relationship between
User and Space (User isLocatedIn Space) and Physical and
Location (Physical isLocatedAt Location). Practically this
means Location ontology consists of different types of ways to
measure a physical entity’s place such as Address, GPS or some
other global coordinate system.

TABLE III. ENVIRONMENT DATA USED IN DEMONSTRATION
Information

Time is also an important factor for many context aware
applications so Temporal ontology that is related Space class by
hasTime relationship. This is actually understandable because
the space (location) has a relation with time in real world: e.g.
time in USA is different than time in Korea.
To be able to compare the standard data description we
present in Fig. 6 the same part (Device) that was presented in
previous chapter when introducing Home Resource Model
resource object. We imitated the resource model exactly and
added only an object property hasFunction from Device class to
Function class to be able to map Device with Function subontology.

Location of the user

Mobile phone GPS

Weather

Internet weather service

Traffic

Dummy data from mobile phone

User’s schedule

Internet calendar app

Home temperature measurement

Home device interoperability system

Heater operation (on/off)

Home device interoperability system

In addition to the dynamic data, the system already has some
static information available like user’s temperature preferences,
the average energy consumption of the heater, BIM (Building
Information Model) of the house and a simplified models how
different weather affects the traffic speed and heating need. The
demonstration utilizes OWL-DL semantic model to handle the
data relationships and SWRL rules to create axioms from the
data model.
A. Software architecture
The high-level software architecture is presented in Fig. 7
where the bottom shows the Inference Module (IM) and the top
the plug-in services. These are connected together with a service
framework which hosts services. The data is saved in an OWL
format and the application rules are created with SWRL. The
implementation utilizes a Jess rule-engine which invokes the
service agents based on the inference results. When an external
event is triggered, the service agents can place the event data
into the ontology and invoke the inference module if necessary.

Fig. 6 Device sub-ontology main classes (left) with Device class properties
(right).

Finally we wish to highlight the Workticket class in the core
ontology that is presented in Fig. 4. This class is designed to be
used by the rule-engine when we are invoking services based on
the reasoning result. Whenever a rule-engine fires a rule that
requires an agent to take an action a new workticket instance
will be generated and placed into Workticket class.
IV.

Service

TEST CASE: HEATING OPTIMIZATION

To make a proof-of-concept for the HNRM standard proposal
as well as to demonstrate the semantic reasoning in an actual
application area it was decided to run a test case with a heating
optimization application. A person, who has a simple heater with
no other control mechanism than on/off switch wants to control
the device based on the available context data from her
environment. The dynamic context data is presented in TABLE III.
Fig. 7 Software architecture of the demonstration test case.

The demonstration case expects the user’s home to reach the
preference temperature when she arrives at home. During other
times the energy consumption should be kept minimal. The basic
scenario is as follows: the user will leave from her workplace at
some time T and the reasoning engine will inference 1) when the

In the ontology, all services can register their underlying
functions, so the applications, which are using the individual
services, can easily invoke correct service. As an example, let’s
assume that a person has a thermometer at home whereas another

342

person has an A/C with a temperature sensor. Both devices
provide the same functionality “temperature measurement”
which can be used by other applications. This service enables the
application developer to design an application that is usable in
different environments. Of course this means the service agents
and their functions need to be registered to the model. This
process is shown in Fig. 8 sequence diagram. In the beginning the
information model (OWL-file) is loaded by the operator to the
inference module and then the related agents can register
themselves. After registration, each agent will activate the
reasoning procedures related to the services it can provide. When
a registered service is identified and activated as a part of
inference results, a “workticket” is generated that provides
information to access the service.

Fig. 9 Agents invoking OSGi services and processing required data and
returning it back to the ontology model.

The activity diagram in
Fig. 10 describes the process of different agents working with
gathering-, processing- and storing data. This process is stopped
only if the application is terminated. The associated rules can
then be deactivated in the ontology.

Fig. 8 Registering the service and application agents into OWL model.

The worktickets are then retrieved by the different agents as
presented in Fig. 9. When new data arrives to the model, other
than appending the data into the ontology, the agents that have a
permission to run reasoning (DoInference) can activate the rule
engine to derive any new inference results. Because many parts
of an application require data processing that is outside the
capability of SWRL, the software agents can perform pre- or
post-processing. An example would be calculating heating
requirement based on the BIM (Building Information Model),
measured temperature, weather and preference temperature,
which is difficult to express with rule language. In the case that
the agents notice no change on data values, they will not activate
the rule engine.

B. Testing
The demonstration uses some existing software modules from
a Java service framework 1 to manage plug-in services, and a
device interoperability system2, which can communicate with the
different home networks. Also we utilized an existing Java based
ontology modeling API 3 for ontology management and a rulebased reasoning engine4 for context based reasoning.
Firstly, the real world concepts are captured by combining a
cloud service and several individual home service agents that
transfer the cloud service data to the context information model.
As was mentioned before, we utilized three types of information
sources: Android mobile phone, generic internet services, and
home device interoperability system, which were presented in
TABLE III.

1

OSGi (Open Service Gateway initiative) developed by OSGi alliance.
Device Interoperability architecture HeRA (Home Resource Architecture)
developed by ETRI.
3
Ontology editor - Protégé v. 3.4.4. developed by Stanford Center for
Biomedical Informatics Research.
4
Jess rule engine developed by Sandia National Laboratories
2

343

Android platform is also running a “result receiver manager”
module which is used to show a simple timestamp when the
user’s heater will be turned on after the inference module has
received the necessary data and calculated the time. The GUI to
this module is presented in Fig. 12. It shows the time in the same
format that is used in Protégé dateTime property.
For the calendar and weather data we developed a simple
Java application for interfacing Google weather and Google
calendar internet services and abstracted the data for the
demonstration. Basically we were interested in when the user
may arrive at home and the need for heating/cooling operations
according to weather information and user’s calendar data.

Fig. 10 Data gathering, processing and storing process.

In the demonstration scenario the Android application first
sends its GPS location and dummy traffic data to a TCP server
via JSON interchange format and the TCP server then provides
an OSGi interface for the software agents to request data from the
server. The identification to the server is done with a mobile
phone number. The SW module setup is presented in Fig. 11.

Fig. 12 GUI for presenting the result of the inference and sending dummy traffic
data to cloud server.

The final information source was device interoperability system
called Home Resource Architecture (HeRA). Instead of a real
test environment, we utilized a simulated temperature sensor and
a virtual UPnP heating device whose state was controlled by
HeRA. The heating device was a simple on/off device and the
temperature sensor reading could be changed manually by web
interface provided by HeRA. HeRA was also included in OSGi
service framework as a plug-in service. The whole
demonstration system therefore is constructed of four PCs and a
mobile phone that is shown in
Fig. 13. Right-most computer was hosting virtual UPnP device.
The laptop second from the right was running OSGi service
framework and Jess rule-engine and was gathering data from
mobile phone and internet services via software agents. The two
computers on the left were hosting HeRA in VirtualBox and
provided a temperature sensor web-access.

Fig. 11 Demonstration architecture for collecting mobile phone data.

344

ACKNOWLEDGMENT
This work was conducted using the Protégé resource, which
is supported by grant LM007885 from the United States National
Library of Medicine.
REFERENCES
[1]

[2]

[3]

[4]
Fig. 13 Demonstration environment for testing.

The test results show that, by extending the proposed data
definition of the HES standard with user and environment
information and utilizing ontology web language format, it is
possible to create context-aware applications that fuse data
intelligently from existing home network and internet services.
The performance of our system is suitable for online control of
the home devices as the delays from starting inference cycle to
execution are in between 1-5 seconds, which is enough for most
services.
V.

[5]

[6]

[7]

CONCLUSIONS

[8]

In this paper we described possible application to utilize the
data model that was defined by the ISO/IEC standard proposal of
home network resource management (HNRM 30100-2). The data
model supports different type of services: internet based services,
mobile services, services provided by home appliances that are
connected to a home network and the service integrator’s
applications. In addition to this we extended the model to be able
to support user’s life pattern data, which utilizes fused
information such as home arrival time or the heating pattern of
the user’s home.

[9]

[10]
[11]

[12]

We also described the design of home network agent
architecture and verified the context information model with a
semantic reasoning engine, where OSGi service framework
provides data into ontology based reasoning engine. The
demonstration scenario illustrated that it is possible to integrate
various devices in different home networks and fuse information
sources to create new useful services and to optimize the service
efficiency by analyzing context data.

[13]
[14]

[15]

345

J. Lee, D. Jung, Y. Kim, Y. Lee, and Y. Kim, “Smart Grid solutions,
services, and business models focused on telco,” Network Operations and
Management Symposium Workshops (NOMS Wksps), April 2010
IEEE/IFIP, pp. 323-326
Y. Son, T. Pulkkinen, K. Moon, and C. Kim, “Home energy management
system based on Power Line Communication,” IEEE Transactions on
Consumer Electronincs, vol. 56, no. 3, pp. 1380-1386, Aug. 2010.
Koolwaaij et al, “Context watcher: sharing context information in everyday
life,” in Proceedings of the IASTED conference on Web Technologies,
Applications and Services (WTAS), 2006
Y.Son, T. Pulkkinen, and K. Lee “ISO/IEC CD 30100-1 INFORMATION
TECHNOLOGY - Home Network Resource Management – Part 1:
Requirements,”
http://www.iso.org/iso/search.htm?qt=30100&searchSubmit=Search&sort=
rel&type=simple&published=on
Y.Son, T. Pulkkinen, and K. Lee “ISO/IEC CD 30100-2 INFORMATION
TECHNOLOGY - Home Network Resource Management – Part 2:
Architecture,”
http://www.iso.org/iso/search.htm?qt=30100&searchSubmit=Search&sort=
rel&type=simple&published=on
Ji-Yeon Son, Jun-Hee Park, Kyeong-Deok Moon, Young-Hee Lee,
“Resource-Aware Smart Home Management System by Constructing
Resource Relation Graph,” IEEE Transactions on Consumer Electronics,
Vol. 57, No. 3, Aug. 2011
D. Zhang, T. Gu, X. Wang, “Enabling context-aware smart home with
semantic technology,” International Journal of Human-friendly Welfare
Robotic Systems Vol. 6 No. 4 pp. 12-20, 2005
T. Gu1, H. Pung1, D. Q. Zhang, “A middleware for building context-aware
mobile services,” IEEE 59th Vehicular Technology Conference. VTC
2004-Spring, 17-19 May 2004, Milan, Italy
H. Chen, T. Finin, A. Joshi, “The SOUPA Ontology for Pervasive
Computing,” Whitestein Series in Software Agent Technologies, Springer.
2005.
H. Chen, et al., “Intelligent agents meet the semantic Web in smart
spaces,” Internet Computing, IEEE Vol. 8 , No. 6, pp. 69-79, 2004
P. Valiente-Rocha and A. Lozano-Tello, “Control model of domotic
systems based on ontologies,” 2nd Int. Conference on Agents and
Artificial Intelligence, pp. 470–473 Valencia,Spain, Jan. 22-24, 2010
P. Valiente-Rocha and A. Lozano-Tello, “Ontology and SWRL-Based
Learning Model for Home Automation Controlling,” in J.C. Augusto et al.
(Eds.): Advances in Intelligent and Soft Computing Vol. 72 – Ambient
Intelligence and Future Trends-International Symposium on Ambient
Intelligence (ISAmI 2010, AISC 72), pp. 79–86, 2010
P. Fahy and S. Clarke “CASS – a middleware for mobile context-aware
applications,” Workshop on Context Awareness, MobiSys, 2004
P. Korpipää, J. Mäntyjärvi, J. Kela, H. Keränen and E-J. Malm, “Managing
context information in mobile devices,” IEEE Pervasive Computing, Vol.
2, No. 3, pp.42–51, July–September, 2003
C. Bettini et al. “A survey of context modeling and reasoning techniques,”
Pervasive Mobile Computing, Doi: 10.10 I6/j. pmcj.2009 .06.002, 2009

Improving Multistage Network Performance Under
Uniform and Hot-Spot Traffics
Jih-Kwon Peir

IBM T. J. Watson Research Center
Yorktown Heights, NY 10598

Yann-Hang Lee
Computer and Information Sciences Department
University of Florida
Gainesville, FL 3261 1

Abstract

mance by about 30% under uniform traffic. For nonuniform
traffic, this design provides more balanced link utilization.
which prolongs the time to reach tree saturation. Therefore,
the network can tolerate transient hot-spot traffic much better.

The major performance drawbacks of multistage interconnection networks are due to link contention and massage
blockage. To remedy this problem, we propose a switch design which uses two independent routes between each input
port and each output port within a switch. With proper
routing control mechanisms, the adverse effect on link conflicts can be greatly reduced. Various simulations are used
to illustrate the performance improvement of this new design
under both uniform and hot-spot traffic patterns.

1

In the following section, we describe a number of previous switch designs. The details of the new switch design
with static and dynamic routing controls are provided in
Section 3. In Section 4, we investigate the network performance through extensive simulation. Finally, we conclude
this study in Section 5 .

Introduction

2

A key issue in designing high performance multiprocessor
systems is t o provide an efficient communication mechanism
among processors and memories. Multistage Interconnection
Networks (MIN) such as Omega [l]and its equivalences [a]
have been proposed and used in several prototype parallel
systems [3, 4, 51. The main advantage of these networks is
their cost-effectiveness. However, performance of this type
MINs may be limited by link conflict and message blockage
due to the sharing of links among different paths. Another
noticeable drawback of the multistage network is the severe
performance degradation under nonuniform (hot spot) traffic [6]. Such traffic may favor a small number (hot) memory
modules and cause heavy contention. If the nonuniform traffic persists, messages may fill up all the buffers along the path
to the hot module a.nd block the messages to other memory
modules.

We consider a multistage interconnection network design which
uses 2 x 2 switches as the building block. A conventional 2 x 2
switching element with buffers is shown in Figure 1. Each
input/output port in the switch has an associated register
(or latch) for receiving or sending messages between stages.
Buffer is added along the path to each output register and
serves as a waiting queue when the output register is busy.
Messages are routed from the input registers to the output registers according to their corresponding destination
address bit. When two messages from both input registers
destine to a single output port a t the same time, a conflict
occurs. Then, one of the messages is blocked and will retry
a t the next cycle. This conflict delay may propagate back
t o the proceeding stages and prohibit a successive message
from transmission. One slight modification of the original
design was proposed in [7] which uses two queues for each
output port, each one of the queues is connected with an input register. This split queue design allows two messages t o
be routed into their corresponding output queues of the same
output port in one cycle. However, the conflict of transmitting messages from two queues to the next stage can not be
avoided.

In this paper, we describe a switch design which improves
network performance under both uniform and hot spot traffic. The central idea of this design is to provide two independent routes from each input port to each output port in a
switch. Messages routed through a switch are clustered into
two groups which utilize two independent route. Simulation
shows that the new switch design improves network perfor-

The split queue design eliminates the conflict on queue
insertion which in turn increases link utilization. However,
when an output queue is full, the input register connected to
this queue can be blocked if it receives a message t o be routed
to the queue. As a result, even if there are free spaces in the

The work performed by Y . H. Lee was supported in part
by a grant from the Florida High Technology and Industry
Council.

TH0328-5/90/0000/0548/$01.OO 0 1990 IEEE

Conventional Switch Designs

548

With two independent paths, message blockage can be reduces. Unfortunately, the problem of tree saturation due to
non-uniform tra.ffic patter is not completely alleviated. We
propose an enhancement scheme which dynamically identifies potential hot messages and selectively route the messages
destined to other memories with higher priority.

queue of the other output port to which messages waiting
in the previous stage will be routed, no transmission can lie
done until the blocked input register becomes free.
Message combining [4] was suggested to alleviate the hot
spot contention problem [6]. The basic idea of message combining is t o incorporate some hardware in each switching
element t o trap and combine messages which are destined
for the same memory location. Because of the hardware
complexity, messages traveling through the network become
much longer.

3.2

In order to maintain a normal flow for messages to the memories other than the hot spot, these messages should be routed
with higher priority. Thus, the switches need to identify hotspot messages dynamically, and then, to use these information for a proper message selection from the output queue
for transmission.

Another solution suggested in [SI is to limit the number of
messages with the same destination address to be resided in
each queue. This way, the queues along the path to the hot
module will not be occupied completely with hot messages.
However, because only one hot message is allowed in each
queue, the issuing of hot messages from processors will be
blocked very quickly. Unless each processor issues message
infinitely and follows request independence assumption, the
network bandwidth can be saturated quickly and limited t o
the bandwidth of the hot module.

3

To identify a potential hot-spot message, n counters are
implemented for each queue, where n is equal to the number of destination address bits (number of stages). Upon
enqueueing a message, each counter is incremented if the
corresponding destination address bit is a 'l', while each
counter is decremented if the bit is a '0'. When a message is
dequeued, the counters will be decremented or incremented
accordingly. When the counter for the destination bit of the
(z
2) stage is greater or less than certain threshold values, a potential hot-spot congestion is recognized, where i
represents the current stage. The destination address of the
hot spot is determined from all the counters. The destination address bit is equal to '1' if the corresponding counter
is greater than 0. The bit is equal to '0' otherwise. This address is then used for selecting a message out of the waiting
queue. The selected message is the first one with a different
address. Notice however, in the new switch design, the destination bits for the current and the next stages are used to
determine a particular output queue for a routing message.
Therefore, at most, ( n - 2) counters are required for each
output queue.

A Switch Design with Two Independent Routes

+

In this section, we describe a new switch design in which two
independent routes are provided to reduce message blockage.
The basic switch design with static routing control will be
given first, followed by an enhancement with dynamic routing control.

3.1

Dynamic Routing Control

Basic Switch Design

The detail of the new switch design is illustrated in Figure
2. Each input/output port is with two registers marked by
up(V) and low(L). Each output register has an associated
output queue. The routing control inside a switch is very
simple. The message resided in the up input registers is
routed to the upper output port, while the message resided
in the low input registers is routed to the lower output port.
The destination address bit for the next stage is used to
decide which of the two queues to be routed into. The received messages in each input register are clustered into two
groups and are transmitted to the next stage alternatively.

4

Performance Evaluation

In this section, we compare the performance of different
switch designs through simulations. The five switch designs
are:

( A ) Conventional switch with a single queue for
each output port,

In other words, the messages received subsequently in an input register will join different output queues and share no
links afterwards.

(B) Conventional switch with split queues,

(C) Switch with two independent routes,

From this basic design, we can further improve the performance by combining the two output queues at each output
port as a shared queue 191. This approach utilizes the queue
spaces much better. In a shared queue implementation, two
counters are used to maintain the number of u p and low
messages in each output port. When either counter exceeds
certain threshold, the corresponding incoming messages will
not be allowed to store into the queue.

(D) Switch with two independent routes and shared
queues, and

(E) Switch with two independent routes using
dynamic routing control.
We simulated a 64 x 64 Omega network with uniform
and nonuniform traffic patterns. It takes one cycle t o route

549

a message from an input register to an output register or
an output queue and to transmit a message across stages.
The total size of the queue(s) at each output port is 8 (4
for each split or up/low queue). The threshold used in the
shared queue implementation is 6, i.e., the messages in either
queue ( u p or low) of a output port can not exceed 6. In the
dynamic routing scheme, a potential hot spot is identified in
each queue when the absolute value of the counter for the
destination bit of the stage following the next stage exceeds
2.

The hot-spot message rate issuing from processors can be
determined as: r x ( h N (1 - m h ) ) ,where 1’ represents the
message issuing rate per processor per cycle, h is the fraction

+

of references directed to a hot memory module from each
processor, 112 is the number of hot spots, and N is the size of
the network. A number of combinations of these parameters
have been simulated. We present one of the interesting case
in the following, where r = 0.7, h = 0.0125, and N = 64.
The results of message delays are shown in Figure 4.
We omit scheme (C) because it has similar performance as
scheme (D).

To include a practical processing model in MIMD environments, we assume that each processor issues r requests
at every clock cycle, where 0 5 r 5 1. We also assume that
there is no queue in each processor to hold blocked messages.
Therefore, a processor stops issuing a request whenever the
destined input register of the initial stage is occupied.

4.1

When the hot-spot traffic begins, the delay of scheme

(E) rises much slower than the delay of any other scheme.
This is because scheme (E) degrades the transmission priority of the hot-spot messages waiting in the queues along
the paths to the hot memory module. All other messages
can flow through the network without being blocked by the
hot messages. The delay of scheme (D) rises slower than the
delays of schemes ( B ) and ( A ) for the same reason that there
are less messages contributed t o the tree saturation. On the
other hand, when the hot-spot traffic disappears, scheme (E)
takes longer to recover than other schemes. This is due to
the assumption that all processors stop issuing requests if
the corresponding input register of the first stage is occupied. Therefore, under the scheme (E), there are many more
hot messages issued and queued in the network which take
a longer time to be digested by the hot-spot module.

Uniform Traffic

In Figure 3, we show throughputs under different request
rates for the first four schemes under uniform traffic. We
omit scheme (E) because it performs very close to scheme
(C). Also, we inclde the performance of a cross bar design
which is indicated by the curve (F).
There are several interesting observations. First, Scheme

(B) performs much better than scheme ( A ) . This is because
in scheme (B), two messages from both input registers are allowed to route into the queues of an output port in one cycle.
The conflict of queue insertion disappears. Secondly, There
is a noticeable performance improvement from scheme (B)
to scheme (C) in both throughput and average delay. This
is due t o two facts: one is that messages destined t o different
output ports of the next stage will not block each other in
the current stage, the other one is the evenly distributed traffic resulted from messages selection and clustering. Finally,
we can observe that the addition of the shared queue implementation further improves network throughput. In the
best case, a throughput rate of 91.5% can be reached, which
is about 30% and 11% improvement over the conventional
schemes (A) and (B) respectively. The new design provides
a comparable performance t o that of a higher cost crossbar
design.

4.2

5

Conclusion

We propose in this paper a new switch design for multistage
interconnection networks. The designed switch is with two
independent paths from any input port to any output port.
A look-ahead routing control based on the message’s destination bit of the next stage is suggested t o utilize these
two paths and t o regulate the traffic across stages. Messages
successively entering an input port are likely destined t o different output ports. Therefore, a more evenly distributed
traffic can be achieved to reduce the probability of link conflicts upon message transmission. Specially, the dynamic
routing control can prolong the period of reaching tree saturation and attain a comparable performance as that of a
uniform traffic pattern.

Nonuniform Traffic

In case of nonuniform traffic, we adopted a periodic traffic
pattern which oscillates between uniform and nonuniform
traffic patterns as shown in Figure 4. The first, the third
and the fifth regions have uniform traffic while the second
and the fourth regions consist of nonuniform traffic. In order t o understand the network performance under various
nonuniform patterns, we assume the second region has a single hot spot of memory module 0, while the fourth region
has two hot modules 0 and 32. The length of each region
is 1000 cycles which is much longer than the estimated time
for tree saturation.

References
[l] D. H. Lawrie, “Access and Alignment of Data in An
Array Processor,” IEEE Trans. on Computers, Vol. C24, Dec. 1975, pp. 1145-1155.
[2] C. Wu, and T. Feng, “On A Class of Multistage Interconnection Networks,” Aug. 1980, pp. 694-702.
[3] D. D. Gajski, D. J. Kuck, D. H. Lawrie, and A. H.
Sameh, “Cedar - A Large Scale Multiprocessor,” Proc.

550

1983 I d 7 Conf. on Parallel Processing, Aug. 1983, pp.
524-529.
[4] A. Gottlieb et al., “The NYU Ultracomputer

- Designing an MIMD Shared Memory Parallel Computer,”
IEEE Trans. on Computers, Vol. (2-32, Feb. 1983, pp.
175-189.

[5] G. F. Pfister, et al., “The IBM Research Professor Prototype (RP3): Introduction and Architecture,’’ Proc.
1985 Int’l Coiif. on Parallel Processing, Aug. 1985, pp.
943-948.
[6] G. F. Pfister, and V. A . Norton, ‘‘Hot Spot Contention and Combining in Multistage Interconnection
Network,” IEEE Trans. on Computers, Vol. (2-34, Oct.
1985, pp. 943-948.

REOUESI RATE

Figure 3. Throughputs of multistage netwroks with
different switch designs

[7] M. Kumar and J . R. Jump, “Performance Enhancement
in Buffered Delta Networks Using Crossbar Switches
and Multiple Links,” Jour. of Parallel and Distributed
Computing, Vol. 1, No. 1, 1984, pp. 81-103.

[8] D. M. Dias and M. Kumar “Preventing Congestion in
Multistage Networks in the Presence of Hotspots,” Proc.
1989 Int ’1 Conf. on Parallel Processing, Aug. 1989.
[9) Y. Tamir, and G. Frazier, “High-Performance MultiQueue Buffers for VLSI Communication Switches,”
Proc. 15th Symp. Computer Architecture, May 1988, pp.
343-354.

I

”

1

OUT

Figure 1. A conventional 2 x 2 switch design
I

I

Figure 4. Performance of foure switch designs under
hot-spot traffic

Figure 2. A 2 x 2 switch design with two independent routes

551

Integrated Scheduling with Garbage Collection for Real-Time Embedded
Applications in CLI
Okehee Goh and Yann-Hang Lee
CSE, Arizona State University
Tempe, AZ 85287
{ogoh, yhlee}@asu.edu

Ziad Kaakani and Elliott Rachlin
Honeywell International Inc.
Phoenix, AZ
{Ziad.Kaakani,Elliott.Rachlin}@honeywell.com

Abstract
We present a schedulable garbage collection for realtime applications in virtual machine environments. The design objective is to make the pause time caused by garbage
collection operations controllable, and the invocation of
garbage collection predictable. Thus, real-time applications can be schedulable along with garbage collection.
We develop a prototype for a schedulable garbage collection in MONO CLI execution environment. A cost model of
garbage collection is established based on measured WCET
to predict the execution time and overhead of garbage collection operations. A scheduling algorithm of garbage collection and application tasks is presented to illustrate how
the time and memory constraints of real-time systems can
be met. The experiment result of the scheduling algorithm
for a periodic task set on the prototype is included in the
paper.

1. Introduction
Real-time embedded systems have been changing
rapidly these days as the application areas of the systems
get proliferated from industrial controls to home automation, communication consumer gadgets, medical devices
etc. They show apparent trends [16] shortly summarized as
”increased software portions to support complex features”,
and ”connectivity and heterogeneity”.
Currently most real-time systems are developed in C language and the portion of C++ language is increasing in that.
However, the trends in real-time embedded systems show
that the demands can be met using high-level languages and
also virtual software execution environment that allow high
productivity and high portability.
Virtual software execution environments, known as
Virtual Machine (VM), are gaining popularity especially
through JVM (Java Virtual Machine) and CLI (Common

Language Infrastructure). Especially, CLI is designed as
a target of multiple languages (CLI-compatible languages),
and it allows writing applications with multiple high level
languages due to its easy interoperability. Given the beneﬁts of VM—portability, productivity, safety etc., applying
VM to real-time embedded systems can lead to low production cost, fast time-to-market, improved system integrity
etc. Consequently, VM environment for real-time embedded applications can enormously expand the applicability
of VM.
The real-time embedded systems are fundamentally
characterized as requiring timed operations, being constrained by limited resource, and running multiple tasks
concurrently. These characterizations indicate that in realtime embedded systems, a feasible scheduling algorithm
is essential to meet the time-constraints of real-time tasks
which are running concurrently while competing for constrained resources. To make VM applicable for real-time
embedded applications, the services provided by the runtime environment should be schedulable. However, both
JVM and CLI, which are not designed for real-time applications, include operations that may cause extended blocking delay unpredictably. These operations include garbage
collection, dynamic class loading, JIT etc. A Schedulable
VM can be ensured by follows. Firstly VM’s operations
causing unpredictable blocking delay should be deterministic. They should be preemptible and allow the pause time
due to the operations to be controlled. Secondly, the size
of pause time should be set small enough so that real-time
applications with various timeliness requirements can be ﬁt
to the environment. Thirdly, resources for applications must
be predictable and available for timely computation.
Garbage collection (GC, hereafter) which we can ﬁnd
thorough references in [8] automatically reclaims notanymore referenced memory to reﬁll available free memory. It helps relieve application developers from errors on
mishandling memory references as well as memory leak.
However, despite the beneﬁts of GC, GC’s unpredictability in terms of response time and invocation is an obstacle

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

of meeting timing constraints and memory availability for
real-time applications.
In this paper, we establish a scheduling algorithm to ensure the applications’ timeliness under the GC operations
in VM. The scheduling model is basically to periodically
allocate CPU cycles required to ﬁnish garbage collection.
We design and develop a prototype on MONO CLI environment for a schedulable garbage collection (S-GC, hereafter)
such that the activity of GC is preemptible and the pause
time of GC operations is controllable with ﬁne granularity. A cost model of garbage collection is derived from the
prototype implementation of the garbage collection and the
cost model is used to get WCET of GC for scheduling. A
scheduling algorithm is proposed and is examined through
an experiment using a periodic task set on the prototype.
This paper is extended to embrace an integrated scheduling
model of garbage collection with applications from our previous paper [6] proposing the design of schedulable garbage
collection in CLI environment.
The paper is organized with Section 2 presenting an
overview of S-GC’s platform, Section 3 introducing the design of S-GC, Section 4 presenting the performance results
and cost model of the S-GC, Section 5 suggesting a scheduling algorithm for real-time periodic tasks and S-GC, and examining an example of the scheduling algorithm in MONO
runtime, Section 6 discussing related work, and Section 7
drawing a conclusion.

2. CLI, MONO, and BDW GC
We have implemented S-GC by extending BDW garbage
collector v6.1 [3] employed in MONO v0.25[18], which is
an open-source implementation of CLI. Here, we give an
overview of CLI, MONO, and BDW garbage collector.
CLI[5] is aimed to make it easy to write components and
applications with multiple languages and for multiple platforms. It deﬁnes a rich set of types to allow CLI-aware
languages to interoperate and allows each component to
carry self-describing information. Applications of multiple
languages are translated into intermediate language codes
which are executed in a virtual execution system (VES).
The VES in CLI, similar to JVM, is an abstract stack-based
machine featuring loader, veriﬁer, JIT compiler, garbage
collector, security system, multiple threading, exception
handling mechanism etc.
MONO[18] is an open source development platform
based on .NET framework and runs on Linux platform of
various architectures. Following ECMA standard, MONO
provides C# compiler, VES, and class libraries etc. The
VES in MONO is composed of well-optimized JIT and
AOT, and a BDW garbage collector etc.
BDW Garbage Collector[3] (BDW GC, hereafter) was
designed to support languages such as C, C++ etc. It is

a conservative mark-sweep GC that works without cooperation from a compiler or runtime library. To reduce a
pause time caused by GC operations, BDW GC has evolved
to support ’mostly parallel GC’ (partly incremental) for
uniprocessor systems, and parallel markers (not incremental) for multiprocessor systems.

3. Design of Schedulable Garbage Collection
The mostly parallel GC in BDW GC imposes several
limitations that prevents the GC from being schedulable.
Firstly, GC is triggered on allocation requests encountering
memory shortage and performed by the application thread
that makes the requests. Triggering garbage collection on
a basis of allocation makes collection work dependent on
allocation patterns of applications. Secondly, the virtual
memory protection for write barrier is a system dependent
feature so that it limits the portability of CLI. A long pause
time may exist during an initial step and a termination step
to re-scan both dirty pages and a root-set when the virtual
memory protection mechanism is applied to protect all heap
pages. Thirdly, the GC only works incrementally in mark
and swap phases and the control of pause time in root-scan
phase is not ﬂexible.
To address the problems mentioned above, the BDW GC
in MONO is modiﬁed. Firstly, garbage collection is performed by a concurrent task so that garbage collection can
be subject to thread scheduling and can be scheduled at certain time intervals. Secondly, the GC operations are implemented such that all operation phases are carried out incrementally and their pause time is controlled using a small
work unit. Thirdly, a write barrier is implemented to protect
per an object, rather than a block. This is done by applying
barriers to object referencing CIL (Common Intermediate
Language) instructions when an application is compiled by
MONO JIT. We run single garbage collector task targeting
uniprocessor systems.
The operations of S-GC can be depicted in Figure 1
where GC cycle refers to an entire work of GC–from the
moment that the GC is triggered until all garbage memory
is reclaimed. GC increment refers to a small portion of GC
operations performed in a GC active cycle. GC increments
are distributed in a GC cycle and are interleaved with the
execution of mutators. Apparently, the S-GC is controlled
by (i) when a GC active cycle is triggered, (ii) how long a
GC increment is, and (iii) how the S-GC and real-time application tasks are scheduled.

3.1. Concurrent Garbage Collector
We designate a concurrent task to carry out the GC operations. The task (GC task, hereafter) can be triggered periodically or when the amount of free memory is less than

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

GC increment

...

...

...

...

GC cycle

Minimum interval between GC cycles

Figure 1. GC operations
a threshold, and once the cycle starts, the execution of GC
increment is limited to a targeted pause time. Then, we can
apply real-time scheduling algorithms to address the application deadlines and the consistent progress of GC operations.

3.2. Incremental Garbage Collection
The co-scheduling of GC and real-time application tasks
requires GC’s activity to be controllable such that the pause
time due to each GC increment is able to easily ﬁt into mutators’ time-constraints.
A GC cycle in mark-and-sweep garbage collection is
composed of initialization phase, root-scan phase, mark
phase, sweep phase, and clear phase. We have made all
phases constituting the GC cycle run incrementally and the
pause duration of an increment of each phase be controlled
with the extent of basic work unit of a corresponding phase.
The basic unit of work indicates a minimum work of GC
that must be done before GC task relinquishes CPU to give
mutators a chance to run. The time taken to ﬁnish the
amount of a work unit in each phase indicates a preemption delay if there are no other factors affecting preemption
of the GC task.
In root-scan phase, the root-set, including static data,
registers, and thread stacks, is scanned. For static data, the
BDW GC in Linux scans the entire data segment because it
does not have information about which global variables actually refer to heap objects. To reduce the size of the root-set
to be scanned, we register actual global variables referring
to heap objects (a registered root-set) when they are created.
In root-scan phase, the registered root-set is scanned rather
than entire data segment. Our experiment shows that the
size of static data scanned in root-scan phase is decreased
from about 340KB (the size of data segment of MONO) to
about 5KB, and the time taken for this is very negligible.
Subsequently, scanning thread stacks becomes a major parameter that determines the cost of root-scan phase. This
can be further divided such that a basic work unit of the
incremental activity is to scan one thread stack at a time.
Since there may be changes in the stacks that have not yet
scanned in incremental root-scan, an extended write barrier,
to be explained in Section 3.3, is incorporated to protect the
changes.

The other implementation issue of handling threads in
root-scan phase is that GC task should ﬁgure out the range
of currently active stack frames of threads. This is speciﬁc
to MONO running in Linux because CLI threads are constructed using pthreads, and the pthread is created with default size of stacks. Knowing the maximum active range of
a stack of a thread prevents the GC from scanning the entire stack of the thread. This can be done after obtaining a
top stack-pointer of the thread stack. BDW GC in MONO
uses a signal-handler to have a garbage collector communicate with mutators and CLI system threads for the purpose
of retrieving the current top stack-pointer of the threads. As
a garbage collector must wait until all threads respond to
the signal, it is likely to lead to a non-deterministic waiting
delay. To address this issue, we refer /proc/ pseudo ﬁle to
get a current status of processes in Linux. This technique
is available in Linux where pthreads are implemented as
processes. This approach eliminates the waiting delay for
responses from all threads.
The basic work units in mark phase, sweep phase, and
clear phase are the KByte size of heap memory to be
marked, the number of heap memory blocks to be reclaimed, and the number of memory blocks to reset mark
bits, respectively.

3.3. Write Barrier in Incremental Rootscan and Mark phases
We use Yuasa’s snapshot-at-the-beginning algorithm[19]
to trap mutators’ updates on heap pointers during mark
phase of a GC cycle. MONO includes a JIT compiler that
translates Common Intermediate Language codes(CIL) into
native codes. Among about 220 CIL instructions, the ones
that require a write barrier due to their update operations to
object references are Stind.ref, Stﬂd, Stsﬂd, Stelem.ref. To
apply a write barrier for these CIL instructions, we modiﬁed the JIT to generate internal calls including write barrier
code as well as corresponding update operations. As they
are implemented as MONO internal calls in MONO runtime
system, the calls are altered using indirect function pointers
such that they points to internal procedures with write barrier while GC cycle is in progress, and otherwise to internal procedures without write barrier. This help avoiding the
overhead incurred for checking whether GC is in progress
or not.
Yuasa’s algorithm reclaims objects which have become
unreachable (i.e. garbage) by the time GC cycle is initiated.
On the other hand, lives object at the time the GC starts
are treated live in the same cycle. To do this, the algorithm
preserves the old references (content of source operand) of
objects being updated in update operations if the references
are white (not yet marked) such that write barrier makes
the references gray (marked and pushed into a mark stack).

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

This algorithm is applied in mark phase.
In our incremental root-scan, scanning a stack of thread
is conducted thread by thread; mutators can interleave after GC task ﬁnishes scanning one stack of a thread. In this
phase, Yuasa’s algorithm, making gray the old reference of
object being updated, is not complete because mutators can
interleave and make changes on stacks before GC task ﬁnishes obtaining all root set. In order to address this problem,
we extended Yuasa’s algorithm to be applied in incremental
root-scan phase where threads are scanned thread by thread.
The extended write barrier prevents loss due to updates on
stacks in root-scan phase. The extended write barrier grays
a new reference (a target operand) as well as an old reference of object being updated on update instructions. This
algorithm is very similar to write barrier suggested in [4]
where the write barrier is used to synchronize the mutators
graying their own stacks and GC task starting marking at
the same time.
In addition to the existing objects, the objects allocated
during GC cycle are marked live to ease a termination condition and help avoiding a long-pause in a termination in
mark phase.

4. Experiment
4.1. Experiment Environment
The experiment of S-GC focuses on the performance of
S-GC, especially pause time. In addition, a cost model of
GC and the given application characteristics can be established using the measured parameters in the experiment.
Among our benchmarking applications, DirectedGraphs, and ThreadGraphs are from SSCLI v1.0 [14], AHC
is from Microsoft research center[13], and Multithreads and
Periodics are devised applications to simulate periodic activities. With our knowledge, no benchmark applications
designed with time-constraints are available in CLI. Our
benchmark applications do not completely ﬁt into that domain either. However, they are still appropriate to measure
the pause time caused by GC as well as the parameters for
GC cost model.
In addition to S-GC, we also compare the pause time
of S-GC with a stop-the-world GC of BDW GC (Stop-GC,
hereafter). The experiment of mostly parallel BDW GC
is not presented here because MONO does not incorporate
the incremental features of the BDW GC. The experiment
is conducted in a PC workstation with a 1.5GHz Pentium
IV processor and 256MB memory. To have a high resolution timer and preemptive kernel, TimeSys’ Linux/RealTime(v4.1.147)[17] is used. To minimize inﬂuence from
kernel’s activity, the experiment is carried out in single user
mode of Linux. Also, a ﬁxed sized heap memory allocated

at an initialization of CLI is locked in physical memory using mlock() system call to prevent paging.

4.2. Controllable GC’s Pause Time

Table 1. Proﬁle of Stop-GC & S-GC

Benchmark

DirectedGraphs
MultiThreads
Periodics
ThreadGraphs
AHC

Stop-GC
Max.
Total
GC
App.
Pause
Time
(μs)
(ms)
10014
6215
7008
775
3754
2839
3705 11645
8850
4694

S-GC
Max.
Total
GC
App.
Pause
Time
(μs)
(ms)
964
6166
1028
814
967
2843
984 11821
961
4737

Table 1 shows the garbage collection proﬁle for Stop-GC
and S-GC chosen as the worst case after 20 times trial for
each Stop-GC and S-GC. For Stop-GC, the size of heap for
the experiment is determined with a minimum size of heap
with which benchmarks run without suffering from out-ofmemory. For S-GC, we setup 1.5 times larger heap than that
of Stop-GC because incremental GC requires reserving free
memory before starting GC cycle, and we get 50% space
overhead as a lower bound of space that guarantees operating S-GC successfully, based on our experimental observation. The precise space overhead should be examined using
maximum live memory, maximum amount of memory allocated to applications, GC task’s deadline etc. in Section 5.
The targeted maximum pause time caused by GC increment
in S-GC is set to 1ms.
The experiment shows that the maximum pause time for
all benchmarks but Multithreads does not exceed the speciﬁed pause time in the given experimental environment. The
pause time for Multithreads also can be met within the targeted pause time by reducing the runtime parameters.

4.3. Cost Model of Garbage Collection
Scheduling real-time applications requires a precise prediction of the cost and overhead of GC operations. As each
GC cycle in S-GC is composed of initialization, root-scan,
mark, sweep, clear phases, we can deﬁne the execution time
of a single GC cycle with equation (1). A cost equation can
then be derived for each phase by identifying the signiﬁcant
cost variables including heap size, number of objects, etc.
The linear coefﬁcients of the equation can be extrapolated
with the measured worst case execution time (WCET) of
the basic operations in the experiment. The details on how
the cost equation is derived is skipped due to the limitation

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

of pages.
EGC

=

Initialization + Root-Scan
+M ark + Sweep + Clear

(1)

Figure 2 compares GC cycle’s measured cost (the plots
named with (M)), and estimated cost (the plots named with
(E)) using the cost model. Some benchmarks (i.e., MultiThreads) shows that the estimated cost is much larger than
the measured cost in mark phase. The difference comes
from the fact that the cost coefﬁcient used in the cost model
is derived from the worst case path among the associated
operations, but the measured cost indicates an average cost
whose operations may not always hit the worst case path.
This indicates that deriving practical WCET of GC cost
needs to be reﬁned by considering the distribution of pointers as well as the size or the number of live objects as taken
in [1].

and periodic operation phase. In the initialization operation phase, the system is conﬁgured such as the required
class libraries are loaded, application tasks are initialized,
and static objects are constructed, etc. In periodic operation
phase, task instances arrive periodically and get executed.
The instances may construct objects but we assume that the
memory objects created in one instance of a task are not
referred by successive instances of the task. That is, the
memory allocated by one instance of a task in steady phase
becomes collectable once the instance ﬁnishes. If there are
objects to be referred by multiple task instances, they can be
allocated as static objects. We denote the maximum amount
of memory allocated by an instance of a task Ti as Ai .
P GCI

E GCI

D GC
P GC

P GC

(a)
Heap

8000
DirectedGraphs(M)
MultiThreads(M)
Peridics(M)
ThreadGraphs(M)
AHC(M)
DirectedGraphs(E)
MultiThreads(E)
Peridics(E)
ThreadGraphs(E)
AHC(E)

7000

Average Time (us)

6000

H Allocate

5000

Time

4000

(b)

3000

Figure 3. GC scheduling and allocation of
heap memory

2000

1000

0
Initialization

Root−Scan

Mark
GC Phases

Sweep

Clear

Figure 2. Comparison of GC Cost

Our scheduling approach for S-GC task is depicted at (a)
of Figure 3.
• GC task is assigned with the highest priority.

5. Scheduling GC
This section ﬁrstly presents a proposed scheduling algorithm for real-time periodic tasks and S-GC, and then shows
an experiment result of the scheduling algorithm in MONO
runtime.

5.1. GC Scheduling Algorithm and Analysis
To demonstrate how the GC task can be scheduled with
application tasks, we set up a scheduling approach using
ﬁxed priority scheduling algorithm . The application tasks
are assumed to run periodically with their deadlines equal
to their periods. The priorities application tasks are determined based on rate monotonic scheduling algorithm except
garbage collector, which is assigned with the highest priority. Each task’s worst case execution time is known a priori.
During the execution of application tasks, memory requests must be satisﬁed. To simplify the memory request model, we assume that the real-time systems conduct phased execution with initialization operation phase

• The minimum interval between two consecutive GC
cycles is determined while the task set including the
GC task is schedulable using a ﬁxed priority scheduling algorithm. The minimum interval is denoted as
PGC .
• Based on the minimum interval, a threshold to trigger
GC cycle is set to a speciﬁc percentage of free heap
memory (HT H ).
• Once GC cycle is triggered, a ﬁxed execution time
quantum for each GC increment (EGCI ) is assigned.
The GC increments are invoked periodically with a
ﬁxed period (PGCI ) until the GC cycle completes.
The deadline of GC cycle for GC task is denoted with
DGC .
Predicting the upper bound of live memory (i.e. the
memory used by live objects) is important because the
size of live memory determines the execution cost of SGC based on mark-sweep garbage collection algorithm, and
also determines the required minimum size of memory. The

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

memory objects survived from the initialization operation
phase in the system are likely to last during the system’s
lifetime. Thus, the maximum live memory consists of live
memory allocated in the initialization operation phase (LC ),
and live memory allocated in the periodic operation phase
by all active task instances(LD ). Thus, assuming all N
tasks, Ti where i = 1, 2, ...N , have their active instances at
the same moment, we have the maximal live memory equal
to
N

Ai
(2)
Lmax = LC +
i=1

In terms of heap memory, we can obtain a bound on
the allocated memory following the curve depicted at (b) of
Figure 3. When a GC cycle ends, the allocated heap memory are used for three types of objects: (1) the long live
objects allocated in the initialization phase and the ones requested by active task instances (2) the objects that are freed
after the S-GC cycle begins and (3) the allocated heap as
new task instances arrive since last GC cycle. Then, the allocated heap will reach the maximum just before the next
GC cycle reclaims garbage objects. The maximal allocated
heap memory, which must be less than the total heap size
H, can be expressed as
N

PGC + DGC
(
 × Ai ) ≤ H
Pi
i=1
(3)
where Pi is the period of task Ti .
Since the GC is running at the highest priority and the
execution time of a GC invocation EGC can be computed
via the cost model of garbage collection given in equation
(1) of Section 4.3, we have the following equation for the
deadline of each GC cycle, DGC , which depends upon the
parameters EGCI and PGCI :

HAllocated = Lmax +

DGC = 

EGC
EGC
 × PGCI + EGC − 
 × EGCI (4)
EGCI
EGCI

The threshold of free heap memory (HT H ) to trigger GC
cycle is computed using the equation:
HT H =

N

DGC
(
 × Ai )
Pi
i=1

(5)

To ensure that application tasks can meet their deadlines, schedulability analysis can be done through either
Rate Monotonic Analysis [12] or Response Time Analysis
[9]. While applying Rate Monotonic Analysis checking the
total utilization of a task set with the task set’s utilization
bound, EGCI and PGCI are used to compute the utilization of TGC . Response Time Analysis checks whether the

worst-case response time of a job that occurs at a critical
instant meets before the job’s deadline. With this analysis,
preemption delays caused by the GC must be included in
the schedulability analysis. Using [9]’s response time calculation equation, we get the worst-case response time Ri
for task a Ti ,
Rin+1 = Ei +


∀j ∈hp(i)

(

Rin
 × Ej ) + P GCDelayi (Rin )
Pj

(6)
where Ei and Pi is an execution time and a period of Ti , respectively, Ri0 =Ei and hp(i) is a set of application tasks that
have a higher priority than that of Ti . The preemption delay
Rin
caused by GC, P GCDelayi (Rin ) is  PGCI
EGCI if Rin is
Rn

i
EGC otherwise. The
less than or equal to DGC , or  PGC
n
equation (6) iterates until Ri is converged, starting from
Ri0 .

5.2. Example of GC Scheduling
The experiment for the scheduling algorithm with a periodic task set is conducted in MONO with S-GC. This experiment is designed as a feasibility study for the proposed
garbage collection scheduling algorithm on actual runtime
environment. Besides that, the heap allocation distribution
according to the behavior of GC is examined.
The task set given as an example consists of three tasks
running periodically, and each task’s execution time, period, and worst case memory allocation per instance are
presented in Table 2. The tasks are scheduled through Rate
Monotonic scheduling algorithm [12] so that the task with
a shorter period has a higher priority.
GC task, TGC , is assigned with the highest priority in the
task set and its execution time and period of GC increment
(EGCI and PGCI ) are chosen to make the task set including TGC schedulable according to Rate Monotonic schedulability analysis. Table 3 summarizes the TGC ’s scheduling parameters as well as the lower bound of heap memory
size and the free heap memory threshold to trigger GC cycle. The equation for the execution time of a single GC
cycle (1), and the equation (4) derive EGC and DGC , respectively. The lower bound of heap memory is set with
the maximum heap allocated HAllocated derived from the
equation (2) where PGC is equal to DGC .
In the experiment, garbage collection is triggered in two
different ways: triggering GC cycles at free heap memory
threshold HT H , and triggering GC cycles at every GC period PGC (hereafter, the former is called ”GC triggered at
free memory threshold” and the latter is ”GC triggered at
period”.) The system environment for the experiment is
same as speciﬁed in Section 4.
The invocation behavior of GC cycle and the distribution of heap memory allocation over time are presented in

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

Table 2. Task set
Pi
(ms)
27
200
250
400

Ai
(K)

Table 3. GC scheduling parameters for a task set

800
760
616

EGCI

PGCI

EGC

(ms)
1

(ms)
27

(ms)
5

6000

6000

5000

5000

4000

Allocated Heap (K)

Allocated Heap (K)

TGC
T1
T2
T3

Ei
(ms)
1
50
50
100

3000

1000

1000

1500
2000
2500
3000
3500
4000
Time elapsed since a system started (ms)

4500

5000

5500

(a) GC triggered at free memory threshold

HAllocated

HT H

(KByte)
172

(KByte)
6500

(KByte)
2476

3000

2000

1000

Lc

4000

2000

500

PGC
DGC
(ms)
135

0

500

1000

1500
2000
2500
3000
3500
4000
Time elapsed since a system started (ms)

4500

5000

5500

(b) GC triggered at period

Figure 4. GC cycle invocations and heap memory allocation
(a) and (b) of Figure 4. The bars on the top of the graph
show the invocation of GC cycle: each bar indicates one
GC cycle, and the width of the bar indicates the duration of
GC cycle. In (b) of Figure 4, the width of the bars appears
relatively thin due to frequent invocations presented in the
small scale of graph.
With the experimental parameters, the GC triggered at
period has more frequent invocations of GC cycles; it has
44 times of GC cycles compared to 15 times of that of the
GC triggered at free memory threshold. However, while GC
triggered at free memory threshold shows consistent memory reclamation rates (from 2966KBytes up to 3672KBytes)
at each GC cycle, GC triggered at period shows big variation on that (from 0KBytes up to 4556KBytes). The frequent invocation of GC cycles without gaining much free
memory can result in signiﬁcant overhead even though
the task set is schedulable. This result indicates that we
should take account of the status of memory allocated in
the scheduling of GC operations.
The GC triggered at free memory threshold has its GC
cycle PGC varies from 209ms up to 400ms. The reasons
of the gap of the observed PGC from the derived PGC in
Table 3 are as follows. Firstly, because the memory allocation is not evenly distributed, there is some variation on the

length of the interval of GC cycle. Secondly, the derived
PGC is the minimum interval for the worst case that guarantees the schedulability of the task set along with GC task.
One approach to efﬁciently exploit the CPU resource with
GC task triggered at free memory threshold is to schedule
background tasks together, which do not have hard deadline.

6. Related Works
Bacon et al. [2, 1] presented an incremental mark-sweep
GC that supports memory defragmentation with a page unit
in Jike’s RVM. GC is scheduled by interleaving mutators
and a garbage collector using ﬁxed time quanta to show a
consistent minimum mutator utilization. They also showed
that a cost model established to derive WCET of GC can
be simpliﬁed practically by minimizing the number of required application parameters. Compared to our S-GC, the
remaining issues of the GC seem that a pause in a root-scan
phase done atomically should be able to be controlled and
GC must be able to run as a concurrent task so that more
ﬂexible scheduling algorithms can be applied to GC.
Henriksson [7] suggested a semi-concurrent scheduling

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

algorithm motivated to prevent garbage collection from interrupting high priority tasks. In a system that assigns hard
real-time tasks a high priority and soft real-time tasks a low
priority, garbage collector task performs GC work for high
priority tasks while the high priority tasks do not run. GC
work for low priority tasks is done incrementally at memory allocation time by themselves. Even though it ensures
high priority tasks from disturbance of GC, applying this
scheme in Virtual Execution Environment such as JVM,
CLI etc. should be carefully considered as tasks in VES
mostly share common objects and the scheme requires distinguishing memory objects according to tasks.
Kim et al. [10] suggested that a sporadic server with
a highest priority conducts garbage collection in copying
garbage collection algorithms; the sporadic server is assigned with the shortest period compared to mutators so that
the server can have the highest priority. Because this limits the size of a budget as well as the period of the server,
they extended the algorithm by employing dual servers, one
server for conducting an initialization of GC cycle (ﬂipping
from-space and to-space) and the other server for scanning a
heap and evacuating live objects in [11]. However, applying
this algorithm is limited to copying algorithms.
Robertz [15] emphasized to determine GC cycle time
that guarantees that GC cycle will be completed until available memory is exhausted, and lets a process scheduler determine the size of GC increment implicitly based on the
scheduler’s algorithm. However, determining the size of
GC increment cannot be leaved to process scheduler’s algorithm as its lower bound is affected by the ﬂexibility of
controlling the pause due to GC (e.g. minimum work units
of each phase constituting GC cycle).

7. Conclusion
We have designed a schedulable garbage collection for
embedded applications in CLI such that all phases constituting a garbage collection cycle are carried out incrementally and their pause time is controlled in a small work unit.
A garbage collector task concurrently runs with application tasks so that an integrated analysis for task scheduling and heap memory availability. The experimental data
on the prototype shows that the proposed S-GC can provide
controllable pause time for each GC increment. The cost
model established using measured coefﬁcients of each GC
phase and the characteristic parameters of applications can
provide reasonable prediction for WCET of GC. A scheduling algorithm of garbage collection is used to illustrate how
the time and memory constraints of real-time applications
can be met. The experiment result of the scheduling algorithm using periodic task set on the prototype shows that
the timeliness of real-time applications can be guaranteed
in VM environment along with garbage collection.

References
[1] D. F. Bacon, P. Cheng, and V. T. Rajan. Controlling fragmentation and space consumption in the Metronome, a realtime garbage collector for Java. In ACM SIGPLAN 2003
Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES’2003), 2003.
[2] D. F. Bacon, P. Cheng, and V. T. Rajan. A real-time garbage
collector with low overhead and consistent utilization. ACM
SIGPLAN Notices, 38(1):285–298, Jan. 2003.
[3] H.-J. Boehm and M. Weiser. Garbage collection in an uncooperative environment. Software Practice and Experience,
18(9):807–820, 1988.
[4] D. D. and L. X. A concurrent, generational garbage collector for a multithreaded implementation of ml. In Twentieth
Annual ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, New York, NY, 1993. ACM
Press.
[5] ECMA. Ecma-335 common language infrastructure, 2002.
[6] O. Goh, Y.-H. Lee, Z. Kaakani, and E. Rachlin. A schedulable garbage collection for embedded applications in cli.
In 11th International Conference on Real-Time and Embedded Computing Systems and Applications (RTCSA05), July
2005.
[7] R. Henriksson. Predictable automatic memory management for embedded systems. In OOPSLA’97 Workshop on
Garbage Collection and Memory Management, 1997.
[8] R. E. Jones. Garbage Collection: Algorithms for Automatic
Dynamic Memory Management. John Wiley and Sons, July
1999.
[9] M. Joseph and P. Pandya. Finding response times in realtime systems. Comp.Journal, 29(5), 1986.
[10] T. Kim, N. Chang, N. Kim, and H. Shin. Scheduling garbage
collector for embedded real-time systems. In Proceedings
of the ACM SIGPLAN 1999 Workshop on Languages, Compilers, and Tools for Embedded Systems(LCTES’99), pages
55–64, 1999.
[11] T. Kim and H. Shin. Scheduling-aware real-time garbage
collection using dual aperiodic servers. In RTCSA, 2003.
[12] C. L. Liu and J. W. Layland. Scheduling algorithms for multiprogramming in a hard-real-time environment. Journal of
the Association for Computing Machinery, 20(1), Jan. 1973.
[13] Microsoft
Corp.
CLI
benchmarks.
http://research.microsoft.com/ zorn/benchmarks.
[14] Microsoft
Corp.
Microsoft
SSCLI.
http://msdn.microsoft.com/net/sscli.
[15] S. Robertz and R. Henriksson. Time-triggered garbage
collection-robust and adaptive real-time GC scheduling for
embedded systems. In Proceedings of the ACM SIGPLAN
Languages, Compilers, and Tools for Embedded System,
2003.
[16] Sun Microsystems Inc. The real-time java platform, 2004.
[17] TimeSys Corporation. Timesys linux/real-time user’s guide,
version 2.0, 2004.
[18] Ximian. MONO. http://www.go-mono.com.
[19] T. Yuasa. Real-time garbage collection on general-purpose
machines. Journal of Software and Systems, 11(3):181–198,
1990.

Proceedings of the Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing
0-7695-2561-X/06 $20.00 © 2006

IEEE

Table Driven Proportional Access based Real-Time Ethernet for Safety-Critical
Real-Time Systems
1

Daeyoung Kim, 2Yoonmee Doh, 'Yam-Hang Lee

1

Computer Science and Engineering, Arizona State University, USA
2Computer & Information Science & Engineering, University of Florida, USA
{ dkim, yhlee}@asu.edu, ydoh@cise.ufl.edu
real-time Ethernet for safety-critical real-time systems
using COTS Ethernet hardware and software methods. In
this paper, we propose a Table Driven Proportional
Access (TDPA) protocol in the standard Ethernet MAC
layer to achieve guaranteed real-time communication.
The TDPA protocol is a good candidate to implement
cyclic scheduling which is practically used in safetycritical real-time systems. The protocol uses a master to
signal the starting instance of slots that are allocated to
channels of fixed capacity. There are four types of
channel, for control, single message, multiple messages,
and Ethernet traffic, respectively. A complete scheduling
approach for partition-based systems can be applied to
establish the requirement for the capacity of each channel.
There are several commercial products available that
claim to provide a real-time performance guarantee over
LANs. IEEE 802.12 uses Demand Priority Access to
provide users with guaranteed bandwidth and low latency
[2]. National Semiconductor's Isochronous Ethernet
includes 96 64-Kbps B channels for real-time traffic as
well as a 10-Mbps P channel for normal Ethernet traffic
[3]. All of the above schemes require modifications to the
existing network hardware. On the other hand, E T H E R
provides real-time guarantees for multimedia applications
with a software approach [4]. It uses a timed token similar
to FDDI and P1394 to control the packet transmission in
the network. However, their scheme cannot fulfill the
requirements of safety-critical real-time systems such as
integrated modular avionics (IMA) standards. For
example, E T H E R does not support scalable multicast
real-time sessions, or a multi-frame cyclic schedule. Also,
all message transmissions must follow the non-contention
based round-robin polling method, even if messages have
no real-time communication requirements.
The rest of the paper is organized as follows. We
introduce the proposed Strongly Partitioned Real-time
Ethernet (SPRETHER) architecture and system model in
Section 2. The details of the protocol are presented in
Section 3 and its scheduling approach is shown in Section
4. The prototype platform and performance measurement
are discussed in section 5 . A short conclusion follows in
section 6.

Abstract
The Ethernet technology has received much
attention in embedded system industries because of its
cost efficiency, high availability, and popularity. This
trend is not an exception even in safety critical real-time
systems such as integrated modular avionics systems. To
overcome the lack of deterministic characteristics in
Ethernet protocol, we propose, in the paper, a sofmareoriented approach based on table-driven proportional
access. In addition to the protocol details, performance
and schedulability analyses, as well as a prototype
platform, are described in the paper.

1. Introduction
Given that safety-critical real-time systems require
deterministic behaviors in their communication operations,
expensive and specialized communication networks have
been used in the areas of avionics, defense, and space
applications. For example, the MIL-STD and
Aeronautical
Radio
Inc.
(ARINC)
standard
communication networks are popular for building
avionics systems [l]. One of the problems of using these
networks is that it is difficult to find engineers who are
experienced with them. Also, this type of communication
networks is expensive and has a limited bandwidth. An
alternative way is to make use of popular and COTS
network technology wherever the communication delays
can be made predictable.
Currently, Ethernet is the dominant network
technology because of its cost-effectiveness, high
bandwidth, and availability. With a Carrier Sense
Multiple Access/Collision Detect (CSMNCD) MAC
protocol, Ethernet is inherently incapable of guaranteeing
deterministic accesses due to possible packet collision and
random back-off periods. However, in order to take
advantage of COTS Ethernet in real-time systems, it is
preferred not to change the standard network hardware
components and protocol so that most advanced and
inexpensive Ethernet products can be employed without
any modifications. The issue is then how to design an
easily accessible, reliable, deterministic, and affordable
356
0-7695-1414-6101 $17.00 0 2001 IEEE

2.

minimum inter-amval time (T,), message size ( M J , and
deadline (DJ.
We assume that the scheduling of all real-time
message streams is done off-line, and a cyclic table for
message transmission and reception can be constructed.
To utilize the shared media of an Ethernet, we adopt a
Table Driven Proportional Access (TDPA) based protocol
on top of Ethernet MAC protocol. The TDPA protocol is
implemented using a synchronized RX and TX table in
each node. All transmissions and receptions are invoked
based on timing, and spatial information stored in the
table. The proposed network model is shown in Figure 2.
It has the following characteristics:
Software
Based
Frame
Synchronization:
Synchronization of message transmission and reception
based on the cyclic table is essential to guarantee realtime constraints in the TDPA protocol. Since our
approach employs COTS Ethernet hardware, it is required
to develop software based frame (cyclic table)
synchronization.
Publish/Subscribe Communication Model: The
communication mechanism we adopted in the
SPRETHER supports a publishlsubscnbe communication
model, which has proved to be useful for the upgrade and
evolution of real-time systems and is a fundamental
communication model of the IMA system.
CSMA/CD
Support
for
Non-Real-Time
Applications: One of the advantages of the SPIRIT
model is to support various applications of different
criticalities. So non-real-time applications can be run on
the same processor with hard real-time applications. The
proposed real-time Ethernet is required to support
CSMNCD Ethernet protocol, which can be used to
transmit non-real-time TCP/UDP/IP packets.

SPRETHER Architecture and Model

The SPRETHER has been developed for the Strongly
Partitioned Integrated Real-Time System (SPIRIT), which
emphasizes the integration of real-time applications in a
common multiprocessor platform [SI.The SPIRIT utilizes
multiple standardized processor modules in building
functional components of comprehensive real-time
systems. While permitting resource sharing among
applications, the integration approach employs temporal
and spatial partitioning to set up the application
boundaries needed to maintain system predictability, realtime response, and dependability. Each processor can host
multiple partitions in which applications can be executed
using the assigned resources. Spatial partitioning implies
that a partition cannot access other partition's resources,
like memory, buffers, and registers. On the other hand,
temporal partitioning guarantees a partition's monopoly
over the use of a pre-allocated processing time without
any intervention from other partitions. As a result, the
applications running in different partitions cannot
interfere with each other. To facilitate communications
between applications, each partition can be assigned with
one or more communication channels. An application can
transmit messages during the time slots assigned to its
channel and access exclusively the channel buffers. In
Figure 1, we simplify and generalize the SPIRIT model to
focus on communication aspects of the SPRETHER.
Node 1
-

Node n

1

I

Ua.1.r

Cyclic SPRETHER Scheduler (Ethernet)

SI.".

Shy.

I

Figure 1 SPRETHER Communication Model
A channel is a basic scheduling entity of the
SPRETHER. A channel server can support single or
multiple message streams. To support non-real-time
applications, a dedicated non-real-time channel server is
provided. A two-level hierarchical scheduling method is
applied to schedule channels and messages. A channel
server, which supports multiple real-time message
streams, provides a fixed-priority preemptive scheduling.
Then, a cyclic schedule assigns Ethernet bandwidth to the
channel servers of multiple partitions. Note that the
preemption unit of fixed-priority scheduling is a fixed
size packet and a message is segmented into multiple
packets for transmission. A message stream is specified
by several parameters, including message period or

Figure 2 SPRETHER Architecture

3. SPRETHER Protocols
3.1.

Table Driven Proportional Access Protocol

SPRETHER supports four different channels; single
message stream channel. (SMC), multiple message stream
channel (MMC), non-real-time channel (NRTC), and
control channel (CCH). The channels are configured
357

initialized and allocated at system boot time. An
operational example of the TDPA based SPRETHER
protocol is given in Figure 4.

following a time-multiplexing approach that is defined in
a cyclic scheduling table as illustrated in Figure 3.

Nod.2

1

Nod. 1

I
I

Figure 3 Illustrations of Channels and Gaps
The control channel carries SPRETHER control
messages and is a broadcast Ethernet channel (with an
Ethernet broadcast MAC address) from the master node to
all slave nodes. The most important control message is
the New Interval Start (NlS) packet that is broadcast to all
slave nodes by the master. The packet is used to establish
a global frame synchronization according to the cyclic
scheduling table, as well as to indicate the beginning of
the next transmission interval that is assigned as a SMC,
MMC, or NRTC within a frame.
Single or multiple message stream channels carry all
reai-time message and are configured as another
broadcast channel in Ethernet (with a fixed multicast
MAC address). According to the cyclic scheduling table,
the transmission intervals are assignment to SMCs MMCs,
and NRTC. There is one sender per SMC and MMC
which is responsible to publishing its message to all
possible subscribers. Using an expiration timer and
segmenting messages, a sender must ensure that its
transmission doesn't overrun the allocated intervals. Thus,
there will be no collision in SMCs and MMCs.
Non-real-time messages can only be transmitted via a
non-real-time channel that is allocated with intervals
according to the scheduling table. Once a node detects the
beginning of non-real-time channel based on NIS packets,
it can send out any waiting non-real-time messages
following the conventional Ethernet CSMNCD mode and
with the constraint that the transmission should not
overrun the allocated interval. A detailed discussion of
this feature will be given in Section 3.4.
The gaps between two consecutive transmission
intervals can be one of two types; Gapl and Gap2. Gapl
tolerates possible jitter caused by a sender at the end of
the current slot. This jitter could be a result of clock
drifting or asynchronous services in the node. Gap2
compensates for the propagation delay and processing
time of a NIS packet. The worst-case delay of gaps should
be calculated and included in the channel scheduling and
capacity assignment. The cyclic scheduling table is
constructed off-line and put into the SPRETHTER kernel
space. The data structures such as the cyclic table, and the
transmission and reception buffers of the channel, are

Figure 4 Example of TDPA Protocol
Single Message Stream Channel (SMC)

3.2.

A SMC channel can be easily supported by the
SPRETHER because it transmits only one message
stream. The capacity allocated to a SMC of message
stream i must be larger than Ml/min(Tl, DJ and must
compensate for the drift of synchronization due to the
propagation delays and processing times of NIS and data
packets. We illustrate the protocol activities of the SMC
channel in Figure 5.

SML

.SYOI

suo.suc u-q.
1*L

SYS P u n

la.-

h m r d
L.nm

Figure 5 Analysis of SMC Channel Activity
In the SMC protocol, there are two overhead gaps;
SGupl and SGupZ. Table 1 shows the parameters that
constitute the overheads of the SMC channel.

Table 1 Parameters Used in the SMC Protocol
Notation
MIG

I

I

Description

Minimum Interframe Gap of Ethemet ( 9 . 6 ~ ~ )
(refer Table 4)

I
CAT

358

I

(refer Table 4)
Ethernet Frame Overhead (26 Bytes = 26 x 0 . 8 p
2 0 . 8 ~+
~ )SPRETHER Packet Overhead (12 Bytes
12 x 0.8us = 9 . 6 ~ s=) 3 0 . 4 ~ s
Channel Allocated Time

=
=

expiration of the channel interval, and determine a packet
of suitable size for transmission.
There is a trade-off choosing the size of a fixed
packet. If we choose a small packet size, a message will
be segmented into many packets, which causes an
increased number of inter-packet gaps (MGap.3) and
MMO overheads. On the other hand, if we select a large
packet size, the blocking delay may result in an increase
of the bandwidth requirement for the channel. So it is
required to select the optimal packet size based on the
characteristics of arrival message streams.
In the MMC protocol, there are three types of timing
gaps; MGupl, MGup2, and MGup3. Table 2 shows the
parameters that are used in obtaining utilization of a
MMC channel.

Then, the utilization of the SMC channel can be obtained
with the following equation:
SGapl + S ( N I S ) + SGap2 + SMO
UtilsMc = 1 -

CAT

We show the utilization graph of the SMC protocol in
Figure 6. According to the figure, we can obtain 50% and
70% utilization when CAT is 499 ps and 832 ps
respectively.
SMC Channel UP112abon
,
,
,
,
,
,
,
,
,
,
7,

Table 2 Parameters Used in the MMC Protocol
(MIG, Preparation time of NIS

99.2 ps,

MGapl

Max

MGap2

Max (MIG, Processing Time of NIS} = 33.84 ps

=

Channel AlbCabOn Time (us)

MGap3

Figure 6. Utilization of SMC Channel
NIS packet transmission time

3.3.

Multiple Message Streams Channel (MMC)

MMO

A multiple message channel can be used to serve
several message streams simultaneously. We assume that
messages are with fixed priorities that follow ratemonotonic or deadline-monotonic ordering. Since a
transmitting packet should not be stopped arbitrarily, a
higher-priority message that just arrived could be blocked.
To reduce the blocking factor caused by non-preemption
of packet transmission, we segment a message into
packets of fixed size. The bigger the packet size, the more
the blocking delay. This blocking factor is unavoidable in
Ethernet because when the packet is sent to the Ethernet
hardware, it cannot be cancelled by the arrival of a
higher-priority packet (or message). We illustrate the
protocol activities of the MMC channel in Figure 7.

]
MML

I

=

86 ps (refer Table

Ethernet Frame Overhead (26 Bytes = 26 x 0.8ps =
2 0 . 8 ~+~ SPRETHER
)
Packet Overhead (12 Bytes =
12 x 0 . 8 ~ =
s 9 . 6 ~=~30.4
) ps
Pure Message Size in Bytes x 0.8 ps

Then, the utilization of MMC channel can be obtained
with the following equation:

The fully analyzed utilization of the MMC channel is
illustrated in Figure 8. A utilization of below zero means
that it is not schedulable.

Fixed PackelSire (bytes)

0

0

Channel Allocation Time (UP)

Figure 7 Analysis of MMC Channel Activity

Figure 8 Utilization of MMC Channel’

‘If the last packet is sent just before the time of
expiration of the current channel interval, the transmission
will cause an overrun. To avoid an overrun, a MMC
channel must monitor the amount of time left before the

We also show the utilization of the MMC channel
when the size of fixed packets is 128 bytes in Figure 9.

359

the Ethernet hardware either succeeds in transmitting the
packet by the end of the time interval, or cancels and
postpones the failed transmission due to a collision. The
value of the SGT for packet j should be set to D - S(MJ
where S(MJ is the expected time being taken to transmit
packet Miand D is the end of the current channel interval.
The implementation of the SG technique varies with
the Ethernet chipset being used. For instance, in the case
of the Motorola MPC860, its CPM (Communication
Processor Module) supports the Stop Gracefully
command.

3.5.

Channel Abcsuon Time (us)

In addition to non-real-time messages, which are
transmitted with Ethernet packets, SPRETHER adds two
types of packets; control and real-time data packets. A
control packet is used to synchronize the transmission and
reception, and to implement other control functions. A
real-time data packet is used to carry either SMC or MMC
real-time messages. We show the format of a packet in
Figure 11. A SPRETHER packet is encapsulated in the
data area of an Ethernet frame, and distinguished by the
TYPE word of the Ethernet frame header. The description
of fields is shown in Table 3.

Figure 9 Utilization of MMC Channel when
Packet Size is 128 Bytes
3.4.

Packet and Frame Format

Non-Real-Time Channel (NRTC)

We allow a node to use CSMAJCD mode operation
for non-real-time communications. We illustrate an
NRTC in the cyclic time frame and show NRTC packet
transmission behaviors in Figure 10.

Figure 10 Stop-Gracefully Technique
Figure 11 SPRETHER Packet Format
A node, which participates in NRTC mode operation,
must not overrun the scheduled NRTC time interval.
However, a standard C S M N C D protocol does not
guarantee it because of the retransmission with random
back-off algorithm. As the number of participating nodes
increases, we can see that the statistical upper bound of
expected completion time of the packet transmission
becomes unacceptable in real-time communication. So we
use an enforcement technique with which we can stop a
packet retransmission if there is a collision. Even though
we cancel the on-going packet transmission, we aim to
achieve the maximum utilization of NRTC.
We call the enforcement method the Stop Gracefully
(SG) technique. Whenever a sender requests a
transmission to Ethernet hardware, it sets a local timer to
the value of SGT (Stop Gracefully Timer value). If the
transmission succeeds before timer expiration, it cancels
the timer. Otherwise it issues a Stop Gracefully command
to the Ethernet hardware upon the timer expiration. Then

Table 3 The SPRETHER Packet Field Description
Field
version
type
CAT
Data
Length
Major/
Minor

360

I
I

Description
version
of
a
packet.
type of packet.
channel allocation
time in pseconds.
total length of
real-time data.
flags for indication
of Major and
Minor frame.

I

Field

I

Description

I Interval# I serial # of an interval
Channel#
Flags
Fragment
Offset
Message
Identifica
-tion

in a cyclic table.
identifier of channel.
flag identifies the final
fragmented packet.
offset
field
for
fragmentation.
identifier
of
the
message stream.

MSIZE. It is an important job of the system integrator to
find the optimal MSIZE.

The first parameter, channelNumber, designates the prescheduled channel number. The second parameter, mode,
can be used to inform the SPRETHER module of the
mode of operation, RD-ONLY, WR-ONLY, and
RD-WR. It returns a corresponding identifier, spid, with
which a task can send and receive real-time messages.
int sp-close (spid) : This removes the real-time
session itself from the SPRETHER module.
int sp-send (spid, flag, srnsg, length): This requests
a transmission of the message stored in the buffer smsg of
specified length. According to the value of the flag, the
sender can either be blocked until the message is
transmitted, or continued.
int sp-receive (spid, flag, rmsg, length): This
requests to receive the next available message in the
buffer rrnsg. If the flag is block-mode-call, it waits until
the next message arrives.

Figure 12 Architecture of MMC Channel Server
Suppose that there are p message streams, MS,, in a
MMC channel server, s k , listed in priority order MSl<
MS2<... <MS, where MSI has the highest priority and MS,
has the lowest. In order to evaluate the schedulability of
the MMC channel server Sh let's consider the case that s k
is served at a dedicated bandwidth of Pk, normalized with
respect to the bandwidth of s k .
Based on the necessary and sufficient condition of
schedulability in [6][7],message stream MS, is
schedulable
if
there
exists
a
t E H i= { IT, I j = 1,2,...i; 1 = 1,2,..., [Di/Tj
u{D,
} ,such

4. Scheduling Real-Time Messages
The scheduling approach first finds the necessary
capacity for each real-time channel, and then obtains a
distance-constrained cyclic schedule for all channels. The
distance-constrained cyclic schedule is used to build RX
and TX tables of each node for the TDPA protocol.
4.1.

\,

MMC C h n m l Sm.r

Scheduling Requirement of Real-Time Channel

A}

We first find the necessary capacity for the SMC
channel server. Since the SMC channel server serves only
one message stream, it is quite straightforward to find the
necessary capacity. A message stream of the SMC is
associated with three parameters; message size, Mk,
period, Tk,and deadline Dk. Then the capacity assignment,
which will be used in cyclic scheduling in the following
subsection, requires finding the channel capacity, P k ,

that

=z

w,cp*,,,

( M G v 3 + MMO + M

,=I

,)

Pk

1
(Pk,

.[$I

1

+

(MGup3 + MMO + M , )

Ph

+ MS/Z€ s r

and the channel cycle, /fk . The cyclic scheduler allocates

The expression Cyi
t) indicates the worst
cumulative message demand, including overheads on the
SPRETHER made by the message streams with a priority
higher than or equal to MS, during the interval [Qt]. We
now
define
Bi(Pk)=maxrEH,{r - Wi(Pk,t)}and

Pk portion

BO(Pk)

of total SPRETHER bandwidth at every

pkperiod. The following formula is used to obtain the
channel cycle and capacity for each SMC channel server.

,,,,

Bi(Pk). Note that, when MSi is

represents the total period in the
schedulable, Bi(Pk)
interval [0, D,]that the SPRETHER is without any
message streams with a priority higher than or equal to
that of MS,. It is equivalent to the level-i inactivity period
in the interval [0, D,][SI. By comparing the message
transmission at server sk and at a dedicated bandwidth of
Pk, we can obtain the following theorem:
Theorem 1 . The set of message streams, Ak, is
schedulable at server Sk that is with a channel cycle ,uk and
a channel capacity P , , if
a) Ak is schedulable at a dedicated bandwidth
utilization of
and
b, p k BO (Pk>/(I - Pk
Proof The message transmission at server S, can be
modeled by message streams MS,, MY2,...,MS,, of A P and
an extra message stream MSo that is invoked every period
7~and has message size MO=(l-pP)pk. The extra message
stream MSO is assigned with the highest priority and can

The overheads, such as SGapl, S(NIS), and SGap2
are excluded from the above formula because they are
dependent on cyclic scheduling. So they will be included
in the schedulability criteria for cyclic scheduling later.
Then we find the necessary capacity for the MMC
channel server. Since the MMC channel server schedules
local messages in a fixed-priority driven method, the
logical architecture of a channel server can be depicted as
in Figure 12. The channel server schedules the current set
of arrived messages and puts the highest priority message
into the top of the buffer. One of the important design
criteria is to select the fixed buffer size, MSIZE in Figure
12. With the scheduling analysis of message (channel)
requirements, we must decide on a practically feasible

Pk,

361

preempt other message streams. We need to show that,
given the two conditions, any message stream MS, of A k
can meet its deadline even if there are preemptions caused
by the invocations of message stream MSo.
To simplify the proof we introduce a new term, Vi=
MI + MGap3 + MMO. According to the schedulability
analysis in [6][7], message stream MS, is schedulable at
server s k if there is a t E H iUG j , such that

4.2.

Distance Constrained Cyclic Scheduling

By the schedulability requirement analysis, we can
obtain a feasible set of n channel servers, which have a
pair of channel capacity and cycle (PI, ,U)), ( P I , PI), ... ,
(Pn,,u,,)and the set be sorted in the non-decreasing order
of &,. The set cannot be directly used in a cyclic schedule
that guarantees, the distance constraint of assigning ,8k
bandwidth capacity for every ,uk period (distance) in a
channel. To satisfy the distance constraint between any
two consecutive channel capacity allocations, we can
adopt the pinwheel scheduling approach and transfer { p k }
into a harmonic set through a specialization operation [ 9 ] .
Note that, in [9], a fixed message size is allocated to each
channel and would not be reduced even if we transmit
message more frequently. This can lead to a lower
utilization after the specialization operations. In our
channel-scheduling solution, we allocate a certain
percentage of bandwidth capacity to each channel. When
the set of channel cycles { P k } is transformed into a
harmonic set { m k } ,this percentage doesn’t change. Thus,
we can schedule any feasible sets of (Ph ,ud as long as the
total sum of P k is less than 1.
Given a base channel cycle ,U, the algorithm finds a
mi for each ,U,that satisfies:
mi = p * 2J 5 pi < p * 2 = 2*mi,
To find the optimal base ,U in the sense of bandwidth
utilization, we can test all candidates ,U in the range of

where G, ={fqk 1 1 = 1 , 2 , . . . , 1 ~ , / q k j ) .
If MS, is schedulable on a bandwidth capacity of P k ,
there
exists
a
ti* E H I
such
that

B, (Pk = t,* - W,( P k ,t,*) 2 B, (bk 2 o for all i= I,2,.. ., p .
Note that K ( P k , tl) is a non-decreasing function of t,.
Assume that t , * = m , u k + d
where 6 < , u k . If 6 2 B O ( P k ) ,

J+’

(,u,/2, p l ] and compute the total capacity
The above inequality implies that all message streams MS,
are schedulable at server S k . On the other hand, if
6 < B, (pk), then, at ti = m,uk < t,* , we have

c k p : . To

obtain the total capacity, the set of pk is transferred to the
set of mk based on corresponding p and then the least
capacity requirement,
, for channel cycle i n k is
obtained from Theorem 1 for the MMC channel server. In
the case of the SMC channel server, P,” is the same as
P k . With the set of capacity assignments we have found
{ <P: > m1)>(P: > mz I,..., (P,“> mn )I > we can build a
distance-constrained cyclic schedule. In Figure 13, we
show an example of a cyclic schedule that guarantees
distance constraints for the set of channel capacity
requirements, A(0.1,12), B(0.2,14), C(0.1,21), D(0.2,25),
E(0.1,48), and F(0.3,50). We use the optimal base of 10 to
convert the channel cycles to 10, 10, 20, 20, 40, and 40,
respectively.

Since tl E ci, the set of message streams, A k is
schedulable at server s k .
When we compare the transmission sequences at
server s k and at the dedicated bandwidth, we can observe
that, at the end of each channel cycle, S k has put the same
amount of bandwidth capacity to transmit the message
streams as the dedicated network. However, if the
message streams are transmitted at the dedicated network,
they are not blocked and can be completed earlier within
each channel cycle. Thus, we need an additional
constraint to bound the delay of message transmission at
server S,. This bound is set by the second condition of the
Theorem and is equal to the minimum inactivity period
before each message’s deadline. +

Figure 13 Example of a Cyclic Schedule
The optimal ,U is selected in order to minimize the
total capacity, as shown in the left part of the equation
below. The exact schedulability criterion is as follows.

362

Total Capacity =

C ni * (Gap1+ S(NIS)mi+ ~ a p 2+) [/3:
n

* mi

1,

The measured data were used in Section 3 to
calculate the utilizations of SMC and MMC. A11
synchronization procedures have been implemented
within the SPRETHER Ethernet driver of Hard-Hat RealTime Linux. The synchronization overhead is composed
of the NIS packet preparation time by the master node,
packet transmission delay, and NIS packet processing
time at the slave nodes. The overall overhead to process a
NIS packet is approximately 2 19.04 ps.

i=l

ni = the number of fragmentation in the channel cycle,
Gap1 = SGap1 (SMC) or MGapl (MMC)
Gap2 = SGapZ (SMC) or MGap2 (MMC)

In the distance-constrained cyclic schedule, the
channel capacity can be allocated multiple time intervals
within a channel cycle. For an example from Figure 13,
channel F is allocated two time intervals, with 0.1 and
0.175 of the full bandwidth respectively. So the ni,which
corresponds to the channel F, is 2.

6. Conclusion

5. Prototype Platform

We proposed a real-time Ethernet, SPRETHER, for
safety-critical real-time systems. Since the SPRETHER is
implemented with a TDPA-based cyclic scheduling
protocol, it can be used in hard real-time systems such as
integrated
modular
avionics.
Non-real-time
communications can also be integrated as SPRETHER
allocates a CSMAKD Ethernet channel embedded in the
cyclic schedule. For further study, we are investigating
multi-segments Ethernet configuration to solve scalability
issue and dynamic real-time connection management. In
addition to replicating SPRETHERs for fault-tolerance,
we plan to look into the possible integration of
redundancy management and channel scheduling to
achieve maximal performance.

We have prototyped SPRETHER on a network of
Motorola MBX860 PowerPC-based embedded controllers
with the HardHat real-time Linux operating system, as
shown in Figure 14. The MBX860 equips a 40MHz
PowerPC core and 36 MB DRAM memory. 20MB of
DRAM is allocated for the ram-disk based root file
system. We use Red Hat Linux as a cross-development
environment. Real-time Linux on the MBX860 is booted
with TFTP. After booting, the cross-development
environment is disconnected from the Ethernet network,
except the passive Ethereal network protocol analyzer.
The experimental applications on real-time Linux are
initiated and monitored by a serial console attached to the
target MBX860 board.

7. Reference
“Backplane Data Bus,” ARINC Specification 659,
Aeronautical Radio Inc., Annapolis, MD, Dec. 1993.
A.R. Albrecht and P.A. Thaler., “Introduction to 100VGAnyLAN and the IEEE 802.12 local area network
standard, ‘‘ Hewlett-Packard Journal, 46(4), Aug. 1995.
Xiaonong Ran and W.R. Friedrich. , “Isochronous LAN
based full-motion video and image server-client system
with constant distortion adaptive DCT coding,”
Proceedings of the SPIE - The International Society for
Optical, pp. 1030-1041, 1993.
C. Venkatramani and T. Chiueh., “Design,
implementation and evaluation of a software-based realtime Ethernet protocol,”ACM SIGCOMM 95,1995.

Figure 14 SPRETHER Prototype Platform

Y.H. Lee, D.Kim, M. Younis, J. Zhou, and J. McElroy,
“Resource scheduling in dependable integrated modular
avionics,” Proc. of IEEE/IFIP International Conference
on Dependable Systems and Networks, pp. 14-23,2000.
J. Lehoczky, “Fixed priority scheduling for periodic task
sets with arbitrary deadlines,” Proc. IEEE Real-time
Systems Symposium, pp. 201-209, 1990.
J. Lehoczky, L. Sha, and Y . Ding, “The rate-monotonic
scheduling algorithm: exact characteristics and average
case behavior, ‘‘ Proc. IEEE Real-Time Systems
Symposium, pp. 166-1 71, 1989.
J. Lehoczky and S. Ramos-Thuel, ‘‘ An optimal algorithm
for scheduling soft-aperiodic tasks in fixed-priority
preemptive systems,” Proc. IEEE Real-Time Systems
Symposium, pp. 110-123, 1992.
C.C. Han, K.J. Lin, and C.J. Hou, “Distance-constrained
scheduling and its applications to real-time systems,”
IEEE Trans. on Computers, 45(7), 8 14-826, 1996.

Table 4 presents time’overheads to process a NIS
packet, which is essential to implement synchronization
for a software based TDPA protocol.

Table 4 NIS Packet Processing Overheads

interrupt)
HardHat Linux interrupt latency for Timer

I

1

NIS uacket transmission delav

17.6 p

I

86

11s

219.04 s

I

1
363

336

IEEE TRANSACTIONS ON COMPUTERS, VOL. 40, NO. 3, MARCH 1991

Proof: From Lemma 4,we have

\

\k=l

j=O

k=l

\

j:

1

\ j=o

From (27), d e t ( C f l k ’ B j 2 - j ) can be expressed as a product
of K small polynomials. The filter associated with it can be implemented on a K-stage cascaded structure. From Lemma 5, N(mk - 1)
multiplications are required in the kth stage. The total number of
N(mk - 1) = N(c:=, mk - I;).
multiplications is E:=’=,
When mk is not a prime number, the kth polynomial on the
right-hand side of (27) can be decomposed further. In the case
M =
pk where pk is prime, the total number of multiplications
is reduced to N (E:=’=,
pk - K ) . If M is a power of two, then this
expression simplifies to N log, M.

n:=’=,

C. Stability
We can rewrite (10) as

(E,”=,
wJ.z”-))
H(z)=

det (C,”r,’BJ.zM-’-J

det (19” - B”)

I.

[3] -,
“A stabilized parallel implementation of direct-form recursive
filters,” Rep. TR-CS-88-07, Comput. Sci. Lab., Australian National
Univ., May 1988.
[4] H. H. Lu, E. A. Lee, and D. G. Messerschmitt, “Fast recursive filtering
with multiple slow processing elements,” IEEE Trans. Circuits Syst.,
vol. CAS-32, pp. 1119-1129, Nov. 1985.
[5] A. L. Moyer, “An efficient parallel algorithm for digital IIR filters,” in
Proc. IEEE Znr. Conf Acoust., Speech, Signal Processing, Apr. 1976,
pp. 525-528.
[6] C.L. Nikias, “Fast block data processing via new IIR digital filter
structure,’’ IEEE Trans. Acousr., Speech, Signal Processing, vol. 32,
no. 4, Aug. 1984.
[7] K.K. Parhi and D.G. Messerschmitt, ‘‘Concurrent cellular VLSI
adaptive filter architectures,” IEEE Trans. Circuits Sysr., vol. 10,
pp. 1141-1151, Oct. 1987.
[8] -,
“Pipelined VLSI recursive filter architectures using scattered
look-ahead and decomposition,” in Proc. IEEE Znf. Conf Acousr.,
Speech, Signal Processing, Apr. 1988, pp. 2120-2123.
191 -,
“Pipeline interleaving and parallelism in recursive digital fdters, Part I: Pipelining using scattered look-ahead and decomposition,”
submitted for publication.
[lo] J. Zeman and A.G. Lindgren, “Fast digital filters with low roundoff noise,” IEEE Trans. Circuits Sysf., vol. CAS-28, pp. 716-723, July
1981.
[ll] B. B. Zhou, “Systolic architectures for parallel implementation of digital
filters,” Ph.D. dissertation, Australian National Univ., Sept. 1988.

(28)

Suppose that the original algorithm before the modification is
stable. Then the roots of det(1.z - B)are all in the unit circle. This
means that the eigenvalues ziof B are all in the unit circle. I,t is clear
that the eigenvalues zf” of B M are also in the unit circle and closer
to the origin than the corresponding zi.Thus, stability of the original
algorithm implies stability of our modified algorithm.

Optimal Scheduling of Signature
Analysis for VLSI Testing

IV. CONCLUSIONS
In this paper, we have introduced a new method of Z domain
derivation for obtaining parallel algorithms for direct-form recursive
filters. Using this method, parallel algorithms with guaranteed stability can be derived. Also, the additional complexity required for this
purpose can be reduced through a decomposition technique which was
originally introduced by Parhi and Messerschmitt [7]and extended
to more general cases for direct-form recursive filters in this paper.
Because of the regularity and modularity of the derived algorithm,
very efficient pipelined and/or parallel VLSI architectures can also
be constructed [2], [3], [8], [9].
We have recently learned that Parhi and Messerschmitt [8], [9] have
obtained a similar result using a different approach. The disadvantage
of their method is that the decomposition technique can be applied
only when M is a power of two. Our method has no such limitation.

VLSI circuits. We present a simple algorithm to optimally schedule the

ACKNOWLEDGMENT
We thank an anonymous referee for pointing out Moyer’s work
which is concemed with a parallel algorithm for first-order recursive
filters [5]. It can be seen that Moyer’s work is a special case of our
result.

REFERENCES
[l] C.W. Barnes and S. Shinnaka, “Block shift invariance and block
implementation of discrete-time filters,” IEEE Trans. Circuits Sysr.,
vol. CAS-27, pp. 667-672, Aug. 1980.
[2] R. P. Brent and B. B. Zhou, “A two-level pipelined implementation of
direct-form recursive filters,” Rep. TR-CS-88-06, Comput. Sci. Lab.,
Australian National Univ., Apr. 1988.

Y.-H. Lee and C.M. Krishna
Abshc -Signature analysis has become a popular way of

sting

signature analyses. The objective is to minimize the mean testing time
per VLSI circuit.
Index Tenns-Self-testing,

signature analysis, test optimization, VLSI

testing.

I. INTRODUCTION
VLSI circuits are tested by applying inputs to them, and comparing
the circuit output against that of a perfect circuit. Even moderately
sized circuits can require an enormous number of test inputs to ensure
sufficient coverage (i.e., the probability that a faulty unit is caught by
the testing), and so it is impractical to store the entire test output for
later comparison to the desired output. For this reason, compression
techniques are frequently used in practice [2], [5]. Tests are applied
to the circuit in quick succession, and the circuit output is stored in
compressed form. Once the test is over, the compressed output can be
examined for signs that the circuit is malfunctioning, by looking for
the “signature” of the faults that can occur. The process has therefore
got two phases: a test-application phase and a signature-analysis
phase.
Manuscript received March 14, 1988; revised February 27, 1989. The work
of C.M. Krishna was supported in part by the Office of Naval Research
under Grant “14-87-K-0796
and that of Y.-H. Lee by the Florida High
Technology and Industry Council.
Y.-H. Lee is with the Computer and Information Sciences Department,
University of Florida, Gainesville, FL 32611.
C. M. Krishna is with the Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA 01003.
IEEE Log Number 9040076.

0018-9340/91/0300-0336$01.00 0 1991 IEEE

337

IEEE TRANSACTIONS ON COMPUTERS, VOL. 40, NO. 3, MARCH 1991

If the total number of tests is large, it makes sense to break down
the application of the test to parts, checking the compressed output
at the end of each part for circuit malfunction. If the circuit is found
to be malfunctioning, one can dispense with the rest of the testing,
thus potentially saving some test time. In this paper, we present a
simple algorithm do this, i.e., to optimally interleave test-application
and result-checking phases. The objective is to minimize the mean
testing time per Vi31 circuit.
The literature on signature testing is very large. The concept was
introduced in [6]. Several authors have studied the use of multiple
signature analysis: for example, [3], [7],The focus of these works is
on the increased fault coverage or the reduced error escape probability
that multiple signatures provide over a single signature. In this paper,
on the other hand, our focus is on splitting up the test sequence in
such a way as to optimize the mean test time per circuit.
In the next section, we present a simple algorithm which shows
how to optimally schedule the test-application and the signatureanalysis phases. The section is largely self-contained so that readers
who are not interested in the mathematical underpinnings of the
algorithm, but simply want to consider using it, can stop there. In
Section 111, we present the mathematical proofs associated with the
algorithm.
11. THE ALGORITHM
We begin with specifying some notation. Let T be the total time
needed to exercise the chip with a given set of test patterns and
F ( t ) be the coverage after applying test pattern for period t. That
is, F ( t ) is the probability that application of t seconds' worth of
the test stream results in a faulty chip being uncovered as faulty.
Note that 0 5 F ( t ) 5 1 and F ( t ) is nondecreasing. We denote by
F-' the inverse function of F, i.e., F - ' ( F ( t ) ) = t. The chips to be
tested are assumed to be independent, identical, and with a constant
probability of being faulty which is denoted by w, where 0 < w < 1.
For simplicity, we denote w F ( t ) by @(t).
We assume that F ( t ) is a mixture of exponential distribution
functions. This type of coverage function is frequently used to
model random pattern generation and signature analysis. In [lo], an
argument is made for the use of the hyperexponential distribution,
which is a weighted sum of exponential distributions. Mixtures
of exponential distributions are both monotonically increasing and
concave. The concavity of F ( t ) expresses the fact that the rate at
which faults are uncovered per unit time of testing is a nonincreasing
function of how long the system has already been under test. If
aliasing did not occur, this would be exactly realistic because the
initial tests can catch the easy-to-detect faults.
Due to aliasing, however, these assumptions on F ( t ) are approximations. Work by Williams et al. [13] has shown that aliasing can
cause F ( t ) to be a damped oscillation. However, the oscillations die
away quickly, and F ( t ) is usually indeed a mixture of exponential
functions o f t for any but very small t. Our assumption, which is made
for analytical tractability, is therefore an acceptable approximation.
Suppose that an analysis scheme consists of N signature analy.
overhead
ses and each analysis has a constant overhead T ~ This
represents the time required to do the following:
Scan the compressed output, checking to see if a fault is
uncovered.
--If there is a fault, declare the chip faulty and stop.
Else,
-Restore the signature register to its previous value and resume
the testing process from where it was interrupted.
The maximum possible value of N that can be used, called N,,, is
generally determined by how much memory is available: after each
signature analysis, the compressed output has to be compared to the
output of a good circuit for that sequence of tests, and the latter has
to be stored in memory.
Denote the test-application time between the (i - 1)th and the ith
signature analyses as A,, where i = 1 , 2 , . . . ,N and
A, = T.
In addition, we define t, = E;=,A3.

Define the total test process time to be the sum of the time for
applying test patterns and the time spent for analysis. The test process
will be ended either by a fault being discovered, or the input test
stream being completely applied without any fault being discovered.
Thus, the total test process time is a random variable and depends
upon when, if ever, an erroneous signature occurs and is analyzed.
With a given N and t,, the average test time can be formulated as

xi"=;'
t, +
The denvation is quite simple, and is as follows. T + N r , is the
*(!*){t*+1

-

79).

time it would take to apply all the tests to the circuit, given that there
are N signature analyses interleaving the test stream. Subtracting
from this the term J ~ ( t 1 ., . , t N - 1 ) corrects for the fact that if a
fault is detected by a signature analysis before the entire test set has
been applied, there is no need to continue to apply further tests: the
circuit has been found to be faulty already.
The objective is to minimize S, the mean test time per circuit.
The algorithm is the following. Readers interested in the mathematical proof of correctness will find it in Section 111. We only present in
this section the case that rs # 0: rs = 0 corresponds to the unrealistic
case where the signature-analysis time is zero. For completeness, we
do consider this case in the mathematical derivations in Section 111,
and present the algorithm there.

The Algorithm
1) Set L = N,,, t ~ - 1= T. Compute t L - Z , t L - 3 , . .. ,tl iteratively ( t ~ - 2 2 t ~ - 32 . . . 2 t l ) using the implicit equation

+

F(t*-l) = F ( t * )- f ( t * ) [ t *-+t*
,

79).

Stop at z = L* for which the equation yields F ( t L * - 1 ) 5 0.
Set K' = L - L' - 1.
2) Define .J& = maxost,s T ~ _ , ~ T { J N ( ~ ~ , . . . , ~ToN - ~ ) } .
obtain J&, we need to determine the arguments t l , . . . , t N - 1
that maximize J N . This is done by expressing t l , . . . , tN--2 as
functions of tiv-1, and adjusting t,+' so that t o = 0. Use the
equation

+

F t.) - f(t.)[t,+l- t , rs)]
t,_' = ( "F- -' '( ( F ( t z ) - f(tt)[tz+1 - t l ) ]

if i > 1
if i = 1.

In other words, start with t~ = tlv-1 = T and keep decreasing
t ~ - 1and calculating t l , . . . ,t N - 2 using the above formula.
Keep decreasing t N - 1 until t o as given by the above expression
is zero, i.e., until

F(t1) - f ( t l ) { t 2 - t l } = 0.
These values of t , , 1 5 i 5 N , are those for which J N
is maximized. Apply a standard line-search algorithm, e.g.,
Dichotomous or Fibonacci search [l], over the range N E
(1,. .. ,K * } , to obtain the optimal N , called N', for which
J f i is maximized, i.e., 51;. = max{.J& IO < N 5 K * } .
Some numerical results are presented in Figs. 1-3 and Table I.
These results relate to the benchmark circuit C1355 of Brglex,
Pownall, and Hum [4]. This is a circuit of 546 gates and a total
of 1355 lines.
Using simulation, Huisman [8], [9] obtained the exposure probabilities of the 1574 possible faults in this circuit. Only single faults
were considered. The exposure probability of a fault is the probability
that a randomly chosen test pattern will uncover it. If the exposure

338

IEEE TRANSACTIONS ON COMPUTERS, VOL. 40, NO. 3, MARCH 1991

-1

61

1.1

I

1.1

1.2

@.a

I

1.4

I

I

I

1.6

1.6

1.7

I

1.8

1.9

1 .I

Fault P r o b a b i l i t y , w
Fig. 1. Optimal number of analyses.

probability of a fault f is zf,the coverage provided by a test of
length M is given by [8]

C f ( M )= 1 - (1 - "f)Y
Since 0 < zf 5 1 and the total number of possible tests is very
large, we can clearly write this as an exponential distribution:

for suitably defined Xf. If kf is the probability of the circuit being
faulty because of the existence of fault f, and F is the set of all
possible faults, the overall coverage function is then the mixture of
exponentials:

This relates the coverage to the number of test patterns applied. For
the circuit in this example, it is estimated-based on the computed
exposure probabilities-that a coverage of 0.9926 is possible with a
random sequence of 2000 tests. That is, if a coverage of 0.9926 were
judged sufficient, T = 2000.
Given that each test takes a certain amount of time to apply, we
can speak in terms of test time, rather than in terms of the number
of test patterns applied.
Fig. 1 shows how the optimal number of analyses depends on the
.
obvious
defect probability w and the signature analysis time T ~ For
reasons, the optimal number is a nondecreasing function of the defect
probability, and a nonincreasing function of the signature analysis
time. Fig. 2 shows the minimized average testing time against the
number of analyses. (For purposes of comparison, note that if
N = 1-the traditional case of applying the whole pattern and then
checking the signature-the test time would be T T~ = 2000 + T~
in this case.) As the number of analyses, N, increases, so does the

+

signature-analysis overhead. But the probability that a fault is caught
before all the tests have been applied tends to increase with N .
This would cause the test process for a given chip to be terminated
early. We therefore have a tradeoff here, and Fig. 2 illustrates it.
As the number of analyses increases from 1, the resulting increased
probability of an early test termination initially more than makes up
for the increased signature-analysis time. However, after a certain
point, the increase in the signature-analysis time is greater than the
expected decrease due to an early test termination, and any further
increase in the number of analyses increases the expected testing time.
Table I provides an example of the placements of the signature
analysis epochs. We can see from this that the interanalysis intervals
are very different in size: the earlier intervals are shorter than the
later ones. This can be explained from the fact that, under our
assumptions, the coverage function has a nonincreasing hazard rate.
Fig. 3 compares the average test time afforded by the optimal method
to the test time resulting from making the interanalysis intervals equal
in length. At N = 1, the average test times are equal by definition.
As N -+ 00 also, the test times are equal. The reason is that when
a fixed interval is divided into a very large number of subintervals,
the difference in subinterval length between the optimal placement
and the equidistant-placement strategies goes to zero. However, the
optimal placement method promises, as we can see, meaningful
reduction in the average testing time for the optimal number of
analyses, N * .
111. MATHEMATICALDETAILS
To minimize S, we need to determine the optimal dimension N
and the corresponding t , values. In the following, we shall divide the
optimization problem into two parts:
Define J~(t1,. t ~ =) E:;' @ ( t i ) ( t i + l - t i T ~ ) With
.
a
given N, solve the nonlinear programming problem PI which
a ,

+

339

IEEE TRANSACTIONS ON COMPUTERS, VOL. 40, NO. 3, MARCH 1991

T

T = 2000
w

-

"ax

0.5

= 40

Fig. 2. Minimized average testing time.

Proof: If no interior point of \ I I N satisfies VJ,(~'*) =
0, the optimal solution must be in the boundary. It means
that at least one inequality of the constraints in PI is active.
Let t,+l be one such inequality. Since F ( t ) is increasing,
it is easy to see that J N ( t 1 , . . . , t , ,
t 1 + 2 , . . .,t r v - 1 )
5
J N ( t 1 , . . . , t , , t z + z , t 1 + 3 , . . . , t N - 1 , T ) . Thus, there exists an integer
0 < k < N such that t: is equal to T for all k 5 i 5 N - 1.

is to find

,tN-1)
S ( N ) = T + N r s - J k . Solve the nonlinear programming
problem P2 which is to find
min

O<NS".ax

S(N).

The second result of the corollary follows because of the concavity
Q.E.D.
of E:;'
@(t)(t:+l- t: 7 9 ) .
The condition that V J N ( ~=) 0 can be rewritten as a set of
equations.

+

The above two nonlinear programming problems are difficult to
solve for an arbitrary coverage function. However, the process of
solving them is simplified if the coverage function is a mixture of
exponential functions with respect to the test period. As we have
explained in Section I1 above, this is a realistic assumption.
Let Q N = {(tl,tz,...,tN-l)I 0 5 ti 5 t z . . . 5 t r J - 1 5 T }
be the set of feasible solutions of problem P I .Also, let int(QN) be
the interior of Q N . It is clear that Q N is a nonempty convex set in
EN-^. In what follows, we denote the vector { t l , . . . ,t e } by 3 The
dimension of this yector will be obvious from the context.
For any vector t E Q N , we have the following theorem.
Th_eorem1: If F ( t ) is increasing and concave in 0 5 t 5 T, then
J N (t ) is concave at t' E int(QN).
Proof: Consider the function @ ( t , ) ( t l + l t , T ~ ) .Its Hessian
matrix is negative semidefinite for any 0 5 t , 5 t,+l 5 T when F ( t )
is increasing and concave. This implies that @(t,)(tt+l- t , T ~ is)
concave at any t' E Q N . The theorem follows since the summation
Q.E.D.
of concave functions is concave.
The next corollaries follow naturally due to the concavity of J N .
Corollary 1: If t" E int(QN) and vJ~(t'*)
= 0, then t'* is the
global optimal solution of the problem PI.
Corollary 2: If no interior point of Q N satisfies V J N(t'*)= 0,
then there exists an integer 0 < k < N which defines the global
solution of the problem Pl, f * ,such that t: = T for all k 5 i 5
N - 1 and ( t ; ,t ; , . . .,t i - l ) satisfies VJk (t'*) = 0.

F(t,-1) = F ( t , ) - f(t,)(t,+l- t ,

+

T ~ )

for 1 5 i

where f ( t ) is the first derivative of F ( t ) . The following lemmas
are needed to prove the existence and uniqueness of the solution of
V J N ( 2 ) = 0.
Lemma 1: If F ( t ) is a mixture of exponential distributed functions, then F ( t ) has a nonincreasing likelihood ratio [14]and

(3)

+

.
~

-1

(2)

+

.-

5N

Proof: It can be easily shown that logf(t) is convex and that
f'(t)/f(t) is nondecreasing. The above inequality is the result of
the fact that, if a l b 5 c l l d l and a l b 5 czldz, then alb 5
(c1
c ~ ) / ( d l dz). It is trivial to verify that F ( t , ) / f ( t , ) 2
f(tt)lf'(tz) and F(tt-l)lf(tl-l) 2 f(tz)/f'(tt) when F ( t ) is a
mixture of exponentials (f(t) = E:=, a p a t e - a s t ,where E, a t =
1).The result now follows after some minor algebraic manipulations.
Q.E.D.
Lemma 2: If t Z p 1satisfies the iterative equation F(t,-1) =
F ( t , ) - f(tz)(tz+l- t , + T ~ and
)
F ( t ) is a mixture of exponential

+

+

.
-

-1

340

IEEE TRANSACTIONS ON COMPUTERS,VOL.

40, NO. 3, MARCH

T = 2000
w = 0.5

,,N

= 40

E = Equal Placement
0 = Optimal Placement

Number

of

Analyses, N

Fig. 3. Comparison of checkpoint placement strategies.

distribution functions, then t , - 1 is increased and A, is decreased as
t N - 1 is increased.
Proofi Taking the derivative of F(t,-1) with respect to t N - 1 ,
we have

TABLE I

THE OFTMALSCHEDULES
OF SIGNATURE
ANALYSES

(4)

where f ( t )is, of course, the second derivative of F(t). When F ( t )
is a mixture of exponential distribution functions, from the above
lemma, we have
(a)

After substituting the above inequality into (4), we have, for 1
i 5 N-2,

T = 2000, w = 0.5 and r,

= 10.

5

and

Thus, we obtain, for all i,

Q.E.D.
Theorem 2: If r. = 0, then V J N(t') has a unique solution,
Z'* E h t ( @ N ) , which maximizes J N ( ~ Also,
) . S ( N ) is minimized
at N = Nmw.

(b) T = 2o00, w = 0.5 and r, = 40.

1991

341

IEEE TRANSACTIONS ON COMPUTERS, VOL. 40,NO. 3, MARCH 1991

’

Proof: Note that F ( t l ) / f ( t l )is increasing of tl when F ( t )
is increasing and concave. Let t“ = (t:,tL,..-,tk-l)
E QN
and satisfy the set of F(t,-1) = F ( t , ) - f(t,)(t,+l- t,), for
2 5 i
N - 1. One possible t“ is (T,T , . . .,T). By decreasing
th-l and solving F ( t : - , ) = F ( t , ) - f(ti)(ti+l- t i ) iteratively, for
2 5 i 5 N - 1, a new i‘ can be found in which ti is decreasing
and th - ti is increasing (from Lemma 2). A unique intersection
of F ( t : ) / f ( t i )and ti - ti exists as the result of decreasing th-l.
Thus, V J N(t ) = 0 has a unique solution, t“ E int Q N , which
maximizes JN (t’).The second part of the theorem follows from the
fact that JI; is increasing of N .
Q.E.D.
The intuitive explanation for this is clear: when it costs nothing
to do the signature analysis (which is what the T~ = 0 case
represents), it is optimal to do it the maximum number of times that
the memory constraint (expressed through Nm,) will allow. To obtain
the optimal times tl,t z , . .. ,tN,,,
simply use the equation resulting
from V J N (t’) = 0, namely f(t,-1) = F ( t , ) - f(tl)[tl+l- t , T ~ ]
setting t ~ , , = T.
Theorem 3: If T~ # 0, then there exist a finite K*, K * =
max15k5Nm{k
I t’* E ~k and v J k ( i * ) = 0). M S O , S ( N ) is
convex for 1 5 N 5 K’ and there is an integer N*, 1 5 N * 5 K * ,
for which S ( N * ) is minimized.
Proof? The existence of h’ is shown by the following: With
L = N,, and ~ L - I = T, t , can be calculated iteratively based on
the equation F ( t , - l ) = F ( t , ) - f ( t l ) ( t , + l- t , T ~ ) .The iterative
calculation of t , should stop at i = L* where F ( t , ) - f ( t z + l )
(t,+l - t , T ~ 5
) 0 or L’ = 1. From Lemma 2, it can be-shown
that for k 2 L - L*, no interior point of q k satisfies V J k (t ) = 0.
On the other hand, for k < L - L*, a solution of v J k ( t ‘ ) = 0 can
be found by continuously decreasing tLp1 and iteratively solving
F(t,-l) = F ( t , ) - f(t,)(t,+l- t , T ~ ) ,for L - k 1 5 i 5
L - 2 Such that F ( t L - k ) - f ( t L - k ) ( t L - k + l - t L - k 7 s ) = 0. K’
as defined as in the theorem is equal to L - L* - 1.
Next, we shall show that N* which minimizes S ( N ) is less than
or equal to K*. Note that, for all N > K*, JI; = J;(* (N K*)@(T]T,.
Since @ ( T )= w F ( T ) < 1, S(iV) is increasing for all
N > K .Thus, N* 5 K*.
To prove that S ( N ) is convex for 1 5 N 5 K * , let us define
G ( k ) = Jkf (K* - ~ ) @ ( T ) TNote
, . that G ( k ) = J ~ * ( i l k where
)
c k E @ K * , ( u k , l , U k , 2 , . . ’ ,U k , k - i ) is the Optimal SOlUtiOn Of J k (t‘)
and U k , , = T for all k 5 i < K * . Since J K * (t’) is concave on QK.,
J K * ( 2 ) is concave on the convex hull of { ill, Jz, . . . ,U ; } Thus,
G ( k ) is concave which implies S ( N ) is convex for 1 5 N 5 K * .

+

+

+

+

+

+

+

+

.

Q.E.D.
From Lemma 2, Theorem 2, and Theorem 3, we obtain the algorithm which minimizes the mean testing time: this has been described
in Section 11.

IV. DISCUSSION
We have presented a simple algorithm which minimizes the mean
testing time for VLSI circuits. By breaking up the testing process
into subintervals, and analyzing the signature at the end of each
subinterval, we can abort future tests if the circuit is found to be
faulty, thus saving test time.

REFERENCES

M. S . Bazaraa and C. M. Shetty, Non-Linear Programming: Theory and
Algorithms. New York: Wiley, 1979.
N. Benowitz et al., “An advanced fault isolation system for digital
logic,” IEEE Trans. Comput., vol. C-24, no. 5, pp. 489-497, May 1975.

,

D. K. Bhavsar and B. Krishnamurthy, “Can we eliminate fault escape
in self-testing by polynomial division,” in Proc. Int. Test Conf, 1984.
F. Brglex, P. Pownall, and R. Hum, “Accelerated ATPG and fault
grading via testability analysis,” in Proc. IEEE Int. Symp. Circuits Syst.,
1985.
R. David, “Feedback shift register testing,” in Dig. Papers, 8th Annu.
Int. Conf Fault Tolerant Compur., 1978.
R. A. Frohwerk, “Signatureanalysis: A new digital field service method,”
Hewlett-Packard J., May 1977.
S.Z. Hassan and E. J. McCluskey, “Increased fault coverage through
multiple signatures,” in Proc. FTCS-14, 1984.
L. M. Huisman, “The reliability of approximate testability measures,”
IEEE Design Test Comput., vol. 5, no. 6, Dec. 1988.
-, private communication.
E. J. McCluskey, S . Makar, S . Mourad, and K. D. Wagner, “Probability
models for pseudorandom test sequences,” in Proc. IEEE Int. Test Conf,
1987, pp. 471-479.
J. E. Smith, “Measures of the effectiveness of fault signature analysis,”
IEEE Trans. Comput., vol. C-29, no. 6, pp. 510-514, June 1980.
J. A. Waicukauski, V. P. Gupta, and S . T. Patel, “Diagnosis of BIST failures by PPSFP simulation,” in Proc. Inc. Test Conf., 1987, pp. 480-485.
T. W. Williams, W. Daehn, M. Gruetzner, and C. W. Starke, “Aliasing
errors in signature analysis registers,” IEEE Design Test, Apr. 1987.
S . M. Ross, Stochastic Processes. New York: Wiley, 1983.

Subcube Allocation in Hypercube Computers
Shantanu Dutt and John P. Hayes

Abshact-In hypercube computers that support a multiuser environment, it is important for the operating system to be able to allocate
subcubes of different dimensions. Previously proposed subcube allocation
schemes, such as the buddy strategy, can fragment the hypercube excessively. We present a precise characterization of the subcube allocation
problem and develop a general methodology to solve it. New subcube
allocation and coalescing algorithms are described that have the goal
of minimizing fragmentation. The concept of a maximal set of subcubes
( M S S ) , which is useful in making allocations that result in a tightly packed
hypercube, is introduced. The problems of allocating subcubes and of
forming an MSS are formulated as decision problems, and shown to
be NP-hard. We prove analytically that the buddy strategy is optimal
under restricted conditions, and then show using simulation that its
performance is actually poor under more realistic conditions. We suggest
a heuristic procedure for efficiently coalescing a released cube with the
existing free cubes. This coalescing approach is coupled with a simple
best-fit allocation scheme to form the basis of a class of MSS-based
strategies that give a substantial performance (hit ratio) improvement
over the buddy strategy. Finally, we present simulation results comparing
several different allocation and coalescing strategies, which show that our
MSS-based schemes provide a marked performance improvement over
previous techniques.
Index Term-Allocation algorithms, coalescing algorithms, hypercube
computers, hypercube m e n t a t i o n , multiprocessors, NP-complete problems, processor allocation, subcube packing.

ACKNOWLEDGMENT
The authors wish to record their appreciation to the referees for
pointing out some errors in the first draft, and augmenting the
references. In addition, Dr. L. Huisman of the IBM T.J. Watson
Research Center supplied us with test data which made our numerical
example possible.

Manuscript received March 29, 1988; revised July 1, 1989. This work was
supported by the Office of Naval Research under Contract N00014-85-K0531.
The authors are with the Advanced Computer Architecture Laboratory,
Department of Electrical Engineering and Computer Science, University of
Michigan, Ann Arbor, MI 48109.
IEEE Log Number 9040073.

0018-9340/91/0300-0341$01.00 0 1991 IEEE

Secure Localization and Location Verification
in Sensor Networks
Yann-Hang Lee1 , Vikram Phadke2 , Jin Wook Lee3 , and Amit Deshmukh4
1

3

Computer Science and Engineering Dept.,
Arizona State University, Tempe, AZ 85287, USA
2
Qualcomm USA, San Diego,CA 92121, USA
Samsung Advanced Institute of Technology(SAIT), South Korea
4
Siemens USA, San Diego, CA 92126, USA

Abstract. Evolving networks of wireless sensing nodes have many attractive
applications. Localization, that is to determine the locations of sensor nodes, is a
key enabler for many applications of sensor networks. As sensor networks move
closer to extensive deployments, security becomes a major concern, resulting in
a new demand for secure localization and location proving systems. In this paper, we propose a secure localization system based on a secure hop algorithm. A
location verification system that allows the base station to verify the locations of
sensor nodes is also presented. It prevents a node from falsifying reported location information. The evaluation of the secure localization system shows that the
proposed scheme performs well under a range of scenarios tested and is resilient
against an attacker creating incorrect location information in other nodes.

1 Introduction
Distributed sensor nodes can employ communications to form wireless ad hoc networks
capable of collaborative processing and collecting valuable information. The physical
location of these sensor nodes can prove to be useful in geographic routing, node authentication and location critical applications, such as target tracking. To determine the
locations of sensor nodes, either centralized approach [1] or distributed algorithms
[2] [3] have been proposed. Niculescu et al. [3] insisted that a positioning algorithm has
to be distributed because in a very large network of low memory and low bandwidth
nodes, even shuttling the entire topology to a server in a hop by hop manner would put
too high a strain on the nodes close to the observer(also called base station).
When sensor networks are deployed in remote and often hostile environments, security properties of these networks shall be critical. Attacks on such nodes can disrupt a
distributed location sensing algorithm resulting in nodes storing and propagating erroneous location information. Hence, it is imperative to look into secure localization for
sensor networks with the consideration of compromised nodes.
Most of the existing work in localization does not consider any security issues. To
best of our knowledge, there have been three works proposed for location verification


This work was supported by DARPA (Defense Advanced Research Projects Agency, USA)
IXO NEST (Network Embedded Software Technology) program.

X. Jia, J. Wu, and Y. He (Eds.): MSN 2005, LNCS 3794, pp. 278–287, 2005.
c Springer-Verlag Berlin Heidelberg 2005


Secure Localization and Location Verification in Sensor Networks

279

for wireless networks so far. Sastry et al. in [4] introduced the secure in-region verification problem to address false location claims. The disadvantage of this approach is that
it requires ultrasound and time-of-flight techniques and their focus is on one-hop verification with an echo back protocol. The authors of paper [5] designed a location proving
system that offers integrity and privacy. The round-trip latency of wireless communication between a node and location manager was chosen to determine the proximity of the
node to the location manager. This idea requires a quite precise time synchronization.
Recently, Capkun et al. in [6] proposed two mechanisms for the position verification,
based on multilateration and Time Difference Of Arrival(TDOA) techniques. They also
proposed mechanisms for verifying nodes’ mutual distances. However, the mechanisms
are based on GPS and radar technologies for all nodes which may not be available to
sensor nodes of limited resources.
In this paper, we show the use of one-way hash functions and existing trilateration
techniques to create a Secure Localization System (SLS). The proposed system is resilient against various types of attacks. We also propose a Location Verification System
(LVS) based on the secure localization system. Location verification is the process of
verifying location claims from sensor nodes. We believe that location verification has
an important role in various applications especially those, where resources are granted
or computation is performed based on location information.
In section 2, the threat model we attempt to address is provided. Sections 3 present
our Secure Localization System. Then we propose our Location Verification System in
section 4. Section 5 gives the results from simulation experiments. Section 6 presents
the conclusions drawn from this work.

2 System and Threat Models
The localization system focused in the paper is based on hop count and trilateration.
This is similar to the APS [3] where a distance vector exchange is taken place so that
all nodes get distances in hops to all landmarks. Hop count information gets propagated
from landmarks following the minimum hop spanning tree. The system is assumed with
the following characteristic:
– Individual nodes are not trusted: Nodes are subject to compromise from capture and
reprogramming attacks. We do not assume presence of tamper resistance hardware.
– Base station and landmarks are trustworthy: We assume that either some kind of
tamper resistance is provided or other methods are used to prevent base station and
landmarks against compromise.
– Landmarks have a priori knowledge of their own locations with respect to some
global coordinate system.
– Sensor nodes are not mobile during the localization phase.
– Radio links are vulnerable: we assume that these links are insecure. They are vulnerable to eavesdrop, replay and injection attacks.
Almost all of the distributed localization systems rely on intensive communication
and exchange of information between nodes. Below are a list of some generic attacks
for hop-based localization systems: systems.

280

Y.-H. Lee et al.

– Modification Attack: The most direct attack against a localization system is to target the localization information exchanged between nodes. Malicious nodes may
adversely affect the working of the system by falsifying/modifying hop count information.
– Replay Attack: In case of hop-based localization systems, hop count information
should be incremented to reflect the number of hops to a Landmark. A malicious
may just replay the hop count it received, thus create a false hop count information.
– Spoofing Attack: A malicious node may spoof localization information. In the example of hop based localization systems hop counts may be spoofed.
– Sinkhole Attack: In a sinkhole attack, the attacker’s goal is to attract nearby nodes
creating a metaphorical sinkhole. This can be achieved by advertising a low hop
count and infecting neighboring nodes. Although the source of the attack is a bad
node, the attacks gets propagated via good nodes who forward false localization
information.
– Impersonation Attack: Most distributed localization systems assume the presence
of some Landmarks. Impersonation attack involves malicious nodes impersonating/pretending to be these Landmarks. Attackers can propagate incorrect information while impersonating Landmarks and thus cause nodes to localize to wrong
locations.

3 Secure Localization System
In this section, we present a secure localization system based on hop by hop trilateration.
Using the proposed secure hop (SecHop) algorithm, every node can securely determine
the number of hops it is from each of the Landmarks. Similar to APS [3], every node
then estimate range between node and each Landmark, and then compute its location
by solving trilateration equations.
3.1 Algorithm
The SecHop localization algorithm has four phases of operations, which we now describe in detail.
Commitment Distribution - Phase I. Each Landmark will start with a seed s(0, 1)n
and compute a set of Links to form a one-way hash chain. n number of links are created,
where n is the estimated network diameter. In our notation scheme, the Links created by
Landmark i will be shown as below.
i
Link1i , Link2i , Link3i , . . . , Linkn−1
, Linkni

(1)

However a Landmark needs not store all these links. Of particular interest is Linkni
which is called ‘commitment ’ to the chain. At the start of the localization process each
Landmark will broadcast its nth link, i.e., Linkni . It is also possible to bootstrap the
commitment corresponding to the Landmarks into the nodes prior to deployment. This
mechanism prevents impersonation attack under which a malicious node may pretend
to be a Landmark.

Secure Localization and Location Verification in Sensor Networks

281

Algorithm 1. SecHop Localization
when a localization packet is received
hrecv = received hop count and Linkrecv = received Link
if new Landmark i then
if FnetDiam−hrecv (Linkrecv ) == Linkni then
store hop count hi = hrecv for this Landmark i
i
store Linkhi = Linkrecv and compute Linkh+1
broadcast new packet for this Landmark
else if lower hop count then
if Fhi −hrecv (Linkrecv ) == Linkhi i then
store hop count hi = hrecv for this Landmark i
i
store Linkhi = Linkrecv and compute Linkh+1
broadcast new packet for this Landmark
when a calibration packet received
if first packet then store avgDistPerHop
if calibrated and number of Landmarks ≥ dimensions of space then triangulate

Secure Hop By Hop Propagation - Phase II. In the classical distance vector exchange
approach to get distances in hops to all Landmarks, malicious nodes may spoof, alter
or replay distance vector updates resulting in corruption of distance vector at nodes,
and eventually causing them to localize incorrectly. We propose the use of SecHop algorithm (as illustrated in Algorithm 1.) based on one-way hash functions to prevent
against these attacks. The proposed SecHop algorithm is described below.
Each node maintains a table whose attributes are (Xi , Yi , hi ) and exchanges distance vector updates only with nodes it can communicate via a local broadcast. The
localization algorithm begins with each Landmark broadcasting its ID, hop count and
the first element of the hash chain. The Landmark initiates the localization process by
sending the following packet, called ‘positioning’ packet as shown below:
Landmark i → Broadcast(i, HC = 0, Link1i )

(2)

Essentially a Landmark broadcasts to the nodes around it that its hop count is 0 corresponding to itself, and it provides the Link corresponding to hop count 1, i.e., Link1i .
If a normal node gets the positioning packet, it checks whether it has already has such
a positioning packet corresponding to the same Landmark in question. If so, the hop
count is validated by checking the authenticity of the corresponding Link. If the hop
count and Link are verified and found to be authentic, the information corresponding to
the smallest hop count is retained and the other discarded. The hop count is incremented
by 1 while the Link of the chain corresponding to the next hop count is calculated and
is forwarded to the current node’s neighbors. If Link authentication fails, the hop count
and corresponding Link are discarded.
Correction Information Propagation - Phase III. This phase is similar to that proposed originally in APS [3]. We call ‘correction’ for the average distance travelled per
hop. At the end of SecHop phase, each Landmark gets all positioning packets from all
other Landmarks and knows the number of hops between itself and other Landmarks.

282

Y.-H. Lee et al.

With the known Landmark positions, it can compute the average distance per hop. All
Landmarks now broadcast the average hop distance (embedded in correction packet) to
the network.
Location Estimation - Phase IV. Once a node with unknown location gets has estimates to a number of (≥ 3) Landmarks, it can compute its own location using trilateration [3]. Each node uses the correction packet and uses that as the average distance
travelled per hop for calculating its distance to all Landmarks it is connected to. The
trilateration is performed using the Landmark coordinates and the corresponding distances to each of them from this node. This distance to each Landmark is calculated as
a product of the average hop distance of the nearest Landmark (in terms of number of
hops) and the number of hops to that Landmark.
3.2 Security Analysis
Comparing with APS, the Link information is added to the hop count broadcast packets
in SecHop localization. Thus, the hop count can be checked. In this subsection we
discuss security properties of the proposed secure localization system.
Landmark Impersonation Attack. In subsection 3.1, we mentioned the need for
countermeasures against Landmark impersonation attack. We propose the use of public
key cryptography for authentication to prevent against Landmark impersonation attack
wherein a node pretends to be a Landmark and floods fraudulent information into the
network causing nodes to localize incorrectly. Authentication for a node is done just
once in a network session so public key mechanism is not much overhead.
Modification Attack. Since we take a hop-distance vector based approach we can view
the hop by hop propagation phase as equivalent to advertising routes to Landmarks. In
SecHop algorithm, given that a malicious node receives a localization packet with a
metric of hop count h hops corresponding to Landmark i, three conditions as described
below hold.
– The malicious node can generate localization packets for h hops or longer routes
to the Landmark i. This is because a node is able to use a one-way hash function
to determine next Link of the one it received (forward Links of the one-way hash
chain). However, this behavior may not be very harmful in most cases because most
nodes will choose the lowest hop count available. Sending out the same Link that
it received is replay attack. The replay attack can also be viewed as a one node
wormhole. Although we do not propose any countermeasures against this attack,
systems such as TIK [7] can handle this attack.
– A malicious node cannot generate a Link less than hop count h for Landmark i.
– A malicious node cannot generate a Link for Landmark j, where i = j.
An adversary, that has not compromised any node and therefore does not possess
the crypto-keys, cannot produce any localization packets as it will be rejected by its
neighbors.

Secure Localization and Location Verification in Sensor Networks

283

Spoofing Attack. A node that has either been compromised or been introduced by
an adversary cannot spoof localization information because even though it can claim to
have an arbitrary hop count value, it will not be able to generate a Link corresponding to
the hop count. Thus, even if a good node that contained the commitment corresponding
to Landmark i is turned into a malicious node by an adversary, the adversary cannot
spoof a link from the commitment.

4 Location Verification System
Location verification is the process of verifying location claims from nodes in the base
station [4]. We present a scheme that shall allow a centralized entity like a base station
to be Verifier (V) and verify location claims from nodes, called Provers (P), in the network. We envision that the location verification scheme will be used in conjunction with
the secure localization system proposed in this work. It is possible however, in principle, to combine the proposed secure location verification system with other location
determination schemes.
4.1 Design
A location claim from a node will be of the form:
(node’s ID, Claimed Location, {Verification Tokens})
Verifier V returns ‘success’ if can verify the authenticity of the claim else it returns
‘failure’˙ The crux of our scheme is that each Prover P needs to prove to the Verifier that
the hop count h it claims to have corresponding to each Landmark i is indeed the actual
hoop count and the hop count has not been spoofed or altered. Each node has a hashed
Link for every reachable Landmark. It can use the Links to prove the authenticity of the
its hop count claim. For P to prove that it has hop count h corresponding to Landmark
i, it should provide the V with Linkhi .
In order to verify location claims we require that nodes not be able to modify the
Links, i.e., given a Linkhi that corresponds to hop count h the node should not be able
i
to compute forward Linkh+1 or a backward Linkh−1
. A malicious node cannot create
a backward Link because of the one-wayness of hash functions, however a malicious
node that wants to claim that it is in a different location from its actual location can
create forward Links by using the one-way hash function thus pretending to have a
higher hop count than it actually has and eventually fooling the verifier into thinking
that the prover is in a different location than it actually is.
We propose some modifications to the secure localization system in order to facilitate
the verification process. The aim is two-fold.
1. Even though it is difficult to prevent a malicious node from generating a higher hop
count and corresponding forward Link we propose a scheme to prevent the node
from being able to report this higher hop count to the verifier V.
2. Prevent framing or blackmail attacks. These are attacks where in a malicious node
successfully frames another node. In the context of location verification it could
mean causing the legitimate location claim from a good node to be rejected by the
verifier.

284

Y.-H. Lee et al.

Neighbor Authentication and Non-repudiation. Each node maintains a one-way
hash chain for every reachable Landmark in the system. The chain is used to authenticate the source of Link that a node broadcast.
For notational convenience we represent the one-way hash chain element as ALink
(being different from the Link chain is the SecHop algorithm). For example, the ALink
chain corresponding to Landmark 3 for node 45 is shown below.
3
3
3
3
3
ALink1,45
, ALink2,45
, ALink3,45
, . . . , ALinkn−1,45
, ALinkn,45

We assume that the n th ALink values or commitments corresponding to every node are
known to all neighboring nodes and the Verifier V. When a node propagates hop count
information corresponding to a certain Landmark it also attaches a ALink from its oneway hash chain corresponding to the Landmark. Every node reveals an ALink from the
end towards beginning, i.e., in a direction opposite to that of computation. For example
3
, to authenticate hop count n-1 corresponding to Landmark
node 45 reveals Linkn−1,45
i
3. In general to authenticate a hop count h, for Landmark i, node N reveals Linkn−h,N
.
A neighboring node that receives a hop count and an ALink from a neighboring node
can authenticate the hop count using the commitment information corresponding to the
neighboring node. The added hash chains at each node provide the following security
features.
– Authentication: The one-way hash chain at every node allows a node to authenticate localization information from neighboring nodes.
– Non-Repudiation: At each node commitments corresponding to each Landmark
are revealed to the verifier and the neighboring nodes. This provides for non repudiation and prevents a malicious node from framing other nodes.
During location verification a prover node P sends a verification token that has the
i
where i indicates the Landmark. ‘N ’ is some
following format, Linkhi , ALinkn−h,N
neighbor of P and is the source of both links and P = N .
4.2 Security Analysis
Our main aim in the location verification scheme was to prevent an attacker(malicious
node M) from claiming a higher or lower hop count h´ than its actual hop count h. We
explain these concepts below.
– Link Alteration-Backward Links: Consider a malicious node M that is the Prover
P. If the actual hop count of M is h, the lowest hop count that it can claim is h.
This is because this node will receive a Linkh from one of the neighboring nodes.
Due to the one-way property of the one-way hash chain it cannot derive the link
corresponding to hop count h-1, i.e., Linkh−1 .
– Link Alteration - Forward Links: If we only use one-way hash chains corresponding to Landmarks the M will be able to claim a hop count (h + ∆), because it is
able to calculate forward links. However in our location verification scheme the
i
. To fake a higher
verification token also contains a link of the form ALinkn−h,N
i
hop count of h, M is expected to send ALinkn−(h+∆),N (M = N ). Given that M
i
will have received a link of the form ALinkn−h,N
corresponding to hop count h,
i
it is infeasible to compute ALinkn−(h+∆),N .

Secure Localization and Location Verification in Sensor Networks

285

– Link Spoofing: A malicious node does not know the seed of any other node in the
network and so it cannot spoof a Link or an ALink.
– Framing: A malicious node M may still attempt to frame other nodes. It may send
a garbage link and node P will report a corrupt link and its claim may be rejected
by V. This attack is thwarted because one-way hash chains at each node facilitate
neighbor authentication and non repudiation.

5 Simulation Results
We chose the NS-2 simulator to evaluate the performance of SLS and to realistically
model physical radio propagation effects such as signal strength, and interference capture effect. We also used an implementation of the S-MAC protocol, which has been
specifically designed for sensor networks. The one-way hash function being used in
our simulations is MD5. We generate random topologies of 100 nodes and place these
nodes in a 400m by 400m square grid with uniform distribution. The radius of radio
range is assumed to be 70m. We place 3 Landmarks such that they are on the edges
of the network topology. This is because distributed localization algorithms generally
perform better when the Landmarks surround most of the nodes.
5.1 Location Error
We define ‘Location Error’ as the difference between the actual location of the node
and the calculated location (as determined by the localization algorithm). The average
location error is the location error averaged over all nodes. Figure 1 shows the location
error in percentage of the radio range for different scenarios:
– Hop-Based-No Attacks: There are no attacks to the localization system. This scenario is simulated to determine a baseline location error so that we can evaluate
other attack scenarios against it.
– Hop-Based-Sink-Hole: In this scenario the malicious nodes in the network attack
the localization system by broadcasting a very low hop count, i.e., 1. to infect as
many nodes as possible.

Fig. 1. Location Error

Fig. 2. Node Infection

286

Y.-H. Lee et al.

– SLS-Sink Hole: This represents the case in which the proposed SLS is in use and
the localization system is under sink hole attack.
– SLS-Replay: This scenario represents the case of replay attack or one node wormhole attack on SLS.
– SLS-Fwd: The malicious nodes attempt to thwart SLS using a forward attack. Forward attack is most effective in C-shaped networks, i.e., anisotropic topology.
As can be seen in the Figure 1, SLS performs much better in the face of different attacks
on the hop propagation based localization systems. This is because the one-way hash
chain of Links allows a node to verify the authenticity of the hop counts in localization
packets that it receives. As can be seen from SLS-Replay and SLS-Fwd curves, even
more subtle attacks like the replay attacks do not have a major impact on the location
error.
5.2 Node Infection
With respect to the hop by hop propagation based localization systems, an infected
node is defined as one that includes a malicious node as part of its route path to any
Landmark. In Figure 2, we show the the percentage of infected nodes in several attack
scenarios similar to the ones described in the context of the location error. The results
show again that the SLS is SLS robust against Alteration/Modification/Spoofing attacks
as well as Sink Hole attacks etc. As can be seen from the SLS-Replay curve more
subtle attacks like the replay attacks result in more nodes being negatively impacted by
malicious nodes. However as the location error curves in Figure 2 clearly illustrates the
impact in terms of location error is rather limited.
5.3 Network Diameter Estimation
The network diameter is estimated so that a commitment can be generated at every
Landmark which then forms the basis of SLS. In this experiment, we study the effect of
the difference between estimated network diameter, nest , and actual network diameter,

Fig. 3. Effective Link Verification

Fig. 4. Computational Overhead

Secure Localization and Location Verification in Sensor Networks

287

nact , on proposed solution. Results are shown in Figure 3. The X axis represents nest
as a fraction of nact . The Y axis represents the percentage of feasible verification operations. The results show that successful verification can be more than 80% when the
estimated diameter is 0.8 of the real network diameter.
5.4 Computational Overhead
The overhead resulting from the computation of Link Authentication is shown in Figure 4 where the average number of hash function operations per node corresponding to
three Landmarks is plotted. Two scenario are included in the figure. The dotted curves
represents that in every Link is authenticated by iterative application of hash function
to obtain Link n and then matching it against the Commitment. The solid curve represents the results from a simple yet efficient optimization in which the first Link a node
receives is authenticated with the same approach. However for subsequent verifications
of any link Link j where j < i iterative application of hash function to obtain Link i is
sufficient. This results in reduction of computational overhead.

6 Conclusion
This work presents the SLS as a secure distance vector based localization system which
is robust against multiple uncoordinated attacks of compromised and malicious nodes.
SLS is based on a computational security model and relies on one-way hash functions.
A simulation-based evaluation and a security analysis are included in the paper. In addition, we explain the location verification problem in the context of sensor networks and
describe how SLS can be extended to facilitate location verification. This work does
not take into consideration the mobility of sensor nodes nor possible collusion attacks.
Such scenarios will entail an extension of the proposed secure localization system to
address the variation in operational parameters that result form the mobility of nodes
and to eliminate falsified information from propagation through the networks.

References
1. L. Doherty, K. S. J. Pister, L. E. Ghaoui, Convex Position Estimation in Wireless Sensor Networks, Proceedings of IEEE infocom 2001. pp. 1655-1663.
2. C. Savarese, K. Langendoen, J. Rabaey: Robust Positioning Algorithms for Distributed AdHooc Wireless Sensor Networks, USENIX Tech. Annual Conference, 2002. pp. 317-328.
3. D. Niculescu, B. Nath, Ad Hoc Positioning System(APS), IEEE GlobeCom 2001. pp. 29262931.
4. N. Sastry, U. Shankar, D. Wagner, Secure Verification of Location Claims, Proceedings of the
2003 ACM workshop on Wireless Security, 2003. pp. 1-10.
5. B. Waters and E. Felten, Secure, Private Proofs of Location, Technical report TR-667-03,
Princeton University, 2003.
6. S. Capkun and J. P. Hubaux, Securing position and distance verification in wireless networks,
Technical report EPFL/IC/200443, May 2004.
7. A. Perrig, Y. C. Hu and D. B. Johnson, Wormhole Protection in Wireless Ad Hoc Networks,
Technical Report TR-01-384, Department of Computer Science, Rice University, 2001.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. 14, NO. 9, SEPTEMBER 1988

1307

Dynamic Transaction Routing in Distributed Database
Systems

Abstract-In this paper, we investigate dynamic transaction routing
strategies for locally distributed database systems in which the database is partitioned and distributed among multiple transaction processing systems, and the incoming transactions are routed by a common front-end processor. In this environment, if a transaction issues
a database request referencing a nonlocal database partition, the request has to be shipped to the system owning the referenced partition
for processing. Various dynamic strategies are studied. Their performance is compared with that of the optimal static strategy. A new class
of dynamic transaction routing strategies which take into account routing history and minimize the estimated response time of incoming
transactions is proposed and found to provide a substantial improvement over the optimal static strategy. The robustness of the strategies
is further studied through sensitivity analysis over various transaction
loads, communication overheads and database reference distributions.
Index Terms-Distributed database systems, load balancing, performance analysis, queueing models, transaction routing strategy.

I. INTRODUCTION
HEN processing power is distributed over multiple
computer systems, load sharing is critical in achieving higher aggregate throughput and better response time.
To attain appropriate sharing, arriving tasks are allocated
according to some strategy. Strategies are static in nature
if allocation decisions are based solely on the static characteristics of arriving tasks and the processing systems.
Other strategies, in which allocation decisions depend
upon not only the static characteristics but also the current
system state, are referred to as dynamic.
Numerous load sharing strategies, both static and dynamic, have been studied for different types of distributed
systems. Queueing network analysis, mathematical programming, and other techniques can be used to obtain
performance estimates and then to derive optimal static
strategies; for example, the optimal deterministic allocation of tasks in [16], [17], and optimal probabilistic assignments in [12], [18]. On the other hand, certain optimalities in the dynamic approach have also been
discovered. For instance, the send-to-shortest queue strategy is found to be the best for the case of Poisson arrivals
and identical exponential servers [20] and the round-robin
Manuscript received March 17, 1986; revised March 24, 1987.
P. S . Yu is with IBM Thomas J . Watson Research Center, Yorktown
Heights, NY 10598.
S. Balsam0 is with the Dipartimento di Informatica, University of Pisa,
Pisa, Italy.
Y . H. Lee is with the Department of Computer and Information Sciences, University of Florida, Gainesville, FL 3261 1.
IEEE Log Number 8822454.

strategy becomes optimal when the queue length at each
server cannot be observed, provided that all servers have
the same initial state [IO]. A survey on load sharing strategies in distributed systems can be found in [ 191. For other
more complex cases, like heterogeneous servers, multiple
classes of tasks and different arrival and service time distributions, simulations have been adopted to study the
performance of various strategies [I], [4], [61. In [SI, [91,
it has been demonstrated that a simple probe limit scheme
based on queue length threshold is quite effective to improve performance.
In these previous studies, it is assumed that incoming
tasks can be serviced completely at any processing system. This implies that either all tasks are purely computational or requested resources, e.g. database or files, are
shared or replicated among all processing systems. Now,
consider a locally distributed database environment’ as
shown in Fig. 1. The database is partitioned among the
various processing systems, and the arriving transactions
are routed to one of the processing systems by a common
front-end system. If a transaction issues a database request referencing a remote database partition, the request
has to be shipped to the system owning the referenced
partition for processing. This is referred to as a remote
database request. Thus, a new dimension, the reference
pattern or the reference locality, has to be considered in
load sharing strategies.
The reference pattern of database requests not only
causes frequent load fluctuation but also complicates routing decision making. When the database is partitioned,
the processing associated with each transaction can be divided into two categories. The first category denoted as
routing dependent processing, is to be executed at the processing system to which a transaction is routed. The application process belongs to this category. The other is
partition dependent processing, which is a service request
against a particular database partition, e.g., the database
requests, and can only be executed at the processing system owning the partition. To balance the loads among
processing systems, only the routing dependent processing can be used as leverage. In addition, different transaction routing schemes affect the number of remote database requests and, thus, incur different communication
loads which are critical to performance. This is in sharp
‘By locally we mean so close together that communication delay is negligible, e.g., the entire system is located in the same machine room.

0098-5589/88/0900-1307$01.OO 0 1988 IEEE

1308

lEEE TRANSACTIONS ON SOFTWARE ENGlNEERlNG. VOL. 14, NO. 9, SEPTEMBER 1988

Fig. 1 . The configuration of a locally distributed database system.

contrast with the case of homogeneous system without remote requests between processing systems.
In this paper, different dynamic strategies for transaction routing in a locally distributed database environment
are studied. The major concerns of designing a dynamic
strategy are 1) what information is crucial to decision
making and what is the overhead of collecting the information, and 2) how to use the available information to
make the routing decisions. We propose a class of dynamic strategies which can provide superior performance
as compared to the optimal static routing scheme, yet require little effort to maintain the dynamic information.
These strategies are based on routing history of currently
active transactions along with transaction characteristics,
and attempt to minimize the estimated response time of
an incoming transaction. Also considered is a class of
strategies based on instantaneous queue length information. Well known strategies such as joining the shortest
queue belong to this class. This class of strategies can be
costly to implement as frequent message exchanges are
required to update the queue length information.
In the next section, the models of locally distributed
database environment and transactions are described. In
Section 111, various transaction routing strategies are discussed. Section IV presents simulation results on response time under different routing strategies. Detailed
sensitivity analyses are also provided. We summarize the
results in Section V.
11. MODELDESCRIPTION
We consider a locally distributed transaction processing
system as shown in Fig. 1. The system consists of N
transaction processing systems and a front-end system,
connected by an interconnection network. The transaction
processing systems execute transaction application processes and handle database requests. The whole database
is partitioned into N databases which are denoted as DB,,
. . . , DB,, where DB, is attached to the transaction processing system PI.All database requests to DB, are as-

sumed to be handled by the processing system p , . The
processor speeds and the I/O access speeds at different
processing systems are assumed to be identical.
Transactions submitted by users enter the system
through the front-end system where transactions get formatted and are routed to one of the processing systems.
After a transaction processing is completed, output messages will be mapped into the user's screen format and
delivered back to the user via the front-end system. A load
sharing strategy is employed at the front-end system to
determine the assignment of an incoming transaction to a
processing system.
At the assigned processing system, a transaction invokes an application process which may issue a number
of database requests. The application process of a transaction will be executed completely at the assigned processing system, whereas database requests will be executed at the processing systems owning the database
partitions. During the execution of database request, U0
device will be accessed if the required data is not in the
main memory of the processing system. The flow of transaction processing is shown in Fig. 2, where a transaction
will be completed after several iterations of application
processing segments and database requests. Transactions,
then, can be characterized into different classes by 1) the
processing service demand of each application processing
segment, 2) the number and reference distribution of database requests, and 3) the processing and I/O service demands of each database request. For simplicity, we assume that these service demands are exponentially
distributed. Also, at the end of each application processing segment, fixed probabilities of issuing a database request to a particular database or terminating the transaction processing are assumed for each class.
Based on the sequence of transaction processing, we
construct the model of transaction processing as shown in
Fig. 3. Let there be K transaction classes in the system
and let txk denote a class k transaction, k = 1,
* , K.
For the kth class, transactions arrive according to a timeinvariant Poisson process with rate hk. The mean processing service demands of an application processing segment and a database request of txk are ak and b k , respectively. Both ak and bk can be estimated by measuring the
pathlengths of application processing and database request. For each database requests issued by t&, we assume that, an I/O device will be accessed with a fixed
probability pi', and the service time of each I/O access is
exponentially distributed with mean d k . When the execution of an application processing segment is completed,
transaction txk may issue a database request to database
DB, with probability Pk, or may terminate with probability P k O . The Pk, is referred to as the database request probability of transaction k to DB,. Thus, the total processing
load of incoming transactions per unit of time to the whole
system becomes

-

K .

YU et

U/.:

1309

DYNAMIC TRANSACTION ROUTING
applleation
proceasing

Input
formatting

appilcation
procnalng

opplicotlon
procesaing

datobora
roqueat
to DBI
(wlth IO aceera)

output
for mottlng

dotobase
requeat
to DBI
(without IO access)

Fig. 2 . The sequence of transaction processing.

0: opplicotion

process

I: locol dotobose requests
ov: communication overhead

P1 - DE1

r: receiving service and
remote database requests

application
processing

f

1 - formatting

I

~T
.................

submodel for
database requests (I k r)

Fig. 4. Model for locally distributed database system.

Fig. 3 . Model of transaction processing.

Among this total processing load, there is a portion associated with the processing of database requests. We denote the processing load of database requests per unit time
at Pias follows:

Notice that S and Sp only depend upon the characteristics of transactions and are independent of the transaction
routing decisions. When a database request is issued, it
must be shipped from processing system PI to PJ if a transaction being executed at PI issues a database request to
OBJ,where i # j . After the request gets processed, the
result will be sent back. Both P, and PJ have to perform
sending and receiving services. The service demands of
sending and receiving a database request or the results of
a request are referred to as communication overhead and
are assumed to be exponentially distributed with mean c.
The system model is illustrated in Fig. 4. A single
server processor sharing queue is used to model the processor at each processing system. On the other hand, the
I/O subsystem of each processing system is modeled as
an infinite server queue. This is to correspond a global or
aggregate representation of a more complex U 0 subsystem. Note that this choice is mainly to simplify the model.
Extensions to capture I/O contention can be done by explicitly considering data allocations and multiple disk
servers.

In the model, the transmission delay of shipping database requests in the network is assumed negligible. This
assumption while reasonable in a locally distributed system, would not be in a geographically distributed system.
The other simplification in the model is that the overhead
or the delay due to data conflict and two-phase commitment is not included. It would be otherwise necessary to
define a more complicate model than the one presented
here. In addition, as shown in [21], the routing decisions
to minimize transaction response times under the optimal
static routing strategy are not affected whether lock contention is considered.
111. DYNAMIC
TRANSACTION
ROUTINGSTRATEGIES
We now consider different dynamic transaction routing
strategies which can be employed at the front-end system.
As pointed out before, the two major concerns are what
information to maintain and how to make a routing decision based on the available information. To decide on the
dynamic information, we need to understand which information can be easily maintained at the front-end, and yet
provide valuable insight for making the routing decisions.
One class of strategies proposed are based on routing history of active transactions along with some static profile
on transaction characteristics. Here, active transactions
denote the transactions currently under (or waiting) execution in the processing system. This class of strategies
incurs little overhead, as routing history can be maintained easily in the front-end system. The essential issue
is to estimate load conditions of the processing systems
and then make a good routing decision based on this estimate. Different alternatives have been explored, and

1310

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. 14, NO. 9. SEPTEMBER 1988

some of them lead to very robust performance as shown
in the next section.
The other class of strategies considered is based on instantaneous queue length information of each processing
system. The queue length of PI is referred to the number
of tasks being executed at processing system P I , where a
task is either application processing, database request
processing or communication overhead * The instantaneous queue lengths are costly to collect, as frequent message exchanges between the front-end system and the processing systems would be required. Surprisingly, we find
that even ignoring the overhead of collecting the state information, the latter class of strategies leads to inferior
performance compared to the former.

. . *
, L N ) .Note that although quite a few parameters appears in the above formulas, they are derived from either
system parameters ( c and p ) , or workload parameters ( a k ,
b k , d k , pio, and pkJ) which are provided from the static
transaction profile. We now present three different ways
to estimate L, based on routing history.
I ) Service Time Based Strategy: Consider the moment
that a new transaction arrives. Let mkl be the number of
class k transactions assigned to the processing system P I ,
and not yet completed, i.e., [ m k l ]constructs the routing
history table. Let probk( i , j ) denote the probability that
a class k transaction assigned to processing system P, is
waiting for or receiving processing service at P, . Hence,
we can express the expected queue length LJ of P, as

N

A . Strategies Based on Routing Histoly

Consider a set of dynamic strategies where the routing
decision process is based on routing history of active
transactions along with transaction characteristics. These
strategies, first, estimate the response time of an incoming
transaction given that the transaction is routed to the processing system p,. Then the incoming transaction is routed
to the processing system PI which provides the minimum
expected response time.
The routing history of active transactions is maintained
by the front-end system in a routing history table. Each
time a transaction txk is routed to P I , the entry in the kth
row and the ith column of the routing history table is incremented by one to reflect the new arrival and its routing.
Furthermore, when a transaction is completed and an output message is returned to the user through the front-end,
the entry in the corresponding row and column of the table
is decremented by one to reflect the departure. Note that
there is a negligible overhead to maintain the table in the
front-end system and requires no communications of instantaneous state information from the processing systems.
Next the issue of estimating the response time is considered. The expected response time of an incoming
transaction depends upon the transient behavior of the
system and the future arrivals. For efficient implementation at the front-end, a steady-state analysis is applied to
estimate the mean response time using transaction characteristics and mean queue length at each processing system. From the derivation in the Appendix, the expected
response time for a transaction txk routed to PI is given as

(3.2)
for; = 1, . . . , N . Assuming that the fraction of time
that a type k transaction is either 1) waiting or receiving
processing service at Pi,or 2) receiving I/O service, is
proportional to the corresponding service time, one can
approximate the unknown probabilities in (3.2) as follows

where the normalizing constant Ckiis given by
-

(3.1)
where Li is the mean queue length of Pi and L

=

(Ll,

'Note that the number of tasks receiving IO services at system i is not
included in the queue length.

/

N

N

\

Notice that the probabilities probk ( i , j ) are normalized
by assuming that the transactions assigned to Pi receive
either processing or I/O service.
Hence, given
= (mki:k = 1 , * . . , K ; i = 1 ,
. . . , N ) and the approximate probabilities probk( i, j )
based on the service time proportions, one c a 3 derive estimates from (3.2) of the mean queue length L . Each incoming transactionds then routed to the processing system
Pi such that Rki(L ) = min, 5 j 5 { Rkj( L ) }. In summary, this strategy uses service times to estimate mean
queue length and selects the processing system which provides minimum response time to route an incoming transaction. We shall call it MRT.ST strategy. The information needed in MRT. ST includes the routing history table
m and a static profile on transaction characteristics { a k ,
bk, c , P k O , Pki,
2) Residence Time Based Strategy: The residence time
based strategy, which will be called MRT.RT, is different
from MRT.ST only in the computation of probabilities
p r o b k ( i , j ). With MRT.RT strategy, the probability
prob,(i, j ) is proportional to the residence time at the
1

j#i

K

1311

YU et al.: DYNAMIC TRANSACTION ROUTING

processing system P J , instead of the service time. An iterative approach based on the MVA equations [ 131 is applied in order to d e r k e the probability probk(i, j ) and the
mean queue length L .
The residence time Tk( i , j ) in PJ of a transaction of
class k initially assigned to PI can be written as

Tk(i,j )

= &(i,j ) ( N k ( i , j )

+

=

(bk

Uk

+ c)pkj(Lj - p r & ( i , j ) +

+ bkpk, + C

J =

I

pkJ

+

(3.7)
On the other hand each processor utilization is given by
K

+

1
1) Pckr

I+'

( L i - probk(i, i)

9

(3.4)

1)

where Nk (i, j ) is the mean number of tasks that a type k
transaction assigned to P, finds in PJ , and Sk( i, j ) is its
total service time at PJ . Values of Sk ( i , j ) can be simply
derived from the service times expressed in (a.3) of the
Appendix. Nk (i, j ) can be approximated by N k ( i , j ) =
LJ - probk(i, j ) which is similar to the well known BardShweitzer's algorithm [2], [15]. Thus, the probability
probk(i, j ) which is proportional to the residence time
Tk( i , j ) is given as
probk(i,j)

for i = 1 ,
, N . Combining (3.1) and (3.6), the expected response time, for a class k transaction assigned to
Pi, can be written as

I) 1

(3.5)

P cki

.

r

,'

J =

I pki(bk

N

r

j# i

+ ')PkJ]'

ak

+ bkpk, -k C

I[

( L I - probk(i, i)

Note that the only unknown quantity in (3.8) is the probability distribution pkiof routing an arriving transaction of
type k to processing element Pi. Since, in such a model,
the routing probability distribution corresponds to fractions of transactions assigned to each P i , one can introduce the following approximation

l N

+-

pJ=l

(bk

+ c ) p k j ( ~ -j p r o b k ( i , j ) +

1)

J f l

N

+ Pl'dk

e

j=1

Pkj.

Hence, given a system state 6 ,the mean queue lengths
are computed by iterating (3.5) and (3.2), starting with
zero values for both queue lengths LJ and probabilities
p r o b k ( i , j ) ,f o r k = 1 , *
, K, i , j = 1 , * . , N . The
MRT.RT strategy uses the same static and state-dependent information as the MRT.ST strategy. The iteration
between (3.5) and (3.2) should not impose any substantial
overhead during decision making.
3) Utilization Bused Strategy: The Utilization Based
Strategy routes each arriving transaction to the processing
element that offers the minimum response time computed
by using an approximation of server utilizations. (The
strategy is simply called MRT.UT.) The approximation
of server utilizations is based on the number mkl in the
routing history table.
Let pI denote the utilization of the processing element
P I . Since each PI has a fixed service capacity and uses the
processor sharing discipline, one can write
L,

PI

= -

1 - PI

e

+ 1)

J f l

(3.6)

(3.8)

j#i

where the normalizing constant Ck,is given by

ck, = -P

i

j= I

(3.9)
mkj

In other words, the MRT.UT approximates the steadystate utilization pi by assuming a routing probability at the
front-end and by setting it proportional to number of active transactions in the system.
Hence, given a routing history table G = ( m l I ,
,
m,,), estimates of utilizations can be computed by (3.8)
and (3.9), and then, by (3.7) estimates of the expected
response time can be derived. MRT.UT strategy routes
each arrivi-l transaction to the precessing system Pisuch
that Rki( L ) = min, s j 5 N { R k j (L ) } . This strategy is
based on state-dependent information ?i z and static information {L,
ak, bk, P ~ OP k, i , P ; k = 1 , * * * , K ; i = 1,
*

*

a

,N}.

B. Strategies Based on Instantaneous Queue Length
We now consider a class of strategies based on the instantaneous queue length at each processing systems. Two
different approaches of making routing decisions are considered. The first approach selects the processing system
which minimizes the estimated response time as before,
and the second one selects the system with minimal queue
length. Although the overhead to maintain instantaneous
queue length can be quite costly, in the following analysis
we shall ignore this overhead, as the objective is to understand, in the presence of remote calls, whether instantaneous queue length can provide more robust estimate on
the load condition.

1312

I E E E T R A N S A C T I O N S ON S O F T W A R E E N G I N E E R I N G . VOL

1) Minimum Response Time: Let ?? = ( n , ,
* > nN)
where n, is the instantaneous queue length of the processing system P I , for i = 1, 2, . . , N . Assume that the
instantaneous queue IengLh Z is available to the frontend. We then estimate L by ??, i.e., the expected reThus, this strategy
sponse time is estimated by Rr,(
requires state information Z and static information { a r ,
br, c, pko,pL,,p }. We refer to this strategy as MRT.IQL
for minimum response time based on instantaneous queue
length. Note that in the presence of remote database calls,
the instantaneous queue length may not be representative
of the mean queue length.
2) Minimum Instantaneous Queue Length: The minimum instantaneous queue length strategy (denoted as
MQL) routes each arriving transaction to the processing
system that has the least number of tasks being executed.
The minimum queue length strategy selects the processor
element PI such that n, = min, <,
{ n, } . If the minimum
is not unique, a processing system, among the ones that
achieve the minimum, is randomly chosen. This strategy
is only based on the state information ?? and does not
require any static information.
The minimum instantaneous queue length strategy, also
known as the send-to-shortest queue policy, is optimal for
a system with identical exponential queues, a single Poisson arriving transaction stream and where each transaction can be executed completely on any processor. For
more complex systems with multiple classes and/or transactions with distributed processing requirement, the minimum queue length strategy is not in general optimal.

s).

IV. SIMULATION
STUDYA N D PERFORMANCE
COMPARISONS

In the following, we use simulation to investigate the
effectiveness of the proposed dynamic routing strategies.
Mean transaction response time is the main concern and
is used to indicate system performance under the different
load sharing strategies. To compare the performance of
the strategies given in Section 111, we also consider the
optimal static load sharing strategy under which an incoming transaction is routed to a processing system according to a predefined routing probability. The optimal
static load sharing strategy takes account of all static information, i.e., { hk, a k , b k , c, pko,p k i , p } , to determine
the routing probabilities such that the mean transaction
response time is minimized. The details of solving this
optimization problem have been given in [21], where a
simplex reflection method is used to find the optimal routing probabilities.
A . Description of the Simulations
In order to evaluate dynamic load sharing strategies, we
simulated the model for the locally distributed database
system illustrated in Fig. 4. The simulation was implemented using RESQ [ 141. The routing decision is implemented as a separate function and is invoked upon a transaction arrival at the front-end system. In addition, for all

14, NO. 9. S E P T E M B E R 1988

simulation runs, 95 percent confidence level estimates
were obtained. The simulations were run until the relative
width of the confidence interval (width divided by midpoint) was less than 0.1.
In the experiments reported in the following, we assume that there are three transaction processing systems
( N = 3 ) and three transaction classes ( K = 3 ) . Based on
data from some IBM IMS systems [ 7 ] ,[22], the average
number of database requests per transaction is set to 15
for all transaction classes, i.e., p r o = 0.0625 for k = 1,
2, 3. The matrices 1 /( 1 - p r o ) [p r r ] ,which indicate the
distribution of database requests, are given in Table I to
reflect low, middle, and high localities of database requests.
To study the impact of processing load on database requests and request shippings, various system parameters
are assigned with the following values. We assume that
the processor speed is 7.5 MIPS and regard ah + br as a
unit of service demand for all k . The mean service time
of this unit is assumed to be 0.004 second (equivalently,
30K instructions in pathlength). The service demand of
shipping a request, c, is defined in terms of this unit. The
IO access time, d k , and the probability of having IO access pio are assumed to be 40 ms and 0.7 for all transaction classes, respectively. Also, we denote the ratio of bk
to ak + bk as r k , which represents the complexity of database requests.
The load of a processing system could be due to the
services of transaction application processing, database
request processing, and communication overhead processing. Let p,, = S / N p , where S is given in Section 11,
which indicates the average processing utilization per system due to application processing and database requests,
and is independent of the routing decisions. By changing
the arrival rates h k ,we can study the transaction response
times under different p,,. The load of a processing system
PI due to processing database requests, denoted by ,oh( i ) ,
is routing independent and is equal to Sf/p, where Sf is
given in Section 11. By changing the arrival rates subject
to a fixed p p , we can vary the relative value of the ph ( i )
which represent the partition depending processing utilizations.

B. Performance Comparisons
First, we study the effectiveness of routing strategies
under different processing loads p,,. The incoming transactions are assumed to have middle locality in regard to
the distribution of database requests, and rr = 0.3 for all
transaction classes. The partition dependent load on each
processing system is assumed to be equal, i.e.,
p h ( I ) : p h ( 2 ) : p h ( 3 ) = 1 : 1 : 1 . Figs. 5 and 6 show the
transaction response times versus p,, for c = 0.05 and c
= 0.25 (1.5K and 7.5K instructions in terms of pathlength), respectively.
When the communication overhead of shipping remote
requests is low, all dynamic load sharing strategies have
better performance than the optimal static strategy. When
the communication overhead is high, the optimal static

~

YU

1’1 (11.:

1313

DYNAMIC TRANSACTION ROUTING

TABLE I
THEDISTRIBUTION
O F DATABASE
REQUESTSUSEDI N EXPERIMENTS
low

htaha~c
Tx t y p e I
T x type 2
Tx t y p e

3

1

i n i d d l c locality

locality

2

3

I

0.65 0 . 2 0 0.15
0 . 1 7 0 . 5 2 0.31
0.21 0.21 0.58

strategy becomes better than the MRT.UT, MQL, and
MRT.IQL strategies. Over all strategies considered,
MRT.ST and MRT.RT strategies yield the best performance. Neither of these two strategies needs instantaneous state information. By maintain a routing history table in the front-end system, both strategies can be
implemented easily.
In Fig. 5(c), the mean communication load at each processing system versus the processing load is plotted under
the strategies studied. The communication load at each
processing system is due to the service of remote request
shipping and can reflect the number of remote database
requests. The optimal static strategy leads to the smallest
communication load. This implies that most transactions
are routed to the preferred processing system which owns
the database partition referenced by the majority of their
database requests. In general, except for MRT. UT, the
communication loads under the other five strategies are
increased either linearly or concavely. When the processing load is 0.81, the communication load introduced under the MRT.UT makes the system saturated. This seems
to suggest that the system under MRT.UT tends to be
unstable once the processing load is high.
Both the MQL and MRT.IQL strategies use instantaneous queue lengths to make transaction routing decisions. The instantaneous queue length may fluctuate frequently due to the issuing of remote database requests. As
a consequence, the decision processes using MQL and
MRT.IQL observe more instances where the system has
unbalanced load than under the other strategies. Thus, decision making tends to balance the loads regardless of introducing additional remote database requests. When the
communication overhead is high, these additional remote
database requests lead to an increase of system utilization
and transaction response time. Both the MQL and
MRT.IQL strategies end up with inferior performance
compared to the optimal static strategy.
With the same setups as the above experiments, we investigate the performance of load sharing strategies under
unbalanced pb(i). Figs. 6(a) and 6(b) show the transaction response time under different strategies when
p h ( 1 ) : p h ( 2 ) : p b ( 3 ) = 0.5: 1 : 1. In general, the results
have the same tendency as shown in Figs. 5(a) and 5(b).
Since the underlying load due to database requests at P , ,
i.e., ph( 1 ), is small, all strategies tend to route more
transactions to P I and, thus, introduce more remote shippings than under the case with balanced p b ( i ) . The increase in response time is more vivid when c = 0.25.

I

2

3

0 . 1 5 0.11 0 . 1 4
0 . 0 7 0 . 8 2 0.11
0.11 0.06 0.83

9ign locality

1

I

2

3

0.90 0 . 1 0 0 . 0
0 . 0 7 0 . 8 7 0.06
0.11 0.03 0.86

C. Sensitivity Analyses
In Figs. 5 and 6 , it has been shown that both MRT.ST
and MRT.RT are superior to other load sharing strategies.
We shall proceed further to investigate the robustness of
these two strategies through sensitivity analyses on various system and workload parameters. Specifically, we
vary communication overhead, distribution of database
requests, degree of balancing on partition dependent load,
and service demand of database requests.
I ) Sensitivity to Communication Overhead: The relationship between transaction response time and communication overhead is illustrated in Fig. 7, where simulation results for various communication overheads are
presented for MRT.ST, MRT.RT, and the optimal static
strategies. Two groups of curves are shown; one is for p p
= 0.57, the other is for pp = 0.71. The response times
increase more than linearly in the second group, which is
under higher processing load. Fig. 8 shows the increase
in mean communication load at each processing system
when the communication overhead of shipping database
requests and results becomes large.
It can be observed that the mean communication load
under the optimal static strategy is less than that under
MRT.ST or MRT.RT. The larger shipping loads under
MRT.ST and MRT.RT indicate that more transactions are
routed to a nonpreferred processing system to eliminate
temporary load unbalance. Despite the increase in communication load, the dynamic routing strategy balances
the loads among processing systems, eliminates possible
bottlenecks, and thus reduces the response time. The other
interesting observation in Fig. 8 is that the number of nonpreferred routings under MRT.RT is greater than that under MRT.ST. In contrast to MRT.ST, the MRT.RT’s approximation captures queueing effects. Thus, the
difference in estimated queue lengths between processing
systems under MRT.RT tends to be larger than that under
MRT.ST. As a consequence, more transactions are routed
to a nonpreferred processing system under MRT.RT.
2) Sensitivity to the Degree of Balance of Database Request Processing: In Fig. 9, the performance of MRT.ST,
MRT.RT, and the optimal static strategy is studied
for fixed pp’s and different proportions
of
pb( 1 ) : p b ( 2 ) : p b ( 3 ) , which is changed from 0 . 5 : 1 : 1 to
2.5 : 1 : 1. This is achieved by changing the arrival rates
of different transaction classes. When the loads of serving
database requests are unbalanced, it is certainly more difficult to trade off sharing transaction processing load with

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 14. NO. 9, SEPTEMBER 1988

-

C - 0 . 05
r,- 0.3
& ( 1 ):&(2):pb(3)-1 : 1 : 1

UOL

ii

m-

.-i
I-

x

-

il

C

0

::
LL

c-

LL

C

C

z:: N -

1

U
C
y1

e

6

s

UR1.Ul

e

Y-.la

-

um.m

C
O
C

--

r"

m1.
Sl
9 -

I

06

04

I

0.a

I

I

0.4

I

I

I

0.8
0.8

0.6

Processing Load p

Processing Load p ,

(a)

(b)

I

,

:

0.4

CptirnOl s1atic

,

0.8

0.6

Processing Load p ,

(C)
Fig. 5 . (a) Mean transaction response time vs. processing load ( c = 0.05).
(b) Mean transaction response time vs. processing load ( c = 0 . 2 5 ) . (c)
Mean communication load vs. processing load under different strategies.

r

,g

c-0.05
ri- 0 . 3
A(1 ):&(2):pb(3)-0.5:1 : 1

i

m-

k-

0
C

0
In

[L
0,

OPIlmol stotic

Optimol Slotic

Ym .Ioc
h

um.m
W.Sl

r.-

C

.U

MU1.m

C
VI

0
c
O

0 -

f
01

I

I

I

I

,

1

I

I

I

I

I

I

I

YU ef al.: DYNAMIC TRANSACTION ROUTING
r

D

1315

-0 3

with MRT. RT strategy
MRT.ST strategy
-+-+ : with optimol static
-;

tr:
with

-

&(1):pb(2):&,(3)-1:I :1
m -

u

-

yf

C
0

IY

c

-

0

1

n

I

0

I

I

1

I

03

02

0.1

1

0.5

04

Communication Overhead

Fig. 7 . Mean transaction response time versus communication overhead.
r,-O.3

-:

with MRT.RT strategy

: with MRT. ST strategy
: with optimal stotic

tt

q(l):pb(2):k(3)-1 :1:1

J
I

0

,

I

0

I

/

,

0.2

0.1

1

I

0.3

1

r

0.4

I
0.5

Communication Overhead

Fig. 8. Average communication load versus communication overhead.

-

-

c-0.25
r,-O.

.Ee

U

3
: with MRT. RT strategy

: with M R T . ST strategy
: with optimal static

-

o
C

::

p,-0.71

0-

wU
c
0

-

0
U

?
7
0
C

+

m -

pp-0.57

U

I

m -

0

destined, and 2) additional communication load introduced during balancing the loads of processing systems.
In addition, the MRT.ST strategy is much more sensitive
to the degree of balancing of P b ( i ).
The unbalanced loads in the processing systems can also
be used to differentiate the performances under MRT.ST
and MRT.RT. Since the routing decision is based on the
difference of estimated loads between processing systems,
the accuracy in estimation only becomes clear when the
loads are quite different. As shown in Fig. 9, the performance under MRT.RT can be a lot better than that under
MRT.ST for highly unbalanced cases. When
pb ( 1 ) : P b ( 2 ) : &, ( 3 ) = 2.5 : 1 : 1, the response time under
the MRT.ST gets even worse than that under the optimal
static strategy.
3) Sensitivity to the Database Reference Pattern: Under the MRT.RT strategy, Fig. 10 shows the
mean response time versus processing load for the different reference distributions of database requests defined in
Table I. As expected, the cases with less locality have
longer response times. When p p = 0.81 and c = 0.25,
the processor utilization of the low locality case can reach
0.99. The decrease in the locality leads to higher communication load. When the locality of database request
decreases, the benefit of routing transactions to the preferred processing system decreases. Thus, more transactions get routed to a nonpreferred processing system to
balance the load. Fig. 11 shows the percentage of transactions routed to nonpreferred processing systems versus
processing load. Clearly, the percentage of non-preferred
routing in the case with low locality is larger than that in
the cases with middle or high locality. The changes in
transaction routing under different processing load can be
studied in Fig. 11. When the utilization increases, the
communication load incurred from assigning transactions
to a nonpreferred processing system may deteriorate
transaction response time more than the gain from balancing the load. Thus, this percentage will be reduced to
avoid system saturation.
4) Sensitivity to Service Demand of Database Requests: In [21], the performance impact of database request complexity has been studied under static routing
strategy. Two types of database requests are considered:
database calls, e.g., DLI1 calls in IMS or a SQL queries
in DB2, and IO requests. Note that the ratio rk which is
defined as the ratio of bk to ak b,, indicates the complexity of database requests. A database call can be represented by a larger r,, whereas a small rk is used for IO
requests. By changing the ratio r k , we can study the performance impact of the complexity of database requests
as well as the proportion of partition dependent load.
Consider the MRT.RT and the optimal static strategy.
When rk increases, the routing dependent load decreases.
Intuitively, this would result in an increase in response
time, since the routing mechanism now has less leverage
to balance the system. However, as shown in Fig. 12 for
p,, = 0.71 and c = 0.25, the increase of transaction response time only occurs if the underlying load (partition
dependent load) is unbalanced.

1

2

3

Balancing Factor of Partition Dependent Service

Fig. 9. Mean transaction response time versus degree of balancing on partition dependent load.

reducing communication overhead load. As shown in Fig.
9, when the pb(i)’s are unbalanced, the response time increases apparently because of 1) the queueing effect at the
processing system for which most database requests are

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. 14. NO. 9, SEPTEMBER 1988

1316
with MRT.RT strotegy

rk-O. 3

-k

f

--

low locolity

- - middle locolity

--

high locolity

C-0.05

06

08

Processing Load p,

Toto1 N o . of Active Tasks at Processors

Fig. 10. Mean transaction response time versus processing load

:[

Fig. 13. Distributions of the total number of queue lengths at processing
systems.

with MRT RT strotegy
r,-O. 3
- - l o w locality
- - middle locolity
- - high locality

+-+

c-0.05

:

Fig. 11. Nonpreferred routing frequency versus processing load

52-

Fig. 14. Distributions of the index of unbalancing

c-0.25
6-0.71

-

.E
w

-

c

,a

:
OPlimOl SIOliC

$ 1 P~IZ~%(JI.

I-

m-

o.5.,

:,

LL

c

-

.U

0

>

/

-

-

OPlimDl *lOliC

$

%I1 Py2,.%la-

MR1.W

r.-

c

?
c
0

I”

w-

0

0

5 ) Queue Length Distribution: The above simulation
results, in general, present average response time or average loads. It is also important to examine the distribution
of processor queue lengths in the complex under different
load sharing strategies. For each transaction arrival, we
sample two quantities to study the processor queue lengths
in the simulations. The first one is the total number of
queue lengths at the processing systems. With the notation ni defined in Section 111, this number is equivalent to
Erzl ni. Fig. 13 shows the distributions with different
communication overheads under MRT.RT and the optimal static strategy. The curves in Fig. 13 not only confirm
that the transaction response time under MRT.RT is
shorter than that under the optimal static strategy, but also

I

I

I

,

I :I

:,

YU

1317

et al. : DYNAMIC TRANSACTION ROUTING

ing among the processing systems. For a perfectly balanced system, this index should be equal to either 0 or l .
In the distributed database environment considered, this
index will have more variation due to remote database requests. Fig. 14 illustrates the distributions of this index.
Note that the MRT.RT strategy really minimizes the index of unbalancing among the processing systems.

V. CONCLUSION
In this paper, we have studied different strategies for
dynamic transaction routing in a locally distributed database environment. A new class of dynamic strategies is
proposed. This class uses the routing history and transaction characteristics to route the incoming transactions
so as to minimize their response times. Two dynamic
strategies in this class, MRT.ST and MRT.RT, provide
substantial performance improvement over the optimal
static routing strategy.
In summary, dynamic strategies can be superior to the
optimal static strategy if good routing decisions are made
using appropriate information about the system state. A
good routing decision needs to consider not only balancing the load but also reducing remote processing requests.
The state information used in making decisions must be
able to characterize the average system behavior instead
of the instantaneous load fluctuation. Although in this paper we specifically examined load balancing strategies in
the locally distributed environment where the database is
partitioned, the same strategies can be applied to any system with distributed resources and to other database environment such as data sharing [ 2 2 ] , or replicated databases.

system PI to P, . In this case, there are two components in
the expected response time: the first, Rg: (Ll), is due to
the communication overhead in PI of shipping the request
and receiving the result, and the second, RL,(LJ), is due
to the service of a remote database request in P, , which
includes receiving the request, processing the request and
sending the result-back. Hence, the total expected response time &,( L ), for an arriving type k transaction
assigned to processing system P, can be written as
N

PkO

R i ~ ( L+
~ )PkiRLi(Li) + J = l PkiRgy(Li)
J f l

N

1

where pio dkis the expected I/O service time for each database request and pkj is the database request probability
of a class k transaction to D B j . Here, an infinite server
model is assumed for the 1 0 devices. More elaborate
models for the IO can easily be incorporated in (a.l), if
needed.
On the other hand, each component in the expected response time in (a. 1) can be approximated by

Ri;(L,) = Si;(L; + 1 )
*
fork = 1,
* , K , i = 1,
, N , wherex = a, 1, ov,
r, and Sii is the service time of txk on Pi for service x .
Given the mean service demand for txk of application processing segments, database requests and sending or receiving requests, i.e., a k , bk, and c, respectively, one can
write

APPENDIX
CALCULATION
OF MEANTRANSACTION
RESPONSETIME
Consider the system model shown in Fig. 4. Let us assume that the class k transactions are routed, according to
a certain probability distribution, to the processing system
P, . Then the system can be represented by a product form
open queueing network [3] with K chains corresponding
to the transaction classes and N processor sharing queues
representing the processors, each connected with an infinite server queue representing the U0 servers. If the
steady state queue length L = ( L l , * . , LN), where L,
denotes the mean queue length of P I , for i = 1, * , N ,
is known, then the mean response time of a 2 arriving type
k transaction routed to Pi,denoted as Rkr(L ), can be approximated as follows.
Let Rg, ( L ,) denote the expected response time for completing the application process for a class k transaction
txk, which is routed to a processing system Pi. Consider
the processing time of a database request at a processing
system given there is no I/O access. The expected response time of serving a local database request to DB, at
P,, for a transaction txk routed to Pi, is denoted by RL, (I,).
For a database request to DB, issued from P , , where i #
j , a database request must be shipped from processing

-

where p is the processing speed of each processor. Hence,
by (a.2) and (a.3), the total expected response time given
by (a.1) for transaction txk routed to Pi can be rewritten
as (3.1).
ACKNOWLEDGMENT
We would like to thank S . Lavenberg and J . Wolf for
their comments and suggestions; D. Cornel1 for discussing the optimal static routing strategy; and A. MacNair
for assistance with the use of the RESQ package.
REFERENCES
A. K. Agrawala, S . K. Tripathi, and G . Ricart, “Adaptive routing
using a virtual waiting time technique,” IEEE Trans. Software Eng.,
vol. SE-8, no. 1, pp. 76-81, Jan. 1982
Y. Bard, “A model of shared DASD and multipathing,” Cornmun.
ACM, vol. 23, no. 10, pp. 564-572, Oct. 1980.
F. Baskett, K . M . Chandy, R. R. Muntz, and F. Palacios, “Open,
closed and mixed networks of queues with several classes of customers,’’ J . A C M , pp. 248-260, Apr. 1975.
M . J. Carey, M . Livny, and H. Lu, “Dynamic task allocation in a
distributed database system,’’ Dep. Comput. Sci., Univ. WisconsinMadison, Tech. Rep. 556, Sept. 1984.
K. M. Chandy and D. Neuse, “Linearizer: A heuristic algorithm for

1318

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 14, NO. 9, SEPTEMBER 1988

queueing network models of computing system,’’ Commun. ACM,
vol. 25, no. 2 , pp. 126-134, Feb. 1982.
161 Y-C. Chow and W. H. Kohler, “Models for dynamic load balancing
in a heterogeneous multiple processor system,” IEEE Trans. Comp u t . , vol. C-28, no. 5 , pp. 354-361, May 1979.
[7] D. W. Cornell, D. M. Dias, and P. S . Yu, “Analysis of multi-system
function request shipping,” IEEE Trans. Software Eng., vol. SE-12,
no. 10, pp. 1006-1017, Oct. 1986.
[8] D. L. Eager, E. D. Lazowska, and J. Zahorjan, “Adaptive load sharing in homogenous distributed systems,’’ Technical Report 84-10-01,
Dept. Comput. Sci., Univ. Washington, Tech. Rep. 84-10-01, Oct.
1984.
[9] -,
“A comparison of receiver-initiated and sender-initiated adaptive load sharing,” Performance Evaluation Review, vol. 13, no. 2,
pp. 1-3, Aug. 1984.
[IO] A. Ephremides, P. Varaiya, and J. Walrand, “A simple dynamic
routing problem,” IEEE Trans. Automat. Contr., vol. AC-25, no. 4 ,
pp. 6901693, Aug. 1980.
R. Nelson and D. Towsley, “Comparison of threshold scheduling
policies for multiple server systems,” IBM, Res. Rep. RC-I 1256,
July 1985.
L. M. Ni and K. Hwang, “Optimal load balancing in a multiple processor system with many job classes,” IEEE Trans. Software Eng.,
vol. SE-11, no. 5 , pp. 491-496, May 1985.
M. Reiser and S . S . Lavenberg, “Mean-value analysis of closed multichain queueing networks.” J . A C M , pp. 313-322, Apr. 1980.
C . H. Sauer, E. A. MacNair, and J. F. Kurose, “The research
queueing package version 2: Introduction and examples,” IBM,
Yorktown Height. NY. Res. Reo. RA-138. 1982.
[ 151 P. Shweitzer, “Approximate analysis of multiclass queueing networks of queues,” in Proc. Int. Conf. Stochastic Control and Optimization. Amsterdam, The Netherlands: North-Holland, 1979.
[16] H . S . Stone, “Multiprocessor scheduling with the aid of network flow
algorithm,” IEEE Trans. Sofrwure Eng., vol. SE-3, no. 1, pp. 8593, Jan. 1977.
“Critical load factors in two processor distributed systems,”
1171 -,
IEEE Trans. Software Eng., vol. SE-4, no. 3, pp. 254-258, May
1978.
[I81 A. N. Tantawi and D. Towsley, “Optimal static load balancing in
distributed computer systems,” J . ACM, pp. 445-465, Apr. 1985.
[I91 Y.-T. Wang and R. J. T. Morris, “Load sharing in distributed systems,” IEEE Trans. Comput., vol. C-34, no. 3, pp. 204-217, Mar.
1985.
[20] W. Winston, “Optimality of the shortest line Discipline,” J . Appl.
Prob., vol. 14, pp. 181-189, 1977.
[21] P. S . Yu, D. W. Cornell, D. M. Dias, and A. Thomasian, “On coupling partitioned database systems,” in Proc. 6th In?. Conf. Distributed Computing Systems. May 1986, pp. 148-157.
[22] P. S. Yu, D. M. Dias, J. T. Robinson, B. R. Iyer, and D. W. Cornell,
“On coupling multi-systems through data sharing,” Proc. IEEE, vol.
75, no. 5 , pp. 573-587, May 1987.

Philip S. Yu (S’76-M’78-SM’87) received the
B.S. degree in electrical engineering from National Taiwan University, Taipei, Taiwan, Republic of China, in 1972, the M.S. and Ph D degrees in electncal engineering from Stanford
University, Stanford, CA, in 1976 and 1978, respectively, and the M.B.A. degree from New York
University, New York, NY, in 1983.
Since 1978 he has been with IBM Thomas J.
Watson Research Center, Yorktown Heights, NY
Currently he is the Manager of the Architecture
Analysis and Design Group. His current research interests include computer architecture, database management systems, performance modeling,
workload analysis, computer networking, and VLSI testing.
Dr Yu is a member of the Association for Computing Machinery.

Simonetta Balsamo received the Doctoral degree
in Scienze dell’Informazione from the University
of Pisa, Pisa, Italy, in 1980.
She is currently a Research Associate with the
Dipartimento di Informatica, University of Pisa,
Italy. During the period May 1985-May 1986 she
was at the Thomas J. Watson Research Center,
Yorktown Heights, NY, as a Visiting Scientist.
Her research interests are in the area of performance modeling, analysis, and synthesis of computer/communication systems and distributed systems.
Dr. Balsamo is a member of the Association for Computing Machinery.

Yann-Hang Lee (S’81-M’84) received the B.S
degree in engineering science and the M.S. degree
in electncal engineering from National Cheng
Kung University in 1973 and 1978, respectively,
and the Ph.D degree in computer, information,
and control engineering from the University of
Michigan, Ann Arbor, in 1984.
From 1984 to 1988, he was a Research Staff
Member in the Architecture Design and Analysis
Group at IBM Thomas J. Watson Research Center, Yorktown Heights, NY. Since August 1988,
he has been an Associate Professor in the Department of Computer and
Information Sciences, University of Florida, Gainesville. His research interests include distributed computer systems, database processing systems,
fault-tolerant computing, VLSI testing, performance evaluation, and stochastic decision theory.
Dr. Lee is a member of the IEEE Computer Society and the Association
for Computing Machinery.

339

IEEE TRANSACTIONS ON COMPUTERS, VOL. 31, NO. 3, MARCH 1988

Optimal Design and Sequential Analysis of VLSI
Testing Strategy
PHILIP s. Y u ,

SENIOR MEMBER, IEEE,

c. M. KRISHNA, MEMBER,

Abstract-In this paper, we present a new method for
determining the optimal testing period and measuring the
production yield. With the increased complexity of VLSI circuits,
testing has become more costly and time consuming. The design
of a testing strategy, which is specified by the testing period based
on the coverage function of the testing algorithm, involves
trading off of the cost of testing and the penalty of passing a bad
chip as good. We first derive the optimal testing period assuming
the production yield is known. Since the yield may not be known
a priori, we next develop an optimal sequential testing strategy
which estimates the yield based on ongoing testing results, which
in turn determines the optimal testing period. Finally, we present
the optimal sequential testing strategy for batches in which N
chips are tested simultaneously. The results will be of use whether
the yield stays constant or varies from one manufacturing run to
another.

Wafer
fabrication

IEEE, AND

-

YANN-HANG LEE,

Wafer Test

--+I

Dice. Pick
Assembly

MEMBER, IEEE

-

Chip
Functional Test

Apart from the cost, two factors need to be considered when
determining the test lengths. The first is the production yield,
which is the probability that a product is functionally correct at
the end of the manufacturing process. If the yield is high, we
may not need to test extensively since most chips tested will be
“good,” and vice versa. The other factor to be considered is
the coverage function of the test process. The coverage
function is defined as the probability of detecting a defective
Index Terms-Consistent estimation, design of experiments, chip given that it has been tested for a particular duration or a
performance analysis, sequential decision, VLSI testing strategy. given number of test patterns. If we assume that all possible
defects can be detected by the test process, the coverage
I . INTRODUCTION
function of the test process can be regarded as a probability
S the density of VLSI products increases, their testing distribution function of the detection time given the chip under
becomes more difficult and costly. Generating test test is bad. Thus, by investigating the density function or
patterns has shifted from a deterministic approach, in which a probability mass function, we should be able to calculate the
testing pattern is generated automatically based on a fault marginal gain in detection if the test continues.
model and an algorithm, to a random selection of test signals
In general, the coverage function of a test process can be
[1]-[4]. No matter whether deterministic or random genera- obtained through theoretical analysis as suggested in [5]-[8],
tion of testing patterns is used, the testing pattern applied to or experiments on simulated fault models [9]-[12]. With a
the VLSI chips can no longer cover all possible defects. given production yield, the fault coverage requirement to
Therefore, the question of sufficient testing needs to be attain a specified defect level, which is defined as the
explored in detail.
probability of having a “bad” chip among all chips passed by
Consider the manufacturing processes for VLSI chips as a test process, has been studied in [4], [5], and [13]. Also,
shown in Fig. 1. Two kinds of cost can incur with the test Agrawal and Seth have used simple curve-fitting techniques to
process: the cost of testing and the cost of accepting an characterize VLSI yield from test data [14]. However, no
imperfect chip. The first cost is a function of the time spent on attempt is done in optimization of the total testing cost and ontesting or, equivalently, the number of test patterns applied to line estimation of the production yield.
the chip. The cost will add to the cost of the chips themselves.
The determination of the optimal testing period or test
The second cost represents the fact that, when a defective chip length which minimizes the total testing cost is the central
has been passed as good, its failure may become very costly issue of this paper. We first obtain the optimal test period
after being embedded in its application. An optimal testing under the assumption that the production yield is known. Next,
strategy should trade off both costs and determine an adequate we examine the case in which the production yield is
test length (in terms of testing period or number of test unknown. After a chip is tested, the test result will be fed
patterns).
back to estimate the production yield. The estimated yield,
then, is used to determine the optimal testing period of the
Manuscript received January 28, 1986; revised January 28, 1987. This
currently tested chip and subsequent chips. The problems we
work was supported in part by the National Science Foundation under Grant
have to solve, namely the estimation of the yield and the
NSFDMC-8504971 to C. M. Krishna.
P. S. Yu and Y.-H. Lee are with IBM Thomas J. Watson Research Center, determination of the test period, are similar to the decision
Yorktown Heights, NY 10598.
problems that occur in sequential experiments [ 151, [ 161.
C. M. Krishna is with the Department of Electrical and Computer
However, the decisions on the optimal test period and the
Engineering, University of Massachusetts, Amherst, MA 01003.
sampling period (i.e., testing period) are correlated. The
IEEE Log Number 8716363.

A

OO18-9340/88/03OO-0339$01.OO 0 1988 IEEE

340

IEEE TRANSACTIONS ON COMPUTERS, VOL. 37, NO. 3, MARCH 1988

information obtained from testing may be “censored,” i.e., a
faulty chip may not be captured by the test process.
In the following discussion, we use a continuous test period,
instead of the number of test patterns applied, to describe the
test process. A test with period t means that a sequence of test
patterns is applied for t units of time to the chip under test.
This is analogous to the number of test patterns applied to the
chip. Meanwhile, we assume that the detection time of a bad
chip is with nonincreasing hazard rate. I This implies that the
marginal utility of testing does not increase as the test
continues. The order of test patterns applied should be
arranged so that the hazard rate function is nonincreasing. For
random testing, the exponentially distributed function, which
has a constant hazard rate, is commonly used as the coverage
function [4]-[7].
In the next section, the testing strategy for a given yield is
studied. The optimal test period and the properties of the
optimal test strategy are obtained. When the production yield
is not available, we introduce a sequential test procedure in
Section I11 which determines both the optimal test period and
the yield simultaneously during testing. The persistence and
consistency of the sequential test procedure are proved. In
Section IV, we extend this to test chips in batches. We
conclude with Section V.
11. OPTIMAL
TESTING
POLICY
ASSUMING
YIELDIS KNOWN
Let the coverage function of a testing process with respect to
a testing period t be F(t), which is the distribution function of
the detection time given the chip under test is defective. We
assume that F ( t ) is continuous with derivative f(t), 0 I F(t)
I 1 , and F ( a ) = 1. We also assume that all chips to be tested
are stochastically identical and denote the defect probability
which is the probability of a chip being defective as w , where 0
< w < 1. Then, the production yield, which is the probability
that a product is fault-free, becomes 1 - W . If testing is
applied for a period t, the probability of detecting a defective
chip is wF(t) and the probability that a chip which has passed
the test is defective is w F ( t ) , where E(t) = 1 - F(t).
Let the cost of testing be c, per unit time and the cost of
failing to detect a faulty chip be cy When the test is performed
until either a fault is detected (the chip is rejected) or the
period t has elapsed (the chip has passed the test), the mean
total cost becomes

C ( t , w)=c,(l-wF(t))t+crwE(t)+c,w

l‘

tdF(t).

(1)

To obtain the optimal testing period, which minimizes the total
testing cost C ( t , U), we use Theorem 1 below.
Theorem 1: If F(t) is with a nonincreasing hazard rate, h(t)
= f(t)/(l - F(t)), the optimal testing period t*(w) which
minimizes C ( t , w ) is equal to 0 when f ( 0 ) < c,/wcf, or
satisfies the following equation, otherwise
CS

J(t)=-

0 c/

’

(1-wF(t)).

Proof: From ( l ) , we obtain the derivatives of C ( t , U )
with respect to t.
C’(t, U)

ac
= -=c,-c,oF(t)-c/wf(t)
at

=w(c,h(t)-c,)f(t)-wc/(l - F ( t ) ) h , ’ ( t ) .
Since the coverage function is with nonincreasing hazard
rate,f(t) is decreasing and h(t) cc h(0) = f ( 0 ) . Whenf(0) <
cs/wcf, C’(0, U ) > 0 and h(t) < c,/wcf which implies that
wf(t)/(l - wF(t)) < cs/cfand C’(t, U ) > 0 for all t. Thus,
C(t, U ) is increasing function of t and the optimal testing
period is 0.
It can be observed that the right-hand side of (2) is a
nonnegative and nonincreasing function oft. Givenf(0) 2 c,/
wcr and f ( w ) = 0, there must be at least one solution which
satisfies (2). Let t*(w) satisfy (2). It can be easily shown that
C”(t*(w), U ) > 0 and wf(t)/(l - wF(t)) is nonincreasing
with respect to t. Thus, t*(o)is unique and minimizes C ( t ,
U).
0
It follows from Theorem 1 that when the test has been
performed for a period t*(w), the cost saved from detection of
bad chips by continued testing is equal to or less than the cost
of continuing the test. Thus, testing should then stop immediately. The minimized mean total cost becomes

C*(O) = C ( [ * ( U ) , w ) = C/Ut*f( t*)

+ c p E ( t * )+ c,w

(3)

Corollary I : The optimal testing period t * ( w ) is nondecreasing with respect to U , and the minimized mean total cost
C*(w) is a concave function of W .
Proof: Assume that 0 < col I w2 < 1. If f * ( w ~ )= 0,
t*(wz) must be greater than or equal to t*(ol).
Note that wf(t)/
(1 - wF(t))is nondecreasing with respect to w for 0 Iw I 1
and is nonincreasing with respect to t if h(t) is nonincreasing.
If t*(wl) > 0, we obtain
wzf(t*(w1))
olf(t*(wl)) --- cswzf(t*(wz))
1 -w2F(t*(w1))> 1 -w1F(t*(w1)) c/ 1 -wzF(t*(wz)) .
Thus, we have t*(wl) It*(w2).
To prove that C * ( w ) is concave, we consider the case that
wlOw < w < 1 where wlow = c,/f(0)cf. From (3), C * ( w ) is
continuous with respect to W . By differentiating C*(w) twice
with respect to a,we obtain
dC*(w) - C * ( w )
do
W
d2C*(w)
do2

-

c,t*(w) w

-1
w2

E

(t*)- cs

( C * ( w ) - c,t*(w))

(2)

The hazard rate of a random variable is defined as (dF(t)/dt)/(l - F(t)),
where F(t) is the probability distribution function of the random variable.

l‘*td F ( t).

dw
-

dt*(w)
- _c, -

w

dw

1“

F ( t ) dt

34 1

YU et al.; VLSI TESTING STRATEGY

coverage function: exponential dis
with mean 1

7-

.

0

z

%

-

Iw

O

0.4

0.2

1 .o

0.8

0.6

PROB. OF HAVING FAULT ( w )

PROB. OF HAVING FAULT ( w )

Fig. 2. Optimal testing periods for exponential coverage function.

Fig. 4. Optimal testing periods for Weibull coverage function.
c.-1

c*-1 .o
ratio-c,/c,
coverage function: exponential dis .
with mean 1

.o

rotio-c./c,

coverage function: Weibull dis.
with mean 1
shape parameter 0 . 7 5

z
n
w

-1

P
0

I

1

0.2

I

I

0.4

I

I

0.6

I

1

I

0.8

I
1 .o

PROB. OF HAVING FAULT ( w )

Fig. 3.

Minimized mean costs for exponential coverage function.

Since t*(o)is nondecreasing, the second derivative of C * ( w )
is, then, nonpositive. Thus, C * ( w ) is concave for wlow < w <
1. Note that, when 0 < w 5 wlow C * ( w ) = cfw is a linear
function of w with slope cf. Also, dC*(o)/dw < cffor all wlOw
< w < 1. Thus, C*(w) is concave for all 0 < w < 1. 0
Two examples are presented in Figs. 2-5, where the
coverage functions are assumed to be exponential and
Weibull, respectively. The test times in the examples are
normalized such that the mean time to detection is 1 when a
bad chip is tested. The costs are also normalized with respect
to c, which is assumed to be a unit. Figs. 2 and 4 show the
optimal testing periods with respect to U , and the minimized
expected costs are illustrated in Figs. 3 and 5, respectively, for
these two coverage functions. It is observed that the optimal
testing period and the minimized expected cost are greater in
the case of the Weibull distribution: this is because of the long
tail of the Weibull distribution. Also, when w approaches one,
the testing is ended mostly with a detection. Thus, the
minimized expected cost will converge to 1 . It is interesting to

0

0.2

0.4
0.6
PROB. OF HAVING FAULT ( w )

0.8

10

Fig. 5 . Minimized mean costs for Weibull coverage function.

notice that, when w approaches either 0 or 1, we have a small
total cost. This is implied by the low uncertainty regarding the
existence of defect. On the other hand, when w is close to the
middle of [0, 11, the total testing cost becomes high.
Furthermore, the value of wlOw is quite small in the above
cases, especially, wlow = 0 when the coverage function is the
Weibull distribution.
AND MEASUREMENT
OF YIELD
111. OPTIMAL
TESTING
POLICY

The previous section presents the optimal testing policy
when the production yield is known. Since the measurement of
yield should be based on the results of testing, in many cases,
it will not be realistic to assume that the production yield is
known before testing. In this section, we shall discuss the
optimal testing policy for uncertain yield. We assume that a
large number of chips will be tested sequentially. Before any
chip is tested, prior information about the yield will be used to
determine the optimal testing policy. Then, the results of the
tests are used to refine the estimate of the yield which will be

342

IEEE TRANSACTIONS ON COMPUTERS, VOL. 37, NO. 3, MARCH 1988

Dhfribulim

Observotion L . Fvnstion d YiddD
Decision:
*sling Period
ield Estimotion
Optimal Testing
U

4
I

G;(D(t))=

Experiment:
Perform Testing

[' w t i ( w l D ( t ) ) d w = G i -

1 - djF( t )

*

(7)

Let
C(t, ti) be defined as.the expected total testing cost
'7
'

I

when the testing period is t given the density function E;(w) of

is fixed during the testing of the ith chip, we have
dC(t,IfE;)&(U)
= C(t, 6;)
for any given testing period t, since C(t, w )
U;.

resling Rcsulb

Fig. 6. Flow diagram of sequential testing and yield estimation

applied to design the testing policy of the chip being tested and
all subsequent chips. Fig. 6 illustrates the whole process where
the measurement of yield, the control of the testing period, and
the testing process are integrated.
Let the prior probability of being defective before testing
the ith chip be described by a random variable U;. ai is
dependent on the prior information on yield and the results of
previous tests. We express the uncertainty of w; in terms of a
probability distribution defined on [O, 11. Let t i ( w ) be the
generalized probability density function [15] of ai, i.e., ti is a
probability density function if wi is continuously distributed, or
a probability function if wi is discrete. The mean and the
variance of 0; are denoted by 6;and a:, respectively. With an
initial density function tI(w) of wI, where a: # 0, the
sequential testing procedure can be given as follows.
For testing the ith chip i = 1 , 2, *

is a linear function of w as indicated in (1). From Theorem 1,
this implies that t*(Gj) is the optimal testing period. However,
when the event D(t) is observed during the current testing
period, the prior information should be updated. The updated
distribution should be used to modify the current testing
period. The optimal testing period depends on not only the
prior distribution of w but also on what happens during testing.
Thus, to determine the optimal testing period in this case, the
current testing result has to be taken into account. To obtain
the optimal testing period, we need the following lemma.
Lemma 1: For all t 2 0. Gi(D(t))is decreasing with respect
to t and 0 I Gi(D(t)) Id; I Gi(D(t))5 1.
Proof: The lemma follows immediately from (6) and
(7).
0
Theorem 2: If F ( t ) is with nonincreasing hazard rate and
the prior density function of w is &(U), the testing period t * ( [ i )
2 0 which satisfies

t*(Gj(D( t ) ) )= t

(8)

e ,

1) is equal to zero if and only if t*(G;) = 0, and 2 )
minimizes C ( t , ti),where t*(w) is defined in Theorem 1.
Proof: At first, we show that t*(Ei) which satisfies (8)
exists. Since Gi(D(t)) is decreasing of t (from Lemma 1) and
t*(o) is nondecreasing (from Corollary l), t*(Gi(D(t))) is
nonincreasing o f t . Thus, there must exist one and only one
nonnegative t which satisfies (8).
Next, we prove that the t*(ti)
which satisfies (8) is equal to
zero if and only if t*(Gi) = 0. When t*(G;) = 0, t*(d@(t)))
= 0 for all t since Gi(D(t))is decreasing. Thus, t*(t;)
= 0 is
the only solution of (8). On the other hand, if t*(E;) = 0,
In what follows, we show how to integrate the process of t*(Gi) = t*(dj(D(O)))= 0. This also implies that t*(Ei) > 0 if
estimating the failure probability and simultaneously adapting and only if t*(di) > 0.
the test process to determine the optimal testing period t*([;). Note that, when t*([;)> 0, (8) can be rewritten as
Let the events observed from a test be D(t)and D(t),when a
G;(D(t*(ti)))f(t*(Ei))- cs
fault is detected and no fault is detected after testing has been
1
-Gi(D(t*(Ei)))F(t*(Ei))
*
applied for t, respectively. Based on which event is observed,
information about wi can be updated and used to estimate ai+1. Suppose that we have been applying testing for a period 0 I t
Bayesian statistical methods [15] are adopted here to update < t*(&)and have not detected a fault yet. The updated mean
prior information. When the event D(t) or D(t) occurs, the of w; becomes di(D(t)).Testing should be continued since
posterior density function of wi becomes
1. Calculate the optimal testing period, t*(ti),
based on the
current information of wi which is given by ti(o).
2 . Test the ith chip until either
a. no fault is detected at the end of testing period t*(&):the
chip is then passed as good.
b. a fault is detected before the end of testing period t*(Ei):
testing then stops and the chip is rejected.
3. Based on the test result and [ ; ( U ) , calculate the posterior
density function of wi which will be used as the prior
density function of U;+ in the subsequent test.

-./

( t *(ti))If(t )

di (D( t )If(t )

(4)

1 --dj(D(t))F(t) 1 -wi(D(t*(t;)))F(t)

Gi(D(t*(Ei)))f(t*(Ei))- cs

2

-./

1 -cs;(D(t*(~;)))F(t*(Ci))
(5)
It follows that the mean values of the posterior w; conditioned
on the occurrence of D(t) or D(t) are

Also, if t
wi(D(t))f(t)

> t*(E;) 2 0, we have
<

oi(D(t))f(t*([i))

1 - G @ ( t ) ) F ( t ) - 1 -Gj(D(t))F(t*(Ej))

*

343

YU et al. : VLSI TESTING STRATEGY

-

rotio-0.0001
coveroge function: Weibull dis.
with mean 1

shape parameter 0 . 7 5

-

0 -

vor(wib0
vor(wib0.05

(4-0.5

a

0
w

-

I

0

4

0

12

16

TESTING PERIOD

Fig. 7.

Optimal test periods with estimated yield.

The above inequality implies that the saving from detecting a
bad chip is less than the cost of continuing the test at t > t*(E).
Testing should not be continued if we have not detected a fault
after testing for period t*([).(Note that the last inequality in
the above equation occurs when t*([,) = 0.)
0
Equation (8) can be interpreted as follows. When the testing
begins, we have the testing period t*(W,) which is optimal for a
fixed [,(U). If no fault is detected after a testing period t <
t*(tI),&(U) will be updated and then we have a new optimal
testing period, i.e., t*(W,(D(t))),which is greater than t.
When no fault is detected after the testing period t*((,),the
optimal testing period is equal to the elapsed testing time.
Testing should not be then continued. In Fig. 7, we present
solutions of (8) for a Weibull-distributed coverage function.
and the diagonal line are the
The intersections of t*(Wl(D(t)))
respective optimal testing periods. Notice that, when of
increases, W , ( b ( t ) ) decreases quickly, i.e., the optimal testing
period becomes short.
Corollary 2: For any prior density function El(w) of U,,
which has the mean G I , t*(&)= 0 if t*(W,) = 0, otherwise, 0
< t*([,)It*(W,).
Proof: The corollary follows quickly from the proof of
Theorem 2 and the fact that W,(D(t))IW,.
0
Corollary 3: If t*(Wl) > 0, then Prob (t*([,)> 0) = 1 for i
= 1 , 2 , 3,
Proof: The corollary is proved by induction. From
Corollary 2, Prob ( t * ( t 1 )> 0) = 1 if t*(Wl) > 0. We need to
show next that Prob (t*(E,+])> 0) = 1 if Prob (t*(&)> 0)
e - . .

= 1.

When Prob (t*([,) > 0) = 1, based on Corollary 2 and
Theorem 1, we have Prob (W,> cs/f(0)cf)= 1. Let us assume
that &(w) is known and, thus, t*(,$,)is determined from (8). If
there is a fault detection during the testing of ith chip, W,+ =
W,(D(t))> W,> cs/f(0)cf.Thus, by Theorem 1, t*(W,+J> 0
which implies t*([,+I) > 0. When there is no detection during
the testing period t*([,),due to the fact that f ( 0 ) = h(0) 1

-

- 1- W i ( b ( t * ( ~ i ) ) ) F ( t * (-~c,, ) )*
Therefore, t*(Wi+ > 0 as in the previous case. It follows that
0
Prob (t*(&+J > 0) = 1.
Corollary 3 brings to light an interesting property, which is
called persistence, regarding the above sequential testing
procedure. Consider the case that wlow = cs/f(0)cf > 0. If 01
IwlOw,no test will be performed and no observation will be
obtained to update the initial information of w . On the other
hand, if the original information about w indicates that the
optimal testing period is greater than zero, i.e., 01 > colow,
testing should be applied to all subsequent chips and the
optimal testing period would not be equal to zero no matter
what testing results are observed.
After testing the nth chip, we will have obtained a posterior
density function [,,(U). Its mean W, can be used as an estimator
of the true defect probability. The sequence 0,i = 1, 2, *
can be regarded as a discrete stochastic process. In the
following, the asymptotic behavior of wi and the consistency of
the sequential testing procedure are investigated. A proof of
consistency for the case that w is discrete is provided.
A consistent estimator is defined as follows. Let n =
( w l q O w < w < l} and wT be the true probability of having a
defective chip. The estimator W, of wT is called consistent [ 161
if for every E > 0 and every wT E Q
lim Prob { ~ W , - U T I > E } =O.
n-m

(9)

Theorem 3: If the coverage function is with nonincreasing
hazard rate and the initial probability function of wI is with
mean Wl > wlow, t l ( w T ) # 0, and U: # 0, W, is a consistent
estimator of UT for every W T E a.

344

IEEE TRANSACTIONS ON COMPUTERS, VOL. 31, NO. 3. MARCH 1988
n : number of

Defect Probability

Fig. 8.

cnlps tested

U

Estimated density function of w under the optimal testing strategy.

Proof: With Wl > wlow, the optimal testing period will
never be zero. Let D and D represent the events of a detection
and no detection, respectively. The result of the nth testing is
denoted as E, where E,, E { D , D}.The probability function
of w at wT before the nth testing is .$,(aT)(i.e., Prob (U, =
wT)). Also, define 9, = {wlE,(w) > O}. Given tl(o),and E;,
i = 1 , 2 , * n - 1 , [,(aT)and W, can be determined by (4)
and (6), or (5) and (7), iteratively. Thus, the testing period of
the nth chip can be determined by (8), and the following
equation can be obtained by taking the expectation with respect
to the event occurring during the nth testing period.

approach of obtaining (lo), we have

e ,

E[En+dwr)lt1(w),El, E2,

*.*,

En-I1

Thus, the sequence t l ( w T ) , t2(wT), * is a submartingale.
Since En(wT)is the probability of w at U T , E[&&)] I1 <
00.
Based on the submartingale convergence theory [17],
(,(aT)converges in distribution and
lim E[Prob ( w , = w ~ ) ] = lim E [ , $ , ( w ~ ) ] = E [ 5 m ( w ~ ) ] .
n-m

n-m

Since t*(E,) > 0 for 1 In < 00, F(t*(t,)) > 0 for all n. We
take the expectations on both sides of (10) and let n
W . It
becomes that
-+

r

The equality exists only if 1) E[(W, - w T ) 5 = 0 as n
00,
i.e., W, satisfies (9), or 2) E[F(t*(E,))]= 0 as n
00, i.e.,
t*(E,) converges to zero or
Prob (W, E [0, wl,,]) = 1 .
In the following, we shall prove that the second condition
cannot occur when W T > wlow. Notice that with the same
-+

-+

Since the last term on the right-hand side of the equation is
always greater than zero and less than one, the sequence (W, wT), n = 1, 2,
*,
is a submartigale if Wl I UT, or a
supermartigale, otherwise. In both cases, this sequence
converges in distribution and E [ /(3, - UTI] is monotonically
nonincreasing. With Wl > wlow, wT > wlow, we have, for all n,
E[& - wlow] L (01 - wlow) if $1 IW T , or E[& - wlow] 2
(WT - wlnW), otherwise. Thus, the second condition cannot
occur.
0
Corollary 4: With the same conditions stated in Theorem 3,
except that 0 5 wT Iwlow,W, converges to wlOwin probability
and limn+..,, t*(<,) = 0.
Proof: The corollary can be shown based on Corollary 3
and the part of the proof in Theorem 3.
Theorem 3 gives a positive and strong answer of the
sequential testing procedure relating to the estimation of the
defect probability. That is, given wT E 3, Wl > wlOw, and
tl(wT) # 0, the estimated defect probability W, after the
testing of the n chips will converge to the true defect
probability with probability 1 when n approaches infinity.
Thus, the sequential testing procedure proposed not only
minimizes the testing cost of each chip, but also provides a
consistent estimator on the defect probability. In Fig. 8, we
show the expected density function of w after testing n chips
(i.e., E[t,(w)]) under the optimal testing strategy. The
coverage function is assumed to be with exponential distribution. For all three cases, wT = 0.05, 0.33, and 0.80, the
estimated w converges to the true defect probability wT. Notice
that when w is close to either 0 or 1 the uncertainty of having
defects is small. Thus, the estimates in these cases converge
rapidly.

-

345

YU et al.: VLSI TESTING STRATEGY

It should be pointed out that D is the region where the
sequential testing procedure is effective in estimating wT. If wT
< wlow,the estimated result will converge to wloW. Although
the estimated production yield, in this case, cannot converge to
the true yield, the optimal testing period in the sequential
testing procedure converges to zero, i.e., the correct testing
period is still obtained.
IV. SEQUENTIAL
BATCH
TESTING
STRATEGY
We now extend the above results of sequential testing
procedure to cover sequential batch testing in this section.
Assume that instead of testing chips one at a time, we do so in
batches of size N each. We further assume that the tester treats
all functioning chips (the chips in which no defect has been
detected yet by the test process) in a batch identically, i.e.,
testing stops for all the functioning chips in a batch at the same
time. If the production yield is known, every chip should be
tested with the testing period defined at Theorem 1. This is
independent of the batch testing or the batch size. In contrast,
when the production yield is unknown, all testing results of the
chips in one batch should interfere with the estimation of the
yield and, then, affect the optimal testing period of the
currently tested batch and subsequent batches. Thus, the
policy for sequential batch testing with unknown production
yield should differ from the policy studied in the previous
section where the batch size can be regarded as 1. The other
difference is that, when a defective chip is detected and
rejected, the testing of the remaining batch should be
continued whereas, if the batch size is one, the testing will then
be performed on the subsequent chip.
Let the events observed from the testing of a batch be D N ( t )
and DN(t),if at least one defective chip and no defective chip
is detected after the testing has been applied for a period t ,
respectively. The probability of having the events D N ( t )and
DN(t),given the defect probability is w , are 1 - (1 u F ( ~ )and
) ~(1 - u F ( ~ ) )With
~ . a prior generalized probability density function [,(a)
of U, before testing the ith batch, the
posterior density function of w, becomes

&(DN(t))instead of wi(D(t)).Lemma 2 below then allows US
to carry our results in the sequential case to the batch
processing case in determining the optimal stopping time for
the test.
Lemma 2: For all t 2 0, Wj(DN(t))
is decreasing with
respect to t and 0 5 W;(DN(t))I6;IW;(DN(t))5 1 .
Proof: Since, given w1 5 w2 and tl 5 t 2 ,

1 - UlF(t1)

1 - U2F(tl)

dw.
( wThus,
)
oi(DN(tl))
is
where A(t) = (1 - ~ F ( t ) ) ~ [ ~
greater than wi(DN(t2))in the sense of likelihood ratio [18].
This implies that w(DN(tl)) is stochastically greater than
w(DN(t2)),i.e., Prob (w(DN(tl))2 c) L Prob (w(DN(tz))1
c) for any constant c. Thus, G,(DN(t))is decreasing with
respect to t.
Based on the fact that w; is stochastically greater than
wi(D(t))and (1 - w F ( ~is) a) ~
decreasing function of U , we
have the following inequalities which lead to the second part of
the Lemma.2

L

1: (1 -(1 - ~ F ( t ) ) ~ ) [ ; ( w )
dw.

0

Theorem 4: If the coverage function is with nonincreasing
hazard rate and the prior density function of w is [;(U), the
optimal testing policy, for a batch of Nchips, is to stop testing
if no bad chip is detected after testing for the period t*([i, N)
which satisfies
t*(Wi(DN(t))) =t

if the event DN(t) or DN(t) occurs, respectively. Let
oi(DN(t))
and wi(DN(t))represent the random variables with
density functions &(U lDN(t))and [ ; ( U IDN(t)),and means
Wi(DN(t))
and Gi(DN(t)),respectively. The following lemma
and theorem for batch testing are analogous to Lemma 1 and
Theorem 2 in the previous section. We only present the proof
of the lemma. The proof of the theorem is omitted since it is
similar to that of Theorem 2.
From our assumption that all functioning chips in a batch
are treated identically by the tester, it follows that the equation
governing stopping times for the batch
is identical to that
which derives t* for the sequential case, except that we have

(13)

where t*(w) is defined in Theorem 1.
Corollary 5: Given a prior density function ti and N > 1 ,
t*([;,N ) It*([;,N - 1).
Proof: The proof is based on the facts that 1) t*(w) is
nondecreasing of w and 2) w(DN- l ( t ) )is stochastically greater
than w(DN(t)).
0
Theorem 4 indicates that testing of a batch of N should not
be continued if there is no detection after the testing period
t * ( [ i , N).
If there is a detection at t, where t < t*(E;, N),
the
The Theorem that, if a random variable X is stochastically greater than
2 E [ f (Y)] for all nondecreasing
another random variable Y , then E[f(X)]
functionsf[18], is applied here.

346

IEEE TRANSACTIONS ON COMPUTERS, VOL. 31, NO. 3, MARCH 1988

defective chip will be rejected and the testing of the remaining
N - 1 chips should be continued for at least (t*(&(oIDN-l(t)))
- t) which is always greater than zero. This is similar to
the situation in which, with the initial prior density function
&(U), the defective chip is first tested and is detected after the
test period t. Then, the testing of a batch of N - 1 is started.
The optimal testing period for the batch of N - 1, if no
detection occurs, is the optimal testing period with the prior
density function Ei(w lD(t)),i.e., t*(&(w I DN-l(t))).
With an initial density function ll(w) of w , where U : # 0,
we shall summarize the sequential batch testing procedure,
which minimizes the total testing cost of every chip, as
follows.
For testing the ith batch of N chips, i = 1, 2,

the second batch are good, then El = D 3 ,E, = D 2 ,E3 = D',
and E4 = D 3 .Let 9, be the posterior probability function of w
and a batch of K , chips be tested after the event E, - I . The
elapsed testing time of these K, chips is denoted as t,. It is
clear that { t n }is a subsequence of {v,}. Given v,(o)and E;, i
= 1, 2, * *
m - 1, vm(w), K,, and fm are all determined
according to the above procedure. Thus, the optimal testing
period following E,-, for the batch of K , chips can be
determined by Theorem 4. Taking the expectation on the event
E,, we can obtain
e ,

Eh,+

I

~ ( U T )771,

El,

* *

' 9

Em-

I

I = vrn(wT)

a ,

1. (Initialize the working parameters.) Let K = N which is the
size of batch under test and O(w) = [,(U) to describe the
information on the defect probability. Also, define f t o be
the elapsed testing time with initial value 0.
2. Calculate t*(O, K) which is the optimal testing period of
the batch or the remaining batch.
3. Perform testing on the batch or the remaining batch until
a. no defective chip is detected before the elapsed testing
time reaches t*(O, K). Then, all chips in the batch or
the remaining batch pass the test. The posterior density
function of U;, which will be used as the prior density of
ai+ is calculated using (12) based on O(w), K, and the
elapsed testing time;
b. a chip is found to be defective at the time where f <
t*(O, K). Then, O(w) is updated to wO(w)/W, where W is
the mean based on density hnction O(w), and K is set to
K - 1. If K = 0, set E;+ l(w) to the updated O(w) and
begin to test the next batch; otherwise, testing of the
remaining batch of K - 1 is continued from step 2
above. (Note that the elapsed testing time Twill not be
reset until the testing of a new batch starts.)

<

The above procedure for batch testing also has the properties of persistence and consistence. The proof for persistence
is as in Corollary 3, with DN(t*)replacing D(t*). The proof
for consistency for a discrete w is given below.
Theorem 5: If the coverage function is with nonincreasing
hazard rate and the initial probability function of w , w l , is with
mean Wl > wlOwand U: > 0, the estimator Gfl of the true defect
probability, U T , is consistent for every wlow < wT < 1,
provided that t l ( w T ) # 0.
Proof: Our strategy will be to show that { t n }is a
subsequence of {v,} (to be defined below) in which W,
converges to U T in probability.
The sequence 7(),
is generated as follows. Set v1 = E l .
Using the procedure mentioned above, the testing periods and
probability function of w are calculated when batches are
tested sequentially. The events which occurred could be a
detection or no detection when a batch of r chips is tested,
which are denoted as D r and Dr, r = 1, 2,
N,
respectively. The events which occurred are numbered as E l ,
E,,
E,,
whereE, E { D r , D r l r = 1, 2 ,
N}.
For example, given N = 3, if there are two detections of bad
chips during the testing of the first batch and all three chips of
e ,

- . e ,

e . . ,

e . . ,

where gm(w) is defined by

and

E[gm

gm (a)v m (U)

=
tlm(4fO

Note that g,(w) is a single value function of w if t*(y,, K,) >
f, and indicates the probability of having no detection of bad
chip during [0, t*(vm,K,)] conditioned on that no bad chip in
the batch of K, chips is detected during [0, f , ] . Clearly, the
sequence vl(wr), v2(oT),. . * , is a submartingale and converges in distribution. In addition, we can have

E[E[grn+1(w)I-grn(0~)1771,El, E,,

* * . )

Em-11

Notice that the last term of the above equation is always less
than 1 and greater than 0. By a process analogous to that used
in Theorem 3, we conclude that the testing period of a new
batch of N chips can never be zero (i.e., F(f*(ym,N))# 0).
On the other hand, from Lemma 2 and Corollary 1, the
optimal testing period of a remaining batch is prolonged after a
detection. Thus, we have C*(vm, K,) > f, for all m. This
inequality and (14) imply that lim,+- E[@, - UT)'] = 0 .
So, the conclusion of consistency follows.
0

V. DISCUSSION
In this paper, we have presented optimal testing strategies
for both cases of a given yield and unknown yield. The design
goal of testing procedures is to minimize the total testing cost
which includes both the cost of testing process and the penalty
of incomplete testing. When the production yield is unknown,
the sequential testing procedures take the accumulated information in the process of testing into account for modifying the
testing process. So the production yield can be estimated online and the cost of testing is minimized based on the best
knowledge of the production yield. The results, in practice,
will be of use whether the yield stays constant or varies from
one manufacturing run to another. Theoretically, the approach

347

YU et al.: VLSI TESTING STRATEGY

shows a way to integrate the processes of information
collection and adaptive optimization.
There are many interesting and useful extensions to this
work. One of these is the idea of testing with recall. Suppose
that one did not subject a device to one continuous testing, but
instead could recall it for further bouts of testing, based on the
information received by testing other devices in the interim
period. It would be interesting to study under what conditions
this new procedure is superior to the strategies presented here.
Another problem is to obtain the optimal test length if the total
number of chips to be tested is known. This problem differs
from the present one in that not only do we have to take into
account the influence of past test results (as we do here) but
also the effect of refining our estimate by testing the (current)
batch on the expected cost of testing the remaining chips. One
way to solve this problem would be dynamic programming.

REFERENCES
D. P. Siewiorek and L. K-W. Lai, “Testing of digital systems,” Proc.
IEEE, vol. 69, pp. 1321-1333, Oct. 1981.
R. A. Rasmussen, “Automated testing of LSI,” Computer, vol. 15,
pp. 69-78, Mar. 1982.
T. W. Williams, “VLSI testing,” Computer, vol. 17, pp. 126-136,
Oct. 1984.
T. W. Williams and N. C. Brown, “Defect level as a function of fault
coverage,” IEEE Trans. Comput., vol. C-30, pp. 987-988, Dec.
1981,
T. W. Williams, “Sufficient testing in a self-testing environment,” in
Proc. Inr. Test Conf.,1984, pp. 167-172.
J. J. Shedletsky and E. J. McCluskey, “The error latency of a fault in a
combinational digital circuit,” in Proc. Fault-Tolerant Comput.
Symp., 1975, pp. 210-214.
J. J. Shedletsky, “Random testing: Practicality vs. verified effectiveness,” in Proc. Fault-Tolerant Comput. Symp., 1977, pp. 175-179.
J. E. Smith, “Measures of the effectiveness of fault signature
analysis,” IEEE Trans. Comput., vol. C-29, pp. 510-514, June
1980.
T. W . Williams and E. B. Eichelberger, “Random patterns within a
structured sequential logic design,” in Proc. Int. Test Conf., 1977,
pp. 19-26.
C. Timoc, F. Stott, K. Wickman, and L. Hess, “Adaptive self-test for
microprocessor,” in Proc. Int. Test Conf.,1983, pp. 701-703.
V. D. Agrawal and P. Agrawal, “An automatic test generation system
for Illiac IV logic boards,” IEEE Trans. Comput., vol. C-21, pp.
1015-1017, Sept. 1972.

[I21 V. D. Agrawal, “An information theoretic approach to digital fault
testing,” IEEE Trans. Comput., vol. C-30, pp. 582-587, Aug. 1981.
1131 V. D. Agrawal, S. C. Seth, and P. Agrawal, “Fault coverage
requirement in production testing of LSI circuits,” IEEE J. SolidState Circuits, vol. SC-17, pp. 57-61, Feb. 1982.
[14] S. C. Seth and V. D. Agrawal, “Characterizing the LSI yield equation
from wafer test data,” IEEE Trans. Cornput.-Aided Design, vol.
CAD-3, pp. 123-126, Apr. 1984.
[IS] M. H. DeGroot, Optimal Statistical Decisions. New York: McGraw-Hill, 1970.
[16] H. Chernoff, Sequential Analysis and Optimal Design. SIAM,
1972.
1171 J. L. Doob, Stochastic Processes. New York: Wiley, 1953.
[I81 S. M. Ross, Stochastic Processes. New York: Wiley, 1983.

Philip S. Yu (S’76-M’78-SM’87), for a photograph and biography, see this
issue, p. 320.

C. M. Krishna (S’78-M’84) received the B Tech
degree from the Indian Institute of Technology,
Delhi, in 1979, the M S degree from Rensselaer
Polytechnic Institute, Troy, NY, in 1980, and the
Ph D degree from the University of Michigan, Ann
Arbor, in 1984, all in electrical engineering
Since September 1984, he has been on the faculty
of the Department of Electrical and Computer
Engineering, University of Massachusetts, Amherst His research interests include reliability
modeling, queueing and scheduling theory, and
distributed architectures and operating systems

-

Yann-Hang Lee (S’81-M’84) received the B.S.
degree in engineering science and the M.S. degree
in electrical engineering from National Cheng Kung
University, in 1973 and 1978, respectively, and the
Ph.D. degree in computer, information, and control
engineering from the University of Michigan, Ann
Arbor, in 1984.
Since December 1984, he has been with IBM
Thomas J. Watson Research Center, Yorktown
Heights, NY. His current research interests include
distributed computer systems, database processing,
fault-tolerant computing, VLSI testing, performance evaluation, and stochastic decision theory.

Partition Scheduling In APEX Runtime Environment for Embedded Avionics
Software
Yann-Hang Lee and Daeyoung Kim
Real Time Systems Research Laboratory
CISE Department, University of Florida
Gainesville, FL 32611, USA
fyhlee, dkimg@cise.ufl.edu
Abstract
Advances in the computer technology encouraged the
avionics industry to replace the federated design of control units with an integrated suite of control modules that
share the computing resources. The new approach, which
is called integrated modular avionics (IMA), can achieve
substantial cost reduction in the development, operation
and maintenance of airplanes. A set of guidelines has
been developed by the avionics industry to facilitate the development and certification of integrated systems. Among
them, a software architecture is recommended to address
real-time and fault-tolerance requirements. According to
the architecture, applications are classified into partitions
supervised by an operating system executive. A generalpurpose application/executive (APEX) interface is defined,
which identifies the minimum functionality provided to the
application software of an IMA system. To support the temporal partitioning between applications, APEX interface
requires a deterministic cyclic scheduling of partitions at
the O/S level and a fixed priority scheduling among processes within each partition. In this paper, we propose a
scheduling scheme for partitions in APEX. The scheme determines the frequency that each partition must be invoked
and the assignment of processor capacity on every invocation. Then, a cyclic schedule at the O/S level can be constructed and all processes within each partition can meet
their deadline requirements.

1. Introduction
Advancements in technology have enabled the avionics
industry to develop new design concepts, which result in
highly integrated software-controlled digital avionics. The
new approach, referred to as Integrated Modular Avionics (IMA), introduces methods that can achieve high lev-

Mohamed Younis and Jeff Zhou
Advanced System Technology Group
AlliedSignal
Columbia, MD 21045, USA
fyounis, zhoug@batc.allied.com

els of reusability and cost effectiveness compared to the
existing federated implementations of avionics [2]. The
IMA approach, based on the concept of partitioning, utilizes
standardized modules in building functional components of
avionics systems. While integration enables resource sharing, some boundaries need to be enforced to maintain system predictability and to prevent bringing down the whole
system because of a failure of a single function, either due
to generic faults or due to missed deadlines.
Failure containment is very crucial for the integrated
environment to guarantee that a faulty component cannot
cause other components to fail and to risk generating a total system failure. For instance, in a passage jet, a miss of
task deadlines in the entertainment subsystem must not negatively influence any critical flight control subsystems such
as the flight manager. An IMA system is called strongly
partitioned if the boundaries between the integrated functions are clearly defined so that a faulty function cannot
interfere with or cause a failure in any other functions.
Strong functional partitioning facilitates integration, validation, and certification. Following the IMA guidelines, the
cost of both development and maintenance is expected to
decrease because of mass production of the building blocks,
lower levels of spares, and reduced certification costs.
A software architecture is recommended among the IMA
guidelines to address real-time and fault-tolerance constraints for the integrated environment [4]. In the architecture, applications are classified into partitions supervised
by an operating system executive. Within each partition,
a partition executive manages application tasks and intrapartition task communication. To communicate across partition boundaries, a message mechanism provided by the
operating system executive must be invoked. The operating
system executive manages processor sharing among partitions. A general-purpose application/executive (APEX) interface is defined in [4], which identifies the minimum functionality provided to the application software of an IMA

system. To support the temporal partitioning between applications, APEX interface requires a deterministic cyclic
scheduling of partitions at the operating system level and a
fixed priority scheduling among tasks within each partition.
Thus, the tasks in one partition can only be executed during
the fixed partition window allocated to the partition and a
task overrun cannot cross the partition window boundaries.
This scheduling approach not only restrains a task failure
within the partition, but also facilitates system upgrade and
the integration of additional functions without the need to
reconfigure the whole system.
Apparently, task execution in each application partition
is affected by the two-level scheduling structure of APEX
that consists of a low-level cyclic schedule at the operating system level and a high level fixed priority schedule at
partition executive level. A different two-level hierarchical scheduling scheme was proposed by Deng and Liu in
[7]. The scheme allows real- time applications to share resources in an open environment. The scheduling structure
has an earliest-deadline-first (EDF) scheduling at the operating system level. The second level scheduling within each
application can be either time-driven or priority-driven. For
acceptance test and admission of a new application, the
scheme analyzes the application schedulability at a slow
processor. Then, the server size is determined and server
deadline of the job at the head of ready queue is set at runtime. Since the scheme does not rely on fixed allocation
of processor time or fine-grain time slicing, it can support
various types of applications, such as release time jitters,
non-predictable scheduling instances, and stringent timing
requirements.
In this paper, we are looking into the single processor
scheduling issues for partitions in APEX environment. In
[6], Audsley and Wellings discussed the characteristics of
avionics applications under the APEX interface. They provide a recurrent solution to analyze task response time in an
application domain and show that there is a potential for a
large amount of release jitter. However, the paper does not
address the issues of constructing cyclic schedules at the
operating system level. To remedy the problem, our first
step is to establish scheduling requirements for the lower
level cyclic schedules such that task schedulability under
the given high level fixed priority schedules in each partition can be ensured. The approach we adopt is similar to
the one in [7] of comparing the task execution in APEX environment with that at a slow processor. The cyclic schedule
then tries to allocate partition execution intervals by stealing task inactivity periods. This stealing approach resembles the slack stealer for scheduling soft-aperiodic tasks in
fixed priority systems [9]. Once the schedulability requirements are obtained, suitable cyclic schedules can be constructed. Following the partitioning concept of IMA, the
operating system level cyclic schedule is flexible to support

Partition 1

Partition 2

Partition 3

Partition 4

APEX
Operating System
(scheduler, exception handling, health monitor,
logical communications, partitioning)
COEX
Hardware Interface System
(device driver, interrupt handler, memory management,
context switching, BIT)

Hardware
(processor, MMU, clock, interrupt controller,
communication media, BITE)

Figure 1. APEX software architecture
system upgrade and integration. It is designed in a way that
no complete revision of scheduling algorithms is required
when the workload or application tasks in one partition are
modified.
In the following section, we give a brief overview of the
APEX software architecture on which applications are executed in separate partitions. In Section 3, we focus on the
two-level scheduling mechanism and give the main theorem
on the schedulability requirements. A numerical example is
presented to illustrate the schedulability requirements with
respect to various task characteristics. Then, in the same
section, we show how to use the requirements to construct
the cyclic schedules in the operating system level. Finally,
a short conclusion follows in Section 4.

2. APEX Architecture: An Overview
As shown in Figure 1, the IMA software architecture
consists of hardware interface system, operating system,
and application software [4]. The hardware interface system
is comprised of device driver, interrupt handler, memory
management unit, and context switch function. Combining with the hardware in a cabinet, they form a core module
to support operating system and application execution via
the COEX (core-executive) interface. The operating system
executive implements a set of services defined in the APEX
(application-executive) interface such that application software can use the system resource and control the scheduling
and communication between its internal processing tasks.
One of the objectives to be achieved with the APEX interface is portability. With the standardized APEX interface, application software can be developed without regard
to hardware implementations and configurations. Also, a
hardware change can be transparent to application software.
The other important objective is to set up an execution environment such that the applications of multiple criticalities
can be integrated. This requires a robust partition such that

an application cannot affect any other applications in terms
of the content in their address spaces and the allocated execution times.
In order to support the spatial partition between applications, the tasks of an application are running in a predetermined area of memory. Any memory accesses (at least,
write access) outside of a partition’s defined area are prohibited by memory management unit. For temporal partition,
the operating system maintains a major time frame during
which each partition is activated in one or more scheduled
partition windows. In other word, through a predefined configuration table of partition windows, the APEX impose a
cyclic scheduling to ensure that each partition receives an
fixed amount of processing time. This arrangement of a
cyclic schedule can also allow message-driven executions.
The IMA architecture assumes the existence of the ARINC 659 backplane bus [3] and the ARINC 629 data bus
[1] for communications between core modules in a cabinet
and between cabinets. Since both buses provide periodic
message windows, message transmissions and partition activation windows can be synchronized to reduce end-to-end
delays.
Within each application partition, the basic execution
unit is a process or task. Tasks in a partition share the resources allocated to the partition and can be executed concurrently with other tasks of the same partition. A task comprises the executable program, data and stack areas, process
state, and entry point. Its attributes also include base and
current priorities, time capacity, and deadline. The scheduling algorithm within each partition is based on priority preemption. When the operating system determines to initiate
a new partition window according to it cyclic schedule, the
executing task in the old partition window is preempted. On
the other hand, in the newly activated partition, the ready
task with the highest current priority among those in the
partition is selected to run. In addition, the operating system
provides messaging mechanisms for inter-partition communication, and time management mechanisms to release periodic tasks and to detect missed deadlines.

3. Partition Scheduling in APEX Architecture
In this section, we consider the scheduling requirements
for a partition server, Sk , which executes the tasks of an
application partition Ak according to a fixed priority preemptive scheduling algorithm and shares the processing capacity with other partition servers in the operating system
level. Let the application Ak consist of 1 ; 2 ; : : : ; n tasks
. Each task i is invoked periodically with a period Ti and
takes a worst-case execution time (WCET) Ci . Thus, the
total processor utilization demanded by the application is
k = ni=1 CTii . Also, upon each invocation, the task i
must be completed before its deadline period Di , where

P

τ1

τ2

τ2 τ3 τ1

τ1 τ3 idle

τ2 τ1 τ2

αkηk
ηk

Figure 2. Task/partition scheduling in APEX

Ci  Di  Ti .

At the system level, the partition server Sk is scheduled
periodically with a fixed period. We denote this period as
the partition cycle, k . For each partition cycle, the server
can execute the tasks in Ak during an interval k k where
k  1 and is called partition capacity. For the remaining interval of (1 k )k , the server is blocked. It is our
objective to find a pair of parameters k and k such that
the tasks of Ak meet their deadlines. In Figure 2, we show
an example execution sequence of a partition that consists
of three tasks. During each partition cycle, the tasks are
scheduled to be executed for a period of k k . If there is no
active task in the partition, the processor is idle and cannot
run any active tasks from other partitions.

3.1. Partition Schedulability Requirements
As required in the APEX specification [4], the processes
(or tasks) of each partition are running under a fixed priority preemptive scheduling. Suppose that there are n tasks
in Ak listed in priority ordering 1 < 2 <    < n
where 1 has the highest priority and n is the lowest. In
order to evaluate the schedulability of the partition server
Sk , let’s consider the case that Ak is executed at a dedicated
processor of speed k , normalized with respect to the processing speed of Sk . Based on the necessary and sufficient
condition of schedulability in [10, 8], task i is schedulable if there exists a t 2 Hi = flTj jj = 1; 2; : : : ; i; l =
1; 2; : : : ; bDi =Tj cg [ fDi g, such that

Wi (k ; t) =

Xi Cj d t e  t:
j =1 k

Tj

The expression Wi (k ; t) indicates the worst cumulative
execution demand on the processor made by the tasks with
a priority higher than and equal to i during the interval
[0; t]. We now define Bi (k ) = maxt2Hi ft
Wi (k ; t)g
and B0 (k ) = mini=1;2;:::;n Bi (k ). Note that, when i is
schedulable, Bi (k ) represent the total period in the interval [0; Di ] that the processor is not running any tasks with a
priority higher than or equal to that of i . It is equivalent to
the level-i inactivity period in the interval [0; Di ] [9].
By comparing the task executions at server Sk and at a
dedicated processor of speed k , we can obtain the following theorem:

Theorem 1 The application Ak is schedulable at server Sk
that is with a partition cycle k and a partition capacity k ,
if
a) Ak is schedulable at a dedicated processor of speed
k , and
b) k  B0 (k )=(1 k )
Proof : The task execution at server Sk can be modeled by tasks 1 ; 2 ; : : : ; n of Ak and an extra task 0
that is invoked every period k and has an execution time
C0 = (1 k )k . The extra task 0 is assigned with the
highest priority and can preempt other tasks. We need to
show that, given the two conditions, any task i of Ak can
meet its deadline even if there are preemptions caused by
the invocations of task 0 . According to the schedulability
analysis in [10, 8], task i is schedulable at server Sk if there
is a t 2 Hi [ Gi , such that

Xi C d t e
j =1

j

Tj

+

C0 d

t
et
k

, where Gi = flk jl = 1; 2; : : : ; bDi =k cg.
If i is schedulable on a processor of speed k , there
Wi (k ; ti ) 
exists a ti 2 Hi such that Bi (k ) = ti
B0 (k )  0 for all i = 1; 2; : : : ; n. Note that Wi (k ; ti ) is
a non-decreasing function of ti . Assume that ti = mk + Æ ,
where Æ < k . If Æ  B0 (k ),

Xi C d ti e
j =1

j



Tj

+

t
C0 d i e
k

=

k Wi (k ; ti ) + (m + 1)C0

 k (ti B0 (k )) + (m + 1)C0

k (ti B0 (k )) + (m + 1)(1 k )k
k (ti B0 (k )) + (1 k )(ti Æ) + B0 (k )
ti + (1 k )(B0 (k ) Æ)
ti

=


=



The above inequality implies that all tasks i are schedulable at server Sk . On the other hand, if Æ < B0 (k ), then,
0
at ti = mk < ti , we have

Xi C d ti e
0

j =1

j

Tj

t
C0 d i e
k
0

+

 k (ti B0 ()) + mC0
 k (ti Æ) + m(1 k )k
=
=

Since ti
Sk . 2
0

k ti + (1 k )ti
ti
0

0

0

2 Gi , the application Ak is schedulable at server

When we compare the execution sequences at server Sk
and at the dedicated processor, we can observe that, at the
end of each partition cycle, Sk has put the same amount of
processing capacity to run the application tasks as the dedicated processor. However, if the tasks are running at the
dedicated processor, they are not blocked and can be completed earlier within each partition cycle. Thus, we need an
additional constraint to bound the delay of task completion
at server Sk . This bound is set by the second condition of
the Theorem and is equal to the minimum inactivity period
before each task’s deadline.
An immediate extension of Theorem 1 is to include the
possible blocking delay due to synchronization and operating system overheads. Assume that the tasks in the partition adopt a priority ceiling protocol to access shared objects [12]. The blocking time is bounded to the longest
critical section in the partition that accesses the shared objects. Similarly, additional delays caused by the operating
system level can be considered. For instance, the partition
may be invoked later than the scheduled moments since the
proceeding partition just enters an O/S critical section. In
this case, we can use the longest critical section in the operating system level to bound this scheduling delay. These
delay bounds can be easily included into the computation of
Wi (k ; ti ).

3.2. A Scheduling Example
Theorem 1 provides a solution to determine how frequent a partition server must be scheduling at the O/S level
and how much processor capacity it should use during its
partition cycle. It is easy to see that B0 (k ) and k are
increasing functions of k . This implies that if more processor capacity is assigned to a partition during its partition
period, the tasks can still meet their deadlines even if the
partition cycle increases. To illustrate the result of Theorem
1, we consider an example in Table 1 in where four application partitions are allocated in a core module. Each partition
consists of several periodic tasks and the corresponding parameters of (Ci ; Ti ) are listed in the Table. Tasks are set
to have deadlines equal to their periods and are scheduled
within each partition according to a rate-monotonic algorithm [11]. The processor utilization demanded by the 4
partitions, k , are 0.25, 0.15, 0.27, and 0.03, respectively.
Following Theorem 1, the minimum level-i inactivity
period is calculated for each partition and for a given capacity assignment k , i.e., B0 (k ) = mini maxt2Hi (t
i Cj d t e). The resulting inactivity periods are plotj =1 k Tj
ted in Figure 3 for the four partitions. It is easy to see that,
when k is slightly larger than the processor utilization, the
tasks with a low priority (and a long period) just meet their
deadlines, and thus have a small inactivity period. On the
other hand, when  is much larger than the processor uti-

P

tasks
(Ci ; Ti )

Partition 2
(util.=0.15)
(2,50)
(1,70)
(8,110)
(4,150)

Partition 3
(util.=0.27)
(7,80)
(9,100)
(16,170)

Partition 4
(util.=0.03)
(1,80)
(2,120)

500
Partition 1
Partition 2
Partition 3
Partition 4

450
400
350

Partition Cycle

Partition 1
(util.=0.25)
(4,100)
(9,120)
(7,150)
(15,250)
(10,320)

300
250
200
150

Table 1. Task parameters for the example partitions

100
50
0
0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

alpha

Figure 4. The maximum partition cycles for
different processor capacity assignments

120
Partition 1
Partition 2
Partition 3
Partition 4

Mininum Inactivity Period

100

80

60

40

20

0
0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

alpha

Figure 3. Inactivity periods of the exmple partitions

lization of the partition, the inactivity period is bounded to
the smallest task period in each partition. This is due to
the fact that the tasks with a short period cannot accumulate
more inactivity period before their deadlines. The curves in
the figure also show that an increase of k after the knees
wouldn’t make the inactivity periods significantly longer.
In Figure 4, the maximum partition cycles are depicted
with respect to the assigned capacity k . If the points below
the curve are chosen to set up cyclic scheduling parameters
for each partition at O/S level, then the tasks in the partition
are guaranteed to meet their deadlines. For instance, the
curve for partition 2 indicates that, if the partition receives
28% of processor capacity, then its tasks are schedulable as
long as its partition cycle is less than or equal to 59 time
units. Note that the maximum partition cycles increase as
we assign more capacity to each partition. This increase is
governed by the accumulation of inactivity period when k
is small. Then, the growth follows by a factor of 1=(1 k )
for a larger k .
Figure 4 suggests possible selections of (k ; k ) for the 4
partitions subject to a total assignment of processor capacity

not greater than 1. From the Figure, a feasible assignment
for (k ; k ) is (0.32, 36), (0.28, 59), (0.34, 28), and (0.06,
57), respectively. In the following subsection, we shall discuss the approaches of using the feasible pairs of (k ; k )
to construct cyclic schedules in the APEX O/S level.
To reveal the properties of the parameters (k ; k ) under
different task characteristics, we present two evaluations on
the task set of Partition 1 in Table 1. We first alter the task
deadlines such that Di is equal to 1:0Ti ; 0:8Ti ; 0:6Ti ; and
0:4Ti for all tasks. The tasks are then scheduled according to the deadline monotonic algorithm within the partition [5]. The maximum partition cycles are plotted in Figure 5. As the deadlines become tighter, the curves shift to
the low-right corner. This change suggests that either the
partition must be invoked more frequently or should be assigned with more processor capacity. For instance, if we fix
the partition cycle at 56 time units and reduce the task deadlines from 1:0Ti to 0:4Ti , then the processor capacity must
be increased from 0.34 to 0.56 to guarantee the schedulability. The result of demanding more processor capacity for
tasks with tight deadlines is similar to the channel bandwidth reservation for real-time messages in ATM network.
If messages of a channel have a tight deadline, a significant
bandwidth must be reserved while the message transmission
is multiplexed with other channels.
The second experiment, shown in Figure 6, concerns
with the partition schedulability under different task execution times. Assume that we upgrade the avionics cabinets
with a high speed processor. Then, the application partitions are still schedulable even if we allocate a less amount
of processor capacity or extend partition cycles. In Figure 6,
we show the maximum partition cycles for Partition 1 of Table 1. By changing task execution times proportionally, the
processor utilizations are set to 0.25, 0.20, 0.15 and 0.1. It

500

350
D=1.0 * T
D=0.8 * T
D=0.6 * T
D=0.4 * T

450
400

250

Partition Cycle

350

Partition Cycle

util.=0.25
util.=0.20
util.=0.15
util.=0.10

300

300
250
200
150

200

150

100

100
50
50
0

0
0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

alpha

Figure 5. The maximum partition cycles for
partition 1 under different task deadlines

is interesting to observe the improvement of schedulability
when k is slightly larger than the required utilization. For
instance, assume that the partition cycle is set to 56. Then,
for the 4 different utilization requirements, the processor capacities needed for schedulability can be reduced from 0.34,
to 0.28, 0.212, and 0.145, respectively. On the other hand,
once a more than sufficient capacity is assigned to the partition, the maximum partition cycle is relatively independent
of the processor utilization, and is mainly affected by task
deadlines and periods.

3.3. Cyclic Schedule at the APEX O/S Level
Given the schedulability requirement of (k ; k ) for
each partition server Sk , a cyclic schedule must be constructed at the APEX O/S level. Notice that the pair of parameters (k ; k ) indicates that the partition must receive
an k amount of processor capacity at least every k time
units. The execution period allocated to the partition needs
not to be continuous, or to be restricted at any specific instance of a scheduling cycle. This property makes the construction of the cyclic schedule extremely flexible. In the
following, we will use the example in Table 1 to illustrate
two simple approaches.
1. Unique partition cycle approach: In this approach,
the O/S schedules every partition in a cyclic period equal
to the minimum of k and each partition is allocated an
amount of processor capacity that is proportional to k . For
instance, in the example of Table 1, a set of feasible assignment of (k ; k ) can be transferred from f(0.32, 36), (0.28,
59), (0.34, 28), (0.06, 57)g to f(0.32, 28), (0.28, 28), (0.34,
28), (0.06, 28)g. The resulting cyclic schedule is shown in
Figure 7(a) where the O/S invokes each partition every 28

0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

alpha

Figure 6. The maximum partition cycles for
partition 1 under different processor utilizations

time units and allocates 8.96, 7.84, 9.52, and 1.68 time units
to partitions 1, 2, 3, and 4, respectively.
2. Harmonic partition cycle approach: When partition
cycles are substantially different, we can adjust them to
form a set of harmonic cycles in which j is a multiple of
i , if i < j for all i and j. Then, the O/S cyclic schedule runs repeatedly every major cycle which is equal to the
maximum of k . Each major cycle is further divided into
several minor cycles with a length equal to the minimum of
k . In Figure 7(b), the cyclic schedule for the example is
illustrated where the set of (k ; k ) is adjusted to f(0.32,
28), (0.28, 56), (0.34, 28), (0.06, 56)g. The major and minor cycles are 56 and 28 time units, respectively. Note that,
partitions 2 and 4 can be invoked once every major cycle.
However, after the processing intervals for partitions 1 and
3 are assigned in every minor cycle, there doesn’t exist a
continuous processing interval of length 0.28x56 time units
in a major cycle. To schedule the application tasks in partition 2, we can assign an interval of length 0.22x28 in the
first minor cycle and the other interval of length 0.34x28
in the second minor cycle to the partition. This assignment
meets the requirement of allocating 28% of processor capacity to the partition every 56 time units.
Comparing Figures 7(a) and 7(b), there are a less number of context switches in the harmonic partition cycle approach. The reduction could be significant if there are many
partitions with different partition cycles. In such a case, an
optimal approach of constructing a set of harmonic cycles
and assigning processing intervals should be sought in order to minimize the number of context switches. The other
modification we may consider is that, when a partition cycle

A1

A2

A3 A4 A1

A2

A3 A4 A1

A2

A3 A4 A1

A2

A3

A4

(a) Unique partition cycle
A1

A3

A4 A2

A1

A3

A2

A1

A3

A4 A2

A1

A3

A2

(b) Harmonic partition cycle

Figure 7. Example cyclic schedules at the O/S
level

is reduced to fit in the cyclic schedule, the originally allocated capacity becomes more than sufficient. We can either
redistribute the extra capacity equally to all partitions or to
keep the same allocation in the partition for future extensions.
In addition to the flexible cycle schedules, the choice of
k is adaptable as long as the sum of all k is less than
or equal to 1. The parameter k must be select to ensure
partition schedulability at a dedicated processor of speed
k . For instance, if the task set is scheduled according to
a rate monotonic algorithm, we can define a minimum capacity equal to k =n(21=n 1) which guarantees the partition schedulability. Additional capacity can be assigned to
the partition such that the partition cycle can be prolonged.
In the case that new tasks are added into the partition and
the modified task set is still schedulable with the original
capacity assignment, we may need to change the partition
cycle and construct a new cyclic schedule at the O/S level.
However, as long as the new cyclic schedule is subject to
the requirement of (k ; k ) for each partition, no change to
other partitions is necessary to ensure their schedulability.

4. Conclusion
The partition scheduling in APEX environment is described in this paper. To guarantee the schedulability of
fixed priority algorithms at each partition, the necessary requirements for the cyclic schedule at the operating system
level are established. Represented by partition cycle k and
partition capacity k , the requirements can be easily used
to construct flexible cyclic schedules. In addition, as long
as the requirements can be satisfied for a partition, a modification of the task priority or task characteristics in any
partition does not require a global change of scheduling algorithms.
For simplicity, we exclude several factors that may introduce additional delays to task execution in the derivation of
k , and k . As in [6], the factors are listed as task blocking delay, partition delay (e.g., due to the critical sections at
the O/S level), task release jitter, and operating system overheads (such as interrupt serving, context switch, etc.) Note
that, since blocking delay and release jitter depend upon the

tasks within the partition, their delay effect can be included
in the computation of task inactivity periods and task deadlines. At the operating system level, the partition delay can
be deducted from the allocated processor capacity. To count
for the operating system overheads, we can model them as a
linear function of processing interval if the minimum interoccurrence time is known. Then, a portion of the allocated
processor capacity can be taken away from each partition.
We are currently extending the proposed scheme to address the scheduling issues of multiple processors and data
transmission on the ARINC 659 backplane bus [3] and ARINC 629 inter-cabinet network [1]. The task is challenging
as we aim at task and message scheduling to meet the endto-end timing requirements of the system. The scheduling
requirements devised in this paper can be used to define a
promising base such that additional scheduling constraints
can be incorporated.

References
[1] Multi-transmitter Data Bus, ARINC Specification 629.
Aeronautical Radio Inc., Annapolis, MD, October 1990.
[2] Design Guide for Integrated Avionics, ARINC report 651.
Aeronautical Radio Inc., Annapolis, MD, November 1991.
[3] Backplane Data Bus, ARINC Specification 659. Aeronautical Radio Inc., Annapolis, MD, December 1993.
[4] Avionics Application Software Standard Interface, ARINC
Specification 653. Aeronautical Radio Inc., Annapolis, MD,
January 1997.
[5] N. Audsley, A. Burns, M. Richardson, and A. Wellings.
Hard real-time scheduling: the deadline-monotonic approach. Eighth IEEE Workshop on Real-time Operating Systems and Software, pages 133–137, 1991.
[6] N. Audsley and A. Wellings. Analysing apex applications.
Proc. IEEE Real-Time Systems Symposium, pages 39–44,
Dec. 1996.
[7] Z. Deng and J. W.-S. Liu. Scheduling real-time applications in an open environment. Proc. IEEE Real-Time Systems Symposium, pages 308–319, Dec 1997.
[8] J. Lehoczky. Fixed priority scheduling for periodic task sets
with arbitrary deadlines. Proc. IEEE Real-Time Systems
Symposium, pages 201–209, Dec. 1990.
[9] J. Lehoczky and S. Ramos-Thuel. An optimal algorithm for
scheduling soft-aperiodic tasks in fixed-priority preemptive
systems. Proc. IEEE Real-Time Systems Symposium, pages
110–123, Dec. 1992.
[10] J. Lehoczky, L. Sha, and Y. Ding. The rate-monotonic
scheduling algorithm: exact characteristics and average case
behavior. Proc. IEEE Real-Time Systems Symposium, pages
166–171, Dec. 1989.
[11] C. L. Liu and J. W. Layland. Scheduling algorithms for
multiprogramming in hard real time environment. J. Assoc.
Comput. Mach., 20(1):46–61, 1973.
[12] L. Sha, R. Rajkumar, and J. P. Lehoczky. Priority inheritance
protocols: An approach to real-time synchronization. IEEE
Trans. on Computers, 39(9):1175–1185, Sep. 1990.

SOCA (2012) 6:65–79
DOI 10.1007/s11761-011-0086-7

ORIGINAL RESEARCH PAPER

Service-oriented smart home applications: composition,
code generation, deployment, and execution
Wu Li · Yann-Hang Lee · Wei-Tek Tsai ·
Jingjing Xu · Young-Sung Son · Jun-Hee Park ·
Kyung-Duk Moon

Received: 1 May 2010 / Revised: 18 July 2011 / Accepted: 19 July 2011 / Published online: 14 September 2011
© Springer-Verlag London Limited 2011

Abstract A smart home usually has a variety of devices
or home appliance, instead of designing software for a specific home, this paper proposes a service-oriented framework
with a set of ontology systems to support service and device
publishing, discovery of devices and their services, composition of control software using existing control services that
wrap devices, deployment, and execution of the composed
service in an computing environment, monitoring the execution, and recovery from device failure. The ontology systems
specify semantic information about devices, services, and
workflows used in various smart home, and users can compose and recompose services for their specific needs. New
devices, workflows, and services can be added into ontology.
Most of the steps in this process can be automated including
code generation. For example, service composition will be
carried out in three steps: abstract workflow design, function
construction, and device discovery, and different codes can
be generated for different computing platforms such as Java
and Open Services Gateway initiative environments. In this
way, a variety of smart home can be constructed rapidly using
the framework by discovery and composition using existing
services and workflows. This paper illustrates this framework using a media control example to illustrate the ontology,
discovery, composition, deployment, execution, monitoring,
and recovery.

Keywords Ontology · Smart home · Composition ·
Code generation · Service-oriented

W. Li (B) · Y.-H. Lee · W.-T. Tsai · J. Xu
School of Computing, Informatics, and Decision Systems
Engineering, Arizona State University, Tempe, AZ 85287, USA
e-mail: Wu.li@asu.edu
W.-T. Tsai
Department of Computer Science and Technology,
Tsinghua University, Beijing, China

1. Integrating devices in a smart home to compose complex and intelligent control services (interoperable with
workflows and other services via various standards);
2. Discovering devices and their control services in a smart
home;
3. Executing services and recovering them in case that these
services or devices fail.

Y.-S. Son · J.-H. Park · K.-D. Moon
Electronics and Telecommunications Research Institute,
Daejeon, South Korea

The first issue can be addressed by adopting a serviceoriented computing (SOC ) by hiding the underlying

1 Introduction
Smart home [1,2] is an interesting application of distributed
embedded systems where digital appliances, sensors, PDAs,
and handheld computers are integrated to provide e intelligent services. Differing from statically deployed applications
in traditional embedded systems [3] designed by embedded
system experts, intelligent services in a smart home allow
users who have limited computing knowledge to compose
software services to control appliances. In this way, instead
of taking days and months to install and deploy new control
software by embedded system experts, end users can compose these services and execute these services promptly.
For example, a temperature control service that manages
the temperature in a smart home can be an intelligent service. It first checks households’ profiles to obtain the preferable temperature, infers the cooling and heating functions
from available appliances, and determines an energy-efficient
algorithm to adjust room temperature. In another important
aspect, if any device fails to work, the execution environment
will find a replacement device with a new control service.
This example shows three issues in a smart home:

123

66

SOCA (2012) 6:65–79

implementations and composing these devices as services
for smart homes.
This approach has been taken by DPWS [4], FCINT1
[5–7], SENSEI2 , and SOFIA3 projects.
UbiQ Scenario Control [8] designed by Advantech offers
a centralized control for home automation systems, lighting
control, home security systems, emergency warning, doorway intercom, and community service. It uses a scenario
manager to configure the controls of different devices. Nokia
Home Control Center [9], an open platform that allows third
parties to integrate smart home solutions and services, has a
control panel to allow the end users to monitor and control the
device. However, both solutions define functions statically,
and if end users need to personalize the controls or create
new function flows, they need to wait for service providers
to deliver new features.
MisterHouse [10] is an open-source home automation program. It is written in Perl, and it executes actions by time,
voice, and serial data. MisterHouse offers an execution platform for supporting different protocols. The protocol supported by MisterHouse includes 1-wire, EIB, Inteon, RS232,
Universal Powerline Bus, X10 Infrared, X10 Powerline, X10
Radio Frequency, xAP, xPL, Z-Wave, and Zigbee. But it
needs programming expertise in Perl, and this can hinder
some residents from developing their own control software.
To model and compose home services, this paper uses
the service-oriented [11] approach that utilizes software service to build the required functionalities, e.g., Open Services
Gateway initiative framework (OSGI) bundles [12], Web Services [13], service composition [14], DPWS, and SODA.
Rather than building static applications, service-oriented
approach composes applications by discovering and invoking services based on published services.
Using SOC, devices wrapped as services allow underlying
communication protocols to be hidden from users. While
composing the workflow, users can use the composition capabilities of SOC without the need to deal with protocol issues
among devices. This approach simplifies the smart home
function development dramatically because no coding in Perl
or C or Java will be needed and also allow new services
(including device services) to be discovered and composed
to perform specific features. Sample SOC approaches for
smart home include OSGI bundles [12], Web Services [13],
service composition [14], DPWS, and SODA.
Several workflow languages are available to be used for
service composition such as BPEL, OWL-S, and PSML (Process Specification Modeling Language) [15]. This paper uses

1

http://www.fcint.ro/.

2

http://www.ict-sensei.org/.

3

http://www.sofia-project.eu/.

123

PSML as the modeling language as it allows modeling, composition, code generation, ontology, and analysis capabilities
from a single language [16].
The second issue can be addressed by a smart home knowledge base.
Ontology has received significant attention recently due to
Semantic Web [17,18], sometimes also labeled as Web 3.0.
Ontology specifies concepts, relations, and classifications in
a given domain. Most ontology languages, such as RDF and
OWL [19,20], are usually represented in XML.
Ontology can be integrated with software. It is used in
SOC to facilitate intelligent service discovery and composition [21]. In [22], a Getting-up scenario demonstrates that
ontology is useful to describe the context. In the European
SENSEI and SOFIA project, ontology and its reasoning are
used extensively for storing context information as well as for
intelligent behaviors. Ontology is also used to model the relations between devices and services in a home environment
and to manage software configurations [23]. This paper takes
the PSML approach of using ontology in modeling, composition, code generation, execution, and policy enforcement, in
other words, ontology is fully integrated with many aspects
of service-oriented development from modeling to analysis,
from analysis to code generation, from code generation to
deployment, and from deployment to policy enforcement and
monitoring, and then back to modeling. Due to space limitation, this paper will not present policy enforcement and they
are covered in [24,25].
A third issue can be addressed by generating code for
different execution environments and executing the code in a
fault-tolerant execution environment.
Service execution is important, and modern cloud computing platforms often address this in PaaS (Platform-asa-Service). Some well-known PaaS include GAE (Google
App Engine) and Amazon EC2. While these PaaS are powerful, applications developed must follow their protocols. This
paper will not address this approach but uses a conventional
service platform for execution.
Usually, the XML files of services, not executable if alone,
are executed by two ways: (1) by an XML interpreter (2) Via
code generation. The first approach, by interpreters, requires
a sophisticated workflow engine that understands domain
knowledge, and thus, multiple interpreters may need to be
developed for different application domains. Code generation focuses on generation code from the PSML specifications. The domain knowledge will be encoded in the design
template. In other words, the most significant difference is
that interpreters need to process domain knowledge at runtime, but domain knowledge can be encoded at design time.
While interpreters may have the advantage that it can process
various applications intelligently at runtime, the design may
be overly complex. The code generation requires a separate code generation for each application, but the code will

SOCA (2012) 6:65–79

67

Service Execution & Management

Service Modeling & Composition
Workflow Design

Scheduling

Execution

V&V Checking

Smart Home Knowledge Base

Availability
Checking

Policy
Checking

Ontology System
Environment
Ontology

Resource
Management

When the picked
Plan cannot pass
the collaboration

Refer
To
Device
Ontology

Refer
To

Refer
To

Function
Ontology

Policy/
preference
Ontology

Service
Ranking

Composition &
Collaboration

Refer
To

Registry System

Device

Service

Service Deployment
Local/Distributed
Deployment
Parameter Setting

Service
Template
Code Generation

Household
Profile

Protocol

Service
Ranking

V&V Checking

Availability
Checking

Device View

Code
Template

Policy
Checking

Code
Resource control

Fig. 1 System architecture for service-oriented smart home

be lightweight. Moreover, by changing the code generation
template, it can support code generation to different target
systems. Several code generation techniques [26,27] have
been proposed using existing tools.
For example, CMSCreator [28] can generate VB.net and
C# code, CodeFuture FireStorm/DAO [29] produces Java
code, and uml2php [30] results in PhP code. Similarly,
My Generation [31] and CodeSmith [32] have the similar
capabilities. This paper chooses CodeSmith because it can
be integrated with the existing PSML modeling tool written
in C#.
In code generation, the main task is to design the code
template to fit the application domain. For example, in a
smart home project, service template requires generating
code for the initialization of devices, invoking drivers, scheduling parameters, internal workflows, and the service recovery mechanisms. Depending on the target platform, different
templates can be written, e.g., templates for the OSGI services that are represented as bundles will be different from
templates for Web Services that uses SOAP protocol. Additional issues such as resource allocation, device communication, and failure recovery also need to be addressed after the
initial code is generated.
In conclusion, this paper makes the following contributions:
• Utilizing a knowledge base that uses both ontology and
registry information to aid modeling, code generation,
deployment, and execution of control services.

• Offering an agile service-oriented approach for modeling
control services that allow end users to compose their
functions rapidly.
• Generating code from the PSML specifications, deploying, and executing the code in a fault-tolerant environment.
The paper is organized as follows: Sect. 2 presents an overall architecture; Sect. 3 shows the knowledge base; Sect. 4
presents the modeling and composition mechanism of smart
home services; Sect. 5 illustrates the algorithm for code generation of the Java-based execution environment; Sects. 6
and 7 present the deployment and execution environment;
Sect. 8 uses the MediaControl example to illustrate the proposed approach; finally, Sect. 9 concludes this paper.
2 System architecture
The overall architecture of the system is illustrated in Fig. 1.
It includes a smart home knowledge base and four components corresponding to four phases of development: (1) service modeling & composition (2) code generation (3) service
deployment, and (4) service execution & management. Note
that in most service-oriented development projects, the development process follows a cyclical process from step 1 to step
4, and one such process is shown in Fig. 1.
Smart home knowledge base: This stores knowledge
in this project including the knowledge about devices,

123

68

SOCA (2012) 6:65–79

Fig. 2 Schema design for registry system

services (such as service templates, service ranking), control
polices, household profiles, and protocols. Besides that it
also includes an ontology system, which contains semantic knowledge specification and the reasoning functions of
smart home.
Service modeling & composition: This supports service
specifications using PSML [15] including service policies in
the ontology.
Code generation: This generates the code from the PSML
model using the CodeSmith templates. The code can be generated for different target systems, including Web Services
and OSGI services.
Service deployment: This deploys the code in an execution engine. The deployment can be done locally or remotely.
Service execution & management: This provides an
execution environment where a service can be loaded and
scheduled to run. It allows services to deliver/receive the
commands and messages to/from the devices in the household. If a device fails to work, the execution engine will
change the control software using different services to
mitigate the failure.

3 Knowledge base
This has two components: a registry system and an ontology
system.

123

3.1 Registry system
Registry system maintains the devices, services (service templates and rankings), polices, household profiles, and protocols. Different from the ontology system to be discussed in
next section, registry system stores the value for the system,
whereas the ontology system describes the type, relations,
classification and performs the reasoning using these data.
Each registry has a schema for the storage and query purposes. Figure 2 shows the schema design.
• Device registry: This records the device IDs, types, functions, statuses, locations, energy consumption, brand, and
communication protocols.
• Household profiles: This includes resident profiles, environment profiles, and policies. A resident profile is mostly
static as it stores the basic information of residents
(names, addresses, phones, fax numbers, and home page
addresses) and their preferences. An environment profile records both static and dynamic information. The
dynamic information can be obtained periodically via
Web Services, such as weather.com. A policy can be a
static policy or a dynamic policy generated by the system automatically based on the profile information and
current weather conditions. Environment profile and policy are just stored item by item, but they can be mapped
through ontology system.

SOCA (2012) 6:65–79

• Service template registry: This stores service templates
that can be reused in service composition. A basic abstract
workflow is offered by each service template, and a service developer can reuse it by filling the device to form an
executable service. Effort can be saved by reusing these
templates other than reinventing the wheel each time.
• Service registry: This stores the services composed.
• Service ranking registry: This stores and manages the
service ranking information. It helps to speed up the
recomposition process.
• Protocol registry: This stores the protocols and driverrelated information for the protocols.
Registry relations:
• Household profile registry links to service registry through
the associated services.
• Service registry links to device registry through related
devices.
• Service registry associates with service ranking registry
through service name.
• Service registry links to service template through the service type.
• Service template registry connects to device registry
through device types.
• Device registry associates with protocol registry by protocol names.
3.2 Ontology system
Ontology system supports query and semantic-based discovery for services and devices. The ontology system consists
of the following ontology systems:
• Device ontology: This describes the concepts related to
devices.
• Function ontology: This describes service templates and
the concepts related to functions.
• Policy/Preference ontology: This describes system constrains and preference rules about using devices and services.
• Environment ontology: This describes the information
related to the smart home environment including physical
environment and personal information.
Relations:
Relations are used to distinguish different associations
among classes and individuals. In a smart home, the following relations are usually used: SubclassOf/SuperclassOf,
Compose/ComposedOf, Function/FunctionOf, Equivalent,

69

Disjoint, Overlapping, Complement, InverseOf, Own/Owned
by, and Handled by/Handle.

Cross-references:
• Interontology systems: The four ontology systems are
cross-referenced as shown in Fig. 1. In device ontology,
semantic information of the function of each device is
described using concepts defined in the function ontology. In the policy/preference ontology, the rules and
constrains will include conditions referring to the environment ontology, function ontology, and service ontology. In the environment ontology, personal profiles will
refer to specific services in the function ontology.
• Interontology systems and registry systems: An entity
in an ontology system may cross-reference to another
entity in the registry system. Cross-references are more
of typeof references. For example, references between
device ontology and device registry is a 1: N mapping
of types and instances. When room temperature is higher
than the preferred temperature, the user may invoke a
cooling function described in the function ontology. One
of the cooling functions may require using an air conditioner. The available air conditioner can be found in the
device registry, and its attributes and interfaces can be
obtained through the device ontology.

These relations are subject to changing as smart home
evolves. The design of the ontology can follow the ontology design patterns as described in [33].
All the four ontology systems are developed using PSML
modeling tool. Next, part of this section will talk about how
to operate on ontology systems and registry systems.
A sample part of the device ontology is shown in Fig. 3.
Devices are categorized into two classes: low energy and
high energy. For example, air conditioners and heaters often
consume more energy than fans.
Figure 4 shows a sample of the function ontology. Smart
home functions can be divided into five classes: AccessControl, Security, ApplianceControl, InformationSupply, and
Sensor. The service MediaControl is the subclass of ApplianceControl. It is composed of two functions: ContentDirectory and RenderingControl.
Figure 5 shows some cross-references among the function ontology, device ontology, and device registry. The
cross-references is used to construct control models. While
developing a model, the developer will get function information from the function ontology. Then, these functions
will be required to link to devices that contain the functions.
In this process, a cross-referencing check will be required
from function ontology to device ontology.

123

70

SOCA (2012) 6:65–79

Fig. 3 Device ontology

Device
(disjoint)
subclass

Low Energy

High Energy
(disjoint)
subclass

(disjoint)
subclass

Electronic
Fan

Home
Theatre

…

Air
Conditioner

Home
Radior

Electronic
Heater Fan

Temperature
Control All-in-One

…

(disjoint)
ComposeOf

MediaServer

...

MediaRender

Fig. 4 Function ontology

Function
(overlapping)
subclass

Access
Control

Security

Appliance
Control

(disjoint)
subclass

(disjoint)
subclass

(disjoint)
subclass

Security
Image

Alarm

Door
Control

Window
Control

Curtain
Control

Information
Supply

Sensor
(disjoint)
subclass

Media
Control

Temperature
Control

Laundry

(disjoint)
subclass

(disjoint)
ComposeOf

(disjoint)
ComposeOf

Content
Directory

Rendering
Control

Heating

Cooling

Washing

Drying

Meter

Binary

Fig. 5 References between
function ontology, device
ontology, and device registry

Alarm

Security
Image

Curtain
Control

Media
Control

Temperature
Control

Laundry

(disjoint)
subclass

(disjoint)
ComposeOf

(disjoint)
ComposeOf

Content
Directory

Rendering
Control

Heating

Cooling

Washing

Drying

Binary

Function Ontology
FuntionOf

FuntionOf

Device
(disjoint)
subclass

Small Power

Large Power

(disjoint)
subclass

(disjoint)
subclass

Electronic
Fan

…

Home
Theatre

Air
Conditioner

Electronic
Heater Fan

Home
Radior

…

(disjoint)
ComposeOf

MediaServer

typeof

...

MediaRender
typeof

Device Ontology

Device Registry
Sumsung
Content
Directory

Sony
Content
Directory

3.3 Maintenance operations
Both the ontology and registry systems will not be static and
subject to change. Common changes include the following:
adding a new item, deleting an existing item, and modifying

123

LG TV

Sumsung TV

GE TV

…

an existing item. In [25], these operations to the ontology
system are described.
Add new information requires significant updating and
checking. An item added must satisfy the constraints in the
ontology system and registry system. If a new relation is

SOCA (2012) 6:65–79

added into existing classes, the original and derived relations
need to be examined to make sure there is no violation. In
addition, the consistency checking needs to be performed
over updated relations and update if necessary.
Most of time, a delete operation will delete information
in the registry system, and keep the information in the ontology system intact, as the ontology system is related to type
information but the registry system is related to instances
of types. Discovery of the device starts from the ontology
and then references to the registry system. It is possible even
though unlikely that a type will have no instances. On the
other hand, an instance should have its type definition. In
general, deleting an existing item from an ontology system is
difficult and should be discouraged because the information
stored is often cross-referenced to other items in the ontology
systems or registry systems. Deleting an item will not only
affect the current ontology system, but also the changes may
propagate to the other referenced ontology systems.
There are two kinds of modifications. One is moving an
existing item. This kind of modification is difficult because
moving is essentially deleting an item or a relation from one
place, while adding the same item in another place. Thus, this
kind of modification has both the complexities of adding and
deleting operations.
The other is modifying the attributes of the item. The modified attribute must satisfy the constraints in the ontology system. Any propagated changes need to be checked to make
sure the system is consistent.

4 Modeling and composition
This project uses a three-phase design for service composition: (1) abstract workflow design; (2) function construction;
(3) device discovery. Each phase addresses a separate issue:
workflows, functions, and devices.
Initially, the service developer designs an abstract workflow with steps (as blocks) and their relations (behavior tags
in PSML [15]) as specified. Then, the developer associates
a block in the workflow with the functions in the function
ontology. Last, the functions are mapped to detailed devices
by interacting with the knowledge base for suitable devices.
In other words, a workflow is constructed in three stages
focusing on just one aspect of the concerned workflow.
PSML is a modeling language that can be used for
modeling, composition, analysis, simulation, and policy
enforcement for service-oriented applications. The PSML
ACDATER models, including Actor, Condition, Data, Action,
Timing, Event and Relation, define the basic elements for
applications. Among these model elements, Actors, Conditions, and Actions contain their internal workflows, and they
can be used as a component in workflows. Workflows will
be described by a set of behavior tags.

71

Process

MediaServer.
CONTENT_D
IRECTORY

Gid001.CON
TENT_DIRE
CTORY

Process

MediaRender.
RENDERING
_CONTROL

Gid005.REN
DERING_C
ONTROL

Abstract
Workflow
Design

PSML
ACDATER
Models

Function
Construction

Devices

Smart Home
Device
Discovery

Fig. 6 Three phases of service workflow construction

The existing PSML methodology offers a modeling tool
that supports the abstract workflow design and service construction. However, the original PSML modeling tool does
not associate functions with devices. Hence, this paper introduces a device discovery phase in addition to the PSML modeling processes.
• Phase 1 (Abstract workflow design): This step composes a workflow using existing behavior tags in PSML
such as Process, Select, and Link, and these provide the
minimal process control to design a workflow. A Process
denotes an execution unit, a Select denotes a condition,
and a Link denotes the relations among a Process and a
Select. Abstract workflow design generates a workflow
without function semantics. This workflow needs to connect to PSML model elements to get the semantic context. The abstract workflow design phase in Fig. 6 shows
a constructed abstract workflow that contains only a start
point, two processes, and two links between the processes
and the starting point. It is a simple workflow with no
semantics.
• Phase 2 (Function construction): This step links the
abstract workflow with functions. The functions are
described by the ACDATER models and stored in the
repositories. Hence, the construction process first discovers the required functions through the semantic reasoning
in the ontology, and then, these functions are filled into the
abstract workflow for describing the desired scenarios.
For instance, the workflow in the abstract workflow
design phase of Fig. 6 is used to describe the MediaControl. After specifying the related functions, the original
empty block of the workflow can now be used to describe
a MediaControl service, in which it first turns on at the
ContentDirectory and then plays the selected content
through the media RendingControl.
• Phase 3 (Device discovery): This step links the composed functions with devices in a smart home. This phase
involves the discovery of devices by function types. Since
each function might match more than one device in a
repository, it is possible that more than one candidate

123

72

SOCA (2012) 6:65–79

Fig. 7 Composed workflow candidates for device discovery phase

service will be found. Figure 7 shows an example of
the generated candidates after the semantic reasoning in
the device ontology. Both ContentDirectory and RenderingControl devices have two candidates matched for the
composed workflow.
Let be the number of candidates for, and the total number of
candidates is where n is the number of functions in the workflow. Thus, it is easy to derive that Fig. 7 has four generated
candidates.
Following the above three phases, composite services can
be generated. The modeling process also involves atomic services, and these are essentially devices directly wrapped by
PSML ACDATER model.
The modeling process is described in the following algorithm. This algorithm also involves interaction with registries
and ontology systems in each phase.

The modeling process has two steps:
First, the system takes a new service request, and it checks
whether an existing service can be used. Otherwise, the service composition process will be carried out.
Second, after collecting needed information from device
ontology, the system replaces the functions by those services associated with devices. This process is described in

123

the device discovery phase. Furthermore, during device linking, checking availability filters out unavailable or infeasible services, and policy checking makes sure that new
services do not cause any system conflict. Meanwhile, services are ranked based on the criteria, such as reliability
and/or energy consumptions. Any newly generated service
is checked against the existing deployed services in case it
conflicts with the existing system. The workflow designer
selects the best match after all these steps.

5 Code generation
In this phase, the model constructed earlier will be converted
into executable code to be deployed in the execution engine.
The first step is the translation process where PSML model
is serialized as XML file after device discovery. The algorithm for XML generation is shown below. It is a BFS
(breadth-first search) graph traversal algorithm and uses a
queue to record traversed nodes. When the node is pushed
out of the queue, the corresponding XML generation function will write the content to a XML file. In the meanwhile, it
also calculates the subsequent unprocessed nodes and pushes
them into the queue for further processing. The output of the
algorithm is an XML specification of the workflow.

The second step is code generation, where each element
is first put to code generator and the code generator generates the target code based on the CodeSmith templates. The
templates are executable, and they convert the XML descriptions of the models to executable code. Code generation
process can adapt to different target platforms. This paper
implemented two converters: (1) the Web Service converter
that generates Java code that can communicate with others using various SOAP protocols in an emulator and (2)
the OSGI converter that generates OSGI bundles to interact with the devices through the UPnP protocol. The beginning phases for generating OSGI services are the same as
the Java Web Services one. The difference is the OSGI code
generation template that generates code for the OSGI environment (Fig. 8).

SOCA (2012) 6:65–79
Fig. 8 Mapping process in
code generation

73
Device View

Code Converter
(Code Smith Template)

Code

Variable Converter

Package/Inport
Definition
Class Definition

Function flow
Converter

Constructor

Element Definition

Select step
Workflow
Process step

Assign step

Function

Function Flow

Scheduling
parameter
Scheduling
Converter

The following algorithm defines the code generation process. XMLModel contains all the necessary information.
Write* functions write pieces of code according to conversion rules. This is essentially a reverse process to the XML
generation. XML generation serialized the graph of steps
into a set; the code generation process will build the graph
by function flow through function invocations.
Functions are built for each step, and workflow is retained
through function invocations.

Task Scheduling

remotely after a domain expert fixed a bug. Furthermore,
service provider can offer various services through a marketplace (Fig. 10).
Security is an important issue. Firewalls can be set at each
home gateway to protect the internal home network against
outside world.
As shown in Fig. 1, before execution, it is necessary to set
parameters for services, check resource availabilities and policy consistency, and finally set the control lock for resources
assigned.
In implementation, this paper builds sets of sockets and
seversockets for passing the compiled source code in a distributed way. The case study part shows an example of the
distributed service deployment process.

7 Service execution

Figure 9 shows a code generation example of the MediaControl. The left-hand side is the PSML element view, the
middle is the XML model for this PSML model, and the righthand side is the generated Java code. Note that all classes and
objects are assigned with UUID to ensure uniqueness.
6 Distributed deployment
Service deployment can be local or remote. Residents in a
smart home can develop their own service by composition
and then deploy the services locally. However, it will be more
convenient to offer residents to get services via the web and
then deploy them remotely. Distributed deployment comes
with its advantages. For instance, it can update a service

The service execution phase executes and monitors a generated service code after deployment. This section presents
a Java-based execution engine. This execution engine interacts with the generated Java code of the services. The Java
code interacts with the smart home devices through an ETRI
developed Web Service APIs using SOAP, which will interact
with a service emulator later on in the case study.
The execution engine has three main functions: execution,
messaging, and monitoring.
Execution of the services is through a thread pool. Each
service is implemented as a thread and is deployed in the execution pool of the engine (Fig. 11). To interact with these services from the execution engine, a set of interfaces is designed
as shown in Fig. 12. For instance, the execution engine calls
Execute() function to start the service.

123

74

Fig. 9 Example code generation from a graphic model to XML representation to Java code

Fig. 10 Distributed deployment for smart home services

123

SOCA (2012) 6:65–79

SOCA (2012) 6:65–79

75

Fig. 11 Execution environment
Monitoring interface

S

S

S

...

Execution Pool
Knowledge
Base

Registration Query
Client

Message Bus

Device
Manager

Fig. 12 A service interface for the execution engine

Messaging is done through the message bus. The message bus allows the code to interact with the device manager.
The device manager converts and translates messages into
a network protocol for communicating with devices. This
implementation interacts with the ETRI implemented Web
Service APIs. The following three functions interact with
the message bus:
1. The CallExecutableElement() function sends a message
to the device function invocation.
2. The CallCondition() interface allows a read of the device
condition, status, or input data.
3. The NotificationListener() function inserts a callback
mechanism, which is usually triggered when the device
manager receives device status changes.
Monitoring offers an interface to check execution statuses.
The thread pool of the execution engine maintains all the references to the running services in the runtime. Thus, whether
or not a service is ready/on/off/faulty can be detected through
this interface.
In fact, the execution runtime also implements some fault
handling capabilities, including handling broken devices in
a workflow or handling the failure of an entire workflow.
To implement these capabilities, the execution engine
requires interacting with the knowledge base. Thus, the registration query client is used for interacting with the knowledge
base for the discovery of the desired workflows or devices at
runtime.

As shown in the above algorithm, to handle the failure devices recoverable in a workflow, device replacement
is required. The generated code carries the function and
device type it needs. With the contained device type information, execution environment queries the knowledge base
for functions with similar capabilities to the faulty devices.
Once a device is matched, the faulty device is then replaced
by the new one. Device replacement usually does not change
the behavior of the workflow.
Another case is that the entire workflow fails to function. For instance, the current workflow contains defective
devices, and there are no device candidates available for
replacement. Hence, the workflow can be replaced with a new
one. For instance, a cool-down service implemented by air
conditioning can be replaced by another workflow composed
of electrical fans if no other air conditioning is available.
Both replacements result in the termination of the running
service and redeployment of a new service. More advanced
operations, such as rollback or compensation activities for
device and workflow replacement, can be added in the future.

123

76

SOCA (2012) 6:65–79

Fig. 13 Modeling and repository

Fig. 14 System running log

8 Case study
This section presents an example the MediaControl to illustrate the concept presented. The MediaControl first turns on
the ContentDirectory and then turns on the RenderingControl. The involved devices are offered by the ETRI service
emulator with two media servers from Samsung and Sony
and three media rendering devices from LG, Samsung, and
GE. First, this workflow is modeled and composed, then
a Java code is generated, and it is then deployed and executed. During the execution, the engine invokes the service
provided by the emulator. The failure recovery is also illustrated.

8.1 Service modeling and composition
Ontology with cross-references is used in this demonstration. Specifically, the device ontology cross-references to the

123

function ontology is referred in Fig. 5, and the resulting model
is shown in Fig. 6.
The system has six service candidates for MediaControl.
Of all the candidates, a best match of the workflow is selected
from Fig. 13.
The running log of the system is shown in Fig. 14 that
shows the processes of automatically adjusting the running
plan to fulfill the required MediaControl function.
8.2 Code generation
Code generation is implemented in CodeSmith as shown in
Fig. 15. The CodeSmith template first reads the XML file
generated from the PSML model and then outputs the executable Java code. Depending on the CodeSmith template,
different codes can be generated for the different target systems, for instance, Java code for OSGI environment or Java
code for the Web Service compatible execution runtime. The
generated code for this section is shown in Fig. 9.

SOCA (2012) 6:65–79

77

Fig. 15 Code generation using
CodeSmith

Fig. 16 Distributed service
deployment

8.3 Distributed service deployment
The generated code is compiled and deployed remotely
through the web (Fig. 16). The domain experts and the home
service marketplace are located at ASU Tempe campus, and
service code is then deployed remotely to a smart home external to ASU.

Gid001 Samsung media server to cover the faulty one.
Figure 18 shows that after the Sony server is marked as faulty,
Samsung media server and GE media render now are marked
as busy. Samsung media service is the replacement to the
defective Sony service; however, the function of GE media
renders remains unchanged.

9 Conclusion
8.4 Service execution and fault handling
After deploying the service in the ETRI emulator, it starts
the execution according to the scheduling parameters. For
example, MediaControl workflow example turns on two different devices Gid004 Sony media server and Gid005 GE
media rendering device. In the emulator shows below, they
are marked as busy (Fig. 17).
To demonstrate the failure handling capabilities, Sony
server is marked as faulty in the emulator. The execution
engine then queries ontology and retrieves a similar device

This paper presented a novel framework for service-oriented
smart home using ontology systems and used the semantic information in the ontology for discovery, composition,
deployment, automated code generation, deployment, execution, and fault recovery. In the framework, the smart
home models are composed in a revised PSML modeling
environment for smart home, then code is generated automatically via the composed workflow model. After deployment, the software can recover from failures at runtime. The
framework also allows context awareness [34].

123

78

SOCA (2012) 6:65–79

Fig. 17 Begin emulation of
Plan3

Fig. 18 Device replacement
after Gid004 becomes faulty

Acknowledgments This work is supported partially by the IT R&D
program MKE/KIAT, South Korea, 2010-TD-300404-001, Home Information Remote Aggregation and Context Inference–Prediction Technology Development; European Regional Development Fund and the
Government of Romania under the grant no. 181 of 18.06.2010; US
National Science Foundation project DUE 0942453 and Department of
Defense, Joint Interoperability Test Command.

References
1. Edwards WK, Grinter RE (2001) At home with ubiquitous computing: seven challenges, In: Ubicomp 2001: ubiquitous computing.
Lecture notes in computer science, vol 2201. Springer, Berlin,
pp 256–272

123

2. Lo CC, Chen DY, Chao KM (2010) Dynamic data driven smart
home system based on a service component architecture, pp 473–
478
3. Gu T, Pung HK, Zhang DQ (2004) Toward an OSGi-based infrastructure for context-aware applications. IEEE Pervasive Comput
3(4):66–74
4. OASIS, Devices Profile for Web Services(DPWS). http://
docs.oasis-open.org/ws-dd/ns/dpws/2009/01 Last Accessed:
07/30/2011
5. Tsai WT, Petrescu S, Bucur L, Chera C (2011) A service-oriented
intelligent building management. In: Proceedings of international
conference on control systems and computer science, vol 2. Editur
POLYTECHNICA Press, pp 676–681
6. Elston J, Tsai WT, Li W, Bucur L (2011) Software architecture with
ontology for intelligent building management. In: Proceedings of

SOCA (2012) 6:65–79

7.

8.

9.
10.
11.
12.
13.

14.

15.

16.

17.

18.

19.
20.

international conference on control systems and computer science,
vol 2. Editur POLYTECHNICA Press, pp 682–686
Bucur L, Tsai WT, Petrescu S, Chera C, Moldovcanu F (2011)
A service-oriented controller for intelligent building management.
In: Proceedings of international conference on control systems and
computer science, vol 2. Editur POLYTECHNICA Press, pp 665–
670
UbiQ Scenario Control, Advantech. http://www.advantech.com/
products/UbiQ-Scenario-Control/sub_1-2JKQ1X.aspx. Accessed
29 Nov 2009
Nokia, Available: http://www.nokia.com/press/press-releases/
showpressrelease?newsid=1273474. Last Accessed 29 Nov 2009
Misterhouse http://www.misterhouse.net. Accessed 29 Nov 2009
Chung JY, Chao KM (2007) A view on service-oriented architecture. Serv Oriented Comput Appl 1(2):93–95
OSGi Alliance, Available: http://www.osgi.org. Last Accessed 29
Nov 2009
Reyes Álamo JM, Wong J, Babbitt R, Yang Hen-I, Chang
Carl K (2009) Using web services for medication management
in a smart home environment. In: ICOST 2009, Tours, France, vol
LNCS5597. Springer, pp 265–268
Reyes Álamo JM, Sarkar T, Wong J (2008) Composition of services
for notification in smart homes. In: Second international symposium on universal communication, Osaka, Japan, IEEE. pp 75–78
Tsai WT, Xiao B, Huang Q, Chen Y, Paul R (2006) SOA collaboration modeling, analysis, and simulation in PSML-C. In:
Proceedings of the 2nd IEEE international symposium on serviceoriented applications, integration and collaboration (SOAIC’06),
October 2006
Tsai WT, Paul RA, Xiao B, Cao Z, Chen Y (2005) PSML-S: a
process specification and modeling language for service oriented
computing. In: The 9th IASTED international conference on software engineering and applications (SEA), pp 160–167
Noy NF, Sintek M, Decker S, Crubezy M, Fergerson RW, Musen
MA (2001) Creating semantic web contents with Protege-2000.
IEEE Intell Syst 16(2):60–71
Paolucci M, Kawamura T, Payne TR, Sycara KP (2002) Semantic
matching of web services capabilities. In: 1st international semantic
web conference
OWL-S: Semantic Markup for Web Services Available: http://
www.w3.org/Submission/OWL-S. Accessed 22 Nov 2004
OWL Web Ontology Language Reference, Available: http://www.
w3.org/TR/owl-ref/. Accessed 10 Feb 2004

79
21. Xu J, Lee YH, Tsai WT, Li W, Son YS, Park JH, Moon KD (2009)
Ontology-based smart home solution and service composition.
In: International conference on embedded software and systems
(ICESS) 2009, pp 297–304
22. Kim E, Choi J (2006) An ontology-based context model in a smart
home. In: Workshop on ubiquitous web systems and intelligence
(UWSI 2006), pp 11–20
23. Meshkova E, Riihijarvi J, Mahonen P, Kavadias C (2008) Modeling the home environment using ontology with applications in
software configuration management. In: International conference
on telecommunications (ICT) 2008, pp 1–6
24. Zhou X, Tsai WT, Wei X, Chen Y, Xiao B (2006) Pi4soa: a policy
infrastructure for verification and control of service collaboration.
ICEBE, IEEE Computer Society, pp 307–314
25. Tsai WT, Sun X, Huang Q, Karatza H (2008) An ontology-based
collaborative service-oriented simulation framework with Microsoft Robotics Studio. Simul Modell Practi Theory 16(9):1392–
1414
26. Lee YH, Li W, Tsai WT, Son YS, Moon KD (2009) A code generation and execution environment for service-oriented smart home
solutions. In: International conference on service-oriented computing and applications
27. Tsai WT, Fan C, Chen Y, Paul R (2006) Ddsos: a dynamic distributed service-oriented simulation framework. In: Proceedings of the
39th annual symposium on simulation, IEEE Computer Society,
2006, pp 160–167
28. AJAXCMSCreator (6.52) (2009) Available: http://www.
developerinabox.com/. Last accessed 21 Jul 2009
29. Code Futures (2009) FireStorm/DAO Architect Edition. Available:
http://www.codefutures.com/architect/. Last accessed 21 Jul 2009
30. UML2PHP (2008) UML2PHP. Available: http://www.uml2php.
com/. Last accessed 21 Jul 2009
31. MyGeneration Software (2008) MyGeneration 1.3. Available:
http://www.mygenerationsoftware.com. Last accessed 21 Jul 2009
32. CodeSmith Tools, LLC (2009) CodeSmith Tools 5.1.3. Available:
http://www.codesmithtools.com/. Last accessed 21 Jul 2009
33. Tsai WT, Wu B, Jin Z, Huang Y, Li W (2011) Ontology patterns for service-oriented software development. Softw Pract Exp
(Accepted)
34. Li W, Lee Y-H, Na Y, Tsai W-T, Son Y-S, Son J, Park J, Moon K-D
(2010) Context-aware service management for service-oriented
smart home. Telecommun Rev 20:610–624

123

Adaptive Transaction Routing in a Heterogeneous Database Environment
Avraham Leffl
Philip S . Yu
Yann-Hang Lee2
IBM Thomas J. Watson Research Center
P. 0. Box 704
Yorktown Heights, NY 10598

Abstract

transaction that is routed to its preferred system will make fewer
remote database calls because most of its requests reference the
local database partition.

In this paper we examine the issue of transaction routing
in a heterogeneous database environment where transaction
characteristics like reference locality imply that certain processing systems can be identified as being, in general, more suitable
than others for a given transaction class. Routing which ignores
these distinctions in an attempt to balance system load can degrade system performance. We consider an adaptive routing
strategy which (i) estimates the response time based on a steady
state analysis, (ii) monitors how well actual response time conforms to the estimate, and (iii) adaptively adjusts future estimates through a feedback control based on (ii). We find that the
adaptive strategy greatly enhances performance robustness as
compared to the results of the strategy without feedback. The
feedback process employed alleviates the sensitivity to accurate
estimations of workload and system parameters. Various simulation studies are used to illustrate the adaptive strategy's
robustness.

1.O

In studying the performance of such a transaction processing system, the reference locality distribution, i.e. percentage
of database calls issued by a transaction to each database partition, has to be considered. An effect of reference locality distribution is that each transaction class is faced with a set of
heterogeneous servers. Although the communication delay is
unimportant,' remote database calls increase both execution time
(due to communication overhead) and overall system utilization.
On the one hand, therefore, preferred locality routing ignores the
advantages offered by load balancing in a distributed system.
On the other hand, because of the additional overhead associated
with remote database calls, routing an incoming transaction to
the system with the lightest load may provide worse performance
than routing the transaction to its "preferred" system. Thus
transaction routing strategies need to strike a balance between
sharing the load among systems and reducing the number of remote calls.

Introduction

Dynamic routing strategies for this environment have been
studied in [Yu86, Yu88] and [Lee88]. A class of dynamic strategies based on an attempt to minimize each incoming transaction's response time, termed the MRT strategy, has been
shown to be better than the optimal static strategy [Ni85,
Tant851. In this paper we introduce a new class of dynamic
strategy, referred to as the adaptive strategy, that adds feedback
control to the basic MRT strategy. That is, the system can
monitor how well past system behavior conforms to expectations; deviations from expected behavior are then systematically
taken into account in subsequent routing decisions. The MRT
strategy requires knowledge about transaction reference locality,
and also requires accurate estimates of path length and configuration parameters. In contrast, the adaptive strategy can correct for inaccurate information, and thereby greatly enhances
performance robustness.

Transactions are increasingly processed in a multicomputer environment. Effectively using the available processing power implies that load balancing is done so that incoming
transactions are processed at underutilized systems. When the
systems are homogeneous, routing transactions to a processing
system with shorter queue length can lead to good performance.
However, when the systems are heterogeneous, the question of
determining which processing system will yield optimal response
time becomes more difficult.

In this paper we analyze a distributed database environment that is heterogeneous. The heterogeneity is a result of
partitioning the database among the various processing systems;
incoming transactions are routed to one of the processing systems by a common front-end system. The environment is depicted in Figure 1.1. If a transaction issues a database request
which references a non-local database partition, the request, referred to as a remote database call, must be shipped to the system
owning the referenced partition for processing. With regard to
database requests issued by a transaction, we can identify a specific system as its preferred system: that is, the system to which
the transaction sends most of its requests. Routing decisions can
thus be classified as being either "local"/"preferred system"
routing or "remote"/"non-preferred system" routing. The
local/remote distinction refers to the fact that, on average, a

*

In the next section, we present the model of the heterogeneous transaction environment. Then, the MRT strategy is described in Section 3. In Section 4, we perform a regression
analysis of the actual response time against the response time
estimate and remote routing decision of the MRT strategy. In
Section 5 the Adaptive strategy is devcloped in an attempt to use
feedback control. The performance and robustness of the strategy are evaluated through simulations and these are compared
to the MRT strategy. We summarize the results in Section 6.

'

Computer Science Department. Columbia University
Computer and Information Science Department, University of Florida

406

CH2706-0/89/0000/0406$01.00 0 1989 IEEE

We assume a locally distributed system in which the components are so
close together that propagation delay is negligible; e.g.. the entire system
is located in the same machine room.

2.0

Notice that S and Sp only depend upon the characteristics of
transactions and are independent of the transaction routing decisions. When a transaction being executed at P, issues a database request to DB, , where i # j , the database request must be
shipped from processing system P, to P,. After the request gets
processed, the result will be sent back. This is called a remote
database call in which both P,and P,have to perform sending and
receiving services. The service demands of initiating a remote
database call and of receiving the results of the call are referred
to as communications overhead and are assumed to be exponentially distributed with mean c.

Model Description

The locally distributed transaction processing system
studied has been described in [Yu86, Yu881 and is summarized
in the following. The system consists of N transaction processing
systems P, where i = 1,2,...,N,and a front-end system, connected
by an interconnection network as shown in Figure 1.1. There is
a partitioned database which consists of DB,, ...,DB,, where
OB, is attached to the processing system, P,. All database requests to DB, are assumed to be handled by the processing system P,.The processor speeds are assumed to be identical and are
denoted as jt.

The processing system model is as follows. A single server
processor sharing queue is used to model the processor at each
processing system. On the other hand, the 1/0 subsystem of
each processing system is modelled as an infinite server queue.
This corresponds to a global or aggregate representation of a
more complex 1/0 subsystem.

Transactions submitted by users enter the system through
the front-end system where a routing strategy is employed to
route the incoming transactions to processing systems. At the
assigned processing system, a transaction invokes an application
process which is interleaved by a number of database requests.
Transactions, then, are characterized by (1) the processing service demand of each application processing segment, (2) the
number and reference distribution of database requests, and (3)
the processing and 1/0 service demands of each database request. For simplicity, we assume that these service demands are
exponentially distributed. Also, at the end of each application
processing segment, fixed probabilities of issuing a database request to a particular database partition or terminating the transaction processing are assumed for each class. Hence, the number
of database calls issued by a transaction is geometrically distributed.

In order to study routing strategies we developed a simulation for the above model. This is similar to the one described
in [Yu88]. In the simulation, the distributed environment is assumed to consist of three transaction processing systems
(N= 3) with three transaction classes (K = 3). Based on data
from some IBM IMS systems [Com86, Yu871, the average
number of database requests per transaction is set to 15 for all
transaction classes, i.e., plo = 0.0625, for k = 1,2,3. The reference localitydistribution, [q,,], is given in Table 2.1.

In addition, we assume that the processor speed is 7.5
MIPS and the pathlengths of U, and b, are 21K and 9K instructions, respectively. The additional overhead of serving a
remote database call, c, is chosen to be 3K and 1% to represent
low and high rommunications overheads. We denote the relative
communication cost, c / ( u + b), by 9 . Thus 9 equals 0.05 for
low communication overhead. and equals 0.25 for high communication overhead. The IO access time, d,, and the probability,
pio , of having an IO during a database call are assumed to be 40
ms and 0.7 for all transaction classes, respectively. The arrival
rates, A,%, are adjusted so that the total processing load and individual processing loads of database calls are as indicated.

Let there be K transaction classes in the system and let rx,
denote a class k transaction, k = 1,...,K . For the k-th class,
transactions arrive according to a time-invariant Poisson process
with rate A,. The mean processing service demands of an application processing segment and a database request of rx, are U, and
b,, respectively. Both U, and b, can be estimated by measuring the
pathlengths of application processing and database request. For
each database request issued by rx, we assume that an 1/0 device
will be accessed with a fixed probabilitypiO, and that the service
time of each 1/0 access is exponentially distributed with mean
d, . When the execution of an application processing segment is
completed, transaction rx, may issue a database request to the
database partition DB, with probability p,, , or may terminate
with probability plo. The matrix (1/(1 -pm))b,,] shows the
distribution of database calls issued by tx, and is referred to as
the reference locality distribution. For a given transaction a,,
we call the processing system P, its preferred system if p , is the
maximum ofp,,, j = 1,2,...,N.

3.0

Response T i e Based Dynamic Routing

Strategy
Previous studies of dynamic load balancing approaches,
such as [Wang85, Eagr85, Eagr86, Agra82, Care84.Chow791,
assume that incoming tasks can be completely serviced at any
processing system. This implies either that requested resources,
such as database or files, are replicated or shared among all
processing systems, or that all tasks are purely computational.
Under the data partitioning model, of course, these assumptions
are invalid. A class of dynamic strategies based on an attempt
to minimize each incoming transaction’s response time, referred
to as the MRT strategy, was proposed and studied in (Yu86,
Yu881. It uses readily available information at the front-end
system, such as previous routing decisions of transactions currently in the complex, in a steady-state approximation that estimates the response time of each incoming transaction. Other
studies have demonstrated that system performance can be
greatly improved if the system attempts to minimize the response
time of incoming jobs [Ferr86,Zhou87].

With the above definitions, we can see that the average
processing utilization of serving transaction application process
and database requests in the whole system is

Of this utilization, a portion is associated with the processing of
database requests. We denote the utilization based on processing
database requests at 4as follows:

407

routed to the specified system. The hypothesis is certainly reasonable in a closed queueing network with a fixed population and
homogeneous servers. For our system, we shall hypothesize a
general model to describe the actual response time: namely,
Rt = ctk + &R; + p p where R; represents actual response
time of the i-th class k transaction, Rb is its estimated response
time whose calculation is described in Section 3, and P = 1 if a
remote routing decision is taken for that transaction, or 0 otherwise. ctk is adopted to capture the system fluctuation; that is, the
variation from the MRT steady-state approximation due to the
departures and arrivals over the course of a transaction. pi represents the degree of relative correspondence between actual
and estimated response times. The third parameter, pi, is intended to show the impact of increased system utilization due to
a remote routing decision. The hypothesis under the MRT
strategy is a special case with uh = 0, /3: = 1 and = 0.

The MRT strategy, first, estimates the average queue
length or utilization of each processing system Pp Then, the expected response times of an incoming transaction, if it were
routed to the processing system 4, for i = 1,...,A', are estimated.
The processing system which provides the minimum expected
response time is chosen to execute the transaction. In the following, we describe the implementation of MRT strategy.
Under the MRT strategy, the routing decisions of active
transactions are maintained by the front-end system in a routing
history table. Each time a transaction mkis routed to Pi, the entry
in the k-th row and the i-th column of the routing history table
is incremented by one to reflect the new amval and its routing.
Furthermore, when a transaction is completed, the entry in the
corresponding row and column of the table is decremented by
one to reflect the departure. Note that there is a negligible overhead to maintain the table in the front-end system, and that no
sampling of instantaneous state information from the processing
systems is required.

We have performed regression analysis of actual and estimated response times, together with the MRT-based routing decisions derived from simulation results. The values and
confidence intervals of ah, ,and /?: are shown in Tables 4.1 and
4.2. The balanced case is the one where the utilization based on
processing database requests is the same for each processor, i.e.
S,b = S,b = Si . The specific unbalanced case considered is one
in which the utilization based on processing database requests
at PI is half of the utilization of the other processors. That is,
Si:S,b:S!= 0.5:1:1 . At the 99% confidence level: in all cases
we find that the hypothesis of ah = 0 and = 1 should be rejected. We note that the coefficients on estimated response time
vary between 47% and 87% depending on the transaction class,
the load, communication cost, and system unbalance. ah,which
captures the variation from MRT's steady-state approximation,
varies in similar fashion. This indicates that MRT is not entirely
correct in assuming that estimated response time is always equal
to actual response time. Clearly, other factors are involved as
well.

The routing history table can be used to calculate the average queue length, and thus get an estimate of the expected response time. The expected response time of an incoming
transaction depends upon the transient behavior of the system
and the future arrivals. For efficient implementation at the
front-end, a steady-state analysis is applied to estimate the mean
response time using transaction characteristics and mean queue
length at each processing system. Based on the mean-value
equations [Reis80], the expected response time for a transaction
routed to Pi is approximated as

i#i

U

j#i

The role of a remote routing decision in affecting actual
response time, modeled by pi, is less clear. Sometimes, as in the
high load and communication case of the unbalanced situation,
transaction class 1 incurs a substantial increase in response time
when routed to a remote system; this increase was not predicted
by MRT. Some cases have pi coefficients with smaller magnitude, that are nevertheless statistically significant at a 99%
confidence level. In other cases we cannot reject the MRT hypothesis that = 0.

-

where L,is the mean queue length at Pi and L = (4,
..., ~ 5 ~Note
).
that although quite a few parameters appear in the above formulas, they are derived from either system parameters (c and
p), or transaction characteristics (uk,b,, dk,pi0,and pk,) which are
provided from the static transaction profile. The only unknown
variable in the above equation is L.

-

The MRT strategy estimates 4 with a residence time calculation. This approach has shown to provide the best performance over other approaches considered in [Yu86, Yu881. It
regards the numbers of active transactions indicated by the
routing history table as fixed populations in a closed-queueing
network. Naturally, it is possible to calculate the mean queue
lengths based on mean-value algorithm [ReisSol. However, this
approach is impractical when we consider the complexity of the
MVA. The MRT strategy uses an approximation based on
Bard-Schweitzer's algorithm [Schwe79, Bard801 to calculate the
residence time of each transaction at each processing system.
Then, the utilization and queue length of each processing system
are computed. Details of the approach can be found in [Yu88].

4.0

In analyzing the regression results we observe the following. First, as system load increases the correlation between R t
and R; increases as well. Conversely, as load increases, the
magnitude of ahdecreases. This effect is caused by the nature
of MRT's approximation of actual system state by a steady state
approximation. When many transactions are active in the system, arrivals and departures do not change the system state from
that existing at the time of the routing decision, to the extent that
they affect a system with few transactions. When the load is
relatively low (and the routing table contains few entries) then

'

Regression Analysis of Actual Response Time

In the MRT strategy, the estimated response times are regarded as the actual response times given that the transaction is

408

Individual transactions are not, of course, independently distributed. The
estimates for the coefficients are still unbiased. Because of the serial
correlation the true confidence intervals will be somewhat larger than
those shown in Tables 4.1 and 4.2. However, we can ignore this for
practical purposes because more than 40,OOO transactions were used in
the regressions. Even grouping the dependent transactions together
leaves the confidence intervals unchanged.

the effect of an arrival or departure has a magnified effect on
increasing the variation between the routing history table of active transactions and the system state over the course of the
transaction. ak captures this variation; pi captures the degree to
which the MRT approximation is valid. Therefore, when the
system is more heavily loaded, the approximation improves with
a consequent change in the ahand 8: coefficients.
The part of the cost of a remote routing decision not captured by MRT's estimation is modeled by pi. Some of the observed variation in this coefficient is due to the fact that, when
making a remote decision, MRT can overestimate the Dotential
gain. If the MRT approximition leads to a remote decision, and
the remote system subsequently becomes more utilized, then the
additional communication inherent in a remote decision causes
actual response time to be larger than predicted. For this reason,
fli increases as communication cost increases. However, we observe that the cost of a remote decision also depends on the extent of system unbalance. Let P, be the preferred system of txi,
and let Pd be the remote system to which the transaction is
routed. Clearly, if 1)communication costs are high and 2) many
transactions with preferred system Pd arrive over the course of
the transaction, then a remote decision will involve a substantial
penalty. The underutilization of Pd that is recorded by the routing table is only temporary; the transaction's actual response
time will be substantially greater than predicted as Pd becomes
more utilized. If either of these conditions do not apply, then the
effect of a remote decision is accurately modeled by MRT's estimation. In the unbalanced case presented in Table 4.1, transaction class 1 has large 8: coefficients when communication costs
are high because P2 and P3 have twice the database utilization of
Pl. In contrast, when a transaction of class 2 or 3 is routed to
Pl then MRT's approximation which predicts that system 1 will
remain underutilized over the course of the transaction is valid.

5.0

More specifically, upon transaction arrival the front-end
uses the MRT calculation described in Section 3 to make a initial
estimate of the transaction's expected response time. In a previous processing period the system performed a regression of the
equation R t = at + &RF + pt# as explained in the previous
section. This regression is done for each of the k transaction
class. The front-end uses the and /3$ coefficients to adjust the
MRT estimate of the the transaction's expected response time
at each of the processing systems. Note that ak is irrelevant in
making routing decision as it adjusts the response time of each
candidate system in the same way. Non-preferred routing is
done only when the modified estimate at the remote system is
still less than the estimate for the local system.
When a transaction leaves the system its actual performance statistics are added to the running totals accumulated at the
front-end.5 In performing the regression, the front-end solves the
following two linear equations for each transaction class k to
obtain /3: and /?$ coefficients. Assume nik to be the number of
class k transactions arrived during the observation period. To
avoid proliferation of transaction classes, one can lump all
transaction types with the same preferred processing system into
one class.

The Adaptive Strategy

In the previous section we saw that the MRT strategy often
overestimatesthe benefit of doing a remote routing. As a result,
the strategy tends to do more marginal non-preferred routing
than it should. This is an inherent limitation of the MRT strategy, and is due to the approximation used to calculate the response time of transactions at each processing system. We
cannot, however, avoid this approximation: determining transient solutions is too difficult a task for on-line processing, and
including future transaction routings in response time estimation
is completely infeasible. The general model specified in Section
4 indicates, though, that as far as minimizing response time is
concerned, the estimated response time is a good indication of
actual response time- except that it systematically ignores an
additional cost associated with making a remote routing decision.
As we have seen, this cost varies with system load, communication cost, and system unbalance. These observations motivate
the development of an "adaptive" routing strategy, which attempts to 1) quantify the additional cost of remote routing under
MRT and 2) take this cost into account when making subsequent
decisions.

5.1

Then, as transactions are processed and leave the system, the
front-end records the actual response time of the transactions.
This enables the system to monitor how well actual response time
conforms to the estimate. Finally, the front-end adaptively adjusts future estimates by periodically recalculating coefficients
that are used to weight factors in the response time estimation.

The Strategy

The Adaptive strategy consists of three steps. In the first,
the system estimates the response time of incoming transactions.

i= 1

i=l

i= 1

where
m.

(5.3)

(5.4)
(5.5)

Equations (5.1) and (5.2) are easily rewritten so that the parameters can be calculated with running totals of each transaction's statistics: RF,P,P,
R F P , RPP',PP and (RF)2 .
Such accumulation imposes negligible overhead as the information which consists solely of the estimated and actual response
times and whether the routing decision was local or remote is
readily available in the front-end.
Periodically, the system recalculates the coefficients,
and pi , using the equation
= E(Y).+~) + (1 E)Y, . Here
Y.+~ represents either coefficient, 8: or P: , that the system will

-

Because of the inherent serial correlation among response time values
we use the entire history of observations in the regression.

use in the next ( n + l ) period of transaction processing; yrn+, represents the coefficient that the regression actually yields in the
n-th period. That is, the system uses a weighted sum of the previous period's coefficient and the actual p: and p1 determined by
the regression to yield the next period's coefficient. This calculation introduces two free parameters: E which determines how
rapidly the system adjusts to changes in the coefficients, and w
the window size of transaction processing in which the previous
coefficients are used without modification.

5.2

routing decisions. Transactions issue database calls according to
the actual reference distribution [q,,] during execution. In these
simulations, [e,]is set to the distribution defined in Table 2.1.
The estimated distribution, [q,,)], is set according to the following: given a percentage of inaccuracy x, if (1 + x)q,, < 1, then
:4
, = (1 x)q,, and for all i # k, q,,' = qJ1 - xq,,/(l
qd)
(i.e. the locality of the preferred system increases with a ratio x
whereas the localities of the non-preferred systems decrease
proportionally), otherwise:4
, = 1 and q,,' = 0 for all i # k. The
response time; are plotted in Figure 5.3. When weaker localities
are assumed in making routing decisions, the response time can
degrade substantially. Non-preferred routing can create a vicious cycle such that after a transaction is routed to a nonpreferred syslem, it increases the load on that system, thereby
forcing subsequent transactions with this system as a preferred
system to be routed in turn to a non-preferred system. This vicious cycle is referred to as swapping phenomenon [Lee88]. We
define a swapping ratio statistic in the following way. Let rx, and
rx, have preferred systems P,and P,respectively. If rx, is routed
to P,while P,is currently serving tx, then a "swap" occurred. The
swapping ratio is the percentage of routings in which swapping
occurred: these ratios are indicated in Figure 5.3. The swapping
ratio can reach 80% under the MRT as x = - 0.3. This explains
why the performance of the MRT degrades badly in this cast.
However, when the adaptive strategy is used, variations in response times are quite small when x is iu the range of -0.3 to
0.2. It is clear that the adaptive approach is much more robust
than the MRT when inaccurate reference locality distribution is
assumed. Under the Adaptive strategy, the router is able to get
a better understanding about the true cost or benefit of routing
to remote systems. The value of p: indicates how the router
compensates for the basic dynamic estimation of response time,
and represents, for each transaction class, the degree of bias towards remote routing. In Figure 5.4 we show the average p$
values for the case where the router has inaccurate reference locality information. When the router assumes a weaker locality
than actually exists, then MRT does more remote routing in an
attempt to do load balancing than it does when the router has
accurate information. In Figure 5.4 we see that the Adaptive
strategy increases the 8:value as the percentage change in locality accuracy becomes increasingly negative. Because of the
change in the coefficient the router realizes that routing to a remote system involves a higher cost than expected. The result is
that less remote routing is done. Note that when the router assumes a stronger reference locality than actually exists (encouraging more local routing), Figure 5.3 shows that M R T s
performance actually improves somewhat as compared to the
case where the router has accurate information. In this situation,
M R T s performance is similar to that of the Adaptive strategy.
This further supports the discussion in Section 4, in which we
claimed that the MRT does a lot of marginal remote system
routing. Thus, when the router assumes stronger reference locality, and is thereby mistakenly "forced" to be more conservative than the true strategy, response time actually improves.
Under the Adaptive strategy, as the percentage change in locality
accuracy increases (thus discouraging remote routing), the
value decreases from its value in the case where accurate information is available. The trend is shown in Figure 5.4. This tells
the router that a greater degree of remote routing can be done
than expected: as a result more load balancing is achieved.

+

Choice of parameters

The potential problem of the free parameters in the adaptive
strategy must be addressed. The strategy is completely selftuning except for 1) E , which sets the weights on the next period's
coefficient, and 2) w, the window size of transactions processed
before a new calculation of the coefficients. In Figure 5.1 we
show that the adaptive strategy is robust with respect to the
weighting of the coefficients. In the figure the window size is
held constant (the coefficiects are recalculated whenever loo0
transactions of any class have been processed; then a new window starts from scratch), and E varies from 0.05 to 0.45. Simulations in which we set E to 0.25 provide slightly better results
overall than other E values. Note that the response times under
the Adaptive strategy in Figure 5.1 are only slightly better than
under MRT. When the front-end router is supplied with completely accurate information, simulations show that the Adaptive
strategy reduces the percentage of remote routings by as much
as 15%. These facts imply that that MRT makes many nonpreferred routings where there is only a marginal difference between preferred and non-preferred routing on overall system
performance. The real performance strength of the Adaptive
strategy is its robustness with respect to inaccurate parameter
values. This robustness is discussed in Section 5.3.

In Figure 5.2 we examine what happens when the weight,
P, for

the coefficients is held constant at 0.25, and the size of the
window of transaction processing, w, varies. Although w varies
from 100 to lO,OOO, the adaptive strategy appears almost unaffected by the change in this parameter. When we set w to specify
an interval in which loo0 transactions of any class have been
processed, response time values are slightly better than those
with different window size. Note that Figures 5.1 and 5.2 are for
the balanced case. Similar results hold for the unbalanced case
[Leff88]. Because of the adaptive strategy's robustness with respect to the two free parameters, it is reasonable to be confident
that, although in other simulations we always set E to 0.25 and
w to lo00 transactions, the results are not, in fact, brittle.

5.3 Robustness of the Adaptive Strategy
The advantages of the adaptive strategy, as compared to
MRT performance, are the following. MRT relies on precise
knowledge of system and workload parameters. In practice,
these may be hard to determine accurately, and they may vary
over time. The performance of the Adaptive strategy is robust
with respect to workload and system parameters; M R T s performance is not. Consider the cases in which the router uses inaccurate locality distributions to estimate queue lengths and
response times when making routing decisions. Although the
determination of the preferred system for a transaction class is
fairly easy, determining the precise reference locality is a difficult
task. Simulations were conducted for cases where an inaccurate
locality distribution [e'] is assumed in the router for making

-

The response time estimate in Section 3 depends on various
system parameters, of which communication overhead is critical.

410

As shown in [Yu86, Yu881, MRT does much less remote routing
when communication overhead is high than when communication overhead is low. When communication overhead is low,
MRT can afford to do more load balancing, and thus reduces
overall response time. Consider the situation where the router
does not have accurate information on communication cost. This
may happen either because system pathlength information is not
readily available or because it is not easily determined. Simulations were run for various inaccurate assumptions; in Figure
5.5. we graph mean response time against the TJ values (defined
in Section 2) assumed by the router. The true value of 9 is 0.25:
because the router has inaccurate information about the actual
communication cost q vanes from 0.05 to 0.45. When the router
underestimates the communication cost (low T J ) , MRT does
much load balancing that it really should not do. Because the
MRT strategy has no way to determine that its load balancing
efforts arc not working as they should, thc swapping ratio (in the
worst case) approaches loo%, and response time is almost
double that of the case of accurate information. The adaptive
algorithm, in contrast, gets feedback that the remote system
costs more than predicted. Even though the adaptive strategy
does not know why remote decisions turn out badly, it simply
imposes a higher fi5_ to reduce the number of non-preferred
routings. In Figure 5.6 we show the average fi5_ values for the
situation in which the router has inaccurate information about
communication overhead. These values indicate, for each transaction class, the router's bias towards doing remote routing for
that class. When the router is given information which undcrestimates the true communication overhead, the bias against remote routing steadily increases for each transaction class. The
router is therefore able to compensate for the inaccurate information, and does less non-preferred routing than the nonfeedback strategy does.
When the router overestimates
communication cost (high v), then the Adaptive strategy compensates by reducing the bias against remote routing. At an extreme, the & value actually becomes negative to encourage more
non-preferred routing.

6.0

Without feedback, the quality of the routing decision depends
heavily upon accurate information. We demonstrated that, by
adding the feedback control mechanism, the dependency on accurate estimation on these parameters has been greatly reduced
and better robustness is achieved. With the feedback mechanism, the system can detect that its attempts at load balancing
are not succeeding as they should, and takes this into account in
subsequent decisions. Although we illustrate the adaptive routing concept in a specific heterogeneous environment, we naturally expect that this concept can be applied to other types of
heterogeneity as well.

References
[Agra82] Agrawala, A. K., Tripathi, S. K., and Ricart, G . ,
"Adaptive Routing Using a Virtual Waiting Time
Technique," Vol. SE-8, No. 1, Jan. 1982, pp. 76-81.
[BardSO] Bard, Y., "A Model of Shared DASD and Multipathing," Conini. ofthe A C M , Vol. 23, No. 10, (Oct.
1950), pp. 564-572.
[C are841

Carey, M. J.. Livny, M.. and Lu, H., "Dynamic Task
Allocation in a Distributed Database System,"
Computer Science Technical Report 556, University
of Wisconsin-Madison, Scp. 1984.

[Chow791 Chow, Y-C. and Kohler, W. H., "Models for Dynamic Load Balancing in a Heterogeneous Multiple
Processor System," IEEE Trait. 011 Conipirters, Vol.
C-28, No. 5, (May 1979), pp. 354-361.
[Corn561 Cornell, D.W., Dias, D.M.. and Yu, P.S., "Analysis

of Multi-system Function Request Shipping", Vol.
SE-12, NO. 10, Oct. 1956. pp. 1006-1017.
[Eage85] Eager. D.L., Lazowska, E.D. and Zahorjan, J., "A
Comparison of Receiver-Initiated and SenderInitiated Adaptive Load Sharing", Perforniarice
Evaluatiotz Review, Vol. 13, No. 2 (Aug. 1954), pp.
1-3.

Conclusion

A load sharing mechanism is critical to the performance
of distributed database systems. It is often conceived as a
trade-off between the additional communication/transfer cost
incurred, and the benefit achieved by reducing response time of
processing. In a heterogeneous environment, routing-toshortest-queue type strategies do not perform well. More elaborate mechanisms are needed to capture the heterogeneity
introduced by database partitioning and different reference localitics. An adaptive strategy is proposed which not only uses a
steady state analysis to estimate the response time at each candidate system, selecting the one with minimum response time,
but also adaptively improves the estimate based on feedback of
actual response time achieved. It monitors the actual behavior
of the system to sec whether its expectations are in fact realized.
If its expectations are not realized, then the strategy compensates
by changing the extent of remote routing that is done in trying
to share the load. The estimation of response time requires information on transaction characteristics (e.g. reference locality),
system pathlength and configuration parameters such as eommunication overhead, processing capacity at each site, etc.

41 I

[Eage86]

Eager, D.L., Lazowska, E.D. and Zahorjan, J.,
"Adaptive Load Sharing in Homogenous Distributed
Systems", Vol. SE-12, No. 5, May 1986, pp.
662-675.

[Fed61

Ferrari, D., "A study of Load Indices for Load Balancing Schemes," Proc. of FJCC, Nov. 1986.

[Lee851

Lee, Y-H., Yu, P.S., Leff, A., "Robust Transaction
Routing in Distributed Sytems", Proc. Itit'!. Synipositrni on Databases in Parallel arid Distribrrted Systenis, Dec. 1988, pp. 210-219.

[LeffSS]

Leff, A., Yu. P.S., and Lee, Y-H., "Adaptive Transaction Routing in a Heterogeneous Database Environment", IBM Research Report RC 14114,
Yorktown Heights, NY, Oct. 1988.

[Ni85]

[Reis801

Ni, L.M. and Hwang, K., "Optimal Load Balancing
in a Multiple Processor System with Many Job
Classes," IEEE Transactions on Software Engineering, Vol. SE-11, No. 5 , May 1985, pp. 491-496.

I

I

Database Partition
Transactionclass 1
Transactionclsa2
Tran&ctionclass3

Reiser, M., Lavenberg, S. S., "Mean-Value Analysis
of Closed Multichain Queueing Networks" Journal
of ACM, Apr. 1980, pp. 313-322.

1

2

0.75
0.07
0.11

I

3

0.11 0.14
0.82 0.11
0.06 0.83

[Schwe79] Schweitzer, P., "Approximate Analysis of Multiclass
Queueing Networks of Queues", Proc. Int'I. Conf
on Stochastic Control and Optimization North
Holland, Amsterdam, 1979.
[TantgS]

Tantawi, A. N. and Towsley, D., "Optimal Static
Load Balancing in Distributed Computer Systems,"
Joitrnal of ACM, Apr. 1985, pp. 445-465.
Coefficienuof the New Model

[Wang851 Wang, Y.-T., and Morris, R.J.T., "Load Sharing in
Distributed Systems", IEEE Trans. on Conipriters,
Vol. C-34, No. 3, (March 1985), pp. 204-217.

[Yu86]

[Yu87]

[Yu88]

S - 0.71 c - 3K

I

Yu, P.S., Balsamo, S., and Lee, Y.-H., "Dynamic
Load Sharing in Distributed Database Systems,"
P~oc.Of FJCC, NOV.1986, pp. 675-683.

I

10x2

Txl

I

3.06 f 1.62

Tx2

I

2.88 f 1.00

~~

I
I

I

Tx2
rx3

I
I

2.71 f 0.75
1.84

0.81

0.24 f 0.41
0.32 f 0.25

I
I

rxi

S-0.8lc-lSK

[Zhou87] Zhou, S . , and Ferrari, D., "A Measurement Study
of Load Balancing Performance," Proc. 7th Int'l.
Cotif. on Distributed Conpiiting System, Sep. 1987,
pp.490-497.

2.57to.93

0.58 f 0.09

I 0.22 f 0.33

I

0.72 t 0.10

I

I

0.14 f 0.360

7
n

S-0.71c-3K

I n t e r m i o n Network

Roatainp
System N

S

S

- 0.71 c

- 0.81 c

S - 0.81 C -

PorlilionadDolabom

DBN

I
I

15K

3K

15K

I

o.mt0.12

1.92 t 0.58

0.68 f 0.07

0.13 f 0.21

0.66 f 0.08

0.16 f 0.22

Txl

l.llf0.46

0.8220.047

1.53f0.82

rx2

1.85 i 0.57

0.82 2 0.04

0.15

Tx3

1.27 f 0.58

0.86 f 0.04

0.005 f 0.41

0.39

d oapa faUab.l.md use

~ffKieflIS
Of

I

the New Model

Tx1

12.63f0.82

10.53fO.13

10.06t0.19'

Tx2

I

I

I 0.17 f 0.16..

2.67 f 0.86

0.52 f 0.13

TX3

2.42 f 0.93

0.55 f 0.14

0.26 f 0.22

Txl

2.37 f 0.64

0.61 f 0.08

0.57

Tx2

2.19 t 0.67

0.64 f 0.09

0.41 f 0.32

Tx3

2.00 f 0.71

0.66 f 0.09

0.52 f 0.35

Txl

1.09 f 0.57

0.78 i 0.07

0.29 f 0.23

f

1.58 i 0.59

0.73 i 0.07

0.17 2 0.24 *

rx3

0.76

0.84 i 0.08

0.26 f 0.18 **

Txl

I

0.64

0.91 f 0.42

I

0.87 f 0.04

I

0.57 f 0.30

Tx2

1.74 f 0.57

0.78 f 0.05

0.48 f 0.44

T.3

0.97 f 0.62

0.84 f 0.06

0.78 t 0.51

T~bk4.2 Regarb. oapp f a B.Lmd

aae

I
I

0.27

rx2

-

412

~~~~

2.15 f 0.62

Confidence intervals are at 99% k w l unkrr indicated ochnviv
MRT hypothesis cannot be rejected at ch 95% kvel
** C o d i d e m interval is at 95% kwl

Figure 1.1 The confiuntion of a distributed transaction processing system

~

0.i5f0.35.

Tx3

I

front-end

0.26.

0.10

Tx2

Tab* 4.1 R

1

~

0.67 f 0.16

~

~-0.81c-3~

Yu, P.S., Balsamo, S., and Lee, Y.-H., "Dynamic
Transaction Routing in Distributed Database Systems," IEEE Transactions on Software Engineering,
Vol. SE-14, No. 8, September 1988, pp. 1307-1318.

.... .

I

I 0.47 f 0.25 1
I 0.48 f 0.15 1

1.74 t 1.04

rx3

Yu, P.S., Dias, D.M., Robinson, J. T., Iyer, B. R., and
Cornell, D. W., "On Coupling Multi-systems
Through Data Sharing," IEEE Proceeding, Vol. 75,
No. 5 , May 1987, pp. 573-587.

Romring
sptem 1

I

I

Performance of the Adaptive strategy
with different weights for the coefficients
(balanced database load)

.

.

Performance of the Adaptive strategy
with different window sizes between regressions
(weighting of coefficient Is .75/.25)
1 007

.

.

c=JK. S= 71
c=15K.S=.71
c=3K. S= 81
c=15K.S= 81
4 cases r H h balancod dolabase load

.
c=3K,

S= 71

c=15K,S=.71
c=3K, S= 81
c=I5K.S= 81
4 cases wiih balanced dolabas. load

npurr 5.1

Remote System Bias under Adaptive strategy
with inaccurate locality information assumed by router

Performance of the routing strategies
with inaccurate locality distribution
assumed by front-end route&-- .81

*

c= 16K
balanced case

2

S= .81
c= 1SK

ooh

0

I

T,

balanced Case

\

000:

,

I

,

,

- 250 - 2'33 - 150 - 100 - 050 000
050
100
Perconfago chango in locality accuracy

.o. RT

(MRT) 0131
(Adoptwe)

.m. SWR

,

,

150

200

(MRT) e S W R (Adaptlve)

ngurr 5.4

Performance of the routine strate ies
with inaccurate communicatlon overftead
arsumed by front-end router

Remote System Biar under Adaptive strate y
with Inaccurate communication overhead assumed %y router

8= 0.81
balanced c a w

.
....

S= 0.81
balanced
balanced case
case

k

0

m-

OmOOOJ,
050

,

,

I

I

I

1

I

'

-ow!

100
150
200 ,250
XK)
350
400
450
Communicofion overhrad assumed bv front-end roufor
so. RT (MRT)
SWR (MRT)
R l (Adaptwe)
SWR (Adaptlve)

I*.

050

le

Rprr SJ

I

I

I

I

Fiprc 5f

413

I

I

8

100
150
200
250
300
350
400
Commvnicoiion ovwhood ossumod by froni-end rouior

I

450

2011 International Joint Conference of IEEE TrustCom-11/IEEE ICESS-11/FCST-11

Efficient Java Native Interface for Android Based Mobile Devices
Yann-Hang Lee, Preetham Chandrian, and Bo Li
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, AZ, 85287

to interact with native code components in their applications.
The NDK includes tools and builds files that can be used to
generate native code libraries from C and C++ sources, and to
embed native libraries into an application package file. Thus,
with NDK, libraries can be developed to build activity, handle
user inputs, manage hardware sensors, and invoke platform
specific operations by calling the drivers of the underlined
Linux system.
As more and more features and applications are being
added to the mobile devices, there is a need for an efficient JNI.
As illustrated in [7][8], the overhead incurred while making
JNI calls can be costly during the data transfer between the
JVM address space and the native address space. While it is
claimed in [7] that using the JNI with the native code is faster
than using Dalvik virtual machine, the communication delay in
JNI is not negligible, e.g., it takes 0.15 micro-second to pass a
string. In fact, the overhead can be significant when reflection
and serialization operations are included for passing arguments.
Note that serialization is to marshal and de-marshal the data
passed between Java and native modules, and can create deep
and multiple copies of objects as arguments of function calls.
For serializing non-primitive data, reflection mechanism will
be invoked to introspect the metatdata of binary class format to
discover object fields and methods at runtime.
In this paper, we report an experiment of developing an
efficient JNI mechanism. In the current JNI implementation,
each call to the JNI is treated as a new call without the
information of its previous history. Thus, a lot of information
collected in previous operations is lost. If this information were
available for the subsequent calls, the execution time of the JNI
can be reduced. For instance, the overhead of searching the
class structure can be avoided, if we can cache the class
structure and the field identifiers of object attributes. Also, the
transfer of data can be simplified if we can pin the memory
address of the object or array that we intend to manipulate.
The proposed solution alters the JNI environment structure
by including a hash map to store the recently accessed field
identifiers and methods. It also caches the class structure in the
JNI environment structure. A new method is introduced to pin
objects to their current memory location so that the same
addresses can be used in future accesses. This decreases the
time of referencing the objects and avoid additional calls to JNI
as we already know its location in the memory.
In the following, we briefly introduce the JNI mechanism
and Android platform. A concise survey of the related work in

Abstract— Java has been making its way into the embedded
systems and mobile devices like Android. The Java platform
specifies the Java Native Interface (JNI) which allows Java code
that runs within a JVM to interoperate with applications or
libraries that are written in other languages and compiled to the
host CPU. JNI plays an important role in embedded system as it
provides a mechanism to interact with libraries specific to the
platform and to take the advantage of fast execution of native
programs. To address the overhead incurred in the JNI due to
reflection and serialization, this paper proposes to cache class,
field, and method information obtained from reflection for
subsequent usage. It also provides a function to pin objects to
their memory locations such that they can be accesses through
the known reference. The Android emulator is used to evaluate
the performance of these techniques and we observed that there
was 10- 30 % performance gain in the Java Native Interface for
two Android applications.

I.

INTRODUCTION

With the introduction of JavaME (Micro Edition) by Sun
Micro System for mobile and embedded devices, Java has
applied widely to embedded systems and mobile devices. A
notable application is the Android phone platform on which
mobile computing applications are developed in Java and run
on Dalvik virtual machine [1][2]. The applications like contact,
email, browser settings, Bluetooth etc., come with the Android
package. Additional applications can be acquired through
Android Market [3] and installed in individual phones.
While the Java applications on Android platform are
portable across various phone devices, there are several issues
to be considered for this approach:
• Standard Java libraries do not support hardware-platform
specific features that are needed by the applications,
including user interface and graphic rendering.
• The developers may want to use libraries written in
different languages because of the fact that these libraries
are more efficient than their Java counterparts.
The second issue has a profound impact to the user
experience not only in fast response time of application
operations, but also in energy consumption and battery life.
To address these issues, JNI (Java Native Interface) has
been specified and applied to integrate native code and Java
program in a single application [4][5]. It has played a major
role in developing application which needs to interact with
native code (C/C++). Android uses JNI in its NDK (Native
Development Kit) [6], which is a toolset that helps developers
978-0-7695-4600-1/11 $26.00 © 2011 IEEE
DOI 10.1109/TrustCom.2011.162

1202

JNI mechanisms and performance improvement is given in
Section III. Our design and implementation follows in Section
IV. In section V, we show the evaluation results with two real
Android applications. Finally, we describe the future work in
the conclusion section.
II.

achieving isolation against other suspicious applications. Each
application runs in its own Linux process. Furthermore, each
managed piece of code executes in a virtual machine (DVM).
As a result each application is sand-boxed from the other
applications running at any given time. All IPC is achieved via
the mechanisms provided by the Binder module. A second
level of isolation builds upon the capability of underlying
Linux to strongly isolate data/files of one user from the other.
This is achieved by allocating a unique user-id to each installed
application on a particular system. Android starts the process
when any of the application code needs to be executed, and
shuts down the process when it's no longer needed and other
applications are in need of resources.
JNI (or NDK in Android) is a part of the Java virtual
machine implementation. It enables the two-way
communication between native and Java programs. Once JNI
finds a method that needs to be executed, it transfers the control
to the DVM interpreter. The interpreter checks weather the
function is Java function or native. If the function is Java then
the byte code is interpreted into the architecture specific code.
When a native function is called, JavaVM and JNIEnv
arguments will become the function interfaces to the supported
JNI functions. JNI function FindClass should be called from
the native function to find the class by name. Similarly, to get
an instance field or method, native program can invoke JNI
functions getFieldId and getMethodId with class, field (or
method) names. For these invocations, symbolic lookups based
on the name and descriptor of the field or method is required.
Symbolic lookups are relatively expensive and can be avoided
by caching field and method IDs in static variables in native
programs, as suggested in [4].

JNI AND ANDROID PLATFORM

A. JNI
The Java Native Interface is designed to allow the
programmer to take advantage of the Java virtual machine, but
still can use code written in other languages and libraries. With
JNI, Java applications call native methods in a similar way as a
simple Java method invocation. The only difference is that the
method it invokes has the keyword native in its prototype and
these Java calls are internally translated to invoke the native
methods or library. On the other hand, the JNI supports an
invocation interface that allows the invocation of Java virtual
machine code from the native code. This is done via a native
library that uses Java API.
When a native function is called, one of the arguments
passed to the function is the JNIEnv interface pointer. The
JNIEnv pointer links to a memory location containing a pointer
to a function table. Each entry in the function table points to a
JNI function. The JNIEnv interface pointers links to threadlocal data and is organized like a C++ virtual function table.
Native methods always access data structures in the Java
virtual machine through one of the JNI functions. The other
arguments provide instance or class information. If the native
method is an instance method, the argument is a reference to
the object on which the method is invoked, similar to this
pointer in C++. For a static native method, it is a reference to
the class in which the method is defined.
The Java native interface provides various functions that
can be called to access the Java objects and methods [4]. For
instance, to access arrays, it provides get<Type>ArrayRegion .
If an array contains objects in Java, each element can be
accessed individually, and whenever we access the data, it is
copied from the Java heap to native region.

III.

RELATED WORK

From the discussion above it is clear that if we could
decrease the time required to retrieve an object in JNI and the
time taken for the reflection and serialization of the data that is
being transferred from the JVM to the native space, a better
performance and a save of battery life of the device can be
obtained. Arrays are the main area of concern as a huge amount
of data needs to be moved to and from between JVM and
native spaces. If the class object is huge, the time spent on
finding the attributes that we are interested in becomes
significant. In [11], Hirzel et. al., discussed the issues of
runtime pointer analysis while using reflection, dynamic
loading of libraries, and other JVM mechanisms, as well as
some of the JNI APIs which rely on finding and loading of
classes using reflection.

B. Android Architecture
Android is a software stack for mobile devices that
includes an operating system, middleware, and key
applications. The major components of the Android operating
system include Linux kernel, Android runtime (including
Dalvik VM [9][10] and core library), C/C++ libraries (as
system components, e.g., OpenGL, media framework, SQLite,
etc), and application framework (for phone functions, e.g.,
location manager, activity manager, etc.)
Android applications are written in the Java programming
language. The Java code, all the necessary data and resource
files of the application are bundled by the “aapt” tool into an
Android package. This file can then be used for installing the
application on devices. All the code in a single Android
package file is considered to be one application.
Android application sandbox model uses the process
separation provided by Linux kernel as the primary means of

A. JNI Bridge
One reason to use Java is that it can be ported easily on
different platform as the application runs inside the virtual
machine. But if the application uses native calls to the libraries
that are specific to the platform, the porting becomes difficult.
This paper on JNI Bridge [12] describes the challenges and
solution so that the JVM supports the native calls on different
instruction set architectures and dynamic translators are used to
translate native calls based on the underlying architecture. To

1203

handle the JNI up-calls and data marshaling, a simulated
JNIEnv object in the IA-32 execution environment is used to
enable 32-bit native libraries to call 64-bit function pointer.
They also use marshaling tables to map 64-bit references to 32bit references by intercepting the up calls and wrapping it with
the reference and during the down call the corresponding 32-bit
reference is used. To avoid the data movement when
GetPrimitiveArrayCritical JNI API call is made, the reference
is directly taken from the JVM internals. The JVM-independent
implementation has to resort to Java reflection to obtain this
information.

D. Inlining Java Native Calls At Runtime
This technique inlines the native functions using JIT in java
applications. The callbacks to the JNI are transformed into their
equivalent lightweight operations [15]. IBM TR JIT [15]
compiler is used as it supports multiple JVMs and class library
implementation. The control flow for the TR JIT consists of
phases for intermediate language (IL) generation, optimization
and code generation. The inliner is enhanced so that it can
synthesize the opaque call to the native function. In addition, a
callback transformation mechanism is introduced that replaces
the expensive callbacks with compile-time constants and byte
code equivalents, while preserving the semantics of the original
source language.

B. Interfacing Java to the Virtual Interface Architecture
This paper in [13] explores the use of user-level network
interface for the communication between the Java heap and
native buffer. It describes two approaches: the first approach
manages the buffer between the Java heap and the native space
which requires the data to be copied while the second approach
uses a Java-level buffered abstraction and allocates space
outside the Java heap. This allocated space can be accessed like
array in the Java space. The second approach eliminates the use
of copying the data but the native garbage collector has to be
modified.
The first level of Javia (Javia-I) [13] manages the buffers
used by VIA in native code (i.e. hides them from Java) and
adds a copy on the transmission and reception paths to move
the data into and out of Java arrays. Javia-I can be implemented
for any Java VM or system that supports a JNI-like native
interface. (Javia-II) introduces a special buffer class that,
coupled with special features in the garbage collector,
eliminates the need for the extra copies. In Javia-II [13], the
application can allocate pinned regions of memory and use
these regions as Java arrays. These arrays are genuine Java
objects (i.e. can be accessed directly) but are not affected by
garbage collection as long as they need to remain accessible by
the network interface. This allows the application to manage
the buffer and to send or receive Java array directly. The issues
of memory management when application creates memory
outside the Java heap are also discussed in the paper.

E. Janet
Janet [16] is the Java language extension which enables
convenient development of Java to native code interfaces by
completely hiding the JNI layer from the user. The source file
is similar to ordinary Java source file except that it may contain
embedded native code (in terms of native method
implementations), and the native code can easily and directly
access Java variables as Java code would. It enables efficient
direct access to Java arrays from the native side. However,
when the array is to be processed by external routine the array
pointer has to be used. Java types are converted by Janet
generally to native types having the same name. The array
conversion introduces no performance reduction on platforms
where appropriate Java and native types are equivalent, but it
requires allocation and copying of the whole array in the case
when they are different.
Other research work includes the analysis of the use of
reflection and its internal working analyses [17]. The paper
proposes a class named SmartMethod which transforms the
calls made by the use of reflection to direct calls that will be
carried out similar to the standard Java method invocation.
SmartInvokeC [17] tool is used to generate the stub of a class
from its byte code, so to invoke a method we no longer need to
use the JNI. The call is made from a C stub that is generated.
To retrieve and invoke method faster the necessary information
is hashed. Tamar et al., provides a memory management
scheme for thread local heap in [18]. This technique determines
the objects that are local and global and uses this information to
avoid unnecessary synchronization.
In addition, the technique used to profile native code that is
a part of the application is discussed in [19]. Most of the
profiling tool like 'hprof' do not segregate the time spent in
native code if we have this information then we can find the
parts of the code that can be improved further. The paper [19]
introduces a profiling tool based on JVM tool interface. The
technique involves introduction of a wrapper methods for the
native function prototype in Java. It has the same method name
and signature as that of the native method but not a native
method. This wrapper function calls the J2N_Begin which
recodes the time stamp and other profiling information. Then it
calls the native method. Upon return the wrapper calls
J2N_End is called which records the exit timestamp.

C. Jeannie
Instead of JNI, Jeannie, a new foreign functional interface
is designed in [14]. Through Jeannie, programmers can write
both the Java code and the native code in the same file. Jeannie
compiles these files down to their respective JNI calls. This
enables static error detection across the languages and
simplifies the resource management. It addresses the issues of
JNI being unsafe as it does not require dynamic checks. By
integrating and analyzing both Java and C together, the
compiler can produce error messages which can prevent many
maintenance issues. The compiler is implemented using rats!
[14]. To access string from C in Java, conversion from UTF-8
to UTF-16 is made, and vice versa is done while accessing
strings from Java in C. The array region is still copied from the
Java heap to C memory space when access is made.

1204

IV.

DESIGN AND IMPLEMENTATION

The keys used in hash function are strings and are of UTF8
format. To compute the hash of the string we use the
predefined function in the UtfString file and the formula to
calculate the hash is
hash = key *31 + value of character
This hash is computed for the whole length of the string.
Here we presume that add and look up happen more frequently
than the remove. Whenever remove function is called there
should be an explicit call to the free function as well as the
remove function does not invoke it internally. If the free
function is not called then there are high chances that we will
be running out of space in the local reference table of the JNI
environment. Similarly the free function also handles the global
references that we have created for the values that we store.

To reduce the invocations to search operations and data
copying operations, our design focuses in 5 methods of JNI
interface, findClass, getFieldID, getMethodID, pinObject and
unpinObject. The first 3 JNI functions are invoked to obtain
accesses to object fields and methods. The modification is to
keep the information obtained from the first invocation
available for subsequent usage. For the last two methods, the
objects in memory are pinned to their existing location via JNI,
and would not be moved by garbage collection. Thus, object
data can be accessed directly from the memory heaps.
A. Hashing JNI fields
There are several ways to store the data and use it in the
future so the process of the next call made to the function can
be expedited. Hashing is the technique used here to make the
JNI calls efficient. As a JNI call accesses object fields in the
Java domain it first needs to know the exact memory locations
of the fields. Note that each function in the JNI API has the
access to the pointer of the JVM environment that it is running
in. This JVM environment pointer contains the pointer to the
heap and the references to the objects. It can also access the
structure of the classes that are loaded and that are present in
the class path or in the jar file that is included in the class path.
When we make a JNI request to access a field, the JNI API first
checks if the class is loaded or not. Then it accesses the class
structure and goes through the field list. If it finds the field that
we are looking for it returns the fieldId to the caller and the
caller can access this field's data using this Id.
To accelerate this search process, hashing technique is
used. There are three hash tables. The first one, refEntryTable,
is to hash and store the class structures. The second one,
methEntryTable, is used to store the offset and the methodId of
methods that are called through the JNI regardless of whether it
is native method or Java method. The third one, fieldEntryJni,
is for the fields that are accessed by the JNI in its native
domain. These hash tables are initialized when the new JNI
environment variable is created when dvmCreateJNIEnv is
called during API invocation.
The hashing bypasses the search operations that need to be
made every time when we need to access a method or field.
Now due to hashing the symbolic, search operation is made
during the first access and any future access will use the value
obtained from the hash table. One problem with this approach
is that we will not know the size of the hash table that we need
to create. This issue is addressed by increasing the size by a set
amount when the hash table reaches its limit.
To deal with possible collisions in hash tables, a probing
technique is used where a newly inserted entry is saved in the
next available free slot. This technique is chosen over the
chaining for collision resolution as it uses the memory in an
optimal way. During hash table accesses, we need not address
the synchronization issue as JNI API is executed by a single
thread and there is only one thread accessing the JVM
environment variable.

B. Find Class
The find class in the JNI is used to load the class given the
fully qualified class name in the JVM pointed by the JNI
environment object. The find class takes in two arguments: the
JNI environment object and the class name. The findClass first
make a check by calling dvmGetCurrentJNIMethod, this
method check if the current thread is executing a native method
if so it returns the method by inspecting the interpreter stack.
Then we get the class descriptor form the class name, load the
class from its class loader, and add the local reference in the
reference table. This is an important step as if the class does not
have a reference then this class cannot be accessed in the native
stack. Finally, the reference is returned to the caller who can
use it to access fields and methods.
If the class was already loaded then we could reuse this
instance of the class to get the fieldIds and methodIds. To
accomplish this we store the reference of the class in the hash
table refEntryTable in the JNI environment variable. Every
time the findClass is called we check if the class is already
loaded by looking up in the hash map. If not we proceed as
usual and load the class. After loading the class we add this to
the hash map. If the class we are looking for is present in the
hash map we validate its reference as it may be garbage
collected. Then this instance is returned to the caller. Note that
the class descriptor, obtained by calling the method
dvmNameToDescriptor is used as the key for hashing operation
rather than the class name. This is because that the two classes
can have the same name but their descriptors are different. The
flowchart of findClass with hash table in illustrated in Figure 1.
C. Get Field
The JNI getFieldId method is used to get the instance field
given the field name, its signature, and the Java class. To
access any field in a Java object, we need to use this function.
It returns the fieldId in the class structure through which we
can get the offset from the object pointer. The Java class
structure is obtained from the findClass method. Once the
reference in the local reference table of the JNI is obtained, a
call to dvmDecodeIndirectRef is made to receive the reference
of the actual class. If the class is initialized, the steps to find the
filedId are as follows

1205

• It first checks if the class is loaded and initialized. If not
an exception is thrown.
• It then goes through the list of methods in vtable to check
if the method and the signature are present and checks if
the method is static. If a virtual function is found and is
not static, the methodId is returned.
• If it is not a virtual function then it goes through the
directMethods list to check if it is present in the list. If
present, it checks if the method is static or not. If static
then it throws exception else returns the methodId.
• The method's class may not be the same as the class that
is supplied. In this case, it must be a virtual method and
the class must be a superclass which should be initialized.
To simplify the process of getMethodId, we add the method
Id to the hash table represented by methEntryTable. When a
call is made, we check if the method is present in the hash
table. If the method is found, we check if the class and its super
class are initialized or not. This is done by checking the hash
table for the refEntryTable. If the class is not initialized, we
initialize it and add it to the refEntryTable and to the local
reference. The key used in the hash table is the class descriptor,
the method name, and the signature. The complete operation of
the getMethodId is illustrated in Figure 3 Error! Reference
source not found.Error! Reference source not found.Error!
Reference source not found.. If we find that the method is
static then we throw an exception as static methods cannot be
accessed by the getMethodId. Also check is made to see if there
is a code segment to execute in the method while returning the
method Id so that we do not catch any abstract method.

Figure 1 findClass execution flow
• The class object is searched for the given field name and
signature.
• This is done by walking through the ifields of the class
that are provided. If found, the filedId is returned.
• If it is not found in the class, then it needs to check any
super class. If yes, a search for the ifields of the super
class is carried out. If it is found, the fieldId is returned.
Otherwise, an exception is thrown.
The execution time of getfieldId method can be substantial
due to the search operation of multiple lists. However, once a
fieldId is found, it can be saved in a hash table to avoid
following search for the same field as depicted in Figure 2.
Note that multiple classes may have the same field name and
signature. To make the key unique in hash table, a combination
of field name, field signature, and class descriptor should be
considered. In addition a check for the loaded class is required
when we return the fieldID from the hash table. If the class that
the field belongs to is not loaded, any further use of the Id will
generate exception. This can be taken care of by looking into
the hash table of findClass. If the class is not loaded then an
exception is asserted.

E. Pinning and Unpinning Object
To access objects, applications need to go through the
several steps in JNI. This means that they need to do find class
first then find the field offset and the get the value by invoking
the get method for the object. This is because the garbage
collector may move the object. However, if we pin the object,
we are guaranteed that the object will be in same memory
location. This is very helpful as we can access array of objects
if we know the starting location in memory and size of the
object instead of calling getObjectArrayElement.
Two new functions, pinObject and unpinObject, are
implemented to avoid the complex process of accessing
objects. If the memory location of an object is pinned and is
known in the native space, its field can be accessed by knowing
the offset. Pinning not only allows the access to the object but
makes sure that the memory location of the object would not be
changed. Obviously, it is important to unpin the object once we
are done using it as these can be picked up by the garbage
collector.
The pinObject method adds an entry to the global DVM pin
reference table. This is done by making a call to
dvmAddToReferenceTable(&gDvm.jniPinRefTable,
(Object*)Obj)

D. Get Method
The getMethodId JNI function is used to find the methodId
for a given set of method name, signature, and class. If the
class is loaded, the offset from the class pointer can be obtained
based on the methodId. The method can then be invoked. The
following steps are performed by this function.

is the field
present in
hash table ?
no
is the field
present in
ifield of the
class?

yes

yes

no
No field can be found,
an exception is
thrown

no

is the field
present in
ifield of
super class?

yes

If the class is loaded,
return fieldId, else
throw exception

This assures that, once we add this object to this reference
table, the garbage collector will no longer move or reclaim this
memory location. The unpinObject function removes the

Figure 2 The operation of getFieldId Mothod

1206

The time is logged into the system log file using the
command LOGI(...). This is a predefined function in the Dalvik
virtual machine which internally uses printf command to write
strings into the system log buffer and then the buffer is flushed
to Android Debug Bridge console. The log statements have
been put in the JNI_ENTRY and JNI_EXIT macro. These
macros are invoked each time there is a call to JNI API. In each
of the JNI API we log the method name so that we can identify
the time taken in each of these calls. In addition, these log
messages are tagged with their corresponding processId and
threadId.

reference from the global DVM reference table. We also
remove the global reference that we added. To access the
global DVM reference table, a mutex lock over the table must
be obtained.

A. Profiling Results
As explained earlier, we use the logging facility provided
by the Android Operating System and the system clock to
profile the JNI calls. In the first experiment, each of the
applications Lunar Lander and JetBoy is executed 1000 times.
The average execution time of each JNI call is listed in Table 3.

Time (ms)

Find
Class

Get
Method
ID

Get
Filed
ID

JNI

708.2

147.8

133.2

269.4

308

80.7

JNI with
changes

606.4

117.1

114.9

198.4

281.6

82.2

% of
change

-14.4%

-8.6%

1.8%

Get
Dvm GetStatic
Static Create Method
FiedlID JNIEnv
ID

-20.8% -13.7% -26.4%

(a) Lunar Lander

Figure 3: The operation of getMethodId
V.

EVALUATION

Time (ms)

Find
Class

Get
Method
ID

Get
Filed
ID

JNI

638.4

206.3

133.2

362.3

308

80.7

572.4

168.5

92.9

294.4

281.6

82.2

-8.6%

1.9%

JNI with
changes
% of
change

Experiment has been carried out for the comparisons
between the JNI in the Dalvik virtual machine and the JNI with
the proposed changes. To evaluate the execution time in the
JNI we record the system clock in microseconds. This is
accomplished by the following function is used to get the CPU
time.

-10.3%

Get
Dvm GetStatic
Static Create Method
FiedlID JNIEnv
ID

-18.3% -30.3% -18.7%

(b) JetBoy
Table 1: The average execution time of JNI methods in Lunar
Lander and JetBoy applications

static inline u8 getClock()
{
#if defined(HAVE_POSIX_CLOCKS)
struct timespec tm;
clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tm);
if (!(tm.tv_nsec >= 0 && tm.tv_nsec < 1000000000)) {
LOGE("bad nsec: %ld\n", tm.tv_nsec);
dvmAbort();
}
return tm.tv_sec * 1000000LL + tm.tv_nsec / 1000;
#else
struct timeval tv;
gettimeofday(&tv, NULL);
return tv.tv_sec * 1000000LL + tv.tv_usec;
#endif
}

The reduction in execution time of JNI method calls is due
to the fact that the subsequent calls to these methods need not
go through the symbolic search for the object to get the field or
class. The calls still require when the object is initialized so that
the reference is present in JVMs local reference. There are
some differences of the percentage of reductions in the two
applications. JetBoy implements an interactive music
soundtrack adapted to user action. In Lunar Lander, users try to
land on the moon by controlling the lander using keystrokes. It
demonstrates the drawing resources and animation in Android.
So, based on the classes, methods, and fields, defined in the
applications, they experience various execution times of JNI
operations and results in different reduction. The results on the

1207

last two JNI methods in the rightmost columns are the same as
the methods are invoked to start the applications and don’t
depend upon application data.
While there are 10% to 30% reductions in the execution
times of application-dependent JNI calls, it is interesting to
look into the overall impact to application’s execution. We
consider JetBoy application and record its execution times in
Java, native calls and Linux OS. For this we use gprof to
profile the emulator. To address the issue of OS scheduling that
is done by the kernel when multiple applications are running,
only one application is loaded into the emulator so that all the
resources are used by this application. The application changes
the music based on the events that occur in the game. It uses
the JET library that is provided by Android to play the music
files. OpenGL is used to render the graphic content that is used
in the game. Both music playing and graphic rendering use JNI
by the means of NDK library. The measurement is taken for
approximately 2 seconds after the application is started. The
results are shown in the following figure.

improvements of native method and the one with the modified
JNI can be noted when the array size increases and more data
to be moved from the Java heap to the native space. Even with
the JNI overheads, the use of native methods can be valued in
mobile devices in terms of execution time and energy
consumption.

Dalvik Java
(ms)
JNI modified
(ms)
Android JNI
(ms)

10
00
20
00
30
00
40
00
50
00
60
00

50
0

90
80
70
60
50
40
30
20
10
0

Figure 5 Execution times of heap sort of various size arrays
Environment
Java
JNI
OS and others

with original JNI
1.49
0.37
0.1

with modified JNI
1.48
0.31
0.1

VI.

CONCLUSION

JNI has been an important part of the Java virtual machine.
It is widely used in embedded applications as an interface to
architecture specific libraries in native code (C/C++) which can
improve the performance of the applications and provide
access to platform hardware devices. Our technique reduces
the overhead of the JNI functions for accessing Java objects,
fields, and methods. The pinning of objects helps the
programmer to reference the data through its memory location
rather than copying the object into the native space. There is a
performance gain of 10-30% in the JNI functions achieved
using the proposed technique.
Our analysis shows that there are still a significant overhead
in JNI that should be consider further. This overhead is caused
due to the separate executions spaces. Currently we are looking
into the stack when the transfer is being made to the JNI and
back. Also the profiling of the JNI is done using the execution
time of the applications. To gain a deeper insight for the areas
of improvement, the instruction profile in the JNI should be
inspected.

Figure 4 Execution times of JetBoy with the original modified
JNIs
There are 1173 and 1184 calls made to the JNI interface
during the program execution with the original and modified
JNI. Time is captured in the JNI API using the system time
stamp. The number of JNI calls is recorded in a static variable
declared in the JNIEnv variable. Once the JNI destroy was
called we printed it to the log. Then the execution times are
summed up all the count. Though the number of calls remained
almost same there is an improvement in the JNI time as the
table shows. This is mainly due to the calls made to the
findClass and getFieldId API methods. Note that more than
75% of JetBoy execution time is in Java space. This percentage
should be reduced substantially if the Android JIT is applied in
the experiments.
To investigate the effect of data movement, we test the
running time of a heap sort algorithm on a data array. First, the
heap sort is implemented in Java and recorded the execution
time. Then, using JNI, the heap sort is implemented in the
native domain. Then we ran the same code and the data on the
modified JNI. In Figure 5Error! Reference source not found.,
the average execution times of 1000 runs are shown. The

REFERENCES
[1] “Android
Open
Source
Project”,
http://source.android.com/index.html
[2] Damianos Gavalas and Daphne Economou “Development
Platforms
for Mobile
Applications: Status and
Trends” IEEE Software, Vol. 28, Issue 1, Jan.-Feb.
2011, page. 77-86.
[3] “App – Android Market”, https://market.android.com/.
[4] S. Liang. The Java Native Interface: Programmer’s Guide
and Specification. Addison-Wesley, June 1999.
[5] Java
Native
Interface.
http://download.oracle.com/javase/1.3/docs/guide/jni/inde
x.html.

1208

[6] “Android Native Development Tools – What is NDK,”
http://developer.android.com/sdk/ndk/overview.html.
[7] Sangchul Lee
and Jae Wook Jeon “Evaluating
Performance of Android Platform Using Native C for
Embedded Systems” 2010 International Conference on
Control Automation and Systems (ICCAS), pp. 1160 1163.
[8] Dawid Kurzyniec and Vaidy Sunderam, “Efficient
cooperation between Java and native codes – JNI
performance benchmark.” 2001 International Conference
on Parallel and Distributed Processing Techniques and
Applications, (PDPDA), 2001.
[9] Google,
“Android
2.3
User
Guide”,
www.google.com/googlephone/AndroidUsersGuide2.3.pdf.
[10] David Ehringer, “The Dalvik Virtual Machaine
Architecture,”
http://davidehringer.com/software/android/The_Dalvik_V
irtual_Machine.pdf.
[11] Martin Hirzel, Daniel Von Dincklage, Amer Diwan, and
Michael Hind, “Fast online pointer analysis”, ACM Trans.
Programming Languages and Systems, Vol. 29, Issue 2,
April 2007.
[12] Miaobo Chen, Shalom Goldenberg, Suresh Srinivas,
Valery Ushakov, Young Wang, Qi Zhang, Eric Lin, and
Yoav Zach, “Java JNI Bridge: A Framework for Mixed
Native ISA Execution”, Proceedings of the International
Symposium on Code Generation and Optimization (CGO
’06), pp. 65—75.
[13] Chi-Chao Chang and Thorsten von Eicken “Interfacing
Java to the Virtual Interface Architecture” JAVA '99

Proceedings of the ACM 1999 conference on Java
Grande, pp. 51-57.
[14] Martin Hirzel and Robert Grimm, “Jeannie: Granting Java
Native Interface Developers Their Wishes” Proceedings
of the 22nd annual ACM SIGPLAN conference on Objectoriented Programming Systems and Applications
(OOPSLA '07), pp. 19 - 38
[15] Levon Stepanian, Angela Demke Brown, Allan Kielstra,
Gita Koblents and Kevin Stoodley, “Inlining Java Native
Calls At Runtime”
Proceedings
of
the
1st
ACM/USENIX international conference on Virtual
Execution Environments (VEE '05), pp. 121--131.
[16] Marian Bubak, Dawid Kurzyniec, and Piotr Luszczek
“Creating Java to Native Code Interfaces with Janet
Extension” Proc. SGI Users's Conference, Oct. 2000, pp.
283—294.
[17] Walter Cazzola “SmartMethod: an Efficient Replacement
for Method” Proceedings of the 2004 ACM symposium on
Applied computing (SAC'04), pp. 1305 - 1309,
[18] Tamar Domani, Gal Goldshtein, Elliot K. Kolodner,
Ethan Lewis, Erez Petrank, and Dafna Sheinwald,
“Thread-Local Heaps for Java” Proceedings of the 3rd
International Symposium On Memory Management
(ISMM '02), pp. 76-87.
[19] Walter Binder, Jarle Hulaas and Philippe Moret “A
Quantitative Evaluation of the Contribution of Native
Code to Java Workloads” Proceedings of the 2006 IEEE
International Symposium on Workload Characterization
(IISWC 2006), pp. 201-209.

1209

Real-Time Communication in Multiple Token
Ring Networks
Yann-Hang Lee and Li-Tao Shen
Department of Computer and Information Sciences
University of Florida, FL 32611

within the given time constraints [8,91. The different performance metrics, reliability requirements, and performance
trade-offs suggest that the control protocols previously developed for traditional communication applications may not
be well suited for time-constrained applications.

Abstract
For communication networks in distributed real-time environments, it is required that the delivery time of individual message meet the timing constraints and the average
transfer delay for all messages be minimized. These special real-time requirements are usually not satisfied by the
existing network protocols. This paper presents a communication architecture and a dynamic control protocol for
real-time communication in multiple token ring networks.
The network can be formed by multiple channels through
bandwidth subdivision of a high speed ring. A flexible preemption and dynamic load allocation scheme is developed
which can reduce the lost percentage of critical packets and
maintain a high channel utilization at the same time. This
performance improvement is demonstrated with extensive
simulation results.

1

Token ring network systems have been studied and also
widely used during the past decade [4]. The basic single
token ring network consists of a number of stations N attached on a ring and a control token rotates around the
ring, station by station. If the station receiving the free
token has message packets to transmit, it converts the free
token into a connector and then follows the connector with
its sending message. If the station has no message waiting
for transmission, it simply passes the free token to the next
station. The source station has the resposibility to remove
the packet from the ring and to generate a new free token
after removing the packet and then passes it to the next
stat ion.

Introduction

Compared to other structures, the ring networks have
the good properties of bounded transfer delay and better
channel utilization [l, 31. However, the transfer delay will
be considerably high under moderate and high load. It is

Real-time communication networks are distinguished from
the normal network systems with the introduction of time
constraints. They are used to insure on-time ddivery of
messages and to support distributed real-time computation. The performance measures of such networks differ
from those of the conventional networks. The principal performance considerations for the conventional network control protocols are to maximize the system throughput and
to minimize the average delay. In real-time communication system, however, the principal performance consideration is rather the percentage of messages that are delivcrcd

cven worse when the messages are critical packets with realtime requirements. This is due to the fact that most current
protocols aim to minimize the average transmission delay
and have no mechanism to favor the critical packets.
For these real-time network systems, it is obvious that
the round-robin methods resulted from token rotation are
not appropriate. The priority based methods, which aim to
favor messages according to their priorities, are the currently
prevailing techniques. A real-time scheduling method for
prioritized messages has been proposed and investigated by
Strosnider [ll]. Using IEEE 802.5 token ring protocol, it
!.as been shown that a better real-time performance can be

This research is supported in part by a grand from Florida
High Technology and Industry Council.

CH2933-0/90/0000/0146$01.OO 0 1990 IEEE

146

erating on several slower channels so that the propagation
delay and remaining penalties become a smaller fraction of
the packet transmission time, and (ii) they can be easily implemented by expanding the existing interface technologies
based on medium speed.

achieved by choosing a proper packet size and working in
priority mode.
Currently a high speed optical fiber ring network known
as FDDI is being developed and standardized. The key characteristics of FDDI include optical transmission medium,
fair and robust control protocol, a datarate of 100 Mbit/s,
a ring length of up to 100 km and up to 500 stations on
the ring. A fully decentralized priority mechanism is used
in FDDI supporting synchronous and asynchronous transmission modes. Synchronous transmission is real-time sensitive and the delay of synchronous transmission is limited
by reserving an appropriate bandwidth at ring initialization. The remaining available bandwidth can be used for
asynchronous operation. Performance analysis has shown
that the throughput and the real-time response of the FDDI
cannot be optimized simultaneously, especially for long ring
lengths [a].

Several approaches have been suggested for multiple ring
networks [5]: (i) separate queues with simultaneous transmissions, (ii) single queue with simultaneous transmissions,
(iii) single queue with single transmission. The analysis
and simulation results showed that the single queue with simultaneous transmissions protocol has a better performance
than the other two protocols but its interface design is more
complex than the others.
In this paper, we propose a new protocol which is designed for real-time communication in multiple token ring
networks. With proper channel allocations and priority
reservations, the protocol can reduce the percentage of messages that miss their time constraints and also maintain a
high channel utilization. It is a dynamic control policy, flexible to any kind of system load, and easy to implement.

Though the priority based and bandwidth allocation methods are much better than the conventional methods in
real-time applications, there are still some problems:

0

The rest of the paper is organized in the following way.
In the next section, the proposed protocol is introduced and
discussed in detail. In Section 3, simulation models based on
various control protocols are discussed. System assumptions
and simulation results are also presented. Finally, Section
4 summarizes the results of the research and outlines some
future work.

The introduction of a high bandwidth channel may be
accompanied by only insignificant increase in system
capacity, and the increase of channel bandwidth can
only be partially utilized;

0

The distribution of time-constrained messages is usually not predictable. A static allocation of bandwidth
is hard to meet the general real-time requirements;

0

To preempt the normal token rotation, a lot of time
is wasted in doing token reservations and hence the
system utilization is low. It is especially important in
the high-speed network systems;

0

The starvation and unnecessary delay of low priority
messages is quite common;

0

The total percentage of messages that miss their critical time constraints is still high though that of the
most urgent ones is decreased;

2

2.1

A Real-Time Communication
Protocol
System Structure Model

The real-time communication model can be specified as follows: for each packet P,, i = 1,2,. . . , m, there are two principal parameters A, and E,. A, is the arrival time of P, and
E, is its expire time before which P, must be transmitted
and received. We define the deadline duration for P,, D,,
as the difference between A, and E,, as shown in Figure 1.
The packets that miss their deadline are considered to be
useless and lost no matter whether they are received or not.

For a high speed transmission medium, it is attractive
to partition the medium into multiple channels. It was also
shown that for a given system bandwidth, the system capacity can be increased by bandwidth subdivision [6]. These
channels can form a multiple ring network and have two
main advantages: (i) they increase network capacity by op-

We assume that the network considered consists of a
physical ring which is “divided” into k channels and is op-

147

erated as k token rings, C1, c z , . . . , C k . This way, we c u i
decrease the ratio of propagation delay and packet transmission time in each channel so that a high utilization can
be achieved. Then we group each two rings C,and C,,,as
a pair < C;,C,+l
>, where i is an odd number. To allocate
packets to the two rings of a pair, we classify the packets
into two groups according to their deadline values and a
predefined threshold D L ,

attained through a proper allocation. The protocol consiqi
of four parts as follows:
A. Allocation of Incoming Packets
When packets arrive at a station, they are routed to consecutive pairs of the network in a round robin manner. In
each pair, < C,,C,+I >, all the tight deadline packets are
routed to C,and all the loose deadline packets are routed to
C,+l. So now we have a multiple channel token ring network
with k/2 subsystems of the same property.

tight deadline packet if D;5 D L
loose deadline packet if D; > D L

B. Wait-Until (TYUT) Control Policy and Switching
In the next subsection, we will propose a dynamic load
control protocol which aims to utilize this multiple token
ring network with the consideration of timing constraints.
The most important concern is the opportunity provided
in this architecture such that the input load can be allocated dynamically into different rings according to different
system requirements and the potential performance can be
achieved in real-time applications.

Within each pair of rings, < C;,C;+l >, the wait-until
(WUT) control policy is adopted to switch an alert packet
from C; to C,+,. Then, the alert packets will preempt the
so that the on-time delivery
loose deadline packets in
can be guaranteed. The first ring of a pair C,is operated in
non-priority mode and allows no token reservation. When
the free token arrives, it send out a tight deadline packct if
its waiting queue is not empty, otherwise it passes the free
token to the next station. At the same time, it checks and
switches those alert packets, that are going to fail to meet
their deadlines, to the next channel C;+l which was allocat,ed
to those loose deadline packets. The checking process is
done based on the following formula:

To design the network controller, we intend to have a
simple interface design. After the division of physical ring
into channels, medium or low speed interface can be incorporated into each channel. Control mechanisms then can be
associated with each ring and arriving packets can be routed
with simple scheme. Thus, we eliminate the consideration
of an integrated network controller for multiple rings in each
station.

2.2

Tsiack

5 Tswitch

where
is the packet slack time. The value Ts,,,itch classifies alert packets and functions as a maximum sliding window to control the flow of switching as shown in Figure 2.

Dynamic Load Control

On the other hand, the second ring of a pair, C;+,, works
in a reservation mode and an alert packet switched from
C,can reserve the token and preempt the normal token
rotation. This will guarantee that all alert packets can be
sent out immediately before the loose deadline packets are
delivered.

Based on the architecture given before, an advanced dynamic control protocol is proposed to make token ring networks achieve a high performance in distributed real-time
applications. The proposed control protocol is based upon
the following idea. The critical packets with tight time C O I I straints need not necessarily request for token reservation
immediately after they arrive in to the system. They can
still wait for the chance of transmission like all the other
packets until their slack time passes certain limit, that they
are going to fail to meet their deadlines. We call these packets as alert packets. The alert packets have the privilege to
do reservation at once to preempt the normal packet transmission until all alert packets have been sent out. Since each
token reservation wastes certain rotation time, the preemption due to alert packets should be minimized. This can be

The benefit of this method is that it guarantees real-time
requirements while still keeping relatively high channel utilization. By using preemption, the real-time requirements
can be met. The channel utilization is still kept high because the transmission of alert packets has no interference
with the tight deadline packets and it only affects those
loose deadline packets. Hence only the channel C;+l within
a subsystem < CtrCifl > is degraded by reservation and
the lost percentage of loose deadline packets may not be increased due to the nature of their time constraints.

148

C.Migration of Loose Deadline Packets
With preemption and switching, two rings of a pair may
be unbalanced under certain conditions. This is due to the
fact the channel assigned to loose time packet has the inclination to be overloaded while the channel assigned to tight
deadline packet is possibly under utilized.
This problem can be alleviated through a proper classification of tight and loose deadline packets by the threshold
D L . However, this may still have a load fluctuation from
time to time. A dynamic load-balance policy called migration is suggested. The key operations of this policy are:
whenever a station at ring C;gets a free token, it sends out
a tight deadline packet if it has a nonempty waiting queue.
Instead of passing the token immediately if its waiting queue
is empty, it will fetch a loose deadline packet from channel
C;+1and send it out in channel Ci.This migration of loose
deadline packet from Ci+,to C;should only be done when
C , is under-utilized. To identify the utilization of Ci, we embed a checking mechanism within each pair. The detection
is based upon investigating the token cycling time (TcYcling)
in each station.
1
Tcycling I
- Msize
X
where Tcycling
is the period between the time that a station releases the free token and the time it receives the free
token again. Tcycling
can be easily observed by C;when it
receives the free token. The 1 / X is the packet interarrival
time at each channel which can be measured over a long
interval, and Msize is a constant that defines the migration
window size. As shown in Figure 3, when the above inequality is valid, C;has a low utilization and the average number
of packets arrived during the last token rotation is less than

in ring networks: exhaustive service and non-exhaustive service. In an exhaustive service system, whenever a station get
a free token, it sends out all the packets in its waiting queue.
In a non-exhaustive service, every station only sends out
one packet whenever it gets a free token. The later scheme
prevents any station from monopolizing the service and is
useful for real-time applications. But in high speed network
system, the packet transmission time is much smaller and
hence exhaustive service is more suitable if the physical ring
length is longer than the packet length. However, the token may be held by a busy station and the packets with
short deadlines in other stations maynot get the chance to
be transmitted. Thus, transmission of multiple packets per
token receipt can only be adopted under an underutilized
load.
In a paired ring < C;,Ci+,>, both rings can be in the
multiple packet mode. While ring C;+lis working in a reservation mode, some loose deadline packets can possibly follow the switched alert packet (from channel Ci)to make full
use of channel bandwidth. While channel Ci is working in
a migration mode, more than one loose time packets can
possibly be fetched from channel C;+lso that the channel
bandwidth is not wasted. However, the maximum nQmber
of multiple packets Nmaxhas be limited. We define it as the
following and illustrate in Figure 4:

Where Dmin is the minimum deadline value, and Tpacket is
the average packet transmission time on a ring. Based on
the detection of the last token cycle delay on the ring, N,,,
is determined by means of the packet transmission delay to
maximize the exiting channel utilization.
The operation of the proposed protocol is illustrated in
Figure 5 . In summary, the multiple ring network system
works in the following way: all the packets arriving to a

Msiae.
So, a migration of packets from C
i,, to C;occurs whenever the latter one is under utilized and thus the load is
dynamically balanced within the system. In addition, for
each receipt of the free token in C;, a packet (either a tight
deadline packet or a loose deadline packet migrated) can be
transmitted. This is especially effective in high speed rings
due to the high ratio of propagation delay and packet transmission time so that a high utilization can be achieved.

station are distributed into paired rings by a round-robin
manner. Within each pair, there are two separate waiting
queues attached on two rings, and tight deadline packets
and loose deadline packets are queued and transmitted in
their own channel respectively. A dynamic control protocol
allows the switching of alert packets to preempt the loose
deadline packets and at the same time allows the migration
of loose deadline packet whenever the tight deadline packet
channel is under utilized. In this way, every paired channel
and thus the whole system is guaranteed to be working in
a balanced state and to not only reduce the lost percentage

D. Multiple Packet Transmission
There are normally two different packet service policics

149

of message packets but also increase the system utilizatioii.
The dynamic controlling policies like switching and migi ation in both directions are the key points that make such a
kind of system achieve a remarkable performance improvement.

3

1

SYSTEM PARAMETERS IN EXPERIMENT
network length
20 km
transmission speed
200,000 km/s
10 MbDs
bitrate / channel
delay per station
3 bits
97 bits
overhead Der Dacket
station number
30

!

Evaluation and Simulation
Table 1. System Parameters

Though some analytical models and comparisons of
throughput-delay characteristics of ring networks have been
given, they are usually only applied to simple cases. It is
widely believed that studies of performance behavior using
complex control policies are beyond analytical methods and
therefore extensive simulations are required [IO].

3.1

System Assumptions and
mance Measures

With different arrival rate to each channel A, we vary the
network utilization from 60% to 80% under the conventional
token ring protocol. The packet deadline durations are dist~ributeduniformly over the range of 5Trotate
and 25TTOt,,,
where Trotate
is the free token rotation time. Although a
real communication load may not follow the above assumption, it is our purpose to stress the network with a heavy
load and equally distributed deadline durations. This will
enable us to examine the important performance measures
such as packet lost percentage and channel utilization, which
are defined as:

Perfor-

The simulation model is set up based upon the following
assumptions:

Packet Lost Percentage (L): A packet is considered to
be lost if it always has no chance to be sent out and
its waiting time has passed its deadline.

Arrival packets should be sent out and/or received
within a certain period of time (deadline), otherwise
they will be meaningless no matter they can be received or not. 'Those packets who have passed thcii
deadlines will be viewed as lost-packets and will be
discarded.

L=

no. o f packets lost
(no. of packets lost + no. of packets sent out)

Channel Utilization (U): Token idle time is the time
spent when a station has nothing to send and passes
the free token to next station.

All packets are classified into different classes based
on their deadline durations. The deadline durations
are uniformly distributed over a constant range. Thus,
the loads incurred by tight deadline packets and loose
deadline packets in a paired ring depend upon the
value of the threshold DL.

U=

packet transmission t i m e
(packet transmission t i m e
token idle t i m e )

+

All packets arrival into each station in the network
follow a Poission process with a constant arrival rate.

3.2

All stations are equally distanced on a ring;

It is obvious that different system performance can be
achieved by using different medium access protocols. To
evaluate the protocol proposed in the previous section, we
investigate and compare the performance of the following
protocols.

The lengths of all the packets are exponentially distributed with a constant mean.

Various system parameters are defined as in the following table:

Simulated Control Protocols

1. FCFS

The simplest way to run this kind of system is by using

150

5. MWUT

the first-come first-serve policy (FCFS) at each ring. Packets arriving at a station will routed to rings in a round robin
manner. Thus, the total load is distributed equally among
the multiple rings. In each station. there is an independent
waiting queue connected to each channel C,,
a = 1,2,. . . . A .
When a ring C; catches a free token, it sends out the eariiest
arrived packet in its waiting queue if the queue is not empty;
otherwise the free token is passed to the next station. This
scheme actually functions as a network with several independent single token rings with IEEE 802.5 protocol.

Unlike W U T protocol, MWUT (Multiple packet WUT)
adopts Single-Token-Multiple-Packet policy.
Based on
WUT, MWUT allows more than one packets to be sent
out whenever a free token arrives. The maximum number
of packets per transmission N,,,, is set dynamically and is
dependent of the current network load..
6. MWUT-MIG

Based on MWUT protocol, the MWUT-MIG protocol
allows migration of loose deadline packets from the second
channel to the first channel in a pair so that the loads can
be dynamically balanced and to have a full utilization of all
the resources.

2. Priority Transmission (PR)
This approach is to consider the different priorities between all incoming packets. There are various policies to
decide which packet in a channel should be the next one to
send out. To meet the real-time requirements, it is quite
natural to set a high priority to those packets with tight
deadline constraints. So we choose those “urgent” packets
and assign high priority to them based on their slack time
so that they can be send out as early as possible.

3.3

Simulation Results

Extensive performance simulations are developed based on
those protocols discussed above. In Figure 6, we show the
lost percentages of tight and loose deadline packets with
different packet arrival rates. In Figure 7, the channel utilizations are illustrated. The classification threshold D L is
set to equally partition packets into two classes. The results
show that the lost percentage of tight deadline packets in
FCFS protocol is very high while that in priority-based pro-

As in FCFS, all channels are treated as the same and
independently. To allow tight deadline packet to use token
reservation, two queues for tight and loose deadline packets
respectively are associated with each channel. Packets arrived will be inserted into a queue after comparing its deadline duration with the slack time of existing packets. Tight
deadline packets have a high priority and can reserve the
channel by using reservation bit of the token. The channel
can be reset to allow loose deadline packet’s transmission
until all tight deadline packets have been processed (sent
out or thrown away)

tocol is relatively low. This is because the former one has no
mechanism to favor real-time requirements. However, since
the P R protocol uses reservation extensively for tight deadline packets, a lot of channel bandwidth is wasted by favoring the most urgent packets. This leads to a much high lost
percentage of loose deadline packets. WUT protocol aims
to overcome this weak point of priority-based protocol and
has decreased the total lost percentage significantly. With
Tswitch= 2Trotate,
the simulation results show a much robust
utilization. However, since only tight deadline packets can
be switched to the second channel, the loads at paired rings
are not balanced.

3. Wait-Until Protocol (WUT)

This one is called Wait-UnTil. Different from former
ones, WUT groups each two channels as a structured pair
< C,,Cifl >. The initial packet allocation, the wait-until
policy and switching described in the previous section are
applied.

With the addition of migration, we can make a dynamic
load balance among the paired rings. The lost percentage of
loose deadline packet is reduced signaficantly with a slight
impact on tight deadline packets. The curves of MWUT in
Figure 6 and 7, represnet the cases where migration window
Msize = 0.2. Further performance improvements can be
obtained by adopting multiple packet transmission policy.
Note that this multiple packet transmission is dynamically
controlled through token cycling time and the limit N,,,.

4. WUT-MIG

In addition to WUT, WUT-MIG (Wait-Uti1 with MIGration) allow losse deadline packets to migrate from their
channel Ci+l to channel C;whenever the latter is under
utilized. In WUT-MIG, Msize,which functions as a flow
control window, is set up to limit the migration load,

151

In Table 2, 3 and 4, we show the impacts of different
Tsw,tch,M,,,, and D L . The performance of the network is
relative insensitive to the choice of the threshold D L once
Tswltchand A/r,,,, are set properly. Since Tswltch
and M,,,,
control the migration of packets among two channels of a
pair dynamically according the network load. The initial
classification become less crucial. Note that T s w t t c h and
M,,,, have different purposes: Tswztch
aims to switch tight
deadline packets such that they can preempt the normal
transmission of loose deadline packets and meet with their
real-time requirements, whereas M,,,, intend to use the underutilixed rcsourccs

I

L o s t D e r c e n t a e e ftieht-loose) V.S. T..,,.+A
control protocols 1.5TVotote 2T,,t.t,
2.5T,,tat, 3Tr,,,,,
WUT-MIG
6.8-32.0 3.5-27.5 2.2-26.8 1.8-28.3
2.9-12.6 1.9-10.2
MWUT-MIG
1.2-7.8
0.9-8.5

lization due to the reservation schemes. This performance is even worse in the high-speed communication
systems.
As may be noted, the protocols based on wait-until
scheme (WUT) can achieve a better performance comparing to the PR protocols. It can increase the system
utilization significantly while still guarantee the timeconstrained requirements. That is because WUT policy allows all the packets to be sent out immediately if
they are not time-out, and dynamically preempt the
loose deadline packets only when necessary to guarantee the real-time requirements.

The combined dynamic protocol employing multiplepacket policy (MWUT-MIG) performs the best on all
accounts. The main reason for the better performance
is that it dynamically balances the traffic load among
available channel resources and cleverly schedules the
transmission order. Therefore, it minimizes the lost
percentages of all kinds of messages and still maximizes the system utilization.

I

Table 2. The Lost Percentages with different TAWIIc~
L o s t p e r c e n t a g e (tight-loose) V.S. U,,,,
control protocols
0.2
0.4
0.6
08
WUT-MIG
11 3.5-27.5 I 8.0-18.7 I 11.8-18.3 I 12 7-17 0
MWUT-MIG
1.9-10.2 4.6-5.8
6.5-4.1
6 6-3.7

11

1

1

I

11

I

I

1

Table 3. T h e Lost Percentages with different A[,,,,

4
Threshold (DL)
16Tr0ta~c17T,.t.te
18Tr0m
3.6-34.4 5.2-34.5 6.4-34.5
8.7-24.7 9.4-24 4 10 3--26.2
12-13 2 13-11 4 2.0-11 9 2.7-13.7 3.1-13.6
1.8-8.3 2 0-8 9 2.4-9.9 2.4-10.4 3.3-11.2

Lost Percentage (tight-loose) V.S.
13T,,,,,,
14T,0im~c1 jT,,,.t,
WUT
1.2-33.4 1.8-32.5 2 4-32.5
WUT-MIG
7.1-18.8 7.4-19.5 8 6-22.1

control protocols

MWUT
MWUT-MIG

0.9-15.4
2.0-7 6

In this paper, we examine the perfromance requirements
for real-time communication. Instead of conventional average transmission delay, lost percentages of specific message

application situations, it can also be applied to con-

* As expected, FCFS performs badly

concerning about
the real-time requirements. FCFS has the best performance in channel utilization, but the performance
of this protocol is not acceptable. The studies of this
protocol, thus, demonstrate the importance of prioritization.

*

Conclusion

ventional communications as well as time-constrained
communications.
2. It can minimize the lost percentage of those critical
message packets while maintain a high channel utilization.

3. It is easy for the implementation by possibly expanding the existing ring interface technologies.

The performance of priority-based protocol (PR) gives
a better performance in trying to meet the timeconstraints of tight deadline messages. However, as
a total lost percentage, it is still unacceptable. The
main problem of this kind of protocols is the low uti-

From the studies, we can see that a network control protocol plays a very important role in system design and sys-

152

tcm perfGrmance. In addition to its traditional role as an
arbiter of channel accessing and sharing, a control protocol
also serves as a distributed scheduling mechanism by imposing an implicit or explicit transmission order on the packets
distributed among the stations in the ring. This scheduling function has shown to critically affect the distribution
of packet delays and thus the real-time performance of the
protocol. Future works to be continued in this research, include the study of approximate analytical models and the
closc~investigation of the characteristics of the real-time
cominunication.

[9] J . F. Kurose, M. Schwartz and Y. Yemini, “Multipleaccess Protocols and Time-constrained Communication,” AGM Computing Surveys, vol.16, No.1, pp.43-

70.
[lo] Sauer, Macnair, “Simulation of Computer Communication Systems,” Prentice Hall, New York, 1983.

[ll] J.K.Strosnider, Tom Marchok, and John Lehoczky,
“Advanced Real Time Scheduling Using the IEEE
802.5 Token Ring,” Proceedings Real-time Systems
Symposium, 1988, pp.42-52.

References
[1] Adarshpal S. Sethi and Tuncay Saydam, “Performance

Analysis of Token Ring Local Area Networks,” Computer Networks and ISDN Systems, Vo1.9, No.3, March
19S5, pp.191-200.

[2] Alexander Schill, Martin Zieher, “Performance Analysis of the FDDI 100 Mbit/s Optical Token Ring,” Proc.
of t h e IFIP TCS/WGS.d Workshop on High Speed Local Area Networks, Feb.1987, pp.53-74.
[3] 0.J.Boxma and B.Meister, “Waiting Times in MultiQueue Systems with Cyclic Service,” IBM Zurich
Research Lab, 8803 Ruschlikon, Switzerland, Jan.21,
1985.
[4] Werner Bux, “Local Area Subnetworks: A Performance
Comparison,” IEEE transactions on Communications,

Vol.COM-29, N0.10, Oct.19S1, pp 1465-1473.
[5] C.II.Chen and L.N.Bhuyan, “Design and Analysis of
Multiple Token Ring Networks,” INFOCOM 88, 1988,
pp.477-4S6.
[6] 1.Chlamtac and A.Ganz, “Design and Analysis of Very
High-speed Network Architectures,” IEEE Transactions on Communications, Vo1.36, No.3, Mar.l9SS,
pp.252-262.
[7] Dionysios K. and Albert0 L.G, “Performance of
Integrated Packet Voice/Data Token-Passing Rings,”
IEEE Journal on Selected Areas in Communications,
Vol.SAC-4, No.6, September 19S6.

[SI John A.Stankovic, “Real-Time Computing Systems:
The Next Generation,” Real-Time Systems, Feb 18,
1958, pp.14-36.

153

400

0

500 0

600.0

700 0

ARQIVAL RATE
Fig I TlhlE ATCIBUTES

OF P A C I C C I
600 I

LOST PERCENTAGE FOR LOOSE PACKETS

I

300
400 0

500 0

600 0

700

ARRl\lAL PbTE

900

154

UTILIZATION OF CHANNEL Ci+l IN <C!.Ci+l

>

I

0

2015 IEEE 17th International Conference on High Performance Computing and Communications (HPCC), 2015 IEEE 7th
International Symposium on Cyberspace Safety and Security (CSS), and 2015 IEEE 12th International Conf on Embedded Software
and Systems (ICESS)

Automatic Parallelization of Simulink Models for
Multi-core Architectures
Cumhur Erkan Tuncali, Georgios Fainekos, Yann-Hang Lee
School of Computing, Informatics and Decision Systems
Arizona State University
Tempe, AZ, USA
{etuncali, fainekos, yhlee}@asu.edu
of an embedded control algorithm, the worst case execution
times of the blocks and a computation budget (deadline), can
we automatically partition the blocks onto the different cores
so that the real-time constraints are satisﬁed?

Abstract— This paper addresses the problem of parallelizing
existing single-rate Simulink models for embedded control applications on multi-core architectures considering communication
cost between blocks on different CPU cores. Utilizing the block
diagram of the Simulink model, we derive the dependency graph
between the different blocks. In order to solve the scheduling
problem, we describe a Mixed Integer Linear Programming
(MILP) formulation for optimally mapping the Simulink blocks
to different CPU cores. Since the number of variables and
constraints for MILP solver grows exponentially when model
size increases, solving this problem in a reasonable time becomes
harder. For addressing this issue, we introduce a set of techniques
for reducing the number of constraints in the MILP formulation.
By using the proposed techniques, the MILP solver ﬁnds solutions
that are closer to the optimal solution within a given time
bound. We study the scalability and efﬁciency of our consisting
approach with synthetic benchmarks of randomly generated directed acyclic graphs. We also use the Fault-Tolerant Fuel Control
System demo from Simulink and a Diesel engine controller from
Toyota as case studies for demonstrating applicability of our
approach to real world problems.

In particular, we focus on control models built in the
Simulink [2] MBD environment. Our goal is to produce a
framework where non-determinism in the control algorithm
is reduced or minimized to the extent possible. Especially
in safety-critical systems, scheduling in a predictable and
deterministic manner is highly important for veriﬁcation and
satisfying the certiﬁcation requirements that are mandated by
regulatory authorities. For example, multi-core architectures
are classiﬁed as highly complex in the 2011/6 ﬁnal report
of European Aviation Safety Agency (EASA) [3] and in the
Certiﬁcation Authorities Software Team position paper CAST32 Multi-core processors [4]. These classiﬁcations highlight
the difﬁculty of certifying safety-critical systems that are based
on multi-core architectures.
Our approach is based on keeping timing properties of
parallelized software as simple as possible. For this purpose,
we are aiming at having separate executables for each core
while Simulink blocks are allocated in each core and executed
in a predetermined order. In other words, we set the priorities
of each block inside each core.

Keywords—Multiprocessing, embedded systems, optimization,
model based development, Simulink, task allocation.

I.

I NTRODUCTION

Model Based Development (MBD) has gained a lot of
traction in the industries that develop safety critical systems.
This is particularly true for industries that develop CyberPhysical Systems (CPS) where the software implements control algorithms for the physical system. Using MBD, system
developers and control engineers can design the control algorithms on high-ﬁdelity models. Most importantly, they can
test and verify the system properties before having a prototype
of the system. The autocode generation facility of MBD tools
provides additional concrete beneﬁt which helps in eliminating
programming errors.

The contributions of this paper are,

However, currently, the autocode generation process of
commercial tools focuses on single-core systems. Namely, at
the model level, there is no automatic support for producing
code that runs on a multi-core system. This is problematic
since advanced control algorithms, e.g., Model Predictive
Control algorithms [1], are computationally demanding and
may not be executed within the limited computation budget of
a single-core embedded system. In this paper, we address this
problem at the model level. Namely, given a data ﬂow diagram

providing a practical solution to the Simulink model
parallelization problem,

•

improving available Mixed Integer Linear Program
(MILP) formulations in the literature for ﬁnding better
solutions within a ﬁxed and practically feasible time
for industrial size models,

•

solving the multi-core mapping problem while considering the timing predictability of the parallelized
application for ease of veriﬁcation and certiﬁcation,
and

•

developing a toolbox for automating parallelization of
Simulink models to multi-core architectures.
II.

R ELATED W ORK

There is a large amount of research being done on the
optimization of scheduling multiple tasks on multi-core processors or multiple processors in the literature. In [5] Anderson

This research was partly funded by the NSF awards CNS-1446730 and
IIP-1361926, and the NSF I/UCRC Center for Embedded Systems.

978-1-4799-8937-9/15 $31.00 © 2015 IEEE
DOI 10.1109/HPCC-CSS-ICESS.2015.232

•

964

et al. propose a Pfair [6] based scheduling method for realtime scheduling on multi-core platforms where the system
has multiple tasks and task migration is allowed. For optimal
mapping of tasks to CPU cores, Yi et al. [7], Bender [8] and
Ostler et al. [9] discuss integer linear programming techniques
which constitute a base for our optimization formulation.
Cotton et al. discuss the use of mapping programs to multi
processors in [10]. Tendulkar et al. discuss the application
of SMT solvers in many-core scheduling for data parallel
applications in [11]. In [12], Feljan et al. propose heuristics
for ﬁnding a good solution for task allocation problems in a
short time instead of searching for an optimal solution.

III.

P ROBLEM D ESCRIPTION

We are addressing the problem of automatically parallelizing existing Simulink models for embedded control applications on multi-core architectures in an optimal way and in a
reasonable time.
We are focusing on single-rate, single-task embedded
control applications which are modeled in Simulink and in
which the execution order of blocks is determined only by
dependencies coming from connections between blocks. Our
target models cannot start execution of next iteration before
ﬁnishing the execution of the current iteration.

There are studies focusing on parallelization of Simulink
models. In [13], Kumura et al. propose methods to ﬂatten
Simulink models for parallelization without giving a detailed
description of the optimization formulation. In that work,
Simulink blocks are considered as tasks. To achieve thread
level parallelism in multi-core, Canedo et al. introduce the
concepts of strands for breaking the data dependencies in
the model. A strand is deﬁned as a chain of blocks that are
driven by Mealy blocks [14]. The proposed method searches
for available strand split points in Simulink models and it is
heavily relying on strand characteristics in target models. In
[15], Cha et al. is focusing on automating code generation for
multi-core systems where the parallel blocks are grouped by
user-deﬁned parallelization start and end S-functions into the
model.

Our target platform is Qorivva MPC5675K-based evaluation board [19]. The processor is a dual-core 32-bit MCU
from Freescale targeting automotive applications. The μC/OSII from Micrium [20] is ported on the target and a library to
support Simulink code generation is devised for the platform
[21]. We handle inter-core data communications by utilizing
available shared memory and inter-core semaphores which
are used for synchronization between tasks across cores and
protecting global critical sections as described by Bulusu in
[21]. For the purpose of utilizing this approach in Simulink,
we model transmission and reception of data between different cores with two separate S-function blocks which implement inter-core transmission and reception using inter-core
semaphores and shared memory. We will refer to these Sfunction blocks as inter-core communication blocks.

There are studies on task parallelization as [9], [7], [8].
However, to apply the similar approaches, Simulink blocks
must be considered as tasks. Given that most realistic models
may consist of a signiﬁcant number of blocks, either these
methods fail to ﬁnd an optimal solution in a reasonable
amount of time or they rely on available loop level parallelism
or functional pipelining as described in [9]. Deng et al.
study model-based synthesis ﬂow from Simulink models to
AUTOSAR runnables [16] and runnables to tasks on multicore architectures in [17]. The authors extend the Firing
Time Automation (FTA) [18] model to specify activations
and requested execution time at activation points. They deﬁne
modularity as a measure of number of generated runnables and
reusability as a measure of false dependencies introduced by
runnable generation. The authors use modularity, reusability
and schedulability metrics for evaluation of runnable generations. They also propose different heuristics and compare
their results with the results obtained by utilizing a simulated
annealing algorithm. Although this work is targeting a similar
problem to our target problem, they are providing experiment
results for systems with less than 50 blocks and they are not
considering inter-core communication and memory overhead.

A. Solution Overview
We approach the problem in ﬁve steps which are illustrated
in Fig. 1. First, creating a directed acyclic graph which
represents dependencies between blocks. Task-data graphs are
discussed in [9]. We use a similar approach using blocks
instead of tasks, worst case execution times of blocks instead
of amount of work associated with tasks and using size
of data communication between blocks. Here we will refer
to this kind of graphs as “block dependency graphs”. Our
second step in approaching the problem is ﬁnding an optimal
or near optimal mapping of blocks to different CPU cores
by formulating a Mixed-Integer Linear Program (MILP) and
solving the resulting optimization problem with off-the-shelf
MILP solvers. The third step is automatically updating the
original Simulink model by adding inter-core communication
blocks where necessary in accordance with the most optimal
solution. The next step is generating separate code for each
target core by automatically commenting out the blocks that
are not mapped to the core for which code is being generated.
Finally, we compile the generated code and deploy it on the
target platform.

Our work mainly differs from the other works in literature
by
1.

providing a complete ﬂow for automatically parallelizing a single-rate Simulink model,

2.

incorporating the communication cost in the optimization problem,

3.

having total available shared memory constraints, and

4.

being able to handle large models with more than 100
blocks in a reasonably short time.

IV.

M ILP F ORMULATION

In this section we present our MILP formulation for the
parallelization problem. Our MILP formulation for optimal
solution is based on the formulations proposed by [5], [6]
and [7]. We introduce an extension to these formulations by
dividing the cost of communication to the transmission and
reception parts. In Subsection D, we describe our techniques
for reducing the number of constraints for allowing the MILP
solvers to ﬁnd better solutions within a feasible time.

965


	





is used in the program formulation to dominate other terms
allowing constraints to be ignored under certain conditions.

 	
	
	
 	
		

 			
		 	
!	"				

B. Variables
bip : A Boolean variable indicating whether block Bi is
mapped to core Pp or not. It is deﬁned for all Bi ∈ B and for
all Pp ∈ P . If Bi is mapped to core Pp , then bip takes value
1. If Bi is mapped to another core, then bip will takes value
0.

 #	$						
				"	
 !	

 $					

dik : A Boolean variable indicating whether block Bi
executes before or after Bk when both blocks are mapped to
same core. It is deﬁned for all Bi , Bk ∈ B with i < k. If Bi
executes before Bk , then dik takes value 1 and if Bi executes
after Bk , then dik takes value 0.

 					

si : The start time for the execution of block Bi . It is deﬁned
for all Bi ∈ B. The lower bound for the variable si (best case
start time) is denoted by bsi . It is determined by the best case
completion time for all of the blocks from which there is a
path to Bi in G. In the best case, all of this workload before
Bi is distributed equally on all of
 The best case
 the cores.
start time of Bi is calculated as
k∈Ki wk /m where Ki =
{Bk : Bk ∈ B ∧ there exists a path f rom Bk to Bi in G}.
The upper bound for the variable si (worst case start time)
is denoted by wsi . It is determined by the best case completion time for all of the blocks to which there is a path
from Bi in G and the block Bi itself, subtracted from the
deadline. The
 worstcase start
 time of Bi is calculated as
deadline − wi + k∈Yi wk /m where Yi = {Bk : Bk ∈
B ∧ there exists a path f rom Bi to Bk in G}. For all i, k
such that Bi , Bk ∈ B and for all p such that Pp ∈ P .

Fig. 1. Steps of going from a single-core Simulink model to multi-core target

A. Notation and Constants
The number of CPU cores available at the target architecture is denoted by m. The set of CPU cores is deﬁned as P =
{Pp : p ∈ [1, m]}. The number of nodes in the dependency
graph is denoted by n where each node corresponds to a block
in the ﬂattened and merged Simulink model. Merging of blocks
is done on the ﬂattened model as described in subsection D.
We describe the dependencies between blocks with the block
dependency graph. This is a directed acyclic graph G = (B,
E), where B = {B1 , B2 , , Bn } is the set of nodes and E is the
set of edges in G. Each node Bi corresponds to a Simulink
block with a worst case execution time wi and each edge Eik
represents a data dependency from block Bi to block Bk . The
set of leaf nodes in B, i.e., set of blocks which do not have
any output ports is denoted by L and the set of start blocks,
i.e., the set of blocks which do not have any input ports is
denoted by S. We use Z for the set of deleted connections from
the blocks that introduce delays (e.g., Unit Delay, Memory,
Integrator, etc) to successor blocks. These connections exist
in the original model, but they are deleted when forming the
directed acyclic graph for removing cycles from the model.
Such a connection is represented by Zik ∈ Z.

f : The completion time after executing all blocks. The
lower bound for variable f is 0 and the upper bound is the
deadline.
C. Objective Function and Constraints
The objective function for the optimization problem is minimizing f while the constraints for the optimization problem
are deﬁned as follows:

The size of the data transfer from block Bi to Bk in bytes
is deﬁned as cik . When Bi and Bk are mapped on different
cores there will be a communication cost for transferring cik
bytes of data between the cores. The communication cost is
divided into transmission and receiving parts where tik denotes
the transmission part of the communication time for sending
cik bytes of data from block Bi to block Bk when they are
mapped on different cores and rik denotes the receiving part
of the communication time for sending cik bytes of data from
block Bi to block Bk when they are on different cores.

1) Every block shall be assigned to a single core:

∀i : Bi ∈ B,
bip = 1

(1)

Pp ∈P

2) Delay introducing blocks and their ﬁrst successor blocks
shall be assigned to the same core:
∀i, k : Zik ∈ Z and ∀p : Pp ∈ P, bip − bkp = 0

The maximum allowed execution time for one iteration of
the model on the target multi-core architecture is given by the
deadline. It is either taken as a user input or calculated as the
overall worst case execution time on a single-core architecture.
The size of a global semaphore structure in bytes is denoted
by sSize and the size of total available shared memory in
bytes is deﬁned as totMem. Data alignment size in bytes
(word size) is denoted by aSize. A very large value (MAX)

(2)

3) The ﬁnishing time of each leaf block shall be less than
or equal to the completion time for executing all blocks: This
constraint is serving for the purpose of being able to formulate
the objective function minimize(maxBi ∈L (si + wi ))) as
minimize(f ).
∀i : Bi ∈ L, si + wi ≤ f

966

(3)

1) Partially ordering independent blocks: In order to reduce the execution time of a model by parallelization, the
model must preferably have a large number of blocks that
are independent to each other. If all blocks are dependent to
each other, then there can be no multi-core mapping that will
improve the execution time and, thus, the best solution will be
mapping all blocks to the same core.

4) If there is a dependency from block Bi to Bk , block Bk
shall not start execution until (i) Bi ﬁnishes execution and
transmission of its output data to its successor blocks that are
mapped on other cores (which we temporarily deﬁne as fi
below) and (ii) Bk ﬁnishes receiving all of its input data that
are sent by the blocks on other cores: Considering that Bi is
mapped to core Pp and Bk is mapped to core Pq where p can
be equal to q:
∀i, k : Bi , Bk ∈ B, Eik ∈ E, ∀p, q : Pp , Pq ∈ P,

[rlk (1 − blq )] + (2 − bip − bkq )M AX
f i ≤ sk −
Bl ∈B

where fi = si + wi +



Bl ∈B [til (1

Typically, in an industrial size model with a large number
of blocks, both the number of blocks that are independent to
each other and the number of blocks that are dependent to each
other becomes large. In this case, when we consider all possible combinations of execution orders (priorities) between these
independent blocks, the number of constraints introduced by
inequalities (5) and (6) becomes very large. As a consequence,
ﬁnding an optimal solution within a feasible time becomes
harder.

(4)

− blp )].

5) Execution of independent blocks that are mapped to
same core cannot overlap: Considering Bi and Bk are mapped
to core Pp , we have two different constraints for this requirement.

We address this problem by deciding the execution order
between certain independent blocks in advance. That is, before
formulating the optimization problem, we decide the values of
the dik variables for these block pairs. Since our execution
order decision is valid only when these blocks are mapped
onto the same core, this should not prevent these blocks to
be mapped on different cores and, hence, be executed in a
different order than what we specify.

∀i, k : i < k, Bi , Bk ∈ B, Eik ∈ E, ∀p : Pp ∈ P,
f i ≤ sk −



[rlk (1 − blp )] + (3 − bip − bkp − dik )M AX

Bl ∈B

f k ≤ si −



(5)

Our partially ordering heuristic is based on comparing
the execution start time frames of independent blocks. The
execution start time frame of a block is deﬁned as the time
frame between its best and worst case start time values.
The best and the worst case start time values of a block
Bi ∈ B are deﬁned in the subsection IV-B as bsi and wsi
respectively. For all independent block pairs B
i ∈
 B and
Bk ∈ B, if (bs(i) ≤ bs(k))∧ (ws(i) < ws(k)) ∨ (bs(i) <
bs(k)) ∧ (ws(i) ≤ ws(k)) then we decide Bi to execute
before B
>
 k and set dik to 1. Else if (bs(i) ≥ bs(k))∧(ws(i)

ws(k)) ∨ (bs(i) > bs(k))∧(ws(i) ≥ ws(k)) then we decide
Bi to execute after Bk and set dik to 0.

[rli (1 − blp )] + (2 − bip − bkp + dik )M AX

Bl ∈B

Where, fi = si + wi +

(6)


and fk = sk + wk +

Bl ∈B

[til (1 − blp )]

Bl ∈B

[tkl (1 − blp )]

Since M AX is a very large constant, (5) will be valid when
block Bi executes before Bk i.e., when dik = 1 and (6) will
be valid when block Bi executes after Bk i.e., when dik = 0.

2) Fully ordering independent blocks: Even though ordering independent blocks using the partially ordering heuristic
improves the performance, this is not enough for models with
very large number of blocks. For example we could not ﬁnd
a feasible solution to models with more than 100 blocks with
this approach. For dealing with those large models we propose
deciding the execution order of all the independent blocks
when they are mapped on the same core. The logic in fully
ordering heuristic is based on comparing the midpoints of the
execution start time frames for these blocks. For independent
blocks Bi ∈ B and Bk ∈ B we decide Bi to be executed
before Bk if the average of bsi and wsi is smaller than the
average of bsk and wsk . With this approach, dik variables of
MILP formulation change to constant values. Our discussion
on the case when these blocks are mapped to different cores
in previous subsection is still valid.

6) Total memory needed for semaphores and communication buffers shall be less than or equal to total amount of
available shared memory:
∀i, k : Bi , Bk ∈ B, Eik ∈ E, ∀p : Pp ∈ P




 C 	

ik
·aSize ·|bip −bkp | < totM em
sSize+
aSize
Bi ,Bk ∈B
(7)
D. Improving Solver Time
The number of variables and constraints in the MILP
formulation grows exponentially as the number of blocks in the
model increase. Consequently, the MILP solver starts failing
in ﬁnding optimal or near optimal solutions for the problem in
a reasonable time. In this section, we introduce our techniques
for addressing this issue.

3) Merging highly coupled blocks: In this heuristic we
merge blocks Bi and Bk when block Bk is the only block
connected to the output port(s) of block Bi and block Bi is
the only block connected to the input port(s) of block Bk . The
merging operation copies all incoming and outgoing edges of
Bk to Bi except the edge Eik . Then it updates wi with wi +wk
and ﬁnally deletes Bk .

We say two blocks are dependent to each other if there
exists a directed path between corresponding nodes in the DAG
representation of the model and we say that two blocks are
independent if there is no directed path between these nodes.

967

merged together without introducing cycles between blocks.
An exception to this is a subsystem including a delay introducing block. In this case, the blocks inside such a subsystem
are not merged into a single block since this can cause a cycle
in the dependency graph. In such a subsystem, predecessor
blocks of a delay introducing block are only merged with other
predecessor blocks and successor blocks are only merged with
other successor blocks. In other words, a predecessor and a
successor of a delay introducing block are never merged. The
ﬂow of the process up to this point is illustrated in the simple
model in Fig. 2. In the next step, the block dependency graph is
annotated with estimates of WCET. Fig. 4 gives an illustration
of a simple block dependency graph.

4) Merging small blocks with large blocks: In this heuristic
we merge blocks Bi and Bk based on their ratio of execution
times. If block Bk is the only block connected to the output
port(s) of block Bi and the WCET of block Bi is very small
when compared to the WCET of block Bk , then block Bi is
merged into block Bk . If block Bi is the only block connected
to the output port(s) of block Bk and the WCET of block
Bk is very small when compared to the WCET of block
Bi , then block Bk is merged into block Bi . We ﬁnd this
technique useful for reducing the number of blocks of concern
in a way that parallelization will be focused on blocks with
higher impact on execution time. The ratio between the worst
case execution times of the blocks for determining a merge
operation can be deﬁned depending on how much reduction is
needed in the number of blocks.

The block dependency graph and the number of CPU cores
on the target architecture are used in generating the MILP
formulation presented in Section IV. The MILP solver returns
the best solution found for mapping blocks to the available
CPU cores and the execution order between these blocks.

The merging methods described above can be used for
decreasing the number of nodes in very large models where
the MILP solver can no more ﬁnd a good solution. These two
techniques are also dependent on the structure of the model.
Although, in general, they assist in ﬁnding better solutions,
there can be cases where the number of nodes cannot be
reduced to an acceptable level.
V.

The solution from the MILP solver is used to add inter-core
communication blocks between the blocks which are mapped
on different CPU cores. The relevant outputs of a block which
are sending data to a block on a different core are connected
to inter-core data transmitting S-function blocks. Similarly,
corresponding inter-core data receiving S-function blocks for
each transmitter are connected to the relevant inputs of the
block which is receiving data on a different core. The intercore communication blocks are added by setting unique IDs
that set each pair of transmitting and receiving blocks to use a
dedicated inter-core semaphore and a dedicated shared memory
location.

I MPLEMENTATION

In this section we describe the details of the implementation
of our tool in MATLAB.
Our tool accepts as an input a Simulink model that is
ready to compile as well as the desired depth of blocks
to be parallelized. It loads the model, reads speciﬁc block
information, e.g., block type, parents, etc., and all the relations
between blocks along with the width and size of the data on
the ports. For data types that are not built-in, the user input is
required to deﬁne the data size in bytes. Using this information
the model is ﬂattened by taking blocks inside sub-systems
out of their parent blocks. The remaining blocks like input
and output ports of subsystems, emptied subsystem container
blocks and ‘Goto’ - ‘From’ pairs, which are converted to line
connections, are discarded from the set of blocks.

An example of the transformation is given in Fig. 3. The
output of B1 is connected to the input of B2 in the original
model. This connection is then replaced by inter-core communication blocks. After adding all needed communication
blocks, we set the priority attributes of the blocks using the
execution start time values obtained from the optimization
solution.
As the last step, a copy of the model is created for every
CPU core. Each copy of the model corresponds to a CPU
core and the blocks which are mapped on other cores are
commented out. Code generated from each of these models
can be compiled to create separate executables for each core.

We represent all these dependencies in a directed graph
where a directed edge represents a data communication from
its source to its destination. Since determining Worst Case
Execution Times (WCET) is not in scope of this paper, we
assume that the WCET values for each of the blocks are
already determined. If there exists a cycle in the directed
graph, this means that there is a corresponding block in
the cycle which creates a data dependency from a previous
iteration of model execution. We will refer to these blocks
as delay introducing blocks. In these cases we break the
connection from delay introducing blocks to their successors
for transforming a directed graph to a directed acyclic graph.
Since the connection from delay introducing blocks to their
successor blocks are deleted, our MILP solution can never
introduce inter-core communication mechanism between these
blocks even if they are mapped on different cores. For dealing
with this issue we force the delay introducing blocks and their
successor blocks to be mapped on the same core in the MILP
formulation.

VI.

E XPERIMENTS

For studying the scalability and efﬁciency of our approach,
we utilize randomly generated directed acyclic graphs with
different number of nodes. We present results of these experiments in subsection VI-A and results of our case studies in subsections VI-B and VI-C. We use SCIP [22] from Achterberg
as MILP solver which is interfaced with MATLAB through
the Opti Toolbox [23] by Currie and Wilson. Experiments are
run on a 64-bit Windows 7 PC with Intel Xeon E5-2670 CPU
and 64 GB RAM.
A. Randomly Generated DAGs
For evaluating performance of our approach, we generate
DAGs in which the WCET, communication costs and connections between blocks are assigned randomly. Then we solve

After all of the cycles are cleared, the blocks that are
originally inside subsystems up to the desired model depth are

968



!




"



























	
















	




















#






	















	
















	









Fig. 4.








ordering heuristics (respectively denoted as basic, partial and
full) and corresponding solver run-time values are presented in
the table for different problem sizes. We also present the ratio
of the solutions found over all the experiments. For a problem
size, the lines corresponding to the approaches which could
not return any solutions are discarded in the table. As it can
be seen from the results presented in Table I, as the number
of blocks in a model increases, any heuristic that (partially)
sets the execution order performs better both in terms of
solver run-time and optimality of solutions. According to our
observations, for ﬁnding an optimal mapping, the basic MILP
formulation performs best when there are less than 30 blocks.
The partially ordering heuristic performs best when there are
30 to 50 blocks. For more than 50 blocks in the model, the
fully ordering heuristic outperforms other approaches in terms
of the achieved speed-up and the ability to return a solution.
The basic MILP formulation fails to return any solution for
models with 70 or more blocks. The partially ordering heuristic
fails to return any solution for models with more than 110
blocks. Although this detail is not illustrated in Table I because
of averaging, according to our experimental results, the fully
ordering heuristic can occasionally achieve very low speedup values compared to the other approaches when there are
less than 20 blocks in the model. However, this issue is
not observed when there are large number of blocks. This
behavior is parallel to our expectations since optimization
can signiﬁcantly reduce the effect of possible non-optimal
execution order decisions by trying large number of different
mapping of blocks to different cores.






!



	



"















#





















Fig. 2.



Flattening models and merging blocks

the problem for a dual-core system with the basic MILP
formulation which is given in Section IV and with the partially
and fully ordering heuristics for deciding the execution order
of independent blocks. We set ﬁve hours (18,000 sec) as an
acceptable upper time limit for the solver run time. Here,
we present a comparison of the performance of these three
approaches in terms of the average speed-up achieved, the
average solver time and the ability to ﬁnd a solution in the
given time limit. The speed-up is computed as the overall
single-core worst case execution time of the model divided
by the overall worst case execution time of the parallelized
model.
Given inﬁnite solver time, the basic MILP formulation
is expected to ﬁnd more optimal solutions than the other
approaches do for any problem size. However, when the solver
time is limited (5 hours in our experiments), it fails to ﬁnd satisfactory solutions for large problems. Table I gives a comparison of the performance of the used approaches. Average speedup achieved by basic MILP formulation, partially and fully












	





In Fig. 5, we illustrate the comparison between the two
heuristics and the basic MILP formulation in terms of the
achieved speed-up over the number of nodes. The solid lines in
the plot represent how much average speed-up is achieved by
each approach. The dashed lines represent the corresponding
minimum and maximum speed-up for each approach. For very
small number of nodes, the basic MILP formulation is better
than the other approaches. However, when the number of nodes
increases, ﬁrst, the partially ordering heuristic and, then, the
fully ordering heuristic perform best.






	















	










Fig. 3.

Block dependency graph for a simple model






	

In Fig. 6, we illustrate the comparison between the two
heuristics and the basic MILP formulation in terms of the
average solver time over the number of nodes. Each line in
the graph represents the average solver time spent for each

Inter-core communication blocks

969

# Nodes

Average
speed-up
1.48
1.47
1.46
1.68
1.71
1.46
1.48
1.62
1.55
1.2
1.66
1.67
1.09
1.55
1.59
1.54
1.75
1.39
1.7
1.38
1.61
1.08
1.64
1.04
1.67
1.56
1.62
1.61

Approach
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Full
Full
Full

10-15

30

40

50

60
70
80
90
100
110
130
150
170




C OMPARISON OF DIFFERENT APPROACHES

Average
solver time
2
1
0.5
2620
1558
26
9256
2091
606
18000
12481
5174
18000
17400
11685
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000



% found
Solutions
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
64%
100%
100%
100%
100%
100%
100%
60%
100%
50%
100%
30%
100%
100%
100%
100%


")$ 

TABLE I.






 



 



 





 
 









	














	

&'($ )* &)%$

Fig. 6.

Comparison of solver time between different approaches

model has 1 input port, 1 output port and 53 blocks after
discarding the trivial blocks as described in Section V.
We performed parallelization on a completely ﬂattened
graph. The obtained block dependency graph from this model
is presented in Fig. 7 where the blocks mapped to core 1 and
core 2 are illustrated as the nodes colored with red and blue,
respectively.
We achieved a speed-up value of 1.78 with the partially
ordering heuristic within 5 hours of solver time. A speed-up
value of 1.92 was achieved with the fully ordering heuristic.
The basic MILP solution could only achieve a speed-up value
1.19 because it was unable to ﬁnd the optimum solution within
the given time limit of 5 hours. This result is parallel with the
outcomes of the experiments carried on randomly generated
DAGs.

approach. As it is expected, due to the time limit given to the
solver, as the number of nodes increases, the solution times for
all approaches converge. However, the experiments on models
with less number of nodes suggests that the proposed heuristics
can improve the solver time. In the graph it can be observed
that the average solver time for proposed heuristics (as a
function of node count) is smaller than the basic formulation.
Combining the data in Fig. 5 and Fig. 6, we can see that the
fully ordering heuristic returns better solutions within shorter
solver run-time compared to the other approaches.

C. Case Study: Toyota Diesel Engine Controller
We used the Diesel engine controller model from [1] as
a case study from industry. The original model contains both
controller and plant parts. The controller part of model has
1004 blocks when ﬂattened as described in Section V, it has
7 inputs that are merged into a single input bus signal and 6
outputs that are merged into a single output bus signal. Since
the model has cycles inside the subsystems, our tool ﬂattens
the model by searching all blocks inside subsystems, breaks

B. Case Study: Fault-Tolerant Fuel Control System
As a case study, we used the fuel rate control subsystem of
the Simulink Fault-Tolerant Fuel Control System demo. This

 





 





 


 	



 	

 

 




 

 

 







 









 

 







 
	












 




 

 




 	



 	



 


 	

 
 





 
 !

	



&'($ )* &)%$

 
 !



 
 !







	

 


 	





 

 




 



 

 

 





 




 



  	













 

 







 		



 

 


 





 




 






 




 







 
 

 	






 












 



 











	
"#$$%
#







 





Fig. 5.

 	








 	




 






 	

 




 	




 	

 

Fig. 7. The block dependency graph and its partition onto two cores for the
fuel control system case study

Comparison of speed-up values between different approaches

970

the cycles as described in Section V and merges blocks inside
subsystems (when possible) without introducing new cycles.
For parallelizing this model we set the target model depth as
2. After merging deep blocks of each subsystem, the block
dependency graph is generated from the model with merged
blocks. The generated block dependency graph contains 153
nodes and a total of 184 connections between these nodes. Our
target platform for this case study is the dual-core architecture
from Freescale which is described in Section III. In our target
hardware setup we have a total of 3.8 KB shared memory
available.

[4] Certiﬁcation Authorities Software Team, “Position paper CAST-32
multi-core processors,” Federal Aviation Administration, Tech. Rep.,
2014.
[5] J. H. Anderson, J. M. Calandrino, and U. C. Devi, “Real-time scheduling on multicore platforms,” in Real-Time and Embedded Technology
and Applications Symposium, 2006. Proceedings of the 12th IEEE.
IEEE, 2006, pp. 179–190.
[6] S. K. Baruah, N. K. Cohen, C. G. Plaxton, and D. A. Varvel,
“Proportionate progress: A notion of fairness in resource allocation,”
Algorithmica, vol. 15, no. 6, pp. 600–625, 1996.
[7] Y. Yi, W. Han, X. Zhao, A. T. Erdogan, and T. Arslan, “An ILP formulation for task mapping and scheduling on multi-core architectures,” in
Design, Automation & Test in Europe Conference & Exhibition, 2009.
DATE’09. IEEE, 2009, pp. 33–38.
[8] A. Bender, “Design of an optimal loosely coupled heterogeneous
multiprocessor system,” in European Design and Test Conference, 1996.
ED&TC 96. Proceedings. IEEE, 1996, pp. 275–281.
[9] C. Ostler and K. S. Chatha, “An ILP formulation for system-level application mapping on network processor architectures,” in Proceedings
of the conference on Design, automation and test in Europe. EDA
Consortium, 2007, pp. 99–104.
[10] S. Cotton, O. Maler, J. Legriel, and S. Saidi, “Multi-criteria optimization
for mapping programs to multi-processors,” in Industrial Embedded
Systems (SIES), 2011 6th IEEE International Symposium on. IEEE,
2011, pp. 9–17.
[11] P. Tendulkar, P. Poplavko, I. Galanommatis, and O. Maler, “Many-core
scheduling of data parallel applications using SMT solvers,” in Digital
System Design (DSD), 2014 17th Euromicro Conference on. IEEE,
2014, pp. 615–622.
[12] J. Feljan and J. Carlson, “Task allocation optimization for multicore
embedded systems,” in Software Engineering and Advanced Applications (SEAA), 2014 40th EUROMICRO Conference on. IEEE, 2014,
pp. 237–244.
[13] T. Kumura, Y. Nakamura, N. Ishiura, Y. Takeuchi, and M. Imai, “Model
based parallelization from the simulink models and their sequential C
code,” in Proceedings of the 17th Workshop on Synthesis And System
Integration of Mixed Information Technologies (SASIMI 2012), 2012,
pp. 186–191.
[14] A. Canedo, T. Yoshizawa, and H. Komatsu, “Automatic parallelization
of simulink applications,” in Proceedings of the 8th annual IEEE/ACM
international symposium on Code generation and optimization. ACM,
2010, pp. 151–159.
[15] M. Cha, K. H. Kim, C. J. Lee, D. Ha, and B. S. Kim, “Deriving highperformance real-time multicore systems based on simulink applications,” in Dependable, Autonomic and Secure Computing (DASC), 2011
IEEE Ninth International Conference on. IEEE, 2011, pp. 267–274.
[16] AUTOSAR. (2015) AUTOSAR speciﬁcation. [Online]. Available:
http://www.autosar.org
[17] P. Deng, F. Cremona, Q. Zhu, M. Di Natale, and H. Zeng, “A modelbased synthesis ﬂow for automotive CPS,” in Proceedings of the
ACM/IEEE Sixth International Conference on Cyber-Physical Systems.
ACM, 2015, pp. 198–207.
[18] R. Lublinerman and S. Tripakis, “Modular code generation from
triggered and timed block diagrams,” in Real-Time and Embedded
Technology and Applications Symposium, 2008. RTAS’08. IEEE. IEEE,
2008, pp. 147–158.
[19] Freescale Semiconductor Inc. (2015) Qorivva MPC5675K. [Online].
Available: http://www.freescale.com/
[20] Micrium
Inc.
(2015)
μC/OS-II.
[Online].
Available:
http://micrium.com/rtos/ucosii/
[21] G. R. Bulusu, “Asymmetric multiprocessing real time operating system
on multicore platforms,” Ph.D. dissertation, Arizona State University,
2014.
[22] T. Achterberg, “SCIP: solving constraint integer programs,” Mathematical Programming Computation, vol. 1, no. 1, pp. 1–41, 2009.
[23] J. Currie and D. I. Wilson, “OPTI: lowering the barrier between open
source optimizers and the industrial MATLAB user,” Foundations of
computer-aided process operations, Savannah, Georgia, USA, pp. 8–
11, 2012.

For a model of this size, both the basic MILP formulation
and the partially ordering heuristic fail in ﬁnding a solution in
10 hours. However, by merging blocks of subsystems with
depth more than 2 and with our fully ordering heuristic,
our tool returned a solution to the given problem within
an average of 1.2 hours of solver time. Here the average
is taken over different sets of worst case execution time
assignments. The suggested multi-core mapping by the tool
achieves 1.44x speed-up on average. This result is parallel with
our expectations based on experiments carried on randomly
generated DAGs and illustrates applicability of our approach
to reasonably large problems in industry.
VII.

C ONCLUSION

In this paper we presented our approach for parallelizing
a single-rate Simulink model on a multi-core architecture. We
proposed a heuristic for partially deciding execution order of
independent blocks when they are mapped to the same core.
According to the experimental results with randomly generated
DAGs and our case study with the fuel system controller, this
proposed heuristic improves optimality of found solutions in
a reasonable time for a realistic size of models with around
50 to 60 blocks in our experimental environment. For models
with larger number of blocks, we proposed another heuristic
in which the execution order of all the independent blocks is
decided in advance. With this approach our tool could handle
models with larger than 150 blocks. We also presented this
heuristic together with block merging methods on a case study
from the industry where our tool reduced 1004 blocks to 153
nodes on the dependency graph by merging blocks deeper than
a speciﬁed value and solved the problem on this 153 nodes.
The results from the case study illustrate how our approach
can handle models which can contain more than 1000 blocks.
For the future work, we consider extending this work
by introducing heuristic methods for solving the optimization problem, studying multi-rate models and models with
blocks that have priority assignments. Furthermore, we plan
to incorporate worst case execution time (WCET) tools in our
framework.
R EFERENCES
[1]

[2]
[3]

M. Huang, H. Nakada, S. Polavarapu, R. Choroszucha, K. Butts, and
I. Kolmanovsky, “Towards combining nonlinear and predictive control
of diesel engines,” in American Control Conference (ACC), 2013.
IEEE, 2013, pp. 2846–2853.
Simulink, version 8.5 (R2015a). Natick, Massachusetts: The MathWorks Inc., 2015.
“EASA/2011/6 ﬁnal report,” European Aviation Safety Agency, Tech.
Rep., 2012.

971

2014 IEEE 17th International Symposium on Object/Component-Oriented Real-Time Distributed Computing

Dynamic Analysis of Embedded Software using Execution Replay
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
Arizona State University
Tempe, AZ, 85281
ywsong@asu.edu, yhlee@asu.edu

puts. Thus, program behavior will depend upon the time that
input events arrive as well as the input value received. Also,
it is a common practice to use asynchronous functions to
increase response time. Hence, embedded software will be
sensitive to probe effect caused by instrumentation overhead
as the timing of input events and the order of thread execution can be easily deformed. On the other hand, the probe
effect is a less concern in desktop applications which usually
perform predefined work load (e.g., file input) with fixed
loop counts and synchronous functions. Even if there were
execution changes due to instrumentation overhead from
analysis tools, analysis results would be amortized with repeated executions of the same code, e.g., consistent data races and call-path analysis results.
In Tables 1 and 2, the probe effect is illustrated in the execution of two embedded programs. The two programs are
based on the class projects of Embedded Systems Programming in Arizona State University [5]. Each result is from the
average of 10 runs. The first program is a QT [24] application which draws lines following the inputs of mouse movement. The program consists of three threads. The first thread
receives mouse movement packets and sends them to a
POSIX message queue (MQ). The second thread receives the
input packets from the MQ and draws lines on a display device. The last thread performs a line detection algorithm with
the received mouse inputs. We collected mouse movement at
a normal sampling rate of 300 inputs per second and then fed
the inputs to the application with variant speeds. If the first
thread was delayed and was not ready to receive an input, we
counted it as a missed input. The program is instrumented
using two dynamic analysis tools, i.e., the cache simulator
[12] using PIN and our implementation of the FastTrack data
race detector [8]. The workload of the program is very light
as it only spends less than 10% of CPU time. However, the
instrumented execution may miss up to 45% of the inputs.
The impact of probe effect caused by the instrumentation is
obvious since analysis results may be misleading when the
input data are missed.
The second program shown in Table 2 is a MQ test program with six threads and two MQs. The two sender threads
send items to the first MQ and the two router threads receive
the items from the first MQ and send them to the second MQ
with timestamps. Finally, two receiver threads receive the
items from the second MQ. We used asynchronous functions
for queue operations and, if a queue is empty or full, the
thread sleeps a fixed time interval and retries. We count the
numbers of occurrences that the queues become empty or
full as a way of measuring different program behaviors. In

Abstract—For program optimization and debugging, dynamic
analysis tools, e.g., profiler, data race detector, are widely used.
To gather execution information, software instrumentation is
often employed for its portability and convenience. Unfortunately, instrumentation overhead may change the execution of
a program and lead to distorted analysis results, i.e., probe
effect. In embedded software which usually consists of multiple
threads and external inputs, program executions are determined by the timing of external inputs and the order of thread
executions. Hence, probe effect incurred in an analysis of embedded software will be more prominent than in desktop software. This paper presents a reliable dynamic analysis method
for embedded software using deterministic replay. The idea is
to record thread executions and I/O with minimal record overhead and to apply dynamic analysis tools in replayed execution. For this end, we have developed a record/replay framework called P-Replayer, based on Lamport’s happens-before
relation. Our experimental results show that dynamic analyses
can be managed in the replay execution enabled by P-Replayer
as if there is no instrumentation on the program.
Keywords – program dynamic analysis; probe effect;
deterministic replay; profiling, embedded software

I.

INTRODUCTION

Dynamic program analysis is widely used to collect execution information while programs are running. The approach is widely applied to aid optimization and debugging
of the programs. For the collection and analysis of program
execution, software instrumentation is inevitable unless
hardware-assisted approaches are available. For instance,
dynamic binary instrumentation tools such as INTEL PIN
[17] and Valgrind [22] are most commonly used since they
do not require source code and recompilation of program.
However, instrumentation overhead from the tools is very
high no matter how trivial the analysis is.
The instrumentation overhead can perturb the execution
of a program and lead to different execution paths, and consequently misrepresent analysis results. This is so called the
probe effect [10]. One example is the delayed executions of
input events caused by the instrumentation overhead. Consequently, arriving external inputs may be lost or the deadlines
for real-time applications may be missed. The instrumentation overhead may also lead to different order of thread operations on a shared resource and produce different execution
results.
In embedded systems, applications run with multiple
threads interacting with each other as they share resources.
The execution of threads is often triggered by external in1555-0885/14 $31.00 © 2014 IEEE
DOI 10.1109/ISORC.2014.16

166

Table 1. QT application with mouse inputs
(% of inputs missed out of 4445 mouse movement inputs)

inputs/sec
150
300
450

Native
execution
0.0%
0.0%
0.0%

PIN
Cache
16.8%
36.1%
45.5%

the debugging and optimization process for embedded
programs.
2. We present a record/replay framework that can be incorporated with dynamic analysis tools.
3. We show that for thread profilers the real execution
time after removing measurement overhead can be accurately recovered with the record/replay framework.
The rest of paper is organized as follows. In the following section, the dynamic analysis method with deterministic
replay is described. Section III presents the design and implementation of P-Replayer. In section IV, the performance
evaluation of P-Replayer and the accuracy of dynamic analysis with P-Replayer are presented. A concise survey of related works is described in section V and we conclude the
paper in section VI.

Race detector
0.3%
1.2%
1.9%

Table 2. POSIX Message Queue application
(# of Queue full/# of Queue empty)

Queue
Length
5
10

Native
execution
1.3/7.5
0.5/8

PIN
Cache
8.3/191.9
2.5/146.8

Race detector
5.5/56.3
2.4/37.9

this program, there is no external environment affecting the
program execution, but the execution is determined by order
of thread executions on the shared MQs. As the results show,
instrumentation overhead from the tools has changed the
relative ordering of thread operations on the shared MQs
which, in turn, leads to different status of the message
queues.
The other concern, followed by the data shown in Tables
1 and 2, is that it will be very hard to know if there exists any
probe effect on an instrumented program execution. If we
take any serious measurement to find possible probe effect,
the measurement itself would incur instrumentation overhead
that can lead to execution divergence. Even if we know that
there were some changes in program execution, we still
would not be able to know how the changes affect the results
of the analysis.
To make dynamic analysis tools as non-intrusive as possible, hardware-based dynamic analyses can be used. Intel
Vtune Amplifier XE [13] exploits on-chip hardware for application profiling. ARM RealView Profiler [25] supports
non-intrusive profiling using dedicated hardware or simulator. However, they are specific for performance profiling and
do not support the analyses that require significant
processing capabilities. Sampling based analyses [2, 9, 13,
19, 28] can also be used, but the analysis accuracy decreases
when sampling rate is reduced to limit any measurement
overhead.
In this paper, we present a dynamic program analysis method for embedded software using deterministic replay. The
idea is to record thread executions and I/O events with a minimal recording overhead and to apply dynamic analysis
tools on replayed executions. With the record/replay, dynamic analyses that were not feasible due to probe effect can be
performed accurately. For this end we have developed a
record/replay mechanism, called P-Replayer. P-Replayer is
implemented as a record/replay framework based on Lamport’s happens-before relation [14]. The design goal is to
minimize recording overhead to prevent probe effect during
recording and to have minimal disturbance on analyzed program executions during replaying. The contributions of this
paper are:
1. We present a dynamic analysis method that makes analyses of a program feasible and faithful, and expedites

II.

DYNAMIC ANALYSIS WITH EXECUTION REPLAY

For dynamic analysis of a program, instrumentation
overhead is not avoidable whatever optimization techniques
are used. The overhead can result in different execution behavior. It is possible to repeat the same analysis again and
again hoping that eventually we see the true program behavior without the overhead. This repeated running is not even
feasible if the program execution depends on the timing of
external inputs.
Consider the idea of using a record/replay framework for
dynamic analysis as shown in Figure 1. First, an execution of
a program is recorded. If the overhead of recording is small
enough to avoid probe effect, we can assume that the recorded execution is as same as the original program execution.
Second, we apply dynamic analyses on the replayed execution which has the same thread executions and I/O inputs as
the recorded one. Thus, the analyzed program will be executed as if there is no probe effect.
Moreover, the analyses of a program can be expedited
with reproducible execution. When we find an unexpected
execution during testing a program, the first step will be to
locate the cause of the problem. We may need to run various
analysis tools to locate the cause. This will be very time consuming and multiple trials may be needed since it can be
hard to reproduce the execution given the significant overhead of the analysis tools. Instead, we record the program
execution during the test run. As the execution is reproducible in replayed execution, analysis runs can be performed in
the same program execution.
During a deterministic replay, measurement overhead
from profilers can also be calculated to obtain accurate execution time measurement. In thread profiling, execution
times of a program’s activities such as function execution
time can be measured for each thread. Since the measurements can incur probe effect, several execution time estimation algorithms [18, 27] have been proposed to recover the
real execution time. In the approaches, the real execution
time can be recovered with the considerations of three factors: 1) thread local measurement overhead, 2) event reordering, and 3) execution time difference due to the event reordering. As an example of the factors 2) and 3), consider the
take and give operations performed on a semaphore. Assume

167

• Program order: If a and b are in the same thread and
a occurs before b, then a  b.
• Locking: If a is a lock release and b is the successive
lock acquire for the same lock, then a  b.
• Communication: If a is a message send and b is the
receipt of the same message, then a  b.
• Transitivity: If a  b and b  c, then a  c.
In P-Replayer, a data race detector is included as an analysis tool and for managing execution replay in the presence
of data races. It also provides an approach for recovering real
execution time without measurement overhead of thread
profilers.

Record
Test 1

Test 2

Test k

x

……

Faulty behavior
/Performance anomaly

Replay
Replay
with tool 1

Replay
with tool 2

Replay
with tool n

……



Debugging tools
/profilers

Bug/Bottle neck
Located

A. Record/Replay Operations
During recording operation, happens-before relations for
all events in a program execution are traced and saved into a
log file, and the same happens-before relations are to be enforced during execution replay. A general approach [26] to
obtain the happens-before relation is to use Lamport clock
[14]. In the approach, a Lamport clock is maintained for each
thread and the clock is incremented and synchronized by the
happens-before relation. A timestamp (Lamport clock value)
is attached to each recorded event. During the subsequent
replay operations, the corresponding event is delayed until
all other events having smaller clock values are executed.
This can enforce a stronger condition than necessary for replaying the partially ordered events. In our approach, we use a
global sequence number to order the events traced in recording operation. This sequence represents a total order of
events and is used to index the set of events during execution
replay.
To identify the happens-before ordering, the event log of
an event consists of the two sequence numbers for the event
itself and the event immediately before it, plus thread id, the
function type and arguments for the event. So, for an event b,
let a be the event happened before b immediately. For the
execution of the event b, the sequence numbers of both
events a and b are recorded in the event log of the event b.
For an input event, the received input is saved into the log
file as well. All logging operations are performed in a dedicated logging thread to avoid possible jitters caused by file
operations.
In a subsequent replay operation, an event table is constructed from the recorded log. The event table contains a list
of events for each thread. Using the sequence number in the
record log, events are indexed to represent the happenedbefore relations. Thus, based on the table, the next event for
each thread that is eligible for execution can be identified
and the same happens-before relations as the recorded ones
are used to schedule thread execution. The replay scheduling
is performed inside replay wrapper functions, thus the replay
operation is implemented purely in application level.
A replay wrapper function consists of three parts. Firstly,
a thread looks up the event table for the events happened
before its current event. If any of the events that should be
happened before are not executed, the thread is suspended
waiting for a conditional variable. In the second part, when a
thread can proceed with its current event, the thread carries

Thread
profiler
Thread profile with replay
0 sec 1 sec 2 sec …

T1
T2

Estimated results
T1
T2

0 sec

……

1 sec 2 sec …

……
Tk

Tk

Execution time estimation without overhead

Figure 1. Dynamic analyses with execution replay

that in the real execution, the semaphore is given first and is
taken afterward. Hence there is no blocking. In the instrumented execution, if the semaphore give operation had
started late due to the delay of instrumentation overhead,
then the thread taking the semaphore would have been
blocked. The estimation algorithms [18, 27] can account for
the blocking time and then reorder the events. However, if
there is a change of program execution paths, then there will
be no way to recover the real execution.
To avoid the above-mentioned problem, thread profiling
can be applied in a replayed execution. Note that the execution with the profiling is deterministic as the event reordering
is handled by the replay scheduling. The overhead compensation for the reordering events is no longer needed. Therefore, as long as we can identify the overhead caused by the
replay mechanism, the total overhead from the thread profiling tool on a replayed execution is simply the sum of the
replay overhead and the thread local measurement overhead
from the profiler.
III.

P-REPLAYER

In this section, we describe a record/replay framework,
called P-Replayer. The framework is based on our Replay
Debugger [16], and is optimized and generalized for dynamic analysis tools. The framework is designed to have minimal
record and replay overheads. To enable execution replay, we
consider the happens-before relation [14] among events
which are execution instances of synchronization or IO function calls. The record/replay operations are implemented in
the wrapper functions for events. The happens-before relation over a set of events in a program’s execution is logged
during recorded execution and are used to guide the thread
execution sequence in execution replay. The happens-before
relation between two events, denoted “”, is the smallest
relation satisfying,

168

C. Execution Time Estimation
In this subsection, we present an approximation approach
to estimate execution time that can account for the overheads
of replay operation and thread profilers. First, we present the
algorithm that can estimate the real execution time without
replay at program level. Second, the algorithm is refined to
estimate the real execution time at thread level without the
overheads of profilers and replay operation. The replay overhead is approximated based on the assumptions that 1)
threads run on multiple cores, 2) events are evenly distributed to each core, 3) the record overheads for all event
types are same, and 4) the number of worker threads is greater than or equal to the number of cores.

out the original function. If the function is for an input event,
the thread reads input value from the log file instead of actual reading from an I/O device. Lastly, the event for this
wrapper function is marked as executed and the thread wakes
up (signal) other threads waiting for the execution of this
event. Note that the total order of events based on the sequence numbering is used only for indexing events and the
replay operation follows the partial order of happens-before
relations.
B. Handling Data Races
For efficient record and replay operations, only synchronization and I/O events are considered. However, one drawback is that a replay may not be correct in the presence of
data races. A data race occurs when a shared variable is accessed by two different threads that are not ordered by any
happens-before relation and at least one of the accesses is a
write. Assume that, in the recorded execution, there is a data
race on a variable that is used to decide alternative execution
paths. Since there is no synchronization operation to enforce
any happened-before relation, the order of accessing the variable is not recorded. This implies that, if an alternate accessing order from the record one is chosen in replayed execution, the replayed program may take a different execution
path.
We use a similar approach as RecPlay [26] to handle the
presence of data races in replaying operations. RecPlay
records an execution of a program as if there is no data race
and any occurrences of data races are checked during replay
operation using a data race detection tool. If data races are
found, the replay operation is repeated after removing the
races. The approach is correct since a replayed execution is
correct up to the point where the first race occurs as shown in
[4]. However, most data race detection tools incur substantial
overheads. It may not be practical to fix every data race during replaying operations.
Instead, P-Replayer detects the occurrence of an unexpected event (a different event from the recorded one) during
replay process and stops the process at the location of the
unexpected event. Then, the race can be detected with a race
detection tool and fixed. The detection of an unexpected
event is done in the replay wrapper by comparing the current
events with the events in the record log. After fixing the race
that results in different execution path, the replay can be
safely used with various analysis tools including a race detection tool for detecting races that cause errors other than a
change of execution paths.
We have implemented a data race detection tool based
on FastTrack [8] and the tool is modified to be integrated
with P-Replayer. FastTrack algorithm detects data races
based on happens-before relations in a program execution.
However, the replay scheduler invokes additional locking
operations which appear as extra happens-before relations
for the FastTrack detector. As a consequence, some data
races may not be detected in the replayed execution. To correct the missed detections, we alter the FastTrack approach
to discount the synchronization operation introduced by the
replay scheduler.

1) Execution Time Estimation at Program Level
The estimated execution time of a program execution,
Cestimate, can be calculated by subtracting the replay overhead
Oreplay from the replayed execution time Creplay, such that,
Cestimate = Creplay – Oreplay

(1)

The replay overhead Oreplay is the sum of 1) replay initialization time Cinit, 2) extra execution time, Ce, spent in replay
wrapper functions, and 3) blocking time, Bu, by the replay
scheduling that leads to extra delay of replay execution, i.e.,
Oreplay = Cinit + Ce + Bu

(2)

The extra execution time Ce is classified into two parts.
Note that for two events a and b with a  b, the execution of
event b can be delayed until event a is executed or can be
executed with no-delay if event a has already been executed.
The two possible ways of event executions contribute to various amount of overheads, thus we account them differently.
Let nnd and nd be the numbers of events of no-delay and delayed execution, respectively. Let cnd and cd be the execution
times for no-delay and delayed events, respectively. Then, Ce
can be expressed as,
Ce = nnd*cnd + nd*cd

(3)

Threads can be blocked by the replay scheduling through
global locking and by delayed events. The blocking of
threads leads to additional execution time only when the
number of ready threads becomes less than the number of
cores, i.e., when the cores are underutilized. Let ng be the
number of occurrences that threads are blocked by the global
locking and let bg be the average delay caused by each global
locking. Also, let bd be the total execution delay caused by
the delayed events due to the replay scheduling at the end of
a replay execution. Then, Bu can be expressed as,
Bu = ng*bg + bd

(4)

Combining Equation (3) and (4) into Equation (2) gives,
Oreplay = Cinit + nnd*cnd + nd*cd + ng*bg + bd (5)
2) Overhead Measurements
The replay initialization time Cinit, and the counter values,
nnd, nd, and ng, can be measured during a replay execution.
The blocking delay, bd, is calculated using the algorithm in
Figure 2 based on the utilization of the cores. On the other
hand, the per event overheads, cnd, cd, and bg, are hard to

169

n = number of runnable threads
M = number of cores

//At start of event delay in thread T
ts-T = get_timestamp();

//At start of every delayed event
if ( n==M ) //start measurement
ts = get_timestamp();
else if (n < M)
tmp = get_timestamp();
bd += (tmp- ts )*((M-n)/M);
ts = tmp;
n--;

//At end of event delay in T
te-T = get_timestamp();
bT += (te-T - ts-T);

//At end of every delayed event
if ( n < M )
tmp = get_timestamp();
bd += (tmp- ts )*((M-n)/M);
ts = tmp;
n++;

time instant. For instance, if a function in a thread T is invoked at ts and finishes at te, then the execution time of the
function is measured as te-ts. If Cestimate-T(t) is the estimated
execution time of a thread T up to time t, then the real execution time of the function can be estimated as,
Cestimate-T(te)- Cestimate-T(ts)

int blocking_overhead(thread T)
{
if T is not blocking from replay
return bT;
else
return \
(bT + get_timestamp()-ts-T);
}

We can start the measurements after the initialization of a
replay execution, thus Cinit=0. All counter values (nnd, nd, and
ng in Equation (5)) are maintained for each thread. Note that
all per-event measurements (cnd, cd, and bg in Equation (5))
are measured for concurrent executions on multiple cores.
Since each thread can only be run on a single core, the perevent measurements for each thread, denoted as cnd, cd, and
bg, can be approximated as the product of M and cnd, cd, and
bg, respectively, where M is the number of cores in the system. The blocking delay (bd) is replaced with accumulated
blocking time for each thread and it can be calculated as
shown in Figure 2. Then, the replay overhead in a thread T at
a given instant t can be represented,

Figure 2. (Left): the time measurement in which the cores are underutilized due to the delayed events by the replay scheduling. (Right): the
measurement of blocking overhead for a thread T.

measure online in the execution on multi-core systems. To
avoid the online measurement, we assume they can be
viewed as constants for a given system and can be estimated
offline using measurement programs.
The measurement program for cnd consists of multiple
threads which invoke their own locks. Thus, threads are independent of each other and there is no overhead from delayed events. Hence, nd*cd=0 and bd=0. To avoid any idle
CPU core caused by global locking, extra threads running
busy loops and with no synchronization and IO event are
added. Hence, the blocking overhead from the global locking
becomes zero (ng*bg=0) since all cores are utilized fully. The
execution time without replay, Cm1, is measured and we can
assume that Cm1 = Cestimate. Then, with Equation (1) and (5),
cnd can be calculated using the following equation:

Oreplay-T(t)=nnd-T(t)* cnd + nd-T(t)* cd + ng-T(t)* bg+ bT(t) (5)
Let the thread local overhead from the profiler in a thread
T up to a given instant t be Oprofile-T(t). Then, the estimated
execution time for a thread T at time t can be expressed as,
Cestimate-T(t) = t – ( Oreplay-T(t) + Oprofile-T(t) )
IV.

(1)

EVALUATION

The remaining constant bg is calculated using a similar
measurement program for measuring cd but without the extra
threads with busy loops.

In this section, we show the effectiveness of the analysis
approach in replay execution through several benchmark
experiments. First, we show the overheads of P-Replayer.
Second, we present the evaluation results of the execution
time estimation algorithm. Lastly, the accuracy of dynamic
analyses using P-Replayer is presented. All experiments
were performed on an Intel Core Duo processor running
Ubuntu 12.04 with kernel version 3.2.0.
The two programs shown in section I are used to illustrate the effect of the minimized probe effect in record
phase. All other experiments were performed with 11
benchmarks for desktop computing to reveal the efficiency
and accuracy of the dynamic analysis methods performed in
the replay phase. The benchmarks are from PARSEC-2.1 [1]
and from real-world multithreaded applications: FFmpeg [6],
a multimedia encoder/decoder; pbzip2 [11], a data compressor; and hmmsearch [7], a tool set for bioinformatics.

3) Execution Time Estimation at Thread Level
In thread profiling, the execution times of threads’ activities can be measured. As presented in section II, the measurement overhead from the thread profiler consists of 1)
thread local overhead and 2) execution time differences due
to event ordering. When the profiler runs in a replayed execution, the latter overhead is contained in the replay overhead for delayed events (nd*cd + bd). Therefore, the total
overhead from the profiling on a replay execution is simply
the sum of the replay overhead and the thread local overhead
of the profiler. To estimate the real execution time without
the overheads, we need per-thread measurement at a given

A. Overhead of Record/Replay Operations
Table 3 shows the overheads of the record and replay operations in P-Replayer. “Number of events/sec” column
shows the number of recorded events per second in the execution of each benchmark program, and from the column
we can have a general idea of how big the overheads will be.
The overheads of record and replay operations are 1.46%
and 2.78% on geometric mean, respectively. The results suggest that P-Replayer will be suitable for dynamic program
analysis in replay execution. One exceptional case is fluidanimate which incurs noticeable record/replay overheads due
to the large number of events in the execution.

Cm1 = Creplay – (Cinit + nnd*cnd)
A similar program is used for measuring cd, where a lock
is shared among all threads. Hence, nd*cd0. Using the extra
threads with busy loops, the program keeps ng*bg=0 and
bd=0. Assuming that the execution time without replay, Cm2,
is equal to Cestimate, cd can be calculated using the following
equation:
Cm2 = Creplay – (Cinit + nnd*cnd + nd*cd )

170

Table 3. Record/Replay overhead

Record
Replay
(sec)
(sec)
6.054
6.055
5.073
5.161
3.620
4.887
9.843
9.813
2.288
2.323
6.674
6.677
8.208
8.711
4.071
4.092
3.052
3.157
5.396
5.381
26.624 26.699

Native
execution

Record

PIN Cache

FastTrack

0.0%
0.0%
0.0%

16.8%
36.1%
45.5%

0.3%
1.2%
1.9%

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Queue
Length

Native
execution

Record

PIN Cache

FastTrack

Table 5. Revisit of Table 2 with recording operation

5
10

1.3/7.5
0.5/8

1.3/8.1
0/7.1

8.3/191.9
2.5/146.8

5.5/56.3
2.4/37.9

Tables 4 and 5 are the revisits of Tables 1 and 2 with the
additional measures collected in record phase. For the QT
application (shown in Tables 1 and 4), the QT libraries are
not instrumented in all the tools and no event recording is
done in the execution of the library code. The results in both
tables suggest that multithreaded program executions can be
recorded by P-Replayer with a minimal probe effect.

6.050
5.027
2.054
9.823
2.196
6.643
7.750
3.781
3.053
5.297
26.550

6.055
5.161
4.887
9.813
2.323
6.677
8.711
4.092
3.157
5.381
26.699

6.041
5.122
2.144
9.812
2.247
6.677
7.838
3.859
3.052
5.378
26.595

Error

inputs/sec

0.0%
0.0%
0.0%

Benchmark
Program

Estimation (sec)

Table 6. Execution time estimation without replay

Table 4. Revisit of Table 1 with recording operation

150
300
450

Number of
Record
Replay Overevents/sec
Overhead
head
2,200.5
0.07%
0.08%
2,066.2
0.92%
2.67%
2,163,267.3
76.24%
137.93%
9.8
0.20%
-0.10%
17,487.2
4.19%
5.78%
1.5
0.47%
0.51%
75,623.9
5.91%
12.40%
38,417.1
7.67%
8.23%
3,560.4
-0.03%
3.41%
622.2
1.87%
1.59%
3,644.7
0.28%
0.56%
1.46%
2.78%
9.78%
17.31%

Replay
(sec)

Base time
(sec)
6.050
5.027
2.054
9.823
2.196
6.643
7.750
3.781
3.053
5.297
26.550

Base time
(sec)

PARSEC

Benchmark
Program
facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Geometric mean
Average

-0.2%
1.9%
4.4%
-0.1%
2.3%
0.5%
1.1%
2.1%
0.0%
1.5%
0.2%
1.24%

100%
90%

% of overhead

80%
70%
60%
50%
40%
30%
20%

B. Execution Time Estimation
Based on the overhead analyses in Section III, the replay
overhead for each benchmark can be measured and calculated with Equation (5) and the real execution time can be
estimated using Equation (1). In Table 6, the estimated execution times of the benchmark programs are listed in column 4 where column 5 gives the estimation error. On average, the estimation error is 1.24%.
The replay overhead is classified into the 5 categories in
Equation (5). In Figure 3, the relative overhead in the 5 categories is illustrated for the four benchmarks that have more
than 5% of replay overhead, i.e., fluidanimate, x264, dedup,
and streamcluster. In the chart, the items are correspondent
to the five categories of Equation (5). The applications, dedup and streamcluster, show relatively more percentages of
blocking overhead caused by delayed events (around 50%)
than the other two applications. This implies that they expe-

10%
0%
fluidanimate

x264

dedup

streamcluster

Benchmark
Initialization

No-delay

Delayed

Global-lock

Serialization

Figure 3. The decomposition of the replay overhead is shown for
benchmarks that have more than 5% replay overhead

rience more per event disturbance caused by the replay scheduling than the other two applications.
C. Accuracy of Analysis in Replay Execution
In this section, we present experimental results to show
the accuracy of dynamic analysis in replay execution. Once
we have a recorded execution which is as same as the original execution, the analysis result in replay execution will be
as close as the analysis without any instrumentation. Since it
will be very hard to know what the analysis results will be
without any instrumentation, we assume that the 11 bench-

171

Table 7. Data race detection with FastTrack

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Slowdown

Without
replay

With
replay

140
86
85
27
74
12
151
244
120
68
83
99

139
82
116
27
77
12
148
251
120
69
86
103

Benchmark
Program

8907
2
1
13
1300
0
0
1053
1
0
1

Slowdown
Without
replay

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

37
50
59
59
77
25
41
58
66
51
28
50

With
replay
37
50
127
59
79
24
43
52
66
50
29
56

# of different
function
entries

Number of
races detected

Benchmark
Program

Table 8. Flat profiling comparison from Callgrind

0
0
7
0
0
0
0
4
0
1
0

Table 9 Cache simulation results from PIN Cache

Benchmark
Program
facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Slowdown
Without
With
replay
replay
472
462
336
341
398
731
395
387
411
424
92
91
286
301
395
454
478
478
377
381
498
475
376
411

Cache miss rate - Without replay
L1L1L2Instruction
Data
Unified
0.00%
5.92%
12.02%
0.02%
4.73%
16.89%
0.00%
17.13%
1.15%
0.00%
2.77%
2.23%
1.34%
4.07%
8.10%
0.00%
4.06%
43.46%
0.00%
4.06%
7.25%
0.00%
6.55%
49.14%
0.01%
4.52%
11.56%
0.00%
4.03%
13.54%
0.00%
0.78%
7.14%

Cache miss rate - With replay
L1L1L2Instruction
Data
Unified
0.00%
5.92%
12.07%
0.02%
4.74%
16.14%
0.00%
18.72%
1.58%
0.00%
2.77%
2.23%
1.32%
4.07%
9.62%
0.00%
4.08%
43.12%
0.01%
3.93%
7.66%
0.00%
6.42%
48.95%
0.01%
4.61%
11.26%
0.00%
4.03%
14.40%
0.00%
0.78%
7.10%

fetches a negligible percentage of instructions. For fluidanimate, after removing functions used in the replay execution,
the same function entries are shown in the same order.
Table 9 shows the comparisons of PIN Cache simulator
running in normal program execution and in replay execution. As can be seen from the comparisons, the differences
are negligible. There are only two measures of cache miss
rates with a more than 10% discrepancy between normal
program execution and replay execution.

mark programs show no (or negligible) probe effect from the
instrumentation and compare the analysis results with the
ones collected from replay execution.
Table 7 compares analysis results of the FastTrack race
detector in normal program execution and in replay execution. The two approaches locate the same data races. For
facesim, there was a data race that may result in different
execution paths. P-Replayer stops the replay execution after
encountering the event diverging from the recorded one.
Then, the race is detected with the FastTrack detector and
fixed with correct synchronization. The data in the table
shows the detection result after fixing the race for facesim.
Table 8 compares the flat profiling results from Callgrind
[3]. The flat profiling lists the functions in decreasing order
that have fetched most instructions. For each benchmark
program, we compare the top 10 function entries from normal program execution and replay execution, and the number
of different function entries is shown in the last column of
Table 8. There are three cases showing different function
entries. For streamcluster, the function entries are different
from the 6th entries due to the functions invoked for replay
execution. However, from the 3rd function in the list, the
functions have used less than 1% of total instructions. Similarly, the 10th function entries are different in pbzip2 which

V. RELATED WORKS
Event-based replay has been a preferable choice for program analysis and debugging. Instant Replay [15] is one of
the earliest works that record and replay a partial order of
shared object accesses. RecPlay [26] has proposed a
record/replay mechanism that has low record overhead and a
data race detection algorithm. However, the replay overhead
is too high (around 2x) for uses with dynamic analyses. Replay Debugger [16] uses a similar approach as RecPlay for
record/replay operations, but it focuses on debugging techniques integrated with GDB.
As an effort to avoid probe effect, several hardware-base
dynamic analysis tools have been proposed. To detect data
races, CORD [23] keeps timestamps for shared data that are

172

[11] J. Gilchrist. Parallel BZIP2, http://compression.ca/pbzip2/.
[12] Intel PIN tool. http://software.intel.com/en-us/articles/pin-a-

presented in on-chip caches. With simplified but realistic
timestamp schemes, the overhead can be negligible. DAProf
[21] uses separate caches for profiling of loop executions. In
the approach, short backward branches are identified and
profiling results are stored in the cache. Both approaches are
non-intrusive with acceptable accuracies of analysis results.
However, the requirement of extra hardware mechanisms
may make the approaches impractical.
Moreno et.al [20] has proposed a non-intrusive debugging approach for deployed embedded systems. In the approach, the power consumption of a system is traced and
matched to sections of code blocks. Thus, a faulty behavior
in the execution of the code blocks can be identified only
with an inexpensive sound card for measuring power consumption and a standard PC for the power trace analysis.

dynamic-binary-instrumentation-tool.
[13] Intel Vtune Amplifier XE 2013. http://software.intel.com/en-

us/intel-vtune-amplifier-xe.
[14] L. Lamport. Time, clocks, and the ordering of events in a

[15]

[16]

[17]

VI. CONCLUSIONS
In this paper, we have presented a dynamic analysis method for embedded software. In the method, the execution of
a program is recorded with minimal overhead to avoid probe
effect, and then the program analysis is performed in replay
execution where event ordering is deterministic and is not
affected by instrumentation. For this end, we have described
a prototype P-Replayer and demonstrated the use of replay
execution for dynamic program analyses on the benchmark
programs. In addition, P-Replayer provides execution time
estimation which can be integrated for thread profiling tools.

[18]

[19]

[20]

ACKNOWLEDGMENT
This work was supported in part by the NSF I/UCRC
Center for Embedded Systems, and from NSF grant
#0856090.
[21]

REFERENCES
[1]
[2]

[3]
[4]

[5]
[6]
[7]

[8]

[9]

[10]

C. Bienia. Benchmarking Modern Multiprocessors. Ph.D.
Thesis. Princeton University, 2011.
M. D. Bond, K. E. Coons, and K. S. McKinley. PACER:
proportional detection of data races. In Proceedings of the
ACM SIGPLAN conference on Programming language design
and implementation (PLDI), pages 255-268, 2010.
Callgrind, Valgrind-3.8.1. http://valgrind.org/.
J-D. Choi and S. L. Min. Race Frontier: reproducing data
races in parallel-program debugging. In Proceedings of the
third ACM SIGPLAN symposium on Principles and practice
of parallel programming (PPoPP), pages 145-154, 1991.
CSE 438 – Embedded Systems Programming, ASU,
http://rts.lab.asu.edu/web_438/CSE438_Main_page.htm.
FFmpeg. http://www.ffmpeg.org/.
R. D. Finn, J. Clements, and S. R. Eddy. HMMER web
server: interactive sequence similarity searching. Nucleic
Acids Research Web Server Issue 39:W29-W37, 2011.
C. Flanagan and S. N. Freund. FastTrack: efficient and
precise dynamic race detection. In Proceedings of the ACM
SIGPLAN conference on Programming language design and
implementation (PLDI), pages 121-133, 2009.
N. Froyd, J. Mellor-Crummey, and R. Fowler. Low-overhead
call path profiling of unmodified, optimized code. In
Proceedings of the 19th annual international conference on
Supercomputing (ICS), pages 81-90, 2005.
J. Gait, A probe effect in concurrent programs. Software
Practice and Experience, 16(3): 225-233, 1986.

[22]

[23]

[24]
[25]
[26]

[27]

[28]

173

distributed system. Communications of the ACM, 21(7):558565, 1978.
T. J. LeBlanc and J. M. Mellor-Crummey. Debugging
Parallel Programs with Instant Replay. IEEE Transactions on
Computers, 36(4): 471-482, 1987.
Y-H. Lee, Y. W. Song, R. Girme, S. Zaveri, and Y. Chen.
Replay Debugging for Multi-threaded Embedded Software. In
Proceedings of IEEE/IFIP 8th Int’l. Conf. on Embedded and
Ubiquitous Computing (EUC), pages 15-22, 2010.
C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G, Lowney,
S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: building
customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
conference on Programming language design and
implementation (PLDI), pages 190-200, 2005.
A. D. Malony and D. A. Reed. Models for performance
perturbation analysis. In Proceedings of the 1991 ACM/ONR
workshop on Parallel and distributed debugging (PADD),
pages 15-25, 1991.
D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace:
effective sampling for lightweight data-race detection. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 134-143, 2009.
C. Moreno, S. Fischmeister and M. Anwar Hasan. Nonintrusive program tracing and debugging of deployed
embedded systems through side-channel analysis. In
Proceedings of the 14th ACM SIGPLAN/SIGBED conference
on Languages, compilers and tools for embedded systems
(LCTES), pages 77-88, 2013.
A. Nair, K. Shankar, and R. Lysecky. Efficient hardwarebased nonintrusive dynamic application profiling. ACM
Transactions on Embedded Computing Systems (TECS),
10(3): Article No. 32, 2011.
N. Nethercote and J. Seward. Valgrind: a framework for
heavyweight dynamic binary instrumentation. In Proceedings
of the ACM SIGPLAN conference on Programming language
design and implementation (PLDI), pages 89-100, 2007.
M. Prvulovic. CORD: cost-effective (and nearly overheadfree) order-recording and data race detection. In Proceedings
of the 12th International Symposium on High-Performance
Computer Architecture (HPCA), pages 232-243, 2006.
Qt Project, http://qt-project.org/.
RealView Profiler. http://infocenter.arm.com/help/index.jsp
?topic=/com.arm.doc.dui0412a/html/chapter_1.html.
M. Ronsse, M. Christiaens, and K. D. Bosschere. Debugging
shared memory parallel programs using record/replay. Future
Generation Computer Systems, 19(5):679-687, 2003.
F. Wolf, A. D. Malony, S. Shende, and A. Morris. TraceBased Parallel Performance Overhead Compensation. High
Performance Computing and Communications, LNCS 3726,
pages 617-628, 2005.
X. Zhuang, M. J. Serrano, H. W. Cain, and J. Choi. Accurate,
efficient, and adaptive calling context profiling. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 263-271, 2006.

Key Management in Wireless Sensor Networks
Yann-Hang Lee1 , Vikram Phadke2 , Amit Deshmukh3 , and Jin Wook Lee1
1

Arizona State University,
Tempe, AZ 85281, USA
{yhlee, Jinwook.Lee}@asu.edu
2
Qualcomm USA,
San Diego, CA 92121, USA
vikram@qualcomm.com
3
Siemens USA,
San Diego, CA 92126, USA
Amit.Deshmukh@icm.siemens.com

Abstract. Wireless sensor networks hold the promise of facilitating
large-scale, real-time data processing in complex environments. As sensor networks edge closer towards widespread deployment, security issues
become a central concern. Key management in wireless sensor networks
is a challenging problem. Computationally complex asymmetric crypto
techniques are unsuitable for use in resource-constrained sensor nodes
and use of symmetric cryptography makes the entire network vulnerable
in the event of node compromise. This paper presents a key management
scheme that satisﬁes both operational and security requirements of distributed wireless sensor networks. The proposed scheme accommodates
other techniques like data aggregation for energy eﬃciency and watchdog mechanism for intrusion monitoring. Also, the proposed scheme is
generic enough so that it can be applied to variety of sensor network
protocols.

1

Introduction

Wireless sensor networks can be used for wide range of applications including health, military and security. Realization of these and other sensor network
applications re-quire wireless ad hoc networking techniques, although many protocols and algorithms proposed for ad hoc wireless network are not well suited
for sensor networks because of sensor node and network constraints [1].
As sensor nodes might be deployed in an unattended and unsecured terrain,
it is possible for an adversary to obtain data that sensor nodes transmit to the
base station. In unsecured environment, sensor networks are so vulnerable that
adversary can attack routing messages during network initialization phase and
disrupt the whole network before it even starts functioning.
Providing conﬁdentiality and authentication is critical to prevent adversary from
compromising the security of a distributed sensor network. However, providing
key management for achieving conﬁdentiality and authentication is diﬃcult due
C. Castelluccia et al. (Eds.): ESAS 2004, LNCS 3313, pp. 190–204, 2005.
c Springer-Verlag Berlin Heidelberg 2005


Key Management in Wireless Sensor Networks

191

to the ad hoc nature, dynamic topology and resource limitations of the sensor
network environment.
Apart from basic functionality, some sensor networks apply techniques like
data aggregation for improving energy eﬃciency or watchdog mechanism for
enhancing security. As these techniques are gaining importance, design of key
management algorithm should facilitate these approaches. Also, sensor networks
tend to be application speciﬁc, with diﬀerent routing and data forwarding protocols. Therefore, key management algorithm should be generic so that it can
be applied to wide range of sensor networks.
Other than providing basic security features such as conﬁdentiality and authentication, characteristics of sensor networks place additional requirements on
key management scheme. Key management scheme should be ﬂexible enough to
accommodate ad hoc nature of sensor networks and it should be scalable to handle large number of nodes typically associated with sensor networks. It should
also be able to provide good support for post-routing establishment and should
have provisions for re-keying to guarantee key freshness.
Our research work proposes key management scheme that tries to address
the issues mentioned above. We propose a combined key management approach
that is more secure than the current implementation of key management scheme.

2
2.1

Background
Wireless Sensor Node

For the purpose of this research, we used Berkeley’s motes as the required hardware and software for these nodes have been in fairly developed stage. University of California, Berkeley provides developmental support [2], while Crossbow
Technologies, Inc. [3] makes these nodes commercially available.
2.2

Assumptions About Wireless Sensor Network

Sensor nodes are randomly deployed in the environment and all nodes report
to single destination, called the base station. Though sensor nodes are resourceconstrained in terms of computational power and energy, the base station is a
computer system that is assumed to have unlimited capability. Sensor nodes are
very much prone to get at-tacked and compromised, but we assume that the
base station is trustworthy.
We assume that they use distributed ﬂooding routing protocol to form the
network topology. The base station initiates the routing discovery by broadcasting a beacon message. Each node on receiving beacon message updates the
neighbor information and broadcasts the beacon message further.
Routing discovery phase, also called as beaconing phase, is followed by data
for-warding phase, in which all sensor nodes forward sensor data back to the base
station. As communication range of sensor node is limited, data is transmitted
from the source node to the ultimate destination, the base station through number of intermediate nodes. Neighbor information gathered in beaconing phase

192

Y.-H. Lee et al.

is used to support such multi-hop routing. Channel reliability is not considered
for beaconing as well as for data forwarding phase. We assume communication
medium to be reliable.
We assume that sensor nodes are not mobile. Though they are deployed randomly; once placed at a particular location, they do not tend to move from
that location. But it is dynamic in nature as new sensor nodes may be added
after network formation or some of the nodes may die down due to energy exhaustion or malfunction. This causes change in neighbor information and overall
network topology. To gather this changing topology, beaconing phase is carried
out periodically.
Data Aggregation and Packet Combining. Sensor networks are distributed
event-based systems that have certain characteristics that diﬀer from traditional
communication networks. Some of the characteristics from data ﬂow point of
view are redundant data ﬂow and many-to-one (nodes to the base station) communication paradigm. There are two simple rules that allow two packets with the
same destination to be combined to form a single packet, should they meet at a
node. For example, the data ﬁelds carried by the two packets may be physically
appended together, or two application-related packets may be aggregated. Packet
Combining is a technique of physically combining several data messages into a
single data message. Also, data aggregation is a technique of combining the data
coming from diﬀerent sources enroute, thus eliminating redundancy and saving
energy by minimizing the number of transmissions. Instead of sending data in
various packets, sensor nodes might aggregate them and send the combined data
as a single packet. In this paper, we use the term ’data aggregation’ for both
techniques. As data communication involves heavy energy consumption, reducing the number of data transmissions helps in conserving signiﬁcant energy for
already energy-constrained sensor nodes. Thus, we assume that when data is
forwarded to the base station, some of the intermediate nodes will participate
in data aggregation. Though criterion for the selection of aggregating node is
outside the scope of this research work, we assume that parent node determines
child-level node’s participation in data aggregation in beaconing phase and such
aggregating node aggregates the received data before forwarding it to the parent
node.
Watchdog Mechanism. A node may behave maliciously by receiving the data
packets; but dropping them or modifying them before forwarding. Watchdog
mechanism is one of the solutions to identify such malicious nodes. When a
node forwards a packet to the parent node, the node switches its state to watchdog mode and veriﬁes that the parent node in the path also forwards the packet
correctly to the grand-parent node. As the sensor node communication is inherently broadcast in nature, the watchdog node can listen promiscuously to the
next node’s transmissions. If the parent node does not forward the packet correctly, then it can be said to be misbehaving. Such misbehavior is considered as
intentional malicious activity. As a response mechanism, a node can change the
parent node, as required information is available with the node during beaconing

Key Management in Wireless Sensor Networks

193

phase. In presence of cryptographic techniques, additional complexity arises due
to the fact that parent node may forward the data using diﬀerent encryption
key. In this case, watchdog node must know the key that parent node uses for
encryption.

3

Key Management

This section describes the proposed key management algorithm for sensor network. The aim of the algorithm is to make sensor network more secure by using
combined key propagation techniques so that it is diﬃcult for any adversary to
perform malicious activities like reading or modifying in-transit data. For this,
we make certain assumptions about the sensor network as speciﬁed in earlier
section.
3.1

Algorithm Initialization

Information at Sensor Node. Each sensor node must be equipped with algorithm related cryptographic data. This data can be embedded into ROM of
each sensor node during manufacturing phase. Thus, this node-related data is
not changed during the operation. This information includes:
– Kpb : Pubic key of the base station
– ID: Identity of each node
– EID: Encrypted identity of each node. This is calculated by using private
key of the base station. As only the base station has this private key, no one
else can compute EID from given ID.
– CONST: A constant number
– RAND: A random number for each node. (This number may be duplicated
in some other random node)
– Fseed : One-way function that takes old seed S as input along with CONST or
RAND; and calculates new seed Snew . As this is one-way function, computing
old seed from new seed is diﬃcult task.
– Fkey : One-way function that takes seed S as input parameter along with
CONST or ID to calculate set of keys for each node.
Beacon Message. Most of the routing protocols for sensor networks require
that each node maintain neighbor information. This distributed neighbor information at each node forms the network topology as a whole. This is done using
beacon messages. Initially, the base station creates and broadcasts a beacon message. Each node, on receiving a beacon message, notes the ID of the sender of
the message. Each node broadcasts its only beacon message after receiving the
ﬁrst beacon message; thus eliminating duplicate beacons.
Generally, the base station initiates the beaconing phase and it is done periodically. Because of its periodic nature, newly added nodes become part of the
network in the next beaconing phase, while dead nodes (possibly because of energy exhaustion or any other malfunction) are removed from the network. The

194

Y.-H. Lee et al.

frequency of beaconing phase depends on the dynamic nature of the network.
If network topology changes quite rapidly, beaconing phase should be carried
out more frequently so as to gather new changed network topology. As beaconing phase involves broadcasting messages, it has high communication overhead
in terms of node-energy. Therefore, if the net-work topology is less likely to be
changed, then beaconing frequency should be low enough.
As these beacon messages are used for collecting neighbor information, only
ID of sender node needs to be included. Routing protocols can pass other necessary routing information through these beacon messages. Our approach also
makes use of beacon messages to pass algorithm-related data to all nodes. We
use beacon messages to generate session keys between pair of nodes. For this
purpose, we add several ﬁelds into beacon message format. Beacon message is
modiﬁed by adding these ﬁelds as follows:

The ﬁrst three Boolean ﬁelds are for notifying data aggregation.
– Aggrparent : This Boolean ﬁeld indicates whether sender node’s parent is
going to aggregate incoming data. True indicates that parent node is going
to participate in data aggregation, while false indicates it is not.
– Aggrown : This Boolean ﬁeld indicates whether sender node is going to participate in data aggregation or not.
– Aggrchild : This Boolean ﬁeld indicates whether child node of sender node
should participate in data aggregation or not.
– EIDown : Encrypted ID of sender node. This is used for node ID authentication.
– ET S: Encrypted Time Stamp. This is used for beacon authentication.
– S: Seed Value. The base station initially generates a random seed. Each node
then computes the new seed using Fseed function.
– Level: Level of the node. This value indicates the number of hops between
the node and the base station. For any node, this is useful in determining
parent, sibling and child level nodes. For node at level n:
– All neighboring nodes at level n-1 act as parent-level nodes.
– All neighboring nodes at level n act as sibling-level nodes.
– All neighboring nodes at level n+1 act as child-level nodes.
We assume no node can decide of its own whether it wants to participate in
data aggregation or not. For each node, it is decided by its parent node. This
decision may be based on various criteria like network limit on number of data
aggregating nodes, number of non-aggregating nodes between two aggregating
nodes, frequency of in-coming data, data load on network path, etc. This decision criterion is out of scope of this research work. For the purpose of this

Key Management in Wireless Sensor Networks

195

research, parent node randomly decides about child node’s participation in data
aggregation.
EID is stored in each node, which is calculated using private key of the base
station. As only the base station has this key, no one can calculate EID using
known ID. We also assume that each ID is formed based on certain rule (for
example, each ID starts with letter ’S’). Therefore, only valid EIDs would be
decrypted to valid IDs using public key of the base station. Any other random
EID would be decrypted to some garbage ID.
At each beaconing phase, the base station encrypts current timestamp to
ETS using its private key. All other nodes can check ETS by decrypting it using
the base station’s public key. As only the base station can generate valid ETS,
any attempt by adversary to initiate beaconing phase would be detected, as
nodes cannot decrypt malicious beacon’s ETS to valid timestamp.
Apart from establishing network topology, we use beaconing phase to establish neighboring relationship and to authenticate participating nodes. Thus, no
other foreign node or unauthenticated node can become part of the network
topology after beaconing phase, as no other node in the network has any information about such node.
3.2

Key Management

Beaconing phase is also used for distributing appropriate key generation information in the network. Seed value is used in the computation of set of keys to
be used for data conﬁdentiality between pair of nodes in data forwarding stage.
This appropriate seed value is passed to nodes through beacon messages and
then, required keys are computed.
Seed Calculation. The value of seed computed at each node and the seed
value forwarded at each node is dependent on the participation of the node and
its parent node in data aggregation. If the node is not participating in data
aggregation phase, then it should not be involved in data encryption-decryption
and should just forward the encrypted data received from child node to its
parent node. As seed is used for calculating the key for data conﬁdentiality, nonaggregating node should just forward the received seed so that aggregating child
(or descendant) node and aggregating parent (or ancestor) node can synchronize
their key-values for data encryption-decryption.
The following table describes how seed value is computed depending on the
node’s and its parent’s participation in data aggregation. For any node, these
values are avail-able as Aggrown (indicates parent node’s data aggregation participation), Aggrchild (indicates node’s data aggregation participation) and S1
(indicates received seed) in ﬁrst received beacon message.
Use of one-way function ensures that it is diﬃcult to get previous values of
seed from the new value. But one can compute forward values of seed from the
given value. When two consecutive nodes in the hierarchy (for example, a node
and its parent-level nodes or child-level nodes) participate in data aggregation,
new seed is computed as a function of received seed and random number. This

196

Y.-H. Lee et al.

Fig. 1. Calculating Seed

introduces randomness in the seed value calculation, which makes calculation of
forward seed values diﬃcult.
Key Calculation. We introduce three key sets for each node:
– Node uses Kchild to decrypt the data it receives from particular child node.
Thus, number of these keys is equal to number of child-level nodes of this
node.
– Node uses Kown to encrypt the data that it wants to forward to parent node.
This is the only key the node uses to encrypt the data.
– Node uses Kparent to decrypt the data the parent node forwards. (for watchdogging) Thus, number of these keys is equal to number of parent-level nodes
of this node.
a) Own Key Whenever a sensor node receives ﬁrst authenticated beacon
message, it calculates its own key Kown based on contents of the beacon message.
For any node, required values are available as Aggrown (indicates parent node’s
data aggregation participation), Aggrchild (indicates node’s data aggregation
participation) and S1 (indicates received seed) in ﬁrst received beacon message.

Fig. 2. Calculating Own Key

When two consecutive nodes in the hierarchy (for example, a node and its
parent-level nodes or child-level nodes) participate in data aggregation, new
seed is computed as a function of received seed & random number and Kown is
calculated as a function of computed seed & ID of the node. Whenever a node
generates any data to send or it is necessary to encrypt the data received from
a child node to forward, it uses Kown to encrypt the data. This is the only key
the node uses for encryption.
b) Parent Key On receiving ﬁrst authenticated beacon message, node marks
sender of the message as its parent node and calculates the parent key Kparent

Key Management in Wireless Sensor Networks

197

based on contents of the beacon message. For any node, required values are
available as Aggrparent (indicates parent of parent node’s data aggregation participation), Aggrown (indicates parent node’s data aggregation participation)
and S1 (indicates received seed) in ﬁrst received beacon message.

Fig. 3. Calculating Parent Key

If a node has multiple parent-level nodes, number of Kparent keys is equal to
the number of parent-level nodes. But the node always computes one parent key
for a particular parent node. Initially the node marks sender of ﬁrst authenticated
beacon message as its parent node and computes Kparent for that parent node.
When the node decides to change the parent node and selects one of parent-level
nodes as its new parent node, it computes new Kparent for the new parent node.
c) Child Key After receiving ﬁrst beacon message, node uses other received
beacon messages to gather neighbor information. Whenever the node receives
authenticated beacon message, it stores aggregation ﬁeld values, plaintext and
encrypted ID, level and seed received. Depending on the level, node (at level
n) classiﬁes neighbors into parent-level nodes (with level n - 1), sibling-level
nodes (with level n) and child-level nodes (with level n + 1). On receiving beacon message from child-level nodes, the node calculates child key Kchild based
on contents of the beacon message. For any node, required values are available
as Aggrparent (indicates parent of child node’s data aggregation participation),
Aggrown (indicates child node’s data aggregation participation) and S1 (indicates received seed) in the received beacon.

Fig. 4. Calculating Child Key

If a node has multiple child-level nodes, number of Kchild keys is equal to the
number of child-level nodes. But the node computes child key for a particular
child only after receiving encrypted data from it. As a performance improvement,
the node may store child keys of few child-level nodes in its memory.

198

Y.-H. Lee et al.

3.3

Beaconing Phase

Beaconing Steps. This section describes in details various steps in beaconing
phase. This phase enables each node to gather neighbor information and to
generate network topology. Our algorithm uses this phase for distribution and
creation of various session keys; thus achieving key management. The operating
sequence in beaconing phase is as follows:
a) Beacon generation steps by the base station:
– The base station encrypts its own ID and current timestamp to EID & ETS
respectively using its private key.
– It generates random seed S, and assumes to be at level 0.
– The base station broadcasts following beacon message.

where X is the data aggregation participation ﬂag for all child-level nodes
(nodes at level 1) of the base station, determined by certain rule. (Analysis of
this rule is out of scope of this research work. For the purpose of this research,
this value is calculated as random value between 0 and 1)
b) Beacon generation steps by individual node: Whenever any node receives
ﬁrst beacon message as,

it creates the new beacon message as

– Data aggregation participation ﬂags B and C are copied to appropriate ﬁelds
in the new beacon message and ﬂag X for child-level nodes is determined by
certain aggregation rule.
– Node ﬁlls EID ﬁeld with its own encrypted ID and copies encrypted timestamp from the received beacon’s ETS ﬁeld.
– Node generates new seed value S2 from S1 depending on the data aggregation
ﬂags, as described in the previous section.
– Node increments its level by 1 to n + 1.
– Node broadcasts this newly created beacon message.
c) Network formation steps:
– The base station initiates the beaconing phase by creating & broadcasting
the new beacon message.
– When a node receives ﬁrst beacon message, it decrypts ET S using public
key (Kpb ) of the base station. If it is decrypted to the valid timestamp,
then the node can assume the beacon message to be authentic. As only the
base station has private key to encrypt the valid timestamp, no one else can
generate valid ET S.

Key Management in Wireless Sensor Networks

199

– The node also decrypts received EID using public key (Kpb ) of the base
station to identify and authenticate source of the beacon message. As only
the base station has private key to encrypt the valid ID, no one else can
generate valid EID. This ensures that no node can fake its ID; thus achieving
node authentication.
– The node marks sender of the ﬁrst authenticated beacon message as its
parent node.
– As described in earlier sections, the node calculates its own key (Kown )
and parent key (Kparent ). The node uses Kown to encrypt all messages that
it sends or forwards and uses Kparent to watchdog parent node in data
forwarding phase.
– The node creates and broadcasts its own beacon message.
– On receiving subsequent authenticated beacon messages, the node stores
data aggregation ﬂags, received seed, level and ID of the sender node. This
helps in gathering neighbor information and in calculating Kchild later.
d) Network reformation steps: Beaconing phase is periodic in nature. The frequency of beaconing phase is deter-mined by dynamic nature of the network. As
beaconing phase helps forming network topology, frequency of beaconing phase
should be high enough in dynamic networks where individual nodes are mobile
and topology keeps changing frequently.
As beaconing phase involves broadcasting beacon messages, it has higher overhead in terms of communication energy consumption. Therefore, choosing right
frequency for beaconing phase is important factor in energy-constrained sensor
networks. As we make assumption about the sensor nodes not being mobile in
nature, we can say that the sensor network as a whole is also static. But new sensor nodes may be added to the network at later stages or sensor nodes may die
down (because of energy exhaustion or node malfunction). This causes updating
of changed neighbor information in some of the nodes. Periodic beaconing phase
facilitates updating of neighbor information in such nodes. Every time the base
station initiates beaconing phase with following steps:
– The base station encrypts its own ID and current timestamp to EID and
ETS respectively using its private key.
– The base station broadcasts a beacon message with newly generated random
seed (S).
– All nodes follow the same steps as in network formation phase to gather new
up-dated neighbor information along with new key information.
– Thus, each node gathers information about newly added nodes and deletes
information about nonfunctional nodes. This accommodates changing network topology.
Whenever a node receives beacon message from old neighbor (one which was
this node’s neighbor node in earlier beaconing phase), the node authenticates
neighbor ID by comparing neighbor EID with the stored information. This helps
in considerable saving in time and energy consumption of the node, as public key
encryption is computationally intensive and requires considerable energy. Thus,

200

Y.-H. Lee et al.

each node makes use of public key encryption only once per neighbor node for
node authentication. This is one of the advantages of the algorithm as it uses
public key encryption sparingly.

4

Analysis of the Algorithm

Earlier sections described various steps in the proposed key management algorithm. This section analyzes the algorithm as a whole to explain its features
that make this algorithm feasible to implement and better alternative option,
compared to other similar key management algorithms.
4.1

Advantages of Proposed Key Management

a) Combined key management approach We know that single key management
strategy is not suﬃcient and feasible for the needs of resource-constrained sensor
network. They require key management technique with high key granularity and
high tolerance to key or node compromise. Conventional public key management
technique is not communication eﬃcient, use considerable battery energy and are
time-consuming. Conventional secret key management technique is simple and
energy eﬃcient, but single node compromise may make the entire network vulnerable. The proposed algorithm makes use of both key management techniques
to exploit their advantageous properties while minimizing their drawbacks. The
proposed algorithm combines several cryptographic techniques as follows:
– Public key cryptography: This is used for beacon message authentication
(by checking ETS), node authentication (by checking EID) and communicating node-identity in secure manner for key setup. Its property of being
more secure than any other scheme is advantageous in key setup operation.
Though, public key technique has higher energy consumption and higher latency, it would only be used periodically in beaconing phase. Any node uses
this technique for each of its neighbor nodes only once in its lifetime.
– Identity based cryptography: This scheme uses identity of a node to create
secret key, which is then used for pair-wise node communication. In beaconing phase, node ID is communicated securely. Identity-based cryptography
makes use of this ID to form secret key. Therefore, a sensor node need not
broadcast its key explicitly, thus reducing communication overhead, as lower
number of bits needs to be broadcast.
– Secret key cryptography: This scheme is more energy eﬃcient and less computationally intensive. The secret key is computed using information distributed in beaconing phase. This secret key is then used for communication
between pair of sensor nodes in data forwarding phase.
b) No Extra Messages This scheme establishes key management using existing messages. It does not require any extra message for key setup. Algorithm
carries out key establishment by distributing cryptographic information with
beacon message. This causes an increase in the size of beacon message; but the

Key Management in Wireless Sensor Networks

201

communication overhead caused by increase in length of original message is lower
than the communication overhead for the setup of new message.
c) Adaptivity Key management algorithm also takes into consideration sensor network that employs data aggregation. The algorithm distributes the keys
in such a way that node not participating in data aggregation can forward encrypted message without any cryptographic operation, while aggregating nodes
can decrypt the data encrypted by its ﬁrst aggregating descendent. Thus the
scheme is tunable to data aggregation, as only aggregating node needs to use
cryptographic operation, while non-aggregating node can just forward the encrypted data.
The algorithm also makes sure that a node can use watchdog mechanism even
for encrypted messages. Every node can calculate the key that its parent node
uses for encrypting outgoing messages. The node can use this key for decrypting
messages forwarded by parent node to carry out watchdogging.
In case of failure on part of parent node, child-level node can forward encrypted packet to its alternate parent and do watchdog on alternate parent. Thus
the scheme considers the dynamic behavior of the network. Also, this scheme is
self-organizing as it makes use of beaconing phase for gathering changing topology information.

5

Simulation Results

For the performance evaluation of proposed key management algorithm for sensor networks, we used network simulator NS-2 (Network Simulator version 2.26).
5.1

Energy Consumption

For any algorithm related to sensor networks, issue of energy consumption is the
most important because of energy-constrained nature of sensor nodes. As energy
consumption is much larger in communication overhead than in computational
overhead, the simulation focuses on the communication cost. The proposed algorithm makes use of beacon message to distribute key-related information. In
data forwarding phase, size of data message is same regardless of use of cryptographic technique, as the size of plain data and its corresponding encrypted
data is same. Therefore, increase in the size of beacon message is the cause for
increased communication overhead in the proposed algorithm. We compared average energy consumption per node for diﬀerent beacon message types. In Fig. 5,
we compared communication overhead for three beacon messages.
– Original beacon message that carries no data
– Beacon message with 16 Bytes of additional data to original message for
beacon authentication only
– Beacon message with 24 Bytes of additional data to original message for
pro-posed algorithm (beacon authentication and data encryption)

202

Y.-H. Lee et al.

Fig. 5. Energy Consumption Comparison

5.2

Robustness to Node Compromise

We consider that sensor nodes are deployed randomly and may not be attended
later. This makes sensor nodes prone to get attacked and compromised. Adversary can get all information from the compromised node, including sensitive
cryptographic data. Apart from public key (Kpb ) of the base station, encrypted
ID (EID) of the node, constant number (CONST) and one-way function, adversary can also gather neighbor information stored in the node; thus making
neighboring nodes vulnerable. Therefore, compromising one node aﬀects its surrounding nodes also as adversary can decrypt messages encrypted by these surrounding nodes. But as possibly diﬀerent key is used by neighbor’s neighboring
node to communicate; eﬀect of compromised node is limited and localized. The

Fig. 6. Eﬀect of Single Node Compromise

Key Management in Wireless Sensor Networks

203

following graph shows average number of nodes aﬀected be-cause of single node
compromise when proposed key management scheme is used.
As only some part of the network is aﬀected, proposed scheme makes the network more robust. The other implemented key management scheme, TinySec [4],
which relies completely on symmetric cryptography, has single key for the entire
network. Therefore, compromising one node makes the entire network vulnerable
as adversary can decrypt any message between any two nodes. Following graph
is another version of the above graph, which compares the percentage network
aﬀected by single-node compromise when proposed algorithm (average 17%) and
TinySec (100%) are used.

Fig. 7. Comparison between Proposed Scheme and TinySec for Aﬀected Network by
Single Node Compromise

6

Conclusion

As with the other networks, it has been acknowledged that cryptographic techniques need to be considered to address issues related to data authentication and
conﬁdentiality in sensor networks. But both forms of cryptographic techniques symmetric and asymmetric - are not adequate when considered alone. Our work
combines both cryptographic techniques to get beneﬁts of both schemes while
minimizing their draw-backs. The proposed key management scheme uses public
key cryptography judiciously for authentication purpose, thus reducing overhead
associated with it. The scheme uses eﬃcient secret key cryptography for data
conﬁdentiality by setting up (possibly diﬀerent) session keys between each pair
of nodes. Having diﬀerent session keys reduces the impact of single-node compromise on the network.
This combined key management scheme is generic in nature so that it can be
ap-plied with diﬀerent routing algorithms for sensor networks. This scheme also
accommodates sensor networks that utilize other techniques like energy-eﬃcient
data aggregation and secure watchdog mechanism.

204

Y.-H. Lee et al.

As a future work, we can make this key management scheme more dynamic so
as to vary security level as per the need. After initial beaconing, the base station
can decide whether nodes should participate in only beacon authentication or in
only data encryption or in both. It can also decide to switch oﬀ entire encryption mechanism temporarily. This helps in conserving energy when there is low
security requirement.

References
1. I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, E. Cayirci: A survey on Sensor
Networks, IEEE Communication Magazine, August 2002, pp. 102-114.
2. TinyOS: http://webs.cs.berkeley.edu/tos/index.html
3. Crossbow Technology, Inc.: http://www.xbow.com
4. C. Karlof, N. Sastry, U. Shankar, D. Wagner: TinySec: TinyOS Link Layer Security
Proposal, July 2002.

Adaptive Selection of Access Path and Join Method

Yann-Hang L e e

Philip S. Y u

C o m p u t e r and Information Sciences D e p a r t m e n t
University of F l o r i d a
Gainesville, FL 32611

IBM Thomas J. Watson R e s e a r c h C e n t e r
Y o r k t o w n Heights, NY 10598

Abstract

For complicate queries, the estimation on the size of intermediate results may be inaccurate. By invoking query optimization dynamically, the actual size of intermediate results can be
used to calculate the cost of succeeding operations. For instance,
in distributed INGRES [Epst80], a dynamic optimization a l p
rithm considers the actual size of intermediate results to determine what is the next piece of the query t o be processed and
which sites should be participated in the execution. A hybrid
strategy proposed in [Yu83] invokes a dynamic optimization only
if the size of' intermediate results after a semi-join is quite
different from the size estimated. Though the actual size is used
in each optimization step, the performance of these approaches
depends substantially upon the accuracy of join selectivity.

Query optimization is crucial to relational database performance. Traditional approaches select the access plan with the
minimum projected cost based on estimated measures, e.g., selectivity. Since estimates can deviate substantial from true values,
the access plan chosen can be far from optimal. W e propose an
adaptive approach which utilizes the information embedded in
indexes to identify the tuples satisfying a given predicate or having a match in a join operation. Then, an access path (index or
table scan) and a join method (index join, nested loop, sort-merge)
are chosen t o construct the results adaptively. This leads to the
optimal evaluation of queries. With an efficient implementation,
the adaptive decision process becomes a part of query evaluation
procedure, such that the overhead of the approach is minimized.

T h e concept of delaying access plan decision until the information becomes available a t run time can be applied t o select or
modify query evaluation steps adaptively. This approach can
enhance the plan selection and lead t o substantial performance
improvement. Although it had been considered mainly in multiway joins, its applicability is far more general. It can be used in a
simple select-type query t o choose an optimal access path or in a
join operation t o refine the choice of join methods. As we'll see in
this paper, an adaptive access plan can be constructed which uses
the existing indexes t o get information first and then t o select an
optimal access path or join method.

1. Introduction
In relational database systems, query optimization is crucial
to their performance. Too often, an access plan is generated
based on the oversimplified assumptions on relation characteris
tics. For example, consider the simple case of retrieving tuples
which satisfy a given predicate from a relation . To make a
choice between index and table scans, one may use attribute
selectivity to estimate the number of qualified tuples [Seli79].
With the assumptions of uniformity and independence of attribute values, a n attribute selectivity is simply equal t o the ratio of
the number of distinct values taken by the attribute t o the cardinality of the relation. If the distribution of attribute values is
with a high degree of skew, the number of tuples actually selected
can vary substantially from the value estimated by the attribute
selectivity. This variation may make the access plan selected
non-optimal and cause significant performance penalties.

In Section 2, we apply the adaptive concept t o eliminate the
need of using selectivity to determine the optimal access path.
T h e index is examined t o identify the number of qualified tuples.
When the number of qualified tuples is found t o go beyond certain
predetermined threshold, a table scan will be used instead of an
index scan. In Section 3, an adaptive join approach is investigated which merges the indexes of join attributes first. Thus, we
identify the tuples which actually have a match. Appropriate join
method is then selected. Comparisons with the conventional
approach are provided which show the new approach can achieve
substantial performance enhancement. Finally, we give a conclusion remark in Section 4.

Histograms or other summary statistics can be used t o p r e
vide a more accurate estimate on the cost of an access plan than
a single aggregate value such as an attribute selectivity. T h e d i s
tribution of attribute values can be beneficial t o the query access
path selection when the predicate expression can be evaluated
during compilation [Piat84, Laks871. If a predicate involves variables, a reference distribution, i.e. the relative frequency each
attribute value is referenced by a query, may be used in conjunction with d a t a distribution to select the optimal access path
[Laks87]. In this case, an access path is selected t o optimize the
average performance over all query executions. It is not neceS
sarily optimal for each individual query.

2. Adaptive Selection of Access Path
W e first examine how to select the optimal access path for
retrieving tuples from a relation R . A simple select query is considered which is represented in the following form.
SELECT F R O M < R >
WHERE < R . A comparison-operator

250

expression >

P to represent the predicate <
R.A comparison-operator expression >. When an index
exists on attribute R.A, the choices on access path is to either

For simplicity, we use

sequentially scan through the whole relation or to use the index
on R.A to retrieve the qualified tuples. These two access paths
are called a table scan and an index scan, respectively.
Consider an index model implemented as a B t t r e e shown
in Figure 1 [Blas77, Seli79, Haer781. Non-leaf nodes of the tree
consist of pointers to their direct descendants and the high key
values of their descendants. Leaf nodes contain sets of (key,
np(key), list of TID’s ) where each TID (tuple identifier) links to
a tuple with a key value key. T h e field, n p ( k e y ) , indicates the
length of the T I D list. This field is necessary to accommodate a
variable length list. In addition, the leaf pages are linked together
to support direct traversal between leaf nodes. To search for the
tuples with a particular key, an index scan is started from the
root, ended with the leaf node containing the key. Then, the
tuples are located through the associated TID’s.
If the tuples of a relation are physically stored in the same
order as their index keys, the index is said to be clustered
[Blas77]. When using a clustered index to retrieve tuples from a
relation, each data page of the relation needs to be read in from
disk at most once. Although a relation can have several indexes
on different attributes, only one of them can be a clustered index.
Other indexes, in which the tuples having the same index value
are likely to scatter through all pages, are called nonclustered
indexes. If tuples are retrieved using a nonclustered index, the
number of pages read can be equal to the number of tuples
retrieved in the worst case, a s u m i n g limited buffering capability.
An index scan then become costly when number of tuples to be
selected is large. Generally speaking, the choice for an index scan
as an access path is quite clear, if a clustered index on R.A is
available. It is the choice of nonclustered index and table scans
that need more careful deliberation during access path selection.

I

Data Pages

I

Figure 1. B-tree structured index

I

Let OR-={
np FR(np)< C,(R)/c }. Then, OR is the
threshold indicating the maximum number of tuples that can be
more economically accessed through a nonclustered index scan
than a table scan.
Given a query (SELECT F R O M R WHERE P), the adap
tive access path selection algorithm is given as the following and
is illustrated in Figure 2:

Note that the performance of a nonclustered index scan
depends on the number of tuples to be retrieved. It will be
appropriate to measure this number and then use it for selecting
between a table scan and a nonclustered index scan. As indicated
in Figure 1, a leaf node of an index tree provides information on
the number of tuples which have a key value. This suggests that
we can obtain the number of tuples to be selected during an index
scan before we read the tuples in. An adaptive decision can then
be made whether to continue the index scan or to switch to a
table scan for the successive tuple retrievals.

Reset np(P)=O
Search next key satisfying
and obtain np(key).

P using the index tree on R.A

np(P)=np(P)+np(key) and the TID list is buffered for
retrieving qualified tuples in case that the index scan is
selected. If np(P)<B,, then go to step 2, else stop the
index search and begin a table scan.
When no additional key which satisfies P can be found, use
the obtained TID’s to retrieve the qualified tuples if
np(P)>O, or no tuple satisfies P , otherwise.

To construct the adaptive access path selection algorithm,
we define the following notations:

We next examine in details how the search at step 2 and the
continuation of the index scan at step 4 are proceeded. When the
predicate P is in the form R.A = value, we need to traverse the
index hierarchy to search for the only key value. T h e leaf node
containing the key (value), np(walue),and a TID l i t is read into
the memory. If n p ( w a l u e ) g R , the tuples are ready to be
accessed through the TID list. Thus, in this case the algorithm
behaves similarly to an ordinary index scan. If the comparisonoperator of P is a range operator, we can traverse the index
hierarchy down to the leaf node which has the smallest key satisfying P to obtain n p ( k e y ) . Then, other keys satisfying P can be
accessed through the chain on the leaf pages until either a key
which becomes out of range (the index scan is to be selected and
TID’s are in the buffer for tuple retrieval) or np(P)>eR (a table
scan will be used).

n p ( k e y ) : the number of tuples whose indexed attribute has
the value key.
np(P):the number of tuples satisfying the predicate P.
C,(R):the cost of the table scan of relation R
c : the cost of accessing one page a t random.
FR(np): the number of data pages accessed to retrieve np
tuples from relation R using a nonclustered index scan. If
the buffer is large enough to fit all accessed pages, this
number can be estimated by the function b ( m , p , k ) as in
[Yao77] or by the approximations suggested in [Whan83].
For a finite LRU buffer, the formula proposed in [Mack851
can be adopted.

25 I

selection
0

2-

.E
0
s
L

I

index =con/

z,/'

-

2 "

I

/

table scon

/

/"

access index
find key satisfies P
np(P)-np(P)+np(&ey)

switch to

Number of tuples selected

I

Select from R which is with 10,000 tuples and 290 pages in disk
<-

Figure 3. Comparisons of the I/O costs of the scan methods
in the example

index scan
Im

are scanned to find the matching tuples. In hash join, the two
joining relations are partitioned based on a common hash function. Join operation is performed on corresponding partitions.
T h e performance of these three methods have been studied in
[DeWi84]. In general, for each method, there exist certain situations that it outperforms others.

Figure 2. Flow diagram of progressive access path selcction
It is easy to see that, in the above algorithm, the measured

Consider a join query of the form

n p ( P ) instead of estimated selectivity is used to determine the
optimal access path. Consider an example in which tuples in relation R are selected if they satisfy the predicate R.A=w where w
is a variable whose value is determined at run time. T h e relation
R has 10,OOO tuples and there are 500 distinct keys in the nonclustered index on attribute R.A. It is assumed that 290 pages
are required to store R . Suppose that there are k tuples satisfying
the predicate. Whe the index scan on attribute R.A is adopted,
290x(1-(1-1/290)
) data pages will be accessed approximately
in a random order under the model in [Card75]. Furthermore,
there are 3 extra index pages needed to be accessed. If a table
scan is used, the whole relation is accessed sequentially. We
m u m e that a sequential access can be speeded up, through prefetching or retrieving a track [Hagm86], by a factor 5 comparing
with a random access. Thus, the cost of the table scan is
equivalent to the cost of accessing 58 pages in random order. In
Figure 3, the costs of these scans are plotted against the number
of tuples selected. The threshold e,, is also indicated in the figure.
T h e adaptive approach proposed takes the index scan when IC is
less than or equal to 8,. It will switch to the table scan after
finding out k is greater than OR. In this case, the overhead
incurred in reading the extra index pages is practically negligible
as compared with the cost of scanning the whole table.

SELECT FROM

< R,, R , > WHERE < R,.A=R,.B >

Employing any of the above mentioned join methods, all tuples in
the joining relations would be retrieved at least once. For
instance, in nested loop join, the whole inner relation has to be
scanned to find a match for every tuple of the outer relation.
Similarly, in sort-merge join or hash join, all tuples of each joining
relation have to be sorted or hashed, respectively. It may occur
that, after scanning through a large number of tuples, we find
that only a small number of tuples are needed to comprise the
result of a join.

s

When the indexes exist on the join attributes of both joining
relations, an alternative is to join the indexes first and then to
retrieve the tuples through the joined indexes. This approach,
called the index join, has been suggested in [Blas77,Whan85].
T h e approach can be efficient when only a small number of tuples
find matches in a join. However, it becomes costly when a large
number of tuples need to be retrieved through the indexes.

To accelerate a join operation, a combined access path
structure [Haer781 and a join index [Vald87] have been proposed.
T h e approaches construct, a priori, a data structure consisting of
pairs of TID's or surrogates to link the tuples of different relations having the same key values or satisfying a join predicate.
Thus, when a join operation is requested, results can be retrieved
through the provided links directly. Certainly, update overhead
will be incurred to maintain consistency of these data structures.
Furthermore, for ad hoc queries, it may not easy to anticipate
joining relations and join predicates such that an appropriate data
structure can be constructed in advance.

3. Adaptive Selection of Join Method
There are three major methods for doing join operations:
nested loop, sort-merge and hash join. Nested loop join is simple
to program and is implemented in most relational database systems. It is efficient when at least one of joining relations can be
fitted into main memory. In sort-merge join, two joining relations
are sorted in join attribute order first. Then, the two relations

252

join'

In the following, an adaptive selection of join method is prop d . W e shall consider the case where the indexes on the join
attributes exist for both relations. T h e approach starts with joining the indexes. Information, such as the indexes of matching
tuples, the cardinality of resulting relation and the number of
tuples in each joining relation satisfying the join predicate, is
obtained. Then, a proper join method and a c e s path can be
chmen to get the join result. Extension to the situation with only
one index on join attribute is also discussed.

load Sf and:S
from indexes

1

construct

Ti?

3.1. Evaluation Principle

index(RI )
index( R2 )

tuples in Ri. W e denote the index scan and the table scan of Ri
as andez(Ri) and tuble(Ri). Once the choice on access paths is
made, join method would be selected as described in the following
for various cases. T h e available buffer space, denoted as Ma,
pages, is also considered when the join method is chosen. Note
that, when the table scan is used to retrieve the tuples of R,.'
from Ri, the scan can be terminated after 4i tuples are selected.
Case 1.

e,
t€e:.

For every tuple

3.2. Adaptive Join Plan
T h e flow of the proposed adaptive j o 9 plan is shown in Figure 4. In the first stage of the 'oin plan,
and Sfare joined to
construct
Note that
is sorted on attribute C / . A as
it would come naturally p m the join operation since the leaf
A
nodes in which S, and S are embedded are sorted. In parallel
with the construction of
, 4, and 4, are also calculated. If
is null, the join result is also null and no further operation is
needed.

s,

tee,,

(1)

read o ~ - ~ , ~ (using
R , )index scan.

(2)

read O ~ - ~ , ~ using
( R , )index scan.

(3)

construct the Cartesian production of o ~ , ~ , ~ ( R
and, )
uB-t.A(R2)'

T h e join method for this case is similar to an index join. It
is very clear that, in the above approach, no tuple from R," is
accessed. T h e resultant relation, obviously, is ordered by the join
attribute. In fact, as long as either o ~ - ~ , ~ (orR O
, )~ - ~ , ~ ( R , )
can fit into the main memory, the only 1/0 required is t h a t for
the index scans of retrieving R;.

T h e second stage of the join plan is to choc6e the optimal
join method and access path for generating join results. From the
above discussion, we know that only 4l and 4, tuples need t o be
retrieved from R , and R,, respectively. T h e access path selection described in Section 2 can be applied here. Given OR, which
is the threshold defined in Section 2, an index scan will be used to
access tuples of R: if the index is clustered or if it is nonclustered
and q5ii<eR,. Otherwise, a table scan will be used to access all

Case 2. indez(R1)and table(R,).
For generality, we assume that the tuples of R , are stored
without regard to the ordered of attribute R,.B. Let the
memory space, in term of pages, required by R: be M(R;')which
can be calculated based on 4i and the tuple length of Ri. With
different Ma",and M(R;'), join methods are suggested for the following situations. It should be mentioned that, join methods for

253

-7-

indez(R1)and andez(R,).

This case occurs when both 4, and 4, are small or the
clustered indexes exist. A straightforward approach is given as
follows:

4i= IRl 1.

-

sort-merge

Figure 4. Flow diagram of adaptive join plan

(q,)}

&

tub/e(RI
table(Rzl

I

sor 1-merge

Let the relation I " L B = S ~ [ A = = Bbe
] Sa~joining count relation. T h e relation e r c o n s i s t s of three attributes: A (or B),
K and K where K , and K2are count attributes obtained from
?4
S: and S,, respectively. Based on .,;"',
we can partition Ri
into two relations: R: and RY, where R:={t I t a i , t.A (or
t.B) E7r
and RY=Ri-R:. It is easy to show that
R,'[A=dR, = R,[A=B]R,. This implies that, to join R I and
R,, we only need to perform the join operation on R: and R / .
All tuples from R: and Rg" can be ignored. In addition, with
, we define an integer di as the sumthe count attributes of
mation of t.Ki for all
Naturally, 4i is the number of
tuples in Ri which have at least one match in the join and

e,.

1
table(RI
index(R2)

I

I}

I

Ir

1

index(RI
tub/e(R,)

I

For each relation R . and its attribute Ri.A, we define a
count relation S;"={(u,k\ u@rA(Ri),k = l ~ ~ _ ~ ( where
R~)
( R i ) is the number of occurrences of value "a"in Ri.A.
St:&
be viewed as a relation with two attributes: an attribute
A and a count attribute, i.e. each tuple consists of a distinct
value appeared in attribute Ri.A and the number of times the
value occurs. As shown in Figure 1, the relation S;" is embedded
in the leaf nodes of the index on Ri.A.

I

up2

access path selection

A %way equi-join operation on relations R , and R , is considered. T h e equi-join operation on attributes A of R, and B of
R , is denoted as R,[A=B]R,. It is assumed that both indexes,
either clustered or nonclustered, on R,.A and R,.B exist. Also,
we let T ( R . )and o ~ - ~ ( denote
R ~ ) the projection of relation Ri
A '
onto attribute A and the set of tuples in Ri satisfying the predicate Ri.A=u, respectively.

10

up1

table(R,) and indes(R,), can be obtained similarly by exchanging R , and R , as in the following discussions.
(1)

3.3. Performance Improvements
T h e adaptive selection of join method can enhance the performance of join operation in three ways. As described above, the
adaptive approach can

M(R,')IM,u-l and M(R,')+M(R,')>Mau: First, R ' is
read into the buffer using the index scan. Then, with R ,$ as
the inner relation and R , as the outer relation, nested loop
join is performed. Note that an additional page is required
for buffering R,.

(2)

M(R,')IMau-l and M(R,')+M(R,')>Mau: Nested loop
join can be used in this situation with R,' as the outer relation and R,' as the inner relation. To obtain R,', selection
needs t o be done. This
of tuples, t€R, and t.BmA(Tf)

(1)

Measure the number of tuples which satisfy the join predicate, R,.A=R,.B. This enables us to select a table scan or
a n index scan as the optimal access path to retrieve these
tuples.

(2)

Identify which tuples satisfy the join predicate before the
join is performed. Thus, a filter can be set up to reduce the
number of tuples t o be considered for the join. T h e buffer
space needed t o fit all tuples of R,' or R,' into main
memory can be much less than the buffer required to join
the whole relations.

(3)

Choose the optimal join method. From previous studies, no
single join method can provide the best performance over all
situations. With the adaptive approach, the optimal join
method is selected based on the size of resulting relation and
the buffer space available.

(e:

can be performed through binary search
in-memory hash table. T h e other alternative
(or R,) in memory and then merge with R,'.
tive may be more efficient if the order on the
is required in the resulting relation.

is sorted) or
is to sort R,'
This alternajoin attribute

(3)

M ( R , ' ) + M ( R , ' ) ~ a u :T h e join methods described in

(4)

M(R,')>Mau-l and M(R,')>Mau-l: Since tuples of
R,' can be accessed efficiently through the index and are

situations (1) and (2) are all applicable here.

T h e adaptive join %ethod involves additional operations
Since both St and
such as joining Sf and S, t o obtain +B
+ B .12 '
S," are sorted, the construction of
is straightforward. Also,
when (1) the cardinalities of S;' and Sf,i.e. the number of distinct values in R,.A and R,.B, are much less than that of R ,
and R , and (2) tuple sizes of the joining relations are large with
many attributes, the overhead of joining the indexes becomes
insignificant comparing with joininj
and R , directly. Overhead may also incur when n A ( e 2) is used t o select out tuples
belonging to R: in the case that table(Ri)is chosen. However, if
R: needs to be sorted (or hashed), the selection process becomes
part of the sorting (or hashing) and incurs no extra cost.

,

ordered, sort-merge join is preferred. W e can combines
selection and sort of R together using linked lists associFirst, we retrieve tuples from
ated with the tuples of
R , sequentially using the table scan. For a tuple r
retrieved, if there is a tuple t of
such t h a t r.B=t.A,
then the tuple r is added t o the corresponding linked list of
t . Otherwise,
and can he discarded. When the
buffer is full, the I' ked lists are concatenated according t o
the order of T$!A
and are written to the secondary
storage as a sorted bucket of R,'. Finally, R,' and sorted
buckets of R,' are merged to complete the join.

2'.:

e:

R1

Consider an example of R,[A=B]R, where IR, I=lM and
IR, 1 4 . 6 M . There are nonclustered indexes on the join attributes. Assume that the tuple lengths of R , and R , are 60 bytes

Case 9. table(R1)and table(R,)
Similar t o the selection of tuples from R , to form R,' when
table(R,) is chosen in Case 2, selection of tuples from R , t o form
R,'
is performed.
When either M ( R , ' ) w a v - l or
M(R,')%au-l, nested loop join of R,' and Rz) can be imple-

and 128 bytes, respectively, and there are 50,000 distinct values in
attribute R,.A and 45,000 in attribute R,.B. T h e size of the
available buffer is assumed t o be 4M bytes. Depending upon the
values of $ i , the adaptive join plan chooses different join methods
and access paths t o perform join operation as shown in Figure 5.

mented efficiently. T h e smaller relation is treated as the inner
relation. Both R , and R , will be scanned once.

T h e values of 6' and 6' are calculated as in the example of SecRI
R2
tion 2 and are indicated in the figure.

When M(R;)>Mau-l and M(R,')>Mav-l, hash join
and sort-merge join should be applied.
(1)

Hash join:

$2"

T h e relative 1/0 costs (in terms of random page access) of
sort-merge join and the adaptive join plan are compared and plotted in Figure 6. In the case of table(R1)and table(R,), We
assume that sort-merge join is taken in the adaptive join plan
T h e cost for sorbmerge join is calculated with the model suggested in [DeWi84]. T h e number of pages accessed is approximated with the model in [Card751 if a nonclustered index scan is
chosen. For table scan, we assume that the speedup due to prefetching in sequential access is with a factor 5. T h e abrupt
discontinuities under the adaptive plan occur when there are
changes of access path and join method. T h e figure shows that
the adaptive join plan outperforms sort-merge join substantially.

can be used as a hash function to partition

R,' and R,. First,
is divided into L partitions,
P,, . . . ,P,, according t o the order of T $ ? . A . Then,
corresponding to each partition P . , we can define
Ri;={t Itai,
t.A (or t.B) E rA(fij)}.
T h e partition
should be done in such a way that, for each lsjg,either
M(R,;)<Mav-l or M(R, . ' ) a a u - l .Then, nested loop
join can be applied t o join
and R , j ' : Note that the
!I
partition of Ri' can be done in parallel with the selection
process of R:.

k

(2)

Sort-Merge join: T h e method of selecting and sorting tuples
in R: described in situation 4 of Case 2 is applied here to
establish sorted buckets of both joining relations. T h e n the
sorted buckets are merged.

3.4. Discussions
As discussed above, the performance improvement of the
adaptive join plan comes from the reduction on the number of
tuples participating in join operation and the optimal selections of

254

Figure 6 . 1/0 cost of sort-merge join a n d adaptive join
Figure 5 . Join methods chosen undcr adaptive join plan
4. Conclusion

e,

access path a d join method. It is possible that the cost of conis close to that of joining R , and R , directly.
structing
This happens when Ri has only few attributes and shows almost
no duplicate value on the join attribute. A t compile time, query
optimizer should check for the above mentioned conditions and
choose an access plan to join the relations directly using one of
the join methods: nested loop, sort merge or hash join, if the conditions occur.

An adaptive approach to select access path and join method
is examined in this paper. By taking advantage of existing information in the index, one can obtain the actual number of tuples
to be retrieved or to be joined. Then, the optimal step can be
chosen without using selectivity. T h e information collection p r e
cess can be made part of the query evaluation procedure. For
instance, the computation of the number of tuples satisfying a
selection predicate, np(P), is combined with an ordinary index
4, and 4, are constructed during
scan. In a join operation,
an index join and can be used to select the optimal step later on.
T h e extra overhead involved tends to be minimum compared
with the penalty on selecting the non-optimal access path which
could be drastic for large relations.
There is a fundamental issue about when to perform the
query optimization, either in compilation time or in run time.
T h e query optimization algorithms performed in compilation time
rely upon good cost estimations. On the other hand, the run time
approach can utilize certain information available during query
execution and avoid the estimation error. It may incur enormous
overhead due to the invocation of query optimization procedure
dynamically. As suggested in the adaptive selection of access
path and join method, this approach can become very effective by
providing simple decision criteria and combining information
acquisition with query evaluation. Further studies should focused
on the adaptive selection of join order for n-way join and distributed query.

e,,

Adaptive join plan can also be extended to optimize a sortmerge join method provided that an index exists in the join attribute. Consider a sort-merge join of R,[A=B]R, where R , has
an index on attribute R,.A. Assume that the index of R , can be
fitted into the memory and there are M, pages remaining as
A .
working area. T h e adaptive join plan starts with loading S , into
memory. An additional count attribute, denoted as K,, is added
to St and values of attribute K 2 are initialized to zero for all
tuples. Next stage of the plan is to sort R , and to construct
simultaneously. For each tuple, t , of R , which is retrieved (wlth
table scan), search for t.B in $.A is performed. If there exists
an sESt such that s.A=t.B, s.K, is incremented by 1 and t will
be attached to the linked list associated with S. Otherwise, t is
discarded. When the working area is full, all linked lists are conA
catenated in the order of attribute S , .A and written into disk as
a sorted bucket. T h e linked lists in memory are reset to null for
sorting the remaining tuples of R,.
After scanning through R,, not only the sorted R,' is
which is formed by eliminating those
obtained, but also
tuples of S;" with a zero K , count. 4, can also be calculated and
used to select an access path for retrieving R,' (i.e. index or table
scan). Subsequent stages of computing join results are analogous
to what was described in Section 3.2 and are omitted here.

Acknowledgement
We would like to thank Seetha Lakshmi for discussions on the
adaptive access plan for join, and Guy Lohman for his valuable
comments.

255

I-

-

References
[Blas77]

Blasgen, M. W. and Eswaran, K. P., "Storage and
Access in Relational D a t a Bases," LBM System Journal, Vol. 16, No. 4, 1977, pp. 363-377.

[Card751 Cardenas, A.F., "Analysis and Performance of Inverted
Database Structures," Comm. ACM, Vol. 18, No. 5,
May 1975, pp.253-263.
[DeWi84] Dewitt, D. J., Katz, R. H., and Olken, F., "Implementation Techniques for Main Memory Database S y e
tems," Proceeding of the 1984 SIGMOD Conference
on the Management ofData, June 1984, pp. 1-8.
[EpstSO] Epstein, R . and Stonebraker, M., "Analysis of Distributed D a t a Base Processing Strategies," Proc. of the
6th Znt'l. Conf. on Very Large Data Bases, Aug. 1980,
pp. 92-101.
(Haer781 Haerder, T., "Implementing a Generalized Access Path
Structure for a Relational Database System," A C M
Trans. on Database Systems, Vol. 3, No. 3, Sep. 1978,
pp. 285-298.
[Hagm861 Hagmann, R . B., "An Observation on Database
Buffering Performance Metrics," Proc. of the 12th Znt'l.
Conf. on Very Large Data Bases, Aug. 1986, pp. 289293.
[Laks87] Lakshmi, M. S. and Yu, P . S., "Impact of Skew on
Access Path Selection in Relation Database Systems,"
to appear in Computer Science and Engineering.
[Mack851 Mackert, L.F. and Lohman, G.M., "Index Scans Using
a Finite LRU Buffer: A Validated 1/0 Model," ZBM
Research Report, RJ-4836, 1985.
[Piat841

Piatetsky-Shapiro, G. and Connel, C., "Accurate Estimation of the Number of Tuples Satisfying a Condition," Proceeding of the 1984 SIGMOD Conference on
the Management of Data, June 1984, pp. 256-276.

[Seli79]

Selinger, P . G., el. al., "Access Path Selection in a
Relational Database Management System," ZBM
Research Report, RJ-2429, 1979.

[Vald87] Valduriez, P., "Join Indices," ACM Trans. on Database
Systems, Vol. 12, No. 2, June 1987, pp. 218246.
[Whan83] Whang, K.-Y., Wiederhold, G., and Sagalowicz, D.,
*'Estimating Block Accesses in Database Organizations:
A Closed Noniterative Formula," Comm. of ACM,
Vol. 26, No. 11, Nov. 1983, pp. 94@944.
[ m a n 8 5 1 Whang, K.-Y., 'Transaction-Processing Costs in Relational Database Systems" ZBM Research Report, RC10951, 1985.
[Yao77]

[yu83]

Yao, S. B., "Approximating Block Accesses in Database
Organization," Comm. of AGM, Vol. 20, No. 4, 1977,
pp.260261.
Yu, C.T. and Chang, C.C., "On the Design of a Query
Processing Strategy in a Distributed Database
Environment," Proceeding of the 1989 SIGMOD
Conference on the Management of Data, May 1983,
pp. 3@39.

256

An Efficient Switch Design for Scheduling Real-Time
Multicast Traffic*
Deming Liu and Yann-Hang Lee
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287
{dmliu, yhlee}@asu.edu

Abstract. In this paper we put forth a switch design in terms of architecture and
service discipline for real-time multicast traffic in packet switching networks. A
parallel switching architecture called POQ (parallel output-queued) is employed, which take the advantages of both OQ (output-queued) and IQ (inputqueued) switch architectures, i.e., non-blocking and low speedup of switch
buffer. Basing on the POQ architecture we propose a hierarchical service discipline called H-EDF-RR (hierarchical earliest-deadline-first round-robin), which
intends to simultaneously schedule both unicast and multicast traffic composed
of fixed-length cells with guaranteed performances. Analyses show that this design can provide tight delay bounds and buffer requirements, and has computational complexity of O(1). These properties make the proposed switch design
well suitable in real-time distributed systems.
Keywords: Packet Switching Network, Quality of Service, Real-Time Communications, Multicasting, Earliest Deadline First Round Robin

1 Introduction
Along with the tremendous development in computer and communication network,
the wide use of optical fiber, packet switching and etc. enables many new distributed
applications such as digital audio, digital video and teleconference. These applications
are often characterized by quality of service (QoS) in terms of bandwidth, delay, jitter
and loss rate. Similarly in many industrial automation and transportation systems,
networking presents the opportunity for system optimization as subsystems can be
integrated and operated cooperatively.
One example is the aircraft databus, which is aimed to support various traffic types
coming from cabin entertainment systems, passage intranet, and avionics instruments.
Under many application scenarios in aircraft communication networks, real-time data
acquisition systems need to send acquired data to multiple destinations with stringent
delay requirements. Usually we can use some traffic models to represent this kind of
multicast traffic. The delay requirement of the multicast traffic can also be stated as
deadlines. It is more important that we build deterministic communication networks,
*

This work was sponsored in part by the Federal Aviation Administration (FAA) via grant
DTFA03-01-C-00042. Findings contained herein are not necessarily those of the FAA.

J. Chen and S. Hong (Eds.): RTCSA 2003, LNCS 2968, pp. 194–207, 2004.
© Springer-Verlag Berlin Heidelberg 2004

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

195

which can efficiently transport both unicast and multicast traffic subject to deadline
constraints. In packet switching networks, switches are developed intending to provide statistical multiplexing and QoS-guaranteed transmission services. Unicasting,
also known as point-to-point, is common in most QoS-guaranteed applications. However many applications such as video-on-demand, distance learning, and data acquisition in avionics systems produce multicast traffic, requiring that the same piece of
data (a packet or a cell) from a source is transmitted to multiple destinations. For
transferring multicast traffic efficiently in switching networks, there must be a thorough consideration in terms of architecture and scheduling in switch design.
Multicasting in a packet switch means that a packet arriving at an input port is forwarded to more than one output ports in the switch. Even though the effect of multicasting can be achieved by transferring the same packet from the source to multiple
destinations in multiple times as unicast does, special switches supporting multicast
traffic are preferred because doing multicasting with point-to-point communication
may result in significant load increase to the network. Presented in [1], a survey of
multicast switches indicates that multicast switches should include a packetreplicating function in order to efficiently convey multicast traffic. Among different
multicast switch fabrics, crossbar network is attractive since it is based on simple
mesh network and thus has no internal blocking inherently. According to different
buffer positions, there are two types of crossbar networks, i.e., OQ (output-queued)
and IQ (input-queued).
In an OQ switch, all packets that arrive from different input ports and are destined
to the same output port are buffered into a queue located at the output port. The service scheduler repeatedly selects a packet from the output queue for transmission. Because of absence of input contention points OQ switches are non-blocking inherently.
As far as QoS is concerned, there are numerous service disciplines that support guaranteed performances with OQ switches [7]. Since all packets are buffered in their own
destination queues as they arrive, the copies of a multicast packet can be delivered to
their destination queues as well. However OQ switches are subject to a fatal drawback
that the speedup factor, defined as the ratio of buffer memory rate to line rate, is as
high as N for an N×N OQ switch since the number of packets that want to enter a
given output buffer in a packet slot can be as large as the number of input ports. The
demand of high buffer rate constrains OQ switches in broadband networks. To avoid
this limitation, designers proposed to limit the number of packets that can be transferred into an output buffer in one packet slot. Nevertheless packet drop is inevitable
in this case, which is not allowed in most real-time applications.
In an IQ switch, packets arriving on each input port are placed into smoothing
buffers, prior to the placement in the destination output ports. During each scheduling
slot, the head packets in all the buffers are candidates for being transferred to their
output ports. If several head packets contend for the same output port, only one of
them is selected according to contention resolution scheme, while the rest remain in
the buffers and contend again in the next packet slot. In contrast with OQ switches
that require high switch fabric speed, the switch fabric speed of IQ switches is the
same as that of input or output lines. The ease of speedup factor leads to a wide use of
IQ switches.
Unfortunately IQ switches suffer from a phenomenon known as head of line (HOL)
blocking. The effect occurs when a packet in any given buffer is denied to access to
its output port, even though there are no other packets requiring the same output port,
simply because the packet in the head of that buffer was blocked in a contention for a

196

D. Liu and Y.-H. Lee

totally different output port. In fact, the delay for a given packet may grow unbounded
even for an offered load less than 100%. Therefore it is very difficult, if not impossible, to guarantee the required QoS for each individual traffic flow. Hence most scheduling disciplines in IQ switches are best-effort instead of hard real-time [11] [12] [13].
The non-deterministic delay caused by HOL blocking can be resolved by a VOQ
(virtual output-queued) structure [4], in which there are N buffers for each input port,
one for each output port in an N×N switch. However we cannot avoid the matching
problem involving high computational complexity in order to find the maximal flow
between input and output ports during each scheduling slot. Also IQ switches have to
face a difficult issue in supporting packets intended for more than one output ports,
i.e., multicasting. If a head packet is of this kind, it has to contend simultaneously for
all the outputs it is intended for. HOL blocking can be aggravated if the contention
resolution schemes are applied independently during each scheduling slot.
Whereas IQ switches require a lower fabric speedup, OQ switches provide higher
throughput. To take both advantages of the two architectures, a new switch structure,
combined input and output queuing (CIOQ) switch, was proposed such that a compromise is made between these two aspects. In the CIOQ structure, there exist buffers
in both input and output sides. Researchers have proved that CIOQ switches can
achieve 100% throughput for unicast traffic with a speedup factor of 2 [5]. Contrary
to the case of unicast traffic, for which IQ switches can yield the same throughput as
OQ switches, it has been shown in experiments and analytical modeling that a
throughput limitation exists in IQ switches (including CIOQ switches since CIOQ
switches have IQ architecture essentially) loaded with multicast traffic [4].
As for scheduling disciplines of multicast switches, there are two basic strategies,
non-fanout splitting and fanout splitting [4]. The fanout is defined as the number of
different destinations that a multicast packet has. During each scheduling slot, the decision about which backlogged packets can be transferred is made according to a
scheduling discipline. The fact that multicast packets have multiple destinations implies that some scheduling disciplines, called non-fanout splitting, may elect to transfer in just one scheduling slot the multicast packet to all destinations, while others,
called fanout splitting, may elect to transfer the packet in several scheduling slots,
reaching non-overlapping and exhaustive subsets of destinations.
In discussion of scheduling disciplines, work-conserving policies are significant in
the sense that they transmit as many packets as possible in each scheduling slot [6].
Obviously when scheduling multicast traffic, non-fanout splitting is non-work conversing policy, while fanout splitting may be work conserving. With the assumption
that the scheduler has no knowledge of the multicast copies of HOL packets, it has
been shown that work-conserving policy provides more throughput than non-work
conserving policy [6]. Thus, in terms of throughput, a fanout splitting discipline could
be better off than a non-fanout splitting discipline. On the other hand, it might introduce a side effect of variant jitters as multiple copies are scheduled for transmission at
different slots. We have known that in addition to imitate a unicast OQ switch with a
speedup factor of 2, a CIOQ can attain an equivalent performance as an OQ switch
for multicast traffic by making copies of each multicast packet in each input buffer
with a speedup factor of F+1 where F is the maximum fanout [5]. We should note
that there is a constraint that the copies of a multicast packet cannot be transferred to
output ports simultaneously. To get extra performance, an intrinsic multicast CIOQ
switch is of our interest, which can transfer copies of a multicast packet simultaneously. The intrinsic performance loss of IQ architecture with respect to OQ architec-

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

197

ture loading with multicast traffic is shown in [4]. The speedup requirement of IQ
switch that offers 100% throughput for multicast traffic depends upon the cardinality
of input or output ports. There is no result about the exact relationship of the two parameters.
QoS-based scheduling for multicast traffic has been investigated recently. Results
in [2] show that HOL FCFS (first come first served) discipline has a performance superior to that of the non-FCFS disciplines and assigning priority according to packet
age in queue is a worthwhile feature for multicast packet switches. In fact, the core of
a multicast traffic scheduler is basically a contention resolution algorithm. Chen and
Hayes [3] suggested a priority-based scheme called cyclic priority scheme to schedule
multicast traffic from the point of view of electronic circuit implementation, using the
revision scheduling, a sequential combination of a non-fanout splitting discipline and
a fanout splitting discipline. The revision scheduling performs well in the terms of
delay-throughput performance. In general, most research results on switching of multicast traffic are based on the perspective of statistical analysis rather than determinism investigation [9].
The complication of multicast scheduling may come from the traffic imbalance
between input and output sides of a switch. Since a multicast packet coming from an
input port is destined to multiple output ports, the traffic injected to the output ports
from multicast traffic could be much larger than that from unicast traffic. Moreover,
given that multiple copies are created at same time, the traffic pattern is quite bursty.
The most multicast disciplines we introduced above cannot be used in hard real-time
communications in that they either assume a statistical model or allow packets to be
dropped.
Integrating unicasting and multicasting scheduling with QoS guarantees is a challenge for IQ switches. However, recognizing that a multicast packet can be considered as multiple unicast packets in parallel, we can employ switches with parallel
structure to achieve the advantages of both OQ and IQ switches, i.e., no-blocking and
low speedup factor. In the rest of this paper we will introduce a parallel switching architecture equipped with a hierarchical service discipline that can transfer both unicast
and multicast traffic with guaranteed performances. Detailed analyses of delay bounds
and buffer requirements suggest that the proposed approach is appropriate for distributed real-time systems loading with multicast traffic.
The rest of this paper is organized as follows. In Section 2 we describe the proposed switching architecture, called POQ (parallel output-queued), and how it supports multicast traffic. Section 3 introduces the H-EDF-RR (hierarchical earliestdeadline round-robin) scheduling discipline designed for this parallel architecture.
Section 4 presents the delay bound and buffer requirement analyses for H-EDF-RR
discipline under POQ architecture. Finally the conclusions are given in Section 5.

2 A Parallel Switch Architecture – POQ (Parallel Output-Queued)
Subject to HOL blocking, a pure IQ switch has a limited throughput of 58.6% with
FIFO input buffers in the worst case [15]. To avoid HOL blocking, the VOQ switch
architecture can be constructed as shown in Fig. 1 where separate queues for all output ports are added at each input port. Thus a buffered packet cannot be blocked by
the packets destined to different output ports. If the fabric speedup factor of VOQ

198

D. Liu and Y.-H. Lee

switches is greater than 1, buffers are required on the output side. Although the VOQ
architecture removes HOL blocking, they still suffer from the problem of input and
output matching because VOQ switches only permit one head packet of all queues in
each input port to be transmitted during each scheduling slot. To increase output
throughput we have to find an optimal match, e.g. maximum, maximal or stable
matching [16]. Almost any optimal matching can involve high computational complexity that is not acceptable in implementing high-speed switching networks. In fact,
for multicast traffic, simulation results and analytical modeling in [4] suggest that IQ
switches cannot yield the same throughput as OQ switches. In other words, 100%
throughput may be attained for any multicast traffic pattern in IQ switches, however,
in the cost of too high speedup factor preventing from physical implementation for
high-speed networks. The computational complexity of matching algorithm and the
high-speedup requirement restrained VOQ switches from applications in transferring
hard real-time multicast traffic.
Due to the difficulties of VOQ switches in supporting real-time traffic, especially
QoS guarantees for multicast traffic, in this paper we bring in a parallel output-queued
(POQ) switch architecture shown in Fig. 2. The architecture of POQ can be regarded
as a derivative of VOQ. The most obvious difference between VOQ and POQ is that
multiple head packets in the queues of an input port in POQ can be transmitted to
their destinations in the same scheduling slot whereas only one can be done under
VOQ. This modification in structure results in a substantial performance improvement. In addition, output buffers are not necessary in POQ architecture. A similar
POQ architecture is discussed in [8] where the authors numerated more drawbacks
than advantages. We will probe the merits of POQ switches under real-time multicast
traffic. Basing on the architecture of POQ switches, we can easily observe its attractive characteristics as follows.
• Buffer speed is only required to be the same as the line speed for both reading and writing operations. In other words, a speedup factor of 1 is enough
In p u t B u ffe r s
O u tp u t B u ffe rs

O ut 1

In 1

… …
D e m u ltip lx e r

In 2

Out 2

… …

.
.
.
In N

N × N
C rossb ar
.
.
.

.
.
.

O ut N

……

Fig. 1. VOQ switch architecture

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

199

for POQ switches for any traffic patterns. When a unicast packet arrives it is
routed to a buffer according to its destination. Similarly when a multicast
packet arrives all its copies are demultiplexed to the corresponding buffers in
a parallel way. During each scheduling slot any output port can take one
packet as long as there are some buffered packets destined to it.
• The throughput of a POQ switch can reach 100%. An N×N POQ switch can
be thought as N N×1 OQ switches that work in parallel, one for each output
port. In contrast to VOQ switches, there is no need to find any optimal
matching.
• Since a POQ switch is essentially OQ switch, all service disciplines developed so far for OQ switches can be applied to POQ switches. There are a
number of service disciplines for OQ switches that support performanceguaranteed services [7]. Instead of using a centralized scheduler, a distributed approach can be adopted such that a scheduler is located at each output
port.
• It is possible to integrate the scheduling of unicast and multicast traffic with
guaranteed performance in a POQ switch. This originates from the fact that
POQ switches, belonging to OQ switches in essence, have the ability of
transmitting multicast traffic inherently. Unicast traffic can just be thought as
a special case of multicast traffic that the fanout of any packet is one.
For expressly describing multicasting service discipline, we shall clarify several
terms used in the following text. A session is a connection in a packet switching network from one source node to one destination node. For unicast traffic, a unicast session can be establish with QoS parameters and will be used to transmit packets between applications at the source and destination nodes. On the contrary, a multicast
session, consisting of multiple overlapping sessions with a unique source and multiple
destinations, is used for multicast traffic. The path of a session is defined as the consecutive switch sessions along the session and each switch session is a pair of input
and output ports of a switch. Under the POQ architecture, a switch session is uniquely
identified by a packet buffer connecting the same pair of input and output ports. When
B u ffe r s

O ut 1

In 1
D e m u lt ip le x e r

In 2

O ut 2

.
.
.

.
.
.

.
.
.
O ut N

In N

Fig. 2. A parallel switch structure - POQ

200

D. Liu and Y.-H. Lee

a packet of a unicast session arrives at an input port, it will be queued in the buffer on
its session path. On the other hand, for an arriving packet of a multicast session, multiple copies will be inserted to the corresponding multiple buffers. As each buffer is
shared by multiple sessions, we must take into account the issues of fairness and
bandwidth reservation for each switch session. In the next section, we will apply a hierarchical service discipline to POQ architecture such that the performances of both
unicast and multicast sessions are guaranteed.

3 Hierarchical Earliest Deadline First Round Robin Scheduling
In a network constructed with POQ switches, we can think a session as a connection
that traverses a sequence of switch buffers from a source node to a destination node.
The buffers are allocated in the switches along the session path and may be shared by
multiple sessions. If we are not concerned about how packets of multiple sessions are
multiplexed inside a buffer, N distributed schedulers, one at each output port, can be
deployed to select a buffer from which the head packet is transmitted to the output
port. We call a scheduler in this level output scheduler. On the other hand, arriving
packets of multiple sessions may join a buffer according to a FIFO order or an order
based on deadline and QoS requirements. We need a scheduler at this level called input scheduler. Thus, an input scheduler institutes a service discipline among the sessions sharing a switch session and an output scheduler determines the order in which
the switch sessions traversing the same output port are served. Apparently, both
schedulers must address the issues of fairness and QoS requirements. As we reveal
the necessity of two levels of schedulers in routing packets in POQ architecture for
both unicast and multicast traffic, we introduce an efficient performance-guaranteed
discipline, H-EDF-RR (hierarchical earliest-deadline-first round-robin), in which
EDF-RR (earliest deadline first round robin) schedulers are used in both the two levels.
EDF-RR proposed in [10] is an O(1) algorithm based on fixed-length cells for OQ
switches. As shown in Fig. 3, it is a frame-oriented round-robin algorithm in nature. A
frame is composed of a number of cells. A session reserves a portion of bandwidth by
holding some cell slots in a frame. Instead of arranging the cells reserved by all active
sessions in any arbitrary or dynamic order, EDF-RR tries to transfer them in an order
such that cells attached to an active session are distributed in a frame as uniformly as
possible. In other words, EDR-RR does its best to mimic ideal GPS (generalized
processor sharing) scheduler with the constraint of non-preempted fixed-length traffic
unit, cells.
To describe EDF-RR, we define that a frame consists of n fixed-length cells. A cell
has, for convenience, the length of 1 in terms of the time that it takes to transmit a cell
from a switch’s buffer to the corresponding output port. Alternatively we just normalize the length of a cell slot to 1. Let K be the total number of active sessions associated with an output port and mi (1 ≤ i ≤ K) be the number of cell slots occupied by
the session i in a frame. n / mi is defined as session i period. The non-preemptive nonidling EDF (earliest-deadline-first) algorithm is used to schedule the order of transmitting cells in a frame. If a session is idle, it will be skipped during the transmission

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

201

and the cell slots it occupies can be reclaimed for backlogged sessions. The EDF-RR
discipline is given as follows.
EDF-RR Discipline
(a) An n-cell frame is partitioned among K active sessions (all unused band-

E D F -R R
S ch ed u le r

S essio n 1
S essio n 2

O u tp u t P o rt

… …
S essio n K

Fig. 3. An EDF-RR scheduler for multiple sessions

width can be considered as one idle active session) such that session i (1 ≤ i
≤ K) transfers mi cells in the frame. Session i is assumed to have cells arrived
at time jpi (suppose a frame starts from time 0) with corresponding deadlines
at (j+1)pi, where p i = n / m i and j = 0, 1, 2, … mi-1.
(b) If all K sessions are backlogged, the frame described in (a) is transferred repeatedly such that in every frame the cells are transmitted in a nonpreemptive non-idling EDF order. Determining transmission order is needed
only when there are sessions established, cancelled or updated, which happen
infrequently. The associated overhead can be ignored since a new transmission order can be computed in parallel to the current transmission, and is
swapped at the next frame boundary.
(c) If there is not backlog any more for a session during the current frame, their
cell slots in a frame are skipped. The remaining backlogged sessions are
transferred in the same order as in (b). In this case the size of a frame is reduced.
Table 1 shows the scheduling order of an example frame with the size of 10, in
which sessions 1, 2 and 3 shares 5, 3 and 2 cell slots respectively (the numbers in Table 1. denote the relevant sessions).
Table 1. The example of scheduling order in a frame

1

2

1

3

1

2

1

2

1

3

According to [10] we have the following two theorems for EDF-RR discipline on
delay bound and buffer requirement.
Theorem 1. If session i traffic flow that consists of a sequence of cells is constrained
by traffic model σ i + mi t (in cells), the delay a cell experiences passing through an
n
EDF-RR scheduler is not more than (σ i + 2) n cell slots.
mi

202

D. Liu and Y.-H. Lee

Theorem 2. If session i traffic flow that consists of sequence of cells and is constrained by traffic model σ i + mi t (in cells) passes through an EDF-RR scheduler
n
without buffer overflow, the buffer size the scheduler needs is no more than σ i + 2
cells.
Theorem 1 gives the delay bound of a cell with EDF-RR. For characterizing delay
bounds of H-EDF-RR scheduler, we need the following lemma.
Lemma 1. For any P backlogged cells of session i scheduled by an EDF-RR schedth
uler, the time interval from the moment of transferring the first cell to that of the P
cell is at most n P cell slots.
mi
This property is trivially true by considering that there is one cell transferred every
n cell slot in any busy interval after the first cell is scheduled [10].
mi

H-EDF-RR service discipline is divided into two levels each of which is an EDFRR discipline. In the high level, an output scheduler is located at every output port
guaranteeing fairness among the switch sessions to the port. In the low level, an input
scheduler is located at each buffer to guarantee the fairness among sessions that share
the same switch session. In other words, an input scheduler decides which cell in the
current queue is available for the scheduling of the corresponding output scheduler.
H-EDF-RR Discipline
(A) Output Scheduling
An n-cell high-level frame for an output port is partitioned among K active switch sessions destined to the output port such that mi cell slots in a
frame can be used to transmit the cells from switch session i (1 ≤ i ≤ K).
EDF-RR is applied to schedule these K switch sessions.
(B) Input Scheduling
A ni-cell low-level frame for switch session i is partitioned among Ki active sessions associated with switch session i such that mih cell slots in the
frame can be used to transmit the cells from session h (1 ≤ h ≤ Ki). EDF-RR
is applied to schedule these Ki sessions.
Since the first step of EDF-RR for both output scheduling and input scheduling
needs to do only if bandwidth sharings of uincast or multicast sessions are updated,
which we suppose to be infrequent events, H-EDF-RR has the computational complexity of O(1) resulting from the fact that it is frame-oriented round-robin. Because
the idle cell slots in any high-level or low-level frame are skipped, H-EDF-RR is a
work-conserving policy.

4 Analysis of Delay Bounds and Buffer Requirements
For analyzing delay bounds and buffer requirements of the H-EDF-RR discipline under a POQ switch, we assume (σ, ρ) traffic model for all active sessions. Denote a
th
session Sih the h session in switch session i. Then for Sih that is constrained by (σih,

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

203

ρih), there are at most σih+ρiht units of traffic during any time interval t. At an output
port of a POQ switch armed with a H-EDF-RR scheduler, the output scheduler is in
charge of K active switch sessions. A n-cell high-level frame is partitioned among the
K switch sessions such that mi cell slots are allocated to switch session i. Similarly an
ni-cell low-level frame is partitioned among Ki sessions that share switch session i
such that mih cell slots are allocated to session Sih. Therefore we have Theorem 3 giving the delay bound for a POQ switch with a H-EDF-RR scheduler.
Theorem 3. If the traffic flow of Sih is constrained by traffic model σ + mih mi t , the
ih
ni n
delay that a cell of Sih experiences in a POQ switch with a H-EDF-RR scheduler is not
more than [(σ + 2) ni + 2] n .
ih
mih
mi
Proof. A H-EDF-RR scheduler can be considered as two EDF-RR schedulers in serial. Thus the delay that a cell in Sih experiences is composed of three parts. One is the
delay that comes from the source flow’s burstiness. The other two result from input
and output EDF-RR schedulers respectively.
The burstiness delay can be bounded by the expression as follows, which can be
regarded as the delay of a cell in Sih when passing through a GPS scheduler with rate
m ih m i reserved for S .
ih
ni n
σ

ih

ni n
m ih m i

(1)

The input scheduler delay can be understood as the delay experienced by a cell of a
uniform traffic flow of rate mih mi passing through the input scheduler, an EDF-RR
ni n

scheduler of output rate mi . By Theorem 1 (note that in Theorem 1 we assume the
n

output rate of a EDF-RR scheduler is 1 cell per cell slot), this part of delay is bounded
by 2ni / mih time units, where one time unit is n / mi cell slots. Thus the input scheduler delay is bounded by
2

ni n
mih mi

(2)

Similarly the output scheduler delay can be understood as the delay experienced by
a cell of a uniform traffic flow of rate mi passing through the output scheduler with
n
output rate of 1 cell per cell slot. By Theorem 1, the output scheduler delay is
bounded by
2

n
mi

(3)

Making a summation of the three parts of delay bounds gives the total delay bound.

204

D. Liu and Y.-H. Lee

[(σ ih + 2)

(4)

ni
n
+ 2]
mih
mi


The portion of the delay of a session Sih cell as shown in (2) results from the blocking of cells of other sessions in switch session i. The portion of delay shown in (3)
comes from the blocking of cells of other switch sessions traversing the same output
port. Since (3) is relatively the small term of (4), it may be ignored in some applications. In the proof of Theorem 3, we just individually get the worst-case delays of uniform traffic for both input and output schedulers. In fact the two worst cases cannot
happen simultaneously and thus the delay bound in Theorem 3 can be as tight as
(σ ih + 2)

(5)

ni n
mih mi

Instead of using formal proof, we give an explanation of (5) below. For simplifying
the explanation, we assume that Sih has uniform traffic model of rate mih mi originally,
ni n
accordingly ignoring its burstiness as we considered the delays of input and output
schedulers in the proof of Theorem 3. A cell c of Sih may experience the worst-case
total delay caused by the H-EDF-RR scheduler when it is the first transferred cell of a
busy interval of Sih. We consider the following three cases. (i) There are no other cells
except c backlogged in switch session i buffer from c’s arrival to departure. But in
this case, we can simplify the total delay of c caused by the H-EDF-RR scheduler
to 2n ( ≤ 2 ni n ) by Theorem 1. (ii) In addition to c, there are some cells backlogged
mi
mih mi
in switch session i buffer from c’s arrival to departure whereas none of these cells was
being transferred when c arrived. Then according to EDF-RR service discipline the
worst-case delay of c caused by the H-EDF-RR scheduler cannot exceed
n
n n , where 2n and ni n result from output and input schedulers respec2 + i
mi mih mi
mih mi
mi
tively. If ni < 2 , c will get chance to transfer in the next cell slot available for
mih
switch session i according to EDF-RR scheduling, and thus the delay of c caused by
the H-EDF-RR scheduler will be at most 2n ( ≤ 2 ni n ). If ni ≥ 2 instead, the
mih
mi
mih mi
delay will be at most 2 n + ni n (≤ 2 ni n ). (iii) In addition to c, there are some
mi

mih mi

mih mi

cells backlogged in switch session i buffer from c’s arrival to departure and one of
these cells was being transferred when c arrived. Then there are at most 2ni cells that
mih

need to be transferred before c since the input EDF-RR scheduler can have at most
2ni cells from other sessions that block c’s transfer. According to Lemma 1, the
mih

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

205

2ni cells can be scheduled by the output scheduler in 2 ni n cell slots because the
mih mi
mih

output rate of switch session i buffer is mi cells per cell slot according to bandwidth
n
reservation scheme of the output EDF-RR scheduler. In other words, the delay caused
by the H-EDF-RR scheduler to c is not more than 2 n i n in this case. Following
m ih m i

the above analysis and also considering the delay from the original burstiness of Sih,
we have (5). In the particular scenario that ni is equal to mi, we may think an EDF-RR
scheduler at the output port that switch session i passes through schedules all sessions
passing this output port such that session Sih shares mih cell slots in a n-cell frame.
Thus (5) is simplified to (σ ih + 2) n , which is consistent with Theorem 1.
mih
The delay bound can be easily extended to the multiple-node case. We suppose that
session Sih traffic flow traversing k nodes is constrained by σ + ρt , where ρ is the
minimum bandwidth reservation for Sih on all the k nodes. The upper delay bound of a
cell in Sih as the cell passes through the k nodes is given by σ + 2k .
ρ

In order to use memory efficiently for POQ switches, we assume that a buffer may
be shared by multiple sessions. Therefore cells from any session can be buffered to
the corresponding switch session buffer as long as the buffer is not full. The detailed
buffer sharing mechanism [14] is beyond the scope of this paper. Basing on this assumption, we have Theorem 4 giving the buffer requirement for a POQ switch armed
with a H-EDF-RR scheduler.
Theorem 4. If the traffic flow of session Sih (h = 1, 2 … Ki, where Ki is the number of
active sessions in switch session i) in switch session i, which consists of a sequence of
cells and is constrained by traffic model σ + mih mi t , passes through a H-EDF-RR
ih
ni n
scheduler without buffer overflow, the buffer size that switch session i requires is not
more than

Ki

∑σ
h =1

ih

+ 2 cells.

Proof. Denote Rih(t1, t2) the traffic coming in [t1, t2] for Sih and Ri(t1, t2) the amount of
traffic arriving in [t1, t2] for switch session i. Then we have
m mi
R ( t , t ) ≤ σ + ih
( t − t ) , h = 1, 2, …, Ki
ih

1

ih

2

ni

n

2

1

ih

+ (∑

Therefore,
R i (t1 , t 2 ) =

Ki

∑R
h =1

ih

(t1 , t 2 ) ≤

Ki

∑σ
h =1

Ki

h =1

According to bandwidth allocation for switch session i,
R i (t1 , t 2 ) ≤

Ki

∑σ
h =1

ih

+

m ih m i
)
(t 2 − t1 )
ni
n
Ki

mih
≤ 1 . Hence
h =1 ni

∑

mi
( t 2 − t1 )
n

206

D. Liu and Y.-H. Lee

This means that switch session i traffic satisfies model (

Ki

∑σ
h =1

ih

, m i ). Since the
n

switch session i flow passes through the output EDF-RR scheduler, by Theorem 2 we
have the buffer requirement as follows.
K

∑σ
i

h =1

ih

+ 2


The H-EDF-RR discipline can guarantee performance of sessions. However it re2
quires N input schedulers and N output schedulers for an N×N POQ switch. This cost
may not be acceptable in terms of electronic implementation. For simplifying the
scheduling, we can remove input schedulers and leave output schedulers only. Theorem 5 shows that even not as good as original H-EDF-RR, this simplification still can
guarantee multicasting delay bound as long as multicast traffic rate is constrained.
Theorem 5. If the traffic flow of session Sih (i =1, 2, … K, h = 1, 2 … Ki, where K is
the number of active switch sessions of an output port and Ki the number of active
sessions in switch session i) in switch session i, which consists of a sequence of cells
K
and is constrained by traffic model σ ih + mi ρ iht and the condition
ρ ih ≤ 1 ,
∑
n
h =1
passes through only an output EDF-RR scheduler, the delay a cell in Sih experiences is
K
not more than ( σ + 2) n .
i

∑
i

h =1

ih

mi

Proof. Refer to the proof of Theorem 4, we know that switch session i traffic satisfies
k
model ( σ , mi ). Since switch session i flow passes through only the output EDF∑
ih
n
h =1
RR scheduler, by Theorem 1 we have the upper delay bound as follows.
i

Ki

(∑ σ ih + 2)
h =1

n
mi


In the analysis above, we do not assume any difference between unicasting and multicasting in that unicasting is looked as the special case of multicasting. Normally
switching networks offer connection-oriented services for real-time traffic. In a networks composed of POQ switches and served by H-EDF-RR disciplines, the process of
setting up a multicast session involves two levels of bandwidth reservation along multiple paths since a multicast session is established from one node to multiple nodes. This
increases the time of establishing multicast sessions. We need to design efficient connection-establishing algorithms to fully employ the advantages of POQ and H-EDF-RR.
Also we have to face some application-dependent problems, for instances, how to determine and optimize frame size for both input and output schedulers and how to determine cell length. These subjects are beyond the discussion of this paper.

5 Conclusions
In this paper, we propose a solution to integrate unicast and multicast traffic scheduling in packet switching networks with guaranteed performances. A parallel switching
architecture, POQ, is introduced that takes the advantages of both OQ and IQ switch-

An Efficient Switch Design for Scheduling Real-Time Multicast Traffic

207

ing architectures, i.e., non-blocking and the low rate of switch buffer up to line speed.
Therefore POQ architecture is endowed the attractive capability of supporting multicast traffic. For efficiently scheduling multicast traffic for POQ architecture, a hierarchical service discipline working on fixed-length cells, H-EDF-RR, is employed
based on EDF-RR discipline that serves OQ switches. Guaranteed performances for a
POQ switch armed with H-EDF-RR disciplines is analyzed in terms of delay bounds
and buffer requirements while loading with multicast traffic. Analytical results show
that guaranteeing performance of multicast traffic is possible in this solution in terms
of both architecture and service discipline.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]

Ming-Huang Guo and Ruay-Shiung Chang, “Multicast ATM Switches: Survey and Performance Evaluation,” SIGCOMM Computer Communication Review, Vol. 28, No. 2,
April 1998.
Joseph Y. Hui and Thomas Renner, “Queueing strategies for multicast packet switching,”
in Proc. IEEE Globecom, San Diego, CA, 1990, pp. 1431-1437.
Xing Chen and Jeremiah F. Hayes, “Access control in multicast packet switching,”
IEEE/ACM Trans. Networking, Vol. 1, Dec. 1993, pp. 638-649.
M. Ajmone Marsan, A. Bianco, et al, “On the throughput of input-queued cell-based
switches with multicast traffic,” INFOCOM 2001, IEEE Proceedings, Vol. 3, 2001, pp.
1664-1672.
Shang-Tse Chuang, Ashish Goel, et al, “Matching output queueing with a combined input/output-queued switch,” IEEE Journal on Selected Areas in Communications, Vol. 17,
No. 6, June 1999, pp. 1030-1039.
Zhen Liu and Rhonda Righter, “Scheduling multicast input-queued switches,” Journal of
Scheduling, Vol. 2, 1999, pp. 99-114.
Hui Zhang, “Service disciplines for guaranteed performance service in packet-switching
networks,” Proceeding of the IEEE, Vol. 83, No. 10, Oct. 1995, pp. 1374-1396.
Yuval Tamir and Gregory L. Frazier, “Dynamically-allocated multi-queue buffers for
VLSI communication switches,” IEEE Transactions on Computers, Vol. 41, No. 6, June
1992, pp. 725-737.
Jeremiah F. Hayes, Richard Breault, et al, “Performance analysis of a multicast switch,”
IEEE Transactions on Communications, Vol. 39, No. 4, April 1991, pp. 581-587.
Deming Liu, Yann-Hang Lee, “An efficient scheduling discipline for packet switching
networks using earliest deadline first round robin,” preparing for submission.
Nick McKeown, “The iSLIP scheduling algorithm for input-queued switches,”
IEEE/ACM Transactions on Networking, Vol. 7, No. 2, April, 1999, pp. 188-201.
C. Minkenberg, “Integrating uincast and multicast traffic scheduling in a combined input- and output queued packet-switching system,” Computer Communications and Networks 2000, Proceedings, Ninth International Conference, pp. 127-134.
Balaji Prabhakar, Nick McKeown, et al, “Multicast scheduling for input-queued
switches,” IEEE Journal on Selected Areas in Communications, Vol. 15, No. 5, June
1997, pp. 855-866.
Rajeev Sivaram, Craig B. Stunkel, et al, “HIPIQS: a high-performance switch architecture using input queuing,” IEEE Transactions on Parallel and Distributed systems, Vol.
13, No. 3, March 2002, pp. 275-289.
M. J. Karol, M.Hluchyj, and S. Morgan, “Input versus output queuing on a spacedivision packet switch,” IEEE Transactions on Communications, Vol. COM-35, 12, December 1987, pp. 1347-1356.
Ge Nong and Mounir Hamdi, “On the provision of quality-of-service guarantees for input queued switches,” IEEE Communications Magazine, December 2000, pp. 62-69.

Voltage-Clock Scaling for Low Energy Consumption in Real-time Embedded
Systems
C. M. Krishna
Electrical and Computer Engineering Dept.
University of Massachusetts
Amherst, MA, 01003
krishna @ e m .umass.edu

Yann-Hang Lee
Computer & Information Science & Eng. Dept.
University of Florida
Gainesville, FL,32608
yhlee@cise.uf .edu

Abstract

immunity, additional level converters, and, most
importantly, an increased delay of circuit elements which
follows the equation:

Low power and energy consumption will always be an
essential requirement in many real-time embedded
applications.
Voltage scaling is a relative novel
approach to reducing energy consumption. The idea is
that a processor can be run either at high or at low
voltage: at high voltage, the clock rate is high but so is
the power consumption; at low voltage the clock rate is
lower, but the power consumption drops by a greater
factor. This immediately suggests a powerful approach to
lowering energy consumption in real-time systems.

delay = K
CVDD

The power-delay tradeoff introduced by a voltage
reduction has a profound impact on low-power design. As
shown in [3], components of a datapath circuit can be
supplied with multiple voltages such the power
consumption is minimized while meeting all the latency
constraints. For instance, under the Dhrystone 1.1
benchmark programs, an ARM7D processor can run at
33MHz and 5 V as well as at 20MHz and 3.3V. The
energy-performance measures at these two operation
modes are 185 MIPS/watt and 579 MIPS/watt, and the
MIPS measures are 30.6 and 19.1, respectively [4]. Thus,
if we switch from 33MHz and 5V to 20MHz and 3.3V,
there will be around (579-1 85)/579=68% reduction in
energy consumption at an expense of (30.619.1)/19.1=60% increase of processing time. The example
prompts a question that whether we can adopt an approach
to scale the supply voltage and the operation clock speed
such that the reduction of energy consumption can be
maximized,and all real-time deadlines continue to be met.

In this paper, we introduce static and dynamic algorithms
to control processor voltage to reduce energy
consumption. We demonstrate by simulation that our
algorithms can sign;f;cantly reduce total energy
consumption.

1.

Introduction

An increasing number of real-time applications are
energy-limited.
Examples include battery-powered
devices such as PCS telephones and solar- or nuclearpowered spacecraft.
One promising power- and energy-reduction technique is
voltage control. For the popular static complimentary
CMOS devices, the dominant source of power dissipation,
caused by the charging and discharging of parasitic
capacitance during switching activities at circuit level, can
be formulated as

2
’dynamic = ‘ L N S W V D D f
where C, is the CMOS circuit output load capacitance,
Nsw is the average number of switching activities per
clock cycle, andfis the clock frequency. This expression
suggests that a reduction in VDD is a highly effective way
of reducing power and energy consumption. However,
voltage reduction comes at the price of lowered noise

0-7695-0306-3/99
$10.00 0 1999 IEEE

-vj--12

where K is a constant and V , is the threshold voltage of
transistors. We therefore have a power-delay tradeoff by
controlling the supply voltage: the power consumption
varies quadratically with the supply voltage, while the
delay increases roughly linearly with the voltage.

The approach of voltage-clock scaling has been
investigated in workstations, notebook computers, and
PDA devices [5, 61. The basic idea is to make a scaling
decision for the next execution interval based on the
processor utilization of previous intervals or anticipated
future execution demands. In [6], the response time of
interactive users and application deadlines have also been

272

In the following section, we first present the system
model for voltage-clock scaling in real-time systems. In
Section 3, we give the details of the two proposed
approaches and show practical implementations. In
Section 4,simulation results are presented to illustrate the
reduction of energy consumption with respect to various
task and system characteristics. A short discussion follows
in Section 5.

taken into account in the voltage-clock scheduling for
individual programs.
For real-time systems having hard deadline
requirements, the voltage-clock scaling must be carried
out under the constraint that no deadline is missed. If the
processor speed is proportional to the circuit delay, the
above equations for delay and power consumption
indicate that the energy consumed to complete a fixed
amount of work will be a quadratic function of the supply
voltage. Due to the convexity of the energy function, a
simple approach of maximizing processor idle period
cannot always be optimal. A better approach would be to
find a supply voltage, as well as an operational clock rate,
such that the processor completes each task just before its
deadline. Ishihara and Yasuura [7] considered the
requirement of completing a set of tasks within a fixed
interval and the number of switching activities for each
task. For tasks with individual release times and deadlines,
a minimum energy scheduler was devised in [8] when
tasks are scheduled by the earliest-deadline-first (EDF)
policy in preemptive systems. The off-line approach
assigns the optimal speed setting to a critical interval that
has the maximum processing demand. It then uses the
critical interval to partition the remaining tasks into two
subsets that are solved recursively. For non-preemptive
systems, heuristic algorithms were also proposed in [9] to
determine resource allocation and task assignment for
power minimization.

2. System Model
Ideally, voltage and clock settings can be made
continuously in the operational range. In this paper, we
assume that the processor can be dynamically configured
in one of two modes: L-mode and H-mode. In L-mode, the
processor is supplied with a low voltage and runs at a
slow clock rate. Thus, task execution may be prolonged
but the processor consumes less amount of energy. On the
other hand, the processor can be set in H-mode, i.e. be
supplied with a high voltage and run at a fast clock rate, in
order to complete tasks sooner with an expense of more
energy consumption. Also, we normalize the processing
speeds and set the speed in L-mode to 1. The relative
processing speed at H-mode with respect to that in Lmode is referred as a; where a2l.
When the processor switches between the two modes,
for the voltage
there will be a switching interval,
regulator and the PLL clock generator to complete the
mode change. During the interval, the processor cannot
function correctly to execute tasks and task execution is
delayed by the interval t , . In practice, the switching
interval is of the order of ps as the switching frequency of
digital controllers for PWM (pulse width modulation) dcdc converters can reach 5OOKHz.

The response-time or deadline constraints are the
essential part of the decision criterion in voltage-clock
scaling problems in real-time systems. In this paper, we
investigate the voltage-clock scaling problem in real-time
systems with fixed priority scheduling. We assume that
the processor can run in one of two operational modes:
high supply voltage and a fast clock rate; or low supply
voltage coupled with a slow clock rate. It is likely that this
simple arrangement of operation modes can be
implemented more practically than variable voltage
settings in real systems. Based on the convexity of the
energy consumption model with respect to the supply
voltage, our goal is to minimize the percentage of time in
the high-voltage-fast-clock mode while meeting all
deadline requirements. We propose two scheduling
approaches in the paper. The first one statically assigns
each task to either high-voltage-fast-clock or low-voltageslow-clock operation modes. The second one dynamically
switches operational modes based on the accumulated
processing workload. If there is an early completion, i.e.,
a task completes before it takes the worst-case execution
time, the dynamic approach can reclaim the unused
processing time and use less of the high-voltage-fast-clock
mode.

The real-time system schedules all tasks according to
either a rate-monotonic or a deadline-monotonic algorithm
[ 10, 111. The two algorithms are chosen since they are the
optimal ones among all fixed priority algorithms and can
lead to a more predictable system. Assume that, in the
~
and T,,, that are
system, there are n tasks, z I , ,...,
numbered in a decreasing priority order. That is, p r i ( q ) >
pri( r+. ..> p i ( ? , ) where pri( r,) is the priority of task r,.
Task r, can be invoked periodically or arrive sporadically
with a low bound of inter-arrival interval. Let T, be the
minimum inter-arrival time between two consecutive
instances of task T, and C, be the worst-case execution
time (WCET) of task T, when the processor is running in
L-mode. Thus, the total processor utilization in L-mode
that is demanded by all tasks is p=x:=,C,/T,

. Also,

upon each invocation, the task 21 must be completed
before its deadline period D,, where C,ID,IT,.

273

subset of tasks and designate their execution in H-mode
such that the resultant processor utilization is bounded to
the schedulable utilization of n(2'"-1) of an n-task system.

To minimize energy consumption while guaranteeing
that all tasks meet their deadlines, the voltage-clock
scaling algorithm must determine when a switch of
processor operation mode is necessary and sufficient. The
problem appears when the task set is schedulable if the
processor is running in H-model entirely, and will miss at
least one deadline if running in L-mode completely. For
instance, with a rate-monotonic schedule and D,=T, for all
i, we will set the processor in L-mode if p is less than
n(2'/"-1). If n(2'"-l)<p_( n(2'/"-l)a, we can keep the
processor running in H-mode exclusively. This may result
in extra energy consumption. A better approach is to
selectively execute tasks in H-mode and, whenever
possible, keep the processor in L-mode in order to
consume less amount of energy. In other words, we should
minimize the execution period in H-mode and, at the same
time, guarantee that no deadline is missed.

Stating the problem formally, it is to partition the set of
tasks into two disjoint subsets, H a n d L, such that
1

Ci

minimized.

To investigate the general cases of Dj5Tj, we can
explore the time-demand analysis method [IZ, 131 for
deadline-monotonic scheduling. Let the task set be
partitioned into H and L. If igH, then the processor
executes task .5;: in H-mode. Otherwise, task .r, will be done
in L-mode. Define W;(t)=Z,r ,. j r ~ C 1 t / r l+ ( l / f f ) x,., ~
j&CJtF1 which is the worst-case cumulative execution
demand made by the tasks with a priority higher than and
equal to z; during the interval [ Q t ] .Then, based on the
necessary and sufficient condition of schedulability in [ 12,
131, task z; is schedulable if there exists a t ~ @ , = ( 1 T ,I
j = 1 , 2,..., i; 1=1,2,...,hD/Tip) u ( D ; ] ,such that Wj(t)I t .
The objective of the partition is again to minimize the
amount of execution time in H-mode, i.e., to minimize

Voltage-Clock Scaling under Fixed
Priority Schedules

In the following, we present a static approach first,
which assigns the operation mode of each task to either H mode or L-mode, in an off-line computation. Then,
whenever a task is scheduled to execute, the processor
enters the assigned mode and stays in the same mode until
the task execution is completed or a context switch occurs.
We then turn into a dynamic approach which schedules
the operation mode based on the available processing
duration in L-mode.

3.1.

c.

The above inequality guarantees the schedulability for all
tasks, whereas the minimization is for low energy
consumption. Given that p =yl=IC/7;.,the problem can
be formulated as the selection of a subset H_c( 1,2,...,n],
such that c,,HC/T; is minimized and is greater than
a(p-n(2'/"-l))l(a-l).This optimization problem can be
treated equivalently as the decision problem of subset sum
which is NP-complete. Consequently, efficient search
techniques should be employed to find a solution if n is
large

Note that we ignore the energy consumption when the
processor is idle. In many modern processors, there are
power-management mechanisms that allow a softwarebased control of transferring from normal operation mode
to sleep, doze, or power-down modes. These low power
modes put the processor in a standby state. However, with
a fixed amount of workload, an increase of idle period
implies additional execution time in H-mode. This cannot
result in a minimal energy consumption given that the
energy consumption is a convex function of the supply
voltage. Hence, our optimization objective should aim at
the elimination of any unnecessary execution in H-mode.

3.

c.

- 1 2+ C L 5 n(2"" - 1) and
-is
a i c H 7;. k L T
iGH

XirHcflj.

Note that the complexity of the schedulability test
based on time-demand analysis is O(n2(T,,/Tmj,)) for a set
of n tasks, where Tm(LT
and T,,, are the maximal and the
minimal task periods, respectively. This is much higher
that the O(n) complexity of the utilization-based
schedulability test. In fact, the schedulability condition of
W , ( t ) l t can be degenerated to the schedulable utilization
of n(2'"-1) if a task set with Di=Ti is chosen appropriately
(see the proof of the schedulable utilization in [lo]). This
suggests that the partition of the task set into H and L
subsets based on the time-demand analysis is also NPcomplete.

A Task-Based Static Scheduling

For a rate-monotonic scheduling with D,=T,, the
schedulable utilization [IO] can be applied to ensure the
satisfactory of deadline requirements and to solve the
voltage-clock scaling problem for real-time systems. Note
that the voltage-clock scaling problem arises if the
processor is overly utilized when it only runs in L-mode.
If task q's execution is done in H-mode, then the
processor's utilization caused by the task is reduced from
Cfl, to C/(aT,). Accordingly, we can select a minimal

Instead of examining various heuristic solutions for the
optimal task operation modes, a simple branch-and-bound
approach has been developed. The approach keeps
assigning task into H-mode if the assignment does not
allow all tasks to meet their deadlines and the utilization is
less than the existing bound of a schedulable assignment.

274

Assume that the system can speed up by a factor of
E I S if running in H-mode, and there are three real-time
tasks, TI, T ~ ,and b. The worst-case execution times
(WCETs) of the tasks in L-mode are 3, 6 , and 12,
respectively. Also, all tasks arrive periodically with
periods 9, 20, and 32, and have their deadlines equal to
their periods. As shown in Figure l(a), if all tasks are
running in L-mode, then q cannot meet its deadline in a
critical instance. A task-based static scheduling, as
described in the above subsection, determines that the
execution of TZ should be assigned to H-mode. Thus, r3
becomes schedulable and the system consumes the least
amount of energy. The execution sequence in a critical
instance with the H-mode assignment is illustrated in
Figure I(b), where the shaded boxes indicate the H-mode
execution periods. In Figure I(c), we show the execution
sequence that the processor stays in L-mode as long as it
has not used up all available L-mode execution durations
associated with the existing tasks. When all three tasks
arrive at time 0, the total L-mode execution duration is 15
(due to the WCET’s of T],and
that are assigned in Lmode). In Figure I(d), the L-mode execution duration is
plotted and, when it reduced to 0, the processor is
switched into H-mode. In other words, if we can trace the
remaining execution duration in L-mode, the decision of
switching between L-mode and H-mode can be made.

If the new assignment comes up with a lower utilization
and makes all tasks schedulable, the bound is updated. If
the utilization is greater than the existing bound, then at
least one task assigned in H-mode must be dropped and a
branch to a new assignment path should be initiated.
It is easy to implement the task-based static approach
in any real-time operating system. Once the operational
mode for each task is determined and attached as one of
the task attributes in the task control block, then, at each
instance of context switching, the dispatch mechanism of
the real-time OS can trigger a switch of the operational
mode if necessary. Thus, the change of the voltage-clock
setting becomes a part of context switch and the switching
interval t , can be included in the overhead of context
switch. An increase of the execution time for each task
from C, to C,+2t, gives an upper bound of schedulability
analysis if the impact oft,, cannot be disregarded.

3.2. A Dynamic Scheduling of Voltage-Clock
Scaling
Differing from the task-based static approach, a
dynamic scheduling approach for voltage-clock scaling
can consider the existing workload imposed on the
system. Instead of running a subset of tasks in H-mode
completely, we should look into the minimal amount of
execution duration in L-mode. If this duration is not zero,
the processor can stay in L-mode no matter which task is
running. On the other hand, when there is no more L-mode
execution duration, then, the processor must be operated
in H-mode until either all waiting tasks are completed or
an arriving task brings in additional L-mode duration. T o
illustrate such an on-line approach, we begin with the
example shown in Figure 1.

The execution sequences in Figure l(b) and l(c) don’t
reveal any difference in energy consumption and both
meet the deadlines. However, if tasks arrive with variant
phases or the real execution times are less than worst case,
the greedy approach of using L-niode execution duration
first can avoid unnecessary switches to H-mode. This can
be illustrated in Figure 2 where we assume that task q has
an arrival phase of 15. Before giving an explanation of the
execution sequence, we investigate the level-i busy cycle
during which the processor is continuously busy executing
tasks with priorities greater than or equal to task q. Let
BC, be a level-i busy cycle that ends with a completion of
task 2;. Thus, in our example, BC3 must contain a duration
of at least 12 time units during which the processor
executes in L-mode. This duration can be made available
for a greedy L-mode execution immediately after BC.?
starts.

deadline

(c). H-mode execution
delayed
15

(d). durution ofl-mode
execution
0

9

18

21

21

30 32

time

Figure I . The execution sequence in a critical instance
for the example tasks

275

In Figure 2(b), we add r3’sL-mode execution duration
of 12 at the beginning of every level-3 busy cycle, i.e. at
the arrival instances of task 71, TZ, or q , while the
processor is idle. If task 8 does arrive during this busy
cycle, its L-mode execution time has already been
included in the computation of the remaining L-mode
execution duration. On the other hand, if the level-3 busy
cycle ends without a completion of task r3, no deadline

T~ arrives

T3 i s

ur I5

r~

(a). H-mode execution
delayed

I T I ,
I
I
I

15

t T 3
I
I

1
1
1

,

T,

Tz

, ,
I
I
I
I

!

t

I

T1

completed
ut 40

TI T

A
I
I
I
I

I
I
I
I

33

36

T2

I

(b). duration of L-mode
execution

0

9

15

18

21

Figure 2. The execution sequence when task

21

30

time

has an arrival phase of 15.

and T~are schedulable in

accumulate the unconsumed L-mode execution duration in
BC,:

We now present a dynamic voltage-clock scaling
algorithm that schedules operation modes based on the
current status in the system. We begin by defining some
notation:

(1). When BC, becomes active at time t ,
R,(t)=C, if i e L: or 0 otherwise.
(2). When a task 5 arrives at t , R,(t) = R,(t ) +
C, i f j e L’and j<i.
(3). After the processor has been operated in Lmode for a duration A , R,(t) = R,(t ) -A.

can be missed since both tasks
L-mode.

1.

TI

OR=(
i I task .r; is not schedulable if all tasks are
running in L-mode ), which consists of all i that
W,(t)> t for tE@, with L = ( 1,2,...,1 1 ) .

2.

BC, : a busy level-i busy cycle which is active
when the processor executes a task of priority
higher than or equal to i.

3.

R,(t) : the remaining L m o d e execution duration
in BC, at time t .

The processor can stay in L-mode at time t if the
minimal R,(t) is larger than 0, for all active BC, where
;EO,. It can switch the execution modes independently of
the execution order determined by task priorities. Also,
the algorithm guarantees that no task misses its deadline
under this dynamic voltage-clock scaling algorithm. Note
that, since we use fixed priority scheduling, task G, where
krr
can always be done before its deadline no matter
which execution mode is used to process the tasks of high
priorities. To justify that task .r;, where iE Q can meet its
deadline, we should look into the amount of processing
time during a level-i busy cycle, BC,: that starts at time t
and ends with a completion of .r; at time t : Since the
processor continuously executes all arriving tasks in either
L-mode or H-mode in BC, the total processing time is

L’and H’: the schedulable partitions of the task
set (1,2,...,n ) for L-mode and H-mode execution
that minimizes &C/I;
and is subject to
If no such partition exists, L’and ”are chosen
without the constraint of R G :
The dynamic voltage-clock scheduling algorithm is
presented in Figure 3. At first, R i s computed based on the
time-demand analysis. Then L’ and H‘ are determined
through a search approach (e.g., a brand-and-bound search
algorithm). At execution time, the algorithm only needs to
deal with a subset of BC, where ie R. BC, is active when
the processor begins to execute a task zj, where .r, has a
priority higher than or equal to z,. It can be set to inactive
if the executing task has a priority lower than Z, or the
processor is idle. The computation of R,(t) is done
according to the following rules which essentially
4.

<

where 1 7 1 ~is the number of task n.completed in BCj<Now,
consider the execution sequence according to a task-based
voltage-clock scaling with the partitions L’and “starting
from t. If task Z, is done at time r’‘and meets its deadline,
then

276

mode switching overhead. Accordingly, by increasing the
execution time of task from Ci to Ci+2f,, the switching
interval t, can be included as a part of task execution in
the scheduability computation.

Algorithm Dyn-VC-Sch:
off-line preparation: compute Q, L : and “for
input task set

the

on-line process:

4.

at the arrival instance of task zjk , where k E L’
for (i=k; iIi2; i++) {
if (isQand BC, is active) R,(t) = R,(t )+ ck
I/ accumulate R,(t )
if (ien a n d BC, is inactive) {
I/ initiate a new busy cycle
R,(t) = R , ( t ) + ck + Cl;
active BC,;

Performance Evaluation

As described in Section 3 , the task-based static
assignment of operation modes and the dynamic voltageclock scheduling can guarantee all tasks to meet their
deadline. Depending upon task characteristics, such as
periods, WCETs, and arrival phases, the time for which
the processor is running in H-mode may vary a lot. In the
following examples, we evaluate the average energy
consumption based on random task parameters in a system
of 10 real-time tasks. For each test case, we measure the
percentages of the time that the processor is in H-mode
and L-mode, and is idle. Under the optimal static
operation mode assignment, these percentages can be
calculated based on the task utilization and the mode
assignment. For the dynamic voltage-clock scheduling,
simulation is used to measure the percentages.

1

1
at the completion instance of task zjk
if no task is waiting, set all BC, to inactive;
else (
if task z, has the highest priority among all
waiting tasks {
for ( i = l ;iq;i++) {
set BC, to inactive; /I BC, is done

T o create a test case, we first generate 10 random task
periods in the range of 100 to 1000, and assign the task
deadlines equal to their periods. The WCETs of the tasks
are chosen in order to have a fixed utilization p which is
set between 0.9 to 0.7*a, where ~ 2 . and
0 1.5. The real
task execution time is then selected randomly such that the
mean execution time is in the range of 0.6*WCET to
l.O*WCET. A test case may be dropped if the timedemand based schedulability test shows that it is
schedulable in L-mode or unschedulable in H-mode.
Otherwise, we compute the partitions of L’and H‘using a
branch and bound search algorithm, and perform a
simulation for the dynamic voltage-clock scheduling.

1

1
1
during task execution:
if (the minimum of R,(t) is larger than zero
among all active BC,)
stay in L-mode;
else process tasks in H-mode ;
Figure 3. A dynamic scheduling algorithm for voltageclock scaling

In Figure 4, we show the evaluation results for variant
0 1.5. The curves with solid
utilization when ~ 2 . and
lines are the percentage measures from the static
assignment method, whereas the dashed lines are based on
the dynamic voltage-clock scheduling. If we run all tasks
in H-mode and keep the processor in idle mode whenever
it is not busy, then the percentage of the time in H-mode is
in the range of 0.27 (i.e. 0.45*0.6) to 0.7 when -2.0.
This is much higher than the measures obtained from the
two proposed methods. For instance, when p=1.0 and the
mean task execution time is 60% of the WCET, the
utilization in H-mode is 0.3 if all tasks are done in H mode. With the static method, the processor only spends
8% of the time in H-mode. Remarkably, the dynamic
method can further reduce this percentage to 1.8%.

where m’i is the number of task Q completed in [r, t’q and
€is the processor idle time during [ t , t’3. This implies that
r ’ l t”-&under the dynamic algorithm and is less than the
task deadline.
The algorithm in Figure 3 also leads us to a fact that
there will be at most two mode changes for each task
arrival. If R,(t) is positive, an arriving task cannot result in
a mode change. When R,(t) is zero, an arriving task may
trigger a mode change from H-mode to L-mode and then a
following change back to H-mode later. In addition,
during BC,, there will be a switch from L-mode to H-mode
when the initial R,(t) of BC, expires and a possible mode
change at the task completion instance of task r,. Since
task r, does not trigger any mode change when it arrives,
these extra two changes can be counted as its share of

277

The curves in Figures 4(a) and (b) show many
interesting phenomena. The dynamic method is very
effective when the utilization is low. This is due to the fact
that many short busy cycles may end during the borrowed
L-mode execution duration under the greedy approach.
Conversely, when the utilization is high and all tasks take
the execution time equal to their WCETs, the dynamic
approach cannot gain much. In some cases, it is worse
slightly then the static approach since the selected
partitions L’ and H’ are subject to additional constraints
and may not be optimal for energy reduction.
We can substitute processor’s electrical characteristics
data of ARM7D [4] to illustrate the reduction of power
consumption under the proposed methods. Using the
simulated task sets, the estimated power consumption are
shown in Table 1 where the mean execution time is
O.g*WCET and the value of a a r e assumed to be 1.5, for
ARM7D. In addition to the average power consumption
for the proposed methods, we also show the result of
running all the tasks in H-mode. When the processor is not
busy, it is put in the idle state and consumes no power at
all. The resultant power consumption estimations indicate
the significant reduction from our proposed scaling
methods. For instance, when Q”I 1.5 and p=0.93, the
average power consumption can be reduced by a factor of
65% (i.e. from 0.082W to 0.028W).

5.

0.88

0.9

0 92

0.94

096

0.98

102

1

104

106

Utilization in L-mode

(a). -1.5

Conclusion

In this paper, we investigate the voltage-clock scaling
problem in real-time embedded systems. If we reduce the
energy consumption by lowering down the operation
voltage and the clock rate, task executions may be delayed
and may not be able to meet the deadline requirements.
This power-delay tradeoff can be reduced to the
optimization problem of minimizing energy consumption
subject to schedulability.

0.8

0.9

I

1.2

1.1

Utilization

in

1.3

1.4

1.5

L-mode

(b) -2.0
Figure 4. The percentage of execution time in H-mode for
the simulated cases (solid lines: static task-based
scheduling; dashed lines: dynamic scheduling)

W e propose two scaling methods to reduce power
consumption. In the first (static) one, each task is assigned
an operational mode. At the execution stage, the task
dispatcher in real-time OS can initiate an operational
mode change according to the assigned mode of the
executing task. The mode assignment is formulated as a
subset sum problem subject to time-demand schedulability
analysis. The second method does an initial mode
assignment and determines the available execution
duration in the low energy consumption mode for each
busy cycle. If this duration is not zero, the processor does
not need to change to the high performance mode and
avoids unnecessary energy consumption. Through
simulation evaluations, we have demonstrated that the

proposed approaches can result in a significant reduction
in energy consumption.

References
[I]

278

G. K. Yeap, Practical Low Power Digital VLSI
Design, Kluwer Academic Publishers, Boston, 1998.

ARM7D
L-mode

20MH2, 3.3V, 19.1MIPS, 0.033Watt

H-mode

33MHz, 5V, 30.6MIPS, 0.165Watt

a

1.5 (=30.6MIPS/19.1MIPS)

~

D

I

I

0.93

1

none

static

dynamic

none

0.99
static

dynamic

H-mode execution

49.6%

11%

3.1%

52.8%

20.3%

9.8%

L-mode execution

0

57.9%

69.8%

0

48.8%

64.5%

in idle state

50.4%

31.1%

27.1%

47.2%

31.9%

25.7%

av.powerconsumption~

I

Scheduling method

I -

I

0.082W

1

0.037W

I

0.028W

I

0.087W

I

0.050W

I

I

0.037W

1

~~

% of the reduction

-

54.5%

65.6%

-

43.1%

57%

Table 1. Estimated power consumption for the simulated task sets in ARM7D processor

A. Raghunathan, N. Jha, and S. Dey, High-level
Power Analysis and Optimization, Kluwer Academic
Publishers, Boston, 1998.

F. Yao, A. Demers, and A Shenker, “A scheduling
model for reduced CPU energy,” IEEE Foundations
of Computer Science, 1995, pp. 374-382.

J.-M. Chang and M. Pedram, “Energy minimization
using multiple supply voltages,” IEEE Trans. VLSI
Systems, Vol. 5 , No. 4, Dec. 1997, pp. 436-443.

I. Hong, D. Kirovski, G. Qu, M. Potkonjak, and M.
B. Srivastava, “Power optimization of variable
voltage core-based systems,” Proceedings - Design
Automation Conference, 1998, pp. 176- 1 8 1.

“Introduction to Thumb,” ARM Documentation,
Advanced RISC Machines Ltd.

C. L. Liu and J . W. Layland, “Scheduling algorithms
for multiprogramming in hard real time
environment,” J . Assoc. Coniput. Mach., Vol. 20,
NO. 1, 1973, pp.46-61.

T. Pering, T. Burd, and R. Brodersen, “The
simulation and evaluation of dynamic voltage scaling
algorithms,” International Symposium on Low
Power Electronics and Design, Aug. 1998, pp. 76-

N. Audsley, A. Burns, M. Richardson, and A.
Wellings, “Hard real-time scheduling: the deadlinemonotonic approach,” Eighth IEEE Workshop on
Real-Time Operating Systems and Sofmare, 1991,
pp. 133-137.

81.
M. Weiser, B. Wilch, A. Demers, and S. Shenker,
“Scheduling for reduced CPU energy,” Proc. of 1”
USENIX Synip. on Operating Systems Design and
Implementation, Nov. 1994, pp. 13-23.

J. Lehoczky, L. Sha, and Y. Ding, “The ratemonotonic
scheduling
algorithm:
exact
characteristics and average case behavior,” Proc.
IEEE Real-Time Systems Symposium, Dec. 1989, pp.
166-17 1.

T. Ishihara and H. Yasuura, “Voltage scheduling
problem
for dynamically
variable
voltage
processors,” International Symposium on Low
Power Electronics and Design, Aug. 1998, pp. 197202.

J. Lehoczky, “Fixed priority scheduling for periodic
task sets with arbitrary deadlines,” Proc. IEEE RealTime Systems Symposium, Dec. 1990, pp. 201 -209.

279

Schedulable Online Testing Framework for
Real-Time Embedded Applications in VM
Okehee Goh and Yann-Hang Lee
Computer Science and Engineering Department
Arizona State University, Tempe AZ, USA
{ogoh,yhlee}@asu.edu

Abstract. The paper suggests a VM-based online testing approach in
which software testing is piggybacked at runtime on a system that operates to serve actual mission. Online testing in VM is facilitated with
a framework that uses persistence service to initialize the testing operation with a consistent system state. The testing operation then runs in
an isolated domain which can be scheduled independently of the operating version. Thus, testing operation cannot cause unbounded pause time
nor spoil the normal operation. We evaluate the feasibility of schedulable online testing with a prototype developed in MONO CLI (Common
Language Infrastructure) and the experiment on the prototype.
Keywords: Online Testing, Virtual Machine, Real-Time Embedded
Applications.

1

Introduction

Nowadays, the applications of real-time embedded systems have proliferated
from industrial controls to home automation, communication consumer gadgets,
medical devices, defense systems and so forth. The apparent trends of the systems include sophisticated features and a short production cycle. The trends
have sought solutions more in software rather than in hardware and also have
led to the application of virtual software execution environment (VM) as a runtime environment. VM, populated with JVM [10] and CLI1 [5], features high
portability from using intermediate code, high productivity and reusability of
object-oriented languages, and a safe runtime environment. The features are
beneﬁcial to the development of real-time embedded systems as the production
cycle and cost can be reduced.
As software plays an increasingly signiﬁcant role in embedded systems, the
demands of upgrading software are anticipated for bug ﬁxing and for extended
functionality. In fact, most embedded systems, which have long lifetimes and require high availability, are generally passive on software upgrade because upgrading software requires to restart the systems, and the newly upgraded software
1

CLR (Common Language Runtime), which is a CLI’s implementation by Microsoft
and an integral part of Microsoft .NET framework, is more popularly known than
CLI.

T.-W. Kuo et al. (Eds.): EUC 2007, LNCS 4808, pp. 730–741, 2007.
c IFIP International Federation for Information Processing 2007


Schedulable Online Testing Framework

731

can introduce new types of bugs and faults. Research work on online upgrade
[12,4], and reconﬁgurable systems [14], has been focusing on providing facilities
to accomplish the software upgrade at runtime. In the meanwhile, there is no
doubt as to the importance of software testing to verify the correctness, the
completeness, and security, especially for mission- or safety-critical software in
which even minor changes of the software require extensive testing [15].
Software testingis generally conducted in oﬀ-line environment with test cases
generated by predetermined inputs. The weakness of the predetermined inputs,
particularly derived from a model-driven formal system speciﬁcation, is that the
testing results are restricted to the correctness and completeness of a given model.
Furthermore, an oﬀ-line testing environment for embedded software means that
the software is tested in a simulation mode and does not participate in an actual
mission. However, the testing of embedded software should be able to deal with
external interferences and unexpected behavior of the target application environment. All possible inputs may not be known ahead, and the generation of complete
test cases for all execution conditions is very problematical.
To overcome the aforementioned limitations, we suggest an online testing environment for software upgrades as a supplementary approach of an oﬀ-line testing. The testing of software upgrades is piggybacked at runtime on the systems
that operate to serve an actual mission. Hence, the execution of the software
dedicated to the actual mission coexists with the execution of the software to be
tested. The apparent beneﬁt of online testing is that testing undergoes not in a
limited runtime environment but in an actual target environment connected to
physical world. That is, the software testing is conducted by using actual inputs.
To simplify terminologies to be used hereafter, the software for an actual mission, and the software to be tested are called software under operation (SUO),
and software under test (SUT), respectively.
Most embedded applications run periodically with long lifetimes. At each period, they conduct computation by taking external input data, and then the
computation results, represented as output data, is used to activate the target
hardware. Some embedded applications’ computation at each period is based
on the state that has been accumulated from computations of preceding periods as well as the newly sampled input data. For online testing of this type of
applications–stateful software, SUT must be able to start with the accumulated
computation state of SUO, and since then, gets applied with the same inputs
that SUO receives. Furthermore, if SUO is characterized by time constraints,
whose virtue includes timely correctness, the online testing piggybacked has to
be nonintrusive: the latency and pause time that SUO encounters due to online testing must be predictable and controllable. Certainly, any faulty behavior
caused by SUT must be isolated to prevent SUO operations from any impact.
In this paper, we aim at a framework of schedulable online testing (SOTF)
for real-time embedded software in VM. With the advent of an online testing
request, the framework provides facilities to enable a testing mode where both
SUO and SUT are executed concurrently. On the termination of testing, the
system returns back to an operation mode of executing SUO only. It achieves

732

O. Goh and Y.-H. Lee

fault isolation by executing SUT in a separate partition. By checkpointing the
accumulated computation state of SUO, SUT begins to execute from a consistent state. In addition, the framework logs the external input data which SUO
receives, and reconstructs the logs for the execution of SUT. Finally, SOTF
employs a preemptible mechanism for checkpointing and recovery of persisted
states and provides the ﬂexibility to resume the testing anytime. Hence, the
timely correctness of SUO can be ensured.
In the following section, we give a discussion of related works. The target
application model of online testing is introduced in Section 3. Then, the approaches and designs of the proposed schedulable online testing framework on
CLI (Common Language Infrastructure)[5]’s open source platform, MONO [17],
are presented in Section 4. In Section 5, the experiment on the prototyped SOTF
is used to show the space overhead incurred by the testing framework. The overhead and the source of latency of online testing with SOTF are identiﬁed. Finally,
Section 6 draws a conclusion.

2

Related Works

One of well-known research areas with a key role of checkpointing and/or logging
is log-based rollback recovery [6]. Logging-based recovery protocol, especially on
.NET framework [2,1], is tuned at component oriented distributed applications
The work was motivated with the problems that process-based recovery protocol cannot detect the failure of components, and checkpointing/recovery in a
process level is very heavy. The prototype employs .NET’s Object Serialization
and Context to support checkpointing/recovery and to enable interceptions of
messages (to aid logging) on persistent components, respectively. The rebinding
of recovered components is done through .NET Remoting’s registry so that other
stable components can access the recovered components.
Simplex architecture[13] is to support the evolvability of dependable real-time
computing systems. The architecture adopts analytical redundant logic: running
a trusted version (a fault-proof component) and an upgrade version (a not-yetfault-proof component) in parallel as separate software. The architecture has decision logic that monitors the behavior of an upgrade version. If a faulty action
is detected from the version, the control of the system is switched to the trusted
version. Resource isolation is emphasized to prevent a trusted version from being
corrupted due to the faulty behavior of an upgrade version. Lee et al.[9] extended
the Simplex architecture for online testing and upgrade of industrial controller in
the Linux OS environment by applying the technique of Process Resurrection [8].
RAIC (Redundant Arrays of Independent Components) [11] is a technology
that uses groups of similar or identical components to provide software dependability and allows component hot-swapping; the architecture allows addition or
removal of components at runtime. When a component is swapped, the state
transfer from an old component to a replaced component is supported, if the
component is stateful. If the components are faulty, as examined using built-in

Schedulable Online Testing Framework

733

testing code on the controller, the controller handles the faulty exceptions and
recovers the application state so that the fault is not exposed to the applications.
The primary diﬀerence of our work from the previous works is that our framework aims a testing facility in VM that allows preemption to reduce blocking
delays. Thus, ﬂexible scheduling of testing can be carried out while ensuring the
timeliness of regular service.

3

Target Application Model

The target application model we envision for SOTF is a closed-loop control system. The systems basically include sensors, control processes, and actuators, and
the control tasks run concurrently and periodically to control a target plant. A
simpliﬁed view of the system can be described by three application-level objects
for the three system components: InputData, ControlProcess, and OutputData.
Figure 1 is a simpliﬁed closed-loop control system applying the schedulable
online testing. In the ﬁgure, InputData indicates the data collected from sensors
and taken by a control process, and OutputData are the computational results
generated by the process and passed to actuators. We assume that a control
process demands upgrades to meet performance enhancement or new business
requirements. The upgrade version’s control process has to access the same InputData as the operation version; that is, the upgrade version maintains the
same frequency and format for the access to InputData as the operation version
does. It is a reasonable assumption because InputData, which is generated by
a sensor as a result of monitoring the target plant, does not get changed unless
the sensor or the target plant gets replaced or upgraded. The same assumption
is applied to OutputData with respect to the actuator.
Consider a stateful control process where the computation result of each periodic operation depends on not only the InputData obtained at the current
period but also the accumulated state from the computations of the preceding
periods. An upgrade version’s control process can start online testing by being initialized with the state of an operation version’s control process at the
SW under
Test
InputData

SW under
Operation

ControlProcess

Logging/Reconstructing

InputData

Sensor

OutputData

Checkpoint/Recovery

ControlProcess

OutputData

Plant

Actuator

Fig. 1. Target application model of Online Testing

734

O. Goh and Y.-H. Lee

moment that online testing is triggered, and by accessing the same InputData
as the operation version.
The concern of online testing with real-time applications is to ensure time
constraints of regular service while online testing is under way. An approach
to address this issue is to schedule the testing as a background task, which
runs when no real-time task is ready. One of the problems that arise under this
scheduling approach is the availability of InputData. When an upgrade version is
ready to run, InputData that primarily stimulates the operation version’s control
process, may not be available any more. Also, the initial state must be preserved
until the upgrade versions start to execute. The foremost important issue is that
the testing operation, the logging of Inputdata, and the preservation of the initial
state must be preemptible. Thus, the testing process would not block the regular
service of the target system.

4

Online Testing Service

To enable schedulable online testing, we present the approaches in VM-based
real-time embedded system. Although the approaches are applicable to both CLI
and JVM, we built a prototype of SOTF in MONO CLI and, in the subsequent
text, refer to speciﬁc technologies and standard class library of CLI.
4.1

Online Testing Framework

SOTF consists of three tasks (in the application layer) to drive online testing,
and two subsystems (in the VM layer) to aid online testing: three tasks including
OTTestManager, OTRecordDriver (OTRecordDrv), and OTReconstructDriver
(OTReconstructDrv), and two subsystems including a preemptible persistence
system and a logging/reconstructing system.
Figure 2 illustrates the architecture of the framework of schedulable online
testing. The roles of the three tasks are as follows. OTTestManager is responsible
Application-layer
OTTestManager

Operation Partition

OTRecordDrv

Control
Process

Test Partition
OTReconstructDrv

Input/Output Data

Control
Process

Input/Output Data

VM-layer
Class Loader
JIT

Checkpoint/Recovery
Garbage Collection

Multi-Threading

Exception Handler

Logging/Reconstructing
Verification
Native code interface

Fig. 2. Online Test Framework

Schedulable Online Testing Framework

735

for triggering online testing when software to be tested is ready. As a response
of the commands from OTTestManager, OTRecordDrv interacts with SUO to
checkpoint a consistent state of SUO and log input data sampled from sensors.
Correspondingly, OTReconstructDrv interacts with SUT to direct the recovery
and reconstruction operations of the persisted state and the logged input data. In
the ﬁgure, SUO and SUT are represented with a composition of ControlProcess,
InputData, and OutputData, as a simpliﬁed model suggested in Section 3.
4.2

An Isolated Testing Environment

As SUT concurrently runs with SUO, the possible faulty behavior of SUT can
aﬀect the operation of SUO. To contain the faulty behavior of SUT, it should
reside in a runtime environment separated from that of SUO. We employ Application Domain [3] in CLI as a facility to provide an isolated runtime environment.
The application domain is a lightweight address space designed as a model of
scoping the execution of program code and the ownership of resources. Sharing
objects between diﬀerent domains is prohibited: that is, objects in one domain
cannot access objects in other domains. Creating multiple application domains
by starting assembly2 with a main entry, is supported at runtime. Additionally, CLI facilitates unloading an application domain at runtime. This allows a
dynamically created SUT domain when a testing operation is requested.
4.3

Preemptible Checkpointing and Recovery

If SUO’s accumulated state from the computations of preceding periods is preserved, then SUT can start with the known initial state. To transfer the state
from SUO to SUT across the domain boundary, we adopt the approach of checkpointing SUO’s state and recovering the state for SUT. The challenge of the
approach, especially for real-time applications, is that the pause time due to the
checkpointing/recovery operation may be unbounded. The unpredictable latency
from checkpointing can hinder the timeliness of SUO if SUO is blocked until the
checkpointing operation ﬁnishes entirely.
To make it possible to bound the pause time due to checkpointing and recovery, our prior work, schedulable persistence system (SP system) [7] is adopted
with which the persistence service runs concurrently with real-time tasks. The
minimal length of the pause time, i.e. the minimal non-preemptible region in the
persistence service, can be adjusted to meet the scheduling needs of real-time
application tasks.
When the persisted state is deserialized, there may be a question whether the
state objects can be useful directly by the SUT. If the state objects of the control
process in SUT is the same object of SUO, i.e. the names of persistent classes
and persistent ﬁelds in SUT is identical to these in SUO, then the persisted
objects generated from SUO can be used to initialize SUT. Otherwise, we can
apply a transformation script to reconstruct the state objects for SUT based on
the persisted SUO objects.
2

Assembly is a minimal unit of reuse, versioning, and deployment in CLI.

736

4.4

O. Goh and Y.-H. Lee

Logging and Reconstructing

After being initialized with the checkpointed state of SUO, SUT is ready to execute. It should receive the sampled input data similar to the one applied to SUO
since the checkpoint. As a solution, the access to InputData by SUO’s ControlProcess is logged and then the access to InputData by SUT’s ControlProcess
is suﬃced with the logs. This logging/reconstructing requires to intercept the
method calls on InputData objects. That is, the method calls by SUO’s ControlProcess to read InputData is post-processed to log the sampled data, and
the method call by SUT’s ControlProcess to read InputData is pre-processed to
reconstruct the sampled data based on the logs. The post- and pre-processing on
InputData objects are done through the Context in CLI which provides an object
with an execution scope. Additional services can be augmented during incoming/outgoing method calls on context-bounded objects which are derived from
the System.ContextBoundObject. This feature has been employed in a loggingbased recovery protocol on .NET framework [2,1] to enable the interceptions of
messages (to aid logging) on persistent components.

5

Experiments

The experiment of the SOTF prototype on MONO CLI is performed to understand the source of latency on a testing sequence and to examine the concerns of
scheduling online testing in an example system. It is conducted with C# benchmarking applications on a PC workstation with 1.5GHz Pentium IV processor
and 256MB memory. To have a high resolution timer and preemptive kernel,
TimeSys’ Linux/RK (real-time kernel v4.1.147) [16] is used. For time measurement, a standard class library System.DateTime.Now.Ticks is used, which gives
100ns resolution. The C# language supports ﬁve levels of thread priorities, Highest, AboveNormal, Normal, BelowNormal, and Lowest. The priorities, are implemented using TimeSys RK’s POSIX real-time FIFO scheduling policy.
5.1

Cost Analysis for Testing Sequence

SOTF is implemented by integrating a wide range of facilities to satisfy the
requirements of online testing such as an isolated testing environment, interceptions of method calls, checkpointing/recovery, and logging/reconstructing.
Using the facilities leads to some overhead. Although the amount of overhead
or cost depends on the techniques employed, these types of overhead or cost
are inevitable. In this experiment, we analyze the cost incurred in every stage
constituting the testing sequence.
The benchmarking application, SUO, used in this experiment is a seismic
event monitor, which computes the rate of seismic events by using both seismometric data newly obtained and seismometric data accumulated from a preceding
duration. The seismic event monitor (SUO) runs periodically every 3ms for 1ms
WCET (Worst Case Execution Time) with AboveNormal priority. The seismometric data read from its InputData object is 20Bytes. The seismometric data

Schedulable Online Testing Framework

737

accumulated from prior computations, a persistent state of SUO’s ControlProcess, will be checkpointed to aid for online testing. The size of the persistent state,
consisting of about 1000 composite objects including primitive types’ ﬁelds, is
about 20000Bytes. Its upgrade version, SUT, embodies a slightly diﬀerent computation approach but generates basically the same results with the operation
version SUO. When online testing starts, SUT runs every 1ms with Lowest priority. To just observe the overhead of the operation in a testing sequence, we
allow the checkpointing on SUO and the recovery of persisted data on SUT to
perform in a nonpreemptible mode. Additionally, the termination condition of
testing is set to 300 periods of SUO’s operation.
Table 1. Time line of a testing sequence
Stages
(1) Receive a testing request

Time (ms)
0

(2) Turn on checkpointing on SUO

10

(3) Start checkpointing/logging on SUO

12

(4) Complete checkpointing

25

(5) Start SUT

29

(6) Complete initialization for testing on SUT

686

(7) Start recovery on SUT

691

(8) Complete recovery, and start testing on SUT

699

(9) Complete logging on SUO

862

(10) Complete testing on SUT

1015

(11) Start unloading of SUT

1044

(12) Complete unloading of SUT

1109

Table 1 shows the cost incurred in every stage constituting the testing sequence. The result is chosen as one with the longest completion time (e.g. the
moment that the unloading of SUT completes since the advent of a testing request) from 20 runs. The time speciﬁed at each stage is the elapsed time since
OTTestManager received the testing request. We speculate the costs involved
to carry out three functions: (1) coordinating SOTF tasks (OTTestManager,
OTRecordDrv, and OTReconstructDrv) and transferring information between
two diﬀerent application domains, (2) conducting checkpointing and recovery,
and (3) starting and unloading software at runtime.
The cost in function (1) attributes to the coordination of SOTF tasks in
diﬀerent application domains. For instance, the OTTestManager task, receiving an event for testing, informs OTRecordDrv to prepare for testing. What is
carried out in this step is that one task ﬁres an event to wake up a dormant
thread, and the data speciﬁcation for testing is transferred from one application
domain (where OTTestManager runs) to the other application domain (where

738

O. Goh and Y.-H. Lee

OTRecordDrv runs). To enable the communication between tasks in diﬀerent application domains, CLI’s AutoResetEvent class and AppDomain class are used.
The result, leading to about 10ms delay, which is quite expensive and mostly
comes from marshaling (to pass objects between the domains), indicates that
the eﬃcient communication mechanism between diﬀerent domains is desired.
Checkpointing and recovery operations take 13ms ((4)-(3)), and 8ms ((8)-(7)),
respectively. The size of ﬁnal persisted data, including metadata for the serialization protocol, is 33125Bytes. Compared to the experiment results (186ms and
69ms for serialization and deserialization respectively) by standard serialization
library, the schedulable persistence system (SP system) in [7] substantially outperforms the standard serialization class libraries.
In Table 1, we also notice the cost for starting and unloading testing software
in a new domain at runtime. The operations are implemented using AppDomain
class’s CreateDomain, ExecuteAssembly, and Unload methods. The delay reaches
to 657ms((6)-(5)) to start a new software, and 65ms((12)-(11)) to unload the
software, respectively. This noticeable delays comes from loading and compiling
not only user classes of the new assembly but also a large portion of system
classes referenced by the user classes.
5.2

Scheduling Online Testing and Space Overhead

In SOTF, it is imperative that the timeliness of applications (SUO and other
real-time tasks) has to be guaranteed while testing is in progress. The simplest
scheduling approach is to treat the testing as a background job so that the testing
operations would have a minimal interference to the applications’ timeliness. One
of the concerns with the background testing job is the nonpreemptible regions
caused by SOTF. The other concerns is the overhead of space that is reserved
to save the logs. Logs produced by SUO have to remain until they are consumed
by SUT. The issue of space overhead also attributes to the characteristic of
embedded software testing, which requires to be exposed to the physical world
for a long period. Thus, it can encounter all possible input data sets. Here, we
consider a schedule example and use it to examine the space overhead in a testing
process.
In this experiment, the SUO (a seismic event monitor) and its corresponding
SUT (an upgrade version of the seismic event monitor) are same with ones in the
previous experiment so that the size of persistent data for checkpointing/recovTable 2. Task Sets (CU: CPU Utilization, time unit: ms)
CU

T1

T2

T3

T4

0.5

0.5/5

1.6/8

1/10

1.5/15

0.6

0.5/5

1.6/8

2/10

1.5/15

0.75

1/5

2/8

2/10

1.5/15

Priority AboveNormal Normal Normal BelowNormal

Schedulable Online Testing Framework

739

ery, and the size of each log are same with the previous experiment. Additionally, service launched in an operation partition includes three more tasks besides
SUO; that is, an operation partition consists of four tasks. Table 2 speciﬁes
three diﬀerent task sets, and their scheduling parameters according to varying
CPU utilization (CU), 0.5, 0.6, and 0.75. The table also speciﬁes WCET, period, and priority of each task, which runs periodically; for example, T1 in the
CPU utilization set 0.5 has 0.5ms WCET and 5ms period, and runs with the
AboveNormal priority. Among the tasks, T1 is SUO which has a correspondent
SUT. SUT, which is not speciﬁed as a task in the table because it does not
account for CPU utilization, runs as a background task (with the Lowest priority). Checkpointing and recovery are carried out in a preemptible mode with
the Highest priority–3ms period, and 2ms WCET. That is, when checkpointing
or recovery is initiated, it runs 2ms every 3ms until it completes the requested
service. In this experiment, we focus on understanding space overhead during
online testing so that we ignore the testing overhead including checkpointing
and recovery although it aﬀects the schedulability of the task sets. To ease the
termination condition of testing, the testing duration is limited to 300 periods
of SUO operation.
12000
CU 0.50
CU 0.60
CU 0.75

Size of Log Data (Bytes)

10000

8000

6000

4000

2000

0

200

400

600

800
1000
1200
1400
1600
Time elapsed since logging starts (ms)

1800

2000

2200

Fig. 3. Space for logs according to varying load

Figure 3 shows the space size of log data for three task sets over time until testing on SUT completes since logging on SUO started. According to the
graph, the execution of SUT, conducting testing by actually consuming the logs,
starts at around 1122ms, 1257ms, 1342ms for CU 0.5, CU 0.6, and CU 0.75,
respectively. This start time is inﬂuenced from higher priority tasks’ loads and
also the overhead of online testing: as we see in the second experiment, SUT
can start once the completion of checkpointing by SUO, the launch of SUT
by OTTestManager, and the completion of recovery by SUT are accomplished,
which take approximately 700ms. Regarding the space for logs, if the duration of
SUO logging (producing logs) is not overlapped with the duration of SUT testing
(consuming logs), the space for logs including metadata is 16500Bytes. In fact,

740

O. Goh and Y.-H. Lee

the result shows that the maximum space size for logs reaches to 9407Bytes,
10727Bytes, 11057bytes for CU 0.5, CU 0.6, and CU 0.75, respectively. The experiment shows that the space for logs reaches to the maximum during the initial
stage of testing; once testing by SUT starts by consuming the logs, the space
required for logs becomes less than during the initial stage. It indicates that
testing can be conducted for long duration without a severe burden of space if
the system can guarantee the maximum space needed in the initial stage.
Besides the space issue, conducting checkpointing and recovery in a preemptible mode shows that it can keep the maximum pause time 2ms and then
their response times become 18ms, and 11ms on average, respectively, due to the
execution of interleaved mutators. It indicates that checkpointing and recovery
do not stop application tasks for 13ms and 8ms as its nonpreemptible mode of
the second experiment.
Conclusively, predicting the upper bound of memory space reserved for logs
has to consider the cost and overhead of online testing and the workload of
applications.

6

Conclusion

In this paper, we depict a VM-based schedulable online testing framework for
testing software upgrade of real-time embedded applications. The testing can undergo with actual input data in a target runtime environment. The framework
is built by integrating a wide range of mechanisms of VM, including an isolated partition for testing, preemptible checkpointing/recovery, and logging/reconstructing the sampled input data. Meanwhile, in order to prevent the testing
from causing adverse eﬀects on the ongoing regular services of the target systems, the testing task runs in the background mode and read in sampled data
via a log buﬀer. The experiment with the prototype of the framework, developed
on MONO, demonstrates the feasibility of online testing in VM environment as
well as the required capacity of the log buﬀer.

References
1. Barga, R., Chen, S., Lomet, D.: Improving logging and recovery performance in
phoenix/app. ICDE 00, 486 (2004)
2. Barga, R., Lomet, D., Paparizos, S., Yu, H., Chandrasekaran, S.: Persistent applications via automatic recovery. IDEAS 00, 258–267 (2003)
3. Box, D., Sells, C.: Essential.NET vol. 1: The Common Language Runtime, 1st edn.
Addison-Wesley, Reading (2002)
4. Dmitriev, M.: The ﬁrst experience of class evolution support in PJama. In: Proc.
of The 8th International Workshop on Persistent Object Systems (POS-8) and
The 3rd International Workshop on Persistence and Java (PJW3), pp. 279–296.
Morgan Kaufmann Publishers, San Francisco (1998)
5. ECMA. Ecma-335 common language infrastructure (2002)
6. (Mootaz) Elnozahy, E.N., Alvisi, L., Wang, Y.-M., Johnson, D.B.: A survey of
rollback-recovery protocols in message-passing systems. ACM Computing Surveys 34(3), 375–408 (2002)

Schedulable Online Testing Framework

741

7. Goh, O., Lee, Y.-H., Kaakani, Z., Rachlin, E.: Schedulable persistence system for
real-time embedded applications in VM. In: EMSOFT, pp. 101–108 (2006)
8. Lee, K., Sha, L.: Process resurrection: A fast recovery mechanism for real-time
embedded systems. In: Real Time and Embedded Technology and Applications
Symposium, pp. 292–301 (2005)
9. Lee, K., Sha, L.: A dependable online testing and upgrade architecture for real-time
embedded systems. In: RTCSA, pp. 160–165 (2005)
10. Lindholm, T., Yellin, F.: The Java Virtual Machine Speciﬁcation, 2nd edn.
Addison-Wesley, Reading (1999)
11. Liu, C., Richardson, D.J.: RAIC: Architecting dependable systems through redundancy and just-in-time testing. In: ICSE 2002 Workshop on Architecting Dependable Systems (2002)
12. Malabarba, S., Pandey, R., Gragg, J., Barr, E., Fritz Barnes, J.: Runtime support
for type-safe dynamic Java classes. In: Bertino, E. (ed.) ECOOP 2000. LNCS,
vol. 1850, pp. 337–361. Springer, Heidelberg (2000)
13. Sha, L.: Using simplicity to control complexity. IEEE Software 18(4), 20–28 (2001)
14. Soules, C.A.N., Appavoo, J., Hui, K., Wisniewski, R.W., Da Silva, D., Ganger,
G.R., Krieger, O., Stumm, M., Auslander, M.A., Ostrowski, M., Rosenburg, B.S.,
Xenidis, J.: System support for online reconﬁguration. In: USENIX Annual Technical Conference, General Track, pp. 141–154 (2003)
15. Stankovic, J.A.: Misconceptions about real-time computing: a serious problem for
next generation systems. Computer Magazine, 10–19 (1988)
16. TimeSys Corporation. Timesys linux/real-time user’s guide, version 2.0 (2004)
17. Ximian. MONO, http://www.go-mono.com

692

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL.

SE-10,

NO. 6, NOVEMBER 1984

Evaluation of Error Recovery Blocks Used for
Cooperating Processes
KANG G. SHIN, SENIOR MEMBER, IEEE, AND YANN-HANG LEE

Abstract-Three alternatives for implementing recovery blocks (RB's) dell [2], has been widely used for backward error recovery.
are conceivable for backward error recovery in concurrent processing. It is a sequential program structure that consists of an accepThese are the asynchronous, synchronous, and the pseudorecovery point tance test(AT), a recovery point(RP), and alternative algorithms
implementations.
ten process.
poess
a nd aterative
ress. A erprocess
Asynchronous RB's are based on the concept of maximum autonomy fora a given
saves its state
at a recovery poit
point
in each of concurrent processes. Consequently, establishment of RB's and then enters a recovery region. At the end of a recovery
in a process is made independently of others and unbounded rollback block, the acceptance test is excuted to check correctness of
propagations become a serious problem.
the computation results. In case an error is detected during
In order to completely avoid unbounded rollback propagations, it is the normal execution or the computation results
fail to pass
necessary to synchronize the establishment of recovery blocks in all
cooperating processes. Process autonomy is sacrificed and processes are the acceptance test, the process rolls back to an old state saved
forced to wait for commitments from others to establish a recovery line, at the previous RP and executes one of the other alternatives.
leading to inefficiency in time utilization.
This procedure will be repeated until the computation results
As a compromise between asynchronous and synchronous RB's we pass the acceptance test or all alternative algorithms are expropose to insert pseudorecovery points (PRP's) so that unbounded roll- hausted. The latter implies the system's inability for
tolerating
back propagations may be avoided while maintaining process autonomy.
We developed probabilistic models for analyzing these three methods fault(s) whereas the former means the system's capability for
under standard assumptions in computer performance analysis, i.e., fault-tolerance.
exponential distributions for related random variables. With these
Unfortunately, for cooperating concurrent processes the rollmodels we have estimated 1) the interval between two successive recoverY back of a process may cause other processes to roll back (this
lines for asynchronous RB's, 2) mean loss in computation power for phenomenon is called rollback propagation) because of interp

the synchronized method, and 3) additional overhead and rollback dis-

.

Manuscript received March 7, 1983; revised October 7, 1983. This
work was supported in part by the National Aeronautics and Space

process ddependencies and imperfect checking of global correctness. Moreover, rollback may propagate to further RP's since
recovery points of individual processes may not provide globally
consistent states for all processes involved. This rollback propagation continues until it reaches a recovery line at which globally consistent states for all involved processes do exist. In the
worst case, an avalanche of rollback propagation, called the
domino effect, can push the processes back to their beginnings,
thus resulting in loss of the entire computation done prior to
the occurrence of error.
A detailed description of the domino effect can be found in
[3]. For convenience in visualizing rollback propagations let
us consider Fig. 1. Process P1 begins to roll back because of
unsuccessful acceptance test AT'. Due to interprocess dependencies the rollback of P1 propagates to the other two processes
P2 and P3. Eventually, the whole system has to restart from
recovery line RL2, undoing the entire computation between
RL2 and AT'. The time interval between the restart point
following an error recovery and the time point at which an
error is detected or the acceptance test fails, called the rollback
distance, can be used to represent the computation loss in rollback error recovery. The rollback distance may be unbounded
in the case of the domino effect.
The domino effect is the major obstacle in implementing the
recovery block scheme for concurrent processing. The process
designer is able to predict neither the time of the occurrence
of process interactions nor that of the appearance of recovery

Computer Science, University of Michigan, Ann Arbor, MI 48109.

points and acceptance tests without considering process charac-

taceincae
RPs are
reusd
in case PRP's
tance
used.

Index Terms-Backward error recovery, conversation scheme, domino
effect, pseudorecovery points and lines(s), recovery block(s), recovery
line(s), rollback propagations.

I.

INTRODUCTION
T HE increasing computation power and rapidly falling cost
of microprocessors and memories have given an impetus
to the development of distributed computing systems. Potential
benefits to be gained through distributed architectures include
enhanced system throughput, structural flexibility, reliability,
and availability. However, there are several issues to be resolved
before the full potential of a distributed processing system can
be realized in practice. For example, the multiplicity of physical and logical system components apparently seem to improve
reliability, but this becomes less obvious when we consider the
issues involved with system reconfiguration, error, recovery,
and detection in a distributed environment. In this paper, we
consider one such issue: the effectiveness of implementing recovery blocks(RB's) in backward error recovery for a set of cooperating processes.
The recovery block (RB), proposed by Horning [1] and RanAdministration under Grant NAG 1-296.
The authors are with the Department of Electrical Engineering and

lines. In addition, it is not desirable to randomly place recovery

0098-5589/84/1 100-0692$O1 .00 C) 1984 IEEE

SHIN AND LEE: ERROR RECOVERY BLOCKS

pI
time l

P2

RP2

|

RP1'

/- - -

RLI --, N--{----

-_

I
xI

Rtz__e
-- __o

2~

l

(

In this paper we propose to employ pseudorecovery points'
(PRP's) to alleviate the rollback propagation problem by allowing a process to restart at a PRP when the process is forced to
_.__;, '---roll back by others as a result of rollback propagation. Hence,
" RP9
we can classify these refinements into two categories, syn"- { o s.-- F-chronized recovery blocks and pseudorecovery points,providing
l
a contrast with the third category called asynchronous recovery
blocks.
To implement a rollback error recovery scheme, we have to
weigh tradeoffs between these three categories and the characteristics of concurrent processes. A satisfactory scheme should
have
(acceptable) delay in process comXo suchduefestures as a low
RP>{
x,
~~~~~pletion torollbacks, the preservation of process autonomy
m l~RP
in concurrent processing, anldprogrammer transparency. Thereoptimal solutions may be a combination of these three
~~~~~~~~~~fore,
categories. A quantitative analysis has to precede any ofsuch
RPs
optimal solutions. For example, it is necessary to determine

Ps

lRPI

ll

693

{

RL2lll
{

\xRP9

R

the

RP'3

AT'-

mean

amount

of computation undone when

processes

roll

back, the optimal interval between two successive synchroniza-

P1 failsinteraction
atAT~

Fig. 1. A history diagram of occurrence of interactions and recovery

points.

teristics. Thus, it is impossible to avoid the domio effect only
by appropriate placement of recovery blocks and it is possible
to have a disaster such as unbounded rollback propagation, a
large rollback distance, and a great number of largely useless
recovery points occupying large amounts of memory space, etc.
Furthermore, detection of rollback propagation and determination of recovery lines will become more complex though they
can be made in a centralized [4], [5] or decentralized manner
[6] -[8] .

tions, the mean size of memory space required to save states,
|etc. However, because the program behavior is unknown and
its execution proceeds stochastically, accurate modeling is in
general very difficult if not impossible.
In this paper, employing standard assumptions in computer
performance analysis, we develop a model to quantitatively
describe the characteristics of rollback recovery schemes as well
as their effectiveness. In the following section, several assumptions are discussed and then a model for asynchronous recovery
blocks is introduced. Using this model, we employ simulation
to present the probability distribution of the interval between
two successive recovery lines and the mean number of states
recorded during that interval. In Sections III and IV, the synchronization method and the implantation of pseudorecovery
points are discussed and evaluated, respectively. The paper
concludes with Section V.
II. EVALUATION OF ASYNCHRONOUS
RECOVERY BLOCKS

Several refinements have been proposed to overcome the
drawbacks in the recovery block scheme. One approach is to
put concurrent processes into a controlled scope, either to synchronize the occurrence of acceptance tests or to direct process
interactions. For the former, Randell [2] has suggested the
conversation scheme which requests every cooperating concurrent process to leave its acceptance test at the same moment
(called test line). He has also proposed a language structure
in an abstract form for the conversation scheme. Other mechanizations of the conversation scheme on the basis of the same
concept but with more flexibility have been devised by Kim
[9] . Synchronized rollback recovery schemes for transactions
using a two-phase commitment protocol or transaction ordering
are also studied in [10] -[12] . Russell has proposed that information be retained for directed interactions from producers to

Let us consider the history diagram in Fig. 1 to illustrate the
activities of cooperating concurrent processes Pi,i = 1 ,2, -* * n.
Process Pi establishes its jth recovery point RPi without synchronizing with other processes. Interprocess communications
are represented by arrowed horizontal lines. Let set A C
{1,D * * , n} be a subset of cooperating process indexes. Then
one may find a combination of RPW for all i CA, which forms
a recovery line for set A, denoted as RL, for the rth recovery
line. For simplicity, superscripts in representing recovery lines
will be omitted in the sequel as long as that does not result in
ambiguity. The interval between two successive recovery lines
RLr and RLr+1 in process Pi is a random variable and denoted
by Xl. Since a recovery line provides globally consistent states

consumers so that rollback propagation can be blocked [13],

1We call it a pseudorecovery point(PRP) since there is no acceptance

the occurrence of interactions; for example ,the branch recovery
point [151 and the system defined checkpoint (SDCP) [16] .

a failed process. But PRP's can be used to prevent rollback propagations
due to interactions with the faulty process as we shall see in Section IV.

[14] . Another approach is to save additional states based on PRP's
test before
the saving of process state at a PRP. The states recorded at
may have been contaminated and thus cannot be used to recover

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL.

694

SE-10, NO. 6, NOVEMBER 1984

to all members of process set A, it is reasonable to assume that B. A Model for Asynchronous Recovery Blocks
Xl is stochastically identical for all i E A. Thus, Xr is used to
Since individual recovery points by themselves may not be
represent the interval between the rth and (r + 1)th recovery
sufficient in rollback recovery due to the possibiltiy of rollback
lines.
propagations, we consider in this paper only the formation of
recovery lines for asynchronous recovery blocks instead of
A. Modeling Assumptions
The requirements of a reefollowing assumptions in our subsequent separate individual recovery points.
for processes Pi i = 1, 2,- , n can be stated as
line
macovery
flos
analyses.
reis
autonomy
Processes:
Cooperative
Autonomous
A])
1) A recovery line has to include one recovery point RP! for
pro.
ro
isriutd
most important requirement in distributed
gardeas sthe
P
garded
cessing. Each process should be executed according to its own each process Pi.
2) Let the moment of establishment of the jth recovery
program and environment, almost as if no other process existed.
in PI be t [RI?]] and let tq be the moment of the qth
point
as
of
others
In actuality, a process may execute independently
from Pi to Pi . For every pair (RP,, RP11') in a
reinteraction
shared
long as there is no conflict with others in accessing
th.ere does not exist an integer k such that tk E
line,
recovery
sources. Since synchronization is not enforced in this category
I if t[I] It[RP]'], or tk E [t[RPJ9],
t[RPN]
of recovery blocks (i.e., asynchronous recovery blocks), pro- Iit[RP]],
This implies that no interaction from Pi
]
otherwise.
cesses will transmit messages or establish their recovery points t [RPfl
can be sandwiched between t[RPJ] and
versa)
P, (and vice
independently of other processes. to
A2) Perfect Local Acceptance Test: Acceptance tests should t [RP ]
The basic idea underlying the model is to trace the occurrence
detect all errors within the local process during the execution
of recovery blocks and thus ensure the correctness of local of both recovery points and interactions. Based on the assumpexecution. It is in general difficult to guarantee complete cor- tions in Section 1I-A, random variable Xr can be modeled by a
rectness, but at least the computation results that have passed continuous-time Markov process starting from a recovery line
the acceptance test should be "acceptable" [3]. The local (RLr) and ending at the next recovery line (RLr+i). For a
EA} where A = {1, 2, *,
acceptance test may or may not detect external errors or erro- set of processes, Q.A =
defined:
are
states
of
types
two
of
the
aware
not
is
process
local
the
neous messages because
a) End states Sr and Sr, : transitions start from Sr where
global system and other processes.
processes have formed the rth recovery line, and end at
all
A3) Probability Distribution of Interactions: Usually, proin
Sr+
which
1 upon establishment of the (r + l)th recovery line.
cess behavior is modeled as an ordered sequence,
Intermediate states S = (x1, x2, -* , xn) , where xi = 0
b)
executhe
on
turn is specified by the program and is dependent
the
if
previous action of Pi was an interaction, and xi = 1 if it
the
tion condition. Even if the processing sequence is given,
of a recovery point.
establishment
was
to
interval between two successive interactions is variable due
and recovery points in a process
interactions
of
Occurrences
conditional branches. Locking and waiting at shared resources
the
system go through these states. Note that both Sr
make it even more uncertain. Nonetheless, the interaction inter- make
val can be modeled by adopting the two assumptions commonly and Sr+1 are equivalent to state (1, 1,.*.*, 1). We can establish
used in the analysis of multiprocessors and computer networks: the following transition rules:
1) constant reference rates in the multiprocessor and 2) expo- RI) The system goes to state (x1, , xil, 0, xi+ 1, * ,
nentially distributed intervals between two successive message x,) from state (xl, , xi ,, 1, xi+ 1, . , xn) with rate Pi
transmissions in the computer network. The interval for two upon establishment of a recovery point in Pi.
,
successive interactions between Pi and P1 is thus assumed to be R2) The system leaves state (xl, . . . ,X- 1, x1.1,
.
.
.
0,
,
,
, x,) and enters state (x,1 ,
exponentially distributed with mean I/Xij and Xiq = Xii for all x ,1, 1, xi+I ,
an
is
there
if
rate
with
*
,
X1,
-,
xn)
*
,
0,
xi,
I
and
i
,n
1,2,
xi+1,
j.
xi
i,j=
A4) Consistent Communications: Let two messages ma and interaction between P7 and Pj.
, O, xi+ , *
* *
mb be sent from Pi to Pi. Consistent communications should R3) The system arrives at state (xl,...
, xn) with transition
satisfy: 1) every message sent from Pi to Pi will be received xn) from state (x1, . .x. _ , 1, xi + ,
j eA}.
and
i
=
=
where
Bi
O,j
rate
the
in
#
YjfE
{jIxj
Xiq
Bi
received
are
by
Pj
eventually by Pi, and 2) ma and mb
state Sr to state
from
transfer
can
directly
The
R4)
system
some
packetin
that
Notice
sent.
are
that
as
they
order
same
=
rate
Pk
transition
with
rebe
to
2%
Sr+
allowed
1
are
messages
networks,
computer
switched
Under these transition rules a Markov model is developed for
ceived by the destination out of order. However, the order can
be kept easily, for example, by time-stamping messages at the three processes P1,, P2, and P3, and presented in Fig. 2. The
time of transmission. single-arrow lines are unidirectional transitions. The doubleAS) Distribution Of Recovery Points: Because of process arrow lines are bidirectional transitions in which left-hand side
independence and the uncertainty of execution conditions, parameters represent leftward transition rates and right-hand
the appearances of recovery points are random and difficult side parameters rightward transition rates. The total number
to model. To avoid complexity, establishment of recovery of states for a set of n processes is 2"~+ 1. This implies a quick
points in a process is assumed to be an independent Poisson expansion of state space as n increases, e.g., for 10 cooperating
processes there are 1025 states.
process with parameter ,u1i for Pi.
Wemak

Wnalyses.

processqirmet
eachsti

{PiIi

-

SHIN AND LEE: ERROR RECOVERY BLOCKS

695

to Srf

entry

------

S

S-t/
+

+

t

t'

\t~~~~23@
/2

/

from
\a Sr

12

13S+

23@F

'

tI2*A2

to state (0,0.0)
Fig. 2. The model of asynchronous RB's for three processes.

entry___

=ny~n

n-(n -1) X

2 where m =22n be the set of states of the preceding
when 1i -y = ,u and Xkey = X for all i, E A. In this case the * n,
m}
reduction is achievable since all intermediate states S = (xl , x2, continuous-time Markov process with the following convention
*.**, x,) containing exactly u l's in (xl, x2, , x,) can be for numbering states:
replaced
single state S" where u =0, 1, 2, *-*, n- 1. A
simplifiedbyora reduced
model .is obtained
under the following
a) SrpcstateO
.
b)
simplified
an intermediate state (x1, x2,t-o
,
state (Edng
xs)so
transition rules and presented ln Fig. 3.
i-1
R1') For un ,a1,n , n - l, the system will move to state xf2ub + 1), and
repacefrom state 3u with transition rate (n - u) , when a new c) Sr -+ state m.
recovery point is formed.
Then, the Chapman-Kolmogorov equation becomes
R2') For all u 2, the system is able to leave state Ifor
stateuI 2 with rate (u(u - t 1) i)/2. rate
urwh)H
=
(1)
R3') For all u > , there is a transition from state tostate
dt
st S with rate (u
- u) X
where H is the ((m + 1) X (m + 1)) transition matrix [h(u, v)]
R4-) sytem cn t f i y The sytmcntase
h
r a
which the (u, v) element is the transition rate from state u
Sr to the terminal state Sr+ I with transition rate nlL
to state v, and 7r(t) is a vector whose kth element is the probaC. The Analysis of Asynchronous Recovery Blocks
bility that the system is in state k at time t. The initial condition
is iT(0)== [1
c
[ O v * * * 0] Theintervalbetweentwo sucWith the model developed above, we can characterize the
lines X
cessive
recovery
to the time needed for transito
rmsae0t
tt is equal
n hrfr,tedniyfnto
behavior of asynchronous recoverybehavior
ofasncroou.rcoerbe
blocks in terms of the degree
of interprocess communications and the distribution of recovery ofXnmeyf(
points. With the exponentially distributed interprocess com- ofX aeyf(), is given by
munications and recovery points, Xr becomes stochastically d
identical for all r. Let X denote a random variable representing
fx(t) = -t lri(t).
(2)
the interval between two successive recovery lines, Li the number of states saved in process P1 during interval X. The proba- 2) The mean value of Li: Since we are only concerned with
bility distribution of X and the mean value of Li are derived the number of recovery points established by process Pi during
below.
interval X, a discrete Markov chain is used.- To compute the
-

=

the

R4.

ietlrmtein

-

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-10, NO. 6, NOVEMBER 1984

696

from state

So

23

XI

S

Alz+ils

from & to Si

\
\A

X5

~~~23

_from S6 and So

~~~~~to se

2

to S4'

from

S4

and S4`

Fig. 4. The construction of states S' and S5 of discrete chain Yd.

mean value of Li,2 a new Markov chain, denoted by Yd,is conC
structed based on the previous model with the following two
steps.
P=1./
Si) Convert the previous model to a discrete model: The
r
new chain Yd has the same states as the previous Markov proXE + Sk Pk be the normalizacess. Let G 1=
p
=12
tion factor. The transition probability from state u to state v
,m, p(u, v) = h(u, v)/G &/
in Yd is equal to: for u, v = 0, 1,
uP(U,V)/
if ut=v,andp(u,u)= 1 - In
cc
S2) Decompose states of discrete model: Arrivals at a state
/
xi,'
Su = (xI, x2
xn) where xi= 1 can be grouped
z
of
|
into two classes. One is formed as a result the occurrences
of RP's in Pi and the other is formed as a result of interprocess
communications and establishments of RP's in processes other
other thanPi. Accordingly,the state Su = (xI, x2,-** xi~,5
j=0
xn) with xi 1 can be split into two states S' and SU, representing the two classes, respectively. Both states have the same
°
departure processes as that of S,. However, all arrivals at state
(n)
6.00
Su due to the occurrence of recovery points in Pi enter state
2.00 NUM1BE1 OF
p=,t
Su whereas all other transitions are made to Su". Hence the
t=lj=ljti k=1
S' is represented by that
number of RP's associated with state
~~~~~~~
.........
ti A=A ror all ij and Al=/,=2= =,A=1.0
of arrivals at S'.
Fig. 5. Mean value of X versus the number of processes.
Fig. 4 shows the conversion and the split of state S2 = (1 , 0, 0)
of the Markov model for the three concurrent processes in Fig. 2.
With the new discrete model, Yd, we can calculate the mean random variables,
p
0
which i.e.,
the reati
raio betweo i the
repeDL, Let
denoted as Ns, and the mean random
number of visits to state Su,u
h
ewe
ealenai
h
ereet
U)wc
(j~~~~~u
value of Li using the following relationship:
density of interprocess communications and recovery point
(3) establishments. In Fig. 5, the mean values of X are plotted as
E(L1) = E E(Ns)
Su E 'I'
a function of n for different values of p. It shows that X increases
drastically when there is an increase in the number of
where T is the state space of Yd.
processes
involved in the rollback recovery. The denisty funcor
fails
the
acceptance
an
error
Suppose process Pi detects
/
tion
of
X,
fx(t), is plotted in Fig. 6. For all the three cases in
=
1,
2,..
,
L1.
RPJ1
where
recovery
points
test at one of its
*
6,
there
is a sharp pulse near t = 0, which is due to direct
in
the
proFig.
to
k
processes
may
propagate
rollback
of
Pi
T'he
cess set, Q2A = {P111 eA} where A = {1, 2,...* ,n}. Let Dr be transitions between Sr and 5r+i and a longer transition time
the rollback distance associated with the k processes and RW, needed once the system enters intermediate states.
With a fixed value of p and varying values of ,u's and X's for
for j=-1,2,* , L1. Then, X represents the supremum ofthese
three processes, we have performed computer simulation and

PROCASES

ispossible tohaveE(X) O, butnorecoverypoint withina
recovery line interval. Therefore, E(L1) #E(X)/MuI.

-the results are tabulated in Table I. The minima of X and L

occur when the distribution of recovery points among these

SHIN AND LEE: ERROR RECOVERY BLOCKS
_

case 1:

case 2:
case 3:

697

8
(Ai,tP,isP)=( o10,100)
(Xi2,Azs,Aj3)=(1,010 10)
(/si,A b,sA)=(0.60,45,0-45), (Xi2.,A,I3)=(0,5.0.5,0.5)
(Aj,A2,As)=(0.6,0.45,0 45), (XA,2,A"A3)=(0.75,0.75,0.75)

~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~

__me

synchronization

request

\ case 1

*

~

~~

P2

P 1X-ready

o

LL

P33-rsady

P22-re

FL)

z

LL

Z
w

case 3

case 2

8L

,

0.40
"4.0
°o.x

-,-

~~~~~TIME(NORMARLIZED)
1I.20RMLIO

,

synchronization

,
2.00
2.

1.60

request

1/

-

Fig. 6. The density function of X, fx(t).

case

E(X)
E (L )

E(L2)

(1.0,1.0,1.0)

2
(1.5,1.0,0.5)

2.598
2.500

3.357
4.847

1

3

3.231
9.693

P3|-pea4nt
_

4

(1.0,1.0,1.0)

(1.5,1.0,0.5)

(1.5,1.0,0.5)

2.600
2.453

4.533

3.203

3.354
4.967
3

2.453
7.360

s|

Pr

(1.0,1.0,1.0) (1.0,1.0,1.0) (1.5,0.5,1.0) (1.5,0.5,1.0) (0.5,1.5,1.0)

2.500
E(L,+L_+L_) _ 7.500

2-edZ

__
_
[
t;eadP3s-reaD

TABLE I
MEAN VALUES OF X AND Li FOR CONSTANT p
(A1,iP,,zs)
(A12,X23,x13)

P

3.022

9.065

111
9.933

processes is uniformly balanced (i.e., p, =2 = /3). The distribution of interprocess communications does play an important role in determining the probability of rollback propagation
but has little effect on X and Li once the set of processes involved in rollback recovery is determined.
III. SYNCHRONIZED RECOVERY BLOCKS

Fig. 7. Establishment of recovery lines upon synchronized requests.

wait for commitments (for establishing a recovery line) from
other processes before it executes an acceptance test. Thus,
all cooperating processes perform their acceptance tests at the
same instant upon receiving the commitments from all other
processes. Let P11-ready be the flags in process Pi to indicate
commitments from P. for j = 1, 2,
,n. The steps for synchronization in each process Pi are described as follows:

1) execute "its own normal process" until "acceptance test;"
2) set Pi ready := ON and then broadcast Pii-ready;

3) while not (all Pi-ready = ON) do
The simplest way of avoiding unbounded rollback propagareceive messages;
tion is to synchronize the establishment of recovery points
if a message
is iP. -ready t1
then set Pi -ready := ON
e
during process execution. In this method, interactions are inelse record the message
hibited between any pair of processes during their establishment
of recovery points. There are three conceivable strategies in
4) do "acceptance test" and record process states.
deciding when a synchronization request is to be issued: 1) at
Establishment of recovery lines upon synchronization rea constant interval, denoted as Ts; 2) when the time elapsed quests is shown in Fig. 7. Synchronization causes the computasince the previous recovery line exceeds a specified value, Ts; tion power to be reduced because processes have to wait for
or 3) when the number of states saved after the previous re- commitments (as in Step 3) from other processes. And, procovery line is larger than a prespecified number Ms. The im- cess autonomy-a principal characteristic of distributed complementation of the first strategy is simple since the synchron- puting systems-is sacrificed. Ley yi be the interval between
ization request is issued without any knowledge of the state of the receiving of a synchronization request and the moment
execution. Nevertheless, some synchronization requests may that process Pi reaches its next acceptance test (in Step 1).
become redundant and unnecessary if they are issued immedi- Then, according to the assumptions in Section II-A, yj is an
atedly after the formation of recovery lines. For the second exponentially distributed random variable with parameter jit.
and third strategies, the rollback distance and the number of Let Z = max {yo, Y2, - , Yn}. The total loss in computasaved states are prevented from becoming too large. However, tion power is CL = St. (Z - y1). The mean loss becomes
for these two strategies, additional overhead will be required
because each process must be aware of the occurrence of arn
1
recovery line. Note that the conversation scheme is a special
CL = n J (1 - Fr(t)) dt (4)
j1
case of the third strategy where Ms = 1.0
Upon the receipt of a synchronization request, every process where Fz(t) is the distribution function of Z and equals '11= has to prepare for establishing a recovery line and also has to (1 - eMlit).

698

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL.

The time interval between two successive recovery lines is a
function of the strategy used for issuing synchronization requests as well as characteristics of the processes involved (e.g.,
patterns of interprocess communications and RP establishments). Let Z' and Z" be random variables having the same
distribution as Z = max {Y1, Y 2,
Y n }; then the maximum
value of this time interval becomes m7Ts + Z' - Z" where m is
the smallest integer i such that Z" < iTs, or Ts + Z'. Observe
that Z' and Z" represent the amount of time required for a
process to be ready for establishing an RP after it received a
synchronization request. For the third strategy, the maximum
number of rollback steps is Ms. Thus, the supremum of this
time interval can be expressed as max Iz1, Z2, * * , z,,} where
Zi= Mnl Yi.

SE-10, NO. 6, NOVEMBER 1984

tion from another process P1 (and therefore not local to Pi),
the contents of PRPJl may have already been contaminated if
this error occurred prior to establishing PRPj . The restart from

the pseudorecovery line formed by both RPJ and all PRP, 's
may just reproduce the same error. Therefore, rollback propagation may continue until every process involved has rolled
back to a pseudorecovery line, say PRL', for which all processes but Pj: have passed at least one of their recovery points.
Since there exists an RPJ in Pi, for i' $4 between PRL' and
td, every state belonging to PRL' is now guaranteed to contain correct information of the corresponding process.3 Also,
note that this pseudorecovery line renders the shortest rollback
distance for backward error recovery in case forced synchronization is not used. An algorithm of rollback recovery with these

pseudorecovery points is given by:
IV. IMPLANTATION OF PSEUDORECOVERY POINTS
1 ) If an error is found in process Pi, set p := i where p is a
In the construction of a recovery block, an acceptance test rollback pointer.
consists of a number of executable assessments provided by
2') Pp rolls back to its previous recovery point RPP. All
the programmer, followed by a state saving. Note that process processes PI affected by the rollback of Pp roll back to their
states can also be recorded upon any other requests whenever respective pseudorecovery points
PRP.
they are considered useful in the rollback recovery. A pseudo- 3') For every affected process P' if the rollback has not
recovery point (PRP) is defined as a recovery point that is estab- passed its most recent recovery point, then set p : i' and go
lished without a preceding acceptance test and is proposed back to step 2.
here as an alternative for avoiding the damino effect in a set In Fig. 8, the establishment of PRP's in processes Pl, P2, and
of cooperating concurrent processes. With a monitor as the p3 iS illustrated. WhenP3 fails its acceptancetest AT, allproand
Kim [15] comuictin
and Kant eas,Kmcesses
interprocess communication means,interprocess
have to restart from the pseudorecovery line formed by
Silberschatz [16] discussed methods for implanting recovery (RP 2,
PRPI2, PRP13) if P1 and P2 are affected by the rollback
points in a central manner. Similarly, we consider a method of p3
for implanting PRP's in the set of cooperating concurrent pro- In the above algorithm, we can find that every process needs
cesses in a decentralized manner. Also, note that the use of to preserve a recovery point for restart in case it fails. Also
PRP's does not require any particular interp
(n - 1) pseudorecovery points are needed for a process to form
cations mechanism (e.g., the implementation does not have to a pseudorecovery line with other processes where n is the total
be based on monitors).
number of concurrent processes. It is therefore required to
To make a recovery point RPjl in process Pi maximally useful save n states for every RP, i.e., one RP and (n - 1) PRP's, and
for rollback error recovery, there should -be corresponding re- all old RP's and PRP's except those in the pseudorecovery lines
covery points in the other processes affected by the rollback {PRLj i = 1, * , n, and RP1 is the most recent RP in Pi} can
propagation from Pi. If such recovery points do not actually be purged when a new recovery point is established, thereby
exist,..ist?
for a given
RPj innprcs
process Piiaa pseudorecovery
fo
ienRj
suoeovr point
otreducing storage requirements for saving RP's and PRP's. Note
in
inserted
to
be
PRPJ1I has
process Pi,. Further, in order to that rollback distance iS bounded by the supremum of y I , y2,
w
avoid the need of tracing recovery points at that particular * -Yn} where
yi iS the interval between two successive reo p
c
p
other
in
of
the
each
PRP
established
is
moment, for RPt a
prois (n Theredtisnthtie need
e covery
cesses involved. An algorithm for implanting PRP's is given every
recovery point iS (n - 1) t, where tr iS the time needed
below.
to record the process state. These overheads should be assessed
1) When Pi establishes a recovery point RP, it broadcasts
I' ,against the gain of process autonomy and avoidance of una PRP implantation request to other processes.
bounded rollback propagations.
2) If P. receives the implantation request, it records its state
V. CONCLUSION
as PRPj'1 upon the completion of the current instruction without an acceptance 'test. Then Pi broadcasts the commitment
We have quantitatively evaluated three different recovery
3) Every process executes its own normal task after it estab- blocks employed in backward error recovery for concurrent
lishes RPJ' or PRPj1i. However, the messages sent to a process processing. The recovery block dealt with in this paper is
by P,' prior to C1' have to be retained in the state saved,
defined in software and includes an acceptance test and a state
Assume that process Pi detects an error at time td which is saving. The environment of concurrent processing considered
prior to the establishment of RP i+ - If this error is local to
Pi then the recovery line (called a pseudorecovery line, PRL1j) 31f the state saved at PRPki were contaminated, then the error should
formed by RP, and all PRP1i 's is able to recover these processes have been detected at tAhe subsequent recovery point, RPJi. Meanwhile,
2

beL'
.

i=1

,

n

P

.

point.

o

ci-

even if the error has already propagated to other processes.

the state saved at RPk is correct by the assumption of perfect local

However, when the error detected in Pi is due to error propaga- acceptance test.

SHIN AND LEE: ERROR RECOVERY BLOCKS

IZW~ ~RPI
P2

PI

timej
RPI

implantation request

-PRP9'

Ps

PRPI 9

PRPI2

PRP 21

699

Re

Pon

{_Xt

restart line

with respect to

the failure of

Ps at AT2

PRP2R

aI

,AT32

:Recovery Point (RP)
CM Pseudo Recovery Point (PRP)
Note: all occurrences of interactions are omitted

Fig. 8. Establishment of pseudorecovery points for rollback error
recovery.

here is not restricted to any particular method of interprocess
communications or system structure. However,the occurrence
of recovery points and interprocess interactions are assumed
to follow exponential distributions. This is principally for
tractability: nonexponential interaction patterns would be ex-

tradeoffs between the loss of computation power during normal operation and the increase in response time due to rollback error recovery. For instance, the asynchronous method
or a longer sychronization period is not acceptable for timecritical tasks in which a delay in system response beyond a
certain value, the system deadline, leads to a catastrophic failure. The implantation of pseudorecovery points is also inefficient for concurrent processes when they establish recovery
points frequently (thus requiring many PRP's to be implanted)
and rarely communicate with each other. In general, if more
knowledge of the execution state in concurrent processes can
be obtained, a better strategy for implementing recovery blocks
can be derived.

tremely difficult to analyze if not impossible.
In this paper, we have considered the distribution of the interval between two successive recovery lines instead of the actual
rollback distanice. The rollback distance after an error is detected is related to the probability of error occurrence, error
detection, rollback propagation, etc. However, the interval
X does represent an upper bound for the real rollback distance.
We have also estimated the overhead required to avoid the
domino effect when recovery or pseudorecovery points are
ACKNOWLEDGMENT
employed. For both the synchronization method and the implantation of pseudorecovery points, the overheads are largely The authors are grateful to R. Butler and M. Holt at NASA
related to the construction of synchronization, RP's and PRP s. Langley Research Center for both financial and technical supThey would become unacceptably inefficient when synchron- port and C. M. Krishna at The University of Michigan for techizations and pseudorecovery points are constructed frequently nical discussions.
but interprocess communications do rarely occur. At the other
extreme, i.e., asynchronous recovery blocks, it may result in a
longr rolbac disancedue o unimitd rolbac proagatons

(in place of synchronization and PRP insertion overheads),
To select a suitable strategy or a combination of these three
methods, we have to first
examine the properties of concurrent
.
processes such as the' amount of interprocess communication
and the distribution of recovery points. Then, we weigh the

REFERENCES

[1] J. Horning, H. C. Lauer, P. M. Melliar-Smith, and B. Randell, "A

program structure for error detection and recovery," in Lecture
Notes in Computer Science, vol. 16. New York: Springer-Verlag,
1974, pp. 171-187.
B. Randell, "System structure for software fault tolerance," IEEE
l ~~~~~~~~~~~~21]
Trans. Software Eng., vol. SE-i, no. 2, pp. 220-232, June 1975.
[31 B. Randell, P. A. Lee, and P. C. Treleaven, "Reliability issues in

700

[4]

[51

[61

[71
[8]

[91

[10]

[111
[121
[13]

[14]

[15]

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. SE-10, NO. 6, NOVEMBER 1984

computing system design," Comput. Surveys, vol. 10, no. 2, pp.
123-165, June 1978.
Y. H. Lee and K. G. Shin, "Rollback propagation detection and
performance evaluation of FTMR2M-A fault-tolerant multiprocessor," in Proc. Int. Symp. Computer Architecture, 1982, pp.
171-180.
Y. H. Lee and K. G. Shin, "Design and evaluation of fault-tolerant
multiprocessor using hardware recovery blocks," Comput. Res.
Lab., Dep. Electrical and Computer Eng., Univ. Michigan, 1982,
Tech. Rep. CRL-TR-6-82; see also IEEE Trans. Comput., vol.
C-33, no. 2, pp. 113-124, Feb. 1984.
P. M. Merlin and B. Randell, "State restoration in distributed systems," in Proc. 8th Int. Conf. Fault-Tolerant Computing, 1978,
pp. 129-134.
W. G. Wood, "A decentralized recovery control protocol," in
Proc. Ilth Int. Conf Fault.Tolerant Computing, 1981, pp. 159164.
K. Tsuruoka, A. Kaneko, and Y. Nishihara, "Dynamic recovery
schemes for distributed processes," in Proc. Reliability in Distributed Software and Database Systems, 1981, pp. 124-130.
K. H. Kim, "Approaches to mechanizations of the conversation
scheme based on monitors," IEEE Trans. Software Eng., vol.
SE-8, no. 3, pp. 189-197, May 1982.
J. N. Gray, "Notes on database operating systems," in Operating
Systems: An Advanced Course, R. Bayer et al., Eds. New York:
Springer-Verlag, 1979, pp. 393-481.
W. H. Kohler, "A survey of techniques for synchronization and
recovery in decentralized computer systems," Comput. Surveys,
vol. 13, no. 2, pp. 149-183, June 1981.
G. Ferran, "Distributed checkpointing in a distributed data management system," in Proc. Real Time Systems Symp., 1981, pp.
43-49.
D. L. Russell, "Process backup in producer-consumer systems,"
in Proc. 6th ACM Symp. Operating System Principles, Nov. 1977,
pp. 151-157.
-, "State restoration in systems of communicating processes,"
IEEE Trans. Software Eng., vol. SE-6, no. 2, pp. 183-194, Mar.
1980.
K. H. Kim, "An approach to programmer-transparent coordination of recovering parallel processes and its efficient implementation rules," in Proc. Int. Conf Parallel Processing, 1978, pp.

58-68.
[161 K. Kant and A. Silberschatz, "Error recovery in concurrent processes," in Proc. COMPSAC, 1980, pp. 608-614.

Kang G. Shin (S'75-M'78-SM'83) was born in
the province of Choongbuk, Korea, on October
20, 1946. He received the B.S. degree in electronics engineering from Seoul National University, Seoul, Korea, in 1970, and the M.S. and
Ph.D. degrees in electrical engineering from
Cornell University, Ithaca, NY, in 1976 and
1978, respectively.
From 1970 to 1972 he served in the Korean
Army as an ROTC officer and from 1972 to
1974 he was on the research staff of the Korea
Institute of Science and Technology, Seoul, working on the design of
VHF/UHF communication systems. From 1974 to 1978 he was a
Teaching/Reserach Assistant and then an Instructor in the School of
Electrical Engineering, Cornell University. From 1978 to 1982 he was
an Assistant Professor at Rensselaer Polytechnic Institute, Troy, NY.
He was also a visiting scientist at the U.S. Air Force Flight Dynamics
Laboratory in the summer of 1979 and at Bell Laboratories, Holmdel,
NJ, in the summer of 1980 where his work was concerned with distributed airborne computing and cache memory architecture, respectively.
He also taught short courses for the IBM Computer Science Series in
the area of computer architecture. Since September 1982, he has been
with the Department of Electrical Engineering and Computer Science at
the University of Michigan, Ann Arbor, MI, where he is currently an
Associate Professor. His current teaching and research interests are in
the areas of distributed and fault-tolerant computing, computer architecture, and robotics and automation.
Dr. Shin is a member of the Association for Computing Machinery,
Sigma Xi, and Phi Kappa Phi.

Yann-Hang Lee received the B.S. degree in engineering science and the M.S. degree in electrical
engineering from National Cheng Kung University, Taiwan, R.O.C., in 1973 and 1978,

respectively.
Currently he is working towards the Ph.D.

'

degree in computer, information and control
engineering at the University of Michigan, Ann
Arbor, MI. Hisresearchinterestsincludedistributed computer systems, multiprocessor, performance evaluation, and fault-tolerant computing.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. 15. NO. 10. OCTOBER 1989

1188

Optimal Dynamic Control of Resources in a
Distributed System

Abstrad-The

various advantages of distributed systems can be re-

allzed unly when their resources are ‘*optimally” (in some sense) contrdled and utilI7~d.For example, distributed systems must he recnnfigured dynamically to rope with component failures and workload
changts. Owing to the inherent dilkulty in formulating and solving
re~iurcecontrd problems, the resnurce control strategies currently
propoacdlused for distributed systems are largely ad hoc.
It is our purpose in this paper to I) quantitatively formulate the
problem of controlling resources in a distributed system so as tu optimize a reward function, and 2) derive optimal control strategies using
Markov decision theory. The control variables treated here are quite
generak fnr example, they could be control decisions related to system
configuration, repair, diagnostics, files, or data. Two algnrithms for
resource contnfl In distributed systems are derived for time-invariant
Bnd periodic environments, respectively. A detailed example to demonstrate the power and usefulness of our approach i s provided.
Index Terms-Markov decinhn process. optimization, performability, recnnfigunrtiun, repair, resource control.

I. INTRODUCTION
ECAUSE distributed computer systems are usually
composed of a large number of resources to achieve
higl performance and reliability and can function (albeit
sometimes in a degraded condition) in a very wide variety
of configurations and environments, managing the resources in such systems is much more difficult than managing a conventional uniprocessor machine. Such difliculty has led to an undesirable trend: the resource control
strategies currently proposedlused are largely qualitative
and ad hoc. We counter this trend with the development
of a rigorous, systematic method for “optimally” (to be
defined below) controlling the resources that make up distributed systems. That is, we quantitatively formulate a
resource control problem, for which solution algorithms
are derived using Mar’:ov decision theory.
A resource control Jecision is needed whenever there
is a significant change in either the operdting environment

B

Manuscript reccived November 9. 1987: revised April 3. 1989. Recommended by B. W . Wah. This work was supported in part by NASA
under Grant NAG- 1-296. by the National Science Foundation undcr Grant
NSFDMC-8504971. and by the Florida High Technology and Industry
Council under Gran1 YE07 I .
K . G. Shin is with the Rcnl-Time Computing Laboratory. Departmcnt
or Electricul Engineering and Computer Sciencc. University of‘Michigan,
Ann Arbor. MI 48109.
C . M . Krishna is with Department of Electrical and Computer Engineering. University of Maswchusctth. Amhcmt. MA 0 1003.
Y .-H. LCC i s with the Dcpnnmenr ol‘ Computer and Inl’ornution Sciences. University of Floridr. Gaincsvillc. FL 3261 I .
IEEE Log Number 8930136.

or in the system, e.g., component failures and workload
changes.
Two parameters characterize the operating environment: the reward structure, and the imposed load. The
former needs some elaboration. The operating environment imposes a value on each of the many services it eceives from the computer. Put more formally, there is a
reward (which could be negative) which accrues from
each job execution, and this reward is a function of the
needs of the application. When the operating environment
changes, such a change can be quantified by the change
in the reward structure or failure rates that this causes.
For example, the reward accruing from a transaction-handling machine in a bank is different at peak banking hours
than it is at, say. midnight. Naturally, it would be useful
to be able to optimally configure or service the system as
a function of the prevailing application needs.
Resource control decisions also have to be made when
the computer changes due to component failures. When,
for instance, is it appropriate to summon a repairman?
Which (degraded) configuration should the system switch
to, prior to repair? Or, consider the problem of allocating
channel bandwidth optimally to members of a set of token-ring networks. Each network has a set of users each
of which pays a certain amount of money for a given quality of service (e.g.. waiting time). Additionally, the system response time is a function of the load offered to the
system. How does one allocate bandwidth amongst the
various networks so as to maximize reward (i.e., customer payment)? When systems are simple, such decisions can be made on the basis of intuition alone. When
they get complex, unsupported intuition is insufficient,
and must be supplemented by rigorous methods enabling
a more precise control. This is especially true when the
performance of the system is an intricate function of parameters which may act at cross purposes to one another.
As we shall show in Section 11, resource control decisions can be viewed as semi-Markovian decision processes. The measure of performance used here is based
on Meyer’s performability [I]. It is one of the most powerful application-sensitive metrics available today, and
incorporates both the traditional measures of performance
(e.g.. throughput) and of reliability. Performability formally accounts for the requirements of the application by
defining accotnplishment levels. The vector of probabilities of meeting the accomplishment levels is performabil-

0098-5589/89/ 10O0- 1 188$01.oO O 1989 IEEE

SHIN et ut.: DYNAMIC CONTROL OF RESOURCES

ity. Suppose we identify a reward with each accomplishment level, Then, the expected reward rate can be
obtained from the performability of the system. This reward rate is also defined as reward structure by Furchtgott 121, and as reward function by Donatiello and Iyer
131, [4]. It is this reward rate that we use to characterize
the performance of the system. The average reward received over infinite time horizon is used as an optimization criterion for resource control in a distributed computer system.
The expected total reward accumulated during a mission lifetime, using reward rates as an optimization criterion, has been studied in [5] for configuring degradable
and nonrepairable systems. The results suggest that the
system should perform not only passive reconjguration
to respond the occurrence of a failure, but also active reconfiguration during the course of operation to respond to
changes in the reward or loading structure with a change
in configuration. The problem is formulated as a dynamic
programming problem with a finite horizon. It can be simplified by identifying the relationships between configurations and switch times (the time instants that the system
performs active reconfiguration).
Algorithms from Markov decision theory are applied to
solve the above resource control problem. Note that, despite the importance of the resource control problem and
a voluminous applied statistics literature on decision processes, there is very little in the computing literature on
using the results of decision theory to control computing
resources optimally.
This paper is organized as follows. In Section 11, the
optimal resource control problem is stated formally. In
Section 111, we show how to use the strategy improvement
procedure from decision theory in taking optimal resource control decisions for the case in which the reward
rate is constant. In Section IV, we turn to the case in which
the reward rate is periodic: Sections 111 and IV between
them encompass the majority of fault-tolerant systems. In
Section V we provide a numerical example. We conclude
with Section VI.
11.

PROBLEM FORMULATION

1189

remainder of this section, we formalize and elaborate on
what we have said above.
Suppose our computer system has n units of some resource. A unit of resource is defined to be the smallest
part of the whole system which may fail to operate, and
which can be repaired, reloaded, or replaced. Examples
are processors, memory modules, I/O channels, shared
tables, file units, and data sets. A unit can provide useful
services when it is fault-free and may become unavailable
or invalid in the event of failure or loss of control.
The system state is the aggregation of all states, dewhich is a finite set. The system state at time
noted by
t can be defined as a stochastic process S( 1 ) .
For a state i E @, the system may be in one of various
operational modes or may take certain actions. For instance, the system might choose to reconfigure itself. Let
Ai be the set of all available actions or operational modes
when S ( t ) = i , and let the system choose action a ( i, t )
from Ai. It is easy to see that transitions between system
states depend upon the current state and the current action
or operational mode. When the availability/functionality
of units are uncorrelated and the failure process in each
unit are Markovian, state transitions will be independent
of the past states and actions. Thus, the system’s behavior
at time t can be fully specified by the pair ( S ( t ) . a ( S ( r ) ,
t ) ) , where S ( r ) E a, and a ( S ( r ) , r ) E Asl,,.
Let p (i, a ( i )) be a reward rate associated with the system state i and the action a ( i ) E A,. This reward rate
represents what the system can achieve, or may lose, per
unit time, with the pair ( i . a ( i )). In addition, we assume
that there exists a cost, c ( i , a ( i ) ) , with the action a ( i )
taken when the system enters state i. The cost c ( i , a ( i ))
represents the instantaneous cost (if any) of taking the action a ( i ) .
A = Uie*Ai is defined to consist of all choices of action or operational mode that the system will take during
its lifetime. When the choice of action at time t depends
only on the system’s state at time t , the strategy is called
stationary. In other words, if we take an action or enter a
particular operational mode when the system enters a new
state, that action or operational mode will be used until
the next state transition takes place. Thus the same action
or operational mode will be used continuously between
two successive state transitions. A stationary strategy can
be specified by a set of actions, { a ( i ) l a ( i ) E A,, i = 1,
2, * * ,n } . With a given stationary strategy r , the action
taken at time t is denoted by a,( S ( 2 ) ) . Also, we assume
that, when the system has no units available, the only action that can be chosen is to repair all or part of the system. Thus, under a stationary strategy r, the reward accumulated during [0, t) can be expressed as

+,

Consider a system which may exist in one of several
states. A state is a compact description of everything about
the system that it is relevant to know. At predefined instants of time, an action or input is applied to it. The system response to that action U is characterized by the matrix of transition probabilities p i j (a), i.e., the probability
of a direct transition from state i to statej under action a.
Assume that there is a reward that the system generates
for its owners per unit time; a reward which is clearly a
function of the system state. Assume also that taking an
Wr.i(t) =
P(s(~)a
, r ( ~ ( 7 ) d7
))
action costs something (zero is a permissible cost).
Maximizing the net reward per unit time by suitably
- /e* 4A a r m W ) (2.
choosing the actions a is one of the most important problems of decision theory. We refer the reader to [ 6 ] ,[7], where S(0) = i is the initial state of the system, and kj(
and [8] for an excellent introduction to the subject. In the the number of visits to statej during [0, t).

-

jl

RaPrOduced with psrrrission of copyright arner.

Further reproduction prohibited.

I190

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. IS. NO. IO. OCTOBER 1989

If, instead of being stationary, there is a periodicity to This equality can be proved easily from the facts that
the reward, failure, or cost structures, then the system is E l ‘I;.] I; and that the strategy is stationary. The optiperiodic. If, as sometimes happens, cost and failure rates mal resource control problem in which V , ( i ) is maxihave different periods, the least common multiple of these mized then becomes a semi-Markov decision problem. Let
determines the system period. If the period is T, then the the optimal strategy be r* such that V + ( i ) = max,
choice of action at time t of a periodic system will depend Vz(i).
only on the system state at time t and on r - [ t / T ] T,
We denote the transition probability from state i to state
where 1x1 is the maximum integer not exceeding x. j under action a ( i ) by pij(a ( i )), and the mean holding
Clearly, any periodicity in the job arrival process can be time at state i by a (i, a ( i )). Then, the mean reward acexpressed through the reward function.
crued from the moment the system enters state i until the
The period of reward. cost, etc., will depend on the moment the next transition occurs can be’ represented by
application in question. For example, the arrival process y ( i , a ( i ) ) p ( i , a ( i ) )a ( i , a ( i ) ) - c(i, a ( i ) ) . Acmight be diurnal, as in most computing centers.
cording to Theorem 7.6 of (81, the optimal stationary
The resource control problem arising from this model strategy can be obtained by the following theorem.
is to determine a strategy ?r* such that the average exTheorem 1: If there exists a bounded function h ( i ) , i
pected reward lim,-.- W r . . i ( t ) / t is maximized with re- = 1.2, * * . , n, and a constant g such that
spect to all feasible strategies. This is a common problem:
f
for instance, if a two-dyad system were to suffer a processor failure, does it reconfigure into a one-triad system,
or into a one-dyad, one-simplex system? Another exam(3.2)
ple is choosing the recovery action to be taken when a file
becomes inaccessible.
then there exists a stationary strategy r* such that g =
111. OPTIMAL
STRATEGY
FOR HOMOGENEOUS
SYSTEMS V f ( i ) = max, V , ( i ).
Notice that the condition in the above theorem will be
A system is homogeneous if transition characteristics
met
automatically when y( i , a ( i ) ) is bounded for all i
between states, the cost of actions, and the reward rate do
not change in time. The distribution of the time that the and action a ( i ), and all stationary strategies give rise to
system spends in state i before leaving it is independent a finite and irreducible state space, despite the fact that
of the time at which the system entered state i. Thus, given we do not know g in advance. The optimal strategy can
an arbitrary stationary strategy, the system can be mod- be determined by Howard’s strategy improvement procedure [ 6 ] . 171. In the discussion that follows, we shall
eled by a semi-Markov process. Moreover, when is fipresent
an algorithm on the basis of the strategy improvenite, the optimal strategy of a homogeneous system must
ment
procedure
that is embedded in the solution of Howbe stationary as shown in [IO].
While few-if any-distributed systems exhibit exact ard’s equations:
n
homogeneity, there are many which are approximately
p g ( a ( i ) )h ( j )
homogeneous. Aging of components and a change in the g a ( i , a ( i ) ) + h ( i ) = y ( i , a ( i ) ) +
J‘I
reward structure are the most frequent causes of a departure from homogeneity. However, in many instances.
(3-3)
components age, and the reward structure changes so
slowly that the system can, for all practical purposes, be where a ( i ) e Ai. Equation (4) stems from Theorem 1 .
It is easy to see from (4) that the functions h ( ) cannot
regarded as homogeneous.
be
uniquely determined, since [ pij( n ( i ))] is a transition
Because we are interested in optimizing the asymptotic
matrix:
the set of equations is dependent. From Theorem
reward rate, the transient states have no effect on our ob1, the absolute value of the h ( 0 ) ’ s does not matter in our
jective and are therefore not considered in this model.
search for the optimal actions: only the relative value
Also, we limit ourselves to the case where there is only
does.
So, we can set one of the h (0)’sto some value, and
one recurrent class in this semi-Markov process. (Recursolve
for
the rest. In the algorithm below, we set h ( n ) =
rence follows from the fact that repair is allowed.) This
0
in
step
3
as the most convenient value.
i s not a limiting factor: even if there are multiple classes,
Algorithm I :
each of them can be considered separately.
I ) Select an arbitrary strategy ?r = { a ( i ) I a ( i ) E A i ,
Since repair is allowed and the total number of states is
i
=
I , * * ,n } .
finite, the expected period between two consecutive visits
2
)
Solve (4) to obtain h(i ), i = 1, *
, n - 1 , and
to state i, denoted by 7;, is finite. Under a stationary stratg,
under
the
strategy
zr. To do this, set h ( n ) = 0.
egy, S ( r ) is a regenerative process. Thus, W * . i ( i ) can be
3) Generate the strategy T ‘ as follows. For each i, deregarded as a renewal reward process with regenerative
period T,. It follows that, under the strategy x , the aver- termine an action a’( i ) for which h ( i ) is maximized. That
is, find a’( i ) to maximize:
age expected reward per unit time can be given as

+

,x

-

11

~ ( ia.( i ) ) +

.Xp c ( u ’ ( i ) )h ( j ) - g a ( i , u f ( i ) ) .
J=I

SHIN rt (11.: DYNAMIC CONTROL OF RESOURCES

I191

If more than one action maximizes the function, choose
the one with the smaller label. ir' consists of these actions: ( ~ ' ( l ) ; . . , a ' ( n ) } . I f s = d , t h e n r = r*and
we can stop, having found an optimal strategy. Otherwise, set T = r f and go to step 2.
A proof that policy improvement procedures of this kind
converge is presented, for example, in [SI. The worstcase complexity of this algorithm is simply the total number of possible strategies: this is because the policy improvement procedure is essentially a contraction m a p
ping, and so the same nonoptimal strategy is never
encountered twice.

-

-

,n,

7

E [0,

+

f

I1

w(i, r ) +

(4.2)

where g is the maximal reward.
It is easy to see that these conditions are satisfied in our
case. Although we do not know g a priori, it is clear from
the problem definition that it is finite. So is w .
The policy improvement procedure can also be applied
to find the optimal strategy, a* in this case. However, for
each possible r. we need to determine both qij( a) and
w ( i , a). A modified algorithm is presented below, in
which dynamic programming is used to find qij( a), w ( i .
T ) , and the improved strategy, at each iteration.
The first step is to discretize the period to make the
algorithm suitable for digital implementation. Let K be a
large natural number, 6 = T / K , and let a ( i , k ) be the
action applied at the system state i when t E [ mT k6,
mT + ( k + 1)6). We are concerned with the optimal

+

- -

T(K) = {a(i, k ) l i = 1,2,
k = 0, 1,

.- - ,K -

-

* *

n,
1, a ( &k) E Ai}.
9

, n, calculate

q o ( x ) and

r ) by the following equations:
Sijb) =

4ij(r,K),

(4.3)

where
I1

q i j ( . ~k, + 1) =

C ~ / j ( a ( kl ,) ) 4i/(.R,k )
/=0
K-l

w(i, r ) =

(4.4 1

R

C C qij(rR,
k) p ( k s , j , ~
k=O j = l

( jk ), ) 6

- c ( 4 k))
with qii(r,0) = 1 and qij(r,0) = 0 ( j
k)) is the cost of taking action a ( j , k).

(4.5 1
f 0). C ( a ( j ,

3) Calculate h ( i ) as follows:
fl

h ( i ) = w ( i , r ) + J,E
qu(a)h ( j ) .
=1

(4.6)

4) F o r e a c h i = 1 , 2 , * - - . , n , d e f i n e 6 ( i , K )= h ( i ) ,
& ( i , K ) = 0, and g i i ( K ) = 0 i f j $ i, g i i ( K ) = 1 otherwise. Fork = K
1 to 0, find actions 6(i, k) for i =
1,2, * . ' , n. which maximize

-

6(i, k)

>

,xqu(?r)h ( j ) - 81

Raproducd rith permission of copyrlsht auwr.

time period of duration 6.
Algorithm 2;
1) Select an arbitrary strategy

= p ( k 6 , i , ci(i, k))b

+

J=!

'

Also, it is convenient here to regard pu(a( i , k)) as the
probability that the state transition i -+ j occurs in one

T)] (4.1)

such that the average expected reward is maximized.
For tractability, we assume that the holding time at state
i under action U ( i ) is exponentially distributed with mean
a ( i , a ( [ ) ) . Then, the system can be modeled by an
embedded Markov chain in which the system's state is
examined every period T. Let the system state at time mT
be s,. With a strategy r , the system will transfer into state
s,,~ at time ( M + 1 ) T. The sequence { s,,, } is a Markov
process, and the state transition probability is defined as
qij(a) = Prob { s,,,+~ = j i s n 1 = i ). Also, denote the reward accumulated between mTand ( m 1) T by w ( i, r )
when s,, = i. Thus, as in Theorem 1, an optimal strategy
exists if there exists a bounded function h ( i ) for i = 1,
2, * . . ,n, and a constant g such that

,n,

* *

k=O,l;**.K-l}.

w ( i,

In this section, we consider the case where the reward
rate is a periodic function. Suppose that, at time t , the
reward rate associated with state i and the action a ( i ) is
p ( 7 , i , a ( i ) ) , where 7 = t
[ t / T ] T . The period T i s
obtained from practical considerations: it may be a day,
a week, or any other period natural to the application. Our
objective is to find a strategy
*

r * ( K ) = { a & k ) ( i = 1,2,

2) For each i = 1, 2,

1V. SYSTEMSWITH PERIODICREWARDS

z = { u ( i , 7 ) l i = 1, 2,

strategy

n
j- I

pU(6(i, k)) i(j , k

+ 1)

(4.7)

where a ( i , k) E Ai.
S)Leta'(K)={a(i,k)li= l , . . . , n , k = 0 , 1 ,
*
, K - 1 }. If r f (K ) = a( K),then stop the algorithm
with r = r ' ( K ) . Otherwise, set r ( K ) = r ' ( K ) , w ( i ,
t ) = &( i , 0), and qij( +) = .&( i , 0), and go to step 2.
In (10). one can set h ( n ) equal to some positive constant and solve for the rest of the h ( i )'S.
For finite K,this algorithm produces nearly-optimal actions and not necessarily optimal ones, since the actions
take place at specific epochs in the operating interval
(namely at multiples of 6). It is easy to show, from the
fundamentals of dynamic programming, that the algorithm tends to optimal as K --* 00 I IO]. It is trivial to show
(by contradiction) that if there are two numbers kl and k2.
with kt = mkl for some natural number tn > I , then the
algorithm with K = k2 produces a policy which is at least
as good as that with K = k l . This nearly optimal issue is
9

Further r e p d u c t i o n prohibited.

-

1192

SOFTWARE

IEEE TRANSACTIONS ON

less troublesome from a practical standpoint than it first
appears, since, for example, computers are not expected
to reconfigure themselves more than a certain number of
times an hour because of the overhead required for each
reconfiguration. b can then be chosen appropriately. For
instance, in the numerical example that follows, we use S
= 3 minutes. This means that reconfiguration can take
place up to 20 times an hour: something that should be
perfectly adequate for the example in question.
Theorem 2: Algorithm 2 converges.
Proofi We exploit the well-known fact that the strategy-improvement procedure converges for homogeneous
systems. For every periodic system S,,with Jinite action
set A,,. transition functions q y ' ( e), and a reward structure, we can construct a discrete-time (with time-period
T )homogeneous s stem Sh with finite action set Ah, transition functions q ( e ) , and a reward structure, such that
for every policy i,in the periodic system, there is a
corresponding action a, E Ah such that q y ' ( r a ) =
(h)
qlj ( G I ,
the expected reward per time T of taking action a,
with S,,in state h is equal to that per period (of length T )
due to policy rm with S,,in the corresponding state p at
the beginning of the period.
Clearly, running the strategy-improvement procedure
for Sh is equivalent to running it for S,,. Convergence is
thus established.
Q.E.D.

ENGINEERING. VOL.

.............

10,000

IS. NO. IO. OCTOBER 1989

........

"

I

EXp0Cl.d

?ewe14
PI how

-.............

.................

300

...*............

"

100
i

l

l

1

4

0

ia

8

20

16

I4

T h e 01 day

Fig. 1. Reward function.

(x,

300

Repairman
muIIIpllw

.

2
1

0

4

s

12

10

20

24

T l n r of day

V. EXAMPLE
Let us consider a fault-tolerant multiprocessor system
which uses n-modular redundancy, where n = 2 or 3, i.e.,
it can operate either in dyads or in triads. When it operates
in dyads, it can detect one failure in any dyad, but not
mask it. As a result, the affected computation must be
rolled back'or restarted. The cost of this is expressed
through a penalty function, described in greater detail below. When the system operates in triads, however, it can
mask up to one failure per triad by voting. Clearly, rollback or restart is not needed in this case. There is a tradeoff here: if the system is configured in triads, the system
can sustain failures without having to roll back or restart,
while if the system is configured in dyads, there will tend
to be more of them, and as a result, the throughput (when
there is no failure) can be expected to be greater.
There is a repairman on call. There is a cost associated
with summoning the repairman, and a nonzero time taken
by him to get there. There is also a cost per unit time of
keeping the repairman at the site. Handling the repairman
resource also consists of balancing tradeoffs: sending him
away too early might mean he will have to be called
back-thus incurring a cost-while keeping him too long
will also result in the incurral of some cost.
The state of the system expresses two things: whether
the repairman is present, absent, or summoned, and how
many processors are functional. In response to the states

'

'We assume that near-simultaneous failure of more than one processor
in the same triad is vanishingly unlikely.

Cost rate for keoplng repairman

8

rk

(Rtpolrnun multlpller)

Fig. 2. Cost of keeping the repairman.

are a set of actions, which are decisions to do with the
repairman (keep him, send him away, or summon him)
and decisions to do with the configuration (configure in
triads or in dyads). Naturally, the actions that are available depend on the current system state. To take a trivial
example, the action of summoning the repairman is meaningless when the repairman is already at hand.
The action depends on the system state and the reward
structure. The reward structure in this example is particularly simple. The system receives a reward per unit time
equal to the number of clusters (dyads or triads) functioning, minus any costs incurred due to the action at that
state. The penalty incurred when a processor in a dyad
fails is the product of a penalty multiplier and the reward
rate. Because a triad has the failure-masking capability,
there is no corresponding penalty for a processor failure
when processors are configured in triads.
Processors fail, and are repaired (if the repairman is
present), according to an exponential distribution with
' p ; ' , respectively. More specifically, the
mean ~ j - and
following symbols will be used for this example.
p,
p~

repair rate.
failure rate.

rk

cost

per hour of keeping the repairman.

r,vx cost per hour of summoning the repairman.

SHIN cf (11.: DYNAMIC CONTROL OF RESOURCES

I I93

0 1 2 J 4 5 6 7 8 9 4 0
000

LOO

-

6:OO

I

,

,

,

,

,

I

1

1

1

-

-

lime

1inu

01 dcy

d W
12:oo

-

I&W

-

8md

3

1200

-

lam

-

col1
npairmar

repairman
awav

don'l caU
wpriwnan

..--

24:OO

With repairman presevt
No of procraaota

0

0 1 2 3 4 5 6 7 0 9 1 1
000

24:w

II

rmd

1

2

1

No of p r o c o w s
4 5 6 7 6 9 1 1

3

I

,

,

,

,

,

:db '

h)

wpairrnan
away

24.w

I

(11

With repoirmon obsent

With repairmon present

(h)
Fig. 3. Effect of changing r,. r, = 30 000, p, = 0.01. p , = 1.0, penalty
multiplier = 300. (a) r, = 1O00. (b) r, = 5000.

If the repaiman is called at time nS, he arrives at time ( n
116.

+

The reward rates and the repairman costs are periodic
with a period of 24 hours. They are shown in Figs. 1 and
2, respectively. The reward rates per processor group
(dyad or triad) are low until about 7:OO in the morning,
and then rise to their peak value by 8;OO. This value is
maintained, with a break of an hour at noon, until 5:OO in
the evening, after which it declines. This reward rate is

Reproduced with permission of copyright wner.

directly proportional to the job amval rate, and is the
means by which changes in the arrival rate are accounted
for. Repairman costs are greatly magnified when he is
called in after normal business hours. We have set K =
480, i.e., the day is divided down into 480 3-minute seg-

ments.
Numerical results are contained in Figs. 3-7. Fig. 3

deals with the effect of r k , i.e., the cost of having the
repairman on-site on the optimal action. As expected, the

Purthsr reproduction prohibited.

II94

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL. IS. NO. IO. OCTOBER 1989

With repairman present

With repairman absent
fC)

Fig. 3. (Conhcted.) (c) r, = 25

m.

with the reward rate down and the repairman expensive,
even a bad succession of failures does not prompt the
summoning of the repairman: not unless there are fewer
than two processors functional is the repairman summoned.
Fig. 4 considers the effect of the penalty of dyadltriad
failure: as the penalty increases, it shows that the system
configures itself more and more into triads. When the penalty is 200, the only triad formed is when there are only
three processors functional: this is obvious since with just
o
I
i
a
4
a
I
7
8
IO
that many processors available, there will be as many
no. of procesoors funclionol
triads as there can be dyads. As the penalty for failure
increases, triads are preferred more and more, despite the
Fig. 4. Effect of failure penalty on configuration r, = IOOO, r, = 30 OOO.
1,.= 0.01. j i , = 1.0.
reduction in throughput that results. When the penalty is
300, the system now configures into triads for states 10
and 20 (nine functional processors) as well. As the pensystem tends to use him less when he is more expensive. alty rises to 400, this is the case for 6 and 7 functional
As rk increases, it becomes better to send the repairman processors in addition to those mentioned above. Finally,
away during particularly expensive periods (e.g., during when the penalty is 600, the system is always configured
the lunch hour or in the evening), and to accept the ad- into triads except when there are not suficient processors
ditional cost r,, of summoning him later. A sample trajec- to make up even one triad.
tory is plotted in Fig. 3(b). The system starts the day (030
The change in penalty of failure also affects how the
hour) with eight processors functional, and the repair- repairman is handled. In Fig. 5 , we plot the repairman
man away. At 4:OO AM, a processor fails (point b). Still, curves for two penalty multipliers: 200 and 500. As one
the repairman is not called until 6:oO AM, when the cost might expect, when the failure penalty is large, the reof keeping him is sufficiently low. With the repairman pairman is called sooner and retained longer.
present, the system is brought up to eight functional proIn Fig. 6 . we consider the case when the penalty is concessors by about 9:OO AM, (point e ) , to nine processors stant and not a function of time, and show how the chang(pointf) by about 1O:OO AM, and fully functional (point ing reward rate affects the optimum configuration. Below
g) around 11:OO AM. At this point, the repairman is sent three functional processors, there is no decision to be
away. The sample path for the second half of the day can taken; the system has to work in a dyad. When, for inbe interpreted in the same way: at night, for example, stance, there are nine processors functioning, the system

SHIN

CI

1195

til.: DYNAMIC CONTROL OF RESOURCES

No. of proscrlorr
0 1

6.W

2 3 4

5

6

7

0 9 i(

i

lirr
#*
1x00

lk00

24:OO

24:w

With repoirrnon obsent

With r e p o h o n present

Fig. 5. Effect of failure penally on repairman. r, = 1000. r,

=

30 000, p , = 0.01. p r

=

1.0.

no. of processors
0

time

I212

1

I

J

4

s

6

7

8

9

10

-

of day

Fig. 6. Effect o f time of day on configuration with a constant penalty of IO OOO.

is configured into triads from midnight to 7: 12 AM, when
it switches to dyads. It switches back to triads at 12:12
PM, and back again to dyads at 1:12 PM. Finally, at 5:OO
PM, when the reward rate begins to drop, the system goes
back to operating in triads.
Fig. 7 considers the effect of increasing the cost of summoning the repainan. The repairman iii now kept for a
greater number of states when the cost of summoning him
becomes very great: it is better to pay the cost of keeping

Reprodud uith perniraion of eopyrisht oumr.

the repairman under such circumstances. When rss = lo6,
the repairman is sent away only when all processors are
functioning, and it is eight o'clock in the evening (the
time when the reward rate is very small and the cost of
keeping the repairman is especially large).
While all these trends are intuitively clear and do not
need a sophisticated algorithm to determine, the exact epochs at which the repairman should be called or sent away,
and the system configured into triads or dyads, cannot be

Further reproduction prohibited.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. IS. NO. IO. OCTOBER 19R9

11%

No. of proccrsorr
0

1

2 3

5

4

6

7

8

s

~

L

6al-

rim

6:OO

-

-

Ti

.

O f W

-

1l:w

-

1200

1600

-

lam.

danv *.U
mpatwnan

24:oO

With repairman present

600

-

600.

line

lima

ddor
1200

1800

With repairman absent

of Cay

7

t

-

k8.p

npairman
npairman

-

-

1200

-

18:oo

-

88nd

maiman
away

24:W

24:W

'

With repairman absent
(bl
Fig. 7. Effect of summoning-cost on repairman. r, = 3000. p. = 0.01. F,
= 1.0. penalty multiplier = 300. (a) r, = 5000. (b) r, = 50 OOO.
With repairman present

obtained through intuition alone, and do require an algorithm such as this one.

VI. DISCUSSION
Because of its inherent difficulty, the problem of controlling resources in distributed systems has usually been
treated in an ad hoc

manner. Such a trend could remove
the various advantages of distributed systems over uni-

processor systems. In this paper, we have countered this
trend with the development of a rigorous, quantitative
method for optimally controlling reiources in a distributed system.
The-resource control problem is of great practical significance because gracefully degrading systems are being
used increasingly in such commercial fields as banking
and travel. In such systems it is possible to estimate run-

SHIN

et ut.:

I197

DYNAMIC CONTROL OF ReSOURCES

0

000

No. d proccrrws
1 2 f 4 5 6 7

Ne. d pwerrcrr
1 2 3 4 s 6 7 1 9 t a

6910

0,p

I

wpatrmon
away

600

t

L

Im'l

E.u

wPa-

ThM

ddor
12w

-

CwP

t

rrpnirman

1200

t
-

18:OO

1tW

24:M)

24:w
24.00

With repoirmon present

dm'l roll
wpdnnon

(C)
Fig. 7. (Continued.) ( c ) r, = 100 OOO.

ning costs, the benefits from having a certain throughput
at various times of the day, repair costs, etc., much more
accurately than for the computers used in, say, a university computing center. Another application is in real-time
embedded systems, such as those which control airliners
or spacecraft, where the reward rates for various jobs may
vary with the phase of the mission. A third application is,
as was mentioned in the Introduction, in local-area networks such as token rings, where one has the problem of
allocating bandwidth to each of the rings, subject to a
constraint on the total bandwidth allowed.
REFERENCES
111 1. F. Meyer, "Closed-form solutions of performability." lEEE Truns.

Compui.. vol. (2-31. no. 7. pp. 648-657. July 1982.
121 D. G. Furchtgott and 1. F. Meyer, "A performability solution method
for degradable nonrepairable systems." IEEE Truns. Compur.. vol.
C-33. no. 6. pp. 550-554. June 1984.
(31 L. Donatiello and B. R. lyer. "Analysis of a composite performance
reliability measure for fault tolerant systems," IBM Thomas 1. Watson Research Center. Yorktown Heights. NY. Res. Rep. RC-10325.
Jan. 1984; also in J. ACM, vol. 34. no. I. pp. 179-199. Jan. 1987.
141 B. R. lyer, L. Donatiello. and P. Heidelberger, "Analysis of performability for stochastic models of fault-tolerant systems," IBM
Thomas J. Watson Research Center. Yorktown Heights, NY. Res.
Rep. RC-10719. Sept. 1984; also in IEEE Trms. C ~ n i p u r .vol.
.
C35. no. IO. pp. W2-907. Oct. 1986.
151 Y. H. Lee and K. G. Shin. "Optimal reconfiguration strategy for a
degradable multi-module computing system." Comput. Res. Lab.,
Univ. Michigan, Ann Arbor, MI, Tech. Rep. CRL-TR41-84. Sept.
1984; also in J. ACM. vol. 34. no. 2. pp. 326-348. Apr. 1987.
161 R. A. Howard, Dynumic Probabilisric Systems. vol. I!. Semi-Murkov
and Derisiott Processes. New York: Wiley. 1971.
I71 C. Derman. Finiie Siuie Murkoviun Decision Prwerses. New York:
Academic, 1970.
[Si S. M. Ross. Applied Prahuhility Models with Optimiwtim Appliculions. San Francisco. CA: Holden-Day. 1970.

h P d U C S a

with p8Fai88iOn of copyright wner.

191 A. Federgmen and H. C. Tijms, "The optimality equation in average
cost denumerable state semi-Markov decision problcms. recurrency
conditions and algorithms." Appl. Prob.. v d . 15. pp. 356-373, 1978.
[lo] R. Bellman, "Functional equations in the theory of dynamic programming-iv: A direct convergence proof." Ann. Murh., vol. 65. pp.
215-223. Mar. 1957.

Kang G. Shin (S'75-M'7&SM'83)

received the

B.S. degree in electronics engineering from Seoul
National University. Seoul, Korea. in 1970. and
the M.S. and Ph.D. degrees in electrical engineering from Come11 University. Ithaca, NY, in
1976 and 1978, respectively.
He is a Professor in the Depanment of Electrical Engineering and Computer Science. University of Michigan, Ann Arbor, which he joined in
1982. He has been very active and authored/
coaulhod over 150 technical papers in the areas
of fault-tolerant computing, distributed rcal-timc computing, computer architecture, and roboticsand automation. In 1987, he meived the Outstending Paper Award from the IEEE TRANSACTIONS
ON AUTOMATICCONTROL
for a paper on robot trajectory planning. In 1985. he founded the RealTime Computing Laboratoly, where he and his colleagues ace currenily
building a 19-node hexagonal mesh multicomputer, called HARTS,to validate various architectures and analytic results in the area of distributed
real-time computing. From 1970 to 1972 he served io the Korean A m y as
an ROTC officer and from 1972 to 1974 hc was on the research staff of the
Korea Institute of Science and Technology, Scoul, Korea., working on the
design of VHFlUHF communication systems. From 1978 to 1982 he was
an Assistant Professor at Rensselaer Polytechnic Institute, Troy. NY. He
was also a visiting scientist at the U.S. Airiotce Flight Dynamics Laboratory in Summer 1979 and at Bell Laboratories, Holmdel, NI. in Summer
1980. During thc 1988-1989 academic year, he was a Visiting Professor
in the CS Division, Electrical Engineering and Computer Science, UC
Berkeley.
Dr. Shin was the Program Chairman of the 1986 IEEE Real-Time Systems Symposium (RTSS),the General Chairman of the 1987 RTSS, and
the Guest Editor of t h e 1987 August special issue of IEEE TRANSACTIONS
ON COMPUTERS
on Real-Time Systems. He is a Distinguished Visitor of the
IEEE Computer Saciety. He is a member of ACM, Sigma Xi, and Phi
Kappa Phi.

Further reproduction prohibited.

1198

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. VOL..

C. M. Krishna (S'78-M'79) receivcd thc B.Tech.
degree from the Indian Institute of Technology.
Delhi, the M.S.degree from Rensselaer Polytechnic Institute, Troy. NY. and thc Ph.D. degree
fmm the Univenily of Michigan. Ann Arbor, all
in electrical engineering.
Since September 1984. he has been on the faculty of the Depariment of Electrical and Computer
Engineering. University of Msusachusetts. Amherst. He was a Visiting Scientist at the IBM
Thorns J. Watson Research Center during the
Summer of 1986. His research interests include distributed systems architectures and operating systems, real-time systems. reliability mcwleling. and
queueing and scheduling theory.

Raprodud uith pamission of copyright 0y-r.

IS.

NO. IO. OLTOl3ER IYWY

Yann-Hung Lee (S'81-M'M) rcccived thc B.S.
degree in engineering science and the M.S.degree
in electrical engineering from National Chcng
Kung University in I973 and IY78. rcspcctivcly.
and the Ph.D. degree in computer. information.
and control engineering from thc Univcmity ol'
Michigan. Ann Arbor. in 1984.
From 1984 t o 1988. he was U Research StaR'
Member in the AKhitccturc Design and Analysis
Group at IBM Thomas J. Watson Research Ccnler, Yorktown Heights, NY. Since August 1988,
he has been an Associate Professor in the Department of Computer and
Information Sciences, University of Florida, Gainesville. His research interests include distributed computing. parallel processing, performance
modeling. database managemnt systems, fault-tolerant mmputing, and
VLSI testing.
Dr. Lee is a member of the IEEE Cornputer Society and the Association
for Computing Machinery.

Further reproduction prohibited.

,&.A&

Robust Transaction Routing in Distributed Database Systems
Yam-Hang Lee*
Philip S. Yu**
Avraham Leff***
*Computer Science Department
University of Florida
Gainesville, F L 32611

**IBM T. Watson Research Center
f.0.Box 704
Yorktown Heights, NY 10598

Abstract

can, must be shipped to the system owning the referenced partition for processing. With regard to the destinations of database
requests issued by a transaction, we can often identify one system
as the preferred system where the transaction sends most of its
requests. In studying performance of transaction processing system, the reference locality distribution, i.e. percentage of database calls issued by a transaction to each database partitions, has
to be considered. As there is an additional overhead associated
with remote database calls, routing an incoming transaction to the
system with the lightest load may provide worse performance
than routing the transaction to its "preferred" system. Thus
transaction routing strategies need t o strike a balance between
sharing the load among systems and rcducing the number of remote calls.

In this paper, we examine the issue of robust transaction
routing in distributed database systems. A class of dynamic routing strategies which use estimated response times to make routing
decisions is studied in details. Since response time estimation and
decision making depend on the assumed transaction model and
parameters, it is important to examine the robustness or sensitivity to the inaccuracy in the assumptions and parameter values.
Through simulations, we find that the dynamic routing strategy
based strictly on response time is too aggressive in sharing loads
and makes too many non-preferred systcm routing. It is robust
with respect to change in the number of database calls per transaction, but is relatively sensitive to the distribution of database
calls. Two refinements are proposed which improve system performance as well as robustness of routing decisions.

1.0

Dynamic routing strategies for this environment have been
studied in [Yu86]. In previous studies on dynamic load balancing
approaches. such as in ZWangS5, EagrS5, AgraS2,
CareS4.Chow791, it is assumcd that incoming tasks can be completely serviced at any processing system. This implies either that
requested resources, i.e., database or files. are replicated or
shared among all processing systems, or mat all rasks are purely
computational. Under the data partitioning model, of course,
these assumptions are invalid. The major issues considered in

Introduction

The dcmand for high transaction processing rates has grown
rapidly in recent years. These trcnds motivate the development
of a multi-processor or locally distributed system.] There are alternative ways t o form a locally distributed system such as the
data partitioning approach [CornSG], the data sharing approach
[YuS7], and the hybrid data sharing and data partitioning approach [WolfSS]. We will concentrate on the data partitioning
approach, allhough the concept discussed can be generalized to
other approaches.

[YuSG] in designing a dynamic strategy include (1) what information is crucial to decision making and what is the overhead of
collecting the information, and (2) how to use easily collectable
or maintainable information to make the routing decisions. A
class of dynamic strategies based on an attempt to minimize each
incoming transaction's response time, referred to as the MRT
strategy. was proposed and studied in [Yu86]. It uses readily
available information at the front-end system such as previous
routing decisions of transactions currently in the complex. It has
been demonstrated that system performance can be greatly improved if this concept of minimizing the response time of incom-

The locally distributed database environment is shown in
Figure 1. l . The database is partitioned among the various processing systems, and the incoming transactions are routed to one
of the processing systcms by a common front-end system. If a
transaction issues a database request which references a non-local
database partition, the request, referred to as a remote database

'

The work was performed when Yann-Hang Lee was a research staff
meniber at the IBM Thomas I . Watson Research Center.

CH2665-8/88/0000/0210$01.00 0 1988 IEEE

***Computer Science Department
University of Columbia
New York, NY

2 IC)

By locally we mean so close together that communication delay are negligible. e.g.. the entire system is located in the same machine room.

schemes: we seek to reduce the risks of making a non-preferred
system routing decision by being more selective about either the
precondition or the candidate for non-preferred system routing.
Because the number of remote calls is reduced, the communications bandwidth requirement is also reduced. More over, these
strategies are more robust than thc original MRT with respect to
model and parameters accuracy.

Processing
System 1

. . . . .

In the next section, the model of transactions in a locally
distributed database environment is given. Then, the MRT
strategy is described and its robustness is studied. In Section 4,
we introduce the two refinements. Their performances are evaluated with simulations and compared with the M R r strategy.
We summarize the results in Section 5.

Processing

2.0
Partitioned Dotoboses
DB 1

The locally distributed transaction processing system
studied has been described in [YuSG] and is summarized in the
following. The system consists of N transaction processing systems P, where i = 1,2,...,N , and a front-end system, connected by
an interconnection network as shown in Figure 1.1. Thcre is a
partitioned database which consists of DB,, ..., DB,, where DB,
is attached to the processing system, P, . All database requests to
DB, are assumed to be handled by the processing system Pp The
processor speeds are assumed to be identical and are denoted as

D13 N

Figure 1.1 The configuration of a distributed transaction
processing system
ing transactions is used. The same concept can be applied to a
shared database or replicatcd database environmcnt as subsequently applied by [Thorns71 for the latter. In [Ferr86.,ZhmouS7],
a response time oriented load index based on mean-value
equation, is proposed which is a linear combination of queue
lengths. The experiments done in [ZhouS7] take samples of
queue length and use smoothed queue length to calculate a load
index for determining the placement of UNIX commands.

11.

Transactions submitted by users cnter the systcrn through
the front-end system where a routing strategy is employed to
route the incoming transactions to processing systems. At the
assigned processing system, a transaction invokes an application
process which is interleaved by a number of database requests.
Transactions, then, can be characterized into different classes by
(1) the processing service demand of each application processing
segment, ( 2 ) the number and rcfcrcnce distribution of database
requests, and (3) thc processing and 1/0 service demands of each
database rcquest. For simplicity, we assume that these service
demands arc exponentially distributcd. Also, at the ensd of each
application processing segment, fixed probabilities of issuing a
database request to a particular database partition or terminating
the transaction processing are assumed for each class. Hence, the
number of database calls issucd by a transaction is geometrically
distributed.

An issue of great significance from a practical view point is
how critically the quality of the routing decision depend,supon the
accuracy of the model and the assumed parameter values. A
model is just an abstraction of the real world. Some assumptions
in the model may be made for tractability of the analysis. The
behavior of each transaction can vary from the moclcl and even
the average behavior may change from time to tiimi:. Thus a
practical scheme has to be robust to these issues.
In this paper, we examine the robustness of the minimum
response time based strategy (MRT) and considcr various means
to enhance its robustness. The MRT strategy is found to be: fairly
robust, but is somewhat sensitive to the accuracy on the distribution of database calls assumed. Whereas the response time
based approach is basically sound, the MRT sometimes seems to
be too aggressive on sharing the load. We thcn considcr t'wo refinements to the MRT strategy. One is a strategy that imposcs a
threshold critcrion on the load condition bcforc non-prefcrred
system routing, based on MRT, is considered; the other applies a
policy which discriminates long transactions in applying nonpreferred system routing.

Model Description

Based on the sequence of transaction processing, we construct the model of transaction processing as shown in Figure 2.1.
Let there be K transaction classes in the system and let fx, denote
a class k transaction. k = 1,...,K . For the k-th class, transactions
arrive according to a time-invariant Poisson process with rate A,.
The mean processing service demands of an application processing segment and a database request of fxhare U, and bL,respectively. Both U, and bh can be estimated by measuiring the

A common idea underlies these

21 1

(7

the result will be sent back. This is callcd a remote database call
in which both P, and P, have to perform sending and receiving
services. The service demands of initiating a rcmotc database call
and of receiving the results of the call are referred to as communications overhead and are assumed to be exponentially distributed with mean c.

start

input
formatting

The system model is illustratcd in Figure 2.2. A single

1

j

1

server processor sharing queue is used to model the processor at
each processing system. On the other hand, the 1/0 subsystem
of each processing system is modelled as an infinite server queue.
This is to correspond a global or aggregate representation of a
more complex 1/0 subsystem. Note that the model is completely
defined except for the assignment of transactions to the processing system. Given any routing strategy in the front-end system,
simulation can be performed to measure the system performance.

application
procysingl

output
formatting

a : application process

I: local dotobose requests
ov: cornm-unicotion overhead
r: receiving service ond

Figure 2.1 Model of transaction processing

remote dotobose requests

pathlengths of application processing and database request. For
each database requests issued by tx,, we assume that, an 1/0 device will be accessed with a fixed probability p;", and that the
service time of each 1/0 access is exponentially distributed with
mean d, . When the execution of an application processing segment is completed, transaction tx, may issue a database request
to the database partition DB, with probability pk, , or may termi1
nate with probability pho. The matrix -[p,,] shows the distribPkO
otion of dalabase calls issued by rx, and is referred to as the
reference locality distribution. For a given transaction rx,, we call
the processing system P,its preferred system ifp,, is the maximum
of p,, j = 1,2.....N .

//*
P1

- DR1

Front-end system

..................

..................
submodel for
dotobose requests (I k r)

Figure 2.2 Simulation model of a distributed transaction
processing system

With the above definitions, we can see that the average
processing utilization of serving transaction application process
and database requests in the whole system is

3.0
Of this utilization, a portion is associated with the processing of
database requests. We denote thc utilization based on processing
database rcquests at P,as follows:

Response Time Based Dynamic Routing
Strategy

We now examine the MRT strategy proposed in [YuSb].
The strategy, lirst, estimates the average queue length or utilization of each processing system P,. Then, the expected response
times of an incoming transaction, if it were routed to the processing system P,,for i = l,.. ,N, are estimated. The processing
system which provides the minimum expected response time is
chosen to execute the transaction. The flow of decision process
is illustratcd in Figure 3.1. In the following, we describe the implementation of MRT strategy and then examine its sensitivity to
the assumed transaction characteristics.

Notice that S and Se only depend upon the characteristics of
transactions and are independent of the transaction routing decisions. When a transaction being exccuted at P,issucs a database
request to DB,, where i # J ,the database request must be shipped
from processing system P,to P,. After the request gets processed,

212

3.1

Description of The MRT Strategy

Under the MRT strategy, the routing dccisims of active
transactions are maintaincd by the front-end system i n a routing
the entry
history table. Each timc a transaction t-u,is routed to P,,
in the k-th row and the i-th column of the routing history table is
incremented by one to reflect the new arrival and its routing.
Furthermore, when a transaction is complcted, the entry in the
corresponding row and column of the table is decremented by one
to rcflcct the departure. Note that there is a negligible ovcrhcad
to maintain the tablc in the front-end system, and that no sampling of instantaneous state information from the processing systems is required.

Figure 3.1 Flow diagram of the MRT strategy
Let PI,, be the number of active class k transactions assigned
to the processing system
Let prob,(ij) denote the probability
that a class k transaction assigned to processing system P,is
waiting for or receiving processing service at P,. Hence, we can
express the expected queue length L, of P, as

e.

The routing history table can be uscd to estilmate ithe expected response time. The expected response time of a.n incoming
transaction dcpends upon the transient behavior of the system
and the future arrivals. For efficient implementation at the frontend, a steady-state analysis is applied to estimate the mean response time using transaction characteristics and mean queue
length at each processing system. Based on the mean-value
equations [ReisSO]. the cxpected response time for a transaction
routed to P, is given as

N

K

f o r j = 1 , ...,N. Notc that the probabilityprob,(ij) is proportional
to the residence time (including service time and waiting time) at
the processing system P,. Bard and Shweitzer suggested that the
residence time z(ij)in P,of aclass k transaction which is initially
assigncd to P, can be written as
(3.3)
where S,(ij) is its total service demands at P, and is equal to
\
+ bflk,+ c ,-I C pk,,if i = j , or (bk+c)p,, ,otherwise. Thus. the
probabilityprob,(zj) can be expressed as

U,

,#i

where 1is the processing speed at-each processing system, L,is
thc mean queue length at P,and L = (L,,..., h.).
Not'c that although quite a few parameters appear in the above formulas, they
are derived from cither system parameters (c and p ) , or transaction characteristics (U,, bk,d,, pLo , and pkJ) w h c h arc provided
from the static transaction
- profile. The only unknown variable in
the above equation is L. Diffcrent ways of estimating 4, have
been considercd in [YuSG] by appliing some steady state approximations based on routing history table.

(3.4)
/= 1

Hence, given ni,, from the routing history table, the mcan
queue lengths L, are computed by iterating (3.3), (3.4) and (3.2),
starting with zero values for both queue lengths L, and probabilities prob,(ij), for k = 1,....K. ij = 1,...,N . Then, the response
time of a class k transaction routed to P,can be obtained from
Eq. (3.1).

In this paper, we focus on the MRT strategy based upon the
residence time calculation. This approach has shown t o provide
the best performance over other approaches considercd in
[YuSb]. It regards the numbers of active transactions indicated
by the routing history table as fixed populations in a closedqucucing network. Naturally, it is possible to calculate the exact
queuc lengths based on mcan-value algorithm [ReisSO]. llocvever,
this approach is impractical when wc consider the complexity of
the MVA. The MRT strategy uses an approximatio'n 'based on
Bard-Shwcitzcr's algorithm [Shwe79, Bard801 to calculate the
residcnce timc of cach transaction at cach processing system.
Then, the utilization and qucue length of each processirlg system
are computed. A summary of the approach is givcn belolw.

3.2

Performance of the MRT Strategy

In order to study the MRT strategy, we devcloped a simulation for the model illustrated in Figure 2.2. The system is assumed to consist of three transaction processing systems
(N = 3 ) with three transaction classes (K = 3). Based on data
from some IBM IMS systems [CornSb, YuS7], the average number of database requests per transaction is set to 15 for all transaction classes, i.e., pko= 0.0625, for k = 1,2,3. Thc reference
localitydistribution, [ q J , is givcn in Table 3.1
In addition, we assume that the processor speed is '7.5 MIPS
and the pathlengths of U, and b, are 21K and 9K instructions, re-

213

Transactionclass1
Transactionclass 2
Transactionclass3

0.75
0.07
0.11

/

0.11 0.14
0.82 0.11

0.06 0.83

Table 3.1 The reference locality distribution used in simulations
spectively. The additional overhead of serving a remote database
call, c, is chosen t o be 3K and 15K t o represent low and high
communications overheads. The IO access time, dk,and the
probability, p’p , of having an IO during a database call are assumed to be 40 ms and 0.7 for all transaction classes, respectively.
The arrival rates, At’s, are adjusted so that the total processing
load and individual processing loads of database calls are as indicated.

0

The MRT strategy has been shown in [Yu86] to be better
than either the optimal static or other dynamic routing strategies
considered. A summary of the responsc time under the MRT, the
optimal static strategy [NiSS, TantSS], and the strategy of routing
incoming transaction to the system with the shortest queue is
presented in Figure 3.2. An interesting point to investigate is how
sensitive the performance of the optimal static strategy is to the
accuracy of arrival rate estimates. If inaccurate arrival rates are
used to compute the routing probability under the optimal static
strategy, the strategy may no longer work propcrly. This is shown
in Figure 3.3 where the initial A’s are chosen such that the average
CPU utilization, S of serving transaction applications and database calls, is 0.71 and Si = $ = Si . When we increase A, by
x% and decrcase both A, and A, by 0.5xY0 , the system load is
not balanced any more and transactions suffer long response
time. In contrast, the MRT strategy which does not rely on the
arrival rate estimates provides very good performance.

.-e
t-

high N P R , close to 50940, is observed when the communications

Although non-prcfcrred system routing improves the balance of the loads among the processors, it has side effccts because
it incurs remote calls. This put more communications overhead
on the processors and higher communications bandwidth requirement on the links. It can also create a vicious cycle such that
after transaction t x , , with a preferred system PI, is routed to P2,
the change in load conditions forces the next arrival of transaction
r$, with a preferred system P2 , to be routed to P I . This vicious
cycle is referred as s ~ , u ~ ~ D t g p h e ~ i o nThus,
z e ~ ~ awe
. are concerned
with the phenomena of a sequence of ’locally’ optimal decisions
(in terms of estimated response time) that ’globally’-- i.e. over the
entire period of processing-- is sub-optimal in that preferred
system routins would have been the correct decision. In Table
3.2, we include the swapping ratios, SWR, defined as the percentage of non-preferred routings in which we observe a swapping phenomena. This statistic attempts to provide an indication
as to whether the MRT is making too many sub-optimal nonpreferred routings. The SWR’s in Table 3.2 look quite large, es-

Wml ydic

O-

l$

c

m -

e

I-

C

2

f

0 -

c

L

I

1

I

I

I

0.20

overhead is low. Even when the communications overhead becomes high, the NPR is still close to 20%.

e

%

0.15

We next examine the percentage of transactions routed to
non-preferred systems under MRT. This is referred to as NPR ,
the non-pret’erred system routing ratio. In Table 3 2.a balanced
case is first examined where the CPU utilization due to processing
database requests is the same at each system, i.e.. S f = 32 = S: .
We consider both cases of low communications overhcad of
c = 3K and high communications overhead of c = 15K. A very

-qUu

balanced database loads

0.10

Figure 3.3 Sensitivity of routing strategies t o arrival rates

~-15K

2-

0.05

x (percentoge of change in arraival rates)

J

214

ent distribution. Simulations have been conducted for the MRT
strategy in which pko= 0.0625 (the mean number of database
calls is 15), but the actual number of database calls has a different
distribution. In one case, the number of database calls for each
transaction is a constant with value equals to 15. In the sccond
case, a Bernoulli distribution is used where each transaction class
can have either a short run with S database calls or a long run with
45 database calls. The percentage of short ones is 8’7.50/, and
that of long ones is 12.5%. so the mean number of database calls
for each transaction class is still 15. We present thc performance
results of response time under MRT for the three different distributions in Table 3.4. The processing systems arc assumed to
have the same database load. The results show that with the MRT
strategy the response time is insensitive to what the actual distribution is.

serve. Simulations results are shown in Table 3.3. In ‘the case of
low communications overhead, the NPR is close to thse balanced
case; at high communications overhead the NPR is about
30%-40% higher than that of the balanced case. Tlhe SW’R’s are
still quite high in the case of low communications ovcrhcad. but
are significantly lower in the case of high communical.ions overhead.

SWR

RT-

46.6%

19.4%

46.3%

:17.9’%

78.2%

37.6%

85.0%

43.4%

--

mean response time

-

NPR ratio of non-preferred system routing
SWR -ratio of swapping phenomena

S = 0.71

Table 3.2 The MRT performance for balanced case
c=3K

I GeometricDistr.
I constant

S = 0.71

RT

I NPR

,I

SWR

c=3K

I

0.622

I 0.744

49.4%
63.5%

1

I

18.0%

I
I

Bernoulli Distr.

c=3X

I

0.718

I

1.149

49.3%

I

25.3%

I

0.617

0.707
0.711

0.619

0.711

I
1

I
1

cm3K

0.700
0.708

1 0.709

1
I

0.986

I

0.993

I

I 0.972

Table 3.4 The response times under various distrihutions on number
of datahasc calls

69.5%

Next, we examine the sensitivity to accuracy in the assumed
reference locality distribution. In reality. it may be quite difficult
to figure out thc exact locality distribution of database calls, i.e.
to give a precise measure. for instance, that 75% of databasc calls
accesses database partition D B 2 . As indicated by the reference
locality distribution, for each transaction class, thcre is a preferred system which would result in the largest number of local

Table 3.3 The MRT performance for unbalanced case

3.3

0.618

c-15K

~~

cm15K
25.9%

1
1

I
1
I

Sensitivity to Model Assumptions and Parameters
Values

We now examine how much the quality of the routing decision dcpcnds on the accuracy of assumptions about transaction
characteristics used in response time estimation. Somc assumptions arc implicitly made bccause of modelling rcquirciments. For
instance, geomctrical distributions are used for the numlbcr of
database calls. Furthermore, the parameter valuc:~rclatcd to
transaction charactcristics mav. in fact, vary substantially from
time to time. One critical parameter is the reference locality distribution, i.c., the distribution of database calls of one transaction
class with respcct to each databasc partition. Because nonpreferred system routing introduces remote calls, it may expose
the systems to unnecessary communications overhead1 if the gain
on load balancing is overestimated. We want to deteirmine
whether the aggessiveness shown in the MRT strategy backfires
when thc modcl assumptions and paramctcr values ;are not too
accurate.

calls or the smallest numDer of remote calls. It should be noted
that identifying the preferred system for each transaction class is
much easier than figuring out thc exact locality. Simulations are
performed in which two inaccurate estimates of refercncc locality
distributions, shown in Table 3.5, arc used by the router in making routing decisions. The two distributions maintain the same
preferred system as the actual distribution but with differcnt intensity: case 1 wirh stronger preferred system locality while case
2 with weaker preferred systcm locality. However, transactions
continue to issue database calls based on the actual locality distribution shown in Table 3.1. The response times are plotted in
Figure 3.4. With a strong locality distribution (case 1) assumed
by the router, fewer non-preferred system routings are done and
the rcsponse time is improved. This further supports the observation that MRT does excessive non-preferred system routing.
When a weak locality distribution is used, we notice performance
degradation for high trdnsacrion load. in the next sections. we
examine different means to alleviate this problem.

Let us examine the assumption about the number of database calls made by each transaction. The pko specjfie:d in the
model used by the MRT strategy implies a geometric dlistribution.
However, the actual transaction characteristic may have a differ-

215

I

I

I

I

case1

Databasepartition

1

2

3

1

2

3

Transactionclass1
Transactionclass2
Transactionclass3

0.85
0.04

0.07

0.06

0.03

0.08
0.06
0.91

0.60
0.15
0.19

0.18
0.66
0.14

0.22
0.19
0.67

0.90

that, even with a small threshold, this policy can improve the
system performance. We demonstrate that a threshold can also
be applied to the MRT in the environment studied. The new approach not only shortens the response time but, most significantly, reduces the sensitivity to accuracy in the assumed
reference locality distributions.

I

case2

Table 3.5 The false reference locality distributions

As indicated in the previous section. it is clear that a transaction should be routed to a non-preferred systcm only when a
sizeable gain on response time can be achieved. Marginal response time gain does not provide sufficient reward to make a
non-preferred system routing desirable. It is natural t o incorporate the threshold concept into the MRT such that non-preferred
system routing is considered only when there is substantial load
unbalancing. More precisely, our threshold strategy tests whether
the ratio of the utilizations between thc preferred system and the
system with minimum utilization is within a given threshold. IF it
is, then preferred routing is taken. Otherwise, the MRT algorithm
is carried out to make the routing decision. For efficient implementation. the test in the studied threshold strategy is carried out
after the first iteration of the Bard-Shweitzer’s algorithm.

1

I

0.4

1

I

0.5

I

I

0.6

I

I

I

0.7

1

We examine the performance of the threshold strategy under different thresholds. Simulation results are shown in Table
4.1, where the overall response time and NPR arc presented. The
original MRT strategy is equivalent to the case with threshold
equal to 1. For a threshold of 1.5, slightly smaller responsc times
are obtained in all cases examined than those of MRT. Also we
obtain more improvement for the case of high communications
overhead. However, the improvements in average response time

I

0.8

Utilizotion dut to tronsoction processing

Figure 3.4 Sensitivity of the MRT strategy to locality distribution

4.0

are less than 10%. To a lesser degree, this trend can be observed
when the threshold is 2.0. The reduction on NPR is very significant: for instance, with high communications overhead. the
NPR reduces to one third of that under the MRT strategy and,
with low communication overhead, to about one sixth.

Two Refinements of the NIRT Strategy

In this section, we consider two refinements of the MRT
strategy: threshold stratcgy and discriminatory strategy. Both
refinemcnts seek possible performance improvements by reducing non-preferred routing, even if this results in less balanced
load among systems. A more conservative approach is taken that
recognizes the preferred system as the default systcm for the

We also consider the case with a threshold of 3 in Table 4.1.
By ignoring opportunities to balance the load by routing to nonpreferred systems (the strength of the MRT) we worsen the response time significantly for high transaction load. In the
extreme, the response time can reach 0.801 and 1.175 if we always route transactions to their preferred systems (i.e. a bigger

routing decision. In the threshold strategy, a non-preferred system routing is considered only when the preferred system is
comparatively overloaded. However, when non-preferrcd system
routing is considered, the routing decision is again based on the
MRT strategy. In the discriminatory strategy. thc potential candidates for non-preferred routing is limited to the short transactions.

4.1

I

Threshold Strategy

S = 0.71
12.0

threshold

I 1.WRT)I 1.5

C-3K

I

I

I

I
13.0

I l.(MRT)I 1.5

I

I

I

I

S 5 0.81
12.0 13.0

I

I

RT
NPR

0.618
46.6%

0.611 0.622 0.642 0.700
16.3% 8.3% 3.1% 46.3%

0.695 0.720 0.760
13.9% 7.1% 2.9%

c=15K
RT
NPR

0.707
19.4%

0.695 0.701 0.727 0.986
13.6% 6.9% 2.5% 17.9%

0.916 0.956 1.042
11.5% 6.2% 2.6%

Threshold approach has been considered by Eager, et. al.,

€or load sharing in distributed systems with identical nodes

Table 4.1 The performancc comparisons of the threshold and the
MRT strategies

[EageSb, EageSS]. Threshold is used in a location policy to determine the destination of transferring jobs. It has been shown

216

I

threshold i s used), for S = 0.81 and c = 3K and 15K, respectively. This demonstrates that the MRT policy of emphasizing
load balancing is important.

cording to the following: given a percentage of inaccuracy x , if
(1 + x)qkk< 1, then qki = ( 1 + xlq,, and for all i # k ,
qk' = q k i 1 - *"'
) (i.e. the locality of the prcferred system
increases with a m 8 6 x whereas the localities of the non-preferred
systems decrease proportionally), otherwise q,: = 1 and qk,' = 0
for all i # k . The response times and the swapping ratios are
plotted in Figure 4.2. It shows that, when weaker localities are
assumed in making routing decision, swapping ratio can reach
80% under the MRT as x = - 0.3. The performance of the
MRT degrades substantially in this case. However, when a
threshold of 2 is applied, variations in response times are quite
small whcn x is in the range OP -0.3 to 0.2. It is clear that the
threshold approach is much more robust than the MRT when less
accurate reference locality distribution is assumed.

1
-

In analyzing the results of the thrcshold strategy, we make
the following observation. As NPR is vcry low compared to that
of the MRT, we expect the system to bc less balanccd. This is
verified by comparing diffcrcnces between utilizations of the
processing systems through simulation. However, the response
time of the threshold strategy is as good as or better than that of
the MRT. In a further study we examine the probability distribution of system unbalance, defined as max(/,.12.13). min{ll.lz./J
where I, is the instantaneous queue lcngth of P,, arid is sampled
during simulations. The distributions under the MIRr strategy
and the thrcshold strategy with a threshold of 2 are plattted in
Figure 4.1. Wc note that the mcan unbalance of the threshold
strategy is higher than that of the MRT. Also. there is a difference in the tail of the distributions. That is, thc threshold strategy
allows the system to become more unbalanced than the MRT
does. Since therc is a window of unccrtainty as to how unbalanced the system will remain over the course of a transaction, allowing a small unbalancecoupled with the i:caluction in
utilization due to remote call overhead-- leads to better response
time of the threshold approach.

Change of occurocy on localities ( X I

Figure 4.2 Performances of the MRT and threshold strategies
with inaccurate locality distribution

4.2

Discriminatory Strategy

The MRT strategy implicitly assumes, over the course of a
transaction. that the systcrn load will remain the same as at the
time whcn we make the dccision. Actually the system load
changes over thc lifetime of a transaction bccause of dcparturcs
and arrivals. The longcr the transaction, the larger the load dcviation is likcly to be. Hence, a dccision made at a transaction ar-

measurc of system unbolonce

Figure 4.1 Occurrence probabilities of sampled system imbialance

rival may not be optimal during its entire execution period. With
uncertainty about the future, making a decision that involves only
a few remotc calls is less riskier than one involving a large number
of remotc calls. When the load is unbalanced, the router should
try to improve the situation through many small corrections instead ol one large correction.

Consider the cases in which the router uses inaccurate locality distributions to estimatc the queue lcngth and response time
when making routing decisions. Simulations were conducted for
cases wherc an inaccurate locality distribution [qk,']is assumed in
the router for making routing decisions. Transactions issue database calls according to the actual reference distribution [qk,]during execution. In these simulations, [ Q , ] is set to the distribution
defined in Table 3.1. The cstimated distribution, [qk,'],is set ac-

Based on the above obscrvation, a refinement of ithe MRT
is to distinguish between short and long transactions when making

217

In Table 4.3, we show the performance of the
discriminatory strategy under unbalanced database loads, i.e.,
St:S$S$= 0.5:1:1 , We find that thediscriminatory strategy still
does better than the MRT. However, the improvement in re-

routing dccisions. We call this thc discriminatory strategy. For
example. we can apply a larger threshold to longer transactions
in the threshold approach. In the following, we considcr an ideal
situation. Each transaction class consists of two subclasses: one
long and one short which have 45 and 8 database calls, respectively, as in the Bernoulli distribution of Section 3.3. Assume we
can distinguish the subclasses from the input parametcrs associated with each transaction. We discriminate against long transaction by automatically route it to its preferred system. We only
adopt the MRT strategy when short transactions arrive -- i.e. we
attempt to minimize the rcsponse times in situations where, even
if we make a mistake, the exposure is low.

sponse time is not as grcat as in the balanced situation. In the case
of high load and communications overhead (only case wherc there
is a larger than 5% improvcment), long transaction response time
is only reduced by 7%; short transactions by 8%; and overall by
7%. What is more interesting is that in all the cases, the
discriminatory approach not only improves the response time of
short transactions but also that of long transactions.

By automatically doing preferred system routing for long
transactions, the discriminatory strategy gives up some opportunities that MRT can use to balance the systems. On thc other
hand. the discriminatory strategy avoids making non-preferred
routing decisions for long transaction which can result in a large
number of remote calls. In terms of response time we thus have
a trade-off: the discriminatory strategy increases system unbalance, but decreases total processor utilization. In Table 4.2, we
examinc the case for both high and low communications overheads under a balanced database load. We find that. over all
transaction-lcngth categories, and over all cases, the
discriminatory algorithm has lower response time than that of the
MRT. However, the reductions are less than 5% except for the
case of high communications overhead with S = 0.81. In this
case. the short transaction improve response time by 8 % ; the
long transactions by 11%; and overall system response time is
improved by 9%. Thc larger the transaction processing load and
the higher the communication ovcrhead. the larger the risk is For
non-preferred routing: the improvement in response time is thus
more apparcnt.

I

c-3K RT(short)
RT(1ong)

I

I

0.334
1.963

I

1

0.328
1.917

RT(overall)
W R

1

0.384
2.244

I
I

0.372
2.152

RT(overaU)

I

0.619

I

0.606

I

0.709

I

0.684

NPR

I

46.6%

I

40.9%

I

46.3%

1

40.9%

RT(1ong)
RT(overall)
NPR

0.384
2.249

I

0.711
19.5%

I

0.373

0.529

2.152
0.684
19.5%

3.062

2.741

0.972
18.2%

0.885
19.5%

0.727

1.152

1.067

32.89/0

25.3%

35.7%

improvement, the
discriminatory approach shows more robustness than the MRT
when inaccurate locality distribution is used. With the same parameters as in Figure 4.2, we plot the response times and swapping ratios of the discriminatory approach and of the MRT in
Figure 4.3, given that the number of database calls is with
Bornoulli distribution of Section 3.3. The discriminatory approach shows little sensitivity to the inaccuracy in the assumed
locality. The smaller swapping ratio of the discriminatory approach, comparing with that of the MRT, indicates that the risk

I
I

of non-preferred routing is reduced by selecting good candidate
for non-preferred routing, i.e., short transactions.

c=l5K
RT(short)

26.0%

In addition to the performance

-

1

I 0.741

Table 4.3 The performances of the discriminatory and the MRT
strategies (unbalanced database load)

s 0.81

S-0.71

I

L

0.486

5.0

Conclusion

Several conclusions can be drawn from the above discussions and experiments. First, the MRT provides quite a robust
performance even though it uses a steady state formula to estimate queue length and response time. However, it makes too
many non-preferred system routings in its attempt to share the
load among systems. This increases processing load/overhead
and system performance is then very dependent on accurate esti-

Table 4.2 The performances of the discriminatory and the MRT
strategies (halanced database load)
As NPR and utilization statistics indicate, the system is less
balanced under the discriminatory strategy than under the MRT.
Because processor utilization does decrease, the ultimate effect
is to decrcasc responsc time.

218

[BardSO] Bard, Y., "A Model of Shared DASD and Multipathk g , " Coninz. of the ACM, Vol. 23, No. 10, (Oct.
1950), pp. 564-572.
[Care841 Carey, M. J., Livny, M., and Lu. H., "Dynamic Task
Allocation in a Distributed Database System," Computer Science Technical Report 556, University of
Wisconsin-Madison, Sep. 1954.
[Chow791 Chow, Y-C. and Kohler, W. H., "Models for Dynamic Load Balancing in a Heterogeneous Multiple
Processor System," IEEE Tram on Conipirters, Vol.
C-28, No. 5, (May 19791, pp. 354-361.
[Corn861 Cornell, D.W., Dias, D.M.. and Yu, P.S., "Analysis
of Multi-system Function Request Shipping", ZEEE
Tran. on Sofrware Eng., Vol. SE-12, No. 10, Oct.
1956, pp. 1006-1017.
[EageSS] Eager, D.L., Eazowska, E.D. and Zahorjan, J., "A
Comparison of Receiver-Initiated and SenderInitiated Adaptive Load Sharing", Perforniance Evaluation Revzew, Vol. 13, No. 2 (Aug. 1984), pp. 1-3.
[Eage86] Eager, D.L., Lazowska, E.D. and Zahorjan, J.,
"Adaptive Load Sharing in Homogenous Distributed
Systems", IEEE on Soft. Erg., Vol. SE-12, No. 5,
May 8986, pp. 662-675.
[FerrSb] Ferrari, D., "A study of Load Indices for Load Balancing Schemes," Proc. of FJCC, Nov. 1956.
Ni. L. M. and Hwang, K., "Optimal Load Balancing
[NiSS]
in a Multiple Processor System with Many Job
Classes," ZEEE on Soft. Eng., Vol. SE-11, No. 5,
May 1985, pp. 491-496.
(Reis801 Reiser, M., Lavenberg, S. S . , "Mean-Value Analysis
of Closed Multichain Queueing Networks" Journal
of ACM,Apr. 1980, pp. 313-322.
[Shwe79] Shweitzer, P., "Approximate Analysis of Multiclass
Queueing Networks of Queues", Int. Conf on
Stochastic Control and Opfiniization North Holland,
Amsterdam, 1979.
[TantSS] Tantawi, A. N. and Tow.,'ey, D.. "Optimal Static
Load Balancing in Distributed Computer Systems,"
Joirnial of ACM,Apr. 1955, pp. 445-465.
[Thorn571 Thomaskam, A., "A performance Study of Dynamic
Load Balancing in Distributed Systems," Proc. of the
7th conference on Distributed Conipztring Sysrenis Sep.
1957, pp.175-184.
[Wang55] Wang, Y.-T., and Morris, R.J.T., "Load Sharing in
Distributed Systems", IEEE Trans. on Conipztters,
Vol. C-34, No. 3, (March 1955), pp. 204-217.
[Wolf881 Wolf, J., et. al., "Multisyste Coupling by a Combination of Data Sharing and Data Partitioning" Proc. of
the 4-th Ititl. Cotif on Data Engineering, Feb. 1987,
pp. 520-527.
Yu, P.S., Balsarno, S., and Lee, Y.-H., "Dynamic
[Yu56]
Load Sharing in Distributed Database Systems,"
Proc. of FJCC, Nov. 1986, pp. 675-683.
Yu. P.S., Dias, D.M., Robinson, J. T., Iyer, B.R., and
[Yu87]
Cornell, D. W., "On Coupling Multi-systems
Through Data Sharing," IEEE Proceeding, Vol. 75,
No. 5, May 1957, pp. 573-557.
[Zhou57] Zhou, S. , and Ferrari. D., "A Measurement Study of
Load Balancing Performance," Proc. of the 7th COHference on Distribirted Conzputirzg Systenis Sep. 1987,
pp.490-497.

W.01

-1SK
b o b m d d o b b l a Iadm

.....

.....

dlrrlrnlnobry

.....

......

...... .....
.....

.........

!!-

-..*

..........

URT

........

5
....... ....
I

e

8

...............

-

...........

......... ........

...........

........ .....................

I

...............
0 -

-0.3

-0.2

-0.1

0

0.1

0.2

Chonge of occurocy on locolities (XI

Figure 4.3 Performances of the MRT and discrilnlinatlory
strategies with inaccurate locality distribution
(no. of database calls is with Bornoulli distribution).

mation about each system's load. Our study shows that although
MRT strategy is insensitive to the assumption about Ube distribution of the number of database calls, it is sensitive 'to the accuracy of the assumed reference locality distribution. By reducing
non-preferred system routings and allowing wider load unbalancing, it is possible to further improve the system performance.
We have proposed a threshold approach which follows the h4RTs
routing decision only when the estimated load unbalancing is
above a given threshold, otherwise preferred systeini routing is
used. There are two significant results from the threshold approach. The first one is the ability to improve transaction response time even with a dramatic reduction in nomn-preferred
system routing. The other one is. more importantly, the
robustness that the threshold approach provides when the assumed transaction reference locality distribution is inaccurate.
Because risk of doing non-preferred system routing may be
greater for long transactions than for short transactions, we suggested the discriminatory approach in which the MRT sltratmegy is
applied only to short transactions, and the preferred !;ystem is
chosen for long transactions. Interestingly, both short and long
transactions show improvements in their response time. Furthermore, the robustness to inaccuracy in locality distribution is
improved.

References
[Agra82]

Agrawala, A. K., Tripathi, S. K., and liicart, G.,
"Adaptive Routing Using a Virtual Waitilng Time
Technique," IEEE trans. on Sofrware Erg., Vol.
SE-5, No. 1, Jan. 1952, pp, 76-81.

219

370

IEEE TRANSACTIONS ON COMPUTERS, VOL.

(13] L. -T. Wang, "Autonomous linear feedback shift register with on-line

fault-detection capability," in Dig. Papers, 12th Annu. Symp Fault Tolerant Comput. (FTCS-12), Santa Monica, CA, June 22-24, 1982,
pp. 311-314.
[14] L. -T. Wang and E. J. McCluskey, "A new condensed linear feedback
shift register design for VLSI/system testing," in Dig. Papers, 14th Annu.
Symp. Fault Tolerant Comput. (FTCS-14), Kissimmee, FL, June 20-22,
1984, pp. 360-365.
[15]
, "Built-in self test for random logic," in Proc. IEEE 1985 Int. Symp.
Circuits Syst. (ISCAS-85), vol. 3 of 3, Kyoto, Japan, June 5-7, 1985,
pp. 1305-1308.
[16]
, "Condensed linear feedback shift register (LFSR) testing-A
pseudo-exhaustive test technique," Center for Reliable Comput., Stanford Univ., Stanford, CA, Tech. Rep. 85-24, Dec. 1985.

The fault
manlteu
Itser

occurs

c-35,

NO.

4, APRIL 1986

Error Is
detected

I /

|*

I

lFautk

latency

I| ,

Error
latency

MD

I

Ttme

0 |
Detection time
Fig.. 1. A timing diagram for fault occurrence to error detection.
| <

pends on the detection mechanisms used. Fault latency is dependent
on the location and the type of the fault, and the degree of usage of
Measurement and Application of Fault Latency
the concerned unit. In other words, fault latency is closely related
KANG G. SHIN AND YANN-HANG LEE
to the physical property of a fault, whereas error latency represents
the efficiency of the detection mechanisms used.
Abstract -The time interval between the occurrence of a fault and the
A fault-tolerant computer system usually performs the detection
detection of the error caused by the fault is divided by the generation of and isolation of faults and errors, and then reconfiguration and
that error into two parts:fault latency and error latency. Since the moment recovery. These steps must be executed correctly by some of its
of error generation is not directly observable, all related works in the fault-free subsystems. In case of extant multiple faults, these steps
literature have dealt with only the sum of fault and error latencies, thereby are extremely difficult to accomplish, e.g., imperfect failure covermaking the analysis of their separate effects impossible. To remedy this age. It has been shown that an imperfect coverage is the major threat
deficiency, we 1) present a new methodology for indirectly measuring fault to highly reliable computer systems [2], [3]. Thus, the accumulation
latency, 2) derive the distribution of fault latency from the methodology, of latent faults and the near-coincident occurrence of faults should
and 3) apply the knowledge of fault latency to the analysis of two im- be considered in the modeling and verification of a reliable system.
portant examples.
The conventional modeling of a reliable system usually assumes that

The proposed methodology has been implemented for measuring fault
latency in the Fault-Tolerant Multiprocessor (FTMP) at the NASA Airlab.
The experimental results show wide variations in the mean fault latencies
of different function circuits within FTMP. Also, the measured distributions of fault latency are shown to have monotone hazard rates. Consequently, Gamma and Weibull distributions are selected for the leastsquares fit as the distribution of fault latency.
Index Terms -Detection time, fault and error latency, fault injection,
hazard rate, maximum likelihood estimator.

I. INTRODUCTION
A hardware fault is defined as an incorrect state caused by the
physical change in a component, whereas an error is defined to be
the erroneous information/data resulting from the manifestation of
a fault. Even after a hardware fault occurs in a computer system, the
system will remain error-free until the fault manifests itself. Before
its manifestation, the fault is latent and not harmful to any system
operations. Thus, there are two time intervals, as shown in Fig. 1,
between fault occurrence and error detection: fault latency and
error latency. Fault latency is the time interval between the occurrence of a fault and the generation of an error by the fault, whereas
error latency represents the interval between the generation of an
error and the detection of that error by some detection mechanism.
Conventionally, these two latencies are lumped together and called
the detection time or simply latency. Obviously, error latency de-

Manuscript received August 3, 1985; revised December 20, 1985. This work
was supported in part by NASA under Grants NAG-1-296 and NAG-1-492.
Any opinions, findings, and conclusions or recommendations expressed in this
publication are those of the authors and do not necessarily reflect the views

of NASA.
K. G. Shin is with the Division of Computer Science and Engineering,
Department of Electrical Engineering and Computer Science, University of
Michigan, Ann Arbor, MI 48109.
Y. -H. Lee is with IBM Thomas J. Watson Research Center, Yorktown
Heights, NY 10598.
IEEE Log Number 8607590.

the system is recovered from an extant fault if no new fault occurs

during the recovery period; otherwise, a coverage failure results.
The possibility of the accumulation of latent faults during fault latency also needs to be considered, especially when fault latency has
the same order of magnitude as the recovery period. It is thus essential to accurately evaluate both fault and error latencies.
In addition to the analysis of the coverage failure, the knowledge
of fault latency is important to the study of transient faults. A transient fault manifests itself only when its active duration is greater than
fault latency. If fault latency is long in a particular system, it is
possible that most transient faults will disappear before they harm
the system. In such a case, the transient faults captured in this

system cannot represent the true characteristics of all transient faults
in the system.
In the past, several researchers conducted experiments and simulations to investigate faults' manifestations and subsequent error
detections by injecting hardware faults [4]-[13]. Results were
observed through detection mechanisms following those fault injections. They measured the probability of detection and the distribution of detection times, which are the sum of fault and error
latencies, as mentioned earlier. Since there does not exist a direct
way to determine the moment of fault's manifestation, these experiments fail to indicate the moment of error generation that divides the
detection time into the two latencies. Instead, a combined effect of
the inherent fault property and an associated detection operation has
been observed via these experiments. Thus, these experiments
neither help us understand the behavior of a fault (fault occurrence
and manifestation) nor give an accurate measure of the capabilities
of detection mechanisms. In order to remove this inadequacy, we
develop here a new methodology to indirectly measure fault latency.
Once both the detection time and fault latency are measured, error
latency can also be computed.
This correspondence is organized as follows. In Section II, we
present a methodology to measure the distribution of fault latency.
This methodology has been implemented on the FTMP (FaultTolerant Multiprocessor) [14] at the NASA Airlab. Following a
brief introduction of the FTMP in Section III, the results and analy-

0018-9340/86/0400-0370$01.00 © 1986 IEEE

371

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 4, APRIL 1986

ses of experiments are provided in Section IV. Section V demonstrates how the knowledge of fault latency can be applied to
two important examples. The correspondence concludes with
Section VI.
II. METHODOLOGY FOR MEASUREMENT OF FAULT LATENCY

Recall that fault and error latencies are defined as subintervals
forming the detection time. Presented below is a new methodology
for measuring fault latency.
Suppose there are some detection mechanisms which are able to
detect the error generated by a fault f. Let tf represent the favlt
latency of this specific fault, which is a random variable with the
distribution function Ff(t). Denote the faultf with a finite active
duration ta(i) by a transient faultf. When a faultf is injected and
then dejected, we may observe two possible outcomes. If tf . ta(i),
then the fauilt has not yet manifested itself and hence no subsequent
error detection will occur. On the other hand, if tf < ta(i), then the
fault manifests itself, inducing an error which will be captured later
by the detection mechanisms. Let Xi be a random variable representing the outcome of an injection of fi. Then, Xi = 1 if there is a
detection, orX = 0 otherwise. It is easy to see that Prob(Xi = 1)=
pi = Ff(ta(i)). Note that the manifestation of a fault and the error
generated by the fault may take time to propagate and then be
detected, e.g., sequential circuits. Hence, the outcome of a fault
injection is continuously monitored awhile even after the dejection
of the fault.
When the fault fi is injected ni times, a sequence of Xi will be
, ni. Let the total number
observed, denoted by Xij for j = 1, 2,
of detections be di = 2j_IXij. Then, the maximum likelihood estimator of pi can be obtained by

_di

pi=-.
ni

(1)

Note that the maximum likelihood estimator of a binomial distribution is unbiased, i.e., E[p]- pi.
By injecting fauiltsf for i = 1, 2.. , k, we can obtain from (1)
a sequence of estimators {fr}1=- . This sequence can_then be used to
estimate the distribution function of fault latency, Ff(t). Note that
fitting Ff(t) with {pi}=I results in a least square error. Thus, the
estimated distribution function Ff(t) should minimize
X

(Ffi(ta(i))

- T)2 .

(2)

Note that the sampled data in the above experiment are not real data:
they tell us only whether there is a detection or not following each
injection of a transient fault. The sampled data are therefore not
affected by error latency. This also implies that the result of rneasurement is independent of the efficiency of the detection mechanisms used. As long as the error induced by the fault f can be
detected, we can obtain the distribution function of fault latency for

the faultf.

We cannot overemphasize the fact that the moment of error
generation- which divides the detection time into fault and error
latencies-is not directly observable. Although the occurrence of a
logic failure caused by a fault can be identified by comparison, the
logic failure does not always induce an error at the function level.
In other words, there may not exist a sensitized path such that the
faulty signal can propagate to the output stage. Consequently,
the above methodology measures indirectly fault latency. Due to the
"indirect" nature of our measurement, we obtain the distribution of
fault latency instead of actual samples of fault latency. Clearly, this
fact does not allow for any rigorous statistical analysis of our experimental data. Note, however, that the proposed indirect methodology
is the first and the only attempt to measure fault latency.

'This is equivalent to varying the active duration of a transient fault at the
time of injection.

III. EXPERIMENTAL SYSTEM: FTMP
The FTMP at the NASA Airlab is an experimental multiprocessor
system which consists of ten identical line replaceable units
(LRU's). Each LRU includes a processor module which contains
local cache memory, a shared 16K word memory, a 1553 I/O port,
two bus guardian units, a clock generator, and a power subsystem.
Any three processors can be grouped together into a triad. The
processor remaining after forming three triads is reserved as a spare
processor. Ten memory modules are also formed into three triads
and a spare. Communications between processors and the shared
memory are accomplished through serial system buses: a data transmit bus (T-bus), a data receive bus (R-bus), and a polling bus
(P-bus) for resolving bus contention. The system buses are also
arranged as triads by activating three out of five.
System configurations are controlled by bus guardians which
assign the connections between processors and the P-bus or T-bus,
and between shared memory and the R-bus. Two bus guardians at
each LRU form a dyad such that any transmission to system buses
will be enabled only when both guardians agree. The bus guardians
are also-used as a voter for any processor or memory triad. Since
three processors in one triad are operating in tight synchrony, their
respective bus guardians should receive three identical data under a
fault-free condition. When there is a disagreement, an error is considered to have occurred but masked, and the task execution will
continue. Meanwhile, the disagreement will be recorded at an error
latch for later identification of the faulty module or bus.
From the user's standpoint, FTMP is regarded as a three processor
system and has a shared 48K system memory among the three as
shown in Fig. t2. The current executive software dispatches tasks at
three different rates: RI, R3, and R4, which are 3.125, 12.5, and
25 Hz, respectively. Fault detection, identification, and system
reconfiguration are handled by an executive program called the
System Configuration Controller (SCC) which is dispatched at the
slowest rate RI . This is done so the execution of the SCC will have
a minimal effect on the system workload, and the errors generated
by a single fault will have an appropriate system response. For
experimental purposes, there are two application tasks installed on
FTMP: autopilot and display programs.
The associated fault injection system is controlled by a host VAX11/750 computer. The injection extenders can be inserted into any
chips at LRU3 and their respective socket holes such that the electrical connection between pins and the circuit board becomes controllable. Thus, three types of faulty signals, i.e., inverted signal,
stuck-at-], and stuck-at-O, can be injected at the pin level. Before
any injection, the host computer will signal FTMP to activate LRU3
for the fault injection. The detection, identification and reconfiguration intervals are measured by reading a real-time clock and
the responses from FTMP. This information is then transferred from
FTMP to the VAX- 1 1/750 via a 1553 I/O port and a communication
interface. Fault injection operations are processed by the FIS (Fault
Injection System) on the VAX-11/750. The FIS consists of a
command interpreter, an injection handler, and an FTMP-VAX
interface program.
For our experiments, the original FIS has been modified to enable
us to inject transient faults. Additional features are added to the
command interperter such that the active duration of a transient fault
can be specified and passed to the injection handler. Injection ends
if either the response of FTMP indicates the accomplishment of
detection, identification and reconfiguration, or the active duration
becomes larger than the specified value. In the latter case, FIS is
made to wait a few seconds for a possible response from FTMP.

IV. EXPERIMENTAL RESULTS AND ANALYSIS
To measure fault latency and demonstrate the methodology proposed earlier, transient faults were injected to four circuit boards of
FTMP, i.e., CPU Data Path, CPU Control Path, Cache Controller,
and System Bus Controller. The first three boards are in the CAP6
processor/cache region which is constructed with the AMD 2900
series bit-slice microprocessors. The System Bus Controller is re-

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-35, NO. 4, APRIL 1986

372

TABLE I
EXPERIMENTAL RESULTS AND ESTIMATED hf(t,) FOR VARIOUS PARTS OF FTMP:
(a) ON CACHE CONTROLLER, (b) ON CPU CONTROL PATH, (c) ON CPU DATA,
AND (d) ON SYSTEM BUS CONTROLLER

2.

sa-0

Oj0m )

pj

0.0
0.01
0.10
0.50
1.00
5.00
10.00
20.00

0.21
0.30
0.32
0.40
0.59
0.74
0.89

0.0

Pi

21.0
1.27

0.0
0.09
0.12
0.19
0.22
0.39
0.54

0.071

0.23
0.079

0.073

0.058
-

A bloc

_

~~pi+1

-

pi

- t(t1)
t~i) - Pi
The function hf(ti) becomes the (average) hazard rate of fault
latency as t+ I - ti -> 0. Despite the fact that negative numbers
appeared twice in Table I, the fu tions hf(ti) in the table strongly
suggest that the hazard r-ate of fault latency be monotone decreasing.
Thus, two distributions with monotone decreasing hazard rates,
i.e., Weibull and Gamma distributions, are used to fit the experimental results. Estimated parameters are given in Table II where the
h

2Obviously, there is

no use

of such

an

injection.

9.0

0.38

0.201

0.074
0.054
0.049

0.028

0.87

inverted

hjfi (tz )

'ii lti )

35.0
3.59
0.40
0.11
0.078
0.025
0.043

0.0
0.35
0.58

0.83

0.85
0.76
0.79
0.88

-

(a)

Fig. 2. A bloCk diagram Of FrMP.

sponsible for transferring blocks of words between a processor
region and the shared memory. It also serves as a synchronizing
mechanism such that the processors in a triad can be brought into
full synchrony. On each board, several pins are selected for injecting transient faults. Boards and pins are selected arbitrarily. For
each selected pin, stuck-at-0, stuck-at-i, and inverted signals
are injected.
A prime test was applied to each selected pin to observe whether
or not an error is generated and detected after the injection of a
permanent fault (which has an active duration of 3 seconds or
more). In Wimmergren's experiments on FTMP [12], undetected
faults a-re reported to exist. Possible explanation for the existence of
undetected faults are: 1) the circuits are not exerciset, 2) there are
"don't c'are" or redundant pins, and 3) the injected fault does not
cause any logic failure. In our experiments, injection of transient
faults is not made if there is no detection during the prime test. At
certain pins, errors are detected when stuck-at-0 and inverted signal,
but not stuck-at-i, faults are injected. In such a case, injection of
stuck-at-I faults is omitted.2
For each selected pin, transient faults with different active durations are injected 10-40 times repeatedly. At an early experiment,
we found that di lni increases sharply when'the transient durations
are small. Thus, to have good resolution, the active duration of
the transient faults injected, denoted by ti, are not equally distanced.
is, we used a finer resolution for small ti's and a coarser
T'hat
resolution for large ti's. Moreover, since the fault latency at the
System Bus Controller board is much larger than that at the other
boards, t,'s used for testing this board are different from those used
for the others.
Among more than 20000 transient faults injected, only 15 111
results are used for the analysis. The other data are regarded unreliable because: 1) the fault identified by FTMP was not in the LRU
where the fault was actually injected, 2) FTMP crashed during the
fault injection, and 3) one of the detection, identification and reconfiguration times was negative. If the second case occurred, the
injection was performed again. For every i and each type of fault at
different pins on a board, we calculatedpi-= diIni for each board,
which are listed in Table I. In addition, we present hf (ti) in the table
which is defined as

s-a-i

hi (t.

i
L t(mo}

0.0
0.01
0.10
0.50
1.00
10.00

20.00

s-a-0
Pi

67.0
9.09

0.0

0.87

1.87

0.94
0.98
1.00
0.98
1.00

s-a-l

hri)-JPi

2.00
0.01

hi (t.)

inverted
bft

pi

85.0

0.0
0.85
0.98
0.95
0.98
1.00
1.00

9.83

-3.75
1.20
0.11
-

(b)
s-a-O

p

0.0
0.01

0.0
0.25
0.42
0.47
0.59

0.097

0.72

0.030
0.025

O.Q5

0.10
0.50

1.00
5.00
10.00

20.00

25.0

5.87
1.72

0.588

0.038

0.61
0.67

0.79

-

s-a-i

inverted

he (ti )

Pi

0.0

34.0

0.0
0.49

0.48

0.42
0.44
0.088
0.052
0.041

h (t, )

n
t, (ma

j
0.34
0.41
0.55
0.65
0.77
0.83
0.90

2.85
1.89

br (tz )

0.64
0.85

0.90

-

0.91
0.92
0.92
0.93

h, j;)

p-;

49.0
7.35
11.7

0.83
0.40

0.28

0.0
0.0125

(c)
ti (mJ

-

0.0
5.0

0.20

10.0
20.0
50.0
100.0

200.0

300.0

0.0

0.15
0.24
0.30
0.43
0.82
1.00

s-a0

.s-a-i

, t
0.040
-.013

0.0108

0.0020

0.0037
0.0068
0.010

inverted
h, t

0.0

0.0

0.032

0.37
0.42
0.68
0.68

0.0079

0.35
0.56

0.0011

0.85
0.7*

0.18

0.88

1.00

0.050

0.0138

0.0058
0.01

0.63

0.79

0.88

0.038

0.038

0.021
0.016
0.0074
0.0053
0.001
-

(d)
least-squares errors are also included. The experimental results and
the estimated Weibull distribution are plotted in Figs. 3-6.
The estimated parameter for exponential distributions is also
presented in Table II for the purpose of comparison to Weibull and
Gamma distributions. It can be seen that the constant error generation,rate (i.e., exponential distribution) does not model the error
generation well. The mean fault latencies-which are 1/A in the
estimated parameter of the exponential distribution -range from
0.0005 ms of stuck-at-I faults in the CPU Control Path--to 125 ms
of stuck-at-0 faults in the System Bus Controller. This was due to the
different exercise rates at each board. Since each injected stuck-at-0
or stuck-at-I fault does not always represent a logic failure at the
moment of injection, the fault with an inverted signal should have
a shorter fault latency: this is confirmed by the experimental results.
As pointed out earlier, our experimental results give the distribution function of fault latency instead of data samples of fault latency.
Hence, statistical analyses or hypotheses testing are not applicable
to these experimental data. The least-squares estimation with commonly used distributions gives only approximate values of the

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 4, APRIL 1986

373

TABLE II
LEAST-SQUARES ESTIMATION OF THE DISTRIBUTIONS OF FAULT LATENCIES
Exponential

I/ X

error

I/X

4.78
13.07
0.46

0.24
0.08
0.41

4.35
15.24

i-a-0

0.009
0.005

s-a-0

s-a-0
s-a-i

CC

inverted

CPUC

s-a-i

a-a-i
inverted

CPUD

WeibuDl

I_/

a

error

0.35

0.58

0.51
0.20

0.03
0.015
0.009

0.004
0.003

0.0076

0.39

0.0006

0.0025

0.092

0.515
0.628

0.539
0.31
0.115

1.488
0.799
0.030

0.21
0.23
0.29

0.021
0.006
0.0026

125.2

0.063
0.097
0.029

124.9
54.85
39.10

0.89

0.061
0.020

0.036

0.001

0.27

Gamma
a

error

45.89

61.61
82.90

0.24
0.38
0.11

0.02

0.117

0.19

0.09

0.0008
0.0029

56.79

153.9

0.848

0.12
0.13
0.18

0.018
0.0013
0.032

173.2
178.18

0.77
0.44

0.057

CPU Control Path

0.008
0.007

Cache Controller
a-a-0

SBC

s-a-i

46.9
34.4

inverted

6.58
Q.70

0.0045

80.44

0.58

0.021

0.0006

CC-- Cache Controller, CPUC -- CPU Control Path
CPUD

-

CPU Data Path, SBC

-

System Bus Controller

,

Fig. 4. Experimental results and estimated distributions for
stuck-at-I faults.

CPU Control Path

_T
a

CPU Data Path

Cache Controller
0

-4

c
0

0;~

*0

0(

Cache

Controller

41

tC4

*0

I

4.00

8.00

1200

16.00

latency period ls)
Fig. 3. Experimental results and estimated distributions for
stuck-at-0 faults.

parameters. They cannot test whether an underlying model is (statistically) good or bad. Indeed, from the least-squares errors in Table II
it is unclear which distribution has the best fit. However, since the
hazard rate converges to 1 /A and 0 for Gamma and Weibull distributions, respectively, it is possible to distinguish between them once
additional injections with larger active durations are performed.

V. APPLICATION OF ESTIMATED FAULT LATENCY
A direct application' of the knowledge of fault latency is to obtain
error latency from the fault injection experiment. By relating this to
the modeling of faults and detection process one can also investigate
in detail the failure occurrence and detection in the system. In the
discussion to follow, two such applications are given to illustrate the
impacts of fault latency on the system reliability.
Usually, faults in a computer system are classified into three
categories: permanent, intermittent, and transient. Permanent faults
are continuous, and will manifest themselves after their fault latency
period is elapsed. Intermittent faults switch between active and

c

4.00

8.00

12 00

16.00

20.00

latency period las)
Fig. 5. Experimental results and estimated distributions for inverted
signal faults:

benign (or inactive) states. Thus, an intermittent fault will generate
an error when its active duration is longer than its fault latency.
Since the reappearance of an intermittent fault is a regenerative
process, the fault will eventually manifest itself. A more interesting
case happens when a faultis transient. After a transient fault occurs,
there are several possible transitions: the fault disappears before its
manifestation, an error is induced, or a second 'fault occurs, etc. A
transient fault may disappear before its fault latency is elapsed, and
have no effect on the system. When the active duration of a transient
fault ta has a distribution function Fa(t), the probability of generating an error by the transient fault is given as
Prob{ta > tf}= I - Ef[Fa(tf)]
(4)
where Ef[.] is the expectation with respect to fault latency.

374

IEEE TRANSACTIONS ON COMPUTERS, VOL.

c-35,

NO.

4,

APRIL

1986

Mean fault active duration = 10 ms
Distribution of fault latency: Exponential (-and +)

Weibull with the shape parameter

-0.8 (*and X)

'I

0
o 0

x

U)
L-i

0
z ,9

3
L).

stuck-at-0

0
0 0.0
cr o

0.02

0.04

0.08

0.06

MEA FAULT LATENCY (SEC)

0.10

(a)

'b.00

180.00
240.00
300.00
120.00
latency period (as)
Fig. 6. Experimental results and estimated distributions for fault latencies at
System Bus Controller.
60.00

This probability becomes smaller when fault latency is larger in
a stochastic sense. If transient faults are caused by temporary environmental conditions, a system with a longer fault latency is then
less sensitive to its environment than that with a shorter fault latency.
With the information of fault and error latencies, we can also
compute the probability of having accumulated faults when the
system is erroneous, given the occurrence of a fault. Let error
latency and the interarrival interval of faults be denoted by t. and tr,
respectively. Assume that tf, te, tr, and ta are all mutually independent. Also, denote the probabilities that a fault is permanent, intermittent, and transient by pp, pi, and p1, respectively. Then, the
probability of having multiple faults becomes

(pp + pi) Prob{tr
=

<

te + tf} + pt Prob{t.t

te + tf and ta 2

tf}

(pp pi)Ef[Ee[Fr(te tf)]] ptEf[(l Fa(tf))Ec[Fr(te tf)]]
+

+

+

-

+

(5)

where EeL[] is the expectation with respect to error latency and Fr(t)
the distribution function of t,.
The first term in (5) increases with tf, and the second term decreases with tf in case when the transient faults' disappearance rate
is larger than the fault occurrence rate. This observation can be
interpreted as follows. When fault latency is small, transient faults
may easily generate errors and multiple faults may be accumulated
mainly during error latency. On the other hand, when fault latency
is larger-although the system is insensitive to transient
faults- multiple faults may be accumulated during both fault and
error latencies, especially when faults are correlated to one another.
With the assumption'of independence between tf, te, tr, and ta,
example probabilities expressed in (5) are plotted in Fig. 7 (a) and
(b) where error latency, interarrival interval faults, and active duration o f transient faults are all assumed to be exponentially distributed.3 Note, however, that these figures show both the cases of
exponential and Wibull distributions of fault latency..The figures
also suggest that there exists an optimal fault latency in the sense of
3We use exponential distributions for the sake of a clear demonstration of use
of fault latency.

Mean fault active duration = 10 ms
Distribution of fault latency: Exponential (. and+)
Weibull with the shape parameter
= 0.6 (*and >,)
0

Xo
(o

-0

W

U

U.
0

0.

0.9

0.05

0.1

MEAN FAULT LATENCY (SEC)

0.1S

0.20

(b)
Fig. 7. Probability of having multiple fault accumulation versus mean fault
latency: (a) with mean error latency = 10 ms, and (b) with mean error
latency = 100 ms.

minimizing the probability of having accumulated multiple faults.
The mean error latencies used in Fig. 7 (a)'and (b) are 10 and
100 ms, respectively. It can be observed that, when error latency is
large, a small fault latency may not be helpful. This is due to the fact
that the system is less capable of detecting errors.
VI. CONCLUSION AND DISCUSSION
With the injection of transient faults, we have developed a new
methodology for indirectly measuring fault latency. The methodology has been realized, by experiments on FTMP. The FTMP
experimental results show a'large variation in fault latencies for
different circuits. It has also been observed that the haza:rd rate of
fault latency is monotone decreasing. This implies that a fault tends
to be latent if it did 'not generate an error at its earlIy. stage. The
existence of a long fault latency should not be ignored in highly
reliable systems.' To reduce the accumulation of latent faults, addi-

375

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-35, NO. 4, APRIL 1986

tional on-line diagnostics must be incorporated into the area where
a long fault latency exists. Such areas can be identified by the
methodology proposed in this correspondence.
Although two possible distributions are used to fit the experimental results, no underlying model for fault latency can be concluded.
It is mainly because of the unobservability of error generation. More
experiments should be designed to investigate the behavior of a fault
and its effect on system execution. An immediate extension of our
experiments is to make the injections under different system workloads or the execution of different application tasks. We expect to
see some variations of fault latency in certain circuits.
The methodology proposed in this correspondence is very simple
and easy to implement, but quite useful for characterizing fault
behavior. In view of the significant imbalance existing between
analytical modeling and experimental validation in fault characterization, it is necessary to advance experimental techniques to
support/validate theoretical results. The experimental methodology
proposed here has specifically aimed at meeting this need.

Functional Test Generation for Digital Circuits Described
Using Binary Decision Diagrams
MAGDY S. ABADIR AND HASSAN K. REGHBATI
Abstract -This correspondence presents a test generation methodology
for VLSI circuits described at the functional level. A VLSI circuit is
modeled as a network of functional modules such as registers, adders,
RAM's, and MUX's. The functions of the individual modules are described using binary decision diagrams. A functional fault model is developed independent of the implementation details of the circuit. A generalized D algorithm is proposed for generating tests to detect functional as
well as gate-level faults. Algorithms which perform fault excitation, implication, D propagation, and line justification on the functional modules are
also described.

Index Terms -Binary decision diagrams, D algorithm, fault model,
fault detection, functional faults, functional test generation.
I. INTRODUCTION

[1] K. G. Shin and Y. -H. Lee, "Error detection process -Model, design,
and impact on computer performance," IEEE Trans. Comput., vol. C-33,
pp. 529-540, June 1984.
[2] W. G. Bouricius, W. C. Carter, and P. R. Schneider, "Reliability
modeling techniques for self-repairing computer systems," in Proc. 24th
Annu. ACM Nat. Conf., 1969, pp. 295-309.
[3] A. L. Hopkins, T. B. Smith, and J. H. Lala, "FrMP -A highly reliable
fault-tolerant multiprocessor for aircraft," Proc. IEEE, vol. 66,
pp. 1221-1240, Oct. 1978.
[4] S.J. Bavuso et al., "Latent fault modeling and measurement methodology for application to digital flight control," in Proc. Advanced
Flight Control Symp., USAF Academy, 1981.
[5] B. Courtois, "Some results about the efficiency of simple mechanisms for
the detection of microcomputer malfunction," in Proc. 9th Annu. Int.
Symp. Fault-Tolerant Comput., 1979, pp. 71-74.
[6]
, "A methodology for on-line testing on microprocessors," in Proc.
1lth Annu. Int. Symp. Fault-Tolerant Comput., 1981, pp. 272-274.
[7] Y. K. Malaiya and S. Y. H. Su, "Reliability measure of hardware redundancy fault-tolerant digital systems with intermittent faults," IEEE Trans.
Comput., vol. C-30, pp. 600-604, Aug. 1981.
[8] P. Marchal and B. Courtois, "On detecting the hardware failures disrupting programs in microprocessors," in Proc. 12th Int. Conf. FaultTolerant Comput., 1982, pp. 249-256.
[9] J. G. McGougn and F. L. Swern, "Measurement of fault latency in a
digital avionic mini processor," NASA Contractor Rep. 3462, Oct. 1981.
[10]
, "Measurement of fault latency in digital avionic mini processes,"
NASA Contractor Rep. 3651, Jan. 1983.
[11] V. Tasar, "Analysis of fault-detection coverage of a self-test software
program,"'in Proc. 8thAnnu. Int. Symp. Fault-Tolerant Comput., 1978,

The rapidly growing complexity of VLSI systems has significantly increased the need for new efficient and cost-effective test
generation techniques. Classical test generation approaches [1]-[5]
operate at the gate and flip-flop level. As the number of gates and
interconnections becomes prohibitive, generating tests at higher
levels becomes a necessity. Moreover, the implementation details of
the unit under test (UUT) may not be disclosed by the manufacturer.
Hence, the problem of generating tests at the functional level has
become increasingly important for both the designers and the users
of VLSI systems.
Functional testing offers an alternative. Several methods have
been developed, especially for microprocessors [6]-[8]. Many researchers have proposed methods for generating tests for circuits
described using hardware description languages [9]-[11]. Essentially, a fault from a specified fault model is inserted into the functional description, and a mechanism is used to propagate the fault
forward. These approaches were motivated by the same reasons that
motivated our work. However, we favored the use of binary decision
diagrams as the functional description media [12]-[14] because they
model circuits at a level that is closer to its actual implementation.
For a comparison between the two description techniques and their
effect on testing, refer to [15]. Another extension of the D algorithm
to functional primitives [16] has the disadvantage that it assumes a
representation of any logic circuit using a number of built-in primitives for which various kinds of precomputed data are stored.
In this correspondence, we are concerned with formulating a
sound theoretical foundation for automatic test generation procedures for VLSI circuits. These procedures treat the architecture and
the functional description of the UUT as parameters. The correspondence is organized as follows. In Section II, a brief description of
the circuit model is given. A more detailed discussion of this model
can be found in [15]. A functional fault model capable of describing
faulty behavior at a high level is presented in Section III. In
Section IV, a test generation procedure is briefly described. The
procedure is a generalized version of the D algorithm [1] operating
at the functional module level [17]. Algorithms which perform the
basic operations of the test generation procedure, namely, fault
excitation, implication, D propagation, and line justification on the
functional modules, are presented in Section V.

[12] A. L. Wimmergren, "Verification of a fault tolerant multi-processor architecture," Charles Stark Draper Lab., Rep. CSDL-T-782, May 1982.
[13] J. H. Lala, "Fault detection, isolation and reconfiguration in FTMP:
Methods and experimental results," in Proc. 5th IEEEIAIAA Digital
Avionics Syst. Conf., Nov. 1983.
[14] T. B. Smith and J. H. Lala, "Development and evaluation of a faulttolerant multiprocessor (FTMP) computer volume I: Principles of operation," NASA Contractor Rep. 166071, May 1983.

Manuscript received August 15, 1985; revised December 13, 1985. This
work was supported by the National Science and Engineering Research Council
of Canada under Grant A0743.
M. S. Abadir is with the Department of Electrical Engineering-Systems,
University of Southern California, Los Angeles, CA 90089.
H. K. Reghbati is with the Department of Computing Science, Simon Fraser
University, Bumaby, B.C. V5A1S6, Canada.
IEEE Log Number 8607592.

ACKNOWLEDGMENT

The authors are grateful to M. Woodbury for his comments and
C. Liceaga, R. W. Butler, M. Holt, and B. Lupton at the NASA
Airlab for their assistance in the FTMP experiments.

REFERENCES

pp. 65-74.

0018-9340/86/0400-0375$01.00 C 1986 IEEE

	


	

	 !
		


	
			
		
		
		 !"#$"%#&'( 
)*		+
	

		
)

,	*		,		)-

.
		 
/*
0	**	 /1$(22(2$' 
)+	
*
	


3
4)

*	*5	

	3)	
)	 3'2''1 
6)+	

	




ranging from low-power CMOS circuit design to power
management software tools.
Initial power management efforts focused on putting
the system in a low-power/low-performance sleep mode
when it was idle. With the advent of the Advance
Configuration Power Interface (ACPI) standard, such
power management has been successfully employed in reallife systems. However, such approaches depend for their
efficacy on efficient ways to decide when and which device
should be shut down and woken up [1].
Processor cores can operate at different voltage
ranges to achieve different levels of energy efficiency. For
instance an ARM7D processor can run at 33MHz and 5V as
well as at 20MHz and 3.3V. The energy-performance
measures at these two operation modes are 185
MIPS/WATT and 579 MIPS/WATT, and the MIPS
measures are 30.6 and 19.1, respectively[2]. Another
example is Motorola’s PowerPC 860 processor, which can
be operated in a high-performance mode at 50MHz and
with a supply voltage of 3.3V, or a low-power mode at
25MHz and with an internal voltage of 2.4V[3]. The power
consumption in the high-performance mode is 1.3 Watts, as
compared to 241mW in the low-power mode.
The basic concept of power reduction in the variable
voltage processors is a technique called Voltage-ClockScaling in CMOS circuit technology. Power consumption
in CMOS digital circuits is proportional to the square of the
supply voltage (VDD).

Abstract
Scaling down power supply voltage yields a quadratic
reduction in dynamic power dissipation and also requires a
reduction in clock frequency. In order to meet task
deadlines in hard real-time systems, the delay penalty in
voltage scaling needs to be carefully considered to achieve
low power consumption. In this paper, we focus on
dynamic reclaiming of early released resources in Earliest
Deadline First (EDF) scheduling using voltage scaling. In
addition to a static voltage assignment, we propose a new
dynamic-mode assignment, which has a flexible voltage
mode setting at run-time enabling much larger energy
savings. Using simulation results and exploiting the
interplay between power supply voltage, frequency, and
circuit delay in CMOS technology, we find the optimal twolevel voltage settings that minimize energy consumption.

Keywords
Voltage scaling, dynamic reclaiming, scheduling, energy
and power optimization, real-time systems

1. Introduction
In recent years, the rapidly increasing popularity of
portable battery-operated computing and communication
devices have motivated research efforts toward low power
and energy consumption. Battery life is the primary
constraint for energy-constrained systems such as personal
mobile phones, laptop computers, and PDA’s. Significant
reduction in power consumption is possible from techniques

2
Pcmos = C L N swVDD
f

(1)

where CL is the output capacitance, Nsw the number of
switches per clock, and f the clock frequency.
The circuit delay td is given by the following equation
[4]:

* The work reported in this paper is supported in part by NSF under
Grants EIA-0102539 and EIA-0102696.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
CASES’01, November 16-17, 2001, Atlanta, Georgia, USA.
Copyright 2001 ACM 1-58113-399-5/01/0011…$5.00.

td = k

VDD

(VDD − VT )2

(2)

where k is a constant depending on the output the gate size
and the output capacitance, and VT is the threshold voltage.
As the clock frequency is inversely proportional to the

221

arrival instances or phases are not known a priori. We also
propose a new dynamic voltage assignment which can
result in much greater energy saving. We then discuss and
demonstrate how the voltage setting should be chosen so
that the dynamic algorithm and slack period reclamation
can be most effective.
This paper is organized as follows. In Section 2, we
outline the preliminary system model. In Section 3, we
describe the detail algorithms of voltage assignment and
slack period reclamation. To illustrate the effectiveness of
the proposed algorithms, we evaluate the energy saving
performance in Section 4. In the following section, we
present how to get the optimal voltage settings. Finally, a
short conclusion is provided in Section 6.

circuit delay, it is expressed using td and the logic depth of a
critical path, Ld [5].

f =

1
Ld t d

(3)

Obviously, from equation (1), lowering supply voltage will
reduce power consumption. However, it also slows down
the logic.
Based on these equations, a voltage-scaling scheme
was introduced in [6] by dynamically adjusting clock speed,
allowing a system to operate at a lower voltage level to save
energy without missing task deadlines. In [6] and [7], the
future workload is predicted based on past history, and the
appropriate voltage levels selected on the basis of such
predictions.
Clearly, if low energy consumption is a desirable
feature in real-time systems, voltage-clock scaling must
cooperate with the task scheduling algorithms since the
power-delay tradeoff property in low power design affects
meeting the strict time-constraints of real-time systems. For
instance, the execution of a high-priority at a low voltage
and slow clock rate may cause a low-priority task to miss
the deadline due to the additional delay from the execution
of the high priority task.
There have been a number of techniques that combine
voltage-scaling and scheduling algorithms for real-time
systems: we provide a sampling here. Using integer linear
programming (ILP), Ishihara and Yasuura presented a very
interesting result regarding the impact on energy of the
number of available distinct voltage levels [8]. They
pointed out that at most two voltage levels are usually
enough to minimize energy consumption. The concept of
real-time scheduling is first applied to dynamic speed
setting by Pering et al.[9]. A minimum-energy scheduler
based on the EDF scheduling policy was proposed in [10],
where the off-line approach was executed by assigning the
optimal processor speed setting to a critical interval that
requires maximum processing. Similar to the approach for
EDF scheduling in [10], a low energy heuristic for nonpreemptive scheduling is considered by Hong et al. in [11]
and the optimal voltage setting for fixed-priority scheduling
is studied in [12]. These approaches require that the task
release times must be known a priori. For the periodic task
model and rate-monotonic scheduling, two on-line voltagescaling methods [14] were proposed, which change voltage
levels at the execution stage from the initially assigned
levels as such changes become necessary.
In this paper we extend the resource reclaiming
proposed in [13] with EDF scheduling algorithms. Energy
savings are made possible by running tasks in low-voltage
mode during reclaimed slack periods that are released by
tasks that do not consume their entire worst-case execution
times (WCET). The algorithm proposed in this paper
makes the slack time reclamation possible even if the task

2. System Model
In real-time systems, tasks may arrive periodically or
sporadically and have individual deadlines. For Earliest
Deadline First (EDF) scheduling, a task τi is modeled as a
cyclic processor activity characterized by two parameters,
Ti and Ci, where Ti is minimum inter-arrival time between
two consecutive instances and Ci the worst-case execution
time of task τi.
The EDF scheduling algorithm always executes the
earliest-deadline task awaiting service. To apply voltageclock scaling under EDF scheduling, we make several
assumptions as follows:
A1. Voltage switching consumes negligible overhead.
This is analogous to the assumption, made in classical
real-time scheduling theory, that preemption costs are
negligible. Voltage switching typically takes a few
microseconds. In fact, a bound of the total overhead
can be calculated by considering the number of task
arrivals and departures since voltage switches are only
done at task-dispatch moments.
A2. Tasks are independent: no task depends on the output
of any other task.
A3. The worst-case execution demand of each task τi, i.e.,
Ci, is known. The actual execution demand is not
known a priori and may vary from one arrival instance
to the other.
A4. The overhead of the scheduling algorithm is
negligible when compared to the execution time of the
application.
A5. The system operates at two different voltage levels.
Ideally, a variable voltage processor that has
continuous voltage and clock setting in the operational
range is available.
The first four assumptions are analogous to
assumptions made in real-time scheduling theory [15]. As
for A5, we assume a simple setting arrangement that the
processor in a real-time system can be dynamically
configured in one of two modes: low-voltage (L) -mode and

222

high-voltage (H)-mode. In L-mode, the processor is
supplied with a low voltage (VL) and runs at a slow clock
rate. Thus, task execution may be prolonged but the
processor consumes less energy. On the other hand, the
processor can be set in H-mode, i.e. be supplied with a high
voltage (VH) and run at a fast clock rate, in order to
complete tasks sooner at the expense of more energy
consumption. The processing speeds at L-mode and Hmode are denoted as αL and αH, respectively, in terms of
some unit of computational work. Depending upon the
voltage setting for task τi, the worst-case execution time
(WCET) is Ci/αL or Ci/αH.
Assume that in the system there are n tasks, τ1,τ2,...,
and τn, which are numbered in decreasing priority order.
That is, D1≤ D2≤...≤ Dn where Di is the deadline of task τi.
Thus, the total utilization demand by all tasks is

1
αH

1

C

i∈H

(4)

i∈L

This optimization problem can be treated equivalently as
the decision problem of the subset sum, which is NPcomplete. Consequently, efficient search techniques e.g.,
branch-and-bound algorithms, should be employed to find a
solution if n is large.
For dynamic mode assignment, the subsets H and L
are determined for each busy cycle during which the
processor is busy continuously without any idle intervals.
We assume that H = {τ1,τ2, ..., τn} and L = ∅ at the
beginning of a busy cycle. At the first arrival instance of
task τj, the subsets can be modified to H-{τj} and L∪{τj},
respectively, if
Ci
Ci
1
1
+
≤ 1. (5)
α H i∈H −{τ } min(Ti , Di ) α L i∈L ∪{τ } min(Ti , Di )

∑

ρ = ∑i =1 C i Ti . Also, upon each invocation, the task τi
n

∑

j

must be completed before its deadline period Di. This is the
real-time requirement.
To minimize energy consumption in real-time
systems, the voltage-clock scaling problem is imposed
when αL<ρ≤ αH. The task set is schedulable if the processor
is entirely run in H-mode, and misses at least one deadline
if running in L-mode completely. Keeping the processor
entirely in H-mode to meet all tasks’ deadlines results in
extra energy consumption. Therefore, an algorithm is called
for to determine the optimal voltage settings and to
minimize H-mode execution, while guaranteeing that no
deadline is missed.

j

In other words, the tasks will be assigned to the low voltage
mode subject to the schedulability constraint and the
assignment is done in the order of the tasks’ first arrival
instances during each busy cycle. While the dynamic
assignment is not aimed at the minimization of the
execution period in H-mode, it takes advantage of short
busy cycles where no H-mode execution should be invoked.
Given the voltage assignments defined in H and L,
tasks can be dispatched and run in the assigned mode. The
sufficient condition guarantees that, if every task takes up to
its WCET, there is enough system capacity for each task to
be completed before its deadline. When a task consumes
less execution time than its WCET, there is a slack period
to indicate the remaining budget that is no longer needed
before the task’s deadline. Since the slack period will not
exist any more after the task’s deadline, we denote this
deadline as the expiration time of the slack period. Thus, a
waiting task that has a deadline greater than the expiration
time of a slack period can use up this slack period and run
in low-voltage mode. This reclamation algorithm is given in
Figure 1 where the following definitions are used:
ST-Q: the slack-time queue to track the slack periods given
from the tasks that don’t use up the whole WCET’s.
Each element in the ST-Q indicates an amount of
slack period of a completed task. A slack period has
an expiration time that is the deadline of the task
and can be consumed by any other tasks that have a
deadline earlier than the expiration time.
TK-Q: the normal EDF task queue for all tasks in the
system.
cti : the computing time that task τi has consumed for its
scheduled period of the EDF schedule. It doesn’t
include the running time during any available slack
periods of other tasks.

3. Voltage-Clock Scaling under EDF
Scheduling
The VCS-EDF scheduling is composed of mode
assignment and resource reclaiming phases. Assuming
tasks run to their WCET’s on EDF scheduling, mode
assignment picks the voltage setting for each task, i.e. H- or
L-mode, that will minimize the total energy consumed and
ensure schedulability under EDF. The reclaiming phase
dynamically switches voltage settings taking into account
the resources released by tasks that complete their work
ahead of WCET. The difference between WCET of a task
and the execution time it consumes is called a slack period.
The optimization problem can be stated as follows:
Pick H and L such that
• H∪ L = {τ1,τ2, ..., τn}
• H∩ L = ∅
• ∑i∈HCi/Ti is minimized
subject to the well-known sufficient condition1 for the
schedulability of periodic tasks under EDF, i.e.,

1

C

∑ min(Tii , Di ) + α L ∑ min(Tii , Di ) ≤ 1.

The condition is also necessary if Di≥Ti for all i.

223

st i : the slack time of task τi. The initial slack time is equal
to Ci /α - cti at the completion instance of τi, where
α is the speed factor of the assigned voltage mode.
enQueue(element, Q): insert an element into the queue of
Q.
deQueue(Q): delete the first element from the queue of Q.
head(Q): the element at the head of queue Q.
SL-decrement: the flag to indicate the slack period in the
head of ST-Q must be decremented as time
progresses. The flag will be on when a task is
using up the slack period or when the system is
idle.
To track the slack periods for all tasks and to combine
online EDF scheduling and voltage-clock scaling, we
employ two queues, i.e., TK-Q and ST-Q, in which tasks
and slack periods are ordered according to their deadlines
and expiration times, respectively. A task’s slack period is
computed when it completes the actual computation. The
slack period is inserted into ST-Q if it is greater than zero.
At the dispatch instance of task τi, the task is executed at
low voltage if there is a slack period can be reclaimed (i.e.
the expiration time of the slack period is less than the task’s
deadline). Otherwise, it is executed at its assigned voltage
mode. When a slack period is reclaimed by a task or when
the system is idle, the slack period at the head of ST-Q is
decremented as time progresses and is removed from ST-Q
once it is exhausted. On the other hand, a running task
needs to accumulate its computation time when it doesn’t
use up any slack periods. This implies that the execution is
using up other tasks’ budgets and, when the task is done, an
additional slack period can be accumulated due to its
reclamation and possible early completion.

Algorithm Slack-time Reclamation:
at the arrival instance avi of task τi {
if (TK-Q != empty) adjust-ct-st;
cti = 0;
set task deadline to avi+Di;
// enqueue the arriving task
enQueue(τi, TK-Q);
dispatch(TK-Q, ST-Q);
}
at the completion instance of a task {
adjust-ct-st;
τi = deQueue(TK-Q);
if (τi ∈ H) sti = Ci/αH – cti // set slack period
else sti = Ci/αL – cti
set sti’s expiration time to the deadline of task τi
if (sti > 0 ) enQueue(sti, ST-Q);
dispatch(TK-Q, ST-Q);
}
at the end of a slack period {
sti = deQueue(ST-Q);
dispatch(TK-Q, ST-Q);
}
Procedure adjust-ct-st {// adjust accumulated execution
// time and available slack periods
τj = head(TK-Q);
exec_time = the execution time
since the last dispatch instance;
if (SL-decrement==off)
ctj = ctj + exec_time;
else { stk = head(ST-Q);
stk = stk - exec_time;
}
}
Procedure dispatch(TK-Q, ST-Q) { // dispatch a task
// and a slack period, assign execution mode
if (TK-Q != empty) {
τj = head(TK-Q);
SL-decrement = off; // assume no slack time
if (ST-Q != empty ) {
stk = head(ST-Q);
if (deadlinej >= expiration_timek) {
// reclaim
SL-decrement = on;
set voltage_mode to L; // L- mode
set the end of slack period to
(current instance + stk );
}
if (SL-decrement == off) { // no reclamation,
// run the task with the assigned voltage
if (τj ∈ H) set voltage-mode to H;
else set voltage_mode to L; }
}
}
else {
// the system is idle,
stk = head(ST-Q);
// slack period must be
SL-decrement = on; // decremented
set the end of slack period to
(current instance + stk );
}
}

4. Simulation Evaluation of VCS-EDF
Algorithm
As described in Section 3, the static and dynamic
voltage assignment can guarantee all tasks to meet their
deadlines. Depending upon task characteristics, such as
periods, WCET’s, and arrival phases, the amount of time that
the processor is running in H-mode may vary a lot. In this
section, we evaluate their average energy consumption based
on random task parameters in a system of 10 real-time tasks.
For each test case, we measure the percentages of the time
that the processor is in H-mode and L-mode, and is idle. In
addition to the schemes of static and dynamic voltage
assignments with slack period reclamation, we also
considered a fixed voltage assignment which follow the static
assignment but with no slack period reclamation. Note that
the fixed assignment minimizes the energy consumption when
all tasks invoke the worst-case execution demands.
In our simulation, we first generate 10 random task
periods in the range of 100 to 1000, and assign the task
deadlines equal to their periods. We assume that VH and VL

Figure 1. On-line reclamation algorithm for voltage-clock
scaling

224

are fixed and the execution speeds are set to αL=1.0 and
αH=2.0. αL=1.0 represents the normalized processing
speed. Then, relatively the execution speed at αH=2.0 is
double the speed of L-mode. The worst-case execution
demands of the tasks are chosen in the range of αL to αH.
The real task execution demand is then selected randomly
such that the mean is in the range of 0.6 to 1.0 of the worstcase execution demand.
In Figures 2 (a) and (b), we show the evaluation
results for execution demands ranging from 1 to 2. The
curves with dashed lines are the percentage measures from
the fixed voltage assignment, whereas the solid lines are
based on the static and dynamic voltage assignments. With
the fixed mode assignment, the measured percentage in Hmode is approximately proportional to the average of the
actual workload.

The curves in Figures 2(a) and (b) show some
interesting trends. The difference of the measured
percentage in H-mode execution between the fixed-mode
and static-mode assignments increases with the workload.
This indicates the effectiveness of slack period reclamation
in a heavily loaded system. Also, the variance of task
execution time plays an important factor, as an increased
number of slack periods can be useful for a subsequent task
execution. Furthermore, the dynamic voltage assignment is
more effective when the workload is moderate (neither very
low nor very high). This is due to two facts. The first is that
the probability of finding short busy cycles diminishes as
we increase the workload. On the other hand, when the
workload is low, most tasks are started in L-mode in the
fixed assignment. The voltage assignment made by the
dynamic approach at the beginning of each busy cycle
would not be substantially different from the one under the
fixed approach.
We study the improvement of the proposed VCS-EDF
scheduling in Figure 3 where the percentage reductions of
the H-mode execution times over the fixed assignment
approach are depicted. For instance, with αL=1.0, αH= 2.0,
ρ=1.5, and the average execution time is 0.7of Ci, the
percentages of the time in H-mode under the fixed-, staticand dynamic-mode assignments are 35.04%, 26.08%, and
20.28%, respectively. These percentages indicate that,
compared to the fixed approach, the reductions of the time
in H-mode reach (0.35-0.26)/0.35=0.26(26%) and (0.350.20)/0.35=0.43(43%) for static and dynamic approaches,
respectively. The curves in the figure show that the dynamic
voltage scaling can almost completely eliminate H-mode
execution when the workload is low and the real task
execution time is much smaller than the WCET. This
significant result is due to (1) no pre-assignment of H-mode
execution, and (2) effective slack time reclamation. As we
increment the workload, the percentage reductions shrink as
the H-mode become necessary to meet deadline
requirement, even though the reduction of the H-mode
execution is substantial as shown in Figure 2. Figure 3 also
reveals the difference between the static and dynamic
approaches. As long as the workload is not high, there will
be many short busy cycles and few long ones. The dynamic
approach of making a greedy decision at the beginning of
each busy cycle is justified given that there is no knowledge
of subsequent task arrivals.
Using the percentage of the time spent in H-mode, Lmode, and in the idle state, we can measure the savings in
power consumption. Assume that VT=0 and that there is no
power consumption in the idle state (in practice, idle power
consumptions are extremely low, which justifies this
assumption). The ratio of the average power consumption
under the static approach to that of the fixed approach is

Percentage of the time in H-mode

7'
'
(
'
%
'
"
'
7
	

"'
%'

/8		
9)	*	:

('
#'
&'
1'
$'

	
9**	:

2'
'

2

2
2

2
$

2
1

2
&

2
#

2
(

2
%

Utilization in L-mode

2
"

2
7

$

(a) Percentages of the time in H-mode for fixed and

static voltage assignments

Percentage of the time in H-mode

7'
'
(
'
%
'
"
'
7

"'
%'

	

('
/8		
9)	*	:

#'
&'
1'
$'
2'
'


	
9**	:
2

2
2

2
$

2
1

2
&

2
#

2
(

Utilization in L-mode

2
%

2
"

2
7

$

(b) Percentages of the time in H-mode for fixed and

dynamic voltage assignments
Figure 2. The percentage of execution time in H-mode

225

5. Optimal Voltage Setting

PCMOS ( static ) PT H ( static )V H2 + PT L ( static )V L2
=
PCMOS ( fixed ) PT H ( fixed )V H2 + PT L ( fixed )V L2

α L 2 (6)
)
αH
=
α
PT H ( fixed ) + PT L ( fixed )( L ) 2
αH
PT H ( static ) + PT L ( static )(

where PTx(y) is the percentage of execution time in x-mode
under y approach. Similarly, we can compute the ratio
PCMOS(dynamic)/PCMOS(fixed). The ratios are plotted in
Figure 4 for various workloads. In the extreme case, more
than 25% of the energy can be saved by the dynamic
approach over the fixed assignment. Note that the fixed
assignment can outperform any global voltage setting, as it
uses the optimal voltage assignment for each task, but does
not incur any on-line adjustment.
'
7
'
(
'
%
'
"
'
7
	

;			)		

'
"

	
9)	*	:

'
%
'
(
'
#
'
&
'
1
'
$
'
2
'

	
9**	:
2

2
2

2
$

2
1

2
&
2
#
2
(
*	

2
%

2
"

2
7

$

Figure 3. The percentage of the reduction for the time in

H-mode
2
''
</8	
9)	*	:
,	*		

'
7#

'
7'

'
"#

'
"'
	
'
%#

'
%'
2
'

'
(
'
%
'
"
'
7


</8	
9**	:

2
2

2
$

2
1

2
&
2
#
2
(
*	

2
%

2
"

2
7

$
'

Figure 4. The relative energy consumption of fixed, static, and
dynamic approaches

226

In the simulation experiments of Section 4, we assume
fixed αL and αH (i.e. fixed VL and VH). Here, we address
the problem of determining the optimal voltage settings
given the characteristics of the given task set.
If the task execution demands are constant and every
instance of task τi requires a fixed demand of Ci, then VH
should be set to VH* which leads to a execution speed αH*
and ρ/αH *= 1. The processor will have a full utilization of
1. There is no need to have a different VL<VH* for voltage
scaling and any VH less than VH* can result in task deadline
violations. Also, any VH higher than VH* will cause
additional power consumption as the power consumption is
a concave and increasing function of the supply voltage.
When the real task execution demands are random
and are bounded by the worst-case demand, we still need to
set VH=VH* to guarantee schedulability in the worst-case
scenario. On the other hand, any settings of VL are feasible
under the VCS-EDF scheduling and lead to variant
execution times in H-mode and L-mode as well as the
average power consumption. In Figure 5, we have an
example of variant VL (αL) vs. execution time and power
consumption. We first adjust VH such that the processor is
fully utilized under the worst-case scenario. With the actual
execution times, we can utilize the slack periods and run the
tasks in L-mode. The execution sequences for different αL’s
are shown in Figure 5(a). With the assumptions that VT=0
and no power consumption at idle state, we can compute the
relative energy consumption for a special case where the
real workload is 66.7% of the worst-case execution
demand. With a slow execution speed and low power
consumption in L-mode, the processor must stay in H-mode
for a substantial amount of time. Thus, there is only a
marginal energy saving. On the other hand, if we set a high
αL and spend less time in H-mode, the total energy saving is
again diminished as a high VL must then be applied. As
illustrated in Figure 5(b), there exists an optimal setting of
VL, balances the invocation of H-mode execution and the
power consumption in L-mode.
In Figure 6, we present simulation results of the
optimal voltage settings using the DC characteristics of
Motorola’s MPC860 processor. At VH=3.3V, the processor
runs at an operating frequency of 50 MHz and has a power
consumption of 1.3 Watts. In addition, we assume that the
processing speed is proportional to the operating frequency
and that VT=0.7 which is the typical threshold voltage in
most CMOS circuits with 0.25-0.35µm technology. In our
experiments, the worst-case execution demands of 10 tasks
are chosen such that a full utilization is achieved at Hmode. Then, the supply voltage at L-mode, VL, is varying
from 0.7 to 3.3, and the real task execution demands are
generated as a random number such that the mean demand
is a proportion of the worst-case demand. Using the

schedulability condition is satisfied. The, at the online (runtime) stage, tasks are dispatched according to the EDF
algorithm and run at the assigned voltages except during the
slack periods. The other method, a dynamic approach, does
voltage assignment at the beginning of each busy cycle.
Subject to the schedulability constraint, it assigns tasks to
L-mode execution when the first instance of each task
arrives in the busy cycle.
Our simulation results demonstrate that the proposed
two approaches are quite effective comparing to a fixed
optimal task-base voltage assignment that does not perform
any online adjustment. The dynamic approach can
outperform the static one if no H-mode execution is
invoked during short busy cycles. Finally, we discuss the
selection of optimal voltage settings for given workloads.

simulation results of the dynamic VCS-EDF scheduling, i.e.
the percentages of time in H-mode and L-mode execution,
we can compute the average energy consumption for
various VL from the equations (1)~(3). The curves in Figure
6 indicate the optimal VL and the possible power savings
when a good candidate of VL is chosen.
WCET
in H-mode
0

1

Actual
Work

τ1

2 (H)

αL=0

τ1

2 (H)

αL=0.5

τ1

2 (H)

τ1

τ2

4

3

2

τ2

3

4

τ3
5

5

6

7

8

9

τ2

2 (H)

τ3

4 (H)

idle(L)

τ2

2 (H)

idle(L) τ3

4 (H)

2 (L)

τ2

1.5 (H)

τ3

1.5 (L)

τ3

3.625 (H)

τ3

3 (H)

10

11

12

idle(L)

τ2

αL=1.5
αL=2.0

τ2

2 (H)

τ1

2 (H)

τ1

2 (H)

0

1

2 (L)

τ2

2 (L)

τ2

2 (L)

2

3

τ3

1 (H)

2 (L)

τ2=0.5(H)
τ3 2.5 (L)
τ3
5

2
1

2.125 (H)

2
$

τ3 1(H)

3 (L)

4

τ3

6

7

8

9

10

11

12

	

τ1

αL=1.0

(a) Task executions according to the speeds of two modes

%
110
L-mode Execution

100

Relative Energy Consumption

2
=>$
$2&
'
7

90

'
%
80

'
(
'
#

70
1.82
8.33
50

100

21.1

=>$
2%(

'
"

H-mode Execution

60

=>$
$#$

2
2

'
(
'
%
'
"
'
7

=>$
'"%

	
2

2
#

$
	=*	

$
#

1

1
#

Figure 6. The optimal L-mode voltages VL (VH= 3.3 V,

41.7

1.3 Watts, and 50 MHz)

40
66.7
59.4

30

50.0
20

References

38.5
25.0

[1] L. Benini, A. Bogliolo, and G. De Micheli, “A Survey

10
0

WCET

0

0.5

1.0

1.5

2.0

of Design Techniques for System-Level Dynamic
Power Management,” IEEE Trans. on Very Large
Scale Integration Systems, Vol.8, No.3, pp. 299 –316,
2000.

αL

(b) Relative energy consumptions for different αL
Figure 5. An example of finding the optimal αL

[2] “Introduction to Thumb,” ARM Documentation,

6. Conclusion

Advanced RISC Machines Ltd.

In this paper, we investigate the voltage-clock scaling
problem in real-time embedded systems. Under EDF
scheduling, we propose two scaling methods to reduce
power consumption. Both methods utilize L-mode
execution during task slack periods that come from the
discrepancy between the worst-case and the real
computation times. The static approach assigns a fixed
voltage setting for each task in the offline phase such that
the average execution time in H-mode is minimized and the

[3] MPC860 PowerPC Hardware Specification,
MPC860EC/D, Motorola, Dec. 1998.

[4] A. P. Chandrakasan, S. Sheng, and R. W. Brodersen,
“Low-power CMOS digital design,” IEEE Journal of
Solid-State Circuits, 27(4), pp. 473 –484, 1992.

[5] K. Nose and T. Sakurai, “Optimization of VDD and

VTH for Low-Power and High-Speed Applications",
ASPDAC'00, A6.1, pp.469-474.

227

[6] M. Weiser, B. Welch, A. Demers, S. Shenker,

[11] I. Hong, D. Kirovski, G. Qu, M.P otkonjak and M. B.

“Scheduling for reduced CPU energy,” Proceedings
of 1st USENIX Symposium on Operating Systems
Design and Implementation (OSDI’94), pp.13-23.

Srivastava, “Power Optimization of Variable Voltage
Core-based Systems,” Proceedings of the 35th annual
conference on Design Automation Conference
(DAC’98), pp.176–181.

[7] T. Pering, T. Burd, and R. Brodersen, “The
Simulation and Evaluation of Dynamic Voltage
Scheduling algorithms,” Proceedings of International
Symposium on Low power Electronics and Design
(ISLED’98), pp.76-81.

[12] Gang Quan and Xiaobo (Sharon) Hu “Energy
Efficient Fixed-Priority Scheduling for Real-Time
Systems on Variable Voltage Processors,”
Proceedings of 38th Design Automation Conference,
2001.

[8] T. Ishihara and H. Yasuura, “Voltage scheduling
problem for dynamically variable voltage processors,”

[13] C. M. Krishna and Y. H. Lee, “Voltage-Clock-Scaling

Proceedings of International Symposium on Low
power Electronics and Design (ISLED’98), pp. 197–

Adaptive Scheduling Techniques for Low Power in
Hard Real-Time Systems,” IEEE Proceedings of Real
Time Technology and Applications Symposium
(RTAS 2000).

202.

[9] T. Pering and R. Brodersen, “Energy Efficient

[14] Y. H. Lee and C. M. Krishna, “Voltage-Clock Scaling

Voltage Scheduling for Real-Time Operating
Systems," The 4th IEEE Real-Time Technology and
Applications Symposium, Works In Progress Session,
1998.

for Low Energy Consumption in Real-Time
Embedded Systems,” Real-Time Computing Systems
and Applications (RTCSA’99).

[15] C.L.Liu and J. Layland, “Scheduling Algorithms for

[10] F. Yao, A. Demers, and S. Shenker, “A Scheduling

Multiprogramming in a Hard Real-time
Environment,” Journal of the ACM, Vol.20, pp.46-61,
1973.

Model for Reduced CPU Energy,” IEEE Foundations
of Computer Science, pp.374-382, 1995.

228

Pfair Scheduling of Periodic Tasks with Allocation Constraints on
Multiple Processors
Deming Liu Yann-Hang Lee
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ, 85287
{dmliu, yhlee}@asu.edu
Abstract
Pfair scheduling for periodic tasks on multiple
processors in a time slotted environment can not only
guarantee task deadlines to be respected, but also make
tasks execute at steady progressive rates. In this paper we
consider pfair schedulability of periodic tasks with
allocation constraints in the sense that some tasks can
only be assigned to one or more specified processors. The
contributions of the paper lie in the following four
aspects. Firstly, we prove that there exists pfair schedule
in the case that fixed tasks are assigned to disjoined
processor subsets. Secondly, we give a sufficient and
necessary condition of the existence of pfair schedule for
periodic tasks with arbitrary allocation constraints.
Thirdly, we show that pfair schedulability test of periodic
tasks with arbitrary allocation constraints can be done in
polynomial time. Finally, an on-line approximate pfair
scheduling algorithm called HPA (hierarchical pfair
algorithm) is proposed for scheduling fixed and migrating
tasks. With HPA although idea a fixed task is not
guaranteed to respect its deadline completely, the amount
of time by which it misses its deadline is bounded.

1. Introduction
Multiprocessor systems, characterized by their
performance and reliability, have evolved to be a powerful
computing platform widely used in many real-time
applications. To fully utilize the computing resources of
such kind of systems, multiprocessor scheduling is the key
concern of determining when and on which processor a
pending task can get executed.
In typical real-time systems, computation is invoked by
periodic tasks. A periodic task consists of an infinite
sequence of jobs that are released periodically with a
relative deadline equal to task period. In uniprocessor
systems, schedulability test of a set of periodic tasks can
be done as simple as inspecting the total utilization of all

tasks, where the utilization of a task is defined as the ratio
of its execution requirement over its period. For instance,
when tasks are preemptable, EDF (earliest deadline first)
algorithm is optimal if the total utilization of all tasks is
not greater than 1 [4]. Generally when scheduling a set of
tasks with preemption allowed, we assume a zero-time
context switch constraint such that at any time, the
processor takes no time switching from one task to another
task. However, as far as multiprocessor scheduling is
concerned, in addition to zero-time context switch
constraint, task constraint and processor constraint must be
considered [9]. The task constraint states that at any time,
no task can be executed at more than one processor. The
processor constraint prescribes that at any time, no
processor can run more than one task. In a system with m
identical processors, where the three constraints are
required, the necessary and sufficient condition of
schedulability is that the total utilization must be less than
or equal to m [9][10][11]. However, to find a feasible
schedule, nonintegral preemptions may be need, causing
difficulties in implementation and limiting the actual
applications.
Basing on slotted time, Baruah et al. proposed the
concept of pfair (proportionate fairness) scheduling [1][2],
which gives a much stricter requirement than task deadline
requirement. With pfair it is assumed that all input
parameters, including task period, execution time, and
release time, are integers. Accordingly task preemptions
can only happen in integral time instants. This assumption
of limiting parameters to integers makes the scheduling
algorithms easy for implementation. Let a task x’s period
and execution time be px and ex, respectively. With pfair
scheduling, the amount of processing time that a task x
receives between time 0 and an integral time instant t is
either ¬wxt¼ or ªwxtº, where wx=ex/px is the weight or
utilization of task x. In other words, pfair scheduling can
not only guarantee task deadlines to be respected, but also
make tasks execute at steady progressive rates. By
converting the pfair scheduling problem to an integral

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

network flow problem, Baruah et al. proved that
¦ x∈T wx ≤ m , where m is the number of identical
processors and T is the set of all tasks, is both necessary
and sufficient for the existence of a pfair scheduling. A
pfair scheduling is to determine which m (suppose n ≥ m)
tasks are selected to assign the m processors at each time
slot. Although a pfair schedule can be found by solving the
maximum flow of a constructed graph, it unfortunately
needs exponential time in the worst case.
Baruah et al. gave an on-line pfair scheduling algorithm
with running time of O(min{mlogn, n}) per slot in which
deadlines are compared and ties are broken in constant
time by inspecting 4 different parameters [6]. Basing on
Baruah et al’s work, Anderson et al. reduced the number
of tie-breaking inspections from 4 to 2 in PD2 algorithm
[5][12]. Moir and Ramamurthy considered pfair
scheduling of fixed and migrating tasks on multiple
processors in which a fixed task can only be run on a
specific processor, whereas a migrating task can be run on
any processor. They proved that pfair schedule exists in
this case either [3]. We use the allocation constraint of a
task to refer to the requirement that the task can only be
assigned to a given subset of processors. In other words,
allowed to migrate among the given subset of processors,
the task, however, cannot be executed on an arbitrary
processor. Adding allocation constraints to multiprocessor
scheduling is significant in actual applications. For
example, only several hosts in a multiprocessor system can
provide a type of service for few tasks. Similarly in
communication networks, several channels of a data link
can transfer a specific type of data message due to traffic
control scheme, security and routing constraints. In this
paper, we will consider the schedulability of periodic tasks
with more general allocation constraints and investigate
the related feasibilities of pfair scheduling. The
contributions of the paper lie in the following 4 aspects.
Firstly, we prove that there is a pfair schedule under the
constraints that tasks are assigned to disjoined and
dedicated processor sets. Secondly, we give a sufficient
and necessary condition of the existence of pfair schedule
for periodic tasks with arbitrary allocation constraints.
Thirdly, we show that the pfair schedulability test of
periodic tasks with arbitrary allocation constraints can be
done in polynomial time. Finally, an on-line approximate
pfair scheduling algorithm called PHA is proposed for
scheduling fixed and migrating tasks. With PHA although
idea a fixed task is not guaranteed to respect its deadline
completely, the amount of time by which it misses its
deadline is bounded.
The rest of the paper is organized as follows. In Section
2, the related concepts, definitions and notations are
introduced. In Section 3, we use a network flow approach
to prove the existence of pfair scheduling of periodic tasks
with a special allocation constraints, i.e., tasks are assigned

to disjoined and dedicated processor sets. Section 4 gives
the necessary and sufficient condition of existence of pfair
scheduling for periodic tasks with arbitrary allocation
constraints. In Section 5, we show that the condition in
Section 4 can be use to test pfair schedulability in
polynomial time. In Section 6, an approximate on-line
pfair scheduling algorithm is addressed for scheduling
fixed and migrating periodic tasks. Section 7 concludes
this paper.

2. Definitions, Concepts and Notations of
Pfair
In this paper, we take into account the scheduling
problem of synchronous periodic tasks over multiple
processors with allocation constraints in addition to
processor, task and zero-time task switch constraints. We
conform to most of the definitions and notations in [1] and
[3].
• Scheduling decisions occur at integral values of
time, numbered from 0. The real interval between
time t and t+1 (including t and excluding t+1) will
be referred as slot t, where t ∈ N = {0, 1, 2, …}.
• For integers a and b, let [a, b) = {a, …, b-1}.
Furthermore, let [a, b] = [a, b+1), (a, b] = [a+1,
b+1), and (a, b) = [a+1, b).
• We consider an instance Φ of the fair sharing
problem with m processors and n tasks. Without
losing generality, we assume n ≥ m. Specific tasks
will be denoted by identifiers x and y, which range
over T, the set of all tasks. Specific processors will
be denoted by identifiers r0, r1 and so on, which
range over R, the set of all processors.
• A task x has an integer period px, px > 1, an integer
execution time ex, ex ∈ (0, px], and a rational
weight (utilization) wx = ex/px. Note that 0 < wx ≤
1. Without losing generality, we confine our
investigation to the case that ¦ wx = m . If
x∈T

¦x∈T wx < m , we can add one or several dummy
tasks to make
•

¦x∈T wx = m .

A schedule S for instance Φ is a function from T ×
N to {0, 1}, where ¦ S ( x, t ) ≤ m , t ∈ N.
x∈T

•

Informally, S(x, t) = 1 if and only if task x is
scheduled in slot t, 0 otherwise.
The lag of a task x at time t in schedule S is
defined by
lag ( S , x, t ) = w x t − ¦ S ( x, j ).
j∈[ 0 ,t )

•

A schedule is periodic if and only if
∀i, x : i ∈ N, x ∈T :
S ( x, t ) = ie

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

¦

t∈[ 0 ,ip x )

x

•

A schedule is pfair if and only if
∀t, x : t ∈ N, x ∈T : − 1 < lag ( S , x, t ) < 1
Periodic schedules can also be defined in terms of lag
constraints. In particular, a schedule S is periodic if and
only if
∀i, x : i ∈ N, x ∈T : lag ( S , x, ip x ) = 0
It is easily to see each pfair schedule is periodic. The
converse is not true.
With respect to instance Φ of the resource sharing
problem, let earliest(x, i) (latest(x, i)) denote the earliest
(latest) slot during which task x may be executed for the
ith time, where i ∈ N. We call the part of job executed
(exactly one slot) in [earliest(x, j), latest(x, j)] for task x a
subtask of task x. It is not difficult to know that the
following two propositions are equivalent.
• ∀t, x : t ∈ N, x ∈T : − 1 < lag ( S , x, t ) < 1
• The ith subtask of task x is get executed in time
slot window, [earliest(x, i), latest(x, i)] for all i ∈
N, where
earliest ( x, i ) = ¬i / wx ¼
latest ( x, i ) = ª(i + 1) / w x º − 1
Therefore a pfair schedule can also be defined by using
notations of earliest and latest. A schedule is pfair if and
only if the ith subtask of task x gets executed in [earliest(x,
i), latest(x, i)] for all i ∈ N, where earliest(x, i) and
latest(x, i) are defined in aforementioned equations. Note
that, for a pfair schedule, earliest(x, i) ≤ latest(x, i), x ∈ T,
i ∈ N. Furthermore, earliest(x, i+1) – latest(x, i) is either 0
or 1. Note that periodic tasks can arrive indefinitely, but
we only need to find a pfair schedule in a finite interval.
This idea is shown in Lemma 1.
Let L denote the least common multiple of the task
periods, i.e., L = lcmx∈T p x . We quote the following lemma

from [1].
Lemma 1. Instance Φ has a pfair schedule if and only
if a schedule S exists such that
∀x, t : x ∈ T , t ∈ (0, L ] : −1 < lag ( S , x, t ) < 1 .
The correctness of Lemma 1 is apparent since a pfair
schedule for an infinite task sequence can be obtained
from S by scheduling in slot t these tasks scheduled by S in
slot t mod L.
In the following text, we will use a network flow
technique to accomplish most proofs. This approach has
been used in previous work studying pfair scheduling
[1][3]. Here we give a simple introduction to some results
about network flow that we are going to use. For a graph
G = (V, E) (V and E are vertex set and edge set
respectively), in which two special vertices, source and
sink, are given and each edge has an invariable capacity,
we say G has a flow of size q if there is an amount of q
flow carried by G from source to sink so that no edge can
carry a flow more than its capacity. We refer flow

assignment to assigning each edge of G a flow value less
than or equal to its capacity. Then G has a flow of size q if
and only if there exists a flow assignment for G so that the
following equations hold. We refer to these equations as
flow rules.
(i)
fi(source) = 0, fo(source) = q
(ii)
fi(v) = fo(v), ∀v ∈ V, v ∉ {source, sink}
(iii) fi(sink) = q, fo(sink) = 0
where fi(v) (fo(v)) is the total incoming (outgoing) flow of
vertex v.
To prove that a graph G has a flow of size q, a
reasonable approach is to find a flow assignment to make
the flow rules hold. In the following sections, we will use
this approach in several proofs.

3. Existence of Pfair Schedule with Dedicated
Processor Set Constraints
Consider an instance of multiprocessor systems in
which processors are partitioned into disjoint subsets and
each task can only be assigned to either one subset or the
whole processor set. In other word, a fixed task can only
be executed in a dedicated processor subset whereas a
migrating task can be run in any processor.
Let the instance Φ be defined as a tuple {{T0, T1, …,
Tg}, {R0, R1, …, Rg}} that represents a pfair
multiprocessor scheduling problem of periodic tasks with
dedicated processor set constraints, where Ti and Ri, i∈[0,
g], are non-empty partitions of T and R, respectively, and
T0∪ T1∪…∪Tg=T and R0∪R1∪ …∪Rg=R. The dedicated
processor set constraints require that a task in Ti can only
be executed in Ri, if i∈[0, g), or can migrate to any
processor in R if i=g. Let m denote |R| and mk denote |Rk|,
k∈[0, g]. Clearly ¦
mk = m . Basing on the weight of
k∈[ 0, g ]

task x, wx, we define w(Ti) to be total weights of the tasks
in Ti. Thus w(Ti ) = ¦ wx . We also have the
x∈Ti

assumption, w(Ti ) ≤ mi , for i ∈ [0, g). Accordingly w(Tg) ≥
mg.
Theorem 1. Instance Φ is pfair schedulable.
Before proving Theorem 1, we need to define a digraph
G = (V, E) such that V = V0 ∪ V1 ∪ V2 ∪ V3 ∪ V4 ∪ V5 and
E = E0 ∪ E1 ∪ E2 ∪ E3 ∪ E4. The vertex sets and edge sets
are defined as follows.
V0 through V5 are the vertex sets of G.
V0 = {source}
V1 = {¢1, x, i² | x ∈ T, i ∈ [0, wxL)}
V2 = {¢2, x, j² | x ∈ T, j ∈ [0, L)}
V3 = {¢3, x, j² | x ∈ T, j ∈ [0, L)}
V4 = {¢4, k, j² | k ∈ [0, g], j ∈ [0, L)}
V5 = {sink}
E0 through E4 are the edge sets of G. Each edge is
represented by a tuple in the form of (u, v, w) with the

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

meaning that the edge is from u to v with capacity w,
where u, v ∈ V.
E0 = {(source, ¢1, x, i², 1) | x ∈ T, i ∈ [0, wxL) }
E1 = {(¢1, x, i², ¢2, x, j², 1) | x ∈ T, i ∈ [0, wxL), j ∈
[ealiest(x, i), latest(x, i)]}
E2 = {(¢2, x, j², ¢3, x, j², 1) | x ∈ T, j ∈ [0, L)}
E3 = {(¢3, x, j², ¢4, k, j², 1) | x ∈ Tk, k ∈ [0, g), j ∈ [0,
L)} ∪ {(¢3, x, j², ¢4, k, j², 1) | x ∈ Tg, j ∈ [0, L), k ∈ [0,
g]}
E4 = {(¢4, k, j², sink, mk) | k ∈ [0, g], j ∈ [0, L)}
It is easy to see that the vertices of G represent the tasks
and the processors of instance Φ over [0,L). For instance, a
vertex in V1, i.e. ¢1, x, i², acts for the i-th subtask of task x,
and a vertex in V2, i.e. ¢2, x, j², is a subtask of task x to be
executed in slot j. Thus, a flow assignment of 1 on the
edge from ¢1, x, i² to ¢2, x, j² indicates that the i-th subtask
is to be executed in j-th slot. It can also be noticed that V2
is the same as V3 and there is an edge of capacity 1 from
each vertex ¢2, x, j² to ¢3, x, j², forming the edge set E2.
Doing this avoids any task to occupy processors in the
parallel way [3]. Before proceeding to prove Theorem 1,
we need Lemma 2.
Lemma 2. A pfair schedule exists for instance Φ if and
only if there is a flow of size of mL in G.
Proof.
(The if part):
Since all edges of the graph G have integral capacity, if
there is a fractional flow of size mL (an integer) in G, then
there is an integral flow of size mL [7]. Consequently if
there is a flow of size mL, we just suppose that there is an
integral flow of size mL without losing any generality.
Each edge in E0 is filled to its capacity of 1 because the
total number of edges in E0 is equal to

¦

x∈T

w x L = mL.

Thus each vertex in V1 receives a unit flow. Accordingly,
each vertex in V1 sends a unit flow to some vertex in V2.
This guarantees that exactly one unit of flow for any task x
is sent within each [earlist(x, i), latest(x, i)], i∈ [0, wxL).
Therefore, the following condition is satisfied.
∀x, t : x ∈ T , t ∈ (0, L ] : −1 < lag ( S , x, t ) < 1
Since each vertex in V2 has only one outgoing edge of
capacity 1 leading to a vertex in V3, there is no two units of
flow can leave a vertex in V2 and reach a vertex in V3. In
fact, we can think about a vertex ¢3, x, j² as a subtask of
task x to be executed in the j-th slot. Thus, an integer flow
assignment from V3 to V4 will be a task allocation to
processor set. According to the construction of E3, two
vertices in V3, ¢3, x, j1² and ¢3, x, j2² (x ∈ T, j1, j2 ∈ [0, L), j1
≠ j2), cannot have edges led to a common vertex of V4.
This guarantees that occupying a time slot in the parallel
way (using more than one processor) for a task x is
impossible. Hereby the task constraint is satisfied.
The total capacity of edges in E4 is ¦
mk L = mL .
k ∈[ 0 , g ]

Now that we have an integral flow of mL, each edge in E4

reaches its capacity. This means no processor is idle
during each time slot, as well as that no processor executes
more than one task in any time slot. Thus the processor
constraint is held.
As the result, we have shown that there is a pfair
schedule during (0, L]. By Lemma 1, instance Φ has a
pfair schedule.
(The only if part):
Suppose Φ has a pfair schedule. Then there is a pfair
schedule for Φ in (0, L] by Lemma 1. The schedule of a
subtask, including the slot it is executed and the processor
it is assigned, indicates an integer flow of 1 starting from
every vertex of V1 to a vertex in V4. This means through
each edge in E0 exactly one unit flow is transferred
following a path from source to sink. Given that there are
mL edges in E0, there is a flow of size mL in G. ¡
From the correctness of Lemma 2, we may expect there
is a flow of size mL in G. Lemma 3 says that the
conjecture is true.
Lemma 3. G has a flow of size mL.
Proof.
The proof strategy is to find a flow assignment with no
edge overflow at first and then show that the flow rules
hold for this flow assignment. We assign flow values to
the edges of G as follows. f(e) is defined as the amount of
flow assigned to edge e and can be a fractional number
less than or equal to the edge capacity.
f(source, ¢1, x, i², 1)) = 1, x ∈ T, i ∈ [0, wxL)
For x ∈ T, i ∈ [0, wxL), j ∈ [ealiest(x, i), latest(x, i)],
we do flow assignment to (¢1, x, i², ¢2, x, j², 1) as follows.
f((¢1, x, i², ¢2, x, j², 1)) = wx – i + wx ¬i/wx¼, if j =
earliest(x, i) and j = latest(x, i-1), or
i+1 – wx ¬(i+1)/wx¼, if j = latest(x, i) and j = earliest(x,
i+1), or
wx, otherwise.
f((¢2, x, j², ¢3, x, j², 1)) = wx, x ∈ T, j ∈ [0, L)
For E3, the flow assignment of the edge (¢3, x, j², ¢4, k,
j², 1) is:
f((¢3, x, j², ¢4, k, j², 1)) = wx, if x ∈ Tk and k ∈ [0, g), or
[mk-w(Tk)]wx/w(Tg), if x ∈ Tg and k ∈ [0, g), or
mgwx/w(Tg), for x ∈ Tg and k = g
f((¢4, k, j², sink, mk)) = mk , k ∈ [0, g], j ∈ [0, L)
It is easy to see w(Tg) ≥ mk –w(Tk) since w(Tg) ≥ mk,
leading to [mk - w(Tk)]wx/w(Tg)≤ wx ≤ 1 and, given w(Rg) ≥
mg, we have mgwx/ w(Rg)≤ wx ≤ 1. Thus, with the flow
assignment to G, there is no any edge such that the flow it
carries exceeds its capacity.
It is not difficult to show that the flow assignment
makes the flow rules hold for a flow of size mL from the
source to the sink. The detailed checking of flow rules is
omitted for saving space.
As no edge is overflow and all the flow rules are
satisfied, G has a flow of size mL. ¡
Proof of Theorem 1.

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

By Lemma 3, G has a flow of size mL. By Lemma 2, if
there is a flow of size of mL in G, a pfair schedule exists
for instance Φ. Hereby, Φ has a pfair schedule. ¡

4. Pfair Schedulability Condition
Arbitrary Allocation Constraints

with

In this section, we examine the pfair scheduling under
arbitrary allocation constraints in which a task can be
assigned to an arbitrary subset of processors. A set of
periodic tasks with arbitrary allocation constraints may not
be pfair schedulable, even though the condition
¦x∈T wx = m holds. Thus, there is a need to find the
necessary and sufficient condition under which the set of
tasks is pfair schedulable. Formally, we can define an
instance Φ as a tuple {T, R, {P(x)|x∈T}} that represents a
pfair multiprocessor scheduling problem of periodic tasks
with arbitrary allocation constraints. The elements in the
tuple are described as follows. T is a set of n periodic
tasks. R is a ser of m identical processors in the sense that
each processor has the computing capacity of 1. P is a
allocation constraint function defined as P: T → subsets of
R such that for x ∈T, P(x) ⊆ R represents the subset of
processors on which task x can be executed. Similar to
previous notations, each task x has an associated integer
period px, and integer execution requirement ex ∈(0, px].
We define the weight of task x as wx = ex/px and assume
that ¦ wx = m .
x∈T

Theorem 2. Instance Φ is pfair schedulable if and only
if there exists a set of numbers w(x, r) (0 ≤ w(x, r) ≤1) such
that
w( x ,r ) = wx ∀ x ∈ T, and

¦

r∈P ( x )

¦ w( x, r ) = 1 , ∀ r∈ R

x∈{ y|r∈P ( y )}

Similar to the proof of Theorem 1, we need to define a
digraph G = (V, E) such that V = V0 ∪ V1 ∪ V2 ∪ V3 ∪ V4
∪ V5 and E = E0 ∪ E1 ∪ E2 ∪ E3 ∪ E4.
V0 = {source}
V1 = {¢1, x, i² | x ∈ T, i ∈ [0, wxL)}
V2 = {¢2, x, j² | x ∈ T, j ∈ [0, L)}
V3 = {¢3, x, j² | x ∈ T, j ∈ [0, L)}
V4 = {¢4, r, j² | r ∈ R, j ∈ [0, L)}
V5 = {sink}
E0 = {(source, ¢1, x, i², 1) | x ∈ T, i ∈ [0, wxL)}
E1 = {(¢1, x, i², ¢2, x, j², 1) | x ∈ T, i ∈ [0, wxL), j ∈
[earliest(x, i), latest(x, i)]}
E2 = {(¢2, x, j², ¢3, x, j², 1) | x ∈ T, j ∈ [0, L)}
E3 = {(¢3, x, j², ¢4, r, j², 1) | x ∈ T, r ∈ P(x), j ∈ [0, L)}
E4 = {(¢4, r, j², sink, 1) | r ∈ R, j ∈ [0, L)}
Before proceeding to prove Theorem 2, we shall show a
few useful properties of the constructed digraph.

Lemma 4. A pfair schedule exists for instance Φ if and
only if there is a flow of size of mL in G.
The proof of Lemma 4 is similar to that of Lemma 2.
Please note that V4 and E3 are the only vertex set and edge
set different from those of the digraph constructed in
Section 3.
Lemma 5. There is a flow of size mL in G if and only if
there exists a set of numbers w(x, r) (0 ≤ w(x, r) ≤ 1) for
instance Φ such that
(1) and
w( x, r ) = w x ∀x ∈ T

¦

r∈P ( x )

¦ w( x, r ) = 1 ,

∀r∈ R

(2)

x∈{ y |r∈P ( y )}

Proof.
(The if part):
Suppose there exists a set of numbers satisfying (1) and
(2). We do a flow assignment in G as follows and then
show that all the flow rules hold. f(e) is defined as the
amount of flow assigned to edge e.
f((source, ¢1, x, i², 1)) = 1, x ∈ T, i ∈ [0, wxL)
For x ∈ T, i ∈ [0, wxL), j ∈ [earliest(x, i), latest(x, i)],
we have
f((¢1, x, i², ¢2, x, j², 1)) = wx – i + wx ¬i/wx¼, if j =
earliest(x, i) and j = latest(x, i-1), or
i+1 – wx ¬(i+1)/wx¼, if j = latest(x, i) and j = earliest(x,
i+1), or
wx, otherwise.
f((¢2, x, j², ¢3, x, j², 1)) = wx, x ∈ T, j ∈ [0, L)
f((¢3, x, j², ¢4, r, j², 1)) = w(x, r)/L, x ∈ T, r ∈ P(x), j ∈
[0, L)
f((¢4, r, j², sink, 1)) = 1 , r ∈ R, j ∈ [0, L)
Note that with the flow assignment to G there is no any
edge in G such that the flow it carries exceeds its capacity.
Similar to Lemma 3, checking flow rules, we know that
there is a flow of size mL from the source to the sink under
the flow assignment.
(The only if part):
Suppose there is a flow of size mL in G. Then the flow
rules will hold with the flow. Without losing generality,
we define wx(x, r) such that w(x, r)L =
¦ f (( 3 , x , j , 4 , r , j ,1 )) , for all x ∈ T, r ∈ P(x),
j∈[ 0 , L )

where w(x, r)L can be regarded as the total weights task x
have on processor r during [0, L). Obviously, w(x, r) ≥ 0.
Applying flow rule (ii) to the vertices corresponding to
task x in V1, V2, and V3 and making necessary summations,
we have
(3)
¦ f ((source, 1, x, i ,1)) = ¦ w( x, r ) L
i∈[ 0 , w x L )

r ∈P ( x )

Since the total capacity of the edges in E0 is mL and
each edge in E0 has capacity 1, then every edge in E0 reach
its capacity. Hence,

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

¦ f ((source, 1, x, i ,1)) = wxL

(4)

i∈[ 0 , wx L )

Combining (3) and (4), we have

¦ w( x, r ) L =

wxL,

r∈P ( x )

i.e.,

¦ w( x, r ) = wx

(5)

r∈P ( x )

Correspondingly, we can apply the flow rule (ii) to the
vertices for processor r in V4 and make the necessary
summations to obtainx, r), we have
¦ f (( 4, r , j , sink ,1)) = ¦ w( x, r ) L (6)
j∈[ 0 , L )

x∈{ y| r∈P ( y )}

Since the total capacity of the edges in E4 is mL and
each edge in E4 has capacity 1, then each edge in E4
reaches its capacity. Hence,
¦ f (( 4, r , j , sink ,1)) = L (7)
j∈[ 0 , L )

Therefore,

¦ w( x, r ) L = L by combining (6) and (7),

x∈{ y | r ∈P ( y )}

i.e.,

¦ w( x, r ) =1

(8)

x∈{ y| r∈P ( y )}

Please note 0 ≤ w(x, r) ≤ 1 follows trivially, and by (5)
and (8) the set of numbers satisfying (1) and (2) exists.
Accordingly Lemma 5 holds. ¡
Proof of Theorem 2.
Theorem 2 follows directly by applying Lemma 4 and
Lemma 5 in turn. ¡
Even though pfair schedulability test can be done by
finding the maximum flow in graph G, we would not do
that because the size of G depends on L that can
exponentially increase over n. Theorem 2 also implies that
we do not need to take into account L slots anymore in
inspecting the pfair schedulability. Intuitively we may
think the schedulability test can be done in polynomial
time. Actually this conjecture is true. In the next section,
we will see that this problem can be converted to a
network flow problem equivalently.

5. Polynomial Time Pfair Schedulability Test
for Periodic Tasks with Arbitrary Allocation
Constraints
In this section we are going to find a polynomial time
algorithm to do schedulability test for a set of periodic
tasks with arbitrary allocation constraints. The idea is to
convert the schedulability test to a network flow problem
following the conditions in Theorem 2. According to the
allocation constraints in instance Φ, we construct a digraph
G′ that we call the allocation constraint graph of
instanceΦ as follows. Let G′ = (V, E) such that V = V0 ∪
V1 ∪ V2 ∪ V3 and E = E0 ∪ E1 ∪ E2. The vertex sets and
edge sets (including edge capacities) are defined as
follows.
V0 = {source}

V1 = {¢1, x² | x ∈ T}
V2 = {¢2, r² | r ∈ R}
V3 = {sink}
E0 = {(source, ¢1, x², wx) | x ∈ T}
E1 = {(¢1, x², ¢2, r², 1) | x ∈ T, r ∈ P(x)}
E3 = {(¢2, r², sink, 1) | r ∈ R}
Lemma 6. G′ has a flow of size m if and only if there
exists a set of numbers w(x, r) (0 ≤ w(x, r) ≤ 1) for instance
Φ such that
(9) and
¦ w( x, r ) = wx ∀x∈T
r∈ P ( x )

¦ w( x, r ) = 1 ∀r∈R

(10)

x∈{ y|r∈P ( y )}

Proof.
(The if part):
Suppose there is a set of numbers such that (9) and (10)
hold. We make a flow assignment to G′ in the following
way.
f((source, ¢1, x², wx)) = wx, x ∈ T,
f(¢1, x², ¢2, r², 1)) = w(x, r), x ∈ T, r ∈ P(x),
f((¢2, r², sink, 1)) = 1, r ∈ R.
Obviously, there is no edge such that the flow it carries
is larger than its capacity in the flow assignment. It is easy
to inspect the flow rules for all vertices in G′ to show that
the flow assignment gives a flow of size m. We omit the
details.
(The only if part):
Suppose there is a flow of size m in G′. We define w(x,
r) such that w(x, r) = f((¢1, x², ¢2, r², 1)), x ∈ T, r ∈ P(x).
Obviously w(x, r) ≥ 0. Then we apply the flow rules.
Similar to what we we did to Lemma 5, we have
wx =
(11)
w( x, r )
f (( 1, x , 2, r ,1)) =

¦

¦

r ∈P ( x )

r∈P ( x )

¦ f (( 1, x , 2, r ,1)) = ¦ w( x, r ) = 1

x∈{ y | r∈P ( y )}

(12)

x∈{ y|r∈P ( y )}

Please note 0 ≤ w(x, r) ≤ 1 follows trivially, and by (11)
and (12) a set of numbers satisfying (9) and (10) exists. ¡
Theorem 3. The pfair schedulability test of instance Φ
can be done in O((n+m)3).
Proof.
By Theorem 2, determining the pfair schedulability of
instance Φ is equivalent to determining the existence of a
set of numbers, w(x, r) (0 ≤ w(x, r) ≤1) , such that
¦ w( x, r ) = wx , ∀ x∈T, and
r∈ P ( x )

¦ w( x, r ) = 1 , ∀r∈R.

x∈{ y |r∈P ( y )}

By Lemma 6, whether or not a set of numbers w(x, r)
can be found is determined by whether there is a flow of
size m in G′. Therefore a pfair schedulability test for
instance Φ can be done by finding the maximum flow of
G′ and then checking whether the maximum flow is equal
to m.

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

The number of vertices in G′ is bounded by O(n+m).
For any graph G = (V, E), there is a maximum flow
algorithm requiring O(|V|3) time [8]. Consequently, the
pfair schedulability test of instance Φ can be done in
O((n+m)3). ¡

6. On-Line Approximate Pfair Scheduling of
Task Set Containing Fixed Tasks
On-line pfair scheduling of periodic tasks with arbitrary
allocation constraints is still a challenging problem. The
more practical version of the problem is to consider only
allocation constraints in the form of fixed tasks as Moir
and Ramamurthy studied in [3]. In this special case, the
allocation constraint of a task only requires the task to run
on one given processor instead of a subset of processors as
considered in previous sections. A task with this kind of
allocation constraint is called fixed task In this section, we
will use hierarchical pfair algorithm (HPA) to schedule
fixed and migrating periodic tasks. With HPA although an
ideal pfair schedule of fixed tasks cannot be guaranteed to
achieve, a subtask of a fixed task cannot get executed too
much earlier or later than the time window during which
the subtask should run according to the ideal pfair
scheduling. HPA proceeds in two steps, global scheduling
and local scheduling. Firstly, during the global scheduling,
the task set of supertasks and migrating tasks, where a
supertask is obtained by merging all fixed tasks allocated
to a processor, is scheduled by the PD2 algorithm [5][12]
to get a pfair schedule of both supertasks and migrating
tasks. Secondly, during the local scheduling the fixed tasks
forming a supertask are scheduled on the time slots
assigned to the supertask according to the global
scheduling in a pfair manner. The local scheduling is the
same as the problem of uniprocessor pfair scheduling, i.e.,
finding a pfair schedule of multiple periodic tasks over a
single processor.
Let X be the set of fixed tasks allocated to a processor,
then the weight of the formed supertask is expressed as
w = ¦ w , where we assume wX ≤ 1. Without
X

x

x∈ X

confusion, we also use X to denote the supertask. After all
supertasks are formed, the global scheduling is performed
on all supertasks and migrating tasks by the PD2
algorithm. Interested readers can refer to [5][12] for
details. With global scheduling, a pfair schedule is
obtained for all supertasks and migrating tasks. Obviously
a supertask can run only on one processor.
Before describing local scheduling, we give Lemma 7
as follows to obtain a pfair schedule of periodic tasks over
a single processor by simply applying earliest deadline
first (EDF) policy.

¦

Lemma 7. Given a periodic task set T such that
w = 1 , a pfair schedule of T on a processor can be
x

x∈T

obtained by applying the following scheduling.
•
A subtask i, i∈N, of task x can only be eligible to
run on and after time instant ¬i / w x ¼ (the beginning of
time slot ¬i / w x ¼ ).

•
At any time slot, one of eligible subtasks is
selected to run according to earliest deadline first (EDF)
policy, where the deadline of subtask i, i∈N, of task x is
defined as the time instant ª(i + 1) / w x º (inside time slot

ª(i + 1) / wx º -1).
Proof.
According to the definition of eligible subtasks, until
any time instant t, the amount of processor time requested
by all eligible subtasks so far can be expressed as
ªtw º . However,
ªtw º ≥ tw = t. Therefore,

¦

¦

x

x∈T

x

x∈T

¦

x

x∈T

there is no idle time for the processor at any time.
On the other hand, we will prove by contradiction that
subtask i of task x cannot get executed later than time
instant ª(i + 1) / w x º . Let t be an integral time instant at
which a subtask misses its deadline, then the amount of
time needed by the subtasks that should finish execution
before t is ¦ ¬tw x ¼ . Since there is on idle time until t, we
x∈T

have

¦ ¬tw

x

x∈T

However,

¼ > t due to the deadline missing at t.

¦ tw

x

≥

x∈T

This contradicts

¦ ¬tw

x

x∈T

¦w

x

¼ > t, leading to

¦w

x

>1 .

x∈T

= 1.

x∈T

Therefore, a subtask i of a task x is guaranteed to run in
time window [ ¬i / w x ¼ , ª(i + 1) / w x º ], or time slot window
[ ¬i / w x ¼ , ª(i + 1) / w x º -1]. The algorithm gives a pfair
schedule. ¡
During local scheduling of HPA, for any task x of a
supertask X, a new weight is assigned to x as
w ′x = w x / w X . Apparently, ¦ w′x = 1 . Then all fixed tasks
x∈ X

in X are scheduled by applying the algorithm in Lemma 7
over the time slots assigned to X during global scheduling.
It should be noted that during the local scheduling the time
slots assigned to suptertask X form an one-dimension time
slot array numbered in 0, 1, 2, …, over which the Lemma
7 is applied. Theorem 4 gives the time slot window of a
subtask of a fixed task under HPA.
Theorem 4. A subtask i, i∈N, of a fixed task x∈ X, gets
executed
in
time
slot
window
[ ¬i / w x − 1 / w X ¼ , ª(i + 1) / w x + 1 / w X º -1].
Proof.

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

According to pfair property of local scheduling, subtask
i of x, runs at time slot window [ ¬i / w ′x ¼ , ª(i + 1) / w ′x º -1]
in terms of the one-dimension time slot array in local
scheduling. Again according to pfair property of global
scheduling, time slot ¬i / w ′x ¼ ( ª(i + 1) / w′x º -1) in local
scheduling can be get serviced as early (late) as at time slot
¬¬i / w′x ¼ / wX ¼ ( ªª(i + 1) / w ′x º / w X º -1) in the global
scheduling. After introducing 0 ≤ δ1, δ2 < 1, we note that
=
=
¬(i / w ′x − δ 1 ) / w X ¼
¬¬i / w ′x ¼ / w X ¼

¬i / w x − δ 1 / w X ¼ ≥ ¬i / w x − / w X ¼ .

ªª(i + 1) / w ′x º / w X º -1= ª((i + 1) / w ′x + δ 2 ) / w X º 1= ª(i + 1) / w x + δ 2 / w X º -1≤ ª(i + 1) / w x + 1 / w X º -1
Therefore, subtask i of x gets executed in time slot
window [ ¬i / w x − 1 / w X ¼ , ª(i + 1) / w x + 1 / w X º -1]. ¡
Based on Theorem 4, Theorem 5 compares the time slot
window of a subtask under HPA and the idle pfair time
slot window.
Theorem 5. The upper (lower) limit of the window of
subtask i of a fixed task x under HPA is not ª1 / w X º less
(greater) than the upper (lower) limit of the ideal pfair
window of the subtask.
Proof.
The ideal pfair window of the subtask can be expressed
as [ ¬i / w x ¼ , ª(i + 1) / w x º -1]. Comparing the window with
that in Theorem 4, we have
¬i / w x ¼ – ¬i / w x − 1 / w X ¼ ≤ ª1 / w X º ,
since ¬a ¼ − ¬a − b ¼ ≤ ªb º for nonnegative numbers a and b
(a ≥ b).
( ª(i + 1) / w x + 1 / w X º -1) – ( ª(i + 1) / w x º -1) =

ª(i + 1) / w x + 1 / w X º - ª(i + 1) / w x º ≤ ª1 / w X º , since
ªa + bº − ªa º ≤ ªbº for nonnegative numbers a and b.
Therefore, Theorem 5 holds. ¡
By Theorem 5, we can conclude that applying HPA
may lead to a task missing its deadline, but the task does
not miss its deadline so much since in general ª1 / w X º is a
small number. The property of HPA guarantees some
fairness not too bad for allocating resources. HPA can be
used in not only real-time task scheduling, but also packet
scheduling in performance-guaranteed communications.

7. Conclusions
In this paper, we have investigated the pfair
schedulability of periodic tasks with allocation constraints
over multiple processors. Firstly we study a special type of
allocation constraints in which each fixed task can be
assigned to one of disjoint and dedicated processor sets.
We show that this special case is pfair schedulable by

using a network flow approach. Then we consider the
general case that arbitrary allocation constraints are
assumed. Using the similar network flow approach, we
present a necessary and sufficient condition of the
existence of a pfair schedule under the general allocation
constraints. In the next step, we show that pfair
schedulability test can be done in polynomial time by
equivalently converting the problem to a network flow
problem in an allocation constraint graph of O(n+m)
vertices. At last, an approximate pfair algorithm called
PHA is proposed for scheduling fixed and migrating
periodic tasks such that a fixed task’s deadline cannot be
missed too much.

References
[1] S. Baruah, N. Cohen, C. G. Plaxton, and D. Varvel,
“Proportionate progress: A notation of fairness in resource
allocation,” Algorithmica, vol. 15, no. 6, 1996, pp. 600-625.
[2] S. Baruah et al., “Fairness in periodic real-time
scheduling,” Proceedings of 16th Annual IEEE Real-Time
Systems Symposium, December, 1995, pp. 200-209.
[3] Mark Moir and Srikanth Ramamurthy, “Pfair scheduling of
fixed and migrating periodic tasks on multiple resources,”
Proceedings of 20th Annual IEEE Real-Time Systems
Symposium, December, 1999, pp. 294-303.
[4] C. L. Liu and James W. Layland, “Scheduling algorithms
for multiprogramming in a hard-real-time environment,”
Journal of the Association for Computing Machinery, Vol.
20, No. 1, Jan. 1973, pp. 46-61.
[5] J. Anderson and A. Srinivasan, “A new look at pfair
priorities,” Technical report, Dept. of Computer Science,
Univ. of North Carolina, 1999.
[6] S. Baruah, J. Gehrke, C. G. Plaxton, and I. Stoica, “Fast
scheduling of periodic tasks on multiple resources,”
Proceedings of the 9th International Parallel Processing
Symposium, April 1996, pp. 280-288.
[7] L. R. Ford and D. R. Fulkerson, Flows in Networks,
Princeton University Press, Princeton, NJ, 1962.
[8] A. V. Goldberg and R. E. Tarjan, “A new approach to the
maximum-flow problem,” Journal of the ACM, vol. 35, no.
4, October 1988, pp. 921-940.
[9] Laura E. Jackson and George N. Rouskas, “Deterministic
preemptive scheduling of real-time tasks,” Computer, vol.
35, issue 5, May 2002, pp. 72-79.
[10] E. G. Coffman Jr., “Introduction to deterministic scheduling
theory,” Computer and Job-Shop Scheduling Theory, John
Wiley & Sons, New York, 1976, pp. 1-50.
[11] S. Baruah, R. Howell and L. Rosier, “Algorithms and
complexity concerning the preemptive scheduling of
periodic, real-time tasks on one processor,” Real-Time
Systems, 2, 1990, pp. 301-324.
[12] J. Anderson and A. Srinivasan, “Mixed pfair/Erfair
scheduling of asynchronous periodic tasks,” Proceedings
of the 13th Euromicro Conference on Real-Time Systems,
June 2001, pp. 76-85.

Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS’04)

0-7695-2132-0/04/$17.00 (C) 2004 IEEE

SPDA: A Security Protocol for Data Aggregation in
Large-Scale Wireless Sensor Networks
Jin Wook Lee1 , Yann-Hang Lee2 , and Hasan Cam2
1

Networking Lab.,
Samsung Advanced Institute of Technology, P.O. 111, Suwon, Korea 440-600
2
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287-8809, USA

Abstract. In this paper we propose a new key establishment protocol enabling
any data aggregation protocol to be operated securely. This is accomplished by a
bidirectional key distribution scheme based on Forward Key Setup and Backward
Key Setup developed by using synchronized broadcast and multi-level key concept. Our protocol, called SPDA(Security Protocol for Data Aggregation) is well
suited for any data aggregation algorithms and applications. Our analysis results
prove SPDA’s efficiency meaning that its communication cost is manageable.

1 Introduction
The issue of security in sensor networks has been addressed at various levels. In order to
prevent unauthorized entities from intercepting, decrypting, or hijacking data communications, data should be encrypted with either symmetric or asymmetric keys. The keys
must be protected and managed appropriately between the base station and all sensor
nodes and must satisfy several security and functional requirements. To support secured
data aggregation in sensor networks, there must be a key management scheme between
each sensor node and its correspondent data aggregation node. Thus, the collected data
are encrypted at each sensor node and then decrypted at the data aggregation node for
aggregation processing. This paper focuses on a simple key establishment for data aggregation in sensor networks. The objective is to construct an efficient key management
for data aggregation mechanism that can give confidentiality and integrity against malicious intruders. To eliminate the security loophole open to malicious intruders, we
present a mechanism to set up pair-wise symmetric keys for data aggregation operations. The core of a Security Protocol for Data Aggregation (SPDA) mechanism is the
bidirectional key setup scheme that stochastically makes a unique symmetric key between a sensor node and a correspondent aggregation node in the sensor network.

2 Related Works
So far the issue of security in sensor networks has been addressed at various levels.
Secure data aggregation schemes have been introduced in recent literatures [1] [2] [3]
[4] [5]. In the papers [3] [4], the authors attempted to make data aggregation secure by
designing alternate data aggregation schemes such as pattern-based data aggregation
and reference-based data aggregation. They employed a kind of group key management
E. Sha et al. (Eds.): EUC 2006, LNCS 4096, pp. 121–130, 2006.
c IFIP International Federation for Information Processing 2006


122

J.W. Lee, Y.-H. Lee, and H. Cam

scheme, so a group header becomes an aggregator to perform the aggregation algorithm. However, such schemes also have their limitations in performing specific data
aggregation algorithms and specific applications. In another work [2], Przydatek et al.
recently proposed secure information aggregation. Their scheme is also dedicated to
specific applications. Lee et al. [1] proposed a key management for data aggregation
and pointed out that the disadvantage of their scheme was that more data needed to be
passed through the key setup message and stored at each node. Worse yet, compromising a node allows an adversary to understand all data communications originating from
the node’s neighboring nodes. A similar paper to ours is proposed by Hu et al. [5], but
they don’t discuss key management issues in detail. Unlike other works, we provide
a security platform that conforms to any data aggregation scheme and any application
scenarios.

3 SPDA Protocol
Our aim is to construct a key establishment mechanism which enable a node to have
a unique symmetric key agreed with the correspondent aggregator. Our idea is to allow each node to compute a key with seeds originating from both the Forward Key
and Backward Key sent by aggregators. Our design goal is to devise a simple key establishment scheme well suited to any data aggregation algorithm by using pre-defined
aggregators. Only aggregators have a data aggregation algorithm to perform and have
more security information for key generation. Throughout the paper, we use several
terms to describe the protocol as defined in Table 1.
Table 1. Glossary
Glossary

Description

FKeySetup

A protocol packet. Only the base station can generate the initial FKeySetup
packet. All the nodes start the operation by receiving a FKeySetup packet
firstly.
A protocol packet. Only the aggregators can generate their own BKeySetup
packets. All the sensor nodes create their own BKeySetup packet to send data
messages to aggregators.
Forward Level Key. A key hint of the base station.
Backward Level Key. A key hint of the aggregator.
Public key of the base station. This public key is shared by all nodes.
Secret key, partially selected bits of PubK.
Private key that the base station and aggregators share.
Combination key. This key is generated by forward and backward key setups
and is used by aggregators and sensor nodes.
Hop Count. Logical hop count from the base station
Backward Hop Count. Logical hop count from an aggregator

BKeySetup

FLK
BLK
PubK
SPK
PrvK
CK
hc
bhc

3.1 Definition of Keys and Functions
Our approach to key generating functions is motivated by the need to establish a symmetric key of each node as efficient in communication cost as possible. Our idea of

SPDA: A Security Protocol for Data Aggregation

123

key generation introduces symmetry of key with two asymmetric keys. Combining two
different keys produces a high probability of having a virtually unique key in the network if each of the two different keys is not carelessly generated. One key is propagated
from the base station to sensor nodes. The other key is propagated from aggregators to
the base station. The keys are propagated by the relaying procedure of each node. The
relayed keys should not be easily guessed, so we suggest the use of a one-way hash
function as a key relaying function. Each sensor node receives two different keys from
two different neighboring nodes by relaying keys in opposite directions.
We define and use three types of crypto keys and two types of seed keys as below:
– Crypto key
• Public Key (PubK)
• Private Key (PrvK) of the base station and aggregator
• Sub-Public Key (SPK)
• Combination Key (CK)
– Seed key
• Forward Level Key (FLK)
• Backward Level Key (BLK)
All nodes maintain the public key (PubK) of the base station. This public key offers
data confidentiality of a broadcast message during announcement at the base station.
SPK is partially selected bits of PubK (i.e. most significant 64 bits of PubK) and is used
as a secret key among all nodes. The public key processing is costly; sensor node’s
public key processing is performed only during key establishment. Once a sensor node
finishes the key establishment, PubK and SPK are no longer used for data confidentiality so this public key mechanism does not significantly affect the network performance.
We choose 64 bits for a symmetric key and 512 bits for a public key, so the size of all
symmetric keys in this work is 64 bits. Combination Key (CK) is a core cryptographic
key in this protocol; it is computed in each node to use for sensed data message confidentiality. Each CK for each sensor node has a high probability of being unique in a
network. Additionally, aggregators have the private key (PrvK) of the base station that
is pre-installed, so aggregators are said to have the same security power as the base station and can also use this to establish secure channels to other aggregators. During the
key establishment, performing Forward and Backward Key Setup enables each sensor
node to compute its CK by itself and to use it for data encryption afterwards. Forward
Level Key (FLK) and Backward Level Key (BLK) are combined to generate the CK.
The first FLK is created only by the base station, whereas the generating BLK is started
by all aggregators. We are going to explain how to generate and propagate FLK and
BLK in the next subsection.
We suggest applying two one-way hash functions and a combining function to perform the protocol. All nodes have two key-generating functions and one key-combining
function as below:
– One-way function
• Forward Key Generating Function (FFunc)
• Backward Key Generating Function (BFunc)
– Combining function
• Combination Function (CFunc)

124

J.W. Lee, Y.-H. Lee, and H. Cam

3.2 Key Establishment
Our key establishment is done in two stages, Forward Key Setup and Backward Key
Setup. The base station starts a key establishment phase by broadcasting a message enclosing The Seed of Forward Level Key(F LK l ), where l is the level of the base station.
We now address the two key setup stages in greater detail, using Figure 1 to help explain
the Forward and Backward Key Setup protocol.

Fig. 1. Network Illustration of Forward and Backward Key Setup

Forward Key Setup. Key distribution for wireless sensor networks should not ignore scalability. In order to achieve scalability, a flooding-based broadcast is commonly
used for distributing keys. The base station starts the Forward Key Setup stage by sending a F KeySetup packet containing the commitment code of the Forward Level Key
(F LK n , where n is a big enough number and the last element of the key chain. For notation, we use superscript for hop count or level) to all adjacent nodes, which prevents
an adversary from compromising the base station. In the Forwarding Key Setup stage,

SPDA: A Security Protocol for Data Aggregation

125

the concept of ‘level’ is important, meaning that those who have the same hop count
from the base station locate in the same level and have the same FLK. The format of
the first FKeySetup packet is shown below:
broadcast
Base Station −−−−−−→ Neighbors : F KeySetup0
EP rvK { F LK n | n | G }
| ESP K { F LK 0 | hc0 }
| M ACSP K ( F LK n | n | G ),
where G is a gap value of hop-count assigned by the base station.
On receiving the FKeySetup packet, a node starts performing the protocol. The node
naively decrypts the packet with PubK and computes the MAC of the decrypted content
with SPK. The node then verifies the packet with two procedures, MAC comparison and
Commitment key validation. Firstly, the integrity and authenticity of the packet could be
validated by comparison of the computed MAC with the received MAC; however, MAC
comparison, with the syntax verification method is not enough, since a malicious node
that steals SPK is able to forge the whole packet by changing random bits of the content
and computing corresponding MAC with SPK. In such a case, MAC comparison could
not detect this abnormality. Commitment key validation solves the problem. Provided
that hc starts with 0 and increases by one, the node applies FFunc with FLK, ‘n −
G · hc’ times to verify F LK n . Applying a gap value, G, between two consecutive hc
prevents an attacker having no G from generating the next FLK properly.
If the verification fails, the node stops the protocol; otherwise, the node becomes a
level-1 node and prepares its own FKeySetup packet based on application of FFunc for
the next Forward Level Key (F LK 1 = F F uncG (F LK 0 ), meaning G times FFunc application). After an appropriate time, all level-1 nodes transmit their FKeySetup packet
as below. Nodes that receive these kinds of packets for the first time become a part of
the network as level-2 nodes.
broadcast
Level-1 nodes −−−−−−→ Neighbors : F KeySetup1
RELAY[EP rvK { F LK n | n | G }]
| ESP K { F LK 1 | hc1 }
| RELAY[M ACSP K ( F LK n | n | G )],
where hc1 is hc0 + 1.
As a result of completion of the Forward Key Setup stage, all nodes including aggregators have F LK l , where l is a relative hop distance from the base station.
Backward Key Setup. Aggregators may start the Backward Key Setup stage right
after they receive a FKeySetup while sensor nodes wait to receive a Backward Key
Setup packet to start the Backward Key Setup stage. Backward Key Setup is performed
with unicast communication started by aggregators. Once each aggregator receives a
FKeySetup packet, it is ready to start the Backward Key Setup stage by generating the
BKeySetup packet as follows (For notation, we use subscript for node ID):

126

J.W. Lee, Y.-H. Lee, and H. Cam
unicast

Aggregator i −−−−−→ Its parent node j : BKeySetupi
EP ubK { Seedi | bhc0i | i | hci }
| ESP K { BLKi0 | bhc0i }
| M ACSP K ( Seedi | bhc0i | i | hci )
An aggregator, i, chooses a random number as the Seed of the Backward Key, then
applies a one-way function once to make its BLKi0 . We suggest choosing an arbitrary
number for bhc instead of zero. This technique prevents an adversary who compromises
SPK from knowing the relative location of an aggregator by calculating bhc. The Seed
and bhc are encrypted with PubK and reported to an upper-level aggregator which will
use them to compute proper CKs of intermediate sensor nodes in the routing path between two aggregators. Assume that each node chooses one of its neighboring nodes as
the next hop node to reach the base station. An aggregator unicasts its BKeySetup to its
next hop node (say, node j).
On receiving this BKeySetup packet, sensor node j decrypts ESP K {BLKi0 | bhc0i }
with SPK and makes its own BKeySetupj , which contains updated bhc1j (= bhc0i + 1)
and BLKj1 (the output of BF unc(BLKi0 )) as below:
unicast

Sensor Node j −−−−−→ Its parent node k : BKeySetupj
RELAY[EP ubK { Seedi | bhc0i | i | hci }]
| ESP K { BLKj1 | bhc1j }
| RELAY[M ACSP K ( Seedi | bhc0i | i | hci )]
Only aggregators having PrvK are capable of understanding all the contents of the
BKeySetup. Whenever an aggregator receives a BKeySetup packet, it decrypts the
whole packet with PrvK and then keeps the source ID, bhc0i , and Seedi of the source
aggregator in the aggregator list.
3.3 Combination Key Generating
The main purpose of Forward and Backward Key Setup is to allow a node to be able
to have a secret key agreed with an aggregator for data message confidentiality, which
is Combination Key (CK). Once a node generates CK, it could encode a data message
to give to an aggregator lightly and securely. Now we explain how each kind of node
computes its own CK. There are two sorts of nodes in the protocol, such as aggregator
and sensor nodes. Each kind of node does apply differently CK generation functions.
A sensor node which receives FLK and BLK uses them as inputs of CFunc as CKj =
CF unc(F LKj , BLKj ). This key combination is quite unique in the network and also
can be computed with seeds of each key. All aggregators share PrvK and are able to
compute other aggregator’s CK generated with PrvK and their ID as below for aggregator i: CKi = CF unc(P rvK, i). The reason that an aggregator incorporates its ID into
CK is to differentiate CK of other aggregators. Having ID of other aggregators grants
an aggregator the CK generation of others.
3.4 Key Usage
Two different kinds of nodes build the first data message with three different message
fields usage, as below:

SPDA: A Security Protocol for Data Aggregation

127

Fig. 2. Finding relative distance of a source node
unicast

Node k −−−−−→ Aggregator i :
ECKk {M essage} | ESP K {k|0|k} | M ACCKk (M essage) for aggregators
or ECKk {M essage} | ESP K {k|bhck |aggrIDk } |
M ACCKk (M essage|bhck |aggrIDk ) for sensor nodes,
where aggrIDk is the source aggregator ID of bhc that node k receives.
When an aggregator receives the first data message from a node, it searches the
source ID, say k, in the aggregator list. If k is found in the aggregator list, the ID is used
to compute CKk . The aggregator figures out the generation of proper CK for sensor
nodes.
With aggrID, an aggregator can calculate the length of the path between two aggregators and also locate the message source node with bhc. Therefore, the aggregator
is able to find the relative distance of the message source node from aggregators and
calculate its FLK and BLK, as illustrated in Figure 2.
As mentioned above, CK calculation is done only once. After that, all nodes send
smaller sized messages as below:
unicast
Node k −−−−−→ Aggregator :
ECKk { M essage } | k | M ACCKk (M essage).

4 Analysis
In this section, we show the performance of SPDA through approximate numerical
analysis and provide simulation results to validate our numerical analysis. We choose
communication cost as the performance metric because the number of communications
is critical in wireless sensor network. Communication cost is defined as the additional
number of communications per non-aggregator for Backward key setup. We first derive
the basic calculation of the number of nodes. During the Forward key setup, the treestructured network is formed level by level. The percentage of aggregators (denoted by
α) on each level is expected to be the same under uniform distribution. Let N (l) denote
the number of nodes on level l. Thus, the number of aggregators, Nagg (l), is α · N (l).
With a maximum level number of a network, h, the total number of nodes in a whole
network, TN , and the total number of aggregators, Tagg , can be calculated respectively
as:
TN =

h

i=1

N (i), Tagg = α ·

h

i=1

N (i).

128

J.W. Lee, Y.-H. Lee, and H. Cam

Now we calculate the probability of connections between aggregators. To find the number of non-aggregators between two aggregators along a routing path, we note that they
are all located on different levels. In other words, all routing connections between two
nodes are between two levels. For example, the percentage of aggregators on a level
that have aggregators as their parent nodes is α times the number of aggregators on the
level (α · Nagg (l) where l is the level number).
Not all nodes receive a BLK necessary to generate CK. In the tree-structured network
we target, some nodes are not destined to receive BLK because they are not located in
the routing path of any aggregators. We define a tail node as a node that has no chance of
receiving any BLK from any aggregators and does not contribute the network security.
The number of tail nodes contributes directly to the protocol performance as a whole
so the number of tail nodes should be found. The number of tail nodes can be derived
like this. Intuitively, all non-aggregators on the maximum level l are tail nodes because
there is no possibility of getting them to receive a Backward key. In the same way, some
non-aggregators on level l − 1 are going to be tail nodes since they are not selected as
parents by aggregators on level l. In the same way, the number of tail nodes on a level
with h, maximum level number, can be derived as:
Ntail (l) = (1 − α)h−l (1 − α)N (l).
The total number of tail nodes in network is
Ttail =

h


Ntail (i) =

i=1

h


(1 − α)h−i (1 − α)N (i).

i=1

We suggest three scenarios for aggregator’s BLK forwarding to reduce the number of
tail nodes in a network. We categorize three different scenarios for the behavior of an
aggregator: (1) An aggregator unicasts a BLK to only its parent node(Say, Scenario I).
(2) An aggregator unicasts a BLK to all its parent level nodes, not to just the parent
node(Scenario II). (3) An aggregator unicasts a BLK to all its neighbor nodes except its
child level nodes(Scenario III).
In an ideal case, just one Backward key is sufficient for the computation of a nonaggregator’s CK. However, a node may be expected to relaying several Backward keys
to the upper level in SPDA. Note that Backward key setup communication commences
from an aggregator and ends at another aggregator. In order to count the number of
additional communications for Backward key distribution, we first find the sum of all
intermediate non-aggregators that relay the Backward key for aggregators. tn represents
the number of aggregators in the network that travel through n, the number of nonaggregators, to reach another aggregator. tn can be derived with α and Nagg as below.
t0 =

h


αNagg (l) + Nagg (1)

l=2

t1 =

h

l=3

... = ...

(1 − α)αNagg (l) + (1 − α)Nagg (2)

SPDA: A Security Protocol for Data Aggregation

ti =

h


129

(1 − α)i αNagg (l) + (1 − α)i Nagg (i + 1)

l=i+2

(1 − α)2 αNagg (4), for instance, represents the case when that many aggregators on
level 4 send a Backward key; there are 2 non-aggregators relaying the key until the
key reaches an aggregator. Therefore, the total number of additional communications
necessary for Backward key setup of all non-aggregators is calculated thus:
Tf orward = p ·

h−1


h


i=0

i=0

(i · ti ) = p ·

(i · (

h−1


((1 − α)i α2 N (l)) + α(1 − α)i N (i + 1)).,

l=i+2

where p is the scenario factor. For scenario I, p is 1. p is going to be 0.3N and 0.7N for
scenario II and scenario III, respectively. N is the average number of neighbors. 0.3N
is the approximate average number of parent level nodes per an aggregator and 0.7N is
the approximate average number of parent and sibling level nodes per an aggregator.
In conclusion, the average number of forwarding communication per each nonaggregator, Af orward , is
Af orward =

Tf orward
.
Tnon

where Tnon is TN − Tagg − Ttail .
Now we can calculate the communication cost if we know the network parameters,
such as the average number of neighbor nodes(N ), the average number of sensor nodes
in a level(N (l)), the number of aggregators(α), and the maximum number of levels(h).
The results of numerical analysis and simulation are shown in Figure 3(a) and 3(b). We
set the average number of neighbors, N with 10 and maximum number of level in a
network, h with 14 for numerical analysis. We see the numerical results is similar to the
simulation results.

(a) Result from Numerical Analysis

(b) Result from Simulation

Fig. 3. Communication Cost

130

J.W. Lee, Y.-H. Lee, and H. Cam

5 Conclusion
In this paper we proposed SPDA, a security protocol for data aggregation, to offer a
lightweight key establishment and adaptability of any aggregation algorithms for largescale tree-structured sensor networks. The protocol provides data integrity and confidentiality by applying a secret key mechanism established with a key generation and
distribution scheme. The concept of Forward and Backward Key establishment that
plays a key role is presented. By using the protocol, a network administrator is able to
build secure networks for data aggregation. In SPDA, a sensor node generates its own
crypto-key, CK, realizing its uniqueness in a network with a high probability.
There are two main points concerning the proposed protocol. First, SPDA is totally
‘distributed’; i.e. there is no central manager for the protocol, such as a cluster-head
or group-head. Hence, it is advantageous for scalability and also adding or deleting
a sensor node is easy. Second, SPDA is independent of a data aggregation algorithm.
Any data aggregation algorithm could be used together with this protocol. Aggregators should be pre-defined before deployment, at the expense of aggregation efficiency.,
however.

References
1. Y. H. Lee, A. Deshmukh, V. Phadke and J. W. Lee, Key Management in Wireless Sensor
Networks, Proceedings of the first European Workshop, ESAS 2004.
2. B. Przydatek, D. Song and A. Perrig, SIA: Secure Inforamtion Aggregation in Sensor Networks, Proceedings of the 1st ACM International Conference on Embedded Networked Sensor
Systems (SenSys 2003), 2003.
3. H. O. Sanli, S. Ozdemir and H. Cam, SRDA: Secure Reference-Based Data Aggregation Protocol for Wireless Sensor Networks, Proceedings of IEEE VTC Fall 2004 Conference, Sept.
26-29, 2004.
4. H. Cam, S. Ozdemir, P. Nair, D. Muthuavinashiappan and H. O. Sanli, Energy-Efficient Secure
Pattern Based Data Aggregation for Wireless Sensor Networks, To appear in a special issue of
Computer Communications on Sensor Networks.
5. L. Hu and D. Evans, Secure Aggregation for Wireless Networks, Proceeding of Workshop on
Security and Assurance in Ad hoc Networks, Jan 28, 2003.

SPIRIT-pKernel for Strongly Partitioned Real-Time Systems

Daeyoung Kim

Yann-Hang Lee

Mohamed Younis

Real Time Systems Research Laboratory
CISE Department, University of Florida
P.O. Box 116120
Gainesville, FL 32611-6120, USA
dkim@cise.ufl.edu

Dept. of Computer Science and Engineering
Arizona State University
P.O. Box 875406
Tempe, AZ 85287-5406,USA
yhlee@asu.edu

Honeywell International Inc.,
Advanced Systems Technology Group
9140 Old Annapolis Road
Columbia, MD 21045, USA
mohamed.younis@honeywell.com

refer to such kind of systems as Strongly Partitioned RealTime Systems (SP-RTS).
The concept of Integrated Modular Avionics (IMA)
[ 11, which is being braced by the aerospace industry these
days, is a good example of SP-RTS. With the strong
partitioning support in IMA systems, the same processor
can run crucial applications such as control navigation
systems and a non-flight-critical function such as cabin’s
temperature control. Even if the software that controls the
cabin temperature is not certified to the highest level and
has a probability of not working correctly, the navigation
system would not be affected. By integrating applications
from federated computer boxes into smaller number of
high performance sharable computing resources, they can
reduce the interconnection network, weight, power
supplies, and physical volumes of computing resources.
This also achieves the reliability, maintainability, and
dramatic cost reduction. Also a commercial-off-the-shelf
(COTS) approach, which is one of the key benefits of SPRTS, contributes a lot in the development and
maintenance of the system considering long-life time of
such systems.
In this paper, we propose a SPIRIT (Strongly
Partitioned Integrated Real-tIme sysTem)-pKernel that is
a key technology in implementing SP-RTS. In designing
the SPIRT-pKernel, we followed the design concepts of
the second-generation microkernel architecture because it
provides good reference model to achieve both flexibility
and efficiency.
The goals of the SPIRIT-pKernel are to provide
dependable integration of real-time applications,
flexibility in migrating operating system personalities
from kernel to user applications including transparent
support of heterogeneous COTS RTOS on top of the
kernel, high performance, and real-time feasibility. To
support integration of real-time applications that have
different criticality, we have implemented strong
partitioning concept using a protected memory (resource)
manager and a partition (application) scheduler. We also
developed a generic RTOS Port Interface (RPI) for easy
porting of heterogeneous COTS real-time operating

Abstract
To achieve reliability, reusability, and cost reduction,
a significant trend of building large complex real-time
systems is to integrate separated application modules. An
essential requirement of integrated real-time systems is to
guarantee strong partitioning among applications. In this
paper we propose a microkernel, called SPIRIT-@ernel,
for strongly partitioned real-time systems. The SPIRIT@ernel has been designed and implemented based on a
two-level hierarchical scheduling methodology such that
the real-time constraints of each application can be
guaranteed. It provides a minimal set of kernel functions
such as address management, interruptlexception
dispatching, inter-application communication, and
application scheduling. To demonstrate the feasibility of
the SPIRIT-@ernel, we have ported two different
application level real-time operating systems (RTOS),
WindRiver’s VxWorks and Cygnus’s eCos, on the top of
the microkernel. The SPIRIT-@ernel architecture is
practical and appealing due to its low overheads of kernel
services and the support for dependable integration of
real-time applications.

1. Introduction
Recently integrated real-time systems have been the
subject of significant research in both industry and
academia. In an integrated real-time system, applications
of diverse levels of temporal and mission criticality are
supposed to share the same computing resources while
maintaining their own functional and temporal behaviors.
To protect applications from potential interference, the
integrated system must provide spatial separation such
that the memory and 1/0 space of an application are
protected from illegal accesses attempted by other
applications. In addition, the system must support
temporal separation such that the execution time reserved
to one application would not be changed due to either
execution overrun or events of other applications. We

73
1530-1427/00$10.00 0 2000 IEEE

dependability of each application, it is required to realize
the concepts of strong partitioning, spatial and temporal
partitioning.

systems on top of the kemel in user mode. The
satisfactory flexibility and efficiency are achieved by
adopting design concepts of the second-generation
microkemel architecture. The kemel provides minimum
set of functions such as address space management,
interruptlexception
dispatcher,
inter-partition
communication, and partition scheduler, etc. A variety of
operating system personalities such as task scheduling
policy,
exception-handling
policy,
inter-task
communication can be implemented within the partition
according to individual requirements of partition RTOS.
To demonstrate this concept, we have ported two different
application level RTOS, WindRiver’s VxWorks 5.3 and
Cygnus’s eCos 1.2, on top of the SPIRIT-pKemel.
The scheduling policy of the SPIRIT-pKemel is based
on our previous work of the two-level hierarchical
scheduling algorithm which guarantees the individual
timing constraints of all partitions in an integrated system
[2,3,4]. At the lower microkemel level, a distance
constrained cyclic partition scheduler arbitrates partitions
according to an off-line scheduled timetable. On the other
hand, at the higher COTS RTOS level, each local task
scheduler of a partition schedules own tasks based on a
fixed priority driven scheduling. In our previous paper [5]
we advocated an approach for supporting temporal
partitioning and software reuse in IMA system and
introduced a software architecture in which we addressed
time management and privilege instruction emulation.
But, the previous approach has relative weak points. First,
since the time management requires high-precision timer
and reference clock, it cannot be applied to systems that
do not support both timer and reference clock. Second,
emulation of privilege instruction causes frequent kemeluser mode switches and overhead to process the privilege
violations. We solve the two problems with low-overhead
kemel-tick method and generic RTOS Port Interface
(RPI). We also address details of SPIRIT-weme1 in
viewpoint of operating system.
The rest of the paper is structured as follows. We
discuss SP-RTS model and design concepts of SPIRITpKernel in section 2. We describe the kemel architecture
in section 3. In section 4, we present generic RTOS Port
Interface. In section 5, we explain the prototype
implementation and performance evaluation. A short
conclusion is then followed in section 6.

2.1. Strongly Partitioned Real-Time System Model
The SP-RTS is composed of multiple communicating
partitions in which there are also multiple interacting
tasks as shown in Figure 1. The SP-RTS uses the twolevel hierarchical scheduling policy in which partitions
are scheduled by the SPIRIT-pKemel’s cyclic partition
scheduler and the tasks of a partition are scheduled by the
fixed priority driven local task scheduler of each partition.

...
/
/--

[

Distance Constrained Cyclic Partition Scheduler

]

Figure 1. SP-RTS Model
In order to meet the deadlines of periodic tasks within
each partition, the processing capacity dedicated to a
partition is then dependent on how frequently it is served
in the cyclic schedule. In [2,3], we have investigated the
constraints on the allocated capacity (a) and the
invocation period ( h ) and been able to construct a
monotonic function between cy and h for any set of
periodic tasks under either rate- or deadline-monotonic
scheduling algorithms. Then, similar to pinwheel
scheduling [lo], we can compose a distance-constrained
cyclic schedule for all partition in the SP-RTS system.
Following this approach, for a system of n ordered
partitions PI,..., n, each partition server, P,, will be given a
processor capacity of d;. in the period of hi to process its
2 land hi I (divides) hi for i
periodic tasks, where

Cy=lai

< j . By the characteristics of distance constrained cyclic
scheduling, the independence between lower level
partition scheduler and higher level local task scheduler is
achieved.

2. SP-RTS Model and Design Concepts

2.2. Design Concepts of SPIRIT-pKernel

Strongly partitioned real-time system is a system
architecture which supports the integration of multiple,
different levels of temporal and mission criticality for
real-time applications in sharable computing environment.
These integrated applications must be guaranteed to meet
their own timing constraints while sharing resources like
processors, communication bandwidth with other
applications. To guarantee timing constraints and

As a key implementation vehicle of the strongly
partitioned real-time systems, the SPIRIT-pKemel has the
following design concepts at its core:
Ensuring temporal partitioning - Enforcing temporal
separation should be implemented efficiently by avoiding
excessive overhead and feasibly by meeting application’s
timing constraints. The key to achieve temporal
partitioning is the scheduling strategy. This is one of the

74

most difficult jobs to build a feasible schedule for the SPRTS. In our previous work [2,3], we have devised feasible
two-level hierarchical scheduling algorithm that solves a
pair of fixed priority and cyclic scheduling. In the first
prototype of the kernel, we adopt this pair of scheduling
policy.
Ensuring spatial partitioning - Resources allocated to
a partition must be protected from unauthorized access by
other partitions. Also the kernel’s system resources must
be protected from the partitions. In addition to protection,
efficiency and deterministic applications execution are
important features that should not be sacrificed for spatial
partitioning.
Supporting applications based on heterogeneous
COTS RTOS on top of the kernel - Our emphasis is also
in the flexibility of accommodating COTS real-time
operating systems on top of the kernel. For example, in
Integrated Modular Avionics, there is a need to integrate
multiple applications that are originally developed in
different real-time operating systems. If the kernel and
scheduling algorithms are certifiable, it can reduce
tremendous amount of integration efforts and costs.
Restricting COTS RTOS to user mode - In the SPRTS all codes including COTS RTOS kernel of the
partition must be run in user mode. This is a challenge
because we need to develop a secure interaction method
between a kernel and partitions and to modify COTS realtime operating systems in order to enforce such a policy.
Capturing
second-generation
microkernel
architectural concepts - Since the SP-RTS supports
different kinds of application environments in a shared
computing platform, it is better to follow microkernel
architecture than monolithic kernel. The secondgeneration microkernel architecture provides better
flexibility and high performance.

Figure 2. The Architecture of SPIRIT-pKernel

3.1. Memory Management
To guarantee strong spatial partitioning, we use fixed
two-level hierarchy of address spaces, kernel and partition
address spaces. The kernel and partitions have their own
protected address space in which their codes, data, and
stacks are located. Protection of address space is
implemented by a hardware protection mechanism like
the memory management unit of a processor. We believe
that an optimization of address space management to the
underlying processor architecture is the best solution to
achieve both efficiency and deterministic access. A
kernel address space is protected from illegal accesses of
partitions and a partition address space is also protected
from potential interference of other partitions. Since the
kernel is dependable and needed to be certified in safety
critical systems, we can allow the kernel to access the
address spaces of all partitions.
In the SPIRIT-pKemel environment, address space
allocation and access policy are determined at system
integration time and saved in secure kernel area. Like
most real-time operating systems, we excluded the use of
virtual memory concept in which each partition is
allocated independent virtual address space. While
configuring an integrated system, we divide total physical
memory space into non-overlapped regions that satisfy the
memory requirement of each partition. Instead of using
full features of the memory management unit, we use an
address translation from virtual (logical) address to
physical address for protection purpose only. If we
allocate a separate page table per partition for supporting
full virtual memory system, we would face poor
deterministic behavior and performance, and increased
page table size. In the SPIRIT-pKernel, since we use the
same address for both virtual address and physical address,

3. SPIRIT-@ernel Architecture
The SPIRIT-pKernel provides strongly partitioned
operating environment to the partitions that can
accommodate application specific operating system
policies and specialties. A partition can have a flexibility
of choosing its own operating system personalities such as
task scheduling policy, interrupt and exception handling
policy, and inter-task communication, etc. The SPIRITpKernel provides only minimal necessary functions as
shown in Figure 2. Like second-generation micro-kernel
architecture, current implementation of the SPIRITpKernel takes advantages of hardware support from the
PowerPC processor architecture. We believe that the
kernel can be ported to different hardware platform easily
because the kernel requires only a MMU and timer
interrupt support. In this section, we discuss only the
fundamental features of the kernel. The RTOS Port
Interface will be discussed in the following sections.

75

a reference clock and a timer of fine resolution. It has
advantages of accurate time allocation and avoiding
spurious kemel tick overhead, however it increases the
complexity of managing time both in the kemel and
partitions. In the prototype implementation, we use the
first approach as used in MIT’s Exokemel.
Since the SPIRIT-pKernel does not restrict the
scheduling policies of the kernel scheduler and local task
scheduler of a partition, it is easy to use other hierarchical
scheduling policies in SPIRIT-pKemel environment.

the page table size is only dependent on the size of
physical memory.
To achieve high performance as well as protection, we
optimized the memory management scheme of the
SPIRIT-pKemel according to PowerPC architecture. Both
MMU BAT (Block Address Translation) and
Segmentpage functions of the PowerPC are used to
implement spatial partitioning. The BAT and
Segmentpage table mechanisms are used to protect
kernel’s address space and partition’s address space
respectively. Combined use of the two memory
management
techniques of the PowerPC enables us of
achieving very low overhead in kernel-partition switching
and task switching within the partition address space.

3.3. Timers’c1ock Services
Timers/clock services provided by COTS RTOS
generally rely on the time tick interrupt of the kemel,
which has relatively low resolution such as lOms, or 16ms.
The sleep system call is a typical example of this service
category. The other way of providing high-resolution
timers/clock service can be implemented by special timer
device and its device driver.
A partition maintains local time-tick that is
synchronized to the global reference clock, the value of
kemel tick. The local time tick interval of a partition must
be multiples of kemel tick interval. Since we use cyclic
scheduling approach, it is possible that user timer service
event be occurred in the deactivation time of a partition.
The solution is that every time a partition is resumed, it
goes to local event server first. The event server checks
the events that have occurred during the deactivation time
of the partition and delivers events to the local kemel if
needed. The possible delay of an event delivery can be
solved by our previous scheduling works [2,3]. The event
server will be discussed later in the paper

3.2. Partitions Management
An application partition is the basic scheduling entity
of the SPIRIT-pKemel. A partition represents a well
defined and protected, both temporally and spatially,
entity in which different kinds of real-time operating
systems and their applications can be implemented.
Different operating system personalities such as local task
scheduler, communication facilities, synchronization
method, etc., can be implemented within the boundary of
a partition. Due to the two-level static scheduling policy
used in our SP-RTS, temporal and spatial resources are
allocated to the partitions at system integration time.
Therefore, all attributes of the partitions including time
slices, memory allocations, etc., are stored in partition’s
configuration area in the kemel space. The examples of
this configuration information include address range,
cyclic schedule table, value of local time tick slice,
addresses for event server, event delivery object, partition
entry, and interrupt flag.
Basically we do not allow the shared libraries among
the partitions because our goal is to protect against
partition failure, even caused by the COTS kernel itself.
So all partition code including partition’s local kemel are
executed in user mode. If we allow partitions to share
COTS real-time operating system kemel code and its
libraries, a corruption or bug of the shared code affects all
the partitions that share the code. However, we allow
shared libraries among the non-critical partitions to save
the memory space.
As described earlier, we use distance constrained
cyclic partition scheduler to dispatch partitions. There are
two possible methods in implementing cyclic scheduler.
One is to use a SPIRIT-pKemel time tick implemented by
periodic time tick interrupt with a fixed rate. In this case,
the granularity of the execution time allocation to the
partitions and local time tick slice of a partition are
limited by the resolution of the kemel tick. Although it
may decrease the schedulable utilization bound of the
integrated system, it enables building a simpler and more
predictable cyclic scheduler. The other approach is to use

3.4. Inter-Partition Communication
In complex and evolving real-time systems like SPRTS, it is very important to provide scalable and robust
communication facilities for communicating partitions.
The legacy inter-process communication method used in
traditional microkernel architecture is not sufficient
enough to support all requirements of SP-RTS which
needs special features like fault tolerance, one (many)-tomany partitions communications, supporting system
evolution, and strong partitioning.
Therefore, we have selected a Publish-Subscribe model
as basic inter-partition communication architecture of
SPIRIT-pKemel. The proposed Publish-Subscribe
communication model was designed to achieve following
goals:
Supporting a variety of communication model Due to its complexity of SP-RTS, it needs a variety of
communication models such as point-to-point, multicast,
broadcast to serve various requirements of integrated
partitions.
Fault tolerance - In SP-RTS, it is not uncommon to
replicate applications for fault tolerance. With enhanced

76

Publish-Subscribe model we can implement fault tolerant
communication model like masterhhadow model.
Evolvabilityhpgradability - Considering relatively
long lifetime of complex real-time systems such as
avionics, it is easily anticipated that the system will face
modifications for system evolution and upgrade. A basic
Publish-Subscribe model is a good candidate for this
purpose.
Strong partitioning - Basically, all communication
requirements are scheduled statically at system integration
time. So the correctness and compliance of messages
exchanged among partitions are to be checked at the
kemel to protect other partitions from being interfered by
non-compliant messages.
More detailed information about inter-partition
communication is beyond the scope of the paper.

control goes, is delivered to the SPIRIT-pKemel via
Kemel Context Switch Request primitive (4). Finally, the
kemel gives CPU control to proper location informed by
KCSR primitive (5).
Current Partlilon

Next Partlon

m.

3.5. Device Driver Model
The SPIRIT-pKemel provides two device driver
models for exclusive and shared devices. The exclusive
device driver model can be used for the devices that are
safety-critical or exclusively used by a partition. To allow
a partition to access exclusive devices, the device I/O
address must be exclusively mapped to the address space
of the corresponding partition. In safety critical systems
like avionics system, it is preferred to use polling method
than interrupt method because polling method is more
predictable and reliable than interrupt method. The shared
device driver model is used for devices that are potentially
shared by multiple partitions using multiplexing
/demultiplexing methods. More detailed information
about device driver model is beyond the scope of the
paper.

Figure 3. EDO, Event Server, and KCSR Interaction

4.1. Event Delivery Object (EDO)
A partition requires to be informed of hardware events
such as time tick expiration, processor exceptions, and
interrupts. Since these events are caught in SPIRITK e r n e l in supervisor mode, secured and efficient
delivery of an event from the kernel to a partition is
necessary to be processed in local event handling routine
of a partition. In SP-RTS, it is not allowed for the kemel
to directly execute RTOS-specific local event handling
routines for two reasons. First, if an event handler in
RTOS's address space is called in the context of the
kemel (supervisor mode), it may violate the strong
partitioning requirements of the system due to the
possible corruption or overrun of the event handler.
Second, it increases the complexity of kemel's
interrupt/exception handling codes and data structures
because the kemel must have detailed knowledge of the
RTOS design, which is vendor's proprietary in COTS
RTOS. So we have devised generic event delivery method
which can be used to solve the problem. The Event
Delivery Object (EDO), along with event server and
kemel context switching request primitive, is one of the
key components building generic RTOS port interface.
The E D 0 is physically located in the address space of
a partition, so it can be accessed from both the kemel and
its owner partition. We show the abstract structure of
ED0 that is used in our PowerPC prototype in Figure 4.
Based on the E D 0 mechanism, we have developed the
generic interrupt and exception handling routine of the
SPIRIT-pKemel shown in Figure 5. We describe the
usage of E D 0 with an example of task switching caused
by partition's local time tick expiration.

4. Generic RTOS Port Interface (RPI)
The SPIRIT-pKemel provides generic RTOS Port
Interface (RPI) that can be used to port different kinds of
COTS real-time operating systems on top of the kemel.
With the help of RPI, the SPIRIT-pKemel itself can be
built independently without considering COTS RTOS. In
this section, we describe the selected features of RPI such
as Event Delivery Object (EDO), Event Server, usermode Interrupt EnableDisable emulation, and Kemel
Context Switch Request (KCSR) primitive. To give an
overview of RPI mechanism, we depict an example of
partition switching using RPI in Figure 3. In Figure 3,
when a partition is to be preempted, the
interrupt/exception dispatcher saves the context of current
partition and loads the context of next partition (1). The
dispatcher prepares ED0 including new loaded context
and delivers it to the event server of the next partition (2).
The event server does housekeeping jobs for the next
partition and also invokes the scheduler of the partition if
needed (3). The updated partition context, to which CPU

77

If rescheduling is needed in local RTOS kernel and
current resume pointer is within application task not
within a local kemel, the task context which is in ED0
will be saved in task control block and new context will
be transferred to the kernel using KCSR primitive (43.

~ y p e0x01 :Time ra ~xpiratbn

ED0 lnforrnalion
(Saved Context -

Type 0x02 :PanRBnSwkching Cccured
Type Oxo4 :Restanthe Partlion
Type Ox08 :Machine Chock Exceptbn
Type Ox10 :DSI Exception
Type Ox20 :IS1 Exception
Type 0x40 AlQnment Exception
Typo 0x80 :Pmgram Exception
Type Ox100 : Fioanng Point Exceptan
Typo Ox200 : Breakpoint Exception
Type 0x400: UnknownExceptbn

ED0 Object

Figure 4. The Structure of Event Delivery Object
Ex-

Response to local time tick expiration, the local task
scheduler may switch tasks based on its local scheduling
policy and the status of tasks. In Figure 5, upon the arrival
of time tick expiration interrupt, the kernel saves the
context in the E D 0 information area of corresponding
partition and sets E D 0 type as time tick expiration (7,8).
After preparing an EDO, the kemel sets interrupt return
address as the entry address of the partition's Event
Server (9).
After return from the interrupt, now in user mode, the
local event server checks the E D 0 and dispatchs the
proper local event handling routine. In VxWorks case,
receiving a time tick expiration, the local time tick
interrupt handler checks the status of the VxWorks kernel
whether task switching is needed or not. When a task
switching is needed, it saves the context in E D 0
information area in the task control block of the
preempted task. Then the event server issues a kemel
context-switch request to the kemel providing the context
of newly dispatched task. Since all local event-handling
routines are executed in user mode and in the local
partition's address space, we can protect the kemel and
other partitions from the faults of a partition. The overall
procedure of the event server can be found in Figure 6.

Figure 5. Intermpt/Exception Handling Procedure

4.2. Event Server
The event server of a partition is an essential part of
the SPIRIT-pKemel environment. It has a role of a
mediator between the kernel and partitions by executing
partition's local interrupt and exception handlers and
performing housekeeping jobs such as delivering local
kernel events which occurred during the deactivation time
of a partition. Figure 6 shows the generic event server
procedure of a partition. The implementation of an event
server depends on corresponding RTOS. We explain the
handling of partition switching event as an example.
In Figure 6, when an event server is user-mode-called
from the SPIRIT-pKemel, it checks the type of E D 0 and
process the E D 0 properly. If the event is a partition
switching, it executes housekeepingjobs of a partition (3).
Since there may be asynchronous local kernel events that
were arrived in the deactivation time of the newly
dispatched partition, they must be delivered to the local
RTOS kemel before dispatching the task of the partition.

Figure 6. Event Server Procedure

4.3. Kernel Context Switch Request Primitive
In most processor architectures, the context of a task is
composed of both user-mode and supervisor-mode
accessible registers. When a partition's local task
scheduler tries to restore the context of next dispatched
task in SPIRIT-pKemel system, privilege violation
exception may occur because context loader runs in user
mode. To solve this problem, we have devised kernel
context switch request primitive, which delegates the
kernel to perform a context-loading job. The kernel is

78

provided restoring context information with which the
kemel loads and resumes a new task.

Table 1. Measured Kemel Overheads
kt-overhead1

4.4. Interrupt EnablelDisable Emulation

Avg
0.84~

Most COTS real-time operating systems use interruptenable and disable functions to guard critical sections in
implementing system calls and the core of their kernels.
But, in the SPIRIT-FKemel environment, we cannot
allow a partition to execute interrupt control functions
because these functions must be run in supervisor mode.
To solve this problem, it is required to re-write original
interrupt control functions while guaranteeing its original
intention of protecting critical sections from interrupts.
We use atomic set and reset instructions to implement
replacements of original interrupt enable and disable
functions. In PowerPC architecture, atomic set and reset
logical functions are implemented by a reservation-based
instruction set. Disabling interrupts is emulated by atomic
set of interrupt flag, which is located in the partition
address space and shared by the kernel and owner
partition. While the interrupt flag is set, kemel delays the
delivery of an interrupt until the interrupt is enabled. The
interrupt enable function is implemented by atomic reset
instruction that clears the flag.

I

I

Min

0.75p

kt-overhead2
Min
I Max
Avg
9 . 3 8 ~ 9.00p
18.38 .us

I Max
I 3.88 p

I
I

1

To help find the feasible kemel tick resolution, we
calculate rkt-overhead and p-kt-overhead that are the
percentages of the redundant kernel tick overhead and
total kemel tick overhead for partition’s local task
scheduling respectively.
r - kt - overhead =

kt overhead1 * (

p kt overhead =

kt - overhead 1
kt - period
p t period
- 1) + kt - overhead2
kt period
p t - period

In Figure 7, we depict average and worst case kernel
tick overheads that are obtained by varying kernel tick
period, loous, 500us, lms, 2ms, 5ms, with fixed partition
local tick period of 10ms.
Kernel Tick Overheads (Partition Local Tick Period = 10ms)
I

5 ,

5. Performance E valuation
In order to prove and demonstrate the feasibility of the
proposed SPIRIT-pKemel, we have implemented a
prototype on DY4-SVME171 commercial-off-the-self
embedded CPU board. The board houses an 80MHz
Motorola PowerPC 603e and 32MB of main memory. On
top of the hardware platform, we have implemented the
SPIRIT-pKemel that cyclically schedules heterogeneous
COTS RTOS partitions, Windriver’s VxWorks and
Cygnus’s eCos. The size of the SPIRIT-pKemel code is
36 Kbytes. In evaluation prototype we integrate and run
four different partitions, two VxWorks-based and two
eCos-based applications. We measure the execution time
using PowerPC time base register, which has 120 ns
resolution.

I

I
0

1

2

3

4

5

6

Kernel Tick Period (ms)

Figure 7. Kemel Tick Overheads
As illustrated in Figure 7, while the kemel tick period
is bigger than 500ps, each of all four overheads is less
than 1% of the total CPU capacity. These practically
acceptable overheads were possible due to the efficient
design of the kemel tick interrupt handler and low latency
interrupt handling support from PowerPC processor.

5.1. Kernel Tick Overhead
Since the kernel and partition’s local RTOS are
synchronized to and scheduled based on the kemel tick,
the resolution of the kemel tick is important factor that
affects the system performance and schedulability. To
evaluate the overhead of the kernel tick, we obtained two
basic components, kt-overhead1 and kt-overhead2. The
kt-overhead1 is measured when a kemel tick is used for
neither partition switching nor local partition time tick
expiration. The kt-overhead2 is the time used to deliver
the E D 0 type of Time-Tick-Expiration to the event
server of the local partition. We show the result in Table 1.

5.2. Partition Switch Overhead
Based on the off-line scheduled timetable, the kernel
scheduler dispatches partitions. When a partitionswitching event occurs, the kemel scheduler saves the
context of the current partition and reloads saved context
of the next partition. Then CPU control goes to the event
server of newly dispatched partition. Since the kernel has
a generic partition-switching handler regardless of the

79

Table 4. TLB Miss handling Overheads

types of RTOS that are saved or restored, we expect
deterministic partition switch overheads. The Table 2
shows the partition switch overheads.

1

Table 2. Partition Switch Overhead
4
‘ vg

Min

Max

I

Min
0.88 P
1.63 ,us

1.13p

1

Max
3.50 p
4.25 ,us
4.88~

I

The SPIRIT-pKernel is designed to provide a software
platform for Strongly Partitioned Real-Time Systems that
have been significantly studied in both academia and
industries. Using the kernel, we can achieve better
reliability, reusability, COTS benefits, and cost reduction
in building complex real-time systems.
For further study, we are planning to enhance the
kernel in three directions. First, we will build our own
local kernel for a partition instead of using COTS RTOS.
Second, we will extend current uni-processor architecture
to multiprocessor and distributed real-time computing
environment. Third, we will develop other two-layer
scheduling strategies instead of c yclic/priority driven
scheduler pair while guaranteeing strong partitioning
requirements.

As in conventional operating system, kernel-user mode
switch overhead is an important factor for evaluating the
performance of the system. To reduce the overhead, we
use the BAT and segment/page table schemes for the
kernel and a partition respectively. Since both schemes
are executed concurrently during the address translation
and a result is chosen according to the current processor
mode, the only overhead for kemel-user switch is the time
needed to set or reset the processor mode bit in the
machine state register. So we can claim that the pure
overhead due to kemel-user switch while distinguishing
the processor modes, supervisor and user mode, is
ignorable. Instead of measuring the pure kernel-user
switch overhead, we measured kemel-context switch
request primitive, which is essential way of CPU control
transfer, which requires supervisor-mode privilege, in
user mode. The execution time of the primitive is
measured and listed in Table 3.

References
“Design Guide for Integrated Modular Avionics,” ARNIC
Report 651, Aeronautical Radio Inc., Annapolis, MD, 1991.
Y. H. Lee, D. Kim, M. Younis, and J. Zhou, “Partition
Scheduling in APEX Runtime Environment for Embedded
Avionics Software,” Proc. of IEEE Real-Time Computing
Systems and Applications, pp. 103-109,Oct. 1998.
Y. H. Lee, D. Kim, M. Younis, J. Zhou, and J. McElroy,
“Resource Scheduling in Dependable Integrated Modular
Avionics,” Proc. of IEEE International Conference on
Dependable Systems and Networks), Jun. 2000.
Y.H. Lee, D. Kim, M. Younis, J. Zhou, “Scheduling Tool
and Algorithms for Integrated Modular Avionics Systems,”
Proc. of IEEEIAIAA Digital Avionics Systems Conference,
Oct. 2000.
M. Younis, M. Aboutabl, and D. Kim, “An Approach for
Supporting Software Partitioning and Reuse in Integrated
Modular Avionics,” Proc. of IEEE Real-time Technology
and Applications Symposium, May 2000.
D. Engler, M. Kaashoek, and J. O’Toole Jr., “Exokemel:
An Operating System Architecture for Application-Level
Resource Management,” Proc. of ACM SIGOPS, pp. 251266, Dec. 1995.
J. Liedtke, “On m-Kemel Construction,” Proc. of ACM
SIGOPS, pp. 237-250, Dec. 1995.
J. Liedtke, “Toward Real Microkemels,” Communications
of the ACM, Vo. 39, No. 9, Sep. 1996.
H. Hartig, M. Hohmuth, J. Liedtke, S. Schonberg, J Wolter,
“The Performance of p-Kemel-Based Systems,” Proc. of
ACM SIGOPS, France, Oct. 1997.
[lo] M. Y. Chan and F. Y. L. Chin, “General schedulers for the
pinwheel problem based on double-integer reduction,”
IEEE Trans. on Computers, vol. 41, pp. 755-768, June
1992.

Table 3. KCSR Primitive Performance
Min
1.25 ,us

I

A vg
1.41 ,us
2.43 ,us
2.22 p

6. Conclusion

5.3. Kernel-User Switch Overhead

A vg
1.34 ,us

Instruction TLB
Data TLB Load
Data TLB Store

Max
7.50 ,us

5.4. TLB Miss Handling Overheads
In many real-time applications, virtual memory is not
supported and supervisor and user modes of execution are
not distinguished. However, it is an essential principle to
distinguish supervisor and user mode to guarantee spatial
partitioning concept. The SPIRIT-pKernel uses PowerPC
MMU’s segment/page scheme to protect the address
space of partitions. Since this scheme relies on virtual to
physical address translation, the performance of the TLB
(Translation Look-aside Buffer) scheme of the PowerPC
is very important. Considering the fairly uniform
distribution of memory access in SP-RTS environment
due to partitioning, we should pay much attention to the
performance of TLB. In the prototype, it requires 8K page
table enmes for 32Mbytes physical memory. However,
PowerPC provides only 64 page table entries for ITLB
and DTLB respectively. We measured the TLB miss
handling overhead and showed in Table 4.

80

Optimal Reconfiguration Strategy for a Degradable
Multimodule Computing System
YANN-HANG

LEE AND KANG G. SHIN

The University of Michigan, Ann Arbor, Michigan
Abstract. A new quantitative approach to the problem of reconfiguring a degradable multimodule
system is presented. The approach is concerned with both assigning some modules for computation
and arranging others for reliability. Conventionally, a fault-tolerant system performs reconfiguration
only upon a subsystem failure. Since there exists an inherent trade-off between the computation capacity
and fault tolerance of a multimodule computing system, the conventional approach is a passive action
and does not yield a configuration that provides an optimal compromise for the trade-off. By using the
expected total reward as the optimal criterion, the need and existence of an active reconfiguration
strategy, in which the system reconfigures itself on the basis of not only the occurrence ofafailure but
also the progression ofthe mission, are shown.
Following the problem formulation, some important properties of an optimal reconfiguration
strategy, which specify (i) the times at which the system should undergo reconfiguration and (ii) the
configurations to which the system should change, are investigated. Then, the optimal reconfiguration problem is converted to integer nonlinear knapsack and fractional programming problems.
The algorithms for solving these problems and a demonstrative example are given. Extensions of the
optimal reconfiguration problem are also discussed.
Categories and Subject Descriptors: B.2.3 [Arithmetic and Logic Structures]: Reliability, Testing and
Fault Tolerance; C.2.4 [Computer-Communication Network]: Distributed Systems; G. 1.6 [Numerical
Analysis]: Optimization-integer programming
General Terms: Performance, Reliability, Verification
Additional Key Words and Phrases: Degradable systems, dynamic failure, fractional programming,
performability, reward

I. Introduction
Reconfiguration of a system is the process of changing an already existing system
organization or the interconnections among its subsystems. In general, the system
needs to perform reconfiguration for two reasons. The first reason is the dynamic
variations of incoming tasks. The system reconfigures itself to match the special
demands made by the incoming tasks and then executes the tasks more efficiently
than with the previous configuration. In this case, reconfiguration depends on the
This work was supported in part by NASA under grants NAG-l-296 and NAG-l-492. Any opinions,
findings, and conclusions or recommendations expressed in this paper are those of the authors and do
not necessarily reflect the views of NASA.
Authors’ present addresses: Y.-H. Lee, IBM T. J. Watson Research Center, P.O. Box 218, Yorktown
Heights, NY 10598. K. G. Shin, Real-Time Computing Laboratory, Division of Computer Science and
Engineering, Department of Electrical Engineering and Computer Science, The University of Michigan,
Ann Arbor, MI 48 109.
Permission to copy without fee all or part of this material is granted provided that the copies are not
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the
publication and its date appear, and notice is given that copying is by permission of the Association for
Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.
0 1987 ACM 0004-541 l/87/0400-0326 $00.75
Journal ofthe Association for Computing Machinery, Vol. 34. No. 2, April 1987, pp. 326-348.

Optimal Reconfiguration Strategy for a Multimodule System

327

tasks to be executed. Reconfiguration of a system could be accomplished in many
different ways, such as the change of partition [30] or word size [ 161.The second
reason is to make the system tolerate faults that may occur dynamically and
randomly during the mission lifetime. Reconfiguration allows the systemto remain
operational, perhaps in a degraded mode, even in the case of subsystemfailures.
Typical examplesof reconfiguration for fault tolerance include the handling of an
extra stage in permutation networks [l] and the reconfiguration algorithm to
maintain the systemin a safestate [33].
For the purpose of fault tolerance, severalauthors have proposed principles and
procedures for the reconfiguration of computer systems[2, 15, 171.These procedures are all intended to make the system operational in the face of subsystem
failures. Saheban and Friedman investigated the degradation of computation
capability and diagnosability in terms of the number of switchesto connect modules
[25, 261. They also proposed a methodology for the design of reconllgurable
multimodule systems.Fortes and Raghavendraexamined the design of reconfigurable array processors with VLSI chips and analyzed the improved reliability,
performability, and computation capability and the additional hardware cost [9].
We classifythe conventional systemreconfigurations for fault tolerance discussed
above aspassive actions, since they are performed only upon detection of a failure.
Moreover, they assumethat there is only one configuration that the systemwill be
changedto following each reconfiguration. For instance, the systemdegradesfrom
an m module-parallel systemto an m - 1 module-parallel systemwhen a module
failure occurs. Thus, there is no choice concerning when the reconfiguration should
be performed and what configurations the systemshould switch to.
In this paper we are concerned with developing a quantitative method for design
and analysis of the reconfiguration of a multimodule system. Particularly, using
the expected total reward as the optimality criterion, we derive an optimal reconfiguration strategywith which the systemcan be optimally reconfigured during the
entire mission lifetime to maximize the expectedtotal reward.
The term module is used here to mean processor,memory, or bus. We assume
that the environment and workload of the systemare in the steadystatethroughout
the mission lifetime. The systemcan assigna module to befunctioning, redundant,
or spare. Functioning modules execute computation tasks. Redundant modules
are associatedwith functioning modules for verifying the correctnessof computation results or masking erroneous results. Sparemodules do not executeany useful
task before they replace failed modules. Although there is no difference between
functioning and associatedredundant modules in the execution of tasks, they do
have different purposes in a logical sense.
It is well known that the goals of reconliguring a multimodule system are to
enhance both computation capacity and system reliability. In most casesit is easy
to seea trade-off between these two goals. For example, if there were no module
failures and, therefore, no module redundancy were necessary,then the computation capacity would increaseas the number of functioning modules increases.On
the other hand, if module failures are allowed, then providing greater module
redundancy enhances the system reliability at the expense of the computation
capacity. When the number of available modules is finite, it becomesnecessaryto
make a suitable compromise between the system reliability and the computation
capacity. It is the optimal reconfiguration that is desired for the most suitable
compromise in some sense.
From the standpoints of reliability and performance, it is natural to consider
more than one possible way for reconfiguring multimodule systems.An extreme

328

Y.-H.

LEE AND K. G. SHIN

example is whether the system should be configured to an m-module redundant
system or to an m-parallel server system. Obviously, the former offers higher
reliability, whereasthe latter provides higher throughput. Some criterion is needed
to judge the goodnessof different configurations. Depending on the criterion, it is
possible that the best configuration at a particular moment is no longer best at
another moment (seethe example in Section 4). In such a case,reconfiguration is
neededeven if there is no occurrence of failure; we term this active recon$guration.
In this casean optimal reconfiguration strategy must specify the optimal configurations for the entire mission lifetime. Active reconfiguration subsumesthe conventional passivereconfiguration becausereconfiguration can be done at any time,
and not just at failure instants.
This paper is organized as follows: In Section 2 we introduce necessaryconcepts,
notation, and terminology and define the set of feasible configurations when
multimodule systems are to be reconfigured. Then, we develop a criterion, the
expected total reward during the mission lifetime, which will be used for judging
the goodnessof configurations. The need for active reconfiguration is alsojustified
in this section. Section 3 examines the problem of maximizing the expected total
reward and the properties of an optimal reconfiguration strategyduring the mission
lifetime. Also, presented is an algorithm that usesa backward induction to determine an optimal reconfiguration strategy. Actual determination of the optimal
configurations is the subject of Section 4, where we develop solution algorithms
for two integer nonlinear optimization problems. An example is also provided to
demonstrate both active and passivereconfiguration strategies.Concluding remarks
are in Section 5.
2. Reconfiguration Strategy
Consider a multimodule systemthat beginsits mission with moidentical operational
modules. Let the mission lifetime be to, during which no systemrepair is allowedthat is, a nonrepairable system. We consider here only the failures that are caused
by hardware faults and that, along with the progression of the mission, will trigger
system reconfiguration. Transient and intermittent faults from which the system
can recover through retry [ 19, 291 are not considered, since they do not entail
reconfiguration.
As we shall see,the optimal configurations are generated ofline in table form,
therefore, the time overhead of performing (on-line) reconfiguration is simply the
time required for switching tasks and setting up interconnection or routing. (E.g.,
the averagereconfiguration time of FTMP has been measuredto be 82 milliseconds
[31].) This on-line time overhead has little impact on the determination of an
optimal reconfiguration strategy, since in practice the system has to undergo only
a few active reconfigurations during the mission lifetime. Consequently, the overhead of performing reconfiguration is assumed to be negligible in the following
discussion.
2.1 NOTATION AND DEFINITIONS.
Classicalreliability analysestreat the system
failure probability as a function of the components’ failure probabilities using
combinatorial mathematics. Theseapproachesneglectthe effectsof failure recovery
overhead on executing tasks. These effects are significant, particularly when realtime applications are considered. For example, a delay in task execution due to
failure recovery overhead can lead to increased system operational costs or even a
systemcrash. For these reasonsa reliability analysis,including the impact of failure
handling on executing tasks, is more realistic and powerful than the classical

Optimal Reconfiguration Strategyfor a Multimodule System

329

methods [ 181. In addition, system performance should depend on the tasks completed by the system within its mission lifetime. Thus, it is natural and essential
to consider the execution of tasks when reconfiguration strategies have to be
determined.
Consider a set of tasks that are to be executed during the mission lifetime to.
Group the tasks into k classessuch that the tasks in the same class will have the
same influence on the mission. More specifically, the system will gain the same
reward for each task within a class when the task is completed successfully, and
will suffer the same penalty if the task execution is unsuccessful or its completion
is delayed. Although the pattern of the incoming tasks could change in reality, we
assume for simplicity that the combined workload in each task class does not
change throughout the entire mission, that is, the task arrival rate and the required
computations in each task class are constant.’
Because of the failures of individual modules during the mission, when the
remaining mission lifetime (RML) is t E [0, to], the system may have to operate
with only m(t) E (0, . . . , m) modules. Let mi(t) be the number of modules
assigned to the class i tasks. Out of these mi(t) modules, ni(t) computing clusters
are to be constructed. Each computing cluster consists of one functioning module
and some redundant modules for reliability reasons. Let ri(t) be the total number
of redundant modules used for the task class i. These redundant modules are used
in constructing computing clusters as dyads (with one redundant module each),
triads (with two redundant modules each), etc. For notational simplicity, we leave
out the time dependency of mi, ni, and ri in the rest of this paper as long as it does
not cause any ambiguity. Obviously, ni + ri = mi and Cf=I mi I m.’ Since all
computing clusters assigned to the same task class have to be homogeneous insofar
as their capabilities of computation and fault tolerance are concerned, the ri
redundant modules for the task class i are assumed to be equally distributed over
ni computing clusters. Thus, for the task class i there are ri - nilri/niJ computing
clusters with Lri/ni + 1J redundant modules, and ni( 1 + LrJniJ) - ri computing
clusters with Lri/niJ redundant modules, where Lxj is the greatest integer that is
less than or equal to x.
Let Q,,,be the set of all feasible configurations of m modules and given as
fh

=

(@I,

rl),

(n2,

r2),

. . . , (nk,

rk))

1 i
i=l

(ni

+

ri)

5

my

ni, riEI+,ri=Oif&=O,i=

1,2 ,...,

k ,
I-

where I+ is the set of nonnegative integers. Also, denote the set of all configurations
of the system by Q = U?Z Cl,.
Let Y,,,:R+ + Q, be a conjiguration function where R+ is the set of nonnegative
real numbers. A reconfiguration strategy RS,., is defined as
RS,, = (rl;l(i) 1i E [0, t], riz E (0, . . . , m)j.
Hence, given a reconfiguration strategy RS,,,,, the system uses the configuration
rm(t) E RS,,, where RML = t and there are m operational modules available.
’ As we shall discuss in Section 5, this assumption can be easily relaxed.
’ The “less than” or “equal to” relationship is used to include the case in which the system has standby
spare modules.

330

Y.-H. LEE AND K. G. SHIN

crash

degrade
FIG. 1. A model of system degradation.

2.2 RECONFIGURATION MODEL. During the mission lifetime, system degradation is unavoidable owing to module failures. A simple model for system degradation is presented in Figure 1 where state S,,, represents the availability of m
operational modules. The transition from S, to S,-, , m > 1, implies that a module
fails and system operation is recovered subsequently. However, failure of one
module could be fatal to the system, for example, coveragefailure [32] or dynamic
failure [ 181,which results in loss of the whole mission. In such a case, the system
transfers directly to the total system failure state SO.
It is assumed that the times to failure for all modules are independently and
identically distributed random variables. Distribution of the times to failure is
assumed to be exponential with rate X.
Define a stochastic process M(t) that is equal to (i) the number of available
modules when the system is operational with RML = t, or (ii) 0 if the system
crashed when RML > t. This stochastic process is governed by the failure and
recovery processes,3which in turn depend on the system configurations adopted
during the mission lifetime.
Given a reconfiguration strategy RSlO,mO,
sy stem configurations are represented
by another stochastic process, called the reconfiguration process and denoted by
RFlo,m,,(t),t E [0, to]. RFb,,,(t) includes a configuration 7,+&t) E RS,,,,, to be
used at RML = t. The transitions between configurations within RF,,,(t) depend
on failure and recovery processes, as well as on the reconfiguration strategy
is called a configuration trajectory, which repreRS,,,,. A sample path of RFtO,mo
sents a configuration history of the system. When RML = t and the number of
available modules is m, the system reconfigures itself from rm(t) to ym-l(t) if a
recoverable failure occurs, or to Tm(t - 6t) if there is no failure during 6t. Note
that, if -ym(t) = y,,,(i) = T&t - 6t) for all t^E [t - at, t ] and if there is no failure
during this at, then the system uses the same configuration for the period 6t.
For a system that is capable of graceful degradation and reconfiguration, Meyer’s
performability [22] is a useful measure of the system capability. Performability is a
composite measure of performance and reliability that automatically takes into
account the performance degradation due to component failures. To incorporate
the concept of performability, two functions associated with configurations w E Q
are introduced here. The first is a nonnegative and bounded function, p(w), called
the reward rate, which represents the average reward per unit time corresponding
to the computation performed by the system with the configuration w. This function
3 By recovery process we mean general actions to be taken upon occurrence of a failure, for example,
failure detection and masking, task recovery.

Optimal Reconfiguration Strategy for a Multimodule System

331

is analogous to the reward structure defined by Furchtgott [ 131, and the reward
function defined by Donatiello and Iyer [8]. Thus, the total reward accumulated
during the mission lifetime to is a random variable and is given as

S
‘0

WbJ.m,=

0

~(-muiO) dt.

(1)

Note that the probability measure of W,,, is the same as Meyer’s performability

WI-

The reward rates used here can not only express the achievement by the system,
but also can be generalized to include the penalty due to the system crashes. A
penalty rate, which should be negative, can be regarded as the average loss per unit
time when the system is unable to perform any service. Since the reward and
penalty rates only represent the relative indices between different configurations
and system crashes, we simply let p(w) = 0 for w E 00 and use the reward rate as a
general term for the return of the system.
The second function, a(w), called the crash probability, represents the probability
that a module failure causes a system crash. This function indicates the system’s
vulnerability to a module failure when the system configuration is w. a(w) may be
either the coverage failure [32], or the probability of dynamic failure [ 181associated
with the configuration w.
The reward rate p(w) is an implicit function of the number of computing clusters
in each task class. We assume that redundancy in each computing cluster does
not affect its performance.4 On the other hand, a(o) depends on the number
of redundant modules associated with each computing cluster. Since a configuration is completely specified by n and T, where n = [n,, n2, . . . , nk] and r =
rk], the functions p(w) and a(w)
used interchangeably with p(n) and
h,r2,...,
a(n, r), respectively, in the rest of this paper.
Several authors have derived the distribution and the moments of performance
variables for gracefully degradable systems under the restriction that system reconfiguration is allowed only upon failure [8, 13, 221. Moreover, such a system has
exactly one new configuration to choose from upon detection of a module failure.
This, however, is an unnecessarily limiting factor (it might result in a configuration
with less performance and reliability than the actual system’s capacity), since there
are usually several alternative configurations available for a system with multiple
modules. For example, when there are four modules available upon failure, we can
construct one Cmodule redundant computing cluster, or one triad and one simplex,
or two dyads, or four simplexes. Conventional reconfiguration concepts becomes
more inappropriate when we consider the fact that the remaining mission lifetime
has to play an important role in deciding on a new configuration. This fact can be
seen easily with the following two simple cases:In the first the remaining mission
lifetime is very short, in which case the probability of having failures is very small.
Thus, the computation capability is more important than reliability for higher
rewards. In the second case the remaining mission lifetime is long. In this case the
probability of having failures becomes large and any good configuration should be
able to tolerate module failures and minimize the possibility of a system crash.
In the discussion that follows we consider the problem of determining an optimal
reconfiguration strategy, which maximizes the expected reward E[ W,o,,]. This
optimization problem is equivalent to controlling system reconfiguration such that
’ As discussed in the Conclusion, this assumption can be relaxed.

332

Y.-H.LEE AND K.G.SHIN

the system follows a certain configuration trajectory to provide a maximum
expected reward, even in the face of random failures.

3. Derivation of Optimal Reconfiguration Strategy
Let RZ$, = (r%(t) ] t E [0, to], m E (0, . . . , mo)) represent the optimal reconfiguratton strategy that maximizes the total expected reward E[ W,,,]. Since the
optimal configuration y%(t) is completely specified by the values of nf and rf, the
problem is to determine n:(t) and r:(t) for all t E [0, to] and m E {O, . . . , m) that
maximize E[ IQ,,].
3.1 PROBLEM FORMULATION. On the basis of the assumption of an exponential
distribution of failure occurrence, the probability of having a failure during a small
interval 6t is approximately equal to X 6t. If ym(t + 6t) is the configuration used at
RML = t + at, then the cumulative expected reward at that time, WI+lr,m,can be
expressed as

Wt+L5t,t?l
with probability 1 - mX 6t
P(Y& + st))st + w,,
with probability a(y,(t + 6t))mX 6t
= P(Y& + Q))st
(2)
p(y,(t
+
at))&
+
Wl,m-l
with
probability
(1
a(r,(t
+
bt)))mX
at,
-I
wheremE(l,...,
moj and t E [0, to]. Thus, a recursive expression for the expected
reward is derived as follows:
E[ W+a.,] = (1 - mX Gt)E[dAt
+ 6t)W + W,,,]
+ a(r,(t + 6t))mX Wdy,(t + 6t))G)
+ (1 - a(r,(t + 6t)))mAGtE[p(ym(t + st))ht + W&-J.
(3)
On the basis of the exponential distribution assumption, at any moment there is
at most one occurrence of failure, implying that the maximum jump in M(t) is
one. Because of this, when the system reconfigures itself into a new configuration
y,(t) from Tm(t + 6t) or from -ym+l(t+ 6t), it must be in this configuration for a
nonzero mission interval; otherwise, there is no need to move in that configuration
at all. Let the optimal strategy RST+,, = limdl-o+RS~+,,, and the configuration
y,,,(t+) = lima,,,,+ T,,,(t + 6t). Then the following lemma gives a recursive representation of the optimal reconfiguration strategy.
LEMMA 1

RS:+,, = (SW+)) U RS:,,, U RS,t,-,.
PROOF. Suppose that the configuration chosen at RML = t + 6t by a reconfiguration strategy is used at least for the period 6t if there is no occurrence of
module failure, and also assume that there could be at most one failure occurrence
during 6t. Let 6W be the reward gained during this 6t. When there is no failure,
ELW+,t,m] = E[ W,,,] + E[6 W]. Thus, to have a maximum expected reward in this
case, WL,,,, > (-y%(t+ At)) U RS&. Similarly, RS,*,,,, > (y$,(t + 6t)) U RS&,
when we consider the case of one failure occurrence. On the other hand, by
definition, the system does not use any configuration other than (i) y$,(t + 6t) and
(ii) the configurations belonging to RS:, and RS&,. Thus, Lemma 1 follows
immediately, since the assumption at the beginning of this proof becomes true as
6t +o+. cl

Optimal Reconjiguration Strategyfor a Multimodule System

333

The above lemma can be used to determine recursively the optimal reconfiguration strategy. With the knowledge of RS,,,, RSlf,-, , and their respective expected
rewards E[ I+‘$,] and E[ W&J,
the optimal configuration y Z,(t+) can be determined by Theorem 1 below.
THEOREM 1.

y%(t+) maximizes J,,,,,(w)for w E Q,,,,which is defined by

J&w) = P(W)- 4~)m~E[W$J.

(4)

PROOF. Suppose ym(t + 6t) is applied at RML = t + 6t for the period 6t and
then either RS:, or RS&-, is used for the remaining mission lifetime t. Then,
from eq. (3) we get

E[ W+mn 1 -

Wf’$zI = dwn(t + ~t))~t- a(rm(t + 6t))mX6t

E[Wf),,-,]

- mXGt E[WTm - IV&-,].
Notice that the only terms in eq. (5) depending on ym(t + 6t) are

(5)

dy& + 60) - 4rdt + &))mWW&J.
Thus,
+ 6t) maximizes the above expression over u E Q,,,for any 6t > 0.
Combining this with Lemma 1 proves the fact that y$(t+) maximizes Jl,m(w). Cl
We define the optimal reconfiguration problem for deriving y $,(t +) in the
following form:
OR Problem:

J,Jn, r) = p(n) - a(n, r)mXE[W &-,]

maximize

“J

subject to

i

(ni + ri) 5

1-l
niy ri E I'
for i=l,2

m,
,...,

k,

ri = 0

when

ni = 0.

Though Lemma 1 and Theorem 1 provide a recursive relationship necessary for
obtaining RS$,,, the relationship does not yield an acceptable solution. It requires
a solution to the OR problem for all t E [0, to] (thus infinitely many times). As a
remedy for this difficulty, in the next section we convert the OR problem so that
it has to be solved only when y%(P) # y%(t), instead of for all t E [0, to].
3.2 PROBLEM TRANSFORMATION.
Once an optimal configuration at a moment
during the mission is determined, it will be used for a nonzero mission interval.
Given m, it is therefore natural to look for the switch times (in terms of the
remaining mission lifetime) at which the optimal configuration is changed to
maximize the total expected reward. That is, we only have to solve the OR problem
at those switch times instead of for all t E [0, to]. Let s’, E [0, to], j = 0, 1, 2, . . . ,
m=1,2,...,
mo,be the switch times for RS:,,, where s”, = 0. Then, by definition,
$Xsjm) # rZ(s’,‘) = lim61+o+y$(s ‘, + 6t) for j 2 1. As shown in Figure 2 for the
case of no module failure, when the remaining mission lifetime decreasesto s Ji,
the system should reconfigure itself from y $(s’,+‘) to 7 X(s’,) and then keep the
same configuration until the remaining mission lifetime is reduced to si,-‘.
From eq. (5) for two conliruations wI and ~2, both in fi, with (Y(w~)= 44, we
say that o1 is better than 02 if p(w,) > ~(4. On the other hand, if a(~,) # 44,
we need a function A,(@, , w2), which is defined as follows:
A&l,

w2)

=

-4t?l(w2,

WI) =

Pbl) - P(W2)
ma(wl) - ma(w2) ’

334

Y.-H. LEE AND K.G.SHIN
optimal configuration

0

~

Y

0

1
%lu

8,

Y

--w-w

2

Jn

Y

Y

*

A

b

&

remaining mission lifetime
FIG. 2. Switch times and optimal configurations in case of no module failure.

Also, define the following two sets:
L&j,)
G,&‘,)

= (& E a, 1a(3) < a($$(sjm))],
= {; E Q, 1a(;) > a($‘#‘,))).

Lm(si,) and G,,,(si,) represent sets of configurations with the crash probabilities
less than and greater than that of the optimal configuration at RML = si,,
respectively. Then, the following theorem elucidates a useful property of the switch
times s’,.
THEOREM 2. For all remaining mission lifetime t E (s’,-‘, s’,]

PROOF. Since -y%(t) = rZ,(s’,) for all t E (si,-‘, s’,], we have the following
relationship from Theorem 1:

p(yZ,(si,)) - a(yZ,(s’,))mXE[ W&J

I p(w) - a(w)mXE[ R$,,-J

for all w E Q2,.

Therefore,
dY+?,))

- Pb)

ma(r%sI,N - ma(w)
PhwJ)
-P(o)
m4rtNA
- ma(w)

L XE[W;,-,]

for all

u satisfying U(W)C a(y%(t)),

5 WW:,n-,I

for all

0 satisfying (Y(O)>

a(r%(t)).

0

Note that E[ Wrl;n-,] is a function oft. However, the function A,(y%(sjm), w) is
dependent upon configurations only. Once the optimal configuration 7 Z(s’,) is
known, rni&EL,oi) A,(y%(s&), w) and maX&G&j,)
A,($$($),
w) can be determined. The only computation needed to determine s’, is to calculate E[ W:,-,]
recursively by eq. (5) until the inequality (6) becomes invalid, that is, r$,(s(sj,)is no
longer optimal when RML > s’,. Thus, Theorem 2 provides the solution for si,,
which is the intimum of {t ] to > t > s’,-’ and t that does not satisfy (6)). If the
inequality (6) is valid for all t E CS’,-‘,to) then s’, does not exist. Theorem 2 also
presents a special property of an optimal configuration; with an optimal configuration r$(s’,), the function &(yZ(sj,),
w) partitions fl, into two sets, L&i,) and
G,(si,), as shown above. An arbitrary configuration without this property can
never be an optimal configuration regardless of the remaining mission lifetime. It
is important to note that s’, represents the reconfiguration time independent of
the presence or absence of a module failure. Moreover, the solution to the OR

Optimal Reconfiguration Strategyfor a Multimodule System

335

problem is embedded in the process of solving for the s’,. To explore this property
and make full use of it, we present the following lemmas and theorem.
LEMMA 2. E [ WZJ is continuous and nondecreasingin t.
PROOF. Dividing both sides of eq. (5) by 6t and taking limits as bt + 0, we can
write a first-order differential equation of E[ W,,,] with respect to t as follows:

dMW,rnl+ mXE[K,l = p(y,(t+N + mM1 - a(rdt+)MWm-d.
dt

(7)

is bounded for all ym(t) E Q,,,,so is E[ W,,,] for all t E [0, to] and
5 1 for all -ym(t) E Qt,. Thus, dE[W,,,]/dt
is finite, implying the continuity of E[ W,,,] and, therefore, the continuity of
Since p(y&t))

m E (0, . . . , mo). Also, 0 5 a(y,(t))

EW:mI.

To prove that E[ W&J is nondecreasing in t, we must show that
E[ W:+A,,,] L E[ W$,J

for all

At I 0.

Let
RSt+a,m
= (T,Jt) IO 5 i 5 t + At, rm(i) = r$(i - At) if i L At, or r$,(O+) if i < At).
Under the strategy RSl+asm,we use the configuration y%(t) when RML = t + At
and r$,(O’) when RML I At. Then, it is easy to seethat the expected reward based
on RS,,,,, is larger than E[ W&J. Thus, E[ Wf,,,,] L E[ W&l. Cl
LEMMA 3. For real numbers ai and bi, i = 1, 2, 3, where bl > bz > b3,
(al - adl(h - b2) I (al - a3)/(bl - b3) if and only if (a, - a2)/(bl - b2) I
(a2 - ad/@2 - b3). Also, (a, - a2)/(b, - b2) zc (a, - a3)/(bl - b3) if and only if
(al - aMh - b3) 2 (a2 - aMb2 - bd.
PROOF. The proof is trivial and thus omitted.

Cl

Then, +,&,J # 4 if &(sj,) # 4. The following theorem gives a recursive
relationship between rZ(sj,) and rZ,(s’,“) when s’, c 00.
THEOREM 3. Ifs’, < to, then y%(s’,+‘) E &(s’,),

and a(y%(si,+‘)) I a(G)

all

6 E &(s’,).
PROOF. From the definition of switch time, 7 $,(s’,“) has to be considered if
s’, < to. First, we prove that a(rZ(sim+‘)) < a(yZ(si,)). Since E[ W$,,] is nondecreasing, we have

If a(-yZ@im+‘))
for t E (s$, s ‘,‘I. Thus, J&yZ(s&)) 2 J&w) for all w E G&J.
> a(r$(sj,)),
then TX(&) is also an optimal configuration at t E (s’,, s’,“], which
is contradictory to the definition of switch time. Therefore, a(-y f(s’,+‘)) <
a(y$,(sj,)).
This result also implies &(s’,) # 4.

336

Y.-H.LEE AND K.G.SHIN

Next, we prove y$(s’,+‘) E @,,,(s’,). When there is one and only one w E
L,,,(s’,), the theorem automatically holds in case of s’, < to, since rz(s’,“) exists
and a($$(:(si,+‘)) cannot be larger than a(~$,($,,)). Consider a configuration
w E (i 1a(i) < a(rt(s’,“))

< a($&)),

6 E f&J.

Since rZ(s’,“) is optimal and E[ IV&] is continuous (and so is E[ I%$+,]), the
following order can be obtained from Theorem 2:

&t$%sj,+‘), 0) 2 WW;T,-,I = An(rX~j,+‘),r%sj,))

where t E (s’,, s’,“].

Applying Lemma 3, we have Am(y$,($,,), O) L &(r$(~~im+‘), +$s’,)).
For w E Q,,,satisfying a($!,(t~s’,“)) < a(o) < a(?$,(&)), A,J~$,(sim+*), rZ(si,)) 2
E[W$;,,-,] and E[ IV&,] L A,,,(y%(~i,+‘), o) for all t E (s’,, s’,“]. The continuity
in E[W&,]
implies A,(y$(t~~i,+‘), rt(s’,)) L A&r$(~j,‘),
w). From the second
part of Lemma 3, we also obtain A,Jr%(si), W) 2 A,(r$(s’,+‘), $$(s’,)). Thus,
r$(s’,“) E &(s’,).
If there is only one configuration in a,,,(&), the theorem is proved. Suppose
there exist two configurations, w1 and w 2; then &(YWJ,
WI) = &(S($,,),
02)
= &(wI, ~2). T o satisfy Theorem 2, the configuration with the smallest a(w),
w E *&‘,),
becomes optimal for all t E (s’,, s’,+‘]. Cl
Theorem 3 indicates that, while solving for si,+’ in which A,Jr$(&),
W) has to
be minimized over L&J,
the optimal configurations r%~s’,“) can be obtained
simultaneously. Also, we can exclude the configurations w E G,JsQ during the
determination of si,+’ and r%(s’,“). Note that even if &,(s’,) is not empty,
rZ(s’,“) is not always needed since s’, may be greater than to. When there is more
than one element in a,,,(~‘,) and s’, < to, the configuration with the smallest crash
probability in am(&) is optimal for the interval (s’,, s’,“]. Note, however, that all
other configurations in a&s&) are optimal only at lim~l+,-,+(si, + at). Theorem 3
also provides additional properties concerning the optimal reconfiguration strategy
as shown in the following corollaries.
COROLLARY 1. a(r%sL))

< a(r%(s’,)) andp(y$(&))

< p(~%(sjm))jbr all i B-j.

PROOF. In Theorem 3, we proved a(r%(sj,+‘)) < a(~%(&)). Since A&$&,),

r$(s’,“))

I EIW$;,m-l] > 0, we have p(yZ(:(sj,+‘)) < p(y$(&)).

Cl

Definition 1. A reconfiguration process is said to be acyclic if for tl # f2,
w(&)
= yhf(t&2) implies af&td
= x4dt) = m&2) for all t E [h , t21.
From Corollary 1, rZ(sk) # rZ(s’,) for all i # j. Since no repair is allowed
during the mission, the system degrades from the m module system to the m - 1
module system in case of a module failure. Thus, we immediately get the following
Corollary 2.
COROLLARY 2. The reconfiguration process based on RSZ-,

denoted by we,,

is acyclic.
COROLLARY 3. For all m E (0, . . . , mol and j L 0, ifs’, < to, then

PtY%sj+‘))
PtY%$J
a(y%(sE’)) z a(y%sj,))

*

PROOF. Consider a remaining mission lifetime t E (s’,, 92’1. Since E[ W&J is
nondecreasing and continuous in t, dE[ W:,]/dt I 0, and by rearranging eq. (7)

337

Optimal Reconfiguration Strategyfor a Multimodule System
A

tane =
I/

I/ /’

.

min
d4<ol7;(4))

. /

mAIni-4 &J)

4n,

FIG. 3. The coordinatesof optimal configurations.

we have

lMw-3)
mat7 Z4.C’))

= XE[ W&J.

From Theorem 3, we get

=[W&,-,I 2 -4nt~%$J,YX$‘N.
Thus,

cl
Definition 2. The coordinates of a configuration w are defined as (a(w), p(w))
in an x-y plane.
Using Figure 3, we can explain the relationships obtained from Corollary 3. Note
that the slope of the line segment between (a(~$(&)), p(y$(sj,))) and (a(yZ,(sj,+‘)),
p(yZ(si,+‘))) is equal to mA,($#J,
rZ(~j,+‘)). It is easy to see from Figure 3 or
Theorem 3 that A,(yz($,,), y$(s’,+‘)) is increasing in j. Figure 3 also indicates
that the coordinates of the optimal configuration, that is, (a(r$(si,+‘)), p(yZ(s$‘))),
are located within the triangle surrounded by (0, 0), (0, p(yZ(si,)), and (a(rX(si,)),
p(y$,(si,))). When there is no configuration whose coordinates are within this area
or when the inequality (6) is valid for all t E (s,j+* , to), we do not need to consider
the next switch time s’, for the case with the mission lifetime to. Though the
optimal configurations can be indicated from their coordinates, the switch times
have to be computed on the basis of Theorem 2, in which no optimization problem
is involved.

338

Y.-H.

LEE AND K. G. SHIN

As a final solution step for the optimal reconfiguration strategy, we have
developed the following algorithm A(RS), in which y $(s’,) and E[ WFJ are
calculated using r&i(&)
and E[ W&-J for all t E [0, to]. Complete algorithms to
solve for yZ(s’,) will be presented in the next section. When to is large, a different
algorithm, where rX(t + 6t) and E[ W:,,,,] are computed based on r%(t) and
E[W&,] for all m E (0, . . . , m& is better than Algorithm A(RS) insofar as the
storage requirement is concerned. However, the underlying principles are the same.

AlgorithmA(RS)
Step 1. initialization
la. Form = 1,2,. . . , mo, find rt(s!,,) in which n maximizesp(w), w E tL5 When
there is more than one configuration that maximizes p(w), the configuration
with the smallestcrashprobability is chosen.
lb. Find E[ IV:,] = p(y:(s]))( l/X)( 1 - 6” - Ate-‘,‘) for t E [0, to].
lc. Choose6t to digitize t E [0, to].
Id. Set m := 1 and start the following recursivesteps.
Step2. Set m := m + 1, t := 0, and j := 2. If m > mo, stop the algorithm.
Step3. Find YE(&) E +,,,(s’,-‘) and a(-&($,,)) 5 a(G) for ; E (P,,,(s~‘).
Step4. Using rZ(s’,-‘) as the optimal configuration, calculateE[ IV:,,,,,] by eq. (5).
Step5. Set t := t + 6t. If t 2 to then go to Step2
else if A,,,(-&&‘), -yt(s’,)) 2 E[ W$,,-,] then go to Step4
elsej := j + 1 and go to Step 3.

4. Determination of Optimal Configuration
As defined in Section 2, a configuration w E Q, can be specified by (n, r) where
n = [n,, n2, . . . , nk], ni represents the number of computing clusters for the task
class i, r = [r,, r-2,. . . , rk], and ri is the degree of redundancy associated with the
computing clusters in the task class i. It is assumed that the rewards gained from
executing different task classesare independent. Thus, the system reward rate, p(n),
is equal to xpI pi(ni) where pi is the reward rate for the task class i. Furthermore,
since all modules within the system are identical, the crash probability, a(n, r), can
be represented by l/m Cf=l (ni + ri)ai(ni, ri) where ai(ni, ri) is the crash probability
when a module assigned to the task class i fails, given there are ni computing
clusters and ri redundant modules in this task class.
Consider the execution of class i tasks. When there are ni computing clusters,
the task class can be treated as an ni-server system. Let f (x) denote the performance
of an x-server system. Ideally, the performance of the system is additive in x, that
is, f (x, + x2) = f (x,) + f (x2). However, owing to task communications, resource
sharing, and other overheads, the performance of the system is subadditive, that is,
f (x1) + f (x2) 2 f (xl + x2). In most cases,f (x) is a nondecreasing concave function
of x as shown in [3], [5], [ 121, and [34]. Following tradition, we consider here the
casesin which the reward rate for each task class, pi(ni), is a nondecreasing concave
function of the number of computing clusters, ni.
Let hi(ri) be the crash probability of an ri-module redundant computing cluster
in the task class i, given that one of those modules fails. We assume that hi(ri) is a
decreasing convex function of ri for all i = 1,2, . . . , k. Relaxation of this assumption
may lead to a nonconvex programming problem and is discussed in Section 5.
Thus, rihi(ri) is also convex in ri. The crash probability of the task class i then
becomes
A
(Yi(ni,

ri)

= (ri -

nifi)

s,

I

hi(fi + 2) + (ni - ri + TZiFi)f+$
I
I

5 It is easy to prove that y%s!,,) maximizes p(w) from Theorem 1 and E 1WmI = 0.

I

hi(ff + l),

339

Optimal Reconfiguration Strategy for a Multimodule System

where fi = LrJniJ and ni > 0. This implies that (ni + ri)ai(ni, ri) is a piecewise linear
and convex function of ri. When ni?i 5 r; 5 ni(Fi + l), 9 E I+, the ~10~ of the
corresponding linear piece is (A + 2)hi(fi + 2) - (9 + l)hi(fi + 1). When hi(ri) is
convex, it can be shown that an equal distribution of redundant modules over all
computing clusters is optimal; this has been assumed in Section 2.
As indicated in Algorithm A(RS), there are two nonlinear integer programming
problems to be solved for deriving the optimal reconfiguration strategy. The first,
called PO,is a nonlinear integer knapsack programming problem which has to be
solved for determining y Z(s!,,).
PO: maximize
ni

i pi(&)

i=l
k

subject to

(8)

1 ni I m,

i=l

n;EI+

for

i=l,2

,...,

k.

The second, called PI, is an integer fractional programming problem that is to
determine rZ(.ri,+‘), given r$,(s’,) forj L 1. P, can be expressed as
P, : minimize
“iTi

P(Wo)- Et, Pi(&)
given w0 E Q,
ma(w0) - x:bl (ni + ri)(Yi(ni, ri)
k

subject to 1 (ni + ri)tii(ni, ri) C ma(w),
i=l

lli, ri E I+ for i=

1,2 ,...,

k

,IJ, @i + ri) 5 4
j,

bi

k,

(9)

ri = 0 when ni = 0.

+ ri) 5 m,

4.1 AN ALCNRITHM FOR SOLVINGPO. The nonlinear integer knapsack problem
has been considered for various applications, such as resource allocation, portfolio
selection, and capital budgeting. Several methods have been proposed for solving
this problem, for example, dynamic programming approaches [6, 241, the shortest
path algorithm [I 11, and ranking methods [20, 351. Michaeli and Pollatschek
investigated the problem and provided a necessary and sufficient condition for the
optimal solution [23]. To solve the problem PO, since the coefftcients of both the
objective function and the linear constraints are all one, we can employ an
algorithm ‘similar to the one in [lo], which is given below.‘j
Let Ai
= pi(ni + 1) - pi(ni). The principle of the algorithm is to allocate
modules such that the system can have maximum return.
Algorithm A0

Stepl. Set~i:=OfOralli=1,2
,..., k.
Step2. Selecti* suchthat Ai*(ni*) = maxIsis&Ai(
If Ai<ni*) = 0, then terminate the algorithm.
Step3. ni* = ni* + 1 and m := m - 1. If m = 0, then terminate the algorithm.
Otherwise,go to Step2.
TO determine Ai*
= lIlaX~~isk&(ni),
we need to sort Ai(
Clearly, it is not
necessary to sort all Ai
for every m. A,(O) must be sorted during the first
6 Existence of the incremental algorithm in [IO] was brought by one referee to the authors’ attention.
Because of the authors’ unawareness of this reference, Algorithm A0 had been developed and presented
as a new algorithm in the earlier draft of this paper.

340

Y.-H. LEE AND K.G.SHIN

iteration. However, since ni* is changed only in later iterations, Ai*(tZi*)has to be
evaluated and inserted into the previous sorted sequence. Note that there are at
most m iterations required for this algorithm to terminate. Another advantage of
Algorithm A0 is that all r$,(sA) can be solved at once for all m E (0, 1, . . . , m,).
By assuming m = m. at the beginning, yZ(s!,,) is obtained at the end of the mth
iteration.

4.2 AN ALGORITHM FOR SOLVING P1. The solution of PI can be divided into
two levels: the lower level is to determine rt, i = 1, 2, . . . , k, by minimizing the
objective function (9) for a given n; the higher level problem is to determine nt by
minimizing the objective function (9) with the calculated rf from the lower level.
Since the only place that ri’s appear is in the denominator of the objective function,
the lower level problem can be stated as follows:
P2: minimize

j

(ni

+

ri)%(R,

ri)

5
k

subject to

k

C. ri 5 m - C. nip
i=l

riEI+

i=l,2

for

,...,

k,

(10)

i=l

ri = 0 when

ni = 0.

By letting riai(O, ri) = ri, the last constraint can be eliminated. Then, the problem
P2 is an integer knapsack programming problem that is to minimize the sum of
nonlinear functions. If (ni + ri)ai(ni, ri) is convex with respect to ri, we can apply
an algorithm similar to A0 in which we choose minM&((ni + ri + l)ai(%, ri + 1)
- (ni + ri)ai(ni, Ui)) in place of maxl&=k (/Ji(ni + 1) - pi(&)) in Algorithm Ao. When
(ni + ri)ai(ni, ri) is not convex, the lower level problem becomes a nonconvex
programming problem. There is no guarantee that the solution obtained by
Algorithm A0 is the global minimum.
Let P(n) = rnin,,zf=, (ni + ri)ai(ni, Ti) obtained from solving Pz, given n. Thus,
the problem P, can be converted to the following form:
P3:

~(~01 - dn)
. .
mlnlnfnlze ma(wo) - p(n)
subject to /3(n)< ma(w) and p(n) C P(WO),

(11)

k
2

EliI

Wl,

niEI+

for

i=l,2

,...,

k.

i=l

By Corollary 3, the triangle area surrounded by (0, 0), (0, p(nO)),and (( l/m)P(n’),
p(n’)) is a feasible region described in terms of the configuration coordinates. Since
there does not exist an explicit form of mapping from the configuration coordinates
to n, an enumeration is the only means of finding whether or not a configuration
is within this triangle region of the configuration coordinates.
Consider the use of an explicit enumeration (or brute force enumeration) for
solving P3. For every possible combination of ni satisfying the constant z F=i ni I
m, we map the combination into r:, B(n), and (p(wo) - p(n))/(ma(oo) - /3(n)).
Then, the configuration with the minimum ratio is chosen as the optimal configuration. When there are k task classesand m modules available, the total number
of combinations in n to satisfy the constraint x:, ni 5 m is (“‘tk). Thus, when the
size of the system is moderate, explicit enumeration approach is reasonable. For
example, when k = 3 and m = 12, the total number of enumerations is 455.
There are two important and advantageous properties associated with explicit
enumeration: (i) The coordinates of all feasible configurations for m available

Optimal Reconfiguration Strategyfor a Multimodule System

341

modules, that is, (( l/m)fi(n), p(n)), can be obtained from the enumeration. Thus,
from Theorem 3, we can easily determine all optimal configurations, yZ(s’,) for
j= 1,2, . . . . (ii) When we solve problem P2 for r: with m = mo, the other r:‘s
are determined simultaneously for all m E (C&, ni, . . . , mo). We obtain the
coordinates, incorporated with p(n), of all feasible configurations for m E { 1, . . . ,
m). Therefore, the optimal configurations 7 E(s’,) for m E ( 1, . . . , mo) can be
determined. These two properties lead to a situation in the determination of a
reconfiguration strategy in which explicit enumeration has to be conducted only
once.

Remarks on Fractional Programming Problems. For a general fractional programming problem, the objective function in eq. (11) is no longer convex or
concave with respect to ni even if /3(n) is convex or concave with respect to ni
[27]. For a continuous nonlinear fractional programming problem, some equivalent
or dual problems have been proposed in [7] and [28]. With integer constraints,
Chandra and Chandramohan [4] suggestedapplying a branch and bound method
after solving the continuous problem. However, no example or analysis is provided
to show the efficiency of their algorithm.
A recent survey on the methods of solving the fractional programming problem
[ 141 has discussed three different state-of-the-art approaches. The first approach
uses variable transformation and is probably the most efficient method for linear
fractional programming problems. The second approach deals with the problem as
a nonlinear programming problem and applies a suitable search algorithm to find
the solution. The third approach usesan auxiliary parameterized problem suggested
in [ 141, which is briefly discussed below. Let F3 denote the feasible region for Pj .
Define the auxiliary problem Q(q) by

Q(O):zn

~4~0)

-

dd

-

dmhd

-

P(n)).

Let z(q) be the minimum value of Q(q) and n*(q) be the optimal solution of Q(q).
If there is an t* such that z(v*) = 0, we can have
*
t

_
-

P(WO
m&0)

-

dn*h*N
-

P(n*(a*N

P(WO)
=

m&0)

-

p(n)
-

P(n)

for all n E F3. The above equation implies that an optimal solution of Q(a ‘) is also
an optimal solution of P3. Hence, an algorithm is needed to search for q* such
that z(v*) = 0 by solving Q(t) iteratively. The complexity of the algorithm is based
on the efficiency of solving Q(V) and the search algorithm, for example, Newton’s
method or binary search. Meggido [21] proposed an algorithm that combines the
search for t* and the dynamic programming approach to Q(V). The resulting
algorithm requires O(km’log m) evaluations of B(n) and p(n).
Note that the problem Q(a) is the same as the OR problem, that is, a nonlinear
knapsack programming problem. It might be more efficient to solve the OR
problem directly instead of searching for v* and solving Q(o) iteratively. When
/3(n) is convex with respect to ni, the objective function becomes convex with
respect to n;, implying that Algorithm A0 can be applied. The algorithm A ,, which
includes Algorithm Ao, is provided below using an operator Pj and a function Ai
defined by
Pj(ll) E (n,, n2, . . . , nj-1, ni + 1, nj+l, . . . , nk)

Ai(n)s APi(

- p(n)- WW,,m-,l(PU’i(n))- P(n)).

342

Y.-H. LEE AND K. G. SHIN
TABLE I. CRASH PROBABILITIES hi(n) AND REWARD RATES pi(n) USED IN THE EXAMPLE

n
h,(n)
p&j
hz(n)
p*(n)
h&z)

1

0.6
1.0
0.5
0.8
0.3
0.6

p3(n)

2
0.3
1.8

0.25
1.5
0.1

1.2

3
0.1

2.4
0.1
2.2
0.05
1.7

4
0.05
2.9
0.05
2.8
0.01
2.2

5
0.005
3.3
0.025
3.4
0.005
2.7

6
0.003
3.6
0.013
4.0
0.003
3.2

7
0.002
3.8
0.005
4.6
0.002
3.6

8
0.001
3.9
0.002
5.1
0.001

4.0

Algorithm A,
Step 1. Set L := 0 and U := to.
Step2. a. Setth:=L+h(U-L)/Nforh=

1,2 ,..., N.
b. Forh= I,2 ,..., N
bl. Setdm:=mandni:=Ofori=
1,2 ,..., k.
b2. Solve for P(Pi(n)) as indicated in the solution of Pz and A,(n) for i = 1,
7

I,

b3. $.ieit’i*h.such that Ai* = maxl,i,kAi(n). If A,*(n) 5 0, go to (b5).
bk n := Pi,(n) and dm := dm - 1. If dm # 0, go to (b2).
b5. Set y%(h) := n.
Algorithm A, determines the optimal configurations for a specific number of
partitions of the mission lifetime, that is, N. If the optimal configurations at th and
th+, are different, the solution can be relined by setting L = th, u = th+, and then
repeating Step 2 of A I . This will lead to a more accurate switch time between th
and th+, . Although Algorithm A, only provides one-level partitions, it is easy to
extend more refined partitions.
4.3 AN EXAMPLE OF THE OPTIMAL RECONFIGURATION STRATEGY. Consider a
system with 12 modules at the beginning of the mission. The module failure rate
is assumed to be 0.0005. The tasks to be executed during the entire mission are
grouped into three classeswhose respective reward rates pi(ni) and crash probabilities hi(n) for i = 1,2, 3 are listed in Table I. In addition, we include the constraints
ni L 1 for i = 1, 2, 3, indicating that at least one computing cluster must be
available for each task class throughout the entire mission.
To satisfy the constraint ni > 1 for i = 1, 2, 3, we must begin with assigning one
module to each task class. Applying the explicit enumeration given in Section 4.2,
we can find all possible configurations and the respective switch time for a mission
with infinite mission lifetime. The optimal configurations y%(si,) for 4 5 m 5 12
are listed in Table II, whereas Table III gives the switch times for each optimal
configuration. The optimal reconfiguration strategy is obtained from the combination of both of these two tables. Notice that certain optimal configurations will
never be used. For instance, $(.s<) is one, since s: = 00.
When the remaining mission lifetime is known, the optimal configuration can
be found from Tables II and III. The optimal reconfiguration strategy is derived
ofiline in table form before the mission. For on-line use of the strategy, the system
only needs to look up the tables of optimal configurations and switch times (e.g.,
Tables II and III). In this way, the system follows an optimal reconfiguration
trajectory using (i) the switch times in the same row of the two tables for the case
of no module failure or (ii) row changes in the tables and then the switch times in
case of module failures. In Figure 4, given that the mission lifetime is 1000, the

I

I

I

I

-^o
--t-4
V

344

Y.-H. LEE AND K. G. SHIN
TABLE III.
m

s”,

4
5
6
7
8
9
10
11
12

0
0
0
0
0
0
0
0
0

s:.

sf

1194
436
349
755
254
227
206
188
104

SWITCH TIMES FOR THE EXAMPLE
Sit

00
1391
938
1364
573
506
455
413
173

Sit

co
1150
2813
635
643
541
490
314

Q,
739
760
574
602
378

S:

Sk

1379
1108
807
716
447

m
m
1098
1321
646

reward rate

S:,

Sil

1611
00
QI

00

crash probability

8.l

reward rate
-

%2&)

5.1

41)
rr:,c
1

Yli(&)
----------4
3.l
1000

r---J

0.5

c-----------

-r-- I”
: crash probability
:

Y;*(4)

4.1

r---J

r--I
I

0.4

J

0.2

y;1(4:1)r-----;
,--,A

0.1
1

750

500

250

0

Remaining Mission Lifetime
FIG. 4.

Example of optimal reconfiguration trajectory. Module fails when the remaining lifetime is
800,520,400,3 IO.

configuration trajectories and the respective reward rates and crash probabilities
are plotted if the module failures occur when RML are 800, 520, 400, and 310.’
5. Discussion
5.1 CONCLUDING REMARKS. In this paper we have addressed the problem of
reconfiguring nonrepairable multimodule systems. Since we have treated the problem for general multimodule computing systems, both the problem formulation
’ These.are random and known only via failure detection mechanisms. For a demonstrative purpose,
we used arbitrarily chosen values.

345

Optimal Reconfiguration Strategy for a Multimodule System

and the solution approach of this paper have high potential use for designing the
growing number of fault-tolerant multiprocessors and computer networks.
Given multiple modules, computing clusters with appropriate redundancy for
each task class are formed so that the resulting system may meet both requirements
of performance and reliability in an optimal fashion. Because of the inherent tradeoff between performance and reliability, we need to determine system configurations that specify the number of computing clusters and redundant modules for
each task class. In addition to the conventional passive reconfiguration strategy,
which is invoked only upon detection of a module failure, we have shown the need
of an active reconfiguration strategy that allows the system to reconfigure itself as
the mission progresses, regardless of the occurrence of module failure. Thus, the
active reconfiguration strategy provides the optimal configurations by taking into
account both the degradation of the system due to module failures and the
progression of the mission.
Using the expected total reward as the criterion for determining the optimal
configurations, we have explored the properties of the optimal configurations,
which are useful for deriving solutions. A feasible region is described in terms of
the configuration coordinates. Although it is easy to find the configuration coordinates, the inverse mapping from the coordinates to a configuration does not exist
in closed form, thereby requiring less elegant enumerations.
In order to derive the optimal configurations, two nonlinear integer programming
problems have to be solved. The first is a knapsack problem, which can be solved
through a simple but elegant algorithm. The second is a fractional programming
problem, which is basically a nonconvex programming problem. In addition to an
explicit enumeration, we have discussed the other approaches known for solving
the fractional programming problem. Since both the explicit enumeration and the
fractional programming may become very complicated when the system size (i.e.,
mo and k) is large, some other approximations or heuristic algorithms need to be
explored.
As shown in the example of Section 4.3, the optimal reconfiguration strategy
can be represented by two tables: one is for optimal conligurations and the other
for switch times. Although the solution procedures for obtaining these two tables
are complex, they can be computed off line. The real reconfiguration during the
mission is performed just by looking up these two tables.
5.2 EXTENSIONS.Several assumptions that we have used can be relaxed by
employing a more complex optimization procedure. For instance, the reward rate
could be affected by the degree of redundancy incorporated in a task class. In such
a case we need to change p(n) to p(n, r). The optimization problem Pi can no
longer be decomposed into two levels. When the concavity of pi(ni) and the
convexity of a(ni, ri) do not exist, the optimization problems become nonconvex
and thus are more difficult to solve.
We can also remove the assumption that the reward rate and the crash probability
must be stationary, that is, independent of the mission lifetime. For such a case,
let p(w, t) and a(w, t) be used in place of p(w) and a(w), respectively. The problem
OR, then, becomes to maximize
J&o)

= P(W,t) - 4~

t)mWW,f,-,I

for

wE O,.

Obviously, this time dependency does not increase any complexity if we solve the
problem OR directly.

346

Y.-H.

LEE AND K. G. SHIN

In place of the total expected reward, a generalized objective function for the
optimal configurations could be defined as E[R( wl,,,)] where R is a function of
IV. Note that R ( IV) should be nondecreasing but may not be continuous. It could
be a step function as in the example in [ 131, or any other discontinuous function
with finite jumps. When R (IV) is not additive, that is, R ( W, + Wz) # R (WI) +
R (IV,), the optimal configuration at the current moment is dependent on the total
reward accumulated up to the current moment. Hence, the determination of an
optimal configuration must consider both the past and future configuration trajectories, thereby making the optimization problem very complex and difficult. The
difficulty can be foreseen by comparing the optimal contiguration problem with
the general stochastic optimal control problem. The optimal control problem
usually uses the summation (or integration) of the functions of variables as an
objective function to be optimized. However, the optimal configuration problem
requires the use of a function of the summation of variables, making the decomposition of these variables impossible. This is a matter for further research.

Appendix. Glossary of Notation
S-L: Set of all feasible configurations when the system has m operational
modules.

%?a1: Reconfiguration function defined at time t E [0, to] and with a
$&s~):
a(w):
(Yi(njpri):
P(n):
P(w):
Ah,
4:
G,(s’,):
L,( S’,):
M(t):
RFt,,,m,,(t
):
RML:
W,, :

RS&n,:
w,m:

configuration in Cl,.
Set of configurations in L,Jsjm) that minimizes the function
Am(&&), w). If r’,” exists, it must be a member of this set.
System crash probability when the system is with configuration w,
given a module failure.
System crash probability when a module assigned to the task class i
fails.
The solution of Problem Pz, where CF=l (ni + ri)a(ni + ri) for ri,
subject to CF=, ri I m - Cf=, ni, etc., for a given n = (nl, n2, . . . , nk).
Reward rate associated with the configuration w.
The ratio of p(w,) - p(w~) to ma(w,) - mar(wz).
Set of configurations that belong to Q,,,and have crash probabilities
greater than that of the optimal configuration at RML = s’,.
Set of configurations that belong to Q, and have crash probabilities
less than that of the optimal conliguration at RML = s’,.
The system state when the remaining mission lifetime is t. M(t) is
equal to the number of available modules if no crash occurs before t,
or 0 otherwise.
The stochastic process defined on the configuration used at RML = t
under a given reconfiguration strategy RS,,.
Remaining mission lifetime.
A reconfiguration strategy specified by the configuration functions
y,,,(t) where 0 5 m I m. and 0 5 t 5 to. Given a strategy, the system
usesthe configuration defined by T,,,(t) E RS,,,,, when RML = t and
there are m modules available.
The optimal reconfiguration strategy that maximizes the expected
total reward for the system with mission lifetime to and m. operational
modules initially.
The total reward accumulated for the system with mission lifetime t
and m operational modules initially. Since the failure occurs stochastically, W,,,,,is a random variable.

Optimal Reconfiguration Strategyfor a Multimodule System

347

hi(ri):

Crash probability when a module, assigned to a computing cluster
with ri redundant modules for the task class i, fails.
m(t), m: The number of total available modules in the system when RML = t.
mi(t), mi: The number of available modules assigned to the task class i when
RML = t.
ni(t), ni: The number of functioning modules assigned to the task class i
(or the number of computing clusters for the task class i) when
RML = t.
ri(t), ri: The number of redundant modules assigned to the task class i (or ni
computing clusters) when RML = t.
si,:
The jth switch time when the system is with m available modules.
The system should use the configuration rz(s’,“) continuously when
s’, < RML 5 si,+’ and change to rm(s’,) when RML = s’,.
ACKNOWLEDGMENT.
The authors wish to thank C. M. Krishna, Michael Woodbury, and anonymous referees for carefully reading this paper and making many
useful comments. Also, they are indebted to Rick Butler and Milton Holt of NASA
for their technical and financial support.

REFERENCES
1. ADAMS, G. B., AND SIEGEL, H. J. A fault-tolerant interconnection network for supersystems.IEEE
Trans. Comput.C-31 (May 1982), 443-454.
2. BARIGAZZI, G., CIUFFOLETTI,A., AND STRIGINI, L. Reconliguration procedure in a distributed
multiprocessor system. In Proceedingsof the 12th International Symposium on Fault-Tolerant
Computing.1982, pp. 73-80.
3. BHANDARKAR,D. P. Some performance issues in multiprocessor systems. IEEE Trans. Comput.
C-26, 5 (May 1977), 506-5 11.
4. CHANDRA, S., AND CHANDRAMOHAN,M. A branch and bound method for integer nonlinear
fractional programs. Z. Angew.Math. Mech. 60, 12 (1980), 735-737.
5. CHU, W. W., HOLLOWAY,L. J., LAN, M.-T., AND EFE, K. Task allocation in distributed data
processing. Computer13, 11 (Nov. 1980);57-70.
6. COOPER,M. W. Tbe use of dynamic programming methodology for the solution of a class of
nonlinear programming problem. Naval Rex Lag. Q. 27 (1980), 89-95.
7. CRAVEN,B. D., AND MOND, B. On fractional programming and equivalence. Naval Res.Lag. Q.
22(1975),405-410.
8. DONATIELLO,L., AND IYER, B. R. Analysis of a composite performance reliability measure for
fault tolerant systems. J. ACM 34, 1 (Jan. 1987), 179-199.
9. FORTES,J. A. B., AND RAGHAVENDRA,C. S. Dynamically reconligurable fault-tolerant array
processors. In Proceedingsof the 14th International Symposium on Fault-Tolerant Computing.
pp. 386-392, 1984.
10. Fox, B. Discrete optimization via marginal analysis. Management Sci. 13, 3 (Nov. 1966),
210-216.
11. FRIEZE, A. M. Shortest path algorithms for knapsack type problem. Math. Prog. II (1976),
150-157.
12. FULLER, S. H., OUSTERHOUT,J. K., RASKIN, L., RUBINFIELD,P. L., SINDHU, P. J., AND SWAN,
R. J. Multi-microprocessors: An overview and working example. Proc. IEEE 66, 2 (Feb. 1978)
216-228.
13. FURCHTGC~TT,
D. G. Performability models and solutions. Comput. Res. Lab. Rep. CRLTR-884. The University of Michigan, Ann Arbor, Mich., 1984.
14. IBARAKI, T. Solving mathematical programming problems with fractional objective functions. In
Generalized Concavity in Optimization and Economics, S. Schaible and W. T. Ziemba, Eds.
Academic Press, Orlando, Fla., 1981, pp. 44 l-472.
15. KAIN, R. Y., AND FRANTA, W. R. Interprocess communication schemes supporting system
reconliguration. In Proceedingsof the IEEE COMPSAC80.IEEE, New York, 1980, pp. 365-37 1.
16. KARTASHEV, S. L., AND KARTASHEV, S. P. A multicomputer system with dynamic architectures.
IEEE Trans. Comput.C-28, 10 @ct. 1979), 704-72 1.

348

Y.-H.

17. KIM, K. H.

Error detection,

reconfiguration

and recovery

in distributed

LEE AND K. G. SHIN
processing system. In

Proceedings of the 1st International Conference on Distributed Computing Systems. Oct. 1979,
pp. 284-295.
18. KRISHNA, C. M., AND SHIN, K. G.

Performance

measures for multiprocessor

controllers.

In

Performance ‘83: Ninth International Symposium on Computer Performance, Measurement, and
Evaluation. 1983, pp. 229-250.
19.

LEE, Y. H., AND SHIN, K. G. Gptimal design and use of retry in fault tolerant
systems. Comput. Res. Lab. Rep. CRL-TR-28-84.
Univ. of Michigan, Ann
1984.
20. Luss, H., AND GUPTA, S. K. Allocation of effort resources among competing
5 (1975), 613-629.
21. MEGIDW, N. Combinatorial
optimization
with rational objective functions.

real-time computer
Arbor, Mich., May
activities. Oper. Res.

Math. Oper. Res. 4

(1979), 414-424.
22. MEYER, J. F. On evaluating the performability
of degrading computer systems. IEEE Trans.
Comput. C-29, 8 (Aug. 1980), 720-731.
23. MICHAELI, I., AND POLLAISCHEK, M. A. On some nonlinear knapsack problem. Ann. Disc. Math.
I (1977), 403-414.

24. MORIN, T. L., AND MARSTEN, R. E. An algorithm for non-linear knapsack problem. Management
Sci. 22, 10 (June 1976), 1147-I 158.
25. SAHEBAN, F., AND FRIEDMAN, A. D. Diagnostic and computational reconfiguration in multiprocessor system. In Proceedings of the 1978 Annual ACM Conference (Washington, D.C., Dec. 4-6).
ACM, New York, pp. 68-78.

26. SAHEBAN, F., AND FRIEDMAN, A. D. A survey and methodology of reconfigurable multi-module
system. In Proceedings of the IEEE COMPSAC78. IEEE, New York, 1978, pp. 790-796.
27. SCHAIBLE, S. Minimization
of ratios. J. Opt. Theory Applicat. 19,2 (June 1976), 347-352.
28. SCHAIBLE, S. A survey of fractional programming. In Generalized Concavity in Optimization and
Economics, S. Schaible and W. T. Ziemba, Eds. Academic Press, Orlando, Fla., 198 1, pp. 4 17-440.
29. SHIN, K. G., AND LEE, Y. H. Error detection process-model, design and its impact on computer
performance. IEEE Trans. Comput. C-33,6 (June 1984), 529-540.
30. SIEGEL, H. J., MCMILLEN, R. J., AND MUELLER, P. T., JR. A survey of interconnection
methods
for reconfigurable parallel processing system. In Proceedings of the AFZPS 1979 National Computer
Conference. AFIPS Press, Reston, Va., 1979, pp. 529-542.
31. SMITH, T. B., III, AND LALA, J. H. Development and evaluation of a fault-tolerant multiprocessor
(FTMP) computer. Volume IV: FTMP executive summery. NASA Contractor Rep. 172286. NASA
Langley Research Center, Langley, Va., Feb. 1984.
32. STIFFLER, J. J., AND BRYANT, L. A. CARE III phase report-Mathematical
description. NASA
Contractor Rep. 3566. NASA Langley Research Center, Langley, Va., Nov. 1982.
33. TROY, R. Dynamic reconfiguration:
An algorithm and its efficiency evaluation. In Proceedings of
the 9th International Symposium on Fault-Tolerant Computation. 1979, pp. 44-49.
34. WULF, W. A., AND BELL, C. G. C.mmp-A
multi-mini-processor.
In 1972 Fall Joint Computer Conference. AFZPS Conference Proceedings, vol. 41. AFIPS Press, Reston, Va., 1972,
pp. 765-777,
35. ZIPKIN, P. H. Simple ranking methods for allocation of one resource. Manage. Sci. 26, 1
(Jan. 1980), 34-43.
RECEIVED SEPTEMBER1984; REVISED FEBRUARY 1986; ACCEPTED JUNE 1986

Journal of the Association for Computing Machinery, Vol. 34, No. 2, April 1987.

2009
2009 International
International Conferences
Conference on
on Embedded
Embedded Software
Software and
and Systems
Systems

Ontology-based Smart Home Solution and Service Composition
Jingjing Xu1, Yann-Hang Lee1, Wei-Tek Tsai1, Wu Li1, Young-Sung Son2, Jun-Hee Park2, and
Kyung-Duk Moon2
1

Computer Science and Engineering Dept.
Arizona State University
Tempe, AZ 85287

2

Electronics and Telecommunications Research
Institute
Daejeon, South Korea

Abstract

Ontology has received significant attention in recent
years with the emergence of semantic web [1][2][3][6].
It is an explicit specification of conceptualizations
which organizes the semantic information as the
knowledge base of the specific application domains
[4][5][6]. Furthermore, its use is extended to ServiceOriented Computing (SOC) to facilitate more
intelligent service discovery and composition [6][8].
The current ontology languages, such as RDF and
OWL [9][10], are often represented in XML and can
be processed by machines. They support the
specification of concepts, relationships, and associated
classification and reasoning mechanisms for data.
Ontology has been established for context-aware and
smart home applications [11]. For instance, in [12], a
Getting up scenario is described to show whether the
ontology-based model is valuable for description of
context information in a home domain. In [12], an
ontology-based model of the Tele-health Smart Home
to initialize the Bayesian networks to recognize patient
activity. Furthermore, ontology is used to model the
relations between devices and services in a home
environment and to manage software configurations
[14].
In our smart home solution, ontology is introduced
to deal with the three issues stated above base on the
semantic information. Note that it is possible that a
requested function cannot be fulfilled by a single
device but a set of devices working together. Hence, in
addition to the specification of static relationships
between concepts in ontology systems, we introduce
the application template [15] for the service
composition specification. This paper presents our
investigation of constructing an ontology-based
knowledge representation for the semantic content and
process model of smart home applications. In the
following section, we propose an ontology-based
architectural design of smart home. In Section 3, we
go through the details of handling different scenarios
that may occur in a smart home system. To illustrate
the automatic function plan generation base on
semantic information, a temperature control
application scenario is used as case study in Section 4.
Finally, the conclusion is made in Section 5.

Given the diversity of home environment,
appliances, and residents, the applications for smart
homes must be configurable and adaptive. Instead of
programming each household, we propose an
ontology-based framework to facilitate the automatic
composition of appropriate applications. The system
composes appropriate services depending upon the
available equipments in each individual household
automatically. Meanwhile, it dynamically adjusts the
environment parameters to match the customer needs
and to encompass the available resource. With its
supporting on customized function template editing,
customers are able to specify their usual behavior
templates as different mode.

1 Introduction
Smart homes have emerged as a focused application
area of distributed embedded systems where digital
appliances, sensor devices, PDAs, and handheld
computers are integrated to facilitate intelligent
services for households. Smart home is different than
the traditional home on its ability to perform functions
by integrating appropriate appliances automatically.
Besides the automation, it also considers the different
habits of the residences and adjusts the setting
accordingly. For instance, to older people, the home
temperature should not be too low. However, for a
party, the guests will get sweaty with such a temporary
level.
To realize the aforementioned intelligence in smart
homes, there are three issues need to be addressed:
• Dynamically compose service plans to fulfill the
requested functions base on available devices.
• Automatically supply and rank a list of functions
that can be supported by a set of available devices.
• Dynamically adjust the system according to the
environment and resident information.
This work was supported by the IT R&D program of
MKE/IITA, 2006-S-066-02, Development of High Reliable
Adaptive Middleware for u-Home.

978-0-7695-3678-1/09 $25.00 © 2009 IEEE
DOI 10.1109/ICESS.2009.60

295
297

profile. It also responses for the resource control and
monitoring. When exception happens, it is in charge of
switching and deploying another executable plan to
the system. Each part will be discussed in the
following sections.

2 Smart Home System Architecture
2.1 Ontology for Smart Homes
To alleviate the limitation of the keyword search,
ontology is introduced to support the semantic
discovery of devices in the SOC framework for smart
homes. Four domain ontology systems – devices,
function concepts, environment profile, and policy
ontology, represent the knowledge base in smart home
systems. Device ontology describes the concepts
related to devices. Function ontology, as the core of
the whole knowledge base, describes the concepts
related to functions and templates are attached for
each composable function. Policy/preference ontology
describes the basic system constrains and preference
rules about the use of devices and functions. Those
rules may include conditions refers to environment
ontology. Environment ontology describes the
classification of environment profile, which includes
natural environment profile and person profile.

Knowledge Base
Knowledge Base supports the semantic knowledge
specification and the reasoning.
Knowledge Base
Device
Ontology

Refer To

Refer
To

Function
Ontology

Refer
To

Policy/
preference
Ontology

Refer
To

Environment
Ontology

Refer To

D1
D2

Household Data Manager
Device
Registry

User Template
Registry

...
D2

Household
Profile

Ranked Local
Function Plan

Plan Deployment
Parameter
Setting

Resource
control

Composition &
Collaboration

Monitoring

Service Manager
V&V Checking

Availability
Checking

When the picked
Plan cannot pass
the collaboration

Policy
Checking

Service
Ranking

Composition &
Collaboration

Figure 2 The Smart Home Architecture
As shown in Figure 1, function ontology is the core
of the knowledge base. It is different from other
ontology systems on its support of function template
specification. There are two kinds of function concepts
in the function ontology: atomic and composed. Both
of them can be referred by other three ontology
systems. The composed function concept has at least
one function template associated to it. Besides
ontology specification, it supports an important
function: raw plan logic generation. When one
function request comes, the inference query will be
performed. All the atomic functions and composed
functions, which can fulfill the request wholly or
partially, are returned with ranking information.

Figure 1 Ontology Systems
The ontology systems are cross-referenced to each
other as shown in Figure 1. The knowledge base
covers the necessary semantic information needed by a
smart home system. Moreover, the application
template is introduced in function ontology
specification, by which the system is informed about
how to assemble sub-functions to an integrated service.
2.2 Smart Home System Architecture
The proposed architecture supports the construction
and execution of smart home applications based on the
knowledge description in an ontology system. It
includes four main components: Knowledge Base,
Household Data Manager, Service Manager, and Plan
Deployment as show in Figure 2.
Knowledge Base provides general knowledge
representation related to smart home functioning.
Household Data Manager is the data center of the
whole system. It will store information about a
specific house. Service Manager is responsible for all
the checking and constructing the executable plan
according to the information stored in Household
Database. Plan Deployment sets the final parameter
according to the resident profile and environment

Household Data Manager
Household Data Manager stores the detail
information of the devices, user templates, the resident
and environment profile, and ranked local function
plans (RLFP) in a specific house.
The manager hosts a device registry which
performs as a UDDI [16]. The household devices
registration and the device discovery operation are all
ontology based. Besides the basic device information,
the associating protocols are also saved to enable the
communication between devices. The other registry,
user template registry, allows user register their

298
296

system. To save unnecessary time on the function plan
updating, the system only generates specific function
plans when they are requested. Function plan is
generated base on device ontology, function ontology,
and current available devices information. The control
flow of initial function construction is shown in Figure
3. It involves three phases: knowledge query, plan
construction, and plan deployment.
System enters into knowledge query phase when a
new function request comes in. It includes two steps.
First, all related function concepts are extracted from
function ontology with weight by inference query. For
the composed function, the corresponding plan logic
will also be included in the query results. Then, all
related device concepts are extracted from device
ontology base on the query result from the first step.
The elements in the plan logic in this stage are still
concepts on the knowledge level.

preferred functions and environment parameter
combinations.
Except the device information, household profile
keeps the other local information. It includes three
main aspects: environment profile, resident profile,
and policy. Environment profile records both static
and dynamic information. The dynamic information
can be obtained periodically via the web services, such
as daily weather. Resident profile is more static. It
includes the basic information and the self preference
of the residents. Policy has two parts: the static policy
and preference constrains, which is generated by
system automatically based on the knowledge base
and the local profile information. The RLFP are saved
locally to speed up the recomposition process.
Service Manager
Service manager mainly supports three functions:
verification and validation (V&V) checking, plan
ranking, and the composition and collaboration. V&V
checking performs availability checking and policy
checking. Service ranking is performed base on the
preference rules. The highest ranked function plan will
be selected and the details of protocol matching will
make sure that the plan is actually executable.
Plan Deployment
Plan deployment handles the remaining issues after
an executable function plan is generated. It includes
parameter setting, resource control, monitoring, and
recomposition and collaboration. After the parameter
setting and resource control phases, the selected
function plan will become executable and can be
performed in the smart home. The plan execution
status is monitored. If the current plan is no longer
feasible for the system due to device or network
failures, a new plan will be applied to continue
fulfilling the function. Since the generated plan is
stored, the system does not need to go through the plan
generation path again.

Figure 3 Initial Function Construction
After collecting needed knowledge from knowledge
base, system goes into plan construction phase, where
the result function concepts are all replaced by real
devices and various checking is performed. For
example, availability checking will filter out
unfeasible function or function plan, and policy
checking will make sure new function plan will not
cause any system confliction. Meanwhile, all the rest
function plans will be ranked base on the weight
generated from the first phase and also the reliability
and other attributes. Among all function plan
candidates, system will pick the best one to deploy and
save the whole list into RLFP database. Then, system
enters into the last phase: plan deployment.
In plan deployment phase, the collaboration details,
such as communication protocol and signal format, are
determined and established to make sure the selected
plan is executable. If any problem is found, the system
will go through the collaboration part again until an
executable plan is confirmed. After parameter setting

3 Service Composition
When system is running, it will meet three kinds of
situations: a function request comes from user, a
running device becomes unavailable, and a new device
is added into the system or an unused device is
removed. For the first two situations, system has to
initial/adjust a running function plan, while for the
third situation, RLFP database has to be maintained.
The detailed methodologies are discussed in the
following subsections respectively.
3.1 Initial Function Construction
The available devices in a house are not fixed.
Devices will be removed/added from time to time.
These changes may affect the function support of a

299
297

and resource control, the system will deploy the
function plan finally and monitor it.

3.3 Device Registration and RLFP Maintenance
The system available devices pool changes from
time to time. Some existing ones may be removed
while new devices will be added. Device upgrading
can be treated as device removing and adding. Not all
of them will affect stored RLFP. For example, if the
removed device does not have dependency with any
requested function, no matter the function is running
or pending, system will simply remove its registration
information. More details will be discussed in the
following.
Removing device
When a device is removed from system for any
reason, system has to response accordingly to what
kind of device it is. With the dependency relations
between the function template and device ontology,
the system can trace from the device concept, under
which the removed device registered, back to the
affected function concepts, then the RLFP. If the
device is not related with currently requested function,
its registration will be simply removed. If the device is
currently running, system has to perform
recomposition as shown in section 3.2. If it is not used
currently, however in the candidate function plan, the
related function plan will be removed from RLFP.
Adding device

3.2 System Recomposition
When a running device fails, instead of going
through initial function construction process again,
system recomposition process will be invoked to
achieve constantly function supply. System
recomposition has the same plan deployment phase as
in initial function construction. Because of pre-stored
reusable function plans, the knowledge query and plan
construction phases are eliminated, which accelerates
system response time.
Figure 4 shows the work flow of the recomposition
process. Instead of generate function plan list again,
system will pick function from RLFP database to
replace the broken one. Because there might be more
than one device are broken, or the broken device is
applied to fulfill more than one functions at the same
time, system will first run dependency checking to get
affected function list. Then, it will keep getting the
next candidate function plan until it can enable the
collaboration among the devices in the plan. If there is
more than one function to be re-composited, system
will perform confliction checking among the plan set,
such as power and bandwidth limitation, or if the an
exclusive device is used in two function plans. Once a
function plan is selected, the deployment phase is
carried out same as in initial function construction
scenario.

When a new device is added into system, it has three
different cases. Its processing logic is shown in Figure
5. Same as in device removing, the dependency
checking will be performed first.
If the device concept is not related with any current
requested function, system will simply keep the
registration information for future use.
If the device concept is related with currently
requested function, no matter if the function has a
feasible plan already running or not. The system has to
update the RLFP database. System will perform
dependency checking and find out if there is any
function that is affected by this change. If there is any,
system will further check if the affected function is in
pending status.
If any new function plan can be composed because
the new device being added, the system will go
through the plan construction phase as in section 3.1.
The generate function plan will be stored in the RLFP
database when the function request is fulfilled by the
current system already. Otherwise, user will be
notified that the function is ready. If user chooses to
apply it, system will deploy the new generated
function plan as in plan deployment phase in initial
function construction.

Figure 4 System Recompostion

300
298

New Device
Dependency Checking

Ranked
Function
Plan
No

4 Case Study

Related with Current
Running Function?

In this section, a “temperature control” example is
used to show how the proposed smart home system
generates function plan automatically and performing
service recomposition.

Plan Logic

Related with
Pending Function?

Yes

Yes

Generate Function Plan

Perform “Initial
Function Construction”

attribute must satisfy the constraints in the ontology
system. Any propagated changes need to be checked
for the system consistency.

Function
Ontology

Availability Checking

Notify User: Function is
availalbe

Policy Checking
Apply?

Device
Ontology
Device
Manager

4.1 Device and Function Ontology
The example device ontology of a sample smart
home is illustrated in Figure 6. Devices are
categorized into two classes: small power and larger
power. The categorization is base on the technical
parameters of the devices.

Policy/
preference

Service Ranking

Yes
Plan Deployment

Store Ranked Plan
End

Figure 5 New Device Adding
3.4 Ontology System Maintenance
Normally, the changes over ontology specification
will not affect the running function plan. However,
ontology maintenance may need to be performed for
the future use. There are three basic changes over
ontology system: adding a new item, deleting an
existing item, and modifying an existing item.
Adding any new information requires significant
update and checking. An item added must satisfy the
constraints in the ontology system. If a new relation is
added onto existing classes, the original and derived
relations needs to be examined to make sure there is
no violation among them. If a new item is added under
a given concept, all the relations associated with the
concept must be verified with the new item. In
addition, the consistency checking needs to be
performed over updated relations.
Deleting an existing item from the ontology in
general is difficult and should be discouraged because
the ontology systems are cross-referenced. Deleting an
item will not only affect the current ontology system
but the changes may propagate to the other referenced
ontology systems. Also, if a device concept is
removed, any device that references to this item will
realize it has referenced to a null item and will never
be visible to the system.
There are two kinds of modifications. One is
moving existing item. This kind of modifying is
difficult because moving is essentially removing an
item or a relation from one place, while adding the
same item in another place. Thus, all the complexities
of both adding and deleting will be there. The other is
modifying the attributes of the item. The modified

Figure 6 Device Ontology
Figure 7 shows the function ontology of the smart
home system. Functions that a smart home can supply
are divided into five classes: access control, security,
appliance control, information supply, and sensor. The
composed function “Temperature Control” is the
subclass of “Appliance Control”. It is composed by
two functions: heating and cooling. The function
template shown in Figure 8 describes how the
temperature control is composed using heating and
cooling services.
4.2 Policy Ontology
Figure 9 shows a simple policy classification.
Under each class, there will be several policy
templates. For example, every house has some
constraints about power consumption. It can be a
specific maximal AC current or expressed as “if two
devices are running in parallel, at most two of them
can be large-power devices according to device
ontology definition”. An example formal expression is
as following.
Status(A, “running”) ∩
∩ Status(C, “running”) =>

¬ (LargePower(A)
LargePower(C))

301
299

∩

Status(B,

“running”)

LargePower(B)

∩

Figure 7 Function Ontology
The other example of policy specification related
with temperature control is expressed in the following,
which states that “when the temperature control
function is running, the window needs to be closed”,
functionStatus(temperatureControl,
Status(window, “closed”)

“running”)

=>

The function can be attached to the policy
enforcement mechanism. For instance, the function
WindowControl.closeAll is attached to the 2nd policy.
To be compliant with this policy, the corresponding
function, i.e., WindowControl.closeAll, will be
performed.

Figure 8 Function Template Sample

4.3 Environment Profile Ontology
Environment profile includes two kinds of profiles:
Natural environment profile and person profile.
Natural environment indicates the characters of the
weather in different area. Person profile indicates the
age range and some other issues related with people
and will affect the preference of smart home parameter
setting. The example profile ontology is shown in
Figure 10.
Figure 9 Policy Ontology

Figure 10 Environment Profile Ontology

302
300

4.4 Composed Function Plan
In the device ontology specification, the concepts
defined in function ontology are referred. When the
device is registered into system, since it is based on
the device ontology, the device is registered with its
function information implicitly. As shown in Figure 11,
five devices are registered into system with functions
related with “temperatureControl” directly or
indirectly. They will be used when the
“temperatureControl” function is composed.
The system composed four different function plans
for “temperatureControl” according to the template
shown in based on the available devices. As an
example, one of them is depicted in Figure 12.

Once the ontology is specified, it refers to several
parameter tables. For example, for four seasons
specified in Table 1, the best indoor temperatures are
set differently to provide comfort indoor atmosphere.
Table 1 Temperature parameter setting 1
LTemp
HTemp

Spring
16
28

Summer
17
25

Autumn
16
28

Winter
16
25

The temperature will be further adjusted according
to person profile as shown in Table 2.
Table 2 Temperature parameter setting 2
LTemp
HTemp

Baby
LTemp+1
HTemp-1

Child/Adult
LTemp
HTemp

Senior
LTemp+1
HTemp+1

Figure 11 References between function ontology, device ontology, and device register
fulfill the required “temperatureControl” function.
When the deployed function cannot be satisfied,
system proposes to invoke the most related function
“cooling”, base on the function ontology definition,
such that it may be able to partially fulfill the
requested function.

5 Conclusion
Given the diversity of home environment,
appliances, and residents, the applications for smart
homes must be configurable and adaptive. In this
paper, we proposed a system architectural design for
smart home ontology and service composition. The
proposed system consists of a smart home knowledgebase (ontology), a household database, a service
manager, and a plan deployment component. A generic
knowledge representation is used to facilitate the
composition of appropriate applications based on user
profile and available appliances.
There are further optimizations and functionalities
can be addressed in the future. In addition, automated

Figure 12 A sample plan generated automatically
The running log of the system is shown in Figure 13.
The system automatically adjusts the running plan to

303
301

ontology and the process composition are parts of the
smart home systems.

code generation for a selected execution platform will
be the immediate tasks. This will allow us to test any
integration issues that could be raised when the

Figure 13 System running log

Reference
[1] N. F. Noy, M. Sintek, S. Decker, M. Crubezy, R.
W. Fergerson, and M. A. Musen. “Creating
Semantic Web Contents with Protege-2000”,
IEEE Intelligent Systems 16(2):60-71, 2001.
[2] M. Paolucci, T. Kawamura, T. R. Payne, and K.
P. Sycara. “Semantic Matching of Web Services
Capabilities”, 1st International Semantic Web
Conference, 2002.
[3] R. Chinnici, J. J. Moreau, A. Ryman, and S.
Weerawarana. “Web Services Description
Language (WSDL) Version 2.0. June 27, 2007.
http://www.w3.org/TR/wsdl20/.
[4] D. Fensel. “Ontologies: Silver Bullet for
Knowledge
Management
and
Electronic
Commerce”, Springer-Verlag, Berlin, 2001.
[5] “What
is
an
Ontology,”
http://wwwksl.stanford.edu/kst/what-is-an-ontology.html.
[6] A. Bernaras, I. Laresgoiti, and J. Corera.
“Building and Reusing Ontologies for Electrical
Network Applications”, ECAI96, 12th European
conference on Artificial Intelligence Ed., John
Wiley & Sons Ltd., pp. 298-302
[7] M. P. Papazoglou and G. Georgakapoulos.
“Service-Oriented Computing”, CACM, October
2003, 46(10).
[8] W. T. Tsai, "Service-Oriented System
Engineering: A New Paradigm", IEEE
International Workshop on Service-Oriented
System Engineering (SOSE), October 2005, pp.
3 - 8.
[9] “OWL-S: Semantic Markup for Web Services”,
http://www.w3.org/Submission/OWL-S, Nov 22,
2004.
[10] “OWL Web Ontology Language Reference, Feb
10, 2004 ”, http://www.w3.org/TR/owl-ref/
[11] T. Gu, X. H. Wang, H. K. Pung, D. Q. Zhang.

[12]

[13]

[14]

[15]

[16]

304
302

"An Ontology-based Context Model in
Intelligent Environments", In Proceedings of
Communication Networks and Distributed
Systems Modeling and Simulation Conference,
San Diego, California, January 2004.
E. Kim and J. Choi. “An Ontology-Based Context
Model in a Smart Home”, Workshop on
Ubiquitous Web Systems and Intelligence
(UWSI 2006), pp. 11-20.
F. Latfi, B. Lefebvre1 and C. Descheneaux,
“Ontology-Based Management of the Telehealth
Smart Home, Dedicated to Elderly in Loss of
Cognitive Autonomy”, Third International
Workshop,
OWLED
2007,
OWL: Experiences and Directions.
E. Meshkova, J. Riihijarvi, P. Mahonen, and C.
Kavadias. “Modeling the home environment
using ontology with applications in software
configuration
management,”
International
Conference on Telecommunications, 2008. ICT
2008, pp. 1-6.
W. T. Tsai, B. Xiao, Q. Huang, Y. Chen, and R.
Paul, "SOA Collaboration Modeling, Analysis,
and Simulation in PSML-C", Proc. of the Second
IEEE International Symposium on ServiceOriented
Applications,
Integration
and
Collaboration (SOAIC’06), October 2006.
UDDI
Technical
Committee.
Universal
Description, Discovery and Integration (UDDI)
http://www.oasis-open.org/committees/uddi-spec.

1586

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 52, NO. 12,

DECEMBER 2003

Voltage-Clock-Scaling Adaptive Scheduling
Techniques for Low Power in
Hard Real-Time Systems
C.M. Krishna, Senior Member, IEEE, and Yann-Hang Lee, Member, IEEE
Abstract—Many embedded systems operate under severe power and energy constraints. Voltage clock scaling is one mechanism by
which energy consumption may be reduced: It is based on the fact that power consumption is a quadratic function of the voltage, while
the speed is a linear function. In this paper, we show how voltage scaling can be scheduled to reduce energy usage while still meeting
real-time deadlines.
Index Terms—Power-aware computing, real-time systems, dynamic voltage scaling.

æ
1

INTRODUCTION

M

ANY applications impose severe power and/or energy
constraints on embedded systems. Examples include
battery-powered devices and spacecraft relying on solar or
nuclear power.
Voltage control is a powerful mechanism for reducing

energy consumption. In CMOS devices, the power consumption is proportional to the square of the voltage:
Pcmos ¼ CL Nsw V2DD f;

ð1Þ

where
.
.
.

Let powerðvÞ denote the ratio of the power consumed by the
processor at voltage v to that at vh . Then, from (1), we have
powerðvÞ / ðv=vh Þ2  ð1=slowðvÞÞ:

CL is the circuit output load capacitance.
Nsw is the number of switches per clock cycle.
f is the clock frequency.

To do a unit of work (i.e., the work that would keep the
processor busy for one unit of time at voltage vh ), we
therefore need the following energy at voltage v:

The circuit delay obeys the following equation [12]:
CL VDD
;
¼
KðVDD  VT Þ

an obvious trade off between the power consumed and the
speed of the circuit.1
Define slowðvÞ as the factor by which the processor is
slower at supply voltage v than it is at vh , the maximum
allowed voltage. Then, from (2), we have


v vh  VT 
slowðvÞ ¼
:
vh v  V T

ð2Þ

where

EnergyðvÞ / powerðvÞ  slowðvÞ
 2
v
:
¼
vh

ð3Þ

. C.M. Krishna is with the Electrical and Computer Engineering Department, University of Massachusetts, Amherst, MA 01003.
E-mail: krishna@ecs.umass.edu.
. Y.-H. Lee is with the Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287. E-mail: yhlee@asu.edu.

The idea of exploiting the trade off between speed and
energy consumption has attracted increasing attention since
the early papers in this field were published (e.g., [4], [28]).
Benini and De Micheli present an excellent survey of
system-level power optimization methods, including but
not restricted to, voltage variations [2]. Lin et al. cast the
problem into the integer programming model and also
present heuristics when dealing with timing and resource
constraints [17]. In [5], a circuit may choose from among
multiple voltage levels to reduce power consumption while
satisfying latency constraints. In [31], the Dhrystone 1.1
benchmarks were run on an ARM7D processor at two
voltage-frequency combinations: (5.0V, 33 MHz) and (3.3V,
20 MHz) yielding 185 MIPS/watt and 579 MIPS/watt,
respectively. Yao et al. derived a static voltage-control

Manuscript received 24 Aug. 2000; revised 26 Oct. 2001; accepted 14 Apr.
2003.
For information on obtaining reprints of this article, please send e-mail to:
tc@computer.org, and reference IEEECS Log Number 112768.

1. Another approach would be to reduce the threshold voltage, VT , but
doing so tends to increase the leakage power. We focus on handling VDD in
this paper.

.
.
.

K is a constant depending on the process and gate
size
VT is the threshold voltage.
 varies between 1 and 2. In this paper, we assume
 ¼ 1:3 for all our numerical experiments.

Since the clock frequency is proportional to the inverse of
the circuit delay, from the relations in (1) and (2), we have

0018-9340/03/$17.00 ß 2003 IEEE

Published by the IEEE Computer Society

KRISHNA AND LEE: VOLTAGE-CLOCK-SCALING ADAPTIVE SCHEDULING TECHNIQUES FOR LOW POWER IN HARD REAL-TIME SYSTEMS

heuristic to reduce energy consumption, assuming that the
power usage is a convex function of the clock rate [30].
Ishihara and Yasuura formulated the voltage-scheduling
problem as an instance of integer programming [11]. They
point out that two voltage levels are usually sufficient: A
larger number of available levels does not contribute much
so long as the two levels are carefully chosen. Nonpreemptive scheduling is considered in [8] in a system where the
time taken to change voltages is considered to be negligible.
Voltage control in a problem involving scheduling sporadic
tasks in the midst of an ambient periodic workload and the
processor and cache design are considered in [10]. The
authors explicitly allow for voltage transition times; they
point out that voltage transitions are fast: of the order of 10
to 100 sec per volt can be achieved. Elsewhere, Burd and
Broderson discuss the design of a variable-voltage processor which can switch voltages at the rate of 24 sec per volt
[3]. Ma and Shin explain how to do energy-adaptive
scheduling in mobile applications using their Emerald
operating system [20]. The focus here is not on voltage
scheduling; rather, it is on picking tasks to be executed
based on the energy consumption and criticality. In a
similar vein, Qu and Potkonjak present a partitioning and
linear approximation and dynamic programming heuristics
for maximizing Quality of Service (QoS) in the face of
energy limitations [25]. Shin and Choi slow the processor to
avoid idling it if the current workload would otherwise be
guaranteed to finish before the next job arrival [26]. A study
of periodic real-time tasks which can consume energy at
possibly varying rates is presented in [1]. Unsal et al.
consider ways in which files can be replicated to reduce the
energy required to reference files in remote locations [29].
A benchmark suite and simulation environment for
voltage scaling are presented in [24]. In [16], we introduced
dynamic voltage-scheduling algorithms for fixed-priority
task systems. Other voltage-scheduling algorithms can be
found in [23], [14].
In this paper, we focus on hard real-time dynamic-priority
task systems, where meeting critical task deadlines is of
paramount importance [15]. (One example where this is the
case, and energy is constrained, is spacecraft.) We show
how to schedule voltage settings so that energy consumption is reduced, while still guaranteeing that all task
deadlines are met.
We show how one can build a voltage-scheduling
algorithm based on the Earliest Deadline First (EDF)
scheduling algorithm. A key feature of this process is
dynamically reclaiming resources released by tasks which
finish ahead of their Worst-Case Execution Times (WCET).
When tasks are released, there is no information about
exactly how much time they will consume: They are
therefore executed under the assumption that they will
consume their WCETs. This is necessary since, in hard-realtime applications, missing deadlines can have disastrous
consequences. However, the actual execution time is often a
very sensitive function of the task input and it is common to
have such tasks finish well before consuming their WCETs.
For some experimental studies on this, see, for example, [7].
When tasks finish ahead of schedule, the reduced loading
on the computer can be exploited by running the processor

1587

at a lower voltage, thereby saving energy while still meeting
all task deadlines.
The paper is organized as follows: In Section 2, we
outline the system model that is assumed. We then present,
in the following sections, specific algorithms for two cases:
when all the tasks run within the same periodic frame and
when the tasks have arbitrary periods. The paper concludes
with a brief discussion in Section 5.

2

SYSTEM MODEL

Most real-time systems in critical embedded applications
use periodic workloads. That is, each task, Ti , has a period,
Pi , and an iteration of Ti is released each Pi time units. The
deadline of a task is equal to the period. That is, a task
iteration must be done by the time the next iteration of that
task is released. The worst-case execution time of each task
is assumed to be known.
There is a huge literature on the problem of allocation
and scheduling of tasks in real-time systems; for a survey,
see [15], [18]. The typical approach is to carry out an
allocation of tasks to processors and then to run a
uniprocessor scheduling algorithm on each of the processors to decide when each task will execute. In this paper, we
focus on the problem of uniprocessor scheduling.
Our assumptions are as follows. A1 to A4 are analogous
to typical assumptions made in real-time scheduling theory:
A1 Voltage switching consumes negligible overhead. This is
analogous to the assumption, made in classical real-time
scheduling theory, that preemption costs are negligible.
Voltage-switching typically takes just a fraction of a
millisecond [3], while the task execution times in realtime systems are generally significantly higher: typical
times are on the order of milliseconds or even greater (for
example, see the real-time task sets used in [26]).
Furthermore, in the approaches considered here, each
task can trigger at most two voltage switches during its
execution. One can therefore incorporate the worst-case
switching overhead into the worst-case execution time of
each task.
A2 Tasks are independent: No task depends on the output
of any other task.
A3 The worst-case execution time, ww ðiÞ, of each task Ti is
known. This can be found by either static analysis of the
code [22] or through profiling. The actual execution time
is not known, however, and may vary from one iteration
to the next: It is a random variable. The distribution of
this random variable can be empirically determined by
profiling. It is only when a task completes that its actual
execution time is known. We therefore do not require the
existence of any special compiler-inserted annotations
that announce the progress of the task during its
execution (as is done, for example, in [21]).
A4 The overhead of the scheduling algorithm is negligible
when compared to the execution time of the application
workload.
A5 Tasks must be run to completion if any benefit is to be
accrued from them: They cannot be terminated prematurely and still provide useful output. (Examples of the

1588

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 52, NO. 12,

DECEMBER 2003

latter type are iterative tasks used in signal processing
[27] or other applications).
A6 Tasks are periodic, with the deadline being the period.
The period of task Ti is denoted by Pi . The task phasings
are known in advance [6].
We need consistent ways to specify the unfinished work. At any
time t, the unfinished work is the total time it would take the
processor, running at its highest speed, to execute all the work
that has arrived in the processor up to time t and is still
unfinished. The relative slowdown of a processor at voltage v
is denoted by slowðvÞ (1  slowðvÞ < 1). Thus, five seconds of
workload would take 5 slowðvÞ seconds at voltage v. The
power consumption at voltage v is given by pðvÞ.
In the following two sections, we present our voltagescheduling algorithms. We start by considering the case of
scheduling tasks in a minor cycle under a cyclic algorithm.
The second algorithm focuses on EDF algorithms in a
situation where the task periods may be different from one
another.

3

CYCLIC SCHEDULING

For each periodic frame, all tasks are released at the
beginning of the frame and must finish by the end of the
current frame. We assume that the tasks have a predefined
execution order. Key to the algorithm is a function, ðtÞ.
ðtÞ has the property that if the amount of unfinished work
at time t is no greater than ðtÞ, there is enough processing
capacity to meet all deadlines under the scheduling
algorithm that is being used. Note that ðtÞ is a sufficient
condition for schedulability.

3.1 Algorithm Description
We now describe the algorithm informally. It consists of
two phases. In the offline (or preprocessing) phase, socalled because it is executed before the system is actually
used, we obtain the function ðtÞ over the duration of the
frame. In the online phase, when the system is operating,
we set the voltage level to guarantee that the unfinished
work will never exceed ðtÞ at time t.
3.1.1 Two Voltage Levels
Consider the case where two voltage levels, vh and v‘ are
allowed, with vh > v‘ . The greedy heuristic we follow does
the following: It always uses v‘ when the unfinished work is
guaranteed to be below ðtÞ and vh otherwise.
If the actual workload were to be known in advance,
then we could calculate ðtÞ with the actual, instead of the
worst-case, workload and the above heuristic can be easily
shown to be optimal (i.e., it minimizes the total energy
consumption under the constraint that all tasks finish before
the frame ends). Since we don’t know the actual workload
and it is imperative that no deadlines be missed, we must
use the worst-case workload.
Implementing this algorithm is easy: When a task starts
executing (or restarts following a preemption), check the
worst-case unfinished work and compare this with ðtÞ. If
at ðtÞ, run at vh until the task is either completed or is
preempted by another task. If below ðtÞ, determine the
next time  when the worst-case unfinished work will equal

Fig. 1. ðtÞ and ðtÞ for the two-voltage algorithm.

ðÞ. Execute at v‘ until either  is reached or the task has
completed or been preempted.
Example. Let the period of the frame be 10 units and the
worst-case execution times of the four tasks be as follows
for tasks T1 ; T2 ; T3 ; T4 , respectively:
ww ð1Þ ¼ 2; ww ð2Þ ¼ 3; ww ð3Þ ¼ 2; ww ð4Þ ¼ 1:
Let the actual execution times for some iteration instance
be wa ð1Þ ¼ 1; wa ð2Þ ¼ 2; wa ð3Þ ¼ 2; wa ð4Þ ¼ 1. (Of course,
these actual execution times are unknown to the system
at the beginning of that iteration and only become
apparent as the tasks complete). Let slowðv‘ Þ ¼ 2.
The worst-case utilization is
uw ¼

4
X

ww ðiÞ=10 ¼ 0:8:

i¼1

We therefore have:

8
if 0  t  2
ðtÞ ¼
10  t if 2  t  10:
The processor will begin executing at voltage v‘ at
time 0. If it continued indefinitely at v‘ , its worst-case
unfinished work, ðtÞ, would intersect ðtÞ at t ¼ 4: So, it
can continue at v‘ up to the minimum of the finishing
time of T1 and t ¼ 4 before reassessing. At time 2 (i.e.,
wa ð1Þ  slowdownðv‘ Þ), it finishes task T1 . The worst-case
remaining work, ðtÞ is now 6. T2 therefore can start at
low voltage. Executing at v‘ , the system is guaranteed
not to cross ðtÞ until t ¼ 6. By coincidence, T2 finishes at
t ¼ 6. Now, the worst-case remaining work is 3, while
ð6Þ ¼ 4. We can therefore run at v‘ until at least the
earlier of the times when T3 is completed or the worstcase unfinished work crosses ðtÞ. The latter time is
equal to 8. At time 8, T3 is as yet unfinished and the
processor switches to vh until the completion of T3 at
time 9. When it takes up the execution of T4 at time 9, it
has no choice but to run at vh .
Fig. 1 shows the functions ðtÞ and ðtÞ.
Hence, the voltage schedule of the processor is: v‘
over the interval ½0; 8 and vh over ð8; 10.

3.1.2 Infinite Number of Voltage Levels
Suppose instead that we have an infinite number of possible
voltage levels possible, in the range ½v‘ ; vh . The energy
required to do one unit of work (shown in (3)) is a convex

KRISHNA AND LEE: VOLTAGE-CLOCK-SCALING ADAPTIVE SCHEDULING TECHNIQUES FOR LOW POWER IN HARD REAL-TIME SYSTEMS

TABLE 1
Default Parameter Values

function of voltage. Denote by P the duration of the frame.
If the actual total workload, wa , were known in advance, it
would be optimal to find vinf such that slowðvinf Þ ¼ P=wa
and to set the voltage v ¼ maxfv‘ ; vinf g.
Again, since we do not know the actual workload in
advance, we have to work with the worst-case workload,
updating our information as tasks get completed.
The heuristic for this case is the following: When a task
starts executing (or resumes after being preempted) at
time t, determine the worst-case remaining work ww . Find
vinf such that slowðvinf Þ ¼ ðP  tÞ=ww and set the voltage
v ¼ maxfv‘ ; vinf g.
Example. Consider the same task set as in the
previous subsection.
ðtÞ remains unchanged. To
begin with, we have worst-case unfinished work,
ðtÞ ¼ 8 so that we pick a voltage v1 ¼ maxfv1 ; v‘ g
such that slowðv1 Þ ¼ 10=8 ¼ 1:25. At time 1:25, T1
finishes. Once this happens, ð1:25þ Þ ¼ 6 and the
remaining time is 10  1:25 ¼ 8:75. The voltage to
use now is v2 ¼ maxfv2 ; v‘ g such that
slowðv2 Þ ¼ 8:75=6 ¼ 1:46:
T2 completes at 1:25 þ 1:46  2 ¼ 4:17. Upon this completion, the worst-case unfinished work is ð4:17þ Þ ¼ 3.
Since the remaining time is 10  4:17 ¼ 5:83, pick a
voltage v3 ¼ maxfv3 ; v‘ g such that
slowðv3 Þ ¼ 5:83=3 ¼ 1:94:
When T3 completes, do the same for T4 .
Implementing multiple levels of voltage is obviously
more expensive than implementing just two levels. In the
following subsection, we provide numerical results that
indicate the extent to which having an infinitely variable
voltage improves the energy consumption. Users must
judge for themselves whether the improvements are worth
the expense.

Fig. 2. Impact of worst-case task utilization.

1589

3.2 Numerical Results
Our results assume a period of 10 time units. The time unit is
defined as the duration required to execute one unit of work
at high voltage, vh . Default values for the other parameters are
shown in Table 1: Unless otherwise stated, the parameters
have these values. These values are typical of modern
processors. Throughout, we ignore the energy spent while
the processor is idling. Since this energy is directly related to
the duration for which the processor is idle, it will be greater
in the baseline case of always running the processor at high
voltage: As a result, the calculated energy consumption of our
voltage-scaling heuristic is a smaller percentage of the baseline
than is shown in the figures.
We begin by considering the impact of worst-case
utilization for a task set of 15 tasks in Fig. 2. If ww ðiÞ is the
worst-case execution time of task Ti , the actual task
execution times vary uniformly in the range
½a  ww ðiÞ; ww ðiÞ, 0 < a  1. A high value of a indicates
that the task execution times are quite deterministic.
For low values of worst-case task set utilization, the
system runs at the lowest allowed energy level. In this case,
this is at about 44 percent of the high-voltage energy level:
Thus, for these parameter values, regardless of the deadline,
the maximum energy savings possible is about 56 percent.
As the utilization increases, the scope for energy savings
declines monotonically as the need to meet the deadline
forces the system to run at a higher voltage.
Fig. 3a indicates the performance of the heuristic when
compared to an oracle that knows in advance the actual
execution times of each iteration of all the tasks. Every time
that a task completes provides an opportunity for the
algorithm to update its information about the actual
execution times. If the task granularity is small, i.e., the
same total worst-case task-set utilization is composed of
many tasks, then such an update will happen more often
and the heuristic energy consumption will run close to the
oracle value. If, on the other hand, there is a very small
number of large tasks, then such updates will occur but
seldom and the performance will degrade compared to the
oracle value.
In Fig. 3b, we study the impact of the variability of task
execution times on the energy consumption. As a increases
from 0 to 1, the variability decreases and the energy
consumed by the heuristic quickly approaches that of the
oracle.

1590

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 52, NO. 12,

DECEMBER 2003

Fig. 3. Comparison to an all-knowing oracle. (a) Impact of number of tasks. (b) Impact of a.

In Fig. 4, we consider the ratio of the energy consumed by a
two-level oracle to that of the infinite-level oracle (a ¼ 1). The
latter may choose any voltage in the range ½v‘ ; 1:65 volts. The
curves in Fig. 4 start with a straight line, representing the
region in which the entire workload can be run at v‘ and still
finish at less than the available period. Following this, the
energy consumption drops closer to the infinite voltage case,
reaching a minimum when the workload, when run at v‘ , just
fits in the period. Beyond this point, the two-level algorithm is
forced to run at least partly at vh . By contrast, an algorithm
with an infinite number of voltage levels would be able to use
a voltage intermediate between v‘ and vh , thus consuming
less energy.
In Fig. 5, we compare the performance of the system with
an infinite number of voltage levels with that of its twolevel counterpart. If a is close to 1 (i.e., a task’s actual
execution times do not vary much from one execution to
another), then having an infinite number of levels markedly
reduces energy consumption over a certain range of worstcase utilization. However, if a is small (meaning that a
task’s execution times vary considerably), there is a range of
worst-case utilizations over which having an infinite
number of voltage levels actually consumes more energy
than having just two levels. The reason is that the heuristic
chooses the voltage level based on the worst case and, when
a is small, the system with an infinite number of voltage
levels tends to pick a higher operating voltage than turns
out, in hindsight, to have been strictly necessary.

Fig. 4. Infinite voltage levels versus two.

4

TASKS

WITH

ARBITRARY PERIODS

In this section, we consider sets of periodic tasks. Unlike in
Section 3, these tasks may have arbitrary periods.
The algorithm has two phases: offline and online. The job
of the offline phase is to generate a curve, ðtÞ, of
unfinished work as a function of time, with the property
that if the maximum unfinished work at that time is no
greater than the curve, it is possible for the processor to
meet all its deadlines (even if every task runs to its WCET).
The online algorithm sets the voltage levels in such a way
that the worst-case unfinished work remains below this
curve, when taking into account resources released by tasks
which finish execution before consuming their WCET.
The offline phase proceeds as follows: Let u be the worstcase utilization of the processor at high voltage (i.e., the
utilization if every task ran to its WCET and was run at high
voltage). Construct, for each task Ti , a corresponding task
T0i with the same period as Ti , but whose execution time is
deterministic and is equal to 1=u times the WCET of Ti .
The set of tasks, T0 ¼ fT01 ; T02 ;    ; T0n g, has a utilization of
1 by construction. Now, use the EDF scheduling algorithm
to schedule it. ðtÞ is the unfinished work, according to this
schedule, of tasks currently awaiting service. Define by
offline taskðtÞ the task (if any) running at time t under the
offline schedule.
The online phase is equally easy to describe. EDF is used
here as well. Let online taskðtÞ denote the task that is
running under the online schedule at time t. At any time, t,

Fig. 5. Impact of worst-case task utilization: infinite levels case.

KRISHNA AND LEE: VOLTAGE-CLOCK-SCALING ADAPTIVE SCHEDULING TECHNIQUES FOR LOW POWER IN HARD REAL-TIME SYSTEMS

1591

Fig. 7. Impact of runtime variance.
Fig. 6. Impact of v‘ .

the processor will be running at low voltage except when all
of the following conditions are satisfied:
offline taskðtÞ ¼ online taskðtÞ.
The worst-case unfinished work under the online
algorithm is equal to ðtÞ.
If both these conditions are satisfied, the processor runs at
high voltage.
The proof that no deadlines are missed by the online
algorithm is quite straightforward and the details are left to
the reader. It is based on the fact that task arrivals occur at
exactly the same time for the online and offline schedules
and have the same deadlines (and, hence, the same relative
priority) and that the task execution time in the online case
is no greater than the corresponding execution time in the
offline case.
.
.

4.1 Numerical Results
The experimental setup for these runs is as follows: All
execution times are specified in terms of the high-voltage
setting. The task periods are chosen randomly to be integers
between 2 and 8. Except where otherwise stated, we used
the parameter values in Table 1.

Fig. 8. Impact of number of tasks.

The parameters of importance are the worst-case processor utilization, uw , the average processor utilization, uav , the
number of tasks, ntasks , and the value of a. Three of these
parameters are related through the equation,
ua ¼ uw ð1 þ aÞ=2. Energy consumption is expressed as a
percentage of the consumption if the processor was run at
high voltage when executing the workload.
Consider first the impact of v‘ and the average actual
utilzation, uav , shown in Fig. 6. For very low values of uav ,
the lower values of v‘ do better: At such low utilizations,
almost the entire workload can be executed at v‘ . For each
value of v‘ , there is some given utilization beyond which the
work has to be executed at least partially at high voltage.
The greater the value of v‘ , the greater this threshold point
happens to be. As uav increases, the energy savings fall
monotonically since an increasing fraction of the workload
is executed at high voltage.
The data in Fig. 6 are compared against an asymptotic
lower-bound calculation of the energy consumption, assuming an infinite number of voltage levels and deadlines
set at 1. This calculation obtains a non-real-time asymptote
and is carried out as follows: Assume that the actual
utilization is known in advance and that the only requirement is that the processor is fast enough that the average
processor utilization requirement is no greater than 1 over
the long term. That is, we do not consider the deadline

1592

IEEE TRANSACTIONS ON COMPUTERS,

requirements of individual tasks: All that we ask from the
lower-bound calculation is that, asymptotically, the processor should run fast enough that the average incoming
workload rate equals the average workload finishing rate.
For each utilization, we will then get a voltage that is just
enough to fully utilize the processor and the energy
consumption associated with this voltage level is computed.
It is clear that, due to deadline considerations, a real-time
system with an infinite number of voltage levels can be
expected to perform worse than this asymptote, especially
when task execution time variances are considerable and
task granularity is not very fine.
Fig. 6 can be used to obtain an upper bound of the extent
to which limiting ourselves to two voltage levels diminishes
energy savings. It also provides guidance as to which
voltage levels are appropriate. For example, v‘ ¼ 0:7V gives
good results up to an average utilization of just under 0:5;
v‘ ¼ 0:9V is better than v‘ ¼ 0:7V for average utilizations
beyond this point, and so on.
Next, consider the impact of a (see Fig. 7). As a decreases,
the execution time of the tasks becomes more variable. This
means that the a priori information we have about the tasks
decreases. For very small values of actual average utilization (0:3 in this figure), the entire workload can usually be
done at low voltage and, so, the energy consumption is
44 percent of the high-voltage consumption. For average
actual utilization of 0:5, the value of a begins to make an
impact. For small values of a, the high variance of the
execution time means that the system must run at high
voltage for longer (since it has to guarantee that the
deadlines will be met even if everything runs to its worstcase execution time). As a increases and the execution time
does not vary as much any more, the system can be run at
low voltage for longer and the energy consumption
declines. When the actual utilization is 0:7 or 0:9, the
fraction of time the system has to be run at high voltage
increases and the energy consumption goes up.
The impact of ntasks is in the frequency with which slack
time is released. If the entire workload consists of, say, ntasks ¼
2 tasks, the system will have less opportunity to use for
another task that the slack released by an early completion. In
general, the lower the value of ntasks , the greater is the
deleterious impact of small values of a priori information, i.e.,
of small values of a. This is illustrated in Fig. 8, where, for
small values of a, a 2-task system performs noticeably worse
than systems with a greater number of tasks.

CONCLUSIONS

In this paper, we have described simple algorithms for
voltage scaling in real-time systems. These algorithms
exploit the fact that power consumption tends to drop
quadratically with voltage, while circuit delays (and, thus,
the clock period) increase only linearly. Our algorithms
have offline and online components. The offline component
assumes that the tasks run to their worst-case execution
times and computes the voltage settings to minimize energy
consumption. The online component starts with the offline
voltage settings as a base and then reclaims any time
resources that are released by tasks which finish ahead of
their predicted worst-case execution times, thus making for

DECEMBER 2003

a further round of energy savings. Our results indicate that
significant energy savings are possible, while guaranteeing
that all tasks will continue to meet their deadlines.

ACKNOWLEDGMENTS
The authors wish to thank the referees and the editor for
their careful reading of the draft manuscript and their
constructive suggestions. This research was supported in
part by the US National Science Foundation under grants
EIA-0102696 and EIA-0102539. Any views expressed here
are those of the authors and do not necessarily reflect those
of the funding agency.

REFERENCES
[1]

[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]

[13]

5

VOL. 52, NO. 12,

[14]
[15]
[16]
[17]
[18]
[19]

H. Aydin, R. Melhem, D. Mosse, and P.M. Alvarez, “Determining
Optimal Processor Speeds for Periodic Real-Time Tasks with
Different Power Characteristics,” Proc. Euromicro Conf. Real-Time
Systems, 2001.
L. Benini and G. De Micheli, “System-Level Power Optimization:
Techniques and Tools,” ACM Trans. Design Automation for
Electronic Systems, vol. 5, pp. 115-192, Apr. 2000.
T.D. Burd and R.W. Broderson, “Design Issues for Dynamic
Voltage Scaling,” Proc. Int’l Symp. Low-Power Electronics and
Design, pp. 9-14, 2000.
A. Chandrakasan, A. Sheng, and R.W. Broderson, “Lower-Power
CMOS Design,” IEEE J. Solid-State Circuits, pp. 472-484, 1992.
J.-M. Chang and M. Pedram, “Energy Minimization Using
Multiple Supply Voltages,” IEEE Trans. VLSI Systems, vol. 5,
no. 4, pp. 436-443, Dec. 1997.
J.K. Dey, J. Kurose, D. Towsley, C.M. Krishna, and M. Girkar,
“Efficient Online Processor Scheduling for a Class of IRIS RealTime Tasks,” Proc. ACM SIGMETRICS, 1993.
R. Ernst and W. Ye, “Embedded Program Timing Analysis Based
on Path Clustering and Architecture Classification,” Proc. IEEE
Int’l Conf. Computer-Aided Design, pp. 598-604, 1997.
I. Hong, D. Kirovski, G. Qu, M. Potkonjak, and M. Srivastava,
“Power Optimization of Variable Voltage Core-Based Systems,”
Proc. ACM Design Automation Conf., pp. 176-181, 1998.
I. Hong, M. Potkonjak, and M.B. Srivastava, “On-Line Scheduling
of Hard Real-Time Tasks on Variable Voltage Processor,” Proc.
Int’l Conf. Computer Aided Design, pp. 653-656, 1998.
I. Hong, G. Qu, M. Potkonjak, and M. Srivastava, “Synthesis
Techniques for Low-Power Hard Real-Time Systems on Variable
Voltage Processors,” Proc. Real-Time Systems Symp., 1998.
T. Ishihara and H. Yasuura, “Voltage Scheduling Problem for
Dynamically Variable Voltage Processors,” Proc. ACM Int’l Symp.
Low-Power Electronics and Design, pp. 197-199, 1988.
M.M. Khellah and M.I. Elmasry, “Power Minimization of HighPerformance Submicron CMOS Circuits Using a Dual-Vdd DualVth (DVDV) Approach,” Proc. 1999 ACM Int’l Symp. Low-Power
Electronics and Design, pp. 106-108, 1998.
C.M. Krishna and Y.-H. Lee, “Voltage-Clock-Scaling Adaptive
Scheduling Techniques for Low Power in Hard Real-Time
Systems,” Proc. Real-Time Applications Symp., 2000.
S. Lee and T. Sakurai, “Runtime Voltage Hopping for Low-Power
Real-Time Systems,” Proc. 37th Design Automation Conf., pp. 806809, June 2000.
C.M. Krishna and K.G. Shin, Real-Time Systems. New York:
McGraw-Hill 1997.
Y.-H. Lee and C.M. Krishna, “Voltage-Clock Scaling for Low
Energy Consumption in Fixed-Priority Real-Time Embedded
Systems,” Real-Time Systems, to appear.
Y.-R Lin, C.-T. Hwang, and A. Wu, “Scheduling Techniques for
Variable Voltage Low Power Designs,” ACM Trans. Design
Automation of Electronic Systems, vol. 2, no. 2, pp. 81-97, Apr. 1997.
J.W.S. Liu, Real-Time Systems. Upper Saddle River, N.J.: Prentice
Hall, 2000.
C.D. Locke, “Softwre Architecture for Hard Real-time Applications: Cyclic Executives vs. Fixed Priority Executives,” J. Real-Time
Systems, vol. 4, pp. 37-53, 1992.

KRISHNA AND LEE: VOLTAGE-CLOCK-SCALING ADAPTIVE SCHEDULING TECHNIQUES FOR LOW POWER IN HARD REAL-TIME SYSTEMS

[20] T. Ma and K.G. Shin, “A User-Customizable Energy-Adaptive
Combined Static/Dynamic Scheduler for Mobile Applications,”
Proc. Real-Time Systems Symp., pp. 227-236, 2000.
[21] D. Mosse, H. Aydin, B. Childers, and R. Melhem, “CompilerAssisted Dynamic Power-Aware Scheduling for Real-Time Applications,” Proc. Workshop Compiler and OS for Low Power, 2000.
[22] C. Park and A.C. Shaw, “Experiments with a Program Timing
Tool Based on Source-Level Timing Schema,” Computer, vol. 24,
no. 5, pp. 48-57, May 1991.
[23] T. Okuma, T. Ishihara, and H. Yasuura, “Real-Time Task
Scheduling for a Variable-Voltage Processor,” Proc. 12th Int’l
Symp. System Synthesis, pp. 25-29, Nov. 1999.
[24] T. Pering, T. Burd, and R. Brodersen, “The Simulation and
Evaluation of Dynamic Voltage Scaling Algorithms,” Proc. ACM
Int’l Symp. Low-Power Electronics and Design, pp. 76-81, 1998.
[25] G. Qu and M. Potkonjak, “Achieving Utility Arbitrarily Close to
the Optimal with Limited Energy,” Proc. ACM Int’l Symp. LowPower Electronics and Design, pp. 125-130, 2000.
[26] Y. Shin and K. Choi, “Power-Conscious Fixed Priority Scheduling
for Hard Real-Time Systems,” Proc. Design Automation Conf.,
pp. 134-139, 1999.
[27] A. Sinha, A. Wang, and A. Chandrakasan, “Algorithmic Transforms for Efficient Energy Scalable Computation,” Proc. ACM Int’l
Symp. Low Power Electronics and Design, pp. 31-36, 2000.
[28] M. Weiser, B. Welch, A. Demers, and S. Shenker, “Scheduling for
Reduced CPU Energy,” Proc. USENIX Symp. Operating Systems
Design and Implementation, pp. 13-23, 1994.
[29] O.S. Unsal, I. Koren, and C.M. Krishna, “Power-Aware Replication of Data Structures in Distributed Embedded Real-Time
Systems,” Proc. Fifth Int’l Workshop Embedded/Distributed High
Performance Computing Systems and Applications, 2000.
[30] F. Yao, A. Demers, and S. Shenker, “A Scheduling Model for
Reduced CPU Energy,” Proc. 36th IEEE Symp. Foundations of
Computer Science, pp. 374-382, 1995.
[31] Introduction to Thumb. ARM Documentation, Advanced RISC
Machines, Ltd., 1995.

1593

C.M. Krishna received the BTech degree in
1979 from the Indian Institute of Technology,
Delhi, the MS degree in 1980 from Rensselaer
Polytechnic Institute, and the PhD degree in
1984 from the University of Michigan, all in
electrical engineering. Since 1984, he has been
on the faculty of the Department of Electrical and
Computer Engineering at the University of
Massachusetts at Amherst. His research interests include power-aware computing, real-time
systems, systems evaluation, and fault tolerance. He has coauthored
(with K.G. Shin) a textbook on real-time systems, as well as volumes of
readings in computer architecture and real-time systems. He is a senior
member of the IEEE.
Yann-Hang Lee received the PhD degree in
computer, information, and control engineering
from the University of Michigan, Ann Arbor, in
1984. He was a research staff member at the
IBM T.J. Watson Research Center in Yorktown
Heights, New York, from 1984 to 1988, and was
a professor in the Department of Computer and
Information Science and Engineering, University
of Florida, from 1988 to 2000. He is currently a
professor in the Department of Computer
Science and Engineering, Arizona State University. His research
interests are in the areas of real-time systems, software engineering,
communication networks, computer architecture, and performance
evaluation. His current research projects are focused on various
aspects of real-time embedded systems and are funded by NASA,
FAA, DARPA, and NSF. He also collaborates with Honeywell International, United Technology Research Center, and Motorola Labs, on
practical applications of real-time embedded systems. He is a member
of the IEEE.

. For more information on this or any computing topic, please visit
our Digital Library at http://computer.org/publications/dlib.

ITB: Intrusion-Tolerant Broadcast Protocol in Wireless
Sensor Networks
Jin Wook Lee1 and Yann-Hang Lee2
1

Networking Technology Lab.,
Samsung Advanced Institute of Technology, P.O. 111, Suwon, Korea 440-600
2
Real-Time Embedded Systems Lab.
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287-8809, USA

Abstract. A large-scale wireless sensor network relies on broadcast protocol for
data communication. This paper presents an idea for a base station to reliably and
securely broadcast announcing messages to the network in an intrusion-tolerant
way. Our Intrusion-Tolerant Broadcast Protocol based on Mutual Verification
scheme guarantees that no forged information can be propagated to the network
if an intruder is assumed to not compromise the base station. We show that our
proposed protocol successfully runs and also demonstrate the effectiveness of the
protocol by introducing a theorem and the proof of the theorem.

1 Introduction
A large-scale wireless sensor network may consist of thousands of nodes randomly
deployed in an open environment. A typical communication in such a network would
be a base station delivering commands to every sensor node in the network and then
the sensor nodes sending sensed data from their close proximities to the base station
for further analysis. For such a communication pattern, a broadcast protocol based on
flooding would be an efficient solution. But there is a crucial issue concerning floodingbased broadcast protocols in wireless sensor networks - broadcast authentication.
It becomes critical that, once a message is sent out from the base station, there is a
way for the node to guarantee that the message arrives at the recipient with its original content. A solution to this broadcast authentication is the private/public -key-based
digital signature. TESLA-based protocols, proposed by Perrig et al. and Liu et al. as
a broadcast authentication protocol, require a digital signature to distribute the initial
parameters and all nodes to perform pre-setup procedures before undertaking a real
task such as assigning the last key of a hash chain and synchronizing a global time
clock [2] [3] [5]. Such pre-setup procedures create considerable overheads. We aim at
an efficient and secure broadcast protocol for sensor networks. A novel broadcast protocol is proposed based on the notions of intrusion-tolerance and mutual verification.

2 Intrusion-Tolerant Broadcast Protocol
Intrusion-Tolerant Broadcast(ITB) Protocol we propose here introduces a mechanism
to verify the behavior of neighbor nodes, detect and respond against threats during the
M. Gerndt and D. Kranzlmüller (Eds.): HPCC 2006, LNCS 4208, pp. 505–514, 2006.
c Springer-Verlag Berlin Heidelberg 2006


506

J.W. Lee and Y.-H. Lee

broadcast. Our goal is to flood the broadcast messages to the network without allowing
faked messages’ propagation. We target a large-scale wireless sensor network consisting
of a base station and a large number of sensor nodes. We use several terms to describe
the protocol as defined in Table 1.
Table 1. Glossary
Glossary

Description

Level(l)
physical hop distance from the base station
Level Key(LK) a value generated by one-way hash function
and distinct according to level(l)
Shared
a symmetric key that is shared by all nodes.
Key(SK)
Pre-installed key.
Private
a private key of the base station. It is not
Key(P rvK)
faked or generated by a node.
Level Synchro- the protocol to assign the time duration for
nization
sending or receiving a message
Time interval time duration for sending or receiving specific protocol messages
BRTS
Broadcasting Request To Send
BCTS
Broadcasting Clear To Send
BM
Broadcasting Message

2.1 Control Packet
The key idea of ITB protocol is that letting two-hop away nodes exchange the verification information of broadcast messages and allowing the nodes to verify each other
before taking further actions. In order to realize the idea, we apply the concept of the
typical RTS/CTS handshaking of 802.11 medium access control protocol [4], which is
used to avoid collisions in wireless multi-hop networks. During RTS/CTS handshaking,
a node is affected by two-hop away nodes’ communication by receiving a RTS or a CTS
control packet. We take advantage of this concept to deliver the verification information to two-hop away nodes in the routing path before delivering a data message. The
protocol proposes three packets, BRTS, BCTS, and BM, in its broadcast handshaking
mechanism. Consider a sensor node i which is l hops away from the base station and is
in the radio range of a upstream node k of hop distance l−1. When a broadcast message
is propagated from node k to node i, the packets that node i will send out in turn are:
BCT Sil : ESK {M AC(BMil ), LK l+1 }
BRT Sil : ESK {M AC(BMil+1 ), LK l+2 }
BMil : ESK {LK l } | EP rvK {M essage}
In the packet formats, Ex {M } represents that message M is encrypted with a key
x. Note that MAC of BRTS and BCTS is Message Authentication Code of BM rather
than BRTS or BCTS themselves. Specially, the MAC in a BCTS that a node receives

ITB: Intrusion-Tolerant Broadcast Protocol in Wireless Sensor Networks
upstream
Node k on level l-1
Time

507

downstream
Node i on level l

Node j on level l+1

BCTSkl-1 : ESK{MAC(BMkl-1), LKl}

BRTSkl-1 : ESK{MAC(BMkl), LKl+1}
Memcmp

BCTSil : ESK{MAC(BMil), LKl+1}

BCTSil : ESK{MAC(BMil), LKl+1}

BMkl-1: ESK{LKl-1} | EPrK{Message}
BRTSil : ESK{MAC(BMil+1), LKl+2}

BRTSil : ESK{MAC(BMil+1), LKl+2}

MAC Hash

BCTSjl+1 : ESK{MAC(BMjl+1), LKl+2}
BMil : ESK{LKl} | EPrK{Message}

BMil : ESK{LKl} | EPrK{Message}
BRTSkl+1 : ESK{MAC(BMjl+2), LKl+3}

BMjl+1 : ESK{LKl+1} | EPrK{Message}

Fig. 1. Intrusion-Tolerant Broadcast Protocol in a single routing path

from upstream nodes will be used to verify a BM which the node will receive later. This
allows two-hop away nodes to make an agreement with each other. For MAC, the use
of collision-proof checksums is recommended for environment where a probabilistic
chosen-plaintext attack represents a significant threat [7]. We apply MD5 as a MAC
generation algorithm. Level Key(LK) is a verification key as an output of one-way hash
function. LK of upstream nodes is not easily guessed due to its one-wayness.
2.2 Broadcast Step
The base station starts broadcasting a message by firstly sending a BCTS to its adjacent nodes. The reason that the base station firstly sends a BCTS is that each node
starts its protocol operation on receiving a BCTS from one of parent-level nodes as
shown in Figure 1. Originally CTS is a response of RTS. However, it is not possible for
the neighbor nodes of the base station to receive a BCTS from the base station because
the base station does not respond to any BRTS. Therefore, the protocol forces the base

508

J.W. Lee and Y.-H. Lee

station to generates an initial BCT S00 (Assume that the level of the base station is zero
and the ID of the base station is zero) that contains LK 1 and a MAC of the first BM
to start a broadcast. After the initial BCTS packet sent, it is ready to start a general
forwarding cycle of sending a BRTS, receiving a BCTS and sending a BM.
Assume nodes i and j are two neighboring nodes with hop distances l and l + 1,
respectively. When node i is ready to forward the broadcast message, it first sends a
BRTS as follows:
broadcast

Node i of level l −−−−−−→ Neighbors :
BRT Sil : ESK {M AC(BMil+1 ), LK l+2 }
After receiving BRTS from node i, all level l+1 nodes including node j prepare their
own BCTSs based on the received BRTS. Again, during appropriate time interval, node
j transmit its BCTS as below:
broadcast

Node j of level l+1−−−−−−→ Neighbors :
BCT Sjl+1 : ESK {M AC(BMil+1 ), LK l+2 }
As a neighbor of node j, node i can verify the BCTS packet sent from all level l+1
nodes as well by memory comparison. If the verification is completed without detecting
any faults, node i broadcasts the real message enveloped in a BM like below:
broadcast

Node i of level l −−−−−−→ Neighbors :
BMil : ESK {LK l } | EP rvK {M essage}
The protocol is illustrated pictorially in Figure 1. Note that, before forwarding the
broadcast message, node i has already sent a BCTS in response to the BRTS from its
parent node. Eventually node j hears this BCTS. Hence, node j of level l+1 receives
three packets from node i in the sequence of BCTS/BRTS/BM. Each packet embraces
LK l+1 , LK l+2 , and LK l respectively. From one-wayness of Level Key, LK l is used to
verify LK l+1 and LK l+2 . If the verification is completed without detecting any faults,
the node officially accepts the broadcast message. Otherwise, the message is discarded
and no forwarding operation is taken.
2.3 Two-Hop Away Communication
The protocol of each node is started up on receiving a BCTS sent by a parent-level
node. However, a node does not have a way to verify this BCTS itself since the BCTS is
originally destined to grandparent-level nodes (two-hop away nodes which are nearer to
the base station, for instance, node j for node k in Figure 1) and the node does not have
any information of this BCTS. Hence, the node stores this BCTS naively and keeps it
unverified. The verification for a BCTS can be done by grandparent-level nodes. If this
BCTS is not valid, grandparent-level nodes stop to broadcast the main message packet,
BM and eventually a node does not receive the BM associated with the previously
received BCTS. A BM has also a clue to verify the previously received BCTS. The LK
of a BM is a key of verification for the received BCTS. Parent-level nodes cannot fake
a BM and a BCTS without grandparent-level nodes’ or child-leve nodes’ detection.

ITB: Intrusion-Tolerant Broadcast Protocol in Wireless Sensor Networks

509

3 Mutual Verification
In the protocol, each node receives three protocol packets from each of its upstream
nodes and broadcasts out three protocol packets to its downstream nodes. The protocol
provides a way to mutually verify each protocol packets. When the verification fails, a
sender restrains itself from sending out the broadcast message or a receiver discards the
received message. As a consequence, the broadcast operation stops along the broadcasting path that fails in the verification. The verification operation in the protocol relies on
three computation actions, memory comparison, one-way hash function computation,
MAC computations. Memory comparison is a lightweight action as compared to oneway hash function and MAC computation. Computational overhead of MAC computation is much more than that of one-way hash function. Therefore, a verification should
be done in a way that the results of MAC and one-way hash function computation are
stored to reduce the frequency of MAC and one-way hash function computation.
3.1 BCTS Verification
A BCTS packet is sent out on a recipient of a BRTS packet. There are three types of
BCTS’s that a node may receive. The first one is in an initial state that a node receives
the BCTS’s from its parent-level nodes. On receiving of the BCTS in the initial state,
a node starts the protocol by storing the information of the BCTS coming from parentlevel nodes. At this time the node dose not know whether the received information is
legitimate or not. Only the source node of the BRTS can verify the BCTS since the
payload of the BCTS should be identical to the payload of the corresponding BRTS.
If the verification succeeds, the source takes the next action of sending a BM. Once a
node receives the first BCTS, it can do the comparison to check out any BCTS received
successively in the same broadcast window if there are multiple neighbor nodes. Subsequently, the node can receive a BRTS from a parent node in the upstream and enter
a broadcast window of sending out a responding BCTS. There might be the second
type of the BCTS that are originated from its sibling-level nodes. The verification for it
does not require one-way hash function computation. A node compares its own BCTS
with the ones from its sibling nodes. Lastly, there are a BCTS coming from child-level
nodes as a response of its BRTS. It should have the same content as the BRTS. Memory
comparison is good enough to verify them.
3.2 BRTS Verification
In the same way of the BCTS case, there are three types of BRTS packets that a node
receives from its neighboring nodes. On receiving the first BRTS from one of its parentlevel nodes, a node can verify the BRTS by comparing the LK field of the stored BCTS.
The LK in the BRTS should be the output of one application of one-way hash function
on the LK in the stored BCTS. If any verification is not succeeded, the node discards
the stored BCTS and the received BRTS from the source node and registers the source
node as a malicious node. Once a node succeeds in verification, the subsequent verifications are done by just comparison to reduce computational overhead. The other
two types of BRTS packets are from the sibling nodes and the child nodes. The BRTS

510

J.W. Lee and Y.-H. Lee
BCTSl : ESK{MAC(BMl), LKl+2}

verify

verify

BMl : ESK{LKl} | EPrvK{Message}

Fig. 2. Mutual Verification

from the sibling nodes should be identical to the one the node is sending out and the
ones from the child nodes should have the correct LK (after one application of the hash
function). It is possible to find any discrepancy in the comparison of these BRTS with
the node’s own BRTS. However, besides noting the possible malicious behavior in the
neighboring nodes, the node with the verified BRTS can proceed to send out the broadcast message.
3.3 BM Verification
The verification of a received BM is done with the stored MAC and LK. When a node
receives a BM from one of parent-level nodes for the first time, it computes the MAC
for the whole message and decrypts the BM with the shared key, SK to extract LK. The
computed MAC should be the same as the stored MAC which is included in the previously received BCTS. In addition, the node runs one-way hash function twice with the
extracted LK and make a comparison with the stored LK of the received BCTS/BRTS.
If this verification fails, a node discard all protocol packets coming from the same node.
The notion of mutual verification is shown pictorially in Figure 2.
3.4 Effectiveness of the Protocol
We prove the effectiveness of the protocol through introducing a theorem based on three
propositions.
Proposition 1 (Concurrency). The neighbor nodes of node N receive the same message at the same time when node N transmits the message.
Proposition 2 (Mutual Exclusion of Transmission). The neighbor nodes of node N
are not able to transmit any messages successfully at the same time when node N transmits a message.
Proposition 3 (Watchdog). The neighbor nodes of node N are able to monitor node
N’s transmission under any collision avoidance protocols.
Theorem 1 (Mutual Verification). A node cannot modify and forward the relaying
messages successfully without being detected by one-hop away nodes if one-way function and MAC is secure.

ITB: Intrusion-Tolerant Broadcast Protocol in Wireless Sensor Networks

511

Proof. Assume there are three nodes in a routing path, which are node A, node B and
node C. Let say that node A is a source node, node B is a relay node and node C is a
destination node.
Suppose for contradiction that node B, an adversary that can modify a relaying message without being detected by node A and successfully forward the modified message
to node C. As a result, node C receives a different message from the message that node
A sent to node B. A proof of effectiveness of our protocol is divided by two parts according to the messages.
The behavior of node B on receiving a BCTS and a BM can be categorized as below;
Case I: On receiving a BCTS from node A,
– Scenario 1) Node B relays the received BCTS to node C
– Scenario 2) Node B modified the received BCTS and forwards it to node C
Case II: On receiving a BM from node A,
– Scenario 3) Node B relays the received BM to node C
– Scenario 4) Node B modified the received BM and forwards it to node C
Proof of Case I)
There are two scenarios for node B’s action to avoid node A’s detection in case that
node B receives a BCTS from node A. One is that node B naively relays the received
BCTS to node C. The other is that node B relays the received BCTS to node A and
forwards a modified BCTS to node C at the same time. Scenario 1 is intuitively correct
as the behavior of node B is correct. Let us consider Scenario 2. In order for node B
to send a modified BCTS successfully to node C without node A’s detection, it should
be able to broadcast two different messages simultaneously, the original BCTS to node
A and a modified BCTS to node C. However, this is contradiction of Proposition 1
and 2. Node A and node C exist within the radio range of node B, so node A receives
the message from node B if node C receives it and vice versa. Therefore, node A is
able to detect node B’s misbehavior when node B modifies and forwards a BCTS. As
a result of node A’s detection, it does not forward a BM, which means node B should
be able to create a BM that matches a modified BCTS in order to let node C trust
node B.
Proof of Case II)
In proof of part I, we prove node B cannot forward a modified message without node
A’s detection. When node B relays a correct BCTS and receives a correct BM from
node A, node B can choose the action like Scenario 3 and 4.
Scenario 3 is legitimate while Scenario 4 is needed to be proven. If node B receives a
BM from node A, the transmission role of node A is finished at this moment. Hence, we
can say node C has already received the correct BCTS from node B. If node B modifies
the BM and forwards it, node C can detect the modification by using the previously
received BCTS. Therefore, node B cannot disguise node A and node C at the same time
(Proposition 1 and 2), which is contradiction.

512

J.W. Lee and Y.-H. Lee

4 Evaluation
We now are interested in evaluating the cost of the ITB protocol and demonstrating its
applicability to the resource-constraint sensor devices. We quantify the computational
overhead a node pays to securely broadcast a message and simulate energy consumption
of the protocol. Communication and computational overheads will be considered and
we justify our claim with a simulation result of energy consumption. Let na denote the
average number of neighbor nodes for a node.
Communication Overhead. We define Communication Overhead to be the average
amount of data a node is required to transmit and receive for a broadcast. Firstly, we
discuss the number of communications a node is involved. In ITB protocol, the number
of outgoing communications per each node is constant while the number of incoming
communications is heavily dependent upon the number of neighbor nodes. Note that
three outgoing transmissions(including BCTS, BRTS, and BM) are just necessary for a
node to verify protocol packets and relay the broadcast message in ITB protocol. These
transmissions are all broadcasts so retransmission is not applied. Hence, message complexity of outgoing communication is O(1). The number of incoming communications
is associated with the network density. A node receives one BCTS, one BRTS, and one
BM packet from a single parent-level node. A node also receives the same sort of packets from its sibling-level nodes and child-level nodes. Therefore, the average number of
incoming communications per node is calculated as 3 · na . Hence, message complexity
of incoming communication is O(n) where n is the average number of neighbor nodes.
From the standpoint of the number of communications, ITB protocol requires three
more times number of outgoing and incoming communications than a simple flooding
broadcast protocol does. However additional amount of data transfer for the protocol
is only small bits excluding broadcast message body (Note that we assume the size of
LK and MAC reasonably is 64 and 128 bits, respectively). In conclusion, ITB protocol is efficient in terms of communication overhead as the size of broadcast message
increases.
Computational Overhead. Computational Overhead in ITB protocol is defined as the
number of cpu cycle counts running security algorithms such as memory comparison,
one-way hash function, and MAC. We analyze the expected number of such security
algorithms runs for broadcasting a single 6-byte message. Thanks to the deterministic
number of communications for a node, the computational overhead can be calculated
through analyzing the each algorithm.
In order to verify incoming messages’ validation a node makes a memory comparison between stored information and newly receiving information. Clearly the number
of comparison is dependant upon the number of incoming messages since the comparison is used for verification. When a node is awaken by receiving a BCTS from a
parent-level node, it just stores the content of the BCTS without any comparison. As a
result of analysis of our algorithm, we obtain the total number of comparisons is less
than 12 · na − 5, where na is the average number of neighbor nodes. The numbers of
one-way hash function runs and MAC computations are obvious. They are counted four
times and two times runs for a single broadcasting message respectively. Let Cc , Ch ,

ITB: Intrusion-Tolerant Broadcast Protocol in Wireless Sensor Networks

513

and Cm denote the cost of computing a single comparison, the cost of computing a single hash function, and the cost of computing a single MAC computation, respectively.
The total cost of computational overhead is derived as below:
(12 · na − 5) · Cc + 4 · Ch + 2 · Cm

(1)

The total computational cost is measured by the cpu cycle count of ITB algorithm.
From PowerTOSSIM cpu cycle profiling [8], we get cpu cycle counts (34 cycles for
memory comparison, 20228 cycles for one-way hash function, and 36437 cycles for
MAC computation). As we see in Figure 3(a), the number of CPU cycles slightly increase as the number of neighbors increases. We claim that the computational cost of
ITB protocol does not heavily rely on the number of neighbors nodes since the number
of neighbor nodes affects only the number of memory comparisons. In conclusion, our
ITB protocol is designed in consideration of scalability and efficiency.
To justify our claims of computational overhead, we implement the protocol and run
it with MICA2 energy model (Refer to Table 2) to collect energy consumption data. We
perform the PowerTOSSIM to simulate our protocol with real implementation code for
MICA2 [8]. As can be seen in Figure 3(b), energy consumption caused by ITB protocol
does not proportionally increase as the network density increases, which prove that our
claims is reasonable.
Table 2. Energy Model for MICA2

5

1.6

model

energy(mA)

CPU active
CPU idle
CPU init
Radio RX
Radio TX
LED

8.93
3.2
3.2
7.03
21.48
2.2

Computational Overhead in ITB protocol

x 10

Energy Consumption for ITB per a node
2200
2000

1.58

1800
Energy Consumption (mA)

1.59

CPU cycle counts

1.57
1.56
1.55
1.54
1.53

1600
1400
1200
1000
800

1.52

600

1.51

400

1.5

1

2

3

4
5
6
7
Number of Neighbors (density)

8

(a) Computational Overhead

9

10

Total energy consumption
CPU
Radio
Led

200

1

2

3

4
5
6
7
Number of Neighbors (density)

8

(b) Energy Consumption

Fig. 3. Average amount of power consumption for ITB protocol per node

9

10

514

J.W. Lee and Y.-H. Lee

5 Conclusion
In this paper we have proposed Intrusion-Tolerant Broadcast Protocol to realize secure
broadcast via Mutual Verification mechanism. By using the proposed protocol, the base
station is able to disseminate important messages to the network without propagating
modified or garbage messages. Our protocol is secure in terms that any bad broadcasting
messages modified by a compromised node are not propagated more than two-hop away
from the compromised node. Our protocol makes any attacks of compromised node
detectable, verifiable and responsible. Our analysis showed that ITB protocol is quite
lightweight in a sense that communication and computational overhead are O(n), where
n is the average number of neighbor nodes(i.e., network density) respectively. Through
discussing a theorem, we demonstrated the effectiveness of ITB protocol.

References
1. Y. H. Lee, A. Deshmukh, V. Phadke and J. W. Lee, Key Management in Wireless Sensor
Networks, In Proc. of the First European Workshop (ESAS 2004), pages 190–204, 2004.
2. A. Perrig, R. Canetti, J. D. Tygar, D. Song, Efficient and secure source authentication for
multicast, in Network and Distributed System Security Symposium (NDSS’01), 2001.
3. A. Perrig, R. Szewczyk, J. D. Tygar, V. Wen, D. Culler, SPINS: Security Protocols for Sensor
Networks, Wireless Networks Journal (WINET), 8(5), pages 521–534, 2002.
4. A. E. F. Clementi, A. Monti and R. Silvestri, 802.11 Specification, in ACM-SIAM SODA’01,
2001.
5. D. Liu and P. Ning, Efficient Distribution of Key Chain Commitments for Broadcast Authentication in Distributed Sensor Networks, In Proc. of the 10th Annual Network and Distributed
System Security Symposium, pages 263–276, 2003.
6. A. Chakrabarti, A. Sabharwal and B. Aazhang, Multi-Hop Communication is Order-Optimal
for Homogeneous Sensor Networks, in Proceedings of the third international symposium on
Information processing in Sensor Networks, pages 178–185, 2004.
7. J. Kohl and C. Neuman, The Kerberos Network Authentication Service (V5), in RFC 1510,
1993.
8. V. Shnayder, M. Hempstead, B.-R. Chen and M. Welsh, PowerTOSSIM, http://www.eecs.
harvard.edu/ shnayder/ptossim/.

RTSOA: Real-Time Service-Oriented Architecture
W.T. Tsai, Yann-Hang Lee, Zhibin Cao, Yinong Chen, Bingnan Xiao
Computer Science and Engineering Department
Arizona State University,
Tempe, AZ 85287-8809, USA

Abstract
This paper extends the traditional Service-Oriented
Architecture (SOA) to a new Real-Time SOA (RTSOA)
and proposes a framework for the new architecture, by
providing real-time service modeling, design, code
generation, simulation, deployment, execution,
orchestration, and management. The concepts,
architecture, and enabling technique are studied for
Real-time SOA. In addition, an efficient algorithm is
derived in the paper to find the optimal composition of
real-time services subject to the real-time constraint.
1.

Introduction

As SOA penetrates into more and more application
areas, applying real-time technologies to SOA becomes
a necessity. Large companies such as CISCO, IBM,
Microsoft, and TIBCO are pushing the technologies to
enable real-time processing in SOA frameworks. Given
that the characteristics of Real-Time SOA (RTSOA)
are different from the current SOA, it is necessary to
establish real-time support in SOA frameworks. This
includes the approaches to meet timing constraints and
to be predictable in many SOA aspects of modeling,
composition, orchestration, deployment, policy
enforcement, and management.
Real-time issues have been discussed in a number
of studies. Kazi discussed the agile Real-Time
Enterprise Architecture in [14], which is a combination
of SOA and EDA for real-time business. Bodhare
discussed service infrastructure optimization for realtime SOA, including redundancy, service scheduler
engine, and service auditing in [2]. Most of these
discussions focused on the quality of service enabled
from the service provider’s point of view, but do not
address the overall issues of service integration and
interoperability from the service consumer's or
application builder’s point of view.
This paper discusses the critical issues to enable
RTSOA from both the service providers’ and service
consumers’ sides. On the service providers’ side,
RTSOA provides the mechanism for real-time
scheduling, resource management, and processing. On
the service consumers’ side, it provides support for
real-time service modeling, application composition,

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

planning, service deployment, service execution,
scheduling, orchestration, and management.
The rest of this paper is organized as follows.
Section 2 discusses the real-time issues in SOA and the
overall design of the RTSOA. Section 3 elaborates the
major components of RTSOA frameworks. Section 4
presents a heuristic search algorithm for the optimal
service composition. Comparisons between the
proposed heuristic and exhaustive search algorithms
are given in Section 5. Section 6 concludes the paper
and discusses future work.
2.

Real-time Perspectives in SOA

In RTSOA, service providers can enable real-time
services, i.e. invocations of services must be completed
within specific timing constraints. As well, the
operations facilitated by SOA framework can follow
real-time requirements. One of the examples is that the
operation of composing services into SOA applications
has bounded response time and resource usage.
For each SOA application, its lifecycle consists of a
feedback loop of modeling, assembling, deployment,
and management [11], as shown in Figure 1. The
lifecycle of an RTSOA application follows a similar
cycle. Thus, it is necessary to examine the support to
common SOA techniques such as discovery,
composition, analysis, simulation, deployment,
orchestration, and management with respect to the
needs of real-time computing.

Figure 1: Real-Time SOA
Modeling and Specification: In this phase,
individual services are specified, including functional
and real-time properties. The workflow for the
application is also depicted in this phase. Note that
real-time properties should include the performance
and timing requirement as well as workload
characteristics, such as the service deadlines and
invocation frequency.

Discovery and Assembly: Once the services are
modeled and specified, the services required by the
application can be discovered and assembled into the
application. In this phase, control logic code is also
generated to dynamically reconfigure and recompose
the application. After the application is composed,
multiple analyses including completeness &
consistency checking, model checking (verification),
simulation and testing (validation) can be performed to
evaluate the system behaviors base on the application
model and to analyze the real-time properties of the
composed application. The steps of discovery,
assembly, and validation should have bounded
response times such as ESB (Enterprise Service Bus) []
Deployment: After the application is assembled and
the composite service model evaluation results satisfy
the requirements, the composed application can be
deployed for execution. To permit real-time response
for the application, the deployment step should make
reservation of required resources at service provider
and consumer sites, as well as interconnection
networks.
Execution and Management: During the execution
time, predetermined points of interests are monitored,
policies are enforced, and data are collected, so that the
system performance and behavior of the application
can be evaluated. The service execution and
monitoring should be scheduled and the reserved
resource (CPU time, memory region, and network
bandwidth) must be guaranteed. If the performance is
not satisfactory, or faults are detected, reconfiguration
or recomposition can be performed by changing by
rebinding to better services or refining the composition
specification. The recomposed application needs to be
verified and validated.
Conceptually, services are connected to an abstract
bus interconnection, such as ESB [11], which is
independent of network routing and topology. Via the
bus, services can be discovered, connected, and
mediated. For RTSOA, the abstract bus is not only for
interconnectivity and brokering, but also is with quality
of service parameters for reliable real-time
communication.
The RTSOA can be further enhanced by adding an
SOA back-end to the architecture. In addition to
provide support for real-time services, the new RTSOA
framework needs to provide support for discovery and
dynamic composition, deployment, and runtime
monitoring in real-time. The real-time discovery and
composition can be realized if service information can
be cached by the SOA back-end and be retrieved
within bounded search time. Similarly, with an
assigned CPU and network bandwidth, the SOA backend engine can carry the operations of SOA framework.
For instance, the service verification and validation

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

may be done by the SOA back-end using the allocated
CPU cycles and network bandwidth. As a consequence,
verified services can be saved in a service repository to
facilitate the service discovery for RTSOA.
3.

Components of RTSOA Framework

To implement RTSOA, almost every techniques
and protocols in traditional SOA need to consider the
possible implications to real-time constraints. This
section enumerates several key components in a
RTSOA framework.
3.1. Real-Time Communication
Since the communication between services may
happen between remote nodes in SOA application,
real-time communication is a basic issue that needs to
be resolved in RTSOA. To achieve real-time
communication, messages to be exchanged in the
service communication need to be serialized in realtime
for
marshalling/unmarshalling;
Channel
bandwidth between the services need to be reserved;
Fault handling in application composition and
communication backbone need to employ forward fault
recovery;
Typically, the communication channels between
service providers and consumers are formed with
multiple layers of protocols. Services and applications
first wrap application data in XML. Then, a message
protocol, SOAP, is utilized to encode data in XMLbased model. At transport layer, HTTP and encrypted
HTTPS are commonly used over TCP/IP. For TCP
connections, the two Internet QoS models, integrated
services and differentiated services, can be invoked if
there is a proper QoS routing support in the network.
In addition to the QoS at transport layer, the SOAP
protocol processing may have a notable performance
deficiency due to the slow and memory intensive
message encoding and serialization [16]. It will be
required to streamline the SOAP protocol processing,
such as using a table-based serialization approach [10],
the code-generation approach for XML serialization in
gSOAP [7].
3.2. Service Modeling for Real-Time Properties
In additional to the functional service modeling, the
real-time properties of services also need to be
included in the service specification. A service may
provide various levels of service quality and, at each
service level, service specification may need to include
the following information:
• Minimum and maximal response times
• Service capacity (such as a number of service
invocations that can be accepted per unit of time)

• Degree of concurrency (the maximal number of
service consumers that the service provider can
be bounded to simultaneously)
• Cost and required resource

from each services it invokes. For example, assume a
workflow specification for an application is composed
by a sequence of services:

Service specification represents the service quality
that can be guaranteed by the provider under the
existing resource reservation. This implies that the
service specification on real-time properties will be
adjusted dynamically when additional resource is
granted or commitments are made to service
consumers.

The top level application specification may include
an end-to-end timing requirement for the entire
workflow, for example, 10 seconds from A to D. The
expected requests to the services are 100
requests/seconds. At the lower level services, timing
properties may be specified as services A and B require
2 seconds to finish and services C and D requires 3
second to finish. In Section 4, an optimization problem
is formulated and solution algorithms are discussed.

3.3. Repositories for Real-Time Composition
For dynamic and real-time composition in RTSOA,
advanced service discovery is necessary and the
discovered services are stored in a repository that can
be accessed in real-time. A three layered repository
structure is used in RTSOA, as shown in the Figure 2,
where, the repositories contain the links to pre-verified
services and their real-time service property. Those
verified services are ready to be used in a real-time
application without additional testing. At runtime, the
active services bound into the application will be
stored in the service cache, the hot backup services are
stored in memory, and the cold backup services are
stored in remote service repository.

Figure 2: Three-Level Real-Time Discovery
3.4. Dynamic Service Composition
In RTSOA, applications can be dynamically
composed in real-time. The service requirement is
specified in the workflow of the composite service
(application), as shown in Figure 3. Services to be
included in the composite service (application) are
decided based on the service ontology. In RTSOA,
verified services are stored in the service cache or
memory, and thus can be accessed rapidly according to
the service requirements. In the service composition,
different services will be selected at the run time. Then,
the application will be ready for deployment.
The service composition operation is built on an
optimization process in selecting the suitable services
and to decide the quality level the application needs

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

AÆBÆCÆD

Figure 3 Real-Time Composition
3.5. Real-Time Service Deployment
In RTSOA, the composed service needs to be
deployed for execution and the deployment operation
must be done in real-time. Generally, there is an
application host in the SOA framework for hosting the
composed application. The simple control code and
workflow specification will be deployed to the
application host. Furthermore, there need to be an
orchestration engine and data collection engine on the
application host. To achieve real-time deployment,
reserved bandwidth is necessary to ensure the data
transfer can be finished in real time. Also, the data
volume for deployment needs to be small so that the
time for deployment can be reduced. Minimum
installation or setup on the application host is also
necessary to optimize the deployment.
The deployed application may consist of the
serviced selected in the composition step. A binding
process must be carried out between the application
host and the service providers to ensure the reservation
of resource and the guaranteed quality of service.
3.6. Data Collection and Policy Enforcement
At the application run time, data need to be
collected for different analyses and policy
enforcement. A real-time policy engine will be
deployed in the RTSOA framework. Application
behaviors need to be checked against the policies
specified in real time. These policies include functional
policies as system behavioral constraints and non-

functional policies including timing policies. If a
policy is violated, the policy engine needs to execute
the forward compensation operations for recovery.
3.7. Real-time Service Execution Environment
In the RTSOA framework, the execution of service
providers and consumers are governed by the
underlined execution environment, such as operating
systems and virtual machines. In addition, competing
execution entities may share system resources that are
managed by the execution environment. Hence, a
proper support to ensure QoS of services and
applications must be provided by the environment.
In our RTSOA framework, we focus on VM-based
execution environment, such as JVM (Java Virtual
Machine) and Microsoft CLI (Common Language
Infrastructure). They have been adapted widely in the
existing SOA applications and enable an abstract
computing environment for software portability. In
addition, programs developed in Java or CLIcompatible languages are claimed secure due to typesafety and security sandbox model. The ensured safety,
portability, and reusability of OO languages make
VMs attractive to SOA and the integration of the two
approaches can complement with each other in terms
of software portability and interoperability.
Both JVM and CLI can be viewed as layered
software libraries to provide system and language
support operations, such as thread management,
garbage collection, dynamic class loading, namespace,
JIT (Just-in-Time compilation), AOT (Ahead-of-Time
compilation), serialization, and programming language
interface (e.g. JNI). To lead to QoS guarantees for
RTSOA, a deterministic behavior in VM must be
ensured, i.e., these VM services must be bounded in
their WCET (worst-case execution time). Furthermore,
VM’s operations should be preemptible and
schedulable in the similar manner as application
threads, and allow the pause time due to the operations
to be controlled [1][9] .
Scheduling of application services and VM
operations should address both the timing requirements
of real-time operations and the dynamic features of
SOA. A hierarchical scheduling mechanism [15] can
be adopted:
1. At partition level: The notion of partitioning is
used to denote the ownership of system resource,
including CPU time, memory, and network
connection at each service node. It consists of
spatial and temporal partitioning among services/
applications and acts as a brick wall such that task
execution in a partition cannot be affected by any
requests in other partitions. Partition can be
assigned to each application domain in CLI, or a
namespace in JVM. The CPU time can be reserved

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

in the form of partition cycle, η, and partition
capacity α, i.e. for each partition cycle, the server
can execute the tasks in the partition for an
interval at least αη where α is less than or equal to
1.
2. At thread level: Multiple threads of a partition
share the system resource owned by the partition
and interact with each other to complete service
invocations and applications. Threads can be
scheduled according to their priorities and run
concurrently to handle multiple and overlapping
requests.
An example of the hierarchical scheduling
mechanism is to enable a service provider in a partition
where a thread pool is initiated and consists of a fixed
number of threads. With distinctive priorities,
differentiated services can be connected to service
consumers. At partition level, the service provider can
be scheduled periodically with a fixed CPU budget or
as a sporadic server which is with constant bandwidth
and replenishment period. Similarly, one or more
partitions can be scheduled for VM operations,
including garbage collection, class loading and JIT, etc,
as shown in Figure 4.
3.8. Mechanisms for Real-time Guarantee
Real-time mechanisms for both the service
providers and service consumers (applications) need to
be designed to guarantee operations to be executed in
real time. At the service provider side, issues in the
real-time mechanisms include:
• Message queue design
• Message priority
• Operation preemption
• Multi thread scheduling
GC

Service
provider

Class loading, JIT, etc.

Figure 4: Partition scheduled for VM operation
In addition to a first-come-first-serve queue for
incoming service request messages, the RTSOA
framework also needs to prioritize the message queue
according to different criteria including timing
priorities.

Different service consumers may subscribe to
different level of services that guarantee different
levels of quality of services (QoS). Consumers with a
high subscription rank can be assigned with higher
priority than consumers with a low rank. The
scheduling mechanism need to take care of the
scheduling based on subscription level and timing
priority. In addition, CPU execution budget and
network bandwidth must be reserved for differentiated
service. Overrun management can be installed to
ensure the proper sharing of system resource.
When composing a new real-time application in
RTSOA, even if the verified services are selected
based on the service requirement specified in the
application, the services may not collaborate with each
other correctly because of timing compatibility. Real-

time V&V is necessary to verify that the services
selected are compatible with respect to the timing
constraints.
In the application workflow specification, timing
constraints will be specified for all operations in the
services. The task scheduling and planning
mechanisms need to coordinate the collaborations
among the services to guarantee that the timing
constraints can be satisfied based on the service QoS
levels that this application has subscribed to.
RTSOA is different from current SOA. The realtime requirements for RTSOA bring unique
characteristics to the RTSOA framework. The
characteristics of RTSOA and the comparison with
current SOA is shown in Table 1.

Table 1: Comparison of Current SOA and RTSOA
Current SOA
RTSOA
Basic
SOA

Ontology

Discovery

SOA
Management

Message
Exchange
Deployment

SOAP as the message
exchanging protocol
General service deployment
after the application is
composed or recomposed

Orchestration

The service collaboration
needs to be orchestrated at
runtime
Policies need to be enforced
at the service runtime.
Service modeling for general
service information
Service composition based
on service ontology
Service verification and
validation are needed before
services are deployed.

Policy
ServiceOriented
System
Engineering

Service
ontology
and
collaboration ontology is
needed
for
service
composition
and
collaboration.
UDDI and ebXML as
registry for service discovery

Modeling
Composition
V&V

Simulation

Execution
Reconfiguration

Both online and offline
simulations
can
be
performed
for
service
behaviors evaluation.
After a composite service is
composed and evaluated, it
can be put into execution
Service reconfiguration is
needed when reliability or
performance is not low.

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

Ontology specification and matching will be done offline to
avoid the real-time issues. In addition to the general service
information, real-time properties of services need to be
included in the service ontology and collaboration ontology
Instead of the general service discovery in the current SOA,
RTSOA can discover and choose only those verified services
in the database to ensure real-time performance.
Real-time SOAP or similar protocols are needed for real-time
message exchange
Real-time deployment mechanism is needed for deploying
dynamically composed applications to the application host.
Reserved deployment bandwidth is necessary for timing
guarantee.
Real-time service orchestration is needed
Policy enforcement needs to be carried out in real time and at
runtime. Real-time system recovery on policy violation.
In addition to the general service modeling, real-time service
properties also need to be modeled.
Real-time service composition is required. It requires verified
services to be available.
Verification and validation are done offline before adding the
services into the verified service repository. RTV&V may be
needed in the case when verified services are not available.
RTV&V is also used for service timing compatibility
verification.
Verified service simulation is done offline before adding the
services into the verified service repository. Real-time online
simulation mechanism is needed for dynamically composed
application evaluation.
Real-time properties need to be enabled in the service
execution. Execution data will be collected for timing policy
enforcement.
Service reconfiguration needs to be carried out in real time to
satisfy the timing constraints.

4.

Real-time Composition and Optimization

4.1. Problem Definition
In SOA applications, the payment for
services is based on the usage. The same service
can be provided at different levels, in terms of
response time and cost. Hence, to select services
for application composition, there are two targets
to optimize: the total cost and the end-to-end
execution time. More concretely, one can
represent the optimization problem based on the
following constraints and assumptions:
• The application must complete one iteration in
MaxTime and can use up to a MaxCost budget;
• The application will call n services in a
sequence: S1, S2, ..., Sn;
• Each service can guarantee k levels of service
quality. For level l service Si, we use Ci (l) and
Ti (l) to represent the cost and response time of
service Si at level l. We assume service levels
are sorted in an ascending order of response
time (as well as a descending order of cost).
Thus, the level 1 has the shortest response time
and the highest cost;
• Given a composition {l(i), i = 1, 2, …, n} as
the service level we choose for service Si,
where 1≤l(i)≤ k, we define the end-to-end
response time and total application cost as:
Sum(T) = T1( l(1)) + T2( l(2))+ ... + Tn( l(n))
and
Sum(C)= C1( l(1)) + C2( l(2))+ ... + Cn( l(n))
We can have two optimization targets based
on the constraints and assumptions:
• Objective 1: Find the appropriate levels l(i) for
each service to get the minimal cost Sum(C) in
all combinations, under the constraint that the
end-to-end response time is less than the
MaxTime, i.e., Sum(T)≤ MaxTime;
• Objective 2: Find the appropriate levels l(i) for
each service to get the minimum execution
time Sum(T) in all combinations such that the
total application cost is bounded to MaxCost,
i.e., Sum(C)≤ MaxCost;
4.2. Algorithm Design
This section presents a greedy algorithm to
illustrate how to get the optimized result. We use
the objective 1 as the example in this section.
Similar algorithms can be applied to meet the
objective 2.
An exhaustive search can certainly find the
optimal composition by calculating the time and
cost for all possible combinations, i.e., for all
service levels and for each service. Some

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

reduction can be easily added, for instance, when
a composition of {l(i)} cannot meet the end-toend deadline constraint, there is no need to
consider a different composition {l′(i)} where
l′(i)≥ l(i)for all i. Since the number of
combinations is kn, the complexity of this
algorithm is exponential.
Obviously, this algorithm cannot be used in
the RTSOA. There exist algorithms to find the
best result under certain conditions. For example,
if the cost and response time functions are
treated as continuous and convex, Lagrange
multiplier can be introduced to find the minimal
value of a convex function. For real-time
composition, one may prefer a fast heuristic
algorithm that can result in a suboptimal solution
with a guaranteed computation time.
For each service Si, we assume the levels of
services are sorted by the response time in
ascending order. A heuristic search can start
from the solution of l(i)=1, i.e. all services are
done with the best quality and the maximal cost.
If the solution is feasible, we can reduce the cost
by lowering the quality of a service subject to the
end-to-end deadline constraint. The cost
reduction process first selects the service Sj that
gives the maximal marginal gain of
Ci (l (i )) − Ci (l (i ) + 1)
Ti (l (i ) + 1) − Ti (l (i ))
for all i = 1, 2, …, n. At the same time, the
change of service level of Sj from l(j) to l(j)+1
should not increase the end-to-end response time
beyond the required deadline. The process can be
repeated until no such a service can be found.
The algorithm is summarized as follows.
A heuristic search for optimal composition:
service set S={S1, S2, S3, ..., Sn};
l(i)=1, ∆Ti= Ti( l(i)+1)-Ti( l(i)), and
∆Ci= Ci( l(i))-Ci( l(i)+1), for i=1,2,…, n;
compute Sum(T)= T1( l(1)) + T2( l(2))+ ... + Tn( l(n));
create a sorted list of S according to the response time
in ascending order;
while ( the sorted list is not empty) {
remove Sk which is with the maximal ∆C/∆T from
the list;
if Sum(T)+ ∆Tk ≤ MaxTime then {
l(k)=l(k)+1;
recomputed ∆Ck and ∆Tk;
Sum(T)= Sum(T)+ ∆Tk ;
insert Sk back to the sorted list;
}
}

The heuristic search algorithm can be
implemented with a sort list of the marginal
gains of all services. The time complexity of this
algorithm is O(k*n*log(n)) where, k*n is the
maximal number of iterations of the reduction
process and O(log(n)) complexity is required to
maintain the sorted list in each iteration. The
efficacy of the algorithm is further validated by
the simulation in the next section.
5.

Simulation

The optimization algorithms are implemented
to evaluate the relationship between the response
time and the cost. In this evaluation, it is
assumed that five services are used in the
composition. Each service has 10 levels of
response time. The cost of each service depends
on its response time. The shorter the response
time, the higher the cost. A set of sample data is
given in Table 2, in which two formulas are used
to generate the data:
1. Cost = 100/(Response Time)
2. Cost = 100/(1+log(Response Time))
C/T

L1
L2
L3
L4
L5
L6
L7
L8
L9
L10

100 / 1
59 / 2
47 / 3
41 / 4
38 / 5
35 / 6
33 / 7
32 / 8
31 / 9
30 / 10

100 / 1
50 / 3
33 / 5
25 / 7
20 / 9
16 / 11
14 / 13
12 / 15
11 / 17
10 / 19

100 / 1
50 / 4
33 / 7
25 / 10
20 / 13
16 / 16
14 / 19
12 / 22
11 / 25
10 / 28

Minimal
Cost

201.48

2

20

10,10,

100 / 1

802.60

3

30

10,10,10,

59 / 3

8799.39

4

40

10, 10, 10, 10,

Execution
Time
(µsecond)

Selection of
Combination

47 / 5

93184.90

5

50

10, 10, 10, 10, 10,

41 / 7

984984.25

6

70

8, 8, 8, 8, 8, 10,

38 / 9

10691487.68

7

96

6, 6, 6, 8, 8, 8, 8,

35 / 11

112725981.00

8

124

6, 6, 6, 6, 6, 6, 6, 8,

33 / 13
32 / 15
31 / 17
30 / 19

In the exhaustive algorithm, all possible
combinations are evaluated to obtain the
minimum cost, under the constraint that the total
response time does not exceed the given max
response time. In this example, we set the
required max response time to 50. The
exhaustive algorithm finds the minimum cost is
116. One of the compositions is (S1,l(8), S2,l(7),
S3,l(6), S4,l(5), S5,l(6)). The response time for this
composition is exactly 50. It uses 89235.23
microseconds to find this optimal result.
On the other hand, we can fix the cost to
compute the best response time. If we set the
total cost allowed to 150, the same exhaustive
algorithm can find the best response time for the
same data, which is 34 seconds. The
combination that generates the best response

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

Table 3. Response time versus cost for the
Exhaustive Algorithm
Service
Number

Table 2. Response time versus cost
S1
S2
S3
S4
S5
100 / 1
50 / 2
33 / 3
25 / 4
20 / 5
16 / 6
14 / 7
12 / 8
11 / 9
10 / 10

time is (S1,l(6), S2,l(4), S3,l(4), S3,l(4), S4,l(4)). This
exhaustive algorithm uses 63040.87 microseconds to find this result.
The exhaustive algorithm can always find the
optimal solution, but its time complexity is
exponential. In the next experiment, we vary the
service number from 2 to 8, and each service
uses the same cost model. We select the simple
one Cost = 100/rtime. The execution time of the
algorithm for the exhaustive algorithm increases
significantly, as shown in Table 3. By giving the
same data samples to the heuristic algorithm, the
simulation results are listed in Table 4. Error!
Reference source not found. compares the
response times of the exhaustive and heuristic
algorithms. As can be seen, its performance does
not change significantly when increasing the
service number. The minimal costs found from
both algorithms are exactly the same, but the
combination results are different in some cases.

To fairly compare these two algorithms, we
randomly generate 15 sets of data samples.
These samples have different service numbers,
and the cost model of each service is randomly
picked up, as discussed in the previous section.
Although the service combinations found are
different, in all case the best results are found.
Table 4. Response time versus cost for the
heuristic Algorithm
Service
Number

Minimal
Cost

45.25

2

20

72.57

3

30

10,10,10

128.18

4

40

10, 10, 10, 10

157.37

5

50

10, 10, 10, 10, 10

188.65

6

70

10, 8, 8, 8, 8, 8

203.51

7

96

8, 8, 8, 8, 6, 6, 6

224.46

8

124

8, 6, 6, 6, 6, 6, 6, 8

Execution
Time
(µsecond)

Selection of
Combination
10,10

1000000000
100000000
Exhaustive
Algorithm

10000000
Response Time

1000000
100000
10000

Heuristic Algorithm

1000
100
10
1

2

3

4
5
6
7
Number of Services

8

Figure 5. Execution time comparison

6.

Summary

This paper presented a framework for
RTSOA and various design considerations to
achieve real-time communication, processing,
and composition. The key issues include a preverified repository of services, reserved
bandwidth, computing capacity, and trade-off
between performance and cost. An efficient
algorithm is developed to find the optimal cost
for a given response time. The same algorithm
can be used to find the best response time for a
given cost. Its execution time and the capacity of
finding the best solution are evaluated and
compared with an exhaustive algorithm.
References
[1] D. F. Bacon, P. Cheng, and V. T. Rajan. ‘‘A
Real-Time Garbage Collector with Low Overhead and
Consistent Utilization,’’ ACM SIGPLAN Notices,
38(1):285---298, Jan. 2003.
[2] S.
Bodhare,
‘‘Optimizing
Service
Infrastructures’’,
http://blogs.ittoolbox.com/eai/
optimization/archives/optimizing-serviceinfrastructures-3928
[3] D. Chappel, Enterprise Service Bus: Theory in
Practice, O’reilly Media, 2004.
[4] Y. Chen and W.T. Tsai, Introduction to
Programming Languages: Programming in C, C++,
Scheme, Prolog, C#, and SOA, second edition,
Kendall/Hunt Publishing, 2006.
[5] CISCO SYSTEMS, ‘‘CISCO’s relationship with
TIBCO makes the real-time business more visible and
secure’’, http://www.cisco.com/application/pdf/en/us/
guest/products/ps6438/c1625/cdccont_0900aecd802f5
ebf.pdf
[6] DARPA: OWL-S: www.daml.org/services/owl-s/
[7] R. van Engelen, “An XML Web Services
Development Environment for Embedded Devices,”
http://www.cs.fsu.edu/~engelen/cases03.html
[8] X. Fu, T. Bultan, and J. Su, “Formal Verification
of E-Services and Workflows,” Proc. Workshop on
Web Services, E-Business, and the Semantic Web,
LNCS 2512, Springer-Verlag, 2002, pp. 188–202.
[9] O. Goh, Y.-H. Lee, Z. Kaakani, and E. Rachlin.

Proceedings of the Second IEEE International
Symposium on Service-Oriented System Engineering (SOSE'06)
0-7695-2726-4/06 $20.00 © 2006

‘‘A Schedulable Garbage Collection for Embedded
Applications in CLI,’’ The 11th International
Conference on Real-Time and Embedded Computing
Systems and Applications (RTCSA05), July 2005.
[10] J. Helander and S. B. Sigurdsson, “Self-Tuning
Planned Actions Time to Make Real-Time SOAP
Real,” The 8th IEEE International Symposium on
Object-Oriented Real-Time Distributed Computing
(ISORC'05), pp. 80-89.
[11] IBM, "IBM SOA Foundation: An Architectural
Introduction
and
Overview",
http://download.boulder.ibm.com/ibmdl/pub/software/
dw/webservices/ws-soa-whitepaper.pdf.
[12] Intel:
Service-Oriented
Enterprise,
The
Technology Path to Business Transformation.
http://www.intel.com/ business/bss/technologies/soe/
soe_backgrounder.pdf
[13] N. Kavantzas, D. Burdett, T. Fletcher, and Y.
Lafon, “Web Services Choreography Description
Language”, Version 1.0, W3C Working Draft 17 Dec.
2004. http:// www.w3.org/TR/ws-cdl-10/
[14] A. Kazi, “Enabling Real-Time Business Through
Service-Oriented&Event-Driven Architecture”, http://
www.bijonline.com/index.cfm?section=article&aid
=19
[15] D. Kim, Y. H, Lee, M. F. Younis, “SPIRITµKernel for Strongly Partitioned Real-Time Systems,”
The 6th International Conference on Real-Time and
Embedded Computing Systems and Applications
(RTCSA 2000), pp. 73-80
[16] C. Kohlhoff and R. Steele, "Evaluating SOAP for
High Performance Business Applications: Real-Time
Trading Systems", Proc. of WWW'03, 2003.
[17] Frank Leymann, Web Services Flow Language,
Version 1.0, May 2001. http://www-306.ibm.com/
software/ solutions/webservices/pdf/WSFL.pdf
[18] OASIS: ebXML: http://www.ebxml.org/
[19] OASIS: Business Process Execution Language
for Web Services (BPEL4WS), 2003. http://
xml.coverpages.org/ bpel4ws.html
[20] R. Paul, “DoD Towards Software Services”,
Tenth IEEE International Workshop on Objectoriented Real-time Dependable Systems (WORDS 05),
February 2005, pp. 3-6.
[21] M. P. Singh, M. N. Huhns, Service-Oriented
Computing, John Wiley & Sons, 2005.
[22] W.T. Tsai, Ray A. Paul, Bingnan Xiao, Zhibin
Cao, Yinong Chen, "PSML-S: A Process Specification
and Modeling Language for Service Oriented
Computing", The 9th IASTED International
Conference on Software Engineering and Applications
(SEA), Phoenix, November 2005, pp. 160-167.
[23] W.T. Tsai, C. Fan, Y. Chen, and R. Paul,
“DDSOS: A Dynamic Distributed Service-Oriented
Simulation Framework”, in Proceedings of 39th
Annual Simulation Symposium (ANSS), Huntsville,
AL, April 2006, pp. 160-167.
[24] O. Zimmermann, S. Milinski, M. Craes, F. and
Oellermann, "Second Generation Web ServicesOriented Architecture in Production in the Finance
Industry", OOPSLA’04, Oct. Vancouver, 2004.

Annual IEEE International Computer Software and Applications Conference

A Systematic Approach for Integrating Fault Trees into System Statecharts
Omar El Ariss1, Dianxiang Xu1, W. Eric Wong2, Yuting Chen2, and Yann-Hang Lee3
1

Department of Computer Science, North Dakota State University
{omar.elariss, dianxiang.xu}@ndsu.edu
2
Department of Computer Science, University of Texas at Dallas
{ewong, yxc079200}@utdallas.edu
3
Department of Computer Science and Engineering, Arizona State University
yhlee@asu.edu
analysis is transformed into safety requirements
through the integration with the statechart. In this way
fault-based testing on a system could be conducted
through deriving test cases from the safety
requirements. This ensures that all the conditions that
participate in system failures, which are covered by
the integrated fault trees, will be covered at least once
by the test cases.
The integration of fault trees into the system
statechart is not a straightforward process and brings
with it some challenges. First, fault trees and
statecharts are heterogeneous models where
mismatches might occur due to missing (or additional)
functionalities between the models or due to different
naming conventions. Second, the evaluation and
interpretation of fault tree leaf nodes can denote state
transition, occurrence, bounded state etc. The
interpretation can also be ambiguous (does gas leak
means the gas valve cannot close or the valve is
functioning normally but there is excess of gas
because the valve was left open). Fault trees can have
different interpretations (like gas leaks > 4s means it
should continuously leak for 4 seconds or it could leak
discontinuously). In addition, the interpretation is
subjective (can differ from one person’s interpretation
to another).
The proposed integration method for fault tree
analysis tries to take into consideration these problems
and to deal with them by using a systematic set of
transformation rules that integrates both models (fault
tree and system specifications) into a common model
(statechart). This transformation to statechart
representation tries to maintain the structure of the
fault tree analysis as much as possible so that it is
amenable to look at the statechart and clearly identify
the components that represent the fault tree. In
addition, the method allows to check whether there are
some events and transitions in the fault tree that are
already represented by the system statechart, and be
able to identify which of those components should be
briefly modified and which components can be left as
they are without being wrongly represented twice.

Abstract
As software systems are encompassing a wide
range of fields and applications, software reliability
becomes a crucial step. The need for safety analysis
and test cases that have high probability to uncover
plausible faults are necessities in proving software
quality. System models that represent only the
operational behavioral of a system are incomplete
sources for deriving test cases and performing safety
analysis before the implementation process. Therefore,
a system model that encompasses faults is required.
This paper presents a technique that formalizes a
safety model through the incorporation of faults with
system specifications. The technique focuses on
introducing semantic faults through the integration of
fault trees with system specifications or statechart.
The method uses a set of systematic transformation
rules that tries to maintain the semantics of both fault
trees and statechart representations during the
transformation of fault trees into statechart notations.

1. Introduction
Validation and verification of a system is an
important and crucial step in software quality
assurance. As software testing plays an extremely
important role in system validation and verification,
testing strategies and test cases should be as
comprehensive as possible in order to deliver high
confidence in the software. Fault trees [1] are an
engineering practice that is commonly used during the
analysis of faults in critical systems. Those tree
notations describe how certain behavior of system
components can combine to result in a system hazard
or catastrophe. On the other hand, specification model
usually represent only the operational or correct
behavior of the system. Therefore, fault tree analysis is
an excellent candidate to fill in the fault coverage gap
by the specification or system model. The addition of
fault tree analysis to this specification model will
result in a system model that takes into consideration
faults or undesirable conditions. Therefore, safety

0730-3157/08 $25.00 © 2008 IEEE
DOI 10.1109/COMPSAC.2008.19

120

The remainder of the paper is organized as follows.
Section 2 gives some background and discusses
related work done on this topic. In section 3, the
conversion rules are introduced while in section 4 the
transformation steps are discussed. We conclude in
section 6.

easily dropped and replaced by a fault tree that shows
the results of the analysis.
The second difference is in the way the integration
to statechart is done. [2, 8, 9] base their integration on
minimal cut sets, where each cut set at a time is
integrated to the statechart. The assignment of a
meaning (a duration calculus formula) for the leaf
nodes in the cut set is done, and is not shown how,
before the conversion rules are applied. Determining
whether a leaf node is an event, a state, a state
transition, or a bounded state is not a trivial step. This
process should check with the system statechart in
order to identify whether this leaf node can be
represented by the system or should be considered as a
safety assumption. Therefore, it is better for this step
to be done during the integration with the statechart,
where a leaf node can be easily identified whether it
can be represented by the system or not. Our proposed
method takes this issue into consideration and directly
integrates each gate at a time with its inputs to
statechart notations without first assigning formulas
for each leaf node. The last difference between the
proposed method and [2, 8, 9] is that their method
does not take time into consideration, where
conversion rules for leaf nodes that are bounded in
time are not represented.

2. Background and Related Work
Early work done on fault-based testing focused
mainly on syntactic errors; papers [2, 3] give a
comprehensive coverage on the research done on this
topic. This type of fault-based testing neglects
semantic errors which are harder or more complex to
identify, analyze and correct. In [4], fault tree analysis
is applied for the first time in software safety analysis,
while in [5] different approaches are discussed and
compared.
In [6], the paper proposes direct interpretation of
software requirements from fault trees by using a
common semantic model for both safety analysis and
software requirements specifications. This common
model is constructed through the use of duration
calculus [7], which is a real time interval logic.
Through this common model the deduction of safety
requirements is done through considering every fault
tree on its own, then making sure that the root node
(duration calculus formula) does not occur. Derivation
steps for every gate (AND, OR and Priority AND) are
given because requirements deduction from the
duration calculus model differs from one gate to
another. [2, 8, 9] propose a fault-based approach for
generating test cases. The analysis of fault trees is
represented as duration calculus formulas, which is
composed of a collection of cut sets. A cut set stands
for the set of basic events that trigger a failure if they
occur simultaneously. Each cut set formula is then
integrated with the statechart system model through
the application of a set of conversion rules.
Our proposed method differs from previous
research [2, 6, 8, 9] done on integration of fault trees
into statechart, and can be seen from three aspects.
The first difference is in the representation of the
semantics of fault trees. All previous methods
represent the analysis of fault trees in a separate model
that is based on duration calculus. The usage of an
intermediary model introduces time overhead, might
restrict or modify the initial meaning of the
representation, and make the automation process (if
possible) harder. The proposed method use conversion
steps that directly transform every gate and its inputs
into statechart notations. A Boolean expression
formula that represents the analysis results is only
used as guidance for the conversion steps and can be

3. Conversion Rules
In the following subsections, rules to convert the
AND, OR, XOR, Priority AND gates, and elapse of
time into statechart notation will be introduced. The
inhibit gate is not considered because it can be
represented as a two input AND gate. The NOT gate is
also not considered because it introduces non coherent
trees, thus increasing the analysis complexity [9]. The
statechart representation used here is based on [10].

3.1 Representing the AND Gate
The inputs of an AND gate can either be
represented as a set of transition states (method 1), or
as a set of orthogonal states (method 2). As the
number of inputs of the AND gate increases, the
representation of the AND gate using method 1
becomes harder and even impossible to use. Method 2,
on the contrary, has a degree of complexity that does
not depend on the number of inputs of the AND gate.
In addition, method 1 doesn’t have a general or fixed
form of representation as method 2, so as the number
of inputs changes, the representation of method 1
changes. For an example an AND gate with 2 inputs
can be represented by either using method 1 (figure 1)
or method 2 (figure 2).

121

B

A

A

B

A

The representation of the XOR gate differs from
the OR gate representation only when A and B occur
at the same time making a transition back to the superstate. Therefore, the new state should be now formed
into a new state having two sub-states as shown in
figure 4.

B

0
Faults

Incr
Decr

1
Fault

Incr

Figure 1. Method 1
Figure 2. Method 2
From figure 2, it can be seen that for every gate
input an orthogonal state is formed, as indicated by
method 2. The event Incr indicates the transition to a
state where one of the inputs (either A or B) is now
present. While a Decr event indicates the loss of
availability of one of these inputs. So for an AND gate
with two inputs, two consecutive Incr events are
needed for the gate to occur. The statechart notation is
divided into three orthogonal components. The top
two components represent the gate inputs while the
bottom component represent the AND gate combining
all the standalone inputs. This state shows the
interaction of the inputs together, so when one out of
two of the inputs have occurred then the current substate will be in “1 Fault”.

3.4 Representing the PRIORITY AND Gate
The priority AND is represented, as shown in
figure 5, as a set of consecutive states and transitions.
Here the order of the occurrence of the states is
important and should not be violated.

Figure 5. A priority AND gate statechart notation

3.5 Representing an elapse of time
An elapse of time or a threshold of some duration
can be represented, as figure 6 shows, using a bounded
state. When the bounded time bound is reached, a
timeout transition to the new state happens.

3.2 Representing the OR Gate

< bound

An OR gate of two inputs can be seen as a
transition of one state (initial state) to another state if
either A, B, or both A and B occurs. Depending on
what A and B means, the statechart might differ, but a
general definition is given in figure 3. The initial state
will include the steps (depending on the definition of
A) to reach A. The initial state plus the integration of
A can be seen in figure 3 as the super-state. If A
happens then a transition from the super-state to the
new state will occur. If B occurs at any time, an
immediate transition from any current sub-state in the
super-state to the new state will happen. Depending on
the definition of B, and if it doesn’t stand for a
transition but something else, figure 3 will not change
but only an orthogonal state representing B will be
added. The occurrence of both A and B together is not
represented because no additional information is
added. This model can be generalized to represent OR
gates with more than 2 inputs.

Initial
state

Event B
New
state
A
The super-state

4. Transformation Steps:
Step 1- Deduce a safety requirement formula: The
semantics of a fault tree can be deduced from the
semantics of the root node. While the semantics of the
root node is defined from the semantics of the
intermediate nodes, gates and edges. Intermediate
nodes are defined by the semantics of the leaf nodes,
edges, and gates in the sub-trees in which the
intermediate nodes are roots. Therefore, the semantics
of a fault tree is only defined from the semantics of the
edges, gates and leaf nodes. The result of fault tree
semantics can either be represented by a Boolean
expression formula or by a new constructed fault tree
that only shows the root node (the catastrophe), gates,
and the leaf nodes that are responsible for the
occurrence of this catastrophe.
Step 2- Evaluate every leaf node in the formula:
- Check if the leaf node has a simple definition or if it
is composed of more than one simple definition. If
the leaf node does not have a simple definition, then
decompose it into simple definitions. Where every
simple definition leaf node can be:
o true or false constants
o an occurrence of a state

Figure 3. An OR gate

B
A
A
B

New
state

Figure 6. An elapse of time Statechart notation

3.3 Representing the XOR Gate

Initial
state

timeout

New state

The super-state

Figure 4. XOR gate

122

o an occurrence of an event: a transition from one
state to another
o an elapse of time
o a threshold of some duration
- Modify the safety requirement formula (deduced in
step 1) to reflect the changes made
Step 3- Deduce the Semantics table from the
statechart and the fault tree. This step will help in
identifying existing parts (if there are any) that are
already present in the statechart. The semantics table is
a table that shows for every leaf node in the safety
requirement formula (from step 2) the equivalent
interpretation, if available, of the statechart. This is
done through the following steps:
- Find the component or part of the statechart that has
the functionality of the leaf node.
- Deduce the set of transitions from the statechart
component.
- Check if the semantics of the leaf node has a similar
or exact meaning to the set of transition. If a
similarity is found, then the statechart transition and
the leaf node are added to the table. If no similarity
is found, then only the leaf node is added to the
table
without
any
equivalent
statechart
representation.
Step 4- Construction Step:
- Requirements (or leaf nodes) that are not
implemented by the system should be included as
Assumptions. Requirements that are implemented
should be included as Safety Commitments. Then,
Safety Requirements = Assumptions => Safety
Commitments
- If the inputs of the operand have an equivalent
statechart representation in the semantics table, then
use these equivalent statechart representations
during the construction (the steps below) of the
statechart notation for the operand.
- Construct the new statechart by working on the
formula from left to right starting with the first
operand then the next operand, and so on. When the
operand is located, work on the operand and its
inputs according to the conversion rule that
represents the gate as a statechart. Work recursively
if the inputs contain any other operands.
- Add the new representations of the fault tree as
orthogonal states to the original statechart.
- Modify the main state of the system’s statechart by
adding a transition, where the event is the same
action that is produced by the statechart
representation of the fault tree.

modified statechart (integrated model) will show how
the system behaves when an error occurs and will act
as the basis model in order to derive fault-based test
cases. The motivation for this work is due to the
importance of incorporating failures into the system
specifications and to improve on the limitations and
ambiguities of previous proposed methods. The
current approach improves on previous work done by
directly transforming fault tree notations into
statechart representations and by incorporating time
through the usage of bounded states. The intention of
this work was to maintain as much as possible the
semantics of both models without sacrificing one
semantic in order for it to be integrated into the other
model.
Although the set of steps and conversion rules
proposed are intended to be general and applicable to
most critical systems, the need for special
modifications and for subjective interpretation is still
needed in certain cases. Future work will concentrate
on deriving fault-based test cases from the system
statechart which was integrated with failures. In
addition, further work might focus on adding
probabilistic features to the system specifications
through the use of the probabilities in the fault trees.

6. References
[1] U.S. Nuclear Regulatory Commission, Fault Tree
Handbook. NUREG-0492, Washington, D.C., Jan. 1981.
[2] M. A. Sanchez, and M. A. Felder, “A Systematic
Approach to Generate Test Cases based on Faults.”,
ASSE2003, Buenos Aires, 2003.
[3] T. Y. Chen, D. D. Grant, M. F. Lau, S. P. Ng, and V. R.
Vasa, “BEAT: A web-based Boolean expression fault-based
test case generation tool”, IJDET , 2006, pp. 44-56.
[4] N.G. Leveson, and P.R. Harvey, “Analyzing Software
Safety”, IEEE Transactions on Software Engineering, vol. 9,
no. 5, 1983.
[5] N. Leveson, “Software Safety: Why, What and How”,
Computing Surveys, vol. 18, No. 2, 1986, pp. 125-163.
[6] K. M. Hansen, A. P. Ravn, and V. Stavridou, “From
Safety Analysis to Software Requirements”, IEEE
Transactions on Software Engineering, vol. 24, no. 7, 1998.
[7] C. Zhou, C. Hoare, and A.P. Ravn, “A Calculus of
Durations”, Information Proc. Letters, vol. 40, no. 5, 1991,
pp. 269-276.
[8] M. Sanchez, J. Augusto, and M. Felder, “Fault-Based
Testing of E-Commerce Applications”, VVEIS2004,
Portugal, 2004, pp. 66-71.
[9] Dasso, A., and A., Funes, Verification, Validation and
Testing in Software Engineering, Idea Group Publishing,
2007.
[10] D. Harel, “Statecharts: A visual formalism for complex
systems”, Science of Computer Programming, vol. 8, 1987,
pp. 231-274.

5. Conclusions

In this paper, a new approach that integrates fault
trees into system statechart is introduced. This

123

Parallel Fair Round Robin Scheduling
in WDM Packet Switching Networks
Deming Liu1 , Yann-Hang Lee1 , and Yoonmee Doh2
1

Department of Computer Science and Engineering, Arizona State University
Tempe, AZ 85287, USA
{dmliu,yhlee}@asu.edu
2
Information and Communications University
Munji-dong, Yuseong-gu, Daejeon, 305-714, South Korea
ydoh@icu.ac.kr

Abstract. Assuming data traﬃc of ﬁxed-length cells, we propose a
frame-oriented scheduling discipline, PFRR (parallel fair round robin),
for WDM optical switches by applying pfair (proportionate fairness)
scheduling so that deterministic communication performance is guaranteed. Bandwidth reservation for an active session is performed by holding
a number of cell slots for the session in a two-dimension frame, which
is transferred iteratively over the multiple channels in an optical ﬁber.
To determine the transmission order of cells in a frame, pfair scheduling
is used so that the cells belonging to a session are distributed over the
frame as uniformly as possible. Through the analysis by network calculus
and by network simulator, it is shown that PFRR possesses tight delay
bounds and lenient buﬀer requirements. Also, with a minor modiﬁcation
to PFRR, a new service discipline called MPFRR is proposed that can
be used as traﬃc regulator to support sessions with jitter requirements.

1

Introduction

Bringing WDM (wavelength division multiplexing)to packet switching networks
results in new constraints on packet scheduling algorithms, especially in the
cases of real-time applications in that, at a switch, multiple sessions compete for
multiple physical channels working parallel through an optical ﬁber. Thus, not
only must a scheduling algorithm determine the order of packet transmission,
but also select the channels that packets should be routed. The lack of packet
scheduling algorithms supporting hard real-time services over WDM optical networks probably results from the intractability of most multi-resource scheduling
problems since the two classes of problems are almost equivalent.
Even in switching networks without WDM, adding parallel links is more costeﬃcient than using a high-bandwidth single link to increase the bandwidth of an
existing link between two switch nodes [3]. We use session constraint to refer to
the requirement that packets from a session cannot be transferred on multiple
physical channels concurrently. Anderson et al. suggested two reasons for which
session constraint is preferred [3]. (i) Session constraint avoids the necessity to
H.-K. Kahng and S. Goto (Eds.): ICOIN 2004, LNCS 3090, pp. 503–513, 2004.
c Springer-Verlag Berlin Heidelberg 2004


504

Deming Liu et al.

include a sequence number in each of the parallel transferred packets and sort
these packets at the receiving switch. (ii) Session constraint is compatible with
currently deﬁned standards such as ATM. We give the third advantage of session
constraint from the viewpoint of switch implementation. If parallel transfer of
packets of a session is allowed, high rate buﬀer, diﬃcult for implementation, is
required for the session so that parallel arriving packets from the session can
be put into the corresponding queue. Since we can regard a wavelength channel
inside a WDM optical ﬁber as a processor, and a session as a task, multiprocessor scheduling algorithms are applicable for packet scheduling in WDM optical
networks [4].
To provide a fair access to resources in multiprocessor system and QoS service
guarantee, the concept of pfair (proportionate fairness) scheduling can be applied. Basing on slotted time and assuming that all input parameters, including
task period, execution time, and release time, are integers, Baruah et al. [5][6]
proposed pfair scheduling which can not only guarantee task deadlines to be respected, but also make tasks execute at steady progressive rates. Making use of
pfair scheduling, we will propose a frame-based service discipline, PFRR (parallel fair round robin), which extends the result of single channel switching in [2] to
the parallel switching scenario. Similar to EDF-RR (earliest deadline ﬁrst round
robin) algorithm in [2], PFRR takes the advantage of low on-line computational
complexity by assuming that setting up a frame is a infrequent event, and possesses tight delay bounds and lenient buﬀer requirements. The rest of this paper
is organized as follows. Section 2 describes the PFRR service discipline. Section
3 gives frame algorithms and their computational complexity. In Section 4, delay
bound and buﬀer requirement are analyzed by applying network calculus. Section 5 includes some simulation results. In Section 6, some application-related
issues are addressed. Finally conclusions are given in Section 7.

2

Parallel Fair Round Robin Scheduling

To describe PFRR, we need some deﬁnitions of pfair scheduling in the parallel
packet switching domain, similar to what Baruah et al. used in the multiprocessor
scheduling domain [5].
• Scheduling decisions occur at integral values of time, numbered from 1. The
real interval between time t -1 and t (including t -1 and excluding t ) will be
referred as slot t, t ∈ N, where N denotes {1, 2, ...}. Time instant 0 is the
beginning of a backlog period during which some sessions are backlogged.
• We consider an instance Φ of the fair sharing problem with m channels of
identical bandwidth and n backlogged sessions over a frame over L time
slots. Without losing generality, we assume n ≥ m. Speciﬁc sessions will be
denoted by identiﬁers x and y, which range over T, the set of all sessions.
A time slot is referred to a time interval in the general meaning. Without
confusion, we also use time slot (or slot) of a frame to refer to a column of
the frame and cell slot to an element in the frame. Thus, a time slot contains

Parallel Fair Round Robin Scheduling in WDM Packet Switching Networks

505

m cell slots. Since there are m parallel channels, a frame has two dimensions,
m×L, and contains mL cell slots.
• Data traﬃc consists of ﬁxed-length cells. The amount of traﬃc that a cell
carries is 1 unit. The amount of traﬃc that is transferred in a time slot for
a channel is a cell.
• A session x has a reservation demand ex deﬁned as the number of cell slots
in the frame that are held for transferring cells from session x. We assume
that 0 < ex ≤ L and deﬁne the weight (or utilization) wx of session x as
wx = ex /L. Note that 0 < wx ≤ 1 and wx is a rational number.
 Without
losing 
generality, we conﬁne our investigation to the case that x∈T wx =
m. If x∈T wx < m , we can add one or several dummy sessions to make

x∈T wx = m. Dummy sessions are always idle.
With respect to instance Φ of the resource sharing problem, let earliest (x, i)
(latest(x, i)) denote the earliest (latest) slot during which the ith cell of session
x gets serviced.
earliest(x, i) = (i − 1)/wx  + 1.
(1)
latest(x, i) = i/wx 

(2)

A schedule is pfair if and only if the ith cell of session x gets serviced in
[earliest (x, i), latest(x, i)] for all i ∈ N. Note that, for a pfair schedule, earliest (x,
i) < latest(x, i), x ∈ T, i ∈ N. Furthermore, earliest (x, i+1) - latest(x, i) is
either 0 or 1. In other words, there is at most one slot in which both the ith cell
and the (i+1)th cell of x can get serviced. It is obvious that if we can guarantee
pfair over a frame, then pfair over inﬁnite time can be guaranteed by repeating
the pfair frame every L slots.
PFRR Discipline
a. Forming a pfair m×L frame F respecting both session constraints and allocation constraints by using the algorithms in Section 3 so that each row of F
corresponds to a channel and each element of F represents a cell slot assigned
to a session.
b. If all sessions are backlogged, the cells are transferred frame by frame through
the m channels such that each frame is identical to frame F.
c. If not all sessions are backlogged in slot t, two cases are addressed as follows.
c.1. If the m sessions in a time slot t of F that are receiving service are all
idle, then time slot t is skipped such that the sessions in time slot t +1
of F get serviced immediately.
c.2. Let Ridle (t) denote the set of idle channels in a time slot t of F and
∗
Tbusy
(t) the set of backlogged sessions that do not get serviced in t. Let
∗
q = min{|Ridle (t)|, |Tbusy
(t)|}. If some of the m sessions in the time slot
∗
(t) to ﬁll q idle channels
t of F are idle, select q sessions from Tbusy
in Ridle (t). Then the cells in time slot t are transferred except for the
ones that violate allocation constraints.

506

Deming Liu et al.

With the pfair scheduling in a frame, if all sessions are backlogged, the number of cells scheduled for a session x of instance Φ between time 0 and an integral
time instant t is either wx t or wx t by the deﬁnition of pfair [5]. Thus step
b of PFRR guarantees deterministic performance of the scheduling. Step c of
PFRR tries to make the scheduler work-conserving. Let Sx (a, b) be the number
of cells transferred in the interval [a, b] for session x under PFRR. The following
theorem gives the minimal communication capacity guaranteed for each session.
Theorem 1. In any busy interval of session x scheduled with PFRR, we always
have
Sx (0, t2 ) − Sx (0, t1 ) > wx (t2 − t1 ) − 2
(3)
where the busy interval of session x begins at time 0 and, t1 and t2 are any two
time instants in the busy interval with t2 ≥ t1 .

3

Pfair Frame Scheduling

Baruah et al. gave an on-line pfair scheduling algorithm PD (pseudo-deadline)
with running time of O(min{mlogn, n}) per time slot for periodic tasks over
identical processors [7] in which deadlines are compared and ties are broken in
constant time by inspecting 4 diﬀerent parameters. Basing on Baruah et al’s
work, Anderson et al. reduced the number of tie-breaking inspections to 2 in the
algorithm PD2 [8][11]. Applying Anderson’s multiprocessor algorithm, we can
obtain a pfair frame for instance Φ of cell scheduling problem. Related deﬁnitions
are given as follows.
• A session with weight less than 1/2 is called a light session; a session with
weight at least 1/2 is called a heavy session.
• Each cell to be scheduled in the frame is attached a pseudo-release and
a pseudo-deadline. Let c(x, i) denotes the ith cell of session x, i ∈ N and
r(c(x, i)) (d(c(x, i))) the pseudo-release (pseudo-deadline), then,
r(c(x, i)) = (i − 1)/wx  + 1.

(4)

d(c(x, i)) = i/wx .

(5)

The interval [r(c(x, i)), d(c(x, i))] is called the window of c(x, i) denoted by
win(c(x, i)). The size of win(c(x, i)) (the number of time slots over which
win(c(x, i)) spans) is denoted by |win(c(x, i))|.
• According to the deﬁnition of pfair, r(c(x, i + 1)) is either d(c(x, i)) or
d(c(x, i)) + 1. Then b(c(x, i)) is deﬁned to distinguish these two possibilities. b(c(x, i)) = 1 if r(c(x, i + 1)) = d(c(x, i)), b(c(x, i)) = 0, otherwise.
• Consider a sequence c(x, i), . . . , c(x, j) of cells of a heavy session x such that
|win(c(x, k))| = 2 ∩ b(c(x, k)) = 1 for all i < k ≤ j and either |win(c(x, j+1))|
= 3 or |win(c(x, j + 1))| = 2 ∩ b(c(x, j + 1)) = 0. Then we deﬁne d(c(x, j)) to
be the group deadline for the group of cells c(x, i), . . . , c(x, j). We also deﬁne

Parallel Fair Round Robin Scheduling in WDM Packet Switching Networks

507

that cells belonging to light session have group deadline 0. Accordingly, cells
of heavy sessions always have larger group deadline than the cells of light
sessions. Every cell c(x, l) has a group deadline that is denoted by D(c(x, l)).
The pfair frame algorithm is a priority-based algorithm. Session x s priority
at time slot t of the frame is deﬁned to be (d(c(x, ix)), b(c(x, ix)), D(c(x, ix))),
where ix is the number such that ix -1 cells already are scheduled for session x
before time slot t. Session priorities are ordered according to the principle that
(d , b , D ) ≤ (d, b, D) if and only if
(d < d ) ∪ ((d = d ) ∩ (b > b )) ∪ ((d = d ) ∩ (b = b ) ∩ (D ≥ D ))

(6)

At each time slot t, the m sessions that have the highest priorities are selected
(ties are broken arbitrarily) so that exactly one cell from each of the m sessions
is assigned to time slot t.
Anderson et al [8] proved that the aforementioned algorithm gives a pfair
schedule. Since there are L time slots in the frame and the algorithm has the
time complexity of O(min{mlogn, n}) per time slot, then the time complexity
of the frame algorithm is O(min{mLlogn, nL}) per frame. Even though getting
a fair frame takes time as much as O(min{mLlogn, nL}), a fair frame can be
used repeatedly once it is determined. Therefore from the round robin property
of PFRR, the online operation is just a table lookup if we ignore the overhead
of checking whether sessions are idle. Determining a pfair frame is only needed
when there are sessions that start up or terminate. These operations are regarded
as infrequent events in practice.

4

Delay Bound and Buﬀer Requirement Analysis

In this section, we will use network calculus [9][10], a mathematic analysis tool
for networks, to obtain delay bounds and buﬀer requirements of PFRR for both
single-node and multiple-node cases. Also an intuitive explanation is given for
the analysis results.
To analyze delay bounds and buﬀer requirements, traﬃc models must be
established to specify session traﬃc characteristics such as average rate and
burstiness. A bursty traﬃc model of (σ, ρ) is one of them [1]. A session traﬃc
ﬂow is said to satisfy (σ, ρ) model if there are at most σ+ρt units of traﬃc during
any time interval t. σ and ρ denote the burstiness and average rate of the traﬃc
respectively. For example, traﬃc ﬂow coming from the traﬃc regulator of leaky
bucket satisﬁes (σ, ρ) model. According to the deﬁnition of arrival curve, the
statement that a traﬃc ﬂow satisﬁes (σ, ρ) model is equivalent to that the traﬃc
ﬂow is constrained by arrival curve σ + ρt. In this paper we assume session traﬃc
has arrival curve σ + ρt and traﬃc unit is ﬁxed-length cell. In the following text
we do not distinguish server and switch by the convention of network calculus
literature. The results in this section are obtained by applying Theorem 1 and
network calculus. Please refer to [9][10] for the concepts and results of network
calculus.

508

Deming Liu et al.

Lemma 1. If session x passes through a PFRR server, the PFRR server oﬀers
the service curve wx t − 2 for session x.
In the single-node case in which a session traﬃc ﬂow goes through a PFRR
server, we have Theorems 2 and 3 addressing the delay bound and the buﬀer
requirement respectively.
Theorem 2. If the session x traﬃc ﬂow is constrained by arrival curve σx +
wx t, the delay it experiences passing through a PFRR server is not more than
(σx + 2)/wx time slots.
Theorem 3. If the session x traﬃc ﬂow constrained by arrival curve σx + wx t
passes through a PFRR server without buﬀer overﬂow, the buﬀer size that the
server needs for session x is not more than σx +2 cells.
Now we consider the multiple-node case in which a session traﬃc ﬂow traverses multiple PFRR servers. We deﬁne the minimum weight server for session
x is the PFRR server which has the minimum weight among the sequence of
PFRR servers that session x traverses. Lemma 2, Theorems 4 and 5 as follows
are based on the assumption that session x passes through a number of PFRR
servers without any buﬀer overﬂow and the session x traﬃc ﬂow is constrained
by arrival curve wx∗ t + σx , where wx∗ is x s weight at its minimum weight server.
Lemma 2. The output ﬂow from the kth sever for session x is constrained by
arrival curve wx∗ t + ρx + 2k.
Theorem 4. The delay that the traﬃc ﬂow of session x experiences from the
source to the kth server is not more than (σx + 2k)/wx∗ time slots.
Theorem 5. The buﬀer size needed by the kth server for session x is not more
than (σx + 2k) cells.
Apparently if we ignore the computation for judging whether a session buﬀer
is empty or not, PFRR has the computational complexity of O(1). On the other
hand, it is necessary to have buﬀer empty checking taken into account from
the point of view of implementation. The simple way to avoid this overhead
is to prevent the empty cell slots of the idle sessions from being occupied by
other backlogged sessions. But this strategy would waste the expensive resource,
bandwidth. In the more eﬃcient method, those empty cell slots can be used
to transfer traﬃc from backlogged best-eﬀort sessions that do not reserve any
cell slot in the current time slot. If there do not exist best-eﬀort sessions, a set
of sessions with no reservation on the current time slot can be assumed to be
nonempty by some heuristic approaches. Then if some of the found sessions are
really nonempty, they will use those empty cell slots. Otherwise these empty slots
will be discarded. No mater the empty slots are employed or not, the worst-case
performance of real-time sessions will not be impaired.

Parallel Fair Round Robin Scheduling in WDM Packet Switching Networks

509

Fig. 1. The network topology for the simulation

5

Simulation Results

In this section, we will study PFRR’s performance by simulation. We will compare PFRR with another frame-based service discipline for multichannel switching that we call PWRR (parallel weighted round robin).
Given the cell scheduling instance Φ, an m×L frame is constructed in the
following way according to PWRR. The n sessions are scheduled one by one
such that the frame is traversed row by row in the top-down direction. wx L cell
slots for a session x is assigned to the current available row from left to right
starting at the ﬁrst available cell slot if the current row contains enough empty
cell slots to accommodate wx L cells. Otherwise, the current row will be ﬁlled
to full by session x and the rest cells of session x will be scheduled to the next
row starting form the leftmost cell slot. Since a session can occupy at most L
cell slots in the frame, two cells from the same session cannot be assigned to one
column. This guarantees that no session can use more than one channel at one
time slot. Except that PWRR is diﬀerent from PFRR in forming the frame, the
scheduling strategy of PWRR is the same as PFRR, i.e., steps b and c of PFRR.
The network topology for the simulation is shown in Fig. 1, where there
are 2 channels in some links. The number of wavelength channels in each link
connecting two core nodes is 2; the number of channels in each link connecting
a core node and an end node is 1. In the following simulation, there are 4 sessions
established in the network, denoted by source-destination pairs, S1 -D1 , S2 -D2 ,
S3 -D3 and S4 -D4 . From Fig. 1, links Ci Ci+1 (i = 1 ,2, . . . , 5) are multiplexed by
more than one session. The reservation demands allocated to multiple sessions
through the multiplex links and frame sizes of the multiplex links are given
in Table 1. These multiplex links can be scheduled according to PFRR and
PWRR. In the simulation as follows, exponential distributed traﬃc consisting of
a sequence of cells is generated at the source node and destined to the destination
node for each of the 4 sessions. End-to-end delay from source to destination and
queue length at link C1 C2 for session S1 -D1 are observed and compared for
PFRR and PWRR.

510

Deming Liu et al.
Table 1. Reservation demands of sessions and frame sizes for multiplex links
C1 C2 C2 C3 C3 C4 C4 C5 C5 C6
S1 -D1
25
25
25
25
25
S2 -D2
34
34
34
34
34
S3 -D3
15
15
15
15
15
S4 -D4
46
46
46
46
46
Frame size 2×60 2×60 2×60 2×60 2×60

Fig. 2. Cell delay curves and queue length curves

In the simulation, we assume a cell has the length of 100 bytes. The same
simulation loading with the same traﬃc is performed twice so that each time one
of PFRR and PWRR is applied to all multiplex links. The observed cell delay
and queue length of session S1 -D1 for PFRR and PWFQ are shown in Fig. 2. In
the ﬁgure, we see that PWRR exhibits larger ﬂuctuation of packet latency and
queue length than PFRR, thus inferrior to PFRR in terms of fairness.

6

Application of PFRR

In applying PFRR, rescheduling a frame is needed only when there are sessions
to be established, cancelled or updated, which happen infrequently from the
perspective of users. The associated overhead can be ignored since a new transmission frame can be computed in parallel to the current transmission, and is
swapped at the next frame boundary.
It is worthy to consider the application of PFRR in the message communications of a distributed real-time system. Consecutive messages are sent from
a source application to a corresponding destination application. Assume that
each message consisting of P cells is with a deadline requirement, and will be
routed through k switches to reach its destination. To utilize the proposed PFRR
scheme, a session of proper reservation demands and the required buﬀers must
be established to facilitate the message communication. Let us keep the same

Parallel Fair Round Robin Scheduling in WDM Packet Switching Networks

511

assumptions as in Theorems 4 and 5. We deﬁne the session delay for a packet
as the interval between the packet head entering the source node and the packet
completely leaving the destination node. If a packet of session x can be broken
into P cells, the packet’s delay bound along the k PFRR servers that session x
travels through can be expressed as (σx + P − 1 + 2k)/wx∗ .
In other words, bandwidth reservation for session x in a server is performed
by reserving ex cell slots in the corresponding frame. The criteria of how to
determine the frame length L can be based on the facts that (i) L should not
be too smaller, otherwise we cannot guarantee that suﬃcient granularity for
allocating bandwidth, and (ii) the computation of rescheduling a frame increases
as L increases. Thus large frame length may introduce long session setting-up or
updating time.
Besides meeting deadline requirements, the other concern in real-time systems is message jitter. A session may require that the message delivery jitter be
minimized. This can be accomplished if the message cells only use the scheduled
cell slots regardless any empty cell slots in a frame. We call a session with this
requirement a nonwork-conserving session and other sessions work-conserving
sessions. Now let’s consider a scenario of existing mixed types of sessions, in
which some sessions only need to meet their deadline requirements, i.e., workconserving sessions, and others are nonwork-conserving sessions with additional
jitter constraints. Then the question comes up whether we can let a set of backlogged sessions share the empty cell slots in the frame and the other set of
backlogged sessions keep their original scheduled cell slots such that all sessions’
delay bound and buﬀer requirement cannot be deteriorated. We introduce an
MPFRR (modiﬁed PFRR) discipline intended to address this requirement.
MPFRR Discipline
a. Same as step a of PFRR.
b. Same as step b of PFRR.
c. If not all sessions are backlogged in time slot t, we let Ridle (t) denote the set of
∗
idle channels in time slot t of F and Tbusy
(t) be the set of backlogged sessions
that do not get serviced in time slot t excluding the nonwork-conserving
∗
(t)|} sessions from to ﬁll mmin{|Ridle(t) |,
sessions. Select min{|Ridle(t) |, |Tbusy
∗
|Tbusy (t)|} idle channels in Ridle (t). Then the cells in time slot t are transferred
except for the cells that violate their allocation constraints.
Lemma 3. In any busy interval of a nonwork-conserving session x with
MPFRR, we always have
wx (t2 − t1 ) − 2 < Sx (0, t2 ) − Sx (0, t1 ) < wx (t2 − t1 ) + 2

(7)

where the busy interval of session i begins at time 0 and, t1 and t2 are any two
time instants in the busy interval with t1 ≤ t2 .
MPFRR is a hybrid of work-conserving and non-work-conserving policies.
Since the cells of nonwork-conserving sessions are always transferred relatively

512

Deming Liu et al.

at the same positions, the performances of these sessions are kept unchanged no
matter how many idle slots there are in the frame. On the other hand, delay
bound and buﬀer requirement of work-conserving sessions are still guaranteed
when some sessions are idle. This is due to the fact that the cell slots allocated to
backlogged work-conserving sessions in the frame are deﬁnitely used by themselves no matter there are any idle sessions or not. If there are some empty
cell slots in the frame in the case of existing idle sessions, more cells may be
transferred for work-conserving sessions in these cell slots

7

Conclusions

In this paper, we proposed an applicable packet-scheduling algorithm for realtime WDM parallel switching networks. The algorithm is a round-robin based
discipline with ﬁxed-length cell being the basic unit for transmission and scheduling. Bandwidth reservation for a session at a switch is carried out through reserving a number of cell slots in a two-dimension frame. The pfair algorithms
are used to determine cell slot assignment in the frame with or without allocation constraints considered so that cell slots of a session are distributed as
uniformly as possible in the frame. Through the analysis by network calculus
and by network simulator, it is shown that PFRR takes the advantage of low
on-line computational complexity, and possesses tight delay bounds and lenient
buﬀer requirements. In addition, a modiﬁed version called MPFRR can be used
as traﬃc regulator to support sessions with jitter requirements.

References
[1] Hui Zhang, ”Service disciplines for guaranteed performance service in packetswitching networks,” Proceeding of the IEEE, Vol. 83, No. 10, Oct. 1995, pp.
1374-1396.
[2] Deming Liu and Yann-Hang Lee, ”An eﬃcient scheduling discipline for packet
switching networks using earliest deadline ﬁrst round robin,” The 12th International Conference on Computer Communicaitons and Networks, Oct. 2003.
[3] J. Anderson et al., ”Parallel switching in connection-oriented networks,” Proceedings of the 20th IEEE Real-Time Systems Symposium, 1999, pp. 200-209.
[4] Laura E. Jackson and George N. Rouskas, ”Deterministic preemptive scheduling
of real-time tasks,” Computer, vol. 35, issue 5, May 2002, pp. 72-79.
[5] S. Baruah, N. Cohen, C. G. Plaxton, and D. Varvel, ”Proportionate progress:
A notation of fairness in resource allocation,” Algorithmica, vol. 15, no. 6, 1996,
pp. 600-625.
[6] S. Baruah et al., ”Fairness in periodic real-time scheduling,” Proceedings of 16th
Annual IEEE Real-Time Systems Symposium, December, 1995, pp. 200-209.
[7] S. Baruah, J. Gehrke, C. G. Plaxton, and I. Stoica, ”Fast scheduling of periodic
tasks on multiple resources,” Proceedings of the 9th International Parallel Processing Symposium, April 1996, pp. 280-288.
[8] J. Anderson and A. Srinivasan, ”A new look at pfair priorities,” Technical report,
Dept. of Computer Science, Univ. of North Carolina, 1999.

Parallel Fair Round Robin Scheduling in WDM Packet Switching Networks

513

[9] Jean-Yves Le Boudec, ”Application of network calculus to guaranteed service
networks,” IEEE Transactions on Information Theory, Vol. 44, No. 3, May 1998,
pp. 1087-1096.
[10] Jean-Yves Le Boudec and Patrick Thiran, Network Calculus: A Theory of Deterministic Queuing Systems for the Internet, Springer, 2001.
[11] J. Anderson and A. Srinivasan, ”Mixed pfair/Erfair scheduling of asynchronous
periodic tasks,” Proceedings of the 13th Euromicro Conference on Real-Time Systems, June 2001, pp. 76-85.

A Voltage Scheduling Heuristic for Real-Time Task Graphs
D. Roychowdhury, I. Koren, C.M. Krishna
Department of Electrical and Computer Engineering
University of Massachusetts, Amherst, MA 01003
droychow,koren,krishna@ecs.umass.edu
Y.-H.Lee
Department of Computer Science
Arizona State University, Tempe, AZ 85287
Yhlee@cs.asu.edu

Abstract
Energy constrained complex real-time systems are becoming increasingly important in defense, space, and consumer applications. In this paper, we present a sensible
heuristic to address the problem of energy-efﬁcient voltage
scheduling of a hard real-time task graph with precedence
constraints for a multi-processor environment. We show
that consideration of inter-relationships among the tasks in
a holisitic way can lead to an effective heuristic for reducing
energy expenditure. We developed this algorithm for systems running with two voltage levels since this is currently
supported by a majority of modern processors. We then extend the algorithm for processors that can support multiple
voltage levels. The results show that substantial energy savings can be achieved by using our scheme. The algorithm
is then compared with other relevant algorithms derived for
hypothetical systems which can run on inﬁnite voltage levels in a given range. Our two voltage systems, using the task
dependencies effectively, can provide a comparable performance with those algorithms in the cases where continuous
voltage switching is not allowed.

1. Introduction
In CMOS devices, energy consumption per cycle is proportional to the square of the voltage, while circuit delay
decreases roughly linearly with the voltage. As a result,
controlling the supply voltage allows the user to trade off
workload execution time for energy consumption.
Over the past few years, many researchers have studied
0 This research has been supported in part by NSF, NGS program, grant
number EIA-0102696

this tradeoff. For hard real-time systems – so-called because the workload is associated with a hard deadline –
the tradeoff is particularly challenging. In such applications, the workload is characterized by analysis or proﬁling, so that its worst-case execution time can be bounded
with reasonable certainty. In most cases, the workload is
run periodically, with the period and deadlines known in
advance. Furthermore, many real-time applications (e.g.,
spaceborne platforms) have constraints on their power or
energy consumption. There is thus an increased need for
power-management techniques for such systems; the a priori knowledge about the workload provides an added opportunity to use such techniques.
Most of the power-aware voltage-scheduling work for
real-time systems has concentrated on independent tasks.
By contrast, in this paper, we consider voltage scheduling
while executing a task graph, which deﬁnes the precedence
constraints between tasks.
The problem can be informally described as follows: we
are given a task graph, the worst-case execution time of each
task and the period at which the task graph is to be executed.
This workload is to execute on a multiple-processor system,
each processor of which has its own private memory. The
task assignment in the processors and the order of the algorithm is also determined apriori and serves as an input
parameter. The problem is to schedule the voltage of each
processor in such a way that the energy consumption is kept
low. Assuming the deadline of the task graph equals its period, there will be no more than one task iteration alive in
the system at any time.
Our algorithm has both an ofﬂine and an online component. The ofﬂine component performs voltage scheduling
based on the worst-case execution requirements. The online
component adjusts the voltage schedule as tasks complete:
in most cases, tasks consume less than their worst-case time,

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

and the remaining time can be reclaimed to run the processors slower than might otherwise be required.
The remainder of this paper is organized as follows. In
Section 2 we provide a brief survey of the relevant literature. In Section 3 we outline our algorithm and in Section 4
we provide some numerical results which serve to show its
effectiveness. The paper concludes with a brief discussion
in Section 5.

2. Literature Survey
Dynamic Voltage Scaling is increasingly being used
to efﬁciently address the problem of energy aware task
scheduling. Initial work on dynamic voltage scheduling
[9, 28] was in the context of non-real-time systems where
the average throughput is the performance metric. Lin, et
al., treated this problem as the integer programming model
and presented heuristics for scheduling dealing with timing
and resource constraints [18]. Chang et al. in [8], present
a dynamic programming technique for solving the multiple
supply voltage scheduling problem in both non-pipelined
and functionally pipelined data-paths. The scheduling problem refers to the assignment of a supply voltage level (selected from a ﬁxed and known number of voltage levels)
to each operation in a data ﬂow graph so as to minimize
the average energy consumption for given computation time
or throughput constraints or both. Results for a ARM7D
processor at two voltage frequency combinations: (5.0V,
33MHz) and (3.3V, 20MHz) are presented in [4], for the
Dhrystone 1.1 benchmarks it yields 185 MIPS/watt and 579
MIPS/watt, respectively. Yao et al. assumed that the power
usage is a convex function of the clock rate [31] and derived a static voltage control heuristic to reduce energy consumption. Ishihara and Yasuura presented a model of a dynamically variable voltage processor and basic theorems for
power-delay optimization [15]. A static voltage scheduling
problem is also proposed and formulated as an integer linear
programming (ILP) problem. They point out that two voltage levels are sufﬁcient and a large number of available levels do not contribute much as long as the two voltage levels
are carefully chosen. Hong et al.[12], present a nonpreemptive scheduling heuristic for low power core-based real-time
SOC based on dynamically variable voltage hardware. In
[13], the problem of voltage control in a problem involving
scheduling sporadic tasks in the midst of an ambient periodic workload is considered. The authors point out that the
voltage transitions are fast: of the order of 10 to 100 sec
per volt. In another instance Burd and Brodersen discuss
the design of a variable voltage processor which can switch
voltage at the rate of 24 sec per volt [6]. In [20], Ma and
Shin present an energy adaptive combined static/dynamic
scheduler in Emerald Operating System to execute tasks in
mobile applications. The emphasis is on achieving effec-

tive use of limited energy by favoring low-energy and critical tasks. Qu and Potkonjak have presented a heuristic for
maximizing system utility which is based on quality of service (QoS) under limited energy resource conditions [24].
Shin and Choi slow the processor down to avoid idling it if
the current workload is guaranteed to ﬁnish before the next
job arrival [27]. A similar slowdown approach for the periodic real-time tasks which can consume energy at possibly
varying rates is presented in [5].
A simulation environment and benchmark suite evaluating voltage scaling algorithms have been presented in [23].
Lee et al. introduced dynamic voltage algorithm for ﬁxed
priority task systems in [17]. In another work Krishna et al.,
show how voltage scaling can be based on Earliest Deadline First (EDF) scheduling algorithms to get energy performance optimizations [16]. In [22], a class of novel algorithms called real-time DVS (RT-DVS) is introduced that
modiﬁes the real-time scheduler and task management service to provide signiﬁcant energy savings while maintaining
real-time deadline guarantee. Researchers have also proposed the concept of compiler directed DVS [21] where the
compiler sets the processor frequency and voltage with the
aim of minimizing energy under real-time constraints.
In [11], the scheduling problem of independent hard realtime tasks with ﬁxed priorities assigned in a rate monotonic
or deadline monotonic manner is addressed. This method
employs stochastic data to derive energy-efﬁcient schedules
taking the actual behavior of the real time systems into account. Several papers have also recognized the need for
both ofﬂine and online approaches to address the issue of
energy efﬁcient scheduling of independent real-time tasks
(see for example, [5, 25]).
Variable voltage scheduling as a low power design technique at the behavioral synthesis stage is discussed in [26].
Given as input an unscheduled data ﬂow graph with a timing constraint, the goal of this paper is to establish a voltage value at which each of the operations of the data ﬂow
graph would be performed while meeting its timing constraint. The authors have used a iterative graph-theoretic
approach to identify critical paths and assign nodes to a speciﬁc voltage level.
An interesting approach to power-conscious joint
scheduling of periodic task graphs and aperiodic tasks in a
distributed real-time embedded system has been proposed
in [19]. Here the authors focus on the problem of efﬁcient
scheduling of a mix of task graphs and independent tasks
and present an effective dynamic energy reduction heuristic
for them. They use a slack-based list scheduling approach
to perform static resource allocation, assignment and
scheduling of the periodic task graphs. The emphasis of
this work is to meet all hard real-time constraints, minimize
the response time of all soft aperiodic tasks and also engage
in dynamic voltage scaling and power management to

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

reduce energy consumption. It is assumed in [19] that tasks
always run to their worst-case execution times.
An initial study of tasks with precedence constraints has
been made in [32]. In this paper, a static evaluation is carried out that deﬁnes the order in which the tasks are to be
executed. This order is kept unchanged, even if the task execution times are much less than the worst-case. When tasks
are completed ahead of their worst-case time, the slack that
is thus released can be used by running the processor(s) at a
lower voltage than would otherwise be required.
By contrast, in this paper, we recognize that considering precedence relationships among the tasks and the entire
shape of the task graph might help us in deriving an algorithm that would achieve signiﬁcant energy savings. We
also believe that an effective technique should comprise
both online and ofﬂine components and hence we design
our ofﬂine heuristics with the online component in mind.
We focus entirely on the computation aspect of this problem in a multiprocessor system and try to come up with a
comprehensive solution using static scheduling and runtime
strategies to achieve energy efﬁcient scheduling of hard
real-time task graphs.

3. The Algorithm
The given task graph, henceforth referred to as the task
precedence graph (TPG), is assumed to have a hard deadline associated with it. Therefore, our algorithm tries to reduce energy expenditure by voltage scheduling in such a
way that the deadline is always met.
In CMOS devices, the power consumption is proportional to the square of the voltage [7, 15]:



   

(1)

where  is the circuit output load capacitance,  is
the number of switches per clock cycle,  is the clock frequency and  is the supply voltage. However, reduction
of power supply voltage causes increase of the circuit delay
denoted by Æ [7, 15]:

Æ

 
   	

(2)

where  is a constant depending on the process and gate
size,  is the threshold voltage, and  varies between 1
 for long channel devices which have no veand 2; 
locity saturation. In this paper, we assume   for all our
numerical experiments.

3.1 System Model
Our model consists of a multi-processor system where
each of the processors is independent and is connected by

a low cost fast interconnection network. The processors
can operate in three voltage levels: 
 ,  , and 
(
     ). 
 and  are voltages at which
the processors can do useful computation whereas  is
the voltage necessary to sustain the system in idle state.
The factor by which the processor is slower at voltage 
relative to when at the highest voltage 
 is

	



 
   	

   

(3)

where  is the threshold voltage.
We deﬁne one unit of execution as the computation performed by the processor at 
 in unit time. Thus, one unit
of execution will take 	
 units of time at voltage .
The ratio of energy consumed per cycle by a processor at
voltage  relative to that at voltage 
 is



  
 




 



(4)

For the task-sets we study, inter-task communication
only happens after each task has ﬁnished its computation.
Such communication consists of transfer of small amounts
of data; the communication cost can be safely ignored as insigniﬁcant in the types of application under consideration.
Since idle(sleep) power consumption is considerably lower
than operational power [1], the energy cost when the processors are idle is also ignored. We should also note that our
algorithm actually decreases the idle time in the processors
compared with the single voltage algorithm. Hence this is
a conservative assumption since the relative gain using our
algorithm would be even higher if we had considered the
energy cost associated with the processor idling.
We have also considered the voltage switching cost as
negligible both with respect to the time needed and the energy expended. This is justiﬁed by the fact that our algorithm has at most one voltage switch within the runtime of
the task and at most one switch at the time of context switching of the tasks. We have accounted for this cost by merging
this effect into the worst case proﬁle information.

3.2. The Details of the Algorithm
Given the tasks we can apply any static task assignment
and ordering heuristic. Any generic multiprocessor static
task assignment algorithm can be followed. The task graph
under this assignment should be able to meet the deadline
if running to their worst case under the highest available
voltage. Any assignment that satisﬁes the above criterion
can act as a valid input to our algorithm. Once we are
given the assignment and task schedule order in the multiprocessor environment, we can apply our voltage scheduling heuristic to minimize the energy expenditure. We follow

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

a two-pronged approach to achieve our objective. The approach includes an ofﬂine component - the voltage scheduling of the TPG based on the static worst-case execution
proﬁle. We use the pessimistic worst case execution proﬁle approach because the system has hard real-time requirements and a deadline miss under any circumstances would
be catastrophic. We follow with an online, the dynamic
slack reclamation, phase.
We will use the following terms for describing our algorithm. The critical path is a set of tasks from a source to
a sink of the TPG that misses the deadline under the current voltage conﬁguration. The reverse slack or rslack of
a critical path is the difference between the deadline and
the worst case execution time of that path with the current
voltage conﬁguration. The start-time of a task is the latest
time, relative to the beginning of the execution of the task
graph, at which the particular task must be invoked, and
commit-time is the time by which the task must complete its
execution.
3.2.1. Static Voltage Scheduling
We apply any static assignment heuristic, such as the
one described in Section 3.2.3, to the TPG. In the cases
where there are not enough processors available to exploit
the parallelism inherent in the TPG, this assignment and
task ordering may lead to new dependency relationships.
We would, therefore, need to modify the original TPG by
adding new edges to accommodate these dependencies. After the assignments and modiﬁcations are done, we apply our static voltage scheduling heuristics to the TPG. In
order to better understand this scheduling heuristic let us
rephrase the optimization problem in the following way.
Let denote the speedup in time associated with each task
. Speedup can be explained as the difference in execution
time of a task when running under our voltage schedule and
running it entirely in  . For each path , we have to
satisfy the constraint

¾



 	  

 

where task  belongs to path ,  is the worst case execution time of the path  without any speedups and  is
the deadline associated with the path . Our objective is to

minimize
where  is the total number of tasks in

the task set. This objective function implies that we are trying to minimize the amount of time needed for the tasks to
run in 	 which in turn means minimizing overall energy
expenditure.
There is a trivial solution for this problem if the deadline is met when all the tasks are run at  . However, the
problem becomes more interesting when some of the paths
become critical paths and a decision has to be made about



Algorithm 1 Static Voltage Scheduling
while list of critical paths not empty do
Assign weights to the tasks
taskId = choose task with maximum weight and if
more than one task has the same maximum weight
choose the one with minimum bottom level value.
pathId = choose the path with minimum rslack among
all the critical paths having taskId as a member task.
Speed up taskId using the following scheme:
if rslack can be covered by changing units from 
to 	 then
Change the appropriate units of taskId to run them
at 	 instead of 
else
Run the entire taskId at 	 and mark the task so that
its weight is never considered during subsequent iterations.
end if
Update the path execution times and remove any path
which now meets the deadline from the list of critical
paths.
end while

which task to speed up. Analyzing the expressions above,
it appears that if we speed up a task that is part of a large
number of critical paths, we affect many paths while paying the energy price only once. Based on this intuition we
formulated the following iterative algorithm to determine
which task needs to run at 	 and for how many execution
units. We start the procedure by assigning all the tasks to
run at  and then speed them up iteratively until there
are no more critical paths left. The weight associated with
each task is dependent on the membership of the task in the
set of critical paths, every time we encounter a task in the
critical path we increment its weight by 1. When we have
to break a tie between tasks of equal weight we choose the
task nearest to the leaf of the TPG. The rationale behind this
is that we would like to schedule a task to run at 	 as late
as we can because during dynamic resource reclamation,
we could potentially re-acquire enough slack to avoid having to run it entirely at 	 . Note that we could formulate
our problem as a linear programming optimization. This,
however, would yield an optimal static scheduling which
would not attempt to increase the opportunity for dynamic
adjustments. We still have experimented with linear programming techniques and the overall results tend to match
closely those of our static heuristic. Our static heuristic appears to give us near-optimal performance in most cases as
well as facilitating the dynamic resource reclamation in the
subsequent step.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

3.2.2. Dynamic Resource Reclamation
Once we have completed the static scheduling of the
paths, we can assign start time and commit time to the individual tasks. Since the static analysis was based on the
worst case execution proﬁle, each task will ﬁnish before or
at its commit time during actual runtime. Thus, its successor
can begin execution earlier if it has no other pending dependencies and we can use this extra slack between start time
and the current time to slow down the processor further under the constraint that this task still ﬁnishes at its commit
time even if it runs to its worst-case execution proﬁle. The
following equation calculates the amount of units of execution to be transferred from running at  to running at 
(given that it takes unit time to execute one unit in  ):
  

	 
  		 


   

(5)

where    is the additional number of units of the
task that should be transferred to the portion executed in
 , 		 
 is the current time and 	 
 is
the start time predicted during the static voltage scheduling based on the worst case execution proﬁle of the tasks.
This transfer of certain units of execution from  to 
results in further energy savings.
The next subsection ﬁrst explains the algorithm, and then
illustrates it through an example TPG on which the whole
procedure is performed. For this example, we used a list
scheduling heuristic as our static assignment algorithm.
3.2.3. An Example Static Assignment Scheme
The assignment problem of a task graph to a ﬁnite number of processors is, in general, an NP-complete problem
[14] and many heuristics have been proposed to address it.
Any of these assignments can be used in conjunction with
our algorithm as long as the assignment makes the real time
task graph feasible under the worst case proﬁle. However,
the task assignment heuristic that is followed would have
an impact on the effectiveness of our voltage scheduling
heuristic. The faster the entire task graph can be completed,
the higher will be the effectiveness of our algorithm. We
employ a list scheduling heuristic adapted from [30] as an
example of how task assignment can be done. We should
emphasize here that other assignment heuristics would also
work with our algorithm. For example, assignments using
genetic algorithms , such as [10], or assignment using simulated annealing, such as [29], can be combined with our
algorithm.
The list scheduling heuristic we consider gives the highest priority to the tasks in the longest paths during the task
assignment [30]. For a particular ﬁxed voltage, this heuristic allows us to ﬁnish the execution of the entire task set in

the least amount of time for most cases. Hence, the scope
for exploiting the slack is likely to be high if we use this
assignment. If this algorithm does not meet the deadline
criteria of the task graph under worst case, we have to use
another task assignment heuristic.
The assignment heuristic is based on the concept of
assigning priorities to tasks by using the concept of the
top level and bottom level for the tasks. We deﬁne top level
as the maximum of the sum of the worst case execution
units from any connected source of the TPG to the given
task (excluding the execution units of the given task) and
bottom level as the maximum of the sum of the worst case
execution units from the given task (including the execution units of the given task) to any connected leaf of the
TPG. The priority of the task is the sum of bottom level
and top level. Once we assign the priority we do an ofﬂine
analysis using a greedy list-scheduling algorithm to assign
tasks to each of the processors and to determine the order
of their execution. The heuristic we follow is that whenever we ﬁnd a free slot in a processor and tasks are ready
to run, we assign the ready task with the highest priority to
that processor.
3.2.4. An Example Taskgraph
We now provide an example to illustrate our algorithm.
The example graph is shown in Figure 1. The number inside
the circle represents the task number while the two numbers
on the side are the worst case execution units (in bold) and
the actual execution units at runtime for some execution instance, respectively. For this example  is chosen at 3.3V
and  at 2V. The deadline of the execution of the task
graph is chosen as 99 for which not all tasks can be executed at  .
28
22.88

1

4
2.5

2

28
26.1

3

30
4
26.8

20 5
11.8

16
14.86

6

7

18
11.2

Figure 1. An example task graph with execution times in
terms of Ú

.

We execute this task graph in a system with three processors. The processor assignment following our heuristic
keeps the graph unchanged in this case. The priority calculation is demonstrated in Table 1 which shows the calculated

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

task ID
1
3
5
7

top Level
0
0
34
34

bottom Level
48
46
20
18

task ID
2
4
6

top Level
0
4
34

bottom Level
54
50
16

Table 1. The priority parameters for the example TPG.
Step
No.

Path chosen

1
2
3
4
5

2–4–6
3–7
1–5
2–4–7
2–4–5

weight
of
task1
1
1
1
0
0
0

weight
of
task2
3
2
2
2
1
0

weight
of
task3
1
1
0
0
0
0

weight
of
task4
3
2
2
2
1
0

weight
of
task5
2
2
2
1
1
0

weight
of
task6
1
0
0
0
0
0

weight
of
task7
2
2
1
1
0
0

task
chosen
4
7
5
4
5

Table 2. The iterative steps of algorithm for the example TPG
Deadline
PROCESSOR 3

TASK 3

PROCESSOR 2

TASK 2

TASK 7

WAIT FOR 3 & 4

Processor 3

3

7

.

TASK 6

TASK 4

Processor 2

2

Processor 1
PROCESSOR 1

TASK 1

4

4

6

1

5

TIME

TASK 5

WAIT FOR 1 & 4

0

20

40

60

80

100

Figure 2. Static Task Assignment and Ordering.
Deadline

Processor 3

3

Processor 2

2

Processor 1

4

6

1
0

20

5

5
40

60

TIME
80

100

Figure 3. Static Schedule.
Ú

Ú

 is represented by taller rectangle.

7

7

4

Figure 4. Actual behavior of tasks at run time.

 is represented by taller rectangle.

values of the top Level and bottom Level of each task. The
priority is determined as the sum of these two parameters.
The task assignment and ordering are shown in Figure 2.
We then apply the static voltage heuristic to the graph.
During the ﬁrst iteration, both tasks 2 and 4 have the max-

imum weight of 3. We choose task 4 since it is nearer to
the leaf of the TPG and speed it up appropriately to make
the path (with minimum rslack) consisting of tasks 2, 4 and
6 meet its deadline. We remove this path from the list of
critical paths and proceed with our algorithm. In the next
iteration, the weights of tasks 2,4,5 and 7 are all 2. We then
choose task 7 and speed it up such that the path consisting
of tasks 3 and 7 meets its deadline. Table 2 describes the
individual iterations in detail. The second column depicts
the path chosen for speeding up, the middle columns show
the weights of the individual tasks in this step of the algorithm, and the last column shows the task that was selected
for speeding up. We continue this iterative procedure until
ﬁnally we obtain the static schedule shown in Figure 3.
We then do dynamic resource reclamation to reclaim any
slack that occurs in runtime. Let us now look at task 4 and
see how runtime variations affect the scheduling. After the

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

3.3. Extension to a Multi-Voltage System
The algorithm described above has been created keeping in mind processors supporting just 2 voltage algorithms.
This can be easily extended to processors running under
multiple voltage levels. The extension can be described as
follows: We can use the same scheduling algorithm and
assign start time and commit time to the individual tasks.
Once we ﬁx the interval, we can ﬁnd an unique voltage level
which can ﬁnish the task in that interval without any voltage switching. After calculating this voltage level, we can
choose the two voltage levels that the processor supports between which the calculated voltage level lies. We then run
the task in the two chosen voltage levels such that the task
ﬁnishes exactly at commit time when running under worst
case.

applied efﬁciently to a wide range of task graphs. The application is a task graph for a random sparse matrix solver
of electronic circuit simulation using the symbolic generation technique, henceforth referred to as sparse matrix.
The sparse matrix has 96 tasks. These task graphs have
been published by the Kasahara Lab [3], and the timings
are based on actual proﬁling done on the OSCAR multiprocessor system. The operating voltages of the processors and
their frequencies have been modeled based on Intel xScale
[2].
The parameters used in the simulations are ÀÁ V,
V, and Ì
V unless speciﬁed otherwise.
ÄÇ
Using these values in (3) and (4), the maximum energy savings possible is about 67.34% if everything can be run at
ÄÇ . The execution units of the individual tasks are uniformly distributed in the range  % of their worst case
proﬁle. We have varied  in the simulations and the results are presented below. We ﬁrst compare the energy savings that we get when our scheduling method is followed,
with a system where there is no voltage scheduling: that is,
all tasks have to run in a predeﬁned ÀÁ . The results are
shown in Figure 5: our algorithm yields considerable energy savings. As the variance of the tasks’ execution times
increases, we see that we can get increasing savings from
the algorithm due to the increasing slack that we can exploit at runtime. Yet, even in the case of worst-case execution (A=100), the plots demonstrate that signiﬁcant energy savings can be achieved because of our static algorithm. Similarly, when we vary the number of processors,
we can exploit the parallelism more and hence have better
performance with an increasing number of processors (see
Figure 6).
70
A = 25

60
% Energy Saving

static scheduling, the start time of task 4 is 9 and the commit
time is 63 and it has been scheduled to execute 19.2 units in
ÄÇ and 10.8 units in ÀÁ . Assume now that the preceding task, task 2, did not take its worst case time to execute
and ﬁnished instead at time 5.625. So now task 4 could be
started at 5.625 instead of at 9 and we can use this extra
time to slow it down further such that its worst-case commit
time still remains at 63. Thus, at the time of invocation it
is scheduled to execute 21.9 units in ÄÇ and 8.1 units in
ÀÁ . Figure 4 shows the effect of dynamic resource reclamation on our static algorithm. In addition, we have used
the simplex method to solve the corresponding linear programming optimization problem and found the speedups required by different tasks under their worst case proﬁle. The
simplex method yielded the same sum of speedups as our
algorithm. However, the distribution of the speedups was
quite different. For example, if we did static scheduling following the simplex method’s solution we would schedule
the entire task 2 at the highest voltage, ÀÁ . This would
have led to inefﬁcient use of the slack resulting from actual
execution time being less than the worst case.
Even though we have shown an example with a single
task graph, the algorithm can be used for multiple task
graphs as long as they have the same period. After their
assignment, the multiple task graphs can be cast as a single
task graph and the same algorithm can be applied to achieve
our objective.

50
40

A = 100

30
A = 50
20

A = 75

10
0
100

150

200

250

300

350 400
Deadline

450

500

550

60

Figure 5. Energy savings after runtime adjustments for
the sparse matrix with differing variance in execution time
(for 12 processor system).

4. Numerical Results
We have performed extensive simulation experiments of
the algorithm described in the previous section. Here we
present our results from a real-life application as a case
study. We follow it up with our results obtained for a set
of random task graphs to show that the algorithm can be

The plots in Figure 7 show the savings achieved by dynamic resource reclamation over the static scheduling. As
predicted, we can see that the greater the variance in execution time, the better the performance. Since our adjustment

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

1
2
3
4

Voltage (V)
1.75
1.40
1.20
1.00

Frequency (MHz)
1000
800
600
466

70
65
60 20 processors
55
50
45
40
35
16 processors
30
25
20
12 processors
15
100 150 200 250 300

70
A = 25

65
60
% Energy Saving

% Energy Saving

Table 3. The Operating Voltages and corresponding Frequencies.

8 processors

A = 50

55
50
45
40

A = 100

35
A = 75

30
25
350 400
Deadline

450

500

550

20

60

0

2

4

6

8

10

12

D+1

Figure 6. Energy savings after runtime adjustments for
the sparse matrix with varying number of processors.

Figure 8. Average Energy savings after runtime adjustments for the random graphs with differing variance in execution time (for 12 processor system).

45
40

A = 25

20
18

30

16

A = 50

25
20

% Energy Saving

% Energy Saving

35

A = 75

15
10
5
A = 100

0
−5
160

14

A = 75

12
10

A = 50

6
A = 25

4
180

200

220

240

260 280
Deadline

300

320

340

36

2
0
150

Figure 7. Energy savings due to dynamic resource reclamation for sparse matrix (12 processor system).

A = 100

8

200

250

300
Deadline

350

400

45

Figure 9. Comparison with Inﬁnite Voltage Algorithm
[32] for sparse matrix (12 processor system).

is fast and happens only during context switch, we achieve
substantial savings with relatively little overhead.
We now present results for random graphs to show the effectiveness of our algorithms. We chose 60 randomly generated graphs, each consisting of 50 tasks. For each task
graph we calculated the deadline necessary to schedule the
task graph under  and  and to schedule it under
0 to 10
 and  . We varied the deadlines  from
	  . For
such that   means     	

each deadline  we calculated the average relative gain and
presented the results in Figure 8. The plot shows that the
algorithm performs well for a large variety of task graphs.
Next we compared our algorithm with a dynamic voltage adjustment algorithm, referred to as LSSR-N [32], that

chooses from an inﬁnite number of voltage levels (see Figure 9). For this inﬁnite level algorithm, voltage adjustments
have been considered only at the time of context switches.
Here we relaxed the constraint that the  has to be ﬁxed at
a particular value and instead allowed it to have any value in
the voltage range speciﬁed.
 for the subsequent experiments was chosen as the minimum uniform voltage at which
the tasks can execute so that the longest path meets the
deadline under the worst case scenario. Our two-voltagelevel algorithm actually outperformed this inﬁnite-level algorithm in most cases. This shows that considering the
overall structure of the task graph at the time of voltage
scheduling does provide substantial beneﬁts over dynamic

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

% Energy Saving

slack sharing heuristics.
Finally, we measured the energy savings if we had used
multiple voltage levels for our algorithm instead of two voltage levels (see Figure 10). The system supporting multiple voltage levels, as expected, exhibited higher energy
savings than the two-voltage level system. The multiple
voltage-level algorithm chose the appropriate voltage from
any of the voltage levels supported (see Table 3), while the
 and 
 .
dual-voltage scheme used 
The results demonstrate that our algorithm can be easily incorporated into more complicated system conﬁgurations to
achieve superior performance. They also show that when
deadlines are more relaxed, the effect of the multiple voltage levels diminishes signiﬁcantly.
20
18
16
14
12
10
8
6
4
2
0
−2
180

A = 25
A = 50

A = 75
A = 100

200

220

240

260

280 300
Deadline

320

340

360

38

Figure 10. Comparison showing how much more energy
savings is possible using multiple voltage level selection is
used instead of dual voltage in our algorithm for sparse matrix (12 processor system).

5. Discussion
In this paper we have considered the problem of an energy efﬁcient voltage scheduling heuristic for task graphs
with precedence constraints. We have described a twopronged approach to solve this problem and have demonstrated that substantial energy savings can be achieved by
considering the relationships among the tasks in the graph.
The focus of this paper has been on exploiting the structure
of task graphs for energy minimization purposes. The voltage scheduling proposed needs at most one voltage switching, and is therefore a relatively low overhead algorithm.
We have presented a simple, low overhead voltage scheduling heuristic for executing task graphs in an energy efﬁcient
way.

References
[1] http://developer.intel.com/design/pca/.
[2] http://www.intel.com/design/intelxscale/.

[3] http://www.kasahara.elec.waseda.ac.jp/schedule/.
[4] Introduction to thumb. In ARM Documentation, Advanced
RISC Machines Ltd.
[5] Aydin H., Melhem R., Mosse D., and Alvarez P.M. Determining optimal processor speeds for periodic real-time tasks
with different power characteristics. In 13th Euromicro Conference on Real-Time Systems (ECRTS’01)., June 2001.
[6] Burd T.D. and Broderson R.W. Design issues for dynamic
voltage scaling. In International Symposium on Low-Power
Electronics and Design, pages 9–14, 2000.
[7] Chandrakasan A., Sheng S., and Brodersen R.W. Low power
cmos digital design. In IEEE Journal Solid State Circuits,
pages 472–484, 1992.
[8] Chang J.-M. and Pedram M. Energy minimization using
multiple supply voltages. In IEEE Trans. VLSI Systems,
pages 436–443, Vol. 5,No. 4,December 1997.
[9] Govil K., Chan E., and Wasserman H. Comparing algorithms for dynamic speed-setting of a low power cpu. In
Proc. MOBICOM, pages 13–25, November 1995.
[10] Greenwood G. W., Lang C., and Hurley S. Scheduling tasks
in real-time systems using evolutionary strategies. In Workshop on Parallel and Distributed Real-Time Systems, 1995.
[11] Gruian F. Hard real-time scheduling for low energy using
stochastic data and dvs processor. In International Symp. on
Low-Power Electronics and Design, August 2001.
[12] Hong I., Kirovski D., Qu G., Potkonjak M., and Srivastava
M. Power optimization of variable voltage core-based systems. In ACM Design Automation Conference, pages 176–
181, 1998.
[13] Hong I., Qu G., Potkonjak M., and Srivastava M. Synthesis
techniques for low-power hard real-time systems on variable
voltage processors. In 19th IEEE Real-Time Systems Symposium., pages 178–187, December 1998.
[14] Hoogeveen J.A., van de Velde S.L., and Veltman B. Complexity of scheduling multiprocessor tasks with prespeciﬁed
processor allocations. In CWI, Report BS-R9211, Netherlands, 1992.
[15] Ishihara T. and Yasuura H. Voltage scheduling problem for
dynamically variable voltage processors. In International
Symp. on Low-Power Electronics and Design, pages 197–
201, 1998.
[16] Krishna C.M. and Lee Y.-H. Voltage-clock-scaling adaptive scheduling techniques for low power in hard real-time
systems. In Real-Time and Embedded Technology and Applications Symposium, pages 156–165, May 2000.
[17] Lee Y.-H. and Krishna C.M. Voltage-clock scaling for low
energy consumption in ﬁxed priority real-time embedded
systems. In Sixth IEEE International Conference on RealTime Computing Systems and Applications, December 1999.
[18] Lin Y.-R., Hwang C.-T., and Wu A. Scheduling techniques
for variable voltage low power designs. In ACM Trans.
Design Automation for Electronic Systems, pages 115–192,
Vol. 2,No. 2,April 1997.
[19] Luo J. and Jha N.K. Power-conscious joint scheduling of periodic task graphs and aperiodic tasks in distributed real-time
embedded systems. In International Conference on Computer Aided Design, pages 357–364, 2000.
[20] Ma T. and Shin K.G. A user-customizable energy-adaptive
combined static/dynamic scheduler for mobile applications.
In Real-Time Systems Symposium, pages 227–236, 2000.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

[21] Mosse D., Aydin H., Childers B.,and Melhem R. Compilerassisted dynamic power-aware scheduling for real-time applications. In Workshop on Compiler and OS for Low Power,
2000.
[22] P. P. and S. K.G. Real-time dynamic voltage scaling for lowpower embedded operating systems. In ACM Symposium on
Operating Systems Principles, pages 89–102, 2001.
[23] Pering T., Burd T., and Brodersen R. The simulation and
evaluation of dynamic voltage scaling algorithms. In International Symp. on Low-Power Electronics and Design,
pages 76–81, 1998.
[24] Qu G. and Potkonjak M. Achieving utility arbitrarily close
to the optimal with limited energy. In International Symp. on
Low-Power Electronics and Design, pages 125–130, 2000.
[25] Raghunathan V., Spanos P. and Srivastava M.B. Adaptive
power ﬁdelity in energy-aware wireless embedded systems.
In Real-Time Systems Symposium, pages 106–115, 2001.
[26] Raje S. and Sarrafzadeh M. Variable voltage scheduling. In
Proc. Int. Symp. Low Power Design, pages 9–14, 1995.
[27] Shin Y. and Choi K. Power-conscious ﬁxed priority scheduling for hard real-time systems. In Design Automation Conference, pages 134–139, 1999.
[28] Weiser M., Welch B., Demers A. and Shenker S. Scheduling
for reduced cpu energy. In Proc. First Symp. on OSDI, pages
13–23, November 1994.
[29] Wells B.E. A hard real-time static task allocation methodology for highly-constrained message passing environments. In International Symposium of Computer Architecture, 1995.
[30] Yang T. and Gerasoulis A. List scheduling with and without communication delays. In Parallel Computing Journal,
pages 1321–1344, Vol. 19 1993.
[31] Yao F., Demers A., and Shenker S. A scheduling model for
reduced cpu energy. In Proc. 36th IEEE Symp. Foundations
of Computer Science, pages 374–382, 1995.
[32] Zhu D., Melhem R., and Childers B. Scheduling with dynamic voltage/speed adjustment using slack reclamation in
multi-processor real-time systems. In 22nd IEEE Real-Time
Systems Symposium, December 2001.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)

0-7695-1959-8/03 $17.00 (c) 2003 IEEE

Circular Window Control Schemes in Fast Packet Switches
Sandra E. Cheung

Randy Chow

Yann-Hang Lee

Department of Computer and Information Sciences
University of Florida
Gainesville, FL 3261 1
Abstract

established from any free inlet to a free outlet. Otherwise, it is blockin . One type of non-blocking switches
is the crossbar fa ric which consists of a square array
of N 2 crosspoint switches, one for each input-output
pair. Provided there are no output conflicts, all packets can reach their destinations. The major drawback
of these fabrics is that the realizability of such fabrics tends to be limited. On the other hand, blocking switches can be based on a class of multistage
interconnection networks referred to as banyan networks. A banyan network consists of log, N stages
binary switching elements and
each comprising of
the interconnection pattern is placed between stages
in such a way that there is a unique path from every
input to every output. The re ular structure of this
type of network and the distri uted routing control
which uses a self-routing algorithm called destination
tag routing, make it very amenable to VLSI implementation. Despite these advantages, banyan networks, as
any blocking switch, suffer from internal link conflicts
which arise every time packets share similar links, even
though they might be destined to distinct outputs.

%

In this paper, we investigate a scheduling algorithm,
called Circular Window Control ( C W C ) scheme, for
packet swatch fabrics. The scheme is simple and e f i cient. It uses a fixed length window with circular transmission sequences t o minimize possible switch and output contentions. The scheme can attain a maximal
throughput of 100% in nonblocking switches as well
as banyan-based blocking switches. The performance
analyses and simulation results of the C W C scheme
are also presented.

1

4

Introduction

High speed networking will form the backbone of
future distributed high-performance computing and
multi-media communication environments. It will enable the exchange of a large volume of information
and provide high speed data transmission required in
the applications involving images, multimedia, and
interactive collaborations. To facilitate high speed
networking, switch-based architecture and cell-level
multiplexing must be employed while the traditional
shared media architecture is no longer adequate due
to the limited switching capacity. Currently, a number of fast packet switches, such as the Starlite [lo],
Knockout [17],and their extensions [ll,8 , have been
designed for asynchronous transfer mode ATM) communication networks. These switches can be used to
interconnect a large number of inlets to outlets for
mixed traffic streams.
Assume that packets conform to the ATM cell format of 53 octets. To sustain an access rate of 1 Gb/s
per inlet, the switch must be capable of processing one
packet per inlet approximately every 400ns. While the
existing ATM switch prototypes developed in CMOS
or BiCMOS technolog are able to meet this process6, 161, the design of an effiing time requirement
cient and inexpensive switch system is still one of the
primary challenges for successful high speed network
deployment. A switch system will have a stringent
cost requirement that the cost per inlet must be no
more than a fraction of the cost of a high-performance
workstation. This implies that the switch design must
be simple and effective.
The architectural designs of fast packet switches
can be classified into two categories: non-blocking and
blocking. A switch is non-blocking if a path can be

%

Regardless of the fabric type, an important implementation issue in the design of a fast packet switch
is to resolve the output contention problem. Due to
the stochastic nature of arriving packets, there may be
more than one packet from different inlets destined to
the same outlet simultaneously. One of the contending
packet can be delivered to the output stream. Other
packets can be saved in a queueing buffer and wait for
an available transmission slot. Queueing buffers can
be implemented in the input port of the switch fabrics or in the output port, or in both. Input queues
are designed to store packets which cannot momentarily be routed through the fabric, and output queues
store packets which need to wait for the service of
the output links. Performance evaluations on these
two queueing schemes show that, with infinite buffers,
the throughput of the input queueing scheme is limited to only 58% due to head-of-line blocking, whereas
the output queueing scheme can sustain a load of up
to 100% [13, 9, 12, 141. However, to utilize the output queues, all contending packets must be able to
join the destined queue simultaneously. With nonblocking switch fabrics, in order to achieve this, either
the switch speed must be increased or multiple paths
of entering the queue must be provided. This require-

I

b,

116
0-8186-7180-7/95
$04.00 0 1995 IEEE

-J--T

ment inevitably increases the complexity of the switch
fabrics even if a modest speedup factor of 4 can lead
to a 99% throughput as demonstrated in [3].
To improve the performance of input queueing
schemes, several reservation approaches have been
suggested. The token ring principle is adopted in [2]
where a stream of tokens is passed through all input
ports to perform output reservations. Arbitrators as
well as contention resolution devices can be added in
conjunction with the switch fabrics [l, 4, 131. These
devices accept the transmission requests, schedule the
transmissions, and then acknowledge back to the input ports whether the packet transmission can be initiated. Though these approaches introduce different degrees of complexity or cause a limitation on the speed
at which the switch can be operated, the performance
improvements are less satisfactory.
It is imperative that, in order to increase the
throughput of input queueing schemes, packet transmissions be regulated. In this paper, we propose a simple and efficient control mechanism which uses a fixed
length window with circular transmission sequences.
The main idea is to minimize possible switch and output port contentions by transmitting packets in a predetermined order. Under a uniformly distributed traffic and with infinite buffers, the throughput of the
proposed mechanism can reach 100% in nonblocking
switches as well as banyan-based switches. With finite
buffers, the mechanism can yield a comparative performance to the output queueing schemes in terms of
blocking probability and average transmission delay.
In the following, we first describe the circular window control scheme and a possible architecture design.
Then, we show that the maximum throughput of the
proposed scheme can attain 100% throughput in both
nonblocking switches and banyan-based switches. The
schedulability of the circular window control scheme
is compared with a random scheduling and a perfect
scheduling algorithms. To further illustrate the efficiency of the proposed scheme, we provide the simulation measures on throughput and mean transmission
delay under uniform and hot spot input traffic.

2

PACKET SWlTCH

L

Figure 1: An Input Queued Packet Switch

Figure 2: CWC Architecture Model
retransmitted during later slots, while blocking packets behind the head of queues which may be destined
to idle outputs or could be transmitted through idle
routes. It is this type of head-of-line blocking which
limits the throughput of input queueing schemes.
To remedy the problem of head-of-line blocking,
packets from input queues should be chosen in order to
minimize the chance of contention. This process can
be done dynamically by comparing the destinations
of the packets at the heads of all non-empty queues.
It may be necessary to look into a number of packets
within a predetermined checking depth. This imposes
a difficult and complex implementation problem no
matter whether the process is done in a distributed or
centralized manner.
The circular window control (CWC) scheme is proposed in the followin . It is a static approach to solving the head-of-line locking described above. Consider an N x N switch, and assume that each inlet
of the switch maintains N subqueues which are dedicated to packets of the same destination, as shown in
Figure 2. The packets arriving a t each inlet join the
subqueues in accordance with their destinations, and
packets with the same destination are queued based
on their arrival times (i.e. FIFO order The CWC
switch is initially set up in such a way t at each inlet
has one subqueue designated as its primary subqueue.
Moreover, no two inlets will have the subqueues of similar destinations a8 their primary subqueues. With N
inlets and each inlet having N subqueues, it is matter
of obtaining a permutation and assigning each inlet
a unique primary subqueue during each transmission
slot.
A group of w consecutive transmission slots is referred to as a window and N consecutive windows as
a frame. The frames of all inlets are synchronized.
Within a frame, the CWC scheme selects one of the
subqueues to be the primary subqueue for transmission at each inlet during a window. The selection is
done in a sequential fashion and started from distinct
subqueues at all inlets. Thus, a transmission window

%

The Design of Circular Window
Control Scheme

A generic packet switch is a switch which routes
packets from N inputs to N outputs, of which all the
lines have the same transmission capacity. We assume
that packets are of the same size, that time is slotted
with the slot size being equal to the transmission time
of a packet on a line, and that the slot boundaries on
all input lines are synchronized. In other words, the
operation of the switch is synchronous.
An input queue switch architecture is shown in Figure 1, which contains buffer space at each inlet t o the
switch. When a packet arrives it is placed in the buffer
space provided there is space, and awaits access to
the switch fabric. When the queues are served in a
FIFO manner, only packets at the head of the input
queues are considered for transmission during the current transmission slot. When contention occurs, all
the contending packets, except the winner, must be

b.

117

is circulating between the subqueues at an inlet and
during each slot, the windows at all inlets are located
at subqueues designated to different outlets. For example, if subqueue i is primary subqueue for the first
w time slots, then subqueue ((i
1) mod N ) will be
the primary subqueue in the next w time slots. When
the packets transmitted are from primary subqueues
of all inlets, the CWC scheme guarantees the pattern
causes no output contention. Furthermore, if the i-th
inlet starts the primary subqueue from the i-th subqueue, the packets from the primary subqueues form
a circular permutation which is passable in a banyanbased multistage interconnection network and lead to
contention-free transmissions.
The CWC scheme is detailed in Figure 3. During the current window, each inlet will attempt to
transmit from its primary subqueue. But if the subqueue is empty, an alternate subqueue is chosen according to several strategies with the risk of having an
output conflict. Potential collisions may be incurred
in this manner however, but the potential successful transmissions may compensate for the otherwise
loss of bandwidth (in particular when the load is not
high). The objective of an alternate queue strategy is
to choose a packet for transmission which will cause
no, or a very small number of, collisions with packets
from the other inlets. In Figure 3 three strategies are
given to illustrate the argument. A trivial choice is to
choose any subqueue which has a packet to send (random selection). The second strategy given is one in
which a packet is chosen from the first subqueue which
has a larger queuelength than the windowsize (flsw).
The advantages of this strategy is two-fold, first not
all the subqueues need to be checked on the average
(and thus reduces the complexity of the scheme), and
second the buffer overflow problem is thus addressed
by selecting packets from these subqueues. The third
strategy addresses purely the buffer overflow problem.
Alternate queue strategies can be switch fabric specific. For non-blocking switches, it is sufficient to provide an input pattern which is a permutation, but for
banyan-based switch fabrics, this condition is not sufficient. Though the same strategies described in the
previous paragraph can be used, they may generate
internal blocking with primary packets and or alternate packets from other inlets. The CWC sc eme can
further specify a priority to be given to packets from
primary subqueues or from alternate subqueues. In
the case of banyan-based fabrics, priority should be
given to primary queue packets as they constitute a
conflict-free pattern (primary packets will never conflict among themselves). The priority can also help
control congestion by assigning a higher priority to
packets from longer queues, thus reducing the probability of dropping packets when the queue length is
finite.
A CWC example is given in Figure 4 where the primary subqueues are shown in a circular fashion of 0, 1,
2 and 3. The window size here is assumed to be 2. The
shaded areas indicate the packets which are selected
from alternate subqueues while the primary subqueues
are empty during these transmission slots. It can also
be seen that the throughput can be enhanced by se-

<

for (inlet = 0; inlet < N; inlet++)
subqueue = (inlet+((current-time/window-size)
HOD N) HOD N;
if (subqueue != EHPTY) {
// transmission from the primary //
consider head-of-queue for transmission;
1 else // obtain another subqueue //
// transmission from an alternate //
STRATEGY-1: choose a random subqueue;
STRATEGY-2: choose the first subqueue whose
queuelength is larger than the
window-size;
STRATEGY-3: choose the longest subqueue;

+

b

<

1
Figure 3: The Circular Window Control Scheme
lecting packets from alternate subqueues rather than
not transmitting any since even non-primary packets
may result in a nonconflicting transmission.

Figure 4: An Illustrating CWC Example
To implement the CWC scheme, we can construct
the subqueues and the CWC control mechanisms in
front of the switch systems. Although the figure 2
shows multiple subqueues per inlet, we can implement
a shared memory buffer for each inlet. During a cell
period, there will be at most one cell read and one
cell write at each memory buffer. For instance, for
a 1.25 Gbps line, a cell period is approximately 340
nsec. During this cell period, each inlet could receive
an arriving cell and deliver one to the switch fabric.
Assuming the buffer is with 32-bit data path and each
cell is 64 bytes (53 octets of ATM cell plus additional
routing and control bytes), a memory cycle time of 10
nsec will be enough to sustain a pair of cell read and
write operations within the 340 nsec period.
As illustrated in Figure 5, the shared buffer is divided into two parts: a cell memory and a pointerqueue buffer. The cell memory keeps all waiting cells
which are accessed through pointers. The pointerqueue buffer implements linked lists of pointers in a
shared memory for subqueues and a free list. A microcode controller can be employed to handle the queueing operations. After examining the destination of an
arriving cell, the controller can obtain a free cell-buffer
and the corresponding pointer from the free list, enqueue the pointer into a subqueue. Then, with the
pointer to the cell memory, the controller writes the
cell plus control information into the cell memory. To

118

ity analysis of the CWC scheme. The schedulability
is measured by considering only the arrivals within a
fixed size frame. No queueing is incorporated across
the frames. The comparisons of a perfect scheduling algorithm, a random selection scheduling and the
CWC scheme are provided.

pointerqueue

m

11"-

3.1

:h
write

Po*

Figure 5: CWC Shared Buffer Design
deliver a cell to the switch, the controller should retrieve a pointer to a waiting cell from a scheduled subqueue. The waiting cell is subsequently read out for
switching. Note that the routing tag should be determined during the write phase and is ready to be
used for switching when a cell is read out from the
cell memory. The dequeue and enqueue operations
must be done during a cell period. Given the example
of 1.25 Gbps transmission rate, the 340 nsec period is
quite enough to handle these operations.
In addition to the cell memory and the pointerqueue buffer, the CWC scheme itself is implemented
in the transmission sequencer. After setting up the
initial parameters and and synchronizing the frames,
the sequencer determines the transmission order of the
subqueues and select the primary subqueue. In case
of an empty primary subqueue, the sequencer must be
able to select an alternate subqueue.
It is incorrect to classify the CWC scheme as an approach on the order of N 2 , simply based on Figure 2.
Although there are N subqueues per inlet, the implementation discussed above shows that there are one
cell read and one cell write operations per cell period
in the cell memory, and one dequeue and one enqueue
operations in the pointer-queue buffer. Thus, the complexity of operations at each inlet is constant. With
a very reasonable width of data path, all eqnqueue,
dequeue, read, and write operations can be carried
out easily during a cell period. Certainly, the input
buffer does not reach the nonblocking property as in
the output queue scheme where a shared buffer must
allow multiple readlwrite operations per cell period.
As in the Growable Switch design 6,7], the cost of an
output buffer approach is to have t f,e data bus capable
of reading or writing an entire cell within 10 nsec for
a transmission rate of 2.5 Gbps.

3

Analytic Models for the
Scheme

Throughput An

Assume that there are infinite buffers at each inlet and that the traffic is uniformly distributed where
all destinations are requested with an equal probability. The switch is under a full load condition, in other
words, a packet arrives at each inlet during each transmission slot. The interesting measure is the maximum
throughput under the CWC scheme.
Let all subqueues be empty initially. Let M i j ( k )
denote the number of packets transmitted up to and
including the k-th frame from the j-th subqueue of
the i-th inlet. Due to the symmetric nature, we may
use the abbreviation M ( k ) to represent Maj(k). The
throughput at each output port is then

Let's consider two revised schemers. The first one is
similar to the CWC except that it transmits no packet
if the primary subqueue is empty. The second one is
similar to the first one, except it considers only the
packets arrived before the current frame for transmission. The packets arrived during the current frame
will be delayed. Let M'(k) and M"(&>be the number
of packets transmitted from a subqueue up to the k-th
frame under these two schemes respectively. Clearly,
M ( k ) 2 M ' ( k ) 2 M"(k)for all k > 0. Under the
second scheme, denote a(k) a the number of packets
arrived at a subqueue during the k-th frame. u(k) follows the Binomial distribution with parameters wM
and I l N for all k . The subqueue length at the end of
the k-th frame, q ( k ) , can be specified as the following
Markov chain:

+ 1) - w

if q ( k ) 2 w
otherwise

With the mean of a(k) equals to w , the balance equation of the above Markov chain has no stationary solution and all states are transient. Thus, Prob{q(k) e
tu) = 0 as k -+ cm and limk,,M"(k)
= wk with
probability 1. This implies that e'l = 8' = e = 100%.
Note that this maximum throughput is achievable
in both nonblocking switches and any banyan-based
switches, as long as the packet permutations formed
by the primary subqueues are passable through the
switches. Moreover, this maximum throughput is not
affected by the strategy of selecting an alternate subqueue when a primary subqueue is empty.
3.2 Schedulability Analysis ~f the CWC

CWC

In the following, we present the performance evaluations of the CWC scheme. We shall first show that
the maximum throughput under the CWC scheme can
reach 100% under a uniformly distributed traffic and
with infinite buffers. Then, we turn to a schedula(bi1-

Scheme
The CWC scheme basically provides a mix of deterministic and stochastic scheduling during a frame.
The deterministic part is incurred when the primary

119

subqueue is not empty, otherwise, the scheme takes a
stochastic approach to select an alternate subqueue.
In order to evaluate the schedulability of the CWC
scheme, we examine the average throughput of a nonblocking switch under the following switch system.
The switch system has a full load such that, during every transmission slot, there is a packet arriving at each
inlet. The packets arrived during the current frame
are batched and are transmitted in the next frame. If
a packet cannot be delivered in the next frame, it is
lost. Thus, we explicitly exclude the effect of queues.
It is also assumed that the packet arrival processes at
all inlets are i.i.d. and the packets' destinations are
uniformly distributed among all outlets. The probability of having i packets destined to a specific outlet
among K arrivals is then:
&(K)=

( 7)

($i(l-

0.751
2
2
4
4
4

wN-1

i=O

1

0.635
0.684
0.644
0.635

I

I

w packets, the inlet will select some packets destined

to other outlets. An alternate packet can be successfully transmitted only if the slot is neither used by
a packet from another inlet's primary subqueue dedicated to the same outlet during the current window,
or when no other alternate packets are destined to the
same outlet.
Thus, the average number of packets transmitted
successfully during a window at an inlet can be approximated by:

--yi
1
N

i c u i ( w ~+
2 )w~

I

0.965
0.914
0.952
0.975

0.831
0.811
0.808
0.877
0.862
0.859

Table 1: Schedulability analysis

/normalsize where 0 5 i 5 K .
Three scheduling schemes are considered here: a perfect
scheduling algorithm, a random selection scheduling, and
the CWC scheme. The perfect scheduling algorithm is
assumed to be capable of transmitting up to W N packets
per outlet. It cannot reach a 100% throughput since, with
the random arrivals, there may be less than w N packets
destined to a single outlet. Thus, the throughput at a
single outlet is:

ep = -(W1N

I

64
4
16
64

0.839
0.825
0.822
0.882
0.871
0.869

wN

w-1

w-1

w-1

i=w

i=O

i=l

j = i k=O

j

where the first two terms count the packets from the
primary subqueue and the last term is for the possible transmissions from alternate subqueues. Thus the
average throughput is:

wNa

ai(W~2))
i=wN

a j ( K ) . The above throughput can
Let Pi(K) =
be formulated as

wN

w-1

w-1

j

i=w

wN-1

When the random selection scheduling algorithm is
used, each inlet selects a packet randomly to transmit.
In the event of an output contention, the losers will
be dropped. Thus, based on the analysis in [15], the
throughput per outlet is

w-1

w-1

i=O

i=O

A numerical comparison of the throughputs is provided in Table 1 with different values of w and N . In
the table, we also show the simulation results of the
throughput under the CWC scheme. The error between the approximation and simulated model is less
than 3%. These numerical results illustrate that the
schedulability of the CWC scheme is much higher than
that of a random selection scheme and is comparative
to a perfect scheduling algorithm.

er = 1 - (1 - -N1) N
The average throughput of CWC scheme under this
operation model can be approximated by examining
the successful transmissions during a window at an
inlet. Assume that the priority is given to the packets
transmitted from the primary queues.
In a given
window with a size of w slots, w packets are guaranteed to be transmitted if the corresponding subqueue
holds at least w packets. If the subqueue has less than

4

Simulation Results
In addition to the maximum throughput analysis

and schedulability evaluation, we have performed simulations for the CWC scheme for various conditions.
The main measures we are interested in are throughput and mean waiting time under finite length queues
and shared queues. For all results reported in the following, the confidence interval is within 5% at 95%
confidence level.

l T h i s assumption does not affect the throughput measure,
but makes the traffic analysis at each inlet simple.

120

0.9 -

0.8 -

0.7

0.4

-

Uniform T r d f i c in CWC Crossbar with No Sharing

1.o

-

Buffer = 2
Buiier = 4
<c* Buffer e
Output Queue
Input aueue

_____.

uniform Traffi~in CWC Banyan Swimhing Frnbic

.o

1

1

o.e

--__

-

0.7

1

0.4

t

1
1

0
.
1
K 0:2

0:s

0:s

0:6

0:e

0:7

0:e

0:s

,io

Offolad I-ond

Figure 7: Throughput of CWC banyan switch systems

Figure 6: Throughput of CWC crossbar switch systems

64, respectively. Under the CWC scheme, the sourcedestination permutations are passable in the banyan
switch if all inlets choose the packets from their primary subqueues. Thus, no switch conflict will occur.
It can be seen that even with subqueue lengths of 2
a significant improvement can be obtained over the
pure input queued switch system. Due to head-of-line
blocking, the pure input queued banyan fabric reaches
saturation at a load of approximately 0.4. The output
queueing approach is ignored here since the approach
requires that all packets be able to join their destination outlets simultaneously and no switch conflict can
be allowed.
In Figure 8, we present the mean waiting time for
CWC crossbar switches with infinite and finite queues.
The mean waiting time under the pure output, pure
input queued schemes are also included in the figure.
The figure shows that the CWC scheme provides a
good tradeoff in terms of throughput obtained and
mean waiting time. Though the CWC architecture implements an input queue, the mean delay is much less
than that of a FIFO input queue switch of the same
size. When the input load is light, a transmission of
packets from an alternate subqueue may not cause a
contention since the corresponding primary subqueue
of another inlet is likely to be empty. On the other
hand, if the load is heavy, most packets are selected
from the primary subqueues which again incur no contention. Thus, the regulated traffic under the CWC
scheme produces much less contentions. Compared to
the output queued switches, the CWC h a a longer
delay. Existing contentions and window circulation
cause the CWC to incur this longer delay.
Another advantage of the CWC scheme is that traffic patterns which are otherwise detrimental in switching systems, such as the “hot spot”, can be tolerated
by the CWC scheme. By eliminating the head-of-line
blocking, CWC switched systems are able to select
packets destined to outlets other than the heavily requested destinations. Having shared queues will eventually saturate all the buffer capacity with hot mes-

In all experiments, we assume that the window size
is equal to one. An advantage of having a window size
of one is the interleaved packets in each output stream.
Assume that the switch is with a high load. Under the
CWC scheme, the packets arrived in an outlet are sent
from the primary subqueues of all inlets. Thus, the
continuous packets in the output stream of the switch
are come from different sources. When this stream
arrives to the next stage of switch systems, they will
likely fan out to different destinations. Certainly the
sequential order of packet transmission between any
pair of source and destination is still preserved.
As shown in the previous section, the maximum
throughput of CWC scheme can reach 100% when the
queue length is infinite. However, if the queue length
is finite, the maximum throughput cannot reach the
maximum. With a crossbar switch, Figure 6 shows the
throughput of the CWC scheme under various buffer
sizes compared to pure input and pure output queueing switch systems. The switch size is 64 x 64 for the
results presented in this figure. The queue length is
assumed to be infinite in both pure input and output queueing switches. The CWC switching systems
considered have finite subqueues of size 2, 4, and 8
at each inlet respectively. The traffic is assumed to be
uniform and the selection of an alternate queue is random. Under these conditions, it can be seen that the
CWC switching systems have throughputs drastically
better than the input queued switches. The throughput of the CWC scheme approaches the output queued
scheme as the buffer size increases.
When a blocking switch fabric such as the banyan is
used, the CWC can improve the throughput by eliminating the head-of-line blocking and switch conflict.
Figure 7 shows the throughput of the CWC scheme implemented for banyan switches of sizes 16 and 64, with
shared or dedicated buffers. The number of buffer
spaces at each subqueue is 2 for dedicated buffers at
each inlet. With shared buffers, each inlet has a total 32 and 128 buffers for the switches of size 16 and

121

,, CWC

0.7

0.4

I

Crossbywith U n i f y nnd HptSpot Tdfl~
,

,

1

D d QurudUnitom,

Q---E~
~--+3Ded OueurlHotSDot
QueurlHotSpot

k
-Shared
Shared QurueNnlform
QurueNnlform
Sharad QueudHotSpot

r

0. i
0.i

0.2

0.3

0.4

0.6

0.6

0.7

0.8

0.9

i.0

0 f f - d Lad

Figure 8: Mean waiting times of CWC scheme vs. outputlinput queueing schemes

Figure 9: Throughput of CWC schemes under hot
spot traffic

-

C W C Crossbar with Uniform md HotSpot Tkdfic:

sages, and cause the throughput to reach a definite
saturation. Dedicated queues will cause many hot
messages to be dropped due to the early saturation of
their subqueues. This is reflected in Figure 9, where
CWC crossbar switches of 64 x 64 with buffer capacity
of 128 buffers per inlet (dedicated or shared , under
uniform or hot spot traffic. We assumed that % of all
arriving packets are destined to a hot spot. When the
arrival rate becomes larger that 78%, the hot spot will
be saturated. Figure 10 shows the delay incurred by
these schemes and the imposed traffic. The mean delay of the shared queue schemes is significantly higher
due to the saturation of the shared queues with the
packets destined to the hot spot. This saturated traffic causes not only more conflicts but also forces nonhot messages to be dropped due to buffer overflow. A
threshold of the maximal buffer space that a subqueue
can occupy must be established to avoid the buffer
possible buffer starvation due to hot spot traffic.
From the above simulation results, we believe that
the performance of the CWC scheme can be improved
in several ways. The results shown until now, assumed
that alternate packets are selected in a random fashion. Selection of packets from alternate queues can be
done in different ways. In the jlsw selection, a packet
is chosen from the first subqueue (in a circular fashion) which has a queue length larger than the window
size. Choosing from the longest queue has as an immediate advantage that the flow is controlled at the
same time and the possible blocking due to insufficient buffer space is reduced. Moreover, priority can
be given to the packet selected from a longer subqueue
when a switch contention occurs. This will reduce the
blocking probability as well as the probability of having an empty primary subqueue. Thus, the probability
of having output contentions will also be smaller.
The CWG scheme can obtain a reasonable blocking probability, even with high input loads. This is
shown in Figure 11 where the blocking probability of

a

240.0

-.

220.0

-

2W.o

-

ieo.0

g

d
:g

160.0

.

D d QueueNnlfonn

Q--a Ded Qu-udHotSpot

Shared QueueNnifom,
-Shared
QueuoMotSpot

-

E?.

E

i40.0

1

120.0

-

lW.0

1

80.0

-

Figure 10: Mean Waiting time of CWC schemes under
hot spot traffic
both the CWC and output queued switches are shown
for input loads of 0.8 and 0.9 for an 8 x 8 crossbar
switch. The curves for the CWC scheme were obtained
by means of simulation whereas those for the output
queued scheme were obtained by analytical means as
described in [9]. For an offered load of 0.8 the blocking probability approaches the blocking probability of
output queued switches quite rapidly.

5

Conclusion

In this paper, we presented a simple and efficient
traffic regulation scheme for input queueing switch
fabrics. It is clear that the scheme can be implemented
in the input queues and be independent of the switch
fabrics. The queues must be synchronized in terms of
the choice of primary subqueues, but no interaction is
needed during the transmission.
The performance gain of the CWC scheme over the

122

-4.0

a-1.0

__------ ..

_ _ _output
_ _ . QUIUIlng p

----

-

~

-

Output Queueing p

cwc p

-_-cwc

0.8

--

[7] Eng, K. Y., and Mark J Karol, “High-Performance
Techniques for Gigabit ATM Switching and Networking,” IEEE I C C , 1993, pp. 1763-1678.

0.8

0.0

p-0.0

[SI Giacopelli,

J. N., et al, “Sunshine: A High-Performance SelfRouting Broadband Packet Switch Architecture,”
IEEE Journal on Selected Areas in Communication, Vol. 9, No. 8 (October, 1991), pp. 1289-1298.

[9] Hluchyj, M. G., and M. J. Karol, “Queueing in
High-Performance Packet Switching,” IEEE Journal on Selected Areas in Communication, Vol. 6,
No. 9 (December, 1988), pp. 1587-1597.

e-4.0

m
0-6.0

’

6-7.0
6.0

16.0

26.0

[lo] Huang, A., and S. Knauer, “Starlite: A Wideband Digital Switch,” IEEE Proceedings of 1984
Globecom, pp. 121-125.

3
Buff-* SLO

[ll] Hui, J., and E. Arthurs, “A Broadband Packet
Switch for Integrated Transport,” IEEE Journal
on Selected Areas in Communication, Vol. 5, October 1987, pp. 1264-1273.

Figure 11: Blocking Probability vs Buffer Size
FIFO input queueing system is significant. With moderate queue size, the throughput can be close to that
of the output queueing switch system. The mean waiting time of the CWC scheme is much less than that
of the FIFO input queueing systems. This achieve
ment is based on the facts that, if the load is high, the
packets transmitted are chosen from the primary subqueues and cause no switch contention, whereas when
the load is low, the selection of packets from alternate
subqueues can utilize the available switch bandwidth.
The most interesting finding is that the CWC scheme
can make a banyan blocking switch behave just like
a non-blocking switch and experience minimal switch
contentions.

[12] Iliadis, I., and W. Deneel, “Performanceof Packet
Switches with Input and Output Queueing,” Proc.
ICC/SUPERCOM 1990, April 1990.
[13] Karol, M. J., M. G. Hluchyj, and S. P. Morgan, “Input Versus Output Queueing on a SpaceDivision Packet Switch,” IEEE Transactions on
Communications, Vol. 35, No. 12 (December
1987), pp. 1347-1356.
[14] Oie, Y., et al, “Survey of the Performance of
Nonblocking Switches with FIFO Input Buffers,”
Proceedings Internataonal Communications Conference, 1990, pp. 316.1.1-316.1.5

References

[15] Patel, J .H., “Performance of Processor-Memory
Interconnections
for
Multiprocessors,” IEEE Transactions on Computers,
C-30, lO(0ctober 1981) pp. 771-780.

[l] Arakawa, N., A. Noiri, and H. Inoue, “ATM Switch
for Multi-Media Switching System,” ISS, Vol. 5,
1990, pp. 9-14.
[2] Bingham, B., and H. Bussey, “Reservation-Based
Contention Resolution Mechanism for BatcherBanyan Packet Switches,” Electronic Letters Vol.
24, No. 13, (June 1988), pp. 772-773.

[16] “A 1.28 Gbps 16*16 CMOS Ship Set for an
Output-buffer ATM Switch,” IEEE Symp. on
VLSI Circuits, 1992, pp.76-77.
[17] Yeh, Y., M. Hluchyj, and A. Acampora, “The
Knockout Switch: A Simple, Modular Architecture for High-Performance Packet Switching,”
IEEE Journal on Selected Areas in Communication Vol. 5, October 1987, pp. 1274-1283.

[3] Chen, J., and T. Stern, “Throughput Analysis,
Optimal Buffer Allocation, and Traffic Imbalance
Study of a Generic Nonblocking Packet Switch,”
IEEE Journal on Selected Areas in Communications, Vol. 9, No. 3, (April 1991).
[4] Cisnercs, A., “Large Packet Switch and Contention Resolution Device,” ISS, Vol. 3, 1990, pp.
77-83.
[5] Cooperman, M., and P. Andrade, “CMOS Gigabitper-second Switching,” IEEE Journal of SolidState Circuits, Vol. 28, No. 6, 1993, pp. 631-639.
[6] Eng, K. Y., Mark J Karol, and Y. S. Yeh, “A
Growable Packet (ATM) Switch Architecture: Design Principles and Applications,” IEEE 5%”
Comm., Vol. 40, No. 2, Feb. 1992, pp. 423-430.

123

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-33, NO. 2, FEBRUARY 1984

Design and

113

Evaluation of a Fault-Tolerant

Multiprocessor Using Hardware Recovery Blocks
YANN-HANG LEE,

STUDENT MEMBER, IEEE, AND

KANG G. SHIN,

Abstract -In this paper we consider the design and evaluation
of a fault-tolerant multiprocessor with a rollback recovery
mechanism.
The rollback mechanism is based on the hardware recovery
block which is a hardware equivalent to the software recovery
block. The hardware recovery blocks are constructed by consecutive state-save operations and several state-save units in every
processor and memory module. Upon detection of failure, the
multiprocessor reconfigures itself to replace the faulty module and
then the process originally assigned to the faulty module retreats
to one of the previously saved states in order to resume faultfree execution.
Due to random interactions among cooperating processes and
also due to asynchrony in the state-savings, the rollback of a
process may propagate to others and thus the need of multiplestep rollbacks may arise. In the worst case, when all the available
saved states are exhausted, the processes have to restart from the
beginning as if they were executed in a system without any rollback recovery mechanism. A mathematical model is proposed to
calculate both the coverage of multistep rollback recovery and the
risk of restart. Also presented is the evaluation of mean and
variance of execution time of a given task with occurrence of
rollbacks and/or restarts.

SENIOR MEMBER, IEEE

design approaches for error detection: 1) detect an error upon
its occurrence, and 2) isolate the erroneous information before it is propagated. For the first approach, the most widely
used techniques are error detection/correction coding, addition of built-in checking circuits (e.g., voting hardware), etc.
Error detection schemes such as consistency test, execution
of validation routines, or acceptance test are typical examples
for the second approach. Following the detection of error, the
faulty components, which are the source of error, are localized and replaced so as to enable the system to be operational
again. In order to recover from an error, the rollback recovery
method or the reinitialization of a fault-free subsystem is
usually invoked in order to resume the failed computation.
Both methods consist of state restoration and recovery point
establishment. In JPL-STAR system [1] the recovery points
are defined by the application program which also takes the
respons'ibility of compensating for the information prior to
the recovery point. Hence, its error recovery capability is
constructed in the application software level. On the other
hand, the strategies used in PLURIBUS [2] are to organize
Index Terms -Fault-tolerant multiprocessor, hardware/ the hardware and software c'omponents into reliable subsoftware recovery blocks, performance of rollback recovery
systems and to mask errors'above the interface level of a
mechanisms, rollback propagation.
subsystem. When an error is detected, the subsystem performs
backward recovery by restarting the subsystem.
I. INTRODUCTION
The conventional restart recovery technique could be
THERE are numerous benefits to be gained from a costly and inept since 1) the computation between the start of
multiprocessor. In addition to the decreasing of hard- task and the time when error is detected has to be undone, and
ware costs and the inherent reliability of LSI components, the 2) if the task is distributed over different processing units
capacity of reconfiguration makes the multiprocessor even in the multiprocessor, it is difficult to provide a consistent
more attractive when system reliability is important. It is task state and to isolate a subtask to prevent the propagaparticularly essential to critical real-time applications that the tion of erroneous information to others. (This may lead to
system be tolerant of failure with minimum time overhead the restarting of the entire task and may result in high reand that the task be completed prior to the imposed deadline. initialization overhead.) The rollback recovery method at the
Hence, one of the major issues of reliable multiprocessor software level is devised to tolerate designfaults but may not
design is to provide the capability of error recovery without be effective for tightly coupled processes since 1) the softhaving to restart the whole task in case of failure.
ware recovery points by themselves in each process are not
In general, the tolerance of failure during system operation sufficient to recover the task unless they belong to the same
is achieved by three steps: detection of error, reconfiguration recovery line [3], and 2) the program designers have to strucof system components, and recovery from error. The purpose ture carefully the parallel processes so that the interacting
of error detection is to recognize the erroneous state and to processes establish recovery points in a well-coordinated
prevent a consequent system failure. There are two basic manner. Several alternatives have been proposed; for example, the conversation scheme [4], the interprocess communication
primitives in a producer-consumer system [5],
Manuscript received July 30, 1982; revised May 15, 1983. This work was
supported in part by NASA under Grant NAG 1-296. Any opinions, findings, the programmer-transparent scheme [6], [7], the system deand conclusions or recommendations expressed in this publication are those of fined checkpoints [8], the decentralized recovery control
the authors and do not necessarily reflect the views of the funding agency.
The authors are with the Department of Electrical and Computer Engineer- protocol [9], etc. These methods could lead to a loss of
ing, University of Michigan, Ann Arbor, MI 48109.
efficiency in the absence of error, the accumulation of a

0018-9340/84/0200-0113$01.00

© 1984 IEEE

114

large amount of recorded states for heavy interprocess
communications, or some undesirable restrictions in csommunication schemes.
However, the concept of the recovery block, proposed by
Randell [3], [4] -and Horning [10], can still be useful for
tolerating hardwar'e faults in the multiprocessor. In this paper, we employ this concept to construct a hardware recovery
block which enables the task to survive processor or memory
failures. In order to resume a failed process, an error-free
process state
which includes the status of internal registers
of the assigned processor and the process variables stored in
memory
should be restored. The hardware recovery block
is constructed in a quasi-synchronized manner and saves all
states of a process consecutively and automatically. This
happens in parallel with the execution of the process by using
a special state-save mechanism implemented in hardware.
The hardware recovery block is different from the software
recovery block which only saves nonlocal states when a
checkpoint is encountered. Moreover, instead of the assertions in the acceptance test of the software recovery"block,
the hardware resources are tested by embedded checking
circuits and diagnostic routines. After an error is detected and
the faulty component is located, the system will be recon,figured to replace the failed hardware module. By loading the
program code and transferring the recorded states into the
replacement module, the original process can be resumed.
The multiprocessor with the hardware recovery block
scheme takes advantage of the fact that a large number of
processors and memory units can be made available at
inexpensive costs for fast recovery from hardware failures.
Furthermore, it only requires a minimal amount of time for
establishing recovery blocks, which in turn significantly improves system performance.
For both hardware and software recovery blocks, the rollback of the failed process alone to the previous state is not
sufficient for concurrent processing. The rollback of one
process may propagate to other processes or to a further
recorded state. (This is called rollback propagation.) The
worst case is when an avalanche of rollback propagations,
namely the domino effect, occurs. The -domino effect is impossible to avoid if no limitation is placed on process interactions [8]. Instead of placing any of such limitations, several
consecutive states are saved so that the processes are allowed
to roll back multiple step's in case of rollback propagation.
(This methiod is here termed the automatic rollback recovery.) The coverage of a multistep rollback -the pTrobability
of having 'a successful rollback recovery when cooperating
processes roll back multiple steps
should be examined to
decide the effectiveness of this method. Both the recovery
overhead and the computation loss resulted from this automatic rollback recovery mechanism should also be studied
carefully. Furthermore, since the time interval between two
consecutive state savings is related to the final performahce
figure of this method, the optimal value of this interval has to
be determined.
This paper is divided into five sections. Since the cdhstpction of hardware recovery blocks in the multiprocessor plays
a basic role, we review it briefly in Section II. The detailed

1EEE TRANSACTIONS

ON COMPUTERS, VOL.

c-33,

NO.

2,

FEBRUARY

1984

description can be found in [ 1 1], [ 12]. In this section, we also
extend our previous design to a general multiprocessor on
which our hardware fault recovery can be implemented. Section III presents an algorithm to detect rollback propagations
among cooperating processes and also proposes a model
to evaluate the coverage of multistep rollback recovery.
Section IV uses the results of Section III and deals with
the analysis and estimation of performance in terms of the
mean and variance of the task completion time. The paper
concludes with Section V.
II. AUTOMATIC ROLLBACK MECHANISM
FOR A

MULTIPROCESSOR

The multiprocessor under consideration has a general
structure and consists of processor modules, interconnection
network, and/or common memory modules. To benefit from
the locality of reference, every processor module owns its
local memory which is accessible via a local bus. Every
processor module can also access the shared memory through
the interconnection network. The rollback recovery operations of a task can be applied to two types of multiprocessors:
in one, there is no common memory, but local memory of one
processor module is accessible by other processor modules
(e.g., Cm* system [13]); in the, other, the system is equipped
with separate common memory modules [14] and restricts
the access of local memory only to the resident processor.
Thes'e two types are representatives of contemporary generalpurpose multiprocessors.
A. Processor Module, Common Memory, and

State-Save Mechanism
A basic processor module (PM) in the multiprocessor comprises a processor, a local memory, a local switch, state-save
memory units (SSU's), and a monitor switch as shown in
Fig. 1. It is assumed that a given task is decomposed into
processes each of which is then assigned to a processor module. The shared variables among these cooperating processes
are located in the shared memory which is either separate
common memory or local memories depending upon the
multiprocessor structure discussed above. Thus, each process in a PM can communicate with oth'er processes (allocated to other PM's) through the shared variables. Each PM
saves its states (i.e., process local variables and processor
status) in SSU's at various stages of execution; this operation
is called a state-save. Ideally, it would be preferable to save
states of all processes at the same instant during the execution
of task. Because of the indivisibility and asynchrony of instruction execution in PM's, it is difficult to achieve this ideal
case without forced synchronization and the consequent loss
of efficiency. In order to alleviate this problem, we employ
a quasi-synchronized method in which an external clock
sends all PM's a state-save invocation signal at a regular
interval, T,. This invocation signal will stimulate every PM
to save its states as soon as it completes the current instruction and then to execute a validation test. If the processor survives the test, the saved state would be regarded as
the recovery poin-t for the next interval. If the processor fails
the validation test or an error is detected during execution of

115

LEE AND SHIN: FAULT-TOLERANT MULTIPROCESSOR DESIGN AND EVALUATION

Time

1PMI
S

IS

CM
-

.I

<*
_ +
<

--

-

_

-

--

-

-

___
-

State-save invocation

Complete the current instruction

Save internal state
Execute validation process

State switch betwen SSU's
I~

~

~

MS

PI
S

Start normal process, SSU update,
SSU transfer, and error detection

2
IPMm
CM,

IPM,

L.~~~~~~~~-I

P - processor
S = switch
MS = monitor switch
LM = local memory

CM = common memory
AC = access controller
SSU = state-sove unit

Fig. 1. The organization of a fault-tolerant multiprocessor
using a rollback recovery mechanism.

the resident process, the system will be reconfigured to replace the faulty component and the associated process will
roll back to one of the previously saved states. The detailed
operations of state saving and rollback recovery are shown
in Fig. 2.
Similarly to a processor module, each common memory
module (CM) also contains state-save memory units and a
monitor switch. These SSU's are used to record the updates
of CM only. The acce-ss requests of CM are managed by an
access queue on the basis of the first-come-first-serve discipline. When a PM refers to a variable resident in a CM, an
access request is sent to the destination CM through the interconnection network and enters the access queue associated
with the CM. When all the preceding requests to this CM are
completed, the access request will be honored and a reply
will be sent back to the requesting PM. When a state-save
invocation is issued, a state-save request is placed at the tail
of every access queue. Thus, the state-save in CM is performed when the requests made prior to the state-save invocation have been completely serviced.
During a state-save interval, besides the normal memory
reference or instruction execution, certain operations are automatically executed; for example, an error correcting code
is used whenever a data is retrieved from memory. Some
redundant error detection units also accompany the processor
module [15], e.g., dual-redundancy comparison, address-inbound check, etc. These units are expected to detect malfunctions whenever the corresponding function units are
used. An additional validation process which could be the
execution of a diagnostic routine is u-sed to guarantee that
the saved state be correct, and thus guards against the existing
fault extending to the next state-save interval.
Suppose there are (N + 1) state-save units for every PM
(and every CM), called SSUI, SSU2, ' * SSUN+1. These units
are used for saving states at (N + 1) consecutive state-save

-1

__Fail
- - - - Retry the process
- - - - Fail again
-

-Declara permanent fauJt, stop processes,
check propagation, and migrate
failed process to other PM

- - - - Resume process

Fig. 2. Sequence of a rollback recovery.

intervals. Thus, each PM or CM is able to keep N valid states
saved in N SSU's and record the currently changing state
in the remaining SSU. As shown in Fig. 3, the SSU1,
SSU2,.* * SSUN are so arranged to record the states for consecutive state-save intervals T(i + 1),* T(i + N) and the
SSUN+j is used to record the updates in the current state-save
interval, T(i + N + 1). To minimize the time overhead required for state-saving, the saving is done concurrently with
process execution. Every update of variables in the local
memory is also directed to the current SSU. When a PM or
CM moves to the next state-save interval, each used SSU will
age one step and the oldest SSU will be changed to the current
position if all SSU's are exhausted. The monitor switch is
used to route the updates to SSU's and to manage the aging
of SSU's. Therefore the state-save mechanism of each PM or
CM provides an N-step rollback capability. In Section III, we
will show that only a small number of SSU's are sufficient to
establish high coverage of rollback recovery for typical multiprocessor applications.
Since the update of dynamic elements is recorded in only
one SSU, the other SSU's are ignorant of it. This fact may
bring about a serious problem: the newly updated variables
may be lost. In order to avoid this, it is necessary to make the
contents of the currently updated SSU identical with that of
the memory or to copy the variables that have been changed
in the previous intervals into the current SSU. A solution to
this problem has been discussed in our previous paper [11].
At each state-switching instant, the current SSU contains not
only the currently updated variables, but also the previously
updated variables. -Consequently, the contents of the current
SSU always represents the newest state of the PM or CM.

116

IEEE TRANSACTIONS ON COMPUTERS, VOL.

i

state-save invocotion

[

state-saving

begins

In

0-

F-

state - save

unit used

u

SSU2

2,

FEBRUARY

1984

III. ROLLBACK PROPAGATION AND MULTISTEP ROLLBACK
_n

_

u

T(t)-T(2) +1-T(3)
SSut

NO.

serviced interactions issued by the rolled-back PM's, which
are still queued in the access queues, are cancelled.

I-Tss 1--

process

c-33,

--

.

SSu3

*

In

_

time

kT(i+N)-T(i +N+I)+T(i+N+2)
SSUN

SSUN+I

SSU1

Fig. 3. State-save operations in one module.

B. Rollback Recovery Operations of a Task
Suppose a task is partitioned and then allocated to M modules (i = 1,2,. ,M). These modules include PM's and
CM's and will be dedicated to this task until its completion.
The state saving of a task implies the state savings of these
modules. The rollback of a process is equivalent to the state
restoration of the associated modules. Since the process state
includes the internal hardware states, local variables, and
global variables, the resumption of a failed process may need
cooperation from common memory and/or other processes.
Moreover, due to arbitrary interactions between cooperating
processes and the asynchrony in state savings among them,
the rollback of one process may cause others to roll back and
it is therefore possible to require a multistep rollback (a detail
of this will be discussed in the next section). In order to make
a decision as to rollback propagation and also to perform
housekeeping jobs (e.g., task allocation, interconnection
network arbitration, reconfiguration, etc.), a system monitor
and a switch controller are included in the multiprocessor.
The switch controller handles the global variables references
and records these references for analyzing rollback propagation and multistep rollback. The system monitor receives the
task execution command and then allocates PM's and CM's
to perform the task. Both devices are defined in a logical
sense. They could be a host computer, or a special monitor
processor, or one of general processor modules in the system.
To deal with the error recovery, the system monitor receives reports from each module regarding the state-save
operations and its conditions. Once an error is detected,
the system monitor will signal "retry" to the module in question. If the error recurs, a permanent fault is declared and
the following steps are taken by the system monitor and the
switch controller.
1) Stop all PM's that are executing processes of the task
in question.
2) Make a decision as to rollback propagation.
3) Resume execution of the processes that are not affected
by rollback propagation.
4) Find a free module to replace the failed one.
5) Transfer the process or data in the failed module to the
replacement module and reroute the path to map references
directed to the faulty module into its replacement.
6) Restore the previous states of the processes affected by
the rollback of the resident process in the faulty module.
7) Any interaction directed to a module to be restored
must wait for the resumption of the module. Old and un-

In order to roll back a failed process, the consistent values
of the process variables and the internal states of the associated PM should be provided. The local variables and internal
states which are saved in the SSU's of a PM are easily obtainable. However, the shared variables - which may be located
in any arbitrary PM or CM and may be accessed by any
process -bring about a difficult problem: the rollback of a
failed process induces the rollback of other processes (i.e.,
rollback propagation occurs). The rollback propagation might
result in another inconsistent state for certain processes of
the task, thereby requiring a multistep rollback.
Furthermore, the hardware may have latent faults which
are undetectable until they induce some errors. In the following discussion, we assume that an error will be detected
immediately as it occurs. So the rollback propagation is used
only to obtain a consistent state. However, it can be easily
extended to the case in which error latency exists and is
bounded by U [16] as follows.
1) First obtain a consistent state which may entail rollback
propagations, and calculate the total rollback distance D.
2) If D : the total computation done then restart
else if D 2 U then done
else go to step 1).
A. Rollback Propagation and Multistep Rollback
In general rollback propagation cannot be avoided if the
processes interact with each other arbitrarily. For the multiprocessor organization in the previous section, a process is
allocated to one PM and/or several CM's and each module
has its own rollback recovery mechanism. So each module
can be regarded as an object for rollback propagation. An
interaction between cooperating processes is implemented as
a memory reference to a shared variable, i.e., a memory
reference across the modules. To avoid the need of tracing
every reference to the shared variables and to simplify the
detection of rollback propagation, we assume that the failure
of a particular module leads to the automatic rollback of all
modules that have interacted with the module during its current state-save interval. Let Pi -* Pj denote the rollback
propagation in which the rollback of process Pi induces the
state restoration in one or more modules containing Pj, that is,
the rollback of Pi causes Pj to roll back. Let the nth state-save
interval of Pi be Ti(n) and the beginning moment of Ti(n)
where Pi saved its state be at ti(n). An example is presented
in Fig. 4, where process PI fails at time tf and saves its state
at tl(n) during state-save interval T1(n). Since interactions
between PI and P2 exist during the time interval [tl(n), tf],
process P2 must roll back to revive the interactions when P1
is resumed. The rollback of P2 will propagate further to other
processes; in this example, P2 -* P4, PI -* P3, and P3 -* P2.
When Wood's definitions [9] are used, the state of process PI
saved at tl(n) can be regarded as a potential recovery initiator
of the saved states of P2, P3, and P4.
In the above example, we can find that the rollback of P3

117

LEE AND SHIN: FAULT-TOLERANT MULTIPROCESSOR DESIGN AND EVALUATION

t4(k)

State-save
Invocation

S ta te -s av e

P1 fails

; t,(n)

Pi

M

n

pin

u

ti (k)

r-T

Pi

0

.,

tj (k-1)

t3(k)

(b)

(a)

t1(k-1)

Pi-Hi

O 1.-1

0

1

o 1

o

1

o -o

o

0

U1

KP2

1

00

KPi

0 00
=

O

1

0

0 00

1

n -7

00

(b)
n'2

RB (n)= 1

otherwise
1
0

Interaction

Fig. 5. Interaction patterns related to rollback propagation.

0 00

101

L

RB3(n)= {

State Saving

0 00

=

(n) {

4(k)

(d)

001

O 0 1 0

RB1

t3jkD
tj (k -1)

1

10

1

KC1 =

o o o 0

0

T0n

( c)

K G2

110

4i (k)

ti (k)

ti (k)

(a)

tj (k )

,

otherwise

B

n'2

RB4(n)=

otherwise

l

In case a), the state saved at ti(k) is restorable forPi only.
A single step rollback of Pi is sufficient to recover its state.
In cases b) and c), both Pi and Pj have to roll back and the
states saved at ti(k) and tj(k - 1) are restorable for Pi and Pj
respectively, while in case d), the states at ti(k - 1) and tj(k)
become restorable.
The necessary condition for recovering a task TK, where
TK = {Pi i = 1, 2, *. M}, with rollback mechanisms can
be obtained from the above definition. The task TK is recoverable from a failure if for all i either Pi is not affected by
the rollbacks of other processes or Pi rolls back to its most
recently restorable state.
cess.

i

1

n=

O

otherwi se

(c)
Fig. 4. An example of rollback propagation and multistep rollback.

and P2 to their most recently saved state still cannot provide
a consistent task state. The reason that a rollback of cooperating processes cannot recover the process states is mainly
due to the occurrence of references between the asynchronous state savings of interacting processes. For convenience, a restorable state for P, is defined as follows.
Definition: Suppose process Pi rolls back to the state saved
at ti(k). This state is restorable for Pi if either of the following
two conditions is satisfied:
Cl) Pi has no interaction with other processes during the
state-save interval Ti(k).
C2) The rollback of Pi to ti(ki) induces the rollback of Pj to
tj(kj) forj = 1, 2,... ,-M andj 0 i, but there is no interaction
needed to be reissued between Pi and Pj during the interval
[ti(ki), tj(kj)] if ti(ki) tj(kj) or [tj(kj), ti(ki)] otherwise.
Consider the cases in Fig. 5. Suppose Pi rolls back to ti(k)
because of failure or rollback propagation from another pro-

B. The Detection of Rollback Propagation
Since every external memory reference is managed by the
switch controller, the switch controller should take responsibility for detecting rollback propagation and deciding
on multistep rollbacks. Suppose there are (N + 1) SSU's in
each module, then the maximum possible number of rollback
steps is N. Let the current state-save interval of module i be
Ti(k), then an n-step rollback will restore the module i to
the beginning of interval Ti(k n + 1) [i.e., the state at
ti(k n + 1)]. For state-save interval Ti(k n + 1),
,N), we assign two matrices KC5(M x M)
(n = 1, 2, 3,
and KP,(M x M) to represent the interactions during
Ti(k n + 1). Every element in both matrices consists of a
single bit. KC5(i,j) is set to 1 if an interaction occurs between
module i and module j during the state-save intervals
Ti(k n + 1) and Tj(k n + 1). If an interaction exists
between the two during module j's previous state-save
interval, Tj(k n), then KP5(i,j) = 1. We also define
RBi(k), k = 1, 2, * , N, to indicate the number of rollback
steps for module i. If module i rolls back n steps, then
RBi(k) = 1 for all k < n. So, if RBi(k) = 0 for all k, then
-

-

-

-

-

-

-

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-33, NO. 2, FEBRUARY 1984

118

module i does not have to roll back. The steps for setting
these elements and checking the rollback propagation are
listed below.
S1) Reset both matrices to zero at the beginning of
the task.
S2) When an interaction is issued from module i and directed to module j, then KC1(i,j) and KC,( , i) are set to 1.
S3) If module i saves its state and moves to the next statesave interval, then for j = 1, 2, * * , M
a) If Pj has already moved to its new state-save
interval, then
KPI(j, i) = KP1(j, i) + KC1(i,j)
where + is logical OR operation.
KC,(j,i) = 0
b) KC,(i,j) = KCn_10ij),

KPn(ij)

c)

KCI(i,j)

=

KPn_1(i,j)

= 0,

for n = N, N-I , * * * , 2,

KPI(ij,)

= 0.

S4) When an error is detected in module i, RBi (1) is set to
and all other RB's are reset to zero.
S5) If RBi(n) = 1 (i.e., module i rolls back at least n
steps), the switch controller checks the corresponding rows
in matrices KCG and KPn, namely KCG(i,j), KCn(j,i), and
KPn(i, j) for j = 1, 2, , M. There are four possible rollback propagations.
i) If KPn(i,j) = 1 then module j has to roll back
(n + 1) steps. Set RBj(k) for all k (n + 1) to 1.
ii) If KPn(i,j) = 0, KCn(i,j) = 1,andKCQ(j,i) = 1,
then module j also has to roll back n steps. Set RBj(k) for
all k ' n to 1.
iii) If KPn(i, j) = 0, KCn(i, j) = 1,andKCn(j, i) = 0,
then module j needs to roll back (n - 1) steps. Set RBj(k) for
all k ' (n - 1) to 1.
iv) If KPn(i, j) = 0 and KC(i, j) = 0, then there is no
direct rollback propagation from module i to module j.
S1), S2), and S3) are used to record interactions. S4)
initiates rollback in module i which may propagate to a farther state in the same module and/or to cooperating modules.
S5) deals with the determination of rollback propagations.
In the condition i) of S5), there is an interaction which
occurred in both the Pi's (k - n + I)th and the Pj's (k - n)th
state saving intervals. Thus, Pj has to roll back (n + 1) steps
to recover this interaction. The conditions ii) and iii) indicate
that an interaction occurred in the Pj's (k n + I)th and
(k n + 2)th state saving intervals, respectively. The corresponding bits of RBj are set for these conditions. Since the
rollback of Pj decided in S5) can only provide a restorable
state for -Pi, recursive checking for every j with RBj(k) = 1 is
necessary. S5) can also be easily implemented by a recursive
procedure which will cease when no more setting of RB's is
needed. The final figure of RB's represents the number of
necessary rollback steps for each process.
An example is shown in Fig. 4, where Fig. 4(a) describes
memory references, Fig. 4(b) is the current contents of
KC and KP matrices, and Fig. 4(c) is the result of rollback
propagation.
one

-

-

C. The Evaluation ofMultistep Rollback
If module i fails at time tf during the kth state-save interval
Ti(k), then we consider a single step rollback of module i to
see if it is sufficient to recover from the failure. The result
may lead to rollback propagations, and thus to multistep
rollbacks as previously discussed. Since the number of statesave units associated with each module is finite, the whole
task may have to restart when all the states recorded in SSU's
are exhausted. In this section a probability model is derived
to evaluate the coverage of the multistep rollback recovery
which indicates the effectiveness of the present fault-tolerant
mechanism. Recall that a module has (N + 1) SSU's and the
task is allocated to M modules including PM's and CM's. To
derive the coverage, the following assumptions are made and
notations used:
A: The access matrix whose element aij represents the
probability of making a reference from module i to module j.
For a memory module i, aij = 0, for all j. The sum of all
elements in one row must be equal to 1 for a processor
module i, i.e., 1!=Mj aij = 1.
biJn: The probability that KPn(i,j) = 0, which means no
interaction occurs during the disparity between module i's
and module j's (k - n + 1)th state saving instants. For simplicity bijn is assumed to be a constant for all n, i.e.,
bij, = bij2 = * = biN = bu. The exact value of bij is difficult to obtain. Since the state-saving invocations are synchronized, there is at most one instruction occurred during
this disparity. An approximate representation is used, i.e.,
bij = Prob[(Bif n Bjj) U (Bii n Bji)], where Bij is the event
that a memory reference from module i to module j occurs at
any arbitrary moment.
fijn: The average probability of having direct rollback
propagation from module i to module j due to an n-step
rollback of module i. We also assumefijn to be a constant, fj,
for all n.
rij: The probability that module j has to roll back because
of the direct or indirect propagations if module i fails and
then rolls back. Note rig= 1 for all i.
,M, in which element
E: The matrix [eij], i, j = 1, 2,
eij is the average execution time for memory references issued
from module i to module j.
Tef: The total execution time of a given task under an error
free condition and without the time overhead for generating
recovery blocks.
Ti(k): The duration of the kth state-save interval of module
i. Because of the asynchrony between state-save invocation
and actual state saving, Ti(k) is a random variable. If Ts. is
long enough such that there is always a state saving following
every state-save invocation, the mean of Ti(k) is equal to T.
To make the analysis simple, this duration is assumed to be
constant and equal to the duration of state-save invocation

interval, T,s
T,: The time overhead for generating a recovery block.
N,: The total number of state savings before task completion in error-free condition. N, = LTe/(Ts -Ts,)J.
Ujk: The average memory reference rate from module i to
module j during the kth state-save interval of module i.
Occurrence of these memory references is assumed to be a

1119

LEE AND SHIN: FAULT-TOLERANT MULTIPROCESSOR DESIGN AND EVALUATION

Poisson process with a time-varying parameter during the
progress of task execution. In general, the memory references by processes can be divided into different phases each
of which has a constant reference rate [17], [18]. Thus, if N,
is moderately large, uijk could be assumed to be a constant
during the kth state-save interval.
To derive the coverage of a multistep rollback, the probability of direct rollback propagation, i.e., fij, should be obtained first. From the above definitions and assumptions,
fj is the average probability that there exists at least one
memory reference between module i and module j during one
state-save interval. It can be expressed as follows:

0

(1)
gi + gi-gi gi
where gij = (1/N,) k (1 - e -uiJkTs) represents the average
probability of having an interaction froni module i to module
j during a single state-save interval. Since the total number of
memory references between module i and module j is equal
fij

to

=

fyi

aij[Tef/(m=jlaimeim)]

following relationship:

=

and

IN, Uqik(Tss

-

Tv),

we

E uijk

=

(Tef aij)/ (Tss

-

T)

aimeim]

(2)

m=l

Also, the maximum value of memory reference rate Uijk
must be less than or equal to the reciprocal of eij, that is,
e ij

(Uik)max

Uijk>

0

Fig. 6. The rollback propagation network.

have the

Nt_M
k=1

: Module i

(3)

and the accumulated coverage frop a single step rollback to
an h-step rollback can be derived by the following recursive
equation:

C(h)

=

C(l)[I

-

C(h

-

1)] + C(h

-

1).

(6)
calcu-

The coverage of the multistep rollback recovery is
lated for an example with the following access matrix:
0.9
0.1
0.03
0.

It is easy to observe that fij is a monotonically increasing
function of gij and gij is a bounded concave function of Uijk.
With the above two constraints we can get the extrema of fij
as follows:
1) The maximum value of fij, denoted as f,, occurs when

0.08
0.85
0.03
0.02

0.02 0.,
0.03 0.02
0.9 0.04
0.0o 0.9

J

This example has the access localities 0.85 and 0.9 for
which correspond tp the experimental results obtained from Cm* [20]. The numnbrital results are presented in
Table I and are also plotted in Fig. 7. These results include
three cases: the best coverage computed fromfiJ for different
values of N1, and the worst coverage computed fromf,. These
[(TS - Tsv) mM=I aimeim]} - h/ei.
To solve for rij fromfij, a fully connected network is drawn results show that only a small number of SSU's is enough to
as Fig. 6 in which every node represents a module, and the achieve a satisfactory cov,erage of rollback recovery. It
link (i,j) connecting node i and node j denotes the re- should be particularly noted that the requirement of a smail
lationship for direct rollback propagation between module i number of SSU's is mandatory for actual implementation. On
and module j. Thenfij can be considered as the probability of the other hand, this conclusion mdst be interpreted in the
having a directly connected link between node i and nodej. context of access localities; the nutnber of SSU's required
The theory of network reliability [19] can be used to solve for a given coverage tbnds to increase with the decrease in
access localities (i.e., when there are heavy interactions).
for r0j
This
tendency, however, should be understood as an inher(4)
rij= U (Dij,q)
ent problem associated! with. multiprocessors rather than
where Dij,q is the probability that the qth path from node i to with the present fault-tolerant mechanism (see [21] for
nodej is connected and U is the probability union operation. the dependence of multiprocessor performance on access
With an additional assumption that the occurrence of failure localities).
is equally distributed over the entire modules in a statistical
sense, the coverage of a single step rollback, denoted by
IV. THE PERFORMANCE OF ROLLBACK RECOVERY MECHANISM
C(1), becomes
Several methods for analyzing the rollback recovery system
have been proposed [22]-[27]. They in general deal with
(5)
C(1) = (1/M) E [l 1 - rij(I .b)
a transaction-oriented database system and compute the

I = Ui, 2
Uij,Nt.
2) The minimum value of fij, denoted as ft' occurs when
there are i) h intervals {h = eli7Iefaij/[(TS. - Ta,) Em-l a. eimI}
in which ui0, = lleij, ii) (N, - h - 1) intervals in which
Uijk = 0, and iii) one interval in which Uijk = {Tefaij/

Uij

q

processes

120

IEEE TRANSACTIONS ON COMPUTERS, VOL. C-33, NO. 2, FEBRUARY 1984
TABLE I
A NUMERICAL EXAMPLE FOR THE COVERAGE OF MULTISTEP ROLLBACKS

XC(i)
case la

case 2b

case 3c

1

0.75067

0.68610

0.44713

2

0.93783

0.90147

0.69433

3

0.98449

0.96907

0.83100

4

0.99612

0.99029

0.90656

5

0.99902

0.99695

0.94834

fij and Nt

=

100

bcase 2: with minimum fij and Nt

=

10

acase 1: with minimum

ccase 3: with maximum fij

Trea,: The total execution time to complete the task when all
failures are recovered by rollbacks instead of restarts.
TJot, m: The time lost due to the jth rollback in module m
which consists of the setup time for resumption, tsb, and the
computation undone by rollback.
Trsv The time lost due to the ith restart which includes the
setup time for restart, ts, and the time between the previous
start and the moment at which error is detected.
TEk: The accumulated effective computation before the
kth rollback when the task can be completed without restart.
XJ(XD): The duration between two consecutive rollbacks
(restarts).
C(i): The accumulated coverage of rollback recovery from
a single step to i steps. This value is calculated by (5) and (6)
presented in the previous section.
Pb(Ps): The probability of rollback (restart) when a failure
occurs.

cases

case

J
(9

=10

a:

LL)
0
>)

-.0

--0-

2.0

4.0

6.0

8.0

NO. OF STEPS
Fig. 7. Rollback coverage versus number of rollback steps.

optimum value of the intercheckpoint interval. Castillo
and Siewiorek studied the expected execution time which
is required to complete a task with the restart recovery
method [28]. All of these approaches either assume the state
restoration is obtainable by a single checkpoint or do not
include the rollback propagation at all. In this section, we
explicitly take into account the problem of multistep rollback
and the risk of restart for the rollback recovery mechanism.
A. Notations and Assumptions
The following notations will be used in the sequel:
T1: The total execution time to complete the given task
with occurrence of errors. It includes the required execution
time under error-free condition, the time loss due to rollbacks and restarts, and the time overhead for generating
recovery blocks.

Pjt(h): The probability of having an h-step rollback given
that the failure is recovered by the rollback.
Pr(m): The probability of having m rollbacks during the
time interval, Treat.
Zr(z), Zst(z): The probability generating functions of
P,(m), Pst(h), respectively.
(tP(S), FDreal(S): The characteristic functions of Tt, Treal,
respectively.
The goal of our analysis is to calculate the mean and variance of the total execution time of a given task, Tt. Recall that
the task is decomposed and then allocated to M modules.
During the normal operation, the small overhead is required
to generate consecutive recovery blocks in each module.
When the jth error occurs, module m spends T{o i,m to recover
from this error-if the error is recoverable by a rollback. Otherwise, the whole task has to restart. T{oiim consists of the setup time which is composed of the decision delay required for
examining rollback propagation, the reconfiguration time,
and the time used to make up for the computation undone by
the rollback. We assume that the task completion be delayed
by max{Troll, m where m = 1, 2, M for the rollback recovery of the jth error. The resultant completion time will be the
upper bound because of the following reasons. First, T{'o,m
can be interpreted as the time lost due to the rollback in
module m. So, the total time lost in all the concerned modules
is 2M=1 T{o t,m. Since the completion of a task is regarded as
the completions of all its processes, the time lost from the
task's point of view could be max{T{i, m} but not larger than
this maximal value. Secondly, the true delay effect on the
completion of task by a rollback will be shortened because of
the possible reduction in the waiting time of process synchronization. To facilitate system reconfiguration, we also
assume the multiprocessor has a sufficient number of standby
modules so that the task may be executed continuously from
start to end without waiting for the availability of modules.
The time needed for error-free execution is regarded as constant and is independent of reconfiguration.
In general, the occurrence of error can be modeled as a
Poisson process with parameter A(t) which equals the reciprocal of mean time between failures [29]. Since A(t) is slowly
time-varying (for example with a period of one day), it is

121

LEE AND SHIN: FAULT-TOLERANT MULTIPROCESSOR DESIGN AND EVALUATION

rollback

&(

task begins

]

X

X

K

2

Tst

Tr
rst

1

Troll

Treal

rst

rollback

t~O

Xt

rollback

1K

-

rollback

O
X-k

K

completion

2

TrolI

T roll

restart

X

T roll

restart
Fig. 8. Task execution phases.

assumed to be constant over the duration of one task execution, i.e., A(t) = A. For simplicity an error is assumed to
be detected immediately as it occurs (see Section III for a
brief description on relaxing this assumption). From the definitions of P,, Pb, and Pj1(h), we have P, = 1 - C(N) when
each module has (N + 1) SSU's. Therefore, the probability of rollback, Pb, becomes C(N). Pjt(h) is equal to
(1/Pb) [C(h) - C(h - 1)] for h = 2, * , N, and
Pst(l) = C(l)/Pb. After the detection of error, the occurrence
of rollback and restart can be regarded as a Bernoulli process, with probability Pb and P, respectively, and independent of the error generation process. Thus they can be
modelled as Poisson processes with parameters Ab = APb and
As = APs, respectively.
B. The Performance Model
The total task execution time, Tt, can be divided into several phases as shown in Fig. 8. The last phase is always ended
with the completion of task. Other phases are followed by a
restart. This implies that the amount of effective computation
at the beginning of each phase is zero. During each phase, the
effective computation between rollbacks is accumulated
toward the task completion. To derive the distribution of Tt,
we should determine the distribution of the duration of the
last phase (which is defined as Treai), the probability of having
R restarts prior to the last phase, and the distribution of the
durations of other phases which are defined as Ts,1 for
R.
i= 1,2,
In the last phase, the task will be executed from the beginning to the completion without any restart. It is assumed that
Tef is much larger than Ts, (Tef >> Ts,) so that the rollback
distance of an h-step rollback can be approximated by hTss.
The effective computation between two consecutive rollbacks becomes (Xr - hTss)+ when a module rolls back h steps
where (X)+ = max{0, X} is a positive rectification function.
With the probability of an h-step rollback, P,,(h), two functions are introduced
N

Z = E ebAbTssPst(h)
h=1

H(t, k)

=

kk

(1
E()
I

i=0

-

Z)i(Z)k-iGk-i(t)

(7)

(8)

where Gki(t) is the (k - i)th order gamma distribution
function with parameter Ab for (k - i) > 0, and Go = 1. In
Appendix A, we show that the distribution function of the
accumulated effective computation after k rollbacks is
Prob(TEk ' t) = H(t, k). Therefore, the probability of k

rollbacks during the time interval T,ral, Pr(k), is given by
Pr(k) = P(TEk+I > Tef) - P(TEk> Tef)
= H(Tef, k) -H(Tef, k + 1) .
*(9)

Trea, is composed of Tef and the time lost due to rollbacks which

is a sum of identically distributed random variables, Tirol, m,
for j = 1, 2,
k. Substituting the probability mass functions of P,(k) and Pjt(h), we get the characteristic function of
Trea, which is given below:

(Freal(S)

=

esTefZr[estsbZst(e-sTSs)]

(10)

From Fig. 8, the total time T, can be represented as the sum
of Treal and the random sum of T',. The characteristic function
of T, derived in Appendix B is given in the following:
n
so
-nstsu(A
¢,D (s) Ie
n=O
As + s
=

{E

(3) (-1 ) iIreal[(i + 1 ) (As + s) I

(11)

This equation presents a general expression of the total execution time. For the system without the rollback recovery
mechanism, we can use PS = 1, Pb = 0, and then IDreal(s) becomes esTef. The result obtained from the above equation is
the same as that in [28]. The mean and variance of the total
execution time can be obtained from - &P1(s)/&sjI=o and

akIt(s)/as2I-0. In Fig. 9, the mean execution time for the

example in Section III is plotted. It is obvious that the overhead of generating recovery blocks has an important effect on
the rollback recovery method. Since the state savings are
performed in parallel with the normal process execution, the
overhead contains only the time required for the validation
test. When the embedded checking circuits are not very much
cost-effective and complex [30], the overhead of generating
recovery blocks can be reduced with a completely selfchecking mechanism. Fig. 10 expresses the variance of exeq
cution time for the previous example. It suggests that the
prediction of the total execution time becomes more accurate
when the rollback recovery mechanism is used. This result is
expected intuitively since the probability of restart is reduced
considerably. In a system with a higher probability of restart,
the system contains a larger and more uncertain recovery
overhead (i.e., larger mean and variance).
Another interesting parameter is the duration of state-save
invocation, Ta,. The interval has two mutually conflicting
effects. Fig. 7 indicates that the increasing of Ts. will induce
more rollback propagations and degrade the coverage (a

122

IEEE TRANSACTIONS ON COMPUTERS, VOL.

c-33,

NO.

2,

FEBRUARY

1984

2857lj
ai)

228.57 -

V)

ci
to

-

171433

with f 'ii

HI

114.2S9

47.43

-

57,144

L

r r) r)

o0:0

with

200.0

400.0

600.0

800.0

f/,

without rollback
capability

285

z

40.291

f/I

33.141

I)

0

ci:

20.0

357.
U)

M
I
LAJ
LLU

(000.0

TIME-FAILURE FREE (sec.)
Fig. 9. Mean of time-overhead versus error-free execution time.

a)

T-

30.0

40.0

50.0

60.0

NO. OF RECOVERY BLOCK
Fig. 11. Mean time-overhead versus total number
of recovery blocks for a given task.

automatically and regularly. We also derived analytically the
of multistep rollback, the coverage of rollback
probability
LLI
and
recovery,
the risk of restart which are usually ignored in
142.
most existing analyses. The results in this work indicate that
the performance of the rollback recovery mechanism is significantly dependent upon the risk of restart which can be
minimized by a higher local hit ratio. So, the improvements
ij
are related to the partitioning, cooperation, and allocation of
200.0
0.0
400.0
600.0
800.0
1000.0
processes. This is a common, inherent issue in the design of
TIME- FAILURE FREE (sec.)
multiprocessors rather than in that of the present faulttolerant system [211.
Fig. 10. Variance of time-overhead versus error-free execution time.
Since the rollback mechanism used here only provides a
recovery
capability to tolerate the hardware faults in prolarger value of N, means a shorter state-save interval). Since
cessor
modules
and common memory modules, further imthe occurrence of error is distributed throughout the stateprovements
are
conceivable to achieve the overall system
save interval, the average computation loss due to rollbacks
In
addition
to memory assignments many program
reliability.
is proportional to the state-save duration. Therefore the ininvolve
file access and input-output interoperations
may
crease of T,S which invokes longer state-save intervals, will
faces
which
also
affect
the
system behavior. These operations
introduce more computation loss and higher probability of
can
not
be
recovered
simply
by a standard rollback procerestart. On the other hand, the percentage of the total time
dure.
other
Thus,
special
recovery
actions, such as execution
overhead for generating recovery blocks is reduced by the
of
recoverable
should
be included (e.g.,
procedures
[
10],
increase of TSS. The optimum value which minimizes the
for
exception
handling
That is,
input-output
operations).
expected execution time can be found in Fig. 11. Fig. 11
additional
recoverable
the
procedures
provided
by
program
shows that there exists a linear relationship between T, and Ts,
when N4 is larger (i.e., T0, gets smaller)' whete the overhead designer are needed to take special related recovery actions.
of generating recovery blocks dominates the 'final result. With the same concept our hardware recovery scheme can be
When TS, is greater than the optimum value, the loss due to extended to provide such special recovery actions by, for
example-, associating separate save units and/or procedures
recovery increases considerably because of the larger time
with each of I/O interfaces, file accesses, etc. In addition, the
loss in each rollback.
reliability of the interconnection network can be obtained by
using redundant hardware to form additional paths (e.g.,
V. CONCLUSION
additiofial stages ih generalized cube network [31]) or by
We considered the design of a hardware recovery mecha- using reliable switches (e.g., 2 x 2 fault-tolerant switching
nism for a fault-tolerant multiprocessor with emphasis placed element proposed in [32]). However, the faults which ocon, a fast state-save operation which tegftires little time curred in the supplementary resources, like SSU's and
overhead. To permit processes to be general and to ensure monitor- switches, do not cause damages to the computation
programmer transparency, recovery poi4ts are established itself but will change the recovery capability. Althoughlthe
H-

214.

123

LEE AND SHIN: FAULT-TOLERANT MULTIPROCESSOR DESIGN AND EVALUATION

performability [33] of the system at a single state is not
affected by SSU's, etc., the overall lifetime performability is
changed because of the degradation of recovery capability. A
higher recovery capability can be gained by using hardware
redundancy. For instance, an additional standby monitor
switch can either test the active monitor switch or replace the
active one whenever it malfunctions.
To deal with the performance of a fault recoverable and
reconfigurable multiprocessor, the delay in the task completion time due to the errors is an important parameter. In
such a system one or more faults which cause the errors in the
computation and the loss of a portion of function capability
may have no serious consequence to the completion of a
given task, but the quality of the recovery procedure largely
determines the distribution of the task completion time.
Thus, the overhead required to treat the contamination of
error, and the effect on the task execution time, should be
included to represent the effectiveness of fault-tolerance. In
addition, for most real-time applications, such as aircraft or
industrial control, etc., one major concern is whether the
required task can be completed prior to a given deadline or
not. The rollback mechanism proposed in this paper not only
offers system modularity and simplicity, but also provides
fast recovery and accurate prediction of the task completion
time. Hence, the present fault-tolerant multiprocessor has a
high potential use for critical real-time applications.
APPENDIX A
CALCULATION OF THE PROBABILITY OF HAVING k
ROLLBACKS DURING THE DURATION TREAL
From the definition of Psjh), the task will roll back h steps
with probability P,,(h) following a failure detection within the
last phase of duration Treal. Let the rollback distance for thejth
rollback recovery be T4311 which is approximately equal to hTs,
with probability P51(h). Thus the accumulated effective computation time before the kth rollback TEk, is given by
k

TEk

=

(Xi - Troll).

E

j=l

(A.1)

Since the occurrence of rollback is a Poisson process with
parameter Ab, the density function of X{ is Aa&Abt. The probability of having (Xi - TB011) = 0 is 2h'1 P,,(h) (1 - e kbhTs)
The density function of (XI - TB11) becomes
N

N

foy(t) = Z PP(h) (1 - e AbhTss) 8(t) + e Abt
h=1

E pst(h)e-AbhTss

h=l

(A. 2)

where 8(t) is an impulse function. Let Z= h=1 Pst(h)eAbhfs.
Then f% is simplified by

f,(t)

(1

Z) 8(t) + e-AbtZ

(A.3)
The characteristic function of TE,k, which is equal to [cP0(s)]k
where tF0(s) is the characteristic function of (Xi - T' 11),
=

-

becomes

'Fte, k(s)

=

A

k

y)

kS kk-jI
(I Z)i(ZWk
-

AA,k )

+

k- i

(A.4)

Taking the inverse Laplace transform, the density function
of TEk [denoted asfte,k(t)] is obtained. Then the distribution
function of TEk becomes

P(TEk

.

t)

Ei=O1

k=

fte, k(T) dT

=

k (
I

Z)i(Z)k-'Gk-i(t) + ( I Z)'

i

(A.5)

where Gk4i(t) is the (k - i)th order gamma distribution
function.
APPENDiX B
CALCULATION OF THE CHARACTERISTIC FUNCTION
OF TOTAL EXECUTION TIME 'D(S)
From Fig. 8, the total execution time T, is the sum of
1 T s, when there are n restarts.
Given the conditional probability of Tt, we can write the

Treal and Trs,, where T =
following equation:

E(Tt Treal)

(B. 1)
E(Trst Treal) It is assumed that the time interval between the (i - 1)th and
the ith restarts, X', is exponentially distributed with mean
1/As. Thus, for a given T,,al, the time lost due to the ith restart,
T'r1, is randomly distributed between tsu to Treal + tsu with
density function, fXsdT, ai(t) given by
Afe-Ast
for 0 S t ' Tr,,.
(B.2)
f rst,Treat(t + tsu) = 1
e-k
-

=

Treal

+

ea

The probability of having n restarts for a given Tral is

(B.3)
PrSIT -l(n) = (e6A3Treal) (1 - e-AslTran.
Since T, = Tr,al + X7n=1 TX1, if there are n restarts before the
task completion, the characteristic function of T, for a given
Treal becomes
00

dtlTrea(5)

-e

E PrsIT,,e(l) [ (rstITi(S)]n
n=O

(B.4)

where (DFr,11IT0(s) is the characteristic function of the time loss
due to a restart for a given T,,al, i.e., the Laplace transformation of f'l T.a(t). By substituting Pr,T,,l) and (Prsdheai(S)
into (B.4) and integrating with the density function of T,a1,
the characteristic function of T, is obtained as (11) in
Section IV.
REFERENCES
[1] J. A. Rohr, "Starex self-repair routines: Software recovery in the JPLSTAR computer," in Proc. 3rd Int. Symp. Fault-Tolerant Comput., 1973,

pp. 11-16.
[2] F. E. Heart, S. M. Ornstein, W. R. Crowther, and W. B. Barker, "A new
minicomputer/multiprocessor for the ARPA network," in Proc. 1973
AFIPS Nat. Comput. Conf., 1973, vol. 42, pp. 529-537.
[3] B. Randell, P. A. Lee, and P. C. Treleaven, "Reliability issues in computing system design," Comput. Surveys, pp. 123-165, June 1978.
[4] B. Randell, "System structure for software fault tolerance," IEEE Trans.
Software Eng., vol. SE-1, pp. 220-232, June 1975.
[5] D. L. Russell, "Process backup in producer-consumer systems," in Proc.
6th ACM Symp. Operating Syst. Principles, pp. 151-157, Nov. 1977.

IEBE TRANSACrIONS ON COMPUIERS, VOL. C-33, NO. 2, FEBRUARY 1984

124

[6] K. H. Kim, "An approach to programmer-transparent coordination of
recovering parallel processes and its efficient implementation rules," in
Proc. 1978 Int. Conf. Parallel Processing, Aug. 1978, pp. 58-68.
[7]
, "An implementation of a programmer-transparent scheme for
coordinating concurrent processes in recovery," in Proc. COMPSAC 80,
pp. 615-621, Oct. 1980.
[8] K. Kant and A. Silberschatz, "Error recovery in concurrent processes,"
in Proc. COMPSAC 80, pp. 608-614, Oct. 1980.
[9] W. G. Wood, "A decentralized recovery control protocol," in Proc. II th
Int. Symp. Fault-Tolerant Comput., 1981, pp. 159-164.
[10] J. J. Homing, H. C. Lauer, P. M. Melliar-Smith, and B. Randell, "A
program structure for error detection and recovery," in Lecture Notes in
Computer Science: Operating Systems. New York: Springer-Verlag,
1974, pp. 171-187.
[11] A. M. Feridun and K. G. Shin, "A fault-tolerant multiprocessor system
with rollback recovery capabilities," in Proc. 2nd Int. Conf. Distributed
Comput. Syst., pp. 283-298, Apr. 1981.
[12] Y. H. Lee, and K. G. Shin, "Rollback propagation detection and performance evaluation of FTMR2M- A Fault-Tolerant Multiprocessor," in
9th Annu. Symp. Comput. Arch., pp. 171-180, Apr. 1982.
[13] R. J. Swan, S. H. Fuller, and D. P. Siewiorek, "Cm*: a Modular Multimicroprocessor," in Proc. 1977 AFIPS Nat. Comput. Conf., vol. 46,
1977, pp. 637-644.
[14] P. H. Enslow, "Multiprocessor organization-A survey," Comput.
Surveys, vol. 9, pp. 101-129, Mar. 1977.
[15] K. H. Kim, "Error detection, reconfiguration and recovery in distributed
processing systems," in Proc. lst Int. Conf. Distribut. Comput. Syst.,
pp. 284-295, Oct. 1979.
[16] J. J. Shedletsky, "A rollback interval for networks with an imperfect
self-checking property," IEEE Trans. Comput., vol. C-27, pp. 500-508,
June 1978.
[17] A. W. Madison and A. P. Batson, "Characteristics of Program Localities," Commun. Ass. Comput. Mach., vol. 19, pp. 285-294, May 1976.
[18] A. P. Batson, "Program behavior at the symbolic level," Computer,
pp. 21-26, Nov. 1976.
[19] S. Rai and K. K. Aggarwal, "An efficient method for reliability evaluation of a general network," IEEE Trans. Reliability, vol. R-27,
Aug. 1978.
[201 S. H. Fuller et al., "Multimicroprocessors: An overview and working
example," Proc. IEEE, vol. 66, pp. 216-228, Feb. 1978.
[21] K. G. Shin, Y. -H. Lee, and J. Sasidhar, "Design of HM2p-A hierarchical multimicroprocessor for general-purpose applications," IEEE
Trans. Comput., vol. C-31, pp. 1045-1053, Nov. 1982.
[22] K. M. Chandy, J. C. Browne, C. W. Dissly, and W. R. Uhrig, "Analytic
models for rollback and recovery strategies in data base systems," IEEE
Trans. Software Eng., vol. SE-I, pp. 100-110, Mar. 1975.
[23] K. M. Chandy and C. V. Ramamoorthy, "Rollback and recovery strategies for computer programs," IEEE Trans. Comput., vol. C-21,
pp. 546-556, June 1972.
[24] E. Gelenbe and D. Derochette, "Performance of rollback recovery systems under intermittent failures," Commun. Ass. Comput. Mach.,
vol. 21, pp. 493-499, June 1978.
[25] J. W. Young, "A first order approximation to the optimum checkpoint
interval," Commun. Ass. Comput. Mach., vol. 17, pp. 530-531,
Sept. 1974.
[26] G. L. Brodetskiy, "Periodic dumping of intermediate results in systems
with storage-destructive failures," Cybernetics, vol. 15, pp. 685-689,
Sept.-Oct. 1979.
[27]
, "Effectiveness of storage of intermediate results in systems with
failures that destroy information," Eng. Cybernetic, vol. 16, pp. 75-81,
Nov.-Dec. 1978.

[28] X. Castillo and D. P. Siewiorek, "A performance-reliability model for
computing systems," in Proc. 10th Int. Symp. Fault-Tolerant Comput.,
1980, pp. 187-192.
[29]
, "Workload, performance, and reliability of digital computing
systems," in Proc. Ilth Int. Symp. Fault-Tolerant Comput., 1981,
pp. 84-89.
[30] W. C. Carter, et al., "Cost effectiveness of a self checking computer
design," in Proc. 7th Int. Symp. Fault.Tolerant Comput., 1977,
pp. 117-123.
[31] G. B. Adams and H. J. Siegel, "A fault-tolerant interconnection network
for supersystems," IEEE Trans. Comput., vol. C-31, pp. 443-454,
May 1982.
[32] W. Lin and C. L. Wu, "Design of a 2 x 2 fault-tolerant switching
element," in Proc. 9th Annu. Symp. Comput. Arch., Apr. 1982,
pp. 181-189.
[331 J. F. Meyer, "On evaluating the performability of degradable computing
systems," IEEE Trans. Comput., vol. C-29, pp. 720-731, Aug. 1980.

..

r

Yann-Hang Lee (S'81) received the B.S. degree in
engineering science and the M.S. degree in electrical
engineering from National Cheng Kung University,
Taiwan, China, in 1973 and 1978, respectively.
Currently, he is working towards the Ph.D. degree
in computer, information, and control engineering
at the University of Michigan, Ann Arbor, MI.
His research interests include multiprocessor and
distributed systems, performance evaluation, and
fault-tolerant computing.

Kang G. Shin (S'75-M'78-SM'83) was born in
Choongbuk Province, Korea, on October 20, 1946.
He received the B. S. degree in electronics engineering from Seoul National University, Seoul, Korea in
1970, and both the M.S. and Ph.D. degrees in
electrical engineering from Cornell University,
Ithaca, NY in 1976 and 1978, respectively.
From 1970 to 1972 he served in the Korean Army
as an ROTC officer and from 1972 to 1974 he was
on the research staff of the Korea Institute of Science
and Technology, Seoul, Korea, working on the design of VHF/UHF communication systems. From 1974 to 1978 he was a
Teaching/Research Assistant and then an Instructor in the School of Electrical
Engineering, Cornell University. From 1978 to 1982 he was an Assistant
Professor at Rensselaer Polytechnic Institute, Troy, NY. He was also a Visiting
Scientist at the U.S. Airforce Flight Dynamics Laboratory in Summer 1979,
and at Bell Laboratories, Holmdel, NJ in Summer 1980 where his work was
concerned with distributed airborne computing and cache memory architecture,
respectively. He also taught short courses for the IBM Computer Science Series
in the area of computer architecture. Since September 1982, he has been with
the Department of Electrical and Computer Engineering at The University of
Michigan, Ann Arbor, MI, where he is currently an Assistant Professor. His
current teaching and research interests are in the areas of distributed and
fault-tolerant computing, computer architecture, and robotics and automation.
Dr. Shin is a member of the Association for Computing Machinery, Sigma
Xi, and Phi Kappa Phi.

A Real-Time Garbage Collector for Embedded Applications in CLI
Okehee Goh and Yann-Hang Lee
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85287
Abstract  We are working on scheduling of garbage
collector as a concurrent thread for time-constrained
applications in Common Language Infrastructure (CLI).
We have implemented an incremental garbage collector
with fine-grained write barrier in MONO, an opensource implementation of CLI. Our collector is based on
existing conservative garbage collector of Boehm et al.
By conducting benchmarking experiment, we will derive
parameters to predict the behavior and overhead of
garbage collection and apply real-time scheduling
algorithms that guarantee the timeliness of applications
without memory starvation.

Ziad Kaakani and Elliott Rachlin
Honeywell International Inc.
Phoenix, AZ
collection, overhead due to incremental GC, amount of
reclaimed memory etc. are a basis of scheduling of GC.
We have implemented an incremental garbage
collector in CLI, running concurrently to be applied for
time-based or work-based scheduling. This WIP report
covers the design of the incremental garbage collector.
Currently, we are gathering experiment data and
developing scheduling algorithms of garbage collection
that guarantee time and space bound.

COMMON
(CLI)

LANGUAGE

INFRASTRUCTURE

CLI is aimed to make it easy to write components and
Index Terms  Common language infrastructure (CLI), applications with multiple languages and for multiple
Incremental garbage collection, Real-time scheduling.
platforms[1]. It is enabled by defining a rich set of types
to allow CLI-aware languages to interoperate, having
INTRODUCTION
each component carry self-describing information,
Common Language Infrastructure (CLI), introduced as translating applications into intermediate language
a core technology of Microsoft .NET and standardized codes, and providing virtual execution system (VES)
by the international standardization organization ECMA executing intermediate language codes (CIL).
in 2002, provides a virtual execution system (VES) that
VES, similar to JVM, is an abstract stack-based
supports multiple-languages as well as multiple machine featuring loader, verifier, JIT compiler,
platforms. The machine-independent intermediate code garbage collector, security system, multiple threads,
that "write once, and run everywhere," contributes to exception handling mechanism etc. The ECMA
expedite the evolution of this VM technology by standard for CLI does not confine a specific garbage
minimizing cost and time-to-market of application collection mechanism on VES so that the
development. However, real-time embedded systems implementation of a garbage collector does not have any
that require timeliness response do not get benefits limitation except awareness of “pinned” type signature.
using CLI as available implementations of CLI are not CLI defines unmanaged pointer type that is not traced
designed for timeliness response.
by a garbage collector. A memory object referenced by
A real-time embedded CLI environment can unmanaged pointer must be “pinned” to prevent a
enormously expand the applicability of CLI, not only garbage collector from moving the object. Unmanaged
for consumer electronics devices but also the embedded pointer referring to managed heap object can happen
systems for home appliances, telecommunication, and when a managed object is passed to managed code that
industry automation. Among VES’s features such as operates with unmanaged code. However supporting
thread scheduling, exception handling etc., we focus on this feature does not affect the design of a deterministic
a garbage collector to make it deterministic. Our garbage collector.
ultimate goal is to schedule a garbage collector to
As well as commercial products Microsoft .NET
ensure applications to meet their deadline and satisfy and WinCE .NET, SSCLI by Microsoft, MONO by
memory requests in CLI. Scheduling a garbage collector Ximian/Novell, and DotGNU Portable .NET are
primarily requires two conditions. Firstly, the activity of available open-source implementations of CLI. C#,
garbage collection (GC) must be controllable as a C++, VB, JavaScript, and Java are available as CLIschedulable unit and should not result in a long pause compatible languages. So far, there is no
time. Secondly, the parameters to predict the behavior implementation aiming to support time-constrained
of a garbage collector must be derived. These embedded applications. Considering the benefits of
parameters such as the execution time of garbage using CLI in embedded applications, we believe that
this need will grow soon.

GARBAGE COLLECTOR IN MONO
We have worked on the garbage collector by Boehm et
al. (BDW)[2] that is associated in MONO[3]. BDW,
designed to support languages such as C, C++ etc., is a
conservative mark-sweep GC without cooperation of a
compiler or a runtime system. In order to reduce a pause
time due to GC, BDW supports a mostly parallel GC
(partly incremental), and a parallel but not incremental
collector for multiprocessor systems.
In mostly parallel GC, GC is triggered per
allocation basis and write barrier tracing mutators’ heap
pointer updates is performed by using dirty pages
through virtual memory protection mechanism of
operating system. The write barrier algorithm is that all
heap pages are protected as read-only at an initial step
of mark phase when incremental garbage collection
starts and the pages flagged as dirty due to mutators’
write operations are re-scanned by a garbage collector
in a termination step of mark phase. The advantages
using virtual memory protection is that firstly, it does
not require a compiler to emit write barrier and
secondly, there is no overhead placed on mutators due
to write barrier.
However, this design imposes limitation on
schedulable GC. Firstly, triggering garbage collection
per allocation makes collection work dependent on
allocation patterns of applications. Secondly, using
virtual memory protection for write barrier is a system
dependent feature that limits portability of CLI. Finally,
in the mark phase, an initial step to protect all heap
pages and a termination step to rescan both dirty page
set and root-set may lead to long pause time.

platform requirements.
Incremental Garbage Collector Using FineGrained Write Barrier
We extended BDW GC to perform root-set scan, mark
and sweep incrementally. The write barrier used to trap
mutators’ heap pointer updates is Yuasa’s snapshot-atthe-beginning algorithm [5].
• Write Barrier: MONO includes JIT compiler that
translates intermediate language codes (CIL) into native
codes. Among about 220 CIL instructions, CIL
instructions that need write barrier due to their store
operation are in Table 1. JIT was modified to generate
the native codes for the instructions that include an
internal call performing write barrier in MONO runtime
system. Applying an indirect function pointer for the
internal calls helps avoid comparison of GC phase each
time. Write barrier in a root-set scan phase is extended
from snapshot-at-the-beginning algorithm.
• New objects allocated in GC cycle: All objects
allocated during GC cycle are marked live to ease a
termination condition and help avoiding a long-pause in
a termination step of mark phase.
CIL
Stind.ref
Stfld
Stsfld
Stelem.ref

Table 1 CIL codes that require write barrier in GC

STATUS, FUTURE WORK AND
CONCLUSION

DESIGN OF INCREMENTAL GC
We extended BDW and JIT compiler in MONO to
address the problems described above. Firstly, the
collector is concurrent for either time-based or workbased scheduling. Secondly, an incremental GC is
implemented by having JIT emit write barrier.
Concurrent Garbage Collector
A traditional incremental GC that performs collection
work at each allocation does not guarantee consistent
collection due to bursty allocation which is general
characteristic of applications [4]. A concurrent garbage
collector, invoked either at the fixed time or at the
threshold of free memory, allows us to apply real-time
scheduling algorithms. The GC invocation interval and
the pause time in each collection cycle, as shown in
Figure 1, is controllable based on applications and
GC invocation interval

collection cycle

Figure 1. GC invocation and scheduling

Descriptions
Store an object reference into the memor
Store a value into a field of an object
Store a value into a static field of class
Store a value into a vector element

Currently, we have finished the implementation of the
incremental garbage collector and are gathering
experiment data. Based on the experiment, we will
devise efficient scheduling algorithms which guarantee
time and space bounds. At this level, the conservatism
of locating heap pointers and deciding liveness of
pointers in a root-set, and memory fragmentation are not
addressed. However, our incremental garbage collector
can establish an experiment environment to schedule
real-time applications in CLI.

REFERENCES
[1]

Common Language Infrastructure, ECMA TC39/TG3, Oct, 2002

[2]

Hans-Juergen Boehm et al. Mostly parallel garbage collection.
ACM SIGPLAN Notices, 26(6):157-164, 1991.

[3]

Ximian/Novell, MONO, http://www.go-mono.com

[4]

David F. Bacon, Perry Cheng, V. T. Rajan: A real-time garbage
collector with low overhead and consistent utilization. POPL
2003

[5]

Richard Jones, Garbage Collection: algorithms for automatic
dynamic memory management, John Wiley & Sons, Ltd, 1999.

Automatic Parallelization of Multirate Block Diagrams of Control
Systems on Multicore Platforms
CUMHUR ERKAN TUNCALI, GEORGIOS FAINEKOS, and YANN-HANG LEE,
Arizona State University

This article addresses the problem of parallelizing model block diagrams for real-time embedded applications
on multicore architectures. We describe a Mixed Integer Linear Programming formulation for finding a
feasible mapping of the blocks to different CPU cores. For single-rate models, we use an objective function
that minimizes the overall worst-case execution time. We introduce a set of heuristics to solve the problem
for large models in a reasonable time. For multirate models, we solve the feasibility problem for finding a
valid mapping. We study the scalability and efficiency of our approach with synthetic benchmarks and an
engine controller from Toyota.
CCS Concepts:

r

Computer systems organization → Embedded software;

Additional Key Words and Phrases: Multicore platforms, embedded control systems, scheduling, optimization, model-based development, Simulink, multirate, task allocation
ACM Reference Format:
Cumhur Erkan Tuncali, Georgios Fainekos, and Yann-Hang Lee. 2016. Automatic parallelization of multirate
block diagrams of control systems on multicore platforms. ACM Trans. Embed. Comput. Syst. 16, 1, Article 15
(October 2016), 26 pages.
DOI: http://dx.doi.org/10.1145/2950055

1. INTRODUCTION

Model-Based Design (MBD) has gained a lot of traction in the industries that develop
safety-critical systems. This is particularly true for industries that develop CyberPhysical Systems (CPSs), for which the software implements control algorithms for
the physical system. Using MBD, system developers and control engineers can design
control algorithms on high-fidelity models. Most important, they can test and verify
the system properties before having a prototype of the system. The (certified) autocodegeneration facility of MBD tools provides an additional concrete benefit that is helping
to eliminate programming errors. Another advantage of MBD is the ability to generate
platform-specific code from a platform-independent model [Kim et al. 2013].
However, currently, the autocode-generation processes of commercial tools focus on
single-core systems. Namely, at the model level, there is no automatic support for
producing code that runs on a multicore system. This is problematic since advanced
control algorithms, for example, Model Predictive Control algorithms [Huang et al.
2013], are computationally demanding and may not be executed within the limited
computation budget of a single-core embedded system. In this article, we address
This research was partly funded by the NSF awards CNS-1446730 and IIP-1361926, and the NSF I/UCRC
Center for Embedded Systems.
Authors’ addresses: Arizona State University, Centerpoint Bldg. STE 203, 660 S. Mill Ave. Tempe, AZ 85281;
emails: {etuncali, fainekos, yhlee}@asu.edu.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2016 ACM 1539-9087/2016/10-ART15 $15.00

DOI: http://dx.doi.org/10.1145/2950055

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15

15:2

C. E. Tuncali et al.

this problem at the model level. Given a dataflow diagram of an embedded control
algorithm, the worst-case execution times (WCETs) of the blocks, and a computation
budget (deadline or sampling period), can we automatically map the blocks of a model
onto multiple submodels that are executed on different cores and satisfy real-time
constraints?
Depending on system requirements, the controller model can have single or multiple sampling rates. For instance, communicating with different hardware systems
may require different subsystems to operate with different sampling periods/rates. In
addition, for satisfying the hardware interface and performance requirements or for
maintaining stability of the system, the system designer has to determine the optimal
sampling frequencies for the subsystems [Åström and Wittenmark 1997]. Most MBD
R
R
tools, for example, Simulink
from MathWorks
, support multirate design. For multirate designs, the problem of mapping the blocks onto the cores requires an analysis
for the interaction between the blocks with different sample rates and a consideration
of the task scheduling algorithm on the target platform.
We focus on control models built in Simulink for demonstration of our approach. Our
goal is to produce a framework that determines the mapping of each block onto a CPU
core and an execution order of the blocks inside the tasks. We aim at creating a single
task for each sample rate on a CPU core, that is, a single task on each core for single-rate
models and possibly multiple tasks on each core for multirate models. A task contains
all blocks with the same sample rate that are mapped on the same core. Our consideration for scheduling the tasks on the target platform is based on the rate-monotonic
scheduling algorithm, which is a fixed-priority scheduling mechanism in which tasks
are statically assigned to CPU cores. Especially, in safety-critical systems, scheduling
in a predictable and deterministic manner is highly important for verification and for
satisfying the certification requirements that are mandated by regulatory authorities.
Multicore architectures are classified as highly complex in the 2011/6 final report of
the European Aviation Safety Agency [EASA 2012] and in the Certification Authorities Software Team (CAST) position paper CAST-32 Multi-core processors [CAST 2014].
These classifications highlight the difficulty of certifying safety-critical systems that
are based on multicore architectures. To our knowledge, there is no certified tool that
can automatically parallelize the generated code to multiple cores. However, certified
R
code generators for single-core platforms are available, for example, Embedded Coder
R
R
from MathWorks
, Esterel KCG
, and so on. Parallelizing the system at the model
level will result in separate models for each CPU core. Then, certified code generators
can be used to generate code from the resulting models. This will help the designer in
better understanding which part of the software runs in which core. The remaining
step toward certification is for guaranteeing the correct partitioning of the model. This
is expected to be a smaller effort compared to doing the parallelization at a lower level,
for example, at the compiler level.
In our previous work [Tuncali et al. 2015], our approach for single-rate models is
explained. Our approach is based on having separate executables for each core while
Simulink blocks are allocated in each core and executed in the execution order that
we compute. In other words, we determine the execution order of the blocks inside
each core while respecting the data dependencies between them. In this article, we are
extending our approach for single-rate models to multirate models. Our approach for
the multirate models is based on seeking a mapping of the blocks onto different tasks
on the CPU cores and an execution order of the blocks within the tasks in order to
satisfy the deadline requirements of all the tasks. After code generation, we have a
separate task for each sampling period in a model. The scheduling of the tasks should
be taken into account for parallelizing multirate models. It must be guaranteed that
the preemptions of the low-priority tasks by the high-priority tasks does not cause
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:3

deadline misses. Our Mixed-Integer Linear Program (MILP) formulation for multirate
models incorporates scheduling-related constraints between the tasks with different
rates.
The contributions of this article are as follows:
1. Providing a practical, automated solution to the block diagram parallelization problem for multicore architectures while considering the deadline requirements and
scheduling of the parallelized application on the target platform, and
2. Extending available MILP formulations for parallelizing control models to allow
multirate execution.
2. RELATED WORK

There is a large amount of research being done on the optimization of scheduling
multiple tasks on multicore processors or multiple processors in the literature.
An exhaustive survey on real-time scheduling techniques for homogeneous multiprocessor architectures is provided by Davis and Burns [2011]. That survey
evaluates different techniques by discussing their advantages and disadvantages.
The authors state the main practical advantage of statically assigning the tasks onto
the processors as the ability to apply available uniprocessor scheduling techniques and
analyses on each processor in the system.
There are multiple studies on task parallelization. For optimal mapping of tasks to
CPU cores, Yi et al. [2009], Bender [1996], and Ostler and Chatha [2007] discuss Integer
Linear Programming (ILP) techniques that constitute the foundation for our optimization formulation. These approaches can be applied to single-rate Simulink models by
substituting the tasks in the formulation with Simulink blocks, that is, considering the
blocks as tasks with dependencies. On the other hand, for multirate models, a set of
blocks with the same rate should be considered as a single task in those approaches,
but this eliminates the parallelization opportunities inside a task. For more realistic
models that consist of a significant number of blocks, ILP-based approaches require
introduction of heuristics to find an optimal or suboptimal solution in a reasonable
amount of time. Yi et al. [2009] make use of available loop-level parallelism or functional pipelining in the system. An efficient constraint programming approach to the
task allocation problem is described by Hladik et al. [2008], who introduce a constraint
programming approach to solve the static task allocation to distributed processors
problem. Their algorithm can also prove nonexistence of a solution when it cannot
find one. Another interesting feature of their algorithm is that it separates the allocation problem from the scheduling problem. Their algorithm incorporates a method
for learning from the schedulability analysis to remodel the allocation problem and
to improve performance. However, the only experiment results given in that work is
with 40 tasks, and the scalability of that approach is not studied. Another constraint
programming–based approach for parallel execution of safety-critical applications is
studied by Puffitsch et al. [2015]. The objective of that work is executing the tasks
of Prelude applications on multi- and manycore architectures in which the tasks are
scheduled by nonpreemptive offline scheduling. Our work differs from that work in the
sense that we are doing the multicore mapping of the blocks in a model that results in
splitting a task into subtasks that run on different cores, in which the tasks in a core
are scheduled by a preemptive rate-monotonic scheduler. Cotton et al. [2011] discuss
the use of Satisfiability Modulo Theories (SMT) solvers and multi-criteria optimization
for mapping tasks to multiprocessors. Application of SMT solvers in manycore scheduling for data-parallel applications is discussed by Tendulkar et al. [2014]. Feljan and
Carlson [2014] propose a heuristic that utilizes information on how the tasks delay
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:4

C. E. Tuncali et al.

each other, for finding a good solution for task allocation problems in a short solver
execution time.
There are also several studies focusing on the parallelization of Simulink models.
Kumura et al. [2012] propose methods to flatten Simulink models for parallelization
without giving a detailed description of the optimization formulation. In that work,
Simulink blocks are considered as tasks. Canedo et al. [2010] introduce the concepts
of strands for breaking the data dependencies in the model to achieve thread-level
parallelism on multicore. The authors define a strand as a chain of blocks that are
driven by Mealy blocks. The proposed method searches for available strand split points
in Simulink models and relies heavily on strand characteristics in target models. Cha
et al. [2011] have focused on automating code generation for multicore systems in which
the parallel blocks are grouped by user-defined parallelization start and end S-functions
into the model. An approach for WCET analysis of Simulink models is described by
Kirner et al. [2000]. Although WCET analysis is crucial for going from Simulink models
to executables on a multicore system, we do not focus on WCET analysis in our work, but
we assume that the WCET of the blocks are readily available as an input. A compilerlevel parallelization of the code generated by Simulink is studied by Umeda et al. [2015],
who propose an automatic parallelization approach using the OSCAR compiler. While
Umeda et al. [2015] propose a compiler-level parallelization approach, we approach
the problem at the model level. We believe that model-level parallelization provides
a better architectural picture for control engineers since they can see the functional
partitioning in the model. This helps the engineers to understand the parallel execution
of their software better and to debug easier.
Deng et al. [2015] study model-based synthesis flow from Simulink models to
AUTOSAR [2015] runnables and runnables to tasks on multicore architectures. The
authors extend the Firing Time Automaton (FTA) [Lublinerman and Tripakis 2008]
model to specify activations and requested execution time at activation points. They
define modularity as a measure of number of generated runnables and reusability as
a measure of false dependencies introduced by runnable generation. The authors use
modularity, reusability, and schedulability metrics for evaluation of runnable generations. They also propose different heuristics and compare their results with the results
obtained by utilizing a simulated annealing algorithm. Although that work is targeting
a similar problem to our target problem for single-rate models, they are providing experiment results for systems with less than 50 blocks, and are not considering intercore
communication and memory overhead.
A study on multi/manycore execution of multirate Simulink models is done by Pagetti
et al. [2014], who describe an approach to execute multirate Simulink models on
multi/manycore architectures. However, for this purpose, the authors propose doing
a translation from Simulink models to Prelude [Forget et al. 2010]. Our work differs
from that since we focus on finding a mapping of blocks on the available CPU cores
for meeting deadline and shared memory–related constraints. We do code generation
directly for execution on the target architecture, while Pagetti et al. [2014] focus on
translation to Prelude without seeking a feasible mapping of the input model/blocks to
the target cores.
A linear programming approach for the partition-scheduling problem for strictly
periodic tasks on multiprocessor Integrated Modular Avionics (IMA) architectures is
studied by Al Sheikh et al. [2012], who incorporate available resource constraints
such as memory limitations along with IMA-related constraints into their linear programming formulation. They are also proposing a game theory–based, best-response
algorithm as a heuristic that is proven to converge. Our work differs from that work
by the heuristics that we introduce and our model-level approach, which creates a
parallelization before the code that forms the tasks is generated.
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:5

A scheduling methodology for multicore systems is described by Elhossini et al.
[2010], in which directed acyclic graphs for the tasks are used for task partitioning. The
approach in that work groups the tasks in a system as ordinary tasks and multirate
tasks. The ordinary tasks are defined as the tasks that must be executed at every
cycle of the system. The multirate tasks are defined as the tasks that do not need to
be executed at every cycle but can be scheduled during idle times of the schedule. The
problem targeted in that work differs from ours since, in our problem, tasks cannot
be grouped as ordinary and multirate tasks as is done in that work. Instead, we are
dealing with a set of tasks consisting of blocks of a model design in which tasks have
distinct rates and must complete within their periods under an available scheduling
algorithm.
The scalability of the constraint programming–based approaches is studied by
Gorcitz et al. [2015], who experimentally showed that the constraint programming–
based approaches can be efficiently used for small- to medium-sized systems, but they
diverge rapidly when the system becomes larger. The authors also state the necessity
of heuristic methods for larger systems, which is along the lines of the experimental
results in our work.
The use of conditional sporadic Directed Acyclic Graph (DAG) tasks by Baruah [2015]
have similarities to our approach for multirate models in which execution order dependencies of the blocks, induced by the block-dependency graphs of different sample rates,
change for different firing times with possible preemptions of the tasks at the firing
times.
Our work mainly differs from the other works in the literature by
1. providing a complete flow for automatically parallelizing single- and multirate block
diagrams,
2. incorporating the communication time cost in the optimization problem both in the
transmitter and the receiver side,
3. having total available shared memory and task priority constraints, and
4. being able to handle large models with more than 100 blocks in a reasonably short
time using the proposed heuristics.
3. PROBLEM DESCRIPTION
3.1. Preliminaries
R
R
Model-based design platforms such as Simulink
from MathWorks
, Esterel SCADE
R

Suite , and Ptolemy [Eker et al. 2003] utilize synchronous block diagrams for describing the model of a system. Figure 1 displays an example of a block diagram from a
simple Simulink model. A precise introduction to synchronous block diagrams is given
by Lublinerman et al. [2009]. Here, we provide a brief summary. A synchronous block
diagram contains blocks (possibly inside other blocks), each with a nonnegative number of input and output ports, and connections between the blocks through their input
and output ports. A block can either be macro or atomic. An atomic block can be defined
as a block that cannot be partitioned into smaller blocks. A macro block encapsulates
a block diagram. A flattening operation on block diagrams removes the hierarchy hidden inside macro blocks by replacing the macro blocks with the block diagrams that
they are encapsulating until no macro blocks are left. Macro blocks correspond to the
virtual subsystems in Simulink. Details on the flattening operation can be found in
Lublinerman et al. [2009].
Every block in a synchronous block diagram has a sampling rate that describes the
rate at which the block is executed during the execution of the system being modeled.
If all blocks have the same sampling rate, the model is referred as a single-rate model.
Otherwise, the model is referred as a multirate model.

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:6

C. E. Tuncali et al.

Fig. 1. Simulink block diagram.

Fig. 2. Intermediate block diagram.

Fig. 3. Flattened block diagram.

For describing the problem that we are targeting, it is convenient to represent the
dependencies between the blocks of a synchronous block diagram in a graph structure.
For this, we flatten the given block diagram. The first step in flattening a block diagram
is removing the hierarchy in it. Figure 2 gives an example of this operation, in which the
“Subsystem” block in Figure 1 is replaced with the block diagram that it encapsulates.
MBD tools typically supply some mechanisms to the user for routing signals inside
the diagram in a visually clear way. For instance, a “Goto”–“From” block pair represents
a data connection from the source of “Goto” to the destination of “From” in a Simulink
model. Since these blocks do not represent any operation done by the system that is
being modeled, we call these blocks virtual routing blocks, and we replace these blocks
with lines representing the data connections between the blocks in the flattened block
diagram. Figure 3 gives an example of such a transformation. The “Goto”–“From” pair
in Figure 2 is replaced with a line in Figure 3. We refer to the block diagram obtained by
performing a flattening operation, that is, removing hierarchy and replacing the virtual
routing blocks with lines, as the flattened block diagram. Every block in a flattened
block diagram is atomic.
Definition 1: A block dependency graph (BDG) G = (V, E) is a graph representation
of a flattened block diagram. It is an acyclic directed graph with the vertex set V =
{vi : i ∈ [1, n]}, where each vi corresponds to a block and |V| = n is the total number of
blocks in the block diagram, with a set E of directed edges.
A similar concept, task-data graphs, are discussed by Cotton et al. [2011]. In contrast
to that concept, we use blocks instead of tasks, the WCETs of blocks instead of the
amount of work associated with tasks, and we consider the data communication size
between blocks. Figure 4 illustrates a sample BDG that corresponds to the flattened
block diagram given in Figure 3. There is a one-to-one correspondence between the
vertices in V and the blocks in the flattened block diagram. We use vi in order to refer
to the block that the vertex vi ∈ V corresponds to. A directed edge (vi , v j ) ∈ E is sourced
from the vertex vi and has the vertex v j as its destination. Such an edge represents the
existence of a direct data connection from the output ports of the block vi to the input
ports of the block v j .
In a flattened block diagram, there can be data connections representing data transfers from previous iterations of their source blocks. We call the blocks with such incoming data connections delay introducing blocks. The edge set E of a BDG excludes such
data connections. Every edge (vi , v j ) ∈ E has an associated positive weight ci, j , which
represents the amount of data transferred from the block vi to the block v j . When the
blocks vi and v j are executed on different CPU cores, there will be a communication
cost in terms of time for transferring ci, j amount of data between the CPU cores. The
communication cost for such a connection is divided into transmission and reception
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:7

Fig. 4. A block dependency graph.

parts, where txi, j denotes the time required for the transmission part of the communication and rxi, j denotes the time required for the reception part of the communication.
For each block vi ∈ V, its WCET is denoted by wi and its sampling period is denoted
by πi .
3.2. Problem Definition

We are addressing the problem of automatically parallelizing synchronous block diagrams on to multicore architectures to satisfy WCET constraints on the target platform.
We have different problem definitions for single- and multirate models.
Assumptions 1 (for single- and multirate models):
(a) All of the CPU cores on the target platform are identical.
(b) The time cost for data communication between a pair of blocks on different CPU
cores is identical for any pair of CPU cores.
(c) The time cost for transferring some data between a pair of blocks on the same CPU
core is zero.
(d) There are no cycles in the given BDG.
(e) The execution order dependencies of the blocks with the same period are only
defined by the data dependencies between them.
(f) All blocks with a period π become available to execute at every π amount of time,
and they all must complete their execution in π amount of time after they become
available.
Assumptions 2 (for multirate models):
(a) Distinct sampling periods in the model are harmonic.
(b) On the target platform, for each CPU core, there will be a separate task for each
distinct period for the blocks with that period and mapped on that CPU core.
(c) On the target platform, tasks within a CPU core will be scheduled by a ratemonotonic scheduling algorithm.
(d) No protected resource is shared between the blocks with different sampling periods.
Any resource sharing between the same-rate blocks is known in advance through
the model structure.
Problem 1 – Single-rate models: Given the number m of available CPU cores on
the target platform and a BDG G = (V, E) such that π1 = · · · = πn = π , where n = |V|,
compute an optimal mapping of the blocks to the target CPU cores and an execution
ordering of these blocks with the objective of minimizing the makespan of all the blocks
within their period on the target platform. Report if no feasible solution can be found
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:8

C. E. Tuncali et al.

Fig. 5. An overview of our approach.

that allows a makespan, that is, the overall completion time of execution of all blocks,
shorter than the period π . We impose Assumptions 1 on this problem.
In Tuncali et al. [2015], we focused on Problem 1 for single-rate embedded control
applications that are modeled in Simulink. In this article, we extend the problem to
the multirate models, and propose a solution for the extended problem.
Problem 2 – Multirate models: Given the number m of available CPU cores on the
target platform and a BDG G = (V, E) such that πi = π j for some vi , v j ∈ V, compute
a mapping of the blocks to the target CPU cores and an execution ordering of these
blocks so that execution of every block can be started and finished within its period on
the target platform. We impose Assumptions 1 and 2 on this problem.
3.3. Solution Overview

In this article, we present a unified solution for Problems 1 and 2. For demonstration
purposes, our target platform is a Qorivva MPC5675K-based evaluation board from
Freescale Semiconductor Inc. The processor is a dual-core 32b MCU that is targetR
ing automotive applications. Micrium μC/OS-II
is ported to our target platform, and
a library to support Simulink code generation is devised for the platform by Bulusu
R
[2014]. Embedded Coder
is used for code generation from the models. In multitasking
mode, which is the case for multirate models, Embedded Coder combines the blocks of
the same rate into a task that is scheduled by a rate-monotonic scheduler on a single
core. For this reason, our approach is based on combining blocks of the same rate in
a single task when they are mapped on the same core. The priorities of these tasks
increase as their sampling periods decrease. Each core on our target platform has a
separate copy of μC/OS-II kernel. The multicore operation is supported by utilizing intercore synchronization and communication protocols through a shared memory space,
as described by Bulusu [2014]. The operating system kernel on each core uses a ratemonotonic scheduling algorithm for scheduling the tasks within the core. Preemption
of the tasks is allowed.
An overview of our approach to Problems 1 and 2 is illustrated in Figure 5. Although
our approach for these two problems has some differences that are explained later
in this section, the illustration given in Figure 5 is applicable to both problems. In
this section, we first explain our approach for single-rate models; then, we extend the
discussion to multirate models.
We approach Problem 1, which is for single-rate models, in five steps, as also
described in our previous paper [Tuncali et al. 2015]: (1) Creating a BDG from the
given block diagram. (2) Finding an optimal or near-optimal mapping of blocks to
different CPU cores by formulating an MILP and solving the resulting optimization
problem with off-the-shelf MILP solvers. Details of our MILP formulation are given in
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:9

Section 4. (3) Automatically updating the original Simulink model by adding intercore
communication blocks where necessary in accordance with the most optimal solution.
We handle intercore data communications by utilizing available shared memory and
intercore semaphores that are used for synchronization between tasks across cores
and protecting global critical sections as described in Bulusu [2014]. For the purpose
of utilizing this approach in Simulink, we model the transmission and reception of
data with two separate S-function blocks that implement intercore transmission and
reception using intercore semaphores and shared memory. We refer to these S-function
blocks as inter-core communication blocks. (4) Generating separate code for each target
core. We first partition the model into separate models for each core. For this, we create
a copy of the model for each core and automatically comment out the blocks that are
not mapped to the corresponding core. Then, we do Simulink code generation from each
of these model copies. (5) Compiling the generated code and deploying it on the target
platform. Since we do separate code generation for each core, we compile the generated
code for each core and deploy each of the executables on its corresponding core.
Our approach for Problem 2, which is for multirate models, follows similar steps as
our approach for Problem 1, with modifications in some of these steps. First, in addition
to creating a BDG for the model as a whole, we create a separate BDG for each distinct
sampling period in the model. In the second step, instead of optimizing with an objective
function, we are utilizing MILP solvers for seeking a feasible solution to satisfy the
WCET limits imposed by the periods of the blocks. For this purpose, we first calculate a
hyperperiod from the distinct periods in which a block may become available to execute
more than once. In our MILP formulation, we are targeting to find a solution for one
hyperperiod since the execution of the system on the target platform repeats itself at
every hyperperiod. Details of our MILP formulation are described in Section 4. For the
multirate models, since the transmitter and the receiver block of some communicated
data can have different periods, communication between such blocks require ratetransition blocks. As a communication mechanism between different rate blocks on
different cores, we utilize the asynchronous three-slot mechanism described by Chen
and Burns [1997]. Implementation details of this mechanism, which we refer to as
intercore rate-transition blocks on our target platform, are explained by Bulusu [2014].
For the blocks with the same rate that are executed on different cores, we use the same
intercore communication mechanism that we use in the third step of our approach to
Problem 1. Although there is no difference in the final two steps of our approach from
the approach to Problem 1, the generated code from a multirate Simulink model has a
separate function for each distinct sample time. We place each of these functions in a
separate task in the executables for the target CPU cores.
4. MILP FORMULATION

In this section, we present our MILP formulation for the parallelization problem of synchronous block diagrams. An MILP formulation for single-rate models was described in
our previous work [Tuncali et al. 2015]. In this article, we are extending this formulation to multirate models. Our MILP formulation is based on the formulations proposed
by Yi et al. [2009], Bender [1996], and Ostler and Chatha [2007]. We introduce an
extension to these formulations by dividing the cost of communication to the transmission and reception parts, by adding a constraint on the usage of the available shared
memory for intercore communications, and extending the formulation to address multirate models. Our MILP formulation for multirate models seeks a feasible solution
that satisfies the constraints induced by the desired block periods and available shared
memory on the target platform without having any objective function. In Section 4.5,
we describe our heuristics to reduce the number of constraints for allowing the MILP
solvers to find better solutions within a feasible time.
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:10

C. E. Tuncali et al.

The blocks mapped on the same core with the same period will be executed in the
same task. Thus, there will be a separate task for each distinct sampling period of the
blocks that are mapped to the same core.
In the rate-monotonic scheduling algorithm, a task that contains blocks with smaller
periods will have higher priority than the tasks that contain blocks with larger periods.
Thus, when low- and high-priority tasks are available at the same time, the tasks with
lower priorities will wait for higher-priority tasks to either complete or to get blocked.
According to our MILP formulation, when a high-priority task waits for data from
another core, no lower-priority task is executed during the data wait time. However,
in an actual execution, a ready, lower-priority task may be executed during this time
when the higher-priority task gets blocked. On the other hand, because our target
platform allows preemption of task executions, an execution of a lower-priority task will
be preempted by a higher-priority task when the higher-priority task becomes ready.
Thus, assuming that the WCETs and the worst-case communication times are available
for the MILP formulation, in an actual execution, a high-priority task will never be
completed later than the time computed by the MILP solver. On the other hand, a lowerpriority task may start to execute before the time that was found by the MILP solver. In
such a case, by applying the interchange argument from Section 12.3 of Lee and Seshia
[2015], we can swap the time reserved by the MILP solver for the execution of the
lower-priority task with any data-waiting time of the higher-priority task. This can be
iteratively applied for any pair of tasks, and it can be shown that, even if a lower-priority
task starts execution while a higher-priority task is waiting for data, the execution
completion time for any task can never be later than its completion time computed
by the MILP solver. Recall that here our assumption is that no tasks with different
periods share a protected resource (Assumptions 2.d). This assumption must hold to
avoid experiencing any anomaly in the scheduling of the tasks as described by Graham
[1969] and Thiele and Kumar [2015]. However, the change in execution ordering of
lower- and higher-period blocks of a Simulink model as described earlier may result
in different execution semantics in the target platform. Thus, the system designer
must use appropriate mechanisms for maintaining the execution semantics by use
of appropriate lock mechanisms, data-transfer mechanisms that ensure determinism,
or ensuring the execution ordering by the task scheduler. Mechanisms for preserving
execution semantics for multirate models are discussed by Caspi et al. [2008]. Details
of such mechanisms are implementation specific, and not discussed in this article.
4.1. Notation and Constants

The notation used for the MILP formulation is given in Table I. The notation given for
the problem description in Section 3 is also utilized in the MILP formulation and, for
convenience, we repeat it in Table I.
The BDG G must be acyclic as it is defined in Section 3. Since algebraic loops are not
allowed in Simulink, the flattened block diagram of a Simulink model cannot have cycles, due to algebraic loops. However, the flattened block diagram of a Simulink model
can have cycles that contain at least one (delay-introducing) block, which is introducing data dependencies to previous iterations of the model execution, for example, Unit
Delay, Memory, Integrator, and so on. When creating a BDG, we discard the incoming
connections to the delay introducing blocks from their predecessor blocks. Since these
discarded connections represent data dependencies to an earlier iteration of the execution, discarding them does not affect the dependency relations that we are formulating.
However, since these deleted connections will not go into our formulation, if no precaution is taken, the formulation will not consider the intercore communication between
a delay-introducing block and its predecessor block when they are mapped to different
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:11

Table I. Notation Used in MILP Formulation
Notation
G = (V, E)
n
V = {vi : i ∈ [1, n]}
wi
πi
E

ci, j
txi, j / rxi, j
Z ⊆ V2 − E
N
m
P = {Pi : i ∈ [1, m]}
r
R = {Ri : i ∈ [1, r]}
rj
j
Vj = {vi : i ∈ [1, r j ]}
Gj = (Vj , Ej )
H
firing time
F = {cγ : cγ < H,
γ ∈ R, c ∈ N}
f = |F|
vi,c
M = {vi,c : vi ∈ V,
c ∈ N, cπi ∈ F}
sSize
cCount
totMem
aSize
MAX

Description
A graph representation of the flattened block diagram, i.e., a BDG, which
is a DAG
The number of blocks in the flattened block diagram
The vertex set, in which each vertex corresponds to a block in the flattened
block diagram
The worst-case execution time of the block vi
The sampling period of the block vi
The directed edge set, where (vi , v j ) ∈ E corresponds to the data connection
from block vi to block v j , excluding the connections to the delay-introducing
blocks
The amount of data transferred from block vi to block v j
The time required for transmission/reception part of the communication
from vi to v j when the blocks are executed on different cores
Set of the connections to delay introducing blocks that are not included
in E
The set of natural numbers
The number of available CPU cores
The set of available CPU cores
The number of distinct periods
The set of distinct sampling periods (“sample time” in Simulink)
The number of blocks with period R j
The set of blocks that have a sampling period of R j , where Vj ⊆ V and
∪rj=1 Vj = V
The induced subgraph of G on the vertex set Vj
The hyperperiod, which is the least common multiple of all distinct periods
in R
Start of each period and its repetitions in the time interval [0, H)
The set of distinct firing times
The number of distinct firing times
cth repetition of a block vi ∈ V in the hyperperiod, where c ∈ N and cπi < H
The set of repetitions of the blocks
The size of a global semaphore structure in bytes
The number of copies of the data communicated in an intercore ratetransition structure
Size of the total available shared memory in bytes
Data alignment size in bytes (word size)
A very large constant that is used in the program formulation to dominate
other terms, allowing constraints to be ignored under certain conditions
(big-M method)

cores. To avoid this issue, we force any delay-introducing block and its predecessor to
be mapped on the same CPU core by introducing a constraint.
Recall that we have assumed that different rate blocks do not share protected resources (Assumption 2.d) because such a case can create scheduling anomalies. Samerate blocks accessing shared resources are assumed to be known in advance. We add
such blocks into the set Z as if there were a connection between these blocks. This
forces such blocks to be on the same core and consequently in the same task due to the
constraints in the formulation.
For a single-rate model, the problem simplifies as follows: for all i, πi = π , r = 1,
R = {R1 = π }, V1 = V, E1 = E, G1 = G, f = 1, H = π , F = {0}, and M = {vi,0 : vi ∈ V}.
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:12

C. E. Tuncali et al.

4.2. Variables

The variables used in the MILP formulation are:
bi, p: A Boolean variable indicating whether the block vi is mapped to the core Pp or
not. It is defined for all vi ∈ V and for all Pp ∈ P. If vi is mapped to core Pp, then bi, p
takes value 1. If vi is mapped to another core, then bi, p takes value 0.
di, j : A Boolean variable indicating whether the block vi executes before or after the
block v j when both blocks are mapped to the same core. It is defined for all pairs of
same-period blocks that do not have any data dependency to each other. When vi and
v j are mapped to the same core, if vi executes before v j , then di, j takes value 1, and if
vi executes after v j , then di, j takes value 0. The variable di, j does not have a meaning
when the blocks vi and v j are mapped to different cores.
dic , j : A Boolean variable indicating whether a block finishes its execution before it
can be preempted by executions of blocks with a smaller period in an upcoming firing
time. It is defined for all blocks vi,c ∈ M and F j ∈ F such that c πi < F j < (c + 1) πi .
The variable dic , j takes value 1 if vi,c finishes its execution before the firing time F j . It
takes value 0 if vi,c starts its execution after all of the blocks in Mk,z that are mapped
to same core with vi finish their execution where k, z satisfy Rk < πi , (z Rk) = F j .
si,c : The start time for the execution of the cth repetition of the block vi , that is, vi,c . It
is defined for all vi,c ∈ M.
4.3. Constraints

—A block shall be assigned to a single core:
m

bi, p = 1
∀vi ∈ V,

(1)

p=1

—Delay-introducing blocks and their predecessor blocks (or protected resource-sharing
blocks) shall be assigned to the same core:
∀zi, j ∈ Z ∧ ∀Pp ∈ P, bi, p − b j, p = 0
(2)
—Start time of every repetition of a block shall be greater than or equal to its firing
time:
(3)
∀vi,c ∈ M, si,c ≥ c πi
—Execution of every repetition of a block shall be completed within the block’s period:
∀vi,c ∈ M, si,c + wi ≤ (c + 1) πi

(4)

—If there is a data connection from a block vi to a block v j in which both blocks have the
same period, then block v j shall not start execution until (i) block vi finishes execution
and transmission of its output data to its successor blocks that are mapped on other
cores and (ii) block v j finishes receiving all of its input data that are sent by the
blocks on other cores:
Considering that block vi is mapped to the core Pp and v j is mapped to the core Pq ,
∀Pp, Pq ∈ P, ∀vi,c , v j,c ∈ M s.t. (vi , v j ) ∈ E, πi = π j = Rk ∈ R, vi , v j ∈ Vk ,
si,c + wi +


vx

∈Vk

[txi,x (1 − bx, p)] ≤ s j,c −


vy

[rxy, j (1 − by,q )] + (2 − bi, p − b j,q ) MAX

(5)

∈Vk

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:13

—Execution of independent blocks with the same period that are mapped to the same
core shall not overlap:
Considering an independent pair of blocks vi and v j , which is mapped to the same
core Pp, we have two different constraints for this requirement:
∀Pp ∈ P, ∀vi,c , v j,c ∈ M s.t. (vi G v j ) ∧ (v j G vi ), πi = π j = Rk ∈ R, vi , v j ∈ Vk ,


si,c + wi +

vx ∈Vk

s j,c + w j +



[txi,x (1 − bx, p)] ≤ s j,c −

[rxy, j (1 − by, p)] + (3 − bi, p − b j, p − di, j ) MAX (6)

v y ∈Vk



[tx j,y (1 − by, p)] ≤ si,c −

v y ∈Vk



[rxx,i (1 − bx, p)] + (2 − bi, p − b j, p + di, j ) MAX (7)

vx ∈Vk

Here, we use the notation (vi G v j ) to represent that no path from vi to v j exists
in G. Since MAX is a very large constant, Equation (6) will be valid when block vi,c
executes before v j,c , that is, when di, j = 1, and Equation (7) will be valid when block
vi,c executes after v j,c , that is, when di, j = 0.
—A block shall (i) either finish execution and output transmission before an upcoming
firing time where smaller period block executions are fired (ii) or start execution after
all the blocks with a smaller period in an upcoming firing time are finished:
∀Pp ∈ P, ∀vi,c , v j,c ∈ M, ∀Fw ∈ F s.t. cπi < Fw < (c + 1)πi
where πi = Rk ∈ R, i.e., vi ∈ Vk , πi > π j = Rl ∈ R, v j ∈ Vl , Fw = c π j , c ∈ N,
si,c + wi +


vx

s j,c + w j +


v y ∈Vl

[txi,x (1 − bx, p)] ≤ Fw + (2 − bi, p − dic ,w ) MAX

(8)

∈Vk

[tx j,y (1 − by, p)] ≤ si,c −



[rxx,i (1 − bx, p)] + (2 − bi, p − b j, p + dic ,w ) MAX (9)

vx ∈Vk

Since MAX is a very large constant, Equation (8) will be valid when block vi,c executes
before Fw , that is, when dic ,w = 1 and Equation (9) will be valid when block vi,c
executes after low period blocks fired at Fw , that is, when dic ,w = 0.
Considering the execution order between a block and an upcoming firing time, the
following constraint must also be added to make sure that if the parameter dic ,w is 0
(block vi,c executes after Fw ), then dic ,w̄ is also 0 for all Fw̄ < Fw (block vi,c executes
after Fw̄ ).
∀vi,c ∈ M, ∀Fw , Fw̄ ∈ F s.t. cπi ≤ Fw̄ < Fw < (c + 1)πi , Fw = c π j , Fw̄ = c πk
where πi > π j , πi > πk, πi , π j , πk ∈ R, c , c ∈ N,
dic ,w̄ < dic ,w

(10)

—Blocks shall not start execution until other blocks with smaller periods from same or
previous firing times finish their execution and transmission of their outputs when
these blocks are mapped to the same core:
∀Pp ∈ P, ∀vi,c , v j,c ∈ M, Fw ∈ F s.t. Fw = c π j ≤ cπi < (c + 1)π j
where c, c ∈ N, πi = Rk ∈ R, i.e., vi ∈ Vk and πi > π j = Rl ∈ R, i.e.,v j ∈ Vl ,

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:14

C. E. Tuncali et al.

s j,c + w j +



[tx j,x (1 − bx, p)] ≤ si,c −

vx ∈Vk



[rxy,i (1 − by, p)] + (2 − bi, p − b j, p) MAX (11)

v y ∈Vl

—Since the intercore communications are done over the available limited shared memory space, the total memory needed for semaphores and communication buffers shall
be less than or equal to the total amount of available shared memory:
⎡



	


m


ci, j
⎣
sSize +
aSize |bi, p − b j, p|
aSize
p=1 (vi ,v j )∈E,πi =π j
(12)
⎤



	



ck,l
sSize + cCount
aSize |bk, p − bl, p| ⎦ < totMem
+
aSize
(k,l)∈E,πk =πl

4.4. Objective Function

For single-rate models, where πi = π for all vi ∈ V, we focus on Problem 1. In this case,
the goal is to minimize the makespan for one iteration of the model execution, and the
objective function for the MILP problem is to minimize π as described in our previous
paper [Tuncali et al. 2015].
For multirate models, that is, πi = π j for some vi , v j ∈ V, we do not impose any
objective function on the MILP formulation as it already includes the scheduling constraints. Since single-rate models can be considered as a special case of the multirate
models, one can also use the multirate approach for parallelization of single-rate models. With this approach, instead of seeking the minimum makespan, one can seek a
feasible mapping for a predetermined makespan, which can also help in decreasing the
solver execution time for single-rate models.
4.5. Improving Solver Execution Time

In this section, we introduce our heuristic techniques for dealing with large models.
The heuristics described in this section are targeting single-rate models, that is,
Problem 1. These heuristics can be adapted for Problem 2 as well. That is, for
multirate models, they can be applied only to the blocks with the same rate. However,
we do not expect them to be as useful as they are for Problem 1 since they can
eliminate opportunities to fit blocks with lower rates in between the completion time
of the higher rate blocks and the upcoming firing times.
For two same-rate blocks, if there exists a directed path between the corresponding vertices in the BDG, then we say that these blocks are dependent on each other.
Otherwise, we say that these blocks are independent of each other.
The majority of the constraints in the MILP formulation are related to the execution
ordering of the independent blocks, that is, the inequalities Equations (6) and (7). This is
because, for blocks dependent on each other, the execution of each block has constraints
related to the directly connected blocks. These relations also impose lower and upper
bounds for execution time of the blocks and limit the search space for each block’s
execution. Thus, the solver execution time is not dramatically affected by increasing
the number of dependent blocks. However, there are constraints between each pair of
independent blocks. The number of combinations for independence relations between
the blocks quadratically increases as the model size increases for most industrial size
applications. The heuristics that we introduce are targeting to decrease the number
of or completely eliminate these constraints for independent blocks. These heuristics
impose some restrictions on the execution ordering of the blocks. Thus, given infinite
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:15

time, the solver will be seeking a less optimal solution. However, since we limit the
solver execution time, it becomes very hard, if not impossible, to find an optimal or
even a feasible solution without heuristics. However, with heuristics, the solver can
find solutions with larger speedup values in a limited execution time, even though the
solutions may not be the globally optimal solutions for the original problem.
4.5.1. Partial Ordering of Independent Blocks. In order to have more parallelization opportunities in a model, the flattened block diagram must preferably have a large number
of blocks that are independent of each other. Typically, in an industrial-sized model
with a large number of blocks, both the number of blocks that are independent of each
other and the number of blocks that are dependent on each other are large. However,
if the number of blocks that are independent of each other is very large, then when
we consider all possible combinations of execution orders between these independent
blocks, the number of constraints introduced by inequalities Equations (6) and (7) becomes very large. As a consequence, finding an optimal solution within a feasible time
becomes harder.
We address this problem by deciding the execution order between certain pairs of independent blocks—say, vi , v j —in advance. That is, before formulating the optimization
problem, we decide the values of the di, j variables for these block pairs. Since the di, j
variables become constants in this case, the MILP solver does not need to seek their
values. Also, since at least one of the inequalities Equations (6) and (7) becomes invalid
(satisfied always), the number of constraints for the MILP formulation decreases. Note
that our execution order decision is valid only when these blocks are mapped onto the
same core. Thus, this does not prevent these blocks from being mapped on different
cores, thus executed in a different order than what we specify.
Our partial-ordering heuristic is based on comparing the execution start time frames
of independent blocks. The execution start time frame of a block is defined as the time
frame between its best-case and worst-case start-time values. The best-case and worstcase start-time values of a block vi ∈ V are defined as bsi and wsi respectively. The
variable bsi is determined by using the best-case completion time for all of the blocks
corresponding to the vertices from which there exists a path to vi ∈ V in G. In the best
case, all of this workload before the block vi is distributed equally on all of the cores.

−
→
→ wk)/m, where Yi = {vk : vk ∈
The best-case start time of vi is calculated as bsi = ( v ∈−
k Yi
V, πk = πi and there exists a path from vk to vi in G}. The variable wsi is determined by
using the best-case completion time for all of the blocks corresponding to the vertices to
which there is a path from vi ∈ V in G and the WCET of the block
vi itself, subtracted
− wk)/m, where
from the deadline. The WCET of vi is calculated as wsi = πi − (wi + v ∈←
k Yi
←
−
Yi = {vk : vk ∈ V, πk = πi and there exists a path from vi to vk in G}.
For all independent block pairs vi , v j ∈ V, if ((bs(i) ≤ bs( j))∧(ws(i) < ws( j)))∨((bs(i) <
bs( j)) ∧ (ws(i) ≤ ws( j))), then we decide vi to execute before v j by setting di, j to 1.
Otherwise, if ((bs(i) ≥ bs( j)) ∧ (ws(i) > ws( j))) ∨ ((bs(i) > bs( j)) ∧ (ws(i) ≥ ws( j))), then
we decide vi to execute after v j by setting di, j to 0. If we compute di, j as 1, then we
replace the di, j in Equation (6) with 1, and we do not add the constraint given by
Equation (7) into the formulation. Similarly, if we compute di, j as 0, then we replace
the di, j in Equation (7) with 0, and we do not add the constraint given by Equation (6).
4.5.2. Total Ordering of Independent Blocks. Even though ordering independent blocks
using the partial-ordering heuristic improves the performance, this may not be enough
for models with a very large number of blocks. For example, we could not find a feasible
solution to models with more than 100 blocks with this approach. For dealing with
those large models, we propose deciding the execution order of all the independent
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:16

C. E. Tuncali et al.

blocks when they are mapped on the same core. The intuition behind the total-ordering
heuristic is based on comparing the midpoints of the execution start-time frames for
these blocks. For independent blocks vi , v j ∈ V, if (bsi + wsi )/2 < (bs j + ws j )/2, then
we decide vi to be executed before v j , and vice versa if otherwise. Thus, we replace
the di, j values in Equations (6) and (7) to the calculated values, and we do not add the
constraint defined by Equation (6) when di, j is 0. Similarly, we do not add the constraint
defined by Equation (7) when di, j is 1 into our formulation. Here, the decided value for
di, j determines the ordering of the blocks only when they are mapped to the same core
and our discussion on the case when these blocks are mapped to different cores in the
previous section is still valid.
4.5.3. Merging Highly Coupled Blocks. In this heuristic, we merge blocks vi and v j when
block v j is the only block connected to the output port(s) of block vi and block vi is the
only block connected to the input port(s) of block v j . The merging operation copies all
incoming and outgoing edges of v j to vi except the edge (vi , v j ) between these blocks.
Then, it updates wi with wi + w j and, finally, it deletes v j .
4.5.4. Merging Small Blocks with Large Blocks. In this heuristic, we merge blocks vi and
v j based on their ratio of execution times. If block v j is the only block connected to
the output port(s) of block vi and the WCET of block vi is very small when compared
to the WCET of block v j , then block vi is merged into block v j . If block vi is the only
block connected to the input port(s) of block v j and the WCET of block v j is very small
when compared to the WCET of block vi , then block v j is merged into block vi . We
find this technique useful for reducing the number of blocks of concern in a way that
parallelization will be focused on blocks with higher impact on execution time. The ratio
between the WCETs of the blocks for determining a merge operation can be defined
depending on how much reduction is needed in the number of blocks.
These merging heuristics can be used for decreasing the number of nodes in very
large models in which the MILP solver can no longer find a good solution. These
merging techniques are also dependent on the structure of the model. Although, in
general, they assist in finding better solutions, there can be cases in which the number
of nodes cannot be reduced to an acceptable level.
5. IMPLEMENTATION
R
In this section, we describe the details of the implementation of our tool in MATLAB
.
Some of the concepts explained here were described earlier in Section 3.
Our tool accepts as an input a block diagram of a Simulink model that is ready
to compile. The user can also input the desired depth of blocks to be parallelized.
The desired depth sets an upper bound on the hierarchical depth for the flattening
operation done on the model block diagram. Even though the desired depth is set, if
there is a delay introducing a block that has a larger depth, it will still be discovered
and flattening will be performed in order to flatten the hierarchy for such a block.
This is because a macro block containing such a delay-introducing block can cause a
cycle in the block diagram that must be handled. In Simulink, public “Goto”–“From”
block pairs can create data dependencies between the blocks in different hierarchical
levels. Thus, for the subsystems that have a public virtual (“Goto”–“From”) connection
with any block outside the subsystem, the user-defined desired depth is ignored and
flattening is performed. Since determining the WCET is not in scope of this article, we
assume that the WCET for each block is already determined and given as an input to
the tool.
The first step in our approach is to create a BDG from the given model block diagram.
Our tool loads the model block diagram, reads specific block information, for example,
block type, parents, and sample time, and all the relations between blocks along with

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:17

the width and size of the data on the ports. For datatypes that are not built in, the
user input is required to define the data size in bytes. The block diagram is flattened
by taking blocks inside subsystems out of their parent blocks and by discarding the
remaining blocks, such as input and output ports of subsystems and the emptied
subsystem container blocks. Simulink provides virtual routing blocks like “Goto” and
“From.” These virtual routing blocks serve the purpose of virtually adding a connection
between the blocks. Thus, these blocks do not really perform an operation and they are
only virtual blocks that help the designer to create connections between the blocks in
a cosmetically nice way. Our tool discards these virtual routing blocks and considers
them as regular lines connecting the blocks in the model. After these operations, we
end up with having the flattened block diagram of the model as illustrated in Figure 3.
Our tool then creates a directed graph representation of the flattened block diagram. The vertices of this directed graph correspond to the blocks in the flattened
block diagram and the directed edges correspond to the connections from one block
to another. Then, the tool removes the incoming edges to delay-introducing blocks.
The delay-introducing blocks contain states that are initialized to a value, and in an
iteration (including the first iteration), their outputs depend on their internal states.
Thus, removing the incoming edges of such blocks removes any possible cycles without
violating execution order dependencies. An issue arising here is that since we remove
these edges, the data connection from a block to a delay-introducing block is no longer
considered and we can fail to model an intercore communication if a delay-introducing
block and its predecessor block are mapped to different cores. For resolving this issue,
we keep track of such block pairs for forcing them to be mapped on the same core. At
this point, we have a BDG representation of the flattened block diagram, as given in
Figure 4. The merge-based heuristics that are described in Section 4.5 are executed on
the BDG in order to obtain a smaller-sized graph, if the user chooses to apply them.
The BDG and the number of CPU cores on the target architecture are used in
generating the MILP formulation presented in Section 4. Our tool takes the MILP
solver to be used as an input option and executes this solver with an upper bound on
solver execution time, which, again, is taken from the user as an input option. For the
multirate models, the tool generates the BDGs for every distinct sample rate value.
The BDG for a sample rate contains the blocks with the same sample rate and the data
dependencies between these blocks. Since the rate-transition blocks have one sample
rate for their inputs and another sample rate for their outputs, they are added to
both sample rate graphs. The WCET of the rate-transition blocks may be different for
different sample rates. When there is more than one sample rate, the problem must
be solved to satisfy the constraints for all tasks in a hyperperiod. For this purpose,
the hyperperiod is calculated as the least common multiple of the distinct sample
periods. During a hyperperiod H, a task containing the blocks with a period π will be
repeated H/π times. The starting time for each of these repetitions in the hyperperiod
is called the firing time. The tool calculates the firing times and determines the periods
and the blocks corresponding to each firing time. The BDGs for the whole model and
for the distinct period values,—together with the hyperperiod, the firing times, and the
periods/blocks corresponding to the firing times—are used in the MILP formulation
described in Section 4. The MILP solver seeks a feasible mapping of blocks to the
CPU cores satisfying the constraints and returns the mapping found and execution
start times for all the executions of the blocks during the hyperperiod. The MILP
solver returns the solution for mapping the blocks to the available CPU cores and
the execution order between these blocks if a feasible solution is found. If no feasible
solution is found, the MILP solver reports this and our tool as well reports this to the
user and exits.

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:18

C. E. Tuncali et al.

Fig. 6. Intercore communication blocks (the coloring of the blocks represents core mapping).

The solution from the MILP solver is used to add intercore communication blocks
between the blocks with the same period that are mapped on different CPU cores. The
relevant outputs of a block that are sending data to a block on a different core are connected to intercore data transmitting S-function blocks. Similarly, the corresponding
intercore data receiving S-function blocks for each transmitter are connected to the
relevant inputs of the block that is receiving data on a different core. The intercore
communication blocks are added by setting unique IDs that set each pair of transmitting and receiving blocks to use a dedicated intercore semaphore and a dedicated
shared memory location. For the multirate models, the intercore rate-transition blocks
that are described in Section 3 are added between the different rate blocks mapped on
the different cores that have a data communication between each other.
An example of the transformation of a block diagram for intercore communication of
the same rate blocks is given in Figure 6. The output of B1 is connected to the input of B2
in the original model. This connection is then replaced by the intercore communication
blocks. After adding all needed communication blocks, we set the priority attributes
of the Simulink blocks using the execution start-time values obtained from the optimization solution. This is done in order to guarantee that for every iteration in the
sampling process of the Simulink model, the total order of the block execution induced
by Simulink is consistent with the partial order of the BDG that we create that imposes
the MILP formulation constraints. Embedded Coder uses the priority attributes of the
blocks with respect to the other blocks in the same subsystem, as an execution ordering
that is reflected to the generated code as long as the priority settings do not conflict
with the data dependencies between the blocks. Because of this behavior of Embedded
Coder, depending on the selected model depth for the parallelization, our tool may be
implicitly suggesting a different structure for the model, that is, splitting subsystems
into more than one subsystem through the execution ordering of the blocks. While this
can be automated, our tool does not change the structure of the model automatically;
rather, it leaves this to the control engineer.
As the final step, a copy of the model is created for each CPU core in which the
blocks that are mapped on other cores are commented out. Code generated from each
of these models can be compiled to create separate executables for each core. For the
multirate models, Simulink generates separate functions for different sample periods.
These functions are executed in different tasks and these tasks are scheduled by the
rate-monotonic scheduling algorithm on the target platform, as described by Bulusu
[2014].
6. EXPERIMENTS

For studying the scalability and efficiency of our approach for the single-rate models,
we utilize randomly generated DAGs with different numbers of nodes. We present
the results of these experiments in Section 6.1 and the results of our case studies
for single-rate models in Section 6.3. We illustrate our approach for multirate models
with a simple example in Section 6.2. We use SCIP from Achterberg [2009] as the
MILP solver, which is interfaced with MATLAB through the Opti Toolbox by Currie
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:19

R
R
R
and Wilson [2012]. Experiments are run on a Microsoft
Windows
7 PC with Intel
R
Xeon
E5-2670 CPU and 64GB RAM.

6.1. Randomly Generated DAGs

For evaluating performance of our approach for single-rate models, we generate random
DAGs in which the WCET, communication costs, and connections between blocks are
assigned randomly. We use the random DAG generator tool provided by Gwinner [2011].
Then, we solve Problem 1 for a dual-core system with the basic MILP formulation
given in Section 4 and with the partial- and total-ordering heuristics for deciding the
execution order of any independent blocks. We set 5h (18,000s) as an acceptable upper
time limit for the solver runtime, which is a time bound that the MILP solver cannot
return a solution on the larger-sized DAGs without using the proposed heuristics.
We have done 500 experiments with different-sized and completely random DAGs for
increasing the confidence level in the benchmarks. Here, we present a comparison of
the performance of these three approaches in terms of the average speed-up achieved,
the average solver execution time, and the ability to find a solution in the given time
limit. The speedup is computed as the overall single-core WCET of the model divided
by the overall WCET of the parallelized model.
Given infinite solver execution time, the basic MILP formulation is expected to find
more optimal solutions than the other approaches do for the same problem size. However, when the solver execution time is limited (5h in our experiments), it fails to find
satisfactory solutions for large problems. Table II gives a comparison of the performance of the different approaches with the number of tests executed for each problem
size. The average speedup achieved by basic MILP formulation, the partial- and the
total-ordering heuristics (denoted as basic, partial, and total, resp.,) and corresponding
solver runtime values are presented in the table for different problem sizes. We also
present the ratio of the solutions found over all the experiments. For a problem size, the
lines corresponding to the approaches that could not return any solutions are discarded
in the table. As it can be seen from the results presented in Table II, as the number
of blocks in a model increases, any heuristic that (partially) sets the execution order
performs better both in terms of solver runtime and optimality of solutions. According
to our observations, for finding an optimal mapping, the basic MILP formulation performs best when there are less than 30 blocks. The partial-ordering heuristic performs
best when there are 30 to 50 blocks. For more than 50 blocks in the model, the totalordering heuristic outperforms other approaches in terms of the achieved speedup and
the ability to return a solution. The basic MILP formulation fails to return any solution
for models with 70 or more blocks. The partial-ordering heuristic fails to return any
solution for models with more than 110 blocks. Although this detail is not illustrated in
Table II because of averaging, according to our experimental results, the total-ordering
heuristic can occasionally achieve very low speedup values compared to the other approaches when there are less than 20 blocks in the model. However, this issue is not
observed when there is a large number of blocks. This behavior is parallel to our expectations since optimization can significantly reduce the effect of possible nonoptimal
execution-order decisions by trying a large number of different mappings of blocks to
different cores.
In Figure 7, we illustrate the comparison between the two heuristics and the basic
MILP formulation in terms of the achieved speedup over the number of nodes. The solid
lines in the plot represent how much average speedup is achieved by each approach.
The dashed lines represent the corresponding minimum and maximum speedup for
each approach. For very small numbers of nodes, the basic MILP formulation is better
than the other approaches. However, when the number of nodes increases, first, the
partial-ordering heuristic and, then, the total-ordering heuristic perform best.
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:20

C. E. Tuncali et al.
Table II. Comparison of Different Approaches

# Nodes
10–15

30

40

50

60
70
80
90
100
110
130
150
170

Approach
Basic
Partial
Total
Basic
Partial
Total
Basic
Partial
Total
Basic
Partial
Total
Basic
Partial
Total
Partial
Total
Partial
Total
Partial
Total
Partial
Total
Partial
Total
Total
Total
Total

Average speed-up
1.48
1.47
1.46
1.68
1.71
1.46
1.48
1.62
1.55
1.2
1.66
1.67
1.09
1.55
1.59
1.54
1.75
1.39
1.7
1.38
1.61
1.08
1.64
1.04
1.67
1.56
1.62
1.61

Average
solver time
(seconds)
2
1
0.5
2620
1558
26
9256
2091
606
18000
12481
5174
18000
17400
11685
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000

% found
Solutions
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
64%
100%
100%
100%
100%
100%
100%
60%
100%
50%
100%
30%
100%
100%
100%
100%

# Tests
150

50

50

50

50
30
30
20
30
10
10
10
10

Fig. 7. Comparison of speedup values between different approaches.

In Figure 8, we illustrate the comparison between the two heuristics and the basic
MILP formulation in terms of the average solver execution time over the number
of nodes. Each line in the graph represents the average solver execution time spent
for each approach. As is expected, due to the time limit given to the solver, as the
number of nodes increases, the solution times for all approaches converge. However,
the experiments on models with smaller numbers of nodes suggests that the proposed
heuristics can shorten the solver execution time. In the graph, it can be observed
that the average solver execution time for the proposed heuristics (as a function of
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:21

Fig. 8. Comparison of solver execution time between different approaches.

Fig. 9. An example multirate block diagram.

node count) is smaller than the basic formulation. Combining the observations from
Figures 7 and 8, we can see that the total-ordering heuristic returns better solutions
within shorter solver runtime compared to the other approaches.
6.2. An Example on Multirate Models

We illustrate our approach for multirate models on a simple synthetic example. Figure 9
illustrates the block diagram of an example model. We included WCET and sampling
period information for each block in Figure 9. The notation for this information is in the
format “block name, (WCET:sampling period).” For instance, the block “A” has a WCET
of 1ms and a sampling period of 20ms. It can be seen that the blocks “A” to “G” have a
sampling period of 20ms, while the blocks “H” to “Y” have a sampling period of 80ms.
The blocks “RT1” and “RT2” are Simulink built-in rate-transition blocks that provide a
mechanism for data transfer between the blocks of different rates. The rate-transition
blocks will have their inputs and outputs executing on different sampling periods. In
this example, “RT1” has a sampling period of 20ms for its input and 80ms for its output,
and “RT2” has a sampling period of 80ms for its input and 20ms for its output. For
the rate-transition blocks, we provide the WCETs for both sampling periods, separated
by a comma. The intercore communication costs are 8μs for transmission and 8μs for
reception for any data connection between the blocks.
The total WCET is 13ms for the blocks with the sampling period of 20ms, including
the executions of rate-transition blocks for this sampling period. The total WCET for
the blocks with the sampling period of 80ms is 90ms. This makes a task generated
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:22

C. E. Tuncali et al.

Fig. 10. Block dependency graph for 20ms.

Fig. 11. Block dependency graph for 80ms.

from the blocks with period 80ms not schedulable without seeking a feasible partition
of the blocks to multiple cores.
Our tool computes the hyperperiod as H = lcm(20, 80) = 80ms, where lcm() computes
the least common multiple of its parameters. The firing times in the hyperperiod are
given as F = {0, 20, 40, 60} in milliseconds, which are the multiples of the sampling
periods during the hyperperiod. Note that the firing time at 80ms is actually the
beginning of the next iteration of the hyperperiod. The blocks with the sampling period
of 20ms are fired at every firing time. On the other hand, the blocks with a sampling
period of 80ms are only fired at the beginning of the hyperperiod. Then, the BDGs for
each rate are created. Figures 10 and 11 give the BDGs for the sampling periods of
20ms and 80ms, respectively.
In the next step, the MILP solver is utilized to find a feasible mapping for the blocks.
Here, we target a dual-core architecture. The updated model with the block-to-core
mapping is given in Figure 12. For any connection between different cores, we add
an intercore sender block to the sending core and an intercore receiver block to the
receiving core. Because of space considerations, we cannot provide the figures of the
model for each core here. In brief, the copy of the model for each core has the blocks of
the other core commented out. Thus, in the generated model for Core 1, only the blocks
that are mapped to Core 1 are active, while all the others are commented out and vice
versa for Core 2. With this suggested mapping and computed ordering of the blocks,
all of the blocks can complete execution in their periods on a multicore architecture.
Figure 13 gives the core mapping of the blocks with the execution-time information in
a hyperperiod. The horizontal lines illustrate the execution of each block. The notation
for the labels is (block name, execution copy in hyperperiod : sampling time). For
instance (RT1, 0 : 20) corresponds to the execution copy 0 of “RT1” block for sampling
period 20ms. Note that the copy counts start from 0 for the first execution. The ratetransition blocks “RT1” and “RT2” execute for both sampling periods. In Figure 13, we
can observe that, at every firing time, first, the blocks with smaller sampling periods
are executed first; then, the blocks from larger sampling periods are executed. For
instance, on the CPU core 1, in the time window starting from the firing time at 0ms,
up to the firing time at 20ms, first, the blocks with a period of 20ms are executed.
After execution of these blocks is completed, blocks with a period of 80ms, which have
a total WCET smaller than or equal to the remaining time to the next firing time, that
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:23

Fig. 12. Updated model for multicore partitioning (contains blocks for both cores).

Fig. 13. CPU core mapping and execution information of blocks.

is, the part of “RT1” for 80ms and the block “J,” are executed. Note that the execution
information is based on WCETs. An actual execution on the target platform would have
the same execution ordering with possibly shorter execution durations and earlier start
times (since an actual execution time may be shorter then the WCET).
6.3. Case Study: Toyota Diesel Engine Controller

We used the diesel engine controller model from Huang et al. [2013] as a single-rate
case study from the industry. The model has 1004 blocks when flattened, as described
in Section 5. It has 7 inputs that are multiplexed into a single-input bus signal and
6 outputs that are multiplexed into a single-output bus signal. Since the model has
cycles inside the subsystems, our tool flattens the model by searching all blocks inside
virtual subsystems, breaks the cycles as described in Section 5, and merges blocks
inside subsystems without introducing new cycles. For parallelizing this model, we set
the target model depth to 2. The BDG capturing a flattened representation of the model
up to the target model depth is generated. The generated BDG contains 153 nodes and
a total of 184 connections between these nodes. Our target platform for this case study
is the dual-core architecture from Freescale, which is described in Section 3. In our
target hardware setup, we have a total of 3.8KB shared memory available.
For a model of this size, both the basic MILP formulation and the partial ordering
heuristic fail to find a solution in 10h. However, by merging blocks of subsystems with
depth more than 2, and using our total-ordering heuristic, our tool returned a solution
to the given problem within an average of 1.2h of solver time. Here, the average is
taken over 20 experiments with different sets of WCET assignments, that is, with
randomly generated WCETs that are proportional to the complexity of the blocks. The
ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:24

C. E. Tuncali et al.

tool suggested multicore mappings with a speedup factor of 1.44, on average. This result
is parallel with our expectations based on the experiments carried out on randomly
generated DAGs, and it illustrates the applicability of our approach to reasonably large
problems in industry.
7. CONCLUSIONS

In this article, we present an approach for parallelizing single-rate and multirate
block diagram models on multi-core architectures. We propose a heuristic for partially
deciding execution order of independent blocks when they are mapped to the same core.
According to the experimental results for single-rate models with randomly generated
DAGs, the MILP solver returns better solutions with this proposed heuristic in a
reasonable limited solver execution time for models with around 50 to 60 blocks in
our experimental environment. For models with a larger number of blocks, we propose
another heuristic in which the execution order of all the independent blocks is decided
in advance. With this approach, our tool could handle models larger than 150 blocks.
We present this heuristic together with block-merging methods on a single-rate case
study from the industry, in which our tool reduced 1004 blocks to 153 nodes on the
BDG, and solved the problem on the reduced BDG. The results from the case study
illustrate how our approach can handle single-rate models that are large enough for
a practical application in industry. The introduced heuristics may not be as useful in
multirate models, because these heuristics may eliminate the opportunity to fit small
blocks in the idle times of the schedule in which no higher-priority task is executing.
For future work, we consider extending this work by introducing heuristics for
solving the optimization problem for the multirate models, and by studying models
with blocks that have priority assignments. In addition, available parallelization under
different triggering conditions should be further analyzed for the event-triggered
models to get more optimal results. For large models, in order to improve efficiency of
the heuristics, investigating control-flow level parallelization opportunities inside the
models is another promising approach to be studied. Similar to the schedule-generation
approach proposed by Lee et al. [2012], a depth-first traversal on the BDG can be done
to explore the model, and the merging of the blocks can be done in a smarter way
based on the parallel paths in the graph. Furthermore, we plan to incorporate WCET
estimation tools in our framework.
ACKNOWLEDGMENTS
We would like to thank Dr. Ken Butts and Toyota Technical Center for providing us the diesel engine model.

REFERENCES
Karl J. Åström and Björn Wittenmark. 1997. Computer-controlled Systems (3rd ed.). Prentice-Hall, Inc.,
Upper Saddle River, NJ.
Tobias Achterberg. 2009. SCIP: Solving constraint integer programs. Mathematical Programming Computation 1, 1, 1–41.
Ahmad Al Sheikh, Olivier Brun, Pierre-Emmanuel Hladik, and Balakrishna J. Prabhu. 2012. Strictly periodic
scheduling in IMA-based architectures. Real-Time Systems 48, 4, 359–386.
AUTOSAR. 2015. AUTOSAR Specification.Retrieved September 7, 2016 from http://www.autosar.org.
Sanjoy Baruah. 2015. The federated scheduling of systems of conditional sporadic DAG tasks. In Proceedings
of the 12th International Conference on Embedded Software (EMSOFT’15). IEEE Press, Piscataway, NJ,
1–10. http://dl.acm.org/citation.cfm?id=2830865.2830866
Armin Bender. 1996. Design of an optimal loosely coupled heterogeneous multiprocessor system. In European
Design and Test Conference (ED&TC’96). Proceedings. IEEE, 275–281.
Girish Rao Bulusu. 2014. Asymmetric Multiprocessing Real Time Operating System on Multicore Platforms.
Master’s thesis. Arizona State University, Tempe, AZ.

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

Automatic Parallelization of Multirate Block Diagrams of Control Systems

15:25

Arquimedes Canedo, Takeo Yoshizawa, and Hideaki Komatsu. 2010. Automatic parallelization of Simulink
applications. In Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation
and Optimization. ACM, 151–159.
Paul Caspi, Norman Scaife, Christos Sofronis, and Stavros Tripakis. 2008. Semantics-preserving multitask
implementation of synchronous programs. ACM Transactions on Embedded Computing Systems 7, 2,
15.
CAST. 2014. Position Paper CAST-32 Multi-core Processors. Technical Report. Federal Aviation Administration.
Minji Cha, Kyong Hoon Kim, Chung Jae Lee, Dojun Ha, and Byoung Soo Kim. 2011. Deriving highperformance real-time multicore systems based on Simulink applications. In IEEE Ninth International
Conference on Dependable, Autonomic and Secure Computing (DASC’11). IEEE, 267–274.
Jing Chen and Alan Burns. 1997. A three-slot asynchronous reader/writer mechanism for multiprocessor
real-time systems. Report-University of York Department of Computer Science YCS.
Scott Cotton, Oded Maler, Julien Legriel, and Selma Saidi. 2011. Multi-criteria optimization for mapping
programs to multi-processors. In 6th IEEE International Symposium on Industrial Embedded Systems
(SIES). IEEE, 9–17.
Jonathan Currie and David I. Wilson. 2012. OPTI: Lowering the barrier between open source optimizers and
the industrial MATLAB user. Foundations of Computer-aided Process Operations. 8–11.
Robert I. Davis and Alan Burns. 2011. A survey of hard real-time scheduling for multiprocessor systems.
ACM Computing Surveys 43, 4, 35.
Peng Deng, Fabio Cremona, Qi Zhu, Marco Di Natale, and Haibo Zeng. 2015. A model-based synthesis flow
for automotive CPS. In Proceedings of the ACM/IEEE 6th International Conference on Cyber-Physical
Systems. ACM, 198–207.
EASA. 2012. EASA/2011/6 Final Report. Technical Report. European Aviation Safety Agency.
Johan Eker, Jörn W. Janneck, Edward Lee, Jie Liu, Xiaojun Liu, Jozsef Ludvig, Stephen Neuendorffer, Sonia
Sachs, Yuhong Xiong, and others. 2003. Taming heterogeneity-the Ptolemy approach. Proceedings of
IEEE 91, 1, 127–144.
Ahmed Elhossini, John Huissman, Basil Debowski, Shawki Areibi, and Robert Dony. 2010. An efficient
scheduling methodology for heterogeneous multi-core processor systems. In International Conference on
Microelectronics (ICM’10). IEEE, 475–478.
Juraj Feljan and Jan Carlson. 2014. Task allocation optimization for multicore embedded systems. In 40th
EUROMICRO Conference on Software Engineering and Advanced Applications. IEEE, 237–244.
Julien Forget, Frédéric Boniol, David Lesens, and Claire Pagetti. 2010. A real-time architecture design language for multi-rate embedded control systems. In 25th ACM Symposium on Applied Computing. Sierre,
Switzerland, 527–534. Retrieved September 7, 2016 from https://hal.archives-ouvertes.fr/hal-00688490
Raul Gorcitz, Emilien Kofman, Thomas Carle, Dumitru Potop-Butucaru, and Robert De Simone. 2015. On
the scalability of constraint solving for static/off-line real-time scheduling. In Formal Modeling and
Analysis of Timed Systems. Springer, 108–123.
Ronald L. Graham. 1969. Bounds on multiprocessing timing anomalies. SIAM Journal on Applied Mathematics 17, 2, 416–429.
Frederik Gwinner. 2011. Transitive reduction of a DAG v1.2. (2011). Retrieved September 7, 2016 from http://
www.mathworks.com/matlabcentral/fileexchange/32723-transitive-reduction-of-a-dag.
Pierre-Emmanuel Hladik, Hadrien Cambazard, Anne-Marie Déplanche, and Narendra Jussien. 2008. Solving a real-time allocation problem with constraint programming. Journal of Systems and Software 81,
1, 132–149.
Meng Huang, Hidemoto Nakada, Srinivas Polavarapu, Richard Choroszucha, Ken Butts, and Ilya
Kolmanovsky. 2013. Towards combining nonlinear and predictive control of diesel engines. In American Control Conference (ACC’13). IEEE, 2846–2853.
BaekGyu Kim, Linh TX Phan, Oleg Sokolsky, and Lnsup Lee. 2013. Platform-dependent code generation for
embedded real-time software. In International Conference on Compilers, Architecture and Synthesis for
Embedded Systems (CASES’13). IEEE, 1–10.
Raimund Kirner, Roland Lang, Peter Puschner, and Christopher Temple. 2000. Integrating WCET analysis
into a MATLAB/Simulink simulation model. In Proceedings of the 16th IFAC Workshop on Distributed
Computer Control Systems. 79–84.
Takahiro Kumura, Yuichi Nakamura, Nagisa Ishiura, Yoshinori Takeuchi, and Masaharu Imai. 2012. Model
based parallelization from the Simulink models and their sequential C code. In Proceedings of the
17th Workshop on Synthesis and System Integration of Mixed Information Technologies (SASIMI’12).
186–191.

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

15:26

C. E. Tuncali et al.

Edward Ashford Lee and Sanjit Arunkumar Seshia. 2015. Introduction to Embedded Systems - A CyberPhysical Systems Approach (2nd ed). LeeSeshia.org.
Haeseung Lee, Weijia Che, and Karam Chatha. 2012. Dynamic scheduling of stream programs on embedded
multi-core processors. In Proceedings of the 8th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis. ACM, 93–102.
Roberto Lublinerman, Christian Szegedy, and Stavros Tripakis. 2009. Modular code generation from synchronous block diagrams: Modularity vs. code size. SIGPLAN Not. 44, 1, 78–89.
Roberto Lublinerman and Stavros Tripakis. 2008. Modular code generation from triggered and timed block
diagrams. In Real-Time and Embedded Technology and Applications Symposium. IEEE, 147–158.
Chris Ostler and Karam S. Chatha. 2007. An ILP formulation for system-level application mapping on
network processor architectures. In Proceedings of the Conference on Design, Automation and Test in
Europe. EDA Consortium, 99–104.
Claire Pagetti, David Saussié, Romain Gratia, Eric Noulard, and Pierre Siron. 2014. The ROSACE case study:
From Simulink specification to multi/many-core execution. In IEEE 20th Real-Time and Embedded
Technology and Applications Symposium (RTAS’14). IEEE, 309–318.
Wolfgang Puffitsch, Eric Noulard, and Claire Pagetti. 2015. Off-line mapping of multi-rate dependent task
sets to many-core platforms. Real-Time Systems 51, 5, 526–565.
Pranav Tendulkar, Peter Poplavko, Ioannis Galanommatis, and Oded Maler. 2014. Many-core scheduling of
data parallel applications using SMT solvers. In 17th Euromicro Conference on Digital System Design
(DSD). IEEE, 615–622.
Lothar Thiele and Pratyush Kumar. 2015. Can real-time systems be chaotic?. In Proceedings of the 12th
International Conference on Embedded Software. IEEE Press, 21–30.
Cumhur Erkan Tuncali, Georgios Fainekos, and Yann-Hang Lee. 2015. Automatic parallelization of Simulink
models for multi-core architectures. In IEEE 12th International Conference on Embedded Software and
Systems (ICESS). IEEE, 964–971.
Dan Umeda, Takahiro Suzuki, Hiroki Mikami, Keiji Kimura, and Hironori Kasahara. 2015. Multigrain
parallelization for model-based design applications using the OSCAR compiler. In Proceedings of the
28th International Workshop on Languages and Compilers for Parallel Computing. 151–165.
Ying Yi, Wei Han, Xin Zhao, Ahmet T. Erdogan, and Tughrul Arslan. 2009. An ILP formulation for task
mapping and scheduling on multi-core architectures. In Design, Automation & Test in Europe Conference
& Exhibition (DATE’09). IEEE, 33–38.
Received October 2015; revised February 2016; accepted May 2016

ACM Transactions on Embedded Computing Systems, Vol. 16, No. 1, Article 15, Publication date: October 2016.

A Code Generation and Execution Environment for Service-Oriented Smart Home
Solutions
Yann-Hang Lee1, Wu Li1, Wei-Tek Tsai1,2, Young-Sung Son3, and Kyung-Duk Moon3
1

Computer Science and Engineering
Dept.
Arizona State University
Tempe, AZ 85287

2

Computer Science and Engineering
Dept.
Tsinghua University,
Beijing, China

Abstract
As smart home environment gets more and more diverse
with newly evolved devices and applications, instead of
programming each household, there is a need for a
knowledge-based framework to facilitate the automatic
composition of appropriate services. The framework should
address the issues of finding suitable devices automatically,
composing feasible plans, and making the composed plans
executable. In this paper, we propose a way to compose the
executable plans for the smart home devices based on
ontology-base process models. A code generator and an
execution environment are then discussed. The generated
code can be deployed and scheduled in the execution
environment. It can also query the ontology and dynamically
find any possible replacements from registered devices at
runtime. To demonstrate the framework, an experiment with
a simple application is included in the paper.

1 Introduction
Smart homes [1][2] have emerged as a focused
application area of distributed embedded systems where
digital appliances, sensor devices, PDAs, and handheld
computers are integrated to facilitate intelligent services for
households. Differing from the fixed applications in
traditional embedded systems, the intelligent services
provided in smart homes are context-dependent and should
be composed with appropriate appliances and services
together. An example service is climate control. An
intelligent climate control service may invoke location
service to know where and when the control function should
be initiated. It should check households’ profiles to obtain
the preferable temperature, infer the cooling and heating
functions to available appliances, and determine the most
energy efficient and quiet approach to adjust room
temperature. The weather service may be called up to figure
out whether any windows should be open during the climate
control service duration.
This example climate control service is merely a generic
function in smart homes which should address the questions
This work was supported partially by the IT R&D program of
MKE/KEIT, [2009-F-027-01, Development of Interoperable Home
Network Middleware for settling Home Network Heterogeneity].

3

Electronics and Telecommunications
Research Institute
Daejeon, South Korea

about when the service should be started, where it should be
applied, and how it is performed. However, to construct such
an intelligent service, service ontology for home
applications and for appliances will be required to build
function plans. Then, to realize such intelligent services, a
framework for smart homes should be able to:
• Compose function plans to fulfill the requested
services base on the knowledge of home services, the
functions and characteristics of home appliances, and
the available devices in each residence.
• Generate and deploy the service code that will invoke
the smart home devices according to the composed
function plan.
• Modify the device invocations and service plans based
on any changes in the context and device status at
runtime.
Ontology has received significant attention in recent years
with the emergence of semantic web [3]. It is a kind of
knowledge representation describing a conceptualization of
a domain. Furthermore, its use is extended to ServiceOriented Computing (SOC) to facilitate more intelligent
service discovery and composition. The current ontology
languages [4], such as RDF and OWL, are usually
represented in XML and can be processed by machines.
They support the specification of concepts, relationships,
and associated classification and reasoning mechanisms for
data. Ontology has been established for context-aware and
smart home applications. For instance, in [5], a Getting up
scenario is presented to show whether the ontology-based
model is valuable for description of context information in a
home domain. In [6], an ontology-based model of the Telehealth Smart Home is presented to initialize Bayesian
networks for patient activity recognition. Furthermore,
ontology is used to model the relations between devices and
services in a home environment and to manage software
configurations [7].
To construct home services, we may adopt ServiceOriented Computing (SOC) approach which utilizes basic
service constructs to build the required functionalities [8].
Rather than building static applications, SOC approach
composes applications by discovering and invoking services
based on published interfaces. By describing home services
as generic operation processes and wrapping up home
1

978-1-4244-5299-6/09/$26.00 (c)2009 IEEE

1. PSML-based
service specification

Device/Service
Registration
Interface

Device

Device

1. device
registration

2 Get home
knowledge
Information

Device

3 Get service/
device
registration
information

uG
pk
X
Y
Z

k thjG
h

ˎ

hmaWZ

ˎ

Y[aXZ

ˎ

ZYaki

ˎ

6 Query for Device
and Service
information at run
time

Service

...
Service

Ontology

k
pk

kG thj
h

z

{ 

h

hmaWZ

m



i

Y[aXZ

m



j

Yia\\

y

z

ˎ

Device

4

5. Scheduling and
execution

Deploy the code

Figure 1 System Architecture for Service-Oriented Smart Homes
used in writing the templates. For instance, in CodeSmith,
appliances as service providers, the SOC computing
the syntax of the template is very similar to ASP.NET.
paradigm can be employed in smart home applications. For
The code generation frameworks can be viewed as a
instance, mBedded Smart Home Extension [9], one of the
generic tool. When we actually want to generate code for a
popular OSGI smart home service platform, offers a set of
specific domain, the major work will fall into the design of
generic components and covers various of scenarios for
the templates. For the home services, the code generation
smart home system, in which includes using residential
templates must include the initiation of devices and
gateways, intelligent home control panels, tablets, advanced
communication channels, scheduling issues (event or time
remote controls, media centers etc. [10] presents an open
triggered), failure recovery, and device sharing among
architecture for home service development. Using the
multiple running services.
“service platform”, can solve the distribution and protocol
heterogeneity problem and further facilitate the dynamic
In the following section, we start with an architecture
composition of distributed and heterogeneous home device
model to illustrate the home service systems. The designs of
services.SOA approach has a great impact for the
home service ontology and device registration are briefly
development of rapid and portable composition of home
introduced. In Section 3, we present the service process
applications in heterogeneous and embedded environments.
models and the procedures for code generation. The
procedure first links functional plans with available devices
In our previous work [13], the construction of an
and create a XML-based intermediate representation. Then,
ontology-based knowledge representation for the semantic
in the second step, the Codesmith templates are incorporated
content and process model of smart home applications was
to generate target code. In Section 4, the service deployment
investigated. The proposed system consists of a smart home
and execution environment is presented, followed by a
knowledge-base (ontology), a household database, a service
simple demonstrative experiment in Section 5. Finally, a
manager, and a plan deployment component. The
conclusion is made in Section 6.
composition of appropriate applications is facilitated by a
generic knowledge representation and is based on user
2 System Architecture
profile and available appliances. In this paper, we report the
In this paper we propose a code generation and execution
research on automated code generation and the runtime
environment for service-oriented smart home solutions. The
environment for service deployment and execution. These
architecture of the system is illustrated in Figure 1. As
steps differ from the SOC web service applications since
shown, the system builds the specification for the services
any control operation of home appliances or embedded
using Process Specification Modeling Language 1 (PSML)
devices should be event-triggered and should be run
[11][12], infers the home services from the ontology, and
continuously once it is activated.
composes the function plan for the services using the
Our code generation approach is based on the pre-defined
registered devices in each household. After that, code is
templates of service process model. To generate source code
generated from the models with the real devices and the new
in a target programming language, several existing tools can
service is registered. Users can then pick up a registered
be employed. For example, CMSCreator [14] can generate
service and deploy the generated code in an execution
VB.net and C# code, CodeFuture FireStorm/DAO [15]
engine. The engine provides an execution environment
produces Java code, and uml2php [16] results in PHP code.
where a service can be loaded and scheduled to run.
The most popular code generation tools, like MyGeneration
[17] and CodeSmith [18], can have the ability to generate
1
PSML is a service-oriented process modeling language developed
any kind of code mentioned above. Both of them are based
by ASU SRL lab and can be used to facilitate service modeling,
on the template and the differences of them are the language
composition, simulation, code generation and static and dynamic
the tools themselves are based on and the style of scripts
analysis.

2

It allows services to deliver/receive commands and
messages to/from the equipped devices in the household.
During execution, if the devices in the service have any
exceptions, the execution engine allows the code to replace
the faulty devices with other functional devices in order to
fulfill the function of the service. In [13], we introduced the
use of ontology to build home services. The ontology
supports function inference query and semantic-based
service discovery for smart home services. In addition,
domain ontology, policy/preference ontology, and function
ontology, are included. Device ontology describes the
concepts related to devices. Function ontology, as the core
of the whole knowledge base, describes the concepts related
to functions. For each composed function, templates are
attached. Policy/preference ontology describes the basic
system constrains and preference rules about the use of
devices and functions. Those rules may include conditions
refers to domain ontology. Domain ontology describes the
classification of domain profiles.

registration repository. As shown in Table 1 of an example
repository, each row consists of device identifier, device
type, function of the device, status of the device, location of
the device and the energy consumption of each device. Note
that device type is referring to device ontology and function
information is referring to function ontology. Besides the
static information of all devices in a household, the
registration also records dynamic information including
which services have been deployed and which appliances
are running.
Although devices are recorded in the device registration
repository, the semantics of the devices are maintained
through the ontology. For example, when the room
temperature is higher than the preferred temperature, we
would like to invoke a cooling function that is described in
the function ontology. One of the cooling functions may be
required to use an air conditioner. The available air
conditioner can be found in device registration repository
and its attributes and interfaces can be obtained from device
ontology.

3 Service Code Generation

Figure 2 Ontology Systems for Smart Homes
These four ontology systems are cross-referenced as
shown in Figure 2. For instance, in device ontology, the
semantic information of the function of each device category
is described using concepts defined in function ontology.
Thus, an air conditioner device can perform a function to
cool down a zone area of a house and is described by several
attributes, such as its capacity in BTU units. In function
ontology, a climate control service may invoke cooling
function by turning on an air conditioner and closing all
windows in the zone area.
While ontology describes the relations and hierarchy of
the generic devices and functions, the specific devices
information of a household is maintained in a device
Device ID
Gid001
Gid002
Gid003
Gid004
Gid005
Gid006
Gid007
…

Device Type
Sensor1
Sensor2
TV
Computer
Computer
Cellphone
AirCondition
…

Function
InLivingRoom
InStudyRoom
DisplayOnTV
DisplayOnComputer
DisplayOnComputer
DisplayOnCellphone
Cooldown
…

To operate appliances, home services are described as
workflows, and each workflow can include multiple devices
or other sub-services. The abstract function workflow of
operation specifications is built first for each service and
then the ontology is referred to obtain the suitable device
types for the operation specifications. Using device types we
can interact with the device registration to get the real
devices. Code generator component produces the code after
we filled the real device information into the service.
We model the composed function flow using
PSML[11][12] for the target smart home applications.
PSML is a service-oriented modeling language designed for
embedded applications. For smart home applications, we
adopt two sub-models of PSML. The element model defines
and classifies model concepts/elements, attributes, and
relations among the models. On the other hand, the
behavior model defines the composition logic to provide
value-added functions by using functions provided by other
services.

Status
ON
ON
ON
ON
ON
ON
ON
…

Location
Living room
Study room
Living Room
Study room
Study room
Bedroom2
…

Energy consumption
0.01kw/s
0.01Kw/s
0.6kw/s
0.4kw/s
0.4kw/s
0.01kw/s
5kw/s
…

Table 1 Example Device Registration Repository

3

3.1 Element Model in PSML
In PSML[11][12], the element model is the metadata for
concepts and includes Actor, Condition, Data, Action,
Relation and Events. An Actor is a system or a device with
a clear boundary that interacts with other actors. A
Condition is a predicate on data elements used to
determine the course of a process. A Data element is an
information carrier that represents the state of an actor or
the status of the entire system. An Action represents an
operational process to change state of an actor or perform
computation. An Attribute specifies various properties such
as security, safety, performance, timing, or reliability
information. An Event represents an observable occurrence
with no time duration, and it can be an input to an actor or
an output from an actor.

3.2 Behavior Model for Smart Home Service
Workflow
The behavior model of PSML forms the workflow
(which later we will call it service or composed function
after we filled the devices inside). Actors, conditions, and
actions can have the internal workflow and thus they all
have behaviors. In PSML language, the smart home
services we are going to develop can include actors,
conditions, or actions.
To describe the internal workflow using the PSML
elements, we include three basic behavior tags: Assign,
Process and Select. None of these behavior tags is
executable by themselves, but they are just a tag to
describe the control flow among the elements. Assign tag
indicates that there is a need to assign a data to an element.
Select tag is to evaluate a condition and Process tag reveals
the invocation of an action or actor.
The workflow we draw using the behavior tag is called
the specification of an element, and this is essentially the
behavior of the element. Actor, condition, and action, can
have their own specification. Specification is generic and
is independent of physical devices. The following
workflow shows a specification view of an action element.

Specification view only displays an abstract workflow.
To provide actual meaning, we need to specify each
behavior tag with an actual PSML element.
Element View connects the abstract specification with
the real PSML elements. For example, the previous
workflow can be used to describe an action EmailNotifier.
When the EmailNotifier gets a new email, it will try to use
a sensor to detect if there is any people in the living room,
if yes, it will send the email to TV and display it on the TV
screen. If the sensor cannot detect anybody in the living
room, then it will try to detect if there is anybody in the
study room. If there is, it will pop up the email on the
computer screen, otherwise it will send a text message to
the registered cell phone. Figure 4 shows how to specify
the ACDATER element in the specification view and
Figure 5 shows the element view of the EmailNotifier.
Finally, we need to get to the Device View of the
workflow in which the PSML elements are connected to
real devices. We do this in three steps:
1) Search with the name of the devices/functions
described in workflow’s Element View.
2) Get the device candidates or function candidates from
the device registration repository, and generate candidate
workflows list as shown in Figure 6 with these devices.
3) Select a workflow and construct the Device View of
the workflow shown in Figure 7. This workflow represents
a composed service using available devices (composed
function plan) and it is saved as a composed plan in the
device registration.
We call the two processes here element linking and
device linking. The element linking in PSML is the process
of linking the specification of a workflow to functions.
Then device linking is the process of linking the functions
with the available devices.

3.3 Code generation from Service Model
After device linking, the actual devices have been
selected and placed in a PSML workflow model. Then, the
model should be converted into executable code that can
be deployed in the execution engine.

Figure 4 Specify Element for the Specification View
Figure 3 Specification View of an Action Element

4

Figure 5 Element View of an Action Element

Figure 7 Device View of an Action Element

Figure 8 Three Views of Workflow Model
Figure 6 Composed Plans Candidates for Device View
The conversion consists of two steps. The first step is a
translation preprocess, in which we serialize the PSML
model as a XML representation. The second step is the
code generation, in which each element type is handled by
its corresponding code converter. Code converter is based
on the Codesmith templates. The templates are executable
and they would convert the XML script to executable code.
The target code can be in different language, thus we can
have the C# converter, Java converter etc. In this paper, we
implemented a Java converter.
Figure 9 shows the code generation based on device
ontology, function ontology, and device registration
repository. PSML timing model[19] also allows us to have
the timing parameter for each element, such as sleep
delays, and periods of iterative execution, and thus in the
code generation we can also convert these timing
information to schedule the execution. Each generated
service code is a Java class and could be deployed to the
execution engine. The generator will generate 1) class
definitions, 2) object initialization, 3) functions (methods),
4) function flow, and 5) task scheduling for each service.
An important feature of the proposed approach is that
code generation is fully integrated with ontology. Each
device has its own ontology specified using PSML
elements, and its action is represented as a function, which
is a part of generated code. This feature allows complete
traceability from ontology to code for management,
debugging, testing, and monitoring purposes.

Figure 10 shows a code generation example of the
EmailNotifier service we mentioned above. It illustrates
how the XML representation of the PSML model is
converted to the Java code. Note that all classes and
objects are assigned with UUID names to ensure the
uniqueness.

4 Dynamic Service Execution
In the execution environment, the generated code from
the previous phase can be executed by a thread in the
execution pool as shown in Figure 11. Every generated
code will implement a set of interfaces. For each service
running inside this execution engine, it must extend the
interface listed in Figure 12. Execution engine will call the
Execute function to start a service and run it in the
execution pool.
Besides that, the execution engine has three sets of
interfaces to be used by running services: registration
query client, message bus, and a monitoring interface.
Registration query client in the execution engine
provides
a
communication
channel
with
the
Device/Service registration. A running thread can use
getDevice function to query the repository and be noticed
if there is any device status changes. With this feature, we
can initiate runtime replacements with new devices,
functions, or workflows.

5

Figure 9 The Mapping Process in Code Generation

Figure 10 Example Code Generation from a Graphic model to XML Representation to Java Code

Figure 11 Execution Environment
The message bus component in the execution engine
allows the code to interact with device manger. A device

manger converts and translates messages into suitable
network protocol for communicating with devices. The
CallExecutableElement call is to send a message for
device function invocation. The CallCondition interface
allows a read of device condition, status, or input data. In
addition, the NotificationListener function inserts a
callback function that will be triggered when the device
manager notices any status changes.
The monitoring interface component in the execution
engine offers an interface to monitor code execution status.

6

It can show the related status of each device involved in
the workflow execution. The execution engine maintains a
list that holds all the reference to the running services.

ii. Code generation using CodeSmith (Figure 14)
iii. Device status after plan0 is deployed which uses
Gid001 Future Home Sensor, Gid002 Future Home
Sensor and Gid004 Lenovo computer(Figure 15)
iv. The Lenovo computer Gid004 becomes faulty and a
device replacement with Gid005 HP computer takes
place (Figure 16).

6 Conclusion

Figure 12 A Service Interface for the Execution Engine.
In a smart home function execution, two kinds of
dynamic replacements can be available:
1. Device replacement
Device replacement means the process of replacing a
device in use with another device that have the same
function. Device replacement would not change the
behavior of the workflow.
2. Workflow plan replacement
Workflow replacement means that we would like to
replace the entire workflow with a different one. Usually,
when a workflow replacement happens, it suggests that the
current workflow is not satisfied with the needs of the user
or cannot proceed with any faulty devices. When a
workflow is compiled, all possible composed plans are
saved in the repository and the required devices are listed
too. Hence, a replacement can take place by terminating
the running plan and deploying a new plan.
To enable the device and workflow plan replacement at
runtime, the generated code carries the function names it
needs, and since it also has the device type information, it
can easily query the repository and see if there are similar
devices that can perform the similar function. Once a
similar device is found, the code would just replace the
candidate device with the current device and we can
execute the workflow with the new device.

5 An Experiment
To demonstrate the code generation and home service
execution, we show the example workflow model of
EmailNotifier service in the tool we developed. In addition,
we run the generated code in an emulator to show our
dynamic replacement ability in the execution engine. We
assume there are two computers at the study room from HP
and Lenovo to delivery email content.
In the following figures, the snapshots are presented for:
i. Workflow model and registration repository (Figure
13)

In this paper, a code generation and execution
environment for service-oriented smart home solutions is
presented. This environment can support modeling the
composed function plans based on the knowledge of home
services, the function characteristics of home appliances,
and the available devices in the residence. Also, this
environment can generate service code ready to be
deployed for execution. Smart home appliances can be
invoked according to the composed plan we modeled.
Finally, it enables dynamic replacements of device
invocations and service plans if there are any changes in
the context and device status at runtime.
For future work, we will investigate the approaches for
code generation based on environment and personal
profiles as well as to support for various analyses such as
energy consumption.

References
[1]

[2]

[3]

[4]

[5]

[6]

[7]

W.K. Edwards and R.E. Grinter, “At Home with
Ubiquitous
Computing:
Seven
Challenges,” Ubicomp
2001:
Ubiquitous
Computing, Lecture Notes in Computer Science, vol.
2201, 2001, pp. 256-272.
Gu, T. Pung, H.K.; Zhang, D.Q. “Toward an OSGibased infrastructure for context-aware applications,”
Pervasive Computing, IEEE, Vol. 3, Issue 4, Oct.Dec. 2004 Page(s): 66 – 74.
N. F. Noy, M. Sintek, S. Decker, M. Crubezy, R. W.
Fergerson, and M. A. Musen. “Creating Semantic
Web Contents with Protege-2000”, IEEE Intelligent
Systems 16(2):60-71, 2001.
“OWL-S: Semantic Markup for Web Services”,
http://www.w3.org/Submission/OWL-S, Nov 22,
2004.
E. Kim and J. Choi. “An Ontology-Based Context
Model in a Smart Home”, Workshop on Ubiquitous
Web Systems and Intelligence (UWSI 2006), pp. 1120.
F. Latfi, B. Lefebvre1 and C. Descheneaux,
“Ontology-Based Management of the Telehealth
Smart Home, Dedicated to Elderly in Loss of
Cognitive Autonomy”, Third International Workshop,
OWL Experiences and Directions. 2007,
E. Meshkova, J. Riihijarvi, P. Mahonen, and C.
Kavadias. “Modeling the home environment using

7

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]
[17]

[18]

[19]

ontology with applications in software configuration
management,”
International
Conference
on
Telecommunications, 2008. ICT 2008, pp. 1-6.
Papazoglou, Michael P. ; Traverso, Paolo ; Dustdar,
Schahram ; Leymann, Frank ; Krämer, Bernd J. :
Service-Oriented Computing: A Research Roadmap,
May 2006
ProSyst. (2009). mBS Smart Home Extension.
Available:
http://www.prosyst.com/products/osgi_ext_smart.ht
ml. Last accessed 14 Oct. 2009.
Andre Bottaro, Anne Gerodolle, Home SOA - Facing
Protocol Heterogeneity in pervasive Applications,
5th IEEE International Conference on Pervasive
Services (ICPS 2008), Sorrento, Italy, July 2008
W. T. Tsai, B. Xiao, Q. Huang, Y. Chen, and R. Paul,
"SOA Collaboration Modeling, Analysis, and
Simulation in PSML-C", Proc. of the 2nd IEEE
International Symposium on Service-Oriented
Applications,
Integration
and
Collaboration
(SOAIC’06), October 2006.
W.T. Tsai, Ray A. Paul, Bingnan Xiao, Zhibin Cao,
Yinong Chen, "PSML-S: A Process Specification
and Modeling Language for Service Oriented
Computing", The 9th IASTED International
Conference on Software
Engineering and
Applications (SEA), Phoenix, November 2005, pp.
160-167.
Jingjing Xu, Yann-Hang Lee, Wei-Tek Tsai, Wu Li,
Young-Sung Son, Jun-Hee Park, Kyung-Duk Moon,
"Ontology-Based Smart Home Solution and Service
Composition,"
International
Conference
on
Embedded Software and Systems, 2009. pp.297-304.
AJAXCMSCreator (6.52). (2009), Available:
http://www.developerinabox.com/. Last accessed 21
July 2009.
Code Futures. (2009). FireStorm/DAO Architect
Edition. http://www.codefutures.com/architect/. Last
accessed 21 July 2009.
UML2PHP. (2008). http://www.uml2php.com/. Last
accessed 21 July 2009.
MyGeneration Software. (2008). MyGeneration 1.3.
http://www.mygenerationsoftware.com.
Last
accessed 21 July 2009.
CodeSmith Tools, LLC. (2009). CodeSmith Tools
5.1.3.Available:
http://www.codesmithtools.com/.
Last accessed 21 July 2009.
Wei-Tek Tsai, Hessam Sarjoughian, Wu Li, Xin Sun,
Timing Specification and Analysis for ServiceOriented Simulation, 42nd Annual Simulation
Symposium (ANSS), March 2009

Figure 13 Modeling and Repository

Figure 14 Code Generation Using CodeSmith

Figure 15 Begin Emulation of Plan 0

Figure 16 Device replacement after Gid004 fails

8

Trust-Propagation Based Authentication Protocol
in Multihop Wireless Home Networks
Han Sang Kim, Jin Wook Lee*, Sandeep K. S. Gupta and Yann-Hang Lee
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287
{hanskim, sandeep.gupta and yhlee}@asu.edu,
*Communication Lab.
Samsung Advanced Institute of Technology (SAIT)
Yong Shi, Republic of Korea

thetruth.leegsamsung.com

Abstract-In this paper, we propose an authentication and secure channel establishment protocol that is reliable and adaptable
for multihop wireless home networks. The main idea is that a
home server hands over its authenticating capability to some already authenticated devices to enable these devices to be able to
authenticate other devices which cannot reach the home server directly due to their physical location and radio power constraints.
Key to our design is a neighbor device authentication protocol,
based on pre-user-injected network password, with minimal reliance on public key cryptographic operation. In addition, our
protocol supports a secure channel establishment among heterogeneous devices after authentication. Through the evaluation, we
show that our protocol is resistant to various attacks.

Index Terms- Security, Authentication, Wireless Multi-hop
home Network.
I. INTRODUCTION

IN the future, homes will use multihop wireless networks
to connect all the devices to one another. Multihop wire-

authenticate home devices using trust-propagation in multihop
wireless home networks. The fundamental idea of the proposed
authentication protocol is that a home server hands over authentication privilege to the already authenticated devices. Eventually all the devices in the network are authenticated and then become authenticators on behalf of the home server. Our protocol
achieves low communication cost and scalability by allowing
multihop peer-to-peer communication of devices.
The rest of the paper is organized as follows: Section II describes our system model, assumptions and design goals. In
section III, we propose our trust-propagation based authentication protocol. In section IV, we presents the security analysis
of our protocol. Finally, we conclude this work in section V.
11. THE APPROACH
A. System Model and Assumptions
Our system model consists of a home server and multiple
home devices, such as TV, home theater system, and PDA. Locations of devices are dynamic but we assume that the mobility of devices is limited within the confines of a home. The
first device activation is done by an user by inputting a memorizable password. For instance, when a user buys a home network device and enters a password agreed with the home server.
This assumption is reasonable because unexpected home devices should not be participating in the home network. We assume that home devices have limited radio transmission power
so a multihop network should be installed at home in near future. Since many of the devices in the network may be batterypowered, our design goal is to make the authentication protocol
as efficient as possible in terms of communication overhead, i.e.
minimize number of messages exchanges, as well as computation overhead, i.e. minimize number of (possibly expensive)

less technology could solve a number of limitations that extend
beyond simply eliminating dead zones in home. Depending
upon the data rate (bandwidth) requirement different wireless
technologies can be employed. For example, emerging wireless standards such as Zigbee (IEEE 802.15.4) [1] are suitable
for many home automation and personal healtcare applications
which require low data rate. Zigbee is optimized for prolonged
battery-powered operation. Due to short range (about 1 5m in a
cluttered office/home environment) of a single Zigbee device,
it supports (and requires) multihop routing to increase coverage
of a Zigbee network. To satisfy the demands ofthe digital home
with high-speed data rates, Bluetooth scattemets [3], multihop
WiFi networks [2] and Mesh networks [4] can be used.
Wireless home networks require a robust authentication
mechanism due to the accessibility to the devices, the hetero- public-key cryptography operations.
geneity of communication protocols and the wireless environment. Our authentication protocol relies on a home server to
propagate authentication to the network. In a common home B. Definitions
We here briefly introduce two key concepts of our protocol.
networking environment, a server-based approach is more adas
we
can
most
of
the
on
Badge: A badge is similar to the concept of digital signavantageous,
impose
cost/complexity
tures in terms of the properties such as its unforgeability,
the home server. In this paper, we suggest a way to efficiently
0-7803-9575-1/06/$20.00 ©2006 IEEE

verifiability, unreuseability, and undeniability. It is used
for determining whether authority for authentication belongs to an individual node. This is in contrast to using
it for verifying whether a public key belongs to an individual. Specifically, in our proposed protocol, Badge is
used by a home server to delegate only its authenticating
capability to an already authenticated device. Upon possessing a badge, an authenticated device can authenticate
other devices on behalf of the home server. Note that the
server gives a device only limited authority, i.e. authentication authority.

.}

......

Fig. 1. One hop Authentication Domain

the home server S or other already authenticated devices and
to try to be authenticated. Following are the four possible responses the new device could expect in response to its beacon
message:
. Server response: If the home server receives the beacon
message from a new device, the server responds to let the
device start the Authentication Process.
. Neighbor response with Badge: If one of the neighbor
devices who has a Badge, this already authenticated device could respond to the new device's request message.
The new device starts the Authentication Process with the
neighbor device.
. Neighbor response without Badge: In case no neighbor
device has the Badge. The beacon message is periodically
broadcasted to allow the device to probe other devices.
. No response: There is no neighbor device in the radio
range of the new device. In this case, the new device is
isolated in the network. In order to get connected to the
home network, the new device should be moved into a location from where the device can reach some other devices
in the network.
Only the home server can generate a Badge and grant it to
a device, even though the device is authenticated by one of its
neighboring devices. For ease of understanding, we list the notations used in the paper in Table I.
TABLE I
GLOSSARY

Fig. 2. Multi hop Authentication Domain wvith Trust Propagation

Authentication Domain: When a user wants to introduce
a new device D to be used in an existing home network,
the device needs to be authenticated by the home server S
of the network. According to our system model, it is not
guaranteed that the the new device is located in the radio
range of a home server. In case D is out of range of 5, it
tries to be connected to other authenticated devices to securely reach S. We define Authentication Domain (AD) as
a group of authenticated devices. Every home network has
just one authentication domain. Each device in a AD is allowed to communicate with other devices in the AD using
a secure channel. Specifically, all nodes that have same
Badge are part of the authentication domain AD(Badge).
Figure 1I-B shows an example of Authentication Domain.
III. TRUST-PROPAGATION BASED AUTHENTICATION
PROTOCOL
Once a new device D is initiated to be added to the user's

home network by being provided with the password, it broadcasts a beacon message (also called hello message) to discover

Notation [ Description
Identity of device X
IDX
Initial shared network password for device
npwd
checking
X's master key which is only shared with a
Kx
home server
Session key for X and Y, shared key between X
Kxy
and Y
A random nonce generated by X
Rx
TS
Timestamp
L
Life time of a key or Badge
ATH
Uses of the Badge. The Badge could be used
for various uses according to this limited authority
REP(X) Report message about authentication of device

RQA(X)
ALG

X

Request message of access to device X
An algorithm indicated by a home server

Our proposed protocol consists of two main phases. The initial authentication phase is followed by the expanded authentication phase. As mentioned earlier, our authentication process
is initiated by the password agreement. At this time, we assume
all devices have the same password in the hardware at the start
of the protocol.
A. Initial Authentication Phase: Phase I
Neighbor devices of the home server get securely authenticated by the four protocol packet exchanges as shown in Fig-

After having received Msg 2, device A sends the server back
a message encrypted with its own private key containing the
random number from the server and its own random number.
When the server decrypts Msg 3 with device A's public key and
retrieves the two random numbers, the server authenticates that
the remote device is actually the device A.

ure 3.

[Msg 4] S X~A: {Badge(A), KA, L}A pub

Badge(A) = {IDA, ATH, L}S priv
Through Msg 1 to 3, not only does the server authenticates deFig. 3. Authentication in one hop from the server
vice A but device A also authenticates the home server (mutual
authentication). After the device A becomes aware of a part
In most of the proposed authentication protocols so far, it has of the network, the home server gives device A Badge and a
been assumed that the shared secret key is manually distributed master key which is only shared between the home server and
and typed, and then authentication protocol and key distribution device A so that it will be used for the remainder secure connection with the server instead of using the asymmetric key. The
are built on this assumption. In a home network, it is unreasonable to expect a network user (a family member) to type a long Badge is a certificate signed and issued by the server as was deshared secret key to every device. It is also unreasonable to scribed in an earlier section. It gives device A a permission to
assume that every device is physically connected to the home
authenticate other devices in its neighborhood on behalf of the
server whenever those devices cannot directly perform authenserver. Therefore, we use a simple password-based scheme.
Before a device is used for the first time in a home network, tication procedure with the server due to their physical distance
from the server. In other words, if a home server puts another
a user manually enters a network password (shared secret) in
the device as its only network authentication key. This network device D's identity instead of A's identity in a Badge, it means
password is a weak human-memorizable password because of that the server has allowed the device D to act as the server for
it's limited length. It is only used for verifying if the device authentication purpose.
is user's network device. We discuss the proposed protocol by
describing the packet exchange one by one. We define nine B. ExpandedAuthentication Phase: Phase II
protocol packets represented by [Msg number].
A device physically located where it cannot directly reach a
After bootstrap, any device broadcasts hello message to anhome
server tries to get authenticated by the aid of an already
nounce its presence. If there is an authenticated device includauthenticated
neighboring device with authentication authority.
ing the home server, the device informs unauthenticated neighThe
procedure
requires devices' relaying messages to the home
bor devices. An unauthenticated device, device A, sends Msg 1
server as shown in Figure 4.
to the home server, S as below.
During Hello message exchange, device B is informed from
device A that device A is already authenticated by a home
[Msg 1] A X S : IDA, A-pub, {IDA, RA, TS}Ipwd
Device A generates a large random number RA as a chal- server. But device B still needs to verify the claim of device
lenge, and then unicasts the random number and time stamp A. First, device B sends Msg 1 to device A.
TS encrypted with a network password, its identity and public
[Msg 1] B X= A : IDB, B-pub, {IDB, RB, TS}Ipwd
key. At manufacture time, each device is given a public/private
Suppose that device A is previously assigned Badge and a
key pair certified by the manufacturer in tamper resistant memmaster key by the server in Phase I, and device B is two hops
ory. When the home server receives Msg 1, it decrypts the
away from the server. The neighbor device A, which is in the
message with the network password the user registered. The
radio
range of a device B, sends back a response to the chalhome server S confirms the freshness of the message (to preof a device A with Msg 5.
lenge
vent replay-attack by an adversary) by examining the decrypted

[Msg 5]

time stamp.

[Msg 2] S X= A: IDs, S-pub, Rs, {IDs, RA, Rs}s priv
After checking the time stamp, the home server sends device A back a message containing the random number (RA)
encrypted with the network password and its own random number (Rs) as a challenge (Refer to Msg 2 above). When device
A gets Msg 2, it uses the server's public key S -pub to decrypt
the last argument of the message to obtain RA and Rs. The
message must have come from the home server, since a malicious device is not able to determine RA and encrypt it with the
server's private key. Furthermore, it must be fresh and not be a
replay since device A just broadcasted RA.

[Msg 3]A

#=

S IDA, {IDA, Rs, RA}A-priv

A X B : IDA, S-pub, RA, {IDA, RB, RA, Badge(A)}IBpub

This response message includes Badge assigned by the
server. This message notably claims that device A is authorized
by a home server and has a permission to authenticate other device with Badge. When device B gets Msg 5, device B can easily verify that not only Badge was issued by the server since it
is encrypted by the server's private key, but also the server gave
device A authority to authenticate others since Badge includes
device A's identity and the authority of authentication.

[Msg 3] B X= A : IDB, {IDB, RA, RB}B-priv
After verifying the Badge sent from device A, device B sends
a response to device A in Msg 3. Of course, device B discards

Device A

1

MSg

ID

Bputb,j

5I

5JD

M5g 3: IDB

ub

ID

? R,A

R

Tp

IA, R*,R,

RAt RB J

SbNer

ptq

Msg

4

4

Badgoe(A)4

fa

b-job

Mg 6:

ffiAIB;IA REP,B 8 pub)

'

ae(

Fig. 4. Authentication in more than one hop from the server

Msg 5 if there is

no

Badge in it or Badge is not issued by the

server.

[Msg 6] A X S: IDA, {IDA, REP(B), B-pub}KA
Authentication of device B is performed by neighbor device A
through Msg I to 3. After the authentication of device B, device
A reports the fact to the server in Msg 6. REP(B) is the report
message. Msg 6 is encrypted with the master key. Note that
the purpose of the master key is to enable the use of symmetric
cryptography for encryption/decryption instead of asymmetric
cryptographic technique, which consume more processing resources. Hence, any communication message between a server
and a device is encrypted using the master key after the master
key has been assigned.

[Msg 4] S X= B: {Badge(B), KB, L}B pub
After receiving Msg 6 from device A, the server generates a
master key and makes Badge for device B, and then sends device B Msg 4 encrypted with device B's public key which was
informed by device A in Msg 6. As a consequence, the trust is
propagated two hops away from the server. In a similar manner,
trust can be propagated multiple hops away from the server.

C. Security Access Controlfor Device Communication
A primary concern in home networks is access control, the
specification of how home network devices are allowed to interact with one another. The devices in an Authentication Domain are all networked with a home server through each master
key. For security purpose, however, they should be still have
only limited access to other devices in the domain. In other
words, even though a device belongs to the domain after being
authenticated, the device access must be controlled by the home
server in regards to what modes of accesses it is allowed to perform on other devices. This access control is managed by the
home server based on an access control list which is an array of
entries with the following format:
subject: an identifier of the device in Authenticated Domain

authorization: an indicator of the rights being granted to
the subject
. security level: for security services, a secure level of device categorized by a server. It is based on the capability

and security service required by the device.
Once a device is part of the Authentication Domain, which
means Badge and a master key has been given to the device,
the device is able to access the home server through the master
key. Whenever a device wants to establish a secure connection
with another device it contacts the home server and follows the
protocol as shown in Figure 5. The device sends an access request (Refer to Msg 7) to the home server. Second, the server
checks the access control list and distributes a session key to
the involved devices separately, which will be used for end-toend communication between the two devices. The length of this
key and algorithm for the communication is decided according
to the security level of access control list in the server. The goal
of access control constrained by a security level is to provide
appropriate access/service for heterogeneous home devices.

[Msg 7] B X= S : IDB, {IDB, RQA(C)}KB
Suppose that a device B and C are in an Authentication Domain, which implies that each of them has its own master key
and Badge. Whenever device B wants to establish secure channel with device C, it sends the server a request access message
(RQA(C)) encrypted by its master key. The server checks the
access right in the access control list. If device B has the access right of the device C, the server generates a session key,
KBC, and then sends it to the both parties separately using the
following device specific Msg 8:

[Msg 8] S X B: {ALG, KBC, L}KB
[Msg 8] S X C: {ALG, KBC, L}KC
This session key is used not only for confidential data transmission but also message origin authentication during secure
communication. That is, any message encrypted with the session key after the authentication is believed to originate from
the peer principal who holds the session key. When the server
distributes the session key, the server assigns the appropriate
length of the session key which depends on the capability of
a device, and also lets devices have a choice for an appropriate encryption algorithm, which will be used for encrypting/decrypting communication message between devices. In
home networks, this session key length-agile and algorithmagile technique [5] [6] is useful for device communication session since various traditional computing and embedded Internet

D6'

i

De'Sice C

S&rver

B

Msg 7. ID, {JDS RQA(C)B
Msg .: {ALG, KE1 i,Q

Msg 8: {ALG

KSc OA

msg 9-fMessage

t

10

1.

Fig. 5. Establishment of a secure connection with another device

devices can be networked. Different applications of heterogedevices need different security requirement for communication sessions with one another since certain devices may
take a long time to encrypt/decrypt messages in each session.
After the setup, device B could send device C data messages
encrypted with the assigned session key.
neous

[Msg 9] B

#=

C: {Message}KBC

IV. EVALUATION
A. Security Analysis
Man-in-the-middle attack: The adversary intercepts the messages between the parties (a device and the server) and replaces
them with its own messages. It plays the role of the device in
the messages it sends to the server and at the same time plays
the role of the server in the messages that it sends to the device.
Prevention: When a device which is more than one hop away
from the server broadcasts a random number and a timestamp
encrypted with a network password, its identity and its public key, an adversary puts himself between the parties on the
communication line where it is one hop from the server. The
parties would end up knowing the adversary's public key as
a server's or the same home network device's. However, assuming that the network password has not been guessed or
compromised, the adversary has no way to construct the encrypted message with a network password, so it is forced to
resend {IDA, RA, TS}Ipwd with it's own identity and public
key. When the server receives this message from the adversary,
the server discards it since the decrypted identity with the network password does not match the identity which the adversary
sends. Note that in case, the adversary tries to use the device's
identity, this would be detected if the device continues to passively hear other on-going transmissions while the authentication process is not completed. If a device overhears a transmission with its own identity being used, it can alert the user in
device-dependent manner that an intruder has been detected.
Badge reuse attack: The Badge is transmitted in the following two cases. One is when a server grants a device a Badge
after authentication. The other is when an authorized device,
which has a Badge, sends it to the device in unauthenticated
domain for authenticating on behalf of a server. The adversary intercepts the message which includes a Badge and reuses

it, impersonating an authorized device, when a device in unauthenticated domain requests for an authentication.
Prevention: The transmission of a Badge is always encrypted
with receiver's public key. This makes it hard for an adversary to get a Badge itself without possessing receiver's private
key. However, we can think that the adversary intercepts the
message encrypted with receiver's public key which includes
a Badge. The adversary then reuses it appropriately when authentication is requested by a device in unauthenticated domain.
In this case, the receiver figures out that the device's identity
does not match with the identity in the Badge when it opens the
Badge.
V. CONCLUSIONS

In this paper, we have proposed a reliable and adaptable
authentication and secure channel establishment protocol for
multi-hop wireless home environment. Central to our design
effort is the authentication by a neighboring device, on behalf
ofthe server, in a multi-hop wireless home networks, while supporting mutual authentication and minimizing reliance on public key cryptographic operation. In addition to this, our protocol supports efficient and flexible channel establishment among
heterogeneous devices after authentication. Through security
analysis, we show our protocol is secure because it is resistant
to various attacks. Further, it is efficient and adaptable for multihop wireless home network because it minimizes overheads
such as communication and computation costs.
ACKNOWLEDGEMENT

This work is supported in part by a grant from the Center for
Embedded Systems (CES).
REFERENCES
[1] Zigbee Specification, version 1.0, http://www.zigbee.org.
[2] S. Lee, S. Banerjee, and B. Bhattacharjee, The Case for a Multihop Wireless Local Area Network, Proc. IEEE INFOCOM, vol. 2, pp: 941-951, Mar.
2004.

[3] Bluetooth Specification, http://www.bluetooth.com.
[4] Multi-hop Mesh Network, http://www.intel.com/technology/comms/cnO2032.htm.
[5] P. Krishnamurthy, J Kabara, and T. Anusas-amornkul, Security in Wireless Residential Network, IEEE Transactions on Consumer Electronics,
Feb 2002.
[6] T.D. Taman et al., Algorithm-agile encryption in ATM networks, IEEE
Computer, pp. 57-64, Sep 1998.

Coverage-Based Testing on Embedded Systems
X. Wu^, J. Jenny Li*, D. Weiss* and Y. Lee^
*Avaya Labs Research, 233 Mt. Airy Rd., Basking Ridge, NJ 07920
{jjli, weiss}@research.avayalabs.com
^Department of Computer Science and Engineering, Arizona State University,
699 South Mill Avenue Tempe, AZ 85281
{xwu22, yhlee}@asu.edu
Abstract
Program

One major issue of code coverage testing is the
overhead imposed by program instrumentation, which
inserts probes into the program to monitor its
execution. In real-time systems, the overhead may alter
the program execution behavior or impact its
performance due to its strict requirement on timing.
Coverage testing is even harder on embedded systems
because of their critical and limited memory and CPU
resources. This paper describes a case study of a
coverage-based testing method for embedded system
software focusing on minimizing instrumentation
overhead. We ported a code coverage-based test tool
to an in-house embedded system, IP phone. In our
initial experiments, we found that this tool didn’t affect
the behavior of the program under test.

1. Introduction
Code coverage testing technology attempts to
quantify progress of program testing. Some academic
study on small programs showed that software
reliability improves as testing coverage increases [1].
Even though large industrial trials and theoretical study
of correlation between coverage and defect detection
are still required, intuitively, low testing coverage is
not acceptable. No developer would deliver software
with only single digit testing coverage.
We developed an automatic coverage testing tool
suite to facilitate the usage of coverage testing.
eXVantage, short for eXtreme Visual-Aid Novel
Testing and Generation, which aims to help developer
to improve testing productivity. It also helps project
managers in keeping track of each developer’s
progress. Figure 1 shows the workflow of our coverage
testing tool suite.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

Developer

Coverage
report

Automatic
Code
Inspection

Y

Analysis
Issues?
N

Program
Instrumentation

Recompile

Program
Structure
Report

Execution
Trace

Testing
Results

Automatic
Run Tests

Existing tests

Figure 1. Workflow of Automatic Unit Testing
Framework
The tool suite includes four major components: 1)
automatic code inspector, 2) program instrumentation
module which also includes program structure
recovery function, 3) automatic test execution
platform, and 4) analysis module. Each component
corresponds to one tool in the accompanying tool suite.
The component shown in the dashed box, recompile,
may not be necessary if the bytecode or object code is
used in the program structure analysis and

instrumentation. It is not part of the tool suite
component because it uses the original program
compiler without any modification or using any new
tool.
The entire framework can be invoked by a script
including five steps: code inspection, instrumentation,
compiling, test execution and analysis. The input to the
script is the directory of the programs to be tested and
analyzed and the output is testing coverage reports.
The workflow works as follows:
1)
Each program must first go through an
automatic code inspector using our stylechecker tool. If issues or problems are
discovered by the checker, they are
highlighted on the program and should be
fixed by developers immediately.
2)
After the program is clean of static problems,
it must go through a program instrumenter
that adds hooks to the program for monitoring
its execution. In the meantime, the program
structure can be recovered from the program.
3)
After the program is instrumented, the
program can then be recompiled (if the
program is the source code) or directly sent to
test execution (if the program is bytecode or
object code). We use a pattern matching
approach to select part or all of the test cases
for execution.
4)
Testing results and program structures are
used by the analysis module to generate
various reports. One typical report is program
testing code coverage report, which helps user
keep track of testing progress. If some parts of
code are not covered, a priority report will be
generated to show which part of code is to be
tested next.
The reports generated by the framework are stored
in a data base. Additional tool is included to depict
historic trend on program changes and code coverage.
Such information is useful for project planning and
process control.
One important issue of coverage testing is the
overhead of monitoring program execution, in
particular, for real-time embedded systems. The probes
inserted to the original program at step 2) can cause
test execution overhead at step 3). The step 3) is the
only step that is platform dependent. Our work on
porting the tool suite to embedded systems focuses on
step 3) in the tool suite’s workflow.
The overhead comes from two sources: CPU
requirement for probe execution and memory
requirement for storing execution traces. The focus of
this paper is on the second issue. Details of the
problem and its solutions will be described later.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

The rest of the paper is organized as follows.
Section two defines in details the porting requirements
and memory challenges. Section three describes our
solution to the problem. Section four presents our
experimental results and section five concludes that
our solution enables embedded system code coverage
testing.

2. Problem description
The target system of our case study is an embedded
system, with MIPS architecture. Due to the real time
feature of the software, porting a code coverage testing
tool to such systems requires to minimize the overhead
caused by the code coverage instrumented probes and
monitored trace data storage and transmission.
The target system is based on the VxWorks Real
Time Operating System (RTOS). VxWorks is one of
the most successful real time operating systems, which
provides multitasking, intertask communication, and
hardware interrupts handling as the three most
important features.
VxWorks has very good performance in terms of
both timing and space. Its raw context switch time
could be as low as 4 micro seconds, and the VxWorks
image we used for this case study has a size of about 4
KB.
Another issue needs to be considered in porting to
an embedded system is the memory mechanism. In the
original version of eXVantage, shared memory is the
way for trace memory buffer maintenance. The
VxWorks image in the target system does not have the
shared memory component included. Furthermore,
VxWorks itself uses a flat memory model, which
means every task could access to every single valid
memory address without any boundary, including
concurrent tasks.
eXVantage was originally developed for Linux
platform, and each process has a pointer stored in the
shared memory pointing to another piece of memory
allocated before the program starts. In this process
specific memory, the header starts with 9 bytes,
including information like process ID, buffer size, and
key value (which will be used during instrumentation).
The rest of the memory is the place where trace data
are temporarily stored for later retrieval through
network connections.
Figure 2 illustrates this memory issue in a diagram.
It shows that each process has its own block of shared
memory. This block of shared memory doesn’t need to
be shared among the original processes. But it needs to
be shared with the new eXVantage process that
handles trace transmission. Since the trace data
memories of various original processes are not shared

among each other, there is no need for a memory
locking mechanism.

Figure 2. Share memory issue of trace data
The eXVantage VxWorks run time system is
composed of three parts: PC Host, the embedded
system, and Ethernet connections. Tornado, the
VxWorks Integrated Development Environment (IDE)
runs on the PC Host, The embedded software
application program runs on the embedded system. The
PC Host and the embedded system are connected via a
serial link. Both of them also have Ethernet
connections, as shown in Figure 3.

Figure 3. The hardware setting
The program that runs on the embedded system the telephone in this system - could be divided into
three layers. The first layer contains user-defined
application tasks APP1 – APPn, VxWorks system tasks,
with the addition of a task, XV_SERVER, created by
eXVantage for memory setting and trace data
transmission; the second layer is the VxWorks library;
and the third layer is the hardware support, which
mainly includes I/O Driver and Board Support Package
(BSP). The addition of this new XV_SERVER task is
part of our solutions that will be explained in the next
section.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

3. Solutions
Our solution to reducing overhead includes several
mechanisms. First we only record the coverage
information instead of the full execution trace, thus the
number of probes and their CPU time requirements are
reduced; second each time a probe is executed we store
data in memory rather than directly writing it into files.
By this way the CPU time requirement for writing file
is eliminated; third we use binary data format for the
trace information other than readable characters so that
the memory requirement is greatly reduced.
Taken VxWorks shared memory issue into account,
we pick up the API “sysMemTop()” which returns the
first byte of memory that is neither used by the system
nor by the user. The trace memory starts from this
point to obtain blocks of memory for each original
process. Since only count information of coverage
execution is recorded, no execution sequence is
required and the total size of the memory can be predetermined. Figure 4 shows the selection of this
memory beginning point. The other arrangement is
kept the same as in Figure 2.
eXVantage runtime solution is divided into two
layers from the software point of view. The upper layer
contains all application tasks plus XV_SERVER task;
the lower layer is composed of all related modules of
eXVantage that provide supports to the upper layer
1) Application Tasks
Application tasks are created by the users and are
instrumented by eXVantage before execution. Such
instrumentation is mainly supported by the
instrumenter module of eXVantage.
2) XV_SERVER Task
After instrumentation, original application tasks are
logically organized into various connected nodes.
When any of the probes on each node is executed, a
piece of information will be written into the
corresponding buffer created and maintained by the
XV_SERVER task.
XV_SERVER task’s execution includes the
following functions:
a) Setting up the three types of buffers supported
in eXVantage, coverage (bit), profiling
(counting) and tracing(ordered); and
b) Configuring network communication in order
to send traces to trace handler program
periodically or reply to its requests.
When the system boots up, XV_SERVER task is
assigned priority 100. Periodic transmission will be
carried out every 5 seconds to send out execution
trace information.

Figure 5. eXVantage VxWorks Run Time Solution
Figure 4. Solution to shared memory issue
Currently only bit buffer has been implemented,
counting and ordered buffer will be added in the near
future; and the ordered buffer will have to be dumped
based on their used buffer size, otherwise there would
be data loss.
Dynamic transmission will be carried out upon the
receiving of requests from a remote trace handler,
which is also going to be implemented later.
The following is a more detailed description of
eXVantage run time software components and its
workflow. Figure 5 shows the workflow of eXVantage
run time solution on embedded systems which includes
four components: APPn, XV_SERVER task, Trace
Handler, and Buffer.
The following is the detailed description of the four
components.
1) APPn
APPn is the original program under test. They were
instrumented with probes that write into the
memory buffer during the execution. Every APPn
task writes execution information directly into its
own buffer when a “logic” node gets executed.
2) Buffer
The buffer temporarily stores the program execution
information. There are three types of buffer
supported by eXVantage: Bit Buffer, Counting
Buffer, and Ordered Buffer.
Bit Buffer shows if the corresponding node is
executed or not; Counting Buffer records how many
times the node has been executed; and the Ordered
Buffer saves the node execution sequence of the
whole application program. For code coverage
testing, only the first one is needed. The latter two
kinds of information have some other usage such as
compiler optimization.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

3) XV_SERVER task
XV_SERVER sends bit and counting buffer data to
Trace Handler on a remote machine every 5
seconds; sends ordered buffer data to the same place
based on its dynamic in-use buffer size, in another
word, the size of data in the buffer. Both of the two
sending operations have to guarantee no
interference to the application tasks’ execution.
In the meantime, the XV_SERVER task also
handles requests sent from the Trace Handler and
responses to them with the trace data as requested
4) Trace Handler
Trace Handler resides on a remote machine. It sends
requests to XV_SERVER task for retrieving trace
data dynamically. It also receives trace data from
XV_SERVER task passively every certain period of
time. Received trace data will be written into trace
files for further analysis as shown in Figure 1.

4. Experiments
By using a timer with resolution of 1/3600, which is
0.278 milliseconds, we obtained the time cost on
several measurements as listed below (Table 1). These
measurements show the cost of instrumentation (probe
time) and XV_SERVER task (buffer initialization and
transmission).
Several other measurements have also been
proposed and the work is in progress:
1) When combining the above time cost values
with the eXVantage overhead on the regular
non-real-time system (<1%), we have
confidence that eXVantage on the embedded
system with VxWorks RTOS could also
achieve an overhead that is at least as low as
1%. However, some more experiments are
needed to prove this.

Measurement

Time cost

Standard
deviation

Trace buffer
initialization

619.80
nanoseconds

0.30
nanoseconds

Execution time
per trace data
send

788.07
nanoseconds

1.79
nanoseconds

Execution time
per probe

115.91
nanoseconds

1.10
nanoseconds

Table 1. The overhead of instrumentation
2) Tornado provides a CPU time measurement
tool as one of its components, which could
show the CPU time spent on each of the tasks
during a specific execution. We are going to
use this tool to measure the task’s execution
time of both un- instrumented and instrumented
code, and apply statistics theory to calculate its
overhead.
3) Another way to check if our instrumentation
affects the original program is to get the
execution sequence of the program’s both uninstrumented and instrumented version, and
compare with each other. This work could be
done by another Tornado tool “WindView”.
WindView is also an instrumentation tool
which instruments VxWorks API in order to
get the execution sequence.
Initial results on the existing test cases show that the
code coverage instrumentation didn’t alter the behavior
of the original program.

5. Related work
As a method of effectively checking the quality of a
software test, also quantitatively monitoring the
progress of testing, code coverage testing has been an
active research topic for more than 10 years. M.R.Lyu
et al 0 developed a coverage analysis tool named
ATAC (Automatic Test Analysis for C), which could
evaluate test set completeness based on data flow
coverage measures and allows programmers to create
new tests intended to improve coverage by examining
code not covered. But ATAC didn’t address embedded
system’s characteristics. W.E.Wong et al [2] proposed
another solution, TestingEmbedded, on top of Χ
Suds/ATAC that works for the embedded system
environment, but up to now it is restricted to
Symbian/OMAP platform only. Y.Y.Cho et al 0

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

suggested a performance evaluation system for
embedded software that it consists of code analyzer,
testing agents, data analyzer, and report viewer; code
coverage analysis is performed by the code analyzer;
report is generated from data analyzer showing the
analysis results and displayed by the report viewer.
However, they didn’t discuss how much overhead
could be caused by this system in the original
embedded software. H. Thane in his doctoral thesis 0
also discussed about the code coverage technique in
the testing phase of distributed real-time system
development, defining the complete coverage for both
single node and the whole distributed system; but
similar to 0, he didn’t take the instrumentation
overhead into consideration, which is quite an
important issue for embedded systems’ timing
behavior. In our coverage based testing technique, we
overcome the drawbacks listed above by designing the
testing environment specifically for our embedded
system
platform,
minimizing
instrumentation
overhead, and applying basic performance analysis to
verify the test overhead on the original embedded
software.

6. Conclusion
We have successfully ported a code coverage
testing tool, eXVantage, from Linux platform to an
embedded system with VxWorks RTOS. The resulted
code coverage testing runtime solution passed basic
test cases we have at hand. Currently it had been
handed to our customer for field usage.
We have also carried out some basic measurements,
of which the experimental result is pretty good.
According to the measurements after our
implementation, our solutions together could provide
us with a good enough performance for embedded
system code coverage testing. When the required
resources become available, more detailed and
comprehensive measurements will be applied and
necessary changes could be made according to the new
results.
Several future works could be valuable to consider:
1) Addition of the two other kinds of buffers,
counting and ordered buffers, which will
require more strict server task and application
tasks synchronization to prevent data loss.
2) Applying eXVantage to other software systems.
We have implemented and tested our porting
on one embedded system. More trials of the
porting are being carried out.
3) Implementing a real time system logger. The
logger works at the field during program
execution to record necessary runtime

information. The information is stored in a
remote trace file. With this trace file, exactly
the same execution can be “replayed” as many
times as users need. This can be quite helpful
for debugging and remote diagnosis.
4) Product line development
a) Language Support
Currently we have already supported
JAVA and C/C++, when there is other
programming language come out in the
future, we could also try to include it in our
eXVantage language support.
b) Platform Support
Up to now eXVantage works well on both
Linux and VxWorks. Next step we may
port it to other platforms, so that
eXVantage can be applied to a much wider
domain of applications
Besides code coverage testing, the users can also
leverage the coverage data collected from the run-time
environment to help in debugging. It can also be used
to profile the execution count to find system
performance stress points. It can also locate bugs by
finding the common parts of the failed test cases and
subtracting the parts in successful test cases. Overall,
porting code coverage testing tool to embedded
systems makes the tool’s related features usable for

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

embedded systems, thus helping improve the quality of
embedded software.

7. References
[1] Michael R. Lyu, J.R Horgan, Saul London, “A coverage
analysis tool for the effectiveness of software testing”, IEEE
Transactions on Reliability, Volume 43, Issue 4, Dec. 1994,
pp. 527-535
[2] W. Eric Wong, S. Rao, J. Linn, and J. Overturf,
"Coverage Testing Embedded Software on Symbian/OMAP"

in Proceedings of the 18th International Conference on
Software Engineering and Knowledge Engineering, San
Francisco, July 2006.
[3] Yong-Yoon Cho, Jong-Bae Moon, and Young-Chul Kim,
“A system for Performance Evaluation of Embedded
Software”, in Transactions on Engineering, Computing and
Technology V1, ISSN 1305-5313, December 2004
[4] Henrik Thane, “Monitoring, Testing and Debugging of
Distributed Real-Time Systems”, Doctoral Thesis, Royal
Institute of Technology, KTH, Mechatronics Laboratory,
TRITA-MMK 2000:16, Sweden, 2000
[5] Barry W. Boehm, Software Engineering Economics,

Prentice Hall Ptr, October 22, 1981

DC2 Scheduling for Aperiodic Tasks in Strongly Partitioned Real-Time
Systems
Daeyoung Kim

Yann-Hang Lee

Real Time Systems Research Laboratory
CISE Department, University of Florida
P.O. Box 116120
Gainesville, FL 32611-6120,USA
dkim@cise.ufl.edu

Dept. of Computer Science and Engineering
Arizona State University

P.O. Box 875406
Tempe, AZ 85287-5406, USA
yhlee@asu.edu
the Integrated Modular Avionics (IMA) systems
according to ARINC standards [ 11.
To support temporal partitioning in SP-RTS
architecture, a two-level hierarchical scheduling is
needed. At the lower level, multiple partitions
(applications) are scheduled according to a cyclic
assignment of execution slots. Thus, a minimum amount
of processing time is assigned to each partition
periodically. On the other hand, at the higher level, each
partition schedules the application tasks based on a fixed
priority algorithm. To achieve scheduling independence in
run time between periodic tasks and their partitions, a
cyclic scheduler must guarantee distance constraint
characteristics [2] of all partitions. Provided distance
constraint in cyclic schedule, periodic task will be
guaranteed to meet its deadline regardless of its arrival
time.
To guarantee the schedulability of periodic tasks in
each partition, a sufficient processing time must be
assigned to the partition. In [3,4], we reported the
partition scheduling condition for meeting task deadlines.
The condition is formulated as a continuous function of
invocation,period (h) and processing capacity (a)for each
partition using the characteristic of level-i inactivity of all
tasks in the partition. In other words, a minimal
processing capacity a must be allocated to a partition if it
is activated once every period h. Using these feasible sets
of all partitions, we can then find a cyclic schedule that
satisfies the distance constraint h and the capacity
constraint a.
In this paper, we focus on designing algorithms for
dynamic scheduling of soft and hard aperiodic tasks in
SP-RTS environment. We assume that there is an
aperiodic task server in each partition. So an aperiodic
task is served by the aperiodic task server which belongs
to the same partition as the aperiodic task. To ensure the
schedulability of the periodic tasks, the server would not
consume any processing capacity allocated to the periodic
tasks in the partition. Nevertheless, there exists available

Abstract
The strongly partitioned real-time system (SP-RTS)
provides temporal and spatial partitioning for its
integrated real-time applications. The SP-RTS
architecture adopts a two-level hierarchical scheduling
mechanism. In the lower level, multiple partitions
(applications) are dispatched under a cyclic scheduling,
whereas, in the higher level, multiple periodic tasks of
each partition are scheduled according to a fixed priority
algorithm. In this paper we focus on designing algorithms
for on-line scheduling of aperiodic tasks in the SP-RTS
environment. The Distance Constraint guaranteed
Dynamic Cyclic (DC’) scheduler that is proposed uses
three basic operations: Left Sliding (LS), Right Putting
( R P ) , and Compacting. These operations dynamically
schedule aperiodic tasks within a distance-constrained
cyclic schedule. We show that the dynamic applications of
these operations do not violate the distance constraint
characteristics of a partition cyclic schedule. In addition,
the slack time calculation of these dynamic operations
can be applied for scheduling hard aperiodic tasks. With
the simulation studies, we observed that the DC’ can
result in a significant performance enhancement in terms
of the average response time of soft aperiodic tasks and
the acceptance rate for hard aperiodic tasks.

1. Introduction
In an integrated real-time system, applications of
different criticality may share the same computing
resources. To protect from potential interference among
applications, the system must provide spatial separation
such that the applications allocated in one partition has a
controlled access to the memory and IO space of the other
partitions. In addition, the system should support temporal
separation such that the execution time assigned to a
partition cannot be changed due to an overrun of other
applications. Such kind of SP-RTS has been adopted for

368
1530-1427/00 $10.00 02000 IEEE

2. System Model

processing capacity such as unused capacity at the
partition level that can be allocated to the aperiodic task
servers. Also there exists pre-allocated processing
capacity for aperiodic tasks, which can be shared by
partitions. Our objective is then to utilize the available
capacity to maximize the performance of aperiodic tasks
while meeting the distance and capacity constraints for
each partition.
There have been substantial research works in the
scheduling of aperiodic tasks in fixed priority systems.
Examples include the sporadic server algorithm [ 5 ] , the
deferrable server algorithm [6], and the slack stealer
algorithm 171. To serve aperiodic tasks in SP-RTS, we
may use one of the above aperiodic servers as a dedicated
aperiodic server of each partition. But, it is not a good
approach because an aperiodic server cannot claim the
spare capacities earned by the aperiodic servers in other
partitions. A number of algorithms that solve the
aperidoic task scheduling in dynamic priority systems
using EDF scheduler can be found in the literature [8].
The Total Bandwidth Server assigns feasible priority to
arrived aperiodic task while guaranteeing deadlines of
other periodic tasks. These solutions cannot be applied to
cyclic scheduling which requires distance constraint. We
can find useful concepts in Shen’s resource reclaiming
[9], Fohler’s slot shifting algorithm[lO], and J. Liu’s
aperiodic task scheduling in cyclic schedule[11).
However, their algorithms are also not applicable to SPRTS because distance constraint is not guaranteed.
To maintain the distance constraints, we propose three
basic operations in our Distance Constraint guaranteed
Dynamic Cyclic (DC’) scheduler for soft and hard
aperiodic tasks. The goal of these operations is to obtain a
maximal amount of slack time upon the arrival of an
aperiodic task. The f i s t operation, left-sliding, is to save
any slack time by scheduling other partitions earlier when
there is no pending task in the current partition. The
second operation, right-putting , steals slack time as much
as possible by swapping the cyclic schedule for partitions
while guaranteeing distance constraint. Since rightputting operation may divide the cyclic schedule into
small segments, the third operation, compacting, is
designed to merge the segments allocated to the same
partition together.
The rest of the paper is structured as follows. We
discuss the system model in section 2. In section 3, we
present DC2 scheduler and its three dynamic operations.
In section 4, we describe acceptance test algorithm for
scheduling hard aperiodic tasks. In section 5, we show
several simulation results about the acceptance testing of
hard aperiodic tasks and the average response time of soft
aperiodic tasks. A short conclusion is then followed in
Section 6.

2.1. Strongly Partitioned Real-Time Systems
In safety-critical complex real-time systems like IMA
system, it needs a model that can support the integration
of multiple, different critical levels of real-time
applications in sharable computing environment. To
guarantee separate timing constraints and fault tolerance
of each application, we use the concepts of spatial and
temporal partitioning. We refer the model as SP-RTS. The
SP-RTS has partitions (applications) and channels
(communications) as its fundamental scheduling entities.
The abstract model of SP-RTS is shown in Figure 1.
A SP-RTS uses a two-level hierarchical scheduler for
both processor and communication scheduling. At the
higher level, partition servers and channel servers,
schedule their tasks and messages according to fixedpriority based algorithms. On the other hand, at the lower
level, partitions and channels are assigned with processing
time and bandwidth in a cyclic manner, respectively. In
order to meet the deadlines of periodic tasks within each
partition, the processing capacity dedicated to a partition
is then dependent on how frequently it is served in the
cyclic schedule. In [3,4], we have investigated the
constraints on the allocated capacity (a)and the
invocation period ( h ) and been able to construct a
monotonic function between a and h for any set of
periodic tasks under either rate- or deadline-monotonic
scheduling algorithms.
Node n

Node 1

...
5
Cyclic Bus Scheduler

Figure 1.The Architecture Model of SP-RTS
Then, similar to pinwheel scheduling, we can compose
a distance-constrainedcyclic schedule for all partitions in
a SP-RTS. To achieve scheduling independence in run
time between periodic tasks and their partitions, it is
essential to provide distance constraint characteristic
within cyclic schedule. Provided distance constraints in
cyclic schedule, a periodic task will be guaranteed to meet
its deadline regardless of its arrival time. Following this
approach, for a system of n ordered partitions PI,,,.,,,
each
partition server, Pi,will be given a processor capacity of

369

soft aperiodic tasks and a higher acceptance rate for hard
aperiodic tasks, we should look for ways to adjust the
table entries dynamically.

a, in any time intervals of the period of h, to process its
periodic tasks, where C,"=,ai
5 1and h, I (divides) hJ for i
<j .

2.2. Scheduling Aperiodic Tasks in SP-RTS
Let's assume that there are infinite number of
aperiodic tasks, {J, I i=OJ ,2,...}. Each aperiodic task has
associated with a worst case execution time of c,.
To maximize the performance for aperiodic tasks, we
are concemed with any available capacity that we can
extract from the low-level cyclic schedule of a SP-RTS.
As long as the distance and capacity constraints are
satisfied for each partition, this available capacity can
then be assigned to any aperiodic tasks. We introduce a
Multi-Periods Aperiodic Server (MPAS) which is a
virtual executive server for aperiodic tasks. When it is
activated, an aperiodic task server at one of the partitions
can use the capacity to execute existing aperiodic tasks.
A MPAS can be constructed using the following
scheme. By removing the duplicate periods in the set of
h,, 1 I i In, and sorting the remaining set in increasing
order, we can obtain the non-duplicative set MPS = {HI,
H2. ..., H J where m I n and H, 1 HJ for i < j. We then
define a set of MPASH,servers, where 15 i Im, that share
the remaining idle processor capacity, (1 ),

Figure 2. Example of Feasible Cyclic Schedule

3. Distance Constraint guaranteed Dynamic
Cyclic (DC") Scheduling
To dynamically schedule aperiodic tasks in a distance
constrained cyclic schedule, we propose to use three basic
operations to manipulate the table entries. These are LS
(Left Sliding), RP (Right Putting) and Compacting
operations. The operations are applied to feasible cyclic
schedules, such as shown in Figure 2, in run time when an
aperiodic task arrives or a server finishes earlier than
scheduled. In this section, we specify the details of each
operation and show that the deadlines of periodic tasks in
each partition will not be missed when the operations are
applied. Using these basic operations, we can then
compose a distance constraint guaranteed dynamic cyclic
(DC') scheduling algorithm.

xp,

evenly, i.e., we assign processor capacity
(1 1/ m ,to each MPASH,.

z;p,

CPH,

3.1. Left Sliding (LS)

=

The LS(x) operation slides the current phase of cyclic
schedule left by x units of time in O(1) time. There are
two situations at which LS(x) will be applied. One is when
Then, the total set of servers in the low-level cyclic
the current partition server or MPAS has finished x units
schedule, including partition servers P and multi-period
of time earlier than the allocated capacity. The other case
aperiodic servers MPAS, can be formulated as follows:
is when the scheduler invokes the partition server or
CS = P uMPAS = CSI [(cPHI.HI),(a1,hi),
(azhz),
MPAS with an allocated capacity of x but f i d s no
... . ( ~ E I ~ E I ) ] ucs2 [ (CPH2fl2)~
(~El+l~hEI+l)~(~El+2~hE1+2)~
pending periodic task in the partition or aperiodic task in
. . . ~ ( a E 2 h E 2 ) ] u.-,ucsm[
( C P H m f l m ) . (aEm-l+lhEm-l+I)J
any partition. Figure 3 shows the example of a LS(1)
(aEm.l+Z,hEm-1+2),
...,(aEm,hEm)],
where all server periods in
operation.
the subset CS, are same and E1 < E2< . . .<E,.
For instance, if we have input partition servers of P =
{P1(117,7), P2(117,14), P3(117,14), P4(1/7,28)}, we can
obtain set of MPAS as {MPAS,(II7,7), MPASl4(II7,14),
MPASz8(II7,28)}. Finally we can obtain cyclic schedule
set CS of {MPAS7(1/7, 7 ) , P1(1/7, 7 ) , MPAS14(117,I4),
Pz(II7, 14), Pj(117, 14), MPAS2,(1I7, 28), Pd(lI7, 2811.
Using the distance constrained cyclic scheduling
Figure 3. Modified Schedule after LS
algorithm proposed in [4], we can obtain the feasible
cyclic schedule shown in Figure 2. Since it allocates the
To reduce the average response time of aperiodic tasks,
MPAS with the minimum period fist, for example,
we can use LS to eliminate idle processing during
MPAS7(1I7, 7 ) , it is guaranteed that every partition server
assigned slots. The advantage against a simple polling
does not cross minor frame boundary. This characteristic
aperiodic server algorithm comes from its dynamic
can lead to a flexible relocation of cyclic schedule entries.
behavior. In a polling aperiodic server algorithm, when an
Note that, in the Figure, we use AS as an abbreviation of
aperiodic task arrives, the task has to wait until the next
MPAS. Given the cyclic schedule of CS, the straightMPAS interval. Using LS, the next MPAS might come
forward approach of scheduling aperiodic tasks is to give
earlier if there are idle periods in a partition. Since both
CPU control to aperiodic tasks when current timeline
partition server and MPAS can begin their execution
belongs to MPAS. However, for a better response time for
370

earlier than the instances defined statically in the cyclic
table, a better average response time should be expected.
In addition, the following theorem shows the distance and
capacity constraints for periodic tasks in all partitions are
still valid under the LS operations.
Theorem 1. The modified cyclic schedule after
application of LS(x) is feasible, i.e., it guarantees the hard
deadlines of periodic tasks in all partition servers.
Proof: We assume that a LS(x) operation takes place at
time @.Thus, we need to show that every periodic task
that has already arrived before @ or will arrive after @ can
meet its deadline. First we consider the tasks which will
arrive after time @.
If the LS operation doesn’t occur, then,
there will be a processor capacity of a; allocated to P, in
every periods [&S, @&h,], where S 2 0. After the
execution of LS(x), the processing capacity allocated
during [@d @&hJ under the modified schedule is
equivalent to that allocated during [&&x, @+&x+hJ in
the original schedule. As a consequence, there will be
exactly the required processor capacity for all partition
servers in any periods after time 4. New periodic tasks
will be able to meet their deadlines. Second we need to
show that the periodic tasks that have already been
released before time @ shall meet their deadlines. For the
tasks that do not belong to the suspended partition, they
will be given their preserved processor capacity earlier
than the instance originally scheduled so that they can
meet their deadlines. For the tasks that belong to the
current partition, they have already finished before time q4
because this partition becomes idle at @ in spite of its
unused remaining capacity. Therefore we can conclude
that the modified cyclic schedule after LS(x) is feasible. I

capacity from a MPAS of the same period that lies
between the current time instance and PLi. If no such
MPAS exists, the operation would not take place at all.
Note that this capacity exchange leads to a revision of
table entries in the cyclic schedule. This implies that the
exchange becomes persistent for all future invocations of
the partition server and the MPAS. Since Rp operation
should track all time interval entries in the cyclic
schedule, its time complexity is O(N), where N is the total
number of time table entries. In Figure 4, we show an
example of an RP operation assuming that an aperiodic
task arrives at time 0. It exchanges the current partition
server PI with MPAS, which is scheduled between [O,
PLI]. This RP operation results in an immediate execution
of the aperiodic task without any violation of distance
constraints. However, the RP operation may split a
scheduled interval into segments. This will increase the
number of allocation entries in the cyclic schedule table
and the number of context switches between partitions. To
reduce the problem of segmentation, a compaction
algorithm can be applied.

Figure 4. Modified Schedule after RP
Theorem 2. The modified cyclic schedule after the
application of RP operations is feasible, i.e., it guarantees
to meet the hard deadlines of periodic tasks in all partition
servers.
Proof: We assume that an RP event occurred at time @
and the invocation of the current partition server P, is
postponed. First, we consider a periodic task that arrives
after time @.If the task does not belong to P I , it can
always meet the deadline. This is because that the
schedules for all partition servers except PI are not
changed after the RP. For P, there was exactly a, capacity
in every period of h, before the RP operation. Assume that
the RP exchanges y units of time for P, from [I$, &y] to
&&y], where a i s a real number. Due to &&y I
PL, = PIST, + h, I@ + h,, there exists a capacity a,
allocated to P, in [@, @hJ. We can do the same exchange
in all iterative periods after @ given that the corresponding
partition server and MPAS are in the same relative
position in the cyclic schedule. Therefore, for every
period h, of [@, m], i.e. [&S, @&hJ, where S 2 0, a
capacity of a, exists for PI. Second, we consider a periodic
task that had arrived before time @. If the task belongs to
P,, where j&’, it will meet the deadline because the
schedule for P, is still unchanged after the RP operation.

3.2. Right Putting (RP)
The Right Putting (RP) operation exchanges the
remaining allocation of the current partition server with a
future MPAS of the same period. In order to meet the
deadlines of periodic tasks and to satisfy the distance
constraint in cyclic schedule after RP, a RP operation is
done based on two parameters: PIST, (Partition Idle Start
Time) and PL, (Putting Limit) for partition server P,. PIST,
is the instance at which the partition becomes idle due to
either no pending task or an early completion of tasks. PL,
is defined as the sum of PIST, and the invocation period h,
of P,. Initially, PIST, is set to zero. According to the
execution of tasks, PIST, will be reset to the instances
when P, is invoked and find no waiting periodic tasks.
Obviously, PIST, is always less than or equal to the
current instance.
An RP operation is initiated when either an aperiodic
task arrives or there is still a pending aperiodic task after
the completion of a partition server. To reschedule the
remaining capacity of the current partition server at a later
interval, the operation brings in an equivalent processing

[&a

371

when the number of context switching exceeds a certain
threshold at the beginning of a major frame. Following
Theorem 1, 2, and 3, we can conclude that DC2 scheduler
guarantees the deadlines of hard periodic tasks in all
partition servers while serving aperiodic tasks. In Figure
5 , we present the abstract algorithm of DC2 scheduler.
The resultant scheduler may lead to an immediate
execution of an arriving aperiodic task and it does not
need to wait for the beginning of the next aperiodic server
interval. Practically the DC2 scheduler should be carefully
designed so that the scheduling overhead is minimized.
For example, we can limit the number of burst arrivals of
aperiodic tasks for a short interval time.

For a task that belongs to PI, we have to show that there
should be capacity a, in any period [PIST,+&,
PIST,+&+hJ where 0 < E < &PIST,. We don’t need to
consider any periodic task that had arrived before PIST,.
Note that there was exactly a, processor capacity in the
period of [PIST,+E, PIST,+E+ hJ before the operation,
where 0 < E < @-PIST;.After an exchange of the capacity
y , which is less than a,, in the period [@, PLJ, there still
exists the same amount of capacity a; in the concemed
interval [PIST,+&, PIST,+&+hJ since PL, I PIST,+&+h,
and [@,PLJC[PIST,+E,PIST,+&+hJ for any 0 < E < &
PIST,. Therefore, the partition server P, will be allocated a
requested processor capacity in every invocation period.
I

loop (
receive a Timer interrupt;
S = getNextServer: // S will be either partition server or MPAS
while ( S doesn’t have any pending task) (
LSClSlJ;
S = RetNextServer;

3.3. Compacting
To reduce the problem of segmentation due to the RP
operations, we propose a Compacting operation. The
operation delays a MAPS entry and exchanges it with the
entry of a future partition server. This contrasts with the
RP operation that first postpones a partition server and
then brings and starts a MAPS immediately. A heuristic
selection is done in a Compacting operation such that the
neighboring entries that belong to the same server can
then be merged in the scheduling table. Similar to RP
operation, the complexity of Compacting is also O(N).
The operation can be applied periodically. For instance
this can be done every several major frames, or at the
time of completion of a certain number of RP operations,
or on the event that the number of context switches
exceed a certain threshold.
Theorem 3. The modified cyclic schedule after the
application of Compacting operation is feasible, i.e., it
guarantees to meet the hard deadlines of periodic tasks in
all partition servers.
Proof: The proof is trivial. Since Compacting
operation uses the similar approach as RP operation for
exchanging scheduling entries, the proof is the same as
RP operation. It also can preserve the distance constraints
guaranteed in the original cyclic schedule after the
adjacent scheduling entries that belong to the same server
in the same minor frame are merged. I

I

Set Timer to current time + IS[;
dispatch S:
sleep;

// IS\ is allocated capacity

I

signals:
Case (S finishes earlier by x units of time) do LS(x):
Case (aperiodic task arrives) do R P and dispatch MPAS if available:

Figure 5. DC2 scheduling algorithm

4. Hard Aperiodic Task Scheduling in DC?
Algorithm
When a hard aperiodic task arrives, the scheduler must
invoke an acceptance test algorithm. Basically, the
algorithm checks the available slack time between the
current time 9 and the task deadline @Of.An acceptance
can only be made if the available slack time is bigger than
the task’s WCET. Thus, the task’s timing requirement can
be met and none of the hard aperiodic tasks admitted
previously may miss their deadlines. With the help of
dynamic behavior of DC2 scheduler, we can take
advantage of future RP operations in addition to the slack
times of current static schedule.

4.1. Low-bound Slack Time

3.4. D e Scheduler

With the current snapshot of cyclic schedule as of
current time 9, the scheduler can obtain a low-bound slack
time between 9 and @+Dlthat exists regardless any future
LS and RP operations. Note that the further slack time can
be added in due to the results of LS and RP operations
before WO,.
To aid the computation of low-bound slack time, we
use three parameters, sf, sl and sr. The sJ; equals to the
sum of processing capacities allocated to all MAPS’S in a
minor frame i and represents the available slack times in
minor frame i. The slk and srk stores the available slack

Based on the three operations LS, RP, and Compacting,
we propose a DC2 scheduler that accommodates aperiodic
tasks within distance constrained cyclic scheduling of
partition servers. When there is no pending aperiodic task,
it behaves like a cyclic scheduler except for the
intervention of LS operations. Whenever a server goes to
idle without consuming all its allocated capacity, LS
operation is invoked. Hoping to use saved slack times, the
scheduler can apply a RP operation when an aperiodic
task arrives. The scheduler invokes Compacting operation

372

time before and after the schedule entry k in a minor
frame. In Figure 6, we show an example that illustrates
the values of these parameters.

EDF scheduling policy. In the decision process, we assign
two parameters, remaining execution time (rc) and
individual slack time(s), for each existing hard aperiodic
task. Using these two parameters and the requirements of
the arriving task, the acceptance test algorithm, as shown
in Figure 7, first checks whether total slack time,
totalslack, as of the current time is sufficient to
accommodate the existing hard aperiodic tasks and newly
arrived one or not. If the total slack time is not sufficient,
it rejects new one. Then it checks the schedulability of
existing tasks. Since the hard aperiodic task whose
deadline is earlier than the new one, its schedulability will
not be affected by the admission of the new task due to
the EDF scheduling policy. However, we should consider
the ones whose deadlines are later than the deadline of the
arriving task, and check the total slack time, sk, for each of
them.

Figure 6. An Example of sf. SI and sr Parameters
The value pair of sl and sr is shown in each scheduled
interval. The scheduled interval for partition PI in minor
frame 0 has left slack time of 1 and right slack time of 2
in the example. We maintain the table of accumulated
slack time from one frame to the other in the cyclic
schedule. For instance, the table corresponding to the
schedule of Figure 6 is shown in below table.The
accumulated slack time from frame 0 to frame 2 is
computed as sfo+sfi+sfi=I I.

obtain totalSlack[@,@D, I= low-bound slack time from 4 to @D,
plus expected slack times fromfuture RP operations:
if ((s, = totalSlack[4, @DJ - c,
rck )e 0 ) retum reject;

- C,",

for all hard aperiodic task k whose deadline is later than @D,
if ((sk = Sk - c, ) e 0) retum reject;
retum admit;

Figure 7. Acceptance Test Algorithm

Using this table and relevant parameters, we can
efficiently calculate the low-bound slack time in interval
[@, @DJ. For example, we assume that a sporadic task
that has a relative deadline 18 arrives at time 5. By
summing up O[sr of Pz at 5],8[the accumulated slack time
from frame fi to frame fi] and I[sl of P3 at 231, we can
obtain a low-bound slack time of 9 for the arriving task.
To guarantee the schedulability of an arriving hard
aperiodic task, we only consider the low-bound slack time
and future RP operations even if we know the slack time
can be increased due to future LS operations. We cannot
take advantage from future LS operations because they
depend on the future usage of partition servers. However,
at the arrival instance, we can exactly expect the extra
slack time obtainable from future RP operations which
exchange the capacity of partition server P, in
@DJ
with that of MPAS's in [@D,, PLJ, for every partition
server PI, where @D, < PL,. The expected future RP
operations must occur while there is at least one aperiodic
tasks pending. The contribution from the future RP
operations can be significant in slack time calculation, as
it will be evaluated in later simulation studies.

In order to prove that our LS and RP operations are
also harmless to admitted hard aperiodic tasks, we now

show that the currently calculated slack time in the
interval [@, @DJ cannot be reduced by future LS and RP
operations.
Lemma 1. Future LS and RP operations will not
jeopardize the hard aperiodic tasks that had already been
admitted before current time @.
Proof: First, considering a future LS operation, as long
as there is an existing hard aperiodic task, the LS
operation cannot be applied to any MPAS. If the LS
operation is applied to a partition server, the slack time
will just be increased. Second, considering a future RP
operation, there are only two cases. In the first case, if the
RP operation exchanges the capacity of a partition server
with the capacity of a MPAS which lies in [@, @DJ, it
does not change the slack time in the interval [@, &DJ.
In the second case, if the RP operation exchanges the
capacity of a partition server with the capacity of a MPAS
it increases the slack time in the
which lies beyond @toi,
interval [@, @DJ. Therefore future LS and RP operations
will not reduce the slack time. I

[e,

4.2. Acceptance Test Considering Future RP
operations

4.3. Dynamic Shck Time Management

In an acceptance test, the algorithm must guarantee
that the existing hard aperiodic tasks, which had already
been admitted, should meet their own deadlines. We
assume that MPAS schedules hard aperiodic tasks with an

Since we apply three dynamic operations, LS, RP, and
Compacting to the distance constrained cyclic schedule in
run time, the slack times available at a certain instance
may be changed. To keep the slack time related

373

parameters, i.e. sf, sr, sl, and frame-to-frame slack time
table, correct in an efficient way, an update must be done
whenever the dynamic operations are applied. At
initialization time, the scheduler calculates these
parameters and the frame-to-frame slack time table based
on initial cyclic schedule. After a completion of LS
operation, the scheduler does not need to change
anything. This is because that LS operation does not
affect the sequence and allocated capacity of the cyclic
schedule. After a completion of RP operation and
compacting, the scheduler must adjust the parameters and
the table.

the limit of paper size, it shows the same result as the first
scenario.
Avg. Response Time (r*=56.Poissonfunction)

5. Simulation Studies of D e Algorithm
Simulation studies have been conducted to evaluate the
DC2 scheduler against the simple polling server and the
LS-only scheduler. In a polling server method, aperiodic
tasks are served only in fixed time interval allocated to the
MPAS. In order to find the scheduling characteristics of
the DC2 scheduler, we also introduce and make a
comparison with the LS-only scheduler that uses LS
operation only. In simulation work, we measured average
response time of soft aperiodic tasks and average
acceptance rate of hard aperiodic tasks. The aperiodic
workload was modeled with Poisson inter-arrival time
and exponential distribution of execution time. We choose
the hard deadline of a hard aperiodic task based on interarrival time of the task like 0.5 or 1.0 times of inter-arrival
time. For each combination of inter-arrival time and
execution time, we randomly generated 25 different
simulation sets. For each random set, we generated 5
partition servers for periodic tasks and simulated it for
2,000,000 units of time. For every random set, we
randomly choose minor frame size from 56 to 200 and
major frame size of 16 times of the chosen minor frame
size. The sum of workloads from all partition servers in
cyclic schedule is 69%.

Avg. Execution Time (e+unential function)

(a)
NormalizedAve. ResponseTime (a=56, Poissonfunction)

00

0

.

,

,

,

,

.

,

.

,

1

2

3

4

5

6

7

6

9 1 0 1 1 1 2 1 3 1 4 1 5 1 6

.

.

,

,

.

,

.

Avg. Execution Time (exponentialfunction)

Figure 8. Average Response Time
As shown in (a), the DC2 scheduler significantly
improves the average response time of soft aperiodic tasks
while compared to the simple polling server and LS-only
scheduler. Since the DC2 scheduler uses both LS and RP
operations, it is worth to investigate contributions from
both operations. As we can observe in (b), RP operation
contributes more in lighter aperiodic task workload (or
lighter processor utilization) while LS contributes more in
heavior aperiodic task workload (or heavier processor
utilization). This is due to the fact that when the processor
utilization goes to higher, there is less opportunity of
success in RP operation. However, still in this situation,
LS operation can flexibly and efficiently manage its
W A S server capacity. Whenever there is an idle server,
the DC2 scheduler can successfully invoke LS operation.

5.1- Response Time
We present measured average response time of soft
aperiodic tasks in Figure 8. We have built two different
simulation scenarios. First we fixed Poisson inter-arrival
time of aperiodic tasks with 56. Then, we varied the
distribution of worst execution time of a task following
exponential distribution of, p between 1 (1.78%
workload) and 16 (29% workload). The simulation result
is shown in Figure %(a). The normalized average
response time of (a) based on that of polling server
method is shown in Figure 8-(b). Second, we fixed
distribution of worst case execution time with exponential
function of p equal to 3. Then we varied Poisson interarrival time of, a between 12 (25% workload) and 40
(7.5%). Even though the result is not shown here due to

5.2. Acceptance Rate

To evaluate acceptance rate of hard aperiodic task, we
compared the average acceptance rates of the polling
server and the DC2 scheduler. Simulation was performed
374

with the sequence of hard aperiodic task that has Poisson
inter-arrival time, a equal to 50 and 200. The worst case
execution time of each hard aperiodic task follows
exponential distribution. For each Poisson inter-arrival
time of, 50 and 200, we varied workloads of hard
aperiodic tasks from 10% to 60%. We also varied
deadline of each hard aperiodic task as 0.5 and 1.0 times
of inter-arrival time of the task. We show the simulation
result in Figure 9.

6. Conclusion
As a response to the demands for strongly partitioned
real-time systems in integrated modular avionics, we have
devised a DC2 scheduling algortihm to schedule both soft
and hard aperiodic tasks. By applying three essential
operations, Lejl Sliding, Right Putting, and Compacting,
in DC2 scheduler, we maximize the slack time to be
consumed by aperiodic task while guaranteeing that all
hard periodic tasks of each partition meet their own
deadlines. Through the simulation studies, we show that
the DC2 scheduler outperforms simple polling server
method and LS-only scheduler which has similar concept
with other resource reclaming algorithm, both in average
response time and in acceptance rate.

Avg. Acceptance Rate (aA.50,Poisson function)

References

0

.

,

5

1

.
0

1

.
5

2

.

,

0

2

5

3

0

“Avionics Application Software Standard Interface,”
“ I C Report 653, Aeronautical Radio Inc., Annapolis,
MD, Jan. 1997.
C.C. Han, K.J. Lin, and C.J. Hou, “Distance-constrained
Scheduling and Its Applications to Real-time Systems,”
IEEE Trans. on Computers, Vol. 45, No. 7, pp. 814-826,
July 1996.
Y. H. Lee, D. Kim, M. Younis, and J. Zhou, “Partition
Scheduling in APEX Runtime Environment for Embedded
Avionics Software,” Proc. of IEEE Real-Time Computing
Systems and Applications, pp. 103-109,Oct. 1998.
Y. H. Lee, D. Kim, M. Younis, J. Zhou, and J. McElroy,
“Resource Scheduling in Dependable Integrated Modular
Avionics,”Proc. of IEEEIIFIP International Conference on

.
3

5

Avg. ExecutionTime (sxpnential hmotton)

Avg. Acceptance Rate (e.200,poison function)
11

1

Dependable Systems and Networks (FTCS-3OIDCCA-8),

Jun. 2000.
B. Sprunt, L. Sha and J.P. Lehoczky, “Aperiodic Task
Scheduling for Hard Real-Time Systems”, Journal of RealTime Systems , vol. 1, pp. 27-60, 1989.
J. K. Strosnider, J.P. Lehoczky, and L. Sha, “The
Deferrable Server Algorithms for Enhanced Aperiodic
Responsiveness In Hard Real-Time Environments,” IEEE
Trans. on Computers, Vol. 44, No.1, pp. 73-91, Jan. 1995.
J. Lehoczky and S. Ramos-Thuel, “An Optimal Algorithm
for Scheduling Soft-Aperiodic Tasks in Fixed-priority
Preemptive Systems,” Proc. of IEEE Real-Time Systems
Symposium, pp. 110-123,Dec. 1992.
M. Spun, G. Buttazzo, “Efficient Aperiodic Service Under
Earliest Deadline Scheduling,” Proc. of IEEE Real-Time
Systems Symposium, pp. 2-11, Dec. 1994.
C. Shen, K. Ramamritham, and J. A. Stankovic, “Resource
Reclaiming in Multiprocessor Real-Time Systems,” IEEE
Trans. on Parallel and Distributed Systems, Vol. 4, No.4,
pp. 382-397, Apr. 1993
[lo] G. Fohler, “Jbint Scheduling of Distributed Complex
Periodic and Hard Aperiodic Tasks in Statically Scheduled
Systems,” Proc. of IEEE Real-Time Systems Symposium,
pp. 152-161,Dec. 1995.
[ l l ] J. Liu, “Real Time Systems,” Book Manuscript, Chap. 5,

I
Figure 9. Average Acceptance Rate
As shown in two simulation results, the DC2 scheduler
outperforms the polling server in acceptance rate. In the
case of Poisson inter-arrival time equal to 50, the
acceptance rates of the DC2 scheduler are 29.5% and
24.6% higher than those of the polling server at the
deadline factor of 0.5 and 1.0 respectively. When the
Poisson inter-arrival time is 200, which is the maximum
possible minor frame size in the simulation, the
acceptance rates of the DC2 scheduler are 21.2% and 17%
higher. The smaller inter-arrival time and the tighter
deadline, the DC2 scheduler shows the advantage of
higher acceptance rate. This is because that the benefit
from future RP operations is limited by the period of each
partition server. Therefore, we can expect much higher
acceptance rate advantage when hard aperiodic tasks
come in burst with tighter deadlines.

1998.

375

On the Existence of Probe Effect in Multi-threaded Embedded
Programs
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ
ywsong@asu.edu, yhlee@asu.edu

Abstract — Software instrumentation has been a convenient and
portable approach for dynamic analysis, debugging, or profiling
of program execution. Unfortunately, instrumentation may
change the temporal behavior of multi-threaded program
execution and result in different ordering of thread operations,
which is called probe effect. While the approaches to reduce
instrumentation overhead, to enable reproducible execution, and
to enforce deterministic threading have been studied, no research
has yet answered if an instrumented execution has the same
behavior as the program execution without any instrumentation
and how the execution gets changed if there were any. In this
paper, we propose a simulation-based analysis to detect the
changes of execution event ordering that are induced by
instrumentation operations. The execution model of a program is
constructed from the trace of instrumented program execution and
is used in a simulation analysis where instrumentation overhead is
removed. As a consequence, we can infer the ordering of events
in the original program execution and verify the existence of
probe effect resulted from instrumentation.

Keywords

- multi-threaded program; event ordering;
reproducible execution; probe effect; profiling, software
instrumentation.

1

INTRODUCTION

In real-time embedded systems, application tasks usually run
in concurrent threads. Threads may interrelate with each other as
they share resource and data. They also interact with the external
environment to receive sensor data and to control actuators. While
the threads are running, any instrumentation to observe program
execution behavior will introduce extra overhead to the execution.
Instrumentation overhead, no matter how small it is, may intrude
and change the execution behavior of the program and,
consequently, introduce probe effect [1][2]. Hence the observed
behavior through instrumentation is not guaranteed to represent
the original program behavior.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than ACM must be
honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from
Permissions@acm.org.
ESWEEK'14, October 12 - 17 2014, New Delhi, India.
Copyright 2014 ACM 978-1-4503-3052-7/14/10…$15.00.
http://dx.doi.org/10.1145/2656045.2656062

Instrumentation operations can perturb program execution in
two ways [2]. First, the occurrence of an execution event is
delayed by the amount of time spent on running instrumented
code. This can change the time of interacting with other threads
and external environment (e.g. reading an input). Second, due to
the changes of the timing of invoking guarded resources and
critical sections, scheduling decision can be different. This, in turn,
can lead to the variations in the sequential order of accessing
shared resources. Therefore, the timing perturbation by
instrumented code can result in a different happened-before
ordering of events [3] and possibly a different program execution
path from the original program. The other related issue is that we
may not be able to know whether there is any change on program
execution paths caused by the timing perturbation.
To obtain a proper observation of multi-threaded program
execution through instrumentation, it is critical to know the effect
of timing perturbation. However, unless we adopt hardware based
monitoring, it might not be possible to know the exact execution
behavior of a program. It may be argued that a comparison of
computation results from instrumented and un-instrumented
programs can reveal any different behavior of program execution.
Note that some benign program behavior may not affect the final
computation results, for instance, a branch decision can be caused
by different conditions in a compound conditional expression. In
addition, for embedded systems, it may be tricky to manage
identical external inputs arriving at the precise instants of the
execution. Another approximation is to measure the overhead of
instrumentation, calculate the execution time by removing the
overhead, and infer the real execution. In a single thread program,
this would be feasible. However, in multi-threaded programs it is
extremely difficult to consider all thread interactions when thread
execution time is changed. Moreover, it is problematic to take into
account kernel states (e.g., run queue state) which may affect
scheduling decision.
There have been several approaches to recover the
performance of parallel programs by compensating the
instrumentation overhead [4][5], but they do not examine the
ordering of the program execution. On the other hand,
deterministic replays [6][7][8] provide reproducible execution that
guarantees the same execution ordering as the one observed in a
recording operation. It is perceivable that any recording operation
should incur some instrumentation overhead since the execution of
the recording itself would have caused perturbation to the original
execution. Nonetheless no research on deterministic replay
examines the issue of any changes to the original program
execution behavior caused by the recording operation.
In this paper, we propose a simulation-based analysis for
embedded software to detect any variations of event ordering

caused by instrumentation overhead. It is assumed that application
tasks are performed by concurrent threads in a priority-based
preemptive scheduling system. The analysis starts with an
instrumented program (e.g., for dynamic program analysis) from
which we want to analyze the impact of instrumentation overhead
to the program execution. The instrumented program is again
instrumented to obtain traces including thread interaction events
and the overhead of instrumentation code. Kernel’s activities as
well as external inputs are also recorded. From the traces, we
construct a simulation of program execution where the
instrumentation overhead is removed. Timing of thread events is
calculated which decides the ordering of events in the simulated
run. Then the ordering information of the original program
execution with no instrumentation overhead is projected. The
contributions of this work are:
1. It provides a novel way of detecting changes in event ordering
due to probe effect.
2. Accurate timing of event occurrence is simulated with the
consideration of all factors affecting the ordering of program
execution, including kernel activities, external inputs, as well
as the thread execution time.
3. It provides an analysis of simulation results for inferring the
ordering of the original program execution.
The rest of paper is organized as follows. In Section 2, a
concise survey of related works is presented. Section 3 describes
the design of the simulation analysis. The implementation details
are explained in Section 4. Section 5 shows experimental results.
The paper is concluded in Section 6.

2

RELATED WORKS

Malony et al. presented the instrumentation uncertainty
principle [4] suggesting that the accuracy of execution
performance is degraded as the degree of instrumentation
increases. Performance perturbation models were proposed to
calculate the true performance from instrumented parallel
programs. The models were further refined in [5]. In the models,
perturbations trigger a change in event execution time and event
ordering is represented by time-based and event-based
perturbation models. In the time-based model, the thread events
are independent while the event-based model considers the
dependency between events for recovering the true performance.
The dependency considered is performance degradation as arrival
time and resource state change. However, the approach assumes
the program execution is fixed no matter there is any
instrumentation overhead or not. Hence, it does not consider how
the program behavior may differ from the un-instrumented
program.
When instrumentation perturbation causes different thread
interleaving, we will be concerned with the potential problems of
data race and execution non-determinism. Data races can result in
arbitrary failures and do not help increase scalability [9]. Several
efficient dynamic data race detection algorithms [8][11] have been
proposed and race detection tools [12][13] are widely used in
practice. In general, the approaches are based on the monitoring of
read/write operations to shared variables among concurrent
threads. However, the delay caused by monitoring operations and
any possible probe effect have not been addressed. Deterministic
multi-threading techniques [14] provide deterministic event
ordering for parallel program execution. In Kendo [15], a thread’s
progress is represented with a logical clock. It is a thread’s turn
when its logical clock is the global minimum and a thread can take
a lock only during its turn. In [16] and [17], thread’s shared

memory is isolated from other threads during a parallel phase.
During a serial phase, the memory updates to shared variables are
applied and locks are taken in a deterministic order. Regardless
their overheads, the approaches don’t consider any external input
events and time-based operations, and cannot be applicable to
embedded software.
It may be argued that instrumentation can be done during
program replay if a reproducible execution can be constructed.
Instant Replay [6] is one of the earliest works that allows cyclic
debugging for parallel programs by tracing and replaying relative
order of events for each shared object in the program. RecPlay
[7][8] based on Lamport’s happened-before relations [3] records
and replays synchronization operations. In the approach, data races
can be detected by checking all shared memory references so that
the program is free of data race before record/replay operations.
Replay Debugger [18] uses a similar method as RecPlay but it
focuses on debugging techniques on embedded software. It is
integrated with GDB and can supply additional debugging
functionalities for users to control thread execution. To reduce
overhead of record and replay, speculative execution and external
deterministic replay are used in Respec [19] that is capable of
online replaying on multiprocessor systems even with data races.
Using speculative execution, the recording process can continue to
execute speculatively instead of being blocked until the
corresponding replay finishes.
Most profiling tools adopt instrumentation approaches. There
have been research efforts to reduce profiling overhead caused by
instrumentation. Froyd et al. proposed a call-path profiler based on
stack sampling [20]. The profiler, called csprof, provides an
efficient way of constructing the calling context tree without
instrumenting every procedure’s call. Zhuang et al. introduced the
adaptive busting approach [21] to build calling context tree with a
reduced overhead while preserving profiling accuracy. In their
approach, unnecessary profiling is avoided by disabling redundant
stack-walking with a history-based predictor. The profiling
overhead has been further alleviated by taking advantage of multicore systems. In shadow profiling [22], shadow processes are
periodically created for running instrumented code while the
original process is running on a different core with minimal
overhead. PiPA [23] exploits parallelism by forming a pipeline to
collect and process profiles. Application execution and profiling
operation are divided into stages that are pipelined and performed
in multiple cores. Kim et al. proposed a scalable data dependence
profiling to reduce runtime and memory overhead [24] by storing
memory references as compressed formats and using pipelining
and data level parallelism for the data dependence profiling. Timeaware instrumentation approaches [27][28] have been proposed to
minimize violation of timing constraints due to instrumentation
overhead. DIME [27] monitors instrumentation time and limits the
program instrumentation to a given time budget in a time period.
A static approach [28] inserts instrumentation code only where the
instrumentation can preserve the worst-case execution time.
Although there are research works on the overhead
compensation for performance measurement and the reduction of
instrumentation overhead for reproducible execution and profiling,
no work has been proposed to reveal the possibility of execution
deviation caused by instrumentation overhead. In this paper, the
focused analysis is to verify if the recorded or observed execution
is a true representation of the original program execution and if
any instrumentation may alter the event ordering of multi-thread
embedded programs as to cause any changes of the intended
program behavior.

3

DESIGN AND ANALYSIS

3.1 Multi-threaded Program Execution
Multi-threaded program execution can consist of a set of
thread interaction events. Such an event, as a sequence of
instructions that the program executes, defines a particular action
(e.g. a system call) to interact with other threads, internal and
external environment. Examples include synchronization,
communication operations, and IO read/write calls. The events can
be totally ordered by the timestamps at which the events take
place. A partial order can also be defined among the events based
on logical dependencies, i.e., happened-before relation [3].
Let’s consider multiple runs of a program and the ordering of
interaction events from each run. If the execution events happen at
different instants, the happened-before ordering of the events may
be different from one run to the other. This may lead to a change
of program execution behavior. For instance, in the sleeping
barber problem, customers may be served in different orders when
they arrive at different instants in separate runs. On the other hand,
a particular customer may find out the waiting room is full and
miss the service in one run. In the other run, if the barber cuts hair
quickly, the customer can find a seat in the waiting room and
receive the service eventually. In this case, the execution path of at
least one thread is changed which results in a different timestampbased ordering of events at thread level.
It is a well-known principle used in deterministic replays
[7][8] that, for two runs of a data race free program, if we supply
the same input data and have the same happened-before ordering
of events, then the two runs must result in the same behavior.
Conversely, if we observe two distinguished runs of a program,
then either inputs and/or the happened-before ordering of one run
must be different from those of the other run.
During an execution of a program, a happened-before ordering
of program events can be dependent upon the external inputs it
receives, including input data value and the timing that new data
arrives. If we have the same inputs, the happened-before ordering
of the program events is decided by the execution instants of
threads events on shared resources and communication. For
example, when two threads compete for a resource, the happenedbefore ordering of the locking events will be decided by the
instants that the two threads issue their resource requests. The
choice on who is going to take the resource first may also depend
upon the kernel’s scheduling policy, e.g., priority based or FIFO,
as well as the kernel’s internal states, including run queue state and
other tasks running in the system including interrupt handlers. So,
when a program is instrumented, we can expect that more CPU
time is spent to execute instrumentation code and the execution
time of each thread becomes longer. This may have a ripple effect
on the instants that program events may take place, and the
happened-before ordering of the events.
3.2 Model of Multi-threaded Program Execution
To model multi-threaded program execution, we assume there
are n concurrent threads, Ti for i=1,…,n, in an embedded program.
The threads are data race and exception free and are scheduled
preemptively according to their priorities. The system state is the
collection of thread local states and a shared global state. The
interactions among the threads and with the external environment
are done through the operations on the global state, which are
represented by interaction events. An interaction event
(abbreviated as event), e, can be a lock/unlock, semaphore
give/take, message send/receive, or input/output operation that is

C1
C2

C1

//thread T1
T1 {
//f1=sem_take(a)
//f1 -> e1
sem_take(a);
}
//priority T1 > T2

CT1(f1) CT1(e1)

CT2(f2) CT2(e2) CT2(f3) CT2(e3)

CT1(f1)

CT1(e1)

CT2(f3)

CT2(f2) CT2(e2)

C2

//thread T2
T2 {
//f2=sem_give(a)
//f2 -> e2
sem_give(a);
//f3=read(0, buf, 1)
//f3(x) -> e3
read(0, buf, 1);
}

ht(e2)

CT2(e3)

ht(e1)

C

T1 enters f1

T2 enters f2 e2
happens

: scheduler execution,

e1
happens

T2 enters f3

e3
happens

: interrupt execution

Figure 1. An example global/local clocks and event executions
for two interacting threads.

performed when a thread invokes an event function f. The notation
Ex:f→e is used to indicate that an event e is generated during the
execution of the event function f . Apparently, the resulting event
of an invocation of f depends upon the local state of the calling
thread, as well as the shared global state. For instance, a nonblocking read from a device can succeed or fail depending on the
availability of input data when the function is invoked.
There are two important incidents during the execution of an
event function f by a thread T. “T enters f” occurs when the
processing begins to take place globally. The entrance gives an
important timing information since it decides a logical order
between events, as well as the possible resultant event to be
generated by the function. For instance, when two threads request
a semaphore concurrently, the moments that they enter sem_wait
function provide an order of the requests and can determine which
thread can take the semaphore successfully and the consequent
global state. The other important incident is when “e happens”. It
symbolizes the result of execution, as event e, is posted and is
available to subsequent execution. Obviously, the invocation of
event functions by a thread form a sequence and an event
happened previously can causally affect any subsequent events [3].
To include OS and device activities in the model, a system
thread, T0, is added. The events that occur in T0 consist of
interrupts, OS scheduling, and the arrivals and updates of device
input data. Thus, an interrupt event of T0 may set a semaphore and
wake up a waiting application thread. Also, an application thread
can read in the latest sensor data if the read operation happens after
a data update event in T0.
To define the occurrence instants, we adopt two types of
clock. Clock C indicates the CPU cycles used globally. Starting
from 0, it is advanced for all activities consuming CPU cycles,
including thread executions, O/S activities such as interrupt
handler and scheduler, and idle process. In addition, Ci is the
thread local clock for thread Ti and is advanced only during the
execution of Ti. Hence Ci represents the CPU time spent on the
execution of Ti. With the clocks, we define:
ht(f) and ht(e) as the C clock values when f is invoked and e
happens, respectively. This is the global timestamp for a call to
function f and for event e.

CTi(f) and CTi(e) as the Ci values when Ti enters fi and when ei
happens as a result of fi ’s execution, respectively.
Based on CTi(f) and CTi(e), we can compute ct(fi,k) which is
thread Ti’s execution time between the instants that the (k-1)-th
event ei,k-1 happens and that the subsequent function invocation fi,k
is entered. Similarly, ct(ei,k) can be obtained as the processing time
from entering fi,k to posting ei,k. As shown in Figure 1, an example
execution of the interacting events performed by threads T1 and T2
and event timestamps are depicted. In addition to the thread local
clocks for T1 and T2, thread events are aligned with the global
clock and the kernel scheduling and interrupt service events are
included.
For the events generated from the executions of the program
threads and the system threads, an event graph G = (V, E) can be
constructed where V is a set of events and edge (ea, eb) ∈ E if ea
∈ V and eb ∈ V and eb is logically dependent on ea. Basically, G
is a partially ordered graph representing the happened-before
relation among events [3]. An example of event graphs is shown
in Figure 2. In the following sections, we use GI = (VI, EI) and
GU= (VU, EU) to represent the event graphs of the instrumented
program PI and the original program without the instrumentation
PU, respectively. To determine whether there is a probe effect
caused by instrumentation, it is assume that the initial states and
the external events, including interrupts and the arrivals of input
data, are identical in the execution of PI and PU. We will then need
to compare GI and GU or at least find a way to check whether GI
differs from GU.
e0 : input update
e1 : read
e2 and e6: events on
semaphore 5
e3, e4 and e5: events on
semaphore 7
ht(e0) < ht(e1) < ht(e2) < ht(e3)
< ht(e4) < ht(e5) < ht(e6)

G
T0
e0

T1
e1
e4
e5

T2
e2
e3
e6

Figure 2. An example partial order graph G with seven events
from program execution

3.3 Simulated Program Execution
We consider the execution of instrumented and uninstrumented programs with the same input. In an instrumented
program, extra code is inserted to record program execution
behavior that we are interested in. For instance, to measure
branch/decision coverage, instrumented code should log the
conditions used at each decision point. Furthermore, additional
instrumentation is added to obtain the trace of thread interaction
events, thread execution time, and the overhead of instrumentation
code. Thus, the event graphs GI can be constructed from the
observed event trace. However, GU of PU is not observable
directly.
With the event functions and the events collected from the
execution of PI, an event-driven simulation can be conducted to
analyze the execution behavior of PU. Since there is no
instrumentation in PU, any instrumentation overhead should be
removed from the thread execution time of PI. As shown in Figure
3, the execution time to be considered in the simulation analysis,
CTiS(f) and CTiS(e), can be obtained from the measured CTiI(f) and

Trace from the instrumented program

CiI
CTiI (f1) CTiI (e1)

CTiI (f2) CTiI (e2)

Execution time (CPU time) for the simulation

CiS
CTiS (f1) CTiS (e1) CTiS (f2) CTiS (e2)
: Instrumentation overhead

Overhead
removed

Figure 3. Execution time with no overhead in simulation analysis
where CiI and CiS are thread Ti’s local clock in PI and in simulation.

CTiI(e) of instrumented program PI. As a consequence, if PU
invokes an identical event function and results in the same event as
in the execution of PI, the event may occur at an early instant. The
change in execution time can also vary the relative order of
program and system events. For instance, an event e of PI that is
invoked after an interrupt may happen before the same interrupt in
PU. This alteration may have a ripple effect. For instance, if the
interrupt service routine signals a ready status, the running thread
that invokes a function call to read the status in PU would not see
the ready status immediately and may be blocked until the
interrupt arrives. The example suggests that, in the simulation
analysis, kernel resource and scheduling operations must be
incorporated to determine the event ordering.
The goal of the simulation is to generate an event graph GS to
represent the un-instrumented execution of all threads and system
events observed in the execution of PI. In the simulation, a thread
Ti will execute a sequence of function invocations and events (fi,k
and ei,k) in timestamp ordering that are collected from PI. The
inter-event execution times of ctS(fi,k) and ctS(ei,k), are calculated
by subtracting any instrumentation overhead from ctI(fi,k) and
ctI(ei,k). The simulation is done with a global clock CS and the
current simulation time is denoted as cur-time, which is advanced
based on thread events and operating system activities. At each
simulation epoch, the ready thread with the highest priority is
chosen as the running thread. The running thread Ti can proceed
to perform its next activity ai,k, where ai,k is either fi,k or ei,k, if
there is no system event in [cur-time, cur-time+ctS(ai,k)]. The
global clock CS is then advanced to the next simulation epoch,
i.e., cur-time+ctS(ai,k). Otherwise, the running thread is preempted
by the arriving interrupt irqj at cur-time=CT0(irqj). After the uninstrumented execution time of irqj, Ti may continue its execution
or a context switch may occur if there is a thread of higher
priority waked up by the interrupt.
As the simulation proceeds from one event to the other, the
graph GS is constructed by adding edges for logical dependence
among threads and system events. For instance, a happened-before
relation is added from a message send event to a subsequent
message receive event. Similar, an interrupt is happened before the
thread’s sem_wait event which gets completed due to a sem_post
event issued by the interrupt. In Section 4.3, the simulation
algorithm implemented for execution in VxWorks is described.
3.4 Analysis of Simulation Results
Once GS is constructed, we will be interested in the execution
behavior of PU that may be inferred based on GS. Note that GS
does not always represent GU, since if the event ordering of PU is
different from that of PI (due to the instrumentation overhead),
then the execution paths of PU and PI might be different. Thus,

there might be an event e such that e ∈ VI but e ∉ VU. Given that
the simulation is based on the events in PI, we have e ∈ VS. This
leads to GU ≠ GS. However, a positive result can be established in
the following theorem when GI and GS are equivalent.

VxWorks’ priority-based preemptive scheduler is configured. Two
IRQs are available during the execution, a 60Hz timer IRQ and a
PS2 keyboard IRQ. The queuing mechanism for tasks blocked on
a semaphore is based on task priority.

Theorem 1: If and only if GS = GI, then GU = GI.

We consider a simplified system in which three kinds of tasks
and system activities can affect the timings of event occurrences
and must be traced in the execution of instrumented program:
application tasks, interrupt handlers, and the scheduler. All
application tasks and kernel operations are run in a flat memory
space and there is no page-fault exception. We also assume there
are no exceptions from the running applications. The priorities of
application tasks are set to be higher than the priorities of any
other system background tasks. Thus, the execution of background
tasks will not affect the analysis of probe effect. The trace data are
sent to the host at the end of the application execution to avoid any
activities including file IO during application execution.

U

I

Proof: to show the “if” part, let’s assume G ≠ G . Then, there
should be a thread that generates different events or experiences
different happened-before relations in the execution of PU and PI.
Let eiI(k) be the first such event of PI (in terms of the global clock
CI) that eiI(k)≠eiU(k), where eiX(k) represents the k-th event
performed by thread Ti of program PX. All events of PI that are
prior to eiI(k) have the identical happened-before relations as their
equivalent events in PU. For thread Ti, it must have executed the
same first (k-1) functions and generated the same (k-1) events, i.e.,
eiI(l)=eiU(l) for l=1,…,k-1. Thus, it should use the same function in
its k-th invocation, i.e. fiI(k)=fiU(k).
To have eiI(k)≠eiU(k), the global states of PI and PU that are
used in the processing of fiI(k) and fiU(k), should be different. This
suggests that at least one extra update is inserted before the
invocations of fiU(k) in the execution of PU. Let this update event
be performed by a different thread Tj and denoted as ejU(k’). As the
instrumentation overhead is removed in PU, this update event is
brought forward and occurs before the invocations of fiU(k) in the
execution of PU, even if ejI(k’) occurs after the invocations of fiI(k)
in the execution of PI. This change in event ordering should be
observed in the simulation analysis, i.e., ejS(k’) occurs before the
invocations of fiS(k) since the same instrumentation overhead is
deducted and the program behavior has not been changed before Ti
makes its k-th invocation. The addition of the happened-before
dependency, ejS(k’) → eiS(k), in GS results in GS ≠ GI.

4.2 Execution Trace and Measurements
To trace the invocation of event functions and the occurrence
of events, we adopt the instrumentation mechanism of the record
framework of Replay Debugger [18]. Since the record framework
already supports the wrappers for tracing event execution, we
added 1) the measurement code for CPU time spent for each
thread and 2) the overhead measurement for the instrumentation
code. The existing interrupt handlers for timer and keyboard are
instrumented to collect the timestamp when an interrupt arrives
and to measure the execution time of interrupt handler. In addition,
the keyboard interrupt handler is customized to directly
communicate with the keyboard driver. For our Atom-based target
processor, x86’s RDTSC (Read Time-Stamp Counter) instruction
is used to collect timestamps.

For the “only if” part, the equivalency of GU and GI indicates
that thread Ti would invoke the same event function fiI(k) and
generate the same event eiI(k) as in the execution of PI, even if the
instrumentation overhead is removed. Thus, the simulated
execution of fiI(k) will generate the same event eiI(k) which has the
same happened-before relations with preceding events. This
implies GS = GI. ■

Task execution time is measured in scheduler hook routines
that are invoked for every context switch. For each task, we keep
the timestamp that the task is switched in and when the task is
switched out. The difference between the current timestamp and
the switched-in timestamp is accumulated to the task CPU time.
The execution time of scheduling operation and context switching
is measured offline using two tasks, task 1 and task 2, where task 2
has a higher priority than task 1. First, we let task 2 be blocked on
a semaphore and when task1 posts the semaphore, task 2 becomes
running. Then, we remove task 2 and just run the sem_post
operation by task 1. The intervals from the invocation of sem_post
to the completion of the call are measured for the two cases. The
difference is considered as the measured execution time of
scheduling operation and context switch.

The theorem implies that, if the logical order of thread events
built in the simulation analysis is as same as the one in the
execution of the instrumented program PI, then GI is the true
representation of GU. On the other hand, if GS ≠ GI, the simulation
failed in a sense that when the partial order begins to be different,
the execution path may also have changed too. This suggests that
the instrumented program may have started to take a different
execution path. Since the simulation uses the same execution path
as the instrumented program, the simulated execution is no longer
a representation of the un-instrumented program. However, we can
find out when and how the execution changes. Let ed be the very
first event of GS that causes a different partial order from GI. Then,
the partial graph of GS for all events priori to ed is the true
representation of the same events in the execution PU. A follow-up
investigation on the trace can be considered to find out how the
instrumentation changes the program execution.

4

IMPLEMENTATION

4.1 Execution Environment
To implement the proposed approach of detecting probe
effect, an execution environment is set up on a single core of a 1.6
GHz Intel Atom processor running VxWorks 6.8 [25]. The

4.3 Simulation Analysis Algorithm
Using the execution trace from the instrumented program
execution and the measurements of the execution environment, the
simulation is performed as shown in Figures 4 and 5. It is
governed by the invocation to event functions and the occurrence
of events and interrupts. The simulation maintains a global clock C
which advances when the running task, event function, or interrupt
service routing is executed. Note that thread T0 is used to represent
system’s external activities and runs concurrently with the
scheduled application threads. It consists of interrupt and input
data change events. The execution time of interrupt events is
measured from ISR routine, whereas the execution time for input
data change event is set to 0.
When a thread is scheduled to run, the simulation clock is
adjusted to the instant of the next interrupt or the subsequent event
function call, whichever comes first. When the global clock C is

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

enter_func(func: f, thread: T, event e)
if f is resource release function
return
if resource not available
if f is synchronous function
set T to pending
else
mark e as “fail to acquire the resource”
else
mark e as “succeed to acquire the resource”
execute_event(event: e, thread: T)
if e is resource releasing event
the resource is released
if any tasks pending for the resource
select a task and set it to ready
eL = event that e logically depends on
VS =VS ∪ e
ES =ES ∪ (eL, e)

Figure 4. Sub-routines for the the simlulation algorithm. The
selection of a task in evecute_event() is based on the system
property of the instrumentd program, e.g., FIFO, priority-based.

equal to the arrival time of an interrupt, the time spent on the
interrupt is added to C. If there is a thread pending for the arrival
of the interrupt, its state is changed to ready once the interrupt is
processed. Then, the highest priority ready thread is scheduled for
execution. If an event function invocation takes place, the resource
required by the function is evaluated. The call can lead a return
with error, a blocked thread, or the execution of event function. A
simulated event happens when an event function is completed. The
system state may be updated (e.g., a message is de-queued) and
blocked tasks may be waked up as the consequence of the
happened event. Whenever needed, the scheduler’s execution time
is added to C to simulate the scheduling operation. When there is
no ready thread, the idle process is simulated by advancing C to
the next interrupt. Note that, interrupts are accepted during task
execution and are delay if they arrive during the execution of event
functions or scheduling operation.

5

EXPERIMENTAL RESULTS

We used two multi-threaded programs, the dining
philosophers and the sleeping barber, from the LTP benchmark
suite [26] in the experiments. The dining philosophers program
has 5 philosopher threads (P1 to P5) with decreasing priorities
from philosopher 1 to philosopher 5. Each philosopher is looping
from thinking, picking up forks, and to eating. The thinking and
eating activities of philosophers 1 and 2 are implemented with
blocking reads for keyboard input. On the other hand, the thinking
and eating activities for philosophers 3, 4, and 5 are replaced with
a simulated computation. When a keyboard pressing interrupt
occurs, a philosopher (1 or 2) will be waked up and may preempt
another running philosopher. Thus, philosophers will be
differently interleaved depending on the timing of keyboard inputs
(e.g., different order of philosophers’ eating). In the sleeping
barber program, the waiting room has three available chairs and
there are one barber thread with the highest priority and 5
customer threads (C1 to C5) with decreasing priorities from
customers 1 to 5. The barber is looping from sleeping if there is no
waiting customer, and to serving a customer. A customer waits in
the waiting room and gets a haircut if a chair is available. He
leaves without a haircut if all the three chairs are occupied. The

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:

do_simulation()
C_cur = 0
// the current global clock
Ci = 0 for all Ti // Ci is the thread clock for Ti
Tr = the highest priority ready thread
next_act = the earliest action of T0 and Tr
while (true) {
Δ=the instant of cur_act – C_cur
cur_act = next_act
C_cur is advanced to the instant of cur_act
if (cur_act == ISR completion)
set any tasks pending for the ISR to ready
else {
if (Tr ≠ null), Cr is advanced by Δ
if (cur_act == IRQ arrival at T0)
next_act=ISR completion
else if (cur_act==input data change)
mark data input event
else if (cur_act==function f invocation)
enter_func(f, Tr, e) // Ex:f→e
else if (cur_act==event e completion)
execute_event(e, Tr)
}
if any change in task state
Tr = the highest priority ready thread
if (cur_act ≠ IRQ arrival at T0)
next_act = the earliest action of T0 and Tr
if all events are executed
break
}
Figure 5. The simulation algorithm

CPU TIME FOR THE DINING PHILOSOPHERS

TABLE I.

Thread
P1
P2
P3
P4
P5
Average

CPU time
(cycles)
29,964,934
29,073,286
161,459,044
161,626,702
160,931,184

TABLE II.

Thread
Barber
C1
C2
C3
C4
C5
Average

CPU time without
overhead(cycles)
20,042,918
20,029,304
153,060,446
153,232,244
153,228,482

Overhead
49.5%
45.1%
5.4%
5.4%
5.0%
8.6%

CPU TIME FOR THE SLEEPING BARBER

CPU time
(cycles)
7,965,912
10,065,480
10,046,790
2,568,782
1,378,142
1,615,038

CPU time without
overhead(cycles)
5,024,050
9,544,202
9,524,506
2,047,280
1,029,454
1,231,966

Overhead
58.5%
5.4%
5.4%
25.4%
33.8%
31.0%
18.4%

barber’s sleep is waked up by a keyboard interrupt. As a
consequence, customers 4 and 5 (with lower priorities than
customers 1 to 3) may not get haircuts depending on how fast the
barber is waked up by keyboard interrupt.
Both programs in the experiments have relatively short
execution times considering manually injected keyboard

Instrumented

290M

295M

300M

310M

305M

Philosopher 2

315M

R

SW(7)

SP(7)

35

38

39

320M

SP(5)SP(7)

Philosopher 4

SW(5)
40

36 37

Simulation
290M

295M

300M

310M

305M
R SW(7)

Philosopher 2

3839

Philosopher 4SP(5) SP(7)
3536

315M

320M

SP(7)
40

SW(5)
37

Keyboard Interrupt
R: read(), SW(s): sem_wait(s), SP(s): sem_post(s)
Number below event: total order number
Time unit: CPU cycles

Figure 6. An example execution of dinning philosopher program where GI = GS but some events are with different
timestamp ordering

operations. A simulated computation is inserted between event
functions to ensure the speed of program execution is comparative
to the rate of manual keyboard entry. As we add wrappers to event
functions and interrupts, instrumentation overhead is added to the
program execution. In addition, extra simulated computation is
inserted in thread execution to represent dynamic analysis or
profiling overheads. These instrumentation overheads are removed
in the following simulation analysis. Tables I and II show the
execution time and overhead from the average of 5 executions of
each benchmark program. . In Table I, the CPU times of
philosophers 3, 4, and 5 are greater than that of philosophers 1 and
2 as simulated computations are used for the thinking and eating
activities instead of blocking reads. In Table II, customers 1 and 2
spend more CPU times than other customers as we inserted
additional simulated computations before entering the waiting
room to delay the entrances of customers 4 and 5. Thus, customers
4 and 5 would not arrive too early while the first three customers
are waiting on the three chairs, and have to leave. There are some
differences in the CPU times spent by customers 3, 4, and 5 as
customer 3 always gets a haircut while customers 4 and 5 may get
haircuts depending on input timing.
Few experimental results of the probe effect are illustrated in
Figures 6 to 8. Figures 6 and 7 are the results of two different
executions of the dining philosophers program with different
inputs and Figure 8 is based on an execution of the sleeping barber
program. In each figure, the event orderings from the instrumented
program and the simulation analysis are compared. We only
illustrate specific time frames with particular threads that show
some variations in event ordering. The horizontal lines show the
timestamps of event occurrences and arrow lines indicate the
logical dependencies between events. The number below each
event denotes the event sequence number in timestamp (total)
ordering. Figure 6 depicts a case when GI = GS but the timestamp
orderings are different. The case appears when philosopher 2 is
waiting for a keyboard input and, as soon as the input becomes
available, it preempts philosopher 4. In the simulated execution,
since thread execution time is reduced due to the removal of the
instrumentation, the events of philosopher 4 happen while
philosopher 2 is still blocked. As a result, the timestamp ordering
of events in the simulation differs from that of the instrumented
program. However, since GI = GS, the simulation is the true

representation of the original program and we can conclude that
there is no probe effect for the instrumentation overhead.
Figure 7 shows a case where the logical ordering (partial) is
altered due to the instrumentation overhead. The timestamp
ordering of events in the simulation is changed in a similar way as
in Figure 6. Events of philosopher 4 occurred earlier than in the
instrumented program execution while philosopher 2 was still
blocking for input. Since GI ≠ GS, the simulation is no longer a
representation of the original program. We notice that the partial
graphs of GI and GS are identical until the 37-th event. Thus, for
each thread, the next event function to be invoked immediately
after the 37-th events should be identical in the instrumented
program and in the simulated execution. After the 37-th event, the
difference in the logical ordering of events is triggered by the
order of the sem_wait(7) events invoked by philosopher 2 and
philosopher 4. We manually inspected the source code and found
out that the change in the logical ordering did not result in a
change of execution path. Hence, any program dynamic analysis
based on execution path is still valid. However, in general, it will
be a very challenging task to find a change of execution path by
manual inspection of source code.
Figure 8 demonstrates a case of a change of execution path
due to instrumentation overhead in sleeping barber program. The
result shows that GI = GS up to the 27-th event. However, in the
28-th event, the sem_wait(4) is invoked much earlier in the
simulation than what happens in the instrumented program. The
resultant partial order graphs are drawn in Figure 9. The second
line is for barber thread and the last line is for customer 5. In the
simulation, the semaphore given at the 25-th event is taken by
customer 5 while in the instrumented program execution the
semaphore is taken by barber thread. In fact, in the instrumented
execution, customer 5 arrives after the barber is waked up by a
keyboard input and begins to cut hair. A chair becomes available
for the arriving customer 5. On the other hand, customer 5 arrives
before the keyboard input, suggesting the barber is still in sleep
mode. The customer should leave as he cannot find any available
chair in the waiting room, i.e., a different execution path. Since the
simulation uses the same observed event from the instrumented
program, the simulated execution cannot be correct after the 28-th
event.

Instrumented
310M

315M
R SW(7)

Philosopher 2

320M
SP(7)

38 39

330M

325M

335M

340M

40

SW(7)

Philosopher 4

SP(7) E
4243

41

Simulation
310M

315M
R SW(7)

Philosopher 2

41 42

Philosopher 4SW(7)
38

320M

330M

325M

335M

340M

SP(7)
43

SP(7) E
39 40

Keyboard Interrupt

Figure 7. An example execution of dinning philosopher programin which GI ≠ GS (i.e., the partial orderings are different after
the 38th event)
Instrumented
23.0M

23.5M

24.0M

24.5M
R SW(4)

Barber

28 29

25.0M
SP(4) Serve the three
customers
30

Customer 5

≈

A chair available
for him

32.5M

SW(4)

SP(2)

67

68

Simulation
23.0M

23.5M

24.0M

Barber

24.5M

25.0M
R SW(4)SP(4)
31 32

Customer 5

SW(4)
28

33

SP(2) SP(4)

Three available chairs taken
A chair NOT available for him

29 30

Keyboard Interrupt

Figure 8. An example execution of sleeping barber program where GI ≠ GS after the 38th event. In the simulation,
customer5 does not get haircut while he does in the instrumented program

6

CONCLUSIONS

Often the most important metric in the dynamic analysis of
multi-threaded programs is the overhead of instrumentation since
researchers are aware of the potential probe effect caused by the
overhead. However, to the best of our knowledge, no research has
proposed a way to detect any changes in program execution when
the programs are instrumented. In this paper, we model the
execution of multi-threaded program according to the happenedbefore ordering of global events. Using the trace of event function
invocations and OS activities, a simulation-based analysis is
presented to detect if the partial order of the happened-before
relation is altered by instrumentation. The experiments of two
simple applications running on VxWorks demonstrate how
instrumentation overhead can lead to changes in the timestamp
ordering and in the partial ordering of the executions.
In this paper, we only consider single core machines with
priority-based preemptive scheduling. As a further step, our
analysis can be extended for multi-core systems in which thread
migration and multiprocessor scheduling should be considered.
Also it would be interesting to attempt a hardware-assisted online
detection mechanism for potential probe effect. Then, remedy
actions, such as synchronization operations, can be inserted to
ensure deterministic event ordering

ACKNOWLEDGMENT
This work was supported in part by the NSF I/UCRC Center
for Embedded Systems, and from NSF grant #0856090.

REFERENCES
[1]
[2]

[3]

[4]

[5]

J. Gait, “A probe effect in concurrent programs,” Software
Practice and Experience, 16(3), pp: 225-233, 1986.
D. Kranzlmüller, R. Reussner, and C. Schaubschläger,
“Monitor Overhead Measurement of MPI Applications with
SKaMPI,” Proceedings of the 6th European PVM/MPI Users'
Group Meeting on Recent Advances in Parallel Virtual
Machine and Message Passing Interface, pp: 43-50, 1999.
L. Lamport, “Time, clocks, and the ordering of events in a
distributed system,” Communications of the ACM, 21(7), pp:
558-565, 1978.
A.D. Malony, D.A. Reed, and H.A.G. Wijshoff, “Performance
Measurement Intrusion and Perturbation Analysis,” IEEE
Transactions on Parallel and Distributed Systems, 3(4), pp:
433-450, 1992.
F. Wolf, A.D. Malony, S. Shende, and A. Morris, “TraceBased Parallel Performance Overhead Compensation,”
Proceedings of the International Conference on High
Performance Computing and Communications, pp: 617-628,
2005.

[12]
[13]
[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]
Figure 9. Examples of the partial order graphs from the execution of
Sleeping Barber program

[22]
[6]

T. J. LeBlanc and J. M. Mellor-Crummey, “Debugging parallel
programs with instant replay,” IEEE Transactions on
Computers, 36(4), pp: 471-482, 1987.
[7] K. D. Bosschere, M. Ronsse, and M. Christiaens, “Debugging
shared memory parallel programs using record/replay,” ACM
Future Generation Computer Systems, 19(5), pp: 679-687,
2003.
[8] M. Ronsse, M. Christiaens, and K.D. Bosschere, “Cyclic
debugging using execution replay,” Proceedings of the
International Conference on Computational Science, pp: 851860, 2001.
[9] S. V. Adve, “Data races are evil with no exceptions: technical
perspective,” Communication of the ACM, 53(11), pp: 84,
2010.
[10] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. E.
Anderson, “Eraser: A dynamic data race detector for multithreaded programs,” ACM Transactions on Computer Systems,
15(4), pp: 391–411, 1997.
[11] C. Flanagan and S. N. Freund, “FastTrack: efficient and
precise dynamic race detection,” Proceedings of the ACM

[23]

[24]

[25]
[26]
[27]

[28]

SIGPLAN conference on Programming language design and
implementation, pp: 121-133, 2009.
DRD, Valgrind-3.8.1. http://valgrind.org/.
Intel Inspector XE 2013. http://software.intel.com/en-us/intelinspector-xe.
T. Bergan, J. Devietti, N. Hunt, and L. Ceze, “The
deterministic execution hammer: How well does it actually
pound nails?,” The Second Workshop on Determinism and
Correctness in Parallel Programming, 2011.
M. Olszewski, J. Ansel, and S. Amarasinghe, “Kendo:
Efficient Deterministic Multithreading in Software,”
Proceedings of the 14th international conference on
Architectural support for programming languages and
operating systems, pp: 97-108, 2009.
T. Liu, C. Curtsinger, and E. D. Berger, “DTHREADS: Efficient
Deterministic Multithreading,” The 22nd ACM Symposium on
Operating Systems Principles, pp: 327-336, 2011.
Kai Lu, Xu Zhou, Tom Bergan, and Xiaoping Wang, “Efficient
Deterministic Multithreading Without Global Barriers,” The
19th ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming, pp: 287-300, 2014.
Y. Lee, Y. Song, R. Girme, S. Zaveri, and Y. Chen, “Replay
Debugging for Multi-threaded Embedded Software,”
Proceedings of IEEE/IFIP 8th International Conference on
Embedded and Ubiquitous Computing, pp: 15-22, 2010.
D. Lee, B. Wester, K. Veeraraghavan, S. Narayanasamy,
Satish, P. Chen, et al., “Respec: efficient online multiprocessor
replay via speculation and external determinism,” Proceedings
of the international conference on Architectural support for
programming languages and operating systems, pp: 77-90,
2010.
N. Froyd, J. Mellor-Crummey, and R. Fowler, “Low-overhead
call path profiling of unmodified, optimized code,”
Proceedings of the 19th annual international conference on
Supercomputing, pp: 81-90, 2005.
X. Zhuang, M. Serrano, H. Cain, and J. Choi, “Accurate,
efficient, and adaptive calling context profiling,” Proceedings
of the 2006 ACM SIGPLAN conference on Programming
language design and implementation, pp: 263-271, 2006.
T. Moseley, A. Shye, V. Reddi, D. Grunwald, and R. Peri,
“Shadow Profiling: Hiding Instrumentation Costs with
Parallelism,” Proceedings of the International Symposium on
Code Generation and Optimization, pp: 198-208, 2007.
Q. Zhao, I Cutcutache, and W. Wong, “PiPA: Pipelined
profiling and analysis on multicore systems,” ACM
Transactions on Architecture and Code Optimization, 7(3), pp:
13:1-13:29, 2010.
M. Kim, H. Kim, and C. Luk, “SD3: A Scalable Approach to
Dynamic Data-Dependence Profiling,” Proceedings of the
2010 43rd Annual IEEE/ACM International Symposium on
Microarchitecture, pp: 535-546, 2010.
VxWorks Kernel Programmer's Guide 6.8.
Linux Test Project, http://ltp.sourceforge.net/.
P. Arafa, H. Kashif, and S. Fischmeister, “DIME: time-aware
dynamic binary instrumentation using rate-based resource
allocation,” Proceedings of the Eleventh ACM International
Conference on Embedded Software, Article No. 25, 2013.
S. Fischmeister and P. Lam, “Time-Aware Instrumentation of
Embedded Software,” IEEE Transactions on Industrial
Informatics, 6(4), 2010.

Scheduling Techniques for Reducing Leakage Power in Hard Real-Time Systems
C. M. Krishna
Electrical & Computer Eng. Dept.
University of Massachusetts
Amherst, MA 01003, USA
krishna@ecs.umass.edu

Yann-Hang Lee, Krishna P Reddy
Dept. of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5406, USA
{yhlee, kpreddy}@asu.edu

nm [25]. Some forecasts predict a five-fold increase in
leakage power per technology generation [26]. At this
pace, the fraction of leakage power consumption could
soon equal that of dynamic power.

Abstract
Modern embedded systems are often severely resourceconstrained. In current research, the reduction of
dynamic power has been the focus. However with
increased chip speed and density in submicron scale, the
static (leakage) power consumption has become an
increasingly significant fraction of the total. Indeed, a
five-fold increase in leakage power per technology
generation has been observed. At this pace, leakage
power could soon equal dynamic power. In this paper we
investigate scheduling policies to reduce leakage power
in real-time systems. We show that with simple scheduling
techniques, overall leakage energy can be reduced by an
order of magnitude.

Recently there have been new architectural techniques to
control leakage power [13][14][15]. These techniques
shut off units that are likely to remain idle for a
substantial time. For instance, a significant reduction in
leakage power can be achieved by disabling cache blocks
dynamically. The generational behavior of the cache lines
is taken advantage of in [13]. Cache lines typically have a
flurry of activity when first brought into use and then
have a period of dead time before they are evicted. The
authors devise different effective techniques to calculate
the dead times so that they can put the cache lines to
sleep. This is due to the spatial locality of reference in
streams of code. In [16], the authors discuss adaptive
schemes, which exploit the different voltage states the
cache lines can be in while still preserving the state of the
cache lines. Their approach is similar to the DVS-based
schemes.

1. Introduction
Reducing power dissipation has become an important
issue in the design of modern day embedded systems. The
design of such systems often involves considering power
as a first-class design constraint. Circuit-level and
architectural techniques remain as the primary approaches
to achieving this goal. Power dissipation in these CMOS
based circuits is mainly due to dynamic power
dissipation, which arises due to switching of the
transistors when performing computation. Dynamic
Voltage Scaling (DVS) [2][5][6][7] techniques have been
designed to exploit the circuit characteristics of CMOS
circuits to reduce energy dissipation by lowering the
supply voltage and clock frequency. [1][3][4] are some of
the papers that deal with techniques when the applications
are time-constrained and deadlines have to be met.

Various clock-gating techniques have been used to
control leakage current. In addition to voltage clock
scaling, IBM’s PowerPC 405LP includes softwarecontrolled multiple clock frequency domains to groups of
IP blocks in this SOC platform [17]. The standby power
management provides freeze, hibernation, and cryo modes
in which different architectural and software states are
saved and various restore delays are incurred. For
instance, using standard LSSD scan, register content is
saved into or restored from external non-volatile storage
in cryo mode. The restore time can be reduced to less than
200µs.

When not switching, CMOS circuits still dissipate
leakage (static) power, which is due to the sub-threshold
current that flows through the transistors. With advances
in technology, static power consumption is becoming an
ever more important fraction of the total power
consumed. For instance, the projected Ioff is 3 nA per
micron for 180 nm, rising to 456 nA per micron for 35

Recent processors for embedded systems like the Intel®
PXA250 applications processor [18] have automatic clock
gating. The chip has several million transistors but not all
of them are used all the time. The chip aggressively works
to temporarily shut down elements that aren’t being used

1

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

simulation results and show the trade-offs between the
various system parameters that influence the overall
energy savings of the system. Finally in Section 5 we
draw our conclusions and suggest future ideas to integrate
our schemes with the DVS based schemes.

by gating off, or disabling, their input. Transistors that
aren’t switching don’t use energy, so the PXA250
processor attempts to keep the entire chip as still as
possible. Major portions of the chip can be idle for one
clock cycle (2.5 ns) and fully active the next. All these
techniques reduce the leakage current. However, there
are still cases where large reductions in leakage power
can only be obtained by putting all of the components into
sleep mode simultaneously. Note that there will be
various overheads incurred in putting a component into
sleep mode and wake up to continue the computation.
Examples include the save and restore internal status,
cache write-back and filling, and component
synchronization. The decision to enter a sleep mode can
only be made when we know there will be an idle period
of significant length. Thus, the reduction in energy saving
can only be realized if the task behavior is already known
and hence can be taken advantage of by the task
scheduler. Taking full advantage of the low-power
hardware features of the board requires software control,
since such features permeate all levels -- from low-level
kernel routines and device drivers, all the way to
application programs. A power conscientious design of
software by keeping in mind the circuit level components
will dramatically reduce power consumption.

2. Power Modeling
2.1. Problem Background
CPU chips are becoming more densely packed with
transistors. In CMOS circuits the current major source of
power dissipation is “dynamic” or switching power,
which is proportional to the square of the supply voltage.
Hence many techniques have been devised to decrease
power consumption by decreasing the supply voltage.
These techniques, when applied to submicron and deepsubmicron regimes, have led to static power consumption
representing an increasing fraction of the total.

2.2. Software Controlled Leakage Power (LP)
In order to reduce the leakage power in a real-time
embedded system we modify existing EDF and RM
scheduling schemes. In the classical real-time model,
tasks arrive periodically. Each task τi is modeled as cyclic
processor activity characterized by two parameters, Ti and
Ci, where Ti is the minimum inter-arrival time between
two consecutive instances and Ci the worst case execution
time of task τi. The task needs to be completed by its
deadline Di which is equal to the end of its current period.
The job of the real-time scheduler is to guarantee that the
tasks will receive enough processing cycles to complete
each invocation before the deadline expires. Tasks are
assigned priorities and executed according to their
priorities. Such a scheduler guarantees that tasks will
meet their deadline given that the task set passes a
schedulability test and no task exceeds its specified worstcase computation bound.

In this paper we propose software-controlled power
management techniques to reduce leakage current in hard
real-time systems. In particular, we incorporate the
processor idle or sleep mode techniques into the two
most-studied real-time schedulers, the Earliest-DeadlineFirst (EDF) and Rate Monotonic (RM) algorithms
[9][22][23][24]. The basic goal of these strategies is to
put the CPU into idle or sleep mode for a duration that
can save overall power by reducing the leakage power.
The proposed strategies delay the task executions to as
late as possible and thereby group inter-task idle periods.
In this way the processor can idle for a longer period with
a smaller number of power transitions. In the past,
various shutdown policies have been proposed (see, e.g.,
[19][20]). The policies are based on either timeout
mechanisms or stochastic methods, which cannot be
applied to real-time systems due to the inherent
uncertainty in these policies. Our techniques, by contrast,
are specifically designed for hard real-time systems. The
proposed techniques are very effective in cases when the
processor is highly underutilized. We show detailed
simulations of the energy-conserving nature of our
algorithms.

The RM scheduler assigns priorities to tasks based on
their periods: the shorter the period, the higher the
priority. The EDF scheduler dynamically schedules tasks
by their deadlines. It executes a ready task with the
earliest deadline first. The leakage power control
algorithms delay the task execution to as late as possible
so as to consolidate the CPU idle period and execute as
many tasks as possible continuously once the busy period
starts. This way we can group many of the inter-task idle
times, reducing the number of idle periods and increasing
the duration of individual idle periods. This reduces the
number of times the CPU is brought into, and out of,
sleep mode, thereby reducing the overhead in terms of
energy and timing costs for bringing the processor from

This paper is organized as follows. In the next section we
describe in detail the basic idea behind the proposed
algorithms and how we can modify the EDF and RM
schedulers to incorporate our ideas. Section 3 presents the
algorithms in detail and in Section 4, we present the

2

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

normal operational mode to sleep mode and vice versa. In
the following sections we consider the leakage power
reduction techniques using the modified EDF and fixed
priority schedulers. In both these techniques we assume
that task deadlines equal their respective periods, tasks are
independent, and that pre-emption overhead is negligible.
The power manager makes the decision when to enter/exit
sleep mode, and is active all the time. In our model,
special-purpose simple hardware like an FPGA controls
the power manager when the processor is in sleep mode
and can determine when the CPU has to wake up so that
no deadlines are missed. Incoming task interrupts are
controlled by an interrupt controller, which contains the
basic functionality to process the interrupt and store
relevant data to be passed on to CPU when it wakes up.

n

∑C

Power

Control

under

n

∑

/ Ti + ( Ck + δ k ) / Tk + ( C j + ∆ j ) / T j = 1, (3)

Ci
i =1,i ≠ k , j

EDF

where the value δk is the elapsed time by which τk has
been delayed before task τj arrives. The busy cycle will be
delayed for an interval ∆j after the instance of τj arrives. It
could be the case that additional task instances with
earlier deadlines than τj arrive before this delay of the
busy instance has expired. Let τl be any such a task that
has been delayed last before the busy cycle starts. Then
the amount ∆l by which this task instance can be delayed
is given by

We modify the existing EDF scheduling technique and
propose a new leakage-control EDF (LC-EDF) technique
to reduce the leakage power. In the classical EDF
scheduling model, a set of n tasks can be scheduled if the
following condition holds
n

∑Ci / Ti ≤ 1.

(2)

If there are no other arrivals during the delay interval, we
basically view the delay as a part of task τk’s execution
time. The well-known EDF utilization bound suggests
that all tasks will continue to meet their deadlines. On the
other hand, two cases arise when an other task instance
arrives before the beginning of the busy cycle. If an
arriving task has an absolute deadline greater than that of
task τk, then the arriving task waits as in the original EDF
scheduling. It is executed in deadline order along with the
tasks whose deadlines are greater than that of task τk once
the busy cycle begins. If an arriving task τj has a deadline
earlier than that of τk, then a new delay time ∆j for τj is
calculated as follows:

3. Leakage Power Control Schedules
3.1. Leakage
Scheduling

/ Ti + (C k + ∆ k ) / T k = 1 .

i

i =1
i≠ k

(1)

i =1

n

That is the total utilization of the task set must be less
than or equal to one. In most of the cases utilization is
strictly less than one and hence this extra utilization can
be used for delaying the tasks while still guaranteeing
schedulability.

∑Ci / Ti + ∑( Ci + δ i ) / Ti + ( Cl + ∆l ) / Tl = 1,

i =1,i∉lp( l ),i≠l

(4)

i∈lp( l )

and lp(l) is the set of indices of tasks that have arrived
before τl has arrived and having deadlines earlier than the
task instances being delayed at their arriving time. As
shown in Figure 1, to determine when the busy cycle
starts, we need to keep track of the accumulated δi/Ti to
calculate the new value of ∆l. We can consider δ as the
pseudo execution time of a task where the task delay time
is tantamount to execution time if the utilization is one.
With this approach, a busy period is delayed for an
interval given by sum of the δ’s of all the tasks preempted
plus the current delay of the earliest deadline task just
before the busy cycle starts. We show next that this
algorithm can achieve a bounded idle time.

We make the following modification to the EDF
scheduler. When the processor is idle, an arriving instance
of task τk that has the earliest deadline can extend the idle
period by an interval ∆k, i.e., the execution of the task(s)
is delayed by an interval ∆k. When this extended idle
period expires, the task begins to execute in addition to all
the other pending tasks, along with any new tasks that
have arrived even if they can be delayed. The idea, as
mentioned above, is to increase the duration of the idle
and busy periods to the extent possible, while still
meeting all task deadlines. The interval ∆k must be
calculated carefully such that all tasks to be executed in
the subsequent busy period can meet their deadlines. If an
instance of task τk is the first arriving task in an idle
period (and would now have been executed in the original
EDF schedule), the delay interval ∆k in the modified LCEDF can be computed as follows:

δj

δk

∆'l

…

…
k

3

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

j

l

Dl

Dj

Dk

improve the response times of the soft tasks. It exploits
the spare capacity present due to sporadic tasks not
arriving at their maximum rate.

Figure 1. Accumulated delays under EDF scheduling

Dual Priority scheduling requires three distinct priority
run queues: upper, middle and lower. Any task in the
upper queue has higher priority than the tasks in the
middle and any task in the middle queue has higher
priority than tasks in the lower queue. Each periodic task
τi has two priorities, one when it is in the lower run queue
and one when it is in the upper run queue, denoted by Pi l and Pi u , respectively. Priorities in both bands are

Lemma: Any idle period in LC-EDF is greater than or
equal to


lk = ( 1 −
k = 1 , 2 ,..., n

min

n



i =1



∑ C i / T i )T k 

(5)

Proof: Consider the situation as shown in Figure 1. Let τl
be the last task delayed before the busy cycle starts. Let it
arrive at instant l after τk, τj ,…etc., have been delayed by
their respective δ’s. And let ∆′l be amount τl can be
delayed. The maximum amounts any τn can delay when it
is the only task for execution or the last one before a busy
cycle starts, as computed from Equations (2) and (4), are
given by

assigned according to the rate monotonic assignment.
Each hard task upon release assumes its lower band
priority; however, at a fixed time offset from its release,
the task is promoted to the upper band. It may, of course,
be pre-empted by other tasks which have higher, lower
band priorities. It may also be pre-empted by any soft task
executing at a priority level in the middle or upper band.
Thus, soft tasks execute in preference to hard tasks, which
are yet to undergo priority promotion. Response time
analyses show that the optimal priority promotion time Yi
for each hard task is given by

n

∆ l = ( 1 − ∑ Ci / Ti )Tl
i =1

∆' l / Tl +

n

∑δ i / Ti = (1 − ∑Ci / Ti )

i∈lp( l )

We need to show that ∆' l +

Yi = Di − Ri

i =1

∑ δ i > ∆ l . Since τ

l

where Ri is the worst case response time of a task
scheduled under normal fixed priority schemes and the
promotion time computed is in fact a lower bound on the
optimal priority promotion time.

has

i∈lp ( l )

the earliest deadline and is the one arrived later that the
instance of τi for i∈ lp(l), we have Tl<Ti and

∆' l / Tl +

i∈lp ( l )

⇒ ∆l +
'

We design an effective shutdown policy based on the
above dual priority approach by introducing a pseudo soft
task instead of the real soft tasks. This pseudo soft task is
in fact the idle time. This task keeps running all the time.
So, whenever a task is released, it is not executed until its
turn comes after it has been promoted to the upper run
queue. In our approach, whenever a task is promoted to
the upper run queue, we simultaneously promote all the
pending tasks in the lower run queue to the upper run
queue. This is in contrast to the dual priority scheduling
scheme where each task waits till it promotion time after
it has been released. This way we can execute all the
pending tasks when any one task gets promoted to the
upper run queue and keep the processor busy. Also if any
new task arrives when a busy cycle has already started, we
promote it to the upper run queue, as in LC-EDF. The
time to sleep is the minimum of all the promotion times
of the tasks in the lower run queue. As can easily be seen,
the minimum idle period with this approach will be

n

∑ δ i / Ti = ( 1 − ∑ C i / Ti ) = ∆ l / Tl
i =1

∑δi > ∆l

i∈lp ( l )

which completes the proof. QED.
The algorithm will be most useful if we have many
relatively short inter-task idle periods that they can be
grouped to form a few long idle periods. This reduces
the overhead involved in putting the CPU to sleep and
then waking it up again.

3.2. Leakage Power Control with Fixed Priority
Scheduling
We can also delay the execution of a task under fixed
priority scheduling. Rate Monotonic (RM) is one such
optimal fixed priority scheduling algorithm. Here, the
tasks are assigned priorities in the decreasing order of task
inter-arrival times. We extend the Dual-Priority (DP)
scheduling model proposed in [21]. This technique
schedules tasks with soft deadlines together with periodic,
sporadic and adaptive tasks with hard deadlines so as to

idle

min

= min {Y i : i = 1 ... n }

Our simulations show that this type of scheduling does
increase the average duration of the individual idle
periods significantly.

4

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

time is spent in sleep mode and with minimal number of
switches to sleep mode. In sleep mode PXA250 dissipates
almost negligible power, 180 µWatts whereas in idle
mode it dissipates 555 mWatts of power.

4. Simulations
In our model the processor is switched to sleep mode
when we know it can idle for more than some threshold
value. In this state the processor consumes negligible
power. We have written a simulator in C, which takes in
various system parameters to quantify the leakage power
consumed. In this study we concentrate on leakage
power reduction in CPUs. To do this we modeled the
recent Intel® PXA250 Applications Processor power
modes and characteristics in our simulator. The four
operating modes provided in this processor are real, turbo,
idle and sleep mode. Run mode is the normal execution
mode. Each of the other power modes must be entered
from run mode. Turbo mode is designed for use when
requiring peak processing is needed and is not considered
in our simulation. In idle mode, only the CPU clock is
stopped; all other on-chip resources are active. Sleep
mode significantly reduces applications processor leakage
power consumption. In this mode, most of the internal
processor state is not clocked and, hence, not preserved.
Since the internal processor state is completely lost, the
critical states must be preserved in memory before
entering this mode. In PXA250, the power manager’s
scratchpad registers are used to store critical states. Thus
entry into the sleep mode must be done gracefully under
operating system control. Before entering sleep mode the
data cache and the mini-data cache must be flushed, and
the write buffer drained. All direct memory access must
also cease.

In our simulation, we have applications with execution
times equal to their WCETs, whose values are selected
from the range 1 ms to 100 ms. Task periods are selected
from the range 10 ms to 125 ms [9][10][11]. The
PXA250 has a clock of 248MHz at 1.3V and has an
assumed latency overhead of 300 µs to enter/exit the sleep
mode. Also the revival energy required to enter/exit sleep
mode is taken as 600µJ, which is assumed to be the
energy consumed to save CPU state and store the contents
in an external memory. This amounts to about 1.1 ms of
energy dissipated in idle mode. In our simulations we
enter into sleep mode, only if the energy savings by
staying in the sleep mode exceed that of costs accrued by
revival energy and latency overheads. Figure 2 shows the
percentage increase in average idle time for different
utilizations using LC-EDF and LC-DP. We see from the
plots that there is a considerable increase in idle periods
when using LC-EDF and LC-DP. Average idle periods for
utilizations between 0.1 and 0.9 ranged from 14.8 ms to
5.65 ms for LC-EDF and LC-DP whereas the average idle
periods for the same utilization for EDF ranged from 6.68
ms to 4.2 ms. As we can see, using the LC-EDF and LCDP the length of the idle periods has more than doubled at
low utilizations. Note that only the schedulable task sets
(based on response time analysis) are used in the
simulation of LC-DP scheme.

In our simulation we have included the energy overhead
to save the state before entering sleep mode. The
scheduler also needs to consider the time needed to enter
and exit this mode. If the idle period savings are greater
than the overhead costs, a change to sleep mode is
invoked. The total idle energy E dissipated while the CPU
is not performing any computation is given by

150

% Increase in average idle interval over EDF

LC-EDF
LC-DP

E = time_in_sleep × sleep_power +
revival_energy × No. of switches + (6)
time_in_idle × idle_power
As we can see, E is the sum of the sleep mode power
dissipation, the total energy cost of switching, and the idle
mode power dissipation. In the idle mode, the power
dissipated is mainly due to the leakage current. The power
dissipated in sleep mode is order of 103 [18] times less
than that in idle mode. Also as mentioned before, there is
a latency overhead in both entering and exiting the sleep
mode. This can get very long depending on the type of the
peripherals attached to the system as entering sleep mode
involves storing the states of these devices as well. As
shown in Equation (6), less energy is consumed if more

100

50

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Utilization

Figure 2. Percentage increase of the average length of
idle periods over EDF.

5

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

1

1

1
EDF
EDF-S
LC-EDF
LC-DP

0.9

0.8

Idle Energy (Normalized)

Idle Energy (Normalized)

0.8

0.7

0.6

0.5

0.4

0.3

0.7

0.6

0.5

0.4

0.3

0.2

0.2

0.1

0.1

0

EDF
EDF-S
LC-EDF
LC-DP

0.9

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0

1

0

0.1

0.2

0.3

0.4

Utilization

0.5

0.6

0.7

0.8

0.9

1

Utilization
a = 0.3

Figure 3. Energy consumption during idle periods
1

With the similar simulation setting, we have plotted in
Figure 3, energy E consumed in different approaches as
calculated from Equation (6). The energy E is normalized
to energy due to energy dissipated in idle mode if the
utilization is zero. The EDF plot shows the highest idle
mode energy dissipated when the processor only goes to
idle mode for different utilizations. The EDF with sleep
mode switched on (EDF-S) shows the energy consumed
in idle mode when we know the next arrival instant of the
tasks. If the idle time is long enough to enter/exit sleep
mode between two tasks, then we enter into sleep mode.
The EDF-S plot shows that at low utilizations the system
was able to spend large amount of idle time in sleep mode
(28% of idle time). The other plots show LC-EDF and
LC-DP idle period energy consumption. Our first
observation was that under low utilizations there was
considerable reduction in the number of idle periods to
enter sleep mode by more than half and the system was
able to stay in sleep mode during most of the idle period.
While using LC-EDF and LC-DP we noticed that at
utilizations between 0.1 - 0.4 the system could stay in
sleep mode entirely to 70% of the idle time available. As
the utilization becomes more than 0.7 we observed that it
was not possible to put the system in sleep mode due to
large latency of sleep mode transitions and increase in
revival costs. Figure 4 shows when the execution times of
tasks are uniformly distributed between a × WCET to
WCET where ‘a’ is the parameter that controls actual
execution time. Clearly, using the approaches discussed in
this paper, energy efficient idle time scheduling can be
achieved by increasing the time spent in sleep time
considerably without violating timing constraints.

EDF
EDF-S
LC-EDF
LC-DP

0.9

Idle Energy (Normalized)

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Utilization
a = 0.8

Figure 4. Energy consumption during idle periods when a
= 0.3 and 0.8.

processor to be designed with deep sub-micron
technology of 70nm. In this model the peak operating
frequency is 240 MHz, operating at V dd = 1.3v.
Leakage power is assumed to be 25 % of the total power.
The dynamic power at this voltage setting is normalized
to 1. The voltage settings at different frequencies are
calculated using the formula for delay[1] where the value
for VTH is taken to be 0.14[25] and is assumed constant.
Once the voltage at some frequency is calculated we can
easily calculate dynamic power and leakage power since
they are proportional to V dd2 and V dd respectively.
Average dynamic power is defined to be dynamic
switching power times the fraction of time the processor
is switching. Similarly the average leakage power is
defined to be leakage power times the fraction of time the
processor is not in sleep mode. Figure 5 shows how
average dynamic and leakage power varies at different

To study the impact of our techniques on futuristic
technologies we have considered another model of a

6

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

[2]

voltage settings. The utilization for the task set at supply
voltage V dd = 0.58v is taken as 0.82. As the voltage
increases utilization for the task set decreases. We see
from the figure that at high voltages and low utilizations,
leakage power is dominant and greater than average
dynamic power and definitely needs to be curbed as much
as possible. When LC-EDF is used the average leakage
power dramatically falls especially at high voltages.

[3]

[4]
0.35
Dynamic
Leakage - No Sleep Mode
Leakage - LC-EDF

[5]

Average Power

0.3

0.25

[6]
0.2

[7]

0.15

0.1
0.5

0.6

0.7

0.8

0.9

1

1.1

1.2

1.3

[8]

1.4

Voltage (V)

Figure 5. Average power vs. Voltage
[9]

5. Conclusion And Future Directions
In this paper we have shown effective software
techniques to schedule tasks so as to reduce leakage and
idle-mode power in hard real-time systems. We have
shown by our simulations that by delaying the task
executions we can have large idle periods and thereby put
the system into sleep mode where power consumed is
negligible compared to the idle mode. We have not
considered the implications of DVS mechanisms when
coupled with our scheduling techniques. Ideally one
would like to execute at a low voltage as much as possible
and go into sleep mode with reduced sleep mode
transitions during the idle periods. We are currently
expanding this work to investigate these issues and build
a functional prototype to validate our methods.

[10]

[11]

[12]

[13]

Acknowledgement
This work is supported in part by the National Science
Foundation under grants EIA-0102696 and EIA-0102539.

[14]

6. References
[1]

Y. Lee and C. M. Krishna. Voltage clock scaling for
low energy consumption in real-time embedded
systems. The 6th Int'l Conf. on Real-Time
Computing Systems and Applications, Dec 1999.

7

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

T. Burd and R. Brodersen. Energy efficient CMOS
microprocessor design. Proceedings of the 28th
Annual Hawaii International Conference on System
Sciences, Jan 1995.
C. M. Krishna and Y. Lee. Voltage clock scaling
adaptive scheduling techniques for low power in
hard real-time systems. The 6th Real-Time
Technology and Applications Symposium, May
2000.
P. Pillai and K. G. Shin. Real-Time Dynamic
Voltage Scaling for Low-Power Embedded
Operating Systems. Proc. of the 18th SOSP, 2001.
K. Flautner, S. Reinhardt, and T. Mudge. Automatic
performance setting for dynamic voltage scaling.
Proceedings of the 7th Conference on Mobile
Computing and Networking, Jul 2001.
M. Weiser, B. Welch, A. Demers, and S. Shenker.
Scheduling for reduced CPU energy. The 1st Symp.
On Operating Systems Design and Implementation,
Nov 1994.
T. Pering, T. Burd, and R. Brodersen. The
simulation of dynamic voltage scaling algorithms.
Symp. On Low Power Electronics, 1995.
D. Katcher, H. Arakawa, and J. Strosnider,
Engineering and analysis of fixed priority
schedulers. IEEE Trans. on Software Eng, Sep
1993.
A. Burns, K. Tindell, and A. Wellings. Effective
analysis for engineering real-time fixed priority
schedulers. IEEE Trans. on Software Eng, May
1995.
C. Locke, D. Vogel, and T. Mesler. Building a
predictable avionics platform in Ada: case study.
Proc. IEEE Real-Time Systems Symposium, Dec.
1991.
J. Liu, J. Redondo, Z. Deng, T. Tia, R. Bettati, A.
Silberman, M. Storch, R. Ha, and W. Shih. PERTS:
A prototyping environment for real-time systems.
Tech. Rep. UIUCDCS-R-93-1802, University of
Illinois, 1993.
N. Kim, M. Ryu, S. Hong, M. Saksena, C. Choi, and
H. Shin. Visual assessment of a real-time system
design: a case study on a CNC controller. Proc.
IEEE Real-Time Systems Symposium, Dec 1996.
S. Kaxiras, Z. Hu, M. Martonosi. Cache Decay:
Exploiting Generational Behavior to Reduce Cache
Leakage Power. The 28th International Symposium
on Computer Architecture, Jun 2001.
L. Li, I. Kadayif, Y-F. Tsai, N. Vijaykrishnan, M.
Kandemir, M. J. Irwin, and A. Sivasubramaniam.
Leakage energy management in cache hierarchies.
Proc. of the Eleventh International Conference on
Parallel Architectures and Compilation Techniques,
Sep 2002.

[15] S. Kaxiras, Z. Hu, G. Nalikar, R. McLellan. CacheLine Decay: A Mechanism to Reduce Cache
Leakage Power. Workshop on Power-Aware
Computer Systems, Nov 2000.
[16] K. Flautner, N.S. Kim, S. Martin, D. Blaauw, and T.
Mudge. Drowsy Caches: Techniques for Reducing
Leakage Power. The 29th Annual International
Symposium on Computer Architecture, May 2002.
[17] G. Carpenter, “Low Power SOC for IBM’s
PowerPC Information Appliance Platform,”
http://www.research.ibm.com/arl/projects/papers/4
05LP.pdf.
[18] Intel® PXA250 and PXA210 Applications
Processors Design Guide, Feb 2002.
[19] Y. Lu, T. Simunic, and G. De Micheli. Software
controlled power management. Proceedings of the
IEEE Hardware/Software Co-Design Workshop,
May 1999.
[20] Y. Lu, L. Benini, G. De Micheli. Requester-Aware
Power Reduction. International Symposium on
System Synthesis, Sep 2000.
[21] R. Davis and A. Wellings. Dual priority scheduling.
Proceedings Real-Time Systems Symposium, 1995.
[22] C. M. Krishna and K. G. Shin. Real-Time Systems.
New York: McGraw-Hill, 1997.
[23] J. Lehoczky, L. Sha, and Y. Ding. The rate
monotonic
scheduling
algorithm:
exact
characterization and average case behavior.
Proceedings of the IEEE Real-Time Systems
Symposium, 1998.
[24] L. Liu, and J. W. Layland. Scheduling algorithms
for multiprogramming in a hard real-time
environment. Journal of ACM, 1973.
[25] D. Sylvester and H. Kaul. Power-Driven Challenges
in Nanometer Design. IEEE Design and Test of
Computers, Nov/Dec 2001.
[26] S. Borkar. Design Challenges of Technology
Scaling. IEEE Micro, 19(4), 1999.

8

Proceedings of the 15th Euromicro Conference on Real-Time Systems (ECRTS’03)
0-7695-1936-9/03 $17.00 © 2003 IEEE

529

IEEE TRANSACTIONS ON COMPUTERS, VOL. c-33, NO. 6, JUNE 1984

Error Detection Process - Model, Design, and
Impact on Computer Performance
KANG G. SHIN,

SENIOR MEMBER, IEEE, AND

YANN-HANG LEE,

Its

STUDENT MEMBER, IEEE

since it may cause a catastrophe in the event that more than
one latent fault becomes active at the same time. Bavuso
et al. investigated the problem of the latent fault and proposed experiments to measure the time interval between the
moments of fault injection and error detection [1]. Their
study indicated that a significant proportion of faults is not
detected even after many iterations of a process.
When an error is generated, it is desired that the error
detection mechanisms associated with the system identify it
immediately. Nevertheless, some errors may not be captured
by error detection mechanisms upon occurrence and then
spread as a result of the subsequent flow of information.
Thus, the damage caused by an error will propagate until it
producing an unreliable result, and (2) estimate the loss of is detected and handled appropriately. See Fig. 1 for a typical
computation due to fault and/or error. The former can be used as error detection process. The delay between the occurrence of
a measure of lack ofconfidence in the computation results whereas an error and the moment of its detection, called error latency,
the latter is important to the timing analysis, particularly for real- is important to damage assessment, error recovery, and estabtime computations. Various error recovery techniques and their lishing confidence in the computation results. This delay has
associated overheads are considered for the estimation of the
computation loss which can be used for analyzing suitability for been defined by Courtois as detection time [2], [3] and by
Shedletsky as latency difference [4]. Courtois also presented
time-critical applications.
Finally, a design problem associated with the error detection results of on-line tests of the M6800 microprocessor that
process is discussed and a feasible design space is outlined.
included the distributions of detection time for certain demechanisms. Shedletsky proposed a technique to
tection
Index Terms -Computation loss, diagnostics, error detection,
evaluate the error latency based on the "fault set" philosophy
latent errors/faults, recovery methods, unreliable results.
and the probability distribution of input signals.
When error latency is significant, there is the possibility of
I. INTRODUCTION
the system putting out incorrect computation results since
D URING the past decade or so, many reliability related there may be some undetected errors at the output phase.
models for fault-tolerant computers have been pro- Also, even if the system detects all errors before the output
posed. Usually, in these models, probability distribution phase, the computation achieved during the latent period may
functions are employed to describe the occurrence of compo- already have been contaminated, and thus be useless. In
nent or system failure. Then, various measures such as practice, error latency is never zero, and in the event of an
reliability, computation capacity, performability, etc. are error the whole system is delayed by the more complicated
estimated and are then used to properly represent system recovery that is required to remove the contamination that is
performance. Such approaches, however, do not consider spread during error latency.
shortcomings in error detection mechanisms and recovery
To evaluate these two effects -the probability of producmethods. By contrast, we set out in this paper to present ing an unreliable result and the computation loss resulting
results of a study that incorporates detection mechanisms and from error - it is necessary to examine the error detection
recovery methods in system performance and reliability.
mechanisms incorporated in computer systems and their
Consider the property of a fault. An input signal to a com- respective capabilities. One may then establish a different
puter may cause the fault to induce some errors, or it may recovery strategy for the errors captured by each distinct
simply be unaffected by this fault and produce a correct detection mechanism, thus obtaining the most appropriate
output. The fault is said to be latent if it does not harm normal possible recovery performance and execution cost. To evaluoperations. The time interval between the moments of fault ate error handling capability including tradeoffs between
occurrence and error occurrence is calledfault latency. For an various detection mechanisms and recovery methods, it is
ultrareliable system, a latent fault is a considerable threat necessary to consider recovery performance and execution
Manuscript received July 20, 1983; revised December 10, 1983. This work cost, taken as a whole.
was supported in part by NASA Grant NAG 1-296.
In this paper, a model is proposed to describe error deThe authors are with the Computing Research Laboratory, Department of
Electrical and Computer Engineering, University of Michigan, Ann Arbor, MI tection processes and to estimate their influence on system
48109.
performance. In the following section, the classification,
Abstract - Conventionally, reliability analyses either assume
that a fault/error is detected immediately as it occurs, or ignore
damage caused by imperfect detection mechanisms and error
latency, namely, the time interval between the occurrence of an
error and the detection of that error.
In this paper we consider a remedy for this problem. We first
propose a model to describe the entire error detection process and
then apply the model to the analysis of the impact of error detection on computer performance under moderate assumptions.
Error latency is used to measure the effectiveness of detection
mechanisms. Due to the presence of error latency, (i) it is possible
to have undetected errors at the end of process execution making
the computation result unreliable, and (ii) even if all errors were
detected before the completion of process, it is required to apply
complicated error recovery resulting in considerable computation
loss. We have used the model to (1) predict the probability of

0018-9340/84/0600-0529$01.00 (© 1984 IEEE

5EEE

530
A

B

A

C

Duration A: A fault exists and active in the system

Duration B:

The

fault becomes inactive

Duration C: Errors exist in the system

Fig. 1. The error detection process.

properties, and associated recovery methods of error detection mechanisms are discussed. Our model is developed in
Section III. Section IV presents the evaluation of the probability of having an unreliable result as well as that of the
average computation loss. Also, the design problem of error
detection is discussed and a feasible design space is outlined.
It is assumed throughout this paper that faults in hardware
components are a potential cause of transition to erroneous
states during normal operation. We also assume for the
present study that there are no design faults in the system. An
error is defined to be the erroneous information/data resulting from fault(s).

CLASSIFICATION OF ERROR DETECTION MECHANISMS
There are various error detection mechanisms which can be
incorporated in a computer system. The basic principle of
these mechanisms is the use of redundancy in devices, information, or time. Based on (i) where they are employed,
(ii) their respective recovery methods, and (iii) performance
measures, error detection mechanisms are divided into the
following three categories.
a) Signal level detection mechanisms: Usually, the
mechanisms in this category are implemented by built-in
self-checking circuits. Whenever an error is generated by a
predescribed fault, these circuits detect the malfunction immediately, even if the erroneous signal does not have any
logical meaning. Typical methods in this category include
error detection codes, duplicated complementary circuits,
matchers, etc. The performance of these detection mechanisms is measured by the coverage, denoted by c, which is
the probability of detecting an error induced by an arbitrary
fault. It is difficult to have a perfect coverage because (i) it
is prohibitively expensive to design detection mechanisms
which cover all types of faults, and (ii) physical dependence
between function units and detection mechanisms cannot be
completely elim'inated.
Since this class of detection mechanisms detects an error
immediately upon occurrence, there is no contamination
through error propagation. This makes the subsequent recovery operations simple and effective. Two kinds of recovery
methods are suitable for this category; one is error masking,
in which redundant information is used to retain correctness,
the other is retry, in which the previous action is reexecuted.
b) Function level detection mechanisms: The deII.

c-33,

NO.

6, JuNE

1984

tection mechanisms in this category are intended to check
unacceptable activities or information at a higher level
than the previous category. Unlike the signal level detection
mechanisms, they verify system operations by functional
assertions on response time, working area, provable
computation results, etc. These detection mechanisms
can be regarded as "barriers" or "guardians" around normal
operations. After an error is generated by a fault, the resulting abnormality may grow very quickly -the "snowball
effect" [3], or "error rate phenomenon" [6] -until it hits the
barriers. Several software and hardware techniques such as
capability checking, acceptance testing, invalid op-code
checking, timeout, and the like can be applied.
The important issues for function level detection mechanisms are error isolation and damage assessment. Both issues
depend upon system structure as well as on inherent properties of the executed programs or tasks. When there are clear
cleavages between subsystems or subtasks, the effective detection assertions can be easily declared, thus permitting
greater error isolation and reducing contamination. Usually,
rollback and restart recovery methods are used to rescue
failed processes. Rollback requires state restoration such that
part of the process can be resumed. The restart method purges
the old computation and then reissues the same task to other
nonfaulty processors.
c) Periodic diagnostics: This method is usually referred to as off-line testing because the processing unit under
test cannot perform any useful task. It is composed of a
diagnostic program which supplies imitated inputs such that
all existing faults are activated and thus generate errors.
Several theoretical approaches have been proposed to determine the probability of finding an error after applying diagnostics for a certain duration (equivalent to the probability of
detecting fault as a function of test duration) [7], [8]. Simulation has also been used to study the coverage of self-testing
programs [9]. All these results have indicated that the effectiveness of the present category is a monotonically increasing
function of testing time. Since the time required for complete
testing (i.e., ensuring 100 percent coverage) is in general too
long, an appropriate policy of diagnostics is to perform an
imperfect test periodically during normal operation and perform a thorough diagnostics when the system is idle.
out

Error
F latency #

Time

TRANSACTIONS ON COMPUTERS, VOL.

III. MODEL OF ERROR DETECTION PROCESS

For analytical convenience, occurrence of a fault is usually
modeled as a Poisson process. Let MTBF be the mean time
between two successive fault occurrences. Also, let F, and
pi i = 1, 2, 3 denote the event and the probability that the
fault is transient, intermittent, or permanent, respectively.
Naturally, Pi + P2 + p3 = 1. When the classification of
faults into these three types is independent of occurrence of
faults, occurrence of event Fi can be modeled as a Poisson
process with rate pi/MTBF. Then, the following model can be
used for a separate analysis of the effects of each type of
faults.

531

SHIN AND LEE: ERROR DETECTION PROCESS

a2(t )

I7/(t

)

Note: The transitions between NF, F, FB, and E, EFB are
dependent on the type of fault.
Fig. 2. The model for error detection process.

A. Model Development
Fig. 2 shows our model of the error detection process. The
model consists of three parts: the occurrence of a fault, the
consequent generation of an error, and the detection of that
error. Since the probability of having multiple faults at any
time is small, they are excluded from the model.1 There are
six states in the model as followvs:
1) NF (nonfaulty): In this state no fault exists in the
system.
2) F (faulty): There is a fault which is active and capable
Of inducing errors, but there are no errors.
3) FR (fault-benign): There is an inactive intermittent
fault.
4) E (error): There is at least one undetected error in the
system and the fault which has caused that error is still
present.
5) EFB (error-fault-benign): At this state the intermittent
fault has become inactive or the transient fault has disappeared after it induced an error.
6) D (detection): At this state, the detection mechanisms
have identified the error in the system. To determine whether
the system has been contaminated or not, twvo substates,
called DI and D2, are included. The system will enter D1 when
the detected error has contaminated at least part of the system. On the other hand, the system enters D2 when an error
is detected before it begins to propagate through the system.
Signal level detection and diagnostics cause transitions from
F to D2. In fact, these transitions can be divided into two
steps: an existing fault induces an error, and the error is
detected immediately following its occurrence.
Let A denote the rate of occurrence of F type faults, i.e.,
A = pj/MTBF i = 1, 2, 3 when transient, intermittent, and
permanent faults are separately considered. Since intermittent faults may become inactive, a benign state has to be
'See Section V for a brief comment on the modeling of multiple faults.

included in the model. Several models of intermittent faults
have been proposed and used for testing and reliability
evaluation [101-f14] In our model, the transitions between
NF, F, FR, and between E, EFB are used to describe the
behavior of intermittent faults. For transient and permanent
faults, FR does not exist, implying that the transition rates
between F and FB, ,L and v, are zero. Similarly, for intermittent and permanent faults the rate of transition from F to NF,
r, equals zero.
Consider the process of generating errors by a given fault.
With the assumption that the signal patterns of successive
inputs are independent, Shedletsky treated the period of fault
latency as a random variable with a composite geometric
distribution for discrete inputs or cycles [8]. Using the
conceptsofinformationtheoryAgrawalpresentedaformula
to estimate the probability of inducing error [15]. For tractability we have assumed in our model an exponentially
distribited fault latency with rate a when a task is executing.
While the diagnostic program is running, the transition duration F to D2 is assumed to be exponentially distributed with
parameter t. If the diagnostic program is executed for period
t following a normal operation period t, and a process swapping period tv as shown in Fig. 3, the coverage of a single
diagnostic, denoted by 9, is equal to 1 - ewe, for
h
execution of diagnostics.
Once the system enters E, the erroneous information starts
to spread until function level detection mechanisms identify
any unacceptable result. There are two paths to D, and they
represent transition rates of /(t) and y(t), respectively. At
state E, since the fault still exists, it is possible that the fault
is captured by signal level detection mechanisms or diagnostics prior to the function level error detection. We exclude
this case from the model because the process has already
become erroneous, and the subsequent signal level detection
has no effect on this error. (Namely, a direct transition from
E to D2 is not included.) It is also possible that there are
multiple errors induced by the same fault or by an old un-

IEEE TRANSACTIONS ON COMPUTERS, VOL.

532

normal

process
swap

operation

4,VFtp
k tn"
Fig.- 3. A cycle of periodic diagnostics.

time

d7r(t)=
dt

detected error when the system is in E or EFB. The function
level detection mechanisms will recognize that the system is
erroneous regardless of which error is captured. However,
the error latency must be measured from the moment that the
first error occurs.
B. Mathematical Description of State Transitions
Let a computer system incorporate the three types of error
detection mechanisms discussed above. For notational convenience number the states NF, F, FB, E, EFB, Di, D2 with i
, 7. Then one can obtain a transition probafor i = 1, 2,
bility matrix H7.7(t) by making use of the model in Fig. 2.

H7x7(t)

-(g

+

T

+ ai(t) + a2(t))

0
0
0
0

0
0

0

0

a1 (t)

I(

-

c)a

a2(t)

tj)

if n(tn + tp + tv) < t < n(tn + t,
otherwise

0

wcc

al(t)

0

0

a2(t)

0

O

O

0

if n(t, + tp + tv) < t C n(tn + tp
otherwise

+

+

r

0

0

0

+ tn

(2a)

tv)

-(/J1

0

=

ca)

0

V

=

+ tn

=

o

0

Since the diagnostic is invoked periodically, transition rates
al(t), a2(t), (3(t), and y(t) are the following functions of time:

9(0)

it)

0

-V

v

=

'=V

0

,u + v

0
0

6,

JUNE

1984

(3)

ITO

where 7iri(t) is the probability that the system is in state i at
time t. Because of the absorbing property of Di and D2, one
can easily see that 1r6(oc) + 1v7(o) = 1.
Assume the initial state that the system begins. with is NF.
When a transient or a permanent fault occurs, the system will
enter the nonfaulty state again after either the fault disappears
or the system is reconfigured to eliminate the source of
the fault.
In case of an intermittent fault, it is possible for the system
to be in FB instead of NF even after some recovery procedures are successfully applied. For example, when the fault
becomes benign during the retry recovery, the system enters
FB. Let SI (or S2) be the event that the system is in state NF

Av

-A

NO.

Hence, the state probabilities ir(t) = [rl(t), r2(t), * ,
ir7(t)] can be obtained by solving the following differential
equation:

diagnostic

i
A ce
ogi

F

c-33,

+

,8(t))

,Ut+T
(3(t)
-(v + y(t)) 7y(t)

o
o

0
0

(1)

0
0

0

(or FB) after recovery from an intermittent fault. This probe represented by a Markov chain shown in Fig. 4
and the transition probabilities between SI and S2, denoted by
81 and 62. These transition probabilities are computed using
(3) and the corresponding recovery performance will be discussed in the next section. Note that, under S2, the same
intermittent fault will be detected by the signal level detection with probability one if it induces an error again.
A task may start execution when the system is in any

cess can

one

of NF, F, FB, E, EFB (but certainly not in D1, D2,).

Using the Markov model in [16], we can calculate (i)
the mean number of visits to state i, i = 1, 2,
5before the system is absorbed into DI or D2 for every Fj j
1, 2, 3, and (ii) the mean time interval, E[Xi Fj I j = 1, 2, 3,
during which the system stays in state i before transition
to DI or D2 takes place. Then, the probability that a task
begins execution when the system is in state i is formulated as follows:
,

(3(t)

=

if

.0
y(t)

n(t,

+ tp +

tj)

< tC

t,)

< t < n(tn + tp + tu) + tn

n(t,

+ tp + tu) + tn

otherwise'

=

if n(tn + tp +

otherwise

el

(2d)

where c is the coverage of the signal level detection; a is
the transition rate that a fault generates an error; (3 and y
represent the transition rates that the function level detection
captures errors in states' E and EFB, respectively; and n is a
positive integer.

E[Xi Fj/]I E[Xk
mi (0 F) = 1-l
O

Fj]

for i

=

1,7.2

,for i

=

69 7.

,

5
(4)

It may be possible that the active duration of an intermit-

tent fault increases every time it becomes active following its

first

occurrence.

This would imply that the transition

533

SHIN AND LEE: ERROR DETECTION PROCESS

61
1-6E

-z
Fig. 4. A Markov chain for the recovery from an intermittent fault.

rates between fault active and fault benign depend on the
duration for which an intermittent fault exists. In such a
case, the model suggested in' [17] can be used in the above
system equations.
It cannot be overemphasized that our modeling of the error
detection process is intended to evaluate the effects of various
detection mechanisms on task execu-tion. This fact is in sharp
contrast to most conventional methods in which models have
been developed and then used to estimate the system reliability or to determine the coverage of failure. For example,
in CARE III [ 14] the error propagation rate is defined by the
user and the model is applied to determine the coverage.2
Note that a transition to DI represents the detection of error
by function level detection mechanisms, whereas D2 is reachable directly from F by signal level detection mechanisms or
diagnostics. The impacts of detection mechanisms on task
execution will be reflected through (3) and the state distribution ir(t I Fj).

cotnputation result to its successors at the end of execution.
A serious situation, namely the propagation of erroneous
information through the system, appears if an error occurs
and cannot be discovered before the end of execution. For
convenience, let us define an unreliable result as follows.
Definition: If there exists at least one error at the moment
of proceSs completion and if the system is at that moment
still unaware of the presence of that error, the process is said
to end with an unreliable result.
An unreliable result may even include the cases of producing wrong and/or no outputs. On the other hand, it may yield
a correct output despite the presence of error if the computation is not contaminated by the error. However, the result
cannot be trusted owing to the presence of an undetected error
at the moment of output. (No one would have much confidence in the computation result under this circumstance!) It
is therefore important to estimate the probability of producing an unreliable result, denoted by Pe, as a measure of lack
of confidence in the computation result.
Let T denote the execution time of a prodess. If T is deterministic, Pe is given byp = 7= pj{Tr4(T I Fj) + r5(T I Fj)},
which is the probability that the system is in E or EFB
at the moment of process completion. When T is a random
variable with density function fT(t), then Pe becomes

Pe-= fo{X 1pj[ r4(t Fj) + rs(tIF1)]}fT(t) dt.

When a diagnostic is scheduled periodically for the process, the resulting Pe becomes a function of the time interval
between the output moment and the time the previous diagnostic has run. The shorter this time interval, the more
reliable the computation result. However, because of the
IV. ANALYSIS AND DESIGN OF ERROR DETECTION PROCESS
uncertainty of the process execution time, it is difficult to
In case of imperfect coverage (i.e., c < 1.0) in the signal schedule periodic diagnostics so that the system is tested just
level detection, and nonzero error latency in the function before the process moves into the output phase. Here, using
level detection, the system will suffer from the following two the proposed model, we can compute the maximum value of
undesirable effects: one is the possibility of putting out Pe, denoted by max(pe), which occurs when the time interval
potentially erroneous results because the system is unaware between the process completion and the last diagnostic is
of the existence of error; the other is the additional recovery equal to tn. Observe that 1 - max(pe) represents the lower
overhead resulting from error propagation through the bound of confidence (or sure confidence) in the computation
system during error latency. With the model proposed in the results and thus can be used for design specifications. Some
previous section and moderate assumptions regarding error simulation results are graphed in Fig. 5 and 6. In Fig. 5,
recovery, we will in this section analyze these two effects and max(pe) starts to decrease sharply only when each diagnostic
then use them to specify the requirements for design of has a higher coverage (4 . 0.95). In Fig. 6, we compare
three different cases: (i) with periodic diagnostics and
error detection.
=
We used p1 0.5 P2 = 0.4, and p3 = 0.1 in simulating c 0.6, (ii) with periodic diagnostics and c 0.8, and (iii)
the impacts, of error detection process on computer per- with periodic diagnostics, c = 0.6, and doubled function
formance. This selection is for a numerical purpose only, and level detection rates. From the model, we can observe that
thus is arbitrary. The choice of these values does not alter the max(pe) is linearly related to the coverage of the signal level
detection and varies exponentially with respect to the funcvalidity of the method developed here.
tion level detection capability. However, perfect coverage
A. Estimation of the Probability of Producing an
and zero error latency are impossible to attain in practice.
Unreliable Result
Thus, the combination of both the signal level and the funcThe execution of a task consists of parallel and/or serial tion level detection mechanisms have to be used to reduce Pe.
execution of processes. We can always partition the task into
processes in such a way that every process receives all the
input data at the beginning of its execution and sends the
2Consequently, even though there are similarities between our model and
CARE III, their purposes are quite different.

-

4EEE TRANSACTIONS

534

-tf

ON COMPUTSRS, VOL.

c-33, NO. 6,

JUNE

1984

case 3

IC

f
10

4.'

L

-

case 1: t, =5.0
case 2: t,=10.0
case 3: t, =20.O

0
0

S.

-4# .

^

.4

--

.4

0.8C

0.40

Coverage of single dia.

4

1.00

0.80

Fig. 5. Max(p3) versus the coverage of a single diagnostic 5 (A = 10-6,
, = 0.2,v = 0.1,r = 0.2, a = 0.2,,B = 0.5, ) = 0.1,co = 20.0,c =
0.6,T= 100).

_co

end
oflRocofrecover

case 1: c =0.6
case 2: c =0.8
case 3: c =0.6 and ,B, y are doubled

Fig. 7. The flowchart of recovery processes.

case 1

I 'C

L
Lo=

I
L.

-4

4-

A

case 2

C

_

.0

/

'

O'0Oc

3

3
~~~~~~~~~~~case

ca

Ca

t.oo

Fig. 6.

10.00

40.00
30USD
20.00
Period of DIl Cycle
0.2, v = 0.1, = 0.2, a
Max(pj versus tn (A = 1O-6t
,l = O.5, y = 0. 1, = 20.0, e = 0.8, T = 100).
r

50.00

=

0.2,

w

impact of these detection mechanisms

on

y

computer per-

formance: computation, loss and execution cost. Computation loss -a system-oriented view -is represented by the
amount of time used for error handling, whereas execution
cost a task-oriented view -shows the effect of error detection and recovery on a particular task in the event that an
error is detected during its execution. After the detection of
an error, one may use one of several recovery methods to
rescue the executing process. Recovery strategies usually

depend on the detection mechanisms and the fault/error
types.
The overhead and efficiency associated with these recovery methods ate briefly discussed in the sequel.3
1) Recovery Strategies and Their Respective Overheads:
If an error is detecttd by a detection mechanism, rollback
or restart can always be applied to recover the process from
the error. It is, however, possible to use masking or retry if
the error is captured by signal level detection mechanisms.
Fig. 7 illustrates four recovery strategies, their applications,
and their application precedence when multiple strategies are
used to recover from a single error. In Fig. 8, a probabilistic
flow diagram between these recovery methods is presented.
Note that a transient fault may not induce any error before
its disappearance. The probability of having an error, given
the occurrence of fault, is P(E) = apl/(a + r) + P2 + P3.
Let Rz,j and pij represent, respectively, the mean overhead
and the probability that the ith recovery method is applied to
recover frotn an error which is generated by Fj, where
= 1, 2, 3, 4 for masking, retry, rollback, and restart, respectively. We also define O j as the conditional probability
that the process is recovered, given that the ith recovery
method is used when Fj occurs. Let pJ' be the probability that
Fj has occurred, given that an error is detected. Expressions
of pJ' j = 1, 2, 3 are listed in the first row of Table I. We can
use Fig. 8 to represent the mean total overhead of recovery
3This discussion is not intended to present a complete detail of error recovery
since it is out of the scope of the present paper. Instead, it is geared toward the
analysis of the effects of the error detection process and its associated recovery
on computer performance.

535

SHIN AND LEE: ERROR DETECTION PROCESS

u: recovery delay
f (u): density function of u
nI=ns=l, and

Prob(n2-n)=62(1-62)"
Fig. 8. Probabilistic flow diagram of recovery processes.

TABLE I
MATHEMATICAL EXPRESSIONS OF VARIOUS PARAMETERS RELATED TO THE THREE DIFFERENT TYPES OF FAULTS

F2
(intermittent fault)

F,
(transient fault)

PIPE,
P(E)

(1 -P)

P2,j

62,]

1

-

R2,j

PF

PE,
e- Ttr
tr

(13j ){ 6wF,) +
PE,

03j

PE,

R3,j

tb +

WhereD=
D( JO1-p46,(t lFj) dt
ftcht

4atch+r

(1

021)}

(I -

P3

P(E)

(I -pi)7r7(oo lF2)

(1 -p)Tr7(oo lF3)

0

0

82

tr

+ P2,2( - 2,2)}

I -T6(oo IF2)D2

tc

2

(1I -p.) {T6(oo F3) + P2,3(l-3

2,3)}

I1-r6(oo IF30D

tch

4,

2

2

tca
+
-tb +

-

and PE I

P2

P(E)

p,.) {T6(oo| F2)

'DI

F3

(permanent fault)

a

RT = j-'=1 P;(14=ipj1R j) for every error detection.
a) Error masking: Most error masking methods employ error-correcting codes in data transfer, memory, and
arithmetic units. Error masking is the most efficient recovery
method when it can be applied successfully. In fact, we can
regard in this case that the error has never occurred since the
system still provides correct results despite the occurrence of
error. Thus, one can assume R1,j = 0, i.e., zero recovery
overhead, and Ol,j = 1. The probability that error masking is
used, P1, 1 = p1jir7(O |IF1) (a/(a + r)) and p,]j = P,Tr7(mIFj)
for all j = 2, 3, depends on the conditional probability that
error occurs due to the faults in the units with error correcting

code and can be corrected by the error correcting code, given
that the error occurs. Here, P1 denotes the coverage by error
masking mechanisms.
b) Retry recovery: Retry can be attempted at various
levels, e.g., at the levels of microinstruction, instruction, or
I/O metaoperations. Retry is useful when the error has not
propagated yet at the time of detection. Reexecutions of the
same operation can produc, a correct result only if the related
fault is transient or intermittent and disappears during retry.
Ideally, the system should apply retry recovery until the fault
disappears if it is transient with a short active duration.
For intermittent or permanent faults, retry recovery is not

536

IEEE TRANSACTIONS ON COMPUTERS, VOL.

helpful. However, after the detection of error by signal level
detection mechanisms, it is very difficult, if not impossible,
to tell the type of fault. Moreover, if it is transient, it is
impossible to predict when the fault will disappear.
Due to the above reasons, assume the system will retry
automatically for a fixed duration tr upon detection of an error
by the signal level detection. Then, we can obtain mathematical expressions of P2.j, 02,j, and, R21j for j = 1, 2, 3 as
listed in Table I.
Recall that for an intermittent fault, when S2 occurs, the
same fault will be detected again by the signal level detection
and retry recovery will be followed. Thus, there are 1/82
retries on the average among which application of the last
retry will be unsuccessful. In case of intermittent faults the
transition probabilities, 81, 82, between SI and S2 are expressed as follows:
81

=

82

=

T7(c F2)

e

(I

-

pi) (

-etr)

(5)

r.(6)

From (5) and (6), it is easy to see that although it is simple
and practical, the above retry method is not intelligent. It
may be more desirable to design a retry mechanism which
can recognize the intermittent nature of the fault following
several consecutive successful retries for the same fault. In
such a case, 82 gets larger, or would become unity if the retry
mechanism is perfect.
c) Rollback recovery: Rollback recovery can be regarded as a type of retry which needs to save process states
during normal operation. When an error is detected, the process rolls back to one of the previously saved states. The
original idea of rollback'recovery is accommodated with acceptance tests for software reliability [ 18]. Here, for rollback
recovery we assume periodic insertion of checkpoints such
that the process can be resumed at any one of these checkpoints [19], [20]. Let to, and tch be, respectively, the overhead
for saving states and the interval between two adjacent checkpoints. Then, the percentage of the overhead for establishing
checkpoints is tol/(tov + tCh). Note that rollback recovery fails
if the states saved are destroyed by a fault, or if the states are
contaminated by error (e.g., due to the presence of error
during the state saving).
The time lost in rollback recovery is the sum of the computation undone and the setup time4 for rollback tb. When we
consider the reoccurrence of error during recovery, it is extremely difficult to determine this time loss. However, when
the fault occurrence rate is very small (typically 10-6 per
second for the IC's manufactured today), we can assume no
error occurrence during rollback. We also assume that only
the most recently saved state is kept in order to minimize the
storage requirements for checkpoints. Then, the time loss in
computation simply becomes the interval between the moment of the last state saving and that of the error occurrence
which cannot be recovered by error masking or retry.
4The setup times for both rollback and restart recoveries are needed for
hardware reconfiguration and software initialization. The hardware reconfiguration is to eliminate the source of error [i.e., fault(s)] for the resident
process in the faulty module.

c-33,

NO.

6,

JUNE

1984

Since the MTBF is in general much greater than the intercheckpoint interval tG, one can assume that the occurrence of
rollback recovery is uniformly distributed within the intercheckpoint interval, given that it is applied. Let ps, be the
probability that the saved state becomes inaccessible or
unusable and p46(t Fj) be the probability distribution
function of error latency for fault type Fj, i.e., the probability that the system is in DI at time t when the system
starts from E. p46(t Fj) is equal to n-6(t Fj) in (3) when

T(0) = [0, O, O, 1, O, 0, 0] .
Then, we obtain P3,j, 03,j and R3,j as listed in Table I.
d) Restart recovery: When restart recovery is applied,
the whole process is reexecuted from the beginning to recover
from an error. Since the system can be reconfigured to replace the faulty component, restart recovery will eventually
succeed as long as there are enough 'resources to replace
faulty components. Hence, we can represent that 04,j = 1 and
P4.j = 1- PiJ - P2,j 02,j - P3,j 03,j. The time wasted in each
restart is the sum of the setup time for reconfiguration and
reinitialization, and the time of error detection Td measured
from the beginning of process execution. For simplicity, we
assume that the moment of restart recovery is uniformly distributed within the task execution period. Thus, the density
function of the overhead involved in restart, fstm,j(t) is equal
to I/T for t, ' t ' T + t,, and R4,j = ts + T/2 where ts is
the setup time for restart. Details of the effects on task execution time by successive restarts can be found in [21], [22].
2) Calculation of Computation Loss and Execution Cost:
Now with the preceding overhead analyses, consider the
computer time that is used for actual computation instead of
error handling. The average computation loss due to a single
error detected, denoted by CL, has to include the overheads
due to periodic diagnostics, periodic insertion of checkpoints, and recovery in the event of error. Define rj as the
percentage of the average computation loss' for each error
detection, which is expressed by

CL3

4

1 /(AP(E))

]=1 _,_j Ri

7

where 1/(AP(E)) is an approximate mean time between two
successive error detections, and o- is the percentage loss due
to periodic diagnostics and insertion of checkpoints and is
given by
tp + ti
tov
+
+
+
tn tp tv tov+ tch

The above equation indicates that the time wasted for
executing periodic diagnostics and checkpointing is a dominating factor in the total computation loss when the system is
highly'reliable (i.e., the system has a small A). In Fig. 9,
plotted are the simulation results for the percentage of the
total computation loss r-, and the mean loss in recovery RT.
The reduction in recovery loss by periodic diagnostics is
small because (i) the diagnostic is useful only if it can capture
faults before they induce errors, and (ii) the diagnostic is
incapable of detecting an intermittent fault when the fault is

537

SHIN AND LEE: ERROR DETECTION PROCESS

*

C
C

8

'

tJuu

Si
8o

C
C
O-4

a

L.
L.

L.

Li CD)

81t

o

U,

CR

.L.0.3
u:

v:

CD

0

_-

mL

3

4USD"

aU.mD

IDLES

7.0-

Period of Dlia. Cycle
Fig. 9. The effects of periodis diagnostics on percentage of total loss (case 1)
tj,andtotalrecoveryloss(case2)RT(A = 10- 6,_ = 0.2, = 0.1,r = 0.2,
a = 0.2, , = 0.5, y=0.1,
20.0, e = 0.8, c = 0.6, T = 100).
v

inactive; (iii) even if the diagnostic identifies a fault, the
system still has to reconfigure or retry to eliminate this fault.
Detecticon mechanisms other than on-line diagnostics are
more advantageous due to their favorable effects and overheads on the computer performance.
Observe that this time loss is related to the system, not to
tasks to be executed on the system. One can therefore regard
this as the system overhead. On the other hand, tasks executing on the system may suffer from delays in execution due
to error detection and recovery overhead which are task
specific. When a task is time critical, the delay in its execution may cause a catastrophe (e.g., loss of human lives,
economic and social disaster, etc.) if the execution is not
completed within a specified time limit called hard deadline,
denoted by tdead. This was termed dynamic failure in [23],
[24]. Also, the running cost-the cost for use of computer as well as controlling an actual system which uses the
computed results- will certainly go up with the increase of
the execution delay. In case of error, based on Fig. 8, we can
write the probability density function of the execution delay
due to the recovery from an Fj type fault, fr(t Fj, T), where T
is the needed time for task completion under a fault-free
condition. These density functions are listed in Appendix A.
Note that for intermittent faults the task may be completed
with successful retries. In the expression forfr(t F2, 7) given
in Appendix A, for simplicity we used the upper bound of
error handling delay; that is, whenever an error occurs, the
task completion is achieved with rollback or restart recovery.
Since the overhead associated with checkpointing and
diagnostics has to be included, the time needed for task
execution under the fault-free condition becomes T (1 + o-)T. For any computation process, the delay in execution may induce an extra cost. For example, in real-time
applications this cost may be the additional energy or fuel
used for the controlled system, the consequence of longer

response time, etc. Given a cost function for the execution
time t, C(t), which is a monotonic nondecreasing function
(see [23), [24] for an example of its detailed derivation), we
can obtain the total execution cost COST and the probability
of dynamic failure Pdyn as below.
J3

-0

COST = X pj
3

j=l T

Pdyn = E pj
jl

C(t)fr(t Fj,
I t) dt

(8)

f,(t I Fj dt.

(9)

00

tdead

C. Design Considerationsfor Detection Mechanisms
Consider the performance and reliability measures Peg Pdyn,
and COST. These measures quantitatively tepresent the
consequences of imnperfect detection mechanisms and then
reflect the effects of detection mechanisms on the system
performance. In this section, these measures are used to address problems in the design of detection mechanisms.
Suppose that the specifications of performance requirements and application tasks are now given. To provide the
required fault tolerance in the design, we have to answer the
following two questions: (i) what kinds of detection mechanisms should be incorporated in the computer system to be
designed, and (ii) what are their properties in meeting the
specifications? In other words, we need to know the coverage
by signal level mechanisms, the error latency in functionlevel mechanisms, and the period of diagnostics. Suppose,
for instance, that the real-time operations and time-critical
processes are now our major design concern. The specifications must include the limit for the probability of failure as
well as the maximum allowable extra cost caused by shortcomings of detection mechanisms.
According to our simulation results in Fig. 9, the avoidance of error by diagnostics appears useful only if the cycle
time of diagnostics is not much greater than the fault's active
period, which is usually small for transient and intermittent
faults. This implies that a frequent application of diagnostics
is needed. However, in such a case, the computation time
wasted for executing diagnostics as well as the total execution cost increases prohibitively, making the periodic use
of diagnostics during normal operation less useful. It also
indicates that the probability of capturing intermittent faults
and the improvement of loss in recovery by diagnostics are
small. Consequently, on-line diagnostics are not useful for
time-critical applications.
As a conservative measure, the probability of failure due
to imperfect detection mechanisnis, denoted by pf, can be
represented by the sum of Pe and Pdyn. From the model, one
can see thatpe is dependent exponentially on error latency but
linearly on coverage c. That is, the decreasing of error latency has a greater impact on Pe than does the increasing of
the coverage. However, an improvement in the coverage will
decrease the probability of error propagation, and thus reduce
the recovery overhead. In Fig. 10, curves with constant pf
and constant COST are plotted, where C(t) is assumed to be
(t - T)2 for t T. Note that for simplicity we used a quadratic incremental cost in the above plotting. It shows the

538

IEEE TRANSACTIONS ON COMPUTERS, VOL.

c-33,

NO.

6,

JUNE

1984

V. CONCLUDING REMARKS
In this paper, we have presented first axgeneral model for
the error detection process and then a method for estimating
two important performance-related parameters of faulttolerant computers. These two are not usually included in the
traditional reliability models. The first parameter, the probability of having an unreliable result, indicates the degree of
)ST== 0.0497
lack of confidence in computation results. Suspicion in the
computation results is wholly due to the imperfect nature of
error detection. Unfortunately, such imperfection cannot be
eliminated completely from any practical error detection
COST 0.0365
mechanism. For the second parameter, we take a more detailed account of the computation loss and execution cost
resulting from the occurrence of error, its detection, and its
subsequent recovery. Since most reliable systems either include error recovery mechanisms with unknown overheads or
may suffer from an erroneous output, any reliability analysis
0.70
c*&40
has
to quantify the above overheads and uncertainty and also
Coverage of signal level detection
has
to
conprovide a good method for estimating these quantities.
Fig. 10. Design space for coverage and mean error latenfc y subject to
.straints of pf and COST (with the same system parameters as in Fig. 9 and
Though there are several assumptions to be justified by
=
tdead = 15 0,
2).
experiment, the model developed in this paper is general
enough to include all aspects from fault occurrence to error
cnthination of the coverage and the mean errcir latency re- detection with various detection mechanisms. Note that in the
quired to attain pf and. COST, below the spec ified values. estimation of the above two parameters, the model provides
The area under both the constant pf and constanit COST lines solutions only when there is one or zero error detection at
indicates the design space for selecting the cov4 erage and the a given time during task execution. The higher order effects -more than one error detected at a time during task
mean error latency. It is clear that perfect sig,nal level detection is within the design space, though it is imipractical. By execution due to multiple faults -are negligible since the
contrast, the combination of small error lateracy and zero probability of such an event is quite small.
signal level detection may not satisfy the specifi[cations. This
Finally, we have outlined a feasible design space in which
a proper combination of different imperfect detection mechacan be seen easily from the fact that with a zenD signal level
detection, every recovery must require rollbac ks and/or re- nisms needed to meet the specifications is indicated. Since
starts. The use of rollbacks and/or restarts fo r recovery is the determination of a feasible design space of detection
mechanisms must integrate the recovery methods used in the
more time consuming than error masking and re try which are
available only to signal level detection mechaniisms. Hence, system, we also briefly presented the performance of various
signal level detection mechanisms must be in(eluded in the recovery methods. Unfortunately; we cannot determine an
design. The curves with constant COST show thCat the average optimal tradeoff between various detection mechanisms beexecution costis insensitive with respect to the coverage of cause of the insufficient understanding of the function level
the signal level detection mechanism. This is d tue to the fact detection and the lack of relations between hardware costs
that all errors induced by intermittent or perrnanent faults and the signal level detection capability. Further research is
have to be recovered by rollback or restart irresjpective of the needed along these directions, especially experiments of pronature of the error detection process, and that b)ecause of the gram behavior under erroneous conditions and the design of
overheads imposed on saving states, recovery p)oints have to function level detection mechanisms.
Also of interest would be an analysis that allows the treatbe placed relatively far apart. It is important to r*ecognize that
ment of simultaneously extant multiple faults. Since most
any sophisticated recovery method will cause a severe delay
in task execution.
faults in the system are likely to be transient or intermittent,
The feasible design space indicated in Fig. l( will provide there is the possibility that the fault latency is large. Note that
the requirements in detection mechanisms for clertain system the retry recovery is applied as a temporary remedy when an
performance specifications. However, it is verry difficult to intermittent fault becomes benign shortly after its occurobjectively determine an optimal c'Wbination c f signal level rence. This intermittent fault may still exist but is inactive.
and function level detection meckiiisms. The main reasons
These would cause faults to accumulate in F and/or FB, thus
for this are that (i) the coverage has to be relaited to actual making the entire system vulnerable to any environmental or
hardware costs, (ii) error latency and perform.ance of func- other events that might activate them. The difficulty with any
tion level detection mechanisms are applicatioin dependent, such model is likely to be a considerable expansion in the
and (iii) the cost of function level detection mechanisms, number of states, thus increasing the model complexity. It is
especially software checking, is neither well s tructured nor likely that in any realistic analysis, some means must be
well understood at present.
sought to reduce the state-space size by approximating suit4

IIz

0.50

Ps

0.60

0.

.0

0.60

539

DETECTION PROCESS

SHIN AND LEE: ERROR

ably. The approach used in CARE III [14], where states are
aggregated and the state transition rates are separately determined, may be an appropriate attempt although the model is
forced to be nonhomogeneous. The nature of such approximations is a matter for further research.
APPENDIX A
DENSITY FUNCTIONS OF TASK EXECUTION TIME

The density functions of task execution time with error
occurrence due to three different types of faults (i.e., transient, intermittent, and permanent) are expressed as follows:

fr(t Fl,T) = {1 - T6(T |F) - 17(TI FI)(1 - Pi)}T(t)
+ ir6(T F) frbs, 1(t, 0)
+ ir7(TIF,)(I - pl)[l - e-rtST(t - tr)
+ (eir)frbs, l(t, 1)]
fr(t F2, T) {1 - IT6(T I F2) - rT7(T F2) (1 -P)}T(t)
+ 7T6(T F2)frbs, 2(t 0) + 7J7(T F2) (1 - P1)
* [1
-n=lI

fr(t F3, T) = {1

2)

2rs(t

)

7r6(T F3) - iT7(T IF3) (1
+ 1T6(T F3)frbs, 2(t, 0)
+ 7r7(T IF3) (1 - pl)frbs,2(t, 1)
-

-PINT(t)

where frbS,j(t, n) is the density function of the time loss in
recovery from an error induced by Fj after n unsuccessful
retries, which is given as follows:
frbs,j(t,

n)

= (1

Fj)Iu-(t-ntr)

-ps,) p46(t -ntr - tb
UT(t

ntr

tch

tCh)}

I

+

dt}]

(1

fstartj(t

Psu)

nt,)

where ST = 8(t - T), UT = u(t T tb)fTtartj(t) fstart,j
(t- 7), and 6(t) and u(t) are impulse and step functions,
respectively.
-

=

APPENDIX B
NOTATIONS

The following notations

are

to represent various measures:

defined and used in the paper

CL: Average computation loss due to diagnostics,
checkpoints, and error recovery for each
error detection.
COST: Average total execution cost for the execution'of a single task.
C(t): T.ask execution cost when it is completed at
time t.

Fj: Event that the fault is transient, intermittent, or
permanent for]

=

1, 2, 3, respectively.

P(E): Probability of having an error given that a
fault occurs.
Ri j: Average time loss when error masking, retry,
rollback, or restart is applied to recover
from an error induced by Fj for i = 1, 2, 3, 4,
respectively.
RL: Average time loss used to recover from an
error.
SI(S2): Event that rollback or restart (retry) is used to
recover from an intermittent fault.
T: Time needed to complete the task execution
under a fault-free condition.
T: Time needed to complete the task execution
when periodic diagnostics and checkpointing
are inserted.
c: Coverage of signal level detection mechanisms.
fr(t Fj, T): Density function of time delay in task execution given that the fault type is Fj and the
fault-free task execution time is T.
fStanj(t): Density function of time loss when restart is
used to recover the failed task.
P46(t Fj): Distribution function of error latancy for
the error induced by Fj, which is calculated
from (3) and with initial condition -r=
[0,0,0, 1,0,0,0].
Pdyn: Probability of dynamic failure in which the
execution of task has missed the specified
deadline.
pe: Probability of having an unreliable result at
the completion of task execution.
pf: Probability of system failure which is defined
as Pdyn + Pe.

pj: Probability of event Fj.
pa': Probability of event Fj given
detected.

an error

is

PSV: Probability that the saved state becomes
inaccessible after the occurrence of fault.
tb: Time needed to set up the task for rollback
recovery.
tch: Time interval between two successive checkpoints.
tdead: Hard deadline associated with a task.
tn: Time interval between two successive diagnostics.
tv: Time needed to establish a checkpoint.
tp: Time needed to swap the executing task for
diagnostics.
tr: Time used for a single retry recovery.
ts: Setup time for restarting a task.
a: Error generation rate by a fault.
,3: Error detection rate by the function level
when the system is in E.
'y: Error detection rate by the function level
when the system is in EFB.
A: Fault occurrence rate.
r: Transition rate that an existing transient fault
disappears.

540

5EEE TRANSACTIONS ON COMPUTERS, VOL. c-33, NO. 6, JuNE 1984

,u: Transition rate that an active intermittent
fault becomcs benign.
v:: Transition rate that a benign intermittent fault
becomes active.
Cl: Detection rate when the diagnostic program
is running.
6: Probability of detecting faults in a single
diagnostic given that faults exist.
81(82): Transition probability from event SI to S2
(S2 to SI).
pg,j: Probability that error masking, retry, rollback, or restart is applied to recover from an
error due to Fj for i = 1, 2, 3, 4, respectively.
Oi,j: Probability that the recovery succeeds
from an error due to Fj when error masking,
retry, rollback, or restart is applied for
i = 1, 2, 3, 4, respectively.
a: Percentage of time loss due to periodic diagnostics and insertions of checkpoints.
,q: Percentage of average computation loss for
each error detection.
Fi(tFJ): Probability that the system is in state i at time
t, given that the type of fault is F1.
ACKNOWLEDGMENT

The authors are grateful to R. Butler and M. Holt of the
NASA Langley Research Center, C. M. Krishna of the University of Michigan, and anonymous referees for assistance in
improving the technical content of this paper.
REFERENCES

[7] N. L. Gunther and W. C. Carter, "Remarks on the probability of detecting
faults," in Proc. 10th Annu. Int. Symp. on Fault-Tolerant Computing,
1980, pp. 213-215.
[8] J. J. Shedletsky, "Random testing: Practicality vs. verified effectiveness," in Proc. 7th Annu. Int. Symp. on Fault-Tolerant Computing, 1977,
pp. 175-179.
[9] V. Tasar, "Analysis of fault-detection coverage of a self-test software
program," in Proc. 8th Annu. Int. Symp. on Fault-Tolerant Computing,
1978, pp. 65-74.
[10] M. T. Breuer, "Testing for intermittent faults in digital circuits," IEEE
Trans. Comput., vol. C-22, pp. 241-246, Mar. 1973.
[11] Y. K. Malaiya and S. Y. H. Su, "Reliability measure of hardware redundancy fault-tolerant digital systems with intermittent faults," IEEE Trans.
Comput., vol. C-30, pp. 600-604, Aug. 1981.
[12] Y. W. Ng and A. A. Avizienis, "A unified reliability model for faulttolerant computers," IEEE Trans. Comput., vol. C-29, pp. 1002-1011,
Nov. 1980.
[13] K. S. Trivedi and R. M. Geist, "A tutorial on the CARE III approach to
reliability modeling," NASA Rep. 3488, 1981.
[14] J. J. Stiffler and L. A. Bryant, "CARE III phase report -Mathematical
description," NASA Rep. 3566, Nov. 1982.
[15] V. D. Agrawal, "An information theoretic approach to digital fault testing," IEEE Trans. Comput., vol. C-30, -pp. 582-587, Aug. 1981.
[16] E. Cinlar, Introduction to Stochastic Processes. Englewood Cliffs, NJ:
Prentice-Hall, 1975.
[17] Y. K. Malaiya and S. Y. H. Su, "Analysis of an important class of nonMarkov system," IEEE Trans. Rel., vol. R-31, pp. 64-67, Apr. 1982.
[18] B. Randell, "System structure for software fault tolerance," IEEE Trans.
Software Eng., vol. SE-1, pp. 220-232, June 1975.
[19] K. M. Chandy, J. C. Browne, C. W. Dissly, and W. Pt. Uhrig, "Analytic
models for rollback and recovery strategies in data base systems," IEEE
Trans. Software Eng., vol. SE- 1, pp. 100- 1 10, Mar. 1977.
t20] E. Gelenbe, "On the optimum checkpoint interval," J. Ass. Comput.
Mach., vol. 26, no. 2, pp. 259-270, Apr. 1979.
[21] X. Castillo and D. P. Siewiorek, '"A performance-reliability model
for computing system," in Proc. 10th Int. Symp. on Fault-Tolerant
Computing, 1980, pp. 187-192.
[22] Y. H. Lee and K. G. Shin, "Rollback propagation detection and performance evaluation of FTM2P -A fault-tolerant multiprocessor," in
Proc. 9th Annu. Symp. on Comput. Arch., 1982, pp. 171-180.
[23] C. M. Krishna and K. G. Shin, "Performance measures for multiprocessor controllers," in Performance '83, A. K. Agrawala and S. K.
Tripathi, Eds. Amsterdam: North-Holland, pp. 229-250.
[24] K. G. Shin, C. M. Krishna, and Y. H. Lee, "The application to the aircraft landing problem of a unified method for characterizing real-time
systems," in Proc. Real Time Syst. Symp., Arlington, VA, Dec. 1983.

[11 S. J. Bavuso et al., "Latent fault modeling and measurement meth[2]

[3]

[4]

[5]
[6]

odology for application to digital flight control," in Proc. Advanced
Flight Control Symp., USAF Acad., 1981.
B. Courtois, "Some results about the efficiency of simple mechanisms for
the detection of microcomputer malfunction," in Proc. 9th Annu. Int.
Symp. on Fault-Tolerant Computing, 1979, pp. 71-74.
,"A methodology for on-line testing on microprocessors," in Proc.
1JthAnnu. Int. Symp. on Fault-Tolerant Computing, 1981, pp. 272-274.
J. J. Shedletsky, "A rollback interval for networks with an imperfect
self-checking property," IEEE Trans. Comput., vol. C-27, pp. 500-508,
June 1978.
H. Ball and F. Hardie, "Effects and detection of intermittent failures in
digital systems," in Proc. AFIPS Conf., Fall 1969, pp. 229-235.
S. Osden, "The DC-9-80 digital flight guidance system's monitoring
techniques," in Proc. AIAA Guidance and Control Conf., 1979,
pp. 64-79.

Kang G. Shin (S'75-M'78-SM'83), for a photograph and biography, see
p. 124 of the February 1984 issue of this TRANSACTIONS.

Yann-Hang Lee (S'8 1), for a photograph and biography, see p. 124 of the
February 1984 issue of this TRANSACTIONS.

Resource Scheduling in Dependable Integrated Modular Avionics
Yann-Hang Lee and Daeyoung Kim
CISE Department,
University of Florida
{yhlee, dkim}@cise.ufl.edu

Abstract
In the recent development of avionics systems,
Integrated Modular Avionics (IMA) is advocated for next
generation architecture that needs integration of mixedcriticality real-time applications. These integrated
applications meet their own timing constraints while
sharing avionics computer resources. To guarantee
timing constraints and dependability of each application,
an IMA-based system is equipped with the schemes for
spatial and temporal partitioning. We refer the model as
SP-RTS (Strongly Partitioned Real-Time System), which
deals with processor partitions and communication
channels as its basic scheduling entities.
This paper presents a partition and channelscheduling algorithm for the SP-RTS. The basic idea of
the algorithm is to use a two-level hierarchical schedule
that activates partitions (or channels) following a
distance-constraints guaranteed cyclic schedule and then
dispatches tasks (or messages) according to a fixed
priority schedule. To enhance schedulability, we devised
heuristic algorithms for deadline decomposition and
channel combining. The simulation results show the
schedulability analysis of the two-level scheduling
algorithm and the beneficial characteristics of the
proposed deadline decomposition and channel combining
algorithms.

1. Introduction
Advances in computer and communication technology
have introduced new architectures for avionics systems,
which emphasize the integration of applications,
dependability, and cost reduction. Away from the
traditional federated implementation for avionics systems,
the new approach, referred to as Integrated Modular
Avionics (IMA) [1], utilizes multiple standardized
processor modules in building functional components of
avionics systems. It allows the applications to be merged
into an integrated system. While permitting resource
sharing, the approach employs temporal and spatial
partitioning to set up the application boundaries needed to
maintain system predictability, real-time response, and

0-7695-0707-7/00 $10.00 ã 2000 IEEE

Mohamed Younis, Jeff Zhou, James McElroy
Honeywell International Inc.
{mohamed.younis, jeff.zhou, james.mcelroy}
@honeywell.com

dependability [2, 6]. For the interactions between
applications, it adopts a message model that can easily
accommodate replicated executions of mission-critical
applications.
Under the IMA architecture, each processor can host
multiple partitions in which applications can be executed
using the assigned resources. Spatial partitioning implies
that a partition cannot access other partition’s resources,
like memory, buffers, and registers. On the other hand,
temporal partitioning guarantees a partition’s monopoly
use of a pre-allocated processing time without any
intervention from other partitions. Thus, a partition is the
sole owner of its resources, such as memory segments,
I/O devices, and processor time slots. As a result, the
applications running in different partitions cannot
interfere with each other. To facilitate communications
between applications, each partition can be assigned with
one or more communication channels. An application can
transmit messages during the slots allocated to its channel
and access exclusively the channel buffers. In this sense,
the channels are spatial and temporal partitions of
communication resource and are dedicated to one
message-sending application.
An application running within a partition can be with
multiple cooperating tasks. For instance, the Honeywell’s
Enhanced Ground Proximity Warning System (EGPWS)
consists of tasks for map loading, terrain threat detection,
alert prioritization, display processing, etc. With the
spatial and temporal partitioning, the EGPWS application
can be developed separately and then integrated with
other applications running in different partitions of an
IMA-based system. Its execution cannot be affected by
any malfunctions of other applications (presumably
developed by other manufactures) via wild writes or task
overruns. However, sufficient resources must be allocated
to the partition and the channels, so that the EGPWS
application can ensure a proper execution and meet its
real-time constraints.
One apparent advantage of IMA-based systems with
spatial and temporal partitioning is that each application is
running in its own environment. Thus, as long as the
partition environment is not changed, an application’s
behavior remains constant even if other applications are

modified. This leads to a crucial advantage to avionics
systems, i.e. when one application is revised, other
applications don’t need to be re-certified by the FAA.
Thus, the integration of applications in a complex system
can be upgraded and maintained easily. It is conceivable
that such architecture with spatial and temporal
partitioning can be useful for integrating general real-time
applications, and will be referred to as a strongly
partitioned real-time system (SP-RTS) in the paper.
In this paper, we investigate the issues related to the
partition and channel scheduling in SP-RTS. To schedule
processor execution, we need to determine which partition
is active and to select a task from the active partition for
execution. According to temporal partitioning, time slots
are allocated to partitions. Within each partition, fixed
priorities are assigned to tasks based on rate-monotonic or
deadline-monotonic algorithms [14, 5]. A lower priority
task can be preempted by higher priority tasks of the same
partition. In other words, the scheduling approach is
hierarchical that partitions are scheduled following a
cyclic schedule and tasks are dispatched according to a
fixed priority schedule. We can conjecture a real system
where partitions are processes with protected memory
spaces and tasks are threads in a process. At process level,
a cyclic scheduling is employed, whereas, in thread level,
thread priorities are compared. The scheme doesn’t need
to make a global priority comparison between threads of
different processes. Similar hierarchical scheduling is also
applied to the communication media where channels are
scheduled in a cyclic fashion and have enough bandwidth
to guarantee message communication. Within each
channel, messages are then ordered according to their
priorities for transmission.
Given task execution characteristics, we are to
determine the cyclic schedules for partitions and channels
under which the computation results can be delivered
before or on the task deadlines. The problem differs from
the typical cyclic scheduling since, at the partition and
channel levels, we don’t evaluate the invocations for each
individual task or message. Only aggregated task
execution and message transmission models are
considered. In addition, the scheduling for partitions and
channels must be done collectively such that tasks can
complete their computation and then send out the results
without missing any deadlines.
A different two-level hierarchical scheduling scheme
has been proposed by Deng and Liu in [8]. The scheme
allows real-time applications to share resources in an open
environment. The scheduling structure has an earliestdeadline-first (EDF) scheduling at the operating system
level. The second level scheduling within each application
can be either time-driven or priority-driven. For
acceptance test and admission of a new application, the
scheme analyzes the application schedulability at a slow
processor. Then, the server size is determined and server

0-7695-0707-7/00 $10.00 ã 2000 IEEE

deadline of the job at the head of the ready queue is set at
run-time. Since the scheme does not rely on fixed
allocation of processor time or fine-grain time slicing, it
can support various types of applications, such as release
time jitters, non-predictable scheduling instances, and
stringent timing requirements.
The scheduling approach for avionics applications
under the APEX interface of IMA architecture was
discussed by Audsley and Wellings [4]. A recurrent
solution to analyze task response time in an application
domain is derived and the evaluation results show that
there is a potential for a large amount of release jitter.
However, the paper does not address the issues of
constructing cyclic schedules at the operating system
level. To remedy the problem, our first step is to establish
scheduling requirements for the cyclic schedules such that
task schedulability under a given fixed priority schedules
within each partition can be ensured. The approach we
adopt is similar to the one in [8] of comparing the task
execution in SP-RTS environment with that at a dedicated
processor. The cyclic schedule then tries to allocate
partition execution intervals by “stealing” task inactivity
periods. This stealing approach resembles the slack stealer
for scheduling soft-aperiodic tasks in fixed priority
systems [11]. Once the schedulability requirements are
obtained, suitable cyclic schedules can be constructed.
Following the partitioning concept of IMA, the operating
system level cyclic schedule is flexible to support system
upgrade and integration. It is designed in a way that no
complete revision of scheduling algorithms is required
when the workload or application tasks in one partition
are modified.
The rest of the paper is organized as follows. In section
2, we describe the system models that describe tasks,
partition servers, messages, and channel servers in SPRTS. Then, we show the overall system scheduling
algorithm and its specific components, such as deadline
decomposition, task and message schedulability checking,
channel combining, and cyclic scheduling for partition
servers and channel servers in section 3. Evaluation
results are presented in section 4. A conclusion is then
given in section 5.

2. System Models
The SP-RTS system model, as shown in Figure 1,
includes multiple processors inter-connected by a time
division multiplexing communication bus such as ARINC
659 [3]. Each processor has several execution partitions to
which applications can be allocated. An application
consists of multiple concurrent tasks that can
communicate with each other within the application
partition. Task execution is subject to deadlines. Each
task must complete its computation and send out the result
messages on time in order to meet its timing constraints.

Messages are the only form of communication among
applications, regardless of whether their execution
partitions are in the same processor or not. For interpartition communication, the bandwidth of the shared
communication media is distributed among all
applications by assigning channels to a subset of tasks
running in a partition. We assume that there are hardware
mechanisms to enforce the partition environment and
channel usage by each application, and to prevent any
unauthorized accesses. Thus, task computation and
message transmission are protected in their application
domain. The mechanisms could include memory
protection controller, slot/channel mapping, and separate
channel buffers.
Node 1

Node n
Cyclic CPU Scheduler

Partition Server

Cyclic CPU Scheduler

Partition Server

Partition Server

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Task

Channel
Server

Channel
Server

Channel
Server

Channel
Server

Channel
Server

Channel
Server

Cyclic Bus Scheduler

deadlines influences the bandwidth allocation for the
message. For example, when the message size, Mi, is 1K
slots, and the message deadline of 10ms, then the
bandwidth requirement is 0.1M slots per second. In the
case of the 1ms message deadline, the bandwidth
requirement becomes 1M slots per second. However, a
tradeoff must be made since a long message deadline
implies a less amount of bandwidth to be allocated, thus
the task computation has to be completed immediately.
For each processor in SP-RTS architecture, the
scheduling is done in a two-level hierarchy. The first level
is within each partition server where the application tasks
are running and a higher priority task can preempt any
lower priority tasks of the same partition. The second
level is a cyclic partition schedule that allocates execution
time to partition servers of the processor. In other word,
each partition server, Sk, is scheduled periodically with a
fixed period. We denote this period as the partition cycle,
ηk. For each partition cycle, the server can execute the
tasks in the partition for an interval αkηk where αk is less
than or equal to 1 and is called partition capacity. For the
remaining interval of (1-αk)ηk, the server is blocked. In
Figure 3, an example execution sequence of a partition
that consists of three tasks is depicted. During each
partition cycle, ηk, the tasks, τ1, τ2, and τ3, are scheduled
to be executed for a period of αkηk. If there is no active
task in the partition, the processor is idle and cannot run
any active tasks from other partitions.

Figure 1. The architecture model for strongly
partitioned real-time systems (SP-RTS)
In our task model, we assume that each task arrives
periodically and needs to send an output message after its
computation. Thus, as illustrated in Figure 2, tasks are
specified by several parameters, including invocation
period (Ti), worst-case execution time (Ci), deadline (Di)
and message size (Mi). Note that, to model sporadic tasks,
we can assign the parameter Ti as the minimum interarrival interval between two consecutive invocations.
Task period Ti

Ci
Computation deadline CDi

Mi
Message deadline MDi

Task deadline Di

Figure 2. Task model and deadlines
In order to schedule tasks and messages at processors
and communication channels, the task deadline, Di, is
decomposed into message deadline (MDi) and
computation deadline (CDi). The assignment of message

0-7695-0707-7/00 $10.00 ã 2000 IEEE

τ1 τ2

ηk

τ2 τ3 τ1

τ1 τ3 idle

τ2 τ1 τ2

αkηk

Figure 3. An illustrative task and partition execution
sequence
Similarly, a two-level hierarchical scheduling method
is applied to the message and channel scheduling. A
channel server provides fixed-priority preemptive
scheduling for messages. Then, a cyclic schedule assigns
a sequence of communication slots to each channel server
according to its channel cycle, µk, and channel capacity,
βk. A channel may send out messages using βkµk slots
during every period of µk slots. Note that we use the unit
of “slot” to indicate both message length and transmission
time, with an assumption that communication bandwidth
and slot length are given. For instance, a 64-bit slot in the
30MHz 2-bit wide ARINC 659 bus [3] is equivalent to
1.0667µs, and a message of 1000 bytes will be transmitted
in 125 slots. For convenience purposes, we define the
conversion factors ST as a slot-to-time ratio based on slot
length and bus bandwidth.

3. Scheduling Approach
The objective of our scheduling approach is to find
feasible cyclic schedules for partition and channel servers
which process tasks and transmit messages according to
their fixed priorities within the servers. With proper
capacity allocation and frequent invocation at each server,
the combined delays of task execution and message
transmission are bounded by the task deadlines. In Figure
4, we show the overall approach which first applies a
heuristic deadline decomposition to divide the problem
into two parts: partition-scheduling and channelscheduling. If either one cannot be done successfully, the
approach iterates with a modified deadline assignment.
We also assume that the initial task set imposes a
processor utilization and a bus utilization less than 100%
and each task’s deadline is larger than its execution time
plus its message transmission time, i.e., Di ≥ Ci + ST∗Mi
for task i.

Message Deadline, MDi = ( Di

Computation Deadline, CDi = Di − MDi
where fi is an adjusting factor for each task. The main idea
of deadline decomposition is that it allocates the
deadlines, CDi and MDi, proportionally to their time
requirements needed for task execution and message
transmission. In addition, the adjusting factor fi is used to
calibrate the computation and message deadlines based on
the result of previous scheduling attempts and the
utilization at processor and communication bus. Since the
message and task deadlines must be lower-bounded to the
transmission time (ST∗Mi) and computation time (Ci),
respectively, and upper-bounded to Di, we can obtain the
lower bound and upper bound of the adjusting factor f as

1
Task and message
deadline assignment

fail

Di (

Determine partition
capacities and cycles
succeed
Processor
cyclic scheduling

cannot
combine

Channel server
initialization and
combining
Integrated schedule
Determine channel
capacities and cycles

fail

succeed
Communication slot
allocation

Figure 4. Combined partition and channel scheduling
approach

3.1. Deadline Decomposition
It is necessary to decompose the original task
deadline, Di, into computation and message deadline, CDi
and MDi, for every task, before we can schedule the
servers for partition execution and message transmission.
A deadline decomposition algorithm is used to assign
these deadlines in a heuristic way. If we assign tight
message deadlines, messages may not be schedulable.
Similarly, if tasks have tight deadlines, processor
scheduling can fail. The following equation is used to
calculate the message deadline and computation deadline
for each task:

0-7695-0707-7/00 $10.00 ã 2000 IEEE

ST ∗ M i
) fi
C i + ST ∗ M i

1
)
C i + ST ∗ M i

≤ fi ≤

Di − C i
ST ∗ M i
)
Di (
C i + ST ∗ M i

Since an adjusting factor of 1.0 is a fair distribution
and always included in the range of fi, we set the initial
value of fi to be 1. The heuristic deadline decomposition,
as show in Figure 5, is similar to a binary search
algorithm in the attempt of finding the right proportion of
task and message deadlines. If we reach the situation that
it cannot assign new value for all tasks, we declare the
input set of tasks as unschedulable.
Initialization for all tasks
MinF = 1 / (Di * (1/(Ck+ ST∗Mk)));
MaxF = (Di-Ci) / (Di * (ST∗Mi /(Ci+ ST∗Mi)));
fi = 1.0;
Iterative change of fk when either partition or channel
scheduling fails
If (Partition scheduling fails) {
MaxF = fi; fi = (MinF + fi) / 2.0;
}
else if (Channel scheduling fails) {
MinF = fi; fi = (MaxF + fi) / 2.0;
}

Figure 5. The deadline decomposition algorithm

3.2. Partition and Channel Scheduling
In SP-RTS, partitions and channels are cyclically
scheduled. The partition cyclic schedule is based on
partition cycle, ηk, and partition capacity, αk. Similarly, a
channel cyclic schedule with parameters, βk and µk
implies that the channel can utilize βkµk slots during a

period of µk slot interval. While tasks and messages are
scheduled according to their priority within the periodic
servers, the cyclic schedule determines the response time
of task execution and message transmission. In this
subsection, we give a short description of the scheduling
theory that can be used to schedule the cyclic partition
and channel servers. A full discussion of the scheduling
theory and the associated proof are given in our previous
paper [10].
Note that, at the system level, the partition server Sk is
cyclically scheduled with a fixed partition cycle, ηk. For
every partition cycle, the server can execute the task in
partition Pk during an interval of αkηk where αk ≤ 1. For
the remaining interval of (1-αk)ηk, the server is blocked.
Suppose that there are n tasks in partition server Sk listed
in priority order such that τ1 < τ2 < τ3 < … < τn where τ1
has the highest priority and τn the lowest. According to
deadline monotonic algorithm, we assume that the highest
priority is given to the task with shortest task deadline. In
order to evaluate the schedulability of the partition server,
Sk, we first consider that the task set is executed at a
dedicated processor of capacity αk. Based on the
necessary and sufficient condition of schedulability
analysis [12, 13], task τi is schedulable if there exists a t 0
Hi = {CDi ∪ lTj | j=1,2,…,i-1; l=1,2,…, CDi/Tj }such
that:

Cj  t 
 ≤t
j =1 α k  T j 

partition scheduling can fail if the sum of the minimum
αk, for all partitions in a processor, is larger than 1.
With Theorem 1, we can depict the plot of maximum
partition cycle vs. the assigned capacity αk. To illustrate
the result, we consider an example in Table 1 in which
four application partitions are allocated in a processor.
Each partition consists of several periodic tasks and the
corresponding parameters of (Ci, Ti) are listed in the
Table. Tasks are set to have deadlines equal to their
periods and are scheduled within each partition according
to a rate-monotonic algorithm. The processor utilization
demanded by the 4 partitions, ρk, are 0.25, 0.15, 0.27, and
0.03, respectively.
Table 1. Task parameters for the example partitions
Partition 1
(utilization=0.25)
(4, 100)
(9, 120)
(7, 150)
(15, 250)
(10, 320)
Partition 3
(utilization=0.27)
(7,80)
(9,100)
(16,170)

tasks
(Ci, Ti)

tasks
(Ci, Ti)

i

The expression Wi(αk, t) indicates the worst cumulative
execution time demand on the processor made by the
tasks with a priority higher than or equal to τi during the
interval [0,t]. We now define Bi(αk) = max t ∈ Hi {t –
Wi(αk, t)} and B0(αk) = min i=1,2,..n Bi(αk), where n is the
total number of tasks in the partition. Note that, when τi is
schedulable, Bi(αk) represent the total period in the
interval [0, t] that the processor is not running any tasks
with a priority higher than or equal to that of τi in the
partition server. Bi(αk) is equivalent to the level-i
inactivity period in the interval [0, t] [11].
By comparing the task executions at server Sk and at a
dedicated processor of capacity αk, we can obtain the
following theorem [10].

Partition 4
(utilization=0.03)
(1,80)
(2,120)

In Figure 6, the curves ηk = B0(αk)/(1-αk ) are plotted
for the example 4 partitions. If the points below the curves
are chosen to set up cyclic scheduling parameters for each
partition, the tasks in the partition are guaranteed to meet
their deadlines.

500
Partition
Partition
Partition
Partition

450
400

1
2
3
4

350

Partition Cycle

Wi (α k , t ) = ∑

Partition 2
(utilization=0.15)
(2, 50)
(1, 70)
(8, 110)
(4, 150)

300
250
200
150
100

Theorem 1.
The partition server Sk is schedulable if Sk
is schedulable at a dedicated processor of capacity αk,
and ηk ≤ B0(αk)/(1-αk)
Note that B0(αk) is a non-decreasing function of αk.
There is a minimum αk such that B0(αk) equals to zero,
i.e., a zero inactive period for at least one task in the
partition. The minimum αk indicates the minimum
processor capacity needed to schedule the partition. Thus,

0-7695-0707-7/00 $10.00 ã 2000 IEEE

50
0

0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

α

Figure 6. Partition Cycles vs. Processor Capacities for the
Example Partitions
For instance, the curve for partition 2 indicates that, if
the partition receives 28% of processor capacity, then its
tasks are schedulable as long as its partition cycle is less

than or equal to 59 time units. Note that the maximum
partition cycles increase as we assign more capacity to
each partition. This increase is governed by the
accumulation of inactivity period when αk is small. Then,
the growth follows by a factor of 1/(1-αk) for a larger αk.
The curves in Figure 6 show that there are sharp rises of
the maximum partition cycle when we increase αk just
beyond the minimum required capacities. The rises
indicate that a small amount of extra capacity can enlarge
the inactive period of a partition server significantly.
According to the design objectives, there are several
methods we can use to choose a set of (αk, ηk) for all
partition servers. For instance, we can calculate the
minimum αk first. If the sum of the minimum αk, for all
partition server Sk, and the reserved portion of processor
capacity, is less than 100%, the extra capacity can be
allocated to all partitions proportionally to their minimum
αk. Then, ηk can be calculated based on Theorem 1. The
other approach is to search for the saddle point in the
B0(αk)/(1-αk) curve where the initial rise just begins to
slow down. The pair (αk, ηk) at the saddle point is used as
the initial capacity allocation and partition cycle. Further
increase or reduction can be done proportionally if the
total capacity allocated is less than or larger than 1.
We can use the same scheduling method of the
partition scheduling for channel scheduling. A channel
server, Gk, transmits its messages according to a fixed
priority preemptive scheduling method. It provides a
bandwidth of βkµk slots to the messages in the channel
during every channel cycle, µk, where βk ≤ 1. For the
remaining slots of (1-βk)µk, the channel server is blocked.
Since each channel server follows the identical two-level
hierarchical scheduling as partition servers, Theorem 1
can directly applied to obtain the pair of parameters (βk,
µk). However, there are several differences. First, only
integer number of slots can be assigned to a channel
server. Thus, we can use either βkµk slots or restrict βkµk
to be integer. The second difference is that the message
arrivals are not always periodic due to possible release
jitters. Release jitters can be included in the schedulability
test if they are bounded by some maximum value [15].
The release jitter can also be eliminated if the
communication controller incorporates a timed message
service that becomes active immediately after the
computation deadline is expired. The last difference is the
assignment of messages into a channel. According to the
principle of partitioning, tasks from different partitions
cannot share the same channel for message transmission.
For the tasks in a partition, we can group a subset of tasks
and let them share a channel server. The grouping can be
done based on the semantics of the messages or other
engineering constraints. Also, the multiplexing of
messages in a shared channel may lead to a saving of

0-7695-0707-7/00 $10.00 ã 2000 IEEE

bandwidth reservation. We should address this issue in the
following subsection.

3.3. Channel Combining
For a channel server that transmits a periodic message
with a deadline MDi and a message size Mi, we must
allocate a minimum bandwidth of Mi/MDi. Since there is a
limitation in the total bus bandwidth, we may not always
assign one channel server to each message. However, we
may be able to combine some messages and let them
share a common channel server. This can lead to a
bandwidth reduction since the reserved bandwidth can be
better utilized by the messages of different deadlines. For
example, given two messages 1 and 2 with parameters
(M1, MD1, T1) and (M2, MD2, T2), respectively, the
minimum bandwidth requirements, in terms of slots per
time unit, for separate channels of messages 1 and 2, and
for the combined channel, can be computed as following:
CB1 = M1/MD1, CB2 = M2/MD2,
CB12 = max{ M1/MD1, (M2+M1* MD2 /T1 )/MD2 }
We assume that message 1 has a higher priority than
message 2 in the above computation. The cost of message
preemption is ignored which can be at most one slot per
preemption since we assume that slots are the basic
transmission units in the communication bus. Notice that
CB12 is not always less that CB1+CB2. However, if
message 1 has a much shorter deadline comparing with its
period and message 2 has a longer deadline than message
1’s period, then the bandwidth reduction CB1+CB2-CB12
becomes substantial. While we reserve a proper amount of
bandwidth for an urgent message, the channel is only
partially utilized if the message arrives infrequently. This
provides a good chance to accommodate additional
messages in the same channel and results in a reduction in
the required bandwidth.
The above equation also implies that the maximum
bandwidth reduction can be obtained by combining the
message with a long deadline and the message with a
short deadline where the period of the latter should be
greater than message deadline of the former. With this
observation, we devise a heuristic channel-combining
algorithm which is shown in Figure 7. The computation of
the minimum bandwidth requirement of a channel
consisting of messages 1,2,…,k-1, and k, is:
j −1
 MD j 
CB12...k = max{(( ∑ M i ∗ 
 + M j ) / MD j )}
j =1,k
i =1
 Ti 

where we assume that message j has a higher priority then
message j+1. Note that the real bandwidth allocation must
be determined according to the choice of channel cycle as

described in Theorem 1. However, in order to calculate
channel cycle and capacity, the messages in each channel
must be known. The channel-combining algorithm
outlined in Figure 7 is developed to allocate messages to
channels for each partition and to reduce the minimum
bandwidth requirement to a specific threshold. If the
combined channels cannot be scheduled, we can further
decrease the target threshold until no additional
combining can be done.
Initialization (Channel combining is allowed to the tasks in
the same partition)
Assign one channel server Gk to the message of each task
Iterate the following steps until the sum of total CBk is less
than the target threshold
determine all pair of combinable channel server Gk and
Gj where the max. message deadline in Gk is larger than
the min. task period in Gj
For every pair of combinable channel servers Gk and Gj {
calculate the bandwidth reduction CBk+CBj− CBkj
}
Combine Gj with the server Gk that results in the
maximum reduction

Figure 7. A heuristic channel combining algorithm

invocation period for every partition, a substantial number
of context switches between partitions could occur. A
practical approach of avoiding excessive context switches
is to use Han’s SX specialization algorithm with a base 2
[9]. Given a base partition cycle η, the algorithm finds a hi
for each ηi that satisfies:
hi = η * 2j ≤ ηi < η * 2 j+1 = 2*hi,
To find the optimal base η in the sense of processor
utilization, we can test all candidates η in the range of

∑kαk

(η1/2, η1] and compute the total capacity

. To

obtain the total capacity, the set of ηk is transferred to the
set of hk based on corresponding η and then the least
capacity requirement, α kh , for partition cycle hk is
obtained from Theorem 1. The optimal η is selected in
order to minimize the total capacity. In Figure 8, we show
a fixed cyclic processor scheduling example that
guarantees distance constraint for the set of partition
capacities and cycles, A(0.1,12), B(0.2,14), C(0.1,21),
D(0.2,25), E(0.1,48), and F(0.3,50). We use the optimal
base of 10 to convert the partition cycles to 10, 10, 20, 20,
40, and 40, respectively.

A

B

C

.1

.2

.1

D
.2

E
.0
2
5

10

A

B

E

F

A

B

C

.1

.2

.075

.1

.1

.2

.1

10

D
.2

F
.0
2
5

A

B

.1

.2

10

20

F
.175

10
20

40

3.4. Cyclic Scheduling for Partition and Channel
Servers
Let a feasible set of partition capacities and cycles be
(α1, η1), (α2, η2), … , (αn,ηn) and the set be sorted in the
non-decreasing order of ηk. The set cannot be directly
used in a cyclic schedule that guarantees the distance
constraint of assigning αk processor capacity for every ηk
period in a partition. To satisfy the distance constraint
between any two consecutive invocations, we can adopt
the pinwheel scheduling approach [7, 9] and transfer {ηk}
into a harmonic set through a specialization operation.
Note that, in [9], a fixed amount of processing time is
allocated to each task and would not be reduced even if
we invoke the task more frequently. This can lead to a
lower utilization after the specialization operations. For
our partition-scheduling problem, we allocate a certain
percentage of processor capacity to each partition. When
the set of partition cycles {ηk} is transformed in to a
harmonic set {hk}, this percentage doesn’t change. Thus,
we can schedule any feasible sets of (αk, ηk) as long as the
total sum of αk is less than 1.
A simple solution for a harmonic set {hk} is to assign
hk=η1 for all k. However, since it chooses a minimal

0-7695-0707-7/00 $10.00 ã 2000 IEEE

Figure 8. Example of processor cyclic scheduling

The basic method of cyclic scheduling for channel
servers is same as that of partition server scheduling. The
only difference is that we need to consider that channel
bandwidth allocation must be done based on integer
number of slots. Let the feasible bus bandwidth capacity
allocation set be (β1, µ1), (β2, µ2), … , (βn,µn). Using the
SX specialization, the set {µk} will be transformed to a
harmonic set {mk}. Then, based on Theorem 1 and the
reduced mk, we can adjust the channel capacity βk to βkh
subject to ∑ β ih mk  ≤ mk . There will be βkh mk slots
n

i =1

allocated to the channel server Gk.

4. Algorithm Evaluation
In this section, we present the evaluation results of the
proposed algorithms for SP-RTS. First, we show the
percentage of schedulable task sets in terms of processor
and bus utilization under the two-level scheduling,
deadline decomposition and channel combining
algorithms. Then, we show that the penalty of the

(N,P,T) = (2,2,4)

1.1

1.1

1.0

1.0

.9

.9

.8

.8

.7

.7

Schedulability

Schedulability

(N,P,T) = (4,3,5)

.6
.5
.4

.6
.5
.4

.3

.3

Proc.
Proc.
Proc.
Proc.
Proc.

.2
.1
0.0

Util.
Util.
Util.
Util.
Util.

0.15
0.30
0.45
0.60
0.75

Proc.
Proc.
Proc.
Proc.
Proc.

.2
.1
0.0

-.1

Util.
Util.
Util.
Util.
Util.

0.15
0.30
0.45
0.60
0.75

-.1

0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

1.1

0.0

Bus utilization

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

1.1

Bus utilization

Figure 9. Schedulability test for configurations (4, 3, 5) and (2, 2, 4)

harmonic transformation even if channel server
scheduling is negligibly small. Finally, the characteristic
behavior of deadline decomposition is illustrated. The
evaluations are done with random task and message sets
that are generated with specific processor and bus
utilization.

4.1. Schedulability Test
A schedulability test of the algorithm is obtained using
the simulations of a system model that composes of four
processors, three partitions per each processor and five
tasks per each partition, i.e., a configuration of (4, 3, 5).
The simulations use random task sets that result in
variable processor utilization of 15%, 30%, 45%, 60%
and 75%. The task periods are uniformly distributed
between the minimum and maximum periods. The total
processor utilization is randomly distributed to all tasks in
each processor and is used to compute the task execution
times. To create message sets, we vary the total bus
utilization from 10% to 90%. Message lengths are
computed with a random distribution of the total bus
utilization and task periods.
Using the scheduling procedure of Figure 4, we first
assign task and message deadlines for each task. Then the
partition capacity and cycle for each partition are
computed and the cyclic schedule for each processor is
constructed. To schedule message transmission, messages
are combined into channels in order to reduce bandwidth
requirement. After channel cycle and capacity are
determined, a cyclic schedule is formed. For the priority
schedules within partitions and channels, we adopt the
deadline monotonic approach to order the task and
message priorities. With all randomly created task sets,
we report the percentage of schedulable task sets among

0-7695-0707-7/00 $10.00 ã 2000 IEEE

all sets in Figure 9. The figure shows the algorithms are
capable of finding proper deadline assignments and, then,
determining feasible partition and channel cyclic
schedules. For instance, consider the case of 60%
processor and bus utilization. Even if the deadlines are
less than task periods, almost 100% of task sets are
schedulable. Figure 9 also reports the test results of the
configuration (2, 2, 4). The curves have the similar trends
as that of the configuration of (4, 3, 5).

4.2. The Effects of Deadline Decomposition and
Channel Combining Algorithm
It is worthy to look into how the bus is utilized in the
channel schedules resulted from the heuristic algorithms
of deadline decomposition and channel combining.
Consider the following measures:
1.

2.

3.

Measure1 is the bus utilization which equals to
the sum of (ST∗Mi)/Ti for all tasks. No real-time
constraint of message delivery is considered in
this measure.
Measure2 is the total bus capacity needed to
transmit messages on time with no channel
combining (i.e., each task has a dedicated
channel). This capacity will be equal to the
summation of (ST∗Mi)/MDi for all tasks and can
be computed after message deadlines are
assigned.
Measure3 is the minimum bus capacity needed
to schedule channels. This measure is equal to
the summation of minimum βk for all channels.
Note that, according to Theorem 1, the minimum
βk for a channel is defined as the minimum
capacity that results in a zero inactive period for

5.

We can expect an order of Measure2> Measure5>
Measure4> Measure3> Measure1 among the measures.
Measure2 should be much higher than other measures as
we allocate bandwidth for each message independently to
ensure on schedule message delivery. With the message
multiplexing within each channel, the on schedule
message delivery can be achieved with a less amount of
bandwidth. However, a bandwidth allocation following
Measure3 cannot be practical since the channel cycles
must be infinitely small. According to Theorem 1,
Measure4 contains additional capacity that is added to
each channel to allow temporary blocking of message
transmission during each channel cycle. Furthermore, in
Measure5, an extra capacity is allocated as we make
integer number of slots for each channel and construct a
cyclic schedule with harmonic periods.
The simulation results of the above measures are
shown in Figure 10. The results confirm our expectation
of the order relationship. However, when we change the
bus utilization from 0.1 to 0.8, the curves are not
monotonically increasing (except the curve of Measure1).
This is the consequence of the deadline decomposition
(DD) algorithm. When channels don’t have enough
bandwidth to meet short message deadlines, the algorithm
adjusts the factor fk and assigns longer deadlines for
message transmission. As shown in Figure 5, the DD
algorithm uses an approach similar to binary search
algorithm and makes a big increase to fk initially. This
results in long deadlines and the reduced capacity
allocations in Measure2-5. In fact, when the bus
utilization is less than 30%, the average number of
iterations performed in the DD algorithm is slightly larger
than 1, i.e., only the initial fk is used to allocate deadlines.
When the bus utilization is raised to 40% to 70%, the
average number of iterations jumps to 1.6, 1.98, 2.0, and
2.04, respectively. It further increases to 11.09 when the
bus utilization is set to 80%.
Figure 10 also illustrates the magnitude of the
measures and the differences among them. The gap
between Measure3 and Measure2 is very visible. This
difference is the product of channel combining algorithm.
In order to meet a tight message deadline, we have to

0-7695-0707-7/00 $10.00 ã 2000 IEEE

30% proc. util. at a (4,3,5) system
1.5
1.4
1.3
1.2
1.1

Capacity

4.

at least one message in the channel. It can be
determined after message deadlines are assigned
and messages are combined into the channel.
Measure4 is the total bus capacity selected
according to Theorem 1. This measure can be
formulated as the summation of βk for all
channels.
Measure5 is the final bus capacity allocated to all
channels based on a harmonic set of channel
cycles and the integer number of slots for each
channel. The capacity is equal to the summation
of βkhmk/mk for all channels.

1.0
.9
.8
.7
.6
.5

Measure2
Measure5
Measure4
Measure3
Measure1

.4
.3
.2
.1
0.0
-.1
0.0

.1

.2

.3

.4

.5

.6

.7

.8

.9

1.0

Bus Utilization

Figure 10. Measures for Bus Utilization and Capacities

reserve a large amount of bandwidth. With channel
combining, messages of different deadlines share the
allocated slots. As long as the message with a shorter
deadline can preempt the on-going transmission, the slots
in each channel can be fully utilized by multiplexing and
prioritizing message transmissions. There is a moderate
gap between Measure3 and Measure4. As indicated in
Theorem 1, we search for a channel capacity and a
channel cycle located in the knee of the curve ηk ≤
B0(αk)/(1-αk) after the initial sharp rise. This implies that a
small increase of βk will be added to Measure3 in order to
obtain a reasonable size of channel cycle. Finally, the
difference between Measure4 and Measure5 is not
significant at all. It is caused by the process of converting
ηk to a harmonic cycle mk, and by allocating an integer
number of slots βkhmk for each channel.
The other way of looking into the behavior of the
deadline decomposition algorithm is to investigate the
resultant decomposition of task deadline, Di. In Figure 11,
we showed the average ratio of message deadline to task
deadline, under different processor and bus utilization. If
the adjustment factor fi is constant, the ratio,

MDi
ST ∗ M i
=(
) fi ,
Di
Ci + ST ∗ M i
should follows a concave curve as we increase bus
utilization (by increasing message length, Mi). For
instance, when the processor utilization is 15%, there are
two segments of concave curves from bus utilization 10%
to 70% and from 70% to 90%. The segmentation indicates
a jump in the adjustment factors resulted from the
deadline decomposition algorithm. In Figure 11, the
concavity and the segmentation can also be seen in other
curves that represent the message deadline ratios of

different processor utilization. When the processor
utilization is high, fi may be modified gradually and
partition scheduling may fail if we introduce a sharp
increase to fi. Thus, the concavity and the segmentation
are not so obvious as the deadline ratio in an underutilized
processor.

execution and transmission, incremental changes, etc. We
are currently looking into different network
infrastructures and communication scheduling algorithms
that can be employed in the scalable IMA-based systems.

References
[1]

1.0
.9

[2]

Percentage of task deadline

.8
.7
.6

[3]

.5

[4]

.4
.3
.2

15%
30%
45%
60%
75%

Proc.
Proc.
Proc.
Proc.
Proc.

.8

.9

Util
Util.
Util.
Util.
Util.

[5]

.1
0.0
0.0

.1

.2

.3

.4

.5

.6

.7

1.0

[6]

Bus Utilization

[7]

Figure 11. The ratio of message deadline to task
deadline

5. Conclusion
In this paper, we present several algorithms in order to
produce cyclic partition and channel schedules for the
two-level hierarchical scheduling mechanism of IMAbased avionics systems. The system model of the IMA
architecture supports spatial and temporal partitioning in
all shared resources. Thus, applications can be easily
integrated and maintained.
The main idea of our approach is to allocate a proper
amount of capacity and to follow a distance constraint on
partition and channel invocations. Thus, the tasks
(messages) within a partition (channel) can have an
inactive period longer than the blocking time of the
partition (channel). Also we use a heuristic deadline
decomposition technique to find feasible deadlines for
both tasks and messages. To reduce bus bandwidth
requirement for message transmission, we develop a
heuristic channel-combining algorithm which leads to
highly utilized channels by multiplexing messages of
different deadlines and periods. The simulation analyses
show promising results in terms of schedulability and
system characteristics.
Based on the work in this paper, we have developed a
scheduling tool for the IMA-based avionics systems. The
tool includes additional features for practical
implementations, such as time-tick based processor
scheduling, non-zero context switch overhead, replication

0-7695-0707-7/00 $10.00 ã 2000 IEEE

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

“Design Guide for Integrated Modular Avionics,”
ARINC Report 651, Aeronautical Radio Inc., Annapolis,
MD, Nov. 1991.
“Avionics Application Software Standard Interface,”
ARINC Report 653, Aeronautical Radio Inc., Annapolis,
MD, Jan. 1997.
“Backplane Data Bus,” ARINC Specification 659,
Aeronautical Radio Inc., Annapolis, MD, Dec. 1993.
N. Audsley, and A. Wellings, “Analyzing APEX
applications,” Proc. IEEE Real-Time Systems
Symposium, Dec. 1996, pp. 39-44.
N. Audsley, A. Burns, M. Richardson, and A. Wellings,
“Hard real-time scheduling: the deadline-monotonic
approach,” Eighth IEEE Workshop on Real-time
Operating Systems and Software, 1991, pp. 133-137.
T. Carpenter, “Avionics Integration for CNS/ATM,”
Computer, Dec. 1998, pp. 124-126.
M. Y. Chan and F. Y. L. Chin, “General schedulers for
the pinwheel problem based on double-integer
reduction,” IEEE Trans. on Computers, vol. 41, June
1992, pp. 755-768.
Z. Deng and J. W. S. Liu, “Scheduling real-time
applications in an open environment,” Proc. IEEE RealTime Systems Symposium, Dec. 1997, pp. 308-319.
C.-C. Han, K.-J. Lin, and C.-J. Hou, “Distanceconstrained scheduling and its applications to real-time
systems,” IEEE Trans. on Computers, Vol. 45, No. 7,
July, 1996, pp. 814--826.
Y. H. Lee, D. Kim, M. Younis, and J. Zhou, “Partition
scheduling in APEX runtime environment for embedded
avionics software,” Proc. of Real-Time Computing
Systems and Applications, Oct. 1998, pp. 103-109.
J. Lehoczky and S. Ramos-Thuel, “An optimal algorithm
for scheduling soft-aperiodic tasks in fixed-priority
preemptive systems,” Proc. IEEE Real-Time Systems
Symposium, Dec. 1992, pp. 110-123.
J. Lehoczky, L. Sha, and Y.Ding, “The rate-monotonic
scheduling algorithm: exact characteristics and average
case behavior,” Proc. IEEE Real-Time Systems
Symposium, Dec. 1989, pp. 166-171.
J. Lehoczky, “Fixed priority scheduling for periodic task
sets with arbitrary deadlines,” Proc. IEEE Real-time
Systems Symposium, Dec. 1990, pp. 201-209.
C. L. Liu and J. W. Layland, “Scheduling algorithms for
multiprogramming in a hard real-time environment,”
JACM, vol. 20, No. 1, 1973, pp.46-61.
K. W. Tindell, A. Burns, and A. J. Wellings, “An
extendible approach for analyzing fixed priority hard
real-time tasks,” Real-Time Systems 6(2), 1994, pp. 133151.

MARCH 1989 VOL. 12 NO. 1

quarterly bulletin of the
IEEE Computer Society

a

technical committee

on

Data
eeri
CONTENTS
Letters to the TC Members
S. Jajodia and W. Kim (issue

1

editors),

Adding Intra—Transaction Parallelism
R.

and

to an

Larry Kerschberg (TC Chair)

Existing DBMS: Early Experience

2

Lone, J. Daudenarde, G. Hallmark, J. Stamos, and H. Young

Parallelizing FAD Using Compile—Time Analysis Techniques
B. Hart, P.

9

Valduriez, and S. Danforth

JAS: A Parallel VLSI Architecture for Text
0. Fnieder, K.C. Lee, and V. Mak

Processing

Parallel Query Evaluation: A New Approach to
T. Haerder, H. Schoning, and A. Sikeler

16

Complex Object Processing

23

Multiprocessor Transitive Closure Algorithms
R. Agrawel, and H.V. Jagadish

30

Exploiting Concurrency

37

in a DBMS
L. Raschid, T. Se//is, and C. Lin

Checkpointing and Recovery

Implementation for Production Systems

in Distributed Database

44

Systems

S. Son
Robust Transaction—Routing
V. Lee, P. Vu, and A. Leff

Strategies

in Distributed Database

51

Systems

58

Sharing the Load of Logic—Program Evaluation
0. Wolfson

SPECIAL ISSUE ON DATABASES FOR
PARALLEL AND DISTRIBUTED SYSTEMS

+
IEEE

flE II5TflWE ~ ~ECT~M
NC ELEC1~S ENOI€E~. IC

IEEE

Computer Society

Editor-In-Chief,

Data

Engineering

Chairperson, TC
Prof. Larry Kerschberg
Dept. of Information Systems
George Mason University

Dr. Won Kim
MCC

3500 West Baicones Center Drive
Austin, TX 78759

(512)

4400

338—3439

University

and

Systems Engineering

Drive

Fairfax, VA 22030

(703) 323—4354
Associate Editors

Vice

Prof. Dma Bitton

Prof. Stefano Ceri

Dept. of Electrical Engineering
Computer Science
University of lilinois
Chicago, iL 60680
(312) 413—2296

Chairperson,

Dipartimento

and

TC

di Matematica

Universita’ di Modena
Via

Campl

213

41100 Modena, italy

Prof. Michaei

Carey
Computer Sciences Department
University of Wisconsin

Secretary,

TC

Prof. Don Potter

Madison, WI

Dept. of Computer Science
University of Georgia

(608)

Athens, GA 30602

53706
262—2252

(404) 542—0361
Prof.

Roger King
Department of Computer Science

Past

campus box 430
University of Colorado

Dept. of Information Systems and Systems Engineering
George Mason University
4400 University Drive
Fairfax, VA 22030
(703) 764—6192

Chairperson,
Jajodia

TC

Prof. Sushli

Bouider, Co 80309
(303) 492—7398

Prof. Z. Moral

Ozsoyoglu
Department of Computer Engineering and Science
Case Western Reserve University
Cleveland, OhIo

44106

(216) 368—2818
Dr. Sunhl Sarin

Distribution

Xerox Advanced Information

Ms. Lori

Technology

4

IEEE

Cambridge Center
Cambridge, MA 02142
(617) 492-8860

The LOTUS

Rottenberg
Computer Society

1730 Massachusetts Ave.

Washington, D.C.
(202) 371—1012

Corporation

has made

a

generous donation to

20036—1903

partially offset

the cost of

printing and distributing four issues of the Data Engineering bulletin.

Database

Engineering

Bulletin is

a

quarterly publication of

the IEEE Computer Society Technical Committee on Database
its scope of interest includes: data structures
Engineering
.

control

Membership

In the Database

member of the IEEE

Computer Society may Join the TC as a
Computer Society may
participating member, with approval from at least

techniques,
intelligent front
ends, mass storage for very large databases, distributed
database systems and techniques, database software design
and implementation, database utilities, database security

Join

as a

one

officer of the TC.

and related

bulletin of the TC free of

and models,

access

strategies,

access

database architecture, database machines,

areas.

Contribution to the Bulletin Is hereby solicited. News items,
letters, technical papers, book reviews, meeting previews,
case studies, etc., should be sent to the Editor.
All letters to the Editor will be considered for publication
unless accompanied by a request to the contrary. Technical

summaries,

papers

are

unrefereed.

Opinions expressed In contributions

are

those of the mdi

vidual author rather than the official position of the TC on
Database Engineering, the IEEE Computer Society, or orga
nizations with which the author may be affiliated.

Engineering Technical Com

mittee Is open to individuals who demonstrate willingness to
actively participate in the various activities of the TC. A

full member.

A non-member of the

members of the TC

Both full members and

entitled to receive the

are

charge, until

participating
quarterly

further notIce.

From the Issue Editors
Sushil Jajodia and Won Kim
On December 5—7, 1988,

IEEE—sponsored symposium named the International Symposium on Da
Systems was held in Austin, Texas. The symposium was an attempt
to encourage interested professionals to focus their research on extending the technology developed
thus far for homogeneous distributed databases into two major related directions: databases for paral
an

tabases for Parallel and Distributed

lel machines and heterogeneous distributed databases.

We selected
on

seven

papers from the

symposium, and added

Databases for Parallel and Distributed

our

Systems.

two new papers to form this

The selection of papers in this issue

special issue

was

based

on

decision to maximize the breadth of research topics to be introduced to the readers. We regret

that

we did not have enough space to include a paper on heterogeneous databases. The papers
selected from the symposium had to be condensed because of page limits on our bulletin. The inter

ested reader may obtain the

this

proceedings of the symposium from

IEEE for

a

broader perspective

on

area.

Adding Intra —Transaction Parallelism to an Existing DBMS: Early Experience by Lone, et. al., and Paral
lelizing FAD Using Compile—Time Analysis Techniques by Hart, et. al. describe approaches to exploit
parallelism in databases in two major research efforts in parallel database machines. Friecler, et. al.
describe a text—retrieval subsystem which uses a parallel VLSI string—search algorithm in JAS: A Paral
lel VLSI Architecture for Text Processing.
Query Evaluation: A New Approach to Complex Object Processing by Haerder, et. al., and
Multiprocessor Transitive Closure Algorithms by Agrawal and Jagadish discuss issues in exploiting par
allelism in operations involving complex data structures, namely, complex objects and transitive clo
sures, respectively. Exploiting Concurrency in a DBMS Implementation for Production Systems by Ras
chid, et. al. describe parallelism in a database implementation of a production system. In Checkpoint
ing and Recovery in Distributed Database Systems, Son outlines an approach to checkpointing in dis
tributed databases and its adaptation to systems supporting long—duration transactions.
Parallel

Robust

Transaction—Routing Strategies in Distributed Database Systems by Lee, et. al., and Sharing the
Logic—Program Evaluation by Wolfson discuss approaches to load sharing in distributed and

Load of

parallel systems.
The authors who contributed papers to this issue were very prompt in meeting our tight deadlines; they
all very professional. The printing and distribution of this issue has been made possible by a

were

generous grant from the Office of Naval Research.

From the TC Chairman
Larry Kerschberg
I am

pleased

to welcome Don Potter as

of the

Secretary

of

our

TC. Further,

on

behalf of

our

TO, I

want to

organization and program
Shuey
Fifth International Conference on Data Engineering, held February 6—10, 1989 at the Los Angeles

congratulate John Canlis,

Richard L.

and their team

on

the excellent

Airport Hilton and Towers. Over 315 people attended the conference.

1

Adding

Intra-transaction Parallelism to

an

DBMS:

Existing

Early Experience
Raymond Lone, Jean-Jacques Daudenarde
Gary Hallmark, James Stamos, Honesty Young
IBM Almaden Research

Center, San Jose, CA, 95120-6099, USA

Abstract: A

loosely-coupled, multiprocessor backend database machine is one way to construct
This software architecture was the ba
a DBMS that supports parallelism within a transaction.
sis for adding intra-transaction paralielism to an existing DBMS. The result is a configuration
independent system that should adapt to a wide variety of hardware configurations, including
uniprocessors, tightly-coupled multiprocessors, aud loosely-coupled processors. This paper evalu
ates our software-driven methodology, presents the early lessons we learned from constructing an
operational prototype, and outlines our future plans.

Jntroduction

1

A database machine based

processors that share

nothing is one way to provide the
Proponents of the loosely-coupled approach claim such an
architecture can achieve scalability, provide good cost-performance, and maintain high availabil
ity DGG*86,DHM86,NecS7,Tan87]. Current database machine activity, both in the lab and in
the marketplace, is often driven by an emphasis on customized hardware or software. Although
hardware and software customizations may improve performance, they reduce the portability and
maintainability of the software, increase the cost of developing the system, and reduce the leverage
one gets by tracking technology with off-the-shelf hardware and software.
We believe the costs of customization outweigh the performance benefits and have taken a
software-driven approach to database machine design that focuses on intra-transaction paralielism.
Our approach is to make minimal assumptions about the hardware; design the DBMS for a generic
hardware configuration; support intra-transaction parallelism; and show how to map the system
To test our beliefs we are prototyping a configurationto particular hardware configurations.
relational
that
is
DBMS
independent
applicable to individual uniprocessors, to tightly-coupled
multiprocessors, and to loosely-coupled multiprocessors. We intend to use simulation, modeling,
and empirical measurements to evaluate this approach to database machine design.
functionality

of

a

on

multiple

conventional DBMS.

The rest of the paper is structured as follows. Section 2 discusses parallelism in the context of a
DBMS. Section 3 presents the goals of our project, which is calied ARBRE, the Almaden Research

Engine. Section 4 discusses the ARBRE design and shows how to apply it
to different hardware configurations. Section 5 compares ARBRE to existing work, and Section 6
presents and evaluates the research methodology used in the project. Section 7 relates our early
experiences and lessons from putting our methodology into practice. The last section describes the
current status of the ARBRE prototype and outlines future plans. Throughout the paper we shali
use the words transaction and query interchangeably.
Backend Relational

Parallelism in

2

a

DBMS

Most

run on a

use

currently available database systems have been implemented to
multiprogramming to support inter-transaction parallelism: while

some

2

single

processor and

transactions

are

waiting

for

I/O’s,

another transaction may execute CPU instructions. Tithe processor is a multiprocessor
engines, then N transactions may execute CPU instructions simultaneously. Most

system with N
systems

parallelism.

Intra-transaction
same

uniprocessor,

the threads not

processors could

as

a

On

transaction in order to reduce the response time for that transaction.

behalf of the

3

single thread and thus do not support intra-transaction
parallelism could be achieved by having multiple threads run on

execute each transaction

waiting for I/O share the one processor. On
simultaneously execute the threads in parallel.

multiprocessor,

a

a

several

Goals

paralielism, we established four goals
use parallel processing in a full-function, relational
project. First, we
DBMS to reduce the response time for a single data-intensive SQL request. This includes exploiting
parallel disk I/O and CPU-I/O overlap inside the request. Second, we wanted to be able to use
additional processors to reduce the response time further for data-intensive operations. Third,
we wanted to be able to use additional processors for horizontal growth to increase throughput.
Fourth, we wanted to maintain an acceptable level of performance for on-line transaction processing
To

gain insight

into the costs and benefits of intra-transaction

wanted to

for the AR,BRE

(OLTP)

environments.

To meet these

goals

we

could first propose various hardware

configurations

with different

num

bers of processors, different speeds, and different communication topologies and primitives. For each
configuration we could then design the most appropriate software organization. Such a methodol

time-consuming, especially if simulation and prototyping activities were needed
to evaluate and validate the various possibilities.
We instead designed the DBMS software to be independent of the hardware configuration, hop
ing to demonstrate that the approach is viable, and that the performance can be almost as good as
if the software had been customized for each hardware configuration—provided the communication
scheme has enough bandwidth, low latency, and reasonable cost.
Our intention is to reuse most of the code of a single-site relational DBMS with no parallelism
and to use several instances of such a DBMS to exploit intra-transaction parallelism. Each DBMS
instance is responsible for a portion of the database. It may execute on a private processor, or it
may be one of several instances sharing a large processor. We call the latter approach virtualization,
ogy would be very

because each instance of the DBMS is associated with
distribution of functions must be added to the

existing

a

virtual processor.

Code

to

support the

DBMS base under both approaches.

strictly interested in the parallelism issues, we are not trying to improve the
of
local
operations performed on a single processor. We accept current systems as
performance
that
and
the hardware and software technology will improve with time.
assume
they are
Since

4

we

are

System

Overview

ARBRE is best viewed
one

or

more

hosts.

as

being

a

backend database machine that is connected to

multiprocessor

Connections to local

database machine is assumed to be at

a

area

networks

are

also

possible.

The interface to the

sufficiently high level so that we can exploit parallelism
delays incurred by separating the backend database

query and minimize the communication
machine from the host.

within

a

We discuss the ARBRE system in three steps.

First,

we

present

our

assumptions

about the

processor and communication hardware. Then we focus on the software and execution strategy.
Finally, we describe how to map ARBRE onto real hardware configurations.

3

Configuration

A Generic Hardware

4.1

require, that ARBRE runs on a loosely-coupled multiprocessor. The
multiprocessor consists of a fixed number of processing sites interconnected by a
communication network that lets each pair of sites communicate. We make no further assumptions
about the network. Each site has its own CPU, memory, channels, disks, and operating system.
The sites run independently, share nothing, and communicate only by sending messages.
We assume, but do

not

hardware of this

ARBRE Software and Execution

4.2
We

assume

every site

runs

the

same

software.

Strategy
site has

Every

instance of the

one

DBMS,

and this

instance alone manages the data kept at that site. The data is partitioned horizontally RE78]:
each table in the relational model is partitioned into subsets of rows, and each subset is stored

by hashing or by key ranges. Key ranges can be
or can
automatically by the system as in Gamma DGG*86}.
by
ARBRE supports both local and global indexes. A local index contains entries for tuples stored at
the site containing the index. A global index is a binary relation associating a secondary key with
a primary key. That binary relation is itself partitioned as is any base table.
Since data is not shared, a site executing a request that involves data managed by another site
uses function shipping CDY86} to manipulate remote data. A function that returns a small amount
of data returns the result directly to the caller. For example, a function that fetches a unique tuple
at

one

The

site.

computes

or

of

tuples

partitioning

be controlled

can

be derived

the user,

determined

aggregate function falls into this category. Other functions

an

in the form of

tuple

streams. A

tuple

stream is

a

may return

large

sets

first-in-first-out queue whose head and

tail may reside at different sites.

The host

application program, which contains SQL
causes an asynchronous request in the host, so

runs

to the database

the

calls to the database.

Each call

it is

important to minimize the
Fortunately, relational queries
information requested. If host-backend

interaction between the host and the backend database machine.
are

at

a

interaction is
same

level and tend to return all and

high

a

problem,
as long

transaction

one

simple way to
processing is

as no

the

only

reduce it is to have the host batch requests inside the
done between requests. A

have the host batch requests from different transactions if the

Raising the level of the query language
the query language could express complex

more

resulting

general approach

is to

increase in response time

also reduce host-backend interaction. For

is tolerable.

can

example,

object

fetch and recursion. The ultimate step

general computation, and we have chosen this approach in our prototype
flexibility.
give
Before being executed the application program and the SQL statements it contains must be
compiled. The query compiler, which converts an SQL statement into a set of one or more compiled
query fragments, uses the database machine for interrogating the catalogs and storing the query
fragments.1 Some compiled query fragments are executed at one site, and other fragments are
sent to multiple sites and executed in parallel. One fragment is called coordinator fragment, and it
is responsible for coordinating the execution of the other fragments, which are called 3ubOrdiflate
is to have the backend do
to

maximum

us

fragments.
Each
same

site

compiled
or

fragment

is executed

at different sites communicate

the host sends

coordinator

query

a

request

fragment

to

some

as

a

separate lightweight thread.

by sending

site in the database

and executes it

as

a

thread.

messages and by using tuple streams. When
machine, this site fetches the corresponding

This thread becomes the coordinator for the

transaction and receives all further calls the host sends
1The compiler

can

reside in the host

or

in the database

on

behalf of this transaction.

machine; there

but the final decision is irrelevant to the paper.

4

Threads at the

are

arguments in favor of both approaches,

The coordinator

fragment

executes

that execute

consults the

uses

as

a

function

hashing

to execute

subordinate

separate thread and generally involves

subordinate

a

shipping

when that

fragnent
or key-range

function

fragment

one

fragments.

Each subordinate

base table. To decide the

involves

a

site(s)

table, the coordinator
table is horizontally parti

base

table that indicates how the

tioned.
How the results of

an

SQL

statement

are

returned to the host

depends

on

the

expected

size of

the results. If the amount of data produced by executing the query is small, the results are returned
to the coordinator which then assembles them and forwards them to the host. On the contrary, if
the amount of returned data is
in order to be returned to the

involving

large, and if the data does not need to be combined with other data
host, we send it directly from each subordinate to the host without

the coordinator.

A dataflow

approach,

similar to the

simultaneous work of many query

one

fragments

used in Gamma and
on

behalf of the

same

proposed

in

{BD82],

controls the

data-intensive transaction.

stream, send their substreams to

others,

Frag

receive the substreams

collectively produce
by others, and consume them. The communication software uses message buffering and a
windowing mechanism to prevent stream producers from flooding stream consumers.
When fragments must exchange large amounts of data, the communication may become a
bottleneck. One way to reduce communication is by a judicious choice of algorithms. For example,
both
a hash-based join works well in a distributed environment, but it requires sending practically
of
such
the
ideas
other
use
as
also
semi-join, the
tables on the network. We are
investigating

ments may

a

sent

possibility

of

completing

access

patterns.

4.3

Mapping

a

join

in the

host, and the

use

of

algorithms

that tolerate skewed data

Sites to Processors

Most database machine research

projects

and commercial

products

use

the

simplest mapping

from

sites to processors: these systems devote an entire physical processor to each site. This approach
is also applicable to AUBRE. In this approach, each site has an operating system that supports a

single instance of the DBMS executing in its own address space. Each DBMS instance supports
multiprogramming for inter-transaction parallellsm, but it has no intra-transaction parallelism.
Intersite communication corresponds to interprocessor communication.
Alternatively, one can map several sites to a single processor. The processor then contains as
instances share a single copy of the
many instances of the DBMS as there are sites, and all the
code. The same communication interface is used, but the implementation exploits fast memory-tomemory transfer, rather than actual communication via a network, among sites that are mapped
to a single processor.
4.4

Other Issues

keep our task manageable, we postponed detailed consideration of several important issues. In
particular, we examined the following issues only superficially: automatic query planning; catalog
management; management and replication of key-range tables; data replication and reorganization;
operational management of a large number of sites; and fault tolerance.

To

5

Related Work

projects, both in universities and industrial labs, are concerned with using multiple pro
cessors to improve performance of relational systems.
Among the systems that are most com
Tandem’s
Gamma
ARBRE
to
are
NonStop SQL2 product Tan87], and the
DGG*86],
parable
Several

‘NonStop SQL

is

a

trademark of Tandem

Computers Incorporated.

5

DBC/lOl23machine
pose processors,

partitioning
tems

are

joins.

as

and

built

by

Teradata

customized

employ
some degree

Nec87].

All three systems

use

systems, and support

operating

of intra-transaction

parallelism.

loosely-coupled general

one or more

pur

kinds of horizontal

The unusual features of these sys

listed below. Gamma has diskless processors to add processing power for operations such
Tandem’s NonStop SQL is a stand-alone computing system that executes applications

and supports end

FastSort,
prietary Ynet3,

DBC/1012

There is

users.

no

support, however, for intra-transaction parallelism except

Teradata’s DBC/1012 has a pro
several processors for a single sort.
which implements reliable broadcast and tournament merge in hardware. The

which

for

exhibits

uses

non-uniformity

of processors: each processor module has

and controllers and is connected to different kinds of

The ARBRE
the

differs

project

software

peripherals.

from these other systems

clearly

specialized

on

two accounts.

of

of that is

First, ARBRE is

logical sites onto real proces
AR.BRE tries to increase the

only project
studying multiple mappings
Second, unlike other multiprocessor backend database machines,
level of parallelism in the return of data to the host by avoiding the coordinator whenever possible.
Another feature of AILBRE is that no site is distinguished by having special hardware or special
software, at least at execution time.
we are aware

sors.

Methodology

6

We chose

a

research

methodology

to

support

main

our

objective, which

is to draw

some

conclu

the architecture of

a configuration-independent parallel DBMS, its
sions, as quickly as possible, on
feasibility, and its expected performance. As a result our methodology was designed around three
principles: (1) build an operational prototype by using sturdy components for the hardware, oper
ating system, and access method instead of constructing our own; (2) concentrate on the run-time
environment, postponing any development of the query compiler; and (3) complement the proto

type evaluation with simulation and modeling. The

rest of this section discusses each

principle

in

turn.

existing components rather than construct new specialized ones because the incre
mental benefits would not justify the cost of construction. We used a general purpose, existing
operating system (MVS) that supports multiple processes in a single address space. We also used
the low-level data manager and transaction manager in System It (B*8l}, an experimental relational
database management system. In addition, we used a prototype high-performance, interprocessor
communication subsystem (Spider) implemented by our colleague Kent Treiber. For hardware we
used brute force, relying on a channel-to-channel communication switch interconnecting multiple
IBM 4381 machines, which are midrange, System/370 mainframes.
We postponed the development of a query compiler and concentrated on query execution strate
gies that exploit parallelism without causing communication bottlenecks. We believe the develop
ment of a query compiler should be relatively straightforward once we have determined a repertoire
of good execution strategies. To support our investigation of execution strategies, we implemented
a toolkit of relevant abstractions. These abstractions fail into 4 categories: a generalization of func
tion shipping, virtual circuits and datagrams, single-table-at-a-time database access, and primitives
dealing with the horizontal partitioning of data. We used the same programming language (C++)
to implement these abstractions as we do to write compiled query fragments. This will make it easy
to migrate useful algorithms from query execution strategies into the database machine interface.
An operational prototype will give us enough information to drive simulations and validate the
results. First, we will instrument and measure a working environment. The information obtained
will then be submitted to a simulator to predict how the same workload will behave on different
We reused

3DBC/1012

and Ynet

are

trademarks of Teradata

Corporation.

6

meaningful results we plan to record events produced by executing real
as
produced by executing synthetic workloads. From the event traces
we will determine data and processing skews and produce probability distributions that concisely
describe these skews. The probability distributions, and not the raw event traces, will drive the
simulations. Given our flexibility in mapping logical sites onto multiple configurations, we anticipate
valldating the simulation results on multiple physical configurations that are easy to produce.
Configuration independence has improved our programming and debugging productivity be
cause we do not work exclusively with the target hardware, operating system, access method, and
communication system. Most of the time we use an IBM RT PC running AIX, which is IBM’s
implementation of the UNIX operating system.4 We use a single address space on the ItT PC and a
simple, main-memory based access method to emulate a multiple site system. Almost all software
is developed and thoroughly debugged in this user-friendly environment before it is run on a target
configurations.
applications as

To obtain

those

well

system.

methodology: (1) The simulations are based on probability
dependencies. (2) Simulation runs may be time consuming.
For this reason we plan to use modeling which, when validated with a more detailed simulation, may
be used to extrapolate our results to other configurations in much less time. (3) Our methodology
does not consider configuration-specific optimizations; these should be identified and studied inde
pendently. Nevertheless, we believe that these drawbacks are tolerable and that our methodology
is appropriate for gaining valuable insight into DBMS parallelism in a short time period.
The

following

are

drawbacks to

our

distributions rather than actual data

Early Lessons

7

Over two years of preliminary research, design, and prototyping have taught us three things: good
building blocks are indispensable, language design is hard, and simulation has its limitations.
learned is not to start from scratch

though
of specialization is often highlighted as an important advantage of
is less fruitful to spend time rewriting mature, highly-tuned code
transaction parallelism.
One lesson

we

If you don’t start from

even

software

simplification

because

backend database machines. It
than it is to

implement

intra

likely modify existing code, in which case it
components. For example, the transaction manager we
two-phase commit and distributed recovery, and adding a two-phase
you will most

scratch,

to have modifiable software

is

important
already had hooks for
commit protocol was straightforward. We have added
thread scheduler, and if we implement global deadlock
used

transaction waits-for

graph

message queues and timers to the DBMS

detection

we

must be able to extract the

from the lock manager.

language design is hard. We initially tried to design a custom
language for coding the query fragments, but discovered that language design without sufficient
experience in the domain of discourse is too slow and required too many iterations. Instead we
are using an existing programming language (C++) and have built a toolkit of useful abstractions.
The toolkit lets us experiment with algorithms without designing and freezing a language and its
interpreter. As we gain experience we will progressively develop our toolkit, using more predefined
constructs and less ad-hoc programming in the fragments. Eventually, a “language” will emerge
that succinctly expresses good execution strategies for query fragments. This language will be the
target of the query optimizer and compiler.
A second lesson

A third lesson
We

we

we

initially thought

urations,
4RT PC

but

learned is that

learned is that
we

simulating

and AIX

are

could

use

interesting

some

the

raw

the exact data

dependencies

trademarks of the IBM

issues may be difficult to

study

in simulation.

event traces in simulations of different hardware

Corporation.

7

would make the simulations too
UNIX is

a

trademark of the AT&T

config
expensive to

Corporation.

run.

Instead,

data

dependencies

and other nonuniformities will be

approximated

with

probability

distributions.

Status and Plans

8

The prototype is operational on three interconnected dyadic-processor 4381 systems. Although
have begun measuring the system for complex queries involving sorts and joins, the results

we

are

to be

reported here. Suffice it to say that for a single data-intensive transaction
We used multiple sites on a single 4381 processor
we have illustrated all aspects of parallelism.
to
exploit I/O parallelism; we used multiple sites on tightly-coupled dyadic
(i.e., virtualization)
processors to exploit CPU parallelism; and finally we used multiple sites on separate 4381 systems
too

to

preliminary

exploit loose coupling.

The prototype will be extremely useful as we begin to study issues that are inherent to DBMS
parallelism, including: the need for sophisticated parallel algorithms; load balancing and process
scheduling; and communication problems, such as convoys, network congestion, and deadlock.
We are also beginning to investigate query optimization and support for high rates of simple
transactions. Skewed data access patterns and a larger number of smaller processors will exacerbate
some of the above problems and may demand innovative solutions.
Our approach to DBMS parallelism, which distinguishes logical sites from physical proces
sors, is a promising approach that can adapt to different hardware configurations, different costperformance trade-offs, and different levels of required performance. We envision a single code
base that is applicable to a cluster of high-end mainframes as well as to a network of powerful

microprocessors.

References

B*81]

M. W.

BD82J

Haran Boral and David J. DeWitt.

Blasgen et al. System
20(1):41—62, January 1981.

R: An architectural overview.

IBM

Systems Journal,

Applying data flow techniques to data base machines.
Computer, 15(8):57—63, August 1982.
D. W. Cornell, D. M. Dias, and P. S. Yu. On multisystem coupling through function
request shipping. IEEE Transactions on Software Engineering, SE-12(1O):1006—1017,
IEEE

CDY86]

October 1986.

DGG*86]

David J.

Kumar,

DeWitt, Itobert

chine. In

DHM86J

Nec87J
RE78]

Gerber, Goetz Graefe, Michael L. Heytens, Krishna B.
Gamma—a high performance datafiow database ma
12th International Conference on Very Large Data Bases,

H.

and M. Muralikrishna.

Proceedings of the
pages 228—237, August 1986.
Steven A. Demurjian, David K. Hsiao, and Jai Menon. A multi-backend database system
for performance gains, capacity growth and hardware upgrade. In Proceedings of the
2nd International Conference on Data Engineering, pages 542—554, 1986.
Philip M. Neches. The anatomy of a data base computer system. In Proceedings of the
2nd International Conference on Supercomputing, pages 102—104, 1987.
D. Ries and It. Epstein. Evaluation of Distribution Criteria for Distributed Database
Systems. UCB/ERL Technical Report M78/22, University of California—Berkeley, May
1978.

Tan87}

Group. Non-stop SQL, a distributed, high-performance, high
availability implementation of SQL. In Proceedings of the 2nd International Workshop
on High Performance Transaction Systems, September 28—30 1987.
The Tandem Database

8

PARALLELIZING FAD
USING COMPILE-TIME ANALYSIS TECHNIQUES

Brian Hart, Patrick Valduriez, Scott Dan forth

Computer Architecture Program
Microelectronics and Computer Technology Corp.
Advanced

Austin, Texas 78759
ABSTRACT
FAD is

database

a

programming language

FAD programs

language.

to

are

with much

be executed

higher expressive

efficiently

Bubba,

on

power than

a

system designed for data—intensive applications. Therefore, parallelism inherent in
must

program

be

automatically extracted. Because of the expressive

tional distributed
sent

general

a

query—optimization techniques

solution

the

to

of FAD programs based

parallelization

a

FAD

power of FAD, tradi

sufficient. In this paper,

not

are

query

parallel computer

a

on

we

pre

compile—time

analysis techniques.

1. Introduction

Ban87, Dan89]

FAD

transient and
As

a

persistent

database

that embeds
allows

objects.

a

strongly typed functional—programming language designed
Bubba,

programming language,

a

query

particular,

highly parallel

a

FAD reduces the

language (e.g., SQL)

into

a

referential object

gress in both

compiled

a

(SIMD

or

of

compiler

FAD

communicating

MIMD)

parallelism

extracts

The

fashion.

program into components and the

optimization techniques

serve as

expressiveness of

In

the presence of

complexity.

FAD.
a

In this paper,

parallelizing

FAD.

ming language,
on

we

we

focus

we

give

an

on

a

a

FAD

Bubba

on

technology.

a

use

solution

are to

these

approach

directly

disjuncts and
a

blending

of

The result is

a

benefits from pro

performance,

FAD is

can

problems

it

by transforming

be executed in

a

parallel

determine the most efficient division of

can create

constructs, such

short overview of the

incorporates

based

as

on

a

Traditional distributed query—

but these must be extended

of object identity

to

sets,

FAD program

efficient location of their execution.

the

MCC.

database system.

(“parallelizes”)

compiler

tuples,

To increase

at

The FAD data model

and relational databases.

called components, which

be addressed

to

manipulating

Bor88] developed

atomic values,

fully supported.

parallel

powerful programming

provide

After

most

the

inherent in

the basis for the

particular,

number of

In this paper,

on

subprograms,

problems

on

implementation

and relational—database

into low—level code to be executed

set

is

programming

with clean semantics whose

parallel processing

The FAD
into

“impedance

for

mismatch” of the traditional

programming language (e.g., C).

sharing KhoS7]

proven concepts from the worlds of functional

strongly typed language

database system

combinations of data structures based

arbitrarily complex
1n

is

data within

difficult

considerably due

to

aliasing problems. Also,

iteration and conditionals, adds

compile—time analysis techniques.

compile time analysis techniques employed for

introduction of Bubba and of the most salient features of the FAD program

FAD

parallelization

which

Bubba.

9

plays

a

central role in

compiling

FAD for execution

2. Bubba

Bubba is

parallel computer system

a

for mainframe systems

ment

amounts

of shared data for

Three constraints
on

and

processing

ful programming
bases

shape

number of concurrent

large

a

the

problems

ability requirements imply
Bor88] gives
includes

the rationale for

one

“small” for

or more

picking

microprocessors,

two reasons:

to have little

attempts

redundancy

multiple workload types

concurrent

complex patterns, imply

the need to support

Figure

a

to

approach

1. Each

local main memory

also used

are

impact

overall

power

data

Large

High—avail

for the Bubba architecture. The

node, called Intelligent Repository (IR),

(RAM)

and

a

disk unit

on

which resides

interface Bubba with other machines. An JR is believed

1) they provide cheap units of expandibility
on

a

trans

and real—time fault recovery mechanisms.

the army of ants

hardware architecture is illustrated in

to

by Bubba:

rich environment for program management and execution.

a

the need for

local database. Diskiess nodes

likely

application types.

be addressed

searches for

large

access to

high—availability requirements. Multiple workloads, particularly

knowledge—based

language through

to

replace

as a

minimization of data movement and thus program execution where the data lives.

imply

simplified

Bubba is intended

applications.

providing scalable, continuous, high—performance per—dollar

shared data, large databases, and

action

for data—intensive

performance,

and

and

2) knowledge

the loss of

conversely,

that IRs

an

a

be

IR is

will lead to

“hefty”

are

exploit locality through clever physical—database design, thereby limiting

to

the class of

applications

for which Bubba would be useful.
The
a

copy of

only
a

shared

resource

distributed

is the interconnect which

operating

management, communication and database functions. In
an

JR but

global object identity

To favor
IRs.

is not

parallel computation

Declustering

is

a

a

on

by the

High availability
a

third copy

on

program

is

which

an

runs

object identity is supported

within

horizontally partitions
access

are

declustered

and distributes each relation

frequency

programs where the data is

of the relation

Kho88]

to

avoid

across

across a

Cop88].
moving

The

data.

individual program is determined by the number of nodes the

occupies.

provided through

checkpoint—and—log

Figure

local

service. Each JR

low—level support for task

data, the database consists of relations which

to execute

Therefore, the degree of parallelism in
data referenced

things, provides

particular,

function of the size and

basic execution strategy of Bubba is

message—passing

a

supported.

placement strategy

number of IRs. This number is

provides

system which, among other

1:

the support of two on—line

copies of all data

on

IRs.

Simplified

Hardware

10

Organization

of Bubba

the IRs

as

well

as

3. FAD
The central ideas of FAD
The FAD type system

provides

corresponds closely

domain of data,

a

domain. FAD data

relatively few: types, data, actions,

are

well

as

distinguished

are

that of

to

an

order—sorted

functions for creating and

as

between values and

Kho87].

The structure of FAD data

disjunct).

The term action is used to indicate

can

change existing objects. Application of

be

simple

or

performing

complex (using
that

function to its arguments denotes

a

data

which

on

they

constructors, for

In

act.

writing

addition, FAD provides

programs. These

are

a

parallel

important

of

product

generalized select—project—join (SPJ) capability.
tion

(group, pump), variable
(do—end, begin—end, abort).
FAD

is

Har88]
Bubba

not

definition

essentially provides
an

are

enhancement

Bubba

(let),

users

the FAD

to

visible in FAD. PFAD is

an

manner

compiler

filter

number of

a

to

of translation

difficulty

to a

IRs. A component may

conditional

4. The FAD

on

with

a

language

very different

language.

The

Bubba.

Designing

a

FAD

based

on

parallel computation.
based

on

a

parallel

compiler performs

which includes schema
information leads

parallel

providing

for set

a

manipula

and control

(or PFAD)

execution model of

FAD with the concepts of

as an

intermediate

language by

which actions will be executed, and the

similarity

between FAD and PFAD elimi

A FAD program is

one

fash

function in

a

set, thus

(whiledo),

may be executed
are

used

one

by the parallel system

input

as

into

partitioned

to

or

at

other components.

between components.

a

FAD program into

compiler is

a

challenging

to

a

low—level object program that may be

research

The

compiled

static type

(typing),

checking,

that combines

issues. A FAD program expresses

program

execution model with

project

accesses

use

constructs

explicit intra—program

correct

and concise

computation
set

as

on

operators

stored in the

optimization

knowledge

of

a

FAD pro

of the Bubba database

statistics, cost functions and data placement information. Utilization of this

efficient low—level programs for execution

run—time type checks while

optimization decisions.

compilation,

communication.

transformation and

compiler has precise

a

such

physical objects (actually

helping

on

Bubba. Static type

The

checking

avoids

the FAD programmer to write correct programs. The

will infer transient types when appropriate. The major characteristic of the compiler is
crucial

applies

a new

iteration

supplements

centralized execution model and may

a

gram. To achieve those functions, the

pensive

at

transient data which

dependencies

compiler Va189] transforms

conceptual objects
database)

parallel

a

centralized execution model. Parallel FAD

abstraction of Bubba that

produce

sets in

on

provided

are

that captures aspects of the

parallel processing and distributed query—optimization
that favor

with respect to the

Compiler

The FAD
executed

set and

functions, called action

order

produce

(if—then—else),

concerning the locations

Those transient data establish dataflow

tuple,

data, and may

returns

statement, which

Other action constructors

components and because the data is declustered, each

one or more

updated

action. Abstraction in FAD

an

higher

sets to

in which actions at different locations communicate. The

nates the
more

reflect decisions

data,

parameterized

are

component and inter—component communication primitives. PFAD is used
the FAD

be shared and

can

provided for operating

set—oriented action constructor is the

each element of the Cartesian

to

fixed set of

A type in FAD

elements of that

on

operators that construct aggregate actions from actions, data,

and functions. A number of FAD action constructors
ion. The most

actions

constructors such as

accesses

allows the creation of user—defined first—order functions, actions that

(action abstractions).

algebra Dan88b].

objects: only objects

computation

a

and functions

compiler optimizes

11

a

to

make

a

ex

compiler

number of

FAD program with respect to communication, disk

main—memory

access,

response time
The

utilization and CPU

total work. The. latter is

or

compiler comprises

four

parallelizing approach

arise when

parallelizing

techniques
sometimes

correct) options.

provide

1)

several factors:

when

5.

for

and

languages

local viewpoint may

The

search

parallel.
stored

tree

on,

sends it

a

to a

is

aggressiveness

a

are

to the

tree).

involve

(more) operations

will be executed in

the central IR.
For

parallelizer

possible

are

data the

be

stored

updates

to

is the

run

to

The

a

problem

the

that this

are some correctness

decisions. So the

correct

possible translations (PFAD programs). The

are

simple

and

correct, but

guaranteed

only minimally

optimizer

uses

and

the IRs it is

FAD program at that central

operations from the
on,

at

the

performs

generated using

heuristics

to

updates there.

set

a

of

strategies

the search tree.

explore

parallelizer

increment

parallel

at

through the program

the IRs which hold

operations need,

but is not

FAD program for the relational

already

some

uses a set

of

at

and

it

so

transforming

persistent data,

those IRs, must be

algebra expression:

sent to a

back.)
set

central IR, where the select and the

Then the

of IRs

two

joins

parallelizer considers executing the select

holding

data from relation R is

parallelizer proceeds

as

illustrated in

a

good

choice.

Figure 2; the select

at

rather
sent to

t’< S 1~1

T

the centralized PFAD program in which relations R, S, and T
and

on

send

general, the

translations.

But there

consider

translates this

the central IR. In

sense to

globally optimal,

PFAD

complexity of

the

to

there

produce only locally

IRs it is stored

moving another

Then any data the

example,

they

are no

at T

and similar

the choices must also be checked; the

strategies generally

join2

equivalent

an

Because of the

viewpoint.

not

may

The

The

from the IRs

two

of possibilities

tree

constraints

FAD program into

techniques used,

viewpoint

some

(There

by

from the

check the choices.

to

those IRs.
The

of the

local

a

a

incremental transformations from their parents,

problems discussed below,

at

with

central IR, executes all the

single

Because of the

than

search

a

correctness

centralized PFAD program which retrieves all needed persistent data

the choices in the search

that

transforms

search tree whose nodes

(they generate
analyses

details)

incrementally

IR, sends all persistent data updates back
Successor nodes

speculative (and

FAD

Parallelizing

trivial translation which is

a

parallelization

alternate translations.

to pursue

It generates

and

instance is influenced

given

heuristic evaluation of

irrelevant because of

or

more

a

produce locally optimal decisions, but

parallelizer explores

root of the

several

(and always correct)
a

as

con

that may

problems

correctness

application of

in

are

compilation phases.

next

flag potential

and type

decisions

optimization

the

by

parallelization technique

a

issues discussed below such that this local

parallelizer needs

to

The

followed by the heuristic

is unavailable

program. It does most of its work
and output

analysis

minimizing

using the dataflow—analysis results; 2) performance, using input

parallelizer (see Har88] for

The

dataflow

3) performance, using

optimizer

Analysis Techniques

input

use a

Selection of

correctness,

from the

input

checking (type inferencing

and annotations to be used

tradeoff space between conservative

a

when available; and

optimizer

is to

static type

may be biased towards

throughput.

object—code generation.

and

FAD program,

a

that

suitable to maximize

subsequent phases:

signment), optimization, parallelization,
veyed using FAD (e.g., filter ordering)
The

Va188]. Optimization

costs
more

are

are

at

So

o

R.

retrieved

performed.

IRs other than
now

R, joini

there
at

are

5, and

resulting translation.

are

in

several

parallel,

operations need,

possible problems.
with the data that
to

One is

to

determine whether the

they will be getting

parallelize operations

that

12

are

at run

time.

sequentialized

operations

Others
in the

are to

input

involved make
determine what

FAD program, to

select at central
at central
at central

joini
join2

select at R
at central
at central

join 1
join2

I select

A

at

jjoinl

at A

Ljoin2

at central

select at A
at S
at central

select at A
at S
at S

joini

Figure
updates

to

joinl
join2

1

select at A
at S
join2 at T

joinl
join2

handle

select at A

joini
join2

2: Search Tree

aliases and non—local

objects,

at

hash—join

at central

select at R
at S

I

joini

J

join2

at

hash—join

Example

and to emulate

global object identity using only

local

object

identity.
5.1 Abstract Evaluation
The

bolic

or

problems discussed above

interpretation elsewhere),

The

particular abstraction

mations

correct

are

The

analyses

analysis

getting

and how

at run

time.

us

5.2 Data—Distribution
The motivation for

based

on

reasoning

about

a

abstract evaluation

program

at

program

properties

about the program,

we

specifically

operations

sense

sym

It ab

using that abstrac

wish

to reason

about.

whether the transfor

if not.

(DD) analysis and object—sharing (OS) analysis.
make

(called

compile—time.

and then evaluates the program

particular

something

object—sharing analysis

An

are

execute on,

they might be corrected

data—distribution

are

checks whether

tool for

is based upon the

The results of the abstract evaluation tell

tion

a

the domain of data that the programs

stracts

tion.

that check the

analyses

abstract

to

be

run

in

parallel, with the data

A data—distribu

that

they

will be

checks the others.

Analysis

a

data—distribution

analysis

is best illustrated with the

following

FAD program:

prog()
let

x

in

if

f()
then g(x)

p(x)

else

h(x)

If the “if—then—else” executes at several IRs, then “x”

meaning “p(x)” might
some

allel

IRs and

program)

if—part

must

have different values

“h(x)” might
results,

be the

be executed

we cannot

same

or

say for

at

at

different IRs,

other IRs.

sure

With respect to

“wholly” present

a

PFAD data

have different values at different IRs,

meaning

While this

“g(x)” might

that

might give

at

each IR executing the

(DD)

item, placed

13

determines this.
means

that

(1)

be executed at

equivalent (to

us

when it will and when it will not.

An abstract evaluation of data—distribution
two terms.

might

the non—par

So the data items used

by

an

if—part.
Before

we

describe DD,

if the data item is

an

we

define

atom, then the

atom’s value
on;

(2)

be considered with respect to the database relation’s

can

if the data item is

the data item is

a

tuple, then the tuple contains

set, then each data item in the set is

a

PFAD data item,

placed correctly

with the database relation’s

is

W~,

only

IR

one

whole item, wrong

one

mented

DwDdistributed
and is

0
The

This is

operations

on

the “if—then—else” would be

5.3

if

to a

six DD values.

are

placed correctly

or

there

duplicates.
placed

cor

to

one

means

FAD

a

set, is

fragmented

over more

than

and “h”

correctly,

not

and is

frag

duplicates.

It may contain

IR.

but

placed

The data item is

a

placed but

set, is

not

correctly

duplicates.

useless data.

operations.
and “h”

“p”, “g”,

“p”, “g”,

and is

As

were

example, consider

an

all “W”, then the DD of the “if—then—else”

“W”, “D”, and

were

the “if—then—else” in

“Dw”, respectively,

then the DD of

“DW”.

Object—Sharing Analysis
When

the

than

over more

If the DD of

With respect

each IR, but it is not

at

placed correctly,

It does not contain

JR.

one

correspond

If the DD of

There

on.

each IR, and either it is

The data item is

anything else and

DD

the above FAD program.
would be “W”.

set, is

a

item, wrong place, with duplicates.

fragmented

other.

than

more

over

(3)

or

duplicates.

place.

distributed item, wrong

D~

placed.

and is

placed;

duplicates.

It does not contain

JR.

placed

The whole data item is present

place.

The data item is

distributed item.

D

at

and is

atom

an

the JR it is

and the atom’s value involved agrees

It does not contain

the data item.

It does not contain

rectly.

tuple

an atom or

function for the IR it is

declustering

containing

attribute that is

that the data item is

means

The whole data item is present

whole item.

W

an

declustering function for

update

executed

data item is

a

must

be

performed

on

at the IR where the data

sating update

so

For

the proper JR.

is,

This is done

by

either

by placing the update somewhere else,

or

update

the

placing

but also

JR),

another

at

so

that it is

a

compen

placing

that it is executed at the IR where the data is.

An abstract evaluation of

in FAD.

(possibly

and that data item aliases another data item

updated,

object sharing is used

“db.D1” is the

example,

the element of “db.Dl” with the

“db.D1” with the

key

“1”.

The

path

to

to

determine this.

Let P be the

key “1”, and “db.D1@1.wage” is the path

paths

form

a

set

of

paths

to

the database relation “db.Dl”, “db.D1@1” is the

partial

order.

For

example,

“db.D1@1” which includes the object “db.D1@i .wage”. Unnamed objects

to

objects

path

to

the wage of the element of

“db.D1” includes the object

are not a

problem

because they

cannot be aliased.

The
sets

of

operations

objects

the set of

on

is the union of the

are

also

kept track of.

objects

operations

it aliases

are

the FAD

objects in the

the OS for “x” is “x=db.Di@?1 “. This
tions and the

to

operations.

two sets.

For

example,

the OS of the union of

The OS of the difference of

two sets

of

two

objects is

the left argument.

objects in

Aliases

paths correspond

For

as

updated

parallelized.
with

a

object “db.D1@1”,

if the variable “x” aliased the

object—sharing information determines

which may be

marked

example,

Further, when

subscript

14

“u”.

For

an

the data needed

object

example,

is

updated,

by

then

the opera

it and all the

if the variable “x” above

was

then its OS would be

updated,

objects it aliases

are

variable “x” above

was

If the

example).

marked

variable

object

an

“x”

above

Another

“c” and

subscript

in the

“wrong” place.

This

was

unique

a

an

sent

the

copy number.

aliased

to

or

For

another

non—local

by

either

if the

“16” is

an

IR, then its OS would be

updated,

then its OS would

object is updated.

placing

the

update

somewhere else, but also

update

example,

“Xc16=db.Dl@lcI6” (the

another IR and

sent to

be corrected

can

by placing

or

and

updated

was

is sent to another IR, it and all the

We have

so

that it is

a

compen

placing

that it is executed at the IR where the data is.

so

problem

aliased twice, but

by

is

“db.D1@lcl6db.D1@Icl?”,

different

copies of the

At run—time, these will be

faithfully.

a

which determines that

executed at the IR where the data is,

sating update

with

If the variable “x” above

“Xc16u=db.D1@lc]6u”,

updated

copied

object

an

sent to another IR, then its OS would be

“Xucj6=db.Dl@luc)6.
be

as

When

“x~—db.DI@1~”.

which

means

that

we

have

the

object, because global object identity

same

represented

as

different

objects

same

is not emulated

objects.

6. Status
The FAD

continues

on

compiler, including

it.

Bubba is

the

has been

parallelizer

being implemented

on

a

operational since November 1988,

40—node Flexible

and work

Computers multiprocessor.

References

Ban87]

F. Bancilhon, T.

Language”,

Bor88]

Briggs, S. Khoshafian, P. Valduriez, “FAD, a Simple
on VLDB, Brighton, England, September 1987

and Powerful Database

mt. Conf.

H. Boral, “Parallelism in Bubba”, mt.

Symp.

Databases in Parallel and Distributed

on

Systems,

Austin, Texas, December 1988.

Cop881

G.

Copeland, B. Alexander, E. Boughter,
Chicago, Illinois, May 1988.

T. Keller, “Data Placement in Bubba”, ACM SIGMOD

mt. Conf.,

Dan89]

S. Danforth, S. Khoshafian, P. Valduriez, “FAD,

MCC Technical

Har88]
on

Kho87]

Database

Programming Language, Rev.3”,

“Parallelizing a Database Programming Language”,
Parallel and Distributed Systems, Austin, Texas, December 1988.

B. Hart, S. Danforth, P. Valduriez,

Databases in

S. Khoshafian, P. Valduriez, “Persistence,

tive”, mt. Workshop

Kho88]

a

Report DB—151—85, Rev.3, January 1989.

on

Database

Sharing

Knowledge

Object Orientation:

a

Symp.

database perspec

Programming Languages, Roscoff, France, September 1987.

S. Khoshafian, P. Valduriez, “Parallel Execution

Machines and

and

mt.

Base Machines,

Strategies for Declustered Databases”,

Kitsuregawa

Database

and Tanaka Ed., Kiuwer Academic Publishers,

Boston, 1988.

Val88J

P.

Valduriez, S. Danforth, “Query Optimization in FAD,

a

Database

Programming Language”,

MCC Technical Report ACA—ST—316—88, Austin, Texas, September 1988.

Danforth, T. Briggs, B. Hart, M. Cochinwala “Compiling FAD, a Database Pro
gramming Language”, MCC Technical Report ACA—ST—019—89, Austin, Texas, February 1989.

Va189]

P. Valduriez, S.

15

JAS:

A

PARALLEL VLSI ARCHITECTURE
FOR TEXT PROCESSING

0. Frieder, K. C. Lee, and V. Mak
Bell Communications Research
445 South Street
Morristown, New Jersey 07960-1910

A novel, high performance subsystem for information retrieval called JAS is introduced. The
of
each JAS unit is independent of the complexity of a query. JAS uses a novel, parallel, VLSI string
complexity
search algorithm to achieve its high throughput. A set of macro-instructions are used for efficient query processing.
The simulation results demonstrate that a gigabyte per second search speed is achievable with existing technology.

Abstract.

1.

Introduction

Many recent research efforts have focussed on the parallel processing of relational (formatted) data via the use of
parallel multiprocessor technology Bar88, Dew86, Goo8 1, Got83, Hi186, Kit84]. In Bar88], the use of dynamic
data redistribution algorithms on a hypercube multicomputer is described. The exploitation of a ring interconnection
is discussed in Dew86, Kit84]; modified tree architectures are proposed in Goo8l, Hil86]; and a multistage
interconnection network as a means of supporting efficient database processing is described in Got83]. However,
except for a few efforts Pog87, Sta86], relatively little attention has focussed on the parallel processing of
unformatted data.
For unformatted data, most of the previous efforts have relied on low-level hardware search support (associative
memory). Even the software approaches on parallel machines Pog87, Sta86] have relied on algorithms best suited
for low level enhancements. In Pog871, parallel signature comparisons were studied on the ICL Distributed Array
Processor (DAP) architecture, and Sta86] discusses the utilization of the Connection Machine for parallel searching.
are found in Sto87] and Sa188].
search
architectures are based on VLSI technology. VLSI technology supports the
Associative-memory
implementation of highly parallel architectures within a single silicon chip. In the past, hardware costs exceeded
software development costs. Thus, software indexing approaches were used to reduce the search time. Currently,

Critical reviews of Sta861

since the design and maintenance of software systems is more costly1 than repetitively structured hardware
components, using VLSI technology to implement an efficient associative storage system seems advantageous.
Furthermore, besides the cost differential, VLSI searching reduces the storage overhead associated with indexing (300
percent if word level indexing is used Has8l]) and can reduce the time required to complete the search.
Two critical problems associated with supporting efficient searching are the I/O bottleneck and the processor
incompatibility. The I/O bottleneck is the inability of the storage subsystem to supply the CPU with queryrelevant data at an aggregate rate which is comparable to the aggregate processing rate of the CPU. Processor
incompatibility is the inconsistency of the instruction set of a general-purpose CPU, e.g., add, subtract, shift, etc.;
and the needed search primitives, e.g., compare two strings masking the second, sixth, and eleventh character. To
these problems, special-purpose VLSI processing elements called data filters have been proposed Cur83,
Has83, Ho183, Pra86, Tak87]. The search time is further reduced by combining multiple data filters on multiple
data streams to form a virtual associative storage system. Thus, the advantages of an associative memory can be

remedy

exploited

without

incurring

the associated costs.

databases2,

With the continued growth of unformatted, textual
a large virtual associative memory should be
based on unconventional I/O subsystems and very high filtering rates to continue supporting adequate response
times. Currently proposed filtering rates have hovered at roughly 20 MBytes per second. We propose an I/O
subsystem called JAS, with filtering rates comparable to the next-generation optical disks and/or silicon memory
systems. JAS consists of a general-purpose microprocessor which issues the search and control instructions that the
multiple VLSI data filters execute. Only the portion of data that is relevant to the query, e.g., related documents in a
text database environment, are forwarded to the microprocessor. In this paper, we discuss the design and usage of a
VLSI text data filter to construct a subsystem for very large text database systems.
The remainder of this paper is organized as follows. Section 2 briefly describes the JAS architecture. A
description of the Data Parallel Pattern Matching (DPPM) algorithm, which forms the basis for the design of our

parallel

1

We

2

The

measure cost

legal

in

terms

of both fmances and human effort.

database Lexis is estimated at

databases have been

growing

at a rate

over

125

GBytes

of information

of 250,000 documents per year

16

Sta86].
Ho1791.

It is

reported

that information retrieval

presented in Section 3. A performance study of the JAS system is
concludes this paper with a discussion of the JAS system.
data filter, is

presented

in Section 4.

Section 5

JAS
System Architecture
Customized VLSI filters are used to ‘perform high-speed subsiring-search operations. The novel string-search
algorithm used in JAS improves the search speed by an order of magnitude as compared to prior CMOS filter
technology (e.g., Tak87]). We decouple the substring-search operation from high level predicate evaluation and
query resolution. Thus, complex queries can be evaluated but do not nullify the simplicity and efficiency of the
2.

search hardware.
A lAS system is

comprised of a single “master” Processing Element (PE) controlling a set of gigabyte per
second “slave” Substring Search Processors (SSPs). While previously proposed text filters Cur83, Has83, Tak87]
evaluate complex queries via integrated custom hardware, in lAS the predicate evaluation and query resolution is
decoupled from the primitive substring-search operations. A complex query is decomposed by the PE into basic
search primitives. In Cur83, Has83], complicated circuitry is required to support state transition logic and partial
results communication for cascaded predicate evaluation. In JAS, since the complexity of an individual query is
retained at the PE level, and only a substring-match operation is computed at the SSPs, only simple comparator
circuitry is required.
Figure 1 demonstrates the processing of a query within a JAS system. Each PE forwards a sequence of patterns
to its associated SSPs. Each SSP compares the data against a given pattern: one pattern per SSP; multiple SSPs
per PE. Whenever a match is detected at a given SSP, the document Match ID (MID) consisting of the address of
the match (Addr), the document identifier (Doc_ID), and the query id (Que_ID) is forwarded to the PE. Once the MID
reaches the PE, the actual information-retrieval instruction which was decomposed to generate the match is evaluated,
and if relevant, the results are forwarded to the host
Table I presents the match-based JAS PE macro-instruction set and the match sequence which implements each
of the lAS instructions. The JAS instruction set is based on the text-retrieval instruction set presented in Hol83].
In the table, the leftmost column presents the actual instruction. A semantic description of the instruction is
provided in italics followed by the control structure implementing the instruction. As seen in Table I, the entire
text-retrieval instruction set, including the variable-length separation match instruction, “A .n. B”, which can not
be efficiently implemented directly via FSA, cellular, or CAM&SLC implementations, can be implemented via a
coordinated sequence of substring-search primitives. Several clarifications are required. It is assumed that the
evaluation of each sequence of subinstructions terminates upon encountering an end-of-document indicator
(END_OF_DOC). The match( set of strings) instruction returns true whenever a match is detected. False is
returned once detecting an END_OF_DOC.
Type(match(strings)) returns the pattern type of the match.
Address(match(A)) returns the starting address of the match. If no match is encountered before END_OF_DOC, the
function returns default. Note that the match instructions “hang” until a match or END_OF_DOC is encountered.
The pseudocode provided is for explanation purposes. For better performance, many optimization are possible. In
all instructions, pattern overlap is forbidden. For queries comprised of multiple text-retrieval instructions, several
sequences of substring search primitives must be employed.
The internal JAS control structure, PE to SSPs, is similar to that of the Query Resolver to Term Comparator of
the PFSA system Has83]. In the JAS system, however, the PE is responsible for the actual evaluation of
information-retrieval instruction (a sequence of match primitives see table I); whereas in the PFSA system, the
Query Resolver is involved in the evaluation of the overall query (a sequence of information retrieval instructions).
-

3.

Substring

Search Processors

Cur83, Fos8O, Has83, Mea76, Pra86, Tak87], we found that the search speeds of
by the single-byte comparison speed of the implementation. Further, we
observed that prior approaches typically exhibit great percentages of redundant comparisons. Recognizing the bytecomparison upper bound for sequential algorithms and realizing the importance of early detection of the mismatch
condition, we have designed a Data Parallel Pattern Matching (DPPM) algorithm to be executed at each SSP. The
DPPM algorithm broadcasts the target pattern one character at a time, comparing each character against an entire
input block in parallel. Each block consists of K bytes- one byte per comparator. The simultaneous processing of
an entire input block from input string W differs from the systolic array, cellular array, and finite-state automata
In

examining prior
existing approaches are

work

all constrained

which operate on W on a byte-by-byte basis. Rather than broadcasting the input data to many
comparators, DPPM broadcasts the characters in pattern Q one by one into all the comparators on a demand-driven
basis. A mismatch-detection mechanism, which inputs a new block immediately upon detecting a mismatch, is used
to improve the throughput achievable for siring searching.
of the current block of W will trigger the loading
For example, a match of q~ of pattern Q with an element

approaches

into

position j

+

1 and

comparison

with q2 of the next target character.

Subsequent comparison outputs (in

this

case, q~ with W~ + i) are ‘and’ed with the previous results, in parallel, to generate new comparison results. The
previous results are shifted one position before the ‘and’ operation to emulate the shifting of the input string. If q1,
On
1’ respectively, then a full match has occurred.
qh match
1,
2’
h
q3,

q2,

Wj~ Wj

+

Wj

+

Wj

+

-

17

the other hand, if, after any

comparison cycle (the broadcast of qj,

and the ‘and’ of the current results with the past

history), all the comparison results are zero and no partial-match iraces generated from the previous input block are
waiting, an early-out flag will be set to indicate that further comparison of the current block of W is unnecessary.
On detection of the early-out flag, the next block of input data is loaded and the search operation restarted from
the first byte of the pattern. Thus, redundant comparisons are eliminated. In our example, ~f q~ fails to match any
element of the current block of W, then the next block is fetched and loaded immediately. In practice, only the first
one or two characters in Q usually need to be tested against the current block of W; a block size of 16 characters
yields roughly an order of magnitude speedup in search throughput over traditional sequential algorithms, assuming
the same comparator technology.
Figure 2 illustrates the algorithm via a concrete example. Assume that a 4-byte comparator array is used, the
pattern to be detected is “filters” and the incoming input stream is “file,filters”. After “file” is compared with the
first character of the pattern string, “f’, a partial match trace is initiated and the next pattern character is compared
against the same input string block. This process continues until the comparison on the fourth pattern character
generates a mismatch. An early-out flag is set, and a new input block is retrieved to resume the search process. It is
necessary to temporarily store the comparison result of the rightmost comparison in register V(i) since the generated
result represents a partial match. This temporary result is used as a successfuVunsuccessful partial match indicator
for the comparison of the next input block. The next block to be loaded is “, fil”, and the pattern matching process
resumes. This trace crosses over the input block boundary and continues until it reaches the end of the pattern
string. This time, the V(i) register is marked with a partial-match success indicator. Eventually, the last character of
the pattern is compared, and the HIT flag is set. Note that if multiple occurrences of the pattern are overlapped
within the input stream, all occurrences will be detected, as shown in figure 3. In figure 3, the pattern to be matched
is “fifi” and the input stream is “XfififiX”. As shown, both the patterns starting at position 2 and position 4 are
detected.
The DPPM

algorithm has several notable characteristics. First, the mismatch-detection capability reduces
redundant comparisons, increasing throughput significantly. The throughput achieved by the parallel algorithm
reduces the need for expensive high speed GaAs or ECL devices. Second, the parallel execution of the algorithm
detects all occurrences of partial matches; therefore no backup is required in either the pattern or the input data.
Three critical implementation aspects of the DPPM engine are the realization of the comparator array, the
required high pattern broadcast rate, and the chip input ports. The propagation delay of the comparator array is
proportional to the log of the number of inputs into the array. Therefore, the comparator array supports high
comparison rates, even when it includes many comparators. The pattern characters are broadcast by Cascade Buffers
Wes85] via the double-layer metal runs to minimize the propagation delay. Using input-buffer design similar to
Cha87] would allow

very

high-speed

communication between the storage devices and the

chip.

JAS Performance Evaluation
To evaluate the performance of the SSP, a functional simulator was written for measurement on an existing
database at Belicore. The 3.7 MBytes database consists of abstracts of 5,137 Bellcore technical memoranda with
topics from communications, computer science, physics, devices, fiber optics, signal processing, etc.. One hundred
different patterns were evaluated. Each set of 25 patterns were randomly selected by sampling vocabulary from each
of four disciplines: linguistics, computer science, electrical engineering, and device physics. The selected patterns
represent typical keywords commonly encountered in queries to the database. A list of the 100 sample patterns is
presented in Appendix A. The pattern lengths vary from 3 to 14 with an average of 7.34 characters. The starting

4.

character of the patterns were roughly uniformly distributed among the 26 English case-insensitive characters.
The patterns were used as inputs to the simulator which measured and collected the number of comparisons used
for each pattern in searching the database. Figure 5 shows the average number of comparison cycles per block, C, at
different block sizes, from 1 to 1024. For all block sizes tested, C is less than 3.2 despite an average pattern length
of 7.34 characters. At a block size of 1, the DPPM algorithm degenerates to a sequential comparison. As the block
size increases, the chance of matching the pattern also increases, and thus requires more comparison cycles.
Figure 6 shows the histogram of the number of comparison cycles used for the pattern “processor” at a block
size of 16. 71% of all blocks require only one comparison, and 93% require two or fewer comparisons. The early
mismatch detection of the DPPM algorithm is effective in eliminating redundant comparisons. From the simulation
experiment, it is observed that the average number of comparison cycles used is almost independent of the pattern

length, but rather depends on how frequently the first character in the pattern appears in the database. Patterns
starting with “a”, “e”, “s”, and “t” require more comparison cycles, while patterns starting with “x” and “z” always
require fewer, regardless of the pattern length.
The filter rate of the SSP is defined as the number of bytes that can be searched in one second, and can be
computed as
Block
Filter

Rate

Size

=

C

x

18

( Cycle

Time

)

Using 50 ns as the cycle time, the filter rates at different block sizes are shown in Figure 7. AL a block size of
16, the filter rate is 222 MBytes per second. This has already exceeded the predicted optical disk transfer rate of 200
MBytes per second and existing memory bandwidth of supercomputers (CRAY, CDC). At a block size of 128, the
filter

rate

reaches 1.2

GBytes per

second.

Figure

8 shows the

Filter

Speedup

=

speedup at different block sizes which is defined

Rate

at

Block

Size

as

K

_____________________________

Filter

Rate

at

Block

Size

1

The speedup curve shows that the DPPM algorithm exhibits a high degree of parallelism, thus speedup can be
achieved effectively by just increasing the block size. Since the predicate evaluation and query resolution are
performed at the PE, only very simple comparators and control circuitry are required in each SSP.
5.

Conclusion
We

an information retrieval subsystem called JAS. JAS incorporates several novel features. In JAS,
decomposed into substring-match primitives. The decomposition of the individual instructions into
search primitives provides a high degree of flexibility, several storage and retrieval schemes that can be
efficiently
supported, independence of the query complexity, and easy implementation of previously difficult instructions such

presented

instructions

as

are

“A.n.B “.
In

conjunction with the decomposition of instructions, a novel Data Parallel Pattern Matching (DPPM)
algorithm and its associated Substring Search Processor (SSP) is proposed. In contrast to previous approaches, the
DPPM algorithm operates on an input block (instead of byte) at a time and incorporates an early mismatch-detection
scheme to eliminate unnecessary comparisons. The SSP, a hardware realization of the DPPM
algorithm,
demonstrates the feasibility of a gigabyte-per-second search processor. A simulation study of the SSP was described.
The study demonstrated the potential for very high-speed text filtering.
References

Bar88]

Cha87]

Baru, C. K. and Frieder, 0., “Database Operations in a Cube-Connected Multicomputer System”, to
appear in the IEEE Transactions on Computers.
Chao, H. I., Robe, T. J., and Smoot, L. S., “A CMOS VLSI Framer Chip for a Broadband ISDN Local
Access

System”, Proceedings of the 1987 VLSI Circuits Symposium, May, 1987.
Curry, T. and Mukhopadhyay, A., “Realization of Efficient Non-Numeric Operations Through VLSI”,
Proceedings of VLSI ‘83, 1983.
Dew86] DeWitt, D. J., et. al., “GAMMA A High Performance Dataflow Database Machine,” Proceedings of
the Twelvth Int’l Conf on Very Large Data Bases, pp 228-237, 1987.
FosSO] Foster, M. J. and Kung, H. T., “The Design of Special Purpose Chips”, IEEE Computer, 13 (1), pp
26-40, January, 1980.
Goo8l] Goodman, J. R. and Sequin, C. H., “HYPERTREE: A Multiprocessor Interconnection Topology,”
IEEE Transactions on Computers, Vol. c-30, No. 12, pp 923-933, December, 1981.
Got83] Gottlieb, A., et al., “The NYU Ultracomputer
Designing an MIMI) Shared Memory Parallel
Cur83]

-

-

Computer, “IEEE Transactions

Computers,

Vol. c-32, No. 2, pp 175-189, February, 1983.
Processors for Text Retrieval”, Database Engineering 4, 1, pp. 16-29,
on

Has8 1]

Haskin, R. L., “Special-purpose

Has83]

Haskin, R. L. and Hollaar, L. A., “Operational Characteristics of a Hardware-based Pattern Matcher”,
ACM Transactions on Database Systems, Vol. 8, No. 1, pp 15-40, March, 1983.
Hillyer, B. and Shaw, D. E., “NON-VON’s Performance on Certain Database Benchmarks,” IEEE
Transactions on Software Engineering, se- 12,4, pp 577-583, April, 1986.
Hollaar, L. A., “Text Retrieval Computer”, IEEE Computer, 12 (3), pp 40-50, March, 1979.
Hollaar, L. A., Smith, K. F., Chow, W. H., Emrath, P.A., and Haskin, R. L., “Architecture and

September, 1981.

H ii 86]

Hol791
Hol83]

Kit84]
Mea76]

Pog871
Pra86]

Sa188]

Operation of a Large, Full-text Information-retrieval System”, in Advanced Database Machine
Architecture Englewood Cliffs, NJ.: Prentice/Hall, 1983, pp 256-299.
Kitsuregawa, M., Tanaka, H., and Moto-Oka, T.,”Architecture and Performance of Relational Algebra
Machine GRACE”, Int’l Conf. on Parallel Processing Proceedings, pp 241-250, August, 1984.
Mead, C. A., Pashley, R. D., Britton, L. D., Yoshiaki, T.,and Sando, Jr., S. F., “128-Bit
Multicomparator”, IEEE Journal of Solid-State Circuits, SC-il, No. 5, October, 1976.
Pogue, C. A. and Willett, P., “Use of Text Signatures for Document Retrieval in a Highly Parallel
environment.” Parallel Computing 4 (1987), pp 259-268, Elsevier (North-Holland).
Pramanik, Sakti, “Perfomance Analysis of a Database Filter Search Hardware”, IEEE Transactions on
Computers, Vol. c-35, No. 12, December, 1986.
Salton, G. and Buckley, C., ‘Parallel Text Search Methods”, Communications of the ACM, 31(2), pp
202-215, 1988.
19

Sta86]

Sto87]
Tak87]

Wes851

Stanfill, C. and Kahle, B., “Parallel Free-text Search on the Connection Machine System”,
Communications of the ACM, 29(12), pp 1229-1239, 1986.
Stone, H. S., “Parallel Querying of Large Databases: A Case Study”, iEEE Con~puter, 20 (10), pp 1121, October, 1987.
Takahashi, K., Yamada, H., and Hirata, M. “Intelligent String Search Processor to Accelerate Text
Information Retrieval”, Proceedings of F~ffh Int’l Workshop on Database Machines, pp 440-453,
October, 1987.
Weste, N and Eshraghian, K., Principles of CMOS VLSI Design: A Systems Perspective Reading,
Massachusetts:

Addison-Wesley,

1985.

APPENDIX

allocation
circuit
domain
field

acoustic
broadband

distributed
fiber

A

amplitude

architecture

banyan

basic

communication

computer

conculTent

design

ear

efficiency
energy
gallium
frequency
hypercube
hopfield
jitter
jaw
limited
language
multi-computer network

environment

intensity
keyboard

markov

message

momentum

nuclear

object
protocol

optic
quadrature

oscillator

output

packet

quantum

queue

resource

retrieval
time
verification

speech
timestamp
vlsi

query
standard
transform
voice

~x~d

x-ray

y-net

hertz

kernel

processor
research

system

telephone

user

wide

utilization
window

zero

zone

Table I.

JAS

erlang
greedy

glottis

fine-grain
high
japanese
knowledge

ground
intelligent

bell
distortion

image

information

junction
locality

k-map
loudness
noise

neural

superconduct

phoneme
recognition
synthesis

ultra-violet

unix

voltage
yield

watt

z-transform

Instruction Set

PE

containing the string A
match( A) then return true else

Find any document

A

if

return false

by

the

string B

A B

Find any document containing the siring A immediately followed
C := AB
(concatenateAandB)
if match( C) then return true else return false

A ?? B

Find any document containing string Afollowed by any two characters followed
C := A##B (concatenateA,##,andB)
if match( C) then return true else return false

(A, B, C) %

n

Find any document

containing

at

least

n

different patterns of the strings A, B,

or

by string B]

C

0;
:= 0;
:= 0;
While not ( END_OF_DOC ) do

count_A
count_B
count_C

:=

Case type (match(
A:

count_A
B: count_B

:=

C: count_C

:=

:=

OR

B

( CASE

of

statement used

only

for

clarity)

1

end;
if count_A + count_B
A

string) )

1;
1;

+

count_C

n

then return true

Find any docwnent containing either of the strings A or B
if ( match( A) or match( B )) then return true else

20

else

return false

return false

A

AND

B

II Find any document containing both the strings A and B

found_A := false
found_B := false
While not (END_OF_DOC) do begin
Case type (match( string)) of
A:

(

CASE statement used

only for clarity)

begin
if

found_B then
if address( matchO)

adds_B
found_A
begin
adds_A := address( matchO);
found_A := true

else if

-

>

length (B)

then return true

then

not

end

end;
B:

begin
found_A then
address( matchO ) adds_A> length (A) then
else if not found_B then begin
adds_B := address( matchO);
found_B := true
if

if

return true

-

end

end;

end;
return

A

...

B

false

Find any document containing the
number of characters by string B
adds_B := default

adds_A

string A followed either immediately

(adds_A default if A is
<> default) do begin
(find last B)

address( match( A))

:=

or

=

after an arbitrary

not

found)

While not ( END_OFDOC ) and (adds_A

temp
if

address( match( B))

:=

temp

~

default then adds_B

:=

temp

end
if

( adds_A

if adds_B

A

.n.

B

-

default) or ( adds_B default) then return false
adds_A> Iength( A) then return true else return false

=

Find any document

=

containing

the

string A followed by string B

adds_A := address( match( A));
if adds_A
default then return false;
length_A := length( A);
While not ( END_OF_DOC ) do
Case type (match( string)) of

within

n

characters

=

A:

begin
temp := address( matchO);
C temp ~ default) and (temp
adds_A := temp;

if

-

( CASE

statement used only for clarity)
(ignore possible overlap A with A

adds_A> length_A) then

end;
B:

(ignore possible overlap

begin
temp := address( matchO);
if ( temp ~ default) and (temp
if temp adds_A length_A
end;
end;
-

return

-

-

adds_A> length_A) then

< n

false

21

then return true

B with A

Input String
block i+1

block I
flU

block i+2
te

*~1I ~

f

E’

•

~~tE.l

•

•

~°~1 ~~1:1

r
S

320Mbytes/sec
1. JAS

Figure

!j~1

•

:

~ ~ .1.1

C)

1

rs

•

vregister~j

Mbytes/Sec
Figure

System Architecture

2.

without

Example

~1~1

.

HIT

Li

Overlap

Input String
block i
XXf I

block i+2

block i+1
fit i

HIT

XXXX

HITv~gister

Figure 3. Example with Overlap

Figure

4.

Substring Search

Processor

4

0.8
~

>-

00
(no

0
~

0.6

a,
U-

0

0.2

Ci

0,0
0
1

100

10

1000

2

10000

Block Size

Figure

5.

Compare Cycle

vs.

Figure

Block Size

U:
.01

1

~

i6o

1000

7. Filter Rate

vs.

5

4

6. Number of

6

7

8

9

10

Comparisons

0.

a,
a)
0.

U)

11

10000

100

10

1000

Block Size

Block Size

Figure

3

Number of Comparisons

Figure

Block Size

22

8.

Speedup

vs.

Block Size

10000

Parallel

Query

Evaluation: A New

T. Harder

Approach

H.

to

A. Sikeler

SchOning

University Kaiserslautern, Department of Computer Science,

Complex Object Processing

P.O. Box 3049, D-6750 Kaiserslautem, West

Germany

Abstract

Complex objects to support non-standard database applications require the use of substantial computing resources because their
powerful operations must be performed and maintained in an interactive environment. Since the exploitation of parallelism with
in such operations seems to be promising, we investigate the principal approaches for processing a query on complex objects
(molecules) in parallel. A number of arguments favor methods based on inter-molecule parallelism as against intra-molecule paral
lelism. Retrieval of molecules may be optimized by multiple storage structures and access paths. Hence, maintenance of such stor
age redundancy seems to be another good application area to explore the use of parallelism.
1. Introduction
Non-standard database

applications such
application objects for

for

VLSI

chip design 1] require adequate modeling
provide many of such desired features; above
au they support forms of data abstraction and encapsulation (e.g. ADTs) which relieve the application from the burden of main
taining intricate object representations and checking complex integrity constraints. On the other hand, the more powerful the
data model the longer the DBMS’s execution paths, since all aspects of complex object handling have to be performed inside the
DBMS. Hence, appropriate means to concurrently execute “independent” parts of a user operation are highly desirable 2].
facilities for their

as

3D-modeling

various

reasons.

workpieces

or

Enhanced data models

The

use of intra-transaction parallelism for higher-level operations was investigated in a number of database machine projects
3]. These approaches focus on the exploitation of parallelism in the framework of the relational model. Complex relational
queries are transformed into an operator tree of relational operations in which subtrees are executed concurrently (evaluation of
subqueries on different relations) 4]. Other approaches utilize special storage allocation schemes by distributing relations
across multiple disks. Parallelism is achieved by evaluating the same subquery on the various partitions of a relation 5, 6].

We

investigate possible strategies to exploit parallelism when processing complex objects. In order to be specific, we have to
identify our concepts and solutions in the framework of a particular data model and a system design facilitating the use of paral
lelism. Therefore, we refer to the molecule-atom data model (MAD model 7]) which is implemented by an NDBS kernel system
called PRJIVIA 8]. We use the term NDBS to describe a database system tailored to the support of non-standard applications.
2, A Model of NDBS Operations
The overall architecture consists of

applications
•

•

to

so-called NDBS kernel and

a

number of different

application-independent kernel

application layers,

The

which map

is divided into three

The storage system provides a powerful interface between main memory and disk. It maintains
access to sets of pages organized in segments 8].

a

particular

layers:

database buffer and enables

structures for basic objects called atoms and their related access paths. For performance
and
redundant
paths
reasons,
storage structures may be defined for atoms.
The data system dynamically builds the objects available at the data model interface. In our case, the kernel interface is charac
terized by the MAD model. Hence, the data system performs composition and decomposition of complex (structured)
objects called molecules.
access

system manages storage

multiple

•

a

the data model interface of the kernel. Our

access

The

application layer uses the complex objects and tailors them to (even more complex) objects according to the application
a given application. This mapping is specific for each application area (e.g. 3D-CAD). Hence, different application lay
exist which provide tailored interfaces (e.g. in form of a set of ADT operations) for the corresponding applications.

model of
ers

The NDBS architecture lends itself

to a workstation-server environment in a smooth and natural way. The application and the
corresponding application layer are dedicated to a workstation, whereas the NDBS kernel is assigned either to a single server
processor or to a server complex consisting of multiple processors. This architectural subdivision is strongly facilitated by the
properties of the MAD model: Sets of molecules consisting of sets of heterogeneous atoms may be specified as processing units.
we start to evaluate our concepts for achieving parallelism to perform data system and access system functions, we
briefly sketch our process (run-time) environment. In order to provide suitable computing resources, PRIMA is mapped to a mul
ti-processor system, i.e. the kernel code is allocated to each processor of our server complex (multiple DBMSs). The DB opera
tions to be considered are typically executed on shared (or overlapping) data which requires synchronization of concurrent
accesses. Due to the frequency of references (issued from concurrent tasks) accessibility of data and synchronization of access
must be solved efficiently.

Before

For this reason,

(running

on a

we

have

particular

designed

a

processor with

closely coupled multiprocessor system as a server complex. Each instance of PRIMA
private memory) uses an instruction-addressable common memory 9] for buffer manage-

23

synchronization, and logging/recovery. Furthermore, each instance of PRIMA is subdivided into a number of processes
which may initiate an aibitrary number of tasks serving as nm-units for the execution of single requests. Cooperation among pro
cesses is performed by establishing some kind of client-server relationship; the calling task in the client process issues a request
to the server process where a task acts upon the request and returns an answer to the caller. In our model, a client invokes a server
ment,

asynchronously, i.e. it can proceed after the invocation, and hence, can run concurrently with this server. To facilitate such com
plex and interleaved execution sequences we have designed a nested transaction concept 10] which serves as a flexible dynam
ic control structure and supports fme grained concurrency control as well as failure confinement within a nested subtransaction
hierarchy. Due to space limitations we can not refine our arguments on these system issues 11].
2.1 The Data System Interface
In order

describe the concepts for achieving parallelism in sufficient detail,
the interfaces involved. It is obvious that the data model plays the

to

ture and

which enable reasonable

parallelism: sufficiently large

data

(result sets), flexible selection of processing sequences,
In

granules,

set

we

have to refine

our

view of the kernel architec

role and determines many essential factors
orientation of request, dynamic construction of objects

major

etc.

language MQL which is similar to the well-known
but only illustrate the most important concepts
of basic element (or building block) in order to repre

system, the data model interface is embodied by the MAD model and its

our

SQL language. Here,

we

cannot

introduce this model with all its

complex details,

necessary for our discussion. In the MAD model, atoms are used as a kind
sent entities of the real world. In a similar way to tuples in the relational model, they consist of an arbitrary number of attributes.
The attributes’ data types can, however, be chosen from a richer selection than in the relational model, i.e. apart from the conven

tional types the type concept includes
•

the structured types RECORD and ARRAY,

•

the

repeating

•

the

special types

Atoms

are

group types SET and UST, both

grouped

IDENTIFIER
to atom

(serving

as

yielding

surrogates)

a

powerful structuring capability

for identification purposes and

types. Relationships between

atoms are

expressed by

at the

attribute level

as

well

as

REF_TO for the connection of atoms.

so-called connections and

are

defmed

as con

nection types between atom types. Connection types are treated in a symmetric way, i.e. connections may be used in either direc
tion in the same manner. Such connection types directly map all types of relationships (1:1, 1 :n, n:m). The flexibility of the data
model is
attributes

•

greatly increased by this direct and symmetric mapping. Connection types
(reference and “back-reference”) one in either involved atom type, e.g.:

FIDs:

SET_OF (REF_TO(Face.EIDs)) in

an atom

EIDs:

SET_OF (REF_TO(Edge.FIDs))

an

In the database, all

atoms connected

instances (Aton

by

network)

in

atom

type

are

REF_TO

Edge

connections form meshed

0

structures

3D-Object

3

2

of

pair

a

type Face.

(atom networks)

illustrated in

Fig.

la.

SELECT ALL

SELECT ALL
FROM

Point

WHERE

Point.No

Face-Edge-Point

-

Edge

134
1

1

24

123

124

Figure
Molecules

1:

134

34

=

134;

c)

b)

Face

as

FROM

WHERE Face.No <2
1

represented by

Point

Edge

23

13

Point

Dynamic construction of molecules

from

a

1

34

Edge

sample geometric object

defmed

dynamically by using MQL statements and have to be derived at run-time. Each molecule belongs to a
description establishes a connected, directed and acycic type graph (cycles occur when recursive types
are specified), in that a starting point (i.e. root atom type) and all participating atom and connection types (for short “-“) are
specified. A particular example of a molecule type is Face-Edge-Point. Such a molecule type determines both the molecule struc
ture as well as the molecule set which groups all molecules with the same structure. At the conceptual level, the dynamic construc
tion of molecules proceeds in a straight-forward maimer using the molecule type description as a template: For each atom belong
ing to the root atom type all children, grandchildren and so on are connected to the molecule structure, terminating after all leaves
of the molecule structure are reached. Connecting children to the molecule structure means perfonning the hierarchical join which is
supported by the connection concept. Hence, for each root atom a single molecule is constructed. Fig. lb shows the result of a
axe

molecule type. The type

molecule construction for
most

important properties

Face-Edge-Point molecules,

MQL request handles

•

An

•

The molecules

as

where the

set

of molecules

was

restricted. Furthermore, it illustrates the

of the MAD interface:
a set

of molecules.

complex objects

consist of

sets

of

atoms

of different type, i.e.

they

are

embodied

by

sets

of interrelated het

erogeneous record structures.
•

Molecule construction is

dynamic

arid allows

symmetric

use

of the

atom

24

networks

(e.g. Point-Edge (Fig. ic)).

2.2 The Access System Interface
The

system provides an atom-oriented interface which allows for direct and navigational retrieval as well as for the
of atoms. To satisfy the retrieval requirements of the data system, it supports direct access to a single atom as well
atom-by-atom access to either homogeneous or heterogeneous atom sets. Manipulation operations (insert, modify, and
access

manipulation
as

delete)

and direct

atoms Identified by their logical address (or surrogate) which is used to implement
REF_TO attributes. Perfonning manipulation operations, the access system is responsi
ble for the automatic maintenance of the referential integrity defmed by the REF_TO attributes. Thus, a manipulation operation
on such an attribute requires implicit manipulation operations on other atoms in order to adjust the appropriate back references.

operate

access

the IDENTIFIER attribute

as

Different kinds of

scan

position

set. and to

on

well

as

single
the

operations are introduced as a concept to manage a dynamically defined set of atoms, to hold a current
successively deliver single atoms. Some scan operations are added in order to optimize retrieval
access. Therefore, they may depend on the existence of a certain storage structure (defmed by the database administrator):

•

in such

a

The atom-type

scan

delivers all

atoms

in

system-defined

a

order

utilizing

the basic storage structure

existing

for each atom

type.
•

The
and

access-path

scan

provides

fast

value-dependent

access

based

on

different

access

path

structures

such

as

B-trees, R-trees,

files.

grid

The sort

scan

processes all atoms following
an entire atom type is

type. However, sorting

specified sort criterion also utilizing the basic storage structure of an atom
expensive and time consuming. Therefore, a sort scan may be supported by an

a

additional storage structure,

•

namely the sort order.
speeds up the construction of frequently used molecules by allocating all atoms of a corresponding
molecule in physical contiguity using a tailored storage structure as a so-called atom cluster. For example, in Fig. 1 each Face
atom and all its associated Edge and Point atoms may be organized to form an atom cluster 121. On a logical level, an atom
cluster corresponds to a molecule. It is described by a special so-called characteristic atom which consists of references to all
atoms belonging to the molecule. This characteristic atom together with all the referenced atoms is mapped onto a single phys

The atom-chister

scan

ical record which in turn is stored in

a

set

of pages.

The

underlying concept is to make storage redundancy available outside the access system by offering appropriate retrieval
operations (i.e. the choice of several different scans for a particular access decision by the optimizer of the data system), where
as in the case of manipulation operations storage redundancy has to be hidden by the access system. As a consequence, maintain
ing storage redundancy in an efficient way is a major task of the access system. However, sequential update of all storage struc
tures existing for a corresponding atom results in a lack of efficiency which is not acceptable. Therefore, exploiting parallelism
seems to be a natural way to speed up a single manipulation operation.

3. Using Parallelism in Ouerv Processing

Query processing operates on sets of molecules which are either extracted from the database (retrieval) or updated, inserted, or
(manipulation). In either case, the complex operation must be evaluated using operations on a single atom at a time. In this
chapter we discuss some techniques to exploit parallelism in executing these atom-oriented operations as well as higher-order
operations on intermediate results.

deleted

Three

phases of query processing can be isolated 13]. The compilation phase checks for the syntactic and semantic correctness
query and perfonus some obviously simplifying query transformations. Finally, it derives an operator tree consisting of
several operator types, most of which accept and produce sets of molecules. The leaves of this operator tree transform heteroge

of

a

atom sets to molecules (construction of simple molecules for retrieval) and vice versa (molecule modification by atom
manipulation). This operator tree is the input of the optimization phase which is expected to reorganize it somehow into an
“optimal” form. This includes reordering, combination, and splitting of operator tree nodes. Furthermore, methods have to be
chosen for each operator, i.e. evaluation strategies (e.g. nested-loop join), storage structure usage (e.g. B-tree), and amount of
parallelism (as described below). Although there is much to be said about these two phases, we concentrate on the discussion of
the third phase, i.e. query evaluation. The input of this phase is the operator tree introduced above. For each node, we need an

neous

active unit
more

than

to

compute its result. Since there

one

node of

an

operator

tree

are

may be

several operator types, we assume
to the same process.

a

process for each of them. As

a

consequence,

assigned

We discuss the operator tree interpretation separately for retrieval and manipulation. In either case, we have to handle a
molecules, each of which consists of a set of atoms. Thus, we investigate parallel handling of molecules (inter-molecule

lelism)

as

well

as

parallel handling

of

a

molecule’s components

set

of

paral

(intra-molecule parallelism).

3.1 Parallelism in Retrieval Evaluation
at the root operator, which needs results from all of its children in the operator tree to compute its own result.
the operator type and the method chosen, children can be evaluated in parallel, e.g. the children of a binary mergejoin operator can be accessed concurrently, while those of a nested-loop join cannot. The evaluation of leaf operators requires a
lot of access system calls to build up simple molecules (hierarchical, non-recursive molecules). Of course, we assume that the

Evaluation starts

Depending

access

on

system is able

seems to

be

a

to

handle

good candidate

arbitrary number of asynchronous calls in parallel. Therefore,
using parallelism within the handling of each molecule.

an

for

25

construction of

simple

molecules

Parallelism Within Construction of Simple Molecules
An obvious strategy

this all child
These

simple molecules is to call the root atom of each molecule via
(which are identifiable by reference attributes) are called,
the children of an atom may be done in parallel (example la).

atoms

accesses to

to construct

of the

root atom

SELECll ALL .~.. I
Face

FROM
WHERE

Edge-Point

Pace.No

123

Strategy:

Strategy:

Call Face;

Call

Call all

Call all

all

edges and
points in parallel;

Ouerv

~ .1..

..........~

If all

Face—Edge Point
WHERE (FOR~A1L Point

Face

OR

Following

~

so on.

..

x-~oordmatc

5)

(EXISTS Edge Length> 10)

Strategy

edges sequentially;
edges fulfil the restriction:
call all points in parallel;

•

call

or

•

call

edges sequentially; then call points sequentially;
edge, call its points sequentially;
call n edges in parallel, call their points sequentially;

or

•

one

Ouery b

Example

scan.

Call Face;

Face;

a

system

FROM

Edge Point
WHERE FORALL Edge.
Length> 10’
FROM

access

then their children, and

1: Three

Ouerv

sample queries

and

parallelism strategy

c

choices

However, in many cases the user is not interested in all molecules of a certain type, but strongly restricts the molecules he wants
to see. In this situation, it would be inefficient to fetch all atoms of all molecules and then throw away most of them by a separat
ed restriction operator.

molecules”

leading

which allows

scan

becomes evident
thus

saving

we

want

to

integrate

the restriction

facility

into the operator “construction of simple
are passed on to the access system
as early as possible. As soon as it

efficient evaluation strategy. Restrictions on the root atom
restrictions. All other restrictions on dependent atoms are evaluated

during

many

Instead,

to a more

access

molecule construction that

a

molecule will be

disqualified,

none

of its

atoms

has to be fetched any more,

system calls (example ib).

Of course, this

approach is contradictoty to the parallel molecule construction proposed above, because we want to fetch as few
possible, if a molecule is disqualified. Therefore, we combine both techniques: Atom types that do not contribute to
molecule qualification should be treated last. Their atoms can be called in parallel. Atom types restricted by an ALL-quantifier
should be called sequentially, since each atom of this type can indicate molecule disqualification. While good strategies for these
extreme cases are easy to fmd, much more complicated situations can be thought of (example Ic). They raise the question
whether in some situation a compromise on the amount of parallelism and unnecessaiy atom accesses should be made, e.g. limita
tion of parallel atom calls to a constant n, thereby limiting unnecessary atom calls to n-i (third choice in example ic). We are still
investigating this case for generally applicable rules to decide the optimal amount of parallelism for each atom type as well as the
atoms as

best sequence of

atom accesses.

The

top-down approaches suggested above are sometimes not the most efficient strategies. When highly selective restrictions
on some child types, a bottom-up approach may be more promising. In this case, the first step evaluates the qualify
child
Since some of these atoms may be orphans, it is necessary to explicitly check the existence of a related root
atoms.
ing
atom. Finally, the whole molecule is constructed for each of the identified root atoms following the same guidelines as sketched
above (example 2b).
are

defined

So far,

we

have discussed

parallelism within the
parallelism, too.

construction of

one

molecule. Since

queries

deal with sets of molecules,

we

should consider inter-molecule

Inter-Molecule Parallelism
The

simple model for the computation of a set of molecules is to build up the first molecule completely, then the second and
thereby preserving the order of molecules induced by construction of simple molecules. Following this control scheme,
cannot be any parallelism among an process and its descendants or ancestors. To enable this kind of parallelism, we pro-

most

so on,

there

SELECT’

ALL

SELECT

ALL

FROM

Face-Edge-Point

FROM

Face-Ed;

26

pose

a

pipeline

mechanism. In

molecule, it builds up
The

particular,

this molecule in

a

whenever the process for construction of simple molecules finds
concurrent task calls the next root atom.

a

root atom

for

a

separate task, while another

defmed this way (which at this point of the discussion is introduced as a model of computation and not as
is very dynamic and complex, since the number of pipeline stages to run through is data
dependent for many operator types and may vary for each molecule. Since this results in varying construction times, order-preser
vation camlot be guaranteed. As a consequence, there must not be any operator with a varying number of pipeline stages in the

pipeline

structure

schedule for

hardware-assignment),

operator

between

tree

a

sort

operator and the

corresponding operator

that relies

on

the sort order.

3.2 Parallelism in ManiDulation Evaluation
As for retrieval

evaluation, we consider intra- and inter-molecule parallelism. Parallelism among several molecules by creation of a
separate task for each of them is possible for manipulation, too. When existing molecules are to be manipulated, tasks emerging
from retrieving them can be continued for manipulation. Within one molecule, either a top-down or a bottom-up strategy can be

applied, both

of them

allowing parallelism

sample manipulation

among

most

of the atoms of

affected

statement

a

molecule

molecule

top-down deletion

DELETE

ALL

FROM

Face-Edge-Point

I

WHERE

Face.No=l;

4’

1

This is

to

tion

is

just a query
strategies; it

wrong with respect to

show evalua-

1

(example 3).

detete
delete
delete

Face

14

13

Edge

bottom_up
delete
delete
delete

semantically
figure 1.
123

124

134

all

Point

3:

Manipulation

of

a

molecule with

top-down

and

deletion

(123), delete (124), delete (134)
(13), delete (12), delete (14)
(I)

access

can

Example

(I)
(13) delete (12) delete (14)
(123), delete (124), delete (134)

system calls in the

be done In

same

line

parallel

bouom-up strategy

4. Maintaining Redundancy by Parallel Operatjlln~

speed up data system operations we have introduced some algorithms for the parallel construction/maintenance of complex
objects represented as sets of heterogeneous atoms. In the following, we discuss the implementation of concurrent maintenance
operations on redundant storage structures used for such atoms. As in the data system, two kinds of parallelism may be distin

To

guished
•

within the

The

access

system.

inter-operation parallelism

This kind of parallelism is
•

The

a

allows for the

prerequisite for

the

parallel execution of an arbitrary number
parallelism introduced in the data system.

intra-operation parallelism however, exploits parallelism

in

executing

a

single

access

of

independent

access

system calls.

system call.

In this

chapteT, we will concentrate on mtra-operation parallelism, since inter-operation parallelism is easily achieved by the
underlying processing and transaction concept. For this purpose, however, the mapping process performed by the access system
has to be outlined in some more detail in order to reveal purposeful suboperations to be executed in parallel.
In order

to

ical record

conceal the storage redundancy resulting from the different storage structures we have introduced the concept of a log
(i.e. atom) made available at the access system interface and physical records stored in the “containers” offered by

the storage system. i.e. each physical record represents an atom in either storage structure. As a consequence, an arbitrary number
physical records may be associated with each atom. For example, the creation of an atom cluster for each Face-Edge-Point
molecule in Fig. 1 would imply that all Edge atoms belong to two atom clusters and all Point atoms to three (due to the proper
of

ties of a

teirahedra). Furthermore, they always belong

The

relationship

ture

related

to

addresses each

between

each

indicating

atom and

all its associated

type. This address

the

the basic storage structure.

physical records is maintained by a sophisticated address struc
the logical address identifying an atom onto a list of physical
location of a corresponding physical record within the “containers” (page address).

single

a

atom

to

structure maps

to the data system, however, the exploitation of parallelism within the access system is limited to the manipulation
operations. Althoug~i most of the retrieval operations are also decomposed into further suboperalions (e.g. in the case of an
access-path scan on a tree structure: read the next entry in order to obtain the next logical address, access the address structure
for the associated physical addresses, access either physical record), these suboperations cannot be executed in parallel due to
the underlying precedence structure. Furthermore, each retrieval operation is tailored to a certain storage structure, thus operat
ing not only on a single atom, but also on a single physical record.

In contrast

On the other hand, each

manipulation operation on an atom may be decomposed in quite a natural way into corresponding manipu
operations on the associated physical records. These lower-level manipulation operations, however, should be executed in
parallel due to performance reasons. There exist (at least) two alternatives to perform such a parallel update:
lation

27

Deferred Update
Deferred

update

means

that

during

a

manipulation operation

on an atom

initially only

one

of the associated

physical

records

(e.g.

in the basic storage structure of the atom type) is altered. All other physical records as well as the access paths are marked as
invalid. Finally, a number of “processes” is initialized which alter the invalid structures in a deferred manner, whereas the manip

ulation

operation itself is finished. Thus, low-level manipulation operations on additional storage structures may still run.
although the manipulation operation on the corresponding atom or even each higher-level operation initializing the modification
is already fmished. This, however, strongly depends on the embedding of deferred update into the underlying transaction con

cept.
In order

to

physical

record is valid. Therefore, all

structure may be used to indicate whether or not the corresponding
operations which utilize the address structure in order to locate a physical record may
determine the valid records, whereas all operations which do not utilize the address structure will access invalid records unless
the appropriate storage structure was already altered by the corresponding “prucess”. Hence, the corresponding storage struc
tures themselves (access paths, sort orders, and atom clusters) have to be marked as invalid and when performing a scan opera
tion on such an invalid structure each physical record has to be checked as to whether or not it is valid. This, however, requires
an additional access to the address structure in order to locate a valid record. Consequently, the speed of a scan operation
degrades, since each access to the address structure may result in an external disk access. In order to avoid this undesired
behaviour, all invalid atoms (or their logical addresses) may be collected in a number of special pages assigned to each storage
structure. These pages may be kept in the database buffer throughout the whole scan operation thus avoiding extra disk access
es. Nevertheless, each physical record has to be compared with the atoms collected in these pages. However, this is not suffi
cient, since each manipulation operation may require a modification of the whole storage structure, e.g. modifying an attribute
which establishes a sort criterion requires the rearrangement of the corresponding physical record within the sort order. This
fact also has to be considered during a scan operation. As a consequence, some of the scan operations may become rather com
plex and thus inefficient. For all these reasons, deferred update seems to be a bad idea.

mark

a

physical

record

as

invalid the address

Concurrent Update
The

problem of maintaining invalid storage structures, however, is avoided by concurrent update. Concurrent update means that
manipulation operation on an atom invokes a number of “processes” which alter the associated physical records and access
paths in parallel. The manipulation operation is finished when all “processes” are completed. When sufficient computing
resources are available, concurrent update may not be more expensive, in terms of response time, than update of a single physical
record if we neglect the cost of organizing parallelism.
each

Depending

on

the software structure of the

access

system, there

are

different ways

to

perform

a

concurrent

update:

Autonomous Components

Each manipulation operation on an atom is directly passed to all components maintaining only a single storage structure type.
Each component checks which storage structures of the appropriate type are affected by the manipulation operation. The corre
sponding storage structures are then modified either sequentially or again in parallel.
As

a consequence, all components have to
system (i.e. insert, modify, and delete of

provide a uniform interface including all manipulation operations offered by the access
a single atom identified by its logical address) as well as all retrieval operations. A
quite simple distribution component directs each request to all components maintaining a storage structure type and collects
their responses. This means, each component initially performs an evaluation phase during which it checks
it has to

perform the desired operation and if so,
appropriate type are really affected.

•

whether

•

which storage structures of the

or not

For this purpose, the addressing component (maintaining the common address structure) and the meta-data system (maintaining
all required description information) are utilized. After the evaluation phase the proper operation is performed on each affected
either sequentially or in parallel, thereby again utilizing two common components: the addressing component in
notify the modification of a physical address and the mapping component in order to transform a logical record (i.e.
atom) into a physical record and vice versa (thus achieving a uniform representation of physical records which is mandatory for
some retrieval operations which use one of the physical records when accessing an atom (e.g. direct access)).

storage
order

structure

to

Thus, it is quite easy

to add a new storage structure type (e.g. a dynamic hash structure as an additional access path structure) by
simply integrating a corresponding component into the overall access system. However, there may be some drawbacks regard
ing performance aspects. During each operation, all components have to perform the evaluation phase although in many cases only
a few or even only one component are affected. Moreover, the addressing component may become a bottleneck, since access to
the address structure has to be synchronized in order to keep it consistent.

General Evaluation Component

These
as

problems, however,

well

as

the evaluation

the address structure. As

may be avoided

by

a

general

evaluation component which

phases in
a

replaces

the

simple distribution component

each of the components maintaining a storage structure type. Additionally, it solely maintains
consequence, this general evaluation component becomes much more complex. It requires dedicated

28

information about each component in order

to decide whether or not a component is affected by an operation, and it has to know
operations offered by either component in order to invoke the corresponding component in the right way. Although these
operations may be tailored to the corresponding storage structure type (e.g. insert (key, logical address) in the case of an access
path structure), it seems to be useful again to provide a uniform interface to all components in order to allow for a certain degree

the

of

extensibility. Such an interface has to consider
ding component (e.g. maintaining logical addresses
In

initial

the different characteristics of each storage structure type and the correspon
instead of physical records) in an appropriate way.

general evaluation component. In our opinion.
problem. Although both software structures,
autonomous components and general evaluation components, have their pros and cons with respect to performance and extensi
bility aspects 14], we prefer the general evaluation component, since it promises better perfonnance. However, more detailed
investigations are still necessary in order to determine the best way which may be a mixture of all. In particular, the influence of
the underlying hardware architecture has to be investigated in more detail.
our

concurrent

design,

update

we

have decided to

seems

to

implement

update

concurrent

based

on a

be the better solution due to the invalidation

5. Conclusion
We have

a

has

on

presented
primarily been

discussion of the essential aspects of parallel query processing on complex objects. The focus of the paper
the investigation of a multi-layered NDBS to achieve reasonable degrees of parallelism for a single user

query. We have derived several design proposals embodying different concepts for the use of parallelism. In the data system,
intra- and inter-molecule parallelism were explored. To exploit the former kind of parallelism seems to be more difficult because

it turns out that it is very sensitive to the optimal degree of parallelism which may vary dynamically depending on the complex
object characteristics. The latter concept is considered more promising because it allows simpler solutions. In the access system
two approaches were investigated. Deferred update seems to provoke more problems than the solutions it might provide where
as concurrent update on redundant storage structures seems to incorporate a large potential for successful application.
PRIMA

Currently, we have fmished the
achieving parallelism in order to
weaknesses at

In the future,

a more

have

a

implementation (single user version) and are integrating the proposed concepts for
practical experiments. Performance analysis will reveal their strength and

testbed for

detailed level.

exploitation of parallelism. Another possibility of parallel execution on
multiple requests within the application; for example, by means of
a window system a user could issue several concurrent calls inherently related to the same task in a construction environment. Oth
er possibilities to specify concurrent actions exist in the application layer where a complex ADT operation could be decom
posed into compatible (non-conflicting) kernel requests. Usually multiple kernel requests are necessary for the data supply of an
ADT operation~ hence, these data requests can be expressed by MQL statements and issued concurrently to kernel servers when
they do not conflict with each other or do not require a certain precedence structure.
behalf of

a

we

wish to

single user

investigate

further concepts for

would be the simultaneous activation of

References

1]

Ditirich, K.R., Dayal, U. (eds.): Proc. mi. Workshop

21

Duppel, N., Peinl, P., Reuter, A., Schiele, G., Zeller,
Stuttgart, 1987.

3]

Special

4]

DeWitt, D., Gerber, R., Graefe, 0., Heytens, M., Kwnar, K., Muralikrishna, M.: GAMMA
Database Machine, in: Proc. VLDB 86, pp. 228-237.

5]

Neches, P.: The Anatomy of

6]

Lone, R., Daudenarde, I., Hallmark, G., Stamos, I., Young, H.: Adding Intra-Transaction Parallelism
Early Experience, IBM Research Report, Ri 6165, San Jose, CA, 1988.

7]

Mitschang,

Issue

on

on

Object-Oriented

H.:

Progress Report

Database Machines, IEEE Transactions On

B.: Towards

a

a

Database

Computer System,

Unified View of

Computers,

in: Proc. IEEE

Harder, T., Meyer-Wegener, K., Mitachang, B., Sikeler, A.: PRIMA
cations, in: Proc. VLDB 87, pp. 433-442.

9]

SEQUENT

Solutions:

Improving

Database Performance,

11] Harder, T., Schoning, H., Sikeler,

An

Proc. 3rd Tnt. Conf.

13] Freytag, J.C.:
14] Carey,

on

A DBMS

Report, University

Computing,

MIT.

High

Performance Dataflow

San Francisco, Feb. 1985.
to an

Existing

in: Proc. Second Tnt. Coni.

Optimization,
Database

1987.

in: Proc. hit.

Symposium
to

Database

29

Engineering,

Vol. 10, No. 2, 1987.

on

appear in

in: ACM SIGMOD Annual Conference, 1987, pp. 173-186.

Systems,

on

Report M1T-LCS-TR-260, MIT., Labo

A.: Cluster Mechanisms Supporting the Dynamic Construction of Complex Objects,
Foundations of Data Organization and Algoritluns (FODO’89), June 21-23, 1989.

Special Issue on Extensible

DBMS:

Prototype Supporting Engineering Appli

Sequent Computer Systems, Inc.,

Reliable

A

Processing Queries on Complex Objects,
Systems, Austin. Texas, 1988, pp. 13 1-143.

A Rule-Based View of Query

M. (ed.):

to

-

Representation,

A.: Parallelism in

Databases in Parallel and Distributed

12] Schoning, H., Sikeler,

Approach

-

#2 of PROSPECT, Research

Spring Compcon,

and Knowledge
pp. 33-49.

8]

Pacific Grove, 1986.

Systems.

Vol. C-28, No. 6, 1979.

Design Data

Expert Database Systems, Tysons Corner, Virginia, 1988,

10] Moss, J.E.B.: Nested Transactions:
ratory of Computer Science, 1981.

Database

MULTIPROCESSOR TRANSITIVE CLOSURE ALGORITHMS

Rakesh Agrawal
H. V. Jagadish
AT&T Bell Laboratories

Murray Hill,

Jersey

New

07974

ABSTRACT
We present parallel algorithms to compute the transitive closure of
linear speed-up with these algorithms.
1.

database relation.

a

Experimental

verification shows

an

almost

INTRODUCTION

operation has been widely recognized as a necessary extension to relational query languages 1, 12, 16]. In
spite of the discovery of many efficient algorithms 3,5,10, 13, 17], the computation of transitive closure remains much more
expensive than the standard relational operators. Considerable research has been devoted in the past to implementing standard
relational operators efficiently on multiprocessor database machines and there is need for similar research in parallelizing the
transitive closure operation.

The transitive closure

Given

a

graph

with

nodes, the computation of its transitive closure is known to be a problem requiring 0(n3) effort. Transitive
a problem in the class NC, implying that it can be solved in poly-log time with a polynomial number of
even less than
practical point of view, however, there are likely to be only a small number of processors

n

closure is also known to be
processors.

From

0(n). Therefore,

a

—

parallel algorithms

the

total execution time is

report

no more

experimentally

on

than 0

observed

that

seek in this paper are ones that require only m (mcn) processors, and for which the
We also present their implementation on a multiprocessor database machine 15] and

we

(n3/m).

speed-ups.

organization of the rest of the paper is as follows. Our endeavor has been to develop the parallel transitive algorithms in an
architecture-independent manner. However, to keep the discussion concrete, we consider two generic multiprocessor architectures:
shared-memory and message-passing. These architectures are briefly described in Section 2. We also present primitives that we use
in algorithm description in this section. Our parallel algorithms are presented in Section 3. Section 4 describes the implementation
of these algorithms on the Silicon Database Machine (SiDBM) 15], and presents perfonnance measurements. We discuss related
work in Section 5, and close with some concluding remarks in Section 6.

The

2. PRELIMINARIES

parallel algorithms that are independent of the exact nature of the underlying multiprocessor so that they may be
implemented on different types of multiprocessors. Of course, the costs of the individual operations will differ with the machine and
communication model used, affecting the resultant performance of the algorithms. We recognize at the same time that it is
impossible to completely divorce the execution of a parallel algorithm for a multiprocessor from any architectural assumptions 6].
We, therefore, concentrate on two generic multiprocessor architectures and keep our architectural assumptions as general as possible.
We seek

2.1

Generic Architectures

We

are

interested in

two

multiprocessor

architectures:

shared-memory

and

message-passing (also

referred

to

as

shared-nothing).

Each processor has some local memory and local mass storage where the database resides. Processors are connected with some
communication fabric. In the case of a message-passing architecture, the system interconnect is the only shared hardware resource,
whereas in the

We

assume

case

of

a

shared-memory architecture,

processors have

access to a

that the database relation whose transitive closure is to be

computed

shared memory.
consists of

a

“source” field,

a

“destination” field,

and other data fields that represent labels (properties) on the arc from a specific source to destination such as distance, capacity,
reliability, quantity, etc. The database relations have been partitioned across processors, so that each processor “owns” a part of the

relation and there is

result)

t

with certain

This paper is

a

replication. Partitioning
specified values for the source
no

caidenscd vcmion of 2],

presented

at

is horizontal; each processor has all the
destination field.

tuples

in the relation

(both original

and

or

the International

Symposium

on

30

Databases in Parallel and Distributed

Systems, Austin, Texas,

December 1988.

2.2

BasIc Primitives

To present

algorithms

in

architecture-independent

an

manner,

we

first define

a

few

primitives

that

we use

in

algorithm description

in

Section 3.
Remote-Get: Access data from non-local memory.
A remote-get is executed by a processor to access a piece of data not owned by the processor. If the remote data is unavailable, the
remote-get is blocked. We shall write remote—get (data) where data is the data that needs to be remotely accessed.

Make data available

Show:

to remote

processors

operation is complimentary to the remote-get operation. A
by the processor to other processors. A processor may

The show

data “owned”

Weshallwrite

owner.

show(data,

not

gain

tomeanshow the

processor_list)

by

a

processor to make available a piece of
data unless it has been shown by its

access to remote

data

toprocessorsin processor_list.

The

could be empty.

processor_list

Set up

Enable-Interrupt:

show is executed

an

interrupt

and the

event

interrupt-handling

routine

A processor may receive notification of an external event provided that it has enabled an interrupt. We write
action) to mean upon the occurrence of the interrupt event, execute the action specified in action.

enable

(event,

operations may be implemented. A processor doing show may
may then access it remotely. This form of implementation normally
exists in a shared-memory system, where the remote location in question is in the shared memory. A second alternative is to do a
show by sending (broadcasting or multicasting if multiple receivers are involved) the data to the other processors. The remote-get

There

different ways in which

are

write the data in

a

remote

a

pair

of show and remote-get

location and the other

processor(s)

inverse of the second scheme.

A third alternative
most message-passing systems.
by writing locally to its own memory and provide this address

intended receivers.

remote access to

requires

then

local

a

access.

This form of communication is found in

A processor may do a show
The remote-get is then accomplished by a

of the type of architecture used,

Irrespective

This expense may simply be the
contention for shared resources such as
access.

remote-get

pairs

in favor of local

a

is the
to

the

this location.

show and remote-get pair of operations is considerably more expensive than a local
of a remote access, but may also include synchronization costs, the effects of

longer latency
a

bus,

etc.

The

parallel algorithms

that

we

devise minimize the number of show and

accesses.

PARALLEL TRANSITIVE CLOSURE ALGORITHMS

3.

We present three
3.1

Iterative

transitive closure

parallel

algorithms:

one

iterative and

two

matrix-based direct

algorithms.

Algorithms

The essential idea of iterative
new

thereof
If

R0

algorithms is to evaluate repeatedly a sequence of relational algebraic expressions in a loop, until no
tuple is generated. Included in this family are algorithms such as semi-naive 5], logarithmic 10, 17] and variations
10, 13]. We consider parallelization of the semi-naive algorithm; other iterative algorithms can be parallelized similarly.

answer

is the initial relation and

closure

R1

of R0

R~

steps executed by the processor p
Semi-Naive

R1

~—

R~

4—

R1

do

computation

processor,

so

and

The

R1.

remote—get(R0)

(R~)~’
u

(Rfl~

R1

—

U

.

—

R0

the transitive

1.1 (Parallel Semi-Naive):

1) if (R~)~’
~ then
2)
(Rfl’ ~- (R~)’1
3)
(R~y ~— (R~/
4)
(R~~ ~— (R~)’1

R0

R1
same

Algorithm

~-

Ra

The closure

tuples generated in an iteration, then the semi-naive algorithm computes
Drawing upon the results in 4], R0 was partitioned on the source field, as also R~
in the i’1’ iteration are shown below.

Algorithm (Uniprocessor):

R0
while Ra
(~>
Ra ~- R~

R~

the set of

shown below.

as

Ra
is

partitioned

in such

that communications and

a

way that the set of result tuples owned by a particular processor are
is minimized. The set-difference and union steps

synchronization

generated

(steps

at

the

3 and 4

performed locally without remote access to any tuple in R~. The composition step (step 2), however, requires
complete R0 because Ra has been partitioned on the source field and it may have a tuple for every
destination value. The relation R0 will have to be remotely accessed. However, provided enough storage is available locally, it may
be possible to remotely access R0 only once at the beginning of the iteration, since R0 does not change from iteration to iteration.
All subsequent computation can then be perfonned locally at each processor. There is no need for synchronizing iterations, and
different processors may even compute for different numbers of iterations, since they independently evaluate their termination
conditions. The algorithm terminates when all processors are done.

respectively) can
Ra to be joined

be

with the

31

to every processor, but makes
processor has access to the complete
graph, it can determine this reachability without any communication with any other processor. The disadvantage is that there may be
significant redundant computation in this algorithm. For example, suppose the graph has an arc from i to j and the reachability
determination for i and j has been delegated to different processors, then both the processors will end up determining complete

graph corresponding to the given relation, this algorithm hands
responsible for determining reachability from a specified set of

the

In terms of the

over

a

nodes.

processor

reachability

for node

complete graph

Since

a

j.

Thus, this algorithm completely avoids communication and synchronization during the transitive closure computation. The price paid
is a relatively more expensive composition step and extra storage requirement with each processor. As such, this algorithm can be
veiy attractive in systems in which communication costs
Matrlx.llased

32

are

such

high,

loosely-coupled multicomputers.

as

Algorithms

uniprocessor algorithm for computing the transitive closure of a Boolean matrix that requires only one pass
adjacency matrix of elements a•~ over a n-node graph, with ~ being 1 if there is an arc from node i
to node j and 0 otherwise, the Warshall algorithm requires that every element of this matrix be “processed” column by column from
is 1, and if it is,
left to right, and from top to bottom within a column. “Processing” of an element aj involves examining if
then making every successor of j a successor of i.

Warshall 19] proposed
Given
over the matrix.

It

3]

shown in

was

a

an nxn

that the matrix elements

1.

In any

2.

For any element a1~ in

row

i, processing of
row

an

can

element aa

be

processed

in any order,

precedes processing

provided

the

following

of the element ~ iff k

<j,

two constraints are

maintained:

and

i, processing of the element aft precedes processing of a~.j iff k <j.

processing orders can be derived subject
algorithms. We now develop algorithms in which

Various

to

giving rise to a whole family of Warshall-derived
processed in parallel, while maintaining the above two

these two constraints,

the matrix elements

are

constraints.
3.2.1

AlgorIthm

M.1

original relation on the source field so that each processor owns all the successors of a contiguous number of nodes (in
adjacency matrix, each processor owns a contiguous set of rows’). Processors are numbered and the processor p owns the
pth partition. Let there be m processors, and hence m partitions, and let b~ and e~ be the beginning and end nodes of the ~th
partition. Each processor executes in parallel the following algorithm, written for the processor p:
Partition the
tenns

of

1)
2)
3)

forqfromltomdo
if p equals q then

/* p drives the computation *1

forifromb~toe~do
1* process elements below the diagonal */
from

for)

b~

to

i—i do process a,j

successors_of(i) into copy_ofti)
show(all, copy_of (i))

4)

copy

5)
6)

forifromb~toe~do
1* process elements above the diagonal */
for

7)
8)
9)
10)

As noted before,

processing

of

an

else

1* q (

p)

is

j from i+1 to e~ do process a•~
driving the computation ~/

forjfromb~toe~do
remote—get(copy_oflj)) I”
for i from

element a.j involves

examining

b~

to

from q ~/

e1, do process a,,

if aq is 1, and if it is, then

making

every

successor

off

a successor

of i (that is, ORing row j into row i). Clearly, for processing an element ~ a processor needs access to both rows i and j. It is
guaranteed that the processor will have available to it the row i, since a processor processes only those elements that it owns and the
matrix has been

partitioned

among processors row-wise, but it may have

Algorithm M.1 is an asymmetric algorithm. For
~f part of step 2 and all other processors execute
processor gets to

1.

The

row

assume

this role

remotely

the

row

j.

each value of q, only one processor (the processor whose number is q) executes the
the else part. The processor executing the ~f part drives the computation, and every

once.

i of the

of the matrix

to access

adjacency matrix corresponds to the successor list of node i(that is, all triples of the relation that have i in the source field). Sirnilariy, the column j
corresponds to the predecessors of the node j. We shall freely alternate between the topic, graph, and adjacency matrix representations of a relation.

32

When the processor p is executing the ~f part, it does not require any remote access at step 3 or step 6, since the rows i and j needed
for processing the element a,, belong to the partition owned by p. The if part is executed in two steps: first the elements below the

processed, followed by a processing of elements above the diagonal. Elements are processed in row-order from top to
the processing of elements below the diagonal, as soon as the diagonal element is reached in a row, a copy of this

diagonal

are

bottom.

During

is made for

row

showing

it to other processors (for reasons that will become clear shortly), and processing continues with the next
not be two physical copies; a simple one-bit pubiic/~,rivate mark associated with each topic will be

Note that there need

row.

Furthermore, a copy is required only in those systems in which other processors have direct access to p’s latest successor
For example, in a message-passing system in which a show involves a broadcast, the copy created at step ~ is really the

sufficient.
lists.

content

of the broadcast message.

When the processor p is executing the if part, all other processors must be executing the else part of the algorithm. (This is not
strictly true. It is possible that while the processor p is still executing the if part, the processor p+l, if it has completed executing

(f part, since the processors need not synchronize for every new value of q). Any of
j that is owned by p before it can execute step 10, which is made available by p at
step 5. However, what p shows is a copy of row i made at step 4. This avoids unnecessary synchronization since p does not have
to wait for every other processor to see row i before making additions to it at step 6. Precedence constraint 2 will be violated if the
processor r gets at step 9 row i that has some nodes added to it by p at step 6. Note that r processes elements in the column order
(from left to right) so that as soon as a row is shown by p. all elements in the partition owned by r are processed that have the same
the else part, may move on to start executing its
these processors, say r, will need access to row

column number

as

the number of this

processors do not have
synchronization, if necessary.

Finally,

to

row.

synchronize

Algorithm M. 1 can be modified so that each
clubbed together and shown (and accessed) as
3.2.2

AlgorIthm

step 1 for every value of q.

at

successor
a

unit.

list is

See

2]

not

shown

as

soon

as

The

it

blocking remote-get

was

ready,

but rather

a

step 9 performs

at

number of lists

are

for details.

Mi

A disadvantage of Algorithm M.1 is that if a processor p is slow in creating and showing the successor list of i at step 5, all other
processors may be blocked at step 9 for the successor list of i. Instead of blocking such a processor q, Algorithm M.2 attempts to
schedule processing of some other element within the partition belonging to q. Instead of assigning contiguous successor lists to the

processors, lists are assigned in a round-robin fashion. Assume, as before, that the graph has
The following algorithm, written for processor p. is executed in parallel by every processor:

/~

start:
set

the

row_received flag

to

now

for

false

(i
if

1* first process elements below the diagonal *1

for(i
if

=p ; I

n

; I

I

j

from 1

to

process elements above the
n ; I = i + m ) do
p ; i

row_received flag
j from i+1 to n

if a•~ has not been processed then
if j does not belong to the partition of p then

f~ get the
if

row

copy_oflj)

j ~/

continue

with

next

diagonal */

then go

to start

outer

loop

with

next

I

else

not block */
enable(on show of copy_oJ(j),
set the row_received flag to true)

loop

processors.

enable(on show of copy_oflj),
the row_received flag to true)

has not been shown then

outer

true

in

set

1* do

continue

is

are

if aq has not been processed then
if j does not belong to the partition of p then
if copy_ojtj) has not been shown then

is true then go to start
i~1 do

row_received flag

for

nodes and there

for

) do

+ m

=

n

remote—get(copy_oftj))
process

wait if there is any

i

a.~

pending

enabled

interrupt

else/” get it *1

remote—get(copy_oflj))
process a,,

successors_of(I ) into copy_ofli)
show(all, copy_of (I))

copy

processor p finds that it cannot process an element a1, since p does not own j, and the
its
owner, then p does not block as in Algorithm M.l. Rather, p enables an interrupt
by
list off is shown, and moves on to process the next row in its partition.
If

a

A processor p shows
matrix. To minimize
owns

from top

back to
the

to

successor

j

has

not

yet been shown
successor

only after

blocking,

bottom.

processing

list of

receive notification when the

p has processed all the elements of row i that belong to the lower-triangular half of the
every processor tries to show the rows it owns as early as possible. A processor shows the rows it
Thus, whenever a processor p is interrupted on a new row being shown by some other processor, p comes

a row

i

successor

to

the top most row in its partition that it has not yet shown to other processors.
to show to the other processors.

list is saved

33

As in

Algorithm M.l,

a

copy of

A final note with regard to the parallel algorithms presented in this section. Although the algorithms have been presented for
reachability computation for the ease expositi~on, they may be trivially adapted to solve path problems using the techniques presented
in 3]. Path computation is a generalization of the simple reachability computation. While reachability computation can tell whether
Indeed, most
a point p can be reached from q, path computation can additionally determine some properties of the path from q top.
of the applications of practical interest require path computation, and the experimental results presented in the next section are for the
performance of these algorithms in computing a bill of materials.
EXPERIMENTAL RESULTS

4.

4.1 The Set

Up
in Section 3 have been

algorithms presented
eight 14 Mhz AT&T

The

implemented

on

the Silicon Database Machine

(SiDBM) 15]

that

presently

consists

single board computers interconnected with the standard VMEbus. Each processor has one
on-board
of
local,
megabyte
memory, and they all have access to 16 megabyte global, off-board memory on the bus. Algorithms
were coded in Concurrent C 7].
of

Two different
second

was

a

results for the
4.2

The

32100-based

implementations of communication mechanisms were tried for our algorithms. The first utilized shared memory. The
passing scheme. In this paper, we present results only for the shared memory configuration; see 2] for the
message passing configuration.

message

Methodology

Synthetic graphs

were

used in the

number of nodes, and the

degree

performance

evaluation

of each node. These

two

Two parameters of

experiments.
parameters

were

varied

a

graph

to create a set

of transitive closure involve

were

identified

of random

as

important:

the

graphs.

decided

to experiment with the
applications
path properties 1, 16],
of
transitive
in
databases
is to compute a bill of
One
closure
common
use
computation
path properties.
materials in a manufacturing situation. All our experiments were rim for this problem. Since a bill of materials cannot be computed
from a graph with cycles, only acyclic graphs were used.

Since

most

practical

database

we

of transitive closure with

performance of a parallel algorithm is its speed-up defined as the time taken by the best serial algorithm to
computation divided by the time taken by the given parallel algorithm. Since our parallel algorithms can be
considered the parallelization of the well-known sequential algorithms, we have taken speed-up to be the time required for the
appropriate sequential algorithm divided by the time taken by the parallelized version. Thus, our speed-up numbers for 1.1 compare
it with the semi-naive, and our numbers for M.l and M.2 compare them with the Warshall algorithm, all memory-resident
A standard metric for the
the

perform

43 The

same

Experiments

performance of the three algorithms on a directed acycic graph (DAG) with 250 nodes and an average degree of
graph had 1236 arcs, and its closure had 28422 arcs. The time taken by all three algorithms follows an approximate
hyperbola, as one would hope if a linear speed-up were being achieved. The two direct algorithms perform considerably better than
the iterative algorithm, irrespective of the number of processors used. The poor performance of the iterative algorithm can be
attributed to the large number of iterations it performs.

Figure
5.

1 shows the

The

Figure 1 also shows the corresponding speed-up obtained. Notice that all three algorithms parallelize quite well. Comparing the
three algorithms, we find that the speed-up obtained by M.1 was consistently better than that obtained by M.2, which we shall
explain shortly. The speed-up obtained by I.! was consistently better than that obtained by M.1. The reason for the better speed-up
with 1.1 is the longer time that it takes to perform the same computation, making even small problems appear “large” in comparison
to the overheads of parallelization. Thus, by using 8 processors, we were able to obtain a speed-up of 6.9 with 1.1 and a speed-up of
5.9 with with M.2.

FIgure

1.

Comparative performance

of the three

1.1: 0

algorithms

M.l:

•

on a

random DAG

M.2:

600.

(nodes

=

250, degree

=

5)

A

8—

6—
400

Time

Speed-up

(seconds)

4

—

200
2—

0

0—
0

2

4

6

8

0

Number of Processors

2

I

I

I

4

6

8

Number of Processors

34

We should also remark at this stage that we are using the
the number of processors is varied. As pointed out in

size

fixed
9],

measure

of

speed-up2

with this

problem

a

in which the

measure

is that

problem

as

size is

kept

fixed and

the number of processors is

to each processor reduces. Thus, when 8 processors are used on a 250-node graph, each processor is
assigned only 31 nodes to work on. This causes severe under-utilization of the local memoiy available with each processor and large
amplification in any load imbalance between processors due to nodes having different number of successors. The 250-node graph
In practice, the fixed quantity often is
was just about the largest graph whose closure could be computed using a single processor.
not the problem size but rather the amount of time a user is willing to wait for an answer: given more computing power, the user
expands the problem to use the available resources.

increased, the work assigned

We also studied the

sensitivity

of the

speed-up

numbers

to

the

graph being

used for the

computation, by vaxying

the number of nodes

and the average degree. We also carefully metered the time spent in performing different activities at each processor, and found that
a large fraction of the CPU time was spent in computing and little time in “control”, which includes synchronization waits for other
processors.
also small.

In fact, the control time is

2] for the detailed

See

In view of the

computation

time

zero

for 1.1 since it

runs

with

no

synchronization

whatsoever.

The communication time

was

results.

dominating

the total time taken, for

suitably large problems,

we

expect that

an

almost linear

speed-

up should be possible even as the number of processors grows. Also, a more complex algorithm such as M.2 is of little value since
all it can do is to reduce the amount of time spent in synchronization waits while adding some overhead for a more complicated
control strategy. In fact, the overhead added appears in most
slightly worse than M.l in most cases.

5.

cases to

exceed the idle wait time eliminated,

resulting

in

performance

RELATED WORK

Valduriez and Khoshafian

proposed in 18] a hash-join-based parallelization of the semi-naive algorithm. As discussed in 2], the
technique is that (almost) every tuple computed must be shown to a different processor and remotely
primary
accessed. What is worse, this communication occurs prior to the elimination of duplicates, and thus could actually involve numbers
of tuples far larger than those actually present in the final result. In fact, in the worst case, 0(n3) tuples are remotely accessed at
each iteration, resulting in large communication. Furthermore, the processors need synchronization at every iteration. We, therefore,
expect our paralleization of the semi-naive algorithm to perform better than the hash-join-based paralleization.
drawback of this

Valduziez and Khoshafian also

proposed

parallel

another

transitive closure

algorithm in 18]. This algorithm partitions the given
assigned to it. The closure of these “transitively
merging them into one component, till one final
parallel sorting algorithms, is used for the merging process.

among processors, and each processor computes the closure of the subgraph
closed” subgraphs is then computed by recursively considering two components,

graph

is obtained. A two-way merge tree, similar to the one used in
Valduriez and Khoshafian found that this algorithm always has inferior response time characteristics when compared to the hashjoin-based parallel semi-naive algorithm. Furthermore, although this point was not addressed in 18], unless the components have a

graph

certain

convexity property so that paths between
“transitively closed” graphs would be as hard as

convex

two

nodes do not criss-cross

when the

graphs

were not

across

computing the closure of two
Dividing a given graph into such

components,

transitively

closed.

components is non-trivial.

Jenq and Sahni 11] have presented a parallel implementation of the all-pairs shortest path algorithm on a NCUBE hypercube. The
Jenq-Sahni algorithm computes the closure in n rounds of processing (n is the number of nodes). A broadcast and a synchronization
of all the processors is required after each round. The largest reported speed up was about 15 using 32 processors on a 96 node
graph. Experience with parallelizing the shortest path problem on Denelcor HEP were reported by Deo, Pang, and Lord in 8], and
by Quinn and Yoo in 14]. The former reported a speed up of about 3.5 with 8 processors on a 100 node graph. The best speed up
reported by the latter was about 9 with 16 processors on a 500 node graph.
6.

CONCLUSIONS

There

are

several,

not

all

independent,

issues

to

be considered when

devising

a

parallel algorithm:

how well the load is balanced

among the processors, how much time is spent by each processor waiting for data or a synchronization message from another
processor, how much inter-processor communication is required., and what speed-up is achieved with multiple processors. In this
paper

we

have

presented

three

parallel

transitive closure

algorithms

that

effectively

balance, high processor utilization, low communication and control overhead, and
7.

almost linear

We have

a

good

load

speed-up.

ACKNOWLEDGEMENTS

We

2.

address all of these issues.

an

are

As

an

10 USC

grateful

to

alternative.

Canaday,

Narain Gehani, Eric

Petajan,

and Bill Roome for their

scaled

help

at

different stages of this work.

measure of speed-up was proposed in 9) in which the problan size is increased with an increase in number of processors. We decided not
because the fixed size measure makes it aintpler to pet ~ir work in perspective with other work, and partly because we fdt that there was no
way for scaling up the transitive closure probtan with the number of processora.

a

this measure~

readily acceptable

Rudd

pattly

35

REFERENCES

11

Agrawal, “Alpha: An Extension of Relational Algebra to Express
Corf. Data Engineering, Los Angeles, California, Feb. 1987, 580-590.
R.

a

Class of Recursive

Also in IEEE Trans.

Queries”, Proc. IEEE 3rd Int’l
Software Eng. 14, 7 (July 1988),

879-885.

2]

R.

Agrawal

Distributed

3]

R.

and H. V. Jagadish. “Multiprocessor Transitive
Systems, Austin, Texas, Dec. 1988, 56-66.

Agrawal,

S. Dar and H. V.

Trans. Database
for

Jagadish.

Closure

“Direct Transitive Closure

Algorithms”,

Proc. Int’l

Algorithms: Design

Symp.

Databases in Parallel and

and Perfonnance Evaluation”, ACM

To appear. (Preliminaxy version appeared as: R. Agrawal and H.V. Jagadish, “Direct Algorithms
Closure of Database Relations”, Proc. 13th Int’l Conf. Very Large Data Bases, Brighton.
Transitive
the

Syst., 1989.

Computing
England, Sept. 1987, 255-266)..
4]

R.

Agrawal, S. Dar and
Angeles, California,

Los

5]
6]

H. V.

of Database Relations”, Proc. IEEE 5th Int’l

Jagadish, “Composition

Data

Conf.

Engineering,

Feb. 1989.

F. Bancilhon. “Naive Evaluation of

Recursively

Defined Relations”, Tech.

D. Bitten, H. Boral, D. J. DeWitt and W. K. Wilkinson, “Parallel
ACM Trans. Database Syst. 8, 3 (Sept. 1983), 324-353.

Rapt. 1)8-004-85, MCC, Austin, Texas,

Algorithms

1985.

for the Execution of Relational Database

Operations”,
7]

R. F. Cmelik, N. H. Gehani and W. D. Roome, “Experience with Multiple Processor Versions of Concurrent C”, AT&T Bell
Laboratories, Murray Hill, New Jersey, 1987. To appear in the IEEE Trans. Software Eng..

8]

N. Deo, C. Y.

Pang

and R. E. Lord, “Two Parallel

Algorithms

for Shortest Path Problems”, Proc. IEEE Int’l

Parallel

Conf.

Processing, 1980, 244-253.
9]

J. L. Gustafson, G. R.
Journal

101

on

Scient~.fic

Montry

Y. E. loannidis, “On the

Data Bases,

and R. E. Benner,

and Statistical

Computation

J. F. Jenq and S. Sahni, “All
Processing, Aug. 1987, 713-716.

12]

R.

of the Transitive Closure of Relational

Pairs Shortest Paths

Kung, E. Hanson, Y. loannidis, T. Sellis,
Workshop Expert Database Systems,

1st Int’l

14]

L.

on

a

Operators”,

Shapiro

Hypercube Multiprocessor”,

a

1024-Processor

Hypercube”,

SIAM

Proc. 12th Int’l

Conf. Very Large

Proc. IEEE Inf I

and M. Stonebraker, “Heuristic Search in Data Base

Parallel

Conf.

Systems”,

Proc.

Kiawah Island, South Carolina, Oct. 1984, 96-107.

H. Lu, “New Strategies for Computing the Transitive Closure of
Bases, Brighton, England, Sept. 1987.
M. J.

of Parallel Methods for

Kyoto, Japan, Aug. 1986, 403-411.

11]

13]

“Development

Computing 9, 4 (July 1988),.

a

Database Relation”, Proc. 13th Int’l

Quinn and Y. B. Yoo, “Data Structure for the Efficient Solution of the Graph
Computers”, Proc. IEEE Int’l Conf. Parallel Processing, 1984, 431-438.

Conf Very Large

Theoretic Problems

on

Data

Tightly Coupled

MIMD

15]

W. D. Roome and M. 1). P. Leland, “The Silicon Database Machine: Rationale,

Workshop
16]

on

Database Machines, Karuizawa,

A. Rosenthal, S. Heiler, U.

Applications”,
171

Proc. ACM-SIGMOD 1986 Int’l

South Carolina,

1

A Practical Approach to Supporting Recursive
Management of Data, Washington D.C., May 1986, 166-176.

Queries Using
April 1986, 197-208.

Join Indices”, Proc. 1st Int’l

(Feb. 1988), 19-42.

5. Warshall, “A Theorem

on

and Results”, Proc. 5th Int’l

“Traversal Recursioit
on

P. Valduriez and S. Khoshafian, “Parallel Evaluation of the Transitive Closure of

Programming 17,
19]

Conf.

P. Valduriez and H. Boral, “Evaluation of Recursive

Systems, Charleston,
18]

Dayal

Nagano, Japan,

and F. Manola,

Design,

Oct. 1987.

Boolean Matrices”, 1. ACM 9, 1 (Jan.

36

1962), 11-12.

a

Conf Expert

Database Relation”, Int’l J.

Database

of

Parallel

Exploiting Concurrency

in

a

DBMS

Implementation
Systems!

for Production

Raschid2

Louiqa
Department

Sellis2,

Timos

of Information

Chih—Chen Lin

of

Systems

Department
Computer Science
and Systems Research (.~ent.er
College Park, MD 20742

School of Business and Management.
University of Maryland,

ABSTRACT
In this paper,

tailor DBMS concurrent execution to

resulting

execution

for

a~

is carried

execution schedules.

parallel
1.

we

concurrent

production system
productions. This research

environment, and

investigate
out in conjunction
strategies
with a novel DBMS mechanism for testing if the left—hand side conditions of productions are satisfied.
We demonstrate the equivalence of a~ serial and a concurrent (interleaved) execution strategy and define
requirements for a correct, serializable execution. We also compare the number of possible serial and
the

Introduction

integration of artificial intelligence (Al) and database management (DBMS) technology has been
3,4]. An important aspect of this integration is identifying functional sinii
la.rities in data.base processing and reasoning with rules. This will allow techniques designed for use iii
either technology to be used in a functionally integrated environment. In this paper, we foctis on tailoring
DBMS concurrent—execution techniques to a production system environnient.
Production systems
of
in
the
form
and
rule—based
the
of
are a good example
productions
represent knowledge
reasoning para
digm. We choose the OPS5 production system I] because of its popularity in the Al domain. Our
research on concurrency in production systems has been carried out in conjunction with a novel DBMS
mechanism for testing if the left—hand side conditions of productions are satisfied 6]. In this paper, we
identify severa.l potentia.l instances for concurrency, and demonstrate the equivalence of a serial and a
We specify the requirenieiit.s for serializability, assuming a
concurrent (interleaved) execution strategy.
2—Phase Locking scheme, and compare the number of serial and parallel execution schedules. For more
details, the reader is referred to 5].
The

the focus of recent research

2.

Production
A

tion

production system

part

memory

modify

on

(WM)

elements.

the WM.

the

a

collection of Condition-Action statements, called prodnclion.s. The condi
(LHS) is satisfied by data stored in a database, composed of working

A

The action part

right—hand side (RHS) executes operations
production system repeatedly cycles through the following operations:

production

r,

determine if

qualifying production

Select: Select

one

production

to

the

out of the

on

the

LHS(r)

conflict

is satisfied

by

the current \\TM contents. Jf

no

candidate, halt.

WM

The

and,

as

following

a

result,

OPS5

additional

conflict set; if there is

production

productions

removes

so.

that

can

a(JcJ

set.

Perform the actions in the RHS of the selected candidate. This will

Act:

may he

fired,

or some

change the content of t.he
productions may he deleted.

Mike from the WM class Emp if he works

on

the first

floor,

Toy department:

This research

Maryland
2

is

the left—hand side

Match: For each

in the

Systems

was

partially sponsored by

the National Science Foundation under Grant CDR—85—00108 and

Office of Graduate Research and Studies under

Also with the

University

of

Maryland Institute

a

Summer Research Award.

for Advanced

Computer Studies (UMIACS).

37

by

the

University

of

(p

Rulel

(Enp IName
(Dept tDno

~Dno <D>)

<s>

Salary

tDname Toy IFloor

<D>

(remove

•+

t

Mike

TMgr <M>)

1

1))

efficiency of OPS5 has been attributed to the Rete algorithm 2]. This algorithm
exploits temporal redundancy and compiles the LHS condition elements into a binary discrimination net
work.
The network is an inherently redundant storage structure 6]. This redundancy results in a
decrease of processing efficiency of the R.ete network implementation with a large database. The lack of
support for universal quantification is also a drawback of the R.ete implementation.
The execution

A Novel DBMS

3.

implementation,

in the DBMS

evaluated against the
Emp(Name.Salary.Dno)

W~N4

name, whose values

hound

ables that

duction,

are

as

are

common

equivalent

are

of OPSS

Implementation

a

WM

production

class

Rulel.

as

a

query

to

be

relation, such as
using
the
~ symbol and a.
Attributes, represented by
simulated

is

a

WM

value ranges are equivalent to selection predicates. \‘ari
the same pro
e.g., <D> in classes Emp and Dept, and occur in

t.o constants

WM cla,sses~

production

the LHS condition of each

treat

Each

classes.

used in

to two

to

we

or

join.

data structure, a COND relation, which is used to link partially matched ttiples
from the WM relations. A COND relation is required for each WM relation (class) and will store match
ing tuple information for all productions that refer to that WM.class. For example, WM classes Emp and
occur on the LHS of the production
Rulel, and are related through the join variable <0>, in this
We introduce

a

new

Dept

When

production.

a.

input into the WM relation Enip, we store this informa
Dept, COND—Dept. Now, when a. tuple is input into the WM relation Dept

<Mike. 10000,D12> is

tuple

tion in the COND relation for

value of Dno=D12, the link information stored in the relation COND—Dept reflects the existence of
matching tuples in Emp. The CON1) relation obviates the need for the join operation, which would other

with

a.

required.

wise be

A brief

description

of the COND relations follows; details

can

he found

in

6].

record the unique production identifier.
rfhe COND relation has the following attributes: (I)
of the same production. (3) Res
conditions
(2) Condition Element Number (cEN) to differentiate among
of Related Condition Elements
list,
A
Wivi
relation.
(4)
trictions on each attribute of the corresponding
RID to

(RcE),

each

RCE, with

through
i~1

RCE

being represented by

default. value of

a

example.
respectively,

an

.2,3,

zero.

four

Assume

and the

(RID, CEN) pair. (5)

a

We illustrate the
relations

A,

B,

use

C,

(B

D,

attributes

with

Bi,

Al,

Ci,

and

Di,

for

(pR2
IA1
IB1

(C

tCl

-+

(

<x>
<~>

‘C’

a’

tA3 <z>)

(A

IA1

<x>

~A2

tB2 <y>
1C2 <y>

‘b’)

(B

~B1

<x>

~B2 <y>

tC3 <z>)

(0

ID1

‘d’

1A2

1B3

4’

))

There will he four COND relations:
to

hit register. comprising one bit. per
the COND relation(s)

Mark

following productions (actions omit.ed):

(pRi
(A

A

of these attributes in

each other

by variables

<x>,

COND—A,

<y> a.nd

<z>

occurring

on

1D2 <y>

1B3

‘b’)

1D3

<x>)

))

(

COND—C, and

COND—B,

1A3 <u>)

a’

COND—D.

the LHS of

capture linkages between tuples of the WM relations A, B, C, and

D

These relations

productions
that.

must

Ri

and

R2.

are

related

Variables

he stored in the COND

relations. The initial contents of these COND relations describe the LHS conditions of the productions.
In the COND relations tha.t are shown below, these initia.l tuples have their Mark bit. values set. t.o 0.

The RCE list indicates which conditions

(involving

other WM

relations)

of the

same

in

production

are

COND relation

IuJ)le
being
inaiching—pol.Iern. There is one Mark bit for each RCE, which if
condition ele
set. indicates that. the matching—pattern is created by a tuple that. satisfies the corresponding
the
relation
WM
in
having
property of
a
(related)
ment. This implies that. there is already some tuple(s)
when
WM
relation.
the
in
current
with
Thus,
be
it.
therefore
tuples
can
the matching-pattern and
joined
deletions in the current relation

affected by insertions or
with at least. one Mark bit.

a.

tuple

set

is called

examined.

A

a

a.

is inserted later in the current- WM relation which nia,t.ches that. patt.erii.

38

we

know in~.mediatelv

that there is

a

match without.

having

to

examine the other WM

relation(s).

COND-B

COND-A
Mark

J

Mark

A3

RCE

bit

RID

CEN

Si

82

53

RCE

bit

‘a’1

<z>

B,2),(C,3:

00

Ri

2

<x>

<y>

‘b’

A,i),(C,3)

00

<x>

‘a’

<U>

B,2),(D,3:

00

82

2

<x>

y>

‘b’

A,i),(D.3)

00

1

4

‘a’

<z>

Ri

2

<x>

7

‘b’

‘a’

<u>

10

RI

2

4

<y>

‘b’

A,1),(C.3)
A,1),(C,3)

01

4

Ri

1

<x’

‘a’

8

01

Ri

2

4

7

b

81

1

4

‘a’

8

11

R2

2

4

<y>

‘b’

82

1

4

‘a’

<u>

01

82

2

4

7

‘b’

R2

1

4

‘a’

<U>

B,2),(C,3
B,2),(D,3
B,2),(C.3
B,2),(C,3
B,2),(D.3
B,2),(D,3

10

1

Ii

R2

2

4

7

‘b’

RID

CEN

Al

Ri

1

<x>

82

1

Ri
82

A2

CO ND-C

A,i),(C,3)
A,1),(D,3)
A,i),(D,3)
A,1),(D,3)

io
ii
10
01
ii

COND-D
Mark

Mark
RID

CEN

Cl

C2

C3

RCE

bit

RID

CEN

Dl

D2

D3

RCE

bit

Ri

3

‘c

<y>

<z>

A,i),(B,2

00

82

3

‘d’

<p

<x>

A,i),(B,2

00

Ri

3

‘c’

7

<z>

82

3

‘d’

3

‘c’

<y>

8

10

82

3

d’

Ri

3

‘c’

7

8

A,i),(B.2
A,1).(B,2
A.1),(B.2

01

Ri

11

R2

3

‘d’

When

7

4
4

A,i),(B,2
A,i),(B,2

Oi

<y>
7

4

A,1),(B,2

11

10

tuple is inserted into, say, WM relation A, two tasks are performed. First, we have to
existing tuples in COND—A to deterniine if this new A tuple satisfies any productions; this
would be the case if COND-A already had a matching—pattern tuple with appropriate linkages (for the
A matching—pattern tuple with both Mark bits set indi
common variables) and with a RID value of Ri.
cates the existence of tuples in the WM relations B and C that together would satisfy the production Ri.
The second task is to store the information on linkages through t.he common variables, i.e., how tuples in
the WM class A interact with B and C, or B and D, as indicated on the LHS of the productions. This
information is stored in the form of matching—patterns in COND—B and COND—C.
a

examine the

tuples B(4.7.b), C(c.7~8), A(4.a.8) and D(d.7.4) in that order.
relations.
Tuple B (4.7, b) will insert two tuples in
COND—A (corresponding to Ri and R2), one tuple in COND-C for Ri and one tuple in CQND-D for R2.
Tuple C(c.7,8) will insert two tuples in COND—A for Ri. The second tuple will have both Mark hits set
indicating the presence of WM elements from B and C simultaneously satisfying Ri. One tuple will also
be inserted into COND-B for Ri. Tuple A(4,a.8) will match with a matching—pattern for Ri with both
bits set; this implies there now exist tuples from A, B and C satisfying Ri and this pattern must be
placed in the conflict set. This tuple also introduces three tuples in COND-B, two tuples in COND—C, and
two in COND—D. Finally, tuple D(D,7.4) will match a matching—pattern in COND—D for R2 with both bits
set; this pattern for R2, is placed in the conflict set. This tuple also inserts two tuples in COND—A and in
COND—B. The contents of the relations are shown in the above tables. Details of the algorithm described

Suppose
tuples

Several

above

4.

are

in

that

we

insert the

will be inserted

6].

Processing Applicable
In

into the COND

DBMS

Productions
all the tasks associated with the execution of

a candidate production
single (complex) transaction. Within this transaction, the first
task is retrieval from the V/M relations; the matching—pattern tuple for a selected production does not
The
store pointers to, or identifiers of, the actual tuples of the WM relations satisfying this production.
attribute values in each matching—pattern will provide the selection criterion that must he applied to the
WM relations. The next task is executing t.he corresponding RHS actions. These actions represent changes

(from

our

the conflict

implementation,

set)

will be defined

as

a

39

insertions, deletions and updates of the WM elements. RHS actions that
add or delete tuples from the WM relations trigger the insertion or deletion algorithm, respectively. Both
algorithms will update the conflict set and also execute the maintenance tasks that update the CONID
relations. The deletion algorithm performs searches similar to the insertion algorithm; the difference is
to

the WM classes and include

matching—patterns from the conflict set, and resets the Mark bits in the COND relations.
An update is equivalent to a delete followed by an insert, and triggers both algorithms. Conceptually,
execution of the production (transaction) completes after updating the WM relations, the conflict set and
that it deletes

the COND relations.
In the Rete network

of OPS5,

implementation

in the conflict set

productions placed

are

executed in

a

serial order. in each cycle, a single production is selected and its RHS actions are then executed; this may
result in changes to the WM. In the next match phase, these updates to the WM are propagated through
the discrimination net.

Consequently, productions

deleted, or productions may
single production, the Rete imple
production in the conflict set, and

in the conflict set may be

When several combinations of the WM elements satisfy a
mentation stores each combination as a separate instantiation of the

be added.

each is executed independently. However, in our implementation, a set—oriented selection will retrieve all
possible combinations of the WM tuples satisfying ea.ch LHS condition. Thus, a selected production can be
simultaneously applied to all possible combinations of the WM tuples, tha.t are retrieved. This leads to
potential intra—produc lion concurrency (within a single production) when executing productions.

Similarly, combinations of the WM elements could simultaneously satisfy different productions.
potential inter—production concurrency for executing the entire conflict set. In the next sec
tion, we explore a concurrent execution strategy that. exploits both forms of concurrency.

This leads to

of

Equivalence

4.1.

Given

an

Serial and Paraliel Execution

a

of transactions, each of which

initial set ~I1

corresponds

to

already satisfied production

an

in the conflict set, we compare the serial execution of these transactions, e.g., in OPS5, with their inter
The serializability criterion is used to show the
leaved execution in a concurrent environment.

equivalence of both execution strategies.

production system, in each step i, a single transaction. 7~ is arbitrarily selected from the
applied (Select and Act). We use the term ~‘arhitrarily”, because the OPS5 conflict, resolu
tion strategies are syntactic. Subsequently, the production system will determine (Match), if, as a result
of applying T1, some other transactions in this conflict set are no longer applicable; if so, these transac
tions will be deleted from the set. Let the set of transactions deleted in step i be del1. Also as a. result of
applying 7~, the production system will determine (Match) if some additional transactions are now appli
cable as well. Let the set of transactions added in step i be add1. The new set of candidate transactions
4’ ~ 7~) —del1 U add1. This process will continue until in step F’, the set. 4’ p is
in step i+ I is ‘1’~÷~
In

a

serial

conflict set and

=

empty.
The selection of each

the

set ‘P

initial

—

set ‘4’

{T1J
and

—

T1, T2

all transactions
‘4’

11+1

is the set

,

in4’1

some

T~,

are

u adds,

‘P2

set

U

11 steps

dc1~)

add1.

—

which is the

same

as

the serial

where each 7.

either executed

i.e.,

entirely possible that. in step 2, 7~ is selected from
In other words, T2 could also be selected from the
transactions add1. Similarly, in subsequent. steps i, 7~ can he
it is

arbitrary; thus,

U ({ ~)

—

described, then after

transactions

is

from the added set of

not,

selected from the set 4’
as

T~

which is the

del1

the set ‘P

production system
happens to be an element

or

deleted and the

set

all the transactions added in the

of

—

If the selection is

~ adds.

will have executed
of the initial set

applicable

sequence

transactions for step

11 previous steps,

of

11
‘J/~. After step J~.
a

which

were

not.

(f~+ I),
selected

2=1

previously. In step (f~+ 1), ~ is
Given this
set of
must

same

transactions. If
be

equivalent

to

initial set ~I1
an

chosen from this set
a.

concurrent execution

is

appropriate protocol
serial schedule T~, T2,

some

strategy would interleave the execution of this

used, and the resulting schedule is serializable, then
,

etc., where each

40

7~.

must he from the initial set. ‘1’

it.
i.

In other
the

4.2.

words, the

production system

concurrent

Concurrent Execution with the DBMS
In

will execute

serial schedule arbitrarily selected

same as some

DBMS environment,

by

an

the serial

equivalent serial schedule
production system.

which will be

Implementation

transaction commits all its changes after it has terniina.ted its execution
commits, these changes are physically made in the database. iii a con
current environment, appropriate locks must be obtained to satisfy the following: First, the interleaved
execution of a set of productions must maintain consistency of the database. i.e., two transactions that
update the same WM relation must be serializable. Second. transactions tha.t are inter—related and a.ffect
each other’s execution, i.e., transactions that delete each other’s matching—pattern tuples from the conflict
set, must interact correctly. For example, when a transaction 7. executes, the selected commit point, must
be chosen to enforce a delay in the execution (and commit) of the transactions in the set. dcli. i.e., transac
tions that are deleted as a. result of previously applyjng 7~. Transactions in this set must either not be exe
cuted or, if executed, their changes must not he committed to the database.
a

a

Once the transaction

normally.

(production)

A transaction

is

positively dependent on a WM relation if the LHS of (lie production is
specific tuples of a WM relation. A transaction is nega(ively dependent
on a WM relation if it is satisfied by the absence of some specific tuples
A transaction is independent of
a WIM relation if it is unaffected by the existence or absence of specific tuples.
In the current definition of
the OPS5 language, the RHS actions of a. production can only delete or update tuples from the WM rela
tions on which it is positively dependent. However, a. transaction ca.n insert tuples into any WM relation.
The production R3 shown below, is positively dependent on the WM relations A and B. It is negatively
dependent on C, and is independent of D.
satisfied

the existence of

by

some

(pR3
(A

IA1

cx>

tA2

(B

IB1

<x>

tB2 <y>
1B3 ‘b’)
1C2 <y>
1C3 <z>)

(C
~

(

~C1

‘c

(remove 2)

(make
The

(1)

1A3 <z>)

‘a’

(D

ID1

tD2

‘d’

following requirements

Each transaction

depends
of these

on

and

7~

are

tuples by

must

used

must

he

obtain

to

1D3

‘d’

met to

an

satisfy

R

‘d’)

)

produce

lock for

the LHS of the

a

serializable execution schedule:

specific tuples of the WM relations it Positively
production. This prevents the deletion or update

other transactions. Note that if any of these retrievals returns an enipty set of
happen if 7~ is in the delete set. of a previously

then the transaction is aborted. This may
committed transaction.

tuples,

(2)

Each transaction

7~ must obtain an R lock for the entire WM relations(s) it. negatively depends on.
subsequently verify that there are no tuples satisfying the search criterion for this negative
dependency. Note that if the verification fails, 7~ is aborted since it is in some delete set.
It

(3)

must

Each transaction ~

must obtain

updates.
consequently,

necessarily
tuples must

a

These would

indefinitely

(4)

in

case

Each transaction

tuples. Again,

(5)

these

an

of

7~.

a.

W lock for

be

tuples

exist and

specific tuples

of the V~vl relation that it deletes

for which it would have
could

not

have been

previously obtained

deleted.

However,

it

or

R

lock;

could

wait

a.n

deadlock.

must

obtain

indefinite wait

Once these locks

are

modify the WtvI

relations and

a

W lock for

implies

a

an

entire WM relation if there is

a.n

insertion of

deadlock situation.

obtained, transaction 7’ can update the matching—pattern tuples in the conflict.,
update the COND relations. It then commits all of its changes a.nd

releases all locks.

In
of

5],

we

have

proved .the correctness of our requirements for serializability by examining
productions (through the WM relations).

inter—dependencies”among

41

all

cases

Consider the

of transactions 7

case

deletes

specific t.uples from R
relation R before 7~ attempts

and

77,.

that

positively dependent

are

and this may affect the execution of 7.. ii
to obtain a W lock, then, 77,. will precede

the WM relation

on

obtains

7~

in the

R.

T~

R lock for the WM

an

equivalent serial

execu

tion and the database will be consistent.
If

obtams

7~

W lock

a

on

the WM relation

R

(and

thus, completes

execution)

before

T~

requests

an

T~. may be in the set del1, so its execution must be delayed until after the update or delete
Changes made to R trigger the maintenance process and propagate changes to the COND rela.

R lock, then
from

R.

tions. The maintenance process can potentially delete the matching-pattern tuple for 7. front the conflict.
must. not commit and release its locks on the WNI relations until the iiiaint.enance
set. For this reason, 7
process

completes.

matching—pattern tuple corresponding to 77,. is unaffected or is deleted before Tj starts execu
tion, then, no further action is required. If Tj has already started execution, it. will be delayed since it. will
not be able to obtain a R lock until 77~ releases its W lock on the t.uples of R. Now, even if the
is deleted, T1 will still be executed. However. 7 will not. be able (.0 process
matching—pattern tuple for
those tuples of R that. have already been deleted by 7 so the database will remain consistent.
If the

It. is also
and vice

This could lead

versa.

to

a

and

delete

T,

deadlock of the

or

update tuples front R, and

tha.t ‘I

is iii the set

del5

transactions.

two

Concurrency

Case of Intra—Production

Special

4.3.

t.hat both 7

possible

Maintaining serializability in the case of intra—productiori
following sets of tuples satisfying R3:

concurrency

requires further attention.

Consider the

{A(4.
If the

a.

t.uple

B

(4,7. b)

was

deleted by

(with the second
simultaneously:

execution of

satisfy

8) ,B(4, 7. b)} and {A(4.

R3

R3

38)

a,

B(4, 7. b)}

(with the first set of tuples), then it. would affect. (-lie subsequent.
tuples). However, if the following combinations of t.uples were t.o

R3

of

set

{A(4,a,8) ,B(4,7.b)} and {A(4,a,8) .B(4.9,b)}

then, execution of

with each combination of

R3

t,uples

in any sequence is

always serializable.

a. production and one combination of the WN4 t.uples as
representing the production and all combinations of tuples.
Nested transactions, however, are expensive to support and this solution may even negate the advantages
A preferable solution would be for each transaction to count. the
of int.ra—production concurrency.
number of times it attempts to delete a tuple. If it. attempts to do so more than once, then there is clearly
inconsistency in the execution of the transaction. Similarly, if a transaction is negatively dependent on a.
WM relation and if the transaction also inserts tuples into the same relation, then there is a possibility for
inconsistent, execution within the transaction. These issues require further consideration.

One solution is

a.

each instantiation of

to treat

nested transaction within the transaction

4.4.

Estimating

the Number of Execution Schedules

The benefits of

concurrent

locking

execution

the

relation

COND relation. In the worst

A second

be measured in severa.l

was.

of opera

First., the number

a

neglecting
or

can

non—interleaved fashion reflects the t.inie of execution. In the best case,
overhead, this will be proportional to the maximum number of updates to any WM

tions that. must. execute in

measure

involves

an

case,

this will reduce

to

the time taken for

estimation of the number of

possible

a

serial execution.

choices for

selecting

a

transac

tion in any step and the resulting number of different execution schedules in each case. This measure is of
interest since different execution schedules result in different final states of the knowledge base. See 5] for
a.

detailed discussion.

A serial system is not constrained to execute in the fashion that we just described, where the initial
in each step i, all transactions in the
sequence of transactions is limited to be in the initial set ‘I’ ~. In fact,
set.

4i~,

which

t.he set ‘4’

~,

was

previously defined,

then in ea.ch step

i,

‘~‘

~

are

candidates for execut.ion. If

we

use

is the number of available choices for

42

‘~‘

I

denote the size of

to

selecting

a

transaction.

Let

‘IS1

represent the number of choices of possible serial schedules, corresponding

,T1.

tions, T1,T2

Then

NSjis

as

to

sequence of

a

f

transac

follows:

I

NS1= III ‘I’]
We

now

the number of

knowledge

estimate the number of
serial

equivalent

base is determined

in the initial set and let

f~

possible

the

I’P~I

=

del1]

—1—

+

equivalent

for

a

sequence of

serial schedule.

of these transactions be

f

Let

I

‘P

i

I

we

tial set ‘I’ ,,.~ of added transactions.

use

THen,

NC(eq)j

where, ‘I’ 11+1

=

andf=f1+f2+
5.

as

can

measure

the LHS of the rest

(i

‘P

I

—

i
to

—

are

).

I del1]

form

a

not

Aft.er

new

ini

follows

‘2

rI(I’P11

U add1

is

we

be the number of transactions

actually executed, i.e.,

operations executed by them, collectively,

the

/1

NC(eq)j=

PS;

The final state of the

transactions.

the number of choices for the concurrent system is

satisfied. At each st.ep i,
the 11 transactions are executed

Iadd1I.

execution schedules for the concurrent

NC(eq)j,

schedules,

by

‘‘i’~4~j

where

is

a

—i—

new

Idel1I)

fI(I’I’,1+1I

x

initial set of transactions made

—i—

Idel,+1I)X

applicable after executing

the

set

‘P

.

Conclusions

problem of concurrent execution of a set of applicable productions in a
production system. We showed the equivalence of a serial and an interleaved
execution. The latter may improve execution efficiency and in the worst case it will be no worse than a
serial strategy (neglecting any locking overhead). Assuming 2—Phase Locking, we specified the require
ments for a correct serializable execution. We also estimated the number of possible execution schedules
for a serial production system compared to the concurrent one.
In this paper

DBMS

we

implementation

0.

References

1]

Forgy, C.L.,

studied the
of

OPS5

a

User’s

Manual,

Tech.

R.eport CMU—CS—81—135, Carnegie—Mellon University

(1981).

2]

Forgy, C.L., Rete: A Fast Algorithm
Artificial Intelligence (19) (1982).

3]

Kershberg, L., Ed.,

for the

Expert Database

Many Pattern/Many Object Pattern Match Problem,

Systems: Proc. From the First
Menlo Park, CA (1986).

International

Workshop,

Benjamin/Cummings Publishing Company, Inc.,

4]

Kershberg, L., Ed., Expert Database Systems: Proc. From the First International Conference.
Benjamin/Cummings Publishing Compa.ny, Inc., Menlo Park, CA (1987).

5]

R.aschid, L., Sellis, T. and Lin, C—C., Exploiting Conèurrency in a
duction Systems, International Symposium on Databases in Parallel
TX

6]

DBMS

Implementation for Pro
Systems, Austin,

and Distributed

(1988).

Sellis, T., Lin, C—C., and Raschid, L., Implementing Large Production Systems in
ment: Concepts and Algorithms, Proc. of ACM—SIGMOD, Chicago, IL (1988).

43

a

DBMS Environ

Checkpointing

and

Recovery

in Distributed Database

Sang

Systems

H. Son

Department of Computer Science
University of Virginia
Charlottesville, Virginia 22903

1. Introduction
recovery mechanism in a database system is well understood. In spite of powerful database
which detect errors and undesirable data, it is possible that some erroneous data may
be included in the database. Furthermore, even with a perfect integrity checking mechanism, failures of hardware
and/or software at the processing sites may destroy consistency of the database. In order to cope with those errors
and failures, database systems provide recovery mechanisms, and checkpointing is a technique frequently used in
The need for

a

integrity checking mechanisms

database recovery mechanisms.
The goal of checkpointing in database systems is to read and return current values of the data objects in the
system. A checkpointing procedure would be very useful, if states it returns are guaranteed to be consistent. In a
bank database, for example, a checkpoint can be used to audit all of the account balances (or the sum of all account
balances). It can also be used for failure detection; if a checkpoint produces an inconsistent system state, one
assumes that an error has occurred and takes appropriate recovery measures. In case of a failure, previous check
points can be used to restore the database. Checkpointing must be performed so as to minimize both the costs of
performing checkpoints and the costs of recovering the database. If the checkpoint intervals are very short, too
much time and resources are spent in checkpointing; if these intervals are long, too much time is spent in recovery.

checkpoint process to return a meaningful result (e.g., a consistent state), the individual read steps of the
be permitted to interleave with the steps of other transactions; otherwise an inconsistent state
can be returned even for a correctly operating system. However, since checkpointing is performed during normal
operation of the system, this requirement of non-interference will result in poor performance. For example, in order
to generate a commit consistent checkpoint for recovery, user transactions may suffer a long delay waiting for active
transactions to complete and the updates to be reflected in the database CHA85]. A transaction is said to be
reflected in the database if the values of data objects represent the updates made by the transaction. It is highly
desirable that transactions are executed in the system concurrently with the checkpointing process. In distributed
systems, the desirable properties of non-interference and global consistency make checkpointing more complicated
For

a

checkpoint

must not

because

need to consider coordination among autonomous sites of the system.

we

Recently, the possibility of having a checkpointing mechanism that does not interfere with transaction pro
cessing, and yet achieves consistency of the checkpoints, has been studied CHA85, F1S82, SON86b]. The motiva
tion for non-interfering checkpointing is to improve system availability, that is, the system must be able to execute
user transactions concurrently with the checkpointing process. The principle behind non-interfering checkpointing
mechanisms is to create a diverged computation of the system such that the checkpointing process can view a con
sistent state that could result by running to completion all of the transactions that are in progress when the check
point begins, instead of viewing a consistent state that actually occurs by suspending further transaction execution.
Figure 1 shows a diverged computation during checkpointing.

Non-interfering checkpointing mechanisms, however,
needs

to

come to

tively

completion.

short.

contract

by

However, for database systems with many long-lived transactions, checkpointing of this kind might

This work
under

may suffer from the fact that the diverged computation
the system until all of the transactions, that are in progress when the checkpoint begins,
This may not be a major concern for a database system in which all the transactions are rela

be maintained

was

not

supported in part by the Office of Naval Research under contract number N00014-86-K-0245, by the Depariment of Energy
DEFGO5-88-ER25063, and by the Federal Systems Division of IBM Corporation under University Agreement WF

number

159679.

44

Fig.
be

for the

practical

It takes

(1)

a

following

Diverged computation for checkpointing

reasons:

time to

long

1.

complete

a

non-interfering checkpoint, resulting

in

high storage

and

processing

over

head.
If

(2)

a

crash

occurs

must re-execute

before the results of a long-lived transaction are included in the checkpoint, the system
transaction from the beginning, wasting all the resources used for the initial execution of

the

the transaction.
In the rest of this paper, we briefly discuss one approach for checkpointing which efficiently generates a con
sistent database state, and its adaptation for systems with long-lived transactions. Given our space limitations, our
objective is to intuitively explain this approach and not to provide details. The details are given in separate papers

SON86b, S0N88].
2.

Non-interfering Approach
In order to make each

to

the

current

or not at

a

in

a

BCPT

of
axe

transaction must either be included in the check
divided into two groups according to their relations

a

(ACPT)

and

before-checkpoint

transactions

(BCPT).

included in the current checkpoint while those belonging to ACPT are not included.
centralized database system, it is an easy task to separate transactions for this purpose. However, it is not easy
distributed environment To separate transactions in a distributed environment, a special timestamp which is

Updates belonging
In

checkpoint consistent, updates

all. To achieve this, transactions
checkpoint after-checkpoint transactions

point completely

to

are

globally agreed upon by the participating sites is used. This special limestamp is called the Global Checkpoint
Nwnber (GCPN), and it is determined as the maximum of the Local Checkpoint Numbers (LCPN) through coordi
•

nation of all

participating sites.

An ACFF can be reclassified as a BCFf if its timestamp requires that the transaction must be executed before
the current checkpoint. This is called the conversion of transactions. The updates of a converted transaction are
included in the current checkpoint
Two types of processes

are

involved in the

checkpoint

point subordinate (CS). The checkpoint coordinator

execution:

checkpoint coordinator (CC) and check
global checkpointing process. Once a

starts and terminates the

has started, the coordinator does not issue the next checkpoint request until the first one has terminated.
At each site, the checkpoint subordinate performs local checkpointing by a request from the coordinator. We assume
that site m has a local clock LCm which is manipulated by the clock rules of amportLAM78J.

checkpoint

Execution of

Request Message

a

checkpoint progresses as follows. First., the checkpoint coordinator
a timestamp LC~. The local checkpoint number of the coordinator

with

dinator sets the Boolean variable CONVERT to false, and marks all transactions
Lamps not greater than LCPNcc as BCPT.

45

at

broadcasts

a

Checkpoint

is set to LCc~. The coor
the coordinator site with times-

4

receiving a Checkpoint Request Message, the local clock of site m is updated and LCPN
checkpoint subordinate of site m replies to the coordinator with LCPNm, and sets the Boolean
On

The
VERT

false. The coordinator broadcasts the GCPN which is determined

to

as

is

set to

LCm.

variable CON

the maximum of the local

checkpoint

numbers.
In all sites, after the LCPN is fixed, all transactions with timestamps greater than the LCPN are marked as
a temporary ACPT updates any data objects, those data objects are copied from the database
buffer
of
the transaction. When a temporary ACPT commits, updated data objects are not stored in the
co the
space
database as usual, but are maintained as committed temporary versions (CTV) of the data objects. The data manager

temporary ACFl~s. If

in each site maintains permanent and temporary versions of data objects. When a read request is made for a data
object which has committed temporary versions, the value of the latest committed temporary version is returned.
When a write request is made for a data object which has committed temporaiy versions, another committed tem

porary version is created for it rather than

overwriting

the

previous

committed temporary version.

When the GCPN is known, each checkpointing process compares the timestamps of the temporary ACPTs
with the GCPN. Transactions that satisfy the following condition become BCPTs; their updates are reflected in the
database, and are included in the current checkpoint

LCPN

<

timestamp(T)

GCPN

The remaining ternporaiy ACPTs are actual ACPTs; their updates are not included in the current checkpoint These
updates are included in the database after the current checkpointing has been completed. After the conversion of all
eligible BCPTs, the checkpointing process sets the Boolean variable CONVERT to true. Local checkpointing is exe
cuted by saving the state of data objects when there is no active BCPT and the variable CONVERT is true. After the
execution of local checkpointing, the values of the latest committed temporary versions are used to replace the
values of data objects in the database. Then, all committed temporary versions are deleted. Execution sequences of
two different types of transactions are shown in Figure 2.
an example, consider a three-site distributed database system. Assume that LC~~ = 5, LC~51 = 3, and
=8. CC sets its LCPN as 5, and broadcasts a checkpoint request message. On receiving the request message,
LCPN of each CS is set to 6 and 9, respectively. After another round of message exchange, the GCPN of the current
checkpoint will be set to 9 by the CC and will be known to each CS. If transaction T, with the timestamp 7 was ini
tiated at the site of CS1, it is treated as an ACPT. All updates by T, are maintained as CFV. However, when GCPN

As

LCcsz

is known, T1 will be converted to
3.

a

BCPT and its updates will be included in the current

Adaptive Approach for Long-lived
It

can

appropriate

be shown that

a

checkpoint.

Transactions

non-interfering checkpointing process

concurrency control mechanisms

will terminate in

SON87]. However, the

amount

Fig. 2. Execution sequences of ACPT and BCVT

46

a

finite time

by selecting an
complete one

of time necessary to

cannot be bound in advance; it depends on the execution time of the longest transaction classified as a
BCPT. Therefore the storage and processing cost of the checkpointing algorithm may become unacceptably high if a
long-lived transaction is included in the set of BCPTs. We briefly discuss the practicality of non-interfering check

checkpoint

in the next section. In addition, all

points

used for the execution of

resources

wasted if the transaction must be re-executed from the

beginning

due to

a

a

long-lived transaction would

be

system failure.

problems can be solved by using an adaptive checkpointing approach. We assume that each transaction
flag with it, which tells whether it is a normal transaction or a long-lived transaction. The threshold to
separate two types of transactions is application-dependent. In general, transactions that need hours of execution can
be considered as long-lived transactions.
These

must carry a

adaptive checkpointing procedure operates in two different modes: global mode and local mode. The glo
operation is basically the procedure sketched in the previous section. In the local mode of operation, a
mechanism is provided to save consistent states of a transaction so that the transaction can resume execution from
its most recent checkpoint.
An

bal mode of

As in the previous approach,
Request Messages. Upon receiving

the

checkpoint

coordinator

begins checkpointing by sending

out

Checkpoint

this request message, each site checks whether any long-lived transaction is
being executed at the site. If so, the site reports it to the coordinator, instead of sending its LCPN. Otherwise (i.e., no
long-lived transaction in the system), non-interfering checkpointing begins. If any site reports the existence of a

long-lived transaction, the coordinator switches to the local mode of operation, and informs each site to operate in
the local mode. The checkpoint coordinator sends Checkpoint Request Messages to each site at an appropriate time
interval to initiate the next checkpoint in the global mode. This attempt will succeed if there is no active long-lived
transaction in the system.
In the local mode of operation, each long-lived transaction is checkpointed separately from other long-lived
transactions. The coordinator of the long-lived transaction initiates the checkpoint by sending Checkpoint Request
Messages to its participants. A checkpoint at each site saves the local state of a long-lived transaction. For satisfying
the correctness requirement, a set of checkpoints, one per each participating site of a global long-lived transaction,

should reflect the consistent state of the transaction. Inconsistent set of checkpoints may result from a nonsynchronized execution ..)f associated checkpoints. For example, consider a long-lived transaction T being executed
sites P and Q, and a checkpoint taken at site P at time X, and at site Q
after X, and received at Q before Y, then the checkpoints would save the
resulting in a checkpoint representing an inconsistent state of T.
at

We

Messages

use

that

message numbers to achieve
are

consistency in
exchanged by participating transaction

a set

of local

at

time Y. If

reception of

checkpoints

a message M is sent from P
M but not the sending of M,

of

a

long-lived

transaction.

managers of a long-lived transaction contain message
transaction use monotonically increasing numbers in the tag of

number tags. Transaction managers of a long-lived
its outgoing messages, and each maintains the tag numbers of the latest message it received from other participants.
On receiving a checkpoint request, a participant compares the message number attached to the request message with
the last tag number it received from the coordinator. The participant replies OK to the coordinator and executes

local

checkpointing only

reports

to

if the request tag number is not less than the number it has maintained.
checkpoint cannot be executed with that request message.

Otherwise, it

the coordinator that the

If all replies from the participants arrive and are all OK, the coordinator decides to make all local checkpoints
permanent. Otherwise, the decision is to discard the current checkpoint, and to initiate a new checkpoint. This deci
sion is delivered to all participants. After a new permanent checkpoint is taken, any previous checkpoints will be
discarded at each site.
4. Performance Considerations

There

performance

measures that can be used in discussing the practicality of non-interfering checkworkload required. The extra storage requirement of the algorithm is simply the
C1’V file size, which is a function of the expected number of ACPTs of the site, the number of data objects
updated
by a typical transaction, and the size of the basic unit of information:
CTV file size NAx(number of updates)x(size of the data object)

pointing:

are two

extra

storage and

extra

=

where

NA

is the

expected

number of ACPT of the site.

The CTV file may become unacceptably large if NA or the number of updates becomes very large. Unfor
are determined dynamically from the characteristics of transactions submitted to the database
system,
and hence cannot be controlled. Since NA is proportional to the execution time of the longest BCPT at the site, it

tunately, they

47

unacceptably large if a long-lived transaction is being executed when a checkpoint begins at the site.
only parameter we can change in order to reduce the CTV file size is the granularity of a data object. The size
of the CTV file can be minimized if we minimize the size of the data object. By doing so, however, the overhead of
normal transaction processing (e.g., locking and unlocking, deadlock detection, etc) will be increased. Also, there is
a trade-off between the degree of concurrency and the lock ranularityR1E79}. Therefore the granularity of a data
object should be determined carefully by considering all such trade-offs, and we cannot minimize the size of the
CTV file by simply minimizing the data object granularity.
would become

The

There is

no extra

requirement

storage

in intrusive

checkpointing echanismsDAD8O, KUS82, SCH8O].

However this property is balanced by the cases in which the system must block the execution of
transactions because of the checkpointing process.

an

ACPT

or

abort

The extra workload imposed by the algorithm mainly consists of the workload for (I) determining the GCPN,
(2) committing ACPT (move data objects to the C’rv file), (3) reflecting the CTV file (move committed temporary
versions from the CTV file to the database), and (4) clearing the C1V file when the reflect operation is finished.
Among these, the workload for (2) and (3) dominates the total extra workload. As in the estimation of extra storage,
the workload for (2) and (3) is determined by the number of ACVFs and the number of updates. Therefore, as long
as the values of these variables can be maintained below a certain threshold level, non-interfering checkpointing
would not severely degrade the performance of the system. A detailed discussion of the practicality of noninterfering checkpointing is given in SON86b].
5. Site Failures

So far,

bility

we

of failures

assumed that

during

a

no

failure

during checkpointing. This assumption can be justified if the proba
extremely small. However, it is not always the case, and we now con

occurs

is

single checkpoint
algorithm resilient

sider the method to make the

to

failures.

During the global mode of operation, the checkpointing process is insensitive to failures of subordinates. If a
subordinate fails before the broadcast of a Checkpoint Request Message, it is excluded from the next checkpoint. If
a subordinate does not send its LCPN to the coordinator, it is excluded from the current checkpoint. When the site
recovers, the recovery manager of the site must determine the GCPN of the latest checkpoint. After receiving infor
mation about transactions which must be executed for recovery, the recovery manager brings the database up to date
by executing all transactions whose timestamps are not greater than the latest GCPN. Other transactions are exe
cuted after the state of the data

objects

at

the site is saved

by

the

checkpointing

process.

An atomic commit

protocol guarantees that a transaction is aborted if any participant fails before it sends a
Precommit message to the coordinator. Therefore, site failures during the execution of the algorithm cannot affect
the consistency of checkpoints because each checkpoint reflects only the updates of committed BCPTs.
In the local mode of
all

participants,

because

a

failures of

or

operation, the failure of a participant prevents the coordinator from receiving OKs from
prevents the participants from receiving the decision message from the coordinator. However,

transaction is aborted

by

an

atomic commit

protocol,

it is not necessary to make

checkpointing

robust to

participants.

The

algorithm is, however, sensitive to failures of the coordinator. In particular, if the coordinator crashes
the first phase of the global mode of operation (i.e., before the GCPN message is sent to subordinates), every
transaction becomes an ACPT, requiring too much storage for committed temporary versions.

during

One

possible solution

to

this involves the

use

of

a

number of

backup

processes; these

are

processes that

can

responsibility for completing the coordinator’s activity in the event of its failure. These backup processes
are in fact checkpointing subordinates. If the coordinator fails before it broadcasts the GCPN
message, one of the
backups takes control. A similar mechanism is used in SDD-l HAM8O) for reliable commitment of transactions.
assume

6.

Recovery

A recovery from site crashes is called a site recovery. The complexity of a site recovery varies in distributed
database systems according to the failure ituationSCH8O]. If the crashed site has no replicated data objects and if
all recovery information is available at the crashed site, local
recovery is sufficient. Global recovery is necessary
because of failures which require the global database to be restored to some earlier consistent state. For instance, if
the transaction log is partially destroyed at the crashed site, local recovery cannot be executed to completion.

When a global recovery is required, the database system has two alternatives: a fast
recovery and a complete
recovery. A fast recovery is a simple restoration of the latest global checkpoint. Since each checkpoint is

globall~

48

state of the database is assured to be consistent. However, all transactions committed during
the time interval from the latest checkpoint to the time of crash would be lost. A complete recovery is performed to
restore as many transactions that can be redone as possible. The trade-offs between the two recovery methods are

consistent, the restored

the recovery time and the number of transactions saved

by

the recovery.

Quick recovery from failures is critical for some applications of distributed database systems which require
high availability (e.g., ballistic missile defense or air traffic control). For those applications, the fate of the mission,
or even the lives of human beings, may depend on the correct values of the data and the accessibility to it. Availabil
ity of a consistent state is of primary concern for those applications, not the most up-to-date consistent state. If a

simple restoration of the latest checkpoint could bring the database to a consistent state, it may
spend time in recovery by executing a complete recovery to recover some of the transactions.

not

be worthwhile to

For the applications in which each committed transaction is so important that the most up-to-date consistent
of the database is highly desirable, or if the checkpoint intervals are large such that a lot of transactions cannot
be recovered by a fast recovery, a complete recovery is appropriate. The cost of a complete recovery is the
increased recovery time which reduces availability of the database. Searching through the transaction log is neces
state

sary for

a complete recovery. The property that each checkpoint reflects all updates of transactions with earlier
timestamps than its GCPN is useful in reducing the amount of searching, because the set of transactions whose
updates must be redone can be determined by a simple comparison of the timestamps of transactions with the GCPN
of the checkpoint. Complete recovery mechanisms based on the special timestamp of checkpoints (e.g., GCPN)
have been proposed in KUS82, SON86a].

After site recovery is completed
site checks whether it has

using either a fast recovery procedure or a complete recovery procedure, the
completed local-mode checkpointing for any long-lived transactions. If any
local-mode checkpoint is found, those transactions can be restarted from the saved checkpoints. In this case, the
coordinator of the transaction requests all participants to restart from their checkpoints if and only if they all are able
to restart from that checkpoint. The coordinator decides whether to restart the transaction from the checkpoint or
from the beginning based on responses from the participants, and sends the decision message to all participants.
Such a two-phase recovery protocol is necessary to maintain consistency of the database in case of damaged check
points at the failure site. A transaction will be restarted from the beginning if any participant is not able to restore
the checkpointed state of the transaction for any reason.

recovering

7.

Concluding

Remarks

During normal operation, checkpointing is performed to save information for recovery from failure. For better
recoverability and availability of distributed databases, checkpointing must allow construction of a globally con
sistent database state without interfering with transaction processing. Site autonomy in distributed database systems
makes checkpointing more complicated than in centralized systems.
The role of the checkpointing coordinator is simply that of getting a uniformly agreed GCPN. Apart from this
function the coordinator is not essential to the operation of the proposed algorithm. If a uniformly agreed GCPN can
be made known to individual sites, then the centralized nature of the coordinator can be eliminated. One way to
achieve this is to preassign the clock values at which checkpoints will be taken. For example, we may take check
points at clock values as a multiple of 1000. Whenever the local clock of a site crosses a multiple of this value,

checkpointing

can

begin.

If the

frequency of checkpointing is related to load conditions and not necessarily to clock values, then the
GCPN will not work as well. In this case a node will have to assume the role of the checkpointing coor
dinator to initiate the checkpoint. A unique node has to be identified as the coordinator. This may be achieved by
using solutions to the mutual exclusion roblemRIC81] and making the selection of the coordinator a critical sec
tion activity.
preassigned

The

properties of global consistency and non-interference of checkpointing results in some overhead and
processing time of transactions during checkpointing. For applications where continuous processing is
so essential that the blocking of transaction processing for checkpointing is not feasible, we believe that a noninterfering approach provides a practical solution to the problem of checkpointing and recovery in disthbuted data
reduces the

base systems.

49

Acknowledgement
The author would like to thank Dr. Won Kim and Professor Robert Cook for their valuable
ments on

the

previous

suggestions

and

com

version of this paper.

REFERENCES

CHA85] Chandy, K. M., Lamport, L., Distributed Snapshots: Determining Global States of Distributed Systems,
ACM Trans. on Computer Systems, February 1985, pp 63-75.
DAD8O] Dadam, P. and Schiageter, G., Recovery in Distributed Databases Based on Non-synchronized Local
Checkpoints, Information Processing 80, North-Holland Publishing Company, Amsterdam, 1980, pp
457-462.

F1S82}

Fischer, M. J., Griffeth, N. D. and Lynch, N. A., Global States of
Software Engineering, May 1982, pp 198-202.

a

HAM8O] Hammer, M. and Shipman, D., Reliability Mechanisms for SDD-1:
ACM Trans. on Database Systems, December 1980, pp 431-466.

KUS82I

Distributed

A

System

System,

IEEE Trans.

on

for Distributed Databases,

Kuss, H., On Totally Ordering Checkpoints in Distributed Databases, Proc. ACM SIGMOD, 1982, pp
293-302.

LAM78] Lamport, L., Time, Clocks and Ordering of Events in Distributed Systems, Commun. ACM, July 1978,
pp 558-565.

RIC81J

Ricart, G. and Agrawala, A., An Optimal Algorithm for Mutual Exclusion
mun. of ACM, Jan. 1981, pp 9-17.

{R1E79]

Ries, D., The Effect of Concurrency Control

System,

4th

Berkeley Conference

on

in

Computer Networks, Com

on The Performance of A Distributed Data Management
Distributed Data Management and Computer Networks, Aug. 1979,

pp 221-234.

SCH8O]

Schlageter, 0. and Dadam, P., Reconstruction of Consistent Global States in Distributed Databases,
Symposium on Distributed Databases, North-Holland Publishing Company, INRIA, 1980,

International
pp 191-200.

SON86a] Son, S. H. and Agrawala, A.,
6th International Conference

An

on

Algorithm for Database Reconstruction in Distributed Environments,
Computing Systems, Cambridge, Massachusetts, May 1986,

Distributed

pp 532-539.

SON86b] Son, S. H. and Agrawala, A., Practicality of Non-Interfering Checkpoints in Distributed Database Sys
tems, Proceedings of IEEE Real-Time Systems Symposium, New Orleans, Louisiana, December 1986,
pp 234-24 1.

SON87]

Son, S. H., ‘Synchronization of Replicated Data
1987, pp 191-202.

in Distributed

Systems,” Information Systems 12, 2,

June

SON88]

Son, S. H., An Adaptive Checkpointing Scheme for Distributed Databases with Mixed Types of Transac
tions, Proceedings of Fourth International Conference on Data Engineering, Los Angeles, February
1988, pp 528-535.

50

Robust

Transaction-Routing Strategies

Yann-Hang

Lee

Philip

S. Yu

in Distributed Database

Systems

Avraham LefI

IBM Thomas J. Watson Research Center
P. 0. Box 704

Yorktown Heights, NY 10598

Abstract In this paper, we examine the issue of robust transaction routing in a heterogeneous distrib
uted database environment. A class of dynamic routing strategies which use estimated response tunes
to make routing decisions has previously been proposed. Since response time estimation and decision
making depend on the assumed parameter values, it is important to examine the robustness or sensi
tivity to the accuracy of parameter values. Two refinements are proposed which improve system
performance as well as robustness of routing decisions. One is the threshold strategy and the other
is the discriminatory strategy.

1 introduction

The

locally distributed heterogeneous database environment is shown in Figure 1.1. The database
partitioned among the various processing systems, and the incoming transactions are routed to one
of the processing systems by a common front-end system. If a transaction issues a database request
which references a non-local database partition, the request., referred to as a remote database call,
must be shipped to the system owning the referenced partition for processing. With regard to the
destinations of database requests issued by a transaction, we can often identify one system as the
preferred system to which the transaction sends most of its requests. In studying the performance
of a transaction-processing system, tbe reference-locality distribution, i.e. percentage of database
calls issued by a transaction to each database partition, has to be considered. As there is an additional
overhead associated with remote database calls, routing an incoming transaction to the system with
the lightest load may provide worse performance than routing the transaction to its “preferred” sys
tem. Thus transaction-routing strategies need to strike a balance between sharing the load among
systems and reducing the number of remote calls.
Dynamic routing strategies for this environment have been studied in Yu88]. In previous
studies on dynamic load-balancing approaches, such as in Wang85, EagrS5], it is assumed that in
coming tasks can be completely serviced at any processing system. Under the heterogeneous data
base model, of course, these assumptions are invalid. A class of dynamic strategies based on an
attempt to minimize each incoming transaction’s response time, referred to as the MRT strategy, was
proposed and studied in Yu88]. It uses readily available information at the front-end system such
as previous routing decisions of transactions currently in the complex. It has been demonstrated that
system performance can be greatly improved if this concept of minimizing the response time of in
coming transactions is used. In Ferr86, ZhouS7], a response-time-oriented load index based on
mean-value equation is proposed which is a linear combination of queue lengths. The experiments
done in Zhou87] take samples of queue length and use smoothed queue length to calculate a load
index for determining the placement of UNIX commands.
An issue of great significance from a practical view point is how critically the quality of the
routing decision depends upon the accuracy of the assumed parameter values. The behavior of each
transaction can vary from the assumption and even the average behavior may change from time to
time. Thus a practical scheme has to be robust to the variation in parameter values.
In this paper, we examine two refinements to the MRT strategy to improve its robustness. One
is a strategy that imposes a threshold criterion on the load condition before non-preferred-system
routing, based on MRT, is considered; the other applies a policy which discriminates long transactions
in applying non-preferred-system routing. A common idea underlies these schemes is that we seek
to reduce the risks of making a non-preferred-system routing decision by being more selective about
either the precondition or the candidate for non-preferred-system routing. Because the number of
remote calls is reduced, the communications-bandwidth requirement is also reduced. These strategies
are shown to be more robust than the original MRT with respect to parameter accuracy.
is

51

In the

next

section, the MRT strategy is briefly described.

in Section 3,

we

introduce the

threshold strategy. Its performance is evaluated with simulations and compared with the MIRT
strategy. Another refinement, the discriminatory strategy, is introduced and examined in Section 4.
We summarize the results in Section 5.

2.

Response-Time-Based Dynamic Routing Strategy

examine the MRT strategy proposed in Yu88]. The strategy first estimates the average
length or utilization of each processing system F;. Then, the expected response times of an
1,...,N, are estimated. The
incoming transaction, if it were routed to the processing system F;, for i
processing system which provides the minimum expected response time is chosen to execute the

We

now

queue

=

transaction.

Under the MRT strategy, the

of active transactions

routing decisions

are

front-end system in a routing-history table. Each time a transaction tXk is routed to
k-th row and the i-th column of the routing-history table is incremented by one
arrival and its
row

routing. Furthermore,

when

a

transaction is

and column of the table is decremented

by

to

one

overhead to maintain the table in the front-end system, and that
state information from the processing systems is required.

negligible
neous

no

by

the

the entry in the
to reflect the new

F1,

in the

completed, the entry
departure.

reflect the

maintained

corresponding

Note that there is

sampling

a

of instanta

The expected response time of an incoming transaction depends upon the transient behavior
of the system and the future arrivals. For efficient implementation at the front-end, a steady-state
analysis is applied to estimate the mean response time using transaction characteristics and mean

processing system. There are different approaches to estimate the mean queue
length/response time as studied in Yu881. In this paper, we focus on the MRT strategy based upon
the residence-time calculation. This approach has shown to provide the best performance over other
approaches considered in Yu881. It regards the numbers of active transactions indicated by the
routing-history table as fixed populations in a closed-queueing network. Naturally, it is possible to
calculate the exact queue lengths based on a mean-value algorithm. However, this approach is im
practical when we consider the complexity of the mean-value analysis. The MRT strategy uses an
approximation based on Bard-Schweitzer’s algorithm Schwe79, Bard8O] to calculate the residence
time of each transaction at each processing system. Then, the queue length of each processing system
is computed.
queue

length

3. Threshold

Threshold

at

each

Strategy

approach has been considered by Eager,

et.

al., for load sharing in distributed systems with

policy to determine the destination
applied to the MRT in the envi
ronment studied. The new approach not only shortens the response time but, most significantly, re
duces the sensitivity to accuracy in the assumed reference-locality distributions.
Under the threshold strategy, a more conservative approach is taken that recognizes the pre
ferred system as the default system for the routing decision. A non-preferred-system routing is con
sidered only when the preferred system is comparatively overloaded.
However, when
the
MRT strategy.
non-preferred-system routing is considered, the routing decision is again based on
sizable
when
That is to say that a transaction is routed to a non-preferred system only
a
gain on re
sponse time can be achieved. Marginal response-time gain may not provide sufficient reward to make
a non-preferred-system routing desirable. We in fact avoid individual optimization in pursuing global
optimization. More precisely, our threshold strategy tests whether the ratio of the estimated response
times between that of the preferred system and that of the system with minimum estimated response
time is within a given threshold. If it is, preferred routing is taken. Otherwise, tbe MRT algorithm is
used to make the routing decision.
In order to study performance of the routing strategies, a simulation is developed Lee88J
which is an extension of the one reported in Yu88]. We consider an environment consisting of three
transaction-processing systems with three transaction classes. Based on data from some IBM EMS
systems Corn86, Yu871, the average number of database requests per transaction is set to 15 for all
identical nodes
of

Eage86, Eage85].

transferring jobs.

Threshold is used in

We demonstrate that

a

threshold

52

a

location

can

also be

transaction classes.

specified.

erwise

Geometric distribution is assumed for the number of database calls unless oth

The

reference-locality distribution

Database Partition

addition,

given

1

in Table 3.1.

2

3

Transaction class 1

0.75

0.11

0.14

Transaction class 2

0.07

0.82

0.11

Transaction class 3

0.11

0.06

0.83

Table 3.1 The reference

In

is

we assume

that the processor

locality

speed

distiibution used in simulations

is 7~5 MIPS and the

pathlengths

of each database call

application processing between database calls are 9K and 21K instructions, respectively. The
additional communication overhead of serving a remote database call, c, is chosen to be 3K and 15K
to represent low and high communications overheads. The 10 access time and the probability of
having an 10 during a database call are assumed to be 40 ms and 0.7 for all transaction classes, re
spectively. The arrival rates are adjusted so that the processing load of database calls on each system
is balanced and the total processing load is as indicated.
To study the robustness of system performance, we would consider two additional measures
besides response time. One is the percentage of transactions routed to non-preferred systems. This
is referred to as NPR, the non-preferred-system routing ratio. Although non-preferred-system rout
ing improves the balance of loads among the processors, it has side effects because it incurs remote
calls. This puts more communications overhead on the processors and higher communicationsbandwidth requirement on the links. It can also create a vicious cycle in that after transaction tx1,
with a preferred system F1, is routed to F2, the change in load conditions forces the arrival of a next
transaction lx,, with a preferred system P2. to be routed to P1. This vicious cycle is referred to as a
swapping phenomenon. Thus, we are concerned with a sequence of ‘locally’ optimal decisions (in
terms of estimated response time) that is sub-optimal ‘globally’-- i.e. over the entire period of proc
essing— in that the preferred-system routing would have been the correct decision. The swapping
ratio, SWl~, defined as the percentage of non-preferred routings in which we observe a swapping
phenomenon, is the other measure considered. This measure attempts to provide an indication as to
whether a routing strategy makes too many sub-optimal non-preferred routings.
Let S represent the average processor utilization in the entire system excluding
0.71 and
communications-processing overhead. Simulation results are shown in Table 3.2, for S
The
MRT
and
WR
where
the
overall
S
are presented.
0.81,
original
strategy is
response time, NPR,
1.
smaller
For
of
threshold
the
with
to
threshold
a
1.15, slightly
case
equal to
equivalent
response
and the

—

times

are

obtained in all

cases

examined than those for the MRT. Also

we

obtain

more

improvement

case
high communications overhead under high load. However, the improvement in aver
time
is still less than 10%. To a lesser degree, this trend can be observed when the
age response
threshold is 1.2. The reduction on NPR and SWR are very significant in all cases.

for the

of

We also consider the

balance the load

by routing

case

to

with

a

threshold of 1.3

in

Table 3.2.

non-preferred systems (the strength

By ignoring opportunities

of the MRT)

we can worsen

to

the

response time significantly. In the extreme, the response time can reach 0.801 and 1.175 if we always
0.81 and
route transactions to their preferred systems (i.e. a bigger threshold is used), for S
=

c

=

is

3K and 15K,

respectively.

This demonstrates that the MRT

policy

of

emphasizing

load

balancing

important.
in

analyzing

the results of the threshold strategy,

we

make the

following observation.

As

NPR is very low compared to that of the MRT, we expect the system to be less balanced. This is
verified by comparing differences between utilizations of the processing systems through simulation.
time of the threshold strategy is as good as or better than that of the M~RT.
further study we examine the probability distribution of system unbalance, defined as
a
max{11,12,/31 min~I1,1,,I31 where I~ is the instantaneous queue length of P1 and is sampled during

However, the response
in

-

,

The distributions under the MRT strategy and the threshold strategy with a threshold
plotted in Figure 3.1. We note that the mean unbalance of the threshold strategy is higher

simulations.
of 1.2

are

than that of the MRT. Also, there is

a

difference in the tail of the distributions. That is, the threshold

53

strategy allows the system to become more unbalanced than the MRT does. Since there is a window
of uncertainty as to how unbalanced the system will remain over the course of a transaction, allowing
a

small unbalance-- coupled with the reduction in utilization due
better response time under the threshold approach.

to a

remote-call overhead--

leads

to

S—0.81

s~0.71
threshold

1.(MRT)

115

RT

0.618

0.616

0.616

NFR

46.6%

13.6%

7.0%

SWR

78.2%

43.3%

8.7%

1.(MRT)

1.15

1.2

0.627

0.700

0.694

0.701

0.711

3.6%

46.3%

17.7%

10.2%

6.3%

3.3%

85.0%

49.0%

15.9%

6.6%

1.3

1.2

1.3

c’=3K

c=15K
RT

0.707

0.705

0.701

0.716

0.986

0.933

0.916

0.941

NPR

19.4%

8.2%

4.2%

2.1%

17.9%

11.0%

6.996

4.4%

SWR

37.6%

17.5%

4.1%

1.0%

43.4%

24.3%

8.4%

2.7%

Table 3.2 l’he

Consider the
queue
cases

performance comparisons

cases

in which the

of the threshold and the MRT

router uses

inaccurate

strategies

locality distributions

to

estimate the

and response time when making routing decisions. Simulations were conducted for
where an inaccurate locality distribution qkl’] is assumed in the router for making routing deci

length

sions. Transactions issue database calls
ecution.

In these

simulations, q~1] is

according

set to the

to

the actual reference distribution

distribution defined in Table 3.1.

q~) dwing

ex

The estimated

~ is set according to the following: given a percentage of inaccuracy x, if
xq~h/(1 ~&*)) (i.e. locality of
(1 + x)q~~ and for all I,’ k, ‘lk~
x)q~~ < 1, then q~’
the preferred system increases with a ratio x, whereas localities of non-preferred systems decrease
I and elk,
0 for all I ,~ k. The response times and the swapping
proportionally); otherwise q&*’
ratios are plotted in Figure 3.2. It shows that, when weaker localities are assumed in making routing
0.3. The performance of the
decisions, swapping ratio can reach 80% under the MRT as x
MRT degrades substantially in this case. However, when a threshold of 1.2 is applied, variations in
response times are quite small when x is in the range of -0.3 to 0.2. It is clear that the threshold ap
proach is much more robust than the MRT when less accurate reference-locality distribution is as

distribution,

(1

+

—

—

—

—

—

—

sumed.

4.

Discriminatory Sirategy

The MRT strategy implicitly assumes, over the course of
main the same as at the time when we make the decision.

a

transaction, that the system load will

re

Actually the system load changes over the
lifetime of a transaction because of departures and arrivals. The longer the transaction, the larger the
load deviation is likely to be. Hence, a decision made at transaction arrival may not be optimal during
its entire execution period. With uncertainty about the future, making a decision that involves only
a few remote calls is less riskier than one involving a large number of remote calls. When the load is
unbalanced, the router should try to improve the situation through many small corrections instead of
one large correction.
Based on the above observation, a refinement of the MRT is to distinguish between short and
long transactions when making routing decisions. We call this the discriminatory strategy. For ex
ample, we can apply a larger threshold to longer transactions in the threshold approach. In the fol
lowing, we consider an ideal situation. Each transaction class consists of two subclasses: one long
and one short which have 48 and 8 database calls, respectively, as in a Bernoulli distribution. Note
that a Bernoulli distribution consisting of 87.5% short transactions with 8 database calls and 12.5%
long transactions with 48 database calls has a mean of 15 database calls just as the geometric dis
tribution considered before. Assume we can distinguish between the long and short transactions from
the input parameters associated with each transaction. We discriminate against a long transaction

54

by automatically
make

a

it

route

transactions arrive

--

i.e.

to
we

its preferred system. We only adopt the MRT strategy when short
to minimize the response times in situations where, even if we

attempt

mistake, the exposure is low.
for long transactions, the discriminatory strategy gives up
balance the system. On the other hand, the discriminatory
non-preferred-system routing decisions for long transactions which can result

By using preferred-system routing
some

opportunities

that MRT

strategy avoids making

can use to

In terms of response time we thus have a trade-off:
in a large number of remote calls.
the
discriminatory strategy increases system unbalance, but decreases total processor utilization. In Ta
ble 4.1, we examine the case for both high and low communications overheads. We find that, over
all transaction-length categories, and over all cases, the discriminatory algorithm has a lower response
time than that of the MRT. The reductions can reach 8% for short transactions and I l% for long
The larger the
transactions when the processing load and communication overhead are high.
transaction-processing load and the higher the communication overhead, the larger the exposure is
for non-preferred routing: the improvement in response time is thus more apparent.

S=0.71
MRT

S=0.81

I Discriminatory

MRT

I Discriminai~ory

c=3K

RT(shorl)

0.334

0.328

0.384

0.372

RT(long)

1.963

1.917

2.244

2.152

RT(overall)

0.619

0.606

0.709

0.684

NPR

46.6%

40.9%

46.3%

40.9%

RT(short)

0.384

0.373

0.529

0.486

RT(long)

2.249

2.152

3.062

2.741

RT(overall)

0.711

0.684

0.972

0.885

NPR

19.5%

19.5%

18.2%

19.5%

c~15K

Table 4.1 The

performances

of the

discriminatory

and the MRT

strategies

In addition to

improved performance, the discriminatory approach shows more robustness than
locality distribution is used. With the same parameters as in Figure 3.2,
and swapping ratios of the discriminatory approach and of the MRT in
times
the
we plot
response
the
number
of database calls is a Bornoulli distribution as described before.
that
Figure 4.1, given
The discriminatory approach shows little sensitivity to inaccuracy in the assumed locality. The smaller
swapping ratio of the discriminatory approach, compared with that of the MRT, indicates that the risk
of non-preferred routing is reduced by selecting good candidates for non-preferred routing, i.e., short
the MRT when inaccurate

transactions.

5. Conclusion

Our study shows that although the MRT strategy leads to pretty good performance, it is sensitive to
the accuracy of the assumed reference-locality distribution. We have proposed a threshold approach
which follows the MRTs

routing decision only

when the estimated load unbalancing is above a given
preferred-system routing. There are two significant results from
the threshold approach. The first is the ability to improve transaction response time even with a dra
matic reduction in non-preferred system routing. The other one is, more importantly, the robustness
that the threshold approach provides when the assumed reference-locality distribution is inaccurate.
Because the risk of doing non-preferred-system routing may be greater for long transactions than for
short transactions, we suggested a discriminatory approach in which the MRT strategy is applied only

threshold, and which otherwise

uses

55

short transactions, and the preferred system for long transactions. Interestingly, both the short and
long transactions show improvements in their response tunes. Furthermore, the robustness to inac
curacy in locality distribution is improved.
to

References

Bard, Y., “A Model of Shared DASD and Multipathing,” Comm. of the ACM, Vol. 23,
1980), pp. 564-572.
Cornell, D.W., Dias, D.M., and Yu, P.S., “Analysis of Multi-system Function Request
Shipping”, IEEE Tran. on Software Eng., Vol. SE-12, No. 10, Oct. 1986, pp. 1006-1017.
Eager, D L., Lazowska, E.D. and Zahorjan, J., “A Comparison of Receiver-Jnitiated and
Sender-Initiated Adaptive Load Sharing”, Performance Ewzluation Review, Vol. 13, No.

IBard8OI

No. 10, (Oct.

ICorn86J

IEage85I

(Aug. 1984), pp. 1-3.
Eager, D L., Lazowska,

2

lEage86I

DistribuU~d

Systems”,

Zahorjan, J., “Adaptive Load Sharing in Homogenous
Soft. Eng., Vol. SE-12, No. 5, May 1986, pp. 662-675.
Indices for Load Balancing Schemes,” Proc. of FJCC, Nov.

E.D. and

IEEE

on

tFerr86I

Ferrari, D., “A study of Load

LLee88I

Lee, Y.H., Yu, P.S., and Leff, A., “Robust Transaction Routing in Distributed Database

1986.
Proc. Intl. Symposium on Databases in Parallel and Distributed Systems Dec.
1988, pp. 210-219.
ISchwe79] Schweitz.~r, P., “Approximate Analysis of Multiclass Queueing Networks of Queues”, ml.
Conf on Stochastic Control and Optimization North Holland, Amsterdam, 1979.
IWang85J Wang, Y.-T., and Morris, R.J.T., “Load Sharing in Distributed Systems”, IEEE Trans.

Systems”.

Computers, Vol. C-34, No. 3, (March 1985), pp. 204-217.
Yu, P.S., Dias, D.M., Robinson, J. T., Iyer, B. R., and Cornell, D. W., “On Coupling
Multi-systems Through Data Sharing,” IEEE Proceeding, Vol. 75, No. 5, May 1987, pp.
on

tYu87I

573-587.

Yu, P.S.. Balsamo, S., and Lee, Y.-H.,

IYuS8l

Database

Systems,”

IEEE Trans.

on

“Dynamic

Soft. Eng.,

Transaction

Routing in
Sept.

Vol. SE-14, No. 9,

Distributed

1988, pp.

1307-13 18.

Zhou, S.

IZhou87]

of the

,

7th

and Ferrari,

Conference

D., “A Measurement Study of Load Balancing Performance,” Proc.
Distributed Computing Systems Sep. 1987, pp.490-497.

on

0
N

a

.0
0

.0
0
0.
V
‘I

C

V

U
U

0

meosure

of system unbolonce

Partitioned Databases
DB1

DBN

Figure 31 Occurrence probabilities

Figure

1.1 The

configuration

of

a

distributcd transaction

processing system

56

of

sampled

system unbalance

G1

SI
C

0
•1

LI

SI

E

0

SI
C

~0

0
0.

-I

0

I,

SI

0

I

C

a
0

0
U

a

a

LI

C
a
‘I
(S

C
0
SI

Change

of occurocy

Figure 3.2 Perfermances of

on

localities (~)

the MRT and threshold

with inaccurate

strategies
locality distribution

I,

SI
C

0
I,
(I

V

*

E

a
•0

•0•

SI
I,

‘a

C

0
a

0

I,

V

0

I

C

a.
0

0
LI
0

a.

LI

C
0
L

LI

C
0
II

Change

Figure

4.1

of accuracy

Performances of the MRT and

locality distribution (no.

on

localities (x)

discriminatory strategies

with inaccurate

of database calls is with liornoutfi

57

distribution).

SHARING THE LOAD OF LOGIC-PROGRAM EVALUATION
Ouri Wolfson

Computer

Science

Dept., The Technion,

Haifa

32000, Israel

ABSTRACT
We propose

a method of parallelizing bottom-up-evaluation of logic programs. The method does not introduce
interprocess communication, or synchronization overhead. We demonstrate that it can be applied when evaluating
several classes of logic programs, e.g., the class of linear single rule programs. This extends the work reported in
WS] by significantly expanding the classes of logic programs that can be evaluated in parallel. We also prove that
there are classes of programs to which the parallelization method cannot be applied.

1. INTRODUCTION

The efficient

bottom-up-evaluation of intentional database relations, defmed by means of recursive logic pro
recently emerged as a very active area of research UI, BR], K]). Two main methods of improving
performance have received most of the attention. One is selection propagation, and the other is parallel evaluation.
grams, has

Selection propagation reduces the number of relevant input-database tuples, by using constants passed as
parameters to the database query processor. This usually necessitates a rewriting of the logic program which defines
the intentional relation. The best known rewriting algorithm for this purpose is “magic sets” (see BMSUI).
Parallel evaluation uses multiple cooperating processors, to reduce the overall evaluation time form start to
fmish. Most efforts in this area have been devoted to characterization of the logic programs which belong to the NC
complexity class UV], K], API). If a program is in NC, it means that its intentional relations can be evaluated

fast, given a polynomial (in the number of input-database tuples) number of processors; they have to commum
extensively, usually through common memory. Unfortunately, this research means very little as far as utilizing
a constant number of processors, particularly if they do not share common
memory (e.g. a hypercube multiprocessor
system’ having 1024 nodes).
very
cate

In this paper we assume an environment with a constant number of processors, which either communicate
by
message passing, or have common memory. The method that we propose is to create rewritten versions of the origi
nal logic program (a la selection propagation), and assign to each processor a different version. Each
processor exe

its version on a local copy of the input database, without communicating with the other processors. At the end,
the union of outputs comprises the output of the original program (completeness). Therefore, if these outputs are
sent to the same device or stored in the same file, the result is equivalent to a single-processor evaluation. Based on
the paradigm of less-generated-tuples-implies-less-work, that lies at the heart of all selection propagation methods,
cutes

and

to

so.

The next

which

we

also subscribe, each processor completes its evaluation before
our method.

1: Consider the

Example

a

single

processor would have done

example demonstrates

DATALOG

following

(see MW])

program called in

MI’S] the canonical strongly linear

(Cs!):

S(x,y):— UP(x,w),S(w,z),DOWN(z,y)
S(x,y):— FLAT(x,y)
Assume that the extensional-database relations UP, FLAT, and DOWN represent a directed graph with three types
of arcs. The csl program defines a tuple(a,b) tobe inS, if and only if there is apath from a to b having k UP
ares, one FLAT arc, and k DOWN arcs, for some integer k.

Given processors

(0,...,r-1 )

we

x mod
gram, with the predicate I
putes the tuples (a,b) for which the
=

that for

a

large

random

graph,

each

r

propose that they share the load
added to the second rule of the

follows. Processor i executes the csl pro
In other words, processor i com
(c,d), with i c mod r3. It is intuitively clear

path

one

goes through a FLAT arc
of the processors generates less

as

program2.

=

tuples.

To demonstrate the time saving for a specific input to the csl program, consider the extensional database rela
tions of Figure 1. UP consists of the tuples (i,i+1) for i
1,...,4, FLAT consists of the tuples (i,6) for i
l,...,5 and
DOWN consists of the tuples (i, i÷1) for i 6,...,9.
=

=

=

1.
2.

see

H]

i=(x+y)modr

worksaswell

3. This works for character-strings

as

well, since the binaiy representation

58

can

be

regarded as

a

natural number

7

8

~1~
9

10

Figure
The

set

1:

Sample input

NEW, defined below, consists of the tupics of S which
NEW

=

{(4,7),

to

the csl program.

are

(3,7),
(3,8),

not in

FLAT.

(2,7),
(2,8),
(2,9),

(1,7),
(1,8),,
(1,9),

(1,10))

Assume that S is computed by the naive evaluation method (see B]). It assigns FLAT to S and then iteratively adds
to S the tuples in the (projection of) UP join S join DOWN. Then in the first iteration, a single processor evaluat

ing

csl

performs

the

join

of

a4-tuples relation (UP), with

In the second iteration the relations

been added to

UP, S. DOWN

S); third iteration 4,12,4 (second

row

a

5-tuples

relation

(S), with

a

4-tuples relation (DOWN).

of sizes 4,9,4, respectively (first row of the set NEW has
has been added); fourth iteration 4,14,4; fifth and last iteration
are

4,15,4.
However, if two processors share the load by having processor i execute the csl program with i
x mod 2
added to the nonrecursive rule, then the arcs (1,6),
(3,6), (5,6) will be assigned to processor 1, and the rest to
processor 0. The maximal computation burden is placed on processor 1, performing five iterations with relations of
sizes 4,3,4,
4,5,4,
4,7,4,
4,8,4,
4,9,4. Due to the smaller S-relation at each iteration, a significant
=

time
sor

saving compared

1, and completes

to the

even

single processor case occurs: Processor 0 has a lower computation burden then proces
faster. If there are five processors instead of two a greater time saving results. In this case

the maximum burden is

4,3,4,

4,4,4,

placed on processor 0, performing five iterations, with relations of sizes 4,1,4,
4,5,4,
respectively.

Similar observations

can

be made if the evaluation is semi-naive

B])

rather than naive.

4,2,4,

13

Let us emphasize that our method removes what for many problem domain~ constitutes the main obstacle to
efficient parallelization, namely synchronization. In a multiprocessor system, often there is economic justification
for increasing the number of processors, as long as performance can be increased. However, the problem is that as
the number of processors grows, the required synchronization may slow processing to the single-processor level,
and

even below that. Therefore, even though there are other ways and operations performed in the
process of
evaluating logic programs that can be parallelized by using multiple processors, our method prevents the synchroni
zation that is usually involved in parallelization. For example, assume that multiple processors are used to parallel
ize the join operation, instead of using them as proposed above. Then at each iteration of the naive or semi-naive
evaluation, each processor would have to exchange its newly generated tuples with the newly generated tuples of
every other processor. This procedure involves a lot of message passing or synchronization in accessing common

memory.
The purpose of this paper is to determine to which programs the load sharing method described above can be
applied. Specifically, we formally define what it means for a program to have a load-sharing scheme, and explore
which programs do have such schemes, and which ones do not, i.e., are not amenable to parallel evaluation by the
method described. We determine that almost all linear programs (each rule has at most one intentional predicate in
the body) have such a scheme. In the class of single rule programs (sirups), defined in CK], the
pivoting ones (see
WS]) do have a load sharing scheme; so does a certain subclass of the simple chain programs. We define a class of
sirups for which we prove that a load sharing scheme cannot exist. Several famous sirups belong to this class (e.g.
path systems, introduced in C]).

Presently, the only other method that we are aware of for speeding up bottom-up evaluation of logic-programs
by using a constant number of processors, is the one introduced in WS]. It resembles the one we proposed above,
except for an important difference. The method is applied only when it can be guaranteed that each new tuple gen
erated in the evaluation process is

computed by

a

unique

processor. The purpose is to

59

partition (rather

than share,

as

in

method) the evaluation load. But consequently, the WS] method is applicable only to a very restricted class
logic programs, called decomposable. For example, in the class of simple chain programs UV], API) only to
the regular ones are decomposable. Therefore, the csl program of example 1 is not decomposable. Intuitively, the
reason for this is that since there may be more than one path between a and b, it is not guaranteed that each tuple is
computed by a unique processor. For instance, in the sample input of example 1, if, in addition to the listed tuples,
the tuple (2,9) is also in FLAT, then the tuple S(2,9) is computed by both pmcessors 0 and 1.
our

of

One last comment regarding comparison with relevant literature concerns the parallel versions of PROLOG
(e.g. G], Si). There has been a lot of research on the subject, but because of the fundamental difference between
bottom-up (or forward chaining) and top-down (or backward chaining) evaluation of logic programs, this research is
not applicable to our problem. Specifically, parallelization methods for PROLOG, which takes the top-down
approach are not applicable in database query processing, which employs bottom-up. The reason for bottom-up is
mainly because database applications are looking for all answers to a query.
The rest of this paper is organized as follows. In section 2, we provide the preliminaries, and in section 3 we
define the concepts of a load sharing scheme, and its potential speedup, and prove initial results about them. In sec
tion 4 we determine that every program in a large subclass of all linear programs has a load sharing scheme, and in
section 5 we prove that a whole class of sirups cannot have a load sharing scheme. In section 6 we discuss future
work.
2. PRELIMINARIES
An atom is

a predicate symbol with a constant or a variable in each argument position. We assume that the
the natural numbers. An R -atom is an atom having R as the predicate symbol. A rule consists of an
atom, Q, designated as the head, and a conjunction of one or more atoms, denoted Q’
~ k, designated as the
and Qk•~~ Arule or
Qk, which should be read “Q ifQ1 and Q2, and
body. Such a rule is denoted Q:— Q’
an atom is an entity. If an entity has a constant in each argument position, then it is a ground entity. For a predicate
R, a finite set of R -ground-atoms is a relation for R.

constants are

into

A DATALOG program, or a program for short, is a finite set of rules whose predicate symbols are divided
disjoint subsets: the extensional predicates, and the intentional predicates. The extensional predicates are

two

distinguished by the fact that they do not appear in any head of a rule. An input to P is a relation for each exten
sional predicate. An output of P is a relation for each intentional predicate of P. A substitution applied to an
entity, or a sequence of entities, is the replacement of each variable in the entity by a variable or a constant. It is
denoted x lIy 1 ,x 2/y 2
xn lyn indicating that xi is replaced by yi. A substitution is ground if the replacement of
each variable is by a constant. A ground substitution applied to a rule is an instantiation of the rule. When we talk
about an instantiation we refer either to the ground rule, or to the substitution; which reference, will be clear from
the

context.

A database for p is a relation for each predicate of P. The output of P given an input!, is the set of relations
for the intentional predicates in the database, obtained by the following procedure, called bottom up evaluation.

BUE1. Start with

an initial database consisting of the relations off.
BUE2. If there is an instantiation of a rule of P such that all the ground atoms in the body are
in the database generated so far, and the one in the head is not, then:
add to the database the ground atom in the head of the instantiated rule, and reexecute BUE2.
BUE3. Stop.

This

procedure is guaranteed to terminate, and produce a finite output for any given P
unique, in the sense that any order in which bottom up evaluation adds the atoms to
same

and!

VEK]). The output is
produce the

the database will

output.

ground atom a be in the output for P, and let s be a minimal sequence of iterations of BUE2 for deriving
corresponds a derivation tree for a; it is a rooted tree with ground atoms as nodes, and a as the root. Node
b haschildrenb1
bk isaninstantiationins.
bk,ifandonlyifb :—b1
Let

a.

To

s

For
assume

the rule;

rule,

some

a

variable which appears in the the head, is called a distinguished variable. For simplicity we
a program is range restricted. i.e. every distinguished variable also appears in the body of

that each rule of

additionally,

An evaluable

r

can

that

predicate is

greater than, modulo,

ables, and

we assume

etc.

by

of the rules of

a

program has constants.

arithmetic predicate (see BR]). Examples of evaluable predicates are sum,
is a restricted version of some rule r, if r and re have exactly the same vari
omitting zero or more evaluable predicates from the body of re. In other words, re

A rule

be obtained

none

an
re

is r with some evaluable predicates added to the body, and the arguments of these evaluable
ofr. For example, ifr is:
S(x,y,z):—S(w,x,y),A(w,z)
then one possible re rule is:
S(x,y,z):— S(w ,x,y), A(w,z), x—y=5
A program

P~ is

a

restricted version of program P if each

one

60

of its rules is

a

predicates

restricted version of

are

some

variables

rule of P.

Note that P~ may have

more

than

one

theruler, thenP1 mayhavetherulere

restricted version of a rule
aswell as therulere’:

S (x ,y

,z

r

of?. To continue the above

example, if P has

):— S (w ,r ,y), A (w ,z), x —y =6

this paper, only resiricted versions of a program may have evaluable predicates. The
input of a program
with evaluable predicates, i.e. a restricted version, is defined as before. The output is also defined as before,
except
that BIJE2 also verifies that the substitution satisfies4 the evaluable predicates in the ground rule;
only then the atom
in the head is added to the database and BUE2 is reexecutcd. In other words, in
considering instantiations for a res

l’hroughout

tricted version of

a rule, bottom up evaluation disregards database atoms which do not
satisfy the additional evalu
Observe that if P’ is a restricted version of P. then for every input, and for
every intentional predi
cateR of P. the relation for R output by P’ is a (possibly improper) subset of the relation for R
output by P.

able

predicates.

predicate Q in a program P directly derives a predicate R if it occurs in the body of a rule whose head is a
Q is recursive if (Q ,Q) is in the nonreflexive transitive closure of the “directly derives” relation. Predi
cate Q derives predicate R if (Q ,R) is in the reflexive transitive closure of the
“directly derives” relation (particu
larly, every predicate derives itsell). A program is recursive if it has a recursive predicate. A rule is recursive if the
predicate in its head transitively derives some predicate in its body.
A

R -atom.

3. LOAD SHARING SCHEMES
In this section

we define and discuss the concept of a load sharing scheme, and establish that it is weaker than
Then the notion of the potential speedup of a load sharing scheme is defined, and we establish the
speedup of a class of sirups called pivoting.

decomposability.
potential

Assume that P is

P,. are restricted copies of P. for r >1. Given and input, for an inten
program, and P1
denote by R the relation output by P~ for R; the relation output by P is denoted R.
Observe that this is a somewhat unconventional notation, since for P. the relation name is different than the
predi
cate name. The set D = (P1
P,) is a load sharing scheme for evaluating some predicate T in P. if the follow
ing two conditions hold:
tional

predicate

1.

For each

2.

There is

R of

input
an

a

P,

we

Ito P

input

,P1,...,P, UT,~T (completeness).

such that for

some

intentional predicate Q which derives T, each

I

IQ~

IQ

<

I

(nontrivial

ity).
In order to

intuitively explain

the above definition, we assume that each processor has a restricted
copy of the pro
set of input base relations, is replicated at each one of r
processors. Alter
the database may reside in memory common to all the processors.

gram P. and the whole

natively,

database, i.e. the

The completeness requirement in the defmition is that no T-atoms are lost by evaluating the relation for T in
each P,. rather than the relation for T in P. Although the requirement is for inclusion in one direction
only, the fact
that UT, does not contain any atoms which are not in T is implied by the fact that each P~ is a restricted version of
P. Thus,

by using multiple processors

and

taking

the union of the T, ‘s, the

exact

relation for T is obtained.

The nontriviality requirement refers to some predicate Q which derives T, i.e. has to be evaluated to deter
mine the output relation for T. Nontriviality says that for some input,
say I, the output of each P for Q is smaller
than the output of? for Q. If, along the lines suggested in BR, Section 4], the load of
an intentional

evaluating

relation is measured in terms of the number of new tuples generated in the process, then the evaluation
by the load
sharing scheme completes sooner for the input I. Any doubt an implementor may have, concerning the load sharing
scheme

performing worse for some input than a single
or so) perform the nonrestricted version of P.

processor,

can

be removed

by having

one

processor

(of the

thousand

In WS] a decomposable predicate is defined. Decomposability with
respect to restricted copies P1
P~ is
similar to the set being a load sharing scheme, with two exceptions. First,
decomposability imposes an additional
restriction, called lack-of-duplication. It requires that for each input I to P ,P1 ,...,P,, and for each i j, the relations
Q, and Q1 are disjoint for any intentional predicate Q which derives T in P. Second, for decomposability, nontrivial
ity requires that for some arbitrary input each T, is nonempty.

Proposition 1: Ifapredicate T inaprogramP is decomposable with respect toD
(P1
sharing scheme for evaluating T in P.
Proof: Nontriviality and Jack-of-duplication imply that there is an input for which each I T1

Pr),

=

Next
Denote

we

<

IT 1.

1

defme the notion of

by Q’

4. for

I

thenD isa load

example, the

Q’

the set

substitution

potential speedup. Let P be a program, and T an intentional predicate in P.
of intentional predicates which derive T. The output of P for T given I, denoted

xIl4,y/8

satisfies the evaluable

predicate x-y=6,

61

whereas the substitution

xJl3,y19

does

not.

o (P ,T,!), is

uQ’.

In other

words, the output contains T-ground-atoms and ground-atoms of other intentional

predicates which derive T. Given a load-sharing scheme D
(P1
Pr) for evaluating T in P. the potential
speedup of D, denoted Ps (D), is the maximal number M for which the following condition is satisfied. For
every
and
n
integer
there
is
an
c,
for
every
which
input I
tO (P ,T/) I
>
and
n,
10 (P ,T,f) I/max 10 (P1 ,TJ) I
M—c. Intuitively, the potential speedup is the number to which the ratio
=

I0(P ,T,J)t/max 10 (P1 ,T,J)l
somewhat

can come

arbitrarily close,

when I is

an

arbitrarily large input.

complicated
are load-sharing schemes (the ones discussed
speedup cannot be achieved, but to which the ratio can come arbitrarily close.
sharing scheme implies that 1Ps (D )r.
since there

The definition is

in section

6) for which the potential
Note that the fact that D is a load-

The potential speedup means that for each one in an infinite set of
inputs, the output of each P is at least
(D) times smaller than the output of P; also, this output reduction occurs for arbitrarily large outputs. When the
load to evaluate T is measured in terms of new ground atoms
generated in the evaluation process, Ps (D) is the ratio
between the load of evaluating T by P. and the maximum load of a
processor, when the sharing scheme is used.
Although we defined the potential speedup based on some infinite set of inputs, for the load sharing schemes that we
Ps

discussing in this paper, it is intuitive that time saving can be achieved for the “average input”. The reason is
each load-sharing scheme discussed in this
paper is obtained by adding the evaluable predicate
(xl+,...,-t-xk)mod r to one of the rules, where xi,...,r* are distinguished variables. For an input which is distri

are

that
i

=

buted

evenly

across a

range of natural numbers, this reduces the number of

newly generated tuples

at each proces

sor.

A single rule program (see CK]), or a sirup for short, is a DATALOG
program which has
predicate, denoted S in this paper. The program consists of two rules. A nonrecursive rule:

a

single intentional

S(xl,...,rn):—B(xI,...,xn)
where the xi’s
not

are

distinct variables; and

one

other, possibly recursive, rule in which the predicate symbol B does

appear.

Assume that R is a set of atoms with each atom having a variable in each
argument position. The set R is
if there is a subset d of argument positions, such that in the
positions of d:
1. the same variables appear (possibly in a different order) in all atoms of
R, and
2. each variable appears the same number of times in all atoms of R.
A member of d is called a pivot. Note that a variable which
appears in a pivot may or may not appear in a nonpivot
position. The recursive rule of a sirup is pivoting if all the occurrences of the recursive predicate in the rule consti

pivoting

tuteapivotmgset. Forexample,therule: S(w,x,x,y,z) :—S(u,y,r,r,w),S(v,x,y,x,w),A(u,v,z)
pivoting, with argument positions 2, 3 and 4 of S being the pivots.

is

Theorem 1: If the recursive rule of a sirup is pivoting, then the
sirup has a load-sharing scheme of any size. The
size of the scheme.
Proof: Assume that argument positions i i,...,i~ of S are the pivots. Consider restricted version P1 of P which has
the same recursive rule as P, and a nonrecursive rule

potential-speedup equals the

S

for j=O

(x 1,...,xn)

:—

B

(x 1,...,xn ), j

=

(xi 1+xi2+

~+~Xik )mod

r

r—1. It is easy to see that (P0,.
,~,.....1j is a load-sharing scheme. Nontriviality and potential
be demonstrated using as input a prefix of the
sequence (B(1,...,I,q,1,...,1) I qr, and q appearing in
..

speedup

can

positioni1). I

4. LINEAR PROGRAMS
In this section we discuss linear
programs. A program is linear if the body of each rule contains at most one
intentional predicate. A rule of a program P is an exit rule if its
body consists of extensional predicates only. An
exit rule re, with extensional predicate symbols B1
Bk, is distinct, if there is no other rule r of P for which the
following condition is satisfied: in the body of r there are some extensional predicates, and every such extensional
predicate belongs to the set (B1
Bk). In other words, re is not distinct if there is another rule of P in which a
subset of the B1 ‘s appears, but no other extensional predicate does. Note that the exit rule
of a sirup is distinct. An
exit rule r1 in P derives a predicate R if the predicate in
re ‘s head derives R. An intentional predicate of a linear
program is distinct if it is derived by a distinct rule.

Theorem 2: If T is a distinct predicate of a linear program P, then there is
evaluating T in P. The potential-speedup equals the size of the scheme.

Proof: Assume

a

load-sharing

scheme of any size for

without loss of generality that the the first variable of the atom in the head of each exit
rule is x. Let
restricted version P1 of P be obtained by adding the predicate
j x mod r to each exit rule, for j_-rO
r—l (all
=

62

the other rules stay the same).
To show completeness, assume that a T-atom, a, is in the
output of P. Consider a minimal-length sequence s of
iterations of BUE2, for deriving a. It can be shown by induction on the length of
s, that s has exactly one instantia
tion of an exit rule. In that instantiation, x is substituted for some
constant, n. Let I
n mod r. Then s is a deriva
tion sequencefora inP,. Thus,a isinatleastoneT1.
ifa
=

(ItmaybeinmorethanoneT~

canbeobtainedbya

sequence of iterations with a different instantiation of an exit rule).
Now assume that the distinct exit rule which derives T in P is:

R(x

) :—B1(

),B2(

)

B~(

Nontriviality and potential-speedup can be demonstrated by using as input the
(B1(i,...,i),B2(i,...,i)
Bk(i,...,i) I 1i~W ) foralargeenoughN. I

)

set

of atoms

In WSJ we have shown that a linear sirup without repeated variables in the recursive
predicate, is not decompos
able if it is not pivoting. In contrast, observe that theorem 2
implies that every linear sirup has a load-sharing
scheme of any size. Another comment is that the above proof does not work if the
predicate T of Theorem 2 is not
distinct. The reason for this is given in W].

5. PROGRAMS WITHOUT A LOAD SHARING SCHEME
In this section we demonstrate that not
every program has a load sharing scheme. Specifically, we provide a
necessary condition for a sirup to have a load sharing scheme (Theorem 3). It turns out that some famous
sirups do
not satisfy the condition. An example is the first
P-complete problem, path-systems C]). The sirup for the prob
lem is:

5(x):— S (y ),S (z ),H (x ,y ,z)
S(x):—B(x)
Another example of

a

sirup without

a

load

sharing scheme,

is

a

variant of

path systems called the blue blooded

frenchman CKJ):

BBF(x):—BBF(m),BBF(f), MOTHER(x,m),FATHER(xf)
BBF

(x):- FRENCH(x)

Some other vanations which have not been defined
thus specification omitted):

previously,

as

far

as we

know,

are

(nonrecursive rule obvious,

S(x,u):— H1(x,y,u),H2(x ,z ,w),S(,y ,u),S(z ,w)
S

(x ,u ):— H (x ,y ,z ,u ,w ),S (y ,u ),S (z ,w)

S(x):— H0(x,w),H1(w ,y),H 2(w ,z),S (y ),S(z).

S(x,w,y):— UP(x,t,u)~(t,u,v),FL4T(v ,w ,z),S(z ~,s),DOWN(r,s,r)
What do the above

sirups
following definition.

have in common? This is what the next theorem establishes. Before

stating

it

we

need the

Given a sirup P. denote by A (P) the set of atoms in the
body of the recursive rule, and by V (P) the set of
variables in A(P). Let R(P)
(x I x is in V(P), and x appears in some S-atom of V(P)}. Let the extensional
graph of P. denoted G(P), be an undirected graph defined as follows. Its set of nodes is V(P)—R (P), in other
words, variables which do not appear in any S-atom in the body of the recursive rule. For two distinct nodes of
G (P), x and y, the edge x—y is in the graph if and
only if there is an extensional-predicate atom, A, in the body of
the recursive rule such that x and y are variables of A.
=

Theorem 3: Given a sirup P, with the recursive rule r, denote
by ST the set of atoms in the
does not have a load sharing scheme if the following conditions are satisfied.
1.

Except for the S-atoms, there are

2.

There are at least two S -atoms in A (P), and the S -atoms in A (P) have
pairwise
them has repeated variables.

3.

Each extensional predicate atom in A (P) has
appears in some extensional predicate atom.

no two

atoms of A

a

(P) which have the

variable which is

63

not

same

in R

body of

r.

Then P

predicate symbol.
disjoint variables,

(P),

and

none

and each variable in R

of

(F)

4.

The

Proof:

graph G (F) has

see

a

distinguished

variable in each

of its connected components.

WI.

It is easy to

verify that path systems and the other
of Theorem 3.

ments

one

sirups

that have been discussed in this section

satisfy

the require

6. FUTURE WORK
An obvious direction for future research is to extend the class of programs which have load sharing schemes,
and the class for which we can prove nonexistence. However, we conjecture that determining whether a program
does or does not have a load sharing scheme, is undecidable. Another question is the following. If a load sharing

scheme exists, does there always exist one with a potential speedup equal to the size of the scheme? For the pro
grams discussed in this paper we have seen that this S the case. Must it be true? Finally, we would like to determine
how to distribute the load when communication among the processors participating in the evaluation cannot be
avoided, and what architectures are preferable. Some answers are provided in CW].

Acknowledgement:

The author thanks Nissim Francez and Oded Shmueli for

helpful discussions and

comments.

References

API

F. Afrati and C. H.

Symp.
B]

on

Papadimitriou

“The Parallel

Complexity of Simple

Chain Queries”, Proc. 6th ACM

PODS, pp. 210.213, 1987.

F. Bancilhon, “Naive Evaluation of

Recursively Defined Relations” in On Knowledge Base Manage
Systems, Brodie and Mylopoulos, Eds., Springer-Verlag.

Systems Integrating
pp.165-178, 1986.

Database and A!

BMSUI

F. Bancilhon, D. Maier, Y.
Programs”, Proc. 5th ACM

Sagiv, J. Ullman “Magic Sets and Other Strange Ways
Symp. on PODS, pp. 1-15, 1986.

BR]

F. Bandilhon and R. Ramakrishnan “An Amateur’s Introduction to Recursive

ment

-

SIGMOD

Conf.

to

Implement Logic

Query Processing”,

Proc.

pp. 16-52, 1986.

C]

S. A. Cook “An Observation

CK]

S. S. Cosmodakis and P. C. Kanellakis “Parallel Evaluation of Recursive Rule Queries”, Proc. 5th ACM
Symp. on PODS, pp. 280-293, 1986.

CW]

S. Cohen, 0. Wolfson, “Why a Single Parallelization Strategy is Not Enough in
appear, Proceedings of the 8th ACM SIGACT-SIGMOD-SIGART Symposium
base
S.

H]

M. T. Heath

K]

P. C. Kanellakis

Gregory,

Parallel

Logic Programming

in PARLOG, Addison

Wesley Publishing Co.

for Industrial and

Applied Mathematics,

Phi

“Logic Programming and Parallel Complexity”, Proc. ICDT ‘86, International Confer
Theory, Springer-Verlag Lecture Notes in CS Series, no. 243, pp. 1-30, 1986.

A. Marchetti-Spaccamela, A. Pelaggi, D. Sacca “Worst Case Complexity Analysis
Logic Program Implementation” Proc. 6th ACM Symp. on PODS, pp. 294-30 1, 1987.

D. Maier and D. S. Warren

“Computing

with

“A Subset of Concurrent

E. Y.

UI

J. D. Uliman “Database

LIV]

J.D. Ullman and A. Van Gelder, “Parallel
Stanford University.

Shapiro

Logic: Introduction

to

Theory:

Logic Programming”, Benjamin-

Prolog

and Its

Interpreter”,

Past and Future”, Proc. 6th ACM

Complexity

of

TR-003 ICOT,

Symp.

on

Tokyo, Japan.

PODS, pp. 1-10, 1987.

Logic Programs”,

M. H. Van Emden and R. A. Kowalski”The Semantics of Predicate

Language”, JACM 23(4)

of Methods for

1987.

SI

TR STAN-CS-85- 1089,

Logic

as

a

Programming

pp. 733-742, 1976.

0. Wollson, “Sharing the Load of Logic Program Evaluation”, Proc. of the Intl.
Parallel and Distributed Systems, pp. 46-55, Austin, TX, Dec. 1988.

WS]

Knowledge-Bases”, to
on Principles of Data

Database

Cummings Publishing Co.,

VEK]

Trade-off’ JCSS 9(3), pp. 308-316, 1974.

“Hypercube Multiprocessors 1986”, Society
ladelphia PA, 1986.
ence on

MW]

Time-Storage

Systems, Philadelphia, PA, March 1989.

G]

MPS]

on

Symp.

on

Databases in

0. Wolfson and A. Silberschatz, “Distributed Processing of Logic Programs” TR466 The-Technion, CS
Dept. Also, Proc. of the ACM-SIGMOD Conf., pp. 329-336, 1988.

64

IEEE

Computer Society

I

‘Jon-profit Org

I

U.S Postage
I
PAID
I Silver Spring. MD I
Permit 1398

1730 Massachusetts Avenue. N W

I

Washington. DC 20036.1903

Dr.

David B
Lornet
Digital E~uiprnerit Corporation
9 cherry Lane
Westford, M~ (11886
USA

Optimal Design and Use of Retry in Fault-Tolerant
Computer Systems
YANN-HANG

LEE

AND

KANG

G. SHIN

University of Michigan, Ann Arbor, Michigan
Abstract. In this paper, a new method is presented for (i) determining an optimal retry policy and
(ii) using retry for fault characterization, which is defined as classification of the fault type and
determination of fault durations. First, an optimal retry policy is derived for a given fault characteristic,
which determines the maximum allowable retry durations so as to minimize the total task completion
time. Then, the combined fault characterization and retry decision, in which the characteristic of a fault
is estimated simultaneously with the determination of the optimal retry policy, are carried out. Two
solution approaches are developed: one is based on point estimation and the other on Bayes sequential
decision analysis.
Numerical examples are presented in which all the durations associated with faults (i.e., active,
benign, and interfailure durations) have monotone hazard rate functions (e.g., exponential Weibull and
gamma distributions). These are standard distributions commonly used for modeling and analyses of
faults.
Categories and Subject Descriptors: B.2.3 [Arithmetic and Logic Structures]: Reliability, Testing, and
Fault-Tolerance; G.3 [Mathematics of Computing]: Probability and Statistics-statistical computing
General Terms: Algorithms, Design, Performance, Reliability, Verification
Additional Key Words and Phrases: Bayes decision problem, estimation, fault characteristic, hypothesis
testing, optimal retry

1. Introduction
Faults in computer systems are usually classified into three types: transient,
intermittent, and permanent [23]. Transient faults die within a certain time of their
generation, intermittent faults cycle between being active and inactive, and permanent faults are (as the term indicates) permanent. When an error induced by an
existing fault is detected, ’ the system retries to recover from the fault. The executing
’ Normally, errors are detected but faults are not. However, if an error is defined as incorrectness in the
user’s program, then the manifestation of a fault captured by built-in detection mechanisms is not the
detection of an error, since the fault did not yet induce an error. In order to avoid an endless pedantry,
we use the term “failure detection” for detection of an error or manifestation of a fault, and sometimes
the term “fault detection” for detection of a fault manifestation.
This work was supported in part by NASA under grant NAG l-296. Any opinions, findings, and
conclusions or recommendations expressed in this paper are those of the authors and do not necessarily
reflect the views of NASA.
Authors’ present addresses: Y.-H. Lee, IBM Thomas J. Watson Research Center, P.O. Box 218,
Yorktown Heights, NY 10598; K. G. Shin, Division of Computer Science and Engineering, Department
of Electrical Engineering and Computer Science, the University of Michigan, Ann Arbor, MI 48 109.
All correspondence should be addressedto Prof. Kang G. Shin at the above address.
Permission to copy without fee all or part of this material is granted provided that the copies are not
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the
publication and its date appear, and notice is given that copying is by permission of the Association for
Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.
0 1988 ACM 0004-541 l/88/0100-0045 $01.50
Journal

of the Association

for Computing

Machinay,

Vol. 35, No. 1, January

1988, pp. 45-69.

46

Y.-H.

LEE AND

K. G. SHIN

tasks can be continued if retry is successful, or other recovery methods are applied
if retry is not successful.
As the term implies, retry consists of restoring the affected process to some faultfree initial state and then rerunning it on the same processor. Retry will become
successful after the existing fault disappears. Clearly, retry is only applicable when
the manifestation of a fault is confined and the process can be restored to integrity.
Although restart and rollback recovery can be viewed as a sort of retry, the retry
used in most computer systems means the repetition of microinstruction(s) or an
instruction. The latter type of retry is typically hardware controlled [4, 51 and thus
has advantages of requiring small time overhead. The time-consuming diagnosis
and reconfiguration of the system can be avoided in the case of a successful retry.
Moreover, such retries are reported to be highly successful owing to the fact that
only a small percentage of faults are permanent [ 1,261. Consequently, we focus in
this paper on hardware-controlled retry in which a failure is detected immediately
upon its occurrence by certain signal-level detection mechanisms [2 I]. We assume
there are some scratchpad memories used in restoring the process to integrity.
Results obtained by Carter et al. [5] indicate that self-checking and retry mechanisms can be incorporated into processors inexpensively and without substantially
degrading performance.
Currently, several commerical machines incorporate retry. In the Honeywell
6000 [ 171, instruction retry is reported to approach an effectiveness rate of 100
percent. Retry in the IBM 360 and 370 series machines is widely used in the
peripheral areas (I/O and storage), as well as in the central processor [ 121. The
UNIVAC 1100/60 uses a hardware timer that goes off after an interval judged to
be long enough to allow transient faults to die out, upon which retry can be effected
[3]. However, we are provided with no discussion or justification regarding the
detailed design of retry, for example, number of retry attempts. This can be seen
more clearly when we consider the statement in [4]: “If successful, computer
operation proceeds; if unsuccessful the above process [i.e., reexecution of the
previous instruction] is usually repeated N times before diagnosis begins.”
Clearly, the usefulness of retry mechanisms arises, as we have noted, from (i) the
smallness of the proportion of permanent faults in any computer system, and (ii)
the fast recovery from nonpermanent faults and thus the small task completion
time and recovery overhead. In the case of a permanent fault, to retry a process on
the affected processor is worse than useless;it is a waste of time. Thus, the number
of retry attempts or retry duration within which retry is applied should be controlled
to maximize the difference between the expected gain in performance that results
from using retry when the fault is transient or intermittent, and the expected loss
that results from using it when the fault is permanent. In this paper, we focus on
the determination of the maximum allowable retry duration r* for the purpose of
reducing expected task completion time. If the retry succeedswithin this duration,
the execution continues. If not, other methods for failure recovery, for
example, rollback or restart following the system reconfiguration, must be used.
(See Figure 1 for a standard procedure for task execution under the occurrence
of fault.)
In addition to the performance gain in caseof a successful retry, the characteristic
of a fault can be monitored through retries. That is, a retry that succeeds within
the retry duration r* implies that the active duration of the fault following its
detection is also less than or equal to r*. Even when the retry fails, it indicates that
the active duration of the fault following its detection is greater than r*. On the
other hand, the detection of a failure gives information regarding the duration
between fault occurrences and the benign duration of an intermittent fault. Thus,

Optimal Design and Useof Retry in Fault-Tolerant Computer Systems

47

Systemdiagnosis
&
MXOfiglJ~~U~
I
I

FIG. 1. Standard procedure for handling failures during task execution.

it becomes possible to observe the nature of a fault through both retry and detection
mechanisms. Note that due to nonzero fault latency [ 19-2 I], the observed nature
of a fault may not be the same as its true nature. Although fault latency can be
measured [22], we ignore this latency in the rest of the paper, since it has no effect
on retry. In the discussion that follows, we treat a fault on the basis of its observed
behavior and its effects on the system. For example, we usethe term active duration
of a fault to mean the active duration of a fault following its detection.
Section 2 presents a brief description of the fault model along with necessary
assumptions and an informal statement of the problem. It should be obvious that
r* depends on fault behavior, and in Section 3, we show how to derive it, given
the fault characteristics. When quantitative descriptions of fault behavior are hard
to come by in the real environments, the combination of retry and detection
enables us to observe the fault characteristics, while determining the optimal retry
policy. We counter this in Section 4 by showing how to use statistical estimation
theory to create a system that learns the fault characteristics as it goes, via retry,
and therefore becomes increasingly more “optimal” in the senseof minimizing the
expected task completion time. In Section 5 we apply Bayes sequential decision
analysis to fault characterization and retry decision. The backward induction for
testing hypotheses is also presented as an example solution to the sequential decision
problem. The paper concludes with Section 6.
In what follows, we use continuous retry instead of the number of retry attempts.
A continuous retry can be understood easily when one considers the following two
cases:detection mechanisms can monitor the presence of a fault continuously or

48

Y.-H.

LEE AND

K. G. SHIN

retry can be performed instantaneously. Conversion between a continuous retry
and its corresponding number of retry attempts is not difficult and is discussed in
Section 6.
2. Fault Model, Assumptions, and Objectives
Consider the behavior of faults in a computer system. Assume that arrival of faults
is a time-invariant Poisson process with rate X. When a fault occurs, it is assumed
to be transient, intermittent, or permanent with probability pt, pi, or pP, respectively. If a permanent fault occurs, it will remain constantly in the system until the
component containing the fault is removed. Once a transient fault occurs in the
system, it will disappear after an active duration, Ty . On the other hand, in case
of an intermittent fault, it may become benign after an active duration, Tq, and
then reappear after a benign duration, Tf’. That is, an intermittent fault cycles
between active and benign states. For simplicity, we assume that Ty , T4, and TF
are mutually independent random varibles with distribution function FF, Fy , and
Ff and density functionsff , x, and fp, respectively. Thus, the characteristic of a
fault can be represented by a 7-tuple
GE

((Pz, Pi, up, A, FP, F4, f’fk

PI + Pi + Pp = 11.

Since the interarrival time of faults is usually much larger than any other
durations, it is reasonably accurate to assume that there is at most a single fault in
the system at any moment. Thus, the above behavior of faults enables us to model
the system with a stochastic process shown in Figure 2. Denote the three possible
states, namely, nonfaulty, fault-active, and fault-benign by NF, F, and FB. When
a fault occurs, the system state changes from NF to F. The system moves back to
NF if the fault is transient and disappears. It remains at F if the fault is permanent.
If the fault is intermittent and becomes benign following an active duration, the
system state changes from F to FB. The system may move back to F when this
intermittent fault recurs-this is referred to as the reappearance of the intermittent
fault. Models similar to this have been widely used in reliability analyses and the
modeling of faults [ 14, 2 1, 251.
When a failure is detected (i.e., the system is in fault-active state), retry is usually
applied as a first-step recovery means. Retry will be successful if the fault disappears
during the retry period, that is, if the system changes to either nonfaulty or faultbenign state during retry. Otherwise, the system is reconfigured and the executing
task is migrated to a nonfaulty component and then recovered via the other means
such as rollback or restart. The advantages of a successful retry are twofold. One is
the avoidance of complicated, time-consuming recovery actions, such as faultisolation, system reconfiguration, and task recovery. The other attractive gain
from retry is to rescue an executing task. Consider a practical case in which a
system (i) becomes faulty once and gets back to normal during execution of a task,
and (ii) never becomes faulty again before the task is completed. In such a case, it
is possible to avoid the overhead of migrating and restarting the task by means of
a successful retry, leading to a faster completion of the task.
In what follows, we derive an optimal retry policy that minimizes the expected
task completion time when failures are detected during the task execution. We
assume that initially no task is started on any faulty or potentially faulty module
(having a benign intermittent fault) and that the system has enough redundancy
so it can be reconfigured and made operational again when retry recovery fails. In
such a case, the associated task is restarted following system reconfiguration.

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

49

FIG. 2. The fault model. NF = nonfaulty,
F = faulty, FB = fault-benign.

When the system enters fault-active state, there is no way to determine whether
the fault is transient, intermittent, or permanent. If retry becomes successful, the
fault is either transient or intermittent. Under the assumption that the benign
duration of an intermittent fault is much smaller than the fault interarrival time,
the fault is declared to be intermittent if the system fails again within a short period
after the disappearance of the previous fault. Thus, a retry policy should specify
two maximum allowable retry durations: one for a new fault and the other for an
old recurring intermittent fault.
Our problem is to derive an optimal retry policy that minimizes task completion
time under the occurrence of faults. When the characteristics of faults (e.g., the
durations associated with the above fault model) are not known a priori, the
problem calls for an adaptive optimization in which the system decides a retry
policy to minimize the task completion time while learning about the fault
characteristics via retry. We solve this problem beginning with a simple case in
Section 3 where the fault characteristics are known, and then the general case
in Sections 4 and 5 where the fault characteristics are unknown.
3. Optimal Retry Policy for Given Cf
3.1 EXPECTED TASK COMPLETION TIME. Let x0 denote the computation time
initially needed to complete the task under a fault-free condition. A failure may be
detected when the amount of computation remaining to complete the task, that is,
residual computation, is reduced to x, where 0 < x < x0. A retry policy is defined
as R = ((r,(x, C’), rz(x, Cf)); 0 C x < XO),where the maximum retry durations are
rl(x, C,) and r2(x, C,), respectively, for the detection of a new fault and an old
intermittent fault when the residual computation is x and the fault characteristic
is C’. For notational simplicity, we shall use ri, whenever convenient, in the sequel,
to represent ri(x, Cf), i = 1, 2.
Let the expected times needed to complete the residual computation x be denoted
by V,(x, C,, R), VZ(x, C,, R), V,(x, Cf, R), and V.(x, Cf, R) when the retry policy
R is adopted and the system is in the following situations: execution starts/resumes
on a nonfaulty system, a new fault is detected, an old intermittent fault is detected
again, and execution continues following a successful retry for an intermittent
fault, respectively. Based on transitions among these situations, one can derive the
following recursive equations for V,(x, C,, R) through V,(x, C’, R). For example,
when the residual computation is x, the task completion time needed following the
task resumption would be x if no new fault occurs within the duration x or would

50

Y.-H. LEE AND K. G. SHIN

be the sum oft and the time needed for completing the computation if a new fault
occurs at the residual time x - t, that is, VX(X- t, C’, R).
x
V,(x, c,, R) = PXx +
(t + v2(x - t, C’, R))AemM dt,
(1)
s0
v,tx, c,, RI = Pr s o” tt + VI@, c,, RI1 CV)
II
+ Pi 8 (t + v,tx, c,, NJ @Xt)

+ (1 ‘p,Ff(r1)
V&G C/v RI = (1 +
s
J’4k

C,,

R)

=

(1

-

F3~2)1~~1@0,

oQ

(t +

v4tX,

Cf,

piF4(r*)){V*(XO,
c,,

c,,

R)

RN

+

r2 +

R)

+

rl

+

ts),

(2)

tsl

(3)

dW),

fx

- F;(x))x+ Jo (t + V~(X
- t, c,, R))

dF;(t),

(4)

where ts is the set-up time necessaryfor system reconfiguration and task reinitialization. The optimization problem can be defined as follows: Find an optimal retry
PoficY,
R* = Kr:tx, c,), $6, c/N; 0 c x < x01,
such that Vx V2(x, C’, R*) = minR V2(x, C’, R) and V3(x, C’, R*) = minR V,(x,
C’, R). Obviously, this optimal policy also minimizes V,(x, C’, R) and
J’4(x,

c,,

RI.

Since the mean time between failures is usually much longer than the other
durations, V,(x, C’, R) can be accurately approximated by x. The bounds of
V,(x, CJ, R) under the optimal retry policy are derived in the Appendix A. Note
that the difference between the upper and lower bounds of V,(x, Cf, R*) is
negligible. Thus, the approximation is used throughout the rest of the paper.
In general, there are no closed-form solutions for r:(x, Cf) and r:(x, Cf).
However, these optimal retry durations can be calculated numerically as explained
below. Let Ax be an arbitrarily small positive value. With the initial condition
K,(O, C’, R) = 0, V,(kAx, C,, R) with k = 0 and any r2 can be computed. Then,
rz is chosen to minimize v,(kAx, C’, R) for the residual computation kAx, k = 1.
By incrementing k we can recursively compute V4(kAx, C’, R), vs(kAx, Cf, R),
and r: for the residual computation kAx. Once V4(x, Cf, R) is known, one can
compute V2(x, C’, R) for any rl and can therefore determine r:(x, Cf).
Define the recovery overhead as the total time required to resume normal
operation following the detection of a failure. When the recovery overheadin place
of the task completion time is to be minimized, rz(x, C’) = 0, Vx E (0, x0), because
the recovery overhead will accrue during reappearancesof an intermittent fault. In
this case,the recovery overhead can be expressedas 6(x, CJ, R) - V,(x, C’, R),
which is the time spent to restore the system to its state immediately before the
failure is detected. The optimal retry duration r:(x, C,) can be determined from
eq. (2) just as we can compute that for minimizing the task completion time.
3.2 FAULT ACXIVE DURATIONS WITH MONOTONE HAZARD RATE FUNCTIONS.
Since Ty is a continuous random variable, one can assumethatfT(t) is continuous
in [0, 00). The hazard rate function of the active duration of an intermittent fault

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

51

is defined by u;(t) =fl(t)/( 1 - F;(t)). When the hazard rate function of the active
duration of an intermittent fault is monotonically increasing, constant, or monotonically decreasing, the optimal retry duration r: exhibits interesting properties.
These properties play a significant role in determining the optimal retry policy,
since the time durations associated with faults are usually modeled to have
monotone hazard rate functions. Typical distributions with monotonically increasing hazard rate functions include the gamma and the Weibull distributions with
the shape parameters greater than 1. When their shape parameters are less than 1,
they have monotonically decreasing hazard rate functions. The exponential distribution has a constant hazard rate. Consider first the nondecreasing hazard rate
function that leads to the following theorem.
THEOREM 1. When up(t) is monotonically nondecreasing in t and VI(x, C’, R)
b x, r: = 0 or r*2 = 00*
PROOF. Using the approximation V,(x, C’, R) e x and differentiating eq. (3)
with respect to r2, we obtain

Since Vd(x, Cf, R) is independent of the past and current retry durations’
My, G), where Y 2 x, V~(X, Cf, R) + ( 1/up(r2)) - (XO+ t,) is nonincreasing
in r2(x, C’). Thus, V3(x, Cf, R) is a concave function of r2. The optimal retry
duration r-z is then equal to 0 or 00. Cl
Following the definition of r:(x, C’), rr(x, C’) = 0 implies that no retry be
attempted for reappearing intermittent faults, whereas r:(x, C’) = 0~)means that
retry should be applied until the intermittent fault becomes benign.
COROLLARY1. When uf(t) is monotonically nondecreasing in t, and if there
exists an xz such that x0 + tS- xz - Rf(xT)E[ q] = 0, where R!(x) is the renewal
function [7] corresponding to the distribution F:(t), then rT(x, C’) = 00 tfx 5 xz
and rT(x, C,-) = 0, otherwise.
PROOF. From Theorem 1, rz(x, C’) is either 00or 0. When r:(x, C,) = CQ,there
exists an r such that the integral JLj(a V,(x, C’, R)/dr2) dr2 becomes negative. Since
V, (x, C,-, R*) is a monotonically nondecreasing function of x, there also exists an
r such that the integral J6 (aV3( y, C’, R)/ar2) dr2 becomes negative when
y 5 x. Thus, r:( y, C,) = 00 Vy 5 x. Using the assumption that the active and
benign durations are mutually independent, we get V4(x, C’, R*) = x +
(E[ZV(x)] - 1)E[ Ty 1,where N(x) is the number of reappearances of the intermittent
fault during the residual computation x, namely, N(x) = inf(n; )=I=, Tpk L x),
where T$ is the benign duration following the kth occurrence of the intermittent
fault. The expected value of N(x), E[N(x)], is equivalent to the renewal function
’ Note that the probability of having a zero benign duration of an intermittent fault should lx zero, that
is, Pr(Tf = 0) = 0. Otherwise, no useful computation can be done.

52

Y.-H.

LEE AND

K. G. SHIN

R:(x) corresponding to the distribution F:(t). Also, V,(x, C’, R) ] r2iDm
5 V,(x, C,,
RI I yo if r:(x, C’) = CQ,that is,
m
(t + V,(x, C’, R*)) dF;(t) = E[T;] + V‘,(x, C,, R*) 5 a@,,) + ts.
s0
From the equality in the right-hand side of the above equation, we obtain xz and
thus the corollary is proved. Cl
Theorem 1 can also be viewed as below using the concept of stochasticordering
between two random variables. A random variable X is said to be stochastically
larger than the other random variable Y if Pr(X > t) 2: Pr(Y > t) V t [ 181. Let
c( ] r) be the remaining life of the intermittent fault after retry has been applied
for the duration r. When the hazard rate function is nondecreasing, c( ] r) is
stochastically larger than c( ] s), provided r I s. Thus, Vs 2 r; if it is worth
continuing retry beyond the retry duration r (in the sense of minimizing the task
completion time), then we should continue the retry even after the retry duration
s. Consequently, the retry continues until the intermittent fault disappears.
Note that when the hazard rate function is nondecreasing, xt is determined by
the mean active duration and is independent of the shape of the distribution. xr
could become negative when E[ 71 is large, that is, intermittent faults have a long
active duration. In such a case, Corollary 1 implies that no retry be applied for
intermittent faults. On the other hand, if the set-up overhead ts is large, x: could
be even larger than ~0, implying that retry be used as a sole means of recovering
from an intermittent fault.
When the hazard rate u;(t) is decreasing, the nice properties stated in both
Theorem 1 and Corollary 1 do not exist. However, there exists at most one root of
eq. (5) that minimizes I’+ In such a case, since there is no closed-form expression
of V,(x, C,, R*), we have to resort to numerical techniques for determining both
r:(x, Cf) and rr(x, C’), as was previously mentioned.
Several numerical examples are shown in Figures 3-5, where the durations are
normalized with respect to ~0, and the active duration of the intermittent fault is
assumed to have the gamma or Weibull distribution. Figure 3 presents xz’s when
the shape parameter cl!‘s of the gamma and Weibull distributions, respectively,
are greater than or equal to 1. Figures 4 and 5 show the optimal retry duration
r:(x, C’); the solid lines for (Y< 1 and the dashed lines for (Y= 1. Note that for the
gamma distribution u:(t) approaches l/j3 as t + 00,where p is the scale parameter.
Thus, it is possible for the derivative of V, to be negative, (i.e., eq. (5) becomes
negative), implying rr(x, Cf) = 00. For the Weibull distribution with (Y < 1, rz
never becomes 03since u;(w) = 0.
Consider the casewhere Tl, v, and TF are all exponentially distributed with the
parameters 7, p, v for the transient fault disappearance rate, the intermittent fault
disappearance and reappearance rates, respectively. SincefF(t) = ve-“‘, the renewal
function R;(x) becomes 1 + vx. From Corollary 1, we have r:(x, Cf) = 00 if
xrxtand

$(x, Cf) = 0
where

if

x>x:,

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

4.00

1.80

sig

param&iO(>ll

53

5.00

4.20

FIG. 3. x&x, Cl) versus the shape parameter a when the hazard rate is
increasing andf, (t) = 3e-“. Case 1: Gamma distribution, B = 0.1. Case 2:
Weibull distribution, @= 0.1. Case 3: Weibull distribution, fJ = 0.2. Case 4:
Gamma distribution, @= 0.2.

V,(x, Cf, R*) then becomes
if x5x:,
V,(x, C,, R*) =

(6)

if x>x:.
The derivative of VZ(X, C’, R) with respect to rl becomes
aV2(x,

C,,

R)

= pP + pmp(-m)tl

- (XI + ts - ~171

ah

+ piexp(-prl)[l

- ixo + ts - V,(X, C’, RIbI.

(7)

With rz(x, C’) determined as in Corollary 1 and V4(x, C,, R*) as in eq. (6), eq. (7)
can have at most two roots. The optimal retry duration r:(x, C’) can be obtained
by examining VZ(X, C’, R) at the boundaries, rl = 0 and rI = ~0,and the roots of
eq. (7). Note that t-7 cannot be infinite as long as pP C 0. Unlike r:, r: does not
have to be zero when x > xr. Several casesof V2(x, C,, R) as a function of rl are
shown in Figure 6 where all parameters are normalized with respect to x0. The
case 2 in Figure 6 shows an example for which two positive roots of eq. (7) exist.

Y.-H. LEE AND K. G. SHIN

4. The optimal retry duration rt(x, CJ) for Weibull distributions
with increasing hazard rate. The density function of the active duration
f;(t) = (u/p Pexp(-P/p).
/3 = 0.2;fP(t) = 36”‘.
FIG.

W-=0.6
(I=

W-1.

I
I
I
I

I
I
I
I
I
I
I
I
I
I

I

I
I
I
I

I
I

I
I
!
I

0.20

0.60

Ret

11 coaputatlon

oio

1.00

(xl

FIG. 5. The optimal retry duration rT(x, C,) for Gamma distributions
with decreasing hazard rate. The density function of active duration
f?(t) l/(B”r(a))ta-‘exp(-to).
,8 = 0.4;fP(t) = 3e-“.

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

55

FIG. 6. V,(x, Cl, R) - x versus the retry during r, for exponentially distributed
durations, x = 0.8, T = 10, p = 12. Case 1: pp = 0.1, p, = 0.5, p, = 0.4, Y = 6.
Case 2: pp= 0.05, p, = 0.55, p, = 0.4, Y = 12. Case 3: pp = 0.05, p, = 0.6,
p, = 0.35, Y = 6.

Figure 7 presents some numerical results on r:(x, C’) as a function of x. Note that
XT depends upon the ratio of Yto cc,whereas r: varies as pl, pi, and pp change.
4. Optimal Retry Policy and Parameter Estimation
In Section 3, we have derived an optimal retry policy for a given fault characteristic
C’. It is, however, very difftcult in practice to know a priori the fault characteristic.
Even if the fault characteristic is measured during device manufacture, it may well
vary as the execution environment and the executing tasks change. Another factor
that makes the fault characteristics time variant is the aging of components, for
example, the bathtub curve of the failure rate as a function of time [23]. Thus, it
is important to determine an optimal retry policy for uncertain fault characteristics.
Detection mechanisms can be useful in collecting data of the duration between
two successivefault occurrences or the benign duration of an intermittent fault for
characterizing the behavior of fault occurrence and reappearance. Retry may lead
to an indication of the active duration of a transient or intermittent fault, which
is, on the other hand, affected by the retry policy applied. More specifically, when
C, is unknown, C, has to be estimated first with the observation of system state
transitions (as shown in the model of Figure 2) with retry and detection mechanisms. Then, the retry duration will be determined based on an estimated Cf. In
such a case,the computer system has to adjust its retry policy using the information
on the fault behavior collected during its past and current retries. See Figure 8 for
a block diagram of such an adaptive optimization.

56

Y.-H. LEE AND K. G. SHIN

0.40

Residual

0.60

coaputatlon

0.60

(x1

1.00

FIG. 7. The optimal retry duration r:(x, C,) for exponentially distributed
durations. Case 1: p,, = 0.1, p, = 0.3, pi = 0.6, T = 6, P = 5, y = 3. Ca 2: pP
= 0.1, pt = 0.6, pi = 0.3, T = 6, p = 5, Y = 3. Case 3: pP = 0.1, p, = 0.3,

~i=0.6,7=12,~=10,v=6.Case4:p,=0.1,p,=0.6,~~=0.3,r=12,
p = 10, Y = 6.

Failwe

Observation
consequence

detected

I

of
of retry

+

Determination
of an optimal
retry policy

l

Estimation
failure

of

characteristics

FIG. 8. Block diagram of the adaptive optimization.

In what follows, we shall limit the discussion only to the characterization of the
active duration of intermittent faults and the simultaneous determination of an
optimal retry policy that minimizes the task-completion time. The reasons for such
a limit are
(1) When Tf, Ty, TT, and Tf are mutually independent, the estimation of these
durations can be treated separately and individually with the same approach.

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

57

(2) Unlike the probabilities of having different types of fault and the fault occurrence interval, the behavior of an intermittent fault is difficult to measure.
(3) The fault occurrence interval and the benign duration of an intermittent fault
can be observed completely via detection mechanisms, whereas the information
collected through retry may become incomplete when the retry stops unsuccessfully.
(4) When retry is applied, the current sample has effects on the current retry
decision during its collection. Thus, simultaneous estimation and decision
must be performed continuously during retry.
Let the active duration of an intermittent fault have the density function form
fl(t ] 8) with the parameter 0 unknown. (0 could be a vector if there are two or
more parameters, for example, the shape parameter (Yand the scale parameter @
for the Weibull and gamma distributions.) If the form of the density function is
not known a priori, we can assume several possible forms and perform a test of fit,
for example, a chi-square test of tit, or hypotheses testing, as is discussed in the
next section.
The samples obtained for the active duration can be represented by a 2-tuple
(Z, t) where Z is a single-bit flag and t indicates a duration. Z = 0 represents a
successful retry, and hence t indicates the active duration of the fault. On the other
hand, when a retry fail, Z = 1 and t is the retry duration. Let (Zi, ti), i = 1, 2, . . . n,
denote the past samples related to the active duration of an intermittent fault.
These resulting samples are type Z progressively censored, following Cohen’s definition in [8] with continuous censoring times. There are several different types of
estimators conceivable for estimating the parameter B on the basis of these progressively censored samples. For the Weibull and gamma distributions, the maximum likelihood estimators have been widely studied as in [8-lo], [ 161, and [28]
when the samples are progressively censored.
When the fault is still active even after the current retry duration r, we shall have
collected an additional sample (1, r) via the current retry. Let e(r) be the maximum
likelihood estimator of B which is based on the samples up to and including
the current sample (1, r). Note that the dependency of 0 on the current sample
(1, r) is expressed explicitly, since the current retry duration will depend on this
estimated e^.
Let the optimal retry durations based on the estimated e*(r) be denoted by rt(x,
i(r)), k = 1,2, for a newly detected fault and an old intermittent fault, respectively.
Use the notation C’(e^(r)) to indicate that the active durations of intermittent faults
have the density function fl(t ] e*(r)), and let R(r) denote the policy that the
maximum allowable retry duration for the current retry is r. Then, the direct
solution of the optimal retry duration is to find the minimum of Vk(x, C/(6(r)),
R(r)), k = 1,2, 3,4. Notice that the retry duration r not only appears in the integral
equations (2) and (3), but also affects the fault characteristic Cf.
Under certain conditions, it can be proved that r:(x, i(r)) is a nonincreasing
function of r. We first derive the results under such conditions and then discuss its
application later in this section.
THEOREM 2. When (a) the active duration of an intermittent fad< has the
density function f y(t ] 0), and (b) for tj ?I tk the ratio (fl(t ] e(tj)))/( f y(t I e(&)))-a
likelihood ratio [ 15]-is nondecreasing in t, then the optimal retry duration is
determined by

rk* = infir; r,*(x, g(r)) 5 rJ.

U-3)

58

Y.-H. LEE AND K. G. SHIN

To prove this theorem, we need the following three lemmas.
LEMMA 1. Under the sameconditions as in Theoremz, let q and Tk be random
variables with the densityfunctionsfl(t 1B(tj)) andfl(t 1e(tk)), respectively,and let
9(t) be a nondecreasingfunction oft; then E[*(Tj)] L E[P( Tk)], provided tj Z tk.
PROOF. Proof of this lemma follows immediately from Lemma 2 of Chapter 3

in [15]. Cl
Let_z((t 1l(G)) be the hazard rate function when the density*function of Tf is
{i(tl O(G)). The following lemma gives the ordering of uy(t I e(tj)) with respect
I’

LEMMA 2. Under the same conditions as in Theorem 2, uy(t I e^(tj)) is a nonincreasingfunction of tj for everyfuced t.
PROOF. For b 5 tk, we have

.fY<SI htj 1)

fF(t I &t, )I
fi(t

1 e^(tk))

s fl(s

I e^(tk))

for all s 2 t. This inequality implies that
fl(t

Jrfl(U

1 e^(tj))

f;(t 1&t,)) S j.:fl(u
Thus, Uy(t I e*(Cj))I

Up(t 1 &tk))

du 1 - Fy(t I e^(tj))
1e*(a)) du = 1 - F;(t 1e^(t,))’
I e^(tj))

if Cj2 tk. 0

I$ V,*(x, i(t)) = minR vk(x, b(t), R), k = 2, 3, 4, where f?(t) iS used in place of
C,@(t)). Note in this case that the active duration of the intermittent fault is
distributed with the parameter i(t) and that all the other distributions are known.
LEMMA 3. Under the same conditions defined as in Theorem 2, ift, > t2, then
(i) V,*(x, &t,)) L V,*(x, &t2)), k = 2, 3, 4,
(ii) r,*(x, e^(t,)) s rz(x, &t2)), k = 1, 2.
PROOF. The proof for k = 3, 4 is done by mathematical induction. Let I/k&,
f(tj), r2(n, j)), k = 3, 4, be the expected times needed to complete the residual
computation x when there are at most n retries to be attempted following the
current one, and let r&z, j) be the maximum retry duration allowed. Also, let
the optimal rep duration to achieve the minimum V&(x, $tj)) be rz(n, j). For
n = 0, V&(x, Q)) = x and
f30(x,

htl))
OD

=

-

V,,O(~,

&t),

$(O,

1))

S W, x, $YO,l))MYt I &hN - fl<t I &d)I
0

4

where q(t, x, y) = t + x when t I y and y + ~0 + ts when t > y. Since *(t, x, ry)
is nondecreasing in t, the right-hand side of the above equation is nonnegative as
a result of Lemma 2. Also, since Vz,(x, d(t2)) is the minimum when the active
duration of the intermittent fault has the density functionfl(t I &a)),
we get
vt,(x,

htl))

22 V,,O&,

@t2),

$64

1)) 2

~f,(x,

ht2)).

Suppose that Y:Jx, 8(t,)) 2 V!,&, &td) and C,,(x, &tlN 2 V&(x, &a))
Vx, provided tl L t2. It is obvious to see from eq. (4) that V&+,(x, &cl))
L

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

59

C,,,+,(X, e^W) vx. Thus,
v&+,(x,

&tl))

-

V3,n+1k

@2),

rZ(n

+

1,

1))

Ca
E

w9

s 0

(x,

cJ+l

@tl)),

rz*(n

+

1,

l))lfX

I &tl))

-

fl<t

I &))I

dt.

(x, e^(t,)),rz(n + 1, 1)) is nondecreasing in t 5 r:(n + 1, 1). Also, since
c,n+l
r:(n + 1, 1) is the optimal retry duration, V&+,(X, &t,)) 5 x0 + ts. Hence,
wt9 Jqn+, (x, @I)), rr(n + 1, 1)) is always nondecreasing in t. The right-hand
side of the above equation becomes nonnegative, resulting in V:,+,(x, e^(t,)) h
we
V3,,+lk
i(t2),
rT(n
+
1,
1))
2
V* 3,n+l(x, &t2)). By mathematical’ induction,
have V,*(x, @t,)) I VE(x, e^(t,))for k = 3, 4.
To prove rz(x, e^(t,)) 5 rF(x, t?(Q), the following cases are examined. When
r:(x, &t,)) = 0, the relation is always true. When rz(x,Ae^(tl))> 0, using Lemma 2
and the first part of this proof, the derivative of V3(x, B(tj), R) with respect to the
retry duration r has the following ordering relationship Vr and t, 2 tz.
wt,

1
CYX,

e^@l

1)

+

7
ui

tr

I OttI

- @iI + ts) = wx,

e^(t*))+ 4-

))

ui

- (x0 + ts),
tr

I e(tZ))

where all retries after the current one are assumed to employ the optimal policy.
Thus, for tl 2 t2, r2(x, &t2)) = UJ when r-:(x, &t,)) = 03, and r2(x, e^(t,)) L
rz(x, &t,)) when r:(x, e^(t,)) is finite.
For the case of k = 2, it is easy to seethat V~(X,e^(t,), R) is a linear combination
of the effects of both transient and intermittent faults. Thus, VZ(X, e*(tj), R*) 2
V2(x, e(tk), R*). Also, the handling of I’2 with respect to ri hy the same
ordering relationship as >hat of V3&with respect to r2. Thus, VZ(X, e(tj ), R*) 2
V2(x, fl(tk), R*), and r:(e(tj)) 5 r:(e(tk)) when tj 2 tk. 0
Lemma 3 shows that r,*(x, e^(tj)), k = 1, 2, is nonincreasing in tj. Thus, there
exists an r such that r 2 rE(x, g(r)). The proof of Theorem 2 is given as follows:
PROOF OF THEOREM 2. Suppose that the retry has been applied for the period r
but the fault is still active. When r:(x, 8(r)) > r, the retry should be
continued since it decreases the expected task completion time. Thus, r,*(x) >
sup(r; t-:(x, g(r)) > r}. Suppose there is an rj E (r; r-:(x, i(r)) 5 r). Then
I/k’(x, &r,), 6) 2 V,*.(x, e^(rj)) 2 Vz,(x, f?(r,*)), where rz is defined as in
eq. (8) and k ’ = k + 1. Thus, the theorem follows. 0

If the maximum likelihood estimator is chosen for g(r), it maximizes the
likelihood function:
(9)
where ~(1, t, 0) is defined as
?a 6 6) =

if
if

I = 0,
I=l.

For the same example in Section 3.2, suppose the active duration of an intermittent
fault is exponentially distributed with an unknown disappearance rate cc.Using a
method similar to the Cohen’s derivation in [8], the maximum likelihood estimator

60
ii(r) for an exponential distribution-

Y.-H. LEE AND K. G. SHIN

which maximizes log &)-is

obtained as

Theorem 2 gives the optimal stopping time for the current retry. Note that the
true value of p is unknown and its maximum likelihood estimator is to determine
the optimal retry duration. In the case of retry for a reappearing intermittent
fault, the optimal retry duration for a given or is either 0 or 03 as shown in
Corollary 1. Using Theorem 2 and eq. (lo), we get the optimal retry duration
as follows

(11)
Note that the gamma distribution has a nondecreasing likelihood ratio for both
LYand /3 [ 151. Furthermore, the estimators provided by Cohen [lo] show that
both the estimated cy and B are increasing in the current retry period r. Thus,
Theorem 2 can be applied directly when the active duration of the intermittent
fault has the gamma distribution. When the distribution of the active duration is
Weibull, Theorem 2 cannot be applied directly due to the fact that the Weibull
distribution has a nondecreasing likelihood ratio with respect to its scale parameter
only. A reasonably good approximation can be obtained by assuming that (Y is
constant during the current retry and /3is estimated using both the past and current
samples as discussed above.
There are some shortcomings when the maximum likelihood estimator is used
for the progressively censored samples. Particularly, the estimator is biased when
the samples are censored. Also, in the case of the exponential distribution, fi does
not contain sufficient statistics of p when the samples are censored and incomplete,
that is, when there exists at least one sample (Ii, ti) with Zi = 1. These shortcomings
can be seen easily from a trivial example: 6 becomes zero when Ii = 1 for all i = 1,
29-**, n. In fact, as shown by van Zwet [27], for most practical casesit is impossible
to obtain unbiased estimators when the samples are Type I censored in a semiinfinite interval. Note, however, that there is no restriction about which estimator
to be used in the foregoing determination of the optimal retry policy, meaning that
estimators other than the maximum likelihood estimator can be used without
altering our method described thus far.
5. Bayes Sequential Analysis and Optimal Retry
In the previous section, the unknown parameters of a distribution are estimated
first, and the optimal retry policy is then determined using the estimated results.
Notice that there could be more subsequent retries for the same intermittent fault
before the task completion. Since the estimated parameter, 8, changes with the
samples obtained via these retries, the usage of constant e(r) for further retries
throughout the rest of the computation to determine r: is not accurate. In other
words, the point estimation approach treated in Section 4 does not include the
possible variation of e^during the subsequent retries in determining rz. In this
section, we shall take the Bayes approach to remedy this problem.
5.1 OPTIMAL RETRY AND BAYES DECISION. &.et the distribution of Tq be
governed by some unknown parameters Wi. The a priori information concerning
Wi is expressed in terms of a probability distribution function defined on Q. Let

61

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

the density function of Wi be [i(w). Denote further the fault characteristics, given
Wiand the prior density function ti, by CJ,,,,,and Cf, b, respectively.
To apply the Bayes decision theory for the retry of an intermittent fault, we
define the risk with a retry policy R, given & and the residual computation x, as
follows:
PI& ti, R) z s v&
n

Gy,

R)&(W) dw,

k = 3, 4.

(12)

Thus, the (optimal) Bayes risk is given as
P,*<x, Et) r i;fd&

ti, RI,

k = 3, 4.

(13)

The optimal retry duration in case of the detection of an old intermittent fault,
r:(x, C’l[,), abbreviated by r:(x, [i), yields the Bayes risk p:(x, [i). Similarly, the
Bayes risk of the retry for a newly detected fault can be defined by eqs. ( 12) and
(13). However, the determination of rr(x, [i) is a one-stage Bayes decision problem.
Once p4(x, [i) and rz(x, Ei) are obtained, the normal form of analysis [2] can be
applied directly for the solution of r:(x, [i).
Following a retry attempt for an intermittent fault, regardless of whether it fails
or succeeds,an event related to the fault active duration Tq is observed. The event
observed during a retry of the duration r is either “success” or “fail.” The “success”
event, denoted by e”(t), occurs when the detected fault disappears after the retry
duration t, which is less than or equal to the maximum allowable retry duration r.
The “fail” event, denoted by e/(r), occurs when the detected fault does not
disappear by the end of the retry duration r. Let S(r) = (es(t); t I r] U {e/(r)).
With the prior density function [i(W), the posterior density function following the
observation of e E S(r), denoted by [i( W] e), i = 1, 2, becomes
de

I wEi

(14)

‘i(w ’ e, = Jn g(e 1w)[i(W) dw ’

where g(e ] w) is the generalized conditional density function for the event e as in
[ 111,that is,
g(e ’ w, =

the density function of Tq at
Pr(e/(r))

t

;; ; 1 ;,:&and
.

t 5 r,

(15)

This posterior density function will become the prior density for the next retry.
Consequently, the system’s behavior is similar to a sequential decision procedure
which determines first a retry policy and then observes the resulting sample. The
procedure will be repeated with a new prior distribution which is determined on
the basis of the new sample observed and the old prior distribution. The decision
on retry and the sampling for fault characterization will continue as long as there
is an occurrence of fault.
The problem of selecting the optimal retry policy can also be treated as the
optimal stopping problem with continuous observations [ 131. Suppose an intermittent fault is detected again when the residual computation is x. Then, retry is
applied for a specified stopping time r. The task will be continued, without applying
recovery methods other than retry, if the fault disappears during the retry period r.
Otherwise, it has to be restarted from the beginning.’ The posterior density function
3For simplicity, it is assumed that there is only one alternative to the retry recovery, that is, restart.

62

Y.-H.

LEE AND

K. G. SHIN

of Wibecomes Ei(W ] e”(t)) or [i( Wl e/(r)), depending on the outcome of retry. The
cost of an observation is the amount of time used for monitoring the fault until its
disappearance (i.e., c(e”(t)) = t) or until the end of retry (i.e., c(e/(r)) = r). The
costs associated with the termination of retry are defined as the amount of time
necessary to complete the residual computation x as follows:
UX,
L(X,

i,

ti I e”(O)

r, [i 1

e/(r))

=

PZYX,

=

X0 +

Ei(w

I fW)),

ts.

The expected loss for the stopping time r: is the same as the Bayes risk defined
in eq. (13). According to the theory presented by Irle and Schmitz in [ 131, there
always exists an optimal stopping time, r2*E [0, m), satisfying eq. (13).
In the next section we solve the sequential decision problem using the backward
induction [ 1l] for testing hypotheses where the prior and posterior information is
described by discrete probability distributions. Note that the minimax method in
[2] cannot be used to solve eqs. (12) and (13), since the decision space-which
consists of all possible maximum retry durations-is neither countable nor finite.

5.2 OPTIMAL RETRY AND HYPOTHESES TESTING.
Suppose that there are a
primary and some alternative hypotheses concerning the active duration of an
intermittent fault. Consider the sequential testing of these hypotheses and the
simultaneous determination of the optimal retry policy; this is not difficult to solve
since both the prior and posterior probabilities lie in the same unit interval (0, I).
For given hypotheses, the initial prior distribution can be assumed to be equally
likely among the hypotheses.
To be more specific, consider an example in which the active duration of an
intermittent fault is assumed to be exponentially distributed with an unknown
parameter I*. Let there be two hypotheses on P, Ho and H1 for CL= CLO
and P = pl,
respectively, and let clo> pI. The uncertainty associated with these hypotheses can
be represented by the probability h of having p = h. We first determine the optimal
retry policy Vh E (0, 1). Then, we consider the problem of testing hypotheses as
well as estimating the expected sample size to reach a certain significance level
under the optimal retry policy.
Consider the optimal retry duration rz(x, h) upon detection of an old intermittent fault. In this case, we get the posterior probabilities given the events e”(t)
and ef(r), denoted by h(t) and Z(r), respectively, as follows:
h(t) =

hhoe-“’
hme-“’ + (1 - h)p,e-“1’ ’

‘(6 = he-,.,,’ :t”I

h)e-,‘,’ ’

where t 5 r,

(16)

(17)

As was discussed in Section 3.2, we can compute xz for a given pi, denoted
by $(pi) i = 0, 1, such that (i) r:(x, 1) = CQif x 5 xz(c(o), or 0 otherwise, and
(ii) r:(x, 0) = 03if x 5 x:(~,), or 0 otherwise. Since x:(h) > xf~,), r:(x, h) = a~if
x I x&),
and rz(x, h) = 0 if x 2 x:(m). Note that the above represents extreme
casesof retry, that is, retries of duration zero or infinite.
For the nonextreme case, that is, the case of x&)
< x < xF(po), let h* =
sup(h; r:(x, h) = 01. Since rT(x, 1) = 00and r:(x, 0) = 0 for x:(h) < x < x:(~,),
we get 0 5 h * < 1. For all h > h *, r-:(x, h) > 0, that is, retry must be applied upon
detection of a failure. Suppose retry has been applied for a small duration
6r < r:(x, h). Then, the memoryless property of the exponential distribution leads

Optimal Design and Useof Retry in Fault-Tolerant Computer Systems

63

to the following equation:
p;(x, h) = (1 - F;@r

l-r

+

Jo m I h)(t +

h(t)) dt.

~46,

(184

By letting 6r + 0 and changing variables, eq. (18a) becomes

d/C@,
h)
dh

(1W
On the other hand, p:(x, h) = ~0 + tSVh % h*. Using the same approach as in
Theorem 1, we can prove that h* satisfies the following equation:
P:(x, h*(O)) = x0 + ts -

1

h*j4, + (1 - h*)Pl -

(19)

From eq. (4) and the definition of pz in eq. (13), p:(x, h) is expressed as
x

p:(x, h) = i (1 - eeYX) + emyX

80

veYYp
f( y, h) dy.

(20)

With the initial conditions r:(x, 1) = 00,pT(x, h) and p:(x, h) for x I X&Q), and
eqs. (l&00),
we can calculate p:(x, h) k = 3, 4 Vx E (x:(p,), x:(m)) with the
following numerical algorithm:
Al. Seth= 1.
A2. Calculatep:(x, 1) and p:(x, 1) Vx E (x:(p,), x&&
A3. Calculate d&x, h)/dh using eq. (18) and p:(x, h - 6h) Vx E (x3rd, x%d). (Note
pT(x, h) and p:(x, h(O)) are both known.)
A4. Calculate p4+(x,h - ah) using eq. (20) Vx E (x&), x:(&). (Note p:(x, h - 6h) is
known Vx.)
A5. Set h = h - I% If h 5 0, terminate the algorithm.
A6. If p:(x, h(O)) < x,, + fs - (l/(h~ + (1 - h)p,)), go to A3. Otherwise, set p:(x, h - ah)
= ~0 + ts and go to A4.

From the test at A6, one can determine h* Vx E (x:(p,), x:(h)) so as to satisfy
eq. (19). Owing to the memoryless property of the exponential distribution,
r:(x, h) = 0 when h s h* or satisfies eq. (17) with E(r) = h* if h > h*. In
Figure 9, r: versus the prior probability h is plotted for various values of the
residual computation x. Intersections of the curves in Figure 9 with the horizontal
axis give the values of h* for different values of x.

Remark 1. In case the active duration of an intermittent fault has a general
distribution (instead of an exponential distribution), a differential equation similar
to eq. (18b) cannot be obtained. In such a case, the original integral equation of
PS(X,&), i.e., the combination of eqs. (3) and (12), has to be used instead.
From the foregoing discussion we can determine the optimal retry policy that is
based on the prior probabilty h. Under this optimal retry policy, we can also
determine trajectories of the posterior probabilities after a large number of occur-

64

Y.-H.

LEE AND

K. G. SHIN

z=O.48

FIG. 9. The optimal retry duration r:(x, h) versus the prior probability h with
w = 10, P, = 5, and Y = 5. x:(h) = 0.63. x&,) = 0.43.

rences and reappearances of intermittent faults have been observed. Let each retry
be numbered by a two-tuple (m, n,) on the basis of occurrences and reappearances
of intermittent faults. The (m, n,)th retry is used to recover from the mth
occurrence of fault in case of1 n, = 0 or from the n,th reappearance of the
mth intermittent fault if n, # 0. For the hypotheses Hi i = 0, 1, let h&n, n,)
represent the posterior probability after the (m, n,)th retry is applied. Also, let nk
be the total number of reappearances of an intermittent fault during the execution
of a task and hi(m) be the prior probability before the mth occurrence of fault,
which is equal to hi(m - 1, nk-, ) by definition. There are now two main problems
to be addressed: (i) Will hi(m) converge to either 0 or 1, namely to the true fault
characteristic as m + a~?;(ii) If converges, how fast will it converge? For convergence, we get the following theorem:
THEOREM
3. Let A4 = inf(m; hi(m) > 1 - t, or hi(m) < E), where 0 < E< 1. If
0 < hi(O) < 1, and ~0 + tS- (l/pi) > 0 for all hypothesesHi and all tasks, then
Pr(A4 < 00)= 1 and E[A4-J< 0~.
PROOF.
Let Si(m) = log(hi(m)/hj(m)) for j # i. Thus, M can be defined as
inf{m; 1Si(m)l > KJ, where K = log((1 - 6)/t). Let

where e(m, n,) is the event observed at the (m, n,)th retry and g(e 1pj) is the
generalized conditional density function defined in eq. (15). (When the retry
duration defined by a retry policy is zero, e(m, n,) is null and zi(m, n,) = 0. Also,

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

65

when n, = 0, the retry duration is rr since the fault type is not known at its first
occurrence.) From Bayes theorem, we have
&(m’)

= Si(m’ - 1) + ‘$ Zi(m’,
n=O
= :,

=

4

a

n)

Zi(WZ,n) + log z.
J

Let yi(m) = C$o zi(m, n) under the optimal retry policy. Si(m) becomes the
sum of independent random variables. After an event is observed, the expected
value of Zi(m, n,) is the Kullback-Leibler information number and is greater than
or equal to zero when Hi is true [6]. In this case, E[zi(m, n,)] = 0 if and only if
the prior probability before the (m, n,)th retry is 0 or 1. Since xo + ts - (l/pi) > 0
for all hypotheses Hi and all tasks executed, Pr(ri # 0) > 0 i = 1,2. Hence, Pr[ vi(m)
= 0] < 1 Vm c M. Following the proof in [24] that the sampling of a sequential
probability-ratio test (SPRT) terminates with probability 1, Pr(M < 00) = 1 and
E[M’l < 00are obtained. 0
Remark 2. Since the tasks affected by intermittent faults do not have to be
identical, the random variables yi( l), yi(2), are independently but not identically
distributed. Moreover, for a fixed m, z,(m, n,)‘s are dependent on one another
because the events observed are controlled by the retry duration that is in turn a
function of the moment of reappearance. However, all zi(m, n,) 2 0 when Hi is
true. The condition, xo + ts - (l/pi) > 0 for all hypotheses Hi and all tasks executed,
indicates that retry is always a useful recovery when an intermittent fault is detected.
In fact, this condition is not necessarily true for all tasks, but Theorem 3 holds as
long as Pr(rt > 0) # 0.
Theorem 3 shows that the expected number of faults observed-that makes the
posterior probability reach either e or 1 - +-is finite. This also holds for other
distributions and retry policies as long as rl # 0 and rz # 0 for some x. However,
it does not provide the average sample size, E[M ] Hi] that is necessary to reach
these termination boundaries K and -K. Also, one has to justify whether or not
the posterior probability at the termination implies the true fault characteristic. In
other words, it is important to know the error probability, Pr(Si(M) < -K ] Hi).
There are two difficult aspects in the evaluation of E[M ] Hi] and Pr(Si(M) <
-K] Hi); one is that yi(m)‘s are not identically distributed, and the other is the
nonexistence of closed-form solutions for both r: and r:. If the same task is
executed repeatedly under the condition x0 + t, - (l/pi) > 0 for all hypotheses,
then yi(m)‘s become independently and identically distributed. Assume further
that initially, both hypotheses are equally likely, that is, ho(O)= h,(O) = 0.5. Using
the characteristics of SPRT in [ 111,the error probability is approximated by
-K
1
Pr(Si(M) C -K] Hi) x eK-- ee-K

=e

-K

.

Even if the same task is executed repeatedly, it is difficult to obtain an exact
solution for E[ yi] because of the dependency between the optimal retry durations
and the observed samples of the active durations. This fact in turn makes it
impossible to obtain the exact solution of E[M ] Hi]. Owing to the above difliculties,
in what follows, we shall derive upper and lower bounds of E[M ] Hi] instead of an
exact solution.
Suppose there are two retry policies R” and R’ with the retry durations (r?, r!)
and (rt , r:), respectively. r?(x, h) and rt (x, h) are defined the same as rr(x, h).

66

Y.-H.

LEE AND

K. G. SHIN

ri@, h) is equal to m if x s X:(pj) and 0 otherwise forj = 0, 1. Let y:(m) and Mj
be C”h
n) and the number of faults observed to reach the termination
n=O Zi (my
boundaries under the retry policy Rj, respectively. Then, (i) Pr(Mj < 00) = 1 and
E[Mj] < CQ,and (ii) E[ yf ] I E[ yi] I E[ yp]. (Note that the indices m are omitted
because of the distributions being identical.) Once E[ y! ] Hi] j = 0, 1 is calculated
as in the Appendix 2, the expected sample size to reach the boundaries 1 - c and
c is bounded by
E[Mo ] Hi] 5 E[M] Hi] I E[M’ ] Hi]
where
E[M’]Hi]

x

K

E[Y: I HiI’

j=o,

1

(see DeGroot [ 1l] for more on this).
The above equations give the error probability and the bounds of the expected
sample size when a certain level of signilicance is to be achieved. These bounds of
E[M] Hi] become tight when the difference between ccoand ~1~is small. Of course,
the expected sample size under the optimal retry policy is larger than that for the
case when the complete information about active duration is observed, that is,
r1 = r2 = co.
Thus far, we have discussed solutions to the problem of sequential retry decision
and hypotheses testing only for the case of exponentially distributed durations.
Notice, however, that (i) the same method, with little modification, can be applied
to the caseswith any other kind of distributions, and (ii) Theorem 3 holds as long
as Pr[y&z) = 0] < 1. Moreover, the method can be extended to the testing of
multiple alternative hypotheses by specifying the prior and posterior probabilities
as a vector, each element of which represents the probability that the corresponding
hypothesis is true.

6. Conclusion
In this paper, we have investigated optimal retry policies and demonstrated the use
of retry to estimate the unknown fault characteristics. Although the data obtaining
from retries are censored, they are the only significant means of monitoring the
fault characteristics. By combining the estimation of fault characteristics and
the decision of retry, the computer system performs an adaptive optimization of
task completion times.
In the discussion of retry policies, retries are assumed to be continuously applied.
In fact, the retry durations should be discrete since the time required for repeated
execution of an operation cannot be cascaded into a single continuous duration.
Since the expected risk is a continuous function of the retry duration, it is not
difficult to find the optimal retry policy that is specified as a number of retry
attempts.
As was pointed out in the discussion of the expected sample size for reaching a
certain level of confidence in hypothesis testing, the test under the optimal retry
policy turns out to be inefficient in the sense of maximizing the information
observed. This is due to the fact that the optimal retry policy is defined to minimize
the total completion time of the task affected by the occurrence of fault. Thus, the
retry policy is a local optimum, that is, “optimal” only for the task involved.
Clearly, the retry policy that gives complete maximum information should have
infinite retry durations, although such a retry policy is totally unacceptable in

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

67

reality. It would be interesting to examine the trade-off between the two extreme
objectives, that is, minimizing the local task completion time and maximizing the
information to be collected. This problem can be formulated as the minimization
of the asymptotically accumulated risk, lim,,,( l/m) CFl E[~k(x, C{)], where j
and m are used to number the successive retries and C{ is the measured fault
characteristic at the jth retry. It also indicates that the global optimal retry policy
should collect more information (it is definitely not complete though) from the
beginning to speed up the estimation of the true fault characteristics and then
implement the local optimal retry policy once the true characteristics are obtained.
Another important aspect is the choice of an accurate model for the fault
behavior. As was discussed in Sections 4 and 5, the optimal retry policy and the
measurement of the fault characteristics are dependent on the family of density
functions that are initially selected. The suitability of chosen models can be
validated through goodness-of-fit tests, for example, chi-square goodness-of-fit.
Although sometimes the expected task completion time may not be minimized
because of the poor choice of model, the information collected via retries can still
be used to check the suitability of the model. Thus, after a sufficiently large number
of samples have been obtained, it is possible to select an appropriate form of
density function and then achieve the minimum task completion time. The other
approach is to begin with hypotheses of various forms of density functions. As
sampling progresses,the parameters associated with the density function forms are
estimated and then the hypotheses are tested.
The work presented in this paper is to incorporate the capability of on-line
estimation (of the fault characteristics) and decision (on optimal retry policies) into
the computer system. The results are a self-adjustable (thus intelligent) system and
a powerful measurement of the fault characteristics. This idea can also be extended
to other applications, for example, the measurement of program behavior and the
simultaneous decision of system configuration or scheduling. Such extensions
would be significant contributions toward the construction of highly intelligent
computer systems.
Appendix A. Bounds of V, (x, Cf, R*)
Clearly, x is a lower bound of V,(x, Cf, R*). To find an upper bound of
V,(x, C,, R*), consider a policy R ’ under which only restart recovery is used
upon detection of a failure. Since rl = 0 under R ‘, we get
VI&, C,, R*) = V,(x, C’, R’) 5 x + F/(x)V~:(xo, C,, R’)

These bounds are very tight. For example, given that the fault occurrence process
is exponentially distributed with MTBF = one week and the task execution
time = one minute, the largest difference between the upper and lower bounds is
less than 0.0001 minute.
Appendix B. The Expression of E[ y{ 1Hi]
The retry duration ri under the retry policy Rj is equal to CQif x I XT(pj), and 0
otherwise. Thus, the complete information will be gathered if an old intermittent
fault is detected again at x 5 x?(gj ) and no information will be obtained if it is
detected at x > X;(pj). Hence, if the retry for a newly detected intermittent fault
when the residual computation is x succeeds, we expect to collect information

68

Y.-H. LEE AND K. G. SHIN

from the successive retries before the task completion as follows:

=

v log; _ cci
b4 )
4

if

exp[-v(x

otherwise.

X

Q X&j),

J

-

Xf(/Jj))]~XT(Pj)

Let the maximum retry duration for a newly detected fault be r:(x) when the
residual computation is x. Also, let #(xd) be the density function of the detection
time of a new intermittent fault, xd, given that it is detected during the task
execution. Then, +(xd) = Xiexp[-Xixd]/( 1 - exp[-hi-]),
where Xi = PiX. Thus,
we have E [ v{ ] Hi] a~ follows:
E[yj] Hi] =

s o* ~60 - x)exP[-clirl(x)lZ~(rt(x))
r:(x)

x0
+

ss0

fh

0

MW(-ptt)$Go

- x)(Zi(t) + E[Zz 1x]) dt G!X,

where Z{(r) = -(pi - pj)r is the information collected from an unsuccessful retry
of the maximum retry duration r and Zi (r) = log(pi/pj ) - [(pi - pj )/pi]r is the
resulting information when the retry succeedsafter the duration r.
ACKNOWLEDGMENTS.
The authors are grateful to C. M. Krishna and anonymous
referees for their careful comments on this paper, and to both R. Butler and
M. Holt of NASA Langley Research Center for their technical assistance.
REFERENCES
1. BALL, M., AND HARDIE, F. Effects and detection of intermittent failures in digital systems. In
Proceedings ofAFIPS Fall Joint Computer Conference, vol. 35. AFIPS Press, Reston, Va., 1969,
pp. 329-335.
2. BERGER,I. 0. Statistical Decision Theory. Springer-Verlag, New York, 1980.
3. BOONE,L. A., LIEBERGOT,H. L., AND SEDMAK,R. M. Availability, reliability, and maintainability
aspects of the SPERRY UNIVAC 1100/60. In Proceedings of the 10th Annual International
Symposium on Fault-Tolerant Computing (Kyoto, Japan). IEEE, New York, 1980, pp. 3-9.
4. CARTER,W. C. A short survey of some aspects of hardware design techniques for fault tolerance.
IBM Research Rep. RC-10811. IBM, Yorktown Heights, N.Y., 1984.
5. CARTER,W. C., Pwrzc~u, G. R., WADIA, A. B., BOURCIUS,W. G., JESSEP,D. C., HSIEH, E. P.,
AND TAN, C. J. Cost effectiveness of self checking computer design. In Proceedings of the 7th
Annual International Symposium on Fault-Tolerant Computing (Los Angeles, Calif.). IEEE New
York,1977, pp. 117-123.
6. CHERNOFF,H. Sequential Analysis and Optimal Design. SIAM, Philadelphia, Pa., 1972.
7. CINLAR, E. Introduction to Stochastic Processes.Prentice-Hall, New York, 1975.
8. COHEN, A. C. Progressively censored samples in life testing. Technometrics 5, 3 (Aug. 1963),
327-339.
9. COHEN,A. C. Multi-eensored sampling in the three parameter Weibull distribution. Technometrics
I7,3 (Aug. 1975), 347-351.
10. COHEN, A. C. Progressively censored sampling in the three-parameter gamma distribution.
Technometrics 19, 3 (Aug. 1977), 333-340.
11. DEGROOT,M. H. Optimal Statistical Decision. McGraw-Hill, New York, 1970.
12. DROULFITE, D. L. Recovery through programming system/360-System/370. In Proceedings
of the AFIPS Spring Computer Conference, vol. 38. AFIPS Press, Reston, Va., 1971, pp. 467-476.

Optimal Design and Use of Retry in Fault-Tolerant Computer Systems

69

13. IRLE, A., AND SCHMITZ, N. Decision theory for continuous observations I: Bayes solutions. In
Transactionsof the 7th PragueConferenceon Information Theory, Statistical DecisionFunctions
and RandomProcesses.1974, pp. 209-22 1.
14. KOREN,I., AND Su, S. Y. H. Reliability analysis of N-modular redundancy systems with intermittent and permanent faults. IEEE Trans. Comput.28,7 (July 1979) 5 14-520.
15. LEHMANN,E. L. TestingStatistical Hypotheses.Wiley, New York, 1959.
16. LEMON,G. H. Maximum likelihood estimation for the three parameter Weibull distribution based
on censored samples. Technometrics17, 2, 1975,247-254.
17. MAESTRI,G. H. The Retryable Processor.In ProceedingsofAFIPS Fall Joint ComputerConference
vol. 41. AFIPS Press, Reston, Va., 1972, pp. 273-277.
18. Ross, S. M. StochasticProcesses.
Wiley, New York, 1983.
19. SHEDLETSKY,
J. J. The error latency of a fault in a sequential digital circuit. IEEE Trans.Comput.
C-25,6 (June 1976), 655-659.
20. SHEDL~KY, J. J., AND MCCLUSKY,E. J. The error latency of a fault in a combinational digital
circuit. In Proceedingsof the 5th Symposiumon Fault-Tolerant Computing(Paris, France). IEEE,
New York, 1975, pp. 210-214.
21. SHIN, K. G., AND LEE, Y. H. Error detection process: Model, design, and its impact on computer
performance. IEEE Trans. Comput.C-33,6 (June 1984), 529-540.
22. SHIN, K. G., AND LEE,Y. H. Measurement and application of fault latency. IEEE Trans.Comput.
C-35,4 (Apr. 1986), 370-375.
23. SIEWIOREK,D. P., AND SWARZ, R. S. The Theoryand Practiceof ReliableSystemDesign.Digital
Press, Educational Services, Digital Equipment Corporation, Bedford, Mass., 1982.
24. STEIN, C. A note on cumulative sums. Ann. Math. Stat. I7 ( 1946), 489-499.
25. STIFFLER,J. J., AND BRYANT L. A. CARE III phase report-Mathematical description. NASA
Rep. 3566. NASA, Washington, D.C., Nov. 1982.
26. TASAR,O., AND TASAR, V. A study of intermittent faults in digital computers. In Proceedingsof
AFIPS National ComputerConference,vol. 46. AFIPS Press, Reston, Va., 1977, pp. 807-8 11.
27. VAN ZWET,W. R. Bias in estimation from type I censored samples. Statist.Neerlandica20 (1966),
143-148.
28. WINGO, D. R. Solution of the three-parameter Weibull equations by constrained modified
quasilinearization (progressively censored samples). IEEE Trans.Reliability R-22, 2 (June 1973),

96-102.
RECEIVED MAY 1984; REVISED MARCH

1985;JULY 1986; ACCEPTED JANUARY 1987

Journal

of the Ascciation

for Computing

Machinery,

Vol. 35, No. I, January

1988.

2014 IEEE 28th International Parallel & Distributed Processing Symposium

Efficient Data Race Detection for C/C++ Programs Using Dynamic Granularity
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
Arizona State University
Tempe, AZ, 85281
ywsong@asu.edu, yhlee@asu.edu

fall into two categories: LockSet algorithms [23, 27] and
happens-before algorithms [7, 19, 21, 22, 25]. In Eraser’s
LockSet algorithm [23], data races are reported when shared
variable accesses violate a specified locking discipline, i.e.,
the variable is not protected by the same lock consistently.
For a given execution path, checking a lock discipline
enables Eraser to detect potential data races as well as ones
that actually happened in the program execution. Eraser may
report many false alarms which hinder developers’ focus on
fixing real problems. Eraser may also report false alarms by
not being able to recognize synchronization idioms, e.g., a
semaphore implementation using mutex locks.
Happens-before detectors are based on Lamport’s
happens-before relation [12] and don’t report false alarms as
the approach only checks any happens-before relation that
actually occurs during the given execution paths. Based on
the execution of a multithreaded program, a partial ordering
of memory and synchronization operations can be defined. A
pair of accesses to the same variable is concurrent when
neither of the accesses happens before the other. The
happens-before relation is realized by the use of vector clock
[5] and there are largely two happens-before detection
methods based on how shared accesses are represented and
compared for the detection.
In the first method [21, 22, 25], a segment is defined as a
code block between two successive synchronization
operations and shared memory accesses are collected in a
bitmap for each segment. In each thread a vector clock is
collected to uncover any concurrent segments of running
threads. If two concurrent segments contain shared memory
accesses, the accesses are reported as data races. This
method may incur a significant overhead in time despite of
several optimization techniques such as clock snooping and
merging segments [21, 22] as it requires set operations with
the collected shared memory accesses.
In the second method [7, 19], other than a vector clock
for each thread, each shared variable has two vector clocks
for reads and writes which record access history of the
variable by every thread. When a thread reads from a shared
variable, the write vector clock of the shared variable is
compared with the vector clock of the thread. A write-read
data race is reported if the vector clock comparison reveals
that the write and the read are not ordered by the happensbefore relation. A similar protocol can be performed for a
write access. Having vector clocks for each shared variable
can result in huge memory consumption. However, for
applications developed in object-oriented languages, the

Abstract—To detect races precisely without false alarms, vector
clock based race detectors can be applied if the overhead in
time and space can be contained. This is indeed the case for the
applications developed in object-oriented programming
language where objects can be used as detection units. On the
other hand, embedded applications, often written in C/C++,
necessitate the use of fine-grained detection approaches that
lead to significant execution overhead. In this paper, we
present a dynamic granularity algorithm for vector clock
based data race detectors. The algorithm exploits the fact that
neighboring memory locations tend to be accessed together and
can share the same vector clock archiving dynamic granularity
of detection. The algorithm is implemented on top of
FastTrack and uses Intel PIN tool for dynamic binary
instrumentation. Experimental results on benchmarks show
that, on average, the race detection tool using the dynamic
granularity algorithm is 43% faster than the FastTrack with
byte granularity and is with 60% less memory usage.
Comparison with existing industrial tools, Valgrind DRD and
Intel Inspector XE, also suggests that the proposed dynamic
granularity approach is very viable.
Keywords-Race detection; Concurrent bug; Multithreaded
programs

I.

INTRODUCTION

Most embedded applications are constructed with
multiple threads to handle concurrent events. Threads are
synchronized for proper sharing of resource and data.
Unfortunately, it is easy to misuse synchronization
operations in multithreaded programming and threads may
be vulnerable to race conditions and deadlocks. Ever
increasing demands of multi-core processors aggravate this
problem not only for embedded applications but for general
desktop applications.
A data race occurs when a memory location is accessed
concurrently by two different threads and at least one of the
accesses is a write. Data races are hard to reproduce, find,
and fix since a data race may only occur in a particular
execution of the program and the race does not necessarily
always cause observable errors in the program execution.
Over the past few years, several techniques have been
developed to detect data races. Static analysis techniques [8,
20, 26] consider all execution paths for possible data races
providing a better detection coverage than dynamic
techniques but they suffer from excessive number of false
alarms. On the other hands, dynamic techniques detect data
races when execution paths are exercised and they largely
1530-2075/14 $31.00 © 2014 IEEE
DOI 10.1109/IPDPS.2014.76

679

T0

memory requirement may be tolerable as large detection
units such as fields or objects can be used.
Data race detection for embedded applications which are
mostly written in C/C++ needs to consider fine granularity of
data access (e.g., byte). It would seem to be a better choice to
use the first happens-before detection method since having a
vector clock for every shared read or write byte may make
the detection infeasible. On the other hand, as shown in
FastTrack [7], the second method can reduce the space and
time overheads of vector clocks from O(n) (where n is the
number of threads) to nearly O(1) with no loss in detection
precision. While it may become feasible to detect races in
C/C++ programs, the overheads using the FastTrack with
fine granularity is still high for C/C++ programs, as
illustrated in our experiment results.
In this paper, we present a dynamic granularity algorithm
for vector clock based race detection. The detection
granularity starts from byte and is dynamically adjusted as
shared memory locations are accessed. A large detection
granularity is adopted when neighboring bytes have the same
vector clock. Thus, instead of multiple copies, a single copy
of vector clock is shared among these neighboring bytes.
Sharing the same vector clock among neighboring memory
locations become feasible since (1) neighboring memory
locations belonging to array or struct tend to be accessed
together, (2) data structures are often accessed together
during initialization even if they are separately protected
afterward, and (3) some groups of shared memory locations
are accessed only for one code block.
In the algorithm, a state machine is associated with a
vector clock for read or write of a memory location and the
state can be Init, Shared, Private, or Race. To minimize
analysis overhead, the sharing decision for each read or write
location is made at most twice for the lifetime of the
location. Peak memory consumption is further reduced by
temporarily sharing vector clock at the Init state. In addition,
the possibility of false alarms caused by sharing vector clock
is minimized as new sharing decision is made after the Init
state.
We have developed a race detector based on FastTrack
for C/C++ program and the dynamic granularity algorithm is
implemented on top of the FastTrack implementation. Our
experimental results on several benchmark programs show
that the race detector using our dynamic granularity provides
43% speedup and 60% less memory over the FastTrack with
byte granularity. Also we provide case studies on two
popular data race detection tools: Valgrind DRD [16, 25] and
Intel Inspector XE [11]. Our dynamic-granularity is about
2.2x and 1.4x faster than Valgrind DRD and Inspector XE,
and consumes about 2.8x less memory than Inspector XE.
The rest of the paper is organized as follows. In the
following section, a brief review of vector clock based race
detectors is presented. Section III presents the proposed
dynamic granularity algorithm. The implementation of data
race detector using dynamic granularity are explained in
Section IV. Section V shows experimental results on the tool
with FastTrack as well as comparisons with Valgrind DRD
and Intel Inspector XE. A concise survey of related works is

<1, 1>
lock(s)
<1, 1 >
unlock(s)
<2, 1>

T1

Wx

Ls

<1, 1>

<0, 0>

<0, 0>

write(x)

<0, 1>
<2, 1>

lock(s)
<2, 1>
write(x)

<2, 1>

Figure 1. An example execution of DJIT+ is shown. T0 and T1 are
vector clocks of thread 0 and thread 1, respectively. Wx and Ls are
vector clocks for write x and lock s, respectively. Solid arrows show
happens-before relations and dotted arrows indicate vector clock
updates by the operations.

described in Section VI and, in Section VII, we conclude the
paper with future work.
II.

VECTOR CLOCK BASED RACE DETECTORS

A. Preliminaries
The happens-before relation [12] over the set of memory
and synchronization operations, denoted “”, is the smallest
relation satisfying,
• Program order: If a and b are in the same thread and a
occurs before b, then a  b.
• Locking1: If a is a lock release and b is the immediately
successive lock acquire on the same lock, then a  b.
• Transitivity: If a  b and b  c, then a  c.
Two operations are concurrent if they are not ordered by
the happens-before relation. A data race occurs when two
memory operations are concurrent and at least one of them is
a write.
A vector clock [5] is a vector of logical clocks for all
threads and used to realize the happens-before relation. A
vector clock is indexed with thread ids. For instance, in
Figure 1, T1[0] gives the logical clock of thread 0 in the
thread 1’s vector clock and after the lock(s) T1[0] = 2. The
execution of a thread is logically divided into code blocks by
synchronization operations and the logical clock for the
thread is incremented in every new code block. The logical
clocks of other threads in a thread’s vector clock are updated
through synchronization operations. Similarly, the access
history of a shared memory location is recorded in vector
clocks for the location. Then, the happens-before relation for
the program execution is realized by the combination of the
vector clocks of threads and shared memory locations.
B. DJIT+
DJIT+ [19] defines the execution of a thread as a
sequence of epochs2 and a new epoch starts from every lock
release operation. DJIT+ detects only the first race for each
1

Other synchronizations such as fork-join can be similarly defined. In this
paper, we discuss synchronizations only with lock operations for clarity.
2
It is defined as timeframes in the paper. But we use epochs for the
consistency of discussion.

680

memory location. For consecutive reads of a memory
location in the same epoch, it is sufficient to check only the
first read for the detection of the first race. This property is
also true for consecutive write operations.
A thread i has a vector clock Ti in which Ti[i] (i.e., its’
own clock) is incremented upon every new epoch. A vector
clock Ls of a synchronization object s is updated when thread
i performs a lock release operation on s and is set to the
element-wise maximum of Ls and Ti. Upon a lock acquire
operation of s by thread i, Ti is updated as the element-wise
maximum of Ls and Ti. By the vector clock updates, happensbefore relations for the program execution are constructed
and a thread’s logical clock is updated. As synchronization
operations play a role in conveying a thread’s clock to other
threads, the thread’s vector clock is known to other threads.
For a memory location x, the access history of read and
write is represented with vector clocks Rx and Wx
respectively. Upon the first read of x in an epoch at thread i,

III.

DYNAMIC GRANULARITY ALGORITHM

FastTrack is a fast and space-efficient race-detection tool
but it still needs to keep vector clocks for each memory
location. This is not problematic for object-oriented
programming languages since detection unit can be either a
field or an object. For C/C++ programs, it is not easy to
detect data structure boundaries (e.g., dynamically allocated
struct or array) and moreover data are often protected in fine
grained (e.g., a byte or a word). A simple way to rectify the
problem is to use a fixed granularity. However using a fixed
granularity (a word) would produce a large number of false
alarms and does not help reducing the overheads for many
cases as shown in our evaluation results.
In this section, we present a dynamic granularity
algorithm which enables vector clock based race detectors to
use detection granularity as large as possible with minimal
false alarms. The algorithm is described on the assumption of
using DJIT+ or FastTrack detectors; a thread’s execution is
defined as a sequence of epochs; and for consecutive
accesses of a memory location in an epoch, only the first
read and write are checked. In the description, two vector
clocks are the same when they are the same size and their
contents are of equal value, and both a vector clock and an
epoch representation are referred to as a vector clock.
The dynamic granularity is realized by sharing vector
clock with neighboring memory locations. The sharing
heuristic is based on the following observations:
1. Neighboring memory locations (e.g., array, struct) tend
to be accessed together whether the locations have data
races or not. Hence, they can have the same vector
clock.
2. At initialization, a data structure is often accessed in its
entirety, e.g., zero-out an array, even if its elements are
protected separately afterward.
3. There are groups of memory locations that are accessed
only in one epoch for the entire lifetime of the location,
e.g., dynamically allocated memory locations that are
used temporarily.
With these observations, the dynamic granularity
algorithm is realized with a vector clock state machine that is
described in the following subsection.

1) A write-read data race is reported if there is another
thread j whose write to x is not known to thread i,
i.e.,Wx[j]  Ti[j]. If there was a synchronization from
thread j to thread i between the write and the read, then
the accesses should have been ordered and Wx[j] < Ti[j].
2) Thread i updates Rx such that, Rx[i] = Ti[i].
A similar protocol can be applied to write operations.
Figure 1 shows an example of how DJIT+ detects a data
race. In the example, the write on x in thread 0 is a data race
since Wx[1]  T0[1].
By the property of checking only the first read and write
in an epoch, run-time performance can be significantly
improved. However, there still exists significant overheads in
time and space for maintaining the vector clocks of shared
memory locations.
C. FastTrack
FastTrack [7] is based on DJIT+ and provides a
significant performance enhancement over DJIT+ with the
same detection precision as DJIT+. FastTrack exploits the
insight that, in most cases the last access of a memory
location can provide enough information for detecting data
races instead of using the full vector clock representation. An
epoch representation denotes the last access of a memory
location. If the last access was by a thread t at logical clock c,
then the epoch is denoted c@t using only two scalars. For all
writes to a memory location, the epoch representation can be
used instead of the full vector clock because all writes to the
location are totally ordered by the happens-before relation
before the first race on the location. This leads to a reduction
in time and space overheads from O(n) (where n is the
number of threads in the execution) to O(1). For read
operations, the epoch representation cannot be used all the
time since read operations can be performed without locking
(i.e., read shared). Thus, read vector clock, Rx, is replaced
with an adaptive representation which uses a full vector
clock only when the read is shared with other threads without
protection. Based on the adaptive representation, the
overhead for reads can be reduced from O(n) to nearly O(1).

A. Vector Clock State Machine
For a memory location, we maintain a read location and a
write location separately. Hence, only the same access type
(read or write) of vector clocks can be shared. Let L be a
location which can be either a read or write location. When L
is accessed for the first time, a vector clock is created for it.
The sharing state of each location is maintained by a state
machine attached to its vector clock as shown in Figure 2.
The state machine basically has four states. In the first epoch
access, the vector clock is temporarily shared with its
neighbor if the neighbor has the same vector clock. When the
second epoch access begins at a location, the shared vector
clock at the location is split and new sharing decision is
made.

681

First Access to L

(Same VC with a neighbor
which is in Init)
and no data race on L

Init

1st EpochShared

L is initiated and L has the same vector clock as L. The
rationale behind the temporary sharing is that there could be
many memory locations that are accessed only in one epoch.
Examples include dynamically allocated memory or groups
of memory locations in a big data structure that are used only
in one epoch. As our experimental results show, having this
Init state saves a considerable amount of memory for some
applications. Upon the next epoch access, L has its own
vector clock and state, and the new sharing decision is made
for the rest lifetime of the location L.
Shared: On the second epoch access of L, if there is no data
race on L (and no read-read conflict for a read location) and
there exists a neighbor that has the same vector clock as L
and is in either Shared or Private, the location L shares its
vector clock with the neighbor. Also, the state of L can
transition from Private to Shared when L becomes a
neighbor of another location L that has the same vector
clock as L.
Private: When there is no neighbor that has the same vector
clock as L, the state of L becomes Private on the second
epoch access.
Race: On a data race, the state of L becomes Race. If there
are memory locations sharing the same vector clock with L,
the sharing is terminated and each of these locations become
Race and is assigned with a private vector clock.

1stEpoch

¬ (Same VC with any neighbor
which is in Init) and no data
race on L
Shared by another
location
1st EpochData Race
Private

Second Epoch Access

(Same VC with a neighbor
which is in Private or Shared)
and no data race on L

2ndEpoch

¬ (Same VC with any neighbor
which is in Private or Shared)
and no data race on L

Shared by another location

Shared
No data race on L

Private
Data Race
Data Race

Data Race

No data race on L
and not shared by
another location

Race
All subsequent
accesses

Figure 2. Vector clock state machine for each read or write location.

B. Dynamic Granularity
The dynamic granularity is achieved by sharing vector
clocks with neighboring address locations. The detection
starts with byte granularity for every location and the
granularity is increased as more neighboring locations share
the same vector clock. The vector comparison to determine
vector clock sharing can be an expensive operation.
However, following the vector clock state machine, there can
be at most two sharing decisions for the lifetime of a
memory location and it requires only O(1) time overhead in
most cases when the FastTrack algorithm is used. In fact, we
can have a significant performance enhancement by the use
of dynamic granularity since, as we change to a large
granularity, multiple accesses may be treated as the same
epoch accesses.

A neighbor of L is a memory location adjacent to L that
is considered to share potentially a vector clock with L. A
location L can have two neighbors that one is located left (a
predecessor of L) and the other is located right (a successor
of L). During the first epoch, the neighbors are the nearest
predecessor and successor that have valid vector clocks. A
new access to a location L initiates a vector clock in the Init
state. This vector clock can be shared with L’s neighbors if
they have the same vector clock and are in the Init state as
well. For an access to a location L in the 2nd epoch, the
neighbors are at locations L-size and L+size where size is the
data size of the access. As long as the neighbors are not in
the Init or Race state, we compare the vector clock of L with
those of its neighbors. If the vector clocks are equal, the state
is set to Shared. Otherwise, it is Private.
Each vector clock can be in one of the following 4 states:
Init: When L is accessed first time, its vector clock is
initiated and is set to this state until the next epoch access.
This Init state is intended to approximate the initialization
process. Note that elements of a data structure may be
initialized together and have the same vector clock during
the first epoch. However, starting from the 2nd epoch, the
elements may be accessed separately and have their own
private vector clocks.
While in the Init state, L can be in the 1st-Epoch-Shared
sub-state if one of the neighbors has the same vector clock
and are in the Init state. Thus, L shares temporarily its vector
clock with its neighbors during the first epoch. When there is
no neighbor that has the same vector clock as the location L,
the state of L becomes 1st-Epoch-Private. The state of L can
transition to 1st-Epoch-Shared when a new neighbor location

IV.

IMPLEMENTATION

We have implemented the FastTrack algorithm for data
race detection of C/C++ programs with fixed (byte and
word) and dynamic granularities. Intel PIN tool 2.11 [13] is
used for dynamic instrumentation of the programs.
A. Instrumentation
To trace all shared memory accesses, every data access
operation is instrumented. If an instruction accesses nonshared memory (e.g., stack), the instrumentation routine
returns immediately. Figure 3 shows pseudocode for
memory read instructions. Memory write can be similarly
described and we omit the FastTrack algorithm for clarity.
When an access is not the first read or write in an epoch,
vector clock updates and data race checking on that access
can be skipped according to the FastTrack algorithm.

682

read_address = 0xba123411
key = (read_address & 0xffffff80)
= 0xba123400

void memoryRead(uint addr, uint size, uint tid)
{
if (nonSharedRead(addr) || sameEpoch(tid, addr))
return;

…
VC
State

Location L = findReadAccess(addr);
if (!L) {
// The first access of addr
L = insertRead(addr, size);
shareFirstEpoch(L, addr, size);
// Temporary sharing
L state = Init;
} else if (Lstate==Init) {
// Second epoch access
split(L, addr, size);
// Split for new sharing
shareSecondEpoch(L, addr, size); // New sharing decision
if (Lcount>1)
Lstate = Shared;
else
Lstate = Private;
}

…

0xba123400

…
11

128 addresses
Upon byte
granularity access
…

…
8 12 16

//if race found on addr, split all vectors sharing with L
// and set states of locations to Race
if (raceFound(addr))
splitAndSetRace(L, addr);

Hash chain entry with
32 pointers (addresses)

…

…
11 12 13

Hash chain entry with
128 pointers (addresses)

Figure 4. Top: A separate chaining hash table implementation.
Each entry can contain m addresses (shown a case m=128).
Bottom: The size of indexing array in a hash entry is changed from
m/4 to m when byte granularity access begins in the entry.

//remember access L into bitmap for this thread
//the bitmap is reset at the next epoch of this thread
insertEpochAccess(tid, addr);
}

V.
Figure 3. Instrumentation code for memory read.

EVALUATION

In this section, we present the effectiveness of our
dynamic granularity algorithm. First, we show our
experimental results of the FastTrack with fixed (byte and
word) and dynamic granularities. Second, analysis results on
the state machine are presented. Lastly, performance
measures of two popular data race detection tools, Valgrind
DRD [16, 25] and Intel Inspector XE [11], are compared
with the FastTrack using dynamic granularity. All
experiments were performed on Ubuntu 12.04 with kernel
version 3.2.0 and Intel Core Duo CPU with 4 GB of RAM.
All experiments were run with 11 benchmarks: 8 from
the PARSEC-2.1 benchmark suite [1] that are implemented
with the POSIX thread library and 3 from popular
multithreaded applications: FFmpeg [4], a multimedia
encoder/decoder; pbzip2 [9], a parallel version of bzip2; and
hmmsearch [6], a sequence search tool in bioinformatics. For
input sets of the PARSEC benchmark programs, the
simsmall input set is used for raytrace while the simlarge is
used for the rest 7 programs. Inputs for the other three
programs are chosen to have similar run times as the
PARSEC benchmark programs.

Checking the same epoch access can be costly since looking
up a vector clock from a global data structure requires
synchronization of threads. To reduce overhead, a per-thread
bitmap is implemented. When the first access is made in an
epoch, the access is set in the bitmap and the bitmap is reset
for every lock release operation. Because the bitmap is a
thread local data structure, checking the same epoch is more
efficient than looking up a global data structure.
The mechanism for dynamic granularity is invoked at
first two epochs for each read or write location. Thus, the
overhead can be negligible. Also it will be straightforward to
apply dynamic granularity into existing data race detection
tools.
B. Indexing Structure
To find the vector clock of each read or write location, a
chained hash table is implemented as shown in Figure 4. For
efficient sequential processing such as deleting vector clock
entries from free() and vector clock sharing process, each
hash chain entry has an indexing array which can contain up
to m pointers for vector clock entries. For a 32-bit address,
the upper address (upper 32-log2m bits of the address) is
hashed into the table to locate the corresponding hash entry.
The vector clock entry for the address is indexed using the
lower address (lower log2m bits of the address).
The size of the indexing array in the hash entry changes
according to memory access patterns, thus saving memory
on the indexing array. When a new hash entry is created, it
starts with an array of m/4 pointers since the most common
access pattern is word access. When a byte access (i.e., the
address is not word or half-word aligned) is detected, the
array is expended to have m pointers.

A. Performance and Detection Precision
Table 1 shows overall experimental results of the
FastTrack with the three different granularities. “Total shared
accesses” column shows the total number of shared reads
and writes during each benchmark program run. “Max. # of
vectors in byte granularity” column indicates the maximum
number of vector clocks present for the execution of each
program in byte granularity run. These two columns,
combined with the number of threads, give us a general idea
of the instrumentation overhead in the detection.
“Slowdown” and “Memory overhead” columns report
runtime and memory overhead of each detection mechanism

683

Dynamic
granularity

Byte
granularity

Word
granularity

Dynamic
granularity

288
146
248
170
49
104
2682
30
95
67
23

Word
granularity

6.1
6.7
2.0
9.5
2.2
6.5
7.7
3.8
3.0
5.7
26.6

Byte
granularity

2
11
3
3
256
3
7
5
9
6
3

# of Detected Data Races

Dynamic
granularity

Base
memory(MB)

93,930,447
83,678,104
11,220,394
3,291,927
12,550,683
7,141,372
9,208,539
2,277,958
7,195,586
8,842,583
961,831

Memory Overhead

Word
granularity

Base time
(sec)

8033
3856
2443
18
3392
359
10003
8030
5790
7239
38050

Slowdown

Byte
granularity

# of threads

facesim
ferret
fluidanimate
raytrace
x264
canneal
Dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Max. # of
vectors in
byte
granularity

PARSEC

Benchmark
Program

Total shared
accesses
(million)

Table 1. Overall experimental results.

138
65
87
27
75
13
152
245
121
64
84
97

138
57
87
27
55
13
76
245
106
49
83
85

102
52
81
27
64
13
85
137
109
39
45
68

8.8
16.6
2.6
2.1
20.8
5.3
1.0
4.8
4.0
5.4
4.9
6.9

8.8
11.7
2.6
2.1
9.6
5.3
1.0
4.8
3.0
4.2
4.9
5.3

4.6
8.9
2.2
2.0
9.0
5.1
1.0
3.7
3.1
3.4
4.3
4.3

8909
2
1
13
1300
0
0
1053
1
0
1

8909
2
1
13
993
0
0
1053
9
0
1

8909
2
1
13
1313
0
0
1079
1
0
1

word granularity is used, non-word-aligned addresses are
masked to word boundary and data races for those locations
are detected as one race. That is how the word granularity
detector reported less number of data races for x264. We
carefully inspected data races from x264 found by the
dynamic granularity detector and noticed that there were 4
write locations which were sharing a vector clock with one
location having a data race. More data races from ffmpeg by
the word granularity detector and from streamcluster by the
dynamic granularity detector are found to be false alarms due
to inaccurate updates of vector clocks when large detection
granularities are used.
Memory Overhead. Table 2 shows the details of memory
overhead. For each granularity, three major overhead factors
are shown. The “Hash” column indicates the maximum
memory used for hash tables and hash entries to index vector
clocks. The “Vector clock” column gives the maximum
memory used to store vector clocks. The third column,
“Bitmap”, is the maximum memory used for bitmap data
structures for checking same epoch accesses. The overhead
is measured based on object size and is slightly
underestimated since the size of memory allocated for a data
object is usually little more than the actual size of the type.
The dynamic granularity algorithm saves a substantial
amount of memory used for vector clock allocations (as
shown in “Vector clock” columns). Another view of memory
savings on vector clocks is shown in Table 3 which lists the
maximum numbers of vector clocks during program
executions. On average, the dynamic granularity detector
uses roughly 4x and 3x less memory for vector clocks than
the byte granularity and the word granularity detectors,
respectively. Indexing costs of the byte granularity and the
dynamic granularity detectors are almost same since the use
of dynamic granularity does not save memory on indexing
vector clocks (as shown in “Hash” columns). The use of
word granularity saves memory on indexing for some
benchmark programs because the addresses are mostly wordaligned, thus using smaller indexing arrays in hash entries.

as the ratios to the run time and maximum memory used in
the un-instrumented program execution. “# of Detected Data
Races” columns show the number of data races detected by
each granularity detector.
Overall Results. The results show that the dynamic
granularity detector is on average 1.43x and 1.25x faster than
the byte-granularity and the word-granularity detectors,
respectively. For memory overhead, on average the dynamic
granularity detector consumes 60% less memory than the
byte granularity detector and 23% less memory than the
word granularity detector.
For benchmarks facesim, fluidanimate, raytrace, canneal,
streamcluster, and hmmsearch, memory consumption is
neither reduced nor does detection become faster when we
switch from byte granularity to word granularity. Since the
sizes of most accesses in those benchmarks are equal to or
greater than a word, no vector clock is created for non-wordaligned locations. Thus, using word granularity does not help
to reduce the overhead of vector clock operations. However,
except for raytrace, canneal, the use of dynamic granularity
enhances the detection both in time and memory space. This
suggests the advantage of using a large granularity crossing
word boundaries. The results from ferret and pbzip2 show
improvements both in word granularity and dynamic
granularity, but the use of dynamic granularity has more
benefits than the use of word granularity. It may be strange
to see that the factor of memory overhead for dedup is 1.0
for all detectors. Note that the maximum overhead does not
always occur when the maximum memory is used in the
benchmark. In fact, dedup uses a large amount of memory
(about 2.7 GB) at the beginning of the execution when the
detection overhead is close to zero. Then, the memory usage
is gradually decreased while the peak memory overhead
from the detectors occurs afterward.
For detection precision, there are few discrepancies
among the detectors as shown in Table 1. With word
granularity, 993 data races are reported for x264 while the
dynamic granularity detector reported more data races. When

684

Table 2. Memory overhead of FastTrack detection with different granularities

Overhead total
(MB)

Hash(MB)

Vector Clock
(MB)

Bitmap (MB)

Overhead total
(MB)

Hash(MB)

Vector Clock
(MB)

Bitmap (MB)

Overhead total
(MB)

288
146
248
170
49
104
2682
30
95
67
23

Bitmap (MB)

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Base
Memory
(MB)

513
458
132
35
77
87
212
11
37
49
12
148

1505
1573
180
53
233
176
148
37
118
141
15
380

132
66
27
15
7
52
57
6
7
17
3
35

2149
2097
339
103
317
315
417
54
162
207
30
563

514
259
132
30
33
87
147
11
18
37
12
116

1503
1072
180
53
89
176
100
36
43
89
15
305

132
57
27
15
7
52
58
6
7
17
3
35

2148
1388
338
97
129
315
305
54
68
143
30
456

517
454
132
35
77
87
214
11
37
50
13
148

273
469
74
22
44
155
88
3
28
4
1
105

131
64
27
15
7
52
56
6
7
16
3
35

921
988
233
72
128
294
358
21
72
70
17
288

Second, speedup comes from the reduction of vector
clock allocation and de-allocation operations. For the case of
pbzip2, the dynamic granularity detector is 1.6x faster than
the byte granularity detector while the percentages of same
epoch accesses are same. On the other hand, the average
number of locations that share a vector clocks is 33.3 under
dynamic granularity as shown in Table 3. This implies that
there will be about 33 times less vector clock creation and
deletion operations. The other interesting case is dedup. The
program has the same percentage of same epoch accesses for
both byte and dynamic granularities and the average number
of sharing vector clocks is only 1.7. However, the dynamic
granularity detector is 1.78x faster than the byte granularity
detector. The reason is that there are an excessive number of
dynamic memory locations in dedup. On average, there is
about 1.7 GB of memory allocated and de-allocated in the 11
benchmark programs whereas it is 14GB in dedup.

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch

93,930
83,678
11,220
3,291
12,550
7,141
9,208
2,277
7,195
8,842
961

93,808
52,375
11,174
3,285
4,803
7,141
6,226
2,245
2,620
5,570
950

Dynamic
Granularity
(thousand)

Word
Granularity
(thousand)

Byte
Granularity
(thousand)

Max. # of vector clocks

16,991
24,689
4,590
1,319
2,019
5,812
5,474
193
1,696
265
53

Avg. sharing
count

Table 3. Maximum number of vector clocks present

Benchmark
program

Dynamic Granularity

Vector Clock
(MB)

Benchmark
program

Word Granularity

Hash(MB)

Byte Granularity

5.5
3.4
2.4
2.5
6.2
1.2
1.7
11.8
4.2
33.3
17.9

B. Analysis of State Machine
The sharing decision for realizing dynamic granularity is
made twice for the lifetime of a location L (read or write). In
the first epoch, L tries to share a vector clock with its
neighbors temporarily. In the second epoch, a new sharing
decision is made for the rest lifetime of L. We make a firm
sharing decision at the second epoch (after initialization of L)
since some groups of data structures can be initialized at the
same segment of code even if they are accessed separately
afterward. This design makes the sharing decision accurate.
The temporary sharing at the first epoch may save a
considerable amount of memory because there could be
groups of locations that are accessed together only once in
the same epoch and if that is the case, we do not have to keep
a separate vector clock for each of them. Notice that there is
no possibility of false alarms by the temporary sharing at the
Init state. Table 5 shows the effectiveness of this design.
Column 2 and 3 show the maximum memory used without
and with temporarily sharing at the first epoch. Column 4

Slowdown. Speedups can be achieved in two ways by the use
of a large granularity. Firstly, in DJIT+ based race detectors
including FastTrack, the vector clock operations are
performed only for the first read and write of a shared
memory location during each epoch. The use of a large
granularity makes multiple accesses as one access. Thus,
there are more same epoch accesses enhancing the detection
performance. In Table 4, we show the percentage of same
epoch accesses along with the slowdown for each benchmark
program. The results suggest that in most cases the
performance gains from a large granularity are consistent
with the percentage of same epoch accesses. For the cases of
canneal and raytrace, as the percentages of same epoch
accesses do not vary noticeably among different
granularities, there is no performance enhancement by the
use of a large granularity.

685

library. The race detection algorithm in DRD is based on
RecPlay [21]. Since DRD does not keep vector clocks for
each memory location in its segment comparison approach,
we expect that DRD uses less memory but is slower than
FastTrack. Intel Inspector XE is a memory and thread error
checker that is capable of detecting various errors including
data races, deadlocks, and cross-thread stack access.
Inspector XE provides a GUI with comprehensive
analysis reports, including the source code location of an
error, calling stack analyses, and suggestion for fixing any
detected errors. Likewise, DRD provides execution context
for each error as well as the location that the error occurs.
Race report from our implementation of FastTrack is not as
comprehensive as the two tools, but we provide the location
of a race along with the previous access location, thread ids,
and the race memory address. The information should be
sufficient for developers to fix the problems easily.
For Inspector XE, the command-line version was used
and only data race detection is enabled. The two tools used
byte granularity and all tools, including our dynamic
granularity version of FastTrack, traced all modules
including shared libraries. For the dynamic granularity
detector, we applied the similar suppression rules as in DRD,
e.g., suppressed data races detected from libc and ld. The
dynamic granularity detector and DRD report the first race
for each memory location while Inspector XE uses a
combination of instruction pointers and timeline when a race
occurs to distinguish races. Thus Inspector XE may report
the same accesses on a specific memory location as multiple
races or multiple accesses issued at the same instruction
points as one race. DRD and Inspector XE classify the
detected data races with execution context, but in the
experimental results we list the raw number of data races
before the classifications.
The comparison results are shown in Table 6. Both
Inspector XE and DRD exited on dedup runs with out of
memory warnings. DRD on fluidanimate and Inspector XE
on ffmpeg ran for more than 24 hours before we stopped the
analyses.
As we expected before the experiment, DRD is slower
than the FastTrack with dynamic granularity and even slower
than the FastTrack with byte granularity, but DRD consumes
less memory than the FastTrack with dynamic granularity.
The comparison results also suggest that the dynamic
granularity detector is as accurate as the other two tools. All
three tools detected the same race for hmmsearch (Inspector
XE reported the same race one more time in a different
timeline). The dynamic granularity detector and Inspector
XE reported the same races for three benchmarks ferret,
fluidanimate, and streamcluster, For raytrace, the dynamic
granularity detector and DRD reported the same races, but
DRD reported more races from pthread library which was
suppressed by the dynamic granularity detector. DRD
detected no race for ffmpeg while the dynamic granularity
detector reported one race. We manually inspected the
source code and found that it was a data race by the two
worker threads accessing a shared variable without
protection.

Table 4. Measures of same epoch accesses

Benchmark
Program

Word
Granularity

Dynamic
Granularity

Byte
Granularity

Word
Granularity

Dynamic
Granularity

Same epoch

Byte
Granularity

Slowdown

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

138
65
87
27
75
13
152
245
121
64
84
97

138
57
87
27
55
13
76
245
106
49
83
85

102
52
81
27
64
13
85
137
109
39
45
68

74%
78%
89%
65%
67%
97%
85%
50%
68%
95%
83%
77%

74%
83%
89%
65%
90%
97%
93%
51%
90%
97%
83%
83%

94%
87%
94%
68%
78%
97%
85%
97%
84%
95%
98%
89%

Table 5. Comparisons of state machines with different
configurations

1317
1302
551
334
442
530
2730
111
294
225
99
721

With Init
state

2180
1808
604
348
470
550
2729
142
301
359
107
873

# of Detected
Data Races
No Init
state

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Sharing at
Init

Benchmark
program

No sharing
at Init

Max.
Memory(MB)

9210
2
18529
13
1315
0
0
1079
1
2
1

8909
2
1
13
1313
0
0
1079
1
0
1

shows the number of detected data races without the Init
state, i.e., no temporary sharing and the sharing decision is
made only once in the first epoch. Comparing with column 5
in which the Init state is added, there could be many false
alarms as the consequence of improper sharing decisions
made only in the first epoch. In addition, the results suggest
that there are considerable numbers of memory locations that
are used only in one epoch.
C. Case Studies
In this section, we present experimental results on two
popular data race detection tools, DRD in Valgrind-3.8.1
[25] and Intel Inspector XE 2013 [11] update5. Also
comparison results with our dynamic granularity on
FastTrack are given. DRD, a tool for programs written with
the POSIX library, detects various errors including data
races, lock contention delays, and misuses of the POSIX

686

Table 6. Performance comparisons of Valgraind DRD, Intel Inspector XE and the FastTrack with dynamic granularity

Average

VI.

102
52
81
27
64
13
85
137
109
39
45

2.2
2.6
-1.9
3.2
8.2
-4.2
2.6
2.9
4.4

6.0
5.0
12.4
4.1
22.1
11.9
-17.5
-8.6
21.9

4.6
8.9
2.2
2.0
9.0
5.1
1.0
3.7
3.1
3.4
4.3

150

98

68

3.6

12.2

4.3

8909
108
-16
988
0
-1067
0
0
1

Dynamic
granularity

128
87
89
17
246
41
-108
-99
64

Intel Inspector
XE

59
748
-42
143
31
-66
120
64
74

Valgrind DRD

Valgrind DRD

Dynamic
granularity

288
146
248
170
49
104
2682
30
95
67
23

Intel Inspector
XE

6.1
6.7
2.0
9.5
2.2
6.5
7.7
3.8
3.0
5.7
26.6

Base
Memory
(MB)

# of Detected Data Races

Dynamic
granularity

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch

Base
time
(sec)

Memory Overhead

Intel Inspector
XE

Benchmark
program

Valgrind DRD

Slowdown

31
4
7
0
218
0
-61
-0
2

8909
2
1
13
1313
0
0
1079
1
0
1

PACER [2] is another sampling based race detector that
periodically samples all threads and offers a detection rate
proportional to the sampling rate. These approaches offer
reasonable detection rate with minimal overhead, but may
miss critical data races.
As an alternative to software only race detectors, several
hardware assisted race detectors have been proposed. In
SigRace [15], data addresses are automatically encoded in
hardware address signatures and in a hardware module. The
signatures are intersected with those of other processors to
detect data races. Greatehouse et al. [10] proposed a demanddriven race detector that utilizes cache performance counters
to detect data sharing between threads. When the data
sharing is detected, a software race detector is enabled and
run until there is no more data sharing. These approaches are
efficient but require specific hardware making them
impractical.
While many researchers have focused on data race
detection algorithms for Java programs, only a few of which
have presented evaluation results for existing data race
detection tools on C/C++ programs. Aikido [18] is a
framework for shared data analysis in which sharing data is
detected using per-thread page protection technique. The
Aikido sharing detector is complementary to dynamic
granularity and is effective to remove the instrumentation
overhead of no-shared memory accesses. IFRit [3] is a
dynamic race detection algorithm for C/C++ programs based
on interference-free region which can limit the range of code
instrumentation. IFRit has been compared with FastTrack
and ThreadSanitizer [24]. Both researches applied the
PARSEC benchmark suite in their performance evaluations,
but only the simsmall input set was used and no memory
overhead was reported. Moreover, none of them made an
attempt to compare their tools with commercial grade data
race detection tools.

RELATED WORK

Aside from the 3 basic approaches for race detections, [7,
19, 21, 22, 23, 25, 27], a variety of hybrid race detectors
have been proposed in which Eraser’s Lockset algorithm is
combined with the happens-before algorithm to have better
detection coverage and to avoid false alarms. O’Callahan and
Choi [17] have proposed a race detection algorithm in which
a subset of happens-before relations is added to a Lockset
based detector. The detector is optimized by detecting
redundant event accesses and by the use of a “two phase”
approach of detailed and simple mode detections. MultiRace
[19] combines DJIT+ and Lockset algorithm and only check
the first access in each time frame. In MultiRace, the number
of detection operations is reduced based on the information
produced from LockSet and false alarms from LockSet are
filtered out by happens-before relations. ThreadSanitizer [24]
is a hybrid race detector for C++ programs that offers
tunable options to users. Its dynamic annotations allow the
detector to be aware of user defined synchronizations. Thus
the tool hides certain false alarms and benign races.
RaceTrack [27] incorporates the happens-before relation into
the LockSet algorithm and only report races caused by
concurrent accesses. One interesting idea in RaceTrack is the
use of adaptive granularity. The detection granularity starts
from object level and becomes field level when a potential
race is detected. Unfortunately, the idea, based on object
references, is not applicable to C/C++ programs.
LiteRace [14] is a sampling based race detector grounded
in the cold-region hypothesis that infrequently accessed areas
are more likely to have data races than frequently accessed
areas. Accesses to code regions of different function units are
sampled while all synchronization operations are collected.
The sampler starts at a 100% sampling rate and the sampling
rate is adaptively decreased until it reaches a lower bound.

687

[12] L. Lamport. Time, clocks, and the ordering of events in a

VII. CONCLUSIONS
In this paper, we have presented a dynamic granularity
algorithm for C/C++ programs that enables vector clock
based race detectors to adjust detection granularity. A vector
clock state machine is employed to determine when vector
clocks can be shared. The state machine also considers the
initialization patterns of data structures. Thus, possible false
alarms due to vector clock sharing can be reduced.
Our experimental results show that the dynamic
granularity detector outperforms the FastTrack with both
byte or word granularity and also outperforms two existing
data race detection tools, Valgrind DRD and Intel Inspector
XE.
We plan to extend our work by optimizing the sharing
algorithm. The current work maintains read and write vector
clocks separately. The decision of sharing read vector clocks
can be guided by the status of write vector clocks. We also
plan to enhance the vector clock state machine to
accommodate access behavior after the second epoch so that
the detection granularity can be changed more dynamically.

[13]

[14]

[15]

[16]

[17]

ACKNOWLEDGMENT
This work was supported in part by the NSF I/UCRC
Center for Embedded Systems, and from NSF grant
#0856090.

[18]

REFERENCES
[1]
[2]

[3]

[4]
[5]
[6]

[7]

[8]

[9]
[10]

[11]

C. Bienia. Benchmarking Modern Multiprocessors. Ph.D.
Thesis. Princeton University, 2011.
M. D. Bond, K. E. Coons, and K. S. McKinley. PACER:
Proportional detection of data races. In Proceedings of the
ACM SIGPLAN conference on Programming language design
and implementation (PLDI), pages 255-268, 2010.
L. Effinger-Dean, B. Lucia, L. Ceze, D. Grossman, and H-J.
Boehm. IFRit: Interference-free regions for dynamic data-race
detection. In Proceedings of the ACM international
conference on Object oriented programming systems
languages and applications (OOPSLA), pages 467-484, 2012.
FFmpeg. http://www.ffmpeg.org/.
C. J. Fidge. Logical time in distributed computing systems.
IEEE Computer, 24(8):28-33, 1991.
R. D. Finn, J. Clements, and S. R. Eddy. HMMER web
server: Interactive sequence similarity searching. Nucleic
Acids Research Web Server Issue 39:W29-W37, 2011.
C. Flanagan and S. N. Freund. FastTrack: Efficient and
precise dynamic race detection. In Proceedings of the ACM
SIGPLAN conference on Programming language design and
implementation (PLDI), pages 121-133, 2009.
C. Flanagan and S. N. Freund. Type-based race detection for
Java. In Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 219-232, 2000.
J. Gilchrist. Parallel BZIP2, http://compression.ca/pbzip2/.
J. L. Greathouse, Z. Ma, M. I. Frank, R. Peri, and T. Austin.
Demand-driven software race detection using hardware
performance counters. In Proceedings of the 38th annual
international symposium on Computer architecture (ISCA),
pages 165-176, 2011.
Intel Inspector XE 2013. http://software.intel.com/en-us/intelinspector-xe.

[19]

[20]

[21]

[22]

[23]

[24]

[25]
[26]

[27]

688

distributed system. Communications of the ACM, 21(7):558565, 1978.
C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G, Lowney,
S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: Building
customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
conference on Programming language design and
implementation (PLDI), pages 190-200, 2005.
D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace:
Effective sampling for lightweight data-race detection. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 134-143, 2009.
A. Muzahid, D. Suárez, S. Qi, and J. Torrellas. SigRace:
Signature-based data race detection. In Proceedings of the
36th annual international symposium on Computer
architecture (ISCA), pages 337-348, 2009.
N. Nethercote and J. Seward. Valgrind: A framework for
heavyweight dynamic binary instrumentation. In Proceedings
of the ACM SIGPLAN conference on Programming language
design and implementation (PLDI), pages 89-100, 2007.
R. O'Callahan and J. Choi. Hybrid dynamic data race
detection. In Proceedings of the ninth ACM SIGPLAN
symposium on Principles and practice of parallel
programming (PPoPP), pages 167-178, 2003.
M. Olszewski, Q. Zhao, D. Koh, J. Ansel, and S.
Amarasinghe. Aikido: Accelerating shared data dynamic
analyses. In Proceedings of the seventeenth international
conf. on Architectural Support for Programming Languages
and Operating Systems (ASPLOS), pages 173-184, 2012.
E. Pozniansky and A. Schuster. Efficient on-the-fly data race
detection in multithreaded C++ programs. In Proceedings of
the ninth ACM SIGPLAN symp. on Principles and practice of
parallel programming (PPoPP), pages 179-190, 2003.
P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH:
Practical static race detection for C. ACM Transactions on
Programming Languages and Systems (TOPLAS), 33(1):1-55,
2011.
M. Ronsse and K. D. Bosschere. RecPlay: A fully integrated
practical record/replay system. ACM Transactions on
Computer Systems (TOCS), 17(2):133-152, 1999.
M. Ronsse, M. Christiaens, and K. D. Bosschere. Debugging
shared memory parallel programs using record/replay. Future
Generation Computer Systems, 19(5):679-687, 2003.
S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T.
Anderson. Eraser: A dynamic data race detector for
multithreaded programs. ACM Transactions on Computer
Systems (TOCS), 15(4):391-411, 1997.
K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: Data
race detection in practice. In Proceedings of the Workshop on
Binary Instrumentation and Applications (WBIA), pages 6271, 2009.
DRD, Valgrind-3.8.1. http://valgrind.org/.
J. W. Voung, R. Jhala, and S. Lerner. RELAY: Static race
detection on millions of lines of code. In Proceedings of the
6th joint meeting of the European software engineering
conference and the ACM SIGSOFT symposium on the
foundations of software engineering (ESEC-FSE), pages 205214, 2007.
Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: Efficient
detection of data race conditions via adaptive tracking. In
Proceedings of the twentieth ACM symposium on Operating
systems principles (SOSP), pages 221-234, 2005.

