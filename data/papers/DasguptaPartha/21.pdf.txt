Ghosh, Sharma, Chakrabarti, & Dasgupta

are useful in practice. One example of such a problem is finding the secondary structure of
RNA (Mathews & Zuker, 2004) which is an important problem in Bioinformatics. RNAs
may be viewed as sequences of bases belonging to the set {Adenine(A), Cytocine(C), Guanine(G), Uracil(U)}. RNA molecules tend to loop back and form base pairs with itself and
the resulting shape is called the secondary structure. The primary factor that influences the
secondary structure of RNA is the number of base pairings (higher number of base pairings generally implies more stable secondary structure). Under the well established rules
for base pairings, the problem of maximizing the number of base pairings has an interesting dynamic programming formulation. However, apart from the number of base pairings,
there are other factors that influence the stability, but these factors are typically evaluated
experimentally. Therefore, for a given RNA sequence, it is useful to compute a pool of
candidate secondary structures (in decreasing order of the number of base pairings) that
may be subjected to further experimental evaluation in order to determine the most stable
secondary structure.
The problem of generating ordered set of solutions is well studied in other domains.
For discrete optimization problems, Lawler (1972) had proposed a general procedure for
generating k-best solutions. A similar problem of finding k most probable configurations in
probabilistic expert systems is addressed by Nilsson (1998). Fromer and Globerson (2009)
have addressed the problem of finding k maximum probability assignments for probabilistic modeling using LP relaxation. In the context of ordinary graphs, Eppstein (1990) has
studied the problem of finding k-smallest spanning trees. Subsequently, an algorithm for
finding k-best shortest paths has been proposed in Eppstein’s (1998) work. Hamacher and
Queyranne (1985) have suggested an algorithm for k-best solutions to combinatorial optimization problems. Algorithms for generating k-best perfect matching are presented by
Chegireddy and Hamacher (1987). Other researchers applied the k-shortest path problem
to practical scenarios, such as, routing and transportation, and developed specific solutions
(Takkala, Borndörfer, & Löbel, 2000; Subramanian, 1997; Topkis, 1988; Sugimoto & Katoh,
1985). However none of the approaches seems to be directly applicable for AND/OR structures. Recently some schemes related to ordered solutions to graphical models (Flerova &
Dechter, 2011, 2010) and anytime AND/OR graph search (Otten & Dechter, 2011) have
been proposed. Anytime algorithms for traditional OR search space (Hansen & Zhou, 2007)
are well addressed by the research community.
In this paper, we address the problem of generating ordered set of solutions for explicit
AND/OR DAG structure and present new algorithms. The existing method, proposed
by Elliott (2007), works bottom-up by computing k-best solutions for the current node
from the k-best solutions of its children nodes. We present a best first search algorithm,
named Alternative Solution Generation (ASG) for generating ordered set of solutions. The
proposed algorithm maintains a list of candidate solutions, initially containing only the
optimal solution, and iteratively generates the next solution in non-decreasing order of cost
by selecting the minimum cost solution from the list. In each iteration, this minimum cost
solution is used to construct another set of candidate solutions, which is again added to the
current list. We present two versions of the algorithm –
a. Basic ASG (will be referred to as ASG henceforth) : This version of the algorithm
may construct a particular candidate solution more than once;

278

Generating Ordered Solutions for Explicit AND/OR Structures

b. Lazy ASG or LASG : Another version of ASG algorithm that constructs every candidate solution only once.
In these algorithms, we use a compact representation, named signature, for storing the
solutions. From the signature of a solution, the actual explicit form of that solution can
be constructed through a top-down traversal of the given DAG. This representation allows
the proposed algorithms to work in a top-down fashion starting from the initial optimal
solution. Another salient feature of our proposed algorithms is that these algorithms work
incrementally unlike the existing approach. Our proposed algorithms can be interrupted at
any point of time during the execution and the set of ordered solutions obtained so far can
be observed and subsequent solutions will be generated when the algorithms are resumed
again. Moreover, if an upper limit estimate on the number of solutions required is known a
priori, our algorithms can be further optimized using that estimate.
The rest of the paper is organised as follows. The necessary formalisms and definitions
are presented in Section 2. In Section 3, we address the problem of generating ordered set of
solutions for trees. Subsequently in Section 4, we address the problem of finding alternative
solutions of explicit acyclic AND/OR DAGs in non-decreasing order of cost. We present two
different solution semantics for AND/OR DAGs and discuss the existing approach as well as
our proposed approach, along with a comparative analysis. Detailed experimental results,
including the comparison of the performance of the proposed algorithms with the existing
algorithm (Elliott, 2007), are presented in Section 5. We have used randomly constructed
trees and DAGs as well as some well-known problem domains including the 5-peg Tower
of Hanoi problem, the matrix-chain multiplication problem and the problem of finding
the secondary structure of RNA as test domain. The time required and the memory used
for generating a specific number of ordered solutions for different domains are reported in
detail. In Section 6, we outline briefly about applying the proposed algorithms for implicitly
specified AND/OR structures. Finally we present the concluding remarks in Section 7.

2. Definitions
In this section, we describe the terminology of AND/OR trees and DAGs followed by other
definitions that are used in this paper. Gαβ = hV, Ei is an AND/OR directed acyclic graph,
where V is the set of nodes and E is the set of edges. Here α and β in Gαβ refer to the
AND nodes and OR nodes in the DAG respectively. The direction of edges in Gαβ is from
the parent node to the child node. The nodes of Gαβ with no successors are called terminal
nodes. The non-terminal nodes of Gαβ are of two types – i) OR nodes and ii) AND nodes .
Vα and Vβ are the set of AND and OR nodes in Gαβ respectively, and nαβ = |V |, nα = |Vα |,
and nβ = |Vβ |. The start (or root) node of Gαβ is denoted by vR . OR edges and AND edges
are the edges that emanate from OR nodes and AND nodes respectively.
Definition 2.a [Solution Graph] A solution graph, S(vq ), rooted at any node vq ∈ V , is a
finite sub-graph of Gαβ defined as:
a. vq is in S(vq );
b. If vq′ is an OR node in Gαβ and vq′ is in S(vq ), then exactly one of its immediate
successors in Gαβ is in S(vq );
c. If vq′ is an AND node in Gαβ and vq′ is in S(vq ), then all its immediate successors in
Gαβ are in S(vq );
279

Ghosh, Sharma, Chakrabarti, & Dasgupta

d. Every maximal (directed) path in S(vq ) ends in a terminal node;
e. No node other than vq or its successors in Gαβ is in S(vq ).
By a solution graph S of Gαβ we mean a solution graph with root vR .

⊔
⊓

Definition 2.b [Cost of a Solution Graph] In Gαβ , every edge eqr ∈ E from node vq to
node vr has a finite non-negative cost ce (hvq , vr i) or ce (eqr ). Similarly every node vq has a
finite non-negative cost denoted by cv (vq ). The cost of a solution S is defined recursively
as follows. For every node vq in S, the cost C(S, vq ) is:


cv (vq ), if vq is a terminal node;



	


cv (vq ) + C(S, vr ) + ce (hvq , vr i) , where vq is an OR node, and

C(S, vq ) =
vr is the successor of vq in S;

	
P


C(S, vj ) + ce (hvq , vj i) , where 1 ≤ j ≤ k, vq is an AND node
cv (vq ) +




with degree k, and v1 , . . . , vk are the immediate successors of vq in S.
Therefore the cost of a solution S is C(S, vR ) which is also denoted by C(S). We denote
the optimal solution below every node vq as opt(vq ). Therefore, the optimal solution of the
entire AND/OR DAG Gαβ , denoted by Sopt , is opt(vR ). The cost of the optimal solution
rooted at every node vq in Gαβ is Copt (vq ), which is defined recursively (for minimum cost
objective functions) as follows:


cv (vq ), if vq is a terminal node;



	



cv (vq ) + min Copt (vj ) + ce (hvq , vj i) , where 1 ≤ j ≤ k, vq is an OR node
Copt (vq ) =
with degree k, and v1 , . . . , vk are the immediate successors of vq in Gαβ ;


	

cv (vq ) + P Copt (vj ) + ce (hvq , vj i) , where 1 ≤ j ≤ k, vq is an AND node




with degree k, and v1 , . . . , vk are the immediate successors of vq in Gαβ .
The cost of the optimal solution Sopt of Gαβ is denoted by Copt (vR ) or, alternatively, by
Copt (Sopt ). When the objective function needs to be maximized, instead of the min function,
the max function is used in the definition of Copt (vq ).
⊔
⊓

It may be noted that it is possible to have more than one solution below an OR node
vq to qualify to be the optimal one, i.e., when they have the same cost, and that cost is the
minimum. Ties for the optimal solution below any such OR node vq are resolved arbitrarily
and only one among the qualifying solutions (determined after tie-breaking) is marked as
opt(vq ).
An AND/OR tree, Tαβ = hV, Ei, is an AND/OR DAG and additionally satisfies the
restrictions of a tree structure i.e., there can be at most one parent node for any node vq
in Tαβ . In the context of AND/OR trees, we use eq to denote the edge that points to
the vertex vq . An alternating AND/OR tree, T̂αβ = hV, Ei, is an AND/OR tree with the
restriction that there is an alternation between the AND nodes and the OR nodes. Every
child of an AND node is either an OR node or a terminal node, and every children of an OR
node is either an AND node or a terminal node. We use the term solution tree to denote
the solutions of AND/OR trees.
We also discuss a different solution semantics, namely tree based semantics, for AND/OR
DAGs. Every AND/OR DAG can be converted to an equivalent AND/OR tree by traversing
280

Generating Ordered Solutions for Explicit AND/OR Structures

the intermediate nodes in reverse topological order and replicating the subtree rooted at
every node whenever the in-degree of the traversed node is more than 1. The details are
shown in Procedure ConvertDAG. Suppose an AND/OR DAG Gαβ is converted to an
equivalent AND/OR tree Tαβ . We define the solutions of Tαβ as the solutions of Gαβ under
tree based semantics.
Procedure ConvertDAG(Gαβ )
input : An AND/OR DAG Gαβ
output: An equivalent AND/OR tree Tαβ
1 Construct a list M , of non-terminal nodes of Gαβ , sorted in the reverse topological
order;
2 while M is not empty do
3
vq ← Remove the first element of M ;
/* Suppose Ein (vq ) is the list of incoming edges of vq
*/
4
if InDegree(vq ) > 1 then
5
for i ← 2 to InDegree(vq ) do
6
et ← Ein (vq )[i];
Replicate
the sub-tree rooted at vq with vq′ as the root;
7
Modify the target node of et from vq to vq′ ;
8
9
end
10
end
11 end
In this paper we use the solution semantics defined in Definition 2.a as the default
semantics for the solutions of AND/OR DAGs. When the tree based semantics is used, it
is explicitly mentioned.
2.1 Example

2, 34

h3i
3, 29

v1

v1

v2

h1i

h1i

h2i

v3

2, 37

v2

v4

2, 8

h1i

v5

h3i

h4i

v6

3, 11

v7
h1i

h1i

h2i

h2i

v9

v10

v11

v12

v13

v14

v15

v9

v10

5

7

6

9

12

15

20

5

7

Figure 1: Alternating AND/OR Tree
281

v6

2, 35

v8

3, 9

h1i

h1i

h1i

h3i

h4i

v7
h3i

2, 41

h4i

v5

40

12
h2i

v3

3, 43

v4

v8

4, 17

h2i

h5i

h1i

35
h5i

2, 89

17

h2i

Figure 2: AND/OR DAG

52

Ghosh, Sharma, Chakrabarti, & Dasgupta

We present an example of an alternating AND/OR tree in Figure 1. In the figure, the
terminal nodes are represented by a circle with thick outline. AND nodes are shown in the
figures with their outgoing edges connected by a semi-circular curve in all the examples.
The edge costs are shown by the side of each edge within an angled bracket. The cost of the
terminal nodes are shown inside a box. For every non-terminal node vq , the pair of costs,
cv (vq ) and Copt (vq ), is shown inside a rectangle.
In Figure 1 the optimal solution below every node is shown using by thick dashed edges
with an arrow head. The optimal solution of the AND/OR tree can be traced by following
these thick dashed edges from node v1 . The cost of the optimal solution tree is 34. Also,
Figure 2 shows an example of a DAG; the cost of the optimal solution DAG is 89.

3. Generating Ordered Solutions for AND/OR Trees
In this section we address the problem of generating ordered solutions for trees. We use
the notion of alternating AND/OR trees, defined in Section 2, to present our algorithms.
An alternating AND/OR tree presents a succinct representation and so the correctness
proofs are much simpler for alternating AND/OR trees. In Appendix C we show that every
AND/OR tree can be converted to an equivalent alternating AND/OR tree with respect to
the solution space.
It is worth noting that the search space of some problems (e.g. the search space of multipeg Tower of Hanoi problem) exhibit the alternating AND/OR tree structure. Moreover, the
algorithms that are presented for alternating AND/OR trees work without any modification
for general AND/OR trees. In this section, first we present the existing algorithm (Elliott,
2007) briefly, and then we present our proposed algorithms in detail.
3.1 Existing Bottom-Up Evaluation Based Method for Computing Alternative
Solutions
We illustrate the working of the existing method that is proposed by Elliott (2007) for
computing alternative solutions for trees using an example of an alternating AND/OR tree.
This method (will be referred as BU henceforth) computes the k-best solutions in a bottomup fashion. At every node, vq , k-best solutions are computed from the k-best solutions of
the children of vq . The overall idea is as follows.
a. For an OR node vq , a solution rooted at vq is obtained by selecting a solution of a
child. Therefore k-best solutions of vq are computed by selecting the top k solutions
from the entire pool consisting of all solutions of all children.
b. In the case of AND nodes, every child of an AND node vq will have at most k solutions.
A solution rooted at an AND node vq is obtained by combining one solution from every
child of vq . Different combinations of the solutions of the children nodes of vq generate
different solutions rooted at vq . Among those combinations, top k combinations are
stored for vq .
In Figure 3 we show the working of the existing algorithm. At every intermediate node
2-best solutions are shown within rounded rectangle. At every OR node vq , the ith -best
cost. For
solution rooted at vq is shown as a triplet of the form – |{z}
i : < child, solidx >, |{z}
{z
}
|
example, at node v1 the second best solution is shown as – 2 : hv2 , 2i, 37; which means
282

Generating Ordered Solutions for Explicit AND/OR Structures

that the 2nd best solution rooted at v1 is obtained by selecting the 2nd best solution of v2 .
Similarly, at every AND node vq , the ith solution rooted at vq is shown as a triplet of the
form – i : |sol vec|, cost triplets. Here sol vec is a comma separated list of solution indices
such that every element of sol vec corresponds to a child of vq . The j th element of sol vec
shows the index of the solution of j th child. For example, the 2nd best solution rooted at v2
is shown as – 2 : |2, 1|, 32. This means the 2nd best solution rooted at v2 is computed using
the 2nd best solution of the 1st child (which is v5 ) and the best solution (1st ) of the 2nd
child (which is v6 ). Which index of sol vec corresponds to which child is shown by placing
the child node name above every index position.

2, 34

h3i

3, 29

v2

h5i

2, 8

v5

1 : hv2 , 1i, 34
2 : hv2 , 2i, 37

v1

h1i

h2i

v5 v6
1 : |1, 1|, 29
2 : |2, 1|, 32

v3

2, 37

35

h1i

1 : hv9 , 1i, 8
2 : hv10 , 1i, 11

v7 v8
1 : |1, 1|, 37
1 : |1, 2|, 40

v4

h3i

h4i

v6

3, 11

v7

1 : hv11 , 1i, 11
2 : hv12 , 1i, 15

12

h3i

h2i

v8

4, 17

h1i

h1i

1 : hv13 , 1i, 17
2 : hv14 , 1i, 20
h2i

h1i

h2i

v9

v10

v11

v12

v13

v14

v15

5

7

6

9

12

15

20

Figure 3: Example working of the existing algorithm
The existing method works with the input parameter k, i.e., the number of solutions to
be generated have to be known a priori. Also this method is not inherently incremental in
nature, thus does not perform efficiently when the solutions are needed on demand, e.g., at
first, top 20 solutions are needed, then the next 10 solutions are needed. In this case the
top 20 solutions will have to be recomputed while computing next 10 solutions, i.e., from
the 21st solution to the 30th solution. Next we present our proposed top-down approach
which does not suffer from this limitation.
3.2 Top-Down Evaluation Algorithms for Generating Ordered Solutions
So far we have discussed the existing approaches which primarily use bottom-up approach
for computing ordered solutions. Now we propose a top-down approach for generating alternative solutions in the non-decreasing order of cost. It may be noted that the top-down
283

Ghosh, Sharma, Chakrabarti, & Dasgupta

approach is incremental in nature. We use an edge marking based algorithm, Alternative
Solution Generation (ASG), to generate the next best solutions from the previously generated solutions. In the initial phase of the ASG algorithm, we compute the optimal solution
for a given alternating AND/OR tree T̂αβ and perform an initial marking of all OR edges.
The following terminology and notions are used to describe the ASG algorithm. In the
context of AND/OR trees, we use eq to denote the edge that points to the vertex vq . We
will use the following definitions for describing our proposed top-down approaches.
Definition 3.c [Aggregated Cost] In an AND/OR DAG Gαβ , the aggregated cost, ca , for
an edge eij from node vi to node vj , is defined as : ca (eij ) = ce (eij ) + Copt (vj ).
⊔
⊓
v1

2, 34

[e2 : 5]

σ2,3 : 5

h3i

3, 29

σ3,4 : 1

v2

h1i

h2i

[e3 : 1]

v3

2, 37

v4

35
h5i

2, 8

h1i

v5

h4i

v6

3, 11

h3i

v7

v8

4, 17

12
[e11 : 4]

[e9 : 3]
h1i

σ9,10 : 3

h2i

h2i

[e13 : 3]
h3i

σ11,12 : 4

h1i

[e14 : 6] h1i

h2i

σ13,14 : 3 σ14,15 : 6

v9

v10

v11

v12

v13

v14

v15

5

7

6

9

12

15

20

Figure 4: Example of OR-edge marking and swap option
Marking of an OR edge : The notion of marking an OR edge is as follows. For an OR
node vq , L(vq ) is the list of OR edges of vq sorted in non-decreasing order of the aggregated
cost of the edges. We define δ(i,i+1) as the difference between the cost of OR edges, ei and
ei+1 , such that ei and ei+1 emanate from the same OR node vq , and ei+1 is the edge next to
ei in L(vq ). Procedure MarkOR describes the marking process for the OR edges of an OR
node. Intuitively, a mark represents the cost increment incurred when the corresponding
edge is replaced in a solution by its next best sibling. The OR edge having maximum
aggregated cost is not marked.
Consider a solution, Scur , containing the edge ei = (vq , vi ), where ei ∈ Eopt (Scur ). We
mark ei with the cost increment which will be incurred to construct the next best solution
from Scur by choosing another child of vq . In Figure 4 the marks corresponding to OR edges
e2 , e3 , e9 , e11 , e13 , and e14 are [e2 : 5], [e3 : 1], [e9 : 3], [e11 : 4], [e13 : 3], and [e14 : 6].
284

Generating Ordered Solutions for Explicit AND/OR Structures

Procedure MarkOR(vq )
1

2
3
4
5
6
7
8

Construct L(vq ) ; /* List of OR edges of vq sorted in the non-decreasing order of ca
values */
count ← number of elements in L(vq ) ;
for i ← 1 to i = count − 1 do
ec ← L(vq )[i] ;
en ← L(vq )[i + 1] ;
δtmp = (ca (en ) − ca (ec )) ;
Mark ec with the pair [en : δtmp ] ;
end

Definition 3.d [Swap Option] A swap option σij is defined as a three-tuple hei , ej , δij i
where ei and ej emanate from the same OR node vq , ej is the edge next to ei in L(vq ), and
δij = ca (ej ) − ca (ei ). Also, we say that the swap option σij belongs to the OR node vq . ⊔
⊓

Consider the OR node vq and the sorted list L(vq ). It may be observed that in L(vq )
every consecutive pair of edges forms a swap option. Therefore, if there are k edges in L(vq ),
k−1 swap options will be formed. At node vq , these swap options are ranked according to the
rank of their original edges in L(vq ). In Figure 4 the swap options are : σ(2,3) = he2 , e3 , 5i,
σ(3,4) = he3 , e4 , 1i, σ(9,10) = he9 , e10 , 3i, σ(11,12) = he11 , e12 , 4i, σ(13,14) = he13 , e14 , 3i, and
σ(14,15) = he14 , e15 , 6i. Consider the node v1 where L(v1 ) = he2 , e3 , e4 i. Therefore, the swap
options, σ(2,3) and σ(3,4) , belong to v1 . At node v1 , the rank of σ(2,3) and σ(3,4) are 1 and 2
respectively.
Definition 3.e [Swap Operation] Swap operation is defined as the application of a swap
option σij = hei , ej , δij i to a solution Sm that contains the OR edge ei in the following way:
′ . Edge e is
a. Remove the subtree rooted at vi from Sm . Let the modified tree be Sm
i
the original edge of σij .
′ , which is constructed at the previous step. Let the
b. Add the subtree opt(vj ) to Sm
′′ . Edge e is the swapped edge of σ .
newly constructed solution be Sm
j
ij
′ from S when
Intuitively, a swap operation σij = hei , ej , δij i constructs a new solution Sm
m
′
Sm contains the OR edge ei . Moreover, the cost of Sm is increased by δij compared to cost
of Sm if C(Sm , vi ) = Copt (vi ).
⊔
⊓

Our proposed algorithms use a swap option based compact representation, named signature, for storing the solutions. Intuitively, any alternative solution can be described as a
set of swap operations performed on the optimal solution Sopt . It is interesting to observe
that while applying an ordered sequence of swap options, hσ1 , · · · , σk i, the application of
each swap operation creates an intermediate alternative solution. For example, when the
first swap option in the sequence, σ1 , is applied to the optimal solution, Sopt , a new solution, say S1 , is constructed. Then, when the 2nd swap option, σ2 , is applied to S1 , yet
another solution S2 is constructed. Let Si denote the solution obtained by applying the
swap options, σ1 , · · · , σi , on Sopt in this sequence. Although, an ordered sequence of swap
options, like hσ1 , · · · , σk i, can itself be used as a compact representation of an alternative
solution, the following key points are important to observe.
A. Among all possible sequences that generate a particular solution, we need to preclude
those sequences which contain redundant swap options (those swap options whose orig285

Ghosh, Sharma, Chakrabarti, & Dasgupta

inal edge is not present in the solution to which it is applied). This is formally defined
later as superfluous swap options. Also the order of applying the swap options is another important aspect. There can be two swap options, σi and σj where 1 ≤ i < j ≤ k
such that the source edge of σj belongs to the sub-tree which is included in the solution
Si only after applying σi to Si−1 . In this case, if we apply σj at the place of σi , i.e.,
apply σj directly to Si−1 , it will have no effect as the source edge of σj is not present
in Si−1 , i.e., after swapping the location of σi and σj in the sequence, σj becomes a
redundant swap option and the solution constructed would be different for the swapped
sequence from the original sequence. We formally define an order relation on a pair of
swap options based on this observation in the later part of this section and formalize
the compact representation of the solutions based on that order relation.
B. Suppose the swap option σj belongs to a node vpj . Now it is important to observe
that the application of σj on Sj−1 to construct Sj , invalidates the application of all
other swap options that belong to an OR edge in the path from the root node to vpj in
the solution Sj . This is because in Sj the application of any such swap option which
belongs to an OR edge in the path from the root node to vpj would make the swap at
vpj redundant. In fact, for each swap option σi belonging to node vpi , where 1 ≤ i ≤ j,
the application of all other swap options that belong to an OR edge in the path from
the root node to vpi is invalidated in the solution Sj for the same reason. This condition
restricts the set of swap options that can be applied on a particular solution.
C. Finally, there can be two swap options σi and σj for 1 ≤ i < j ≤ k such that σi and
σj are independent of each other, that is, (a) applying σi to Si−1 and subsequently the
application of σj to Sj−1 , and (b) applying σj to Si−1 and subsequently the application
of σi to Sj−1 , ultimately construct the same solution. This happens only when the
original edges of both σi and σj are present in Si−1 , thus application of one swap option
does not influence the application of the other. However, it is desirable to use only one
way to generate solution Sj . In Section 3.3, we propose a variation of the top-down
approach (called LASG) which resolves this issue.
Definition 3.f [Order Relation R̂] We define an order relation, namely R̂, between a pair
of swap options as follows.
a. If there is a path from vi to vr in T̂αβ , where ei and er are OR edges, σqi and σrj are
swap options, then (σqi , σrj ) ∈ R̂. For example, in Figure 4 (σ(3,4) , σ(13,14) ) ∈ R̂.
b. If σpq = hep , eq , δpq i and σrt = her , et , δrt i are two swap options such that vq = vr ,
then (σpq , σrt ) ∈ R̂. In Figure 4 (σ(2,3) , σ(3,4) ) ∈ R̂.
⊔
⊓
Implicit Representation of the Solutions : We use an implicit representation for
storing every solution other than the optimal one. These other solutions can be constructed
from the optimal solution by applying a set of swap options to the optimal solution in the
following way. If (σi , σj ) ∈ R̂, σi has to be applied before σj . Therefore, every solution is
represented as a sequence Σ̂ of swap options, where σi appears before σj in Σ̂ if (σi , σj ) ∈ R̂.
Intuitively the application of every swap option specifies that the swapped edge will be the
part of the solution. Since the swap options are applied in the specific order R̂, it may so
happen that an OR edge which had become the part of solution due to the application of an
earlier swap option and may get swapped out due to the application of a later swap option.
286

Generating Ordered Solutions for Explicit AND/OR Structures

Definition 3.g [Superfluous Swap Option] Consider a sequence of swap options Σ̂ =
hσ1 , · · · , σm i corresponding to a solution Sm . Clearly it is possible for a swap option, σi ,
where 1 ≤ i ≤ m, to be present in the sequence such that the original edge of σi is not
present in the solution Si−1 which is constructed by the successive applications of swap
options σ1 , · · · , σi−1 to solution Sopt . Now the application of σi has no effect on Si−1 , i.e.,
solution Si is identical to solution Si−1 . Each such swap option σi is a superfluous swap
option with respect to the sequence Σ̂ of swap options corresponding to solution Sm .
⊔
⊓
Property 3.1 The sequence of swap options corresponding to a solution is minimal, if it
has no superfluous swap option.
This property follows from the definition of superfluous swap options and the notion of the
implicit representation of a solution.
Definition 3.h [Signature of a Solution] The minimal sequence of swap options corresponding to a solution, Sm , is defined as the signature, Sig(Sm ), of that solution. It
may be noted that for the optimal solution Sopt of any alternating AND/OR tree T̂αβ ,
Sig(Sopt ) = {}, i.e., an empty sequence. It is possible to construct more than one signature
for a solution, as R̂ is a partial order. It is important to observe that all different signatures
for a particular solution are of equal length and the sets of swap options corresponding to
these different signatures are also equal. Therefore the set of swap options corresponding
to a signature is a canonical representation of the signature. Henceforth we will use the set
notation for describing the signature of a solution.
v1

2, 39

σ2,3 : 5

h3i

3, 29

σ3,4 : 1
h2i

v3

v2

h1i

2, 37

v4

35
h5i

2, 8

h1i

v5

h3i

h4i

v6

3, 11

v7

v8

4, 17

12
h2i

h2i
h1i σ
9,10 : 3

h3i

σ11,12 : 4

h1i

h1i

h2i

σ13,14 : 3 σ14,15 : 6

v9

v10

v11

v12

v13

v14

v15

5

7

6

9

12

15

20

Figure 5: A solution, S2 , of the AND/OR tree shown in Figure 4
In Figure 5 we show a solution, say S2 , of the AND/OR tree shown in Figure 4. The
solution is highlighted using thick dashed lines with arrow head. The pair, cv (vq ), C(S2 , vq ),
287

Ghosh, Sharma, Chakrabarti, & Dasgupta

is shown within rectangles beside each node vq in solution S2 , and we have used the rectangles with rounded corner whenever C(S2 , vq ) 6= Copt (vq ). Since S2 is generated by applying
the swap option σ(2,3) to solution Sopt , the signature of S2 , Sig(S2 ) = hσ(2,3) i. Consider
another sequence, Σ̂2 = hσ(2,3) , σ(9,10) i, of swap options. It is worth noting that Σ̂2 also
represents the solution S2 . Here the second swap option in Σ̂2 , namely σ9,10 , can not be
applied to the solution constructed by applying σ(2,3) to Sopt as the source edge of σ(9,10) ,
e9 , is not present in that solution. Hence σ(9,10) is a superfluous swap option for Σ̂2 .
Definition 3.i [Vopt and Eopt ] For any solution graph Sm of an AND/OR DAG Gαβ , we
define a set of nodes,
 Vopt (Sm ), and a set of OR edges, Eopt (Sm ), as:
a. Vopt (Sm	) = vq  vq in Sm and solution graph Sm (vq ) is identical to the solution graph
opt(vq )


	
b. Eopt (Sm ) = epr  OR edge epr in Sm , and vr ∈ Vopt (Sm )
Clearly, for any node vq ∈ Vopt (Sm ), if vq is present in Sopt , then – (a) the solution graph
Sm (vq ) is identical to the solution graph Sopt (vq ), and (b) C(Sm , vq ) = Copt (vq )
⊔
⊓
Definition 3.j [Swap List] The swap list corresponding to a solution Sm , L(Sm ), is the list
of swap options that are applicable to Sm . Let Sig(Sm ) = {σ1 , · · · , σm } and ∀i, 1 ≤ i ≤ m,
each swap option σi belongs to node vpi . The application of all other swap options that
belong to the OR edges in the path from the root node to vpi is invalidated in the solution
Sm . Hence, only the remaining swap options that are not invalidated in Sm can be applied
to Sm for constructing the successor solutions of Sm .
It is important to observe that for a swap option σi , if the source edge of σi belongs
to Eopt (Sm ), the application is not invalidated in Sm . Hence, for a solution Sm , we construct L(Sm ) by restricting the swap operations only on the edges belonging to Eopt (Sm ).
Moreover, this condition also ensures that the cost of a newly constructed solution can be
computed directly form the cost of the parent solution and the δ value of the applied swap
′ is constructed form S
option. To elaborate, suppose solution Sm
m by applying σjk . The
′
′ ) = C(S ) + δ
cost of Sm can be computed directly form C(Sm ) and σjk as : C(Sm
m
jk if
ej ∈ Eopt (Sm ). Procedure ComputeSwapList(Sm ) describes the details of computing swap
options for a given solution Sm .
⊔
⊓
Procedure ComputeSwapList(Sm)
1
2
3

4
5
6

L(Sm ) ← ∅; Compute Eopt (Sm );
foreach OR edge ec in Eopt (Sm ) do
if there exists a swap option on edge ec then
/* Suppose ec emanates from OR node vq such that ec = L(vq )[i]. Also ec is
marked with the pair hδtmp , en i, where en = L(vq )[i + 1]
*/
σcn ← hec , en , δtmp i; Add σcn to L(Sm );
end
end

The swap list of the optimal solution, L(Sopt ), in Figure 4, is {σ(2,3) , σ(9,10) }. In the
solution S1 , shown in Figure 6, Vopt = {v6 , v10 }, because except node v6 and v10 , for all
other nodes vi in S1 , opt(vi ) 6= S1 (vi ). Here also rectangles with rounded corner are used
when C(S1 , vq ) 6= Copt (vq ). Therefore, Eopt = {e6 , e10 }. Since there exists no swap option
288

Generating Ordered Solutions for Explicit AND/OR Structures

v1

2, 37

σ2,3 : 5

h3i

3, 32

σ3,4 : 1
h2i

v2

v3

h1i

2, 37

v4

35
h5i

2, 11

h1i

v5

h4i

v6

3, 11

h3i

v7

v8

4, 17

12
h2i
h1i σ
9,10 : 3

h2i

h3i

σ11,12 : 4

h1i

h1i

h2i

σ13,14 : 3 σ14,15 : 6

v9

v10

v11

v12

v13

v14

v15

5

7

6

9

12

15

20

Figure 6: A solution, S1 , of the AND/OR tree shown in Figure 4
on the OR edges, e6 and e10 , the swap list of solution S1 , L(S1 ) = ∅. Hence, for a solution
Sm , L(Sm ) may be empty, though Vopt (Sm ) can never be empty.
Although we use the notation σij to denote a swap option with edge ei as the original
edge and edge ej as the swapped edge, for succinct representation, we also use σ with a single
subscript, such as σ3 , σk , σij etc., to represent a swap option. This alternative representation
of swap options does not relate to any edge.
Definition 3.k [Successors and Predecessors of a Solution] The set of successors and
predecessors of a solution
 Sm is defined as:
′  S ′ can be constructed from S
a. Succ(Sm ) = {Sm
m by applying a swap option that
m
belongs to the swap
 list of Sm }
′′  S ∈ Succ(S ′′ )}
⊔
⊓
b. P red(Sm ) = {Sm
m
m

Property 3.2 For any solution Sm of an alternating AND/OR tree T̂αβ the following state′ ∈ P red(S ), C(S ′ ) ≤ C(S )
ment holds: ∀Sm
m
m
m

The property follows from the definitions. One special case requires attention. Consider
′ ) = C(S ) and S ′ ∈ P red(S ). This case can only arise when a swap
the case when C(Sm
m
m
m
option of cost 0 is applied to Sm . This occurs in the case of a tie.
3.2.1 ASG Algorithm
We present ASG, a best first search algorithm, for generating solutions for an alternating
AND/OR tree in non-decreasing order of costs. The overall idea of this algorithm is as
follows. We maintain a list, Open, which initially contains only the optimal solution Sopt .
At any point of time Open contains a set of candidate solutions from which the next best
289

Ghosh, Sharma, Chakrabarti, & Dasgupta

solution in the non-decreasing order of cost is selected. At each iteration the minimum cost
solution (Smin ) in Open is removed from Open and added to another list, named, Closed.
The Closed list contains the set of ordered solutions generated so far. Then the successor
set of Smin is constructed and any successor solution which is not currently present in
Open as well as is not already added to Closed is inserted to Open. However as a further
optimization, we use a sublist of Closed, named TList, to store the relevant portion of Closed
such that checking with respect to the solutions in TList is sufficient to figure out whether
the successor solution is already added to Closed. It is interesting to observe that this
algorithm can be interrupted at any time and the set of ordered solutions computed so far
can be obtained. Also, the algorithm can be resumed if some more solutions are needed.
The details of ASG algorithm are presented in Algorithm 4.
Algorithm 4: Alternative Solution Generation (ASG) Algorithm

1

2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20

input : An alternating AND/OR tree T̂αβ
output: Alternative solutions of T̂αβ in the non-decreasing order of cost
Compute the optimal solution Sopt , perform OR edge marking and populate the
swap options;
Create three lists, Open, Closed, and TList, that are initially empty;
Put Sopt in Open;
lastSolCost ← C(Sopt );
while Open is not empty do
Smin ← Remove the minimum cost solution from Open ;
if lastSolCost < C(Smin ) then
Remove all the elements of TList;
lastSolCost ← C(Smin );
end
Add Smin to Closed and TList;
Compute the swap list, L(Smin ), of Smin ;
/* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open
*/
foreach σij ∈ L(Smin ) do
Construct Sm by applying σij to Smin ;
Construct the signature of Sm , Sig(Sm ), by concatenating σij after Sig(Smin );
/* Check whether Sm is already present in Open or in TList
*/
if (Sm not in Open) and (Sm not in TList) then
Add Sm to Open;
end
end
Report the solutions in Closed;

The pseudo-code from Line-1 to Line-4 computes the optimal solution Sopt , performs the
marking of OR edges, populates the swap options, and initializes Open, Closed and TList.
The loop in Line-10 is responsible for generating a new solution every time it is executed
as long as Open is not empty. In Line-6 of the ASG algorithm, the solution that is the
current minimum cost solution in Open (Smin ) is selected and removed from Open. The
TList is populated and maintained from Line-7 to Line-10. The loop in Line-13 generates

290

Generating Ordered Solutions for Explicit AND/OR Structures

the successor solutions of Smin one by one and adds the newly constructed solutions to
Open if the newly constructed solution is not already present in Open as well as not added
to TList (Line-16 does the checking). The proof of correctness of Algorithm 4 is presented
in Appendix A. We discuss the following issues related to Algorithm 4.
Checking for Duplication : In order to check whether a particular solution Si is already
present in Open or TList, the signature of Si is matched with the signatures of the solutions
that are already present in Open and TList. It is sufficient to check the equality between the
set of swap options in the respective signatures because that set is unique for a particular
solution. It may be noted that TList is used as an optimization, which avoids searching the
entire Closed list.
Resolving Ties : While removing the minimum cost solution from the Open list, a tie
may be encountered among a set of solutions. Suppose there is a tie among the set Stie =
{S1 , · · · , Sk }. The ties are resolved in the favor of the predecessor solutions, that is,
 

∀Si , Sj ∈ Stie , (If Si is the predecessor of Sj ) ⇒ (Si is removed before Sj )
For all other cases the ties are resolved arbitrarily in the favor of the solution which was
added to Open first.
3.2.2 Working of ASG Algorithm
We illustrate the working of the ASG algorithm on the example AND/OR tree shown in
Figure 4. The contents of the different lists obtained after first few iterations of outermost
while loop are shown in Table 1. We use the signature of a solution for representation
purpose. The solutions that are already present in Open and also constructed by expanding
the current Smin , are highlighted with under-braces.
It.
1
2
3
4

Smin
{}
{σ(9,10) }
{σ(2,3) }
{σ(2,3) , σ(3,4) }

L(Smin )
σ(2,3) , σ(9,10)
∅
σ(3,4)
σ(11,12) , σ(13,14)

5

{σ(2,3) , σ(3,4) ,
σ(13,14) }

σ(11,12) , σ(14,15)

6

{σ(2,3) , σ(3,4) ,

σ(13,14)

σ(11,12) }

7

{σ(2,3) , σ(3,4) ,
σ(13,14) , σ(11,12) }

σ(14,15)

Open
{σ(2,3) }, {σ(9,10) }
{σ(2,3) }
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) , σ(11,12) },
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) },
{σ(2,3) , σ(3,4) , σ(13,14) , σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }
{σ(2,3) , σ(3,4) , σ(13,14) , σ(11,12) },
|
{z
}
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }

Closed
{}
{}, {σ(9,10) }
{}, {σ(9,10) }, {σ(2,3) }
{}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) }
{}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) , σ(13,14) }
{}, {σ(9,10) }, {σ(2,3) },

TList
{}
{σ(9,10) }
{σ(2,3) }
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) ,
σ(13,14) }
{σ(2,3) , σ(3,4),

{σ(2,3) , σ(3,4) }
σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }, {}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) ,
{σ(2,3) , σ(3,4) , σ(13,14) ,
{σ(2,3) , σ(3,4) }
σ(13,14) , σ(11,12) }
σ(11,12) , σ(14,15) }
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) ,
σ(11,12) }

Table 1: Working of ASG Algorithm
291

Ghosh, Sharma, Chakrabarti, & Dasgupta

Before entering the outermost while loop (Line 5), ASG computes the optimal solution
Sopt , populates the swap options, and inserts Sopt to Open. Thus, at this point of time, Open
contains only the optimal solution Sopt ; Closed and TList are empty. In the first iteration
Sopt (the signature of Sopt is {}) is selected and removed from Open. Then the swap list of
Sopt , L(Sopt ), is computed. L(Sopt ), consists of two swap options, namely σ(2,3) and σ(9,10) .
ASG adds two new solutions {σ(2,3) } and {σ(9,10) } to Open. Then solution Sopt is added to
both Closed and TList.
In the next iteration, solution {σ(9,10) } which has the minimum cost among the solutions
currently in Open, is selected and removed from Open, the swap list {σ(9,10) } is computed and
subsequently {σ(9,10) } is added to Open and TList. As it happens, L({σ(9,10) }) = ∅ (owing to
the fact that Eopt = {e6 , e10 } and there exists no swap option on the OR edges, e6 and e10 ),
thus nothing else happens in this iteration. In the next iteration, solution {σ(2,3) } is removed
from Open and ultimately solution {σ(2,3) , σ(3,4) } is added to Open after adding {σ(2,3) } to
Closed as well as to TList. Next two iterations proceed in a similar fashion. Now, consider the
6th iteration. In this iteration, solution {σ(2,3) , σ(3,4) , σ(11,12) } is removed from Open, and its
successor set has only one solution, {σ(2,3) , σ(3,4) , σ(11,12) , σ(13,14) }, which is already present
in Open (inserted to Open in Iteration-5). Therefore, solution {σ(2,3) , σ(3,4) , σ(11,12) , σ(13,14) }
is not inserted to Open again. We have shown up to Iteration-7 in Table 1.
3.3 Technique for Avoiding the Checking for Duplicates in Open
In this section, we present a technique to avoid the checking done before adding a newly
constructed solution Sm to Open to determine whether Sm is already present in Open. We
first explain the scenario with an example, which is a portion of the previous example
shown in Figure 4. In Figure 7-10, the solutions are shown using thick dashed line with
arrow head. Also the rectangles with rounded corner are used to highlight the fact that the
corresponding node in the marked solution does not belong to the Vopt set of that solution.
v4

2, 37
h4i

3, 11
h2i

4, 17
h3i

σ11,12 : 4

h4i

h3i

v7
h1i

v4

2, 44

v8

3, 15

h1i

σ13,14 : 3

h3i

v7

4, 20
h3i

h2i

v8
h1i

h1i

v11

v12

v13

v14

v11

v12

v13

v14

6

9

12

15

6

9

12

15

Figure 8: Solution S3

Figure 7: Running Example

Consider the solutions S1 , S2 and S3 (shown in Figure 9, Figure 10 and Figure 8). Here
(a) L(Sopt ) = {σ(11,12) , σ(13,14) }, (b) Succ(Sopt ) = {S1 , S2 },
(c) Sig(S1 ) = {σ(13,14) }, (d) Sig(S2 ) = {σ(11,12) }, and (e) Sig(S3 ) = {σ(13,14) , σ(11,12) }.
Algorithm 4 constructs the solution S3 (shown in Figure 8) for adding to Open twice –
(i) as a part of adding Succ(S1 ) to Open, and (ii) while adding Succ(S2 ) to Open.
292

Generating Ordered Solutions for Explicit AND/OR Structures

v4

2, 40

h4i

3, 11
h2i

4, 20
h3i

σ11,12 : 4

h4i

h3i

v7

v4

2, 41

v8

h1i

3, 15

h1i

h3i

v7

4, 17
h3i

h2i

h1i

v8
h1i

σ13,14 : 3

v11

v12

v13

v14

v11

v12

v13

v14

6

9

12

15

6

9

12

15

Figure 9: Solution S1

Figure 10: Solution S2

We use the following definitions to describe another version of the ASG algorithm, which
constructs the solutions in such a way that the check to find out whether a solution is already
added to Open is avoided.
Definition 3.l [Solution Space DAG(SSDAG)] The solution space DAG of an alternating
AND/OR tree T̂αβ is a directed acyclic graph (DAG), G s = hV, Ei, where V is the set of all
possible solutions of the AND/OR tree T̂αβ , and E is the set of edges which is defined as:



 Sp , Sm ∈ V, and



E = espm  espm is a directed edge from node Sp to Sm , and


 Sm ∈ Succ(Sp )

Clearly Sopt is the root node of G s .

⊔
⊓

Definition 3.m [Solution Space Tree and Completeness] A solution space tree of an
alternating AND/OR tree T̂αβ is a tree T s = hV t , E t i where V t ⊆ V, where V is the set of
all possible solutions of the AND/OR tree T̂αβ , and E t is the set of edges which is defined
as:



 Sp , Sm ∈ V t , and







s

e
is
a
directed
edge
from
node
S
to
S
,
and
p
m
s  pm
t
E = epm 

 Sp ∈ P red(Sm ), and


 

 ∀S ′ ∈ P red(Sm ), (Sp 6= S ′ ) ⇒ there is no edge between S ′ and Sm . 
p
p
p
The sibling set for a solution Sm , is denoted using Sib(T s , Sm ). A solution space tree T s
for an AND/OR tree is complete if V t = V.
⊔
⊓
It may be noted that the complete solution space tree of an alternating AND/OR tree
is not necessarily unique. It is possible for an alternating AND/OR tree to have more than
one complete solution space tree. However the solution space DAG for any AND/OR tree
is unique.
Definition 3.n [Native Swap Options of a Solution] Consider a solution Sm of an alternating AND/OR tree T̂αβ . Suppose Sm is constructed by applying swap option σij to
solution Sp . Since swap option σij = hei , ej , δij i is used to construct Sm , AND node vj is
present in Sm . The native swap options of solution Sm with respect to swap option σij ,
N (Sm , σij ), is a subset of L(Sm ), and comprises of the following swap options :
293

Ghosh, Sharma, Chakrabarti, & Dasgupta

v1

2, 49

σ2,3 : 5

h3i

3, 32

σ3,4 : 1
h2i

v2

v3

h1i

2, 43

v4

35
h5i

2, 11

h1i

v5

h3i

h4i

v6

3, 11

v7

v8

4, 23

12
h2i
h1i σ
9,10 : 3

h2i

h3i

σ11,12 : 4

h1i

h1i

h2i

σ13,14 : 3 σ14,15 : 6

v9

v10

v11

v12

v13

v14

v15

5

7

6

9

12

15

20

Figure 11: A solution, S4 , of the AND/OR tree shown in Figure 4
a. σjk , where σjk is the swap option on the edge ej
b. each σt , if σt belongs to an OR node vq where vq is a node in Sm (vj )
We use the term N (Sm ) to denote the native swap options when σij is understood from
the context. Intuitively the native swap options for solution Sm are the swap options that
become available immediately after applying σij , but were not available in the predecessor
solution of Sm .
⊔
⊓

Consider the solution S4 shown in Figure 11 where Sig(S4 ) = {σ(2,3) , σ(3,4) , σ(13,14) }. The
solution is highlighted using thick dashed lines with arrow head. We have used the rectangles with rounded corner beside each node vq in solution S4 , where C(S4 , vq ) 6= Copt (vq ).
Suppose S4 is constructed form solution S3 (where Sig(S3 ) = {σ(2,3) , σ(3,4) }) using swap
option σ(13,14) . Here N (S4 , σ(13,14) ) = {σ(14,15) } whereas L(S4 ) = {σ(11,12) , σ(14,15) }. Now
consider solution S6 where Sig(S6 ) = {σ(2,3) , σ(3,4) , σ(11,12) , σ(13,14) ). It is worth observing that applying only the native swap options to S4 instead of all swap options in L(S4 )
prevents the construction of solution S6 from solution S4 . S6 can also be constructed by
applying σ(13,14) to solution S5 , where Sig(S5 ) = {σ(2,3) , σ(3,4) , σ(11,12) }. However, it may
be noted that σ(13,14) is not a native swap option of solution S5 .
3.3.1 Lazy ASG Algorithm
The intuition behind the other version of the ASG algorithm is as follows. For a newly
constructed solution Sm , we need to check whether Sm is already present in Open because
Sm can be constructed as a part of computing the successor set of multiple solutions.
Instead of using the entire swap list of a solution to construct all successors at once and
then add those solutions to Open, using the native swap options for constructing a subset of
the successor set ensures the following. The subset constructed using native swap options
294

Generating Ordered Solutions for Explicit AND/OR Structures

consists of only those solutions that are currently not present in Open and thus can be
added to Open without comparing with the existing entries in Open. The construction of
′ of S
each remaining successor solution Sm
m and then insertion to Open is delayed until
′ is added to Closed.
every other predecessor solution of Sm
Algorithm 5: Lazy ASG (LASG) Algorithm

1

2
3
4
5
6
7
8
9
10
11

12

13

14
15
16

17
18

19
20
21
22
23
24
25
26
27
28

input : An alternating AND/OR tree T̂αβ
output: Alternative solutions of T̂αβ in the non-decreasing order of cost
Compute the optimal solution Sopt , perform OR edge marking and populate the
swap options;
Create two lists, Open and Closed, that are initially empty;
Put Sopt in the Closed list;
Create a solution space tree T s with Sopt as root;
Compute the swap list, L(Sopt ), of Sopt ;
Construct Succ(Sopt ) using L(Sopt );
forall Sm ∈ Succ(Sopt ) do
Add Sm to Open;
end
while Open is not empty do
Smin ← Remove the minimum cost solution from Open ;
/* Suppose Smin is constructed from Sm applying swap option σij
*/
Add a node corresponding to Smin in T s and connect that node using an edge
from Sm ;
Compute the swap list L(Smin ) and the list of native swap options N (Smin , σij );
/* Expansion using native swap options
*/
foreach σtmp ∈ N (Smin , σij ) do
Construct Stmp from Smin by applying σtmp ;
Construct the signature of Stmp , Sig(Stmp ), by concatenating σtmp after
Sig(Smin );
Add Stmp to Open;
end
/* Lazy Expansion
*/
s
forall Sp ∈ Sib(T , Smin ) do
if σij ∈ L(Sp ) then
Construct Sp′ from Sp using σij ;
Construct the signature of Sp′ , Sig(Sp′ ), by concatenating σij after Sig(Sp );
Add Sp′ to Open;
end
end
Add Smin to Closed;
end
Report the solutions in Closed;

The solution space tree T s is maintained throughout the course of the algorithm to
′ is added to Closed. Based on this idea we
determine when every other predecessor of Sm
295

Ghosh, Sharma, Chakrabarti, & Dasgupta

present a lazy version of ASG algorithm, named LASG. After selecting the minimum cost
solution from Open, the algorithm explores the successor set of the current minimum cost
solution in a lazy fashion. For a solution Sm , at first a subset of Succ(Sm ) is constructed
using only the native swap options of Sm . The other solutions that belong to Succ(Sm )
are explored as late as possible as described above. For resolving ties, LASG algorithm
uses the same strategy which is used by ASG algorithm. The details of LASG algorithm
are presented in Algorithm 5. The proof of correctness of this algorithm is presented in
Appendix B.
Consider the example tree shown in Figure 7 and solutions S1 and S2 (shown in Figure 9
and Figure 10). Initially the Open will contain only Sopt and N (Sopt ) = {σ(11,12) , σ(13,14) }.
When Sopt is selected from Open, both S1 and S2 is added to Open. Next S1 will be selected
followed by S2 . Since, N (S1 ) = ∅ and N (S2 ) = ∅, after selecting S1 or S2 no successor
solutions are constructed using the native swap list. Among the predecessors of S3 , S2 is
added last to Closed. After selecting and removing S2 from Open, solution S3 is constructed
from the previously selected predecessor S1 using the swap option σ(11,12) which is used to
construct solution S2 from Sopt .
3.3.2 Working of LASG Algorithm (on AND/OR tree in Figure 4)
Before entering the outermost while loop (Algorithm 5, Line 10), LASG computes the
optimal solution Sopt and constructs Succ(Sopt ). Then
 the solutions 	in Succ(Sopt ) are
added to Open and the contents of the Open becomes {σ(2,3) }, {σ(9,10) } . The contents of
the different lists when a solution is added to Closed are shown in Table 2. The solutions
are represented using their signatures. The solutions that are added to Open as a result of
lazy expansion, are highlighted using under-brace.
Iteration
1
2
3

Smin
{}
{σ(9,10) }
{σ(2,3) }
{σ(2,3) , σ(3,4) }

N (Smin )
σ(2,3) , σ(9,10)
∅
σ(3,4)
σ(11,12) , σ(13,14)

4

{σ(2,3) , σ(3,4) , σ(13,14) }

σ(14,15)

5

{σ(2,3) , σ(3,4) , σ(11,12) }

∅

6

{σ(2,3) , σ(3,4) , σ(13,14) ,
σ(11,12) }

∅

Open
{σ(2,3) }, {σ(9,10) }
{σ(2,3) }
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) , σ(11,12) },
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) },
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }

Closed
{}
{}, {σ(9,10) }
{}, {σ(9,10) }, {σ(2,3) }
{}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) }
{}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }, {}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) , σ(13,14) , σ(11,12) }
{σ(2,3) , σ(3,4) }
|
{z
}
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) , σ(14,15) }, {}, {σ(9,10) }, {σ(2,3) },
{σ(2,3) , σ(3,4) }
{σ(2,3) , σ(3,4) , σ(13,14) }
{σ(2,3) , σ(3,4) , σ(11,12) }
{σ(2,3) , σ(3,4) , σ(13,14) ,
σ(11,12) }

Table 2: Working of LASG Algorithm
While generating the first four solutions, the contents of the different lists for LASG
are identical to the contents of the corresponding lists of ASG (shown in Table 1). For
296

Generating Ordered Solutions for Explicit AND/OR Structures

each of these soltuions, the native swap list is equal to the actual swap list of that solution. It is worth noting that, unlike ASG, for LASG the outermost while loop starts
after generating the optimal solution Sopt , thus while generating the same solution the
iteration number for LASG is less than that of ASG by 1. In the 4th iteration, for solution S4 = {σ(2,3) , σ(3,4) , σ(13,14) } the native swap list is not equal to the swap list as
described previously. The same holds true for solution S5 = {σ(2,3) , σ(3,4) , σ(11,12) } and
solution S6 = {σ(2,3) , σ(3,4) , σ(13,14) , σ(11,12) }. It is important to observe that LASG adds
the solution S6 = {σ(2,3) , σ(3,4) , σ(13,14) , σ(11,12) } to Open after the generation of solution
S5 = {σ(2,3) , σ(3,4) , σ(11,12) } as a part of lazy expansion (highlighted using under-brace
in Table 2). Whereas, the ASG algorithm adds S6 to Open after generating solution
S4 = {σ(2,3) , σ(3,4) , σ(13,14) }.
3.4 Complexity Analysis and Comparison among ASG, LASG and BU
In this section we present a complexity analysis of ASG and LASG and compare them with
BU. We will use the following parameters in the analysis.
a. nαβ and nβ denote the total number of nodes and the number of OR nodes in an
alternating AND/OR tree.
b. d denotes the out degree of the OR node having maximum number of children.
c. m denotes the maximum number of OR edges in a solution.
d. o denotes the maximum size of Open. We will present the complexity analysis for
generating c solutions. Therefore the size of Closed is O(c).
3.4.1 Complexity of ASG
Time Complexity : The time complexity of the major steps of Algorithm 4 are as
follows.
a. Computing the first solution can be done in bottom-up fashion, thus requiring O(nαβ )
steps. The edges emanating from an OR node are sorted in the non-decreasing order
of aggregated cost to compute the marks of the OR edges, the marking process takes
O nβ .d. log d . Since the value
 of d is not very large in general (can be upper bounded
by a constant), O nβ .d. log d = O(nαβ ).
b. The number of swap options available to a solution can be at most equal to the number
of OR edges in that solution. Thus, the swap list for every solution can be built in
O(m) time. For c solutions, generating swap options take O(c.m).
c. Since the size of the successor set of a solution can be m at most, the size of Open, o
can at most be c.m. Also the size of the TList can at most be equal to c (the size of
Closed).
d. The Open list can be implemented using Fibonacci heap. Individual insert and delete
operation on Open take O(1)(amortized) and O(lg o) time respectively. Hence, for
inserting in the Open and deleting from Open altogether takes O(o. lg o) time which
is O(c.m. log(c.m)).
e. The checking for duplicates requires scanning the entire Open and TList. Since the
length of TList can be at most c, for a newly constructed solution this checking takes
O(c + o) time and at most O(c + o) solutions are generated. Since O(c + o) is actually
O(o), for generating c solutions, this step takes O(o)2 time. Also, the maximum value
297

Ghosh, Sharma, Chakrabarti, & Dasgupta

of o can be O(c.m). Thus, the time complexity of this step is O(c.m)2 . Clearly this
step dominates O(o. lg o) which is the total time taken for all insertions into the Open
and deletions from Open.
However, this time bound can be further improved if we maintain a hash map of the
solutions in the Open and TList, and in this case the checking for duplicates can be
done in O(o) time. In that case O(o. lg o) (total time taken for all insertions into the
Open and deletions from Open) becomes dominant over the time required for checking
for duplicates.
f. An upper limit estimate of m could be made by estimating the size of a solution tree
√
which is nαβ for regular and complete alternating AND/OR trees. It is important
to observe that the value of m is independent of the average out degree of a node in
T̂αβ .
Combining
together we get the time complexity
of ASG algorithm as :

 factors

 the above
2
2
2
2
O nαβ + o = O nαβ + (c.m) = O nαβ + c .nαβ = O(c .nαβ )
Howeverif the additional
hash
is further reduced to : 

 map is used the time complexity

√
√
O nαβ + o. lg o = O nαβ + c. nαβ . lg(c.nαβ ) = O nαβ + nαβ .(c. lg c + c. lg nαβ )
Space Complexity: The following data-structures primarily contribute to the space complexity of ASG algorithm.
a. Three lists, namely, Open, Closed, and TList are maintained throughout the course of
the running ASG. This contributes a O(o + c) factor, which is O(o).
b. Since the number of swap options is upper bounded by the total number of OR edges,
constructing the swap list contributes the factor, O(nβ .d) to the space complexity.
Also marking a solution requires putting a mark at every OR node of the AND/OR
tree, thus adding another O(nβ ) space which is clearly dominated by the previous
O(nβ .d) factor.
c. Since the signature of a solution is essentially a set of swap options, the size of a
signature is upper bounded by the total number of swap options available. Combining
the Open and Closed list, altogether (c + o) solutions need to be stored.
Since (c + o)

is O(o), total space required for storing the solutions is O o.nβ .d .
Combining
 the above factors
 together we get the space complexity of ASG algorithm as :
O o + nβ .d + o.nβ .d = O(o.nβ .d)
When an additional
hash map is used to improve the time complexity, another addi
tional O o.nβ .d space is required for maintaining the hash map. Although the exact space
requirement is doubled, asymptotically the space complexity remains same.
3.4.2 Complexity of LASG
Time Complexity : Compared to Algorithm 4, Algorithm 5 does not check for the
duplicates and adds the solution to Open only when it is required. Therefore the other
terms in the complexity remain the same except the term corresponding to the checking for
duplicates. However, here T s is created and maintained during the course of Algorithm 5.
Creating and maintaining the tree require O(c) time. Also during the lazy expansion the
swap list of the previously generated sibling solutions are searched (Line 19 and Line 20 of
Algorithm 5). The size of the swap list of any solution is O(m), where m is the maximum
number of OR edges in a solution. Also there can be at most O(m) sibling solutions for a
298

Generating Ordered Solutions for Explicit AND/OR Structures

solution. Therefore the complexity of the lazy expansion is O(c.m2 ). Since O(c.m2 ) is the
dominant factor, the time complexity of LASG is O(c.m2 ) = O(c.nαβ ).
Space Complexity : Compared to ASG algorithm, LASG algorithm does not maintain
the TList. However LASG maintains the solution space tree T s whose size is equal to
the Closed list, thus adding another O(c) factor to the space complexity incurred by ASG
algorithm. It is interesting to observe that the worst case space complexity remains O(o +
nβ .d + o.nβ .d) = O(o.nβ .d) which is equal to the space complexity of ASG algorithm.
3.4.3 Comparison with BU
The time complexity of generating the c best solutions for an AND/OR tree is O(nαβ .c. log c)
and the space complexity is O(nαβ .c). The detailed analysis can be found in the work
of Elliott (2007). Since, nβ .d = O(nαβ ), the space complexity of both ASG and LASG
algorithm reduces to O(nαβ .c) and the time complexity of LASG is log c factor better than
BU whereas the time complexity of ASG is quadratic with respect to c compared to the
(c. log c) factor of BU. When an additional hash-map is used to reduce the time overhead of
duplicate checking, ASG beats both LASG
 and BU both in terms time complexity, as both
√
O(nαβ ) and O nαβ .(c. lg c + c. lg nαβ ) is asymptotically lower than O(nαβ .c. log c).
However this worst case complexity is only possible for AND/OR trees where no duplicate solution is generated. Empirical results show that the length of Open, o hardly reaches
O(c.m).

4. Ordered Solution Generation for AND/OR DAGs
In this section, we present the problem of generating solutions in non-decreasing order
of cost for a given AND/OR DAG. We present the working of the existing algorithm for
generating solution for both tree based semantics and default semantics. Next we present
the modifications in ASG and LASG for handling DAG.
4.1 Existing Bottom-Up Algorithm
Figure 12 shows an example working of the existing bottom-up approach, BU, on the
AND/OR DAG in Figure 2. We use the notations that are used in Figure 3 to describe
different solutions in Figure 12 and the generation of the top 2 solutions under tree-based
semantics is shown.
It is important to notice that although BU correctly generates alternative solutions of
an AND/OR DAGs under tree based semantics, BU may generate some solutions which are
invalid under default semantics. In Figure 13 we present a solution of the AND/OR DAG
in Figure 2. This solution is an example of such a solution which is correct under tree-based
semantics but is invalid under default semantics. The solution DAG (highlighted using
thick dashed lines with arrow heads) in Figure 13 will be generated as the 3rd solution of
the AND/OR DAG in Figure 2 while running BU. At every non-terminal node, the entry
(within rectangle) corresponding to the 3rd solution is highlighted using bold face. It may
be noted that the terminal nodes, v9 and v10 , are included in the solution DAG though both
of them emanate from the same parent OR node. Therefore, this solution is not a valid one
under default semantics.
299

Ghosh, Sharma, Chakrabarti, & Dasgupta

2, 89

v1

v2 v3
1 : |1, 1|, 89
2 : |2, 1|, 90

h1i
3, 43

v2

1 : hv5 , 1i, 43
2 : hv4 , 1i, 44

v4
40

h1i

v7

1 : hv5 , 1i, 41
2 : hv5 , 2i, 44

3, 43

h1i

v7 v8
1 : |1, 1|, 35
2 : |2, 1|, 38

v2

1 : hv9 , 1i, 9
2 : hv10 , 1i, 12

v6

v4

52

40

1 : hv5 , 1i, 43
2 : hv4 , 1i, 44

v5

2, 35

v8

3, 9
h1i

v7

2, 41

v3

h4i

1 : hv5 , 1i, 41
2 : hv5 , 2i, 44
h1i

v7 v8
1 : |1, 1|, 35
2 : |2, 1|, 38

v6
52

h3i

h4i

17

h2i

h2i

h5i

h1i

h3i

h4i
3, 9

v3

h4i
v5

2, 35

2, 41

v1

h1i

h2i

h5i

h1i

2, 89

v2 v3
1 : |1, 1|, 89
2 : |2, 1|, 90
3 : |1, 2|, 92

1 : hv2 , 1i, 34
2 : hv2 , 2i, 37

v8
17

h2i

v9

v10

v9

v10

5

7

5

7

Figure 13: A solution (tree based semantics)

Figure 12: BU approach for AND/OR DAG

Proposed Extension of BU to Generate Alternative Solutions under Default
Semantics : We propose a simple top-down traversal and pruning based extension of
BU to generate alternative solutions under default semantics. While generating the ordered
solutions at any AND node vq by combining the solutions of the children, we do the following.
For each newly constructed solution rooted at vq , a top-down traversal of that solution
starting from vq is done to check whether more than two edges of an OR node are present
in that particular solution (a violation of the default semantics). If such a violation of the
default semantics is detected, that solution is pruned from the list of alternative solutions
rooted at vq . Therefore, at every AND node, when a new solution is constructed, an
additional top-down traversal is used to detect the semantics violation.
4.2 Top-Down Method for DAGs
The proposed top-down approaches (ASG and LASG) are also applicable for AND/OR
DAGs to generate alternative solution DAGs under default semantics. Only the method of
computing the cost increment after the application of a swap option needs to be modified to
incorporate the fact that an OR node may be included in a solution DAG through multiple
paths from the root node. We use the notion of participation count for computing the cost
increment.
Participation Count : The notion of participation count is applicable to the intermediate
nodes of a solution DAG as follows. In a solution DAG, the participation count of an
intermediate node, vq , is the total number of distinct paths connecting the root node, vR ,
and vq . For example, in Figure 14, the optimal solution DAG is shown using thick dashed
lines with arrow heads, and the participation count for every intermediate OR nodes are
shown within a circle beside the node.
300

Generating Ordered Solutions for Explicit AND/OR Structures

v1

h2i

h1i
1

h1i

v2

3, 43

1

h5i

σ2,5,4 : 1

v4

v1

2, 89

2

h4i

v5

2

h1i

v7

v3

2, 41

σ3,5,6 : 14

1

h1i

2, 35

v2

3, 44

v6

v4

52

40

1

h5i

h1i

h4i

v5

1

1

17
h2i

h1i

2, 41

σ3,5,6 : 14

h1i

v6

2, 35
h3i

h4i

v8

v3

52

h3i

3, 9

σ7,9,10 : 3

h2i

h1i

40
h4i

2, 90

v7

v8

3, 9

σ7,9,10 : 3

17

h2i

v9

v10

v9

v10

5

7

5

7

Figure 15: Solution DAG S1

Figure 14: AND/OR DAG

We use the notation σijk to denote a swap option in the context of AND/OR DAGs,
where swap option σijk belongs to node vi , the source edge of the swap option is eij from
node vi to node vj , and the destination edge is eik from node vi to node vk .
4.2.1 Modification in the Proposed Top-Down Approach
The ASG algorithm is modified for handling AND/OR DAGs in the following way. The
computation of the successor solution in Line 14 of Algorithm 4 is modified to incorporate
the participation count of the OR node to which the applied swap option belongs. The
overall method is shown in Algorithm 6(in the next page).
In order to apply LASG on AND/OR DAGs, apart from using the above mentioned
modification for computing the cost of a newly generated solution, another modification
is needed for computing the native swap options for a given solution. The modification is
explained with an example. Consider the solution, S1 , shown in Figure 15. S1 is highlighted
using thick dashed lines with arrow heads. The pair, cv (vq ), C(S1 , vq ), is shown within
rectangles beside each node vq ; rectangles with rounded corner are used when C(S1 , vq ) 6=
Copt (vq ). Swap option σ(2,5,4) was applied to Sopt to generate S1 . After the application
of swap option σ(2,5,4) , the participation count of node v5 is decremented to 1. Therefore
in S1 there is a path from the root node to node v5 and so node v5 is still present in S1 .
As a result, the swap option σ(7,9,10) is available to S1 with a participation count equal to
1 for node v7 , whereas σ(7,9,10) is available to its parent solution Sopt with participation
count 2 for node v7 . In other words, σ(7,9,10) is not available to S1 and its parent solution
Sopt with the same value of participation count for node v7 . Therefore σ(7,9,10) becomes the
native swap option of S1 . The generalized definition of native swap options for a solution
is presented below.
Definition 4.o [Native Swap Options of a Solution] Consider a solution Sm of an
AND/OR DAG Gαβ , where Sm is constructed by applying swap option σhij to solution
Sp . Since swap option σhij = hehi , ehj , δhij i is used to construct Sm , AND node vj belongs
301

Ghosh, Sharma, Chakrabarti, & Dasgupta

to Sm . Similarly, if the participation count of node vi remains greater than zero after applying σhij to Sm , node vi belongs to Sm . The native swap options of solution Sm with
respect to swap option σhij , N (Sm , σhij ), a subset of L(Sm ), comprises of the following
swap options :
a. σhjk , where σhjk is the swap option on the edge ehj
b. each σt , if σt belongs to an OR node vq where vq is a node in Sm (vj )
c. each σt′ , if node vi is present in Sm and σt′ belongs to an OR node vq where vq is a
node in Sm (vi ).
We use the term N (Sm ) to denote the native swap options when σhij is understood from
the context. Intuitively the native swap options for solution Sm are the swap options that
become available immediately after applying σhij , but were not available in the predecessor
solution of Sm .
⊔
⊓
Algorithm 6: ASG Algorithm for AND/OR DAGs
input : An AND/OR DAG Gαβ
output: Alternative solutions of Gαβ in the non-decreasing order of cost
1 Compute the optimal solution Sopt , perform OR edge marking and populate the
swap options;
2 Create three lists, Open, Closed, and TList, that are initially empty;
3 Put Sopt in Open;
4 lastSolCost ← C(Sopt );
5 while Open is not empty do
6
Smin ← Remove the minimum cost solution from Open;
7
if lastSolCost < C(Smin ) then
8
Remove all the elements of TList;
9
lastSolCost ← C(Smin );
10
end
11
Add Smin to Closed and TList;
12
Compute the swap list, L(Smin ), of Smin ;
/* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open
*/
13
foreach σij ∈ L(Smin ) do
14
Construct Sm by applying σij to Smin ;
15
Construct the signature of Sm , Sig(Sm ), by concatenating σij after Sig(Smin );
16
Let σij belongs to OR node vq , p is the participation count of vq , and δ is the
cost increment for σij ;
17
C(Sm ) = C(Sm ) + p × δ;
/* Check whether Sm is already present in Open or in TList
*/
18
if (Sm not in Open) and (Sm not in TList) then
19
Add Sm to Open;
20
end
21 end
22 Report the solutions in Closed;
It is worth noting that Definition 4.o of native swap option is a generalization of the
earlier definition of native swap option (Definition 3.n), defined in the context of trees. In
302

Generating Ordered Solutions for Explicit AND/OR Structures

the case of trees, the participation count of any node can be at maximum 1. Therefore,
after the application of a swap option to a solution, the participation count of the node,
to which the original edge of the swap option points to, becomes 0. Therefore the third
condition is never applicable for trees.
LASG (Algo. 5) can be applied on AND/OR DAGs, with the mentioned modification
for computing the cost of a newly generated solution and the general definition of native
swap option to generate ordered solutions under default semantics.
4.2.2 Working of ASG and LASG Algorithm on AND/OR DAG
We describe the working of ASG algorithm on the example DAG shown in Figure 2. Before
entering the outermost while loop, TList and Closed are empty, and Open contains the
optimal solution Sopt . The contents of the different lists obtained after first few cycles of
outermost while loop are shown in Table 3. Each solution is represented by its signature.
The solutions that are already present in Open and also constructed by expanding the
current Smin , are highlighted with under-braces. For example, the solution {σ(2,5,4) , σ(3,5,6) }
which is added to Open in Iteration 2 (while constructing the successor solutions of {σ(2,5,4) })
constructed again in Iteration 5 while expanding solution {σ(3,5,6) }.
L(Smin )
Open
σ(2,5,4) , σ(3,5,6) , σ(7,9,10)
{σ(2,5,4) }, {σ(3,5,6) }, {σ(7,9,10) }
σ(3,5,6) , σ(7,9,10)
{σ(3,5,6) }, {σ(7,9,10) }, {σ(2,5,4) , σ(3,5,6) },
{σ(2,5,4) , σ(7,9,10) }
3 {σ(2,5,4) , σ(7,9,10) }
∅
{σ(3,5,6) }, {σ(7,9,10) }, {σ(2,5,4) , σ(3,5,6) },

It.
1
2

Smin
{}
{σ(2,5,4) }

4

{σ(7,9,10) }

∅

{σ(3,5,6) }, {σ(2,5,4) , σ(3,5,6) },

5

{σ(3,5,6) }

σ(2,5,4) , σ(7,9,10)

{σ(2,5,4) , σ(3,5,6) }, {σ(3,5,6) , σ(7,9,10) }
|
{z
}

Closed
{}
{}, {σ(2,5,4) }
{}, {σ(2,5,4) }
{σ(2,5,4) , σ(7,9,10) }
{}, {σ(2,5,4) },
{σ(2,5,4) , σ(7,9,10) },
{σ(7,9,10) }
{}, {σ(2,5,4) },
{σ(2,5,4) , σ(7,9,10) },
{σ(7,9,10) }, {σ(3,5,6) }

Table 3: Example Working of ASG Algorithm on the DAG shown in Figure 2
Now we illustrate the working of LASG algorithm on the example DAG shown in Figure 2. The contents of the different lists when a solution is added to Closed are shown
in Table 4. It is worth noting that for solution S1 = {σ2,5,4 }, the swap list L(S1 ) =
{σ(3,5,6) , σ(7,9,10) } whereas the native swap list N (S1 ) = {σ(7,9,10) }. The solutions that are
added to Open as a result of lazy expansion, are highlighted using under-brace. For example,
in Iteration 7 LASG adds the solution S5 = {σ(2,5,4) , σ(3,5,6) } to Open after the generation
of solution S4 = {σ3,5,6 } as a part of lazy expansion, whereas the ASG algorithm adds S5
to Open after generating solution S1 = {σ2,5,4 }.
4.2.3 Generating Solutions under Tree Based Semantics
Unlike the default semantics, ASG or LASG does not have any straight forward extension
for generating solutions under tree based semantics. In Figure 13 we show an example
solution which is valid under tree based semantics, but invalid under default semantics,
because both OR edges emanating form the OR node v7 , namely e(7,9) and e(7,10) , are
303

Ghosh, Sharma, Chakrabarti, & Dasgupta

N (Smin )
Open
σ(2,5,4) , σ(3,5,6) , σ(7,9,10) {σ(2,5,4) }, {σ(3,5,6) }, {σ(7,9,10) }
σ(7,9,10)
{σ(3,5,6) }, {σ(7,9,10) },
{σ(3,5,4) , σ(7,9,10) }
2 {σ(2,5,4) , σ(7,9,10) }
∅
{σ(3,5,6) }, {σ(7,9,10) },

It.
1

Smin
{}
{σ(2,5,4) }

3

{σ(7,9,10) }

∅

{σ(3,5,6) }

4

{σ(3,5,6) }

σ(7,9,10)

{σ(3,5,6) , σ(7,9,10) },
{σ(2,5,4) , σ(3,5,6) }
|
{z
}

Closed
{}
{}, {σ(2,5,4) }
{}, {σ(2,5,4) }
{σ(2,5,4) , σ(7,9,10) }
{}, {σ(2,5,4) },
{σ(2,5,4) , σ(7,9,10) },
{σ(7,9,10) }
{}, {σ(2,5,4) },
{σ(2,5,4) , σ(7,9,10) },
{σ(7,9,10) }, {σ(3,5,6) }

Table 4: Example Working of LASG Algorithm on the DAG shown in Fugure 2

present in this solution. These two OR edges are included in the solution through two
different paths emanating form the root node, v1 . As the existing bottom-up approach
stores the alternative solutions at each node in terms of the solutions of the children of that
node, this representation allows these different paths to be stored explicitly, thus making
BU amenable for generating alternative solutions under tree-based semantics.
On the contrary, our approach works top-down using a compact representation (signature) for storing the solutions. In this signature based representation, it is currently not
possible to store the fact that a particular OR node is included in the solution through two
different paths which may select different child of that OR node. If we use the equivalent
tree constructed form the given graph, our compact representation will work correctly, because in that case, each node would be reachable from the root node through at most one
path. An AND/OR DAG can be converted to its equivalent AND/OR tree representation
using procedure ConvertDAG (described in Section 2) and then ASG or LASG can be applied on the equivalent tree representation in order to generate the alternative solutions
correctly under tree-based semantics. However, in the worst case, procedure ConvertDAG
incurs a space explosion which will blow up the worst case complexity of both ASG and
LASG algorithms. Using our compact representations to generate the ordered solutions
under tree-based semantics for a given AND/OR DAG while containing the space explosion
such that the worst case complexity of our algorithms remain comparable with BU turns
out to be an interesting open problem.

5. Experimental Results and Observations
To obtain an idea of the performance of the proposed algorithms and to compare with
the existing approach, we have implemented the ASG, LASG and BU (existing bottom-up
approach) and tested on the following test domains.
a. A set of synthetically generated AND/OR trees;
b. Tower of Hanoi (TOH) problem;
c. A set of synthetically generated AND/OR DAGs;
d. Matrix-chain multiplication problem; and
e. The problem of determining the secondary structure of RNA sequences.
304

Generating Ordered Solutions for Explicit AND/OR Structures

It may noted that in our implementation of the ASG algorithm, we have implemented the
more space efficient version of ASG algorithm (without a separate hash-map for storing the
solutions in Open and Closed, thereby incurring an extra overhead in time for duplication
checking). Another important point is that for every test case the reported running time of
ASG and LASG for generating a particular number of solutions includes the time required
for constructing the optimal solution graph. The details of the different test domains are
as follows.
5.1 Complete Trees
We have generated a set of complete d-ary alternating AND/OR trees by varying – (a) the
degree of the non-terminal nodes (denoted by d), and (b) the height (denoted by h).
(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

100 solutions
ASG
LASG
BU
0.027
0.005
0.004
0.216
0.010
0.015
1.170
0.031
0.068
6.072
0.124
0.257
30.434
0.517
1.180
130.746 2.265
4.952
0.046
0.006
0.005
0.528
0.017
0.037
5.812
0.106
0.343
66.313
1.552
3.973
636.822 12.363 31.043
0.144
0.011
0.033
2.916
0.056
0.573
58.756
1.266
7.698
0.334
0.012
0.081
12.227
0.177
2.066
0.699
0.022
0.161
32.620
0.654
7.464
1.306
0.030
0.287
81.197
1.786 15.892

300 solutions
ASG
LASG
BU
0.086
0.014
0.009
1.448
0.035
0.046
10.098
0.094
0.184
57.757
0.348
0.777
278.453 1.433
3.917
T
6.443
13.277
0.196
0.015
0.018
4.764
0.060
0.153
55.170
0.290
1.733
620.996 3.712
14.323
T
34.150 128.314
1.041
0.025
0.092
25.341
0.181
1.561
544.989 3.327
27.063
2.792
0.036
0.400
102.577 0.443
11.717
5.384
0.071
1.418
288.257 1.566
37.758
12.006
0.092
1.833
785.160 4.284 102.431

500 solutions
ASG
LASG
BU
0.186
0.023
0.020
4.137
0.060
0.097
27.354
0.216
0.407
158.520 0.524
1.641
766.201 2.806
7.257
T
10.306 29.703
0.459
0.026
0.042
10.345
0.088
0.457
156.158 0.494
4.913
T
6.607
33.923
T
55.510 303.785
2.610
0.042
0.123
69.596
0.264
2.107
T
5.172
38.606
7.374
0.062
0.930
283.689 0.827
26.994
15.133
0.134
2.235
832.235 2.594
90.465
29.870
0.179
4.322
T
6.890 241.064

Table 5: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions
for complete alternating AND/OR trees (T denotes the timeout after 15 minutes)
These trees can be viewed as the search space for a gift packing problem, where
(a) the terminal nodes represent the cost of elementary items,
(b) the OR nodes model a choice among the items (elementary or composite in nature)
represented by the children, and
(c) the AND nodes model the repackaging of the items returned by each of the children.
Every packaging incurs a cost which is modeled by the cost of the intermediate AND nodes.
Here the objective is to find the alternative gifts in the order of non-decreasing cost.
Table 5 shows the time required for generating 100, 300, and 500 solutions for various
complete alternating AND/OR trees. We have implemented the ASG, LASG and the
existing bottom-up algorithm and the corresponding running time is shown in the column
with the heading ASG, LASG and BU, respectively. We have used a time limit of 15 minutes
305

Ghosh, Sharma, Chakrabarti, & Dasgupta

(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

ASG
12.633
52.770
116.582
287.898
664.789
1785.156
17.270
82.609
335.301
1474.477
9139.312
40.285
213.816
1563.770
64.879
529.738
97.703
1264.828
137.527
2628.461

100 solutions
LASG
BU
13.168
11.047
26.152
48.484
63.254
198.234
173.730
797.234
413.855
3193.234
1257.387 12777.234
17.258
11.688
48.086
111.438
184.375
1009.188
1071.352 9088.938
7872.055 81806.688
24.469
47.453
128.629
767.453
1158.582 12287.453
40.355
88.281
343.254
2217.188
58.191
151.047
862.332
5449.797
90.703
242.219
1995.195 11882.781

ASG
28.105
144.730
341.227
832.562
1767.867
T
47.531
235.855
926.004
3234.523
T
121.336
559.734
3209.145
182.270
1254.715
270.027
2747.238
369.086
4869.551

300 solutions
LASG
BU
32.293
14.266
75.355
69.953
165.824
292.703
399.445
1183.703
804.801
4747.703
2047.859 19003.703
49.230
14.812
134.102
152.062
376.766
1387.312
1656.844 12504.562
9565.598 112559.812
67.102
112.609
284.922
1826.359
1699.191 29246.359
110.480
225.781
596.957
5675.000
148.453
372.141
1273.641 13433.391
205.914
576.594
2627.211 28295.281

ASG
41.676
230.168
566.766
1396.758
2942.629
T
76.270
393.113
1507.973
T
T
199.254
917.824
T
305.891
2008.344
443.656
4203.957
606.133
T

500 solutions
LASG
BU
49.832
16.609
128.934
87.922
269.766
373.172
612.184
1514.172
1197.266
6078.172
2849.617
24334.172
80.980
17.938
219.555
192.688
577.766
1765.438
2238.152
15920.188
11251.035 143312.938
116.535
129.016
451.223
2105.266
2240.012
33725.266
179.801
363.281
858.852
9132.812
245.227
593.234
1695.684
21416.984
317.492
910.969
3273.703
44707.781

Table 6: Comparison of space required (in KB) for generating 100, 300, and 500 solutions
for complete alternating AND/OR trees

and the entries marked with T denotes that the time-out occurred for those test cases. The
space required for generating 100, 300, and 500 solutions is reported in Table 6. It can
be observed that in terms of both time and space required, LASG outperforms both ASG
and BU. Between ASG and BU, for most of the test cases BU performs better than ASG
with respect to the time required for generating a specific number of solutions. The space
requirement of ASG and BU for generating a specific number of solutions has an interesting
correlation with the degree(d) and height(h) parameter of the tree. For low numerical values
of the d and the h parameter, e.g., (d, h) combinations like (2, 7), (3, 5) etc., BU performs
better than ASG. On the contrary, for the other combinations, where at least one of these
d and h parameters has a high value, e.g., (d, h) combinations like (2, 17), (7, 5), (4, 9) etc.,
ASG outperforms BU.
5.1.1 Experimentation with Queue with Bounded Length
Since the Open can grow very rapidly, both ASG and LASG incur a significant overhead
in terms of time as well as space to maintain the Open list when the number of solutions
to be generated is not known a priori. In fact, for ASG checking for duplicates in Open is
actually the primary source of time complexity and storing the solutions in Open is a major
contributing factor in space complexity. If the number of solutions that have to generated is
known a priori, the proposed top-down approach can leverage the fact by using a bounded
length queue for implementing Open. When a bounded length queue is used, the time
requirement along with space requirement decreases significantly.
306

Generating Ordered Solutions for Explicit AND/OR Structures

(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

100 solutions
ASG
LASG
BU
0.011
0.008
0.004
0.030
0.011
0.015
0.051
0.031
0.068
0.125
0.103
0.257
0.473
0.421
1.180
2.129
2.199
4.952
0.012
0.009
0.005
0.031
0.018
0.037
0.133
0.102
0.343
1.246
1.143
3.973
10.713 10.313 31.043
0.019
0.008
0.033
0.071
0.055
0.573
1.099
0.998
7.698
0.025
0.013
0.081
0.201
0.161
2.066
0.036
0.018
0.161
0.543
0.460
7.464
0.042
0.029
0.287
1.940
1.705 15.892

300 solutions
ASG LASG
BU
0.003 0.002
0.009
0.008 0.006
0.046
0.020 0.011
0.184
0.043 0.059
0.777
0.168 0.164
3.917
0.766 1.005 13.277
0.003 0.002
0.018
0.012 0.006
0.153
0.048 0.043
1.733
0.477 0.636 14.323
4.160 5.555 128.314
0.006 0.004
0.092
0.026 0.023
1.561
0.443 0.552 27.063
0.009 0.031
0.400
0.083 0.078 11.717
0.014 0.011
1.418
0.240 0.325 37.758
0.020 0.013
1.833
0.807 0.843 102.431

500 solutions
ASG LASG
BU
0.005 0.004
0.020
0.014 0.008
0.097
0.023 0.017
0.407
0.065 0.058
1.641
0.254 0.346
7.257
1.146 1.492 29.703
0.005 0.004
0.042
0.019 0.010
0.457
0.071 0.061
4.913
0.693 0.905 33.923
6.013 7.890 303.785
0.010 0.006
0.123
0.038 0.033
2.107
0.641 0.808 38.606
0.015 0.008
0.930
0.116 0.153 26.994
0.021 0.010
2.235
0.326 0.431 90.465
0.025 0.022
4.322
0.870 1.125 241.064

Table 7: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions
for complete alternating AND/OR trees with bounded length Open queue for ASG
and LASG
(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

ASG
10.109
23.875
54.609
135.477
361.859
1071.258
12.008
39.469
169.469
971.930
7075.109
20.664
116.609
1082.633
33.344
324.258
51.742
825.859
78.141
1919.805

100 solutions
LASG
BU
2.383
11.047
4.883
48.484
14.883
198.234
54.883
797.234
214.883
3193.234
854.883 12777.234
2.617
11.688
11.160
111.438
88.047
1009.188
780.027
9088.938
7007.852 81806.688
5.016
47.453
57.016
767.453
889.016 12287.453
10.195
88.281
217.715
2217.188
19.773
151.047
657.648
5449.797
35.742
242.219
1677.051 11882.781

ASG
27.781
64.430
141.203
317.445
738.992
1845.562
34.609
101.320
353.477
1529.031
8763.023
56.703
247.320
1607.859
84.422
565.531
121.031
1227.742
169.297
2542.047

300 solutions
LASG
BU
5.508
14.266
8.008
69.953
18.008
292.703
58.008
1183.703
218.008
4747.703
858.008
19003.703
5.742
14.812
14.285
152.062
91.172
1387.312
783.152
12504.562
7010.977 112559.812
8.141
112.609
60.141
1826.359
892.141
29246.359
13.320
225.781
220.840
5675.000
22.898
372.141
660.773
13433.391
38.867
576.594
1680.176 28295.281

ASG
45.789
104.117
225.969
497.508
1114.422
2615.656
57.617
163.102
537.328
2085.367
10457.797
93.031
377.922
2132.516
135.812
806.797
190.758
1628.797
260.406
3163.438

500 solutions
LASG
BU
8.633
16.609
11.133
87.922
21.133
373.172
61.133
1514.172
221.133
6078.172
861.133
24334.172
8.867
17.938
17.410
192.688
94.297
1765.438
786.277
15920.188
7014.102 143312.938
11.266
129.016
63.266
2105.266
895.266
33725.266
16.445
363.281
223.965
9132.812
26.023
593.234
663.898
21416.984
41.992
910.969
1683.301 44707.781

Table 8: Comparison of space required (in KB) for generating 100, 300, and 500 solutions
for complete alternating AND/OR trees with bounded length Open queue for ASG
and LASG

307

Ghosh, Sharma, Chakrabarti, & Dasgupta

We show the effect of using bounded length queue to implement Open in Table 7 (reporting the time requirement) and in Table 8 (reporting the memory usage) for generating
100, 300, and 500 solutions, where the number of solutions to be generated are known beforehand. Table 7 and Table 8 show that in this case both ASG and LASG outperforms
BU in terms of time as well as space requirements. Particularly, ASG performs very well in
this setting, outperforming LASG in some cases.
5.1.2 Experimentation to Compare the Incremental Nature
The proposed top-down algorithms are incremental in nature whereas the existing bottomup approach is not incremental. After generating a specified number of ordered solutions,
our methods can generate the next solution incrementally without needing to restart itself,
whereas the existing approach needs to be restarted. For example, after generating the
first 10 ordered solutions, ASG and LASG generate the 11th solution directly from the data
structures maintained so far by these algorithms and perform necessary updates to these
data structures. Whereas, BU needs to be restarted with input parameter 11 for generating
the 11th solution. In Table 9 we compare the time needed to generate the subsequent 11th
solution and 12th solution incrementally after generating first 10 solutions. In order to have
more clarity in the comparison among the running times of the respective algorithms, we
have used higher precision (upto the 6th decimal place) while reporting the running time
in Table 9. Clearly, both ASG and LASG outperform BU for generating the 11th and 12th
solution in terms of the time requirement.
(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

first 10
0.002403
0.009111
0.028519
0.097281
0.396460
1.561020
0.001692
0.012097
0.097356
0.934389
7.898530
0.005833
0.051598
0.813028
0.051530
0.172475
0.053422
0.502939
0.033831
1.198354

ASG
11th
0.000201
0.001957
0.003311
0.014776
0.063641
0.251839
0.000158
0.001542
0.013046
0.127943
1.082319
0.000650
0.006956
0.110205
0.001327
0.024262
0.002701
0.061417
0.003706
0.156145

12th
0.000201
0.001302
0.003533
0.015929
0.059229
0.277763
0.000151
0.001572
0.014405
0.156579
1.194090
0.000671
0.007196
0.124750
0.001641
0.024438
0.003092
0.069727
0.003846
0.166501

first 10
0.001003
0.003023
0.006700
0.025877
0.102493
0.446899
0.000683
0.004084
0.031159
0.311128
2.811539
0.002143
0.017046
0.294561
0.004638
0.059751
0.005282
0.184584
0.012862
0.466560

LASG
11th
0.000240
0.000714
0.001250
0.004113
0.014490
0.061422
0.000176
0.000583
0.003948
0.033169
0.282836
0.000303
0.002209
0.027612
0.000753
0.006116
0.000636
0.017116
0.001266
0.038792

12th
0.000123
0.000629
0.001346
0.004918
0.020031
0.082366
0.000112
0.000959
0.004604
0.047594
0.387715
0.000582
0.003115
0.037281
0.000652
0.007197
0.001087
0.024042
0.001282
0.061305

first 10
0.001344
0.003596
0.014628
0.059326
0.238418
0.962635
0.001055
0.009507
0.085610
0.778298
7.037050
0.004181
0.044913
0.727766
0.005963
0.152285
0.010895
0.406947
0.018185
0.929941

BU
11th
0.001359
0.003696
0.015046
0.061393
0.246042
0.989777
0.001101
0.009931
0.089379
0.811176
7.313715
0.004434
0.047867
0.775950
0.006358
0.162527
0.011604
0.435398
0.019567
0.989326

12th
0.001397
0.003895
0.015521
0.062717
0.251746
1.015848
0.001133
0.010336
0.093419
0.846578
7.608619
0.004725
0.050940
0.823442
0.006782
0.173191
0.012556
0.465301
0.020896
1.052566

Table 9: Comparison of running time (in seconds) for generating for first 10 solutions and
then the 11th solution and 12th solution incrementally for complete alternating
AND/OR trees

308

Generating Ordered Solutions for Explicit AND/OR Structures

5.2 Multipeg Tower of Hanoi Problem
Consider the problem of Multipeg Tower of Hanoi (Majumdar, 1996; Gupta, Chakrabarti,
& Ghose, 1992). In this problem, ρ pegs are fastened to a stand. Initially γ disks rest on
the source peg A with small disk on large disk ordering. The objective is to transfer all
γ disks from A to the destination peg B with minimum legal moves. In a legal move, the
topmost disk from any tower can be transferred to any other peg with a larger disk as the
topmost disk. The problem of multi-peg tower of Hanoi can be solved recursively as follows.
a. Move recursively the topmost k (k varies from 1 to γ − 1) disks from A to some
intermediate peg, I, using all the pegs.
b. Transfer the remaining γ − k disks from A to B recursively, using the (ρ − 1) pegs
available.
c. Recursively move k disks that were transferred to I previously, from the intermediate
peg I to B, using all the ρ pegs.
It may be noted that there is a choice for the value of k, which may take any value from 1
to γ − 1. Solutions with different values of k may take different number of moves, and the
solution which incurs minimum number of moves is the optimal solution. This choice of
the value of k is modeled as an OR node, and for every such choice, the problem is divided
into three sub-problems. This decomposition into sub-problems is modeled as an AND
node. Therefore, the search spaces of the multi-peg Tower of Hanoi problem correspond to
alternating AND/OR trees.
#disks
8
9
10
11
12
13

100 solutions
ASG
LASG
BU
0.034
0.030
0.069
0.119
0.116
0.264
0.479
0.635
1.310
2.421
2.178
3.171
7.453
7.448 11.437
25.379 25.115 38.458

300 solutions
ASG
LASG
BU
0.104
0.084
0.252
0.314
0.289
0.942
1.706
1.658
3.305
6.573
6.161
12.998
21.232 21.081 43.358
68.574 67.170 140.392

500 solutions
ASG
LASG
BU
0.200
0.138
0.577
0.590
0.458
2.183
2.303
2.829
7.592
10.651
9.678
29.242
35.825
35.663
99.593
112.411 112.470 332.113

#Opt. No.
of Moves
23
27
31
39
47
55

Table 10: Comparison of running time (in seconds) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different
number of disks
#disks
8
9
10
11
12
13

100 solutions
ASG
LASG
BU
ASG
36.664
43.008
416.312
64.516
96.211
111.320
1471.656
131.266
295.672
341.000
5074.219
326.352
957.336
1113.508 17197.312
999.602
3155.086 3664.117 57512.812
3198.156
10339.078 12022.883 190297.969 10412.242

300 solutions
LASG
BU
ASG
80.734
660.062
105.039
154.266
2359.156
166.789
383.453
8161.719
373.453
1158.797 27728.562
1039.367
3719.352 92906.562
3247.547
12078.914 307872.969 10483.570

500 solutions
LASG
BU
117.008
903.812
197.859
3246.656
427.766
11249.219
1204.719 38259.812
3767.617 128300.312
12137.242 425447.969

Table 11: Comparison of space required (in KB) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different number
of disks
We have used the search space of 5 peg Tower of Hanoi problem with different number of
disks, γ, and generated alternative solutions in non-decreasing order of cost using ASG and
309

Ghosh, Sharma, Chakrabarti, & Dasgupta

LASG algorithms. Here the cost function expresses the number of legal moves. The value
of γ is varied from 8 to 13, and in Table 10 and in Table 11, we report the time required and
space required, respectively, for generating 100, 300, and 500 solutions for every test cases.
Experimental results show that the performance of ASG is similar to the performance of
LASG with respect to both space and time. However ASG as well as LASG outperforms
BU with respect to both time and space requirements.
5.3 Randomly Constructed AND/OR DAGs
We have constructed a set of randomly generated AND/OR DAGs and evaluated the ASG,
LASG, and BU algorithm for generating solutions under default semantics. We have used
the proposed extension to the BU algorithm for generating solutions under default semantics.
nαβ

d

60
220
920
33
404
2124
9624
144
744
8844
40884

2
2
2
3
3
3
3
4
4
4
4

100 solutions
ASG LASG
BU
0.027 0.006 0.039
0.060 0.009 0.096
0.363 0.020 0.106
0.020 0.006 0.019
0.203 0.018 0.067
3.550 0.045 0.730
26.659 0.201 14.620
0.065 0.008 0.034
0.877 0.025 0.400
7.422 0.160 26.683
T
1.972
T

300 solutions
ASG
LASG
BU
0.089
0.021 0.158
0.281
0.030 1.100
2.485
0.059 0.266
0.123
0.021 0.098
1.483
0.048 0.257
30.302 0.126 1.681
257.605 0.612 33.382
0.348
0.027 0.217
6.910
0.069 0.994
69.097 0.449 66.558
T
5.819
T

500 solutions
ASG
LASG
BU
0.172
0.033
0.282
0.594
0.051
3.665
6.163
0.100
0.528
0.280
0.032
0.245
4.043
0.083
0.541
85.863 0.215
2.766
710.708 1.194 52.406
0.817
0.049
2.251
18.823 0.118
1.365
194.452 0.927 109.076
T
9.426
T

Table 12: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for AND/OR DAGs (T denotes the timeout after 15 minutes)
nαβ

d

60
220
920
33
404
2124
9624
144
744
8844
40884

2
2
2
3
3
3
3
4
4
4
4

ASG
11.609
23.141
74.082
13.914
48.867
229.820
772.441
30.648
121.535
471.625
2722.938

100 solutions
LASG
BU
8.875
8.125
16.219
31.312
39.000
106.875
10.492
8.172
35.445
66.938
118.707
389.844
339.676 1996.875
17.332
29.609
65.578
287.109
266.078 2729.297
1256.535
T

ASG
32.852
62.516
220.648
46.117
151.363
705.809
2245.938
85.781
381.133
1183.379
T

300 solutions
LASG
BU
30.797
10.906
46.711
49.562
105.852
172.562
32.539
11.297
101.168
98.188
312.246
621.094
825.984 3321.875
53.961
73.359
168.305
737.891
550.477 6945.703
2353.562
T

ASG
54.094
100.555
371.344
77.445
262.816
1200.336
3732.523
140.312
659.434
1927.961
T

500 solutions
LASG
BU
50.035
13.250
74.379
65.188
168.375
230.375
54.602
14.422
163.273
129.438
507.762
852.344
1327.406 4646.875
93.539
86.641
275.594
883.984
843.484 8419.922
3447.809
T

Table 13: Comparison of space required (in KB) for generating 100, 300, and 500 solutions
for AND/OR DAGs

Table 12 and Table 13 compare the time required and space required for running ASG,
LASG and BU for generating 100, 300, and 500 solutions for every test cases. The first
and second columns of every row provide the size (nαβ ) and the average out-degree (d) of
the DAG. The results obtained for this test domain are similar to the results for randomly
310

Generating Ordered Solutions for Explicit AND/OR Structures

constructed AND/OR trees. It may be noted that in terms of both time and space required,
LASG outperforms both ASG and BU. Between ASG and BU, for most of the test cases
BU performs better than ASG with respect to the time required for generating a specific
number of solutions. Whereas, the space requirement of ASG and BU for generating a
specific number of solutions has an interesting co-relation with the average degree(d) and
the size (nαβ ) parameter of the DAG. For low numerical values of the d and the nαβ
parameter, e.g., (nαβ , d) combinations like (60, 2), (33, 3) etc., BU performs better than
ASG. On the contrary, for the other combinations, where at least one of these nαβ and d
parameter has a high value, e.g., (nαβ , d) combinations like (920, 2), (9624, 3), (40884, 4)
etc., ASG outperforms BU.
5.4 Matrix-Chain Multiplication Problem
We have also used the well-known matrix-chain multiplication (Cormen, Stein, Rivest, &
Leiserson, 2001) problem for experimentation. The search space of the popular dynamic
programming formulation of this problem correspond to AND/OR DAG.
DAG
Cnstr.
#matrices
Time
(Sec)
20
0.033
30
0.200
40
0.898
50
3.033
60
8.335
70
19.591
80
41.960
90
82.578
100
151.814

Sopt
Cnstr.
Time
(Sec)
0.001
0.003
0.008
0.016
0.029
0.046
0.071
0.101
0.143

10 solutions

15 solutions

20 solutions

ASG

LASG

BU

ASG

LASG

BU

ASG

LASG

BU

0.003
0.009
0.019
0.047
0.088
0.140
0.209
0.296
0.409

0.002
0.008
0.018
0.048
0.090
0.142
0.212
0.300
0.412

0.206
2.785
15.580
93.267
342.212
862.387
T
T
T

0.004
0.012
0.024
0.062
0.118
0.187
0.280
0.396
0.546

0.003
0.010
0.024
0.065
0.120
0.190
0.282
0.398
0.548

0.288
4.087
23.414
140.513
509.906
T
T
T
T

0.005
0.015
0.030
0.079
0.148
0.235
0.351
0.496
0.688

0.004
0.012
0.030
0.081
0.151
0.238
0.354
0.499
0.683

0.373
5.406
31.112
187.227
678.718
T
T
T
T

Table 14: Comparison of time required (in seconds) for AND/OR DAGs corresponding to
the search spaces of matrix-chain multiplication with different number of matrices, (T denotes the timeout after 15 minutes)
#matrices
20
30
40
50
60
70
80
90
100

ASG
19.641
66.367
156.559
308.984
537.383
859.844
1290.117
1843.828
2537.582

10 solutions
LASG
20.203
69.273
160.227
315.012
545.117
869.160
1301.406
1857.480
2556.883

BU
160.918
555.684
1317.637
2563.965
4411.855
6978.496
T
T
T

ASG
20.543
67.809
157.738
310.277
538.930
862.133
1293.148
1847.602
2542.746

15 solutions
LASG
21.227
70.695
161.785
316.543
546.512
870.867
1303.426
1859.812
2560.043

BU
234.305
821.902
1960.281
3825.223
6592.508
T
T
T
T

ASG
21.914
69.516
158.758
311.551
539.914
863.977
1295.852
1851.164
2549.352

20 solutions
LASG
22.773
72.523
162.852
318.145
547.551
872.219
1305.090
1861.789
2566.992

BU
303.973
1081.902
2594.207
5075.262
8759.441
T
T
T
T

Table 15: Comparison of space required (in KB) for AND/OR DAGs corresponding to the
search spaces of matrix-chain multiplication with different number of matrices

Given a sequence of matrices, A1 , A2 , · · · , An , of n matrices where matrix Ai has dimension pi−1 × pi , in this problem the objective is to find the most efficient way to multiply
311

Ghosh, Sharma, Chakrabarti, & Dasgupta

these matrices. The classical dynamic programming approach works as follows. Suppose
A[i,j] denotes matrix that results from evaluating the product, Ai Ai+1 · · · Aj , and m[i, j]
is the minimum number of scalar multiplications required for computing the matrix A[i,j] .
Therefore, the cost of optimal solution is denoted by m[i, j] which can be recursively defined
as :

m[i, j] =


0,

 min

i≤k<j



if i = j;
	
m[i, k] + m[k + 1, j] + pi−1 × pk × pj , if i < j.

The choice of the value of k is modeled as OR node and for every such choice, the problem
is divided into three sub-problems. This decomposition into sub-problems is modeled as
an AND node. It is worth noting that unlike the search space of 5-peg ToH problem, the
search space of the matrix-chain multiplication problem corresponds to AND/OR DAG.
We have used the search space for different matrix sequences having varying length and
generated alternative solutions in the order of non-decreasing cost. In Table 14, we report
the time required and in Table 15, we report the memory used for generating 10, 15, and
20 solutions for every test cases.
In Table 14, for each test case, we also report the time required for constructing the
explicit AND/OR DAG from the recursive formulation in the 2nd column, and the optimal
solution construction time in the 3rd column. It is interesting to observe that the relative
performance of ASG and LASG for this search space is very similar to that obtained for 5peg ToH search space though this search space for this domain is AND/OR DAG. Both ASG
and LASG perform approximately the same with respect to time and space requirement.
However, the advantage of ASG as well as LASG over BU with respect to both time and
space requirement is more significant in this domain.
5.5 Generating Secondary Structure for RNA
Another relevant problem where the alternative solutions play an important role is the
computation of the secondary structure of RNA. RNA molecules can be viewed as strings
of bases, where each base belongs to the set {Adenine, Cytocine, Guanine, U racil} (also
denoted as {A, C, G, U }). RNA molecules tend to loop back and form base pairs with itself
and the resulting shape is called secondary structure (Mathews & Zuker, 2004). The stability
of the secondary structure largely depends on the number of base pairings (in general, larger
number of base pairings implies more stable secondary structure). Although there are other
factors that influence the secondary structure, it is often not possible to express these other
factors using a cost function and they are typically evaluated empirically. Therefore, it is
useful to generate a set of possible alternative secondary structures ordered by decreasing
numbering of base pairings for a given RNA which can be further subjected to experimental
evaluation.
The computation of the optimal secondary structure considering the underlying principle of maximizing the number of base-pairings has a nice dynamic programming formulation (Kleinberg & Tardos, 2005). Given an RNA molecule B = hb1 b2 · · · bn i where each
bi ∈ {A, C, G, U }, the secondary structure on B is a set of base pairings, D = {(i, j)}, where
i, j ∈ {1, 2, · · · n}, that satisfies the following conditions:
312

Generating Ordered Solutions for Explicit AND/OR Structures

Test Case
TC1
TC2
TC3
TC4
TC5
TC6
TC7
TC8
TC9
TC10
TC11
TC12
TC13
TC14

Organism Name
Anaerorhabdus Furcosa
Archaeoglobus Fulgidus
Chlorobium Limicola
Desulfurococcus Mobilis
Haloarcula Japonica
Halobacterium Sp.
Mycoplasma Genitalium
Mycoplasma Hyopneumoniae
Mycoplasma Penetrans
Pyrobaculum Aerophilum
Pyrococcus Abyssi
Spiroplasma Melliferum
Sulfolobus Acidocaldarius
Symbiobacterium Thermophilum

# Bases
114
124
111
129
122
120
104
105
103
131
118
107
126
110

Table 16: Details of the RNA sequences used for Experimentation
a. if (i, j) ∈ D, then i + 4 < j : This condition states that the ends of each pair in D are
separated by at least four intermediate bases.
b. The elements of any pair in D consists of either {A, U } or {C, G} (in either order).
c. No base appears in more than one pairings, i.e., D is a matching.
d. If (i, j) and (k, l) are two pairs in D, then it is not possible to have i < k < l < j, i.e.,
no two pairings can cross each other.
Test
Case
TC1
TC2
TC3
TC4
TC5
TC6
TC7
TC8
TC9
TC10
TC11
TC12
TC13
TC14

DAG Cnstr.
Time (Sec)
34.464
57.999
26.423
83.943
51.290
46.508
16.766
22.775
18.831
91.419
47.660
22.649
67.913
28.911

Sopt Cnstr.
Time (Sec)
0.042
0.057
0.038
0.065
0.051
0.047
0.029
0.033
0.031
0.073
0.047
0.034
0.061
0.038

ASG
0.094
0.126
0.084
0.144
0.114
0.107
0.068
0.077
0.068
0.167
0.111
0.078
0.140
0.087

5 solutions
LASG
BU
0.095 449.916
0.128 823.493
0.089 363.421
0.152 1089.462
0.116 681.429
0.108 598.419
0.069 210.806
0.078 284.455
0.072 233.999
0.170
T
0.109 627.744
0.079 288.520
0.141 962.641
0.085 366.693

ASG
0.145
0.193
0.135
0.230
0.176
0.166
0.101
0.120
0.109
0.249
0.173
0.116
0.206
0.134

10 solutions
LASG
BU
0.148 893.682
0.198
T
0.133 718.326
0.227
T
0.180 1349.181
0.175
T
0.103 410.817
0.122 559.318
0.111 458.290
0.263
T
0.171 1253.034
0.123 573.602
0.218
T
0.137 724.113

ASG
0.197
0.271
0.183
0.314
0.239
0.226
0.136
0.153
0.144
0.347
0.220
0.165
0.290
0.182

15 solutions
LASG
BU
0.202 1359.759
0.277
T
0.186 1077.094
0.317
T
0.245
T
0.238
T
0.144 621.792
0.165 836.359
0.148 683.411
0.355
T
0.240
T
0.167 849.134
0.288
T
0.186 1072.552

Table 17: Comparison of time required (in seconds) for AND/OR DAGs corresponding to
the search spaces of RNA secondary structure with different number of bases (T
denotes the timeout after 30 minutes)

Under the above mentioned conditions the dynamic programming formulation is as follows.
Suppose P (i, j) denotes the maximum number of base pairings in a secondary structure on
bi · · · bj . P (i, j) can be recursively defined as :
P [i, j] =


0,

n

	 o
max P [i, j − 1], max 1 + P [i, k − 1] + P [k + 1, j − 1] ,
i≤k<j

313

if i + 4 ≥ j,

if i + 4 < j.

Ghosh, Sharma, Chakrabarti, & Dasgupta

Here, a choice of the value of k is modeled as an OR node and for every such choice,
the problem is divided into three sub-problems. This decomposition into sub-problems is
modeled as an AND node. We have experimented with the search space of this problem for
the set of RNA molecule sequences obtained from the test-cases developed by Szymanski,
Barciszewska, Barciszewski, and Erdmann (2005). The details of the test cases are shown
in Table 16.
For every test cases, we report the time required in Table 17 for generating 5, 10, and 15
solutions. For the same setting, the space required is reported in Table 18. In Table 17, for
each test case, we also report the time required for constructing the explicit AND/OR DAG
from the recursive formulation in the 2nd column, and the time required for constructing the
optimal solution time in the 3rd column. We use a high value of time-out (1800 seconds) in
order to gather the running time required by BU. We limit the maximum solutions generated
at 15 because for generating higher number of solutions, BU is timed out for most of the
test cases. It is worth noting that the result obtained for this domain is very similar to the
result obtained for the matrix-chain multiplication problem domain. Both space and time
wise ASG and LASG perform similarly and they outperform BU significantly with respect
to time as well as space requirement.
Test
Case
TC1
TC2
TC3
TC4
TC5
TC6
TC7
TC8
TC9
TC10
TC11
TC12
TC13
TC14

ASG
1647.555
2254.531
1473.852
2606.242
2045.930
1912.227
1101.125
1293.812
1170.094
2984.773
1974.695
1295.141
2438.898
1475.477

5 solutions
LASG
1694.688
2310.008
1516.922
2665.820
2097.414
1963.367
1138.633
1333.336
1207.633
3047.539
2022.906
1335.883
2496.469
1517.828

BU
7409.336
9902.953
6629.891
11358.945
9021.273
8499.570
5087.680
5855.547
5352.477
T
8641.422
5924.664
10657.945
6627.844

ASG
1651.273
2258.773
1477.492
2610.875
2049.844
1916.422
1104.422
1297.750
1173.023
2990.211
1979.344
1297.273
2442.961
1478.555

10 solutions
LASG
1697.797
2315.258
1521.750
2671.711
2101.836
1968.305
1142.023
1338.070
1211.523
3053.977
2030.922
1339.516
2502.625
1521.352

BU
14656.469
T
13103.492
T
17875.430
T
10036.938
11560.203
10562.766
T
17119.820
11701.695
T
13099.055

ASG
1654.367
2262.492
1480.555
2615.719
2052.867
1921.117
1108.047
1302.242
1176.352
2994.773
1983.664
1299.805
2447.172
1482.234

15 solutions
LASG
1700.492
2318.008
1526.797
2675.633
2106.000
1972.172
1144.109
1342.484
1213.906
3059.781
2038.461
1341.914
2506.703
1525.344

BU
21846.156
T
19518.625
T
T
T
14924.820
17211.406
15718.617
T
T
17420.719
T
19519.742

Table 18: Comparison of space required (in KB) for AND/OR DAGs corresponding to the
search spaces of RNA secondary structure with different number of bases

5.6 Observations
The experimental data shows that the LASG algorithm generally outperforms the ASG
algorithm and the existing bottom-up approach in terms of the running time for complete
alternating AND/OR trees and AND/OR DAGs. Whereas, for the other problem domains,
i.e., the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem, and the
problem of determining secondary structure of RNA sequences, the overall performance of
the ASG algorithm is similar to the performance of the LASG algorithm. This behavior
can be explained from the average and maximum length statistics of Open list, reported in
Table 19 - Table 23, for these above mentioned test domains.
314

Generating Ordered Solutions for Explicit AND/OR Structures

In the case of complete trees and random DAGs, for ASG algorithm, the average as well
as the maximum size of Open grows much faster than that of LASG algorithm (Table 19
and Table 20), with the increase in the size of the tree/DAG.
(d, h)
(2, 7)
(2, 9)
(2, 11)
(2, 13)
(2, 15)
(2, 17)
(3, 5)
(3, 7)
(3, 9)
(3, 11)
(3, 13)
(4, 5)
(4, 7)
(4, 9)
(5, 5)
(5, 7)
(6, 5)
(6, 7)
(7, 5)
(7, 7)

100 solutions
ASG
LASG
avg.
max.
avg. max.
235
383
75
159
994
1894
73
120
2427
4709
156
306
5546
10947
524
1149
11744
23291
384
523
24264
48333
655
841
304
549
120
242
1561
3015
172
346
5496
10899
191
289
17336
34542
486
691
53139 106155
1138 1216
734
1427
103
176
3748
7383
194
381
16282
32451
422
488
1216
2352
146
307
7261
14446
249
335
1781
3489
141
276
12362
24651
297
342
2433
4765
261
508
19311
38435
450
529

300 solutions
ASG
LASG
avg.
max.
avg. max.
435
629
179
289
2657
4931
220
528
6935
13537
483
1005
16266
32076
1550 2726
34836
69160
677
1121
T
T
1087 1611
740
1323
341
652
4359
8400
579
1260
16272
32244
387
661
51954 103549
956
1754
T
T
1267 1569
2062
4006
256
503
10928
21489
678
1467
48786
97196
687
1131
3407
6555
496
1053
21652
42972
470
888
5089
9911
507
1126
36868
73323
461
789
7072
13910
747
1483
57754 115116
687
961

500 solutions
ASG
LASG
avg.
max.
avg. max.
545
792
236
372
4103
7569
449
1069
11251
21843
851
1771
26748
52724
2261 3844
57673 114367
983
1824
T
T
1527 2819
1107
1972
539
1007
7026
13588
1012 2084
26904
53271
622
1368
T
T
1460 2672
T
T
1432 1776
3322
6375
452
1065
17932
35222
1265 2837
T
T
1025 1807
5508
10694
852
1742
35850
71054
832
1781
8250
16035
971
2164
61221 121958
749
1573
11595
22809
1204 2273
T
T
984
1922

Table 19: Average and maximum length of Open while generating 100, 300, and 500 solutions for complete alternating AND/OR trees
nαβ

d

60
220
920
33
404
2124
9624
144
744
8844
40884

2
2
2
3
3
3
3
4
4
4
4

100 solutions
ASG
LASG
avg.
max.
avg. max.
181
338
39
63
479
854
77
133
1530
2957
116
227
202
409
58
102
1001
1969
236
447
5008
9911
374
626
14422 28666
394
491
510
990
56
101
2407
4760
253
485
7522
14931
258
437
T
T
749
804

300 solutions
ASG
LASG
avg.
max.
avg. max.
428
768
131
282
1144
2058
210
417
4289
8278
332
639
604
1193
154
281
2958
5799
675 1256
14803 29314
851 1569
43087 85825
746 1339
1374
2563
187
458
7166
14204
590 1018
22254 44062
847 1831
T
T
852 1004

500 solutions
ASG
LASG
avg.
max.
avg. max.
643
1138
219
411
1721
3139
329
612
6902
13305
512
946
978
1875
234
422
4874
9781
1013 1810
24442
48357
1337 2527
71547 142327
1254 2756
2140
3996
376
868
11874
23558
885
1655
36743
72740
1565 3493
T
T
961
1215

Table 20: Average and maximum length of Open while generating 100, 300, and 500 solutions for randomly constructed AND/OR DAGs

Since ASG algorithm checks for the presence of duplicates while expanding a solution, the
time required for duplication checking grows rapidly for these test domains. Hence, the
overall time required for generating a specific number of solutions also increases rapidly
(faster than both BU and LASG) with the increase in the size of the tree/DAG. As a result,
BU outperforms ASG with respect to the time requirement for trees and DAGs. However
315

Ghosh, Sharma, Chakrabarti, & Dasgupta

the memory used for generating a specific number of solutions increases moderately (slower
than BU) with the increase in the size of the tree/DAG. Therefore with respect to space
requirement, ASG outperforms BU for larger trees and DAGs.
Between LASG and BU, the time as well as the memory requirement of BU increases
faster than that of LASG when the degree of the AND/OR tree or DAG increases. This
happens because, for BU, the time taken for merging the sub-solutions at the AND nodes
and memory required for storing alternative solutions that are rooted at different nodes
increases rapidly with the increase in the degree of that node.
On the contrary, for the other test domains, 5-peg Tower of Hanoi problem, matrix-chain
multiplication problem, and the probelm of finding secondary structure of RNA sequences,
the average and the maximum size of Open for both ASG and LASG are comparable (Table 21, Table 22 and Table 23). Therefore, for the LASG algorithm, the time saved by
avoiding the duplication checking is compensated by the extra overhead of maintaining the
solution space tree and the checks required for lazy expansion. Hence the running time as
well as the space requirement are almost same for both algorithms for these three above
mentioned problem domains.
Moreover, due to the low values of the average and the maximum size of Open, ASG
outperforms BU with respect to both time requirement and memory used for these three
test domains. For these three domains also, between LASG and BU, the time as well as the
memory requirement of BU increases faster than that of LASG when the size of the search
space (AND/OR tree or DAG) increases.

6. Ramifications on Implicitly Specified AND/OR Structures
In this section, we briefly discuss use of our proposed algorithms for generation of alternative
solutions in the non-decreasing order of cost for implicit AND/OR search spaces. One
possible way is to extend the standard AO∗ for generating a given number of solutions,
say k, as follows. Instead of keeping only one potential solution graph(psg), at any stage k
psgs can be computed on the explicitly constructed search space and instead of expanding
one node, k nodes, (that is, one node from each psg), can be expanded at once. After
expanding the nodes, k psgs are recomputed once again. Since the cost of the nodes are
often recomputed after expanding nodes, the swap options associated with any such node
have to be updated after every such recomputation.
Another possible approach could be to run AO∗ until it generates the optimal solution.
At this point of time the swap options can be computed on the explicit portion of the
graph and swap option with minimum cost can be applied to the optimal solution. Then
the resulting psg is again expanded further resulting in the expansion of the explicit graph.
The swap options are re-evaluated to incorporate the cost update. Again the next best psg
is computed. This process continues till the second best solution is derived. Now among the
remaining successor psgs of the first solution and the successor psgs of second solution, the
most promising psg is selected and expanded. This process continues till the third solution
is found. Then the successor psgs are also added to the already existing pool of candidate
psgs. These two broad steps, (a) selecting the next best psg from the pool of candidate
psgs, and then (b) keeping on expanding the explicit graph till the next best solution is
found, is continued till k solutions are found.
316

Generating Ordered Solutions for Explicit AND/OR Structures

# disks
8
9
10
11
12
13

100 solutions
ASG
LASG
avg. max. avg. max.
55
92
41
68
66
122
42
71
109
183
53
79
132
218
76
140
219
385
85
147
259
482
118
200

300 solutions
ASG
LASG
avg. max. avg. max.
111
186
91
174
163
331
119
252
216
367
142
283
296
611
177
373
473
776
234
492
675 1240
252
437

500 solutions
ASG
LASG
avg. max. avg. max.
174
375
135
235
265
484
198
382
345
693
234
447
486
882
291
558
668
1200
404
724
1016 1828
377
697

Table 21: Average and maximum length of Open while generating 100, 300, and 500 solutions for 5-peg Tower of Hanoi problem with different number of disks
# matrices
20
30
40
50
60
70
80
90
100

10 solutions
ASG
LASG
avg. max. avg. max.
46
87
25
39
84
162
71
126
73
123
58
90
86
151
75
126
91
144
76
112
136
234
85
122
181
324
94
132
226
414
103
142
307
576
167
259

15 solutions
ASG
LASG
avg. max. avg. max.
68
121
34
59
123
230
94
157
98
182
73
129
120
211
100
169
118
189
94
137
188
329
103
147
258
469
112
157
328
609
122
167
445
823
216
337

20 solutions
ASG
LASG
avg. max. avg. max.
90
176
46
95
160
293
116
192
125
226
90
152
151
266
123
205
151
267
108
160
243
437
117
170
335
607
127
180
427
777
136
190
583 1145
262
477

Table 22: Average and maximum length of Open while generating 10, 15, and 20 solutions
for matrix-chain multiplication problems
Test case
TC1
TC2
TC3
TC4
TC5
TC6
TC7
TC8
TC9
TC10
TC11
TC12
TC13
TC14

5 solutions
ASG
LASG
avg. max. avg. max.
45
84
41
74
50
95
50
95
47
90
46
89
50
93
49
90
47
86
45
74
49
93
47
84
42
81
42
80
46
89
44
84
40
77
39
73
59
116
59
113
55
106
54
105
33
64
31
51
51
98
51
97
41
78
40
73

10 solutions
ASG
LASG
avg. max. avg. max.
93
176
75
125
100
192
94
170
90
168
82
142
101
194
87
155
98
186
87
149
105
200
95
168
83
157
73
119
97
188
86
159
80
147
70
119
128
251
116
212
115
225
110
211
67
116
55
98
103
193
100
185
82
154
69
112

15 solutions
ASG
LASG
avg. max. avg. max.
135
249
95
143
146
266
125
197
132
244
115
210
152
292
119
197
140
246
114
184
155
294
127
206
121
231
92
138
144
277
120
214
115
214
93
146
189
350
161
280
171
317
166
321
95
172
78
135
149
276
140
239
120
231
97
176

Table 23: Average and maximum length of Open while generating 5, 10, and 15 solutions
for generating secondary structure of RNA sequences

317

Ghosh, Sharma, Chakrabarti, & Dasgupta

It is important to observe that both methods heavily depend on incorporating the updates in the explicit DAG like adding nodes, increase in the cost, etc., and recomputing the
associated swap options along with the signatures that use those swap options. Handling
dynamic updates in the DAG efficiently and its use in implicit AND/OR search spaces
remains an interesting future direction.

7. Conclusion
In our work we have presented a top-down algorithm for generating solutions of a given
weighted AND/OR structure (DAG) in non-decreasing order of cost. Ordered solutions
for AND/OR DAGs are useful for a number of areas including model based programming,
developing new variants of AO*, service composition based on user preferences, real life
problems having dynamic programming formulation, etc. Our proposed algorithm has two
advantages – (a) it works incrementally, i.e., after generating a specific number of solutions,
the next solution is generated quickly, (b) if the number of solutions to be generated is
known a priori, our algorithm can leverage that to generate solutions faster. Experimental
results show the efficacy of our algorithm over the state-of-the-art. This also opens up
several interesting research problems and development of applications.

8. Acknowledgments
We thank the anonymous reviewers and the editor, Prof. Hector Geffner, for their valuable
comments which have enriched the presentation of the paper significantly. We also thank
Prof. Abhijit Mitra, International Institute of Information Technology, Hyderabad, India,
for his valuable inputs regarding the test domain involving secondary structure of RNA. We
thank Aritra Hazra and Srobona Mitra, Research Scholar, Department of Comp. Sc. &
Engg., Indian Institute of Technology Kharagpur, India, for proof reading the paper.

Appendix A. Proof of Correctness of Algorithm 4
Lemma A.1 Every solution other than the optimal solution Sopt can be constructed from
Sopt by applying a sequence of swap options according to the order R̂.
Proof: [Lemma A.1] Every solution other than Sopt of an alternating AND/ OR tree T̂αβ
is constructed by choosing some non optimal edges at some OR nodes. Consider any other
solution Sm , corresponding to which the set of non-optimal OR edges is Sβ and suppose
|S β | = m. We apply the relation R to Sβ to obtain an ordered sequence Σ of OR edges
where ∀e1 , e2 ∈ Σ, e1 appears before e2 in Σ if (e1 , e2 ) ∈ R. We show that there exists a
sequence Σ̂ of swap options that can be constructed for Sβ . For every OR edge eij of Σ
(here eij is the ith edge of Σ and 1 ≤ i ≤ m), we append the subsequence of OR edges
ei1 , . . . , eij −1 before eij , where ei1 , . . . , eij are the OR edges that emanate from the same
parent vq , and ei1 , . . . , eij −1 are the first ij − 1 edges in L(vq ).
We get a sequence of OR edges Σaug from Σ by the above mentioned augmentation.
Σaug is basically a concatenation of subsequences τ1 , . . . , τm , where τi is a sequence of edges
ei1 , . . . , eij such that ei1 , . . . , eij are the OR edges that emanate from the same parent vq ,
and ei1 , . . . , eij are the first ij edges in L(vq ). We construct Σ̂ from Σaug as follows. From
318

Generating Ordered Solutions for Explicit AND/OR Structures

every τi , we construct τ̂i = hσi1 ,i2 , . . . , σij −1,ij i, where σik ,ik +1 = heik , eik +1 , δik ,ik +1 i and
i1 ≤ ik ≤ (ij − 1). Σ̂ is constructed by concatenating every individual τ̂i . Hence there exists
a sequence of swap options Σ̂ corresponding to every other solution Sm .
⊔
⊓
Definition A.p [Default Path] From Lemma A.1, every non-optimal solution Sm can
be constructed from the initial optimal solution by applying a sequence of swap options,
Σ̂(Sm ), according to the order R̂. The sequence of solutions that is formed following Σ̂(Sm )
corresponds to a path from Sopt to Sm in SSDAG G s . This path is defined as the default
path, Pd (Sm ), for Sm .
Lemma A.2 The SSDAG of an alternating AND/OR tree T̂αβ contains every alternative
solution of T̂αβ .
Proof: [Lemma A.2] We prove this by induction on the length of the default path Pd of
the solutions.
[Basis (n = 1) :] Consider the swap list of Sopt . The solutions whose default path length
is equal to 1 form the Succ(Sopt ). Therefore these solutions are present in G.
[Inductive Step :] Suppose the solutions whose default path length is less than or equal
to n are present in G. We prove that the solutions having default path length equal to
n + 1 are also present in G. Consider any solution Sm where Pd (Sm ) = n + 1. Let Σ̂(Sm ) =
′ where Σ̂(S ′ ) = hσ , · · · , σ i. Since P (S ′ ) =
hσ1 , · · · , σn , σn+1 i. Consider the solution Sm
1
n
d m
m
s
′
′
′ ∈ V, and swap option σ
n, Sm
n+1 ∈ L(Sm ), there is a directed edge from Sm to Sm in G .
Hence every solution having a default path length equal to n + 1 is also present in G.
⊔
⊓
Lemma A.3 For any alternating AND/OR tree T̂αβ , Algorithm 4 adds solutions to Closed
(at Line 11) in non-decreasing order of cost.
Proof: [Lemma A.3] Consider the following invariants of Algorithm 4 that follow from
the description of Algorithm 4.
a. The minimum cost solution from Open is always removed at Line 6 of Algorithm 4.
b. The cost of the solutions that are added in Open, while exploring the successor set of
a solution Sm (at Line 13 of Algorithm 4), are greater than or equal to C(Sm ).
From these two invariants it follows that Algorithm 4 adds solutions to Closed (at Line 11)
in non-decreasing order of cost.
Lemma A.4 For any alternating AND/OR tree T̂αβ , for every node of the SSDAG of T̂αβ ,
Agorithm 4 generates the solution corresponding to that node.
Proof: [Lemma A.4] From Lemma A.3 it follows that Algorithm 4 generates the solutions
in the non-decreasing order of cost. By generating a solution Sm , we mean adding Sm to
Closed (at line 11 of Algorithm 4). For the purpose of proof by contradiction, let us assume
that Algorithm 4 does not generate solution Sm . Also let Sm be the first occurrence of this
319

Ghosh, Sharma, Chakrabarti, & Dasgupta

scenario while generating solutions in the mentioned order. According to Lemma A.1, there
exists a sequence of swap options Σ̂ = σ1 , . . . , σk corresponding to Sm . Also consider the
′ whose sequence of swap options is Σ̂′ = σ , . . . , σ
solution Sm
1
k−1 . According to Property 3.2,
′
C(Sm ) ≤ C(Sm ). Consider the following two cases:
′ ) < C(S ): Since S
a. C(Sm
m
m is the first instance of the incorrect scenario, and Algo′ is generated
rithm 4 generates the solutions in the non-decreasing order of cost, Sm
prior to Sm .
′ ) = C(S ): Since Algorithm 4 resolves the tie in the favor of the parent solution,
b. C(Sm
m
′ will be
and Sm is the first instance of the incorrect scenario – in this case also Sm
generated prior to Sm .
′ . When S ′ was generated by Algorithm 4,
The swap option σk belongs to the swap list of Sm
m
′
′
that is, when Sm was added to Closed, Sm was also expanded and the solutions which can
′ applying one swap option, were added to the Open list. Since S
be constructed from Sm
m
′ applying one swap option σ , S
was constructed from Sm
was
also
added
to
the
Open
m
k
′ . Therefore S
while exploring the successors of Sm
m will also be eventually generated by
Algorithm 4 - a contradiction.
⊔
⊓

Lemma A.5 For any alternating AND/OR tree T̂αβ , Algorithm 4 does not add any solution
to Closed (at Line 11 of Algorithm 4) more than once.
Proof: [Lemma A.5] For the purpose of contradiction, let us assume that Sm is the first
solution that is added to Closed twice. Therefore Sm must have been added to Open twice.
Consider the following facts.
a. When Sm was added to Closed for the first time, the value of lastSolCost was C(Sm ),
and Sm was added to TList.
b. From the description of Algorithm 4 it follows that the contents of TList are deleted
only when the value of lastSolCost increases.
c. From Lemma A.3 it follows that Algorithm 4 generates the solutions in non-decreasing
order of cost. Hence, when Sm was generated for the second time, the value of
lastSolCost did not change from C(Sm ).
From the above facts it follow that Sm was present in TList when Sm was added to Open
for the second time. Since, while adding a solution to Open, Algorithm 4 checks whether it
is present in TList (at Line 16 of Algorithm 4); Algorithm 4 must had done the same while
adding Sm to Open for the second time. Therefore Sm could not be added Open for the
second time – a contradiction.
⊔
⊓
Theorem A.1 ∀Sj ∈ V, Sj is generated (at Line 11) by Algorithm 4 only once and in the
non-decreasing order of costs while ties among the solutions having same costs are resolved
as mentioned before.
Proof: [Theorem A.1] Follows from Lemma A.2, Lemma A.3, Lemma A.4 and Lemma A.5.
⊔
⊓
320

Generating Ordered Solutions for Explicit AND/OR Structures

Appendix B. Proof of Correctness of Algorithm 5
Definition B.q [Reconvergent Paths in Solution Space DAG] Two paths, (i) p1 =
Si11 → · · · → Si1n and (ii) p2 = Si21 → · · · → Si2m , in the SSDAG G s of an alternating
AND/OR tree T̂αβ are reconvergent if the following holds:
a. Si11 = Si21 , i.e. the paths start from the same node;
b. Si1n = Si1m , i.e. the paths ends at the same node;

c. (∀j ∈ [2, n − 1])(∀k ∈ [2, m − 1]), Si1j 6= Si2k ; i.e. the paths do not have any common
intermediate node.
Definition B.r [Order on Generation Time] In the context of Algorithm 5, we define an
order relation, ≺t ⊂ V × V, where (Sp , Sq ) ∈≺t if Sp is generated by Algorithm 5 before Sq .
Here V is set of vertices in SSDAG G s of an alternating AND/OR tree T̂αβ .
Lemma B.1 Algorithm 5 adds the solutions to the Closed list in the non-decreasing order
of costs.
Proof: [Lemma B.1] Consider the following invariants of Algorithm 5 that follow from
the description of Algorithm 5.
a. The minimum cost solution from Open is always removed at line 11 of Algorithm 4.
b. Algorithm 5 expands any solution, say Sp , in two phases. At the first phase Sp is
expanded using the native swap options of Sp . The solutions that are added to Open
as a result of the application of the native swap options, will have cost greater than or
equal to C(Sp ). In the second phase, i.e., during lazy expansion, Sp is again expanded
using a non native swap option. A solution Sp may undergo the second phase κ times
where 0 ≤ κ ≤ (|L(Sp )| − |N (Sp , σk )|) and σk is used to construct Sp . In every lazy
expansion of Sp , a new solution is added to Open. Consider a solution Sm which is
′ using σ by Algorithm 5 where S ′ ∈ P red(S ). Suppose swap
constructed from Sm
m
j
m
option σi ∈ L(Sm ), and σi ∈
/ N (Sm , σj ), i.e., σi is not a native swap option of Sm .
′ ). Suppose S and S ′ are the successors of S and S ′ respectively,
Clearly σi ∈ L(Sm
m
c
m
c
σi
σi
′ −
constructed by the application of σi , i.e., Sm
→ Sc′ , and Sm −→
Sc . Also let Sc′ is
added to Closed after Sm .
Consider the fact that Algorithm 5 does not apply swap option σi to Sm , that is, Sc is
′ ) ≤ C(S ), C(S ′ ) ≤ C(S ).
not added to Open until Sc′ is added to Closed. Since C(Sm
m
c
c
According to Algorithm 5, σi is applied to Sm (during the lazy expansion), and Sc is
added to Open right after Sc′ is added to Closed. Consider the time period between
adding Sm and adding Sc′ to Closed. During that period, every solution that is added
to Closed has cost between C(Sm ) and C(Sc′ ), i.e., the cost is less or equal to C(Sc ). In
general, the application of a swap option to add a solution to Open is delayed by such
an amount of time, say ∆, so that all the solutions, which are added to Closed during
this ∆ time interval, have cost less than or equal to the solution under consideration.
321

Ghosh, Sharma, Chakrabarti, & Dasgupta

From the above facts it follow that Algorithm 5 adds the solutions to the Closed list in
non-decreasing order of costs.
⊔
⊓
Lemma B.2 Any two reconvergent paths in the SSDAG G s of an alternating AND/OR
tree T̂αβ are of equal length.
Proof: [Lemma B.2] Consider the paths:
σ

σ

σ′

σ

σ′

σ′

1
2
n
1
2
(i) p1 = S1 −→
Sp −→
· · · −→
Sn , and (ii) p2 = S1 −→
Sp′ −→
· · · −−m
→ Sn .

The edges in the paths represent the application of a swap option to a solution. Now p1
and p2 start from the same solution and also end at the same solution. Therefore the sets of
swap options that are used in these paths are also same. Hence the lengths of those paths
are equal, that is, in the context of p1 and p2 , n = m.
Lemma B.3 For any set of reconvergent paths of any length n, Algorithm 5 generates at
most one path.
Proof: [Lemma B.3] The following cases are possible.
[Case 1 (n = 2) :] Consider the following two paths:
σ

σ′

σ

σ′

1
2
1
2
(i) p1 = S1 −→
S2 −→
S3 , and (ii) p2 = S1 −→
S2′ −→
S3 .

It is obvious that σ1 = σ2′ and σ2 = σ1′ . Suppose S2 ≺t S2′ . Here Algorithm 5 does not
apply the swap option σ1 to S2′ . Therefore p2 is not generated by Algorithm 5.
[Case 2 (Any other values of n) :] In this case, any path belonging to the set of reconvergent paths, consists of n different swap options, suppose σ1 , · · · , σn . Also the start
node and the end node of the paths under consideration are Sp and Sm . Consider the nodes
in the paths having length 1 from Sp . Clearly there can be n such nodes.
Among those nodes, suppose Algorithm 5 adds Sp1 to Closed first, and Sp1 is constructed
from Sp by applying swap option σ1 . According to Algorithm 5, σ1 will not be applied to
any other node that is constructed from Sp and is added to Closed after Sp1 . Therefore,
all those paths starting from Sp , whose second node is not Sp1 , will not be generated by
Algorithm 5. We can use the similar argument on the paths from Sp1 to Sm of length n − 1
to determine the paths which will not be generated by Algorithm 5. At each stage, a set of
paths will not be grown further, and at most one path towards Sm will continue to grow.
After applying the previous argument n times, at most one path from Sp to Sm will be
constructed. Therefore Algorithm 5 will generate at most one path from Sp to Sm .
⊔
⊓
Definition B.s [Connection Relation Rc and R̂c ] We define connection relation, Rc , a
symmetric order relation for a pair of OR nodes, vq and vr , belonging to an alternating
AND/OR tree T̂αβ as:
(vq , vr ) ∈ Rc | if in T̂αβ there exists an AND node vp , from which
there exist two paths, (i) p1 = vp → . . . → vq , and

(ii) p2 = vp → . . . → vr
322

Generating Ordered Solutions for Explicit AND/OR Structures

Similarly the connection relation, R̂c , is defined between two swap options as follows. Consider two swap options σiq and σjr , where σiq = hei , eq , δiq i and σjr = hej , er , δjr i. Suppose
OR edges ei and eq emanate from vp , and OR edges ej and er emanate from vt . Now
(σiq , σjr ) ∈ R̂c if (vp , vt ) ∈ Rc .
Definition B.t [Mutually Connected Set] For a solution Sm , a set Vm of OR nodes is
mutually connected, if

∀v1 , v2 ∈ Vm , (v1 6= v2 ) ⇒ {(v1 , v2 ) ∈ Rc }
Consider the set of OR nodes, Vm = {v1 , · · · , vk }, where swap option σj belongs to vj and
1 ≤ j ≤ k. Here the set of swap options V̂m = {σ1 , · · · , σk } is mutually connected.
Lemma B.4 Suppose Sm is a solution of an alternating AND/OR tree T̂αβ , P red(Sm ) =
{S1 , · · · , Sk }, and swap option σj is used to construct Sm from Sj where 1 ≤ j ≤ k. The
swap options σ1 , · · · , σk are mutually connected.
Proof: [Lemma B.4] Since Sm is constructed from S1 , · · · , Sk by applying σ1 , · · · , σk respectively, σ1 , · · · , σk are present in the signature of Sm . Suppose set sσ = {σ1 , · · · , σk }.
We have to show that

∀σa , σb ∈ sσ , (σa , σb ) ∈ R̂c

For the purpose of proof by contradiction, let us assume (σi1 , σi2 ) ∈
/ R̂c . Also Sm is constructed by applying σi1 and σi2 to Si1 and Si2 respectively. Consider the path p1 in SSDAG
of T̂αβ which starts from Sopt and ends at Sm , and along p1 , Si1 is the parent of Sm . Now
along this path, σi2 is applied before the application of the swap option σi1 . Similarly consider the path p2 in SSDAG of T̂αβ which starts from Sopt and ends at Sm , and along p2 ,
Si2 is the parent of Sm . Along this path, σi1 is applied before the application of the swap
option σi2 .
Suppose σi1 and σi2 belongs to OR node v1 and v2 respectively. Since along path p1 , σi1
is the swap option which is applied last, Sm contains node v1 . Similarly along path p2 , σi2
is the swap option which is applied last. Hence Sm contains node v2 . Therefore, there must
be an AND node vr in T̂αβ , from which there exist paths to node v1 and v2 – implies that
(σi1 , σi2 ) ∈ R̂c . We arrive at a contradiction that proves σ1 , · · · , σk are mutually connected.
⊔
⊓

Definition B.u [Subgraph of SSDAG] Consider a solution Sp of an alternating AND/OR
tree T̂αβand mutually connected set Vm of OR nodes in Sp , where ∀vq ∈ Vm , C(Sp , vq ) =
s (S , V ) = hV
Copt (vq ) . The subgraph Gsub
p m
sub , Esub i of the SSDAG with respect to Sp and
Vm is defined as follows. Vsub consists of only those solutions which can be constructed from
Sp by applying a sequence of swap options belonging to Vm , and Esub is the set of edges
corresponding to the swap options that belong to Vm .
s (S , V )
Lemma B.5 The number of total possible distinct solutions at each level d in Gsub
p m

,
where
|V
|
=
n.
is n+d−2
m
n−1

323

Ghosh, Sharma, Chakrabarti, & Dasgupta

Proof: [Lemma B.5] Consider the swap options that belong to the nodes in Vm . With
s (S , V ) is represented by a sequence
respect to these swap options, every solution Sr in Gsub
p m
of numbers of length n, Seq(Sr ), where every number corresponds to a distinct node in Vm .
The numerical value of a number represent the rank of the swap option that is chosen for
a node vq ∈ Vm . According to the representation, at each level:
i. the sum of numbers in Seq(Sr ) of a solution, Sr , is equal to the sum of numbers in
Seq(Sr′ ) of any other solution, Sr′ , in that same level;
ii. the sum of numbers in Seq(Sr ) of a solution, Sr , is increased by 1 from the sum of
numbers in Seq(Sr′′ ) of any solution, Sp′′ , of the previous level.
Hence, at the dth level, there are n slots and d − 1 increments that need to be made to
Seq(Sr ). This is an instance of the well known combinatorial problem of packing n + d − 1
objects in n slots
with the restriction of keeping at least one object per slot. This can be
n+d−2
done in n−1 ways.
⊔
⊓
Theorem B.1 The solution space tree constructed by Algorithm 5 is complete.
Proof: [Theorem B.1] For the purpose of contradiction, suppose Sm is the first solution
which is not generated by Algorithm 5. Also P red(Sm ) = {Spi } and Sm can be constructed
from Spi by applying σqi , where 1 ≤ i ≤ k. From Lemma B.4 it follows that the set of
swap options {σqi | 1 ≤ i ≤ k} is mutually connected. Therefore the set of OR nodes Vm to
which the swap options belong is also mutually connected. Suppose |Vm | = n.
Consider the solution Sq , where Vm is mutually connected, and for 1 ≤ i ≤ k, every σqi
belongs to the set of native swap options of Sq with respect the swap option that is used to
construct Sq . Clearly

∀vt ∈ Vm , C(Sq , vt ) = Copt (vt )

We argue that Sq is generated by Algorithm 5 because Sm is the first solution which is
s of T s rooted at S , where only
not generated by Algorithm 5. Consider the subtree Tsub
q
the edges corresponding to swap options that belong to Vm are considered. Now we prove
s is equal to the
that the number of solutions generated by Algorithm 5 at every level of Tsub
s
number of solutions at the same level in Gsub (Sq , Vm ).
Consider the solution Sq and the set Succ(Sq ). Suppose Succ(Sq , Vm ) is the set of
successor solutions that are constructed from Sq by applying the swap options belonging
′
is the minimum cost solution in Succ(Sq , Vm ). According to
to the nodes in Vm , and Smin
′
Algorithm 5 initially Succ(Smin ) is partially explored by using the set of native swap options
′
of Smin
. Any other non native swap option, σb , that belongs to the nodes in Vm , is used to
′
′
explore Succ(Smin
), right after the sibling solution of Smin
, constructed by applying σb to Sq,
is added to Closed. Consider the fact that for solution Sq , ∀vt ∈ Vm , C(Sq , vt ) = Copt (vt )
holds. Therefore all the swap options belonging to Vm will also be eventually used to explore
′
the successors of Smin
. Similarly the second best successor of Sq will be able use all but
′
.
one swap option, σc , which is used to construct Smin
′
s
The immediate children of Smin in Tsub will consist of all solutions, that can be obtained
′
′
by the application of one swap option in Vm to Smin
. The native swap list of Smin
contains
the swap option ranking next to σc . The swap options, that are used to construct the other
324

Generating Ordered Solutions for Explicit AND/OR Structures

′
n − 1 sibling solutions of Smin
, will be used again during lazy expansion, which accounts
′
′
for another n − 1 children of Smin
. Hence there would be n children of Smin
.
s
Similarly, the second best successor of Sq in Tsub will have n − 1 immediate children.
s will have n − 2 children and so on. Now the children
The third best successor of Sq in Tsub
of these solutions will again have children solutions of their own, increasing the number
of solutions at each level of the tree. This way, with each increasing level, the number of
solutions present in the level keeps increasing. We prove the following proposition as a part
of proving Theorem B.1.
s ) is given by
Proposition B.1 At any level d, the number of solutions N (d, n, Tsub


n
X
n+d−2
s
s
N (d, n, Tsub ) =
N (d − 1, k, Tsub ) =
n−1
k=1

Proof: [Proposition B.1] At second level, there are n solutions. These give rise to

k=1

k+

n−1
X

k+

k=1

n−2
X
k=1

k

k=1

solutions at third level. Similarly at fourth level we have
n
X

n
X

s
s
) + ... + 1
) + N (3, n − 1, Tsub
k.... + 1 = N (3, n, Tsub

We can extend this to any level d and the result is as follows.
s
) = 1
N (1, n, Tsub

s
) = n
N (2, n, Tsub


n
X
n+1
s
N (3, n, Tsub ) =
k=
2
k=1

s
N (4, n, Tsub
)

=

n
X
k=1

s
N (3, k, Tsub
)

=




n+2
3

s by induction on the depth d.
We determine the number of solutions at any level of Tsub

[Basis (d = 1) :]

s ) = n.
Clearly, N (1, n, Tsub



[Inductive Step :] Suppose, at dth level the number of solutions is n+d−2
= n+d−2
n−1
d−1 .
Therefore at d + 1th level,

 



n
X
n+d−2
n+d−3
n+d−1
s
s
N (d + 1, n, Tsub ) =
+
N (d, k, Tsub ) =
+ ··· + 1 =
d−1
d−1
n−1
k=1

Since Algorithm 5 does not generate duplicate node, and from Proposition B.1 the
s (S , V ) at any level is equal to the number of solutions in
number of solutions in Gsub
q
m
s (S , V ) is also generated by
s
that level of Tsub , at any level the set of solutions in Gsub
q m
s (S , V ), will
s
Algorithm 5 through Tsub . Therefore, the level, at which Sm belongs in Gsub
q m
also be generated by Algorithm 5. Therefore Sm will also be generated by Algorithm 5 – a
contradiction which establishes the truth of the statement of Theorem B.1.
⊔
⊓
325

Ghosh, Sharma, Chakrabarti, & Dasgupta

Appendix C. Conversion between AND/OR Tree and Alternating
AND/OR Tree
An AND/OR tree is a generalization of alternating AND/OR tree where the restriction of
strict alternation between AND and OR nodes are relaxed. In other words an intermediate
OR node can be a child of another intermediate OR node and the similar parent child
relation is also allowed for AND node. We present an algorithm to convert an AND/OR to
an equivalent alternating AND/OR tree.
We use two operations namely, folding and unfolding for the conversions. Corresponding
to every edge, a stack, update-list, is used for the conversions. In an AND/OR tree, consider
two nodes, vq and vr , of similar type (AND/OR) and they are connected by an edge er .
Edges, e1 , · · · , ek emanate from er .
[Folding OR Node :] Suppose vq and vr are OR nodes. The folding of vr is performed
as follows.
• The source of the edges e1 , · · · , ek are changed from vr to vq and the costs are updated
as ce (ei ) ← ce (ei ) + ce (er ) + cv (vr ) where 1 ≤ i ≤ k, that is the new cost is the sum
of the old cost and the cost of the edge that points to the source of ei . The triplet
hvr , cv (vr ), ce (er )i is pushed into the update-list of ei , 1 ≤ i ≤ k.
• The edge er along with node vr is removed from vq .
[Folding AND Node :] Suppose vq and vr are AND nodes. The folding of vr is performed as follows.
• The source of the edges e1 , · · · , ek are changed from vr to vq . One of the edges among
e1 , · · · , ek , suppose ei , is selected arbitrarily and the cost is updated as ce (ei ) ←
ce (ei ) + ce (er ) + cv (vr ) where 1 ≤ i ≤ k. The triplet hvr , cv (vr ), ce (er )i is pushed into
the update-list of ei , whereas the triplet hvr , 0, 0i is pushed into the update-list of ej ,
where 1 ≤ j ≤ k and j 6= i.
• The edge er along with node vr is removed from vq .
The unfolding operation is the reverse of the folding operation and it is same for both
OR and AND nodes. It works on a node vq as follows.
Procedure Unfold(node vq )
1
2
3
4
5
6
7
8
9
10
11

forall edge ei that emanate from vq do
if the update list of ei is not empty then
hvt , c1 , c2 i ← pop(update list of ei );
if there exists no edge et from vq that points to the node vt then
Create a node vt , and connect vt using edge et from vq ;
cv (vt ) ← c1 ;
ce (et ) ← c2 ;
else if c2 6= 0 then
ce (et ) ← c2 ;
end
end

326

Generating Ordered Solutions for Explicit AND/OR Structures

Function Convert takes the root node of AND/OR tree and transforms it to an equivalent
alternating AND/OR tree recursively.
Function Convert(vq )
1
2
3
4
5
6

7

if every child of vq is a terminal node then
if vq and its parent vp are of same type then
Apply f old operation to vq ;
end
else
foreach child vr of vq , where vr is an intermediate AND/OR node do
Convert(vr );
end

Function Revert takes the root node of an alternating AND/OR tree and converts it to the
original AND/OR tree recursively.
Function Revert(vq )
1
2
3
4
5
6

if every child of vq is a terminal node then
return;
Perform unf old operation to vq ;
foreach child vr of vq do
Revert(vr );
end

The overall process of generating alternative solutions of an AND/OR tree is as follows.
The AND/OR tree is converted to an alternating AND/OR tree using Convert function,
and the solutions are generated using ASG algorithm. The solutions are transformed back
using the Revert function. The proof of correctness is presented below.
C.1 Proof of Correctness
Suppose in an AND/OR tree Tαβ two nodes, vq and vr , are of similar type (AND/OR) and
they are connected by an edge er . Edges e1 , · · · , ek emanate from er . Now fold operation
1 is the AND/OR tree which is generated by the application
is applied to vq and vr . Let Tαβ
of the f old operation.
Lemma C.1 In the context mentioned above, we present the claim of in the following two
propositions.
Proposition C.1 The set of solutions of Tαβ having node vq can be generated from the set
1 having node v by applying the unfold operation to v of the solutions of
of solutions of Tαβ
q
q
Tαβ .
1 of T 1 that contains node v , there exists a soluProposition C.2 For every solution Sm
q
αβ
1
tion Sm of Tαβ that can be generated from Sm by applying unfold to vq .

Proof: [Proposition C.1] We present the proof for the following cases. Consider any
solution of Sm of Tαβ that contains node vq .
327

Ghosh, Sharma, Chakrabarti, & Dasgupta

a. vq and vr are OR nodes: There are two cases possible.
1. vr is absent from Sm : Since the fold operation modifies the edge er only, all
1 . Therefore S
the other edges from vq in Tαβ are also present in Tαβ
m will also
1
be present in the solution set of Tαβ and it will remain unchanged after the
application of unfold operation.
2. vr is present in Sm : Since there are k distinct OR edges emanating from vr ,
let any one of those OR edges, say ei , is present in Sm . We prove that there is
1 of T 1 , such that the application of unfold operation to S 1 will
a solution Sm
m
αβ
generate Sm . The application of fold operation to the node vr modifies the source
1 .
and the cost of edge ei from vr to vq and ce (ei ) to ce (ei ) + ce (er ) + cv (vr ) in Tαβ
1 is a solution of T 1 , where the edge e is present in S 1 . Also other
Suppose Sm
i
m
αβ
1 and S
than the subtree rooted at vq , the remaining parts of Sm
m are identical
1
1
with each other. Clearly Sm exists as a solution of Tαβ and the application of
1 generates S .
unfold operation to vq in Sm
m
b. vq and vr are AND nodes: Since vq is an AND node Sm will contain all of the
AND edges that emanate from vq . Therefore edge er and vr both will be present in
1 of T 1 , such that the following holds.
Sm . Consider the solution Sm
αβ
1.
1. vq is present in Sm

2. The subtrees rooted at the children of vq other than vr in Sm are identical with
1 .
the subtrees rooted at those children of vq in Sm
1 and S are identical
3. Other than the subtree rooted at vq , remaining parts of Sm
m
with each other.
1 exists as a solution of T 1 and the application of unfold operation to v
Clearly Sm
q
αβ
1
in Sm generates Sm .
′ of T
1
Any other solution Sm
αβ that does not contain node vq , is a valid solution for Tαβ
as well.
⊔
⊓

Proof: [Proposition C.2] We present the proof for the following cases. Consider any
1 of T 1 that contains node v .
solution Sm
q
αβ
a. vq and vr are OR nodes: Since vq is an OR node, exactly one OR edge ei of vq will
1 . There are two cases possible.
belong to Sm
1 : Since the fold operation modifies
1. ei was not modified while folding vr in Tαβ
the edge er and the OR edges of vr only, all the other edges from vq in Tαβ are
1 . Since e was not modified during folding, the same solution
also present in Tαβ
i
1
Sm is also a valid solution for Tαβ .
1 : Suppose e connects v and v in
2. ei was modified while folding vr in Tαβ
i
q
i
1 and generate solution S .
1
Sm . Apply the unfold operation to the node vq in Sm
m
The edge ei will be replaced with edge er which connects vq and vr and then ei
will connect vr and vi . We argue that Sm is a valid solution of Tαβ since the

328

Generating Ordered Solutions for Explicit AND/OR Structures

subtree rooted at vi is not modified by the sequence of – (a) the folding of vr to
1 from T , and (b) the unfolding of v to construct S from S 1 .
construct Tαβ
q
m
αβ
m
1 will contain all of the
b. vq and vr are AND nodes: Since vq is an AND node, Sm
AND edges that emanate from vq . There are two types of AND edges emanating from
1 and they are (a) Type-1 : the edges from v that are also present in T
vq in Tαβ
q
αβ from
vq , (b) Type-2 : the edges that are added to vq by folding and these edges are from
1 and generate solution
vr in Tαβ . Apply the unfold operation to the node vq in Sm
Sm . Sm will contain Type-1 edges, and another edge er from vq . In Sm , vq and vr
are connected by er and the Type-2 edges are originated from vr . We argue that Sm
is a valid solution of Tαβ since the subtree rooted at nodes pointed by Type-2 edges
1 from T ,
are not modified by the sequence of – (a) the folding of vr to construct Tαβ
αβ
1.
and (b) the unfolding of vq to construct Sm from Sm
′

1 of T 1 that does not contain node v is valid solution for T
Clearly any solution Sm
q
αβ as
αβ
well.
⊔
⊓

Lemma C.2 If function Convert is applied to the root node of any AND/OR tree Tαβ , an
alternating AND/OR tree T̂αβ is generated.
Proof: [Lemma C.2] Function Convert traverses every intermediate node in a depth first
manner. Consider any sequence of nodes, vq1 , vq2 , · · · , vqn of same type, where vqi is the
parent of vqi+1 in Tαβ and 1 ≤ i < n. Obviously, the fold operation is applied to vqi+1
before vqi , where 1 ≤ i < n. In other words, the fold operation applied to the sequence of
nodes in the reverse order and after folding vqi+1 , all the edges of vqi+1 are modified and
moved to vqi , where 1 ≤ i < n. When the function call Convert(vq2 ) returns, all the edges
of vq2 , · · · , vqn are already moved to vq1 and the sequence of nodes, vq1 , vq2 , · · · , vqn are
flattened. Therefore, every sequence of nodes of same type are flattened, when the function
call Convert(vR ) returns, where vR is the root of Tαβ and an alternating AND/OR tree
T̂αβ is generated.
Lemma C.3 If function Revert is applied to an alternating AND/OR tree T̂αβ , the updatelist of every edge in T̂αβ becomes empty.
Proof: [Lemma C.3] Follows from the description of Revert.
Theorem C.1 For any AND/OR tree Tαβ , it is possible to construct an alternating AND/OR
tree T̂αβ using function Convert, where the set of all possible solutions of Tαβ is generated
in the order of their increasing cost by applying Algorithm 4 to T̂αβ , and then converting
individual solutions using function Revert.
Proof: [Theorem C.1] According to Lemma C.2, after the application of function Convert
to Tαβ an alternating AND/OR tree T̂αβ is generated. Consider the intermediate AND/OR
0 , T 1 , · · · , T n are the
trees that are the generated after folding every node in Tαβ . Let Tαβ
αβ
αβ
n . Since T i is generated from T i+1
0 = T , T̂
=
T
sequence of AND/OR trees and Tαβ
αβ
αβ
αβ
αβ
αβ
329

Ghosh, Sharma, Chakrabarti, & Dasgupta

i , where 0 ≤ i < n, according
after folding exactly one node in Tαβ
i can be generated from T i+1 by unfolding the same
solutions of Tαβ
αβ
Lemma C.3, for any solution of T̂αβ , Revert unfolds every node vq in
vq was folded by Convert while transforming Tαβ to T̂αβ . Therefore
can be generated from the solutions of T̂αβ .

to Lemma C.1, the
node. According to
that solution, where
the solutions of Tαβ

References
Bonet, B., & Geffner, H. (2005). An algorithm better than AO∗ ?. In Proceedings of the
20th national conference on Artificial intelligence - Volume 3, pp. 1343–1347. AAAI
Press.
Chakrabarti, P. P. (1994). Algorithms for searching explicit AND/OR graphs and their
applications to problem reduction search. Artif. Intell., 65 (2), 329–345.
Chakrabarti, P. P., Ghose, S., Pandey, A., & DeSarkar, S. C. (1989). Increasing search
efficiency using multiple heuristics. Inf. Process. Lett., 32 (5), 275–275.
Chang, C. L., & Slagle, J. R. (1971). An admissible and optimal algorithm for searching
AND/OR graphs. Artif. Intell., 2 (2), 117–128.
Chegireddy, C. R., & Hamacher, H. W. (1987). Algorithms for finding k-best perfect matchings. Discrete Applied Mathematics, 18 (2), 155–165.
Chen, H., Xu, Z. J., Liu, Z. Q., & Zhu, S. C. (2006). Composite templates for cloth modeling
and sketching. In Proceedings of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 1, pp. 943–950. IEEE Computer
Society.
Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction to Algorithms
(2nd edition). McGraw-Hill Higher Education.
Darwiche, A. (1999). Compiling knowledge into decomposable negation normal form. In
Proceedings of the 16th international joint conference on Artifical intelligence - Volume
1, pp. 284–289. Morgan Kaufmann Publishers Inc.
Darwiche, A. (2001). Decomposable negation normal form. J. ACM, 48, 608–647.
Dasgupta, P., Sur-Kolay, S., & Bhattacharya, B. (1995). VLSI floorplan generation and
area optimization using and-or graph search. In VLSI Design, 1995., Proceedings of
the 8th International Conference on, pp. 370 –375.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artif.
Intell., 171 (2-3), 73–106.
Ebendt, R., & Drechsler, R. (2009). Weighted A∗ search - unifying view and application.
Artificial Intelligence, 173 (14), 1310 – 1342.
Elliott, P. (2007). Extracting the k best solutions from a valued And-Or acyclic graph.
Master’s thesis, Massachusetts Institute of Technology.
Elliott, P., & Williams, B. (2006). DNNF-based belief state estimation. In Proceedings of
the 21st national conference on Artificial intelligence - Volume 1, pp. 36–41. AAAI
Press.
330

Generating Ordered Solutions for Explicit AND/OR Structures

Eppstein, D. (1990). Finding the k smallest spanning trees. In Proc. 2nd Scandinavian
Worksh. Algorithm Theory, No. 447 in Lecture Notes in Computer Science, pp. 38–
47. Springer Verlag.
Eppstein, D. (1998). Finding the k shortest paths. SIAM J. Comput., 28 (2), 652–673.
Flerova, N., & Dechter, R. (2010). M best solutions over graphical models. In 1st Workshop
on Constraint Reasoning and Graphical Structures.
Flerova, N., & Dechter, R. (2011). Bucket and mini-bucket schemes for m best solutions
over graphical models. In GKR 2011(a workshop of IJCAI 2011).
Fromer, M., & Globerson, A. (2009). An LP view of the m-best MAP problem. In Advances
in Neural Information Processing Systems (NIPS) 22, pp. 567–575.
Fuxi, Z., Ming, T., & Yanxiang, H. (2003). A solution to billiard balls puzzle using ao
algorithm and its application to product development. In Palade, V., Howlett, R., &
Jain, L. (Eds.), Knowledge-Based Intelligent Information and Engineering Systems,
Vol. 2774 of Lecture Notes in Computer Science, pp. 1015–1022. Springer Berlin /
Heidelberg.
Gogate, V., & Dechter, R. (2008). Approximate solution sampling (and counting) on
AND/OR spaces. In CP, pp. 534–538.
Gu, Z., Li, J., & Xu, B. (2008). Automatic service composition based on enhanced service
dependency graph. In Web Services, 2008. ICWS ’08. IEEE International Conference
on, pp. 246 –253.
Gu, Z., Xu, B., & Li, J. (2010). Service data correlation modeling and its application in
data-driven service composition. Services Computing, IEEE Transactions on, 3 (4),
279–291.
Gupta, P., Chakrabarti, P. P., & Ghose, S. (1992). The Towers of Hanoi: generalizations,
specializations and algorithms. International Journal of Computer Mathematics, 46,
149–161.
Hamacher, H. W., & Queyranne, M. (1985). K best solutions to combinatorial optimization
problems. Annals of Operations Research, 4, 123–143.
Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. J. Artif. Intell. Res. (JAIR),
28, 267–297.
Hansen, E. A., & Zilberstein, S. (2001). LAO ∗ : A heuristic search algorithm that finds
solutions with loops. Artificial Intelligence, 129 (1-2), 35 – 62.
Homem de Mello, L., & Sanderson, A. (1990). AND/OR graph representation of assembly
plans. Robotics and Automation, IEEE Transactions on, 6 (2), 188 –199.
Jiménez, P., & Torras, C. (2000). An efficient algorithm for searching implicit AND/OR
graphs with cycles. Artif. Intell., 124, 1–30.
Kleinberg, J., & Tardos, E. (2005). Algorithm Design. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA.
Lang, Q. A., & Su, Y. (2005). AND/OR graph and search algorithm for discovering composite web services. International Journal of Web Services Research, 2 (4), 46–64.
331

Ghosh, Sharma, Chakrabarti, & Dasgupta

Lawler, E. L. (1972). A procedure for computing the k best solutions to discrete optimization
problems and its application to the shortest path problem. Management Science,
18 (7), pp. 401–405.
Ma, X., Dong, B., & He, M. (2008). AND/OR tree search algorithm in web service composition. In PACIIA ’08: Proceedings of the 2008 IEEE Pacific-Asia Workshop on
Computational Intelligence and Industrial Application, pp. 23–27, Washington, DC,
USA. IEEE Computer Society.
Majumdar, A. A. K. (1996). Generalized multi-peg Tower of Hanoi problem. The Journal
of the Australian Mathematical Society. Series B. Applied Mathematics, 38, 201–208.
Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for solving mixed integer
linear programming problems. In CP, p. 857.
Marinescu, R., & Dechter, R. (2006). Memory intensive branch-and-bound search for graphical models. In AAAI.
Marinescu, R., & Dechter, R. (2007a). Best-first AND/OR search for 0/1 integer programming. In CPAIOR, pp. 171–185.
Marinescu, R., & Dechter, R. (2007b). Best-first AND/OR search for graphical models. In
AAAI, pp. 1171–1176.
Marinescu, R., & Dechter, R. (2009a). AND/OR branch-and-bound search for combinatorial
optimization in graphical models. Artif. Intell., 173 (16-17), 1457–1491.
Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search for combinatorial
optimization in graphical models. Artif. Intell., 173 (16-17), 1492–1524.
Martelli, A., & Montanari, U. (1973). Additive AND/OR graphs. In Proceedings of the
3rd international joint conference on Artificial intelligence, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Martelli, A., & Montanari, U. (1978). Optimizing decision trees through heuristically guided
search. Commun. ACM, 21, 1025–1039.
Mateescu, R., & Dechter, R. (2008). AND/OR multi-valued decision diagrams for constraint
networks. In Concurrency, Graphs and Models, pp. 238–257.
Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR multi-valued decision diagrams (AOMDDs) for graphical models. J. Artif. Intell. Res. (JAIR), 33, 465–519.
Mathews, D. H., & Zuker, M. (2004). RNA secondary structure prediction. In Encyclopedia
of Genetics, Genomics, Proteomics and Bioinformatics. John Wiley & Sons, Ltd.
Nilsson, D. (1998). An efficient algorithm for finding the m most probable configurations
in probabilistic expert systems. Statistics and Computing, 8, 159–173.
Nilsson, N. J. (1980). Principles of artificial intelligence. Tioga Publishing Co.
Otten, L., & Dechter, R. (2011). Anytime AND/OR depth-first search for combinatorial
optimization. In SoCS.
Pearl, J. (1984). Heuristics: intelligent search strategies for computer problem solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
332

Generating Ordered Solutions for Explicit AND/OR Structures

Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (2nd edition
edition)., chap. Planning, pp. 375–461. Prentice-Hall, Englewood Cliffs, NJ.
Shiaa, M. M., Fladmark, J. O., & Thiell, B. (2008). An incremental graph-based approach
to automatic service composition. IEEE International Conference on Services Computing, 4 (2), 46–64.
Shin, D. H., Jeon, H. B., & Lee, K. H. (2010). A sophisticated approach to composing
services based on action dominance relation. In Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific, pp. 164 –170.
Subramanian, S. (1997). Routing algorithms for dynamic, intelligent transportation networks. Master’s thesis, Virginia Technical Univ., Dept. of Civil Engineering.
Sugimoto, K., & Katoh, N. (1985). An algorithm for finding k shortest loopless paths
in a directed network. Trans. Information Processing Soc. Japan, 26, 356–364. In
Japanese.
Szymanski, M., Barciszewska, M. Z., Barciszewski, J., & Erdmann, V. A. (2005). 5S Ribosomal RNA Database. http://biobases.ibch.poznan.pl/5SData/. Online Database.
Takkala, T., Borndörfer, R., & Löbel, A. (2000). Dealing with additional constraints in the
k-shortest path problem. In Proc. WM 2000.
Topkis, D. M. (1988). A k-shortest path algorithm for adaptive routing in communications
networks. Trans. Communications, 36 (7), 855–859.
Yan, Y., Xu, B., & Gu, Z. (2008). Automatic service composition using AND/OR graph. In
E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing,
E-Commerce and E-Services, 2008 10th IEEE Conference on, pp. 335–338.

333

