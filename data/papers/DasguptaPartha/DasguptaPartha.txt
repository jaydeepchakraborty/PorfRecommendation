Digital microfluidic biochip's (DMFB's) have emerged as an alternative to various in-vitro diagnostic tests and are expected to be closely coupled with cyber physical systems. Efficient-error-free-routing and cross-contamination minimisation are needed during bioassay operations on DMFB. This study proposes a two phase heuristic technique for routing droplets on a two-dimensional DMFB. Initially it attempts to route maximum number of nets in a concurrent fashion depending on the evaluated value of a proposed function named interfering index (IInet). Then exact routing is attempted based on tabulation minimisation process. Remaining nets having interfering index values higher than threshold will be routed considering various constraints in DMFB framework. In second phase another metric named routable ratio (RR) is proposed and depending on RR metric, the routing order among conflicting paths are prioritised to avoid deadlock from there onwards till the droplet reaches its target location. Finally we formulate droplet movement problem as satisfiability problems and solve with SAT based solver engine if higher number of overlapping (â‰¥5) nets exist. Experimental results on benchmark suite I and III show our proposed technique significantly reduces latest arrival time, average assay execution time and number of used cells as compared with earlier methods.Fair Resource Allocation for Heterogeneous Tasks
Koyel Mukherjee, Partha Dutta, Gurulingesh Raravi, Thangaraj Balasubramaniam, Koustuv Dasgupta, Atul Singh Xerox Research Center India, Bangalore, India 560105 Email: {Koyel.Mukherjee, Partha.Dutta, Gurulingesh.Raravi}@xerox.com {Rajasubramaniam.T, Koustuv.Dasgupta, Atul.Singh}@xerox.com

Abstract--We consider the problem of fair allocation of resources to tasks where a resource has to be assigned to at most one task entirely without any fractional allocation. The system is heterogeneous in the sense that the cost may vary across resources, and different tasks may have different resource demand. Due to heterogeneity of resource costs, the cost of allocation for a task in isolation, without any other competing task, may differ significantly from its allocation cost when the task is allocated along with other tasks. In this context, we consider the problem of allocating resources to tasks, while ensuring that the cost is distributed fairly across the tasks, namely, the ratio of allocation cost of a task to its isolation cost is minimized over all tasks. We show that this fair resource allocation problem is strongly NP-Hard even when resources are of unit size by a reduction from 3-partition. Our main results are a 2+ O( ) approximation LP rounding based algorithm for the problem when resources are of unit capacity, and a near-optimal greedy algorithm for a more restricted version. The above fair allocation problem arises in various context, such as, allocating computing resources for reservation requests from clients in a data center, allocating resources to computing tasks in grid computing, or allocating personnel for projects in service delivery organizations. Keywords-Resource allocation, Approximation algorithm, LP rounding, Greedy algorithm

I. I NTRODUCTION In this work we consider a heterogeneous system where each resource has an associated cost and a capacity (or size), and a resource can be allocated entirely to at most one task, without any fractional allocation. Every task has a demand (or capacity requirement) and we would like to minimize the resource allocation cost for meeting this demand. In isolation, i.e., when there is exactly one task in the system, an ideal allocation would select a set of resources that would meet the task's demand while minimizing its total cost. This case of a task in isolation is identical to the minimization knapsack problem [6], a known NP-Hard problem, and we call the corresponding minimum cost of a task, its isolation cost. However, in presence of multiple tasks, due to heterogeneity of resources, the resource allocation

cost for a task may significantly differ from its isolation cost. In this scenario, we would like to minimize the cost for multiple tasks in a manner that is fair across tasks. In particular, when multiple tasks are present, we want to allocate the resources in a way that minimizes over all tasks, the ratio of the allocation cost of each task to its isolation cost. Fair resource allocation is one of the core problems in parallel and distributed computing which arises in multiple settings, as illustrated next. Consider a multitenant data center where multiple tenants request to reserve certain computing capacity over the set of available (physical or virtual) machines. Here the tasks are the reservation requests with their respective demands, and the resources are the machines, each with its computing capacity and cost. We would like to ensure a resource allocation that is fair across tenants, in terms of the costs of resources allocated. From a tenant's point of view, such a fair allocation is preferable than the case where the data center operator tries to minimize the total cost over all resources. A similar problem can be seen in a geographically distributed grid computing environment in which different users request computing resource reservations, where the resource costs can be the monetary cost, or the cost of network communication. Finally, the problem of allocating personnel to project in large service delivery organizations, such that any increase in project cost, due to the presence of other tasks, is fairly distributed, can also be modeled as the above fair resource allocation problem. A. Problem Definition and System Model We study the following problem. Given a set of tasks where each task has a certain resource requirement (also called demand), and a set of resources where each resource has a certain resource supply (also called capacity or size of the resource) and a cost, find a fair allocation of tasks to resources such that the resource requirements of all the tasks are met and each resource is allocated entirely to at most one task. The fairness is

defined with respect to the ratio of the cost of a task when allocated along with other tasks, to the optimal cost of the task, when allocated in isolation (i.e., when other tasks are not present). Our goal is to minimize the maximum of this ratio across all tasks. Henceforth, we refer to this problem as the fair allocation problem. Formally, we are given a set of tasks P , and we have access to a set of resources E . Each task j is characterized by its resource requirement denoted by Dj . Each resource i is characterized by two parameters: a resource supply si and a cost ci . Upon allocating a resource i to a task j , the task incurs a cost of ci and its resource requirement is reduced by si . In order for a task j to successfully execute, it must be ensured that the total size of resources allocated to it is at least Dj . We require all assignments to be integral (0 or 1), i.e., each resource must be entirely assigned to a task and hence cannot be fractionally assigned to multiple tasks. We denote by Ij the isolation cost of a task j , that is the cost that the task would incur if this is the only task that needs to be allocated resources and no other tasks were present in the system. We denote by Cj the cost incurred by the task j when resources are allocated to it along with other tasks in the system; we refer to this cost as the actual cost of the task. The problem objective i is to minimize  = maxiP C Ii . B. Related Work Fairness of resource allocation has been well-studied both from a theoretical and practical point of view. Fair sharing of network resources (such as a wired network links and wireless spectrum) has been extensively studied, and various indices and algorithms for fairness have been developed, such as [5], [7]. More recent work on sharing of system resources has focused on sharing resources in data centers [4], [9], and on a variant of max-min fairness. Our work differs from these [4], [5], [7], [9] in following two ways. Firstly, we consider a system where there are sufficient resources to satisfy requirements of all tasks, but the cost of the resources may vary, which in turn results in increase in cost of task in presence of other tasks. Most earlier work considers a system where the number of resources is limited and may not be able to fully satisfy the demands of all tasks. With the rapid increase in the computing capacity of data centers, our assumption of a large number of available resources from one or more data centers, albeit with different (rental) costs, is increasingly becoming more relevant. Secondly, we give a provable fairness guarantee for tasks, where fairness is quantified by the ratio of actual cost of a task to its isolation cost, which has not been investigated earlier. 2

The combinatorial optimization problem studied in this paper is a mixed packing and covering problem [11], where we need to cover the demand of task using resources subject to packing the selected resources within a certain cost budget. Young [11] studies the fractional version of the problem, which is not NP-hard, and provides efficient sequential and parallel algorithms for the problem. Chakaravarthy et al. [2] study a version of mixed packing and covering problems, where they solve a knapsack cover problem subject to cardinality constraints, and then extend it to multiple matroid constraints. In terms of the resource allocation problem that we study in this paper, the work of Chakaravarthy et al. [2] can be used to find the minimum cost resource allocation for one task, such that the total size of the resources allocated meets the total resource requirement of the task, and the number of resources allocated is at most a pre-specified number. This is slightly different compared to our problem where we try to ensure fairness of resource allocation costs across multiple tasks, such that the total resource requirement of every task is met. Escoffier et al. [3] study some multi-agent optimization problems where the goal is to maximize the satisfaction of the least satisfied agent, where the satisfaction of an agent is defined as the ratio between his utility for the given solution and his maximum possible utility. For some NP-hard problems, assuming a feasible solution exists, they give polynomial algorithms with approximation factors dependent on the number of agents and/or on other problem parameters. Though the objective of the problem studied is quite similar to ours, the underlying problem we study is different, and we provide constant approximations, not dependent on the problem size. Moreover, our techniques are fundamentally different from Escoffier et al. [3]. Online algorithms for the fractional mixed packing and covering problem are investigated by Azar et al. [1], where the packing constraints are known offline, while the covering constraints get revealed online. Their objective is to minimize the maximum multiplicative factor by which any constraint is getting violated, while meeting all the covering constraints. They give a polylogarithmic competitive ratio, and a nearly tight lower bound for the fractional problem. In contrast, we study an integral, offline version of the above problem, for which we give constant approximations. C. Our Contributions This paper makes the following contributions. 1) We first show that the fair allocation problem is NP-Hard in the strong sense even when the resources are of unit size.

2) We formulate the fair allocation problem as a mixed packing and covering problem and give a 2 + O( ) approximation algorithm, based on LP rounding when resources are unit sized. 3) We further show that the LP considered has an integrality gap of 2, hence the bound of 2 + O( ) is essentially tight for this LP. 4) Finally, for a restricted version of the problem, we give a near-optimal greedy algorithm. We would like to note that, although we assume that resources cannot be fractionally allocated to a task, it is straightforward to extend our methods to a setup where a resource can be allocated in multiples of a certain given 1 fraction k , by creating k copies of the resource each 1 with k of the cost and size of the original resource. D. Organization of the Paper The rest of the paper is organized as follows. In Section II, we show that the fair allocation problem is strongly NP-hard even for unit size resources. Then we formulate the problem as an integer linear program and show that a naive relaxation has an unbounded integrality gap in Section III, for unit sizes. We also show that when resources have arbitrary sizes and costs, any trivial modifications of the LP considered will still result in an unbounded integrality gap. Hence, we focus on the unit size resource problem the next section onwards. We give a 2 + O( ) approximate LP rounding algorithm for this problem in Section IV, following which we show that the LP considered has an integrality gap of 2 in Section V which essentially shows that our bound is tight. In Section VI, we present a greedy algorithm, that in a restricted scenario, achieves near optimal performance and finally Section VII concludes the paper. II. NP- HARDNESS In this section, we show that the problem of fair resource allocation to multiple tasks, is strongly NPhard even when the resources are of unit size and have different costs and the tasks are all identical in the sense that the resource requirement of each task is same. The reduction is from 3-Partition. Theorem 1: The feasibility problem of the fair resource allocation to multiple tasks is strongly NP-hard. Proof: Consider an instance of the 3-partition problem. There are n = 3m integers {ai |i  [1, . . . , n]} such that the sum of the integers i[1,...,n] ai = mB , B and each integer ai is strictly between B 4 and 2 , i.e., B B 4 < ai < 2 . The feasibility question is whether there exists a partition of the n integers into m partitions such that the sum of the integers in each partition is exactly equal to B . Note that this would require every partition 3

Minimize  subject to the following constraints: I1. j  P iE xi,j × ci   × Ij I2. j  P iE xi,j × si  Dj I3. i  E j P xi,j  1 I4. xi,j  {0, 1} i  E , j  P Fig. 1. An integer linear program for the fair allocation problem.

to have exactly 3 integers. Now, let us define a fair resource allocation problem, where we have m tasks, each requiring 3 units of resource. We create n = 3m resources, each of unit size, where the cost of each resource is ci = ai . Let us order the integers in the 3partition instance in non-decreasing order of their sizes. Let the sum of the 3 smallest integers be I . The isolation cost of every task is therefore equal to I . The feasibility question we ask is whether  = B I is feasible. If there exists a feasible solution to the 3-partition instance, that implies that  = B I is feasible, since this corresponds to 3 units of resources per task, and the cost incurred by every task is B . At the same time, if there exists a feasible  = B I for the fair resource allocation problem, then this implies that the 3-partition instance is feasible. Every task has received 3 resources, and the cost incurred by every task must be  I = B I I = B . Since we have allocated every resource, the total sum of the costs incurred by all m tasks is mB , hence no task can cost < B , as that would make another task cost > B , which cannot happen,  being feasible. As a result, this requires every task to cost exactly B . Therefore, the resource allocations to the m tasks corresponds to feasible m partitions of the integers in 3-partition, where the sum of the integers in every partition is exactly B . III. ILP FORMULATION , LP RELAXATION AND I NTEGRALITY GAP In this section, we formulate the problem as Integer Linear Program (ILP), then relax it to a Linear Program (LP) and show that the problem in the generic case where resources may have different costs and sizes has an unbounded integrality gap. Recall that the aim is to find an allocation of resources to tasks that minimizes the ratio of actual cost of allocation to the isolation cost, for all tasks. Formally, C we want to minimize  , where  = maxj P Ijj . This is subject to fulfilling the resource requirement Dj for all tasks j  P . We formulate this problem as an ILP, shown in Figure 1. In the formulation given in Figure 1,  represents an upper bound on the ratio of Cj to Ij across all tasks j  P , and since we are minimizing it,  is the smallest possible value for the required objective, i.e., C min maxj P Ijj , that can be achieved by any integral allocation. In the formulation , the inequality I 1 ensures that the cost incurred by a task j due to resource

allocation is at most Cj = Ij , inequality I 2 ensures that the total resource allocated to a task is no less than its resource requirement Dj , inequality I 3 ensures that no resource must be over allocated, and finally I 4 ensures that every resource must be integrally allocated to a task, if at all. A naive linear relaxation of this ILP formulation would relax I 4 to xi,j  0. However, such a relaxation has an unbounded integrality gap as illustrated next. Consider m tasks, each with a resource requirement of Dj = 1, and m resources of which m - 1 resources have a cost 1 and one resource has a cost m. All resources are of unit-size, i.e., si = 1. Any integral allocation would have to allocate the high cost resource to one of the tasks, hence incurring a  = m. However, the linear program 1 would allocate m of each resource to each task. This will meet the resource requirement of every task since 1 m· m = 1, while the cost incurred by every task is 1 -1 1 m m + mm = 2- m . Therefore, the integrality gap is m m  2- 1 > 2   as m  . m To overcome this, we use the parametric pruning technique, similar to the seminal work of Lenstra et al. [8]. We guess the optimal value of  , and solve a feasibility linear program. (Later in the section, we show that  can be guessed in a logarithmic number of iterations.) For each guess of  , we solve the feasibility linear program shown in Figure 2, where in order to avoid giving an unfair advantage to the linear program, we allow the LP to assign a resource i to a task j , only if ci   × Ij .
C1. C2. C3. C4.
iE|ci Ij iE|ci Ij j P xi,j xi,j  0

C1. C2. C3. C4.

iE|ci Ij  si Dj iE|ci Ij  si Dj j P xi,j  1

xi,j × ci  Ij xi,j × si  Dj

xi,j  0

j  P j  P i  E i  E , j  P

Fig. 3. Further restricted LP relaxation (still with an unbounded integrality gap for arbitrary sizes.

xi,j × ci  Ij xi,j × si  Dj 1

j  P j  P i  E i  E , j  P

Fig. 2.

An LP relaxation of ILP shown in Figure 1.

However, if the resources are of arbitrary sizes, then the linear program shown in Figure 2 still has an unbounded gap1 . Further, simple parametric pruning along the size dimension, specifically, allowing the LP to allocate a resource i to a task j , only if si  Dj , still cannot remove the gap. Consider the LP shown in Figure 3. This LP still has an unbounded integrality gap, and trivial modification of this LP cannot remove this gap. We show this with an example. Consider m tasks where each task has a resource requirement of m. Let there
1 Consider a system with 2 tasks, each with a resource requirement of Dj = 1, and 2 resources of which resource 1 has a size = 2 and a cost 1, and resource 2 has a size 1 and a cost L 1. It can be seen that the isolation cost of both tasks is 1. Any integral allocation would have to allocate the high cost resource to one of the tasks, resulting in a  = L. However, the linear program would return a feasible solution 1 for  = 1, by allocating fraction 2 of resource 1 to both tasks.

be 2m - 1 resources of which m - 1 resources have a cost of m and a size of m, and m resources have a cost of m and a size of 1. Any integral feasible solution would have to assign the m - 1 resources of size m to m - 1 tasks and the remaining m resources of unit size to another task. The task receiving the m unit size resources incurs a cost of m2 , whereas the isolation cost of every task is m, which means any integral solution would incur a  of m. Since the isolation cost of every task is m, and the resource requirement of every task is m, and further since i, j : si  Di , every resource is feasible to be allocated to every task by the LP, for every   1. A feasible LP solution therefore, allocates each of the m - 1 resources of size m to each of the 1 m tasks to an extent of m , and one unit size resource integrally to each task. The total resource received by 1 + 1 = m. The total every task is therefore (m - 1)m m 1 cost incurred by every task is (m - 1)m m + m = 2m - 1. Therefore, the LP will return a feasible solution for a -1  = 2m < 2, whereas any integral solution will incur m a  of m, giving an integrality gap of m 2  , as m  . Note that if we try to modify the LP by restricting the allocation of resources which have a large ratio of cost to size, compared to the ratio of isolation cost to the resource requirement of a task, may render the LP infeasible, when a feasible solution exists, or suboptimal. Hence, in the rest of the paper, we will only consider the case where all resources are of unit size, but may have different costs. Guessing  : We next show that we can guess a near optimal  in a logarithmic number of iterations. The minimum value of  is given by min = 1. For every guess of  , we allow a resource i to be allocated to a task j , only if ci  Ij , i.e., ci  Cj . Without loss of generality, we assume that the minimum cost of any resource is 1, since the costs are rational numbers, and can always be made integral by scaling the problem. Similarly, we assume that the sizes of the resources are 1, and the resource requirement for every task Dj is integral. Observation 2: The maximum value of  is max = cmax , where cmax = maxiE ci . Proof: Suppose that the resource requirement of a task j with the highest value of  is Dj . In the worst case, the task is allocated those resources that have the highest cost, hence Cj  Dj × cmax . However, the isola4

tion cost of the task can be expressed as Ij  Dj , since minimum cost of any resource is  1. Therefore, the D ×c maximum value of  is given by   j Djmax = cmax . With that, we can do a binary search for  in the range [min, max ] = [1, cmax ], using a resolution of and guess the optimal  in log2 ( cmax ) iterations. The next theorem shows that a near optimal value of  can be found by such a method. Theorem 3: Let OP T -IN T be the objective function value for an integral optimal solution (where the optimality is with respect to the fairness objective). Let OP T -LP be the smallest  for which the linear program LP is feasible. Then OP T -LP  OP T -IN T + . Proof: Let the smallest  for which the LP is feasible be OP T -LP . By definition of OP T -LP and by the property of binary search it must hold that for OP T -LP - , the LP must have been infeasible. Therefore, there exists no feasible resource allocation satisfying the specified constraints in the LP, such that the cost for every task j is  (OP T -LP - ) Ij and allocation is  Dj , and every resource is allocated to unity. This implies that in any feasible (integral or otherwise) allocation of resources satisfying the resource requirement Dj for every task j , there must exist a task j such that Cj > (OP T -LP - ) Ij . Since any the optimal integral allocation is a feasible allocation (satisfying the LP constraints), this implies that there exists a task j in the optimal integral allocation, such that the cost incurred by j is Cj > (OP T -LP - ) Ij , C therefore, OP T -IN T  I j > (OP T -LP - ). This j completes the proof. In the remainder of the paper, we assume that the resources are of unit size. Note that, with unit-sized resources, the isolation cost Ij of a task j , with demand Dj , is simply the sum of the cost of Dj lowest cost resources. We also assume that there are sufficient resources in the system to satisfy the demand of all tasks simultaneously, but possibly with different allocation costs. IV. LP ROUNDING A LGORITHM In this section, we present a polynomial LP rounding algorithm with a 2 + O( ) approximation to the fair resource allocation problem. The factor is due to the binary search performed over the space of  with a resolution . We later (in Section V) show that for unit size resources, even the LP has an integrality gap approaching 2, hence our rounding algorithm is essentially tight. The rounding proceeds as follows. First, we convert a feasible LP solution to another feasible solution where 5

every resource, if allocated, is allocated up to unity. In other words, no resource has an overall fractional allocation. Once we have done this conversion, we essentially have a perfect fractional matching, where every resource is allocated to unity, and every task's resource requirement is exactly satisfied, while the cost of every task j is  Ij . Now, we convert this to an integral, feasible, perfect matching, such that the total cost of the resources allocated to every task j is at most twice Ij , where the LP was feasible for  . This procedure is described in detail in Section IV-A. We now show how to convert a feasible LP solution to a perfect fractional matching, where every resource is allocated to unity, and every task's resource requirement is exactly satisfied, while the cost of every task j is  Ij . Lemma 4: Let LP -OP T be the smallest value for which the LP is feasible. Then there exists a feasible LP solution  for the same value of  , in which any resource that has a non-zero allocation, is allocated to an extent of 1; formally, if i  E , j  P such that xij > 0 then it holds that j P xij = 1. Proof: Informally, the proof is based on the following reasoning. A more formal proof follows the informal discussion. Depending on whether one or more resources are fractionally allocated, we need to consider two cases. If there exists one resource i that is fractionally allocated (i.e., xi = j P xi,j < 1) then in a feasible solution, some tasks must be over-allocated. This is because (i) total resource requirement D = j P Dj of tasks is integral since individual resource requests Dj of each task are integral, and (ii) resources are unit size, (iii) only one resource is fractionally assigned, therefore, any feasible solution must have assigned  D resources to unity. Hence, there must be over-allocation of resources and this over allocation sums to xi which can be eliminated by readjusting the resource allocation of tasks without violating the feasibility of the solution thereby making all the resource allocations integral. If there exists multiple resources that are fractionally assigned then considering a pair of such resources at a time and by readjusting their resource allocations (decreasing one of the fractional allocation and increasing the other fractional allocation) systematically converts at least one of the fractional resource allocations into an integral allocation without violating the feasibility of the solution. Repeating this process either eliminates all the fractional allocations or leaves one fractional allocation in which case we can use the procedure described for the first case. A formal proof follows now. We prove the claim by contradiction. Suppose that the claim is not true. Then

we need to consider two cases. Case 1: A single resource has an allocation xi = j P xi,j < 1. We have already argued that at least D resources must have been allocated to unity, otherwise, the total resource requirement of D cannot be met. For every task j for which resource i has a non-zero allocation to, we first reduce xi,j by kE xk,j - Dj , without any loss of feasibility. Now, let P be the set of tasks for which resource i has a non-zero allocation after this modification. All the tasks in P are now exactly satisfied in terms of their resource requirements, i.e., k xk,j = Dj j  P . Let us consider two scenarios depending on the cost ci of the fractional resource i. Scenario 1.a. First consider the scenario, where all of the resources allocated to unity, cost at most ci . Since k ,j P xk,j > j P Dj , there must exist a task j , where k xk,j > Dj . Let the highest cost resource allocated to task j be i . Since ci  ci and resource i has a non-zero allocation to task j , resource i is feasible to be allocated to j . Now, we decrease the allocation of resource i to task j by  = min xi ,j , xi,j , kE xk,j - Dj > 0 and increase the allocation of resource i to task j by  , and decrease the allocation of resource i allocation to task j by  . The resultant allocation is still feasible in terms of resource requirements to all tasks, and the cost of every task is at most the cost of the earlier allocation. Since by definition,  > 0, we have decremented xi,j . As long as xi,j > 0, there is always a task j that has been over allocated, and we repeat the above transformations, till we have removed the allocation of resource i to task j , or, xi,j = 0. If xi > 0, then there must exist another task p to which resource i has a non-zero allocation. We repeat the above steps for resource i and task p. We continue till we have removed the allocation of i to every project, without violating feasibility, and removing the fractional resource. This takes a polynomial number of transformations. Scenario 1.b. Now consider the scenario where there exists a resource that has been allocated to unity and whose cost is greater than ci . From the set of resources allocated to unity, let k be the highest cost resource. Clearly, ck > ci . Now consider a task j to which resource k is allocated. Increase the allocation of resource i to task j by  = min (xk,j , 1 - xi ), while decreasing allocation of resource k to task j by  . In the process, we have not violated feasibility since the resource requirement is still met every where, every resource is allocated at most to unity, and the costs of the tasks after this transformation is at most what they were earlier. After this transformation, if we have made resource i allocated up to unity, then resource k 6

becomes the new fractionally allocated resource having the highest cost of any non-zero allocated resource, and hence we can now apply the procedure of Scenario 1.a to reduce its allocation to 0. Otherwise, i.e., if resource i is not allocated to unity then xk,j = 0. Note that, even after this transformation, xk > 0 since  < 1. Therefore, there must exist another task j where resource k has a non-zero allocation. We repeat the above process, till we make resource i allocated to unity. This will always be possible since resource k is allocated to unity, and 1 - xi < 1. Now, resource k becomes the new fractionally allocated resource, and it costs the most among all the resources allocated, hence this reduces to Scenario 1.a. We repeat the transformations outlined in Scenario 1.a, till we reduce xk to 0. Case 2: More than one resource has an allocation < 1. Let us consider a pair of resources (i, i ) both of which are fractionally allocated. Without loss of generality, let ci  ci . Suppose xi + xi  1. In this case, for every task j that resource i has a non-zero allocation to, we make xi,j = xi ,j and reduce xi ,j to 0. In this way, we have reduced the number of resources with fractional allocation by at least 1. We repeat this for every pair of fractional resources, till we are left with at most one fractional resource. Then we perform the transformations described in Case 1. Now, suppose xi + xi > 1. Again, we repeat the above procedure for every task that resource i has a non-zero allocation to, till we come across a task, where xi ,j > 1 - xi , where xi refers to the current fractional allocation of resource i after the above transformations. In this case, we set xi,j = 1 - xi , and xi ,j = xi ,j - xi,j . Now, we have again reduced the number of fractional resources by 1. We repeat the above described procedures for every pair, till we are left with at most one fractional resource, which corresponds to Case 1. This completes the proof. We now describe the rounding algorithm. A. Description of the LP Rounding Algorithm The solution  is a set of connected components in a bipartite graph, with one vertex partition V1 = {i  E| j P xi,j > 0} being the set of resources with non-zero allocation. In fact, after the polynomial transformation described in Lemma 4, V1 = {i  E| j P xi,j = 1}. The other partition V2 is the set of tasks P . We have already argued that we have a perfect fractional matching in this bipartite graph, which we will now convert to a perfect integral matching, without violating the cost constraints too much. From  , we first remove all the edges that are integral, allocating the corresponding resource to the corresponding task, without any loss in feasibility. Let V1 = E be

  1    

2   
  

3   
  

the set of resources who are yet unallocated and V2 = P be the set of tasks, that have still not been fully allocated, after removing the integral edges. Since all the resource requirements are integral, and all resources are allocated to unity, the total number of resource available is no less than the total resource requirement over all tasks, i.e., |V1 |  pV Dp , i.e., we still have a perfect fractional 2 matching. We order the resource vertices in non-increasing order of their costs, and assume henceforth that they are numbered accordingly, namely e1 , e2 , . . . , e|V1 | , where ce1  ce2  . . . ce|V | . The tasks are ordered in 1 any arbitrary order, p1 , p2 , . . . , p|V2 | . Now, we use a procedure, which is somewhat inspired by the analysis of the generalized assignment problem, in the seminal work of Shmoys and Tardos [10] to transform the current fractional matching to an integral matching. The algorithm proceeds as follows. Start with the highest cost resource and allocate it integrally to any task that it has a non-zero allocation to. Now, adjust the allocation of the resources to maintain feasibility. Then, proceed to completely allocate this task. Once done, pick the next highest cost resource, that is not yet integrally allocated and repeat the procedure. We describe this in detail in the following paragraphs. Let pj be a task where resource e1 has a nonzero allocation, in other words, (e1 , pj ) is an edge in this connected component. We allocate resource e1 to resource pj integrally (i.e., xe1 ,pj = 1), and remove e1 from the set V1 . Now, we find the lowest resource index ei such that ek |k{1,...,i} xek ,pj  1, and ek |k{1,...,i-1} xek ,pj < 1. We remove the edge xei ,pj and split the node i into two nodes i1 and i2 , and add two edges xei1 ,pj = ek |k{1,...,i} xek ,pj - 1, and xei2 ,pj = xei ,pj - xei1 ,pj . We have made xe1 ,pj = 1. Therefore to maintain feasibility, we have to make xe1 ,pq = 0 pq  P , q = j . Note that all the resources ek , 1 < k  i, have costs  ce1 , and hence are feasible to be allocated to any task that resource e1 was allocated to. Now, consider a task j to which resource e1 had a nonzero allocation. We know that all of the resources ek , 2  k  i are feasible to be allocated to j . Hence, we add an edge from resource e2 to task j of value xe2 ,j = min (xe2 ,j , xe1 ,j ) to j and decrement xe2 ,j by xe2 ,j . Now, if xe2 ,j = xe1 ,j , we consider the next task j that resource e1 had an allocation to, and repeat the above process, with e2 if xe2 ,j > 0. Otherwise, if xe2 ,j = 0, and xe2 ,j < xe1 ,j , we pick e3 do the same procedure. We continue till either we have removed all the edges (ek , pj ) for all 1  k < i and xei1 ,pj , or we have have removed all edges xe1 ,pq for all q = j , in a 7

1 1

2

3 3  
 

1

2  
 

1

2

3 1  2    

Fig. 4.

Illustration of one iteration of the LP rounding algorithm.

polynomial number of transformations (O(|V1 | + |V2 |)). Note that xei1 ,pj + ek |k{2,...,i} xek ,pj = 1 - xe1 ,pj , therefore, both the events will happen simultaneously, after which we have a feasible solution, where every resource in V1 is allocated up to unity and every task in V2 is fully allocated. If task pj had only one resource requirement, then we have allocated task pj integrally, while not violating the cost constraint of pj , as ce1  Cj . Otherwise, if Dj > 1, then we still have unmet resource requirement in task pj . Now, we find the lowest resource index u > 1, that has a non-zero allocation to task pj , i.e., xeu ,pj > 0 and repeat the above. If xei2 ,pj > 0, then u = i, otherwise, u > i. We allocate resource u integrally to task pj , remove the resource from V1 , and repeat the above procedure. We continue till pj is allocated Dj integral resources. Once task pj is fully allocated integrally, we remove it from V2 , and find the next highest cost resource es remaining in V1 . Let pr be a task that es has a non-zero allocation to. We perform the same transformations on resource es and task pr and continue, till both V1 =  and V2 = . Figure 4 provides an example for one iteration of the rounding algorithm. Observe that we allocate only one resource integrally to a task at a time, and adequately remove some assignments to maintain feasibility of total allocation of any resource, and resource allocation of any task. Since, initially |V1 |  pk V2 Dpk , and we are not violating the size constraints on resource allocations at any iteration, we will always find a resource to allocate to a task yet unsatisfied, that is, if V2 = , that implies V1 = . Not only that, at any iteration, when we change the allocations, we maintain feasibility in terms of the assignment restrictions on the resources; in other words, we allow a resource to be allocated to a task, only if the cost of the resource is at most the total cost

allowed for the task. Moreover, at any iteration, when we are processing the resources allocated to a task q , we are removing Dq vertices from V1 and 1 vertex from V2 . Therefore, in at most |V1 | iterations, we will have allocated every resource and and every task integrally. B. Cost Incurred due to Rounding Algorithm In this section, we argue that the resultant integral allocation will be at most 2Cj for every task j . Note that the first resource e1 that is allocated to any task pj has a non-zero allocation to pj , hence, ce1  Cj . Let the next resource to be allocated be eu , where u > 1. Also, let i be the lowest index such that ek |1ki xek ,pj  1. Note that, u  i. According to the procedure outlined earlier, we must have added two vertices i1 and i2 to the graph, in place of i, and replaced the edge xei ,pj by xei1 ,pj and xei2 ,pj , such that ek |1k<i xek ,pj + xei1 ,pj = 1, and xei2 ,pj = xei ,pj - xei1 ,pj . In this case, C1 = ek |1k<i cek xek ,pj + cei xei1 ,pj . Clearly, C1  cei ek |1k<i xek ,pj + cei xei1 ,pj = cei as ek |1k<i xek ,pj + xei1 ,pj = 1. Therefore, cei  C1 , hence we charge its cost to C1 at no additional cost. Similarly, let the next resource to be integrally allocated to j is ev , v > u. This means ek |ukv xek ,pj > 1, and let ew be the lowest index such that ek |ukw xek ,pj  1. Note that v  w . Again, following the same of procedure of adding two vertices (say w1 and w2 ) and replacing the edges as described above, we obtain cew  C2 and hence we charge its cost to C2 at no additional cost where C2 = ek |uk<w cek xek ,pj + cew xew1 ,pj The total cost Cj = C1 + C2 + . . . CDj , where Ci s are defined above, and we have charged the cost of the k th resource allocated to pj , to Ck-1 , where k  {2, . . . , Dj }, therefore, the total cost of these resources is  Cj . The first resource to be allocated has a cost ce1  Cj . Therefore, the cost of the total integral allocation to task pj is at most 2Cj . We repeat the above argument for every task. C. Approximation Ratio of LP Rounding Algorithm Theorem 5: Given a  for which a feasible LP solution exists, there exists a polynomial time algorithm that gives an integral feasible resource allocation to all tasks, such that the cost of any task pj , is at most 2Ij , where Ij is its isolation cost. Proof: The proof follows from the discussion in Sections IV-A and IV-B. Theorem 6: There exists a polynomial 2 + O( ) approximation algorithm to the integral fair resource allocation problem. Proof: From Theorem 5, we know that given a  , for which a feasible LP solution exists, we can find 8

an integral feasible solution, where every task costs at most 2Cj = 2Ij . From Theorem 3, we know that the lowest value for which the LP is feasible is, LP -OP T  OP T -IN T + , and it can be found in at most log cmax iterations, for any fixed > 0. Hence the resultant value of  incurred by our algorithm is  2LP -OP T  2OP T -IN T + 2 . This proves the theorem. V. I NTEGRALITY G AP FOR U NIT S IZE R ESOURCES In this section we show that the LP has an integrality gap approaching 2. Theorem 7: The LP has an integrality gap  2, hence the LP rounding algorithm of Section IV is tight. Proof: Consider an instance with m tasks, each with a resource requirement of m. Let there be m2 resources of which m2 - 1 resources are of cost 1 and another resource is of cost m. Any feasibly integral solution would have to allocate the resource of cost m to one of the tasks thereby incurring a cost of 2m-1. However, the isolation cost I of every task is m. Therefore, any inte-1 1 gral feasible solution would incur a   2m = 2- m . m For the LP, every resource is feasible to be allocated to every task for every   1, since the isolation cost of every task is m and the resource requirement of every 1 . A feasible LP task is m. Let us consider  = 1 + m solution would allocate to every task, m2 - m unit cost resources integrally, m - 1 unit cost resources to an 1 1 extent of m , and m of the resource of cost m. With such an allocation, every task receives a resource allocation of m units, and every resource is allocated up to 1. Note that the overall number of resources allocated is m2 -m+m-1+1 = m2 . The cost incurred by every task 1 1 1 +m m = m+1- m < m+1, which is (m-1)+(m-1) m m+1 is feasible since  = m . Therefore, the integrality gap 2- 1 is at least 1+ m 1  2, when m  . Hence the proof.
m

VI. G REEDY ALGORITHM In this section, we consider a restricted version of the problem, where the costs of the resources, though arbitrary, vary smoothly across resources. The resources are unit sized as in the previous sections. We give a greedy algorithm which will give a near optimal fairness in resource allocation, when all the tasks have the same resource requirement. We first define some notations. Definition 8: A set of resources are called homogenous, if, when ordered in non-increasing order of their costs, the difference in costs between two consecutive resources is very small,  for some small fixed > 0. In other words, assuming the resources are numbered according to their position in the sorted order, for any i  {1, . . . , |E|}, ci+1 - ci  . We assume that ci  1.

Definition 9: A set of tasks are called identical in terms of their resource requirements, if their resource requirements are identical. In other words, for every pair of tasks j, j  P , Dj = Dj = D, where D  1. In this version of the problem, we assume the resources are homogeneous and give a greedy algorithm which allocates resources to the tasks in iterations, till every task is fully allocated its total resource requirement. In every iteration, the task to allocate a resource is chosen according to a rule which is a function of the total resource requirement of the task, remaining resource requirement and the cost incurred so far by the task due to allocated resources. To the chosen task, the algorithm allocates the next available resource in the sorted order. The choosing rule is as follows: pick the task j with the largest value of f (j ), i.e., j = arg maxkP f (k ), D +C where f (j ) is defined as: f (j ) = j Dj j , where Dj is the total resource requirement of task j , Dj is the remaining resource requirement or current deficit, Cj is the current cost incurred by j by the resources already allocated. Note that the isolation cost of a task depends on the total resource requirement of the task, and is higher for tasks with high resource requirements. The intuition behind choosing this function is that we want to favor the tasks that have already incurred a high cost relative to the total resource requirement (hence, the isolation cost), and also the tasks for which the remaining resource requirement is high relative to the total resource requirement. By favoring a task, we mean allocating it lower cost resources. This is done to balance the ratio of actual costs to isolation costs across resources. The value of  is chosen so that the greedy algorithm behaves in a certain manner to ensure fairness of cost across tasks. 1 . Specifically,  = iE ci The pseudo-code of the greedy algorithm is presented in Algorithm 1. We will prove that when the resources are homogeneous and tasks are identical, the greedy algorithm will give a near optimal fairness ratio. Let us denote the number of tasks |P| = n, in the following analysis for notational ease, and the resource requirement of each as D. Since we have n tasks, each requiring D resources, the greedy algorithm will require nD iterations. Let us define the set of consecutive n iterations [kn + 1, kn + 2, . . . , (k + 1)n] as the k th block iteration. Clearly we have D block iterations. We next prove that the greedy algorithm will allocate resources to the tasks in an alternating round-robin manner, such that in the ith block iteration, where i  {1, . . . , D}, every task will be allocated exactly one resource. Lemma 10: Every task is allocated a resource in every 9

Algorithm 1: Greedy algorithm for fair resource allocation.
Input : P : set of tasks; Dj : total resource requirement in task j  P Dj : current resource deficit in task j  P (initially set to the original resource requirement of j ; Cj : current cost incurred by task j  P (initially set to 0); E : set of resources; ci : cost of a resource i  E Output: xij : assignment of resources to tasks. Set Dj  Dj and Cj  0, j  P . Sort all resources in E such that ci  ci+1 i  {1, . . . , |E| - 1} Set xi,j  0, i  E , j  P . Set i  1; while P =  do Choose the task j , such that
k k j = arg maxkP Dk Set xi,j  1 and allocate resource i to task j . Set Dj  Dj - 1 and Cj  Cj + ci Set i  i + 1. if Dj = 0 then P  P \ j. end

1 2 3 4 5

D +C

6 7 8 9 10 11

end

block iteration (in other words, consecutive n iterations). Proof: Let us consider the first block iteration. Suppose a task j is not allocated at all in block iteration 1. Since a block iteration consists of n rounds, this implies that some task j is was allocated  2 resources in this iteration. Consider the round in the first block iteration, when j was chosen for a second allocation. According to the choosing rule, therefore, j = arg maxkP f (k ), Dj +Cj Dj +Cj  . But Dj = D which implies D D whereas Dj = D - 1, Cj = 0 and Cj = c, where c is the cost of the resource allocated to j . Clearly, D +C Dj +Cj 1 = 1. On the other hand, j D j = 1 - D + D c 1 < 1 . This gives a contradiction. Therefore, D kE ck every task gets an allocation in the first block iteration. Now, let us assume by induction hypothesis that this holds for the first i block iterations, where i < D. Now, all tasks have have i unit resources. If a task j does not get an allocation in the iteration i + 1, then this implies that some task j is was allocated  2 resources in this iteration. Consider the round in the i + 1 block iteration, when j was chosen for a second allocation. Since j = Dj +Cj Dj +Cj arg maxkP f (k ),  . Note that D D Dj = D - (i + 1), whereas Dj = D - i. Because of our choice of , 
Dj D

Also, =1 1 - i+1 D . Therefore, f (j ) - f (j ) < 0. This gives a contradiction. Hence, we have proved inductively the statement of the lemma. Observation 11: When all tasks have received the same number of resources, the next task to be allocated a resource is the task that has incurred the highest cost

Cj 1 D < D , therefore, Dj i -D , whereas D =



Cj D

-

Cj D

<

1 D.

so far. This follows from the function used by greedy as the choosing rule. Lemma 12: At the end of every block iteration i, the cost incurred by any task j , Cj is at most Ci,OP T + 1 n , where Ci,OP T = n e{1,...,i·n} ce is the optimal fair allocation cost when all the tasks are identical, each with a resource requirement of i, and the resources are homogeneous. Proof: At the end of any block iteration i, let the costs incurred by tasks be Cj j  [1, . . . , n]. Since any optimal solution would allocate the first i · n resources from the sorted list to the n tasks, the total cost incurred across all tasks in the greedy solution till iteration i is the same as that of any optimal solution. Therefore, 1 Ci,OP T = n j [1,...,n] Cj . Let Cmin,i denote the lowest cost incurred by any task at the end of block iteration i and Cmax,i denote the highest cost incurred by any task. Clearly, Ci,OP T  Cmin,i by definition. We will prove by induction that for any pair of tasks (j, j ). at the end of block iteration i, |Cj - Cj |  n . At the end of block iteration 1, Cmin,1 = c1 , and Cmax,1 = cn  c1 + n , (where cp is the cost of the pth resource in the sorted order), by our assumption of homogeneous resources. Hence, the base case holds. Now, we assume by induction hypothesis, that the above holds for all block iterations {1, . . . , i - 1}. In the block iteration i, according to Observation 11 and Lemma 10, the task with the highest cost is allocated first, followed by the next highest cost, and so on, till the lowest cost task gets allocated last. Suppose at the end of block iteration i, Cj,i  Cj ,i , where Cj,i denotes the cost incurred by j at the end of iteration i, hence j will allocated before j . But from induction hypothesis, Cj,i  Cj ,i + n . Therefore, Cj,i+1 - Cj ,i+1  c - c + n , where c is the cost of the resource allocated to j in block iteration i + 1, and c is the cost of the resource allocated to j in block iteration i +1. Note that c  c due to the sorted order and from Observation 11. Therefore, Cj,i+1 - Cj ,i+1  n . At the same time, Cj ,i+1 - Cj,i+1 = Cj ,i + c - Cj,i - c  c - c , since Cj ,i  Cj,i . But we know from homogeneous property of resources. c - c  n , therefore, Cj,i+1 - Cj ,i+1  n . This holds for any pair of resources (j, j ). This completes the proof by induction. We have proved that for any pair of tasks (j, j ). at the end of block iteration i, |Cj - Cj |  n . Therefore, Cmax,i  Cmin,i + n  Ci,OP T + n . Theorem 13: For homogeneous resources and identical tasks, the greedy algorithm gives a 1 + approximation to the fair resource allocation problem. 10

Proof: Let Cmax,D be the highest cost incurred by any task at the end of nD iterations, and COP T be the optimal fair allocation cost. For any optimal solution, T OP T = COP where I is the isolation cost of any I resource. From Lemma 12, Cmax,D  COP T + n . T +n Therefore, the resultant greedy  COP I . Hence, greedy n  1 + . We know that C = nDcavg OP T OP T COP T where cavg the average cost of the first nD resources.  n Therefore, OP  1 + nDc < 1 + , since D  1 avg T and cavg  c1  1. VII. C ONCLUDING R EMARKS This paper studied the problem of fair allocation of resources with heterogeneous costs to tasks with heterogeneous resource requirements. We show that the problem is strongly NP-Hard even with unit sized resources, and present an LP rounding approximation algorithm for this version of the problem and a near-optimal greedy algorithm for a special case in which costs of resources do not differ much and resource requirements of tasks are identical. As part of future work, we plan to study the problem where resources have limited heterogeneity in costs, as well as, the online version of the problem. R EFERENCES
[1] Yossi Azar, Umang Bhaskar, Lisa Fleischer, and Debmalya Panigrahi. Online mixed packing and covering. In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 85­100, 2013. [2] Venkatesan T Chakaravarthy, Anamitra Roy Choudhury, Sivaramakrishnan R Natarajan, and Sambuddha Roy. Knapsack cover subject to a matroid constraint. In LIPIcs-Leibniz International Proceedings in Informatics, volume 24. Schloss DagstuhlLeibniz-Zentrum fuer Informatik, 2013. [3] Bruno Escoffier, Laurent Gourv` es, and J´ er^ ome Monnot. Fair solutions for some multiagent optimization problems. Autonomous agents and multi-agent systems, 26(2):184­201, 2013. [4] Ali Ghodsi, Matei Zaharia, Scott Shenker, and Ion Stoica. Choosy: max-min fair sharing for datacenter jobs with constraints. In Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys), pages 365­378, 2013. [5] Raj Jain, Dah-Ming Chiu, and W Hawe. A quantitative measure of fairness and discrimination for resource allocation in shared computer systems. Technical Report TR-301, DEC, 1984. [6] Hans Kellerer, Ulrich Pferschy, and David Pisinger. Knapsack problems. Springer, 2004. [7] H. J. Kushner and P. A. Whiting. Convergence of proportionalfair sharing algorithms under general conditions. IEEE Transactions on Wireless Communications, 3(4):1250­1259, 2004. ´ Tardos. Approximation [8] J. K. Lenstra, D. B. Shmoys, and E. algorithms for scheduling unrelated parallel machines. Math. Program., 46(3):259­271, 1990. [9] Alan Shieh, Srikanth Kandula, Albert G. Greenberg, Changhoon Kim, and Bikas Saha. Sharing the data center network. In NSDI, 2011. ´ [10] David B. Shmoys and Eva Tardos. An approximation algorithm for the generalized assignment problem. Math. Program., 62(3):461­474, 1993. [11] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering. In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science (FOCS), pages 538­ 546, 2001.

Efficient routing and cross-contamination minimization are two interrelated challenging areas in Digital Microfluidic Biochip (DMFB). This paper proposes a two phase heuristic technique for routing droplets on a two-dimensional DMFB. Initially it attempts to route maximum number of nets in a concurrent fashion depending on the evaluated value of a proposed function named Interfering Index (IInet). Next the remaining nets having interfering index values higher than threshold will be routed considering various constraints in DMFB framework. Our method will check all the conflict arises between various routing paths from same assay or from different assays and also with the unsuccessful routing paths from the first phase (if any). If conflict occurs it determines the conflict zone on the chip as Critical Region(CR). Another metric named Routable Ratio (RR) is proposed and depending on RR metric, our method prioritized the routing order among conflicting paths(nets) to avoid deadlock from there onwards till the droplet reaches its target location. Experimental results on Benchmark suites III show our proposed technique reduces latest arrival time upto certain extent but significantly reduces average assay execution time and number of used cell compared to earlier routing methods for DMFB.Multi-mode Sampling Period Selection for Embedded Real Time Control
Rajorshee Raha, Soumyajit Dey, Partha Pratim Chakrabarti, Pallab Dasgupta Department of Computer Science & Engineering Indian Institute of Technology Kharagpur, INDIA {rajorshee.raha, soumya, ppchak, pallab}@cse.iitkgp.ernet.in

arXiv:1506.08538v1 [cs.SY] 29 Jun 2015

ABSTRACT
Recent studies have shown that adaptively regulating the sampling rate results in significant reduction in computational resources in embedded software based control. Selecting a uniform sampling rate for a control loop is robust, but overtly pessimistic for sharing processors among multiple control loops. Fine grained regulation of periodicity achieves better resource utilization, but is hard to implement online in a robust way. In this paper we propose multi-mode sampling period selection, derived from an offline control theoretic analysis of the system. We report significant gains in computational efficiency without trading off control performance.

General Terms
Real Time Control, Automotive Software

Keywords
Real Time Control, Automotive Software

1.

INTRODUCTION

Embedded software-based control systems have traditionally been implemented by assuming fixed sampling rates and fixed task periods [3]. The sampling rate is derived from a control theoretic analysis [9] of the system in a manner that guarantees desired level of control performance at all reachable states of the system. A uniform period can be implemented robustly, since we can analyze the sampling rate of all the control tasks and then choose an appropriate computational infrastructure that can statically schedule a periodic execution of the software components of these control tasks. The schedule does not change during execution and hence the control performance is deterministic. Recent studies confirm a widely accepted belief, namely that a uniform sampling rate is not a good choice when multiple control loops share a common computing resource such as an Electronic control unit (ECU). These studies establish

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

that the sampling period can be regulated to achieve significant benefits in computational performance without any trade off in control performance. In fact, it has also been shown that a non-uniform scheduling strategy can balance the sampling rates among the control loops sharing a ECU in such a way that the overall control performance improves [4, 5]. Uniform sampling rate is typically a pessimistic choice, since we need to choose a sampling rate that guarantees control performance at all reachable states of the system. It is often the case that the selected rate is necessary at only specific control states of the system, whereas at all other states a much lesser sampling rate suffices. Adaptive sampling derives its benefit from this fact by intelligently regulating the sampling period as needed to maintain the desired level of control performance [4]. Fine grained regulation of the sampling rate may theoretically determine the optimal balance between computational efficiency and control performance but such schemes are difficult to implement in practice due to non-determinism in timing introduced by the computational infrastructure (including message delays, execution time variations in different paths of the control software, etc). If the sampling rate is known a priori then it becomes possible to develop an appropriate schedule for all control tasks sharing a ECU with adequate consideration for these types of non-determinism. In this paper we profess the use of coarse grained regulation of the sampling rate. Specifically we propose an approach where for each control loop, a limited number of sampling rates are chosen and a control theoretic analysis is used to determine the switching criteria between these modes. Therefore, for each control loop we have a set of sampling states, and an automaton that captures the switching between these sampling states. The global sampling state of the system is a concatenation of the sampling states of the control loops in the system. For each global sampling state, the exact schedule for the control tasks is precomputed considering all types of non-determinism arising out of the execution of software tasks. The reachable global sampling states are chosen based on available computational bandwidth and the relative priorities of the control loops. The primary objective of this paper is to establish the benefit of using multiple discretely chosen sampling rates. In the process, we also present the following enabling contributions. 1. We outline the basis for choosing the various sampling rates based on use case analysis. 2. We present an analytical approach which determines

the criteria for switching between sampling rates so that control performance is not hampered. 3. We present the construction of an automaton based scheduler, which implements the switching between sampling rates of the controller. We present the necessary background for the work in Section 2. We outline the methodology of multi-mode sampling period selection in Section 3. Subsequently, we use the control theoretic model of an Anti-lock Braking System (ABS) as a running example to validate our approach.

3. METHODOLOGY OUTLINE
In this section, we outline our proposed methodology of multi-mode sampling period selection for embedded real time control. The main steps of this proposed multi-mode methodology are as follows, · Step I : Developing a control theoretic model of the corresponding system. · Step II : Classification of different modes based on different control parameters and selection of best possible sampling rate for the corresponding operating modes. · Step III : Construction of a supervisory automaton for controlling the mode switching.

2.

BACKGROUND STUDY

In this section, we outline the mathematical relation between the sampling period of a discrete time controller and its control stability. Any discrete time feedback control system can be represented as shown in Fig. 1 [2], here x(t) is the input to the system, e(t) is the error signal, u(t) is the controller output and y (t) is the plant output fed back to the controller using a sensor.

Figure 2: Methodology Outline In the following sections we demonstrate the proposed methodology with an extensive analysis of ABS as a running example. In Section 4, we present the control model of the ABS. In Section 5, we divide the driving pattern of a vehicle into multiple modes parameterized by vehicular characteristics like velocity, brake pedal pressure and slip. For each such mode, we choose a sampling frequency which ensures stability guarantee of the ABS. In Section 6, we outline our approach for guard condition selection for switching between different modes, and synthesize a scheduler automaton which may supervise the mode selection depending on vehicular dynamics. In Section 7, we provide experimental results supporting the proposed approach.

Figure 1: Discrete Time Control System Generally, the sampling of the continuous signal is done at a constant rate T , which is known as the sampling period or interval. The sampled signal ek = e(kT ), is the discretized signal with k  N. In general, for any transfer function, the control signal output depends on n previous control signal output instances and m previous error signal instances [2]. The situation may be represented as, uk = -a1 uk-1 - a2 uk-2 - · · · - an uk-n + b0 ek + (1) b 1 e k -1 + b 2 e k -2 + · · · + b m e k -m The discrete signals uk-1 ,uk-2 , .. are the delayed versions of uk by sampling period T, 2T, .. respectively. In Laplace domain, a time delay is introduced into a signal by multiplying its Laplace transform by the operator e-T s . Let the Laplace domain representation of uk and ek be U (s) and E (s) respectively [7]. Hence, uk-1 , uk-2 , .. can be represented in Laplace frequency domain as e-T s U (s), e-2T s U (s) , .. respectively and similarly for ek-1 , .. . Thus, the corresponding Laplace domain representation of Eq. 1 shall be, U (s) = -a1 e-T s U (s) - a2 e-2T s U (s) - · · · + b0 E (s)+ b1 e-T s E (s) + b2 e-2T s E (s) + . . . (2)

4. CONTROL MODEL
ABS is an automobile safety critical driver assistance system which prevents the wheels from locking and avoids uncontrolled skidding. An abstract block diagram of a vehicle with ABS is shown in Fig. 5. For designing the vehicle we used a simplified quarter car model as shown in Fig. 3. Here, m is the mass of the quarter vehicle, Vx is lateral speed of the vehicle,  is the angular speed of the wheel, FN is the vehicle vertical force, Fx is the frictional force transmitted to the road, Mb is the braking torque, R is the wheel radius and J is the wheel inertia. Wheel slip  is given as,  = 1 - R/Vx . The effective braking force is dependent on the frictional force [1] transmitted to the road which is related to FN as, Fx = -µFN , where µ is the frictional coefficient of the road

Substituting eT s with the discrete frequency domain operator z [2] and simplifying this further we get the discrete time transfer function of the controller C (z ) as, m b0 z n + b1 z n-1 + .. + bm z n-m j =1 (z - zj ) n-m = b z (3) 0 z n + a1 z n-1 + .. + an n i=1 (z - pi ) where zj are the zeros and pi are the poles of the transfer function. Behavior of any discrete time controller can be observed by analyzing the poles and zeros of the corresponding transfer function [2]. The positions of the poles and zeros differ for different sampling intervals (T ). Correspondingly, the control stability of the overall system gets effected. More related background is provided in Appendix A.

Figure 3: 1/4 Car Forces and Torques [1]

1

Frictional Co-efficient

design a discrete PID controller for this purpose as, de dt Ki T Kd (z - 1) Mb (z ) = [Kp + + ]E (z ) z-1 T Mb = Kp e + Ki edt + Kd
1

0.8 0.6 0.4 0.2 0 0 Dry Aspalt Gravel Loose Gravel Wet 0.2 0.4 0.6 0.8

(7)

Slip

Figure 4: µ -  Curve [10]

Here Kp , Ki , Kd are the proportional, integral, and derivative gain respectively of the PID controller. T is the sampling interval. Mb (z ), E (z ) are the discrete domain representation of the the braking torque, Mb and the error signal, e = d - , which is the difference between the desired slip (d ) and actual slip (). Observe that when  = d then  = 0. Substituting this in Eq. 5, Mb can be e = 0, i.e.  represented as, J Mb = [( - 1) - mR]Vx (8) R Hence, it is obvious from Eq. 7 & 8 that the control performance, i.e. stability of the ABS controller will vary for different values of T , Vx and .

Figure 5: ABS Overview [1]

surface. Thus, as evident from Fig. 4, the amount of slip will vary depending on road conditions. Relationship between wheel slip and frictional coefficient can be approximated, µ = f (), using a piecewise linear function [6] as, µ= , 1 -2 +   0.2  > 0.2 (4)

5. MODE AND PERIOD SELECTION
The candidate sampling modes and periods for a controller are determined by partitioning its input space based on the use-case scenarios and stability of the control law in those scenarios. For example, in ABS, the adequacy of a sampling rate in a given scenario depends on the urgency of braking (which is a function of vehicle speed and pedal pressure) and the slip ratio (which is a function of the vehicle speed and friction on the road). In general, we select the different possible sampling frequencies of the controller using the following approach. 1. Identify vehicular parameters which impact the controller output (i.e. braking torque in this case). 2. Identify multiple possible driving scenarios and corresponding ranges of vehicular parameters and also the probable driver response. 3. Perform stability analysis followed by identification of maximal acceptable sampling period in each driving scenario. Steps 2 and 3 may have to be iterated to arrive at a gainful combination of sampling modes. A sampling mode is effective towards gaining computational efficiency only if the controller stays in that mode for a non-trivial period of time. Therefore it is necessary to relate the sampling modes with different use-case scenarios. In ABS, we estimate different possible traffic scenarios (city traffic, suburban or medium traffic & highway traffic) and the variation in traffic density, traffic regulations and corresponding average cruising speed and driver reaction to arrive at the sampling modes. We categorize the brake pedal pressure range as low, mild, medium and high, and also consider various speed ranges and slip ratios. For each of the scenarios, we determine the sampling rate at which the controller is stable. Through this study we selected three sampling modes, as outlined below: · N0 Mode: This mode requires the least sampling rate among the three chosen mode, and targets scenarios where the vehicle is cruising at low to medium speeds (such as in city traffic). Considering average cruising speed and the driver reaction, we arrive at the operating sampling rate

3 4

+ ,

where,   [0, 8] and   [-0.1, 0.1]. The non-linear equations for designing a quarter car model can be given as,
1 Vx = - m FN µ R b   = J FN µ - M J 2  = - 1 [ 1 (1 - ) + R ]FN µ +  Vx m J

(5)
1 R Vx J

Mb

Using Taylor series expansion method [6, 8] for linearizing a nonlinear system we obtain a linear (affine) system description from Eq. 5 as, x  = Al x + El + Bl u y = Cl x + Dl u (6)

where, xT = [Vx , ], u = Mb · Vx , y = [], Al , Bl , Cl , Dl are system input and output matrices respectively. El , l = f (x) are the affine term and function telling the validation of linearizion. The formation of this state space equation from the nonlinear equations are described in details in Appendix B. The objective of ABS controller is to decelerate the vehicle as fast as possible, while maintaining its steer ability by minimizing wheel slip. The main components of ABS are the ABS-ECU, hydraulic modulator, and wheel speed sensor. The ECU constantly monitors the wheel rotational speed through the wheel speed sensors, and also measures the actual slip. The controller's task is to maintain the braking torque within a certain range. The braking force is applied to the wheels by the hydraulic modulator. It rapidly pulses the brakes to prevent wheel lock up, even during panic braking in extreme conditions and promises shortest possible distance under most conditions. We can

Ts = 0.2ms in which the ABS achieves satisfactory control performance as given by Fig. 6. For a given velocity (X axis) and slip (Y axis), we carry out the standard unit circle analysis and plot the maximum magnitude among the different pole positions (Z axis) of the transfer function corresponding to Ts = 0.2ms. The stability variation for different slip values is due to the piecewise linear function given in Eq. 4. It may be observed from Fig. 6, all the poles are of magnitude <= 1 for velocity range [0 . . . 85]km/h and slip range [0 . . . 0.65] thus ensuring stable vehicular dynamics. Thus correspondingly brake pedal pressure variation range is selected as [low, mild, medium] estimating the probability of the above mentioned slip range. We carry out similar analysis for the other cruising modes and derive satisfactory sampling intervals.

Unstable
2

2.5

|Pole|

1.5 1 0.5 0 1 0.8 0.6 200 0.4 0.2 150 100 0 50 0 250

Stable

Slip

Velocity (km/h)

Stable

Figure 8: E Mode Stability Guarantee

6. SUPERVISORY AUTOMATA
Our analysis of the different possible driving scenarios and the choice of sampling periods entails the creation of a scheduler which may dynamically switch the controller among different sampling modes. Such a supervisory automaton is shown in Fig. 9. The criteria for switching between the three

Unstable
2

2.5

|Pole|

1.5 1 0.5 0 1 0.8 0.6 100 120 140

Stable Slip

Figure 9: Scheduler Automata
0.4 0.2 0 0 20 40 60 80

Velocity(km/h)

Stable

Figure 6: N0 Mode Stability Guarantee · N1 Mode: This mode uses higher sampling rate than N0, and targets suburban traffic scenarios. The maximum sampling interval with stability guarantee is found to be Ts = 0.15ms as shown in Fig. 7.

modes of the automaton is chosen based on our observations about vehicular parameters vis-a-vis stability. Let "v " and "bpp" be the velocity and brake pedal pressure respectively. The brake pedal pressure is divided in to the ranges low, mild, medium and high. High brake pedal pressure represents the probability of larger value of slip. Medium, mild and low brake pedal pressure signifies lesser value of slip. The guard conditions for switching between states are given as,
n0 = = = (v  [0 . . . 85] & bpp  [low, mild, medium]) | (v  [85 . . . 140] & bpp  [low, mild]) (v  [0 . . . 80] & bpp  [low, mild, medium]) | (v  80 . . . 135] & bpp  [low, mild]) (v  [0 . . . 85] & bpp  [high]) | (v  [85 . . . 140] & bpp  [medium, high]) | (v  [> 140] & bpp  [low, mild]) (v  [0 . . . 80] & bpp  [high]) | (v  [80 . . . 135] & bpp  [medium, high]) | (v  [> 135] & bpp  [low, mild]) e = (v  [> 140] & bpp  [medium, high])

2.5

Unstable
2

 n 0

|Pole|

1.5 1 0.5

n1

Stable
0 1 0.8 0.6 140 160 180 120

Slip

0.4 0.2 0 0 20 40 60

80

100

 n 1

=

Velocity(km/h)

Stable

Figure 7: N1 Mode Stability Guarantee · E Mode: This mode uses a sampling rate that is adequate in all scenarios. Existing approaches for choosing a uniform sampling mode will choose this sampling rate. We choose the sampling frequency for which the vehicle remains stable considering all velocity and brake pedal pressure variations as shown in Fig. 8. The corresponding sampling period for our model is found to be Ts = 0.1ms. We designate this as emergency sampling mode, in case of any driving irregularity the controller switches to this mode, thus ensuring vehicular stability. Z-domain unit circle stability analysis is used to show the vehicular parameters and stability relationship graphically for extensive parameter ranges. Similar observations can be obtained mathematically using other nonlinear system stability [2] criteria like, Lyapunov, Nyquist, Routh-Hurwitz or Bode plot analysis. The effective braking pressure range is varied in each of these modes to achieve satisfactory performance, depending upon driving scenarios. The mode switching and detailed switching criterion are discussed in the next section.

The guard conditions for switching between `N 0' to `N 1'  and vice versa are selected to be `n1 ' and `n 0 ' respectively. Similarly, for `N 1' to `E ' and vice versa, switching condi tions are `e ' and `n 1 ' respectively. The guard conditions are selected ensuring some amount of hysteresis while mode switching. For example, the automaton switches from mode `N 0' to mode `N 1' in case bpp = high and v  [0 . . . 85]. However, the automaton switches from mode `N 1' to mode `N 0' when v  [0 . . . 80] and bpp is not high. Similarly, the automaton switches from mode "N 1" to E when bpp is medium or high and v  [> 140], however, the automaton switches from mode `E ' to mode `N 1' when v  [> 135] and bpp is low or mild. Whenever the automaton makes a transition from a mode with lower sampling period to a mode with higher sampling period (e.g. E to N 1), it ensures that the guard conditions are valid for a certain prefixed number of clock cycles. In that way, unwanted glitches due to faulty sensor readings are expected to be filtered out.

Magnitude (dB)

The main objective of synthesizing the scheduler automaton was to reduce ECU bandwidth requirement. Scheduling in E mode signifies a sampling periodicity of 0.1ms, while the sampling periodicity of N 0 and N 1 modes are 0.2ms and 0.15ms respectively. If we notice the mode switching scenarios, we observe that when the car is cruising at a certain speed and no brake pedal pressure is applied, the controller is scheduled using infrequent sampling periods (N 0 or N 1). Further, in scenarios when the car is cruising at a certain speed and brake pedal pressure is applied, the mode switching will be supervised by the respective scheduler automaton ensuring that it will switch to a more frequent sampling mode. Thereby, in a general cruising scenario, a multi-mode controller ensures nearly 30% - 50% ECU bandwidth saving as shown in our simulation results.

the stability of the system. We empirically observe the variation of stability with sampling rate. For stability analysis, we used unit circle as well as bode plot analysis [2]. We take a moderate velocity, say V = 60km/h and calculate the stability of the system for different sampling intervals. We show one such example Bode plot for stability analysis in Fig. 11. It may be observed that with the sampling interval Ts = 0.01s, the system is unstable. However, if we drastically reduce the sampling interval to 0.1ms, the system becomes stable. In Appendix C.1 and C.2 more such results are given.
Bode Diagram 1 0 -1 -2 -3 10 0.1ms 0.01s

7.

RESULTS

The initial part of this section is devoted towards establishing the motivation for multi-mode sampling through experimental results. The latter part of the section demonstrates the gain in computational bandwidth and the benefit of effective sharing of computational resources between multiple controllers. We analyze the performance of the ABS for different sampling intervals. Observing the µ vs  curve in Fig. 4 carefully, we notice that the peak point on most of the road scenarios belong to the range [0, 0.2]. Hence, for effective braking with maximum possible road friction, we set d = 0.2 as the desired slip. In our experimental setup, when brake force is applied with current velocity V = 100km/h, it is expected to gradually decrease until V = 0km/h and throughout this deceleration phase the slip value should be as close as possible to the desired slip (d =0.2) thus ensuring smooth braking. Finally the slip value should be 1 (normalized slip) when the vehicle comes to rest.
100 90 80 70 60 Speed Speed 50 40 30 20 10 0 0 2 4 6 8 Time 10 12 14 16 18 Vehicle Speed Wheel Speed 100 90 80 70 60 50 40 30 20 10 0 0 2 4 6 8 Time 10 12 14 16 Vehicle Speed Wheel Speed

Phase (deg)

5 0 -5

System: 0.01s Phase Margin (deg): -180 Delay Margin (samples): Inf At frequency (rad/s): 0 Closed loop stable? No System: 0.1ms Phase Margin (deg): -180 Delay Margin (samples): 2.62 At frequency (rad/s): 1.2e+04 Closed loop stable? Yes 10
-2

-10 -3 10

10 10 Frequency (rad/s)

-1

0

10

1

10

2

Figure 11: Stability Variation:Ts = 0.01s, Unstable & Ts = 0.1ms, Stable We have employed our methodology of multi-mode sampling period selection for the ABS running example. We consider a braking scenario with the initial and final speed being 200km/h and 0km/h respectively and compare the minimum stopping distance achieved by our `Multi-mode' ABS Controller with the existing Matlab model of ABS with fixed periodicity. The simulation results are provided in Table 1 considering different possible road surfaces. We obTable 1: Stopping Distance in Kilometer
ABS Controller Existing model Multi-mode Dry Asphalt 3.073 3.080 Road Surface Gravel Loose Gravel 3.424 3.771 3.433 3.795 Wet 4.269 4.289

Slip 1 Normalized Relative Slip 0.8 0.6 0.4 0.2 0 Normalized Relative Slip 1 0.8 0.6 0.4 0.2 0

Slip

0

5

10 Time

15

20

0

2

4

6

8 Time

10

12

14

16

Figure 10: Slip Variation: the Left & Right column figures correspond to sampling time Ts1 = 1s & sampling time Ts1 = 0.01s respectively. We observe that the slip varies significantly for two different sampling rates as given by Fig. 10. Observe from the left column in Fig. 10 that with a choice of moderate sampling rate, the slip varies drastically thus leading to undesired perturbations in vehicular speed while braking. However, as shown in the right column of the same figure, with the higher sampling rate, the slip exhibits a well damped trajectory around the desired value (d =0.2) leading to smoother vehicular deceleration. Our notion of control performance is based on ensuring

serve that the stopping distance is nearly same for both the controllers. For this `panic braking' scenario, the estimated percentage of time spent in each of E , N 1 and N 0 modes is highlighted in Fig. 12. It is evident from Fig. 12 that we can save significant amount of ECU bandwidth (30% - 50%) using a multi-mode controller as compared to the controller with fixed periodicity.

100 90

B/W Requirement (%)

80 70 60 50 40 30 20 10 0 0
Multi-mode (Wet Road) Multi-mode (Loose Gravel Road) Multi-mode (Gravel Road) Multi-mode (Dry Road) Existing (All Road)

200

400

600

800

1000

Time (sec)

Figure 12: ECU, Panic Braking Scenario

Further, we investigate the utility of our multi-mode ABS controller in a general cruising scenario where the car is being driven in a speed range of 0km/h to 200km/h, in various traffic densities. The ECU bandwidth requirement for this scenario is shown in Fig. 13. It is evident that the multimode controller requires much lesser bandwidth compared to the existing controller since the supervisory automaton schedules the controller in the infrequent sampling modes for most of the time as described in Section 6. In that way, we can guarantee a significant amount of bandwidth saving which may be utilized for scheduling other tasks.
100

100

B/W Requirement (%)

80 60 40 20 0 0

ACC ABS

200

400

600 800 Time (sec)

1000

1200

Figure 15: ECU Sharing: ACC-ABS

8. CONCLUSIONS
The present work provides a methodology for adaptively regulating the sampling rate of embedded software based controllers leading to significant reduction of computational resource requirement. Applying the methodology on single controller based systems like ABS has shown that 30% - 50% reduction in ECU bandwidth requirement is possible. Further, it was also shown that the method smoothly scales up for multiple controller based systems. Our future research shall focus on giving a sound formal underpinning to the method of creating multiple sampling modes for software based controllers and creating a tool flow which mechanizes the synthesis of such multi-mode controllers.

B/W Requirement (%)

80

60

40

20

Multi-mode Existing
500 1000 1500 2000

0 0

Time (sec)

Figure 13: ECU, General Cruising Scenario We further demonstrate the promise of multi-mode sampling for a multiple ECU scenario taking an Adaptive Cruise Control (ACC) system as a running example. ACC is an automobile safety critical driver assistance system which automatically adjusts the vehicle speed in order to maintain a safe distance from vehicles ahead. In case of an ACC system, the driver sets a safe cruising speed and a desired safe distance (from preceding vehicle) as controller inputs. The other inputs of an ACC controller which comes from the radar sensor are preceding vehicle speed and approximate distance from preceding vehicle as shown in Fig. 14.

9. REFERENCES
[1] Safety, Comfort and Convenience Systems. Robert Bosch GmbH, 2006. [2] K. ° Astr¨ om and B. Wittenmark. Computer controlled systems: theory and design. Prentice Hall, 1984. [3] G. Buttazzo. Research trends in real-time computing for embedded systems. SIGBED Rev., 3(3). [4] A. Cervin, M. Velasco, P. Marti, and A. Camacho. Optimal online sampling period assignment: Theory and experiments. IEEE Transactions on Control Systems Technology, 19(4), 2011. [5] D. Henriksson and A. Cervin. Optimal on-line sampling period assignment for real-time control tasks based on plant state information. In 44th IEEE CDC-ECC, 2005. [6] H. Khalil. Nonlinear systems. Macmillan Pub.Co., 1992. [7] B. Kuo. Automatic control systems. Prentice Hall, 1962. [8] M. Schinkel and K. Hunt. Anti-lock braking control using a sliding mode like approach. In Proceedings of the ACC,, volume 3, 2002. [9] D. Seto, J. Lehoczky, L. Sha, and K. Shin. On task schedulability in real-time control systems. In 17th IEEE RTSS, 1996. [10] J. Wong. Theory of Ground Vehicles. Wiley, 2001.

Figure 14: ACC System Overview [1] ACC is a drive assist system for highway cruising which monitors the inputs and decides cruising speed or distance to lead vehicle. When the applied brake pedal pressure is high, the ACC is overridden by the braking controller (ABS). The operating modes of ACC controller can be classified into `active', `suspended' and `idle' while our multi-mode ABS operates in three different modes as discussed previously. Generally, the ACC system and the braking controller are mapped to separate ECUs. We can achieve significant reduction in bandwidth requirement by sharing an ECU between these features. When the ACC is suspended, because the applied brake pedal pressure is high, we schedule the ABS controller with frequent sampling rate and the ACC controller with relatively infrequent sampling rates. On the other hand, when the ACC is active, i.e. applied brake pedal pressure is low or null, then the ACC controller is scheduled with frequent sampling rate and the ABS controller is scheduled with relatively infrequent sampling rate. In Fig. 15, we show the bandwidth requirement of both ACC and ABS controllers when scheduled in a single ECU while providing satisfactory control performance.

APPENDIX A. BASIC CONTROL SYSTEM
Any continuous feedback control system[7] can be represented as shown in Fig. 16, where x(t) is the input to the system, e(t) is the error signal, u(t) is the controller output and y (t) is the plant output fed back to the controller using a sensor(H). Correspondingly, a Discrete time Feedback

A.1 Stability: Discrete Time Control System
The stability property of this system can be defined from the impulse response of a system as · Asymptotic stable system: The steady state impulse response is zero. lim y (k) = 0
k

· Marginally stable system: The steady state impulse response is different from zero, but limited. lim 0 < y (k) < 
k

· Unstable system: The steady state impulse response is unlimited. lim y (k) = 
k

Figure 16: Continuous Time Control System Control System can be represented as shown in Fig. 17, where the dotted box highlights the discretized controller [2]. Generally, the sampling of the continuous signal is done

where y (k) is the impulse response of the corresponding system. The impulse response for different stability property is illustrated in Fig. 18. Let us assume a control system with
Stable System
2 Magnitude 1.5 1 0.5 0 0 50 100 150 Time 200 250 300

Marginally Stable System
1 Magnitude 0.5 0 -0.5 -1 0 x 10
25

Figure 17: Discrete Time Control System at a constant rate T , which is known as the sampling period or interval. The sampled signal yk = y (kT ) is the discretized signal with k  N. For understanding the design of a discretized control system, let us first consider a simple continuous domain transfer function for the controller C in Fig. 16 given as follows. C (s ) =
du dt U (s) E (s)

50

100 Time

150

200

250

Unstable System

1 Magnitude

0

-1

=

K (s+a) (s+b)

0

50

100

150 TIme

200

250

300

The corresponding time domain representation shall be, + bu = K ( de + ae) dt The Euler's approximation of the first order derivative is represented as: dx xk+1 - xk  (9) dt t Applying Euler's approximation of first order derivative [2] on the continuous differential equation, we get the following discrete difference equation.
uk+1 -uk t

Figure 18: Impulse Response and Stability input u and output y . The transfer function of any discrete time control system can be represented as C (z ) =
y (z ) u (z )

=

bz ( z - p)

where p is the pole which is in general a complex number and can be written in polar from as p = mej where m is the magnitude and  is the phase. The impulse response of the system can be given as } = b|m|k ejk y (k) = Z -1 { zbz -p Thus, it is the magnitude m which determines if the steady state impulse response converges towards zero or not. The relationship between stability and pole placement can be stated as follows. · Asymptotic stable system: All poles lie inside (none is on) the unit circle, or what is the same: all poles have magnitude less than 1. · Marginally stable system: One or more poles but no multiple poles are on the unit circle. · Unstable system: At least one pole is outside the unit circle. The situation is graphically shown in Fig. 19.

+ buk = K (

ek+1 -ek t

+ aek )

Generally, t, K, a and b are fixed. The digital controller updates the control signal every cycle as per the following equation, uk+1 = -a1 uk + b0 ek+1 + b1 ek (10) where, b0 = K , b1 = K (at - 1), a1 = (bt - 1). In general, for any transfer function, the control signal output depends on n previous control signal output instances and m previous error signal instances which can be represented as[2], uk = -a1 uk-1 - a2 uk-2 - · · · - an uk-n + b0 ek + b 1 e k -1 + b 2 e k -2 + · · · + b m e k -m (11)

The discrete signals uk-1 ,uk-2 , . . . are the delayed versions of uk by sampling period T, 2T,. . . respectively.

wheel slip and frictional coefficient can be approximated by using piecewise linear function as, µ() = , -1 + 2
3 4

+ ,

  0.2  > 0.2

(12)

where,   [4.8, 5.1, 5.46, 6.4] and   [-0.1, 0.1]. Al = 0
2  NR   FV 2 x J N - F m F N R2  V  J x

(13)

Figure 19: Unit Circle: Stability areas in Complex Plane

El = and for  > 0.2 Al = 0

0
R2   - F N J Vx 

(14)

B.

QUARTER VEHICLE MODELING

A quarter car model is shown in figure 20. Here, m is the

FN R +3 ) FN R ± 0.1 V (-  2 J 2 4 V 2 J 
x x



2

2

FN 4m F N R2 J 4Vx 

(15)

El =

(

 2

3 N ± 0.1) F (- 4 m 2 F R2 3 NR ± 0 .2 F -2 ) VN J V  J 
x x

(16)

The stability analysis of this system can be done using Lyapunov, Bode, Nyquist, Unit Circle or Hurwitz stability criteria [8].

C.
Figure 20: 1/4 car forces and torques mass of the quarter vehicle, Vx is lateral speed of the vehicle,  is the angular speed of the wheel, FN is the vehicle vertical force, Fx is tire frictional force, Mb is the braking torque, R is the wheel radius and J is the wheel inertia. Wheel slip  is represented as: =1-
R Vx

STABILITY V/S SAMPLING PERIOD

In this section we provide few examples relating to the variation in stability with respect to sampling periods.

C.1 Simple Examples:
Let the transfer function of any arbitrary system be, U (s ) =
s+0.5 ms2 +bs+u

The relationship between FN and Fx [1] is given as: Fx = -µ()FN where µ() is the frictional coefficient of the road surface. The non-linear equations for designing a quarter car model can be given as[8]:
1 Vx = - m FN µ() R b   = J FN µ() - M J x (1-)-R V   = 

Case 1: Let m=2, b=-0.5, u=1. For Ts =2s the system unstable but for Ts =1s the system is stable. The values of m, b, u are same for both of the sampling intervals. The corresponding stability response is shown in Fig. 21.
Bode Diagram 10 5 0 -5 -10 -15 -135 Phase (deg)
System: Ts_1 Gain Margin (dB): 11.8 At frequency (rad/s): 3.14 Closed loop stable? Yes

 = 

1 -V [ 1 (1 x m

- ) +

Vx R2 ]FN µ() J

+

1 R Vx J

Mb

Magnitude (dB)

Ts_2 Ts_1

-180 -225 -270 -315 -360 -2 10
-1 0 1

For linearizing the system approximation using the Taylor  series expansion can be expressed as [6]: f (, Vx )  f ( , Vx ) d f d f    ( -  ) + + d | ,Vx |   (V - Vx ). From this from dV  ,Vx this we can derive a linear (affine) system description as: x  = Al x + El + Bl u y = Cl x + Dl u l = f (x ) where, Al , Bl , Cl , Dl are system input and output matrices respectively. El are the affine terms and f (x) is the function telling the validity of linearizion and xT = [Vx , ] , u = Mb · Vx , y = []. From Fig. 4 the relationship between

System: Ts_1 Phase Margin (deg): -132 Delay Margin (samples): 9.04 At frequency (rad/s): 0.44 Closed loop stable? Yes

System: Ts_2 Phase Margin (deg): -12.2 Delay Margin (samples): 3.12 At frequency (rad/s): 0.973 Closed loop stable? No

10

10 Frequency (rad/s)

10

Figure 21: Ts = 2s, Unstable; Ts = 1s, Stable Case 2: Let m=5, b=1, u=10. In this case the system stable for both the sampling intervals Ts =2s and Ts =1s. The values of m, b, u are same for both of the sampling intervals. The corresponding stability response is shown in Fig. 22.

Bode Diagram 0 -5 Magnitude (dB) -10 -15 -20 -25 -30 45 0 Phase (deg) -45 -90 -135 -180 -2 10
-1 0 1

Ts_2 Ts_1

System: Ts_2 Gain Margin (dB): 2.43 At frequency (rad/s): 1.57 Closed loop stable? Yes

Case 3: For V = 40 Km/h,  = 0.2, the stability response is shown in Fig. 25.
Bode Diagram 0.5

Magnitude (dB)

System: Ts_1 Gain Margin (dB): 18.3 At frequency (rad/s): 3.14 Closed loop stable? Yes

0 -0.5 -1 -1.5 5

10

10 Frequency (rad/s)

10

Phase (deg)

System: T_N Phase Margin (deg): -179 Delay Margin (samples): 9.58 At frequency (rad/s): 33.1 Closed loop stable? No

0
System: T_E Phase Margin (deg): -180 Delay Margin (samples): 6.72 At frequency (rad/s): 9.38e+03 Closed loop stable? Yes

Figure 22: Ts = 2s, and Ts = 1s, Both Stable

C.2

ABS example

-5 -2 10

10

-1

10 10 Frequency (rad/s)

0

1

10

2

10

3

We provide more examples of stabiliy variation with sampling interval for the ABS controller model. Case 1: For V = 10 Km/h,  = 0.1, the stability response is shown in Fig. 23.
Bode Diagram 0.2

Figure 25: Ts = 0.6 ms, Unstable; Ts = 0.3 ms, Stable Case 4: For V = 15 Km/h,  = 0.3, the stability response is shown in Fig. 26.
Bode Diagram 0.4 T_E T_N

Magnitude (dB)

0.1 0 -0.1 -0.2 -0.3 1 Magnitude (dB) 0.2 0 -0.2 -0.4 -0.6 2 Phase (deg)
System: T_N Phase Margin (deg): -180 Delay Margin (samples): Inf At frequency (rad/s): 0 Closed loop stable? Yes

Phase (deg)

0.5 0 -0.5

1 0 -1 -2 -2 10

System: T_N Phase Margin (deg): -179 Delay Margin (samples): 1.39 At frequency (rad/s): 2.27e+03 Closed loop stable? Yes

T_N -1 -2 10 10
-1

System: T_E Phase Margin (deg): -180 Delay Margin (samples): 2.43 At frequency (rad/s): 2.6e+03 Closed loop stable? Yes

10

0

10 Frequency (rad/s)

1

10

2

10

3

10

4

10

-1

10

0

10 Frequency (rad/s)

1

10

2

10

3

10

4

Figure 23: Ts = 0.6 ms, Stable Case 2: For V = 100 Km/h,  = 0.6, the stability response is shown in Fig. 24.
Bode Diagram 0 -200 -400 -600 -800 180 Phase (deg) 0 -180 -360 -540 -3 10
System: T_E Phase Margin (deg): -180 Delay Margin (samples): 3.34 At frequency (rad/s): 1.88e+04 Closed loop stable? Yes System: T_N Gain Margin (dB): 635 At frequency (rad/s): 108 Closed loop stable? No

Figure 26: Ts = 0.6 ms, and Ts = 0.1 ms, Both Stable From these examples we can observe that sampling period has a major role to play in deciding the stability of a software based controller.

Magnitude (dB)

10

-2

10

-1

10 Frequency (rad/s)

0

10

1

10

2

10

3

Figure 24: Ts = 0.6 ms, Unstable; Ts = 0.1 ms, Stable

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/262602090

AlgorithmsforRotationSymmetricBoolean Functions
Article·September2012

CITATIONS

READS

0
4authors,including: SatrajitGhosh AcharyaPradullaChandraCollege,KOLKATA,I...
13PUBLICATIONS2CITATIONS
SEEPROFILE

79

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyParthasarathiDasguptaon28September2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

INDIAN INSTITUTE OF MANAGEMENT CALCUTTA

WORKING PAPER SERIES

WPS No. 713/ September 2012

Algorithms for Rotation Symmetric Boolean Functions

by

Subrata Das Assistant Professor, Department of Information Technology, Academy of Technology, West Bengal

Satrajit Ghosh Assistant Professor, Department of Computer Science, APC College, West Bengal

Parthasarathi Dasgupta Professor, IIM Calcutta, Diamond Harbour Road, Joka, Kolkata 700104, India

&

Samar Sensarma Professor, Department of Computer Science & Engineering University of Calcutta

Algorithms for Rotation Symmetric Boolean Functions
Subrata Das1 , Satrajit Ghosh2 , Parthasarathi Dasgupta3 , and Samar Sensarma4
Department of Information Technology Academy of Technology dsubrata.mt@gmail.com 2 Department of Computer Science APC College gsatrajit@gmail.com 3 Management Information Systems Group Indian Institute of Management Calcutta partha@iimcal.ac.in Department of Computer Science & Engineering University of Calcutta sssarma2010@gmail.com
1

4

Abstract. Rotation Symmetric Boolean functions (RSBF) are of immense importance as building blocks of cryptosystems. This class of Boolean functions are invariant under circular translation of indices. It is known that, for n-variable RSBF functions, the associated set of input bit strings can be divided into a number of subsets (called partitions or orbits), where every element of a subset can be obtained by simply rotating the string of bits of some other element of the same subset. In this paper we propose algorithms for the generation of these partitions of RSBF s and implement for up to 26 variables. Keywords: Algorithms,cryptography, Symmetric boolean functions,Rotation symmetric boolean functions

1

Introduction

n A Boolean function f n (xn-1 , xn-2 , ..., x0 ) of n variables is a mapping from F2 n to F2 ,where F2 is a n-dimensional vector space over the two-element field F2 . A Boolean function is symmetric if and only if it is invariant under any permutation of its variables [1]. Rotation Symmetric Boolean Functions(RSBFs) are a special class of Boolean function for which the function value will be same for any rotation of its variables [12]. For secret key cryptosystems, balancedness,nonlinearity,correlation immunity,algebraic degree [11] are the different criteria for choosing a Boolean Function for cryptographic applications. RSBFs have good combination of these properties [6],[4]. It is clear to see that there are n 22 boolean functions on n variables, and thus, searching for all these functions

2

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

exhaustively is exponential.Thus, in order to look for RSBFs, it is imperative to have an idea about the number of orbits(i.e. partitions) in rotation symmetric functions. In this paper, we propose three simple algorithms for generating RSBF s of a given number of variables and implement upto 26 variables. Rest of the paper is organized as follows.Section 2 reviews some related recent works. Section 3 introduces some terminologies to be used in subsequent discussions and Section 4 proposes three algorithms A,B and C for generating RSBF s of n variables. Section 5 briefly discusses the implementation of the algorithms A,B and C . Finally, Section 6 concludes the chapter and briefly states the future scopes of work.

2

Literature Review

An extensive study of symmetric Boolean functions, especially of their cryptographic properties has been done in [8]. In [2] Pieprzyk and Qu have studied a special type of symmetric Boolean functions, called rotation symmetric Boolean function.In that paper the authors have suggested that RSBF can be applied as round functions of a hashing algorithm such as MD5. Rotation symmetric Boolean functions (RSBF) are of great research interest for theoreticians as well as for practitioners in the field of cryptography [3]. Any symmetric boolean function (w.r.t. any permutation) is also rotation symmetric, but the converse is not always true [2]. In [4] the authors discuss some new results on rotation symmetric correlation immune(CI) and bent functions and important data structures for efficient search strategy of these functions. They also proved the non existence of the homogeneous rotation symmetric bent functions of degree  3 on a single cycle. In [5] the authors have proposed an efficient implementation of search strategy for rotation symmetric boolean function. In [6] theoretical construction of rotation symmetric boolean functions on odd number of variables with maximum possible algebraic immunity are discussed.The investigation of balanced RSBFs and 1st order correlation immune RSBFs and enumeration formula for n-variable balanced RSBFs where n is a power of prime reported in [7].

3

Preliminaries

A of n variables Boolean function f (xn-1 , xn-2 , ..., x0 ) is said to be Rotation Symmetric if f (xn-1 , xn-2 ,. . .,x0 )=f (xn-2 , xn-2 ,. . .,x0 ,xn-1 )=....=f (x0 , x1 ,. . .,xn-1 ). Thus, a Rotation Symmetric Boolean function remains invariant under cyclic permutation of its variables. The cyclic permutation may be in either left-to-right or in right-to-left order of the variables. Now, for a function f (xn-1 , xn-2 ,. . .,x0 ) of n variables, the number of possible strings of elements is 2n . From the above definition of RSBF, the value of f (xn-1 , xn-2 ,. . .,x0 ) will be the same for a set of input strings of elements,where any input string is obtained by cyclic permutation of some other input string. A set of strings of elements 0 and 1 of n variables which are rotation symmetric is

Algorithms for Rotation Symmetric Boolean Functions

3

said to form an orbit. Figure 1 illustrates the set of 14 orbits for a function of 6 variables.
{(000000)} {(000001) (000010) {(000011) (000110) {(000101) (001010) {(000111) (001110) {(001001) (010010) {(001011) (010110) {(001101) (011010) {(001111) (011110) {(010101) (101010)} {(010111) (101110) {(011011) (110110) {(011111) (111110) {(111111)} orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit 0 1 2 3 4 5 6 7 8 9 10 11 12 13

(000100) (001100) (010100) (011100) (100100)} (101100) (110100) (111100)

(001000) (011000) (101000) (111000) (011001) (101001) (111001)

(010000) (110000) (010001) (110001) (110010) (010011) (110011) (110101) (110111)

(100000)} (100001)} (100010)} (100011)} (100101)} (100110)} (100111)} (101011)} (101111)}

(011101) (111010) (101101)} (111101) (111011)

Fig. 1. Orbits of RSBF for n = 6

Each orbit is also known as a partition. Number of partitions gn satisfies the inequality 2n - 2 gn  2 + (1) n where the equality holds when n is prime[3]. We now introduce a few definitions for the subsequent discussions. Definition 1. Hamming distance d(s1 , s2 ) between two binary strings s1 and s2 is the number of positions where the bit values of s1 and s2 differ. Definition 2. If a boolean function f (xn-1 , xn-2 ,. . .,x0 ) exhibits rotation symmetry, then the period over which it exhibits this property is defined to be the cycle length for the function. Consider a rotation symmetric boolean function whose associated binary strings are {(1011), (0111), (1110), (1101)} obtained through all possible cyclic permutation of the bits containing 3 ones and 1 zero.The RSBF f (x3 , x2 , x1 , x0 ) must have the same value for all the permutations. Cycle length of this function is then 4. Definition 3. For a n-variable RSBF containing gn orbits,the orbits which have maximum cycle length i.e. cycle length n are known as long cycles. If the cycle length is some factor of n then it is known as a short cycle. For instance, in Figure 1, the RSBFs corresponding to orbits 1, 2, 3, 4, 6, 7, 8, 10 and 12 are all long cycles, whereas those corresponding to orbits 0, 5, 9, 11, and 13 are all short cycles. The following observation is then clear:

4

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

Observation 1 For a RSBF of n variables, n > 1, the two trivial orbits containing all zeros and all ones respectively are always short cycles. Lemma 1 If n is prime, then n and an integer.
n r

can be expressed as the product of the number

Proof. Let us assume the result is wrong.So if n is prime, then n r can be exn-1)! n n! pressed as the product of the number n and a fraction. r = (n-r)!×r! =n × (n(- r )!×r ! . n n It is quite trivial that r yields an integer. So to get the value of r as integer (n-1)! n-1)! either the denominator of (n(- r )!×r ! is n or the value of (n-r )!×r ! is an integer.Since n is a prime number so it can not be expressed as product of two or more numbers other than 1 and the number n itself.So the denominator of (n-1)! (n-1)! (n-r )!×r ! i.e. (n - r)! × r! will never be n.Hence it is a contradiction. So (n-r )!×r ! is not a fraction. Lemma 2 For a RSBF of n variables, if n is prime, then there does not exist any orbit of cycle length less than n except for the two trivial orbits containing all zeros and all ones. Proof. Let r denotes the number of 1s in the binary string of n bits. Then, total number of binary strings that can be formed with n bits having r ones is n r . Since n is prime, n can be expressed as the product of the number n and an r integer (by Lemma 1). Moreover, the cycle length of the RSBF cannot be greater than n. Thus, every cycle that can be generated for the RSBF are of full cycle length except for the two trivial orbits containing all zeroes and all ones. Lemma 1 and Lemma 2 clearly indicates that for a composite value of n, the different strings generated for the RSBF will have some short cycles. A closer look at the Boolean strings corresponding to RSBF yields the following observation: Observation 2 If the cycle length of an orbit for a n-variable RSBF is less than n, then there always exists a substring within the corresponding binary strings which is also repeated within every string of the orbit. The length of this repeating substring is defined to be the internal period of the orbit. In Figure 1, for example, for orbit 5, cycle length is 3, and the substring {001} is repeated three times in every element of the orbit, and the internal period is also 3. A formal definition of internal period is given below: Definition 4. For a n-variable binary string corresponding to a RSBF,where n is not prime if d be a divisor of n, then a substring of d bits of the n- variable string is said to exhibit periodicity d if (an-1 , . . ., an-d ) = (an-d-1 , . . ., an-2d ) = . . . = (ad-1 , . . ., a0 ). d is defined to be the internal period of these substrings. For example in Figure 1 orbit 5 is {(001001)(010010)(100100)},where substring (001) has periodicity 3. The following observation follows from the Definition 4.

Algorithms for Rotation Symmetric Boolean Functions

5

Observation 3 Product of the internal period and the number of substrings within a string yields the number of variables of the string. For instance, in Figure 1, for Orbit 9, number of substrings is 3,internal period is 2 and the number of variables is 6.

3.1

Algebraic Normal forms and RSBF

The classical approach to the analysis, synthesis or testing of a switching circuit is based on the description by the Boolean algebra operators. A description of a switching circuit based on Modulo-2 arithmetic (the simplest case of the Galois field algebra [10]) is inherently redundancy-free, and is implemented as the multi-level tree of XOR (addition operator over GF (2)) gates. Definition 5. An n-variable Boolean function f (xn-1 , . . ., x1 , x0 ) can be expressed as a multivariate polynomial over GF (2). More precisely, f (xn-1 , . . ., n 1i<j n i x1 , x0 ) can be written as : f (xn-1 , . . ., x1 , x0 ) = a0 =1 ai xi aij xi xj . . . a1,2,...,n , where the coefficients a0 , ai , aij , . . ., a1,2,...,n  {0, 1} and xi  {0, 1}. This representation of f (xn-1 , . . ., x1 , x0 ) is known as its Algebraic normal form (ANF) or its Positive Polarity Reed-Muller form (PPRM). The number of variables in the highest order product term with non-zero coefficient of an AN F is its algebraic degree. A Boolean function is defined to be homogeneous if all terms of its AN F are of the same degree. Definition 6. A homogeneous function of degree one is called a linear function. Definition 7. A Boolean function of degree at most one is called an affine function. Definition 8. The non-linearity N Lf of a Boolean function f (xn-1 , xn-2 , .., x0 ) is the minimum number of truth table entries that must be changed in order to convert f (xn-1 , xn-2 , .., x0 ) to an affine function. Non-linearity of a Boolean function f (xn-1 , xn-2 , .., x0 ) may be measured as the minimum Hamming distance between truth tables of f (xn-1 , xn-2 , .., x0 ) and its corresponding (same degree) affine function [9]. For a Boolean function f (xn-1 , xn-2 , .., x0 ) of n variables, there are 2n different input values corresponding to the function. From the definition of RSBF, the function has the same value corresponding to each of the subsets generated from the rotational symmetry. Each of these subsets is called a partition. In the following Section, we propose three algorithms for the generation of partitions for rotational symmetry for a given number n variables. For each partition (orbit), we select the first element of the partition as its representative element.

6

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

4

Proposed Algorithms

The proposed algorithm starts with a string of n zeros, which forms the first orbit. Subsequent representative strings are formed by the odd numbers whose binary representation has r initial zeros starting with most significant bit, followed by (r + 1)th bit as 1 and least significant bit as 1. Left rotation of these starting (representative) strings by up to r bits yields an even number. Further left rotation may yield again an odd number. Lemma 3 If a n bit odd number (d) consists of 1st r bits as zeros ,starting with th most significant bit,the (r + 1) bit as one, and the least significant bit as one, then d × 2r+1 > 2n , where n > r  1. Proof. From mathematical inequality, we have 2n-r-1 < (2n-r -1). Now, (2n-r - 1) is the maximum odd number (dmax ) of (n - r) bits. Prefixing r zeros to the left of binary form of dmax also yields dmax . Thus, 2n-r-1 < dmax , i.e., dmax ×2r+1 > 2n . Moreover, the minimum odd number of (n - r) bits is (2n-r-1 + 1). Let dmin denotes this number. It can be easily shown that dmin × 2r+1 > 2n . Thus, all odd numbers between dmin and dmax satisfy the same inequality constraint. Consider an odd number whose binary representation has r initial zeros starting with most significant bit, followed by (r + 1)th bit as 1 and least significant bit as 1. Left rotation of these starting (representative) strings by up to r bits yields an even number. Thus, from Lemma 3, we find that for the representative string s of an orbit, beyond left rotation of a number by r bits starting with s, the number obtained is not necessarily an odd number. Value of this bit string is obtained from Lemma 3. An AV L tree is used to store the Decimal values of all odd numbers from this position up to n - 1 rotations for avoiding possible repetition in future orbit generations. A formal description of the proposed Algorithm A is given in Figure 2. The following result clearly follows from the description of the proposed Algorithm A. Lemma 4 Worst-case time complexity of Algorithm A is 2n . Lemma 5 The proposed Algorithm A requires a maximum space of 2n-1 - gn . Proof. For an n-variable string, the corresponding decimal numbers range ben tween 0 and 2n - 1. Of these, the number of odd decimal values is 22 . We store only odd numbers in the AV L tree. Thus, the total storage requirement n -2 is 2n-1 - gn . Total number of orbits for an n-variable RSBF is gn  2 + 2 n . Since nodes are gradually deleted from the AV L tree, we have space requirement  2n-1 - gn . The sequence of outputs generated by the proposed Algorithm A is illustrated with an example below. Consider n = 5. Thus, the maximum value of the corresponding decimal number is 31. Orbit 1: All zeros.

Algorithms for Rotation Symmetric Boolean Functions

7

Algorithm A Data structures: Cntr :# of orbits, Result[]: starting string of orbit, T : AVL tree Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2. Take the string of all zeros as Orbit 0; store its decimal form in Result[]; 3. Cntr = Cntr + 1; 4. Take the string of first n - 1 zero bits and last bit One as Orbit 1; store its decimal form in Result[]; 5. while last < 2n do 6. Take the binary string s corresponding to next odd number 7. if s  T then 8. store decimal form d of s in Result[]; 9. Cntr = Cntr + 1; 10. while s does not reappear do 11. bit-wise rotate the binary representation of s to generate a set of strings S = {sk |k = 1, n - 1} 12. if d × 2k > 2n then store decimal form of sk in T if sk is odd, where k = r + 1 (by Lemma 3) 13. endwhile 14. endwhile 15. end Fig. 2. Algorithm A for generating orbits for n-variable RSBF

Orbit 2: Starting string must have 4 zeros, and a one, which is decimal 1 {i.e.04 1}. Now, check for some k for which 1 × 2k > 31, k = 1, . . . 4. No such value of k exists. Orbit 3: Starting bit string of this orbit must be the next odd number. Since any rotation of 1 in the starting string of Orbit 2 yields an even number, the starting string of this orbit must have three consecutive zeros, followed by two ones (decimal 3). The condition 3 × 2k > 31, k = 1, . . . , 4 is satisfied for k = 4. Rotate the starting string four times to obtain 10001, having decimal value 17. Since decimal value of this string is odd, it is saved in the AV L tree ( Figure 3(a)). Orbit 4: Starting bit string of this orbit must be the next odd number (after starting string of previous orbit), having decimal value 5, i.e., two zeros followed by 101. For k = 3, 5 × 2k > 31. Rotate 00011 three and four times respectively to yield 01001 (=9) and 10010 (=18). Since the number 9 is odd, it is saved in the AV L tree (Figure 3(b)). Orbit 5: Starting bit string of this orbit must be the next odd number 7 (after 5), i.e., two zeros followed by 111. For k = 3, 7 × 2k > 31. Rotate 00111 three and four times respectively to yield 11001 (=25) and 10011 (=19). Since both these numbers are odd, they are saved in the AV L tree (see (Figure 3 (c)). Orbit 6: Starting bit string of this orbit must be the next odd number after 7 which is not already in the AV L tree. Since the next odd number 9 is already in the AV L tree, the starting string of this Orbit would be 01011 (=11). The

8

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

9 17

17

25 19

Delete 9 17 19

(a)

9 (b)

9

25

17

25

(d) (c) 19

21,13 Delete 13 19 19 (29) 17 25 17 13 21 (f ) (e) ( g) 21 21 29 25 17 25 19 29,27,23

19 25 (23) 17 25 19 29 19 29 25

21 (27) 27

29

17

21

27

17

21

27

(g)

(i)

23

Fig. 3. An example execution of Algorithm A

Algorithms for Rotation Symmetric Boolean Functions

9

number 9 is deleted from the AV L tree ( Figure 3(d)) For k = 2, 11 × 2k > 31. Rotate 01101 (=13) two to four times to yield respectively 01101 (=13), 11010 (=26), and 10101 (=21). The odd values 13 and 21 are stored in the AV L tree (Figure 3(e)). Orbit 7: Starting bit string of this orbit must be the next odd number after 11 which is not already in the AV L tree. Since the next odd number 13 is already in the AV L tree, the starting string of this Orbit would be 01111 (=15). The number 13 is deleted from the AV L tree ( Figure 3(f)) For k = 2, 15 × 2k > 31. Rotate 01111 (=15) two to four times to yield respectively 11101 (=29), 11011 (=27), and 10111 (=23). The odd values 29, 27 and 23 are stored in the AV L tree ( Figure 3(g)(h)(i)). Orbit 8: Starting bit string of this orbit must be the next odd number after 15 which is not already in the AV L tree. Since the next few odd numbers 17, 19, 21, 23, 25, 27, and 29 are already in the AV L tree, these numbers are successively deleted from the AV L tree (see Figure 3g). Thus, the string for this Orbit 8 is 31. The algorithm terminates successfully at this point. Figure 3 illustrates some of the steps using the AV L tree during execution of Algorithm A with an example. Let the symbols RR and LR respectively denote right and left rotations of a bit string by one bit position. For a string of bits s, we define its rotation cousin to be the bit string obtained on application of LR or RR on s. The following result is used to design an improved algorithm. Lemma 6 Right rotation of each of the bit strings corresponding to two consecutive odd integers by one bit yields two consecutive integers. Proof. Let two successive n-bit binary odd numbers be represented as {an-1 , an-2 , . . . , a1 , a0 }2 and {bn-1 , bn-2 , . . . , b1 , b0 }2 and {an-1 , an-2 , . . . , a1 , a0 } + 2 = {bn-1 , bn-2 , . . . , b1 , b0 }. Now RR {bn-1 , bn-2 , . . . , b1 , b0 } by 1 bit yields {b0 , bn-1 , bn-2 , . . . , b1 }. =b0 2n-1 + (bn-1 2n-2 + bn-2 2n-3 + ... + b1 20 ) n -1 n -2 ...+b1 21 ) 22 =b0 2n-1 + (bn-1 2 +bn- 2
n -1

+bn-2 2n-2 ...+b1 21 )+b0 -b0 2 n -1 +bn-2 2n-2 ...+b1 21 +b0 )-b0 n-1 (bn-1 2 =b0 2 + 2 n -1 n -2 1 =b0 2n-1 + (an-1 2 +an-2 2 2 ...+a1 2 +a0 )+2-b0 n -1 n -2 1 =a0 2n-1 + (an-1 2 +an-2 2 2 ...+a1 2 +a0 )+2-b0 [a0 n -2 n -3 ...+a1 20 )×2+2+a0 -b0 =a0 2n-1 + (an-1 2 +an-2 2 2 =a0 2n-1 +(an-1 2n-2 + an-2 2n-3 ... + a1 20 ) + 1

=b0 2n-1 + (bn-1 2

= b0 , since both are odd numbers]

={a0 an-1 , an-2 , . . . , a1 }2 + 1 . Let {an-1 , an-2 , . . . , a1 , a0 } represent a n-bit string, where ai represents a bit, i = 0, . . . , n - 1. Then Lemma 6 may be expressed as {{an-1 , an-2 , . . . , a1 , a0 } RR 1} = {{an-1 , an-2 , . . . , a1 , a0 } + 2 } RR 1.

10

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

4.1

An improved Algorithm

The proposed Algorithm A is improved with a minor modification. We note the following observation: Consider the bit string (for an odd number) having 1 at the right-most position{0n-1 1}. Let this bit string be right-rotated right by 1-bit (i.e., n-bits left-rotate) to form a new bit string P , say P = {11 0n-1 }. The starting bit-string of each orbit is an odd number, generated by simply adding 2 to the starting string of the previous orbit. In the previous algorithm, we had to check all these starting strings in an AV L tree to avoid repetitive occurrences of numbers. Lemma 7 If the starting string of an orbit is P + 1, then the numbers between P + 1 and the number generated by one-bit right-rotation of (P - 1) may be ignored for generating subsequent orbits. Proof. From Algorithm A it is clear that the starting string of the orbits must be odd number.From the previous lemma 6 it is clear that the last rotation cousins are the consecutive numbers if starting strings of the orbits are consecutive odd numbers.When some number misses in the orbit then its corresponding last rotation cousin do no appear.So as soon as the starting string of the orbit becomes P +1 which comes already as the last cousin of some orbit we do not cosier from this to the last cousin of P - 1. The above lemma shows that the n-bit left-rotation (= 1-bit right rotation) of successive odd numbers results in successive numbers. Thus, whenever the odd number becomes (P + 1), all the successive numbers up to the number which is RR of (P - 1) already appears. A formal description of the proposed Algorithm B is given in Figure 4. Following result is clear from the description Algorithm B . Lemma 8 Worst-case time complexity of Algorithm B is 2n . Lemma 9 The proposed Algorithm B requires a maximum space of 2n-1 -gn -a, where a = RR(2n-1 + 1) - (2n-1 + 1). Proof. Follows from Lemma 6. 4.2 A further improved Algorithm

In both the algorithms proposed above, an AV L tree is used to reduce certain iterations. The following observation helps in getting rid of this auxiliary data structure and its associated operations. Observation 4 If the rotation cousin of an odd starting number of an orbit is also odd, and is greater than the value of the next starting string of the next orbit, then this starting string may be discarded. A formal description of Algorithm C is given in Figure 5. Following result is clear from the description Algorithm C .

Algorithms for Rotation Symmetric Boolean Functions

11

Algorithm B Data structures: Cntr :# of orbits, Result[]: starting string of orbit, T : AV L tree Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2.Store the decimal value of 1 0n-1 to some variable P 3. Do not store the value from P to RR of P - 1 in T 4. Take the string of all zeros as Orbit 0; 5. store its decimal form in Result[]; 6. Cntr = Cntr + 1; 7. Take the string of first n - 1 zero bits and last bit One as Orbit 1; 8. store its decimal form in Result[]; 9. while last < 2n do 10. Take the binary string s corresponding to next odd number 11. if s  T then 12. store decimal form d of s in Result[]; 13. Cntr = Cntr + 1; 14. while s does not reappear do 15. bit-wise rotate the binary representation of s to generate a set of strings S = {sk |k = 1, n - 1} 16. if d × 2k > 2n then store decimal form of sk in T if it is odd, where k = r + 1 (by Lemma 3) 17. if d  P then d=RR of P - 1 18. endwhile 19. endwhile 20. end Fig. 4. Improved algorithm B for generating orbits for n-variable RSBF

Lemma 10 Worst-case time complexity of Algorithm C is 2n . The space requirement for Algorithm C is much less than those of the previous algorithms, as it does not use the AV L tree. The proposed Algorithm C is illustrated with an example below. Consider n = 7. Thus, the maximum value of the corresponding decimal number is 31. Orbit 1: All zeros. Orbit 2: Starting string will be {06 1} as none of the rotational cousins of 6 {0 1} is of smaller value. Orbit 3: Starting bit string of this orbit must be the next odd number, i.e., the bit-string {05 12 }, as none of the rotational cousins of {06 1} is smaller than {05 12 }. Orbit 4: Starting bit string of this orbit must be the next odd number (= 5), i.e., {04 101} as none of the rotational cousins of {05 12 } is smaller than {04 101}. Continuing in this manner, we find that up to Orbit 9 the consecutive odd numbers form the starting strings of the respective orbits. Thus, the starting string of Orbit 9 is {03 14 }. The next odd number is 17, i.e., {02 103 1}. We observe that 3rd left-rotate cousin of {02 103 1} is {03 102 1} (=9), which is less than 17. Thus, we discard the number 17, and do not consider it as the starting number of any orbit. For the next consecutive odd numbers between 19 nd 23, none of them

12

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

Algorithm C Data structures: Cntr :# of orbits, Result[]: starting string of orbit Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2. Take the string of all zeros as Orbit 0; store its decimal form in Result[]; 3. Cntr = Cntr + 1; 4. Consider the bit-string s corresponding to the next odd number; store its decimal form in Result[]; 5. while bit-string corresponding to s = {1n } do 6. if a number p has any of its rotational cousins pc < p then goto Step 8 7. take bit-string corresponding to s as the starting string of the next orbit 8. consider the bit string corresponding to the next odd number 9. endwhile 10. end Fig. 5. Improved algorithm (without AV L tree) for generating orbits for n-variable RSBF

have any smaller left-rotate cousin, and hence, they form the starting numbers for the next three successive orbits. The algorithm continues in this manner, and terminates successfully at Orbit 20.

5

Implementation of the Proposed Algorithms

The proposed algorithms have the same worst-case time complexities, and exhibit drastic improvement in space requirements from Algorithm A through Algorithm C . Algorithms A and B use the AV L tree, while the other two do not require this data structure. Algorithm A was implemented in C language on a 32-bit PC running on Intel CPU under Windows environment having 2.27 GHz clock speed. It could generate RSBF s up to 26 bits in reasonable time.

6

Conclusion

In this work, we investigated rotation symmetric Boolean functions. Our major contributions include designing of certain efficient algorithms for generation of RSBF s. Future possibilities of work include (i) possible reducing of the time complexities, (ii) generating RSBF s with more than 30 variables, and (iii) generating cryptographically better RSBF s. The proposed algorithms are of great importance as they can help to construct new cryptographically very strong boolean functions.

References
1. Zvi Kohavi, Switching and Finite Automata Theory, 2nd Edition, Tata-McGraw Hill, 2008.

Algorithms for Rotation Symmetric Boolean Functions

13

2. J. Pieprzyk and C. X. Qu, Fast hashing and Rotatio-symmetric functions, Journal of Universal Computer Science, pp. 20-31, vol. 5, no. 1, 1999. 3. P. Stanica and S. Maitra, Rotation Symmetric Boolean functions - Count and Cryptographic properties, Discrete Applied Mathematics, vol. 156, no. 10, May, 2008. 4. P. Stanica and S. Maitra and J A Clark, Results on Rotation symmetric Bent and Correlation immune Boolean functions in B. Roy and W. Meier (Eds.), FSE 2004, LNCS 3017, pp. 161-177, 2004, International Assoc. for Cryptologic Research. 5. M.Hell, A. Maximov,and S. Maitra, On efficient implementation of search stratgey for RSBF, 9th International workshop on Algebraic and Combinatorila coding theory, ACCT 2004, June 19-25, 2004, Bulgaria. 6. P.Sarkar and S. Maitra, Construction of rotation symmetric boolean function with important cryptographic properties,Advances in cryptology,EUROCRYPT2000,Lecture notes in Computer Science,Vol-1807,Berlin 2000,PP 485-506. 7. ShaoJing Fu, Chao Li and LongJiang Qu, On the number of rotation symmetric Boolean functions, Science China Information Sciences March 2010, vol. 53, no 3, pp. 537?545. 8. A Canteaut and M Videau, Symmetric Boolean functions, IEEE Transactions on Information Theory, vol. 51, no. 8, pp. 2791-2811, Aug 2005. 9. J Butler and T Sasao, Logic functions for cryptography - A tutorial, Proceedings of Reed-Muller Workshop, Okinawa, Japan, 2010, pp. 127-136. 10. N. Deo, Graph Theory with Applications to Engineering and Computer Science, PHI Learning Pvt. Ltd., 2004. 11. A menezes, p Van Oorschot,S Vanstone, Handbook of applied cryptography,1997,CRC Press,Inc. 12. X Zhang, Hua Guo,Yifa Li,Proof of a Cojecture about Rotation Symmetric Functions.

View publication stats

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

6483

Filter Design of Direct Matrix Converter for Synchronous Applications
Anindya Dasgupta and Parthasarathi Sensarma, Member, IEEE
Abstract--Filters for switching ripple attenuation are essential at the input, and sometimes at the output for certain applications, for the deployment of matrix converters (MCs). Due to the absence of inertial elements in the MC structure and the consequent tight input­output coupling, the filter parameters significantly affect its dynamic behavior. This paper presents an exhaustive filter design method for synchronous applications of the MC in power systems. Apart from the usual considerations of ripple attenuation, voltage regulation, reactive current loading, and internal losses, this paper also addresses additional constraints which may be imposed by requirements of dynamic performance and reliable commutation. Rigorous analytical justification of each design step is provided and the sequential design process is summarized. Relevant experimental results are presented to validate the proposed design tool. Index Terms--Filter design, matrix converter (MC), synchronous applications.

I. I NTRODUCTION ECENT YEARS have witnessed a growing interest [1]­ [7] toward the deployment of the matrix converter (MC) in power systems, which are synchronous applications characterized by identical input and output frequencies. The advantage of reduced energy storage needs has inspired the use of MC as a FACTS device [1]. The use of MC for power quality improvement, reactive power control, and other power system applications has been reported in [2]­[4]. Benefits of using MC as a voltage regulator in a distribution system and as a high-performance power supply have been reported in [5]­[7], respectively. The input filter is an essential requirement of this topology for providing a local circulating path to the switching frequency current. Some of the synchronous applications require regulated output voltage and, hence, a second-order ripple filter at the output side, in addition to the input filter. The filter parameters have to be carefully selected such that their inclusion does not degrade voltage regulation, efficiency, and reactive current loading beyond an acceptable limit. Since the

R

Manuscript received April 19, 2013; revised August 19, 2013 and December 2, 2013; accepted February 14, 2014. Date of publication April 14, 2014; date of current version September 12, 2014. This work was supported in part by the NaMPET initiative of DeitY under Grant CDAC/EE/20050026 and in part by the Department of Science and Technology, Government of India, under sponsored project DST/EE/20110021. A. Dasgupta is with the Department of Avionics, Indian Institute of Space Science and Technology, Thiruvananthapuram 695-547, India (e-mail: anindyadgupta@iist.ac.in; anindyadgupta@gmail.com). P. Sensarma is with the Department of Electrical Engineering, Indian Institute of Technology Kanpur, Kanpur 208-016, India (e-mail: sensarma@ iitk.ac.in). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TIE.2014.2317134

only inertial elements in this topology are the filter components, the filter parameters significantly affect the system dynamics. Input filter design has been discussed in [8]­[13], where, from cost and weight considerations, the single-stage LC filter has been found to be the most appropriate topology. Although the set of filter elements (Lf , Cf ) for a particular resonant frequency is infinite, Klumpner et al. [9] advocate a maximum Cf to ensure a minimum input displacement factor (IDF) for low loads. However, in a distribution system, most of the loads are inductive, and this restriction may be relaxed at lighter loads. An exhaustive treatment of input/output filters with focus on reducing electromagnetic interference and common-mode voltages has been provided in [14] and [15]. However, most of these have not investigated the comprehensive design of filters in the context of dynamic performance improvement or reliability of commutation hardware. The only reported approach [13], which considers both steady-state and dynamic objectives, uses a genetic algorithm to derive the parameter values as well as the filter topology. Dynamic stability criteria are based on limits detailed in [16] and [17]. Digital filters are used for measuring input voltage which are purported to improve the system stability limit. It has, however, been shown in [18] that, with a proper choice of system input and output, the derived plant has minimum phase poles for all operating points, but nonminimum phase zeros could appear, depending on input filter parameters and system operating points. Since the plant is inherently stable, imposing a general stability limit on filter design for all operating conditions [13] is unnecessary. Also, analytical justification of the filter design method is not established with any degree of rigor. Output voltage regulation specifications restrict the inductive (series) component of the output filter. Hence, commutation based on output current direction becomes particularly difficult at zero crossing (ZC) due to the high ripple content. The detection of current direction based on switch voltage measurement introduces large delays because of the stray capacitances and large resistances used [19]­[21]. An analysis of the critical window area around the ZC of line voltages, for safe voltagebased commutation (VBC), has been provided in [22]. However, the effect of switching frequency ripple in input voltage on the demarcation of the critical areas has been ignored. In [21], it is shown that, with proper zero vector placement in space vector modulation (SVM), safe commutation can be achieved in spite of voltage measurement inaccuracies. This method is restricted to the operation with the input displacement angle within ± of the voltage ZC, which is to be decided on the basis of the specific input ripple voltage measurements for a given hardware. A closed-form expression of the ripple voltage

0278-0046 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

6484

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE I N OMINAL /R ATED PARAMETERS

Fig. 2.

Single-phase diagram including source inductance.

Fig. 1. Schematic of a 3 Ph MC.

Fig. 3. |iin (nb t)|/|iin (b t)| for (a) m = 1 and (b) m = 0.5.

would therefore provide a complete analytical design tool for this method. For wider control of IDF in applications [23], [24] requiring input reactive power control, the correct measurement of the ripple voltage is still difficult, particularly with high output current amplitude. This paper discusses a filter design approach for the MC which integrates the steady-state performance along with the constraints imposed by the requirements of closed-loop dynamic performance and safe commutation. In the first section, the design of filters to meet certain steady-state performance specifications is detailed. Since modulation strategies for MC do not provide any separate control on the input current amplitude, an external damping resistor is included. Power loss estimation due to this inclusion is presented. Input capacitor sizing for reliable commutation is analyzed in the context of the voltage ripple and consequent problems in VBC. Then, the effect of the grid inductance has been presented. Thereafter, the output filter design is presented in the second section. In the next section, the design guidelines that emerge from the analysis have been summarized to enable a sequential design process. Finally, relevant experimental results on a 6-kVA laboratory prototype are provided for validation. II. I NPUT F ILTER D ESIGN Filter parameters are chosen for the nominal/rated system parameters shown in Table I. In this analysis, an application requiring regulated 3 Ph sine-wave voltage supply has been considered which makes output ripple filters mandatory. Fig. 1 shows the schematic diagram of a 3 Ph MC. The single-phase equivalent of the overall system is shown in Fig. 2 where the Thevenin impedance of the source is modeled as an inductance Ls . The input filter damping resistor is placed across the filter inductor as the conflict between filter efficiency and damping requirements is least with this arrangement [25]. The power stage of MC appears as a current stiff sink to the supply side and as a voltage stiff supply to the load. A phase-locked loop is locked to phase a of the point of common coupling marked as P. Ls , which is usually small at the

distribution level, is initially neglected and would be introduced later to study its impact on filter performance. The selection of filter parameters to satisfy certain criteria are sequentially described in the following sections. A. Attenuation to Switching Ripple and Low-Order Harmonics For Ls = 0, the forward gain of the input filter is defined as Gf v (s) =


is ( s ) iin (s)

=
vs

vcf (s) v s (s )

=
iin

+1 s Rf d s2 Lf Cf + s Rf +1 d
L

L

.

(1)

Denoting the corner frequency as c : (2fc ), the normalized form of its frequency response magnitude is |Gf v (j )| = 1+
2 r Q2 2 )2 + (1 - r 2 r Q2

(2)

which is plotted in Fig. 4. The definitions r =  , c Q = Rd Cf , Lf c = 1 Lf C f (3)

enable generalized analysis. The ratio of the magnitude of the switching ripple components to the fundamental component in iin varies with modulation index m [26]. Fig. 3 shows the magnitude of these component as a fraction of the fundamental obtained from simulation. To restrict these switching ripple components from appearing in is , the first design criterion is set as Spec.1 |Gf v (j 2fs )|  Asw dB. Also, in the deployment site of the MC, the grid voltage spectrum may contain significant low-order (hv ) harmonics. To limit the appearance of these in the filter capacitor voltage vcf , the following restriction is introduced: Spec.2 |Gf v (j 2fb max .(hv ))|  Avh dB. Defining the following frequency ratios: mf hb = max .(hv ) and mf sh = s mf hb b (4)

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6485

TABLE III Lf,max AND Cf,max F ROM R EGULATION AND R EACTIVE L OADING

Fig. 4. |Gf v (j )|(dB) with Q = 1, 3, and 5. TABLE II Q AND fc F ROM F ORWARD G AIN Fig. 5. (a) |GR1 (j )|(dB) for Rd = 1 . (b) |GR2 (j )|(dB).

C. Selection of Damping Resistor Rd Since an external damping resistor Rd is indispensable in the input filter, minimum operating efficiency requires a limit on the maximum loss associated with Rd . Denoting the current through Rd as iRd , the corresponding design criterion is 2 Spec.5 3Rd  n=1 [iRd (jnb )] < 1% of rated load. iRd can be expressed as iRd (s) = GR1 (s)vs (s) + GR2 (s)iin (s) where GR1 (s) =


it is noted that, for given switching frequency and deployment site conditions, these ratios have unique values. These are depicted in Fig. 4 by the vertical lines which are mutually stationary but could slide horizontally. Also included are the criteria defined in Spec.1 and Spec.2, which are depicted as regions demarcated by stationary horizontal lines. The interval [rv , rs ] defines the initial choice of Q (Q(1) ) and, thereby, the (1) (1) acceptable range of fc ([fc,min , fc,max ]). For the experimental model and deployment site of MC, these selections, along with the numerical limits of Spec.1 and Spec.2, are listed in Table II. B. Voltage Regulation and Reactive Current Loading Referring to Fig. 2, the full load regulation of MC output voltage is decided by the fundamental voltage drop in Lf . Restriction on regulation leads to the following design criterion: Spec.3 VLf  b Lf
2 + I2 Icf in,rated  kR Vs .

(6)

iR d ( s ) v s (s )

=
s=j

1 · Rd

2 r 2 )2 + (1 - r
2 r Q2

GR2 (s) =



iR d ( s ) iin (s)

=
s=j

1 · Q

r
2 )2 + (1 - r
2 r Q2

.

(7)

A restriction on reactive current loading is necessary for satisfying stipulations on part-load power factor, which leads to the next criterion Spec.4 Icf  b Cf Vs  kPF Iin,rated . For deriving the boundary values at rated power level, analysis is done considering unity IDF operation with unity modulation index (m) [27] and resistive load unless stated otherwise. The specific inequalities in Spec.3 and Spec.4 obviously translate to (1) (1) maximum limits for Lf (Lf,max ) and Cf (Cf,max ). The rated input current is evaluated from [27] Iin,max = 0.866Io . (5)

The frequency responses of these transfer functions are plotted in Fig. 5(a) and (b), respectively, for Rd = 1 . High Q is particularly desirable in |GR2 | as it implies lesser ripple content in iRd and, therefore, a lower loss. However, high Q in |GR1 | implies a higher loss due to increased low-order harmonics in iRd . The plots reveal a response similar to that encountered with the normalized form of |Gf v | in Fig. 4. As can be expected from this similarity, Q(1) chosen in Section II-A is found to give an even balance between both the conflicting aspects. Therefore, Q(1) is finalized for filter design. The power loss associated with Rd is PR = 3Rd
n 2 2 + |GR2 (jnb )|2 Iin,n |GR1 (jnb )|2 Vs,n

For the rated values in Table I, numerical limits considered for Spec.3 and Spec.4, along with the maximum allowable values (1) (1) Lf,max and Cf,max , are listed in Table III.

(8) where Vs,n and Iin,n are the values corresponding to the nth harmonic order. The calculation of PR using (8) requires the

6486

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE IV H ARMONICS IN iin F ROM S IMULATION AT m = 1

TABLE V Lf,min AND Cf,min F ROM R EGULATION , R EACTIVE L OADING , AND F IXED fc

Fig. 6.

Commutation between input phases a and b.

Fig. 7.

Input current and voltage.

prior selection of fc for evaluating the gains |GR1 (jnb )| and |GR2 (jnb )|. For the experimental model, fc is selected from the allowable range provided in Table II as
(1) = 1 kHz. fc

of intersection, the numerical limits of all or some of Spec.3, Spec.4, and Spec.5 must be made less stringent. For the experimental model, these ranges of Rd are listed in Table V. Since these ranges intersect, (13) defines the final set for selecting Rd . D. Lower Limit of Cf : Maximum Ripple in Input Line­Line Voltage and Consequent Problem in Commutation Fig. 6(a) shows two input phases a and b, which are alternatively switched to an output phase. Commutation based on the polarity of input line­line voltage (vL = vab ) is difficult at its ZC, due to the presence of switching ripple component vrL , as shown in Fig. 6(b). In the following analysis, it is assumed that only fundamental and switching frequency components are present in the input current and filter capacitor voltages. vL can be represented as vL = (vf a - vf b ) + (vra - vrb )
vf L vrL

(9)

1) Rd From Loss Limit: With the harmonic analysis of MC being outside the scope of this paper, the expression for losses is derived assuming sinusoidal io . The harmonic components in (1) iin obtained from simulation are listed in Table IV. Using fc (1) and Q , the gains defined in (7) are evaluated. Subsequently, using these gains and the harmonics listed in Table IV in (8), for the nominal parameters in Table I, PR is obtained in the following form: PR = F1 + F2 R d Rd (10)

(14)

where F1 and F2 are functions of |GR1 (jb )|, Vs , |GR2 (jnb )|, and Iin,n . These are tabulated in Table V. Thereafter, applying the numerical limits of Spec.5, (10) yields two roots which define an allowable range of Rd as follows: Rd,min  Rd  Rd,max .
(1) (1)

(11)

where the subscripts f and r have been used to denote the fundamental and ripple components, respectively. Fig. 7 shows the input current and filter capacitors. It is assumed that the ripple component irx completely circulates through the filter capacitor while if x flows through the source. Hence, vrL = vra - vrb = - 1 Cf (ira - irb ) dt. (15)

This choice of Rd definitely satisfies Spec.1, Spec.2, and Spec.5. However, it is not apparent whether any value of Rd in this band and the corresponding Lf and Cf evaluated using (3) would also satisfy Spec.3 and Spec.4. 2) Rd From Voltage Regulation and Reactive Loading Lim(1) (1) its: The upper bounds Cf,max and Lf,max imply corresponding lower bounds Lf,min and Cf,min for the selected fc in (9). These are listed in Table V. Using (3), since
(1) (1) Q Rd = 2fc Lf . (1) (1) (1) (1)

(12)

Lf,max and Lf,min define a new set of bounds on Rd Rd,min  Rd  Rd,max .
(2) (2)

(13)

Hence, the allowable values of Rd will be those where the two ranges obtained from (11) and (13) intersect. In the absence

irx is synthesized from the output current envelope, which is a direct function of the load. Therefore, an arbitrarily small Cf , chosen to reduce reactive loading at low loads, leads to a corresponding high vrL at high loads. With the usual delays involved, the phase error in the measurement of vrL is significantly more than that of vf L . The consequent inaccuracy in detecting the polarity of vrL around the ZC of vf L increases the chance of a commutation failure. Therefore, the estimation of maximum vrL and analyzing its effect on commutation form a prerequisite for determining the lower limit of Cf . Deriving a closed-form expression for maximum vrL is described in Appendix A. Incorrectly sensing the polarity of vf L may lead to erroneous commutation in the following manner. Fig. 8 shows the plot of ripple current and vrL around ^rL,max . This occurs during the instant when v ^rL reaches v commutation from phase a to phase b. The capacitor voltages

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6487

Fig. 10. Path of short circuit current.

Fig. 8.

Error in measuring vrL around its maximum value.

[22]. The instantaneous values of the fundamental component of the voltages of phases a and b are   3^ 3^ vf a  Vcf , vf b  0 f L = Vcf . (17) 2 2 Since vf L has a substantially high magnitude and delay in sensing it is much lesser than the ripple component, it is justified to assume that its polarity is correctly detected. Also, since vrs is positive, polarity of vL is interpreted to be positive. Now, using (16) and (17)  ^o Ts I 3 ^ Vcf - vL = . (18) 2 4Cf

Fig. 9. Instantaneous positions of if a , if b , vf a , and vf b at beginning of commutation from phase a to phase b.

Therefore, vL becomes negative if ^ ^cf < Io Ts V 4Cf  Cf,min =
(2)

are sampled at frequency 4fs . vrs is the corresponding sensed ripple voltage available for sampling. Noise filtering in voltage measurement and the antialiasing filters invariably cause delays in the sensed signal. The presented plot of vrs is derived using a first-order low-pass approximation of the measurement circuit and filter. At the sampling instant just before commutation, vrs is positive, and vrL is negative with a magnitude Vm . Using (43)  3 ^ I o Ts . Vm  (16) 8Cf At the instant when commutation is about to begin, ¯ Iin is very close to the middle of sector II of the input current hexagon shown in Fig. 17(a) in Appendix A. The 3 Ph waveforms shown in Fig. 9 have been used to distinguish between the position of different variables on the basis of their instantaneous phases, at the start of commutation. The intersection of the vertical line L1 with the three waveforms therefore indicates the position of if a , if b , and if c , respectively, at the start of commutation. At the same instant, the relative position of the fundamental components of input voltages depends on the IDF. Let the intersection of the line L2 with the waveforms indicate the positions of vf a , vf b , and vf c , respectively. Therefore, the input currents lead the voltages, a situation which can arise in applications requiring input reactive power control [23], [24]. From Fig. 9, since vf a and vf b have clearly distinguished instantaneous values, the commutation between these two phases is to be interpreted as an "uncritical" one [22]. For an uncritical commutation, regular two- or four-step commutation methods are suggested [21],

^o Ts I I o Ts  . ^ 4Vs 4Vcf

(19)

Fig. 10 shows the condition that will arise in this situation. Short circuit current isc flows over an interval Tsc , which comprises two turn-on and turnoff times of the insulated gate bipolar transistor (IGBT). Lst in Fig. 10 represents the stray inductance of the circuit. Let vD be the total forward drop of the two IGBTs ^D as the maximum current and diodes. Thereby, by denoting I rating of the devices, the criterion for the safety of the devices is obtained as Isc,max 1 = Lst
Tsc

^D . (-vL - vD ) dt  I
0

(20)

Using (18) in (20), a lower limit of Cf is obtained as
(3) Cf,min

1^ = I o Ts 4

^cf V

^D Lst I + 1.15 vD + Tsc
(2)

-1

.

(21)

Cf,min is, of course, hardware specific. Cf,min may be used as a preliminary check before proceeding with the hardware details. If the converter is operated at unity IDF, then, for maximum vrL to occur at the instant of commutation, vf L has to be zero. The line voltage magnitude vL is then equal to Vm defined in (16). Substituting -vL accordingly in (20) yields another lower limit of Cf as
(4) Cf,min

(3)

3^ I o Ts = 8



^D Lst I vD + Tsc

-1

.

(22)

6488

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE VI Cf,min FOR R ELIABLE VOLTAGE C OMMUTATION

the harmonic performance, further complicates the controller design as discussed subsequently. F. Design Modifications for a Nonminimum Phase Plant The dynamic model of 3 Ph MC in the dq reference frame has been analyzed in [18] where a condition for avoiding right half zeros (RHZs) in the plant transfer function matrix has been described. For the per-phase system depicted in Fig. 2, the input filter output impedance Zso is defined as Zso (s) = - vcf (s) iin (s) .
vs (s)=0

(26)

This scalar transfer function is transformed to a transfer matrix Zso (s) in a synchronous dq reference frame as Zso (s) =
Fig. 11. Magnitude plot of Gf v (s) for Ls of 0.5 and 1 mH.

Zso11 (s) -Zso12 (s)

Zso12 (s) . Zso11 (s)

(27)

For avoiding RHZs, it is necessary that
(3)

Cf,min would be higher than both Cf,min and Cf,min . For unity IDF, the commutation through a third phase [21] would allow using a smaller capacitor. However, operating with a (4) value higher than Cf,min will ensure safe commutation for any operating condition. Moreover, this is ensured irrespective of any particular commutation method and is not dependent on the accurate ZC detection of ripple voltage. (2) (3) (4) Table VI shows Cf,min , Cf,min , and Cf,min , calculated using tabulated device ratings and measured Lst . E. Effect of Ls With the inclusion of Ls , Gf v (s) is obtained as Gf v (s) = +1 s Rf d s3
Ls Lf C f Rd L

(4)

(2)

|Zso11 (jM )| <

^ 2 cos2 i 3V cf . 2 Pin

(28)

+ s2 Cf (Ls + Lf ) + s Rf +1 d

L

.

(23)

Zso11 (j ) has multiple phase crossovers, and M is that phase crossover frequency where |Zso11 (j )| is maximum. Pin is the power at the input terminals of MC, and cos i is the IDF. Importantly, the condition defined in (28) is not confined to any specific modeling approach. The power stage of MC, along with input filters in the small signal/linearized model, is responsible for RHZs. From (28), input filter parameters are one of the factors deciding the emergence of RHZs. The violation of (28) causes four RHZs to emerge in the system, which severely complicates the controller design, particularly if the bandwidth (BW ) requirement is high. Then, either the dynamic specifications have to be relaxed or the input filter has to be designed such that (28) is not violated. With reference to (28) and [18] |Zso11 (jM )|  Zso11 (j )


Under the assumption of underdamped second-order filter Gf v (s)  +1 s Rf d
n s Rf +1 d n+1 L s2 2 c L

 Zso (j )

.

(29)

+

s  c Qc

(24) +1

Assuming underdamped second-order filter and ideal passive elements Zso (j )


where c  1 , Q  Rd (n +1)Lf Cf Cf Lf (1+ n)1.5 , n = Ls . Lf (25)

= |Zso (jc )|  Rd 1 +

Ls Lf

2

.

(30)

Fig. 11 shows the gain plot for varying Ls . Input filter parameters are used from Table VIII. Therefore, as Ls increases, c falls while Q increases. Thus, the gain to lower harmonics is bound to increase. However, the losses are going to decrease as fs components of iin now see a higher impedance parallel to Cf . Hence, knowledge of Ls is necessary before finalizing on the input filter parameters. If n defined is (25) is less than 0.5, then c does not get significantly affected, and neither does the filter performance. However, a high n, apart from deteriorating

So as Ls increases, chances of the plant migrating from minimum to nonminimum phase also increase. Under these circumstances a lower Rd or higher Lf can be chosen to ensure that (28) is satisfied and the plant remains minimum phase. Either of these options degrade the steady-state performance in different ways. Higher voltage drop across Lf lowers the maximum output voltage that can be obtained. A lower Rd on the other hand increases losses. So, for a weak grid, a tradeoff is necessary between the dynamic performance and either filter losses or full-load regulation. The inclusion of the internal resistances (ri ) of passives at M results in higher damping owing to the skin effect and, hence, lower |Zso (jM )| than what is represented by (30).

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6489

TABLE VII B OUNDARY VALUES FOR PARAMETERS OF O UTPUT F ILTER PARAMETERS

Therefore, to avoid a conservative design, using the measured value of ri or a good design estimate of it, to calculate |Zso (jM )|, is necessary. III. O UTPUT F ILTER The criteria defined in Spec.1 to Spec.4 are also followed for output filter design. The procedure, being very similar to the earlier exercise, is therefore briefly discussed. Denoting the resonant frequency of the output filter shown in Fig. 2 as co , the forward gain is obtained as Gvof (s) = 1 vco (s) = 2 von (s) (s/co ) + (s/Qo co ) + 1 (31)

conforming to Spec.1 and Spec.2. As discussed in Section II-C, Q(1) is the optimum value and, therefore, the chosen value for the design. Step 2) Selection of fc from the allowable range obtained using the forward gain plot, Q(1) , Spec.1, and Spec.2 (1) (1) Step 3) Computation of the upper limits Lf,max and Cf,max by using Spec.3 and Spec.4, respectively. Subsequently, these values, along with the selected fc , (1) are used to compute the corresponding Cf,min and Lf,min , respectively. Step 4) Computation of the first allowable set of Rd conforming to Spec.5 and the second set that conforms to Spec.3 and Spec.4 as discussed in Section II-C2. The intersection of these two sets forms the final allowable range of Rd . If they do not intersect, then either one or both of the sets has to be expanded by relaxing the corresponding Spec. to obtain a region of overlap. The first set can be expanded at the expense of allowing higher losses while relaxing the upper and lower bounds of the second set implies poor regulation and higher reactive loading, respectively, than the imposed limits. In this regard, the steady-state requirements for the target application help in deciding which Spec. should be relaxed. This implies starting again from the step related to the redefined Spec. (4) Step 5) Computation of Cf,min for reliable voltage commutation using (19). This has to be treated as the absolute lower limit for Cf if this value is greater (1) than Cf,min . Step 6) Selection of Lf from the interval [Lf,min , Lf,max ]. Choosing Cf from the interval [Cf,min , Lf,max ] or
(4) (1) (1) (1) (1)

where Qo = Ro (Co /Lo ). Since von can be controlled, a virtual damping resistor can be introduced through control to emulate Ro . The selection of the values of Qo , fco,max , fco,min , Co,max , and Lo,max is carried out in the same manner followed during the input filter design. For the input filter, the output current decides the ripple current rating of the filter capacitors. At the output side, however, the proper sizing of Lo allows an additional freedom of using capacitors with a lower ripple current rating. This is clarified using the expression of output admittance, which is obtained as Ymo (s) = 1 + s (Qo /co ) 1 io ( s ) = . von (s) Ro (s/co )2 + (s/Qo co ) + 1 (32)

Thus, for Qo greater than 1 1 |Ymo (js )|  .  s Lo (33)

In SVM [27], at any instant, three output phases are connected to any two input phases. Hence, from Fig. 1, for output phase A v ^on = 2 v ^ab = 1.63Vs . 3 (34)

The resulting peak switching ripple, found using (33) and (34), ^r,rated ) must be lower than the peak ripple current rating (I of Co . Hence ^r,rated ). Spec.6 Lo,min = (1.63Vs,rated )/(s I Table VII lists the described limits for output filter parameters calculated using the nominal values in Table I. IV. S ELECTION OF PARAMETER VALUES The design guidelines from the previous sections are summarized here to facilitate a step-by-step design process. With the nominal power, fundamental and switching frequency, and percentages of different harmonics in the source voltage as design inputs, parameters are selected in the following manner. Step 1) Selection of quality factor Q (= Q(1) ) from the normalized form of the input filter forward gain

from [Cf,min , Lf,max ] as discussed in Step 5. Also, finalizing the value of Rd from the range obtained from Step 4. Step 7) Detecting whether the chosen input filter parameters result in a nonminimum phase plant. If it does, then the feasibility of obtaining a stable closed-loop system in compliance to the dynamic specifications has to be investigated. If a minimum phase plant is desired, then either the full-load regulation (Spec.3) or filter loss (Spec.5) has to be relaxed, and parameters have to be redesigned repeating Steps 3 to 7. Step 8) Selecting Qo , fco , Co,max , and Lo,max following the same steps as their input-side counterparts. Step 9) Selection of Lo,min conforming to Spec.6. Subsequently choosing the values of Lo and Co , thereby completing the design process. The filter parameters are selected following the described steps and using the boundary values listed in Tables II, III, V­VII. The parameter values chosen in this paper are shown in Table VIII, which also lists the specific volume (cm3 /kW) and weight (kg/kW) of the inductors and capacitors. Experimental results are discussed next.

(1)

(1)

6490

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE VIII I NPUT AND O UTPUT F ILTER PARAMETERS

Fig. 13. Open-loop experimental results. vs : 100 V/div, is : 5 A/div, vco : 100 V/div, and io : 5 A/div. Time: 10 ms/div.

Fig. 14. Open-loop results with an Ls of 1 mH. vs : 100 V/div, is : 5 A/div, vco : 100 V/div, and io : 5 A/div. Time: 10 ms/div. TABLE IX M EASURED VALUES

A. Open-Loop Experimental Results MC was operated with m = 1 and unity IDF with io = 8.2 A (fundamental). Figs. 12­14 show the waveforms. Experimentally measured data are listed in Table IX. The voltage waveform across the damping resistor Rd is shown in Fig. 12(a). From the experimental data, its rms value was calculated, resulting in a power loss of 2 W which is much lower than 1% of the input power listed in Table IX. The analysis in Section II-C1 using fundamental vs and io suggests a value (2.1 W) very close to the experimentally obtained value. Referring to Appendix A and particularly (40), operating at unity IDF implies that v ^rL,max is expected almost at the ZC of input line­line voltage vL . Fig. 12(b) shows vL around ZC over 5 ms, and its zoomed view is presented in Fig. 12(c). It is evident that maximum ripple occurs around the ZC zone as expected. In the experimental setup, the measurement of the input filter capacitors with the RCL meter at 10 kHz revealed the values shown in Table IX. The values of Cf from Table IX are used to calculate vra and vrb at the maximum ripple condition as detailed in Section II-D. Subsequently,

Fig. 12. (a) Voltage across Rd : 10 V/div; time: 10 ms/div. (b) vL : 100 V/div; time: 500 s/div. (c) vL : 20 V/div; time: 100 s/div.

V. R ESULTS AND D ISCUSSION Experimental validation was carried out on a 6-kVA MC prototype with IGBT-based four-quadrant switches with the filter parameters listed in Table VIII. The entire control logic was realized on an FPGA platform using ALTERA EP1C12Q240C8 with a sampling frequency of 20 kHz. For commutation, the measured value of vcf was sampled at 40 kHz. A regular four-step commutation strategy, as reviewed in [22], was implemented.

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6491

TABLE X H ARMONICS IN O NE P HASE OF vs , is , AND vco

TABLE XI H ARMONICS vs , is , AND vco W ITH Ls = 1 mH

Fig. 15. Dynamic performance. (a) vco reference command of 140 V. (b) vco reference reset to 0 V. Upper trace: Error voltage in d-axis (116 V/div). Second trace: m (0.2/div). Bottom trace: vco (100 V/div). Time: 10 ms/div.

Fig. 16. Decoupled rectifier-inverter construct.

the v ^rL,max magnitude is found to be 30.8 V. Numerically extracting (filtering) the switching ripple component from the experimental data reveals a maximum ripple listed in Table IX. Thus, a very close agreement is observed between the analytical and experimental observations. Experimental data also revealed an IDF of 0.982 as listed in Table IX. Table X lists the harmonic components in phase a, where the supply voltage vs contains significant low-order harmonics, which, in turn, get transmitted to vco . Since is is a function of both vs and io , it has a higher harmonic content. From the harmonics of vco listed, it is evident that the gain criterion adopted in Sections II and III is effective in the minimal transmission of low-order harmonics from vs to vco . The switching frequency ripple in is was found to be slightly less than 1% of its fundamental component. 1) Performance With Ls Included: Fig. 14 shows the steady-state waveforms, at the same power level, with an externally added Ls of 1 mH. With this inclusion, using (25), c reduces from 1 kHz to 746 Hz, and Q increases from 3.1 to 7.6. The harmonic contents of vs , is , and vco are given in Table XI. Owing to the proximity of input and output corner frequencies (746 and 796 Hz), an increase in the harmonic content of is close to these frequencies is observed. At the instant that this experiment was performed, the total harmonic distortion (THD) of vs was slightly better than the previous situation. However, due to the proximity of the input and output filter corners, the harmonics of is around the output corner have increased, which marginally increased the THD of vco . The measured value of IDF was found to be 0.984. Fig. 15 shows the closed-loop dynamic response of one of the phases of output voltage. The error voltage in vcod , m, and vco are presented in the situation where, initially, a step command of 140 V is introduced and, subsequently, the reference command is reset to zero.

Fig. 17. (a) Input current hexagon. (b) Output voltage hexagon.

VI. C ONCLUSION An exhaustive filter design approach of 3 Ph direct MC (DMC) is presented which, in addition to meeting general requirements of a ripple filter, also addresses the design constraints imposed by dynamic specifications and commutation requirements. Ripple filter design aspects like attenuation and regulation and also MC-specific issues like the damping resistors at the input filter have been addressed. The detailed analytical derivation of the damping resistor losses and input voltage ripple is provided, and experimental results are presented to establish the validity of the analytical conclusions. The minimum input filter capacitor required for reliable VBC has been derived based on the maximum error in ripple voltage sensing. The effect of the grid inductance has been discussed where the possibility of a compromise between the filter and controller designs has been highlighted. The output ripple filter has also been discussed, and experimental waveforms are provided to demonstrate the close agreement of the filter performance with the input specifications. The necessary tradeoffs in filter design, which may be imposed by dynamic requirements, originate from the basic power stage of a 3 Ph DMC. Therefore, the observations are equally applicable to single-phase MCs or indirect MCs which are derived from the basic MC topology.

6492

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE XII ¯in IN S ECTOR 2 AND V ¯OL IN S ECTOR 3 I NPUT ­O UTPUT C ONNECTIONS IN MC FOR I

Ts . The input currents are denoted as If a and If b , respectively. Output currents over the same period are represented as IC and IB . From Table XII and Figs. 9 and 19, If a and If b can be expressed as If a = (IC d d + IB d d ) = mIP sin(60 - SI )
Fig. 18. SI and SV .

If b = (IC d d + IB d d ) = mIP sin SI where IP = IC sin(60 - SV ) + IB sin SV .

(35)

(36)

^rL , is maximum From (15), the peak­peak magnitude of vrL , v when area AP , shown shaded in Fig. 19, is maximum. AP is evaluated as AP = {(IC - If a + If b )d d +(IB - If a + If b )d d }Ts . (37) Substituting (35) in (37), AP gets modified as AP = mIP Ts sin(60 - SI )k (SI , SV ) where  k (SI , SV ) = 1+ 3m sin(SI - 30 )cos(SV - 30 ) . (39)
Fig. 19. Fundamental (If a , If b ) and switching frequency (ira , irb ) components of input currents (ia , ib ).

(38)

A PPENDIX A M AXIMUM vrL B ETWEEN P HASES A AND B Fig. 16 shows the decoupled rectifier-inverter construct of MC. The input current and output voltage stationary vectors for realizing SVM [27] are shown in Fig. 17(a) and (b). I1 (a, c) implies that input phases "a" and "c" are connected to the positive and negative rails of the fictitious dc link, respectively. Also, V1 (+, -, -) indicates that the output phase A is connected to the positive rail and both B and C are connected to the negative rail. Considering an instant when ¯ Iin lies in sector II, requiring ¯ OL is commutation between input phases a and b, and V in sector III, the relevant switching vector combinations and input­output connections are listed in Table III. Fig. 18 shows the rotating space vectors of each hexagon and the two bordering stationary vectors. SI and SV are the angles between the rotating and the trailing stationary vectors. The expression for duty cycles [27] is also indicated in Table XII. Fig. 19 shows the fundamental and switching frequency components of the input currents. The fundamental components of both input and output currents are assumed to be constant over a switching period

Since SI and SV each vary in the closed interval [0 , 60 ] and m varies in [0, 1], it is derived that AP reaches its maximum for SI = 28 , SV = 60 & m = 1. (40)

Therefore, v ^rL is maximum at an instant when if a  if b . Using (40), (38), and (39) in (15), maximum v ^rL is ^in Ts . v ^rL,max  (0.5/Cf )I ^in in MC [27] is The amplitude of the input current I  ^in = mIP = ( 3/2)mI ^o cos(oL ) I (41)

(42)

impedance angle. From (42), the maxiwhere oL is the load  ^o . Substituting this in (41) ^in is ( 3/2)I mum possible I  3 ^ I 0 Ts . (43) v ^rL,max  4Cf

ACKNOWLEDGMENT The authors would like to thank A. Basu and Nandkishore for the support during the hardware fabrication.

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6493

R EFERENCES
[1] J. Monteiro, J. F. Silva, S. F. Pinto, and J. Palma, "Matrix converter-based unified power-flow controllers: Advanced direct power control method," IEEE Trans. Power Del., vol. 26, no. 1, pp. 420­430, Jan. 2011. [2] B. Wang and G. Venkataramanan, "Dynamic voltage restorer utilizing a matrix converter and flywheel energy storage," IEEE Trans. Ind. Appl., vol. 45, no. 1, pp. 222­231, Jan./Feb. 2009. [3] K. Mohapatra and N. Mohan, "Matrix converter fed open-ended power electronic transformer for power system application," in Proc. IEEE Power Energy Soc. Gen. Meet., Convers. Del. Elect. Energy 21st Century, Jul. 20­24, 2008, pp. 1­6. [4] H. Nikkhajoei and M. R. Iravani, "A matrix converter based micro-turbine distributed generation system," IEEE Trans. Power Del., vol. 20, no. 3, pp. 2182­2192, Jul. 2005. [5] A. Garces and A. Trejos, "A voltage regulator based on matrix converter for smart grid applications," in Proc. IEEE PES Conf. ISGT Latin America, Oct. 19­21, 2011, pp. 1­6. [6] P. M. Garcia-Vite, F. Mancilla-David, and J. M. Ramirez, "Per-sequence vector-switching matrix converter modules for voltage regulation," IEEE Trans. Ind. Electron., vol. 60, no. 12, pp. 5411­5421, Dec. 2013. [7] P. Zanchetta, P. Wheeler, L. Empringham, and J. Clare, "Design control and implementation of a three-phase utility power supply based on the matrix converter," IET Power Electron., vol. 2, no. 2, pp. 156­162, Mar. 2009. [8] P. Wheeler and D. Grant, "Optimised input filter design and low-loss switching techniques for a practical matrix converter," Proc. Inst. Elect. Eng.--Elect. Power Appl., vol. 144, no. 1, pp. 53­59, Jan. 1997. [9] C. Klumpner, P. Nielsen, I. Boldea, and F. Blaabjerg, "New Matrix Converter Motor (MCM) for industry applications," IEEE Trans. Ind. Electron., vol. 49, no. 2, pp. 325­335, Apr. 2002. [10] M. Hamouda, F. Fnaiech, and K. Al-Haddad, "Input filter design for SVM dual-bridge matrix converters," in Proc. IEEE Int. Symp. Ind. Electron., Jul. 9­13, 2006, vol. 2, pp. 797­802. [11] D. Gopinath, "Modeling, real-time simulation and design of matrix converters," Ph.D. dissertation, Dept. Elect. Eng., Indian Inst. Sci., Bangalore, India, Sep. 2009. [12] J. Andreu et al., "A step forward towards the development of reliable matrix converters," IEEE Trans. Ind. Electron., vol. 59, no. 1, pp. 167­183, Jan. 2012. [13] A. Trentin, P. Zanchetta, J. Clare, and P. Wheeler, "Automated optimal design of input filters for direct ac/ac matrix converters," IEEE Trans. Ind. Electron., vol. 59, no. 7, pp. 2811­2823, Jul. 2012. [14] T. Kume et al., "Integrated filters and their combined effects in matrix converter," IEEE Trans. Ind. Appl., vol. 43, no. 2, pp. 571­581, Mar./Apr. 2007. [15] T. Friedli, J. W. Kolar, J. Rodriguez, and P. W. Wheeler, "Comparative evaluation of three-phase ac-ac matrix converter and voltage dc-link backto-back converter systems," IEEE Trans. Ind. Electron., vol. 59, no. 12, pp. 4487­4510, Dec. 2012. [16] D. Casadei, G. Serra, A. Tani, A. Trentin, and L. Zarri, "Theoretical and experimental investigation on the stability of matrix converters," IEEE Trans. Ind. Electron., vol. 52, no. 5, pp. 1409­1417, Oct. 2005. [17] D. Casadei et al., "Large-signal model for the stability analysis of matrix converters," IEEE Trans. Ind. Electron., vol. 54, no. 2, pp. 939­950, Apr. 2007. [18] A. Dasgupta and P. Sensarma, "Low-frequency dynamic modelling and control of matrix converter for power system applications," IET Power Electron., vol. 5, no. 3, pp. 304­314, Mar. 2012. [19] K. Sun, D. Zhou, H. Lipei, K. Matsuse, and K. Sasagawa, "A novel commutation method of matrix converter fed induction motor drive using RB-IGBT," IEEE Trans. Ind. Appl., vol. 43, no. 3, pp. 777­786, May/Jun. 2007.

[20] D. Gopinath and V. Ramanarayanan, "Implementation of bi-directional switch commutation scheme for matrix converters," in Proc. Nat. Power Electron. Conf., Bangalore, India, 2007, [CD-ROM]. [21] H. She et al., "Implementation of voltage-based commutation in space vector modulated matrix converter," IEEE Trans. Ind. Electron., vol. 59, no. 1, pp. 154­166, Jan. 2012. [22] J. Mahlein, J. Igney, J. Weigold, M. Braun, and O. Simon, "Matrix converter commutation strategies with and without explicit input voltage sign measurement," IEEE Trans. Ind. Electron., vol. 49, no. 2, pp. 407­414, Apr. 2002. [23] R. Cardenas, R. Pena, P. Wheeler, J. Clare, and G. Asher, "Control of the reactive power supplied by a WECS based on an induction generator fed by a matrix converter," IEEE Trans. Ind. Electron., vol. 56, no. 2, pp. 429­ 438, Feb. 2009. [24] R. Vargas, U. Ammann, B. Hudoffsky, J. Rodriguez, and P. Wheeler, "Predictive torque control of an induction machine fed by a matrix converter with reactive input power control," IEEE Trans. Power Electron., vol. 25, no. 6, pp. 1426­1438, Jun. 2010. [25] R. Erickson and D. Maksimovic, Fundamentals of Power Electronics, 2nd ed. Berlin, Germany: Springer-Verlag, 2001. [26] N. Mohan, T. Underland, and W. Robbins, Power Electronics: Converters, Applications, Design, 2nd ed. Hoboken, NJ, USA: Wiley, 2001. [27] L. Huber and D. Borojevic, "Space vector modulated three phase to three phase matrix converter with input power factor correction," IEEE Trans. Ind. Appl., vol. 31, no. 6, pp. 1234­1246, Nov./Dec. 1995.

Anindya Dasgupta received the B.E.E. degree in electrical engineering from Jadavpur University, Calcutta, India, in 2000; the M.E. degree in electrical engineering from the Bengal Engineering and Science University, Shibpur, India, in 2006; and the Ph.D. degree in electrical engineering from the Indian Institute of Technology Kanpur, Kanpur, India, in 2013. He is currently a Faculty Member in the Department of Avionics, Indian Institute of Space Science and Technology, Thiruvananthapuram, India. His research interests include power converter topologies and their modeling and control.

Parthasarathi Sensarma (M'00) received the B.E.E. degree in electrical engineering from Jadavpur University, Calcutta, India, in 1990; the M.Tech. degree in electrical engineering from the Indian Institute of Technology (IIT) Kharagpur, Kharagpur, India, in 1992; and the Ph.D. degree in electrical engineering from the Indian Institute of Science, Bangalore, India, in 2001. He has held positions with Bharat Bijlee Ltd., Thane, India; CESC Ltd., Kolkata, India; and ABB Corporate Research, Baden-daettwil, Switzerland, where he was a Staff Scientist with the Power Electronics Department. Since 2002, he has been with the Department of Electrical Engineering, IIT, Kanpur, India, where he is currently an Associate Professor. His research interests include power quality, FACTS devices, power converters, and renewable energy integration.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Digital microfluidic biochips in recent years have been developed as a major alternative platform for conventional benchtop laboratory procedures. It offers better precision, scalability, higher sensitivity, lower cost due to smaller sample and reagent volumes. Testing of DMFBs is of major significance in terms of dependability and reliability issues for safety-critical applications. A series of complex microfluidic operations are executed in a compact 2D array within a DMFB. The layout engages a group of cells as transportation path as well as a specific cluster of cells as functional modules to perform basic operations of routing, mixing, splitting, merging, storage and detection. In order to determine the correctness and reliability of results testing of these prescheduled layout are necessary both for transportation (structural) as well as functionality (functional). In this paper we propose a technique for customized testing of a prescheduled layout within a microfluidic array. The test performs both structural as well as functional testing for specified cells that forms the layout. The simulations are carried out in testbenches of benchmark suite III and the results are compared with contemporary methods.Microfluidic based biochips as a composite microsystem offers an alternative platform for conventional laboratory procedures. In recent years a new generation of such lab on chip devices namely Digital Microfluidic Biochip is emerged as a suitable application for concurrent and scalable integration of multiple bioassay protocols. Dependability and accuracy are major issues for safety critical applications of DMFBs specifically in the areas of clinical diagnostics and other biochemical applications. Functional testing is targeted towards assessment of reliability of basic microfluidic operations namely merging, mixing, splitting and incubation. In this paper we proposed a layout specific functional testing method that detects faults in a prespecified group of cells already placed within a 2D array for execution of a given bioassay protocol. The objective is to minimize the test completion time, optimize test resources and customize the test for prespecified cluster of cells to be utilized as modules dedicated for microfluidic operations. The simulation is carried out on testbenches of Benchmark suite III and the results are found to be encouraging.Recent studies have shown that adaptively regulating the sampling rate results in significant reduction in the computational resources of embedded software based control. Selecting a uniform sampling rate for a control loop is robust, but pessimistic for sharing processors among multiple control loops. Fine-grained regulation of periodicity achieves better resource utilization, but is hard to implement online in a robust way. However, an offline control theoretic analysis of the system illustrates the benefits of proper period selection for different modes. Such analysis reveals the necessity to automatically derive a multi-mode scheduler and converge on suitable periods for each mode. This paper proposes a methodology to automatically generate such a scheduler for an embedded real-time control system leveraging its design attributes. The proposed method provides significant gains in computational efficiency without trading off the performance.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/268487117

Designingadaptivelightingcontrolalgorithms forsmartbuildingsandhomes
ConferencePaper·April2014
DOI:10.1109/ICNSC.2014.6819639

CITATIONS

READS

2
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

44

AllcontentfollowingthispagewasuploadedbyYuanWangon05March2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Designing Adaptive Lighting Control Algorithms for Smart Buildings and Homes
Yuan Wang Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu Partha Dasgupta Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract-Artificial lighting is often the main lighting provision for workplaces. This paper describes algorithms for optimizing lighting control in large (smart) buildings that are extensible to smart home use. Systems that provide uniform lighting, under varying outdoor light levels, at occupied locations turns out to be a hard problem. We present methods that work as generic control algorithms and are not preprogrammed for a particular building. Our system uses wireless sensors and wired actuators for the lighting control. But the system does not know about locations and correlations between lights and sensors. The model shows that the control problem is NP-Hard. A heuristic algorithm is proposed and validated to solve the problem to compute approximate optimal solution.

I.

INTRODU CTION

Artificial lighting is essential in large buildings and homes. Lighting has very important effect on people's health and pro ductivity as shown by recent studies [ 1]. Controlling lighting based on occupancy, daylight effects and energy costs can impact energy usage. Currently there are 1.5 million commercial buildings and 1 14 million homes in the US with a growth rate of about 3% each year. Smart buildings and homes need, among other features, the ability to control lighting. Deploying such systems in existing buildings is most feasible if the system uses wireless sensors, simple actuators and does not need custom programming. These goals have driven our research. Automating the lighting control systems such that uniform lighting, under varying outdoor light levels, is maintained at occupied locations turns out to be a hard problem. We are working on algorithms that can be used in generic systems that are not preprogrammed for a particular building. That is the system does not know about locations and correlations between lights and sensors. Several existing approaches are given to increase lighting comfort levels. Some of lighting control systems may be able to provide enough lighting for a workplace, but people may still feel uncomfortable when illumination level is sufficient but not uniform. According to [ 1], three problems are insuf ficient light or uneven light or lights too bright. Controlling lighting levels automatically need customized control systems that rely on extensive pre-programming involving detailed custom models andlor lengthy set-up. When the pattern in a workplace changes, it needs expensive customization and updates performed by trained personnel, which is costly and

time-consuming. The "sequential lighting changes" approach is sometimes used in lighting control system [2] for calculating light settings. This method adjusts light settings based on feedback data, which is able to generate a reasonable result, but the average computational time is high due to the steps of prediction and adjusting. It also increases the number of times lights are switched on/off (unnecessarily) due to its feedback design [2]. In this paper, a Wireless Sensor Network(WSN)-based light ing control system is introduced to efficiently and adaptively control artificial lights to provide a stable and uniform lighting environment. The system measures light at preset locations and turns on or off light switches to achieve the goal. The system is not pre-programmed and learns about effects light switches have on sensors via calibration. The work is formalized using non-linear integer programming model that proves to be NP Hard. A heuristic algorithm is proposed and validated to solve the problem to compute approximate optimal solution. The approach can be used in different places where uniform lighting environment is required. For the sake of brevity and simplicity we only describe the operation of the system while there is no outside lighting. Addition of varying levels of outside lights is an extension(see section V). Also, energy consumption minimization can be incorporated without major changes. The rest of this paper is organized as follows. Section II introduces the background and related work of the problem. Section III presents the heuristic algorithms for lighting con trol. Experimental work and simulation results are discussed in section IV. Section V talks about potential extensions of the algorithm. We conclude the paper in section VI. II.
B ACKGROUND AND RELATED WORK

A. Problem Description

Assume a place has n light switches and m light-level sensors, placed by the human designer of the place. The sensors are connected to the control system via a W SN and the switches are activated via actuators connected to the system. The physical location of lights, sensors and their correlations are initially unknown to the control system. The ultimate task is to compute the positions (on/off) of n light switches. Let x (Xl, ... ,xn ) denote the assignment for light switches where Xi E {O, I} and 0 denotes off and 1
=

978-1-4799-3 106-4114/$3 1.00 ©20 14 IEEE

279

denotes on. The goal is to optimize the comfort level. There are two criteria needed to satisfy, lighting level and lighting uniformity. The former is satisfied when every sensor reading stays in an accepted range i.e. if sensor j's reading is Lj(x), then min  Lj(x)  max. The latter can be satisfied by minimizing the standard deviation(represented by 0") of the sensor readings. Hence the problem can be stated as: minimize
x

TABLE I CALlBRATlON:LIGHTS' IMPACTS ON SENSORS(LUX)

II
81 82

12 230 10

13 350 0

20 680

B. Related Work

(O"(L1(x), ..., Lm(x))) min  Lj(x)  max, j= I, ..., m XiE {O, l}, i=l, ..., n
( 1)

subject to

For lighting impacts, one important feature is that sensor readings are additive [3], i.e. let Impacti j to be the impact of light i on sensor j when only light i is on and it is dark outside, then we have

n Lj(x)= L Impacti j . Xi
i =l

(2)

[4] and [5] gave a general definition of nonlinear integer programming problem. It can be stated as: max/min subject to

f(x) hi(x)=0, iE I= I, ... , p gj(x)  0, jE J =l, ... ,q xE Z n
(3)

where x is a vector of decision variables, and some of the constraints hi, gj : zn ---+ or the objective function f : zn ---+ are non-linear functions. In equation 1, let gl (X) = It can be transformed to: minimize
x

Lj(x)-max, h1(x, y)=Lj(x)-min-y=O, y;::: O, yE Z. (0"(L1(x), ..., Lm(x))) g l (X)  0 h1(x, y)=0 XiE {O, l}, i=l, ..., n y;::: O, yE Z
(4)

lR

lR

subject to

Since equation (4) satisfies the format of equation (3) where the objective function is nonlinear, our problem belongs to nonlinear integer programming problem. According to [4], it is NP-hard. Therefore, any polynomial time computed solution would be an approximation. To find the best setting, a naive approach is to try all 2n positions of n switches, which is un acceptable due to the time complexity. Therefore, we propose a heuristic algorithm for computing an approximate optimal light setting. The heuristic plan has three main steps. First, calibration, i.e. calculating Impacti ,(section III-A). Second, sensors and lights are partitioned into small zones based on calibration data(section III-B). Third, an approximate optimal light setting is generated for each zone (section III-C).

Recently W SN technologies have been applied into a lot of areas such as [6] [7]. It consists of distributed portable wireless sensors to monitor physical conditions, such as light, temperature, humidity, etc. By using W SN, people can easily detect the change of surrounding environments and make cor responding adjustments for actuators. In lighting control area, several existing work [3][8][9] applied W SN technologies into lighting control system for energy conservation purposes. It is involved with using some specific protocols in W SN area such as ZigBee [ 10]. [ 1 1] proposed an approach to integrate small wireless sensor or actuator nodes into an IP-based network so that it is possible to provide web services at each node. Several customized lighting control systems are designed for some specific areas. For example, [8] was mainly designed for entertainment and media production area while [ 12] was mainly designed for theater arts area. These systems generally have some specific requirements that normal lighting control systems do not have. For example, in theater arts area, it needs to capture the positions of actors in real time. Therefore, the objective functions for the lighting control in specific places need to be adjusted accordingly and some particular sensors may need to be enrolled. When more than one lighting re source are involved, such as whole lighting and local lighting, [9] proposed an approach that every user carries a sensor to detect the local intensity. [ 13] introduced an user-friendly interface for a networked lighting control system. [ 14] presented a mathematical model for lighting control system that could be applied into the case that a luminary impact is continuous such as light emitting diodes(LEDs) luminaries and the goal illumination level is given as a single value rather than a particular range. [ 15] used infrared ray communication technology for smart illuminance sensor to retrieve the lighting ID binding with each lighting fixture. Through analyzing lighting ID information, sensors can rec ognize nearby luminaries so that the control efficiency is improved. III.
A. Calibration
CONTROL ApPROACHES

Our system first determines the lighting levels, lighting locations and sensor correlations via calibration. We note that each light has an impact on a set of sensors. Calibration is used for calculating Impacti ;' Suppose there are n lights in an area. The process of calibration is: 1) Turn off all lights and turn on one light at a time 2) Record the light's impact on each sensor 3) Repeat step 1 n times until all lights are counted

280

The impact data will be inserted into a table as shown in Table I. Assume there are 3 lights(h, h and l3) and 2 sensors(sl and S2) in an area. Table I shows all lights' impacts on sensors in lux values. For example, when only h is on, its impact on Sl is 20 lux(Jmpach1) and on S2 is 680 lux(J mpach2).
B. Zoning/Partitioning

From the calibration data we can partition the whole area into smaller zones. The idea of zones is that some switches have no impact on some sensors as they are in different rooms or floors. We assume that every light and sensor can only belong to one zone and artificial lighting is the only lighting resource in the area. Assume there are A sensors and B lights. For each j, j 1,... , A, we define a function fj : {O,l}B -+ which is a mapping from all lights status(on/off) to a non-negative value registered by sensor Sj. It is clear that fJ(x) Lj(x). Using the same example as shown in Table I, if II is on, l2 is off, and l3 is on, h(l,O,1) L1(l,O,1) 1· 20+0·230+1·350 370. All fjs can be collected into F : {O,l}B -+ with for every choice of which lights are on/off gives the value of lux for all sensors. The goal is to partition the whole area for dropping down the computational size (number of lights in a zone). For each j, j I, ... , A, we define a vector gj of length B to indicate which lights are assigned to sensor S j. We also set up a threshold " which is used to decide whether a light is partitioned into a zone or not. When only li is on, if the value of fJ is smaller than " then the ith value of the vector gj is 0; otherwise it is 1. Proceeding with the example of Table I, we show the procedure of getting gl and g2. Let's set, 30. Then since when only h is on, the value of h is 20 which is smaller than,, 1st value of the vector gl is O. The value of 12 is 680 which is larger than ,, 1st value of the vector g2 is 1. Applying this method, in consequence, we have gl (0, I, 1) and g2 (1,0,0), i.e. lights hand b are assigned to sensor Sl while light h is assigned to S2.
=

the problem. If sum of all values in C is still smaller than lowerbound, simply turn on all lights. Part 2 and Part 3 compute candidates of final light setting. A candidate c is a set that only contains light numbers selected to be on, i.e. if i E c then Xi 1, otherwise Xi O. Part 2 computes the base-level candidates. From part 1, it is known at least w lights needed to be turned on to adjust light intensity greater or equal than lowerbound. Therefore, for each base-level candidate setting, it should contain w elements and sum of the w elements' contributions to the
= =

L Ck) should be slightly greater or equal kEsettin g than lowerbound. The proposed approach is based on the
whole zone( fact that lux level light intensity is additive[3] and greedy algorithm, which is shown in Algorithm 1.
Algorithm 1
Output: 2: 3: 4: 6: 7:

lR+,

Base-Level Candidates Selection
setting
Sl, S2 ,

Input: C, n, w, lowerbound

=

base-level candidate

1: searchvalue +-lowerbound/w,

s +- 0

=

=

=

lR

find index of the value that searchvalue in C and put it find index of the value that searchvalue in C and put it add Sl and S2 to s
Si

is the smallest value larger than in Sl is the largest value smaller than in S2

5: for

for

j

E s do = 1 --+ w

-

1 do

=

searchvalue +-(lowerbound

-

8:
9: 10:

change
end for

Sl

to

Si

in line2 and repeat

kEsi

L Ck)/(W - j)

add
Si

Si

to

setting

1 1: end for 12: for 13: 14: 15: 16: 17: for

=

j

E s do = 1 --+ w

-

1 do

searchvalue +-(lowerbound if

-

L Ck)/(W - j)

j is odd then change Sl to Si in line2 and repeat change
S2

=

else

=

18:
19: 20: 2 1:

to

Si

in line3 and repeat

end if end for

C. Final Computation

add

Si

to

setting

The last step is to compute desired light settings. Note that the computations in all zones are independent and can run concurrently. We describe the final computation for a zone, with lights and sensors belonging to that zone only. The proposed lighting control algorithm contains 4 parts. Part 1 counts the minimum number of lights needed to be turned on. Let m to be the number of sensors, C {C1,... ,Cn } to be the set of lights' contributions to the whole
=

22: end for 23: return setting

m

zone where Ci

=

to min x m and upperbound is equivalent to max x m. To count the minimum number of lights turned on, simply find minimum number of elements in the sorted array C such that sum of them are greater or equal than lowerbound. It is obvious that a linear search would be enough to solve

j=l

L Impactij,

lowerbound is equivalent

Part 3 generates more candidates from base-level candidates. Unlike light impacts, standard deviation is not additive, thus increasing number of lights may negatively impact the stan dard deviation. However, due to time and quality tradeoffs, computing more sets likely results in getting closer to the theoretical optimality. Adding <5 lights to compute needs O ( nO ) time. To ensure low response time a low complexity is desirable, and based on the theoretical analysis and simulation experiments, we set <5 2. It indicates that we will at most add 2 lights to the existing candidates. Suppose each base-level candidate has w elements, there are n lights in total. Then if each candidate wants to add an un selected light into itself, there would be n - w choices.
=

28 1

If there are k base-level candidates, finally there would be k x (n - w) new candidates that have w + 1 elements added. Similarly when another unselected light is trying to add into the candidates that have w + 1 elements, total amount of candidates that have w + 2 elements is becoming k x (n - w) x (n - w 1). Thus total number of candidates would be k + k x (n - w) + k x (n - w) x (n - w -1). Note
-

B. Feasibility

The experiment is run at night. Each sensor is placed under each light, at a distance of 60 inches, and they are numbered o to 8. Experimental design and elements are shown in Fig. 1. The steps are: 1) Calibrate lights' impacts on sensors using the approach described in section III-A. 2) Set expected lux level range from 49 to 5 1(Test 1). 3) Turn off all lights. Run control server to compute light settings. When selected lights are turned on, record the reading on each mote. 4) Change the step two's expected lux level range to be 99 to 10 1(Test2), 149 to 15 1(Test3) and 199 to 20 1(Test4) respectively. Rerun step three. The results are shown in Fig. 2. From the figure, we can see for each test, the computed light setting's impact on every mote slightly fluctuates at the middle point of the expected range. Take test 3 as an example. Lights 0, 1,2,3,6,7,8 are selected to be on. Every mote's reading is falling between 149 and 15 1. Average of mote readings for test 3 is 149.22, standard deviation is 3.80. All other tests show the similar trends, which shows the feasibility of our real system, that is using the results of our computation and then applying it to a real, calibrated room, it provides the results that we predicted. Prior to this test, we had already measured the additive property and had some experience with daylight measurements.
300 · Testl, Lights 5,7 on, Avg=49.89, Stdev-2.26 .Test2, Lights 0,1,3,6,7 on, Avg=l00.11, Stdev=3.48 ... Test3, Lights 0,1,2,3,6,7,8 ·
on,

for each candidate

c,

pEe Part 4 calculates the standard deviation of sensor readings generated by each candidate. According to equation 2, for each candidate c, we are able to calculate its impact on sensor j Lj ( c) . Then it is easy to know the standard deviation of all sensors' readings under c. The candidate that generates the lowest standard deviation of sensor readings would be selected as the final light setting.
D. Time Complexity of Heuristic Lighting Control Algorithm

L Cp :s; upperbound.

Suppose there are n lights, m sensors in a zone. Calibration has a time complexity O(n). Zoning/Partitioning has a time complexity O(m n). Final computation has 4 subparts. Part l's time complexity is O(n). The worst case for Part 2 is O(n2). As discussed earlier, the time complexity for Part 3 is nO . In our case, 0 2, which makes Part 3 time complexity to be O(n2). Obviously Part 4's time complexity is equivalent to Part 3, which is also O(n2). Therefore, for every zone, the total time complexity is O(n2). When considering the whole area, suppose there are z zones, and zone Zi(i E [1, z ]) has ni lights. If total number of lights
=

z

is N, we have N

=

L ni.
i=l

Since computations in zones
X
.... u

250

Avg=149.22, Stdev=3.80
on,

are running concurrently, the total complexity is equivalent to O( (max(nl, ..., nz)) 2). It is clear max(nl, ..., nz) :s; N, so the whole area's time complexity is O(N2). IV.
EX PERIMENTAL WORK AND SIMULATION RESULTS

eTest4, Lights 0,1,2,3,4,5,6,7,8 :::J 200 · ·

Avg=199.67, Stdev=4.30 · · · · ·

·

<::"

a. 150 .5 fA, 100
...

.a.

.a.

.a.

.a.

.a.

.a.

.a.

.a.

.a. ·

A. Implementation Details

:::;

·

·

·

·

·

·

·

· ·

To demonstrate the feasibility and effectiveness of our approach, we used an experimental setup. A 8ft x 8ft test cell was instrumented with 9 lights (l5W incandescent, 120V), and 9 sensors connected via a W SN and actuators to a computer. The computer is a Intel Core2 running Linux. The sensors use Crossbow's TelosB motes containing visible light sensors called Hamamatsu S 1087. The sensor network uses the Collection Tree Protocol [ 16] to send lighting data to a designated gateway sensor. Applications running on the motes are implemented in NesC and TinyOS environment. The computer runs a server called SenServer developed in Java and connects to the gateway sensor via serial port. A ProXR Relay Controller is connected to Control Server through USB port to control artificial lights. Since the official driver of the relay controller is designed for Windows platform, Control Server is developed in C# that runs in a Windows machine. It is implemented based on the contents discussed in section III. Eclipse Standard 4.3 and Microsoft Visual Studio 20 10 are used for Java and C# developments respectively.

50

·

·

·

·

·

·

·

·

Mote 10
Fig. 2. Experimental Results of 9 lights and 9 motes

C. Adaptivity and Scalability

The test cell experiments are limited, as the number of lights are low, the space is limited and the geometry is rather simple. To be able to study adaptivity and scalability we did simulations on a more complex, synthetic setup, with randomly generated impact values. Adaptivity and scalability are two important features of the system. Adaptivity means when room pattern changes such as adding or removing some lights, changing lights' posi tions, etc, the system can automatically detect environmental changes and make corresponding adjustments. In other words, when room pattern changes, the lighting control algorithm

282

(c) Testbed Layout Fig. 1. Experimental Design

should be able to adjust light settings accordingly in a short time without any modifications. Adaptivity is also necessary when there is additional illumination due to outside lights. Scalability means when amount of lights is growing large, the lighting control algorithm could still compute the light settings in a reasonable time. Apparently customized algorithms do not have the adaptivity feature and custom systems also use brute force methods to determine lighting levels and hence have exponential complexity, thus not being scalable. When pattern changes, the only part that might get changed in the objective function(Equation 1) is Impacti;' Therefore, to test if the heuristic algorithm is adaptive, Impacti; should be randomly assigned. In addition, expected lux level for this experiment is set between 320 and 500 lux to match normal office lighting range [l7]. The simulation experiment runs as follows: 1) Assume there are 30 lights and 25 sensors in a zone. Each light i has an impact value on sensor j(Impacti;)' There should be 750(30 x 25) impact values. 2) Randomly assign an integer value from 0 to 100(spec ification of 15W, 120V light bulb) to be one impact value. Repeat this step 750 times until all impact values are successfully assigned. 3) Run brute force algorithm(O(2n)), the proposed light ing control algorithm(O(n2)), and a revised O(n3) algorithm(o 3) that adds one more light to the candi date sets of O(n2), respectively. Record computational time, lux level light intensity and standard deviation for each method. 4) Repeat step 1-3 1000 times. Take an average of standard deviation, light intensity and computational time results respectively. 5) Increase light's amount and rerun steps 1-4.
=

to the revised O(n3) algorithm, but the computational time is far less than the latter one. Compared to the brute force, the proposed approach has an extremely better time performance with a similar light intensity performance and a reasonable increase on standard deviation. We also do some other evaluations to verify the system's scalability. For example, we run the above algorithms on very large dataset like 1000 number of lights. Result shows that the proposed O(n2) approach can still compute the light setting in a desired range while other methods are not applicable due to the time performance. When size is 1000, the O(n2) algorithm is running in around 106 time complexity which reduces substantial workloads from brute force's 1.07 x 10301 time complexity. V.
EXT ENDING T HE ALGORIT HM

The result is shown in Fig. 3. Fig. 3(c) describes time per formances of different methods. Since when amount of lights increases, the computational time of brute force method is increasing exponentially, which quickly arrives at a very large point of value while polynomial time algorithms have a pretty low computational time. Therefore, in order to put all data together, for Fig. 3(c), we take a common 10garithm(loglO) on the real computational time data. From the graph, we can see that our proposed 0 (n2) algorithm reaches a similar performance in light intensity and standard deviation compared

Our goal is to produce a stable and uniform lighting environment in workplaces by adjusting artificial lights. In this paper, we assume artificial lighting is the only lighting source in a workplace and there is no other lighting involved. However, when ambient light exists, the algorithm needs to be extended to allow for the impact produced by such light. Ambient light will produce a non-zero and varying impact on different sensors and is not controllable via light switches. This impact will have to be detected and used in the computation, in other words the computation needs to compute deltas between current conditions and desired levels and recomputed switch settings. The idea can be also applied when required illumination levels in an area are different. The selection of the final computation results can be done in an energy efficient manner by computing the energy costs of each light being turned on. From this we can select candidate setting that may not produce the best uniformity but its energy is consumed in significantly lower. In addition, the goal lighting level can be changed for energy savings especially at peak load hours. Placement of sensors is an important point we did not address in the paper. In the current work, we assume sensors in an area are distributed in a good way that can very well capture the light intensity. In the future work, we will do more investigations on sensor layouts, which will better capture the light intensity in an area. Although the brute force method can't be directly applied due to the time issue, it still might be worthy to add into the

283

70 c: 0 ',0:; 60

40 Qj Q 30 'l:I 0 'l:I c: 10

'>

"'

50
.... 0(2AN)
__ 0(NA2)

... 2 "'

...... 0(NA3)

\I')

"' ....

30

31

3

2

33

34

3

5

36

37

38

39

40

Amount of Lights
(a) Standard Deviation
600

in both experimental and simulated scenarios. The underlying system of wireless sensors and wired actuators can be de ployed at homes and in larger building without excessive costs. We build a formal model of the lighting control problem and show it is a hard problem (NP-Hard). A heuristic algorithm is proposed to solve the problem to compute approximate optimal solution. Experimental results show the efficiency and effectiveness of the heuristic algorithm. The proposed approach can be augmented to acconunodate day lighting, failures, manual overrides, and occupancy detectors.
REFERENCES [1] Lighting at work. Health and Safety Executive, 1997. [2] R. Mohamaddoust, A. T. Haghighat, M. J. Motahari Sharif, and N. Ca panni, "A novel design of an automatic lighting control system for a wireless sensor network with increased sensor lifetime and reduced sensor numbers," Sensors, vol. II, no. 9, pp. 8933-8952, 2011. [3] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews, "Intelligent light control using sensor networks," in Proceedings of the
3rd international conference on Embedded networked sensor systems.

.u; =
c:

 500 ..
c: Q) 400 300 200 100

--'"

.......

"I

.....

,.
.... 0(2AN) __ 0(NA2) ...... 0(NA3)

 )( .3

Qj >

30

31

32

33

34

35

36

37

38

39

40

Amout of Lights
(b) Light Intensity

ACM, 2005, pp. 218-229 . [4] R. Hemmecke, M. Koppe, J. Lee, and R. Weismantel, "Nonlinear integer programming," arXiv preprint arXiv:0906.5171, 2009. [5] Wikipedia. Nonlinear programming. [6] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of
the lst ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88-97. [7] Q. Li, M. De Rosa, and D. Rus, "Distributed algorithms for guiding navigation across a sensor network," in Proceedings of the 9th annual international conference on Mobile computing and networking. ACM, 2003, pp. 313-325. [8] H. Park, J. Burke, and M. B. Srivastava, "Design and implementation of a wireless sensor network for intelligent light control," in Proceedings

6.5 5.5 5  c: 4.5 8 4 Qj 3.5 11\ - 3 I- 2.5  2 1.5 0.5

--

----

....-

--

....-

....-

-0(2AN) -0(NA2) -0(NA3)

of the 6th international conference on Information processing in sensor

ACM, 2007, pp. 370-379. [9] M. Pan, L. Yeh, Y. Chen, Y. Lin, and Y. Tseng, "Design and implemen tation of a wsn-based intelligent light control system," in Distributed
networks. Computing Systems Workshops, 2008. 1CDCS'08. 28th 1nternational

Amount of Lights
(c) Computational Time Fig. 3. Comparison of Different Approaches

IEEE, 2008, pp. 321-326. [10] Y. Wang and Z. Wang, "Design of intelligent residential lighting control system based on zigbee wireless sensor network and fuzzy controller," in Machine Vision and Human-Machine Interface (MVHl), 2010 Inter national Conference on. IEEE, 2010, pp. 561-564. [11] L. Schor, P. Sommer, and R. Wattenhofer, "Towards a zero-configuration wireless sensor network architecture for smart buildings;' in Proceedings
Conference on. of the First ACM Workshop on Embedded Sensing Systems for Energy Efficiency in Buildings. ACM, 2009, pp. 31-36. [12] c. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, "Design of a wireless sensor network based automatic light controller in theater arts," in Engineering of Computer-Based Systems, 2007. ECBS'07. 14th Annual

system because it can produce the theoretical best result. One idea is to combine our current work with brute force method. The current work can be used during the initial step to quickly generate an accepted light setting. The brute force method can finely adjust the setting in the backend afterwards. Optimal results can be also stored for future use. VI.
CONCLUSION

IEEE, 2007, pp. 161-170. [13] T. Hiroyasu, A. Nakamura, S. Shinohara, M. Yoshimi, M. Miki, and H. Yokouchi, "Intelligent lighting control user interface through design of illuminance distribution," in Intelligent Systems Design and Applica tions, 2009. ISDA'09. Ninth International Conference on. IEEE, 2009, pp. 714-719. [14] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon, "Intelligent lighting control using sensor networks," in Networking,
IEEE International Conference and Workshops on the. Sensing and Control (lCNSC), 2013 10th 1EEE 1nternational Conference on. IEEE, 2013, pp. 170-175. [IS] M. Miki, A. Amamiya, and T. Hiroyasu, "Distributed optimal control of lighting based on stochastic hill climbing method with variable neighborhood," in Systems, Man and Cybernetics, 2007. ISle. IEEE International Conference on. IEEE, 2007, pp. 1676-1680. [16] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1-14. [17] Wikipedia. Lux. [Online]. Available: http: //en.wikipedia.orglwikilLux

To enable automated lighting control, under varying con ditions of occupancy, needs, outside lighting influences and other perturbations it is essential to have a core algorithm that is effective and adaptive. Such algorithm must be deploy able in a simple, cost effective system without the need for customizations and reprograrmning as conditions change. This paper presents such a core algorithm and tests its effectiveness

284
View publication stats

Digital Microfluidic Biochips (DMFB), a promising platform for Lab-on-chip systems are capable of automated biochemical analysis targeted for medical diagnostics and other biochemical applications. The inherent nature of reconfigurability and scalability enables the device to integrate multiple bioassay protocols within the same array for simultaneous execution. Each execution of Bioassay involves numerous microfluidic operations to be performed successfully within the DMFB. Optical Detection and analysis are one of the significant operations required to be performed in DMFB systems for conclusive diagnosis and testing of targeted parameters within the specified sample. In this paper we propose a new design for automated detection based analyzer system integrated with a multipartioned scalable DMFB for two stage confirmatory detection of multiple parameters for a given set of samples .Multiple bioassays are executed sequentially in two stages and the results are analyzed using an intelligent system with integrated memory containing precharacterized standard outputs. The design prototype is implemented on FPGA platform and the simulations and detection results for a set of specified bioassay protocols are found to be satisfactory and in conformance with conventional benchtop laboratory processes.Reliability has become an integral component of the design intent of embedded cyber-physical systems. Safety-critical embedded systems are designed with specific reliability targets, and design practices include the appropriate allocation of both spatial and temporal redundancies in the implementation to meet such requirements. With increasing complexity of such systems and considering the large number of components in such systems, redundancy allocation requires a formal scientific basis. In this work, we profess the analysis of the redundancy requirement upfront with the objective of making it an integral part of the specification. The underlying problem is one of synthesizing a formal specification with built-in redundancy artifacts, from the formal properties of the error-free system, the error probabilities of the control components, and the reliability target. We believe that upfront formal analysis of redundancy requirements is important in budgeting the resource requirements from a cost versus reliability perspective. Several case-studies from the automotive domain highlight the efficacy of our proposal.Abstract:
Droplet based microfluidic technology in recent years is reckoned as a major driving force for the development of new generation of Lab-on-chip devices. Such devices known as digital micro fluidic biochips are capable of manipulating discrete nanolitre volumes of droplets on a 2D planar array of electrodes. Due to their inherent nature of reconfigurability and scalability these devices are designed to be employed for large scale integration of multiple bioassays within the same grid. In order to enable such applications increasing number of control pin requirements together with high wire planning complexity becomes a major problem. In this paper we have proposed new techniques for interconnection wire routing for the control electrodes operating at identical time sequence. We have defined a double layer dual wire system running in parallel along two separate planes in mutually perpendicular directions. We further proposed an algorithm to develop a feasible wire plan for a given layout with an aim to optimize the overall number of pin count. Multiphasing on same pin has been proposed to resolve the issue of wire planning in cases of cross contamination at any particular site. The proposed technique has been employed in layouts using test benches for Benchmark suite III and selective test benches for benchmark suite I. The objective was to obtain a feasible wire plan with optimum pin utilization and enhanced route performance. The results obtained from simulation of the proposed algorithm on the test benches (mentioned earlier) are found to be encouraging.Ghosh, Sharma, Chakrabarti, & Dasgupta

are useful in practice. One example of such a problem is finding the secondary structure of RNA (Mathews & Zuker, 2004) which is an important problem in Bioinformatics. RNAs may be viewed as sequences of bases belonging to the set {Adenine(A), Cytocine(C), Guanine(G), Uracil(U) }. RNA molecules tend to loop back and form base pairs with itself and the resulting shape is called the secondary structure. The primary factor that influences the secondary structure of RNA is the number of base pairings (higher number of base pairings generally implies more stable secondary structure). Under the well established rules for base pairings, the problem of maximizing the number of base pairings has an interesting dynamic programming formulation. However, apart from the number of base pairings, there are other factors that influence the stability, but these factors are typically evaluated experimentally. Therefore, for a given RNA sequence, it is useful to compute a pool of candidate secondary structures (in decreasing order of the number of base pairings) that may be subjected to further experimental evaluation in order to determine the most stable secondary structure. The problem of generating ordered set of solutions is well studied in other domains. For discrete optimization problems, Lawler (1972) had proposed a general procedure for generating k-best solutions. A similar problem of finding k most probable configurations in probabilistic expert systems is addressed by Nilsson (1998). Fromer and Globerson (2009) have addressed the problem of finding k maximum probability assignments for probabilistic modeling using LP relaxation. In the context of ordinary graphs, Eppstein (1990) has studied the problem of finding k-smallest spanning trees. Subsequently, an algorithm for finding k-best shortest paths has been proposed in Eppstein's (1998) work. Hamacher and Queyranne (1985) have suggested an algorithm for k-best solutions to combinatorial optimization problems. Algorithms for generating k-best perfect matching are presented by Chegireddy and Hamacher (1987). Other researchers applied the k-shortest path problem to practical scenarios, such as, routing and transportation, and developed specific solutions (Takkala, Bornd¨ orfer, & L¨ obel, 2000; Subramanian, 1997; Topkis, 1988; Sugimoto & Katoh, 1985). However none of the approaches seems to be directly applicable for AND/OR structures. Recently some schemes related to ordered solutions to graphical models (Flerova & Dechter, 2011, 2010) and anytime AND/OR graph search (Otten & Dechter, 2011) have been proposed. Anytime algorithms for traditional OR search space (Hansen & Zhou, 2007) are well addressed by the research community. In this paper, we address the problem of generating ordered set of solutions for explicit AND/OR DAG structure and present new algorithms. The existing method, proposed by Elliott (2007), works bottom-up by computing k-best solutions for the current node from the k-best solutions of its children nodes. We present a best first search algorithm, named Alternative Solution Generation (ASG) for generating ordered set of solutions. The proposed algorithm maintains a list of candidate solutions, initially containing only the optimal solution, and iteratively generates the next solution in non-decreasing order of cost by selecting the minimum cost solution from the list. In each iteration, this minimum cost solution is used to construct another set of candidate solutions, which is again added to the current list. We present two versions of the algorithm ­ a. Basic ASG (will be referred to as ASG henceforth) : This version of the algorithm may construct a particular candidate solution more than once;

278

Generating Ordered Solutions for Explicit AND/OR Structures

b. Lazy ASG or LASG : Another version of ASG algorithm that constructs every candidate solution only once. In these algorithms, we use a compact representation, named signature, for storing the solutions. From the signature of a solution, the actual explicit form of that solution can be constructed through a top-down traversal of the given DAG. This representation allows the proposed algorithms to work in a top-down fashion starting from the initial optimal solution. Another salient feature of our proposed algorithms is that these algorithms work incrementally unlike the existing approach. Our proposed algorithms can be interrupted at any point of time during the execution and the set of ordered solutions obtained so far can be observed and subsequent solutions will be generated when the algorithms are resumed again. Moreover, if an upper limit estimate on the number of solutions required is known a priori, our algorithms can be further optimized using that estimate. The rest of the paper is organised as follows. The necessary formalisms and definitions are presented in Section 2. In Section 3, we address the problem of generating ordered set of solutions for trees. Subsequently in Section 4, we address the problem of finding alternative solutions of explicit acyclic AND/OR DAGs in non-decreasing order of cost. We present two different solution semantics for AND/OR DAGs and discuss the existing approach as well as our proposed approach, along with a comparative analysis. Detailed experimental results, including the comparison of the performance of the proposed algorithms with the existing algorithm (Elliott, 2007), are presented in Section 5. We have used randomly constructed trees and DAGs as well as some well-known problem domains including the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding the secondary structure of RNA as test domain. The time required and the memory used for generating a specific number of ordered solutions for different domains are reported in detail. In Section 6, we outline briefly about applying the proposed algorithms for implicitly specified AND/OR structures. Finally we present the concluding remarks in Section 7.

2. Definitions
In this section, we describe the terminology of AND/OR trees and DAGs followed by other definitions that are used in this paper. G = V, E is an AND/OR directed acyclic graph, where V is the set of nodes and E is the set of edges. Here  and  in G refer to the AND nodes and OR nodes in the DAG respectively. The direction of edges in G is from the parent node to the child node. The nodes of G with no successors are called terminal nodes. The non-terminal nodes of G are of two types ­ i) OR nodes and ii) AND nodes . V and V are the set of AND and OR nodes in G respectively, and n = |V |, n = |V |, and n = |V |. The start (or root) node of G is denoted by vR . OR edges and AND edges are the edges that emanate from OR nodes and AND nodes respectively. Definition 2.a [Solution Graph] A solution graph, S (vq ), rooted at any node vq  V , is a finite sub-graph of G defined as: a. vq is in S (vq );   is an OR node in G b. If vq  and vq is in S (vq ), then exactly one of its immediate successors in G is in S (vq );   is an AND node in G c. If vq  and vq is in S (vq ), then all its immediate successors in G are in S (vq );
279

Ghosh, Sharma, Chakrabarti, & Dasgupta

d. Every maximal (directed) path in S (vq ) ends in a terminal node; e. No node other than vq or its successors in G is in S (vq ). By a solution graph S of G we mean a solution graph with root vR .

 

Definition 2.b [Cost of a Solution Graph] In G , every edge eqr  E from node vq to node vr has a finite non-negative cost ce ( vq , vr ) or ce (eqr ). Similarly every node vq has a finite non-negative cost denoted by cv (vq ). The cost of a solution S is defined recursively as follows. For every node vq in S , the cost C (S, vq ) is:   cv (vq ), if vq is a terminal node;     cv (vq ) + C (S, vr ) + ce ( vq , vr ) , where vq is an OR node, and  C (S, vq ) = vr is the successor of vq in S ;    C (S, vj ) + ce ( vq , vj ) , where 1  j  k, vq is an AND node cv (vq ) +     with degree k, and v1 , . . . , vk are the immediate successors of vq in S. Therefore the cost of a solution S is C (S, vR ) which is also denoted by C (S ). We denote the optimal solution below every node vq as opt(vq ). Therefore, the optimal solution of the entire AND/OR DAG G , denoted by Sopt , is opt(vR ). The cost of the optimal solution rooted at every node vq in G is Copt (vq ), which is defined recursively (for minimum cost objective functions) as follows:   cv (vq ), if vq is a terminal node;      cv (vq ) + min Copt (vj ) + ce ( vq , vj ) , where 1  j  k, vq is an OR node Copt (vq ) = with degree k, and v1 , . . . , vk are the immediate successors of vq in G ;   cv (vq ) + Copt (vj ) + ce ( vq , vj ) , where 1  j  k, vq is an AND node     with degree k, and v1 , . . . , vk are the immediate successors of vq in G . The cost of the optimal solution Sopt of G is denoted by Copt (vR ) or, alternatively, by Copt (Sopt ). When the objective function needs to be maximized, instead of the min function, the max function is used in the definition of Copt (vq ).  

It may be noted that it is possible to have more than one solution below an OR node vq to qualify to be the optimal one, i.e., when they have the same cost, and that cost is the minimum. Ties for the optimal solution below any such OR node vq are resolved arbitrarily and only one among the qualifying solutions (determined after tie-breaking) is marked as opt(vq ). An AND/OR tree, T = V, E , is an AND/OR DAG and additionally satisfies the restrictions of a tree structure i.e., there can be at most one parent node for any node vq in T . In the context of AND/OR trees, we use eq to denote the edge that points to ^ = V, E , is an AND/OR tree with the the vertex vq . An alternating AND/OR tree, T restriction that there is an alternation between the AND nodes and the OR nodes. Every child of an AND node is either an OR node or a terminal node, and every children of an OR node is either an AND node or a terminal node. We use the term solution tree to denote the solutions of AND/OR trees. We also discuss a different solution semantics, namely tree based semantics, for AND/OR DAGs. Every AND/OR DAG can be converted to an equivalent AND/OR tree by traversing
280

Generating Ordered Solutions for Explicit AND/OR Structures

the intermediate nodes in reverse topological order and replicating the subtree rooted at every node whenever the in-degree of the traversed node is more than 1. The details are shown in Procedure ConvertDAG. Suppose an AND/OR DAG G is converted to an equivalent AND/OR tree T . We define the solutions of T as the solutions of G under tree based semantics. Procedure ConvertDAG(G ) input : An AND/OR DAG G output: An equivalent AND/OR tree T 1 Construct a list M , of non-terminal nodes of G , sorted in the reverse topological order; 2 while M is not empty do 3 vq  Remove the first element of M ; /* Suppose Ein (vq ) is the list of incoming edges of vq */ 4 if InDegree(vq ) > 1 then 5 for i  2 to InDegree(vq ) do 6 et  Ein (vq )[i];  as the root; Replicate the sub-tree rooted at vq with vq 7 ; Modify the target node of et from vq to vq 8 9 end 10 end 11 end In this paper we use the solution semantics defined in Definition 2.a as the default semantics for the solutions of AND/OR DAGs. When the tree based semantics is used, it is explicitly mentioned. 2.1 Example

2, 34

v1
2 1 1

v1

2, 89 2

3

3, 29

v2
1

v3
35 4

2, 37

v4
1 3

v2

3, 43 5 4

v3

2, 41 1

5

v4
40 4

v5

2, 35 3

v6
52

2, 8

v5
2

v6
12 2

3, 11

v7
3 1

4, 17

v8
1 2

v7
1

3, 9 2

v8
17

1

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

v9
5

v10
7

Figure 1: Alternating AND/OR Tree
281

Figure 2: AND/OR DAG

Ghosh, Sharma, Chakrabarti, & Dasgupta

We present an example of an alternating AND/OR tree in Figure 1. In the figure, the terminal nodes are represented by a circle with thick outline. AND nodes are shown in the figures with their outgoing edges connected by a semi-circular curve in all the examples. The edge costs are shown by the side of each edge within an angled bracket. The cost of the terminal nodes are shown inside a box. For every non-terminal node vq , the pair of costs, cv (vq ) and Copt (vq ), is shown inside a rectangle. In Figure 1 the optimal solution below every node is shown using by thick dashed edges with an arrow head. The optimal solution of the AND/OR tree can be traced by following these thick dashed edges from node v1 . The cost of the optimal solution tree is 34. Also, Figure 2 shows an example of a DAG; the cost of the optimal solution DAG is 89.

3. Generating Ordered Solutions for AND/OR Trees
In this section we address the problem of generating ordered solutions for trees. We use the notion of alternating AND/OR trees, defined in Section 2, to present our algorithms. An alternating AND/OR tree presents a succinct representation and so the correctness proofs are much simpler for alternating AND/OR trees. In Appendix C we show that every AND/OR tree can be converted to an equivalent alternating AND/OR tree with respect to the solution space. It is worth noting that the search space of some problems (e.g. the search space of multipeg Tower of Hanoi problem) exhibit the alternating AND/OR tree structure. Moreover, the algorithms that are presented for alternating AND/OR trees work without any modification for general AND/OR trees. In this section, first we present the existing algorithm (Elliott, 2007) briefly, and then we present our proposed algorithms in detail. 3.1 Existing Bottom-Up Evaluation Based Method for Computing Alternative Solutions We illustrate the working of the existing method that is proposed by Elliott (2007) for computing alternative solutions for trees using an example of an alternating AND/OR tree. This method (will be referred as BU henceforth) computes the k-best solutions in a bottomup fashion. At every node, vq , k-best solutions are computed from the k-best solutions of the children of vq . The overall idea is as follows. a. For an OR node vq , a solution rooted at vq is obtained by selecting a solution of a child. Therefore k-best solutions of vq are computed by selecting the top k solutions from the entire pool consisting of all solutions of all children. b. In the case of AND nodes, every child of an AND node vq will have at most k solutions. A solution rooted at an AND node vq is obtained by combining one solution from every child of vq . Different combinations of the solutions of the children nodes of vq generate different solutions rooted at vq . Among those combinations, top k combinations are stored for vq . In Figure 3 we show the working of the existing algorithm. At every intermediate node 2-best solutions are shown within rounded rectangle. At every OR node vq , the ith -best solution rooted at vq is shown as a triplet of the form ­ i : < child, solidx >, cost. For example, at node v1 the second best solution is shown as ­ 2 : v2 , 2 , 37; which means

282

Generating Ordered Solutions for Explicit AND/OR Structures

that the 2nd best solution rooted at v1 is obtained by selecting the 2nd best solution of v2 . Similarly, at every AND node vq , the ith solution rooted at vq is shown as a triplet of the form ­ i : |sol vec|, cost triplets. Here sol vec is a comma separated list of solution indices such that every element of sol vec corresponds to a child of vq . The j th element of sol vec shows the index of the solution of j th child. For example, the 2nd best solution rooted at v2 is shown as ­ 2 : |2, 1|, 32. This means the 2nd best solution rooted at v2 is computed using the 2nd best solution of the 1st child (which is v5 ) and the best solution (1st ) of the 2nd child (which is v6 ). Which index of sol vec corresponds to which child is shown by placing the child node name above every index position.
1 : v2 , 1 , 34 2 : v2 , 2 , 37

2, 34

v1

3

2

1

3, 29

v2

v5 v6 1 : |1, 1|, 29 2 : |2, 1|, 32
1

v3
35 4

2, 37

v4

v7 v8 1 : |1, 1|, 37 1 : |1, 2|, 40
3

5

2, 8

v5

1 : v9 , 1 , 8 2 : v10 , 1 , 11

v6
12 2

3, 11

v7

1 : v11 , 1 , 11 2 : v12 , 1 , 15

4, 17

v8

1 : v13 , 1 , 17 2 : v14 , 1 , 20

1

2

3

1

1

2

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 3: Example working of the existing algorithm The existing method works with the input parameter k, i.e., the number of solutions to be generated have to be known a priori. Also this method is not inherently incremental in nature, thus does not perform efficiently when the solutions are needed on demand, e.g., at first, top 20 solutions are needed, then the next 10 solutions are needed. In this case the top 20 solutions will have to be recomputed while computing next 10 solutions, i.e., from the 21st solution to the 30th solution. Next we present our proposed top-down approach which does not suffer from this limitation. 3.2 Top-Down Evaluation Algorithms for Generating Ordered Solutions So far we have discussed the existing approaches which primarily use bottom-up approach for computing ordered solutions. Now we propose a top-down approach for generating alternative solutions in the non-decreasing order of cost. It may be noted that the top-down
283

Ghosh, Sharma, Chakrabarti, & Dasgupta

approach is incremental in nature. We use an edge marking based algorithm, Alternative Solution Generation (ASG ), to generate the next best solutions from the previously generated solutions. In the initial phase of the ASG algorithm, we compute the optimal solution ^ and perform an initial marking of all OR edges. for a given alternating AND/OR tree T The following terminology and notions are used to describe the ASG algorithm. In the context of AND/OR trees, we use eq to denote the edge that points to the vertex vq . We will use the following definitions for describing our proposed top-down approaches. Definition 3.c [Aggregated Cost] In an AND/OR DAG G , the aggregated cost, ca , for an edge eij from node vi to node vj , is defined as : ca (eij ) = ce (eij ) + Copt (vj ).  
2, 34

v1

[e2 : 5] 3

2,3 : 5
[e3 : 1] 2

3,4 : 1

1

3, 29

v2

v3
35

2, 37

v4

5

1

4

3

2, 8

v5

v6
12 [e11 : 4]

3, 11

v7
[e13 : 3]

4, 17

v8

[e9 : 3] 1

9,10 : 3

2

2

11,12 : 4

3

1

[e14 : 6]

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 4: Example of OR-edge marking and swap option Marking of an OR edge : The notion of marking an OR edge is as follows. For an OR node vq , L(vq ) is the list of OR edges of vq sorted in non-decreasing order of the aggregated cost of the edges. We define (i,i+1) as the difference between the cost of OR edges, ei and ei+1 , such that ei and ei+1 emanate from the same OR node vq , and ei+1 is the edge next to ei in L(vq ). Procedure MarkOR describes the marking process for the OR edges of an OR node. Intuitively, a mark represents the cost increment incurred when the corresponding edge is replaced in a solution by its next best sibling. The OR edge having maximum aggregated cost is not marked. Consider a solution, Scur , containing the edge ei = (vq , vi ), where ei  Eopt (Scur ). We mark ei with the cost increment which will be incurred to construct the next best solution from Scur by choosing another child of vq . In Figure 4 the marks corresponding to OR edges e2 , e3 , e9 , e11 , e13 , and e14 are [e2 : 5], [e3 : 1], [e9 : 3], [e11 : 4], [e13 : 3], and [e14 : 6].
284

Generating Ordered Solutions for Explicit AND/OR Structures

Procedure MarkOR(vq )
1

2 3 4 5 6 7 8

Construct L(vq ) ; /* List of OR edges of vq sorted in the non-decreasing order of ca values */ count  number of elements in L(vq ) ; for i  1 to i = count - 1 do ec  L(vq )[i] ; en  L(vq )[i + 1] ; tmp = (ca (en ) - ca (ec )) ; Mark ec with the pair [en : tmp ] ; end

Definition 3.d [Swap Option] A swap option ij is defined as a three-tuple ei , ej , ij where ei and ej emanate from the same OR node vq , ej is the edge next to ei in L(vq ), and ij = ca (ej ) - ca (ei ). Also, we say that the swap option ij belongs to the OR node vq .  

Consider the OR node vq and the sorted list L(vq ). It may be observed that in L(vq ) every consecutive pair of edges forms a swap option. Therefore, if there are k edges in L(vq ), k -1 swap options will be formed. At node vq , these swap options are ranked according to the rank of their original edges in L(vq ). In Figure 4 the swap options are : (2,3) = e2 , e3 , 5 , (3,4) = e3 , e4 , 1 , (9,10) = e9 , e10 , 3 , (11,12) = e11 , e12 , 4 , (13,14) = e13 , e14 , 3 , and (14,15) = e14 , e15 , 6 . Consider the node v1 where L(v1 ) = e2 , e3 , e4 . Therefore, the swap options, (2,3) and (3,4) , belong to v1 . At node v1 , the rank of (2,3) and (3,4) are 1 and 2 respectively. Definition 3.e [Swap Operation] Swap operation is defined as the application of a swap option ij = ei , ej , ij to a solution Sm that contains the OR edge ei in the following way:  . Edge e is a. Remove the subtree rooted at vi from Sm . Let the modified tree be Sm i the original edge of ij .  , which is constructed at the previous step. Let the b. Add the subtree opt(vj ) to Sm  . Edge e is the swapped edge of  . newly constructed solution be Sm j ij  from S when Intuitively, a swap operation ij = ei , ej , ij constructs a new solution Sm m  Sm contains the OR edge ei . Moreover, the cost of Sm is increased by ij compared to cost of Sm if C (Sm , vi ) = Copt (vi ).  

Our proposed algorithms use a swap option based compact representation, named signature, for storing the solutions. Intuitively, any alternative solution can be described as a set of swap operations performed on the optimal solution Sopt . It is interesting to observe that while applying an ordered sequence of swap options, 1 , · · · , k , the application of each swap operation creates an intermediate alternative solution. For example, when the first swap option in the sequence, 1 , is applied to the optimal solution, Sopt , a new solution, say S1 , is constructed. Then, when the 2nd swap option, 2 , is applied to S1 , yet another solution S2 is constructed. Let Si denote the solution obtained by applying the swap options, 1 , · · · , i , on Sopt in this sequence. Although, an ordered sequence of swap options, like 1 , · · · , k , can itself be used as a compact representation of an alternative solution, the following key points are important to observe. A. Among all possible sequences that generate a particular solution, we need to preclude those sequences which contain redundant swap options (those swap options whose orig285

Ghosh, Sharma, Chakrabarti, & Dasgupta

inal edge is not present in the solution to which it is applied). This is formally defined later as superfluous swap options. Also the order of applying the swap options is another important aspect. There can be two swap options, i and j where 1  i < j  k such that the source edge of j belongs to the sub-tree which is included in the solution Si only after applying i to Si-1 . In this case, if we apply j at the place of i , i.e., apply j directly to Si-1 , it will have no effect as the source edge of j is not present in Si-1 , i.e., after swapping the location of i and j in the sequence, j becomes a redundant swap option and the solution constructed would be different for the swapped sequence from the original sequence. We formally define an order relation on a pair of swap options based on this observation in the later part of this section and formalize the compact representation of the solutions based on that order relation. B. Suppose the swap option j belongs to a node vpj . Now it is important to observe that the application of j on Sj -1 to construct Sj , invalidates the application of all other swap options that belong to an OR edge in the path from the root node to vpj in the solution Sj . This is because in Sj the application of any such swap option which belongs to an OR edge in the path from the root node to vpj would make the swap at vpj redundant. In fact, for each swap option i belonging to node vpi , where 1  i  j , the application of all other swap options that belong to an OR edge in the path from the root node to vpi is invalidated in the solution Sj for the same reason. This condition restricts the set of swap options that can be applied on a particular solution. C. Finally, there can be two swap options i and j for 1  i < j  k such that i and j are independent of each other, that is, (a) applying i to Si-1 and subsequently the application of j to Sj -1 , and (b) applying j to Si-1 and subsequently the application of i to Sj -1 , ultimately construct the same solution. This happens only when the original edges of both i and j are present in Si-1 , thus application of one swap option does not influence the application of the other. However, it is desirable to use only one way to generate solution Sj . In Section 3.3, we propose a variation of the top-down approach (called LASG) which resolves this issue. ^ ] We define an order relation, namely R ^ , between a pair Definition 3.f [Order Relation R of swap options as follows. ^ , where ei and er are OR edges, qi and rj are a. If there is a path from vi to vr in T ^ ^. swap options, then (qi , rj )  R. For example, in Figure 4 ((3,4) , (13,14) )  R b. If pq = ep , eq , pq and rt = er , et , rt are two swap options such that vq = vr , ^ . In Figure 4 ((2,3) , (3,4) )  R ^. then (pq , rt )  R   Implicit Representation of the Solutions : We use an implicit representation for storing every solution other than the optimal one. These other solutions can be constructed from the optimal solution by applying a set of swap options to the optimal solution in the ^ , i has to be applied before j . Therefore, every solution is following way. If (i , j )  R ^ ^ if (i , j )  R ^. represented as a sequence  of swap options, where i appears before j in  Intuitively the application of every swap option specifies that the swapped edge will be the ^ , it may so part of the solution. Since the swap options are applied in the specific order R happen that an OR edge which had become the part of solution due to the application of an earlier swap option and may get swapped out due to the application of a later swap option.
286

Generating Ordered Solutions for Explicit AND/OR Structures

^ = Definition 3.g [Superfluous Swap Option] Consider a sequence of swap options  1 , · · · , m corresponding to a solution Sm . Clearly it is possible for a swap option, i , where 1  i  m, to be present in the sequence such that the original edge of i is not present in the solution Si-1 which is constructed by the successive applications of swap options 1 , · · · , i-1 to solution Sopt . Now the application of i has no effect on Si-1 , i.e., solution Si is identical to solution Si-1 . Each such swap option i is a superfluous swap ^ of swap options corresponding to solution Sm . option with respect to the sequence    Property 3.1 The sequence of swap options corresponding to a solution is minimal, if it has no superfluous swap option. This property follows from the definition of superfluous swap options and the notion of the implicit representation of a solution. Definition 3.h [Signature of a Solution] The minimal sequence of swap options corresponding to a solution, Sm , is defined as the signature, Sig(Sm ), of that solution. It ^ , may be noted that for the optimal solution Sopt of any alternating AND/OR tree T Sig(Sopt ) = {}, i.e., an empty sequence. It is possible to construct more than one signature ^ is a partial order. It is important to observe that all different signatures for a solution, as R for a particular solution are of equal length and the sets of swap options corresponding to these different signatures are also equal. Therefore the set of swap options corresponding to a signature is a canonical representation of the signature. Henceforth we will use the set notation for describing the signature of a solution.
2, 39

v1

3

2,3 : 5
2

3,4 : 1

1

3, 29

v2

v3
35

2, 37

v4

5

1

4

3

2, 8

v5

v6
12

3, 11

v7

4, 17

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 5: A solution, S2 , of the AND/OR tree shown in Figure 4 In Figure 5 we show a solution, say S2 , of the AND/OR tree shown in Figure 4. The solution is highlighted using thick dashed lines with arrow head. The pair, cv (vq ), C (S2 , vq ),
287

Ghosh, Sharma, Chakrabarti, & Dasgupta

is shown within rectangles beside each node vq in solution S2 , and we have used the rectangles with rounded corner whenever C (S2 , vq ) = Copt (vq ). Since S2 is generated by applying the swap option (2,3) to solution Sopt , the signature of S2 , Sig(S2 ) = (2,3) . Consider ^ 2 = (2,3) , (9,10) , of swap options. It is worth noting that  ^ 2 also another sequence,  ^ 2 , namely 9,10 , can not be represents the solution S2 . Here the second swap option in  applied to the solution constructed by applying (2,3) to Sopt as the source edge of (9,10) , ^2 . e9 , is not present in that solution. Hence (9,10) is a superfluous swap option for  Definition 3.i [Vopt and Eopt ] For any solution graph Sm of an AND/OR DAG G , we define a set of nodes, Vopt (Sm ), and a set of OR edges, Eopt (Sm ), as: a. Vopt (Sm ) = vq vq in Sm and solution graph Sm (vq ) is identical to the solution graph opt(vq ) b. Eopt (Sm ) = epr OR edge epr in Sm , and vr  Vopt (Sm ) Clearly, for any node vq  Vopt (Sm ), if vq is present in Sopt , then ­ (a) the solution graph Sm (vq ) is identical to the solution graph Sopt (vq ), and (b) C (Sm , vq ) = Copt (vq )   Definition 3.j [Swap List] The swap list corresponding to a solution Sm , L(Sm ), is the list of swap options that are applicable to Sm . Let Sig(Sm ) = {1 , · · · , m } and i, 1  i  m, each swap option i belongs to node vpi . The application of all other swap options that belong to the OR edges in the path from the root node to vpi is invalidated in the solution Sm . Hence, only the remaining swap options that are not invalidated in Sm can be applied to Sm for constructing the successor solutions of Sm . It is important to observe that for a swap option i , if the source edge of i belongs to Eopt (Sm ), the application is not invalidated in Sm . Hence, for a solution Sm , we construct L(Sm ) by restricting the swap operations only on the edges belonging to Eopt (Sm ). Moreover, this condition also ensures that the cost of a newly constructed solution can be computed directly form the cost of the parent solution and the  value of the applied swap  is constructed form S option. To elaborate, suppose solution Sm m by applying jk . The   ) = C (S ) +  cost of Sm can be computed directly form C (Sm ) and jk as : C (Sm m jk if ej  Eopt (Sm ). Procedure ComputeSwapList(Sm ) describes the details of computing swap options for a given solution Sm .   Procedure ComputeSwapList(Sm)
1 2 3

4 5 6

L(Sm )  ; Compute Eopt (Sm ); foreach OR edge ec in Eopt (Sm ) do if there exists a swap option on edge ec then /* Suppose ec emanates from OR node vq such that ec = L(vq )[i]. Also ec is marked with the pair tmp , en , where en = L(vq )[i + 1] */ cn  ec , en , tmp ; Add cn to L(Sm ); end end

The swap list of the optimal solution, L(Sopt ), in Figure 4, is {(2,3) , (9,10) }. In the solution S1 , shown in Figure 6, Vopt = {v6 , v10 }, because except node v6 and v10 , for all other nodes vi in S1 , opt(vi ) = S1 (vi ). Here also rectangles with rounded corner are used when C (S1 , vq ) = Copt (vq ). Therefore, Eopt = {e6 , e10 }. Since there exists no swap option
288

Generating Ordered Solutions for Explicit AND/OR Structures

2, 37

v1

3

2,3 : 5
2

3,4 : 1

1

3, 32

v2

v3
35

2, 37

v4

5

1

4

3

2, 11

v5

v6
12

3, 11

v7

4, 17

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 6: A solution, S1 , of the AND/OR tree shown in Figure 4 on the OR edges, e6 and e10 , the swap list of solution S1 , L(S1 ) = . Hence, for a solution Sm , L(Sm ) may be empty, though Vopt (Sm ) can never be empty. Although we use the notation ij to denote a swap option with edge ei as the original edge and edge ej as the swapped edge, for succinct representation, we also use  with a single subscript, such as 3 , k , ij etc., to represent a swap option. This alternative representation of swap options does not relate to any edge. Definition 3.k [Successors and Predecessors of a Solution] The set of successors and predecessors of a solution Sm is defined as:   can be constructed from S a. Succ(Sm ) = {Sm Sm m by applying a swap option that belongs to the swap list of Sm }  )}    Sm  Succ(Sm b. P red(Sm ) = {Sm ^ the following stateProperty 3.2 For any solution Sm of an alternating AND/OR tree T   ment holds: Sm  P red(Sm ), C (Sm )  C (Sm ) The property follows from the definitions. One special case requires attention. Consider  ) = C (S ) and S   P red(S ). This case can only arise when a swap the case when C (Sm m m m option of cost 0 is applied to Sm . This occurs in the case of a tie. 3.2.1 ASG Algorithm We present ASG, a best first search algorithm, for generating solutions for an alternating AND/OR tree in non-decreasing order of costs. The overall idea of this algorithm is as follows. We maintain a list, Open, which initially contains only the optimal solution Sopt . At any point of time Open contains a set of candidate solutions from which the next best
289

Ghosh, Sharma, Chakrabarti, & Dasgupta

solution in the non-decreasing order of cost is selected. At each iteration the minimum cost solution (Smin ) in Open is removed from Open and added to another list, named, Closed. The Closed list contains the set of ordered solutions generated so far. Then the successor set of Smin is constructed and any successor solution which is not currently present in Open as well as is not already added to Closed is inserted to Open. However as a further optimization, we use a sublist of Closed, named TList, to store the relevant portion of Closed such that checking with respect to the solutions in TList is sufficient to figure out whether the successor solution is already added to Closed. It is interesting to observe that this algorithm can be interrupted at any time and the set of ordered solutions computed so far can be obtained. Also, the algorithm can be resumed if some more solutions are needed. The details of ASG algorithm are presented in Algorithm 4. Algorithm 4: Alternative Solution Generation (ASG) Algorithm ^ input : An alternating AND/OR tree T ^ in the non-decreasing order of cost output: Alternative solutions of T 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create three lists, Open, Closed, and TList, that are initially empty; 3 Put Sopt in Open; 4 lastSolCost  C (Sopt ); 5 while Open is not empty do 6 Smin  Remove the minimum cost solution from Open ; 7 if lastSolCost < C (Smin ) then 8 Remove all the elements of TList; 9 lastSolCost  C (Smin ); 10 end 11 Add Smin to Closed and TList; 12 Compute the swap list, L(Smin ), of Smin ; /* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open */ 13 foreach ij  L(Smin ) do 14 Construct Sm by applying ij to Smin ; 15 Construct the signature of Sm , Sig(Sm ), by concatenating ij after Sig(Smin ); /* Check whether Sm is already present in Open or in TList */ 16 if (Sm not in Open) and (Sm not in TList) then 17 Add Sm to Open; 18 end 19 end 20 Report the solutions in Closed; The pseudo-code from Line-1 to Line-4 computes the optimal solution Sopt , performs the marking of OR edges, populates the swap options, and initializes Open, Closed and TList. The loop in Line-10 is responsible for generating a new solution every time it is executed as long as Open is not empty. In Line-6 of the ASG algorithm, the solution that is the current minimum cost solution in Open (Smin ) is selected and removed from Open. The TList is populated and maintained from Line-7 to Line-10. The loop in Line-13 generates

290

Generating Ordered Solutions for Explicit AND/OR Structures

the successor solutions of Smin one by one and adds the newly constructed solutions to Open if the newly constructed solution is not already present in Open as well as not added to TList (Line-16 does the checking). The proof of correctness of Algorithm 4 is presented in Appendix A. We discuss the following issues related to Algorithm 4. Checking for Duplication : In order to check whether a particular solution Si is already present in Open or TList, the signature of Si is matched with the signatures of the solutions that are already present in Open and TList. It is sufficient to check the equality between the set of swap options in the respective signatures because that set is unique for a particular solution. It may be noted that TList is used as an optimization, which avoids searching the entire Closed list. Resolving Ties : While removing the minimum cost solution from the Open list, a tie may be encountered among a set of solutions. Suppose there is a tie among the set Stie = {S1 , · · · , Sk }. The ties are resolved in the favor of the predecessor solutions, that is, Si , Sj  Stie , (If Si is the predecessor of Sj )  (Si is removed before Sj ) For all other cases the ties are resolved arbitrarily in the favor of the solution which was added to Open first. 3.2.2 Working of ASG Algorithm We illustrate the working of the ASG algorithm on the example AND/OR tree shown in Figure 4. The contents of the different lists obtained after first few iterations of outermost while loop are shown in Table 1. We use the signature of a solution for representation purpose. The solutions that are already present in Open and also constructed by expanding the current Smin , are highlighted with under-braces.
It. 1 2 3 4 5 Smin {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } L(Smin ) (2,3) , (9,10)  (3,4) (11,12) , (13,14) (11,12) , (14,15) Open {(2,3) }, {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) } {(2,3) , (3,4) , (13,14) , (11,12) }, {(2,3) , (3,4) , (13,14) , (14,15) } Closed {} {}, {(9,10) } {}, {(9,10) }, {(2,3) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {}, {(9,10) }, {(2,3) }, TList {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4),

6

(13,14)

7

{(2,3) , (3,4) , (13,14) , (11,12) }

(14,15)

{(2,3) , (3,4) } (11,12) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) , {(2,3) , (3,4) , (13,14) , {(2,3) , (3,4) } (13,14) , (11,12) } (11,12) , (14,15) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (11,12) }

Table 1: Working of ASG Algorithm
291

Ghosh, Sharma, Chakrabarti, & Dasgupta

Before entering the outermost while loop (Line 5), ASG computes the optimal solution Sopt , populates the swap options, and inserts Sopt to Open. Thus, at this point of time, Open contains only the optimal solution Sopt ; Closed and TList are empty. In the first iteration Sopt (the signature of Sopt is {}) is selected and removed from Open. Then the swap list of Sopt , L(Sopt ), is computed. L(Sopt ), consists of two swap options, namely (2,3) and (9,10) . ASG adds two new solutions {(2,3) } and {(9,10) } to Open. Then solution Sopt is added to both Closed and TList. In the next iteration, solution {(9,10) } which has the minimum cost among the solutions currently in Open, is selected and removed from Open, the swap list {(9,10) } is computed and subsequently {(9,10) } is added to Open and TList. As it happens, L({(9,10) }) =  (owing to the fact that Eopt = {e6 , e10 } and there exists no swap option on the OR edges, e6 and e10 ), thus nothing else happens in this iteration. In the next iteration, solution {(2,3) } is removed from Open and ultimately solution {(2,3) , (3,4) } is added to Open after adding {(2,3) } to Closed as well as to TList. Next two iterations proceed in a similar fashion. Now, consider the 6th iteration. In this iteration, solution {(2,3) , (3,4) , (11,12) } is removed from Open, and its successor set has only one solution, {(2,3) , (3,4) , (11,12) , (13,14) }, which is already present in Open (inserted to Open in Iteration-5). Therefore, solution {(2,3) , (3,4) , (11,12) , (13,14) } is not inserted to Open again. We have shown up to Iteration-7 in Table 1. 3.3 Technique for Avoiding the Checking for Duplicates in Open In this section, we present a technique to avoid the checking done before adding a newly constructed solution Sm to Open to determine whether Sm is already present in Open. We first explain the scenario with an example, which is a portion of the previous example shown in Figure 4. In Figure 7-10, the solutions are shown using thick dashed line with arrow head. Also the rectangles with rounded corner are used to highlight the fact that the corresponding node in the marked solution does not belong to the Vopt set of that solution.
2, 37

v4
3 4

2, 44

v4
3

4

3, 11

v7
3 1

4, 17

v8
1 2

3, 15

v7
3 1

4, 20

v8
1

2 11,12 : 4

13,14 : 3

v11
6

v12
9

v13
12

v14
15

v11
6

v12
9

v13
12

v14
15

Figure 7: Running Example

Figure 8: Solution S3

Consider the solutions S1 , S2 and S3 (shown in Figure 9, Figure 10 and Figure 8). Here (a) L(Sopt ) = {(11,12) , (13,14) }, (b) Succ(Sopt ) = {S1 , S2 }, (c) Sig(S1 ) = {(13,14) }, (d) Sig(S2 ) = {(11,12) }, and (e) Sig(S3 ) = {(13,14) , (11,12) }. Algorithm 4 constructs the solution S3 (shown in Figure 8) for adding to Open twice ­ (i) as a part of adding Succ(S1 ) to Open, and (ii) while adding Succ(S2 ) to Open.
292

Generating Ordered Solutions for Explicit AND/OR Structures

2, 40

v4
3 4

2, 41

v4
3

4

3, 11

v7
3 1

4, 20

v8
1 2

3, 15

v7
3 1

4, 17

v8
1

2 11,12 : 4

13,14 : 3

v11
6

v12
9

v13
12

v14
15

v11
6

v12
9

v13
12

v14
15

Figure 9: Solution S1

Figure 10: Solution S2

Clearly Sopt is the root node of G s .

We use the following definitions to describe another version of the ASG algorithm, which constructs the solutions in such a way that the check to find out whether a solution is already added to Open is avoided. Definition 3.l [Solution Space DAG(SSDAG)] The solution space DAG of an alternating ^ is a directed acyclic graph (DAG), G s = V , E , where V is the set of all AND/OR tree T ^ , and E is the set of edges which is defined as: possible solutions of the AND/OR tree T   Sp , Sm  V , and   s E = es pm epm is a directed edge from node Sp to Sm , and   Sm  Succ(Sp )

 

Definition 3.m [Solution Space Tree and Completeness] A solution space tree of an ^ is a tree T s = V t , E t where V t  V , where V is the set of alternating AND/OR tree T ^ , and E t is the set of edges which is defined all possible solutions of the AND/OR tree T as:   Sp , Sm  V t , and       s e is a directed edge from node S to S , and p m s t pm E = epm Sp  P red(Sm ), and         P red(S ), (S = S  )  there is no edge between S  and S .  Sp m m p p p The sibling set for a solution Sm , is denoted using Sib(T s , Sm ). A solution space tree T s for an AND/OR tree is complete if V t = V .   It may be noted that the complete solution space tree of an alternating AND/OR tree is not necessarily unique. It is possible for an alternating AND/OR tree to have more than one complete solution space tree. However the solution space DAG for any AND/OR tree is unique. Definition 3.n [Native Swap Options of a Solution] Consider a solution Sm of an al^ . Suppose Sm is constructed by applying swap option ij to ternating AND/OR tree T solution Sp . Since swap option ij = ei , ej , ij is used to construct Sm , AND node vj is present in Sm . The native swap options of solution Sm with respect to swap option ij , N (Sm , ij ), is a subset of L(Sm ), and comprises of the following swap options :
293

Ghosh, Sharma, Chakrabarti, & Dasgupta

2, 49

v1

3

2,3 : 5
2

3,4 : 1

1

3, 32

v2

v3
35

2, 43

v4

5

1

4

3

2, 11

v5

v6
12

3, 11

v7

4, 23

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 11: A solution, S4 , of the AND/OR tree shown in Figure 4 a. jk , where jk is the swap option on the edge ej b. each t , if t belongs to an OR node vq where vq is a node in Sm (vj ) We use the term N (Sm ) to denote the native swap options when ij is understood from the context. Intuitively the native swap options for solution Sm are the swap options that become available immediately after applying ij , but were not available in the predecessor solution of Sm .  

Consider the solution S4 shown in Figure 11 where Sig(S4 ) = {(2,3) , (3,4) , (13,14) }. The solution is highlighted using thick dashed lines with arrow head. We have used the rectangles with rounded corner beside each node vq in solution S4 , where C (S4 , vq ) = Copt (vq ). Suppose S4 is constructed form solution S3 (where Sig(S3 ) = {(2,3) , (3,4) }) using swap option (13,14) . Here N (S4 , (13,14) ) = {(14,15) } whereas L(S4 ) = {(11,12) , (14,15) }. Now consider solution S6 where Sig(S6 ) = {(2,3) , (3,4) , (11,12) , (13,14) ). It is worth observing that applying only the native swap options to S4 instead of all swap options in L(S4 ) prevents the construction of solution S6 from solution S4 . S6 can also be constructed by applying (13,14) to solution S5 , where Sig(S5 ) = {(2,3) , (3,4) , (11,12) }. However, it may be noted that (13,14) is not a native swap option of solution S5 . 3.3.1 Lazy ASG Algorithm The intuition behind the other version of the ASG algorithm is as follows. For a newly constructed solution Sm , we need to check whether Sm is already present in Open because Sm can be constructed as a part of computing the successor set of multiple solutions. Instead of using the entire swap list of a solution to construct all successors at once and then add those solutions to Open, using the native swap options for constructing a subset of the successor set ensures the following. The subset constructed using native swap options
294

Generating Ordered Solutions for Explicit AND/OR Structures

consists of only those solutions that are currently not present in Open and thus can be added to Open without comparing with the existing entries in Open. The construction of  of S each remaining successor solution Sm m and then insertion to Open is delayed until  is added to Closed. every other predecessor solution of Sm Algorithm 5: Lazy ASG (LASG) Algorithm ^ input : An alternating AND/OR tree T ^ in the non-decreasing order of cost output: Alternative solutions of T 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create two lists, Open and Closed, that are initially empty; 3 Put Sopt in the Closed list; 4 Create a solution space tree T s with Sopt as root; 5 Compute the swap list, L(Sopt ), of Sopt ; 6 Construct Succ(Sopt ) using L(Sopt ); 7 forall Sm  Succ(Sopt ) do 8 Add Sm to Open; 9 end 10 while Open is not empty do 11 Smin  Remove the minimum cost solution from Open ; /* Suppose Smin is constructed from Sm applying swap option ij */ 12 Add a node corresponding to Smin in T s and connect that node using an edge from Sm ; 13 Compute the swap list L(Smin ) and the list of native swap options N (Smin , ij ); /* Expansion using native swap options */ 14 foreach tmp  N (Smin , ij ) do 15 Construct Stmp from Smin by applying tmp ; 16 Construct the signature of Stmp , Sig(Stmp ), by concatenating tmp after Sig(Smin ); 17 Add Stmp to Open; 18 end /* Lazy Expansion */ s 19 forall Sp  Sib(T , Smin ) do 20 if ij  L(Sp ) then  from S using  ; Construct Sp 21 p ij  , Sig (S  ), by concatenating  after Sig (S ); Construct the signature of Sp 22 ij p p  Add Sp to Open; 23 24 end 25 end 26 Add Smin to Closed; 27 end 28 Report the solutions in Closed; The solution space tree T s is maintained throughout the course of the algorithm to  is added to Closed. Based on this idea we determine when every other predecessor of Sm
295

Ghosh, Sharma, Chakrabarti, & Dasgupta

present a lazy version of ASG algorithm, named LASG. After selecting the minimum cost solution from Open, the algorithm explores the successor set of the current minimum cost solution in a lazy fashion. For a solution Sm , at first a subset of Succ(Sm ) is constructed using only the native swap options of Sm . The other solutions that belong to Succ(Sm ) are explored as late as possible as described above. For resolving ties, LASG algorithm uses the same strategy which is used by ASG algorithm. The details of LASG algorithm are presented in Algorithm 5. The proof of correctness of this algorithm is presented in Appendix B. Consider the example tree shown in Figure 7 and solutions S1 and S2 (shown in Figure 9 and Figure 10). Initially the Open will contain only Sopt and N (Sopt ) = {(11,12) , (13,14) }. When Sopt is selected from Open, both S1 and S2 is added to Open. Next S1 will be selected followed by S2 . Since, N (S1 ) =  and N (S2 ) = , after selecting S1 or S2 no successor solutions are constructed using the native swap list. Among the predecessors of S3 , S2 is added last to Closed. After selecting and removing S2 from Open, solution S3 is constructed from the previously selected predecessor S1 using the swap option (11,12) which is used to construct solution S2 from Sopt . 3.3.2 Working of LASG Algorithm (on AND/OR tree in Figure 4) Before entering the outermost while loop (Algorithm 5, Line 10), LASG computes the optimal solution Sopt and constructs Succ(Sopt ). Then the solutions in Succ(Sopt ) are added to Open and the contents of the Open becomes {(2,3) }, {(9,10) } . The contents of the different lists when a solution is added to Closed are shown in Table 2. The solutions are represented using their signatures. The solutions that are added to Open as a result of lazy expansion, are highlighted using under-brace.
Iteration 1 2 3 4 Smin {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } N (Smin ) (2,3) , (9,10)  (3,4) (11,12) , (13,14) (14,15) Closed {} {}, {(9,10) } {}, {(9,10) }, {(2,3) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) , (13,14) , (11,12) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (11,12) } Open {(2,3) }, {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) , (14,15) }

5

{(2,3) , (3,4) , (11,12) }



6

{(2,3) , (3,4) , (13,14) , (11,12) }



Table 2: Working of LASG Algorithm While generating the first four solutions, the contents of the different lists for LASG are identical to the contents of the corresponding lists of ASG (shown in Table 1). For
296

Generating Ordered Solutions for Explicit AND/OR Structures

each of these soltuions, the native swap list is equal to the actual swap list of that solution. It is worth noting that, unlike ASG, for LASG the outermost while loop starts after generating the optimal solution Sopt , thus while generating the same solution the iteration number for LASG is less than that of ASG by 1. In the 4th iteration, for solution S4 = {(2,3) , (3,4) , (13,14) } the native swap list is not equal to the swap list as described previously. The same holds true for solution S5 = {(2,3) , (3,4) , (11,12) } and solution S6 = {(2,3) , (3,4) , (13,14) , (11,12) }. It is important to observe that LASG adds the solution S6 = {(2,3) , (3,4) , (13,14) , (11,12) } to Open after the generation of solution S5 = {(2,3) , (3,4) , (11,12) } as a part of lazy expansion (highlighted using under-brace in Table 2). Whereas, the ASG algorithm adds S6 to Open after generating solution S4 = {(2,3) , (3,4) , (13,14) }. 3.4 Complexity Analysis and Comparison among ASG, LASG and BU In this section we present a complexity analysis of ASG and LASG and compare them with BU. We will use the following parameters in the analysis. a. n and n denote the total number of nodes and the number of OR nodes in an alternating AND/OR tree. b. d denotes the out degree of the OR node having maximum number of children. c. m denotes the maximum number of OR edges in a solution. d. o denotes the maximum size of Open. We will present the complexity analysis for generating c solutions. Therefore the size of Closed is O(c). 3.4.1 Complexity of ASG Time Complexity : The time complexity of the major steps of Algorithm 4 are as follows. a. Computing the first solution can be done in bottom-up fashion, thus requiring O(n ) steps. The edges emanating from an OR node are sorted in the non-decreasing order of aggregated cost to compute the marks of the OR edges, the marking process takes O n .d. log d . Since the value of d is not very large in general (can be upper bounded by a constant), O n .d. log d = O(n ). b. The number of swap options available to a solution can be at most equal to the number of OR edges in that solution. Thus, the swap list for every solution can be built in O(m) time. For c solutions, generating swap options take O(c.m). c. Since the size of the successor set of a solution can be m at most, the size of Open, o can at most be c.m. Also the size of the TList can at most be equal to c (the size of Closed). d. The Open list can be implemented using Fibonacci heap. Individual insert and delete operation on Open take O(1)(amortized) and O(lg o) time respectively. Hence, for inserting in the Open and deleting from Open altogether takes O(o. lg o) time which is O(c.m. log(c.m)). e. The checking for duplicates requires scanning the entire Open and TList. Since the length of TList can be at most c, for a newly constructed solution this checking takes O(c + o) time and at most O(c + o) solutions are generated. Since O(c + o) is actually O(o), for generating c solutions, this step takes O(o)2 time. Also, the maximum value
297

Ghosh, Sharma, Chakrabarti, & Dasgupta

of o can be O(c.m). Thus, the time complexity of this step is O(c.m)2 . Clearly this step dominates O(o. lg o) which is the total time taken for all insertions into the Open and deletions from Open. However, this time bound can be further improved if we maintain a hash map of the solutions in the Open and TList, and in this case the checking for duplicates can be done in O(o) time. In that case O(o. lg o) (total time taken for all insertions into the Open and deletions from Open) becomes dominant over the time required for checking for duplicates. f. An upper limit estimate of m could be made by estimating the size of a solution tree  which is n for regular and complete alternating AND/OR trees. It is important to observe that the value of m is independent of the average out degree of a node in ^ . T Combining the above factors together we get the time complexity of ASG algorithm as : O n + o2 = O n + (c.m)2 = O n + c2 .n = O(c2 .n ) However if the additional hash map is used the time complexity is further reduced to :   O n + o. lg o = O n + c. n . lg(c.n ) = O n + n .(c. lg c + c. lg n ) Space Complexity: The following data-structures primarily contribute to the space complexity of ASG algorithm. a. Three lists, namely, Open, Closed, and TList are maintained throughout the course of the running ASG. This contributes a O(o + c) factor, which is O(o). b. Since the number of swap options is upper bounded by the total number of OR edges, constructing the swap list contributes the factor, O(n .d) to the space complexity. Also marking a solution requires putting a mark at every OR node of the AND/OR tree, thus adding another O(n ) space which is clearly dominated by the previous O(n .d) factor. c. Since the signature of a solution is essentially a set of swap options, the size of a signature is upper bounded by the total number of swap options available. Combining the Open and Closed list, altogether (c + o) solutions need to be stored. Since (c + o) is O(o), total space required for storing the solutions is O o.n .d . Combining the above factors together we get the space complexity of ASG algorithm as : O o + n .d + o.n .d = O(o.n .d) When an additional hash map is used to improve the time complexity, another additional O o.n .d space is required for maintaining the hash map. Although the exact space requirement is doubled, asymptotically the space complexity remains same. 3.4.2 Complexity of LASG Time Complexity : Compared to Algorithm 4, Algorithm 5 does not check for the duplicates and adds the solution to Open only when it is required. Therefore the other terms in the complexity remain the same except the term corresponding to the checking for duplicates. However, here T s is created and maintained during the course of Algorithm 5. Creating and maintaining the tree require O(c) time. Also during the lazy expansion the swap list of the previously generated sibling solutions are searched (Line 19 and Line 20 of Algorithm 5). The size of the swap list of any solution is O(m), where m is the maximum number of OR edges in a solution. Also there can be at most O(m) sibling solutions for a
298

Generating Ordered Solutions for Explicit AND/OR Structures

solution. Therefore the complexity of the lazy expansion is O(c.m2 ). Since O(c.m2 ) is the dominant factor, the time complexity of LASG is O(c.m2 ) = O(c.n ). Space Complexity : Compared to ASG algorithm, LASG algorithm does not maintain the TList. However LASG maintains the solution space tree T s whose size is equal to the Closed list, thus adding another O(c) factor to the space complexity incurred by ASG algorithm. It is interesting to observe that the worst case space complexity remains O(o + n .d + o.n .d) = O(o.n .d) which is equal to the space complexity of ASG algorithm. 3.4.3 Comparison with BU The time complexity of generating the c best solutions for an AND/OR tree is O(n .c. log c) and the space complexity is O(n .c). The detailed analysis can be found in the work of Elliott (2007). Since, n .d = O(n ), the space complexity of both ASG and LASG algorithm reduces to O(n .c) and the time complexity of LASG is log c factor better than BU whereas the time complexity of ASG is quadratic with respect to c compared to the (c. log c) factor of BU. When an additional hash-map is used to reduce the time overhead of duplicate checking, ASG beats both LASG and BU both in terms time complexity, as both  O(n ) and O n .(c. lg c + c. lg n ) is asymptotically lower than O(n .c. log c). However this worst case complexity is only possible for AND/OR trees where no duplicate solution is generated. Empirical results show that the length of Open, o hardly reaches O(c.m).

4. Ordered Solution Generation for AND/OR DAGs
In this section, we present the problem of generating solutions in non-decreasing order of cost for a given AND/OR DAG. We present the working of the existing algorithm for generating solution for both tree based semantics and default semantics. Next we present the modifications in ASG and LASG for handling DAG. 4.1 Existing Bottom-Up Algorithm Figure 12 shows an example working of the existing bottom-up approach, BU, on the AND/OR DAG in Figure 2. We use the notations that are used in Figure 3 to describe different solutions in Figure 12 and the generation of the top 2 solutions under tree-based semantics is shown. It is important to notice that although BU correctly generates alternative solutions of an AND/OR DAGs under tree based semantics, BU may generate some solutions which are invalid under default semantics. In Figure 13 we present a solution of the AND/OR DAG in Figure 2. This solution is an example of such a solution which is correct under tree-based semantics but is invalid under default semantics. The solution DAG (highlighted using thick dashed lines with arrow heads) in Figure 13 will be generated as the 3rd solution of the AND/OR DAG in Figure 2 while running BU. At every non-terminal node, the entry (within rectangle) corresponding to the 3rd solution is highlighted using bold face. It may be noted that the terminal nodes, v9 and v10 , are included in the solution DAG though both of them emanate from the same parent OR node. Therefore, this solution is not a valid one under default semantics.
299

Ghosh, Sharma, Chakrabarti, & Dasgupta

2, 89 1

v1

v2 v3 1 : |1, 1|, 89 2 : |2, 1|, 90 2 1 1 : v5 , 1 , 41 2 : v5 , 2 , 44 1 v7 v8 1 : |1, 1|, 35 2 : |2, 1|, 38 3 v8 17 1 3, 9 v7

2, 89

v1

v2 v3 1 : |1, 1|, 89 2 : |2, 1|, 90 3 : |1, 2|, 92 2

3, 43 1

v2

1 : v5 , 1 , 43 2 : v4 , 1 , 44 5 4

2, 41

v3

3, 43 1

v2

1 : v5 , 1 , 43 2 : v4 , 1 , 44 5 4

2, 41

v3

1 : v5 , 1 , 41 2 : v5 , 2 , 44 1

v4 40

2, 35

v5

v6 52

v4 40

2, 35

v5

v7 v8 1 : |1, 1|, 35 2 : |2, 1|, 38 3 v8 17

v6 52

4 3, 9 1 v7 1 : v9 , 1 , 9 2 : v10 , 1 , 12 2

4 1 : v2 , 1 , 34 2 : v2 , 2 , 37 2

v9 5

v10 7

v9 5

v10 7

Figure 12: BU approach for AND/OR DAG

Figure 13: A solution (tree based semantics)

Proposed Extension of BU to Generate Alternative Solutions under Default Semantics : We propose a simple top-down traversal and pruning based extension of BU to generate alternative solutions under default semantics. While generating the ordered solutions at any AND node vq by combining the solutions of the children, we do the following. For each newly constructed solution rooted at vq , a top-down traversal of that solution starting from vq is done to check whether more than two edges of an OR node are present in that particular solution (a violation of the default semantics ). If such a violation of the default semantics is detected, that solution is pruned from the list of alternative solutions rooted at vq . Therefore, at every AND node, when a new solution is constructed, an additional top-down traversal is used to detect the semantics violation. 4.2 Top-Down Method for DAGs The proposed top-down approaches (ASG and LASG) are also applicable for AND/OR DAGs to generate alternative solution DAGs under default semantics. Only the method of computing the cost increment after the application of a swap option needs to be modified to incorporate the fact that an OR node may be included in a solution DAG through multiple paths from the root node. We use the notion of participation count for computing the cost increment. Participation Count : The notion of participation count is applicable to the intermediate nodes of a solution DAG as follows. In a solution DAG, the participation count of an intermediate node, vq , is the total number of distinct paths connecting the root node, vR , and vq . For example, in Figure 14, the optimal solution DAG is shown using thick dashed lines with arrow heads, and the participation count for every intermediate OR nodes are shown within a circle beside the node.
300

Generating Ordered Solutions for Explicit AND/OR Structures

v1
1
1

2, 89 2
1

v1
1

2, 90

2
3, 44 1

v2

3, 43 5 4

v3

2, 41 1 1

1

v2

v3

2, 41 1

1

5
1

4

2,5,4 : 1

3,5,6 : 14
2

3,5,6 : 14

v4
40

v5

2, 35 3

v6
52

v4
40

v5

2, 35 3

v6
52

4
2

4
1

v7

3, 9 2

v8
17 1

v7

3, 9 2

v8

17

1

7,9,10 : 3

7,9,10 : 3

v9
5

v10
7

v9
5

v10
7

Figure 14: AND/OR DAG

Figure 15: Solution DAG S1

We use the notation ijk to denote a swap option in the context of AND/OR DAGs, where swap option ijk belongs to node vi , the source edge of the swap option is eij from node vi to node vj , and the destination edge is eik from node vi to node vk . 4.2.1 Modification in the Proposed Top-Down Approach The ASG algorithm is modified for handling AND/OR DAGs in the following way. The computation of the successor solution in Line 14 of Algorithm 4 is modified to incorporate the participation count of the OR node to which the applied swap option belongs. The overall method is shown in Algorithm 6(in the next page). In order to apply LASG on AND/OR DAGs, apart from using the above mentioned modification for computing the cost of a newly generated solution, another modification is needed for computing the native swap options for a given solution. The modification is explained with an example. Consider the solution, S1 , shown in Figure 15. S1 is highlighted using thick dashed lines with arrow heads. The pair, cv (vq ), C (S1 , vq ), is shown within rectangles beside each node vq ; rectangles with rounded corner are used when C (S1 , vq ) = Copt (vq ). Swap option (2,5,4) was applied to Sopt to generate S1 . After the application of swap option (2,5,4) , the participation count of node v5 is decremented to 1. Therefore in S1 there is a path from the root node to node v5 and so node v5 is still present in S1 . As a result, the swap option (7,9,10) is available to S1 with a participation count equal to 1 for node v7 , whereas (7,9,10) is available to its parent solution Sopt with participation count 2 for node v7 . In other words, (7,9,10) is not available to S1 and its parent solution Sopt with the same value of participation count for node v7 . Therefore (7,9,10) becomes the native swap option of S1 . The generalized definition of native swap options for a solution is presented below. Definition 4.o [Native Swap Options of a Solution] Consider a solution Sm of an AND/OR DAG G , where Sm is constructed by applying swap option hij to solution Sp . Since swap option hij = ehi , ehj , hij is used to construct Sm , AND node vj belongs
301

Ghosh, Sharma, Chakrabarti, & Dasgupta

to Sm . Similarly, if the participation count of node vi remains greater than zero after applying hij to Sm , node vi belongs to Sm . The native swap options of solution Sm with respect to swap option hij , N (Sm , hij ), a subset of L(Sm ), comprises of the following swap options : a. hjk , where hjk is the swap option on the edge ehj b. each t , if t belongs to an OR node vq where vq is a node in Sm (vj )  , if node v is present in S  c. each t i m and t belongs to an OR node vq where vq is a node in Sm (vi ). We use the term N (Sm ) to denote the native swap options when hij is understood from the context. Intuitively the native swap options for solution Sm are the swap options that become available immediately after applying hij , but were not available in the predecessor solution of Sm .   Algorithm 6: ASG Algorithm for AND/OR DAGs input : An AND/OR DAG G output: Alternative solutions of G in the non-decreasing order of cost 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create three lists, Open, Closed, and TList, that are initially empty; 3 Put Sopt in Open; 4 lastSolCost  C (Sopt ); 5 while Open is not empty do 6 Smin  Remove the minimum cost solution from Open; 7 if lastSolCost < C (Smin ) then 8 Remove all the elements of TList; 9 lastSolCost  C (Smin ); 10 end 11 Add Smin to Closed and TList; 12 Compute the swap list, L(Smin ), of Smin ; /* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open */ 13 foreach ij  L(Smin ) do 14 Construct Sm by applying ij to Smin ; 15 Construct the signature of Sm , Sig(Sm ), by concatenating ij after Sig(Smin ); 16 Let ij belongs to OR node vq , p is the participation count of vq , and  is the cost increment for ij ; 17 C (Sm ) = C (Sm ) + p × ; /* Check whether Sm is already present in Open or in TList */ 18 if (Sm not in Open) and (Sm not in TList) then 19 Add Sm to Open; 20 end 21 end 22 Report the solutions in Closed; It is worth noting that Definition 4.o of native swap option is a generalization of the earlier definition of native swap option (Definition 3.n), defined in the context of trees. In
302

Generating Ordered Solutions for Explicit AND/OR Structures

the case of trees, the participation count of any node can be at maximum 1. Therefore, after the application of a swap option to a solution, the participation count of the node, to which the original edge of the swap option points to, becomes 0. Therefore the third condition is never applicable for trees. LASG (Algo. 5) can be applied on AND/OR DAGs, with the mentioned modification for computing the cost of a newly generated solution and the general definition of native swap option to generate ordered solutions under default semantics. 4.2.2 Working of ASG and LASG Algorithm on AND/OR DAG We describe the working of ASG algorithm on the example DAG shown in Figure 2. Before entering the outermost while loop, TList and Closed are empty, and Open contains the optimal solution Sopt . The contents of the different lists obtained after first few cycles of outermost while loop are shown in Table 3. Each solution is represented by its signature. The solutions that are already present in Open and also constructed by expanding the current Smin , are highlighted with under-braces. For example, the solution {(2,5,4) , (3,5,6) } which is added to Open in Iteration 2 (while constructing the successor solutions of {(2,5,4) }) constructed again in Iteration 5 while expanding solution {(3,5,6) }.
It. 1 2 L(Smin ) Open (2,5,4) , (3,5,6) , (7,9,10) {(2,5,4) }, {(3,5,6) }, {(7,9,10) } (3,5,6) , (7,9,10) {(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) }, {(2,5,4) , (7,9,10) } 3 {(2,5,4) , (7,9,10) }  {(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) }, Smin {} {(2,5,4) } 4 {(7,9,10) }  {(3,5,6) }, {(2,5,4) , (3,5,6) }, Closed {} {}, {(2,5,4) } {}, {(2,5,4) } {(2,5,4) , (7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) }, {(3,5,6) }

5

{(3,5,6) }

(2,5,4) , (7,9,10)

{(2,5,4) , (3,5,6) }, {(3,5,6) , (7,9,10) }

Table 3: Example Working of ASG Algorithm on the DAG shown in Figure 2 Now we illustrate the working of LASG algorithm on the example DAG shown in Figure 2. The contents of the different lists when a solution is added to Closed are shown in Table 4. It is worth noting that for solution S1 = {2,5,4 }, the swap list L(S1 ) = {(3,5,6) , (7,9,10) } whereas the native swap list N (S1 ) = {(7,9,10) }. The solutions that are added to Open as a result of lazy expansion, are highlighted using under-brace. For example, in Iteration 7 LASG adds the solution S5 = {(2,5,4) , (3,5,6) } to Open after the generation of solution S4 = {3,5,6 } as a part of lazy expansion, whereas the ASG algorithm adds S5 to Open after generating solution S1 = {2,5,4 }. 4.2.3 Generating Solutions under Tree Based Semantics Unlike the default semantics, ASG or LASG does not have any straight forward extension for generating solutions under tree based semantics. In Figure 13 we show an example solution which is valid under tree based semantics, but invalid under default semantics, because both OR edges emanating form the OR node v7 , namely e(7,9) and e(7,10) , are
303

Ghosh, Sharma, Chakrabarti, & Dasgupta

It. 1

N (Smin ) Open (2,5,4) , (3,5,6) , (7,9,10) {(2,5,4) }, {(3,5,6) }, {(7,9,10) } (7,9,10) {(3,5,6) }, {(7,9,10) }, {(3,5,4) , (7,9,10) } 2 {(2,5,4) , (7,9,10) }  {(3,5,6) }, {(7,9,10) }, Smin {} {(2,5,4) } 3 {(7,9,10) }  {(3,5,6) }

Closed {} {}, {(2,5,4) } {}, {(2,5,4) } {(2,5,4) , (7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) }, {(3,5,6) }

4

{(3,5,6) }

(7,9,10)

{(3,5,6) , (7,9,10) }, {(2,5,4) , (3,5,6) }

Table 4: Example Working of LASG Algorithm on the DAG shown in Fugure 2

present in this solution. These two OR edges are included in the solution through two different paths emanating form the root node, v1 . As the existing bottom-up approach stores the alternative solutions at each node in terms of the solutions of the children of that node, this representation allows these different paths to be stored explicitly, thus making BU amenable for generating alternative solutions under tree-based semantics. On the contrary, our approach works top-down using a compact representation (signature) for storing the solutions. In this signature based representation, it is currently not possible to store the fact that a particular OR node is included in the solution through two different paths which may select different child of that OR node. If we use the equivalent tree constructed form the given graph, our compact representation will work correctly, because in that case, each node would be reachable from the root node through at most one path. An AND/OR DAG can be converted to its equivalent AND/OR tree representation using procedure ConvertDAG (described in Section 2) and then ASG or LASG can be applied on the equivalent tree representation in order to generate the alternative solutions correctly under tree-based semantics. However, in the worst case, procedure ConvertDAG incurs a space explosion which will blow up the worst case complexity of both ASG and LASG algorithms. Using our compact representations to generate the ordered solutions under tree-based semantics for a given AND/OR DAG while containing the space explosion such that the worst case complexity of our algorithms remain comparable with BU turns out to be an interesting open problem.

5. Experimental Results and Observations
To obtain an idea of the performance of the proposed algorithms and to compare with the existing approach, we have implemented the ASG, LASG and BU (existing bottom-up approach) and tested on the following test domains. a. A set of synthetically generated AND/OR trees; b. Tower of Hanoi (TOH) problem; c. A set of synthetically generated AND/OR DAGs; d. Matrix-chain multiplication problem; and e. The problem of determining the secondary structure of RNA sequences.
304

Generating Ordered Solutions for Explicit AND/OR Structures

It may noted that in our implementation of the ASG algorithm, we have implemented the more space efficient version of ASG algorithm (without a separate hash-map for storing the solutions in Open and Closed, thereby incurring an extra overhead in time for duplication checking). Another important point is that for every test case the reported running time of ASG and LASG for generating a particular number of solutions includes the time required for constructing the optimal solution graph. The details of the different test domains are as follows. 5.1 Complete Trees We have generated a set of complete d-ary alternating AND/OR trees by varying ­ (a) the degree of the non-terminal nodes (denoted by d), and (b) the height (denoted by h).
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions ASG LASG BU 0.027 0.005 0.004 0.216 0.010 0.015 1.170 0.031 0.068 6.072 0.124 0.257 30.434 0.517 1.180 130.746 2.265 4.952 0.046 0.006 0.005 0.528 0.017 0.037 5.812 0.106 0.343 66.313 1.552 3.973 636.822 12.363 31.043 0.144 0.011 0.033 2.916 0.056 0.573 58.756 1.266 7.698 0.334 0.012 0.081 12.227 0.177 2.066 0.699 0.022 0.161 32.620 0.654 7.464 1.306 0.030 0.287 81.197 1.786 15.892 300 solutions ASG LASG BU 0.086 0.014 0.009 1.448 0.035 0.046 10.098 0.094 0.184 57.757 0.348 0.777 278.453 1.433 3.917 T 6.443 13.277 0.196 0.015 0.018 4.764 0.060 0.153 55.170 0.290 1.733 620.996 3.712 14.323 T 34.150 128.314 1.041 0.025 0.092 25.341 0.181 1.561 544.989 3.327 27.063 2.792 0.036 0.400 102.577 0.443 11.717 5.384 0.071 1.418 288.257 1.566 37.758 12.006 0.092 1.833 785.160 4.284 102.431 500 solutions ASG LASG BU 0.186 0.023 0.020 4.137 0.060 0.097 27.354 0.216 0.407 158.520 0.524 1.641 766.201 2.806 7.257 T 10.306 29.703 0.459 0.026 0.042 10.345 0.088 0.457 156.158 0.494 4.913 T 6.607 33.923 T 55.510 303.785 2.610 0.042 0.123 69.596 0.264 2.107 T 5.172 38.606 7.374 0.062 0.930 283.689 0.827 26.994 15.133 0.134 2.235 832.235 2.594 90.465 29.870 0.179 4.322 T 6.890 241.064

Table 5: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees (T denotes the timeout after 15 minutes) These trees can be viewed as the search space for a gift packing problem, where (a) the terminal nodes represent the cost of elementary items, (b) the OR nodes model a choice among the items (elementary or composite in nature) represented by the children, and (c) the AND nodes model the repackaging of the items returned by each of the children. Every packaging incurs a cost which is modeled by the cost of the intermediate AND nodes. Here the objective is to find the alternative gifts in the order of non-decreasing cost. Table 5 shows the time required for generating 100, 300, and 500 solutions for various complete alternating AND/OR trees. We have implemented the ASG, LASG and the existing bottom-up algorithm and the corresponding running time is shown in the column with the heading ASG, LASG and BU, respectively. We have used a time limit of 15 minutes
305

Ghosh, Sharma, Chakrabarti, & Dasgupta

(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7)

ASG 12.633 52.770 116.582 287.898 664.789 1785.156 17.270 82.609 335.301 1474.477 9139.312 40.285 213.816 1563.770 64.879 529.738 97.703 1264.828 137.527 2628.461

100 solutions LASG BU 13.168 11.047 26.152 48.484 63.254 198.234 173.730 797.234 413.855 3193.234 1257.387 12777.234 17.258 11.688 48.086 111.438 184.375 1009.188 1071.352 9088.938 7872.055 81806.688 24.469 47.453 128.629 767.453 1158.582 12287.453 40.355 88.281 343.254 2217.188 58.191 151.047 862.332 5449.797 90.703 242.219 1995.195 11882.781

ASG 28.105 144.730 341.227 832.562 1767.867 T 47.531 235.855 926.004 3234.523 T 121.336 559.734 3209.145 182.270 1254.715 270.027 2747.238 369.086 4869.551

300 solutions LASG BU 32.293 14.266 75.355 69.953 165.824 292.703 399.445 1183.703 804.801 4747.703 2047.859 19003.703 49.230 14.812 134.102 152.062 376.766 1387.312 1656.844 12504.562 9565.598 112559.812 67.102 112.609 284.922 1826.359 1699.191 29246.359 110.480 225.781 596.957 5675.000 148.453 372.141 1273.641 13433.391 205.914 576.594 2627.211 28295.281

ASG 41.676 230.168 566.766 1396.758 2942.629 T 76.270 393.113 1507.973 T T 199.254 917.824 T 305.891 2008.344 443.656 4203.957 606.133 T

500 solutions LASG BU 49.832 16.609 128.934 87.922 269.766 373.172 612.184 1514.172 1197.266 6078.172 2849.617 24334.172 80.980 17.938 219.555 192.688 577.766 1765.438 2238.152 15920.188 11251.035 143312.938 116.535 129.016 451.223 2105.266 2240.012 33725.266 179.801 363.281 858.852 9132.812 245.227 593.234 1695.684 21416.984 317.492 910.969 3273.703 44707.781

Table 6: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees

and the entries marked with T denotes that the time-out occurred for those test cases. The space required for generating 100, 300, and 500 solutions is reported in Table 6. It can be observed that in terms of both time and space required, LASG outperforms both ASG and BU. Between ASG and BU, for most of the test cases BU performs better than ASG with respect to the time required for generating a specific number of solutions. The space requirement of ASG and BU for generating a specific number of solutions has an interesting correlation with the degree (d) and height (h) parameter of the tree. For low numerical values of the d and the h parameter, e.g., (d, h) combinations like (2, 7), (3, 5) etc., BU performs better than ASG. On the contrary, for the other combinations, where at least one of these d and h parameters has a high value, e.g., (d, h) combinations like (2, 17), (7, 5), (4, 9) etc., ASG outperforms BU. 5.1.1 Experimentation with Queue with Bounded Length Since the Open can grow very rapidly, both ASG and LASG incur a significant overhead in terms of time as well as space to maintain the Open list when the number of solutions to be generated is not known a priori. In fact, for ASG checking for duplicates in Open is actually the primary source of time complexity and storing the solutions in Open is a major contributing factor in space complexity. If the number of solutions that have to generated is known a priori, the proposed top-down approach can leverage the fact by using a bounded length queue for implementing Open. When a bounded length queue is used, the time requirement along with space requirement decreases significantly.
306

Generating Ordered Solutions for Explicit AND/OR Structures

(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7)

100 solutions ASG LASG BU 0.011 0.008 0.004 0.030 0.011 0.015 0.051 0.031 0.068 0.125 0.103 0.257 0.473 0.421 1.180 2.129 2.199 4.952 0.012 0.009 0.005 0.031 0.018 0.037 0.133 0.102 0.343 1.246 1.143 3.973 10.713 10.313 31.043 0.019 0.008 0.033 0.071 0.055 0.573 1.099 0.998 7.698 0.025 0.013 0.081 0.201 0.161 2.066 0.036 0.018 0.161 0.543 0.460 7.464 0.042 0.029 0.287 1.940 1.705 15.892

300 solutions ASG LASG BU 0.003 0.002 0.009 0.008 0.006 0.046 0.020 0.011 0.184 0.043 0.059 0.777 0.168 0.164 3.917 0.766 1.005 13.277 0.003 0.002 0.018 0.012 0.006 0.153 0.048 0.043 1.733 0.477 0.636 14.323 4.160 5.555 128.314 0.006 0.004 0.092 0.026 0.023 1.561 0.443 0.552 27.063 0.009 0.031 0.400 0.083 0.078 11.717 0.014 0.011 1.418 0.240 0.325 37.758 0.020 0.013 1.833 0.807 0.843 102.431

500 solutions ASG LASG BU 0.005 0.004 0.020 0.014 0.008 0.097 0.023 0.017 0.407 0.065 0.058 1.641 0.254 0.346 7.257 1.146 1.492 29.703 0.005 0.004 0.042 0.019 0.010 0.457 0.071 0.061 4.913 0.693 0.905 33.923 6.013 7.890 303.785 0.010 0.006 0.123 0.038 0.033 2.107 0.641 0.808 38.606 0.015 0.008 0.930 0.116 0.153 26.994 0.021 0.010 2.235 0.326 0.431 90.465 0.025 0.022 4.322 0.870 1.125 241.064

Table 7: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees with bounded length Open queue for ASG and LASG
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions LASG BU 2.383 11.047 4.883 48.484 14.883 198.234 54.883 797.234 214.883 3193.234 854.883 12777.234 2.617 11.688 11.160 111.438 88.047 1009.188 780.027 9088.938 7007.852 81806.688 5.016 47.453 57.016 767.453 889.016 12287.453 10.195 88.281 217.715 2217.188 19.773 151.047 657.648 5449.797 35.742 242.219 1677.051 11882.781 300 solutions LASG BU 5.508 14.266 8.008 69.953 18.008 292.703 58.008 1183.703 218.008 4747.703 858.008 19003.703 5.742 14.812 14.285 152.062 91.172 1387.312 783.152 12504.562 7010.977 112559.812 8.141 112.609 60.141 1826.359 892.141 29246.359 13.320 225.781 220.840 5675.000 22.898 372.141 660.773 13433.391 38.867 576.594 1680.176 28295.281 500 solutions LASG BU 8.633 16.609 11.133 87.922 21.133 373.172 61.133 1514.172 221.133 6078.172 861.133 24334.172 8.867 17.938 17.410 192.688 94.297 1765.438 786.277 15920.188 7014.102 143312.938 11.266 129.016 63.266 2105.266 895.266 33725.266 16.445 363.281 223.965 9132.812 26.023 593.234 663.898 21416.984 41.992 910.969 1683.301 44707.781

ASG 10.109 23.875 54.609 135.477 361.859 1071.258 12.008 39.469 169.469 971.930 7075.109 20.664 116.609 1082.633 33.344 324.258 51.742 825.859 78.141 1919.805

ASG 27.781 64.430 141.203 317.445 738.992 1845.562 34.609 101.320 353.477 1529.031 8763.023 56.703 247.320 1607.859 84.422 565.531 121.031 1227.742 169.297 2542.047

ASG 45.789 104.117 225.969 497.508 1114.422 2615.656 57.617 163.102 537.328 2085.367 10457.797 93.031 377.922 2132.516 135.812 806.797 190.758 1628.797 260.406 3163.438

Table 8: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees with bounded length Open queue for ASG and LASG

307

Ghosh, Sharma, Chakrabarti, & Dasgupta

We show the effect of using bounded length queue to implement Open in Table 7 (reporting the time requirement) and in Table 8 (reporting the memory usage) for generating 100, 300, and 500 solutions, where the number of solutions to be generated are known beforehand. Table 7 and Table 8 show that in this case both ASG and LASG outperforms BU in terms of time as well as space requirements. Particularly, ASG performs very well in this setting, outperforming LASG in some cases. 5.1.2 Experimentation to Compare the Incremental Nature The proposed top-down algorithms are incremental in nature whereas the existing bottomup approach is not incremental. After generating a specified number of ordered solutions, our methods can generate the next solution incrementally without needing to restart itself, whereas the existing approach needs to be restarted. For example, after generating the first 10 ordered solutions, ASG and LASG generate the 11th solution directly from the data structures maintained so far by these algorithms and perform necessary updates to these data structures. Whereas, BU needs to be restarted with input parameter 11 for generating the 11th solution. In Table 9 we compare the time needed to generate the subsequent 11th solution and 12th solution incrementally after generating first 10 solutions. In order to have more clarity in the comparison among the running times of the respective algorithms, we have used higher precision (upto the 6th decimal place) while reporting the running time in Table 9. Clearly, both ASG and LASG outperform BU for generating the 11th and 12th solution in terms of the time requirement.
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) first 10 0.002403 0.009111 0.028519 0.097281 0.396460 1.561020 0.001692 0.012097 0.097356 0.934389 7.898530 0.005833 0.051598 0.813028 0.051530 0.172475 0.053422 0.502939 0.033831 1.198354 ASG 11th 0.000201 0.001957 0.003311 0.014776 0.063641 0.251839 0.000158 0.001542 0.013046 0.127943 1.082319 0.000650 0.006956 0.110205 0.001327 0.024262 0.002701 0.061417 0.003706 0.156145 12th 0.000201 0.001302 0.003533 0.015929 0.059229 0.277763 0.000151 0.001572 0.014405 0.156579 1.194090 0.000671 0.007196 0.124750 0.001641 0.024438 0.003092 0.069727 0.003846 0.166501 first 10 0.001003 0.003023 0.006700 0.025877 0.102493 0.446899 0.000683 0.004084 0.031159 0.311128 2.811539 0.002143 0.017046 0.294561 0.004638 0.059751 0.005282 0.184584 0.012862 0.466560 LASG 11th 0.000240 0.000714 0.001250 0.004113 0.014490 0.061422 0.000176 0.000583 0.003948 0.033169 0.282836 0.000303 0.002209 0.027612 0.000753 0.006116 0.000636 0.017116 0.001266 0.038792 12th 0.000123 0.000629 0.001346 0.004918 0.020031 0.082366 0.000112 0.000959 0.004604 0.047594 0.387715 0.000582 0.003115 0.037281 0.000652 0.007197 0.001087 0.024042 0.001282 0.061305 first 10 0.001344 0.003596 0.014628 0.059326 0.238418 0.962635 0.001055 0.009507 0.085610 0.778298 7.037050 0.004181 0.044913 0.727766 0.005963 0.152285 0.010895 0.406947 0.018185 0.929941 BU 11th 0.001359 0.003696 0.015046 0.061393 0.246042 0.989777 0.001101 0.009931 0.089379 0.811176 7.313715 0.004434 0.047867 0.775950 0.006358 0.162527 0.011604 0.435398 0.019567 0.989326 12th 0.001397 0.003895 0.015521 0.062717 0.251746 1.015848 0.001133 0.010336 0.093419 0.846578 7.608619 0.004725 0.050940 0.823442 0.006782 0.173191 0.012556 0.465301 0.020896 1.052566

Table 9: Comparison of running time (in seconds) for generating for first 10 solutions and then the 11th solution and 12th solution incrementally for complete alternating AND/OR trees

308

Generating Ordered Solutions for Explicit AND/OR Structures

5.2 Multipeg Tower of Hanoi Problem Consider the problem of Multipeg Tower of Hanoi (Majumdar, 1996; Gupta, Chakrabarti, & Ghose, 1992). In this problem,  pegs are fastened to a stand. Initially  disks rest on the source peg A with small disk on large disk ordering. The objective is to transfer all  disks from A to the destination peg B with minimum legal moves. In a legal move, the topmost disk from any tower can be transferred to any other peg with a larger disk as the topmost disk. The problem of multi-peg tower of Hanoi can be solved recursively as follows. a. Move recursively the topmost k (k varies from 1 to  - 1) disks from A to some intermediate peg, I , using all the pegs. b. Transfer the remaining  - k disks from A to B recursively, using the ( - 1) pegs available. c. Recursively move k disks that were transferred to I previously, from the intermediate peg I to B , using all the  pegs. It may be noted that there is a choice for the value of k, which may take any value from 1 to  - 1. Solutions with different values of k may take different number of moves, and the solution which incurs minimum number of moves is the optimal solution. This choice of the value of k is modeled as an OR node, and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. Therefore, the search spaces of the multi-peg Tower of Hanoi problem correspond to alternating AND/OR trees.
#disks 8 9 10 11 12 13 100 solutions ASG LASG BU 0.034 0.030 0.069 0.119 0.116 0.264 0.479 0.635 1.310 2.421 2.178 3.171 7.453 7.448 11.437 25.379 25.115 38.458 300 solutions ASG LASG BU 0.104 0.084 0.252 0.314 0.289 0.942 1.706 1.658 3.305 6.573 6.161 12.998 21.232 21.081 43.358 68.574 67.170 140.392 500 solutions ASG LASG BU 0.200 0.138 0.577 0.590 0.458 2.183 2.303 2.829 7.592 10.651 9.678 29.242 35.825 35.663 99.593 112.411 112.470 332.113 #Opt. No. of Moves 23 27 31 39 47 55

Table 10: Comparison of running time (in seconds) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different number of disks
#disks 8 9 10 11 12 13 100 solutions ASG LASG BU ASG 36.664 43.008 416.312 64.516 96.211 111.320 1471.656 131.266 295.672 341.000 5074.219 326.352 957.336 1113.508 17197.312 999.602 3155.086 3664.117 57512.812 3198.156 10339.078 12022.883 190297.969 10412.242 300 solutions LASG BU ASG 80.734 660.062 105.039 154.266 2359.156 166.789 383.453 8161.719 373.453 1158.797 27728.562 1039.367 3719.352 92906.562 3247.547 12078.914 307872.969 10483.570 500 solutions LASG BU 117.008 903.812 197.859 3246.656 427.766 11249.219 1204.719 38259.812 3767.617 128300.312 12137.242 425447.969

Table 11: Comparison of space required (in KB) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different number of disks We have used the search space of 5 peg Tower of Hanoi problem with different number of disks,  , and generated alternative solutions in non-decreasing order of cost using ASG and
309

Ghosh, Sharma, Chakrabarti, & Dasgupta

LASG algorithms. Here the cost function expresses the number of legal moves. The value of  is varied from 8 to 13, and in Table 10 and in Table 11, we report the time required and space required, respectively, for generating 100, 300, and 500 solutions for every test cases. Experimental results show that the performance of ASG is similar to the performance of LASG with respect to both space and time. However ASG as well as LASG outperforms BU with respect to both time and space requirements. 5.3 Randomly Constructed AND/OR DAGs We have constructed a set of randomly generated AND/OR DAGs and evaluated the ASG, LASG, and BU algorithm for generating solutions under default semantics. We have used the proposed extension to the BU algorithm for generating solutions under default semantics.
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 100 solutions ASG LASG BU 0.027 0.006 0.039 0.060 0.009 0.096 0.363 0.020 0.106 0.020 0.006 0.019 0.203 0.018 0.067 3.550 0.045 0.730 26.659 0.201 14.620 0.065 0.008 0.034 0.877 0.025 0.400 7.422 0.160 26.683 T 1.972 T 300 solutions ASG LASG BU 0.089 0.021 0.158 0.281 0.030 1.100 2.485 0.059 0.266 0.123 0.021 0.098 1.483 0.048 0.257 30.302 0.126 1.681 257.605 0.612 33.382 0.348 0.027 0.217 6.910 0.069 0.994 69.097 0.449 66.558 T 5.819 T 500 solutions ASG LASG BU 0.172 0.033 0.282 0.594 0.051 3.665 6.163 0.100 0.528 0.280 0.032 0.245 4.043 0.083 0.541 85.863 0.215 2.766 710.708 1.194 52.406 0.817 0.049 2.251 18.823 0.118 1.365 194.452 0.927 109.076 T 9.426 T

Table 12: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for AND/OR DAGs (T denotes the timeout after 15 minutes)
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 ASG 11.609 23.141 74.082 13.914 48.867 229.820 772.441 30.648 121.535 471.625 2722.938 100 solutions LASG BU 8.875 8.125 16.219 31.312 39.000 106.875 10.492 8.172 35.445 66.938 118.707 389.844 339.676 1996.875 17.332 29.609 65.578 287.109 266.078 2729.297 1256.535 T ASG 32.852 62.516 220.648 46.117 151.363 705.809 2245.938 85.781 381.133 1183.379 T 300 solutions LASG BU 30.797 10.906 46.711 49.562 105.852 172.562 32.539 11.297 101.168 98.188 312.246 621.094 825.984 3321.875 53.961 73.359 168.305 737.891 550.477 6945.703 2353.562 T ASG 54.094 100.555 371.344 77.445 262.816 1200.336 3732.523 140.312 659.434 1927.961 T 500 solutions LASG BU 50.035 13.250 74.379 65.188 168.375 230.375 54.602 14.422 163.273 129.438 507.762 852.344 1327.406 4646.875 93.539 86.641 275.594 883.984 843.484 8419.922 3447.809 T

Table 13: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for AND/OR DAGs

Table 12 and Table 13 compare the time required and space required for running ASG, LASG and BU for generating 100, 300, and 500 solutions for every test cases. The first and second columns of every row provide the size (n ) and the average out-degree (d) of the DAG. The results obtained for this test domain are similar to the results for randomly
310

Generating Ordered Solutions for Explicit AND/OR Structures

constructed AND/OR trees. It may be noted that in terms of both time and space required, LASG outperforms both ASG and BU. Between ASG and BU, for most of the test cases BU performs better than ASG with respect to the time required for generating a specific number of solutions. Whereas, the space requirement of ASG and BU for generating a specific number of solutions has an interesting co-relation with the average degree (d) and the size (n ) parameter of the DAG. For low numerical values of the d and the n parameter, e.g., (n , d) combinations like (60, 2), (33, 3) etc., BU performs better than ASG. On the contrary, for the other combinations, where at least one of these n and d parameter has a high value, e.g., (n , d) combinations like (920, 2), (9624, 3), (40884, 4) etc., ASG outperforms BU. 5.4 Matrix-Chain Multiplication Problem We have also used the well-known matrix-chain multiplication (Cormen, Stein, Rivest, & Leiserson, 2001) problem for experimentation. The search space of the popular dynamic programming formulation of this problem correspond to AND/OR DAG.
DAG Cnstr. #matrices Time (Sec) 20 0.033 30 0.200 40 0.898 50 3.033 60 8.335 70 19.591 80 41.960 90 82.578 100 151.814 Sopt Cnstr. Time (Sec) 0.001 0.003 0.008 0.016 0.029 0.046 0.071 0.101 0.143 10 solutions ASG 0.003 0.009 0.019 0.047 0.088 0.140 0.209 0.296 0.409 LASG 0.002 0.008 0.018 0.048 0.090 0.142 0.212 0.300 0.412 BU 0.206 2.785 15.580 93.267 342.212 862.387 T T T ASG 0.004 0.012 0.024 0.062 0.118 0.187 0.280 0.396 0.546 15 solutions LASG 0.003 0.010 0.024 0.065 0.120 0.190 0.282 0.398 0.548 BU 0.288 4.087 23.414 140.513 509.906 T T T T ASG 0.005 0.015 0.030 0.079 0.148 0.235 0.351 0.496 0.688 20 solutions LASG 0.004 0.012 0.030 0.081 0.151 0.238 0.354 0.499 0.683 BU 0.373 5.406 31.112 187.227 678.718 T T T T

Table 14: Comparison of time required (in seconds) for AND/OR DAGs corresponding to the search spaces of matrix-chain multiplication with different number of matrices, (T denotes the timeout after 15 minutes)
#matrices 20 30 40 50 60 70 80 90 100 ASG 19.641 66.367 156.559 308.984 537.383 859.844 1290.117 1843.828 2537.582 10 solutions LASG 20.203 69.273 160.227 315.012 545.117 869.160 1301.406 1857.480 2556.883 BU 160.918 555.684 1317.637 2563.965 4411.855 6978.496 T T T ASG 20.543 67.809 157.738 310.277 538.930 862.133 1293.148 1847.602 2542.746 15 solutions LASG 21.227 70.695 161.785 316.543 546.512 870.867 1303.426 1859.812 2560.043 BU 234.305 821.902 1960.281 3825.223 6592.508 T T T T ASG 21.914 69.516 158.758 311.551 539.914 863.977 1295.852 1851.164 2549.352 20 solutions LASG 22.773 72.523 162.852 318.145 547.551 872.219 1305.090 1861.789 2566.992 BU 303.973 1081.902 2594.207 5075.262 8759.441 T T T T

Table 15: Comparison of space required (in KB) for AND/OR DAGs corresponding to the search spaces of matrix-chain multiplication with different number of matrices

Given a sequence of matrices, A1 , A2 , · · · , An , of n matrices where matrix Ai has dimension pi-1 × pi , in this problem the objective is to find the most efficient way to multiply
311

Ghosh, Sharma, Chakrabarti, & Dasgupta

these matrices. The classical dynamic programming approach works as follows. Suppose A[i,j ] denotes matrix that results from evaluating the product, Ai Ai+1 · · · Aj , and m[i, j ] is the minimum number of scalar multiplications required for computing the matrix A[i,j ] . Therefore, the cost of optimal solution is denoted by m[i, j ] which can be recursively defined as :  0, if i = j ; m[i, k] + m[k + 1, j ] + pi-1 × pk × pj , if i < j .

m[i, j ] =

The choice of the value of k is modeled as OR node and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. It is worth noting that unlike the search space of 5-peg ToH problem, the search space of the matrix-chain multiplication problem corresponds to AND/OR DAG. We have used the search space for different matrix sequences having varying length and generated alternative solutions in the order of non-decreasing cost. In Table 14, we report the time required and in Table 15, we report the memory used for generating 10, 15, and 20 solutions for every test cases. In Table 14, for each test case, we also report the time required for constructing the explicit AND/OR DAG from the recursive formulation in the 2nd column, and the optimal solution construction time in the 3rd column. It is interesting to observe that the relative performance of ASG and LASG for this search space is very similar to that obtained for 5peg ToH search space though this search space for this domain is AND/OR DAG. Both ASG and LASG perform approximately the same with respect to time and space requirement. However, the advantage of ASG as well as LASG over BU with respect to both time and space requirement is more significant in this domain. 5.5 Generating Secondary Structure for RNA Another relevant problem where the alternative solutions play an important role is the computation of the secondary structure of RNA. RNA molecules can be viewed as strings of bases, where each base belongs to the set {Adenine, Cytocine, Guanine, U racil} (also denoted as {A, C, G, U }). RNA molecules tend to loop back and form base pairs with itself and the resulting shape is called secondary structure (Mathews & Zuker, 2004). The stability of the secondary structure largely depends on the number of base pairings (in general, larger number of base pairings implies more stable secondary structure). Although there are other factors that influence the secondary structure, it is often not possible to express these other factors using a cost function and they are typically evaluated empirically. Therefore, it is useful to generate a set of possible alternative secondary structures ordered by decreasing numbering of base pairings for a given RNA which can be further subjected to experimental evaluation. The computation of the optimal secondary structure considering the underlying principle of maximizing the number of base-pairings has a nice dynamic programming formulation (Kleinberg & Tardos, 2005). Given an RNA molecule B = b1 b2 · · · bn where each bi  {A, C, G, U }, the secondary structure on B is a set of base pairings, D = {(i, j )}, where i, j  {1, 2, · · · n}, that satisfies the following conditions:
312

 min

ik<j

Generating Ordered Solutions for Explicit AND/OR Structures

Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14

Organism Name Anaerorhabdus Furcosa Archaeoglobus Fulgidus Chlorobium Limicola Desulfurococcus Mobilis Haloarcula Japonica Halobacterium Sp. Mycoplasma Genitalium Mycoplasma Hyopneumoniae Mycoplasma Penetrans Pyrobaculum Aerophilum Pyrococcus Abyssi Spiroplasma Melliferum Sulfolobus Acidocaldarius Symbiobacterium Thermophilum

# Bases 114 124 111 129 122 120 104 105 103 131 118 107 126 110

Table 16: Details of the RNA sequences used for Experimentation a. if (i, j )  D , then i + 4 < j : This condition states that the ends of each pair in D are separated by at least four intermediate bases. b. The elements of any pair in D consists of either {A, U } or {C, G} (in either order). c. No base appears in more than one pairings, i.e., D is a matching. d. If (i, j ) and (k, l) are two pairs in D , then it is not possible to have i < k < l < j , i.e., no two pairings can cross each other.
Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 DAG Cnstr. Time (Sec) 34.464 57.999 26.423 83.943 51.290 46.508 16.766 22.775 18.831 91.419 47.660 22.649 67.913 28.911 Sopt Cnstr. Time (Sec) 0.042 0.057 0.038 0.065 0.051 0.047 0.029 0.033 0.031 0.073 0.047 0.034 0.061 0.038 ASG 0.094 0.126 0.084 0.144 0.114 0.107 0.068 0.077 0.068 0.167 0.111 0.078 0.140 0.087 5 solutions LASG BU 0.095 449.916 0.128 823.493 0.089 363.421 0.152 1089.462 0.116 681.429 0.108 598.419 0.069 210.806 0.078 284.455 0.072 233.999 0.170 T 0.109 627.744 0.079 288.520 0.141 962.641 0.085 366.693 ASG 0.145 0.193 0.135 0.230 0.176 0.166 0.101 0.120 0.109 0.249 0.173 0.116 0.206 0.134 10 solutions LASG BU 0.148 893.682 0.198 T 0.133 718.326 0.227 T 0.180 1349.181 0.175 T 0.103 410.817 0.122 559.318 0.111 458.290 0.263 T 0.171 1253.034 0.123 573.602 0.218 T 0.137 724.113 ASG 0.197 0.271 0.183 0.314 0.239 0.226 0.136 0.153 0.144 0.347 0.220 0.165 0.290 0.182 15 solutions LASG BU 0.202 1359.759 0.277 T 0.186 1077.094 0.317 T 0.245 T 0.238 T 0.144 621.792 0.165 836.359 0.148 683.411 0.355 T 0.240 T 0.167 849.134 0.288 T 0.186 1072.552

Table 17: Comparison of time required (in seconds) for AND/OR DAGs corresponding to the search spaces of RNA secondary structure with different number of bases (T denotes the timeout after 30 minutes)

Under the above mentioned conditions the dynamic programming formulation is as follows. Suppose P (i, j ) denotes the maximum number of base pairings in a secondary structure on bi · · · bj . P (i, j ) can be recursively defined as : P [i, j ] =  0, if i + 4  j ,

max P [i, j - 1], max 1 + P [i, k - 1] + P [k + 1, j - 1] ,
ik<j

if i + 4 < j .

313

Ghosh, Sharma, Chakrabarti, & Dasgupta

Here, a choice of the value of k is modeled as an OR node and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. We have experimented with the search space of this problem for the set of RNA molecule sequences obtained from the test-cases developed by Szymanski, Barciszewska, Barciszewski, and Erdmann (2005). The details of the test cases are shown in Table 16. For every test cases, we report the time required in Table 17 for generating 5, 10, and 15 solutions. For the same setting, the space required is reported in Table 18. In Table 17, for each test case, we also report the time required for constructing the explicit AND/OR DAG from the recursive formulation in the 2nd column, and the time required for constructing the optimal solution time in the 3rd column. We use a high value of time-out (1800 seconds) in order to gather the running time required by BU. We limit the maximum solutions generated at 15 because for generating higher number of solutions, BU is timed out for most of the test cases. It is worth noting that the result obtained for this domain is very similar to the result obtained for the matrix-chain multiplication problem domain. Both space and time wise ASG and LASG perform similarly and they outperform BU significantly with respect to time as well as space requirement.
Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 5 solutions LASG 1694.688 2310.008 1516.922 2665.820 2097.414 1963.367 1138.633 1333.336 1207.633 3047.539 2022.906 1335.883 2496.469 1517.828 10 solutions LASG 1697.797 2315.258 1521.750 2671.711 2101.836 1968.305 1142.023 1338.070 1211.523 3053.977 2030.922 1339.516 2502.625 1521.352 15 solutions LASG 1700.492 2318.008 1526.797 2675.633 2106.000 1972.172 1144.109 1342.484 1213.906 3059.781 2038.461 1341.914 2506.703 1525.344

ASG 1647.555 2254.531 1473.852 2606.242 2045.930 1912.227 1101.125 1293.812 1170.094 2984.773 1974.695 1295.141 2438.898 1475.477

BU 7409.336 9902.953 6629.891 11358.945 9021.273 8499.570 5087.680 5855.547 5352.477 T 8641.422 5924.664 10657.945 6627.844

ASG 1651.273 2258.773 1477.492 2610.875 2049.844 1916.422 1104.422 1297.750 1173.023 2990.211 1979.344 1297.273 2442.961 1478.555

BU 14656.469 T 13103.492 T 17875.430 T 10036.938 11560.203 10562.766 T 17119.820 11701.695 T 13099.055

ASG 1654.367 2262.492 1480.555 2615.719 2052.867 1921.117 1108.047 1302.242 1176.352 2994.773 1983.664 1299.805 2447.172 1482.234

BU 21846.156 T 19518.625 T T T 14924.820 17211.406 15718.617 T T 17420.719 T 19519.742

Table 18: Comparison of space required (in KB) for AND/OR DAGs corresponding to the search spaces of RNA secondary structure with different number of bases

5.6 Observations The experimental data shows that the LASG algorithm generally outperforms the ASG algorithm and the existing bottom-up approach in terms of the running time for complete alternating AND/OR trees and AND/OR DAGs. Whereas, for the other problem domains, i.e., the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem, and the problem of determining secondary structure of RNA sequences, the overall performance of the ASG algorithm is similar to the performance of the LASG algorithm. This behavior can be explained from the average and maximum length statistics of Open list, reported in Table 19 - Table 23, for these above mentioned test domains.
314

Generating Ordered Solutions for Explicit AND/OR Structures

In the case of complete trees and random DAGs, for ASG algorithm, the average as well as the maximum size of Open grows much faster than that of LASG algorithm (Table 19 and Table 20), with the increase in the size of the tree/DAG.
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions ASG LASG avg. max. avg. max. 235 383 75 159 994 1894 73 120 2427 4709 156 306 5546 10947 524 1149 11744 23291 384 523 24264 48333 655 841 304 549 120 242 1561 3015 172 346 5496 10899 191 289 17336 34542 486 691 53139 106155 1138 1216 734 1427 103 176 3748 7383 194 381 16282 32451 422 488 1216 2352 146 307 7261 14446 249 335 1781 3489 141 276 12362 24651 297 342 2433 4765 261 508 19311 38435 450 529 300 solutions ASG LASG avg. max. avg. max. 435 629 179 289 2657 4931 220 528 6935 13537 483 1005 16266 32076 1550 2726 34836 69160 677 1121 T T 1087 1611 740 1323 341 652 4359 8400 579 1260 16272 32244 387 661 51954 103549 956 1754 T T 1267 1569 2062 4006 256 503 10928 21489 678 1467 48786 97196 687 1131 3407 6555 496 1053 21652 42972 470 888 5089 9911 507 1126 36868 73323 461 789 7072 13910 747 1483 57754 115116 687 961 500 solutions ASG LASG avg. max. avg. max. 545 792 236 372 4103 7569 449 1069 11251 21843 851 1771 26748 52724 2261 3844 57673 114367 983 1824 T T 1527 2819 1107 1972 539 1007 7026 13588 1012 2084 26904 53271 622 1368 T T 1460 2672 T T 1432 1776 3322 6375 452 1065 17932 35222 1265 2837 T T 1025 1807 5508 10694 852 1742 35850 71054 832 1781 8250 16035 971 2164 61221 121958 749 1573 11595 22809 1204 2273 T T 984 1922

Table 19: Average and maximum length of Open while generating 100, 300, and 500 solutions for complete alternating AND/OR trees
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 100 solutions ASG LASG avg. max. avg. max. 181 338 39 63 479 854 77 133 1530 2957 116 227 202 409 58 102 1001 1969 236 447 5008 9911 374 626 14422 28666 394 491 510 990 56 101 2407 4760 253 485 7522 14931 258 437 T T 749 804 300 solutions ASG LASG avg. max. avg. max. 428 768 131 282 1144 2058 210 417 4289 8278 332 639 604 1193 154 281 2958 5799 675 1256 14803 29314 851 1569 43087 85825 746 1339 1374 2563 187 458 7166 14204 590 1018 22254 44062 847 1831 T T 852 1004 500 solutions ASG LASG avg. max. avg. max. 643 1138 219 411 1721 3139 329 612 6902 13305 512 946 978 1875 234 422 4874 9781 1013 1810 24442 48357 1337 2527 71547 142327 1254 2756 2140 3996 376 868 11874 23558 885 1655 36743 72740 1565 3493 T T 961 1215

Table 20: Average and maximum length of Open while generating 100, 300, and 500 solutions for randomly constructed AND/OR DAGs

Since ASG algorithm checks for the presence of duplicates while expanding a solution, the time required for duplication checking grows rapidly for these test domains. Hence, the overall time required for generating a specific number of solutions also increases rapidly (faster than both BU and LASG) with the increase in the size of the tree/DAG. As a result, BU outperforms ASG with respect to the time requirement for trees and DAGs. However
315

Ghosh, Sharma, Chakrabarti, & Dasgupta

the memory used for generating a specific number of solutions increases moderately (slower than BU) with the increase in the size of the tree/DAG. Therefore with respect to space requirement, ASG outperforms BU for larger trees and DAGs. Between LASG and BU, the time as well as the memory requirement of BU increases faster than that of LASG when the degree of the AND/OR tree or DAG increases. This happens because, for BU, the time taken for merging the sub-solutions at the AND nodes and memory required for storing alternative solutions that are rooted at different nodes increases rapidly with the increase in the degree of that node. On the contrary, for the other test domains, 5-peg Tower of Hanoi problem, matrix-chain multiplication problem, and the probelm of finding secondary structure of RNA sequences, the average and the maximum size of Open for both ASG and LASG are comparable (Table 21, Table 22 and Table 23). Therefore, for the LASG algorithm, the time saved by avoiding the duplication checking is compensated by the extra overhead of maintaining the solution space tree and the checks required for lazy expansion. Hence the running time as well as the space requirement are almost same for both algorithms for these three above mentioned problem domains. Moreover, due to the low values of the average and the maximum size of Open, ASG outperforms BU with respect to both time requirement and memory used for these three test domains. For these three domains also, between LASG and BU, the time as well as the memory requirement of BU increases faster than that of LASG when the size of the search space (AND/OR tree or DAG) increases.

6. Ramifications on Implicitly Specified AND/OR Structures
In this section, we briefly discuss use of our proposed algorithms for generation of alternative solutions in the non-decreasing order of cost for implicit AND/OR search spaces. One possible way is to extend the standard AO for generating a given number of solutions, say k, as follows. Instead of keeping only one potential solution graph(psg), at any stage k psgs can be computed on the explicitly constructed search space and instead of expanding one node, k nodes, (that is, one node from each psg), can be expanded at once. After expanding the nodes, k psgs are recomputed once again. Since the cost of the nodes are often recomputed after expanding nodes, the swap options associated with any such node have to be updated after every such recomputation. Another possible approach could be to run AO until it generates the optimal solution. At this point of time the swap options can be computed on the explicit portion of the graph and swap option with minimum cost can be applied to the optimal solution. Then the resulting psg is again expanded further resulting in the expansion of the explicit graph. The swap options are re-evaluated to incorporate the cost update. Again the next best psg is computed. This process continues till the second best solution is derived. Now among the remaining successor psgs of the first solution and the successor psgs of second solution, the most promising psg is selected and expanded. This process continues till the third solution is found. Then the successor psgs are also added to the already existing pool of candidate psgs. These two broad steps, (a) selecting the next best psg from the pool of candidate psgs, and then (b) keeping on expanding the explicit graph till the next best solution is found, is continued till k solutions are found.
316

Generating Ordered Solutions for Explicit AND/OR Structures

# disks 8 9 10 11 12 13

100 solutions ASG LASG avg. max. avg. max. 55 92 41 68 66 122 42 71 109 183 53 79 132 218 76 140 219 385 85 147 259 482 118 200

300 solutions ASG LASG avg. max. avg. max. 111 186 91 174 163 331 119 252 216 367 142 283 296 611 177 373 473 776 234 492 675 1240 252 437

500 solutions ASG LASG avg. max. avg. max. 174 375 135 235 265 484 198 382 345 693 234 447 486 882 291 558 668 1200 404 724 1016 1828 377 697

Table 21: Average and maximum length of Open while generating 100, 300, and 500 solutions for 5-peg Tower of Hanoi problem with different number of disks
# matrices 20 30 40 50 60 70 80 90 100 10 solutions ASG LASG avg. max. avg. max. 46 87 25 39 84 162 71 126 73 123 58 90 86 151 75 126 91 144 76 112 136 234 85 122 181 324 94 132 226 414 103 142 307 576 167 259 15 solutions ASG LASG avg. max. avg. max. 68 121 34 59 123 230 94 157 98 182 73 129 120 211 100 169 118 189 94 137 188 329 103 147 258 469 112 157 328 609 122 167 445 823 216 337 20 solutions ASG LASG avg. max. avg. max. 90 176 46 95 160 293 116 192 125 226 90 152 151 266 123 205 151 267 108 160 243 437 117 170 335 607 127 180 427 777 136 190 583 1145 262 477

Table 22: Average and maximum length of Open while generating 10, 15, and 20 solutions for matrix-chain multiplication problems
Test case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 5 solutions ASG LASG avg. max. avg. max. 45 84 41 74 50 95 50 95 47 90 46 89 50 93 49 90 47 86 45 74 49 93 47 84 42 81 42 80 46 89 44 84 40 77 39 73 59 116 59 113 55 106 54 105 33 64 31 51 51 98 51 97 41 78 40 73 10 solutions ASG LASG avg. max. avg. max. 93 176 75 125 100 192 94 170 90 168 82 142 101 194 87 155 98 186 87 149 105 200 95 168 83 157 73 119 97 188 86 159 80 147 70 119 128 251 116 212 115 225 110 211 67 116 55 98 103 193 100 185 82 154 69 112 15 solutions ASG LASG avg. max. avg. max. 135 249 95 143 146 266 125 197 132 244 115 210 152 292 119 197 140 246 114 184 155 294 127 206 121 231 92 138 144 277 120 214 115 214 93 146 189 350 161 280 171 317 166 321 95 172 78 135 149 276 140 239 120 231 97 176

Table 23: Average and maximum length of Open while generating 5, 10, and 15 solutions for generating secondary structure of RNA sequences

317

Ghosh, Sharma, Chakrabarti, & Dasgupta

It is important to observe that both methods heavily depend on incorporating the updates in the explicit DAG like adding nodes, increase in the cost, etc., and recomputing the associated swap options along with the signatures that use those swap options. Handling dynamic updates in the DAG efficiently and its use in implicit AND/OR search spaces remains an interesting future direction.

7. Conclusion
In our work we have presented a top-down algorithm for generating solutions of a given weighted AND/OR structure (DAG) in non-decreasing order of cost. Ordered solutions for AND/OR DAGs are useful for a number of areas including model based programming, developing new variants of AO*, service composition based on user preferences, real life problems having dynamic programming formulation, etc. Our proposed algorithm has two advantages ­ (a) it works incrementally, i.e., after generating a specific number of solutions, the next solution is generated quickly, (b) if the number of solutions to be generated is known a priori, our algorithm can leverage that to generate solutions faster. Experimental results show the efficacy of our algorithm over the state-of-the-art. This also opens up several interesting research problems and development of applications.

8. Acknowledgments
We thank the anonymous reviewers and the editor, Prof. Hector Geffner, for their valuable comments which have enriched the presentation of the paper significantly. We also thank Prof. Abhijit Mitra, International Institute of Information Technology, Hyderabad, India, for his valuable inputs regarding the test domain involving secondary structure of RNA. We thank Aritra Hazra and Srobona Mitra, Research Scholar, Department of Comp. Sc. & Engg., Indian Institute of Technology Kharagpur, India, for proof reading the paper.

Appendix A. Proof of Correctness of Algorithm 4
Lemma A.1 Every solution other than the optimal solution Sopt can be constructed from ^. Sopt by applying a sequence of swap options according to the order R ^ Proof: [Lemma A.1] Every solution other than Sopt of an alternating AND/ OR tree T is constructed by choosing some non optimal edges at some OR nodes. Consider any other solution Sm , corresponding to which the set of non-optimal OR edges is S and suppose |S  | = m. We apply the relation R to S to obtain an ordered sequence  of OR edges where e1 , e2  , e1 appears before e2 in  if (e1 , e2 )  R. We show that there exists a ^ of swap options that can be constructed for S . For every OR edge ei of  sequence  j (here eij is the ith edge of  and 1  i  m), we append the subsequence of OR edges ei1 , . . . , eij -1 before eij , where ei1 , . . . , eij are the OR edges that emanate from the same parent vq , and ei1 , . . . , eij -1 are the first ij - 1 edges in L(vq ). We get a sequence of OR edges aug from  by the above mentioned augmentation. aug is basically a concatenation of subsequences 1 , . . . , m , where i is a sequence of edges ei1 , . . . , eij such that ei1 , . . . , eij are the OR edges that emanate from the same parent vq , ^ from aug as follows. From and ei1 , . . . , eij are the first ij edges in L(vq ). We construct 
318

Generating Ordered Solutions for Explicit AND/OR Structures

every i , we construct  ^i = i1 ,i2 , . . . , ij -1,ij , where ik ,ik +1 = eik , eik +1 , ik ,ik +1 and ^ i1  ik  (ij - 1).  is constructed by concatenating every individual  ^i . Hence there exists ^ corresponding to every other solution Sm . a sequence of swap options    Definition A.p [Default Path] From Lemma A.1, every non-optimal solution Sm can be constructed from the initial optimal solution by applying a sequence of swap options, ^ Sm ), according to the order R ^ . The sequence of solutions that is formed following ( ^ Sm ) ( s corresponds to a path from Sopt to Sm in SSDAG G . This path is defined as the default path, Pd (Sm ), for Sm . ^ contains every alternative Lemma A.2 The SSDAG of an alternating AND/OR tree T ^ . solution of T Proof: [Lemma A.2] We prove this by induction on the length of the default path Pd of the solutions. [Basis (n = 1) :] Consider the swap list of Sopt . The solutions whose default path length is equal to 1 form the Succ(Sopt ). Therefore these solutions are present in G . [Inductive Step :] Suppose the solutions whose default path length is less than or equal to n are present in G . We prove that the solutions having default path length equal to ^ Sm ) = n + 1 are also present in G . Consider any solution Sm where Pd (Sm ) = n + 1. Let (   ^ S ) = 1 , · · · , n . Since Pd (S  ) = 1 , · · · , n , n+1 . Consider the solution Sm where ( m m s     V , and swap option  n , Sm n+1  L(Sm ), there is a directed edge from Sm to Sm in G . Hence every solution having a default path length equal to n + 1 is also present in G .   ^ , Algorithm 4 adds solutions to Closed Lemma A.3 For any alternating AND/OR tree T (at Line 11) in non-decreasing order of cost. Proof: [Lemma A.3] Consider the following invariants of Algorithm 4 that follow from the description of Algorithm 4. a. The minimum cost solution from Open is always removed at Line 6 of Algorithm 4. b. The cost of the solutions that are added in Open, while exploring the successor set of a solution Sm (at Line 13 of Algorithm 4), are greater than or equal to C (Sm ). From these two invariants it follows that Algorithm 4 adds solutions to Closed (at Line 11) in non-decreasing order of cost. ^ , for every node of the SSDAG of T ^ , Lemma A.4 For any alternating AND/OR tree T Agorithm 4 generates the solution corresponding to that node. Proof: [Lemma A.4] From Lemma A.3 it follows that Algorithm 4 generates the solutions in the non-decreasing order of cost. By generating a solution Sm , we mean adding Sm to Closed (at line 11 of Algorithm 4). For the purpose of proof by contradiction, let us assume that Algorithm 4 does not generate solution Sm . Also let Sm be the first occurrence of this
319

Ghosh, Sharma, Chakrabarti, & Dasgupta

scenario while generating solutions in the mentioned order. According to Lemma A.1, there ^ = 1 , . . . , k corresponding to Sm . Also consider the exists a sequence of swap options   ^  = 1 , . . . , k-1 . According to Property 3.2, solution Sm whose sequence of swap options is   C (Sm )  C (Sm ). Consider the following two cases:
 ) < C (S ): Since S a. C (Sm m m is the first instance of the incorrect scenario, and Algo is generated rithm 4 generates the solutions in the non-decreasing order of cost, Sm prior to Sm .  ) = C (S ): Since Algorithm 4 resolves the tie in the favor of the parent solution, b. C (Sm m  will be and Sm is the first instance of the incorrect scenario ­ in this case also Sm generated prior to Sm .  . When S  was generated by Algorithm 4, The swap option k belongs to the swap list of Sm m   that is, when Sm was added to Closed, Sm was also expanded and the solutions which can  applying one swap option, were added to the Open list. Since S be constructed from Sm m  applying one swap option  , S was constructed from Sm was also added to the Open m k  . Therefore S while exploring the successors of Sm m will also be eventually generated by Algorithm 4 - a contradiction.  

^ , Algorithm 4 does not add any solution Lemma A.5 For any alternating AND/OR tree T to Closed (at Line 11 of Algorithm 4) more than once. Proof: [Lemma A.5] For the purpose of contradiction, let us assume that Sm is the first solution that is added to Closed twice. Therefore Sm must have been added to Open twice. Consider the following facts. a. When Sm was added to Closed for the first time, the value of lastSolCost was C (Sm ), and Sm was added to TList. b. From the description of Algorithm 4 it follows that the contents of TList are deleted only when the value of lastSolCost increases. c. From Lemma A.3 it follows that Algorithm 4 generates the solutions in non-decreasing order of cost. Hence, when Sm was generated for the second time, the value of lastSolCost did not change from C (Sm ). From the above facts it follow that Sm was present in TList when Sm was added to Open for the second time. Since, while adding a solution to Open, Algorithm 4 checks whether it is present in TList (at Line 16 of Algorithm 4); Algorithm 4 must had done the same while adding Sm to Open for the second time. Therefore Sm could not be added Open for the second time ­ a contradiction.   Theorem A.1 Sj  V , Sj is generated (at Line 11) by Algorithm 4 only once and in the non-decreasing order of costs while ties among the solutions having same costs are resolved as mentioned before. Proof: [Theorem A.1] Follows from Lemma A.2, Lemma A.3, Lemma A.4 and Lemma A.5.  
320

Generating Ordered Solutions for Explicit AND/OR Structures

Appendix B. Proof of Correctness of Algorithm 5
Definition B.q [Reconvergent Paths in Solution Space DAG] Two paths, (i) p1 = 1  · · ·  S 1 and (ii) p = S 2  · · ·  S 2 , in the SSDAG G s of an alternating Si 2 i1 im in 1 ^ are reconvergent if the following holds: AND/OR tree T
1 = S 2 , i.e. the paths start from the same node; a. Si i1 1 1 = S 1 , i.e. the paths ends at the same node; b. Si im n 1 = S 2 ; i.e. the paths do not have any common c. (j  [2, n - 1])(k  [2, m - 1]), Si ik j intermediate node.

Definition B.r [Order on Generation Time] In the context of Algorithm 5, we define an order relation, t  V × V , where (Sp , Sq ) t if Sp is generated by Algorithm 5 before Sq . ^ . Here V is set of vertices in SSDAG G s of an alternating AND/OR tree T Lemma B.1 Algorithm 5 adds the solutions to the Closed list in the non-decreasing order of costs. Proof: [Lemma B.1] Consider the following invariants of Algorithm 5 that follow from the description of Algorithm 5. a. The minimum cost solution from Open is always removed at line 11 of Algorithm 4. b. Algorithm 5 expands any solution, say Sp , in two phases. At the first phase Sp is expanded using the native swap options of Sp . The solutions that are added to Open as a result of the application of the native swap options, will have cost greater than or equal to C (Sp ). In the second phase, i.e., during lazy expansion, Sp is again expanded using a non native swap option. A solution Sp may undergo the second phase  times where 0    (|L(Sp )| - |N (Sp , k )|) and k is used to construct Sp . In every lazy expansion of Sp , a new solution is added to Open. Consider a solution Sm which is  using  by Algorithm 5 where S   P red(S ). Suppose swap constructed from Sm m j m option i  L(Sm ), and i  / N (Sm , j ), i.e., i is not a native swap option of Sm .  ). Suppose S and S  are the successors of S and S  respectively, Clearly i  L(Sm m c m c i i  -  , and S  constructed by the application of i , i.e., Sm  Sc -  S . m c Also let Sc is added to Closed after Sm . Consider the fact that Algorithm 5 does not apply swap option i to Sm , that is, Sc is  is added to Closed. Since C (S  )  C (S ), C (S  )  C (S ). not added to Open until Sc m c m c According to Algorithm 5, i is applied to Sm (during the lazy expansion), and Sc is  is added to Closed. Consider the time period between added to Open right after Sc  to Closed. During that period, every solution that is added adding Sm and adding Sc  ), i.e., the cost is less or equal to C (S ). In to Closed has cost between C (Sm ) and C (Sc c general, the application of a swap option to add a solution to Open is delayed by such an amount of time, say , so that all the solutions, which are added to Closed during this  time interval, have cost less than or equal to the solution under consideration.
321

Ghosh, Sharma, Chakrabarti, & Dasgupta

From the above facts it follow that Algorithm 5 adds the solutions to the Closed list in non-decreasing order of costs.   Lemma B.2 Any two reconvergent paths in the SSDAG G s of an alternating AND/OR ^ are of equal length. tree T Proof: [Lemma B.2] Consider the paths:
1 2 n 1 2 m  (i) p1 = S1 -  Sp -  ··· -  Sn , and (ii) p2 = S1 -  Sp -  ··· - -  Sn .













The edges in the paths represent the application of a swap option to a solution. Now p1 and p2 start from the same solution and also end at the same solution. Therefore the sets of swap options that are used in these paths are also same. Hence the lengths of those paths are equal, that is, in the context of p1 and p2 , n = m. Lemma B.3 For any set of reconvergent paths of any length n, Algorithm 5 generates at most one path. Proof: [Lemma B.3] The following cases are possible. [Case 1 (n = 2) :] Consider the following two paths:
1 2 1 2  (i) p1 = S1 -  S2 -  S3 , and (ii) p2 = S1 -  S2 -  S3 .









 and  =   . Suppose S  S  . Here Algorithm 5 does not It is obvious that 1 = 2 2 t 2 2 1  . Therefore p is not generated by Algorithm 5. apply the swap option 1 to S2 2

[Case 2 (Any other values of n) :] In this case, any path belonging to the set of reconvergent paths, consists of n different swap options, suppose 1 , · · · , n . Also the start node and the end node of the paths under consideration are Sp and Sm . Consider the nodes in the paths having length 1 from Sp . Clearly there can be n such nodes. Among those nodes, suppose Algorithm 5 adds Sp1 to Closed first, and Sp1 is constructed from Sp by applying swap option 1 . According to Algorithm 5, 1 will not be applied to any other node that is constructed from Sp and is added to Closed after Sp1 . Therefore, all those paths starting from Sp , whose second node is not Sp1 , will not be generated by Algorithm 5. We can use the similar argument on the paths from Sp1 to Sm of length n - 1 to determine the paths which will not be generated by Algorithm 5. At each stage, a set of paths will not be grown further, and at most one path towards Sm will continue to grow. After applying the previous argument n times, at most one path from Sp to Sm will be constructed. Therefore Algorithm 5 will generate at most one path from Sp to Sm .   ^ c ] We define connection relation, Rc , a Definition B.s [Connection Relation Rc and R symmetric order relation for a pair of OR nodes, vq and vr , belonging to an alternating ^ as: AND/OR tree T ^ there exists an AND node vp , from which (vq , vr )  Rc | if in T (ii) p2 = vp  . . .  vr
322

there exist two paths, (i) p1 = vp  . . .  vq , and

Generating Ordered Solutions for Explicit AND/OR Structures

^ c , is defined between two swap options as follows. ConSimilarly the connection relation, R sider two swap options iq and jr , where iq = ei , eq , iq and jr = ej , er , jr . Suppose OR edges ei and eq emanate from vp , and OR edges ej and er emanate from vt . Now ^ c if (vp , vt )  Rc . (iq , jr )  R Definition B.t [Mutually Connected Set] For a solution Sm , a set Vm of OR nodes is mutually connected, if v1 , v2  Vm , (v1 = v2 )  {(v1 , v2 )  Rc } Consider the set of OR nodes, Vm = {v1 , · · · , vk }, where swap option j belongs to vj and ^m = {1 , · · · , k } is mutually connected. 1  j  k. Here the set of swap options V ^ , P red(Sm ) = Lemma B.4 Suppose Sm is a solution of an alternating AND/OR tree T {S1 , · · · , Sk }, and swap option j is used to construct Sm from Sj where 1  j  k. The swap options 1 , · · · , k are mutually connected. Proof: [Lemma B.4] Since Sm is constructed from S1 , · · · , Sk by applying 1 , · · · , k respectively, 1 , · · · , k are present in the signature of Sm . Suppose set s = {1 , · · · , k }. We have to show that ^c a , b  s , (a , b )  R ^ c . Also Sm is conFor the purpose of proof by contradiction, let us assume (i1 , i2 )  / R structed by applying i1 and i2 to Si1 and Si2 respectively. Consider the path p1 in SSDAG ^ which starts from Sopt and ends at Sm , and along p1 , Si is the parent of Sm . Now of T 1 along this path, i2 is applied before the application of the swap option i1 . Similarly con^ which starts from Sopt and ends at Sm , and along p2 , sider the path p2 in SSDAG of T Si2 is the parent of Sm . Along this path, i1 is applied before the application of the swap option i2 . Suppose i1 and i2 belongs to OR node v1 and v2 respectively. Since along path p1 , i1 is the swap option which is applied last, Sm contains node v1 . Similarly along path p2 , i2 is the swap option which is applied last. Hence Sm contains node v2 . Therefore, there must ^ , from which there exist paths to node v1 and v2 ­ implies that be an AND node vr in T ^ c . We arrive at a contradiction that proves 1 , · · · , k are mutually connected. (i1 , i2 )  R   Definition B.u [Subgraph of SSDAG] Consider a solution Sp of an alternating AND/OR ^ and mutually connected set Vm of OR nodes in Sp , where vq  Vm , C (Sp , vq ) = tree T s (S , V ) = V Copt (vq ) . The subgraph Gsub p m sub , Esub of the SSDAG with respect to Sp and Vm is defined as follows. Vsub consists of only those solutions which can be constructed from Sp by applying a sequence of swap options belonging to Vm , and Esub is the set of edges corresponding to the swap options that belong to Vm .
s (S , V ) Lemma B.5 The number of total possible distinct solutions at each level d in Gsub p m d-2 , where | V | = n . is n+ m n -1

323

Ghosh, Sharma, Chakrabarti, & Dasgupta

Proof: [Lemma B.5] Consider the swap options that belong to the nodes in Vm . With s (S , V ) is represented by a sequence respect to these swap options, every solution Sr in Gsub p m of numbers of length n, Seq (Sr ), where every number corresponds to a distinct node in Vm . The numerical value of a number represent the rank of the swap option that is chosen for a node vq  Vm . According to the representation, at each level: i. the sum of numbers in Seq (Sr ) of a solution, Sr , is equal to the sum of numbers in  ) of any other solution, S  , in that same level; Seq (Sr r ii. the sum of numbers in Seq (Sr ) of a solution, Sr , is increased by 1 from the sum of  ) of any solution, S  , of the previous level. numbers in Seq (Sr p Hence, at the dth level, there are n slots and d - 1 increments that need to be made to Seq (Sr ). This is an instance of the well known combinatorial problem of packing n + d - 1 objects in n slots with the restriction of keeping at least one object per slot. This can be d-2 done in n+ ways.   n -1 Theorem B.1 The solution space tree constructed by Algorithm 5 is complete. Proof: [Theorem B.1] For the purpose of contradiction, suppose Sm is the first solution which is not generated by Algorithm 5. Also P red(Sm ) = {Spi } and Sm can be constructed from Spi by applying qi , where 1  i  k. From Lemma B.4 it follows that the set of swap options {qi | 1  i  k} is mutually connected. Therefore the set of OR nodes Vm to which the swap options belong is also mutually connected. Suppose |Vm | = n. Consider the solution Sq , where Vm is mutually connected, and for 1  i  k, every qi belongs to the set of native swap options of Sq with respect the swap option that is used to construct Sq . Clearly vt  Vm , C (Sq , vt ) = Copt (vt ) We argue that Sq is generated by Algorithm 5 because Sm is the first solution which is s of T s rooted at S , where only not generated by Algorithm 5. Consider the subtree Tsub q the edges corresponding to swap options that belong to Vm are considered. Now we prove s is equal to the that the number of solutions generated by Algorithm 5 at every level of Tsub s number of solutions at the same level in Gsub (Sq , Vm ). Consider the solution Sq and the set Succ(Sq ). Suppose Succ(Sq , Vm ) is the set of successor solutions that are constructed from Sq by applying the swap options belonging  is the minimum cost solution in Succ(Sq , Vm ). According to to the nodes in Vm , and Smin  Algorithm 5 initially Succ(Smin ) is partially explored by using the set of native swap options  of Smin . Any other non native swap option, b , that belongs to the nodes in Vm , is used to   explore Succ(Smin ), right after the sibling solution of Smin , constructed by applying b to Sq , is added to Closed. Consider the fact that for solution Sq , vt  Vm , C (Sq , vt ) = Copt (vt ) holds. Therefore all the swap options belonging to Vm will also be eventually used to explore  the successors of Smin . Similarly the second best successor of Sq will be able use all but  . one swap option, c , which is used to construct Smin  s The immediate children of Smin in Tsub will consist of all solutions, that can be obtained   by the application of one swap option in Vm to Smin . The native swap list of Smin contains the swap option ranking next to c . The swap options, that are used to construct the other
324

Generating Ordered Solutions for Explicit AND/OR Structures

 n - 1 sibling solutions of Smin , will be used again during lazy expansion, which accounts   for another n - 1 children of Smin . Hence there would be n children of Smin . s Similarly, the second best successor of Sq in Tsub will have n - 1 immediate children. s will have n - 2 children and so on. Now the children The third best successor of Sq in Tsub of these solutions will again have children solutions of their own, increasing the number of solutions at each level of the tree. This way, with each increasing level, the number of solutions present in the level keeps increasing. We prove the following proposition as a part of proving Theorem B.1. s ) is given by Proposition B.1 At any level d, the number of solutions N (d, n, Tsub n s )= N (d, n, Tsub k =1 s )= N (d - 1, k, Tsub

n+d-2 n-1
n

Proof: [Proposition B.1] At second level, there are n solutions. These give rise to solutions at third level. Similarly at fourth level we have
n n -1 n -2 k =1

k

k+
k =1 k =1

k+
k =1

s s ) + ... + 1 ) + N (3, n - 1, Tsub k.... + 1 = N (3, n, Tsub

We can extend this to any level d and the result is as follows.
s ) = 1 N (1, n, Tsub s ) = n N (2, n, Tsub

n

s N (3, n, Tsub )

=
k =1 n k =1

k=

n+1 2 n+2 3

s N (4, n, Tsub ) =

s N (3, k, Tsub )=

s by induction on the depth d. We determine the number of solutions at any level of Tsub

[Basis (d = 1) :]

s ) = n. Clearly, N (1, n, Tsub

[Inductive Step :] Suppose, at dth level the number of solutions is Therefore at d + 1th level,
n s )= N (d + 1, n, Tsub k =1 s )= N (d, k, Tsub

n+d-2 n -1

=

n+d-2 d-1

.

n+d-2 n+d-3 + + ··· + 1 = d-1 d-1

n+d-1 n-1

Since Algorithm 5 does not generate duplicate node, and from Proposition B.1 the s (S , V ) at any level is equal to the number of solutions in number of solutions in Gsub q m s (S , V ) is also generated by s that level of Tsub , at any level the set of solutions in Gsub q m s (S , V ), will s Algorithm 5 through Tsub . Therefore, the level, at which Sm belongs in Gsub q m also be generated by Algorithm 5. Therefore Sm will also be generated by Algorithm 5 ­ a contradiction which establishes the truth of the statement of Theorem B.1.  
325

Ghosh, Sharma, Chakrabarti, & Dasgupta

Appendix C. Conversion between AND/OR Tree and Alternating AND/OR Tree
An AND/OR tree is a generalization of alternating AND/OR tree where the restriction of strict alternation between AND and OR nodes are relaxed. In other words an intermediate OR node can be a child of another intermediate OR node and the similar parent child relation is also allowed for AND node. We present an algorithm to convert an AND/OR to an equivalent alternating AND/OR tree. We use two operations namely, folding and unfolding for the conversions. Corresponding to every edge, a stack, update-list, is used for the conversions. In an AND/OR tree, consider two nodes, vq and vr , of similar type (AND/OR) and they are connected by an edge er . Edges, e1 , · · · , ek emanate from er . [Folding OR Node :] Suppose vq and vr are OR nodes. The folding of vr is performed as follows. · The source of the edges e1 , · · · , ek are changed from vr to vq and the costs are updated as ce (ei )  ce (ei ) + ce (er ) + cv (vr ) where 1  i  k, that is the new cost is the sum of the old cost and the cost of the edge that points to the source of ei . The triplet vr , cv (vr ), ce (er ) is pushed into the update-list of ei , 1  i  k. · The edge er along with node vr is removed from vq . [Folding AND Node :] Suppose vq and vr are AND nodes. The folding of vr is performed as follows. · The source of the edges e1 , · · · , ek are changed from vr to vq . One of the edges among e1 , · · · , ek , suppose ei , is selected arbitrarily and the cost is updated as ce (ei )  ce (ei ) + ce (er ) + cv (vr ) where 1  i  k. The triplet vr , cv (vr ), ce (er ) is pushed into the update-list of ei , whereas the triplet vr , 0, 0 is pushed into the update-list of ej , where 1  j  k and j = i. · The edge er along with node vr is removed from vq . The unfolding operation is the reverse of the folding operation and it is same for both OR and AND nodes. It works on a node vq as follows. Procedure Unfold(node vq )
1 2 3 4 5 6 7 8 9 10 11

forall edge ei that emanate from vq do if the update list of ei is not empty then vt , c1 , c2  pop(update list of ei ); if there exists no edge et from vq that points to the node vt then Create a node vt , and connect vt using edge et from vq ; cv (vt )  c1 ; ce (et )  c2 ; else if c2 = 0 then ce (et )  c2 ; end end

326

Generating Ordered Solutions for Explicit AND/OR Structures

Function Convert takes the root node of AND/OR tree and transforms it to an equivalent alternating AND/OR tree recursively. Function Convert(vq )
1 2 3 4 5 6

7

if every child of vq is a terminal node then if vq and its parent vp are of same type then Apply f old operation to vq ; end else foreach child vr of vq , where vr is an intermediate AND/OR node do Convert(vr ); end

Function Revert takes the root node of an alternating AND/OR tree and converts it to the original AND/OR tree recursively. Function Revert(vq )
1 2 3 4 5 6

if every child of vq is a terminal node then return; Perform unf old operation to vq ; foreach child vr of vq do Revert(vr ); end

The overall process of generating alternative solutions of an AND/OR tree is as follows. The AND/OR tree is converted to an alternating AND/OR tree using Convert function, and the solutions are generated using ASG algorithm. The solutions are transformed back using the Revert function. The proof of correctness is presented below. C.1 Proof of Correctness Suppose in an AND/OR tree T two nodes, vq and vr , are of similar type (AND/OR) and they are connected by an edge er . Edges e1 , · · · , ek emanate from er . Now fold operation 1 is the AND/OR tree which is generated by the application is applied to vq and vr . Let T of the f old operation. Lemma C.1 In the context mentioned above, we present the claim of in the following two propositions. Proposition C.1 The set of solutions of T having node vq can be generated from the set 1 having node v by applying the unfold operation to v of the solutions of of solutions of T q q T .
1 of T 1 that contains node v , there exists a soluProposition C.2 For every solution Sm q  1 tion Sm of T that can be generated from Sm by applying unfold to vq .

Proof: [Proposition C.1] We present the proof for the following cases. Consider any solution of Sm of T that contains node vq .
327

Ghosh, Sharma, Chakrabarti, & Dasgupta

a. vq and vr are OR nodes: There are two cases possible. 1. vr is absent from Sm : Since the fold operation modifies the edge er only, all 1 . Therefore S the other edges from vq in T are also present in T m will also 1 be present in the solution set of T and it will remain unchanged after the application of unfold operation. 2. vr is present in Sm : Since there are k distinct OR edges emanating from vr , let any one of those OR edges, say ei , is present in Sm . We prove that there is 1 of T 1 , such that the application of unfold operation to S 1 will a solution Sm m  generate Sm . The application of fold operation to the node vr modifies the source 1 . and the cost of edge ei from vr to vq and ce (ei ) to ce (ei ) + ce (er ) + cv (vr ) in T 1 is a solution of T 1 , where the edge e is present in S 1 . Also other Suppose Sm i m  1 and S than the subtree rooted at vq , the remaining parts of Sm m are identical 1 1 with each other. Clearly Sm exists as a solution of T and the application of 1 generates S . unfold operation to vq in Sm m b. vq and vr are AND nodes: Since vq is an AND node Sm will contain all of the AND edges that emanate from vq . Therefore edge er and vr both will be present in 1 of T 1 , such that the following holds. Sm . Consider the solution Sm 
1. 1. vq is present in Sm

2. The subtrees rooted at the children of vq other than vr in Sm are identical with 1 . the subtrees rooted at those children of vq in Sm
1 and S are identical 3. Other than the subtree rooted at vq , remaining parts of Sm m with each other. 1 exists as a solution of T 1 and the application of unfold operation to v Clearly Sm q  1 in Sm generates Sm .  of T 1 Any other solution Sm  that does not contain node vq , is a valid solution for T as well.  

Proof: [Proposition C.2] We present the proof for the following cases. Consider any 1 of T 1 that contains node v . solution Sm q  a. vq and vr are OR nodes: Since vq is an OR node, exactly one OR edge ei of vq will 1 . There are two cases possible. belong to Sm
1 : Since the fold operation modifies 1. ei was not modified while folding vr in T the edge er and the OR edges of vr only, all the other edges from vq in T are 1 . Since e was not modified during folding, the same solution also present in T i 1 Sm is also a valid solution for T . 1 : Suppose e connects v and v in 2. ei was modified while folding vr in T i q i 1 and generate solution S . 1 Sm . Apply the unfold operation to the node vq in Sm m The edge ei will be replaced with edge er which connects vq and vr and then ei will connect vr and vi . We argue that Sm is a valid solution of T since the

328

Generating Ordered Solutions for Explicit AND/OR Structures

subtree rooted at vi is not modified by the sequence of ­ (a) the folding of vr to 1 from T , and (b) the unfolding of v to construct S from S 1 . construct T q m  m
1 will contain all of the b. vq and vr are AND nodes: Since vq is an AND node, Sm AND edges that emanate from vq . There are two types of AND edges emanating from 1 and they are (a) Type-1 : the edges from v that are also present in T vq in T q  from vq , (b) Type-2 : the edges that are added to vq by folding and these edges are from 1 and generate solution vr in T . Apply the unfold operation to the node vq in Sm Sm . Sm will contain Type-1 edges, and another edge er from vq . In Sm , vq and vr are connected by er and the Type-2 edges are originated from vr . We argue that Sm is a valid solution of T since the subtree rooted at nodes pointed by Type-2 edges 1 from T , are not modified by the sequence of ­ (a) the folding of vr to construct T  1. and (b) the unfolding of vq to construct Sm from Sm 1 of T 1 that does not contain node v is valid solution for T Clearly any solution Sm q  as  well.  


Lemma C.2 If function Convert is applied to the root node of any AND/OR tree T , an ^ is generated. alternating AND/OR tree T Proof: [Lemma C.2] Function Convert traverses every intermediate node in a depth first manner. Consider any sequence of nodes, vq1 , vq2 , · · · , vqn of same type, where vqi is the parent of vqi+1 in T and 1  i < n. Obviously, the fold operation is applied to vqi+1 before vqi , where 1  i < n. In other words, the fold operation applied to the sequence of nodes in the reverse order and after folding vqi+1 , all the edges of vqi+1 are modified and moved to vqi , where 1  i < n. When the function call Convert(vq2 ) returns, all the edges of vq2 , · · · , vqn are already moved to vq1 and the sequence of nodes, vq1 , vq2 , · · · , vqn are flattened. Therefore, every sequence of nodes of same type are flattened, when the function call Convert(vR ) returns, where vR is the root of T and an alternating AND/OR tree ^ is generated. T ^ , the updateLemma C.3 If function Revert is applied to an alternating AND/OR tree T ^ becomes empty. list of every edge in T Proof: [Lemma C.3] Follows from the description of Revert. Theorem C.1 For any AND/OR tree T , it is possible to construct an alternating AND/OR ^ using function Convert, where the set of all possible solutions of T is generated tree T ^ , and then converting in the order of their increasing cost by applying Algorithm 4 to T individual solutions using function Revert. Proof: [Theorem C.1] According to Lemma C.2, after the application of function Convert ^ is generated. Consider the intermediate AND/OR to T an alternating AND/OR tree T 0 , T 1 , · · · , T n are the trees that are the generated after folding every node in T . Let T   n . Since T i is generated from T i+1 0 = T ,T ^ = T sequence of AND/OR trees and T     
329

Ghosh, Sharma, Chakrabarti, & Dasgupta

i , where 0  i < n, according after folding exactly one node in T i can be generated from T i+1 by unfolding the same solutions of T  ^ , Revert unfolds every node vq in Lemma C.3, for any solution of T ^ . Therefore vq was folded by Convert while transforming T to T ^ . can be generated from the solutions of T

to Lemma C.1, the node. According to that solution, where the solutions of T

References
Bonet, B., & Geffner, H. (2005). An algorithm better than AO ?. In Proceedings of the 20th national conference on Artificial intelligence - Volume 3, pp. 1343­1347. AAAI Press. Chakrabarti, P. P. (1994). Algorithms for searching explicit AND/OR graphs and their applications to problem reduction search. Artif. Intell., 65 (2), 329­345. Chakrabarti, P. P., Ghose, S., Pandey, A., & DeSarkar, S. C. (1989). Increasing search efficiency using multiple heuristics. Inf. Process. Lett., 32 (5), 275­275. Chang, C. L., & Slagle, J. R. (1971). An admissible and optimal algorithm for searching AND/OR graphs. Artif. Intell., 2 (2), 117­128. Chegireddy, C. R., & Hamacher, H. W. (1987). Algorithms for finding k-best perfect matchings. Discrete Applied Mathematics, 18 (2), 155­165. Chen, H., Xu, Z. J., Liu, Z. Q., & Zhu, S. C. (2006). Composite templates for cloth modeling and sketching. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1, pp. 943­950. IEEE Computer Society. Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction to Algorithms (2nd edition). McGraw-Hill Higher Education. Darwiche, A. (1999). Compiling knowledge into decomposable negation normal form. In Proceedings of the 16th international joint conference on Artifical intelligence - Volume 1, pp. 284­289. Morgan Kaufmann Publishers Inc. Darwiche, A. (2001). Decomposable negation normal form. J. ACM, 48, 608­647. Dasgupta, P., Sur-Kolay, S., & Bhattacharya, B. (1995). VLSI floorplan generation and area optimization using and-or graph search. In VLSI Design, 1995., Proceedings of the 8th International Conference on, pp. 370 ­375. Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artif. Intell., 171 (2-3), 73­106. Ebendt, R., & Drechsler, R. (2009). Weighted A search - unifying view and application. Artificial Intelligence, 173 (14), 1310 ­ 1342. Elliott, P. (2007). Extracting the k best solutions from a valued And-Or acyclic graph. Master's thesis, Massachusetts Institute of Technology. Elliott, P., & Williams, B. (2006). DNNF-based belief state estimation. In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, pp. 36­41. AAAI Press.
330

Generating Ordered Solutions for Explicit AND/OR Structures

Eppstein, D. (1990). Finding the k smallest spanning trees. In Proc. 2nd Scandinavian Worksh. Algorithm Theory, No. 447 in Lecture Notes in Computer Science, pp. 38­ 47. Springer Verlag. Eppstein, D. (1998). Finding the k shortest paths. SIAM J. Comput., 28 (2), 652­673. Flerova, N., & Dechter, R. (2010). M best solutions over graphical models. In 1st Workshop on Constraint Reasoning and Graphical Structures. Flerova, N., & Dechter, R. (2011). Bucket and mini-bucket schemes for m best solutions over graphical models. In GKR 2011(a workshop of IJCAI 2011). Fromer, M., & Globerson, A. (2009). An LP view of the m-best MAP problem. In Advances in Neural Information Processing Systems (NIPS) 22, pp. 567­575. Fuxi, Z., Ming, T., & Yanxiang, H. (2003). A solution to billiard balls puzzle using ao algorithm and its application to product development. In Palade, V., Howlett, R., & Jain, L. (Eds.), Knowledge-Based Intelligent Information and Engineering Systems, Vol. 2774 of Lecture Notes in Computer Science, pp. 1015­1022. Springer Berlin / Heidelberg. Gogate, V., & Dechter, R. (2008). Approximate solution sampling (and counting) on AND/OR spaces. In CP, pp. 534­538. Gu, Z., Li, J., & Xu, B. (2008). Automatic service composition based on enhanced service dependency graph. In Web Services, 2008. ICWS '08. IEEE International Conference on, pp. 246 ­253. Gu, Z., Xu, B., & Li, J. (2010). Service data correlation modeling and its application in data-driven service composition. Services Computing, IEEE Transactions on, 3 (4), 279­291. Gupta, P., Chakrabarti, P. P., & Ghose, S. (1992). The Towers of Hanoi: generalizations, specializations and algorithms. International Journal of Computer Mathematics, 46, 149­161. Hamacher, H. W., & Queyranne, M. (1985). K best solutions to combinatorial optimization problems. Annals of Operations Research, 4, 123­143. Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. J. Artif. Intell. Res. (JAIR), 28, 267­297. Hansen, E. A., & Zilberstein, S. (2001). LAO  : A heuristic search algorithm that finds solutions with loops. Artificial Intelligence, 129 (1-2), 35 ­ 62. Homem de Mello, L., & Sanderson, A. (1990). AND/OR graph representation of assembly plans. Robotics and Automation, IEEE Transactions on, 6 (2), 188 ­199. Jim´ enez, P., & Torras, C. (2000). An efficient algorithm for searching implicit AND/OR graphs with cycles. Artif. Intell., 124, 1­30. Kleinberg, J., & Tardos, E. (2005). Algorithm Design. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA. Lang, Q. A., & Su, Y. (2005). AND/OR graph and search algorithm for discovering composite web services. International Journal of Web Services Research, 2 (4), 46­64.
331

Ghosh, Sharma, Chakrabarti, & Dasgupta

Lawler, E. L. (1972). A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem. Management Science, 18 (7), pp. 401­405. Ma, X., Dong, B., & He, M. (2008). AND/OR tree search algorithm in web service composition. In PACIIA '08: Proceedings of the 2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application, pp. 23­27, Washington, DC, USA. IEEE Computer Society. Majumdar, A. A. K. (1996). Generalized multi-peg Tower of Hanoi problem. The Journal of the Australian Mathematical Society. Series B. Applied Mathematics, 38, 201­208. Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for solving mixed integer linear programming problems. In CP, p. 857. Marinescu, R., & Dechter, R. (2006). Memory intensive branch-and-bound search for graphical models. In AAAI. Marinescu, R., & Dechter, R. (2007a). Best-first AND/OR search for 0/1 integer programming. In CPAIOR, pp. 171­185. Marinescu, R., & Dechter, R. (2007b). Best-first AND/OR search for graphical models. In AAAI, pp. 1171­1176. Marinescu, R., & Dechter, R. (2009a). AND/OR branch-and-bound search for combinatorial optimization in graphical models. Artif. Intell., 173 (16-17), 1457­1491. Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search for combinatorial optimization in graphical models. Artif. Intell., 173 (16-17), 1492­1524. Martelli, A., & Montanari, U. (1973). Additive AND/OR graphs. In Proceedings of the 3rd international joint conference on Artificial intelligence, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Martelli, A., & Montanari, U. (1978). Optimizing decision trees through heuristically guided search. Commun. ACM, 21, 1025­1039. Mateescu, R., & Dechter, R. (2008). AND/OR multi-valued decision diagrams for constraint networks. In Concurrency, Graphs and Models, pp. 238­257. Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR multi-valued decision diagrams (AOMDDs) for graphical models. J. Artif. Intell. Res. (JAIR), 33, 465­519. Mathews, D. H., & Zuker, M. (2004). RNA secondary structure prediction. In Encyclopedia of Genetics, Genomics, Proteomics and Bioinformatics. John Wiley & Sons, Ltd. Nilsson, D. (1998). An efficient algorithm for finding the m most probable configurations in probabilistic expert systems. Statistics and Computing, 8, 159­173. Nilsson, N. J. (1980). Principles of artificial intelligence. Tioga Publishing Co. Otten, L., & Dechter, R. (2011). Anytime AND/OR depth-first search for combinatorial optimization. In SoCS. Pearl, J. (1984). Heuristics: intelligent search strategies for computer problem solving. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
332

Generating Ordered Solutions for Explicit AND/OR Structures

Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (2nd edition edition)., chap. Planning, pp. 375­461. Prentice-Hall, Englewood Cliffs, NJ. Shiaa, M. M., Fladmark, J. O., & Thiell, B. (2008). An incremental graph-based approach to automatic service composition. IEEE International Conference on Services Computing, 4 (2), 46­64. Shin, D. H., Jeon, H. B., & Lee, K. H. (2010). A sophisticated approach to composing services based on action dominance relation. In Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific, pp. 164 ­170. Subramanian, S. (1997). Routing algorithms for dynamic, intelligent transportation networks. Master's thesis, Virginia Technical Univ., Dept. of Civil Engineering. Sugimoto, K., & Katoh, N. (1985). An algorithm for finding k shortest loopless paths in a directed network. Trans. Information Processing Soc. Japan, 26, 356­364. In Japanese. Szymanski, M., Barciszewska, M. Z., Barciszewski, J., & Erdmann, V. A. (2005). 5S Ribosomal RNA Database. http://biobases.ibch.poznan.pl/5SData/. Online Database. Takkala, T., Bornd¨ orfer, R., & L¨ obel, A. (2000). Dealing with additional constraints in the k-shortest path problem. In Proc. WM 2000. Topkis, D. M. (1988). A k-shortest path algorithm for adaptive routing in communications networks. Trans. Communications, 36 (7), 855­859. Yan, Y., Xu, B., & Gu, Z. (2008). Automatic service composition using AND/OR graph. In E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services, 2008 10th IEEE Conference on, pp. 335­338.

333

Digital microfluidic biochip revolutionizes the medical diagnosis process rendering multiple tasks executed on a single chip. Incorporation of multiple functionality makes the design process complex and costly for digital microfluidic biochip. Physical simulation for the device components in a biochip is essential in todays manufacturing industry. In this paradigm, design automation and development of computer-aided-design tool that can perform physical level simulation and testing becomes crucial for a successful biochip design. This paper presents a comprehensive survey on design automation for biochip. Initially, a brief description on popular optimization techniques and some heuristic algorithms to solve various optimization problems is presented, followed by a review on biochip design automation works. Generally, architectural and geometry level synthesis for biochip design is performed using optimization techniques. Hence, some recent works on bioassay analysis, resource binding, and scheduling in geometry level are discussed. Finally the survey concludes with some possible future research directions.We present formal methods for determining whether a set of components with given reliability certificates for specific functional properties are adequate to guarantee desired end-to-end properties with specified reliability requirements. We introduce a formal notion for the reliability gap in component-based designs and demonstrate the proposed approach for analyzing this gap using a case study developed around an Elevator Control System.Handling Fault Detection Latencies in Automata-based Scheduling for Embedded Control Software
Santhosh Prabhu M, Aritra Hazra, Pallab Dasgupta and P. P. Chakrabarti Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, West Bengal, India - 721302. Email: {santhosh.prabhu, aritrah, pallab, ppchak}@cse.iitkgp.ernet.in
Abstract--There has been recent interest in automata-based scheduling for dynamic adaptation of schedules for embedded control software. Recently we have shown how automata-based scheduling can be extended to account for the possibility of faults in application of control. In this paper, we address the problem of automata-based scheduling when latencies are associated with detection of faults. We show that the gametheoretic approach that is used for handling faults under full visibility may be suitably augmented so as to decide the scheduling strategy in the presence of latencies in fault detection. Index Terms--Embedded Control Systems, Automata-based Scheduling, Fault-tolerant System, Reliable Scheduling.

I. I NTRODUCTION Typically, in any software controlled embedded system, a control software is executed on a computational platform (often on ECUs/processors) and interacts with the plant through its sensors and actuators. It enacts a set of periodically scheduled control actions to perform a required activity and achieve the desired performance of the overall embedded system. The control components are scheduled in a way so that it meets some performance requirement like exponential stability. A typical software controller in an embedded system can have multiple software components that run on shared ECUs. Each of these components is usually responsible for some aspect of the control activity. Generally the task of scheduling the components so as to satisfy the control requirement is done by a scheduler. Every time a component gets scheduled, it controls the system by applying some transformation, which can be represented by a transformation matrix. Hence, scheduling a component effectively means determining the transformation matrix to be applied on the system. Alur and Wiess [1], [7] have described an automata based solution to the problem of generating schedules that satisfy the exponential stability [2], [3], [4] requirement. Their approach is based on the observation that the language of admissible schedules is  -regular. It involves using a generating automaton to dynamically generate admissible schedules with respect to the stability criterion. The scheduling problem gets more complicated when there exists a possibility of a fault. A fault causes the chosen control actions of a software component to be applied incorrectly, or not applied at all. When this happens, a mutated transformation matrix can be said to be applied on

the system. In a previous work [5], we have addressed the problem of automata based scheduling in the presence of faults, and shown that it is possible to construct an automaton for scheduling the components, even under the possibility of faults. The approach formulates the scheduling problem as a game, such that a winning strategy in the game denotes a safe scheduling strategy for the scheduler. In [5], we have assumed that the scheduler has full visibility of the faults, i.e, the scheduler knows about the fault as soon as it occurs. However, in many real world scenarios, the fault monitoring mechanism might be such, that the occurrence of the fault is not detected immediately by the scheduler. In this paper, we study the problem of scheduling software components in such situations. The primary contributions of this paper is as follows: · We study the problem of reliable control scheduling where the environment can introduce faults in the applied transformations scheduled by the controller and these faults are not immediately visible to the scheduler. · We formulate this problem as a partial observability game between environment and scheduler. · We derive an automaton which can generate schedules that adheres to the exponential stability criterion under partial observability of faults. · We present a set of experimental results showing the efficacy of our proposed method. This paper is organized as follows. In Section II, we describe the background for the problem of scheduling software components. The problem is presented formally in Section III. Section IV describes the mechanism for handling latencies in fault detection. We describe the implementation details and present the results in Section VI. Section VII concludes this paper. II. BACKGROUND Formally, the dynamics of the physical plant can be approximated by a given discrete-time linear time-invariant system, described as: xp (t + 1) y (t) = Ap xp (t) + Bp u(t) (1) (2)

= Cp xp (t)

The state of the plant is defined by the set of plant variables, xp , the output of the plant is y , and the control input of the

plant is u. The matrices Ap , Bp and Cp denotes the state transition matrix, the input map, and the output map for the plant model. Equation (1) defines the state transition function of the plant, and Equation (2) defines the plant output. The feedback control software reads the plant output, y , and adjusts the control variables, u. The controller can be designed as a linear time-invariant system such that the stability [3], [4] of the whole system (the controller and the plant together) is guaranteed when the output of the plant is fed to the input of the controller and vice versa. Formally, it is assumed that the controller of such an embedded system can be represented as a discrete-time linear time-invariant system: xc (t + 1) = Ac xc (t) + Bc u(t) (3)

At+1 . . . At+L of Z . We shall refer to sequences of transformations satisfying the exponential stability requirement as admissible sequences. It was shown in [7] that the language of admissible sequences is  -regular. The language can be expressed as: L(A) =  -    where  consists of the L-length sequences of transformations that violate the exponential stability criteria. In [1], it was shown that it is possible to construct a finite state automaton, such that any infinite random walk in it represents an admissible sequence of transformations and vice versa. III. P ROBLEM F ORMULATION

The role of the automata-based scheduler described in [1] is to ensure that the sequence of control components that u(t) = Cc xc (t) (4) are scheduled adheres to the admissible patterns with respect where, xc is the state of the controller and Ac , Bc and Cc are, to the stability criteria. In reality, the application of control respectively, the state transition matrix, the input map, and the can be non-ideal due to several other factors such as delay output map for the controller. Equation (3) and Equation (4) in receiving sensory inputs and electrical faults affecting define the controller state transition function and the output the sensors / actuators, even when the software components function, respectively. The dynamics of the composed system are scheduled correctly. Following the terminology of the (the controller and the plant together) can be described by previous section, a fault at a time step is manifested as a transformations of the form: mutation on the transformation scheduled in that time step. In other words, we consider scenarios where the scheduler selected some transformation Ai , but the actual execution Ap Bp C c x(t + 1) = x(t) (5) resulted in a transformation, A ^i , which differs from Ai in Bc C p Ac terms of the updation of the control variables that are affected T T where x = (xT by the fault. p , xc ) is the concatenation of the states of the plant and the controller. In general different types of faults may be manifested We can represent the sequence of control actions applied as different mutations on a transformation matrix, Ai . Our on the environment by one or more controllers, by a sequence approach is capable of handling multiple types of faults, but of transformation matrices, A1 , A2 . . ., where each Ai is we consider a single type of fault for ease of presentation. chosen from an alphabet  of transformation matrices. The We are given a set  of transformation matrices, which matrix in Equation 5 is one such transformation matrix. correspond to simultaneous control actions by one or more We denote by Ci a particular combination of controllers software components. We are also given f , the mutated that may execute together. A word  = 1 2 . . ., (where versions of the transformation matrices in . We assume each i  [1, m]) specifies a schedule, which denotes the that within any given window of length L, a maximum sequence of combinations C = C1 C2 . . .. The sequence of k faults can occur. Also, it is assumed that the fault of transformations corresponding to this schedule is given by is detected by the scheduler after m units of time, where A = A1 A2 . . .. m  L (We believe that our approach will hold even when One of the standard control requirements is that of expo- m > L, but such scenarios are realistically not possible). nential stability. A system (whose state is defined in terms of Such criteria indicating fault detection latencies introduces n variables) is said to be (L, )-exponentially stable, given the a new dimension to the problem formulation and proposed parameters L  N and  (0, 1], if ||x(t + L)||/||x(t)|| < approach in comparison to our previous work [5] where we for every t  N. It follows from control theory and the work provided reliable control schedules under faults with full presented by Weiss and Alur [1], [7] that the exponential visibility. stability requirement can be captured by the following lanHowever, in this work, our problem is to arrive at a strategy guage: for scheduling the transformation matrices from  such that, even in the presence of up to k faults in any window of length ExpStab(L, ) = {  I  : L, and the faults being detected only after m time instants, ||At+L · · · At+1 || < f or every t  N} the exponential stability requirement is not violated. This definition means that an infinite sequence, Z , of transExample 1: Consider a braking system, which reduces the formations satisfies the exponential stability requirement iff speed of a vehicle using a combination of brakes and throttle ||At+L · · · At+1 || < for every L length subsequence control. We are given two transformation matrices AR and

AS , corresponding to application of brakes and adjusting the throttle. Assume that a fault may cause the apply brake action to manifest incorrectly at most once (k = 1) in every 3 cycles ^S being (L = 3), resulting in the transformation matrix A applied. The fault detection latency is assumed to be 1. We are given that the stability requirement admits the following set of sequences: { AS , AR , AS , AS , AR , AR , AS , AS , AR , ^S , AR , AS , AS , AR , A ^S , AR , AS , AS , A ^S , AR , AR , AR , A ^S } AR , AS , AR , AR , A The scheduler must ensure that in every window of three steps, the transformation applied is confined to these sequences ­ even in the presence of faults. IV. S CHEDULING UNDER FAULTS WITH L ATENT V ISIBILITY We have formulated the problem of scheduling with fault detection latencies as a partial observation game between the scheduler, which is the protagonist and the environment, which is the antagonist1 . In every round of the game, the scheduler decides on a transformation matrix to be scheduled, and the environment chooses whether to mutate it or not, constrained by the assumptions in the fault model. The environment's objective is to force an inadmissible sequence to be scheduled, while the scheduler's is to prevent that from happening. Due to the latency in fault detection, the scheduler is unable to immediately determine which of the possible set of moves the environment has opted for. The state space for the game is defined by the set of L length sequences over   f , along with the current turn. We define the game graph as G = (V, E ) as follows: · The set V of vertices consists of two types of vertices. Those where the scheduler makes its move (VS ), and those where the environment makes its move (VE ). In addition to the turn, the vertex also stores the L length history of transformation matrices. The sequences in VS are the ones which are actually applied on the system. On the other hand, the last matrix in the sequences associated with vertices in VE may get mutated by the environment in its move. · The set E of edges contains edges between every vertex u to v from V such that, 1) the players getting the turn in u and v are different 2) If u  VS , sequ [2 · · · L] = seqv [1 · · · L - 1], and the edge is labeled by the matrix seqv [L]. 3) If u  VE , sequ [1 · · · L - 1] = seqv [1 · · · L - 1], the edge is labeled by the matrix seqv [L], and seqv [L] = sequ [L] or seqv [L] = sequ [L]. Here, sequ and seqv denote the sequences associated with u and v respectively.
1 We point out here that the adversarial modeling of the environment has not been made after a qualitative analysis. There may exist non adversarial models of the environment which may also be suitable.

Due to the partially visible nature of the game, the scheduler will not be able to recognize the mutations in the most recent window of length m. The intuitive idea for constructing a winning strategy for the scheduler is quite straightforward ­ we try to come up with a strategy such that the scheduler's response to every move of environment, will guarantee that the scheduler can force victory irrespective of what move the environment has chosen. The responses to the move begin to differ only when the actual nature of the environment's move becomes known to the scheduler. To obtain such a strategy for the scheduler, we first expand the graph as a tree and using the Min-max algorithm[6], we annotate each state with win/loss markings. In order to be able to perform such an annotation, it is necessary to first define the winning and losing leaf nodes. The definition of losing leaf nodes is straightforward. Any state whose L length sequence is present in the set of inadmissible sequences is a loss node. For defining the winning leaf nodes for the scheduler, we use the following lemma: Lemma 1: If the antagonist has a winning strategy from a node v in the game tree, and there is another occurrence of v on the path from the root, then the antagonist has a winning strategy from the previous occurrence of v that does not involve the latter occurrence of v . Proof: The antagonist wins if the state of the game reaches a loss node. Therefore, a winning strategy for the antagonist is a subtree, W , of the game tree, T , such that all leaf nodes of W are loss nodes and at each intermediate node of W , the following condition holds: each node u  VE in W has a single successor from T in W and each node u  VS has all successors from T in W . Consider the smallest subtree, W , which is a winning strategy from a node v . Then W cannot contain another occurrence of v , otherwise the subtree of W rooted at the second occurrence will be a smaller winning strategy. Since the subtrees rooted at each occurrence of v are isomorphic, the winning strategies from each occurrence of v are also isomorphic. So, the smallest winning strategy from the first occurrence of v does not involve the subsequent occurrences of v in the game tree. On the basis of this lemma, we mark as a winning leaf node any state where the protagonist(the controller) has the turn, and has already been seen in the path to it from the root. Once this is done, the Min-max algorithm may be applied directly. Figure 1 illustrates a portion of the Minmax tree, for the controller described in Example 1. The game is assumed to start from the state corresponding to the sequence AS , AR , AS . If the scheduler chooses AR in the first round, the environment will be unable to mutate it, and it will be applied correctly. This takes the game to the state corresponding to the sequence AR , AS , AR . Then, if in the second round, the scheduler chooses AS and the environment chooses not to mutate it, the game reaches the state with sequence AS , AR , AS , which is a repetition, and hence labeled as a winning state. That this tree gives all

possible winning strategies for the controller under the given fault model, when the faults are immediately visible, is stated by the following lemma. Lemma 2: The scheduler can guarantee a schedule satisfying the exponential stability criterion if and only if the protagonist has a winning strategy in our game formulation. Proof (Reproduced from [5]): We recall that a schedule satisfies the exponential stability criterion if the sequence of transformations applied in this schedule does not contain any inadmissible L-length subsequence. A winning strategy for the protagonist in the game is one that prevents the state of the game to reach any LOSS node, which represents a state of the system where the last L transformations constitute an inadmissible sequence. Therefore a winning strategy for the protagonist guarantees a schedule satisfying exponential stability. If the protagonist has no winning strategy, then the antagonist can lead the game to a LOSS node. In other words, the antagonist can choose a suitable sequence of mutations for each choice of transformations by the protagonist, such that an inadmissible sequence of transformations is guaranteed to be applied on the system. Now, it remains to be shown that the transformations that the scheduler can choose in order to satisfy exponential stability is no different from the choices available to it in the game as the protagonist, and that the environment as the antagonist in the game is only as powerful as it is in the real world, in terms of the mutations it can introduce. We make the assumption that any transformation matrix Ai   can be chosen by the protagonist at any instant, irrespective of the history of the game. So, the choices of transformation matrices available to the scheduler in the game is same as the choices available to it for scheduling. Given that only a maximum of k mutations are possible within any L-length window, and that the mutations are governed by no other constraint, whether the transformation matrix scheduled next can be mutated, is determined entirely by the number of mutations that have been introduced within the last L-length window. This restriction is placed on the antagonist of the game too. So, the antagonist is only as powerful as the environment in the real world. Once we obtain the game tree with win/loss markings on each state, we define an equivalence relation R on the set of states. Two states q and q are said to be equivalent with respect to R if they are indistinguishable for the controller. For example, in Figure 1, the states n4 and n5 are equivalent with respect to R, if the fault detection latency is not zero. We now construct a modified game tree, as follows: For every set S = {q1 , q2 . . . qn } of states which are equivalent with respect to R, we look for isomorphic winning strategies that are applicable from all qi  S . All other winning strategies are discarded, by marking the moves as losing moves. If there does not exist such an isomorphic winning strategy, we mark each qi  S as a loss node. After this step, we apply the Min-max algorithm again, and repeat the above procedure. The iteration stops when no more

A S ,AR ,AS AS n2 AS n4 L A R ,AS ,AS AS AR AS n7 SCHEDULER'S TURN ENVIRONMENT'S TURN W - WIN NODE L - LOSS NODE AS AS n9 W AR A S ,AR ,AS AR n8 Round-2 AS n5 n1 AR n3 STATE REPETITION n6 Round-1

AR

AS n10

AR

Fig. 1.
n1 AR n2 AR n3 W AS W n5 AR W n6 W

A section of a Game tree for Example 1
Winning Subtree W n1 AR n2 AR W AS L n7 AR W n8 AS W n5 n6 n4 W AR W AR L AS L n7 n8 AR W n4

W

(full visibility)

(A)

AR n3 W

W

n1 AR n2 W

Winning Subtree (latent visibility)

?

n1 AR L n2 AR n4 L

n3 W AS L n5

AR

AR W

(B)
n4 AR W n8 n3 L AS L n5

AR

AR W n6

AS L n7

AR L n6

AS L n7

AR L n8

Fig. 2. A snapshot showing the re-labeling of game trees (These trees are not necessarily subtrees of the game tree for the speed governor example).

changes are necessary to the game tree. Figure 2 illustrates how the markings on the game trees may change in each iteration of the above process. In the first game tree, nodes n3 and n4 are equivalent, and there exists an isomorphic winning strategy from both nodes, that of choosing AR . Choosing AS , however, is not a winning strategy from n4 , and hence, choosing AS is marked as a losing move from n3 , by relabeling n5 as a loss node. In the second game tree, the nodes n3 and n4 are again equivalent. On applying the same strategy, n5 and n8 gets relabeled as loss nodes, which in turn results in n3 , n4 and n2 being relabeled as loss nodes. Had there been no latency in fault detection, these nodes would have been winning nodes for the scheduler.

Theorem 1: The re-labeled game tree contains exactly those strategies that do not require the scheduler to behave differently in two states that appear similar to it due to fault detection latencies. Proof: Since the re-labeled subtree is obtained by dropping strategies from the original tree, and for no two equivalent states is the set of outgoing transitions different, it is clear that every strategy present in the tree is correct. We now prove that every correct strategy is present in the new tree. Suppose that there exists a subtree for a strategy that was present in the original game tree, but is not present in the new tree. We prove that such a subtree cannot exist. Consider a subtree of height zero. The root of such a tree will be marked as a losing state, even though it may not be among the inadmissible sequences. This is true because it is not possible for the scheduler to distinguish between this state and some state that is actually a losing state. Such states should be marked as loss, since the attempts by the scheduler to take the game to such a state might actually be leading the game to a losing state. Now assume that no strategy of upto h rounds is missed. Consider a strategy of h + 1 rounds. Since the strategy doesn't require the scheduler to behave differently in two states that appear similar, the first move of the strategy must be applicable in all states that are equivalent to the root of the subtree. But if that be the case, the remainder of the strategy would still be in the tree, by our assumption. So, no strategy of h + 1 rounds is missed. Thus, by induction, the re-labeled tree contains all strategies in which the scheduler behaves identically on the same information. V. AUTOMATON FOR GENERATING SCHEDULES From the marked Min-max tree for the game formulation, we construct the automaton for generating the schedules. The automaton is defined by a tuple Q, , , q0 , where:
·

VI. I MPLEMENTATION AND E XPERIMENTAL D ETAILS We have implemented an optimized version of the proposed approach, in which we intelligently prune subtrees based on equivalence even as the Min-max algorithm assigns the initial markings to the nodes. For doing this, we keep track of equivalence between the explored states, and whenever the strategy of a state is modified, the strategies of all states equivalent to it are also modified. Whenever a state is found to be losing, the subtrees rooted at that node and all other nodes equivalent to it are pruned. Table I presents the results obtained from the implementation. The results are obtained for different values of L, different alphabets, different sets of admissible sequences and different values of m and k . Columns 1-3 give the window length (L), maximum number of injected faults (k ) and the latency in fault visibility (m), respectively. The number of transformation matrices are reported in Column 4. Column 5 presents the size of the equivalence classes and the required time for the overall analysis (in seconds) is outlined in Column 6. The execution time was obtained on a 1.6GHz quad core Intel Core i5 with 4 GigaBytes of memory, running 32-bit Linux. The language used for the implementation was Java (OpenJDK 1.6.0 24).
L 10 10 9 9 8 8 8 8 8 8 7 7 6 6 5 5 k 1 1 1 2 4 4 3 3 2 2 1 1 1 1 1 1 m 4 3 3 2 3 4 2 3 2 3 2 3 2 2 2 2 || 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 Number of Equivalence Classes 4119 4637 2072 6303 3550 2346 4822 2736 2336 1501 918 395 248 2720 107 1007 Time (in sec) 16.801 17.681 4.072 46.479 220.706 49.867 60.752 33.95 7.608 5.056 1.512 0.484 0.3 9.621 0.092 1.576

· ·

·

Q, the set of states of the automaton consists of L length sequences from (  f )k , in which not more than k symbols are from f . Also, the last m symbols should necessarily be from .  is, as defined previously, the set of transformation matrices   Q ×  × Q is the transition relation, which is defined as follows. (u, Ai , v )   if Ai is part of a winning strategy from the state with sequence u where protagonist has the turn and, v satisfies the following conditions: ^i and, 1) v [L] = Ai or v [L] = A 2) v differs at most at one position from u, and if it does, the difference will be that the L-m+1th symbol in v is the mutated version of the corresponding symbol in u. q0 is the predefined initial sequence

TABLE I E XPERIMENTAL R ESULTS

The results show that the proposed method works in reasonable time. It can be seen that the time for computing the strategy increases with L, k and the size of . A higher value of L means that a longer history needs to be remembered, and hence, the state space will larger. k and || affect the execution time by determining the branching factor of the Min-max tree. As k and || increases, the branching factor increases, thereby increasing the execution time. On the other hand, the execution time decreases as the latency (m) increases. This is because more subtrees get pruned out, and thereby, less states need to be explored.

VII. C ONCLUSION We have described how latencies in fault detection may be handled in the automata-based scheduling framework. We have shown that the game theoretic approach can be used with certain modifications, in cases where faults are not immediately visible. Experimental results are found to be quite encouraging. ACKNOWLEDGEMENT Pallab Dasgupta and P. P. Chakrabarti acknowledge the support of IGSTC (Indo-German Science and Technology Center) Project. Aritra Hazra is supported by Microsoft Corporation and Microsoft Research India under the Microsoft Research India PhD Fellowship Award. R EFERENCES
[1] R. Alur and G. Weiss. Regular Specifications of Resource Requirements for Embedded Control Software. In the Symposium on Real-Time and Embedded Technology and Applications (RTAS), pages 159­168, 2008. [2] L. Gurvits. Stability of Discrete Linear Inclusion. Linear Algebra Applications, 231:47­85, 1995. [3] J. P. Hespanha and A. S. Morse. Stability of Switched Systems with Average Dwell-time. In Proceedings of the 38th IEEE Conference on Decision and Control, volume 3, pages 2655­2660, 1999. [4] D. Liberzon. Switching in Systems and Control. Birkh¨ auser, 2003. [5] S. P. M, A. Hazra, and P. Dasgupta. Reliability Guarantees in Automata Based Scheduling for Embedded Control Software. To appear in IEEE Embedded Systems Letters (An extended version of this is available at: http://facweb.iitkgp.ernet.in/pallab/TechRep/ESL2013TR.pdf), 2013. [6] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Pearson Education, 2003. [7] G. Weiss and R. Alur. Automata Based Interfaces for Control and Scheduling. In 10th International Workshop on Hybrid Systems: Computation and Control (HSCC), pages 601­613, 2007.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/256537661

ARankingAlgorithmforOnlineSocialNetwork Search
ConferencePaper·August2013
DOI:10.1145/2522548.2523134

CITATIONS

READS

2
4authors,including: DivyaSharma IndianInstituteofManagement
6PUBLICATIONS10CITATIONS
SEEPROFILE

539

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

DebashisSaha IndianInstituteofManagementCalcutta
206PUBLICATIONS1,582CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyParthasarathiDasguptaon16April2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

A Ranking Algorithm for Online Social Network Search
Divya Sharma
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

AKZ Alam
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

Parthasarathi Dasgupta
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

Debashis Saha
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

divyas12@iimcal.ac.in

abula12@iimcal.ac.in

partha@iimcal.ac.in

ds@iimcal.ac.in

ABSTRACT
Online Social Networks have become the new arena for people to stay in touch, pursue their interests and collaborate. In October 2012, Facebook reported a whopping 1 billion users which is testimony of the fact how online social networks have proliferated and made inroads into the real world. Some of the obvious advantages of social networks are (1) 24X7 availability allowing users to stay in virtual touch at any time of the day, (2) get in touch with people who have similar interests and collaborate with them and (3) the ability to be able to search for users to add to one's friend circle. The third advantage is the focus of this paper. With the increasing number of users on online social networks, it is important that when a user searches for another user, appropriate results are returned. This paper identifies three criteria ­ proximity, similarity and interaction, which can be used to rank search results so that more appropriate results are presented to the searching user. Also, this algorithm allows the search ranking to be customized according to the nature of the online social network in question.

search for potential friends and in doing this, return relevant results. The need for such a search technique arises due to the inherent structure of social networks and the behavior of users on the network. Most searches on an online social network are queries containing names of users and a multitude of users may share the same name, which makes the trivial task of searching for friends very cumbersome. In order to overcome this challenge, it would make sense to rank the list of search results in order of decreasing relevance to the user searching for friends. Here we discuss social network search ranking by means of an algorithm which takes into account three important factors that make search results relevant to a user ­ proximity, similarity and interaction. Based on these three factors, search ranks are allotted to search results when one user searches for another user by name. Also this algorithm takes into account the fact that all social networks are not of the same type. When a user searches for friends on a network of classmates, it is more relevant to rank results, which are closer to the searching user in the virtual space higher in the results list. On the other hand, on a social network for football fans, ranking must be done on the basis of sharing common interests like being fans of the same football club or the same football players. Still another example is a social network of acquaintances, where ranking on the basis of the level of interaction among users might be a more useful criteria. Rest of the paper is organized as follows. Section 2 discusses some of the related recent works. Section 3 describes the proposed work including the definitions of the ranking metrics used, the proposed rank function, and the proposed method. Section 4 summarizes the empirical results and Section 5 highlights some of the real-life applications of the proposed method. Finally, Section 6 gives the concluding remarks and discusses scopes for future works.

Categories and Subject Descriptors
H.3.5[Information Storage and Retrieval]: Online Information Services - Web-based services

General Terms
Algorithms, Management

Keywords
Algorithm, Online Social Network, Search Ranking.

1. INTRODUCTION
Social networks consist of sites that allow people to interact and share social experiences by exchange of multimedia objects (text, audio, and video) associated with the people themselves and their actions [7]. Every user on a social network possesses a user profile that contains all the information about the user ranging from basic information like name, date of birth, gender, location to more detailed information capturing educational and professional information and areas of interest. Online social networks have become abstractions of the real world where users interact, exchange and keep in touch. A major challenge, therefore, is the ability of a social network site to allow users to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. Compute '13, August 22 - 24 2013, Vellore, Tamil Nadu, India Copyright 2013 ACM 978-1-4503-2545-5/13/08...$15.00. http://dx.doi.org/10.1145/2522548.2523134

2. RELATED WORK
Recently, there has been lots of interest in the field of online social network search and ranking. The work in [7] focuses on the problem of how to improve the search experience of the users. It suggests seed based ranking instead of text-based ranking by measuring shortest distances between the nodes in a friendship graph. This is the first work that makes use of the friendship graph in a big social network to improve the search experience. The paper in [1] presents a novel social search model for finding a friend with common interests in OSN (Online Social Network) with the introduction of trust value and popularity value. The trust value is calculated by the improved shortest path algorithm with a trust threshold, and the popularity value is obtained by the page rank algorithm iteratively. In order to ensure more accurate search results, [3] demonstrates an algorithm called E.L.I.T.E. which has five essential components - Engagement-U, Lifetime, Impression, Timeframe and Engagement-O. Engagement-U is the affinity between users which is measured by their relationships and other related interests between them, Lifetime is a trace of users' past based on their positive, neutral and even negative interactions and

actions with other users, Impression is the weight of each object determined by the number of positive responses from users, Timeframe is the timeline scoring technique in which an object naturally loses its value as time passes and Engagement-O is the attraction of users to objects which is measured between objects and associated interests of users. Emphasizing on tree based search techniques, [4]compares the efficiency of reliable searching between Maximum Reliable Tree (MRT) algorithm and Optimum Branching Tree (OBT) algorithm and proposes the use of the MRT algorithm that is newly developed based on a graph-based method, as a generic technique which facilitates effective social network search and that can be the most reliable social network search method for the promptly appearing smart phone technologies. [5] explores the correlation between preferences of web search results and similarities among users by presenting an efficient search system called SMART finder. The work provides more information about SMART finder and publishes a quantitative evaluation of how SMART finder improves web searching compared to a baseline ranking algorithm. The concept of user tag feedback scores is employed in [2]. Based on this concept, a tag-based feedback web ranking algorithm is designed. The algorithm can efficiently use t he user's feedback. Searching for another person is one of the most fundamental uses to which social networks are put. However, most of the research work until now does not specifically look into this area. A small quantum of research work which does delve into this topic either provides just a conceptual framework or does not discuss the implementation of the framework in detail. Considering that online social networks are an abstraction of realworld social networks, it is important to understand the factors that influence the associations between the users. From experience it can be said that one person's association with another person is a function of how closely linked the two users are, how many interests they share and how often they interact. We call these three factors proximity, similarity and interaction respectively. Though this is not an exhaustive list of factors that might influence association between two persons, it does give a fairly good idea about the same. This work tries to use this concept of association between users to rank search results when a particular user searches for another user on an online social network. When any framework is to be implemented, it is required that the various factors which are being considered are quantified. The proposed algorithm in this work provides such a means to quantify the factors like proximity, similarity and interaction and then combine them to give a value for association between two persons. Along with providing a conceptual framework, this work also details out the implementation of the framework in practice.

node. It is calculated by running a shortest distance algorithm and finding the minimum distance of the various nodes in the search result from the searching node. Let d ab be the shortest distances between nodes a and b, then proximity pab is calculated as

Proximity may also be calculated as (1 -dab/D), where D is the diameter of the social network, used to normalize dab. This would ensure the maximum value of proximity to be 1.

3.1.2 Similarity
Social networks provide users a platform for interacting with other users who share similar interests, listen to the same kind of music, read books from the same author, follow the same sport, share the same hobbies, etc. On a social network all these details are captured in the user profiles. When a user issues a search for another user, a user who is more similar to the searching user is likely to be a more relevant result in comparison to a user who is less similar. We define a user profile of user a as

where k, l, m, n ... are the interests. Similarity S, between two nodes a and b in a list of search results with n nodes is defined as

Alternatively, similarity may be defined in terms of the Jacquard similarity coefficient as

In this case, we have considered the former definition of in order to define similarity in the context of all possible users' profiles. Though our definition of the might yield very small values of the metric, this would not affect the relative ranking of the users.

3.1.3 Interaction
Social networks provide users different means for interacting with one another. Most online social networks allow users to interact via textual comments, exchange links through shares and like the posts of other users. When two users interact on a social network, there are two factors that can be used to gauge the closeness of these two users ­ frequency of interaction and recency of interaction. Frequency captures volume of interaction (for each of comment, share, like) between two users within a given span of time. This span of time is defined by window size w, defined further in the discussion. Recency captures the time gap between the time of issuance of the search query and the most recent interaction (for each of comment, share, like) between the searching user and the user being searched. Frequency of an interaction of type T between user a and b is defined as

3. THE PROPOSED WORK 3.1 Ranking Metrics
This paper defines three metrics for the purpose of ranking search results:

3.1.1 Proximity
On a social network, a user may be linked directly and indirectly to millions of users. A social network is a myriad web of interconnections and a node which is closer to the searching node in terms of number of hops is likely to be a better search result in comparison to a node which is several hops further away. Proximity measures the closeness of a node from the searching

where Ti is the type of interaction ( i = 1 for comment, i = 2 for share and i = 3 for like) and VabTi is the volume of interactions between users a and b of type Ti. Recency of interaction between users a and b is defined as

where to is the time instance at which the search query was issued, tabTi is the time instance of the most recent interaction between user a and b of type Ti and window size wTi is defined as where users b, c, d, ..., n are the search results of the query. The frequency and recency metrics are then used to define interaction i of type Ti between two users a and b as where 0    1.  is defined as the relative importance of recency in comparison to frequency when quantifying a particular interaction. The weighted interaction metric I between two users a and b is defined as the weighted sum of the three types of interactions (comment, share, like) as where +  +  = 1. Here ,  and  define the percentage importance of the three types of interaction i.e. comment, share and like in the overall interaction metric I.

association function is central to the calculation of the ranks of the search results. We consider that n potential search results are returned by the searching algorithm for a given query. Once this list is obtained, the next steps are to calculate the association and then rank the list on the basis of the association values. The pseudo code for the algorithm is presented below. The variables and functions used in the pseudo code are explained in Table 1. Table 1. Description of Variables/Functions Variable/Function search_results ranked_list window_sizes Description Array of unranked results of the search query Array of ranked results of the search query An array of window sizes for interactions of type comment (index 1), share (index 2) and like (index 3) Cardinality of the set of common interests of the searching user and the search results Array of the association values of the search results Window recency size for comment

common_interests_ cardinality association comment_recency_ window share_recency_window like_recency_window search() compute_ranks() get_window_sizes()

3.2 The Association Function
Once the three metrics proximity, similarity and interaction have been defined, the next step is to define the association between the searching user and the search results based on these three parameters. Association captures the effect of the three metrics in question and returns a composite value which describes how closely the user is associated with each of the search results. As discussed earlier, the importance of each of these metrics may vary according to the nature of the online social network. Therefore, weights are defined to give different weightage to each of these factors while calculating the rank of the search results. The weighted association of a search result, b, being searched by user a is defines as whereµ1, µ2and µ3 are the weights assigned to each of the metrics pab, Sab and Iab. It is important to note that Therefore, µ1, µ2 and µ3 can be defined as the percentage importance of proximity, similarity and interaction in calculating the association according to the nature of the social network. For example, in social network catering to football fans, where similarity is more important than proximity and interaction, µ2 may have a higher value, while on a social network for classmates, proximity is more important, so, µ1may have a higher value.

Window size for share recency Window size for like recency Function to search results of a query potential

Function to rank the search results Function to compute the window sizes for interactions of type comment, share and like Function to compute cardinality of the set of common interests Function to compute association values of the search results Function to sort the list of search results according to association values Function to calculate the value of proximity metric between two nodes Function to calculate the value of similarity metric between two nodes Function to calculate the value of interaction metric between two nodes Function to return the number of interactions of type j between two nodes Function to returns the recency of interaction between two nodes

get_common_interests_ cardinality() compute_association() sort()

get_proximity()

3.3 The Rank Function
The ranks of the search results are subsequently obtained by sorting the results by the weighted association values. The search with the highest weighted association value is given rank 1, the search result with the second highest weighted association value is give rank 2, and so on, until the search result with the lowest weighted association value is given rank n. get_similarity()

get_interaction()

3.4 Algorithm for Computing Search Ranks
The proposed algorithm is a ranking algorithm and, therefore, allows different search algorithms to be used to identify the unranked search results. Once the unranked search results are obtained, they are then ranked using the proposed algorithm. The

interaction_volumej()

recency_of_interaction()

Pseudo Code: 1. Initialize the network N, searching user s, search query q 2. rank_search_results(N, s, q) 3. begin 4. search_results[1 ... n] search(q, N) 5. ranked_results[1 ... n] compute_ranks(s, search_results) 6. end 7. compute_ranks(s, search_results[1 ... n]) 8. begin 9. window_sizes[1 ... 3]get_window_sizes(s, search_results[1 ... n]) 10. common_interests_cardinalityget_common_interests_ cardinality(s,search_results[1 ... n]) 11. association[1 ... n] compute_association(s, search_results, window_sizes, common_interests_cardinality) 12. ranked_results[1 ... n]  sort(association[1 ... n]) 13. end 14. get_window_sizes(s, search_results[1 ... n]) 15. begin 16. comment_recency_window0 17. share_recency_window0 18. like_recency_window0 19. for i = 1 to n 20. comment_recency_windowmax(comment_recency_ window, comment_recencysi) 21. share_recency_windowmax(share_recency_window, share_recencysi) 22. like_recency_windowmax(like_recency_window, like_recencysi) 23. end 24. window_sizes[comment_recency_window, share_recency_window, like_recency_window] 25. end 26. get_common_interests_cardinality(s, search_results[1 ... n]) 27. begin 28. common_interestsinterestss 29. for i = 1 to n 30. common_interestscommon_interests interestsi 31. end 32. common_interests_cardinalitycardinality(common_in terests) 33. end 34. compute_association(s, search_results[1 ... n], window_sizes, common_interests_cardinality) 35. begin 36. for i= 1 to n 37. proximitysiget_proximity(s, search_resultsi) 38. similaritysiget_similarity(s, search_resultsi)/common_interests_cardinality 39. interactionsi get_interaction(s, search_resultsi, window_sizes) 40. associationsi µ1proximitysi+ µ2 similaritysi+ µ3interactionsi 41. end 42. end 43. get_proximity(s,i) 44. begin 45. proximity 1/(1+distance(s,i)) 46. end 47. get_similarity(s, i)

48. begin 49. similarity interestss interestsi 50. end 51. get_interaction(s, i, window_sizes) 52. begin 53. for j = 1 to 3 54. frequencyj 1 ­ 1/interaction_volumej(s,i) 55. recencyj 1 ­ recency_of_interactionj(s,i)/ window_sizej 56. end 57. frequency frequency1 +  frequency2+  frequency3 58. recency recency1 + recency2+  recency3 59. interaction   recency + (1 ­ ) frequency 60. end The search and sort functions in the above pseudo code are place holders for any searching and sorting algorithms that returns the list of search results based on the searching user's query and sort the search results on the basis of calculated association values respectively. Further details regarding the algorithm and its complexity have been described in [6].

4. EMPIRICAL RESULTS 4.1 The Framework
We propose to model a social network in the form of a graph, where the nodes correspond to the users and the edges correspond to the friendship between them. A part of one of the author's online social network was considered and anonymized for the purpose of this paper. Figure 1 shows this network, where a user John issues a search for another user Maria. The search query will return Maria a, Maria b and Maria c. The task now is to rank these results according to relevance to user John.

4.2 Simulation and Results
We now return to the framework described in Figure 1. The profiles of users we are interested in i.e. John, Mariaa, Mariab and Mariac are defined as follows:

The interactions between John and Maria a, Mariab and Mariac are defined in Table 2.The ranking algorithm was then simulated by varying the weights of the different parameters , , , , µ1, µ2 and µ3.

Figure 1. Model Social Network

4.3 Results and Comparisons
Some illustrative results of the proposed ranking algorithm for different weights are mentioned along with a comparison of search results from other popular social network sites and recent research work in this area in Table 3. The small simulation network which was used for computing the results of the proposed algorithm exists on Facebook and Google+, in reality, for one of the authors. It was observed that the results obtained by issuing the same search on these networks can be replicated using the proposed algorithm by varying the various parameters, i.e., , , , , µ1, µ2 and µ3. Table 3 is illustrative of this observation, as results at serial numbers 2, 5 and 6 give the same as those for Facebook and the result at serial number 1 is same as that of Google +. It is, therefore, critical to note that there may not be a unique set of parameters for ranking the search results in a particular fashion and the same search result ranking can be obtained by tuning the various parameters according to the requirements of the social network in which the algorithm is used. Though, this algorithm is not aimed at conjecturing the logic that might have been used to obtain search results by a social network website, it can, however, be used to obtain search results that concur with the results of the website in question to a certain extent. The algorithm discussed in [2], based on trust and popularity was also implemented on the small simulation network. The result obtained was similar to that for Google+. Certain assumptions

were made while implementing the algorithm in [2]. While calculating the contribution of trust in the rank, trust values for two adjacent nodes was taken as 0.5. For the calculation of the contribution of popularity in the rank, the damping factor d was taken to be 0.8 and the initial values of popularity were taken to be 0. The final values of popularity for each node with respect to the various keywords in the profile were obtained by running the circular algorithm until the values converged to a precision of 10%.

4.4 Scalability
The proposed algorithm was tested for scalability by varying the number of potential search results and measuring the time required for calculation of association function for the same. Table 4 shows selected results of the simulations. The time for execution with different number of potential search results was found to increase almost linearly. Figure 2 shows the plot of execution time versus the number of potential search results. The important insight from this analysis is that calculation of the association function for as many as 1 million records is 3188 ms, which means that if we assume each user to have 1000 contacts in his/her network we can calculate the association of a user with other users up to two hops away in about 3 seconds. This can have important implications for the usefulness of the association function. Some likely usages to which the association function can be applied are discussed in a section below.

Table 2.Interaction between Nodes Comment User Volume Mariaa Mariab Mariac 3 9 Most Recent 05/08/2012 24/09/2012 Volume 1 10 Most Recent 05/08/2012 18/07/2012 Volume 12 11 Most Recent 18/09/2012 02/10/2012 Share Like

Table 3. Selected Observations from Simulation Proposed Algorithm S.No 1. 2. 3. 4. 5. 6. 7. 8. 9.  0.5 0.5 0 0 0.5 0  0.5 0.5 0 0 1 1  0.3 0.3 0 0 0 0  0.2 0.2 0 0 0 0 Facebook Google + Rank Algorithm Based on Trust and Popularity [2] µ1 0.34 0.5 0.5 0 0.5 0.5 µ2 0.33 0 0.5 1 0 0 µ3 0.33 0.5 0 0 0.5 0.5 Rank 1 Mariaa Mariac Mariaa Mariaa Mariac Mariac Mariac Mariaa Mariaa Rank results Rank 2 Mariac Mariaa Mariab Mariab Mariaa Mariaa Mariaa Mariac Mariac Rank 3 Mariab Mariab Mariac Mariac Mariab Mariab Mariab Mariab Mariab

Table 4. Selected Results of Simulation No. of Potential Search Results 100 500 1000 5000 10000 50000 100000 500000 1000000 Execution Time (ms) 0.586108 2.867683 6.002439 33.4847 61.12565 174.8376 323.0852 1584.719 3188.202 The results from the proposed algorithm show that depending on the values set for the different parameters namely, , , , , µ 1, µ2 and µ3, suitable results can be obtained. The nature of a particular social network will dictate what values must be set for each of these parameters. The advantages of the proposed algorithm include: a) Intuitiveness: The algorithm uses intuitive concepts like proximity, similarity and interaction to rank search results such that more relevant results may be ranked higher than less relevant ones. Adaptability: The algorithm is not confined to providing suitable search ranks for any particular kind of network and can be adapted for use in any kind of social network by defining the various parameters according to the nature of the network Flexibility: The algorithm can use the search results supplied by any search algorithm. Once the association values of the elements in the search list are obtained, subsequently, any algorithm can be used to sort the elements to obtain the ranked list. Therefore, the algorithm provides flexibility of implementation as it can be easily plugged between the search and sort algorithms in a social network. 4. Weighted association, A, is the weighted sum of proximity, similarity and interaction such that 3. Interaction is defined as the weighted sum of the weighted sum of recency and frequency of a particular type of interaction. It is assumed that the window size, w, will always be a non-zero number. Therefore,

b)

c) Figure 2. Execution Time vs. Potential Search Results

4.5 Observations and Discussion
The nature of the rank function defined by the proposed algorithm leads to some important observations: 1. Proximity by definition is the inversely related to the distance, d, between two nodes, where the distance means the number of hops to reach the destination node from the source node. Therefore, 2. Similarity is the ratio of the cardinality of the set of common interests between the searching user and a user in the search result and the cardinality of the set of the union of interests of the searching user and all the users in the search result. It is assumed that the profile of a user would contain at least one interest and a finite number of interests. Therefore,

5. APPLICATIONS OF THE PROPOSED WORK
Association, defined as a composition of proximity, similarity and interaction in this work, provides a means of quantifying the strength of linkage between two persons. This concept is not new, but, what is novel is how this can be used. Though this work is restricted to its use for search result ranking on social networks, it can be used in lots of other areas in the online space like marketing and advertising.

5.1 Use in Social Networks for Suggesting Friends
It is a common practice to suggest connections to a user on social network websites. The utility of this functionality in a social network is derived from its ability to suggest useful connections which might exist in a user's extended network and may have similar interests as those of the user. The association function can

be used effectively in this regard and can identify potential connections for a user. Depending on the kind of social network, weights can be allotted to the factors of proximity and similarity and the corresponding association function values can be computed. Thereafter, the connections with association function values exceeding a pre-defined threshold can be suggested to a user. In this application, the factor of interaction is not considered as new connections are being suggested to a user with whom s/he does not have any previous communication history.

result, an adaptive algorithm has been proposed which uses intuitive concepts like proximity, similarity and interaction to rank search results according to their relevance in a particular social network setting. The proposed method has ample scope of improvement and future work. The ranking function may consider more aspects apart from those considered here. Considering the huge data volume associated with search through social networks, the proposed algorithm may have to be tuned appropriately for time-efficiency.

5.2 Use in Targeted Advertising and Marketing
Online advertisers can use this framework to identify prospects and then pursue them. An advertiser would be interested in finding people who might have similar affinities as their existing customers. The first step, for a given customer base would be to identify other people in the online social circles of these customers who have similar interests as the customers themselves. This can be done by calculating association values using the factor of similarity alone. Once prospects with high degree of similarity in interests as the existing customers have been found, the next step would be to identify people who may be easily influenced by the existing customers. For this, association values using both the factors of proximity and interaction can be computed. A prospect is likely to convert into a customer if s/he knows that the people who s/he is close to or often interacts with also use the product or service in question. The factors of proximity and interaction quantify this behavior of prospects and, therefore, association values on the basis of these two factors would serve to identify potential customers from the pool of prospects. In this way, targeted marketing and advertising can be carried out and the positive effects of word-of-mouth can made use of. The association function can also be put to other innovative uses like identification of potential employees by job portals, recommendation for downloads of movies/songs, suggestions for online games, etc.

7. REFERENCES
[1] Huang, Chuan, Yinzi Chen, Wendong Wang, Yidong Cui, Hao Wang, and Nan Du. A novel social search model based on trust and popularity." In 3rd IEEE International Conference on Broadband Network and Multimedia Technology (IC-BNMT), (2010), 1030-1034. [2] Jiang, Zongli, and Jingsheng Li. A tag feedback based sorting algorithm for social search. In International Conference on Systems and Informatics (ICSAI), (2012), 1482-1485. [3] Lee, Khuan Yew, and Jer Lang Hong. ELITE--A novel ranking algorithm for social networking sites. In 9th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), (2012), 1009-1013. [4] Lee, Wookey, JJ-H. Lee, JJ-S. Song, and CS-H. Eom. Maximum reliable tree for social network search. In IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing (DASC), (2011),1243-1249. [5] Park, GunWoo, SooJin Lee, and SangHoon Lee. To Enhance Web Search Based on Topic Sensitive_Social Relationship Ranking Algorithm in Social Networks. In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT'09, (2009), 3, 445-448. [6] Sharma Divya, Dasgupta Parthasarathi, and Saha Debashis. A Ranking Algorithm for Online Social Network Search. In Working Paper Series of Indian Institute of Management Calcutta. (2013), WPS No. 724. [7] Vieira, Monique V., Bruno M. Fonseca, Rodrigo Damazio, Paulo B. Golgher, Davi de Castro Reis, and Berthier RibeiroNeto. Efficient search ranking in social networks. In Proceedings of the sixteenth ACM Conference on information and knowledge management, (2007), 563-572.

6. CONCLUSION
Online social networks have become an essential part of the lives of internet users. The importance of these networks will only grow over time and they are likely to become more and more complex and specialized as they evolve. The work presented in this paper tries to provide a generic solution for ranking of search results over social networks. Considering the large volume of searches being performed on social networks, the trivial function of providing relevant search results has become a differentiator for different social networks. This work takes into account the fact that all social networks are not similar and, hence, the same search result algorithm is not likely to be useful for all of them. As a

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/280313140

Anewcrosscontaminationawarerouting methodwithintelligentpathexplorationin DigitalMicrofluidicBiochips
ConferencePaper·March2013
CITATIONS READS

0
5authors,including: PampaHowladar IndianInstituteofEngineeringScienceandTec...
5PUBLICATIONS0CITATIONS
SEEPROFILE

41

PranabRoy IndianInstituteofEngineeringScienceandTec...
46PUBLICATIONS71CITATIONS
SEEPROFILE

HafizurRahaman IndianInstituteofEngineeringScienceandTec...
339PUBLICATIONS871CITATIONS
SEEPROFILE

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

PerformanceOptimizationofDielectricallyModulatedBiosensorsViewproject

ResearchScholarViewproject

AllcontentfollowingthispagewasuploadedbyPampaHowladaron24July2015.

Theuserhasrequestedenhancementofthedownloadedfile.

$QHZFURVVFRQWDPLQDWLRQDZDUHURXWLQJPHWKRGZLWK LQWHOOLJHQWSDWKH[SORUDWLRQLQ'LJLWDO0LFURIOXLGLF%LRFKLSV
6FKRRORI9/6,7HFKQRORJ\(PDLO±SDUWKD#LLPFDODFLQ %HQJDO(QJLQHHULQJDQG6FLHQFH8QLYHUVLW\6KLESXU,1',$   (PDLO±URQPDULQH#\DKRRFRLQSKRZODGDU#JPDLOFRP WKBUXS#\DKRRFRLQUDKDPDQBK#\DKRRFRLQ  $EVWUDFW² 'LJLWDO PLFURIOXLGLF V\VWHPV LQ UHFHQW \HDUV KDYH EHHQ ELRPHGLFDO DSSOLFDWLRQV VXFK DV SRO\PHU FKDLQ UHDFWLRQ SFU >@ GHYHORSHG DV DQ DOWHUQDWLYH SODWIRUP IRU H[HFXWLRQ RI PXOWLSOH HQ]\PH DVVD\V >@ SURWHRPLFV >@ GQD K\EULGL]DWLRQ DQG VRIW FRQYHQWLRQDO ODERUDWRU\ PHWKRGV VLPXOWDQHRXVO\ RQ D VLQJOH SODQDU SULQWLQJ >@ *HQHUDOO\ WKH PHFKDQLVP RI HOHFWUR ZHWWLQJ ' DUUD\ RI HOHFWURGHV WDUJHWHG IRU ELRFKHPLFDO DQDO\VLV DQG SKHQRPHQRQ LV EDVHGRQ FRQWURO RI VXUIDFH WHQVLRQRI D OLTXLGVROLG ELRPHGLFDO DSSOLFDWLRQV 'XH WR LWV GLVFUHWH QDWXUH GURSOHWV FDQ EH LQWHUIDFH E\ DSSO\LQJ HOHFWULFDO SRWHQWLDO DW WKH LQWHUIDFH $W WKH QDQROLWUH VFDOH IRUFHV RI LQWHUIDFLDO WHQVLRQ EHWZHHQ WKUHH SKDVHV PDQLSXODWHG WKURXJK PXOWLSOH UHFRQILJXUDEOH SDWKV GHULYHG E\ GRPLQDWH D GURSOHW¶V K\GURG\QDPLF EHKDYLRU 7KHVH LQFOXGH D IRUFH SUHSURJUDPPHG HOHFWURGH DFWXDWLRQ VHTXHQFHV WKURXJK WKLV SODQDU RQ WKH LQWHUIDFH EHWZHHQ WKH GURSOHW DQG WKH DPELHQW IOXLG DQG D DUUD\ NQRZQ DV GLJLWDO PLFURIOXLGLF ELRFKLS V\VWHP &URVV IRUFH DFWLQJ RQ WKH WULSKDVH FRQWDFW OLQH ZKHUH WKH GURSOHW WKH FRQWDPLQDWLRQ EHWZHHQ KHWHURJHQHRXV VDPSOHV WXUQV RXW WR EH D DPELHQWIOXLGDQGWKHVROLGPHHW(OHFWURFDSLOODULW\WKDWSURYLGHVWKH PDMRULVVXHFRQFHUQHGZLWKWUDQVSRUWDWLRQRIGURSOHWVDQGFRUUHFWQHVV GLVWULEXWLRQ RI IUHH FKDUJHV RQ WKH LQWHUIDFHV EHWZHHQ GLIIHUHQW RI WKH GHWHFWLRQ UHVXOWV IRU WKH ELRDVVD\ SURWRFROV ±ZKLFK LV KLJKO\ SKDVHV PRGXODWHV LQWHUIDFLDO WHQVLRQ IRUFHV WR PDQLSXODWH GURSOHWV VLJQLILFDQW IRU FOLQLFDO GLDJQRVWLFV DQG WR[LFLW\ PRQLWRULQJ >@ 7KHUHE\ ZKHQHYHU D GURSOHW LV NHSW LQ FRQWDFW ZLWK D VROLG DSSOLFDWLRQV,QWKLVSDSHUZHKDYHSURSRVHGDQLQWHOOLJHQWURXWHSDWK HOHFWURGH D ZHWWLQJ IRUFH FDQ DULVH XSRQ DSSOLFDWLRQ RI DQ HOHFWULF H[SORUDWLRQ WHFKQLTXH WKDW DWWHPSWV SDUWLDOO\ RU FRPSOHWHO\ WR DYRLG WKH QXPEHU RI FURVV FRQWDPLQDWLRQ GHSHQGLQJ RQ WKH IOXLGLF ILHOG WKURXJK WKHVH HOHFWURGHV 7KLV ZHWWLQJ IRUFH DFWLQJ RQ WKH WULSKDVH FRQWDFW OLQH FDQ EH DOWHUHG E\ DSSO\LQJ YDULHG HOHFWULFDO FRQVWUDLQWV HPSOR\HG GXULQJ URXWLQJ 7KH SDWK  LV IXUWKHU UHILQHG SRWHQWLDO WR WKH GURSOHW WKURXJK WKH HOHFWURGHV >@ +HQFH E\ XVLQJLQWHOOLJHQWGHWRXUE\LGHQWLI\LQJ]RQHVRIIULFWLRQEHWZHHQWZR DSSOLFDWLRQ RI D SURJUDPPHG VHTXHQFH RI HOHFWULF SRWHQWLDO DW DGMDFHQWURXWHSDWKVWKDWRSWLPL]HVWKHRYHUDOOURXWHWLPHE\UHGXFLQJ FRQVHFXWLYH HOHFWURGHV D VWUHQJWK GLVSDULW\ RI WKLV ZHWWLQJ IRUFH LV WKH RYHUDOO WLPH IRU VWDOOLQJ ZKLOH URXWLQJ ±DV ZHOO DV IXUWKHU FUHDWHGDQGWKLVUHVXOWVLQ PRYHPHQWRIGURSOHWVIURPRQHHOHFWURGH RSWLPL]DWLRQRIUHVRXUFHVWREHXWLOL]HG7KHVLPXODWLRQLVFDUULHGRXW FRQWDFWWRDQDGMDFHQWRQH RQ WHVW EHQFKHV RI EHQFKPDUN VXLWH , DQG EHQFKPDUN VXLWH ,,, 7KH 7KLV PDQLSXODWLRQ RI GLVFUHWH GURSOHWV DOORZV WR FODVVLI\   WKH UHVXOWV VKRZ LPSURYHPHQW LQ RYHUDOO DV ZHOO DV DYHUDJH URXWH WLPH PLFURIOXLGLFRSHUDWLRQVLQWRDVHWRIEDVLFRSHUDWLRQVQDPHO\PL[LQJ DQGPDMRUUHGXFWLRQLQWKHQXPEHURIFURVVRYHUV PHUJLQJ VSOLWWLQJ VWRUDJH DQG WUDQVSRUWDWLRQ +HQFH WKH XVH RI XQLW .H\ZRUGV GLJLWDO PLFURIOXLGLFV &URVV FRQWDPLQDWLRQ URXWLQJ YROXPHV RI GURSOHWV FDQ EH DFFRPSOLVKHG LQ D ' SODQDU DUUD\ RI GHWRXUDOJRULWKPUHVRXUFHXWLOL]DWLRQ HOHFWURGHVFRQILJXUHGIRUDQ(OHFWURZHWWLQJRQGLHOHFWULFV\VWHP7KH ,,1752'8&7,21 PLFURIOXLGLFDUUD\FRQWDLQVDVHWRIEDVLFFHOOVWKDWLVPDGHXSRIWZR 7KH PLQLDWXUL]DWLRQ LQWHJUDWLRQ DQG SDUDOOHOL]DWLRQ RI FRPPRQ SDUDOOHOJODVVSODWHV)LJXUH7KHERWWRPSODWHFRQWDLQVDSDWWHUQHG ODERUDWRU\ SURFHVVHV LQ ODERQDFKLS V\VWHPV KDYH SRWHQWLDOO\ DUUD\ RI LQGLYLGXDOO\ FRQWUROODEOH HOHFWURGHV DQG WKH WRS SODWH LV WUDQVIRUPHG FRQYHQWLRQDO ODERUDWRU\ SURFHGXUHV XVXDOO\ FRDWHG ZLWK D FRQWLQXRXV JURXQG HOHFWURGH 7KH ILOOHU PHGLXP VXFK FXPEHUVRPHDQGH[SHQVLYHLQWRORZFRVWDQGV\QWKHVL]DEOHVHULHVRI DVVLOLFRQHRLODORQJZLWKWKHVDPSOHGURSOHWLVVDQGZLFKHGEHWZHHQ RSHUDWLRQVWKDWLQYROYHVPXOWLSOHFKHPLFDOPDQLSXODWLRQVRQDVLQJOH WKH WZR SODWHV %\ DVVLJQLQJ WLPHYDU\LQJ YROWDJH YDOXHV WR WXUQ PRQROLWKLF SODWIRUP 7KH PDMRU DGYDQWDJH RI VXFK GHYLFHV OLHV LQ RQRII WKH HOHFWURGHV RQ WKH GLJLWDO PLFURIOXLGLF ELRFKLS WKH KLJKHU VHQVLWLYLW\ EHWWHU DFFXUDF\ ORZHU FRVW DV ZHOO DV SRUWDELOLW\ LQWHUIDFLDO WHQVLRQ RI WKH GURSOHWV DUH PRGXODWHG UHVXOWLQJ LQ WKHLU DQG KLJKHU OHYHOV RI V\VWHP LQWHJUDWLRQ 7KH PLQLDWXUL]DWLRQ WUDQVSRUWDWLRQ DURXQG WKH HQWLUH ' DUUD\ DQG H[HFXWLRQ RI LQFRUSRUDWHG LQ WKHVH GHYLFHV OHDGV WR VKRUWHU GHWHFWLRQ WLPHV KLJK IXQGDPHQWDO PLFURIOXLGLF RSHUDWLRQV IRU GLIIHUHQW ELRDVVD\V 7KH WKURXJKSXWDQGUHGXFWLRQRQHQHUJ\DVZHOODVUHDJHQWVFRQVXPSWLRQ RSHUDWLRQV SHUIRUPHG E\ DFWXDWLQJ FRQWURO YROWDJHV WKURXJK WKH LQELRORJLFDOH[SHULPHQWV HOHFWURGHVDUHDOVRFDOOHG UHFRQILJXUDEOH RSHUDWLRQVEHFDXVHRIWKHLU $ SRSXODU FODVV RI FRPPHUFLDOO\ DYDLODEOH ELRFKLSV DUH EDVHG RQ IOH[LELOLW\ LQ ORFDWLRQ DQG LQ H[HFXWLRQ WLPH 6XFK UHFRQILJXUDEOH FRQWLQXRXVIOXLGIORZEHLQJFRQWUROOHGE\PLFURSXPSVPLFURYDOYHV RSHUDWLRQFDQEHFDUULHGRXWDWDQ\SUHVFKHGXOHGSODFHZLWKLQWKH' HOHFWURNLQHWLFVRUHOHFWURRVPRVLV,QODWHVDQDOWHUQDWLYHFODVVRI SODQH+HQFHWKH'0)6RIIHUVDSODWIRUPWKDWSURYLGHVWKHDGYDQWDJH ODERQFKLSV\VWHPZKLFKLVFDSDEOHRIPDQLSXODWLQJGLVFUHWHGURSOHWV RI G\QDPLF UHFRQILJXUDELOLW\ DQG VRIWZDUH EDVHG FRQWURO IRU LQ YROXPHV RI QDQROLWUHV KDV EHHQ HPHUJHG 7KHVH QHZ FODVV RI PXOWLIXQFWLRQDOELRFKLSV GHYLFHVWHUPHGDV'LJLWDOPLFURIOXLGLFELRFKLS'0)%SHUIRUPVWKH 7ZR PDMRU UHVRXUFH FRPSRQHQWV LQYROYHG LQ SHUIRUPLQJ SURFHVVRIGURSOHWV PDQLSXODWLRQWKURXJKGLIIHUHQW PHFKDQLVPV YL] IXQGDPHQWDORSHUDWLRQVLQD'0)%DUHWKH0L[HUVDQG6WRUDJHXQLWV HOHFWURZHWWLQJ>@GLHOHFWURSKRUHVLV>@WKHUPRFDSLOODU\WUDQVSRUW FRQVLVWLQJ RI HOHFWURGHV 0L[HUV DUH XVHG WR SHUIRUP PL[LQJ DQG >@ DQG VXUIDFH DFRXVWLF ZDYH WUDQVSRUW >@ ,Q WKH GLJLWDO VSOLWWLQJ RSHUDWLRQV ZKHUHDV VWRUDJH XQLWV DUH XVHG IRU VWRUDJH RI PLFURIOXLGLFDUFKLWHFWXUHWKHEDVLFOLTXLGXQLWYROXPHLVIL[HGE\WKH GURSOHWV JHQHUDWHG IRU VXEVHTXHQW PL[LQJV 7UDQVSRUWDWLRQ SDWKV DUH JHRPHWU\RIWKHV\VWHPIOXLGTXDQWL]DWLRQZKHUHDVYROXPHWULFIORZ XVHGWRPRYHGURSOHWVDPRQJGLIIHUHQWFRPSRQHQWVHJPL[HUVDQG UDWH LV GHWHUPLQHG E\ WKH UDWH DQG WKH QXPEHU RI GURSOHW WUDQVSRUW VWRUDJHXQLWVZLWKLQWKH'DUUD\,QDGGLWLRQWKHDUUD\PD\FRQWDLQ &RPSDUHGWRFRQWLQXRXVIORZEDVHGWHFKQLTXHVGLJLWDOPLFURIOXLGLFV FHOOV WKDW FDQ SHUIRUP VSHFLDOL]HG RSHUDWLRQV VXFK DV KHDWLQJ RU RIIHUV WKH DGYDQWDJH RI LQGLYLGXDO VDPSOH DGGUHVVLQJ UHDJHQW RSWLFDO VHQVLQJ+HQFH WUDQVSRUWDWLRQ RI GURSOHWV LQ '0)%V ILJXUHV LVRODWLRQ DQG FRPSDWLELOLW\ ZLWK DUUD\EDVHG WHFKQLTXHV XVHG IRU RXW WR EH D PDMRU VWHS UHJDUGLQJ WKH RYHUDOO RSHUDWLRQ DQG UHVXOWV ELRFKHPLFDODQGELRPHGLFDODSSOLFDWLRQ>@ REWDLQHG LQ GLJLWDO PLFURIOXLGLF V\VWHPV 7KH G\QDPLF (OHFWURZHWWLQJRQGLHOHFWULFHZRGLVFXUUHQWO\XVHGDVRQHRIWKH UHFRQILJXUDELOLW\ LQKHUHQW LQ '0)%V DOORZV IRU VKDULQJ RI FHOOV E\ EHVW DFWXDWLRQ PHWKRGV IRU GURSOHW PDQLSXODWLRQ LQ GLJLWDO PXOWLSOH GURSOHWV URXWHV LQ D WLPH PXOWLSOH[HG PDQQHU ,I WKH PLFURIOXLGLF V\VWHPV >@ LW KDV HPHUJHG DV DQ XVHIXO WRRO IRU PLFURIOXLGLFV\VWHPLVGHVLJQHGIRUH[HFXWLRQRIPXOWLSOHELRDVVD\

3UDQDE5R\3DPSD+RZODGDU5XSDP%KDWWDFKDUMHH3DUWKDVDUDWKL'DVJXSWD +DIL]XU5DKDPDQ,QGLDQ,QVWLWXWHRIPDQDJHPHQW&DOFXWWD,1',$

978-1-4673-6040-1/13/$31.00 c 2013 IEEE

50

 )LJXUH±6FKHPDWLFGLDJUDPIRUGURSOHWPRYHPHQWLQD'0)% SURWRFROV LQYROYLQJ KHWHURJHQHRXV VDPSOHV ± WKLV VKDULQJ SKHQRPHQRQ PD\ UHVXOW LQ D PDMRU SUREOHP WHUPHG DV FURVV FRQWDPLQDWLRQ 6XFK FRQWDPLQDWLRQV FDXVHG E\ EHDG UHWHQWLRQ DQG OLTXLG UHVLGXH EHWZHHQ VXFFHVVLYH GURSOHW URXWHV RI GLIIHUHQW GURSOHW VDPSOHV PD\ UHVXOW LQ LQHYLWDEOH HUURQHRXV UHDFWLRQ WKDW SURGXFHV LQFRUUHFW GHWHFWLRQ RXWFRPHV ,Q DGGLWLRQ H[FHVVLYH DQG SURORQJHG FRQWDPLQDWLRQ PD\ UHVXOW LQ EUHDNGRZQ RI WKH HOHFWURGHV DV ZHOO DV HOHFWURGHVKRUWSUREOHPV7KLVILQDOO\JHQHUDWHVSK\VLFDOGHIHFWVDQG SURGXFHLQFRUUHFWEHKDYLRXUVLQWKHHOHFWULFDOGRPDLQ>@ 7KHUHE\ LW EHFRPHV QHFHVVDU\ WR GHVLJQ DOJRULWKPV WKDW SODQ WKH GURSOHWVWREHURXWHGLQGLVMRLQWPDQQHU:LWKIXUWKHULQFUHDVHLQWKH OHYHOVRILQWHJUDWLRQDVZHOODVGHJUHHRIVFDOLQJ±ILQGLQJRIGLVMRLQW URXWH ZLWKLQ D SUHGHWHUPLQHG OD\RXW EHFRPHV LQFUHDVLQJO\ GLIILFXOW 7KLV LQ WXUQ UHGXFHV WKH DYDLODELOLW\ RI WKH QXPEHU RI VSDUH FHOOV ± WKDW FDQ EH XVHG WR UHSODFH WKH GHIHFWLYH SULPDU\ FHOOV ZKLOH UHURXWLQJWRDYRLGIDXOW\FHOOVKHQFHFRPSURPLVLQJZLWKWKHGHIHFW WROHUDQFHIRUWKHJLYHQELRDVVD\ ,QWKLVSDSHUZHKDYHSURSRVHGDFURVVFRQWDPLQDWLRQDZDUHURXWLQJ PHWKRG WKDW LQYROYHV LQWHOOLJHQW H[SORUDWLRQ EHWZHHQ VXLWDEOH URXWH SDWKIRUHDFKVDPSOHGURSOHWV±WKDWQRWRQO\PLQLPL]HVRUHOLPLQDWHV WKHQXPEHURIFURVVFRQWDPLQDWLRQVEXWDOVRZLWKIXUWKHUUHILQHPHQW LQWHUPVRIGHWRXURURSWLPL]HGDOWHUDWLRQWKHRYHUDOOURXWHWLPHDV ZHOODVFHOOXWLOL]DWLRQFDQEHRSWLPL]HG7KHRUJDQL]DWLRQRIWKHUHVW RI WKH SDSHU LV DV IROORZV 6HFWLRQ ,, GLVFXVVHV WKH UHODWHG ZRUNV UHJDUGLQJGURSOHWURXWLQJDQGFURVVFRQWDPLQDWLRQLVVXHV6HFWLRQ,,, IRUPXODWHVWKHGURSOHWURXWLQJSUREOHPVDQGWKHFRQVWUDLQWVLQYROYLQJ FURVV FRQWDPLQDWLRQ 6HFWLRQ ,9 VWDWHV WKH PRWLYDWLRQ DQG WKH FRQWULEXWLRQ RI RXU ZRUN 6HFWLRQ 9 HODERUDWHV WKH GHVLJQHG DOJRULWKP ZLWK DQ LOOXVWUDWLYH H[DPSOH VWDWHG LQ 6HFWLRQ 9, ,Q 6HFWLRQ9,,WKHH[SHULPHQWDOVLPXODWLRQUHVXOWVDUHGLVSOD\HGWRJHWKHU ZLWK WKH FRUUHVSRQGLQJ FRPSDULVRQV ZLWK UHVXOWV REWDLQHG IURP FRQWHPSRUDU\ PHWKRGV )LQDOO\ 6HFWLRQ 9,,, VWDWHV WKH FRQFOXGLQJ UHPDUNVDQGWKHIXWXUHVFRSHVIRUIXUWKHUHQKDQFHPHQW

7KH LVVXH RI FURVV FRQWDPLQDWLRQ ZDV LQLWLDOO\ DGGUHVVHG LQ >@DQG >@,Q>@ZDVKGURSOHWVDUHLQWURGXFHG±WRRYHUFRPHWKHSUREOHP RI FRQWDPLQDWLRQ DW DQ\ SDUWLFXODU VLWH DQG VSHFLILF VWUDWHJLHV DUH GHVLJQHG EDVHOLQH DQG EDVHOLQH  WR V\QFKURQL]HWKH ZDVK GURSOHW RSHUDWLRQZLWKIXQFWLRQDOGURSOHWURXWHRSHUDWLRQV>@DWWHPSWHGWR KDQGOHWKHLVVXHRIFURVVFRQWDPLQDWLRQE\ILUVWXVLQJNVKRUWHVWSDWK URXWLQJ WHFKQLTXHV WR PLQLPL]H  WKH URXWLQJ FRPSOH[LWLHV DV ZHOO DV FHOO XWLOL]DWLRQ DQG WKHUHE\ UHGXFLQJ WKH QXPEHU RI FRQWDPLQDWLRQ VSRWV LQ WKH SURFHVV 7KHQ WR WDNH DGYDQWDJH RI PXOWLSOH ZDVK GURSOHWV D PLQLPXP FRVW FLUFXODWLRQ DOJRULWKP 0&& IRU RSWLPDO ZDVKGURSOHWURXWLQJWRVLPXOWDQHRXVO\PLQLPL]HWKHQXPEHURIXVHG FHOOVDQGWKHFOHDQLQJWLPHKDVEHHQDGRSWHG0RUHRYHUDORRNDKHDG SUHGLFWLRQWHFKQLTXHKDVEHHQXVHGWRGHWHUPLQHWKHFRQWDPLQDWLRQV EHWZHHQ VXFFHVVLYH VXESUREOHPV ,W KDV EHHQ IRXQG WKDW UHVXOWV REWDLQHGLQ>@KDVEHHQYDVWO\LPSURYHGDV FRPSDUHGZLWK>@DQG >@ 7R PDNH WKH ELRFKLS IHDVLEOH IRU SUDFWLFDO DSSOLFDWLRQV SLQ FRXQW UHGXFWLRQ SRVHV DQRWKHU PDMRU GHVLJQ SUREOHP ± ZKLFK KDV EHHQ DGGUHVVHG ZLWK D SLQ FRXQW DZDUH VWUDWHJ\ XVLQJ D WZR VWDJHG ,/3 EDVHG PHWKRG LQWHJUDWLQJ WKH FRQWDPLQDWLRQ LVVXHV LQ >@ ,Q >@ D PHWKRG KDV EHHQ SURSRVHG WKDW XWLOL]HG D JUDSK EDVHG KHXULVWLFV WR VHOHFW WKH EHVW VXLWDEOH SDWK IRU URXWLQJ XVLQJ PXOWLSOH DOWHUQDWLYHURXWHVWRDYRLGFROOLVLRQRUYLRODWLRQRIIOXLGLFFRQVWUDLQWV +RZHYHU WKH DOJRULWKP ZDV IRFXVHG RQO\ RQ FROOLVLRQV WR PLQLPL]H VWDOOLQJUHVXOWLQJLQVXEVWDQWLDOLPSURYHPHQWLQWKHURXWHWLPH,Q>@ WKHFURVVFRQWDPLQDWLRQLVVXHVZHUHQRWDGGUHVVHG±WKLVUHQGHUVWKH PHWKRGWREHVXLWDEOHRQO\IRUKRPRJHQHRXVVDPSOHDSSOLFDWLRQV ,Q DOO RI WKH DERYH PHWKRGV GLIIHUHQW URXWLQJ VWUDWHJLHV DUH SUHVFULEHG IRU RSWLPL]HG FRQWDPLQDWLRQ +HUH ZH XVHG D PHWKRG WR LQWURGXFH LQWHQWLRQDO UHGXQGDQF\ IRU HVWLPDWLQJ DW OHDVW RQH DOWHUQDWLYH IRU HDFK SDWK ZLWK PLQLPXP GHYLDWLRQ IRU HDFK VDPSOH GXULQJ YLUWXDO URXWH HVWLPDWLRQ SKDVH :H WKHUHE\ DWWHPSWHG DQ LQWHOOLJHQW H[SORUDWLRQ VWUDWHJ\ WR VHOHFW LWHUDWLYHO\ PRVW VXLWDEOH PLQLPXP GHYLDWLRQ SDWKV DPRQJ DYDLODEOH DOWHUQDWLYHV WR RSWLPL]H FURVV FRQWDPLQDWLRQ WR D PLQLPXP RU ]HUR $Q HIILFLHQW WUDGH RII VWUDWHJ\ KDV EHHQ DGRSWHG WR DYRLG RU PLQLPL]H VWDOOLQJ XVLQJ DOWHUQDWLYH GHYLDWLRQ RU GHWRXU WR PLQLPL]H WKH ODWHVW DV ZHOO DV DYHUDJHDUULYDOWLPH7HVWEHQFKHVIURPEHQFKPDUNVXLWH,DQG,,,KDV EHHQ XVHG IRU LPSOHPHQWDWLRQ RI WKH SURSRVHG VWUDWHJ\ DQG WKH VLPXODWLRQUHVXOWVDUHGLVSOD\HGLQVHFWLRQ9,,

,,,'523/(75287,1*$1'&5266 &217$0,1$7,21,1'0)%
7KH REMHFWLYH RI GURSOHW URXWLQJ LV WR WUDQVPLW DOO WKH GURSOHWV IURP WKHLU UHVSHFWLYH VRXUFHV WR WDUJHWV ZLWKLQ D ' JULG DUUD\ ZKLOH IXOILOOLQJ DOO WKH FRQVWUDLQWV LPSRVHG IRU WKH WUDQVSRUWDWLRQ $Q HIILFLHQW URXWLQJ VFKHGXOH YLUWXDO URXWH LV RIWHQ UHTXLUHG WR EH GHYHORSHG WRSURYLGH DQ RSWLPDO URXWLQJ LQ WHUPV RI REMHFWLYHVVXFK DV ODWHVW DUULYDO WLPH DQG RYHUDOO FHOO XWLOL]DWLRQ 'URSOHW URXWLQJ SUREOHP LQ '0)%V LV W\SLFDOO\ PRGHOHG LQ WHUPV RI D 'JULG )LJXUH  )RU HDFK GURSOHW WKHUH H[LVWV D VHW RI VRXUFH JULG ORFDWLRQV D VHW RI WDUJHW JULG ORFDWLRQV DQG RSWLRQDOO\ D VHW RI PL[HUV (DFK VRXUFHWDUJHW FRPELQDWLRQ LV GHILQHG DV D QHW $ SLQ QHW KDV D VLQJOH VRXUFH DQG VLQJOH WDUJHW $ FRPELQDWLRQ RI WZR 6RXUFHVRQH0L[HUDQGRQH7DUJHWIRUPVDSLQQHW ,Q FDVHV RI SLQ QHWV GURSOHW PHUJLQJ LV GHVWLQHG DW SUHGHILQHG ORFDWLRQVFDOOHGPL[HUV6HYHUDOPLFURIOXLGLFPRGXOHVPD\EHSODFHG DV SHU VFKHGXOHV IRU PL[LQJ VSOLWWLQJ VWRUDJH GLVSHQVLQJ DQG RWKHU RSHUDWLRQV ZLWKLQ WKH DUUD\ 7KHVH DUH FRQVLGHUHG DV WKH +DUG %ORFNDJHV ,Q RUGHU WR DYRLG FRQIOLFWV EHWZHHQ GURSOHW URXWHV DQG DVVD\ RSHUDWLRQV D VHJUHJDWLRQ UHJLRQ LV GHILQHG DURXQG WKH IXQFWLRQDOUHJLRQRIHDFKVXFKPLFURIOXLGLFPRGXOHV0RUHRYHUWKHUH DUH SRVVLELOLWLHV RI LQWHUVHFWLRQ RU RYHUODSSLQJ RI GURSOHW URXWHV GXULQJWKHLUFRQFXUUHQWURXWLQJLQWLPHPXOWLSOH[HGPDQQHU7RDYRLG VXFK XQGHVLUDEOH EHKDYLRUV IROORZLQJ IOXLGLF FRQVWUDLQWV DUH LQWURGXFHG /HW GL DW [WL \WL  DQG GM DW  [WM \WM GHQRWH WZR LQGHSHQGHQW GURSOHWV DW DQ\ JLYHQ WLPHVWDPS W 7KHQ WKH IROORZLQJ FRQVWUDLQWV GHILQHG DV )OXLGLF &RQVWUDLQWV DUH UHTXLUHG WR EH VDWLVILHG IRU DQ\ WLPHWZKLOHURXWLQJ>@ 6WDWLFFRQVWUDLQW_[WL±[WM_!RU_\WL±\WM_!

,,5(/$7(':25.6
7KH LVVXH RI VLPXOWDQHRXV GURSOHW URXWLQJ ZLWKLQ D '0)% LQYROYHV PDMRUREMHFWLYHVQDPHO\WKHRSWLPL]DWLRQRIWKHODWHVWDUULYDOWLPHRI DOOGURSOHWVIURPVRXUFHWRWDUJHWDVZHOODVWKHRYHUDOOWUDQVSRUWWLPH IRUDOOWKHGURSOHWVDQGRSWLPL]DWLRQRIWKHFHOOHOHFWURGHXWLOL]DWLRQ LQWKHSURFHVV'LIIHUHQWDOJRULWKPVKDYHEHHQSURSRVHGLQDQDWWHPSW WR UHVROYH WKH GURSOHW URXWLQJ LVVXHV ZLWK QHFHVVDU\ RSWLPL]DWLRQ ZKLOH PDLQWDLQLQJ WKH UHTXLVLWH FRQVWUDLQWV WR DFKLHYH WKH REMHFWLYHV VWDWHG DERYH $ GLUHFW DGGUHVVLQJ PHWKRG LV XVHG LQ >@ ZKHUH WKH GURSOHW URXWLQJ SUREOHP LV PDSSHG LQWR D JUDSK FOLTXH PRGHO $Q LQWHJHU OLQHDU SURJUDPPLQJ ,/3 IRUPXODWLRQ EDVHG RQ GLUHFW DGGUHVVLQJ PRGH LV UHSRUWHG LQ >@ 7KH PRYHPHQW RI GURSOHWV DW HDFKWLPHVWHSLVGHWHUPLQHGE\VROYLQJDQ,/3$µGURSOHWPRYHPHQW FRVW¶ LV XVHG DV D KHXULVWLF WR HYDOXDWH FRQJHVWLRQ ZKHQ VROYLQJ WKH ,/3 ,Q >@ G\QDPLF UHFRQILJXUDELOLW\ RI WKH PLFURIOXLGLF DUUD\ LV H[SORLWHG GXULQJ UXQWLPH $ KLJK SHUIRUPDQFH GURSOHW URXWLQJ DOJRULWKP XVLQJ JULGEDVHG UHSUHVHQWDWLRQ LV UHSRUWHG LQ >@ $ SDUWLWLRQEDVHGDOJRULWKPIRUSLQFRQVWUDLQWEDVHGGHVLJQLVSURSRVHG LQ >@ $QRWKHU IDVW URXWDELOLW\ DQG SHUIRUPDQFHGULYHQ GURSOHW URXWHUIRU'0)%VKDVEHHQSURSRVHGLQ>@$UHFHQWZRUNEDVHGRQ 6RXNXS¶V URXWLQJ DOJRULWKP >@ IRU FRQFXUUHQW SDWK DOORFDWLRQ WR PXOWLSOH GURSOHWV DSSHDUV LQ >@)LQDOO\ D SUHSODFHG PRGXOH EDVHG URXWLQJ PHWKRG LV SURSRVHG LQ >@ ZKLFK GLVSOD\V PXFK LPSURYHG UHVXOWVIRUFURVVUHIHUHQFLQJ%LRFKLSV

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

51

'\QDPLFFRQVWUDLQW_[WL±[WM_! RU_\WL±\WM_! RU_[WM±[WL_!RU_\WM±\WL_! 7KLVLPSOLHVWKDWIRUDQ\GURSOHWDWORFDWLRQ[\WKHORFDWLRQV [\[\[\[\[\[\[\ DQG [ \ DUH SURKLELWHG IRU DQ\ RWKHU GURSOHW WR HQWHU DW WLPHVWDPSV W DQG W LQ RUGHU WR PDLQWDLQ WKHVH IOXLGLF FRQVWUDLQWV +HQFH DOO WKH ORFDWLRQV DGMDFHQW WR [ \ DV VWDWHG DERYH IRUP D &ULWLFDO=RQH)LJXUHIRUDQ\GURSOHWDW [\ DWWLPHVWDPS W$ SUHGHWHUPLQHG WLPH OLPLW FDOOHG WKH 7LPLQJ &RQVWUDLQW GHILQHV WKH PD[LPXPDOORZHGWUDQVSRUWDWLRQWLPHIRUDJLYHQVHWRIGURSOHWV 'XULQJ WKLV WUDQVSRUWDWLRQ SURFHVV LQWHUVHFWLRQ EHWZHHQ PXOWLSOH URXWHV PD\ RFFXU ± FDXVLQJ D GURSOHW  DUULYLQJ DW WKH LQWHUVHFWLRQ ORFDWLRQDWDODWHUFORFNF\FOHFDQEHFRQWDPLQDWHGE\WKHUHVLGXHOHIW EHKLQG E\ DQRWKHU GURSOHW DOUHDG\ FURVVLQJ WKH VDLG ORFDWLRQ DW DQ HDUOLHU FORFN F\FOH 7KLV SKHQRPHQRQ LV WHUPHG DV FURVV FRQWDPLQDWLRQ DQG WKH ORFDWLRQV VKDUHG E\ PXOWLSOH GURSOHWV ZKLOH URXWLQJDUHGHILQHGDVFURVVFRQWDPLQDWLRQVLWHV&URVVFRQWDPLQDWLRQ LVXQGHVLUDEOHGXULQJGURSOHWURXWLQJVLQFHLWOHDGVWRHUURQHRXVDVVD\ RXWFRPHV ± DV ZHOO DV   SK\VLFDO GHIHFWV  IRU SURORQJHG RSHUDWLRQ )LJXUH  VKRZV WKH LQVWDQFH RI FURVV FRQWDPLQDWLRQ DW DQ\ JLYHQ WLPHVWDPSZLWKLQD'0)%

· )RUHDFKQHWLIVRXUFHDQGWDUJHWDUHQRWLQWKHVDPHURZRU FROXPQZHKDYHHVWLPDWHGWZRDOWHUQDWLYH0DQKDWWDQSDWKV ZLWK PLQLPXP GHYLDWLRQ LQ RUGHU WR LQWURGXFH LQWHQWLRQDO UHGXQGDQF\ LJQRULQJ DQ\ PRGXOHV FRPLQJ LQ WKH ZD\ RI WKHVHVURXWHVDVREVWDFOHV · 8VLQJDJUDSKEDVHGKHXULVWLFVZHLWHUDWLYHO\HOLPLQDWHGRQH DOWHUQDWHURXWHZKLOHVHOHFWLQJWKHPRVWVXLWDEOHRQHLQWHUPV RIFRQWDPLQDWLRQDYRLGDQFH · 1RZ ZH FRQVLGHU DOO WKRVH SDWKV WKDW FURVV WKURXJK WKH PRGXOHV DQG GHWHUPLQH WZR DOWHUQDWH SRVVLEOH GHWRXUV FRQVWLWXWLQJDJDLQWZRDOWHUQDWLYHV · $JDLQXVLQJDJUDSKEDVHGKHXULVWLFVZHDWWHPSWHGWRVHOHFW WKH VXLWDEOH GHYLDWLRQ WKDW DJDLQ PLQLPL]HV WKH DGGLWLRQDO FRQWDPLQDWLRQLIDQ\FDXVHGE\WKHVHGHYLDWLRQVDVZHOODV WKHQXPEHURIFHOOVFRYHUHG · )LQDOO\ ZH GHILQH IULFWLRQ ]RQHV IRU DOO WKRVH SDUDOOHO URXWH SDWKWKDWFRYHUVFHUWDLQQHLJKERXULQJFHOOV±WKHUHE\LQYLWLQJ SRVVLELOLW\RIVWDOOLQJ · 2QHVWLPDWLRQRIWKHWLPHVWDPSVDWWKHVHQHLJKERXULQJFHOOV IRU WKH FRUUHVSRQGLQJ URXWHV ± LI SRVVLEOH VWDOOLQJ LV SUHGLFWHGZHH[SORUHGIXUWKHUSRVVLEOHGHWRXUWKDWWUDGHVRII ZLWK SRVVLEOH VWDOOLQJ LQ WHUPV RI UHGXFWLRQ LQ RYHUDOO URXWH WLPHWKDWUHVXOWVLQHQKDQFHGURXWHSHUIRUPDQFH

97+(352326('0(7+2'
+HUHZHVWDUWZLWKDJLYHQOD\RXWFRPSULVHGRIDJLYHQQXPEHUQRI SLQ DQG SLQ QHWV7KH REMHFWLYH LV WR GHWHUPLQH Q QXPEHU RI VXLWDEOHURXWHSDWKVWKDWUHVXOWVLQ&QXPEHURIFRQWDPLQDWLRQVZKHUH PLQLPXPQXPEHURIFRQWDPLQDWLRQV &DORQJZLWKODWHVWDUULYDOWLPH 7EHLQJWKHPLQLPXPDVZHOODVWRWDOFHOOFRYHUHG(EHLQJRSWLPXP

$3URSRVHG$OJRULWKP
)LJXUHSLQDQGSLQQHWVLQD'*ULG$UUD\&ULWLFDO]RQHIRU 6LVWKH*UHHQVKDGHGUHJLRQ$KDUG%ORFNDJHLVDOVRLOOXVWUDWHG



 )LJXUH,QVWDQFHRIFRQWDPLQDWLRQZLWKLQD'0)%ZKLOHURXWLQJ

,9027,9$7,21$1'285&2175,%87,21
$V HYLGHQW IURP WKH HDUOLHU GLVFXVVLRQV FURVV FRQWDPLQDWLRQ DYRLGDQFH KDV WXUQHG RXW WR EH D PDMRU LVVXH ERWK LQ FRQWH[W RI UHOLDELOLW\RIUHVXOWVDVZHOODVRSHUDWLRQRIWKHGHYLFH)HZPHWKRGV KDV DOUHDG\ EHHQ SURSRVHG WR WDFNOH WKHVH LVVXHV DV PHQWLRQHG LQ VHFWLRQ ,, ,QLWLDOO\ WKH IRFXV ZDV DW WKH HVWLPDWHG URXWH WUDFNV ± E\ FRQVFLRXVO\DOORZLQJGHWRXURIGURSOHWVZKLOHURXWLQJWRDYRLGFURVV RYHU DQG WKHUHE\ UHGXFLQJ WKH QXPEHU RI FURVV FRQWDPLQDWLRQ ORFDWLRQV)RUSLQGURSOHWVGLIIHUHQWVRXUFHVRIWKHVDPHQHWPD\EH DOORZHGWRVKDUHWKHVDPHSDWKDVPL[LQJPD\QRWLQWHUIHUHZLWKWKH DFFXUDF\RIUHVXOWVLQVXFKFDVHV7KHVHW\SHVRIFURVVRYHURUVKDULQJ RI FHOOV DUH WHUPHG DV DOORZDEOH FRQWDPLQDWLRQ +HQFH LW KDV EHHQ IRXQG WKDW ZKLOH RSWLPL]HG URXWLQJ UHGXFHV WKH QXPEHU RI FRQWDPLQDWLRQ±DWWKHVDPHWLPHWKHWZRPDMRUREMHFWLYHVIRUURXWLQJ KDVEHHQFRPSURPLVHG±E\DOORZLQJDQLQFUHDVHLQODWHVWDUULYDOWLPH DV ZHOO DV FHOO XWLOL]DWLRQ LQ RUGHU WR DFFRPPRGDWH WKH LQWHQWLRQDO GHWRXU ,QWKLVZRUNZHDWWHPSWHGWRUHVROYHWKHVHLVVXHVVLPXOWDQHRXVO\WR RSWLPL]H WKH URXWH SHUIRUPDQFH LQ WHUPV RI RYHUDOO URXWH WLPH WRJHWKHU ZLWK UHGXFWLRQ LQ WKH QXPEHU RI FRQWDPLQDWLRQV 7KH VXPPDU\RIWKHZRUNLVGHSLFWHGEHORZ

,QSXW $0[0'JULGFRQWDLQLQJQQXPEHURIVDPSOHVFRPSULVLQJ RI[QXPEHURISLQDQG\QXPEHURISLQGURSOHWV 3URFHGXUH 6WHS±+HUHHDFKSLQQHWKDVEHHQGHFRPSRVHGLQWRWKUHHSRVVLEOH QHWV QDPHO\ IURP VRXUFH WR PL[HUVRXUFH WR PL[HU DQG PL[HU WR WDUJHW HDFK  SLQ QHW ZH DVVXPH RQH VLQJOH QHW LH IURP VRXUFH WR WDUJHW +HQFHIRUQQXPEHURIVDPSOHVSRVVLEOHQHWVN [\ 6WHS±RXWRIWKHVHNQXPEHURIQHWVOHWSQXPEHURIQHWVVKDUHWKH VDPHURZVRUFROXPQV±WKHUHE\KDYLQJRQO\RQHHVWLPDWHGURXWHSDWK )RU WKH NS QXPEHU RI QHWV ZH HVWLPDWH WZR DOWHUQDWH PDQKDWWDQ SDWKV LJQRULQJ DQ\ SDVVDJH WKURXJK WKH PRGXOHV WHUPHG DV KDUG EORFNDJHV +HQFHWRWDOQXPEHURIHVWPDWHGURXWHSDWKV5 SNS 6WHS±:HILUVWGUDZDGURSOHWLQWHUVHFWLRQJUDSK*' 9'(' ZKHUHY9' UHSUHVHQWVHDFKGURSOHWDQGHYLYM ('H[LVWVLIWKHUH DUHFURVVRYHUVEHWZHHQSDWKVRIGURSOHWYLDQGYMHDFKVXFKHGJHHLV DVVLJQHG DQ ZHLJKW ZLM UHSUHVHQWLQJ WRWDO QXPEHU RI SDWKV FROOLGLQJ EHWZHHQYLDQGYM 6WHS  ± 7KHQ ZH GUDZ D SDWK LQWHUVHFWLRQ JUDSK *3  93  (3  ZKHUHY93UHSUHVHQWVHDFKSDWKDQGHY[Y\(3H[LVWVLIWKHUHDUH FURVVRYHUVEHWZHHQSDWKVY[DQGY\(DFKHGJHHKDVEHHQDVVLJQHG DQZHLJKWZ[\UHSUHVHQWLQJQXPEHURIFHOOVFRQWDPLQDWHGGXULQJHDFK FURVRYHU7KHUHZLOOEH5QXPEHURIYHUWLFHVLQ*3WKDWUHSUHVHQWVWRWDO QXPEHURIHVWLPDWHGSDWKVLQFOXVLYHRILQWHQWLRQDOUHGXQGDQF\ ,IWKHSDWKUHSUHVHQWHGE\YLFURVVHVDPRGXOH0[DWWDFKYL WRDQHZ EORFNYHUWH[0[WKURXJKDGLIIHUHQWFRORUHGJHHPL[ 6WHS±IRUPDWDEOH7SDWKWRPDSHDFKYHUWH[Y[ZLWKLWVDOWHUQDWH Y[LIDQ\RWKHUZLVHNHHSWKHDOWHUQDWHFROXPQEODQN 6WHS±LQ*UDSK*3 DUUDQJHDOOWKHYHUWLFHVLQGHVFHQGLQJRUGHURI GHJUHHVDQGQXPEHUWKHPIURPWR5LQWKHVDPHRUGHU )RUL WR5 )RUYHUWH[YL93LIYLH[LVWVDQGLVQRWGHOHWHGDOUHDG\ ,IGHJYL!GHJDOWHUQDWHYL>LIDOWHUQDWHRIYLH[LVWVLQ7SDWK@ 'HOHWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQYL ,IGHJYL GHJDOWHUQDWHYL>LIDOWHUQDWHRIYLH[LVWVLQ7SDWK@ &RPSXWHZHLJKWYL : Z[\VXPPDWLRQRIZHLJKWVRI DOOHGJHVRILQFLGHQWRQYL

52

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

 &RPSXWHZHLJKWDOWHUQDWHYL : Z[\VXPPDWLRQRI ZHLJKWVRIDOOHGJHVRILQFLGHQWRQDOWHUQDWHYL ,I:!: 'HOHWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQYL (OVHLI:!: 'HOHWHDOWHUQDWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHV LQFLGHQWRQDOWHUQDWHYL (OVH>IRUWKHFDVH: :DQGGHJYL GHJDOWHUQDWHYL@ ,IHLWKHURIYLRUDOWHUQDWHYLKDVHGJHHP[ERXQGWRPRGXOH0[ 'HOHWHWKHFRUUHVSRQGLQJYHUWH[DQGZLWKDOOWKHHGJHV LQFLGHQWRQLWLHYLRUDOWHUQDWHYLZKLFKHYHUDSSOLFDEOH 1H[WL 6WHS  ±  1RZ GUDZ WKH OD\RXW ZLWK UHPDLQLQJ SDWKV DIWHU GHOHWLRQ 7KHUH PD\ VWLOO UHPDLQ VRPH UHGXQGDQW SDWKV IRU FHUWDLQ QHWV1RZ DVVXPH WRWDO 6 QXPEHU RI SDWKV DUH OHIW ZLWKLQ WKH QHZ OD\RXW DIWHU GHOHWLRQ 6WHS  ±  /HW WKHUH EH / QXPEHU RI SDWKV WKDW SDVVHV WKURXJK KDUG EORFNDJHV )RU HDFK VXFK SDWK GHWHUPLQH WZR SRVVLEOH GHYLDWLRQV WR E\SDVV WKH FRUUHVSRQGLQJ EORFNDJHV +HQFH WRWDO QXPEHU RI SDWKV DYDLODEOH5 6// 6WHS  ± $JDLQ GUDZ D SDWK LQWHUVHFWLRQ JUDSK IURP WKLV UHPDLQLQJ OD\RXWDQGUHSHDWIURP6WHSWR6WHSXQWLODOOSDWKVDUHDGGUHVVHG 6WHS±2XWRIWKHVH6QXPEHURIUHPDLQLQJSDWKVILQDOO\REWDLQHG± LGHQWLI\ WKRVH SDWKV WKDW IRUPV ]RQHV RI IULFWLRQ =RQH RI IULFWLRQ LV GHILQHG DV ]RQHV FRPSULVLQJ RI FHOOV WKDW FRQVWLWXWH SDUW RI WZR QHLJKERXULQJ URZV RU FROXPQV   ZKLFK KDV EHHQ XWLOL]HG E\ WZR FRUUHVSRQGLQJURXWHSDWKV 6WHS  ± LQ WKLV ILQDO SDWK LQWHUVHFWLRQ JUDSK IRUP D K\SHUSDWK FRQQHFWLQJDQ\WZRSDWKVYHUWLFHVWKDWIRUPHDFKIULFWLRQ]RQHV (DFKIULFWLRQ]RQHK\SHUSDWKFRQQFWVRQO\WZRYHUWLFHVY[DQGY\ )RUHDFKIULFWLRQ]RQHK\SHUSDWK ,IDOWHUQDWHY[H[LVWVDQGGRHVQRWEHORQJWRDQ\RWKHUK\SHUSDWK 'HOHWHY[ZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQY[ (OVHLIDOWHUQDWHY\H[LVWVDQGGRHVQRWEHORQJWRDQ\RWKHUK\SHUSDWK  'HOHWHY\ZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQY\ (OVHLIQRDOWHUQDWHYHUWH[H[LVWVIRUHLWKHUY[RUY\ 0DUNWKH]RQHDVLQHYLWDEOHIULFWLRQ]RQHDQGH[SORUHDSRVVLEOH GHWRXUIRUHLWKHUY[RUY\ZKLFKHYHUVXLWDEOHDQGFRYHUVPLQLPXP QXPEHURIDGGLWLRQDOFHOOVZLWKQRFRQWDPLQDWLRQV 6WHS  ±  LQ WKH ILQDO SDWK LQWHUVHFWLRQ JUDSK OHW WKH QXPEHU RI UHPDLQLQJ YHUWLFHV EH 93 DQG HGJHV (3 7KHVH HGJHV UHSUHVHQW QXPEHU RI XQDYRLGDEOH FURVVRYHUV DQG VXPZHLJKWV RI HGJHV :3 UHSUHVHQWV WKH WRWDO QXPEHU RI XQDYRLGDEOH FROOLVLRQVUHDUUDQJH DOO YHUWLFHVDJDLQLQGHVFHQGLQJRUGHURIWKHLUGHJUHHV 6WHS± )RUL WR93 ,IIRUYLWKHUHH[LVWVDQDOWHUQDWHYL 'HOHWHDQ\RQHRIWKHVHWZRZLWKDOOWKHLQFLGHQWHGJHVLIDQ\ 1H[WL 6WHS±(VWLPDWHWKHWRWDOQXPEHURIFURVVRYHUVDQGIULFWLRQ]RQHV OHIWRYHUZLWKLQWKHILQDOSDWKLQWHUVHFWLRQJUDSK'UDZWKHILQDOOD\RXW DQGIRUPWKHILQDOGURSOHWLQWHUVHFWLRQJUDSKDVGUDZQLQVWHS 6WHS  ± (VWLPDWH WKH 0DQKDWWDQ GLVWDQFH 'L IRU HDFK QHW During concurrent routing of several droplets, there still remains a possibility of collision or overlap of paths between multiple droplets. In such a case, priorities are given in favour of the droplets with larger Di. For 2-pin nets, each droplet is routed directly from its source to its target. For 3-pin (multi-pin) nets, routing from each of the sources Sx and Sy to the mixer (M) is done concurrently. This part of routing is referred to as the First Generation route. The largest arrival time T between (SX ÆM) and (SY ÆM) is noted. The final mixed droplet from M is next routed to Target T. This is called the Second Generation route. nd For 2 generation route, the timestamp starts from TSM as determined earlier. - Start routing of each net starting with timestamp = 0 for Step 16 -each net. It is also assumed that transition time for a droplet between two adjacent cells to be of One unit.Compute the arrival time for each droplet -- mark the maximum of all arrival times as the latest arrival - it is possible to conclude that route is time T. If T = max (Di) -optimized- because max (Di) represents the critical path among all the
SM

nets. Finally count the total number of cells utilized -- E and the number of cells sharing multiple droplets representing total number of contaminations C. Output: The final layout representing optimized route paths .the overall route time, the latest arrival time and the average arrival time (overall route time/number of droplets) ,the total number of contaminations remaining and number of cells utilized.

9,$1,//8675$7,9((;$03/(
7KH LPSOHPHQWDWLRQ RI WKH DOJRULWKP LV GHPRQVWUDWHG XVLQJ DQ LOOXVWUDWLYHH[DPSOHDVVKRZQLQILJXUH,QILJXUHDDJLYHQOD\RXW KDV EHHQ GLVSOD\HG 7KH OD\RXW LV FRPSULVHG RI  7ZR SLQ DQG 2QH 7KUHHSLQQHWZLWK7ZRKDUGEORFNDJHVQDPHO\0DQG07KHSDWKV IRU HDFKGURSOHW ZLWK FRUUHVSRQGLQJ DOWHUQDWLYHV DUHGLVSOD\HG DV3QL ZKHUHQ DEFGRUHGURSOHWQDPHDQGLLQGLFDWHVWKHFRUUHVSRQGLQJ SDWK QXPEHU IRU WKH VDLG GURSOHW)RU 7KUHH SLQ GURSOHW SDWK QDPH LV LQGLFDWHG E\ 3Q[L DQG 3Q\L IRU VRXUFHV Q[ DQG Q\ WR WKH PL[HU UHVSHFWLYHO\7KH SDWK 3QWL UHSUHVHQWV WKH SDWK IURP PL[HU WR WKH UHVSHFWLYH WDUJHW)LJXUH  E GLVSOD\V WKH GURSOHW LQWHUVHFWLRQ JUDSK WRJHWKHU ZLWK OLQNDJHV WR WKH KDUG EORFNDJHV RU PRGXOHV LQGLFDWLQJ SDWKVIRUWKHVDLGQHWSDVVHVWKURXJKWKHPRGXOH7KHZHLJKWDJHWRWKH HGJHVUHSUHVHQWVWKHQXPEHURISDWKVIRUWKHQHWVDUHLQYROYHGLQWKH FURVV RYHU)LJXUH  F VKRZV WKH LQLWLDO SDWK LQWHUVHFWLRQ JUDSK 7KH YHUWLFHV ZLWK VFDU PDUNV DUH PDUNHG IRUGHOHWLRQ ZLWK FRUUHVSRQGLQJ LQFLGHQWHGJHVVKRZQE\SDWWHUQHGJUHHQOLQH7KHJUH\FRORUHGHGJH UHSUHVHQWV DOORZDEOH FURVVRYHU RU FRQWDPLQDWLRQ DV HYLGHQW IRU WKH VDPHPHPEHUVRIDWKUHHSLQQHW7KHUHGFRORUHGHGJHVFRQQHFWLQJWKH SDWKV ZLWK PRGXOHV UHSUHVHQW SDWKV WKDW FURVVHV RYHU WKH VDLG PRGXOHV)LJXUH  G VKRZV WKH ILQDO SDWK LQWHUVHFWLRQ JUDSK DIWHU LWHUDWLYHVHOHFWLRQRIVXLWDEOHSDWKV)LJXUHHUHSUHVHQWVWKHUHVXOWLQJ OD\RXW ZLWK FXUUHQWO\ VHOHFWHG SDWKV DIWHU RPLVVLRQ RI WKH UHGXQGDQW RQHV)LJXUH  I UHSUHVHQWV WKH DGGLWLRQDO SDWKV LQWURGXFHG DIWHU GHYLDWLRQ WR E\SDVV WKH KDUG EORFNDJHV ,Q ILJXUH  J WKH SDWK LQWHUVHFWLRQ JUDSK LV UHGUDZQ ± ZKHUH WKH IRXU IULFWLRQ ]RQHV DUH LQGLFDWHGE\K\SHUSDWKK\SHUSDWKK\SHUSDWKDQGK\SHUSDWK 7KHSDWKZLWKDYDLODEOHDOWHUQDWLYHLVRPLWWHGWRHOOLPLQDWHWKHIULFWLRQ ]RQHV DYRLGLQJ SRVVLEOH VWDOOLQJ 0RUHRYHU RWKHU SDWKV KDYH EHHQ SUHIHUDEO\VHOHFWHGXVLQJWKHWHFKQLTXHPHQWLRQHGLQ6WHSLQVHFWLRQ 97KHILQDOOD\RXWZLWKURXWHSDWKVDQGFRUUHVSRQGLQJURXWHUHVXOWVDUH VKRZQLQILJXUHK+RZHYHUWKHUHPDLQLQJIULFWLRQ]RQHVIRUPHGLQ WKH OD\RXW DUH IRXQG WR EH IUHH RI VWDOOLQJ ± DV GURSOHWV FOHDUV RI ZLWKRXWDQ\YLRODWLRQRIIOXLGLFFRQVWUDLQWV7KHODWHVWDUULYDOWLPHLV IRXQGWREHDYHUDJHDUULYDOWLPHLVFHOOXWLOL]DWLRQEHLQJ WKH QXPEHU RI XQDYRLGDEOH FRQWDPLQDWLRQV DUH OLPLWHG WR LQGLFDWHG E\ SLQN FHOOV ZLWK DOORZDEOH FRQWDPLQDWLRQ EHLQJ LQGLFDWHGE\JUHHQFHOO

 )LJXUHDLQLWLDOOD\RXWZLWKWZRSLQDQG7KUHHSLQQHW±DQGWZR PRGXOHV0DQG0$OOHVWLPDWHGURXWHSDWKVZLWKUHGXQGDQWDOWHUQD WLYHVDUHVKRZQ

 )LJXUH  E WKH GURSOHW LQWHUVHFWLRQ JUDSK ZLWK PRGXOH OLQNDJHV E\ SDWKVDUHVKRZQ

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

53

 )LJXUHK7KHILQDOOD\RXWZLWKURXWHSDWKVDQGFRUUHVSRQGLQJURXWH UHVXOWV  )LJXUHF,QLWLDOSDWKLQWHUVHFWLRQJUDSKLQGLFDWLQJUHGXQGDQWSDWKV DQGFURVVRYHUGHWHUPLQHGIRURPLVVLRQ

 )LJXUHL)LQDOGURSOHWLQWHUVHFWLRQJUDSKVKRZLQJ)RXUHGJHVZLWK 7KUHHXQDYRLGDEOHDQG2QHDOORZDEOHFRQWDPLQDWLRQ

9,,(;3(5,0(17$/5(68/76
 )LJXUH  G )LQDO SDWK LQWHUVHFWLRQ JUDSK DIWHU LWHUDWLYH VHOHFWLRQ RI WKHVXLWDEOHSDWKV 7KHSURSRVHGWHFKQLTXHIRUH[SORUDWLRQRIVXLWDEOHURXWHSDWKZLWKDQ REMHFWLYH RI PLQLPL]LQJ QXPEHU RI FRQWDPLQDWLRQV DV ZHOO DV HQKDQFLQJ WKH URXWLQJ SHUIRUPDQFH KDV EHHQ DSSOLHG RQ 7HVWEHQFKHV RI%HQFKPDUNVXLWH,DQG,,,7KHGHWDLOHGURXWLQJUHVXOWERWKZLWKDQG ZLWKRXWFURVVFRQWDPLQDWLRQIRUVXESUREOHPVRIWHVWEHQFK,QBYLWUR  LQ EHQFKPDUN VXLWH ,,, KDV EHHQ GLVSOD\HG LQ 7DEOH  7DEOH  GLVSOD\V VXPPDUL]HG UHVXOWV IRU WHVWEHQFKHV ,QB9LWUR ,QB9LWUR SURWHLQDQGSURWHLQRI%HQFKPDUNVXLWH,,,7KHUHVXOWVIRUIHZ WHVWEHQFKHVRIEHQFKPDUNVXLWH,KDYHEHHQJLYHQLQ7DEOH)LQDOO\ LQ 7DEOH  IHZ FRPSDUDWLYH UHVXOWV ZLWK SHUIRUPDQFHV RI FRQWHPSRUDU\ PHWKRGV LV GLVSOD\HG WR HVWDEOLVK WKH HIIHFWLYHQHVV RI WKHSURSRVHGWHFKQLTXH ,W KDV EHHQ IRXQG WKDW LQ PRVW FDVHV IRU EHQFKPDUN VXLWH ,,, FHOO XWLOL]DWLRQ KDV EHHQ LQFUHDVHG EXW 0D[LPXP DUULYDO WLPH DV ZHOO DV DYHUDJH DUULYDO WLPH LQFUHDVHG LQ VRPH FDVHV RQO\ PDUJLQDOO\ 7KLV LPSOLHV WKDW DV PRUH GHWRXUV DUH QHFHVVDU\ WR DYRLG FURVV FRQWDPLQDWLRQ WKDW FRQWULEXWHV WRZDUGV LQFUHDVH LQ FHOO XWLOL]DWLRQ RQ WKHRWKHUKDQG5RXWHWLPHKDVEHHQNHSWWREHRSWLPXPWKDWUHVXOWVLQ RQO\PDUJLQDOLQFUHDVHLQFHUWDLQFDVHVRIWKHDYHUDJHDUULYDOWLPHZLWK DOPRVWQRYDULDWLRQLQPD[LPXPDUULYDOWLPH7KHFRQWDPLQDWLRQVDUH IRXQGWREHUHGXFHGFRQVLGHUDEO\LQDOPRVWDOOWKHFDVHV +RZHYHUIRUWHVWEHQFKHVLQEHQFKPDUNVXLWH,DVQXPEHURIGURSOHWV DUH PRUH LQ FRPSDULVRQ WR JULG VL]H ± OLWWOH VSDFH DUH DYDLODEOH WR SURYLGHDOWHUQDWHSDWKRUQHFHVVDU\GHWRXUWRDYRLGFRQWDPLQDWLRQ6WLOO WKH SURSRVHG PHWKRG KDV DWWHPSWHG WR UHGXFH QXPEHU RI FRQWDPLQDWLRQZLWKRSWLPL]HGURXWHSHUIRUPDQFH 7DEOHGHWDLOHGURXWHSHUIRUPDQFHIRUWHVWEHQFK,Q9LWUR

 )LJXUHHWKHUHVXOWLQJOD\RXWZLWKFXUUHQWO\VHOHFWHGSDWKVDIWHU RPLVVLRQRIWKHUHGXQGDQWRQHV

 )LJXUHIOD\RXWZLWKWKHDGGLWLRQDOSDWKVEHLQJLQWURGXFHGDIWHU GHYLDWLRQWRE\SDVVWKHKDUGEORFNDJHVLQGLFDWLQJWKHIULFWLRQ]RQHV


)LJXUHJWKHQHZSDWKLQWHUVHFWLRQJUDSK±ZKHUHWKHIRXUIULFWLRQ ]RQHVDUHLQGLFDWHGE\K\SHUSDWKK\SHUSDWKK\SHUSDWKDQG K\SHUSDWK 

54

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

>@$ 5HQDXGLQ 3 7DERXULHU  9 =KDQJ & 'UXKRQ DQG -& &DPDUW 3ODWHIRUPH 6$: GpGLpH j OD PLFURIOXLGLTXH GLVFUqWH SRXU DSSOLFDWLRQV ,QWKLVZRUNZHSURSRVHGDVSHFLILFWHFKQLTXHWKDWSHUIRUPVLQWHOOLJHQW ELRORJLTXHV qPH &RQJUqV )UDQoDLV GH 0LFURIOXLGLTXH6RFLpWp H[SORUDWLRQRI FURVV FRQWDPLQDWLRQ DZDUH YLUWXDOURXWH SDWKV WKDWQRW +\GURWHFKQLTXHGH)UDQFH7RXORXVH)UDQFH >@ 6. &KR + 0RRQ DQG &- .LP ³&UHDWLQJ WUDQVSRUWLQJ FXWWLQJ DQG RQO\ PLQLPL]HV WKH RYHUDOO QXPEHU RI FRQWDPLQDWLRQV EXW DOVR WKH PHUJLQJ OLTXLG GURSOHWV E\ HOHFWURZHWWLQJEDVHG DFWXDWLRQ IRU GLJLWDO PLFUR RYHUDOOURXWHWLPHWRJHWKHUZLWKODWHVWDUULYDOWLPHIRUDOOWKHGURSOHWV IOXLGLF FLUFXLWV´ -RXUQDO RI 0LFURHOHFWURPHFKDQLFDO 6\VWHPV YRO   LQYROYHGLQWKH H[HFXWLRQRI WKHVSHFLILHG SURWRFRO,WKDV EHHQ IRXQG SS VR IDU WKDW RQ LPSOHPHQWDWLRQ RI WKH SURSRVHG DOJRULWKP ± DQ >@$OERU]$U]SH\PD6KDQ%KDVHHQ$OL'RODWDEDGL3DXOD:RRG$GDPV HQFRXUDJLQJ LPSURYHPHQW KDV EHHQ DFKLHYHG LQ FRPSDULVRQ WR $FRXSOHGHOHFWURK\GURG\QDPLFQXPHULFDOPRGHOLQJRIGURSOHWDFWXDWLRQE\ FRQWHPSRUDU\ PHWKRGV ,Q WKLV SDSHU ZH KDYH HPSOR\HG  D JUDSK HOHFWURZHWWLQJ &ROORLGV DQG 6XUIDFHV $ 3K\VLFRFKHP (QJ $VSHFWV YRO EDVHG KHXULVWLF WR LGHQWLI\ VXLWDEOH DOWHUQDWLYHV IRU LQWHOOLJHQW SS DYRLGDQFH RI FURVV FRQWDPLQDWLRQV ZKLOH PDLQWDLQLQJ RSWLPXP URXWH >@ <+ &KDQJ *% /HH )& +XDQJ << &KHQ -/ /LQ ,QWHJUDWHG SRO\PHUDVH FKDLQ UHDFWLRQ FKLSV XWLOL]LQJ GLJLWDO PLFURIOXLGLFV %LRPHG SHUIRUPDQFH LQ GLJLWDO PLFURIOXLGLF %LRFKLSV %XW WKHUH VWLOO UHPDLQ 0LFURGHYLFHVYROSS VFRSHV IRU IXUWKHU LPSURYHPHQW XVLQJ ILQHU DGMXVWPHQW RI WKH QHWV >@96ULQLYDVDQ9.3DPXOD5%)DLU$QLQWHJUDWHGGLJLWDOPLFURIOXLGLF XVLQJPHWLFXORXVSODFHPHQWDQGFRPSDFWLRQWHFKQLTXHV ODERQDFKLS IRU FOLQLFDO GLDJQRVWLFV RQ KXPDQ SK\VLRORJLFDO IOXLGV/DE 7DEOH&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWVIRUEHQFKPDUN &KLSYROSS VXLWH,,, >@+0RRQ$5:KHHOHU5/*DUUHOO-$/RR&-.LP$QLQWHJUDWHG GLJLWDOPLFURIOXLGLFFKLSIRUPXOWLSOH[HGSURWHRPLFVDPSOHSUHSDUDWLRQ DQGDQDO\VLVE\0$/'O06/DE&KLSYROSS >@0$EGHOJDZDG$DURQ5:KHHOHU/RZFRVWUDSLGSURWRW\SLQJRI GLJLWDOPLFURIOXLGLFVGHYLFHV0LFURIOXLG1DQRIOXLGYROSS  >@ - %HUWKLHU 0LFURGURSHV DQG 'LJLWDO 0LFURIOXLGLFV :LOOLDP $QGUHZ ,QF1RUZLFK1< >@)6X-XQ=HQJ³&RPSXWHUDLGHGGHVLJQDQGWHVWIRUGLJLWDOPLFURIOXLGLFV  ´,((('HVLJQDQGWHVWIRUFRPSXWHUVSS)HE >@7< +R - =HQJ DQG . &KDNUDEDUW\ ³'LJLWDO PLFURXLGLF ELRFKLSV $ 7DEOH&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWVIRUVHOHFWHGWHVW YLVLRQIRUIXQFWLRQDOGLYHUVLW\DQGPRUHWKDQ0RRUH3URF$&0,((( EHQFKHVLQEHQFKPDUNVXLWH, ,&&$'SS^ >@7VXQJ:HL+XDQJ&KXQ+VLHQ/LQDQG7VXQJ<L+R³$FRQWDPLQDWLRQ DZDUH GURSOHW URXWLQJ DOJRULWKP IRU GLJLWDO PLFURIOXLGLF ELRFKLSV ´ ,&&$'¶ 1RYHPEHU±6DQ-RVH&DOLIRUQLD86$ >@$NHOOD6*ULIILWK(-DQG*ROGEHUJ0.³3HUIRUPDQFHFKDUDFWHU L]DWLRQ RI D UHFRQILJXUDEOH SODQDUDUUD\ GLJLWDO PLFURIOXLGLF V\VWHP´ ,((( 7UDQVDFWLRQV &RPSXWHU$LGHG 'HVLJQ RI ,QWHJUDWHG &LUFXLWV DQG 6\VWHPV SS)HE >@3<XK66DSDWQHNDU&<DQJDQG<&KDQJ³$SURJUHVVLYHLOSEDVHG URXWLQJ DOJRULWKP IRU FURVV UHIHUHQFLQJ ELRFKLSV´ 'HVLJQ $XWRPDWLRQ &RQIHUHQFHSS± >@   + :LOOLDP 6X )HL DQG &KDNUDEDUW\ . ³'URSOHW URXWLQJ LQ WKH V\QWKHVLVRIGLJLWDOPLFURIOXLGLFELRFKLSV´ ,Q3URFRI'HVLJQ$XWRPDWLRQ  7HVWLQ(XURSH >@&KR0LQVLNDQG3DQ'DYLG=³$KLJKSHUIRUPDQFHGURSOHWURXWLQJ DOJRULWKPIRUGLJLWDOPLFURIOXLGLFELRFKLSV´,(((7UDQVDFWLRQV&RPSXWHU $LGHG'HVLJQRI,QWHJUDWHG&LUFXLWVDQG6\VWHPVSS >@7;XDQG.&KDNUDEDUW\³'URSOHWWUDFHEDVHGDUUD\SDUWLWLRQLQJDQGD  SLQ DVVLJQPHQW DOJRULWKP IRU WKH DXWRPDWHG GHVLJQ RI GLJLWDO PLFURIOXLGLF 7DEOHFRPSDULVRQRI&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWV ELRFKLSV´,Q&2'(6,666SS±2FW >@7VXQJ:HL+XDQJ7VXQJ<L+R³$)DVW5RXWDELOLW\DQG3HUIRUPDQFH ZLWKFRQWHPSRUDU\PHWKRGVIRU%HQFKPDUNVXLWH,,, GULYHQGURSOHWURXWLQJDOJRULWKPIRUGLJLWDOPLFURIOXLGLFELRFKLSV´ &RPSXWHU 'HVLJQ,&&',(((,QWHUQDWLRQDO&RQIHUHQFH2FWSS /DNH7DKRH&$ >@ 6RXNXS - ³)DVW 0D]H 5RXWHU´ 3URFHHGLQJV RI WKH WK$&0 'HVLJQ $XWRPDWLRQ&RQIHUHQFHSS >@ 3 5R\ + 5DKDPDQ 3 'DVJXSWD ³$ 1RYHO 'URSOHW 5RXWLQJ $OJRULWKP IRU'LJLWDO0LFURIOOXLGLF%LRFKLSV´3URFRIWKH*/69/6,3URYLGDQFH 5KRGH,VODQG86$ >@=LJDQJ;LDR(YDQJHOLQH)<<RXQJ³'URSOHW5RXWLQJ$ZDUH0RGXOH 3ODFHPHQW IRU &URV5HIHUHQFLQJ %LRFKLSV´,63' 6DQ )UDQVLVFR &DOLIRUQLD86$WK0DUFK >@ <DQJ =KDR DQG .ULVKQHQGX &KDNUDEDUW\³6\QFKURQL]DWLRQ RI :DVKLQJ  2SHUDWLRQV ZLWK 'URSOHW 5RXWLQJ IRU &URVV&RQWDPLQDWLRQ $YRLGDQFH LQ 'LJLWDO0LFURIOXLGLF%LRFKLSV´3URF2I'$&
-XQH$QDKHLP 5()(5(1&(6 &DOLIRUQLD86$ >@ 0* 3ROODFN 5% )DLU DQG $' 6KHQGHURY ³(OHFWURZHWWLQJEDVHG >@ 7: +XDQJ DQG 7< +R ³$ 7ZR6WDJH ,/3%DVHG 'URSOHW 5RXWLQJ DFWXDWLRQ RI OLTXLG GURSOHWV IRU  PLFURIOXLGLF DSSOLFDWLRQV´ $SSO3K\V/HWW  $OJRULWKP IRU 3LQ&RQVWUDLQHG 'LJLWDO 0LFURXLGLF %LRFKLSV 3UR RI $&0  ,63'0DU >@-/HH+0RRQ-)RZOHU&-.LPDQG76FKRHOOKDPPHU³$GGUHVVDEOH >@<DQJ&/<XK3+DQG&KDQJ<:³%LRURXWH$QHWZRUNIORZEDVHG PLFUR OLTXLG KDQGOLQJ E\ HOHFWULF FRQWURO RI VXUIDFH WHQVLRQ´ 3URF RI  URXWLQJ DOJRULWKP IRU GLJLWDO PLFUR IOXLGLF ELRFKLSV´ ,Q 3URFHHGLQJV RI ,((( WK ,QWHUQDWLRQDO &RQIHUHQFH RQ 0(06 ,QWHUODNHQ6ZLW]HUODQG  ,((($&0 ,QWHUQDWLRQDO&RQIHUHQFH RQ &RPSXWHU$LGHG 'HVLJQ SDJHV    >@5&*DVFR\QH DQG -9 9\NRXNDO³'LHOHFWURSKRUHVLV EDVHG VDPSOH >@ 3 5R\ + 5DKDPDQ  5 %KDWWDFKDU\D 36 'DVJXSWD ³$ %HVW 3DWK KDQGOLQJ LQ JHQHUDOSXUSRVH SURJUDPPDEOH GLDJQRVWLF ,QVWUXPHQWV´ 3URF 6HOHFWLRQ %DVHG 3DUDOOHO 5RXWHU )RU '0)%V´3URF RI ,6(' ,((( ,((( ,QWHUQDWLRQDO&RQIHUHQFHSS'HFHPEHU.RFKL,QGLD >@'$$QWRQ-39DOHQWLQR607URMDQDQG6:DJQHU³7KHUPRFDSLOODU\ >@<=KDRDQG.&KDNUDEDUW\³&URVVFRQWDPLQDWLRQDYRLGDQFHIRUGURSOHW DFWXDWLRQ RI GURSOHWV RQ FKHPLFDOO\ SDWWHUQHG VXUIDFHV E\ SURJUDPPDEOH URXWLQJLQGLJLWDOPLFURIOXLGLFELRFKLSV´,((($&0'$7($SU PLFURKHDWHUDUUD\V´-0LFURHOHFWURPHFKDQLFDO6\V

9,,,&21&/86,21

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

55

View publication stats

Digital microfluidic biochip (DFMB) systems have been developed as a promising platform for Lab-on-chip systems that manipulate individual droplet of chemicals on a 2D planar array of electrodes. Because of the safety critical nature of the applications these devices are intended for high reliability and thereby dependability becomes a major issue for the design of DMFBs. Therefore, such devices are required to be tested frequently both off-line (e.g., post-manufacturing) and prior to each assay execution. Under both scenarios, testing is accomplished by routing one or more test droplets across the chip and recording their arrival at the scheduled destination. In this paper, we have proposed a new design of a droplet motion detector based on capacitive sensing, which can be manufactured with the cell electrodes for detection of the presence (arrival) of a droplet at a predetermined location. Using this sensor, we have further proposed a customized testing technique for a specified layout with an objective of 1) optimizing the total number of test droplets for testing a particular bioassay, 2) optimizing the number of dispensers, 3) minimizing the overall test completion time, 4) detection of a specific segment at fault within the given layout, and 5) optimizing the number of locations where the detectors are to be activated. The test simulation has been carried out on two testbenches of Benchmark suite III and the results are found to be encouraging compared to the existing methods.A second generation lab-on-chip device termed as digital microfluidic biochip is developed as a time-multiplexed reconfigurable device capable of executing multiple bioassay protocols simultaneously on a single 2D planar array. They combine biology with electronics and integrate various bioassay operations namely sample preparation, analysis, separation and detection. Optical detection, processing and analysis are considered to be of major significance as these may influence the decisions on diagnosis, detection and testing. In this paper we initially propose two separate circuit designs of digital detection analyzer to be coupled with a biochip for execution of prescheduled bioassay protocols. The two types of analyzers enable automated detailed analysis of the optical detection results based on the data acquired at the detection site through successful Biochip operation for a) homogeneous droplet samples and b) heterogeneous droplet samples. A third circuit that integrates the previous two designs for dual mode operation is also proposed. Synthesis and simulation of all the three proposed designs are carried out using pre-characterized reference data for measurement of different parameters of human blood samples and the corresponding final detection results are displayed and verified successfully.The need for training employees in new skills in an organization generally arises due to the changing skill requirements coming from the introduction of new products, technology and customers. Efficient assignment of employees to trainings so that the overall training cost is minimized while considering the career goals of employees is a challenging problem and to the best of our knowledge there is no existing work in literature that solves this problem. This paper presents TRaining AssigNment Service (TRANS) that minimizes an organizationâ€™s overall training costs while assigning employees to trainings that match their learning ability and career goals. TRANS uses an ORGanization and Skills ontology (ORGS) to calculate the cost for training each available employee for a potential role taking into account constructivist learning theory. TRANS uses TRaining assIgnMent algorithm (TRIM), based on Hungarian method for bipartite matching, for assigning employees to trainings. In our experiments with real-world data, proposed allocation algorithm performs better than the existing strategy of the organization.A Rule-Based Approach for Minimizing Power Dissipation of Digital Circuits
Subrata Das , Parthasarathi Dasgupta , Petr Fiser , Sudip Ghosh§ and Debesh Kumar Das
of Computer Science & Engineering, Jadavpur University, Kolkata, India Email: dsubrata.mt@gmail.com, debeshd@hotmail.com  MIS Group, Indian Institute of management Calcutta, Kolkata, India Email: partha@iimcal.ac.in  Faculty of Information Technology, Czech Technical University in Prague, Czech Republic Email:fiserp@fit.cvut.cz § School of VLSI Technology, IIEST, Shibpur, Howrah, India Email:sudipghosh2005@gmail.com
Abstract--Minimization of power dissipation of VLSI circuits is one of the major concerns of recent digital circuit design primarily due to the ever decreasing feature sizes of circuits, higher clock frequencies and larger die sizes. The primary contributors to power dissipation in digital circuits include leakage power, short-circuit power and switching power. Of these, power dissipation due to the circuit switching activity constitutes the major component. As such, an effective mechanism to minimize the power loss in such cases often involves the minimization of the switching activity. In this paper, we propose an intelligent rule-based algorithm for reducing the switching activity of the digital circuits at logic optimization stage. The proposed algorithm is empirically tested for several standard digital circuits with Synopsys EDA tool and the results obtained are quite encouraging. Index Terms--Switching activity, low-power VLSI circuits, CMOS, power dissipation, dynamic power, logic optimization.
 Department

I. I NTRODUCTION Traditionally, the major concerns of VLSI designers include minimization of the chip area, enhancement of performance, testability, reduction of manufacturing cost and the improvement of reliability. With increasing use of portable devices and wireless communication systems, the reduction of energy consumption and hence the reduction of power dissipation and optimization of chip temperature are current issues in recent VLSI design [1]. Power dissipated by a digital system increases the temperature of the chip and affects battery life of the digital devices [2]. Aggressive device scaling also causes excessive increase in power per unit area of the chip. As such, heat generation and its removal from a chip is a matter of serious concern [3]. In   circuits the three primary sources of power dissipation are [4] 1) The switching activity occurs due to logic transitions. When the nodes of a digital circuit make transition back and forth between two logic levels, parasitic capacitances are charged and discharged. Consequently current flows through the channel resistance of the transistors, and electrical energy is converted into heat [4].

2) The short-circuit current that flows from supply to ground when both the -subnetwork and -subnetwork of a   gate conduct [4]. 3) The leakage current [4] caused by substrate injection at - junctions and sub-threshold effects determined by the fabrication technology. The first two sources of power dissipation are known as dynamic power dissipation and the third one constitutes the static power dissipation. In the present-day technology about 80% of the total power loss occurs due to the switching activity [4]. Thus, minimization of the power dissipation of VLSI circuits necessitates minimization of the dynamic power and hence minimization of the switching activity. Minimization of switching activity can be done at the logic optimization stage. However, the focus of earlier works in logic optimization is primarily on reduction of the number of appearances of literals, minimum number of literals in a Sum of Products (SOP) or Product of Sums (POS) expression and minimum number of terms in a SOP expression [5]. In this paper, we propose an algorithm to obtain for a given input logic expression, an equivalent logic expression with minimized switching activity. II. L ITERATURE R EVIEW Minimization of power consumption of CMOS digital circuits is studied in the past, considering all levels of the design such as physical, circuit and logic level [6], [7]. In digital CMOS circuits the measure of power dissipation is the circuit switching activity or the average number of transitions. Minimization of the average number of transitions of CMOS digital circuits nodes is discussed in [8]. The work in [9] provides an interesting repository of recent techniques of power modeling and low-power design based on high-level synthesis. The evaluation and the reduction of the switching activity in combinational logic circuits considering both the transitions 1  0 and 0  1 at any output node is proposed in [10]. In order to satisfy the classical probabilistic approach that limits the maximum value of the switching activity to 1, the definition of the switching activity as proposed in [10]

was customized in [11]. An algorithmic approach at the gate level using Karnaugh maps for reducing the switching activity in combinational logic circuits is presented in [12]. However, the use of Karnaugh maps restricts the number of variables to around 6. Moreover, for the method proposed in [12], the switching activity can be minimized only for some specific switching functions. In [13] the authors proposed a method to estimate the switching activity using a variable delay model. The work of [14] has discussed the system level dynamic power management in chip multiprocessor (CMP) architectures. [15] proposed an algorithm to minimize logic functions with reduced area and interconnects that will improve the circuit performance. Pre-computation-based optimization for low power that computes the output logic values of the circuit in one clock cycle before they are computed, was discussed in [16]. III. P RELIMINARIES A definition of the switching activity based on the classical probabilistic approach is given in [11]. For a logic expression of a switching function for an output node , let    and    represent the number of 1's and the number of 0's. The probabilities of occurrence of a 0 and a 1 respectively at the output node  are given by the following equations: 0 = 1 =       +          +    (1) (2)

switching activity of the basic gates. For an  , ,    and   gate with  inputs, the output is 0 or 1 for exactly one input combination (input vector). Hence the value of    or    is 1 or 2 - 1. Hence 0 = 21  or 2 -1 2 -1 1 , respectively, and the corresponding  = or 1 2 2 2 . 2 -1 Thus, the switching activity is given by 22 . Hence, as the number of inputs to the above mentioned logic gates increases, the switching activity of these gates decreases. It is clear to see that the switching activity for the   gate is maximum and of value 1 4 . The switching activity for  and   gates is independent of the number of inputs to the gate and is equal to 1 4. B. Calculation of Switching Activity for a Logic Expression Without loss of generality, we assume the logic gates to have at most two inputs. Computation of the switching activity for a logic expression is illustrated by an example. Consider a logic expression  (, , , ) =  +  +  + . Consider Figure 1 and the following logic expressions, 1 = , 2 = , 3 = ., 4 =  + , 5 = , 6 =  +  = 3 + 5 , 7 .  = 6 +  +  = 6 + 4 . The switching activity for 5 is 64 This is due to the fact that the output is 1 only for the input vector 101. As such, the number of 1's and 0's in the output column are 1 and 7 respectively. For the implementation of , a   gate is required for 2 = , having  = 1 4, 3 . The an   gate is required for 1 =  with  = 16 outputs of these   gate and   gate are inputs to a 7 second   gate having   = 64 . 6 can be represented in sum of minterms as 0, 1, 2, 3, 4, 5. Hence the switching (since   = 6,   = 2). The total activity for 6 is 12 64 3 7 12 switching activity for 1 , 2 , 5 and 6 is thus 1 4 + 16 + 64 + 64 . Clearly, the switching activities for 3 =  and 4 =  +  3 each. When represented as a sum of minterms, the are 16 function  is given by  = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 and the switching 39 . (since   = 13,   = 3). Thus, the activity is 256 3 3 3 total switching activity of the circuit is 1 4 + 16 + 16 + 16 + 7 12 39 323 64 + 64 + 256 = 256 (Figure 1). Signal probability can also
a b c
3/16

In the computation of the switching activity, it is assumed that the distribution of 0's and 1's at the primary inputs (PIs) is uniform. Definition 1. For a given node of a circuit the probability of transition either from 0 to 1 or from 1 to 0 is known as switching activity of that node. Thus, the switching activity of node  is given by the composite probability  = 0 × 1 =    ×   (   +   )
2

(3)

f1
1/4

As already mentioned, power dissipation in digital circuits can be reduced by minimizing their total switching activity (TSA). It can be easily shown that the switching activity is maximum when the number of 1's (  ) and the number of 0's (  ) in the output column of the truth table are equal and the switching activity is minimum when the difference between the number of zeroes and the number of ones in the output column in the truth table is maximum. A. Calculation of Switching Activities of Logic Gates To calculate the switching activity of a logic circuit, it is important to determine the switching activity for the constituent logic gates. Based on the classical probabilistic definition of the switching activity [11], we can easily calculate the

7/64

f5
3/16

12/64

f2 f3
3/16

f6
39/256

f

d

f4

Fig. 1. Circuit schematic and switching activity for  +  +  + 

be used to estimate the switching activity. Figure 2 shows the propagation of signal probabilities through the gates. Here it is assumed that at the PIs the signal probabilities are equal. Hence in Figure 1,  () =  () =  () =  () = 1 2. The Algorithm, Compute Signal Probabilities [4], [17], can

be used to estimate signal probabilities at the internal nodes of a logic circuit. From Figure 1, using this algorithm and propagation of signal probabilities through gates, we can 3 obtain  1 =  (). () = 1 4 ,  = 16 . Similarly, the switching activities for nodes 2 to 6 and  can be computed.

A B C D E

3/16

7/64 15/256 31/1024 63/4096

A B C D E F

3/16 15/256 3/16 3/16
Total Switching Activity =2607/4096 Delay=3 SA*d=7821/4096

63/4096

F Total Switching Activity=1643/4096 Delay=5 SA*d=8215/4096

P1 P2

P1.P2 1-P1

P1 P2

1-P1.P2 (1-P1 )(1-P2 )

Fig. 4. Comparison of switching activity between ((((A*B)*C)*D)*E)*F and ((A*B)*(C*D))*(E*F) TABLE I S WITCHING ACTIVITY (SA) AND D ELAY ( D ) P RODUCT       ×  0.1875 1 0.1875 0.2461 2 0.4922 0.4336 2 0.8672 0.4639 3 1.3916 0.6365 3 1.9094 0.7382 3 2.2147 0.8711 3 2.6132 0.8884 4 3.5536 1.0596 4 4.2382   0.1875 0.2461 0.3555 0.3857 0.4011 0.4089 0.4128 0.4147 0.4157    ×  1 0.1875 2 0.4922 3 1.0664 4 1.5430 5 2.0056 6 2.4532 7 2.8894 8 3.3177 9 3.7412

P1 P1 P2

P1 P2 1-(1- P1 )(1-P2) =P1 +P2 -P1.P2

Fig. 2. Propagation of probabilities through gates

The controllability/observability concepts as described in [19] can also be used to calculate the switching activity. Unfortunately, both approaches give inaccurate results in presence of reconvergent paths. The approach presented in [20] tries to partially solve this problem. IV. M OTIVATION OF THE WORK In this paper, we attempt to design a generalized method to minimize the switching activity for logic expressions. The proposed method accepts any  /   expression as input and transforms it into a functionally equivalent multilevel expression with minimized switching activity. Figure 3 illustrates a motivating example. Consider the logic expression of a Full Subtractor with  , ,  as input variables and  and   as output variables. Here only the  is of interest. The logic expressions corresponding to  and   are respectively given by ( +  +  ) and (     ). With the discussions in next few sections, we will find that the total switching activity of this logic expression will be 83 64 . But the logic expression for borrow can also be written as (( +  )  ( +  )) +  and the switching activity for this expression will be 70 64 .
bout=xy+xb in +yb in 15/64 3/16 y 1/4 x 3/16 b in 3/16 y Total Switching Activity =3*(3/16)+2*(1/4)+15/64=83/64
bout=xy+xb in +yb in =(x(y+b in ) + (y+b in ))+yb in

  2 3 4 5 6 7 8 9 10

by the corresponding logic expression. Here, we consider a straightforward realization of the expression. Lemma 1. If a product (or sum) term contains more than two literals (..  = -1  -2  -3  -4  . . .  1  0 ), the function can be implemented as ((-1  -2 )  -3 )  . . .)  1  0 ) (chain representation) instead of ((-1  -2 )  (-3  -4 )  . . .  (1  0 )) if  is even or ((-1  -2 )  (-3  -4 )  . . .  (2  1 )  0 ) (tree representation) if  is odd. The switching activity is less in the chain representation [7]. Figure 4 shows the implementation of a 6 variable (A, B, C, D, E, F) product term. The first implementation switching 2607 activity ( 1643 4096 ) is less in comparison to the second one ( 4096 ). A similar type of operation can be done for a sum term as well. Due to delay in different gates there may be power loss due to glitches. The width of the glitch depends on the delay of logic gates and interconnections [4]. It is clear that glitches can occur in a chain structure and in the tree structure the occurrence of glitches is much less. Hence, minimization of the product of the switching activity and the delay may be a good design parameter. Table I shows the product of the switching activity and the delay for AND gate in the Tree structure and the Chain structure. Here, it is assumed that for each gate the delay is 1. Observation 1. if the switching then for   9 chain structure inputs. For AND and OR gates, it can be shown that activity and the delay product is considered, the tree structure is better, and if  > 9 the is better. Here  is the number of primary

x 1/4 y b in 3/16

15/64

15/64 1/4

y b in 3/16 Total switching activity =2*(3/16)+2*(15/64)+1/4=70/64

Fig. 3. Reduction of switching activity of Full Subtractor

A Rule-based systems for logic synthesis through local transformations has been proposed in [18]. In our paper different rules are used to minimize the switching activity. V. D ESIGN FOR M INIMAL S WITCHING ACTIVITY Based on the discussions in Section III-A and Section III-B, it is observed that the switching activity of a circuit realization depends on its implementation, which can be described

For XOR and XNOR gates the tree structure is better since the switching activities for these gates are independent of the number of inputs. Lemma 2. Let an  variables product term contains an odd number of complemented literals, say, 0 to  literals -1 literals are in are in complemented form and +1 to =   =-1 prime form, k is even, i.e., ( =0  ).(  =+1  ) then the following transformations will reduce the switching activity:  =-1 = ( =0  ).(  =+1  ) =-1  =-1 =( =0  ).(  =+1  ).  =-1 =(0 + 1 . . . -2 + -1 ).(  =+1  ).   =-1 (0 + 1 . . . -2 + -1 ).(  =+1  )
7/64 3/16 7/64

TABLE II I NCREMENT / D ECREMENT OF DELAY AND AREA DUE TO EACH RULE  1 2 3 4 5 6 7 8 9 10 11 12 13                               ( = 2)( > 2)        

z y x

3/16 1/4 7/64

x y z

a) Switching Activity for

zy x

b) Switching Activity for

zyx

zy

=1 . . . . .(1 . . . . .+1 . . .  +1 .2 . . .  ) Here ,  and  are in general different and    . Rule 2: Apply the transformation of Lemma 1 and Observation 1. Rule 3: If a product (sum) term contains even number of complemented literals, then replacing each pair of complemented literals with their   (  ) combination by application of De Morgan's theorem will reduce the switching activity of the entire term. Rule 4: Applying Consensus Theorem [5] i.e. ( +  +  ) = ( +  ). Rule 5: If (  2) and  is even then apply 1 + 2 .3 . . . . . = 1 + 1 + 2 + . . .  Rule 6: If (  3) then apply 1 + 2 .3 . . . . .-1 . = 1 + 1 + 2 + . . . -1 . . Rule 7: For two or more than two variables the Boolean expression 1) ( +  )=(( +  ) +  ). 2) 1 +2 + . . . -1 +1 .2 . . . . . = 1 .2 . . . . .-1 +  +  Rule 8: 1) 2) 3)  = ( ) .   = 1 2 . . .  2 3 . . .  . 1 2 3 . . .  =  =2× ( =0  ).(  =1  ).( ) = =( =0  ).(1 + 2 ) . . . (2×-1 + 2× ). = ( =0  ).(1 + 2 ) . . . (2×-1 + 2× )( )

Fig. 5. Reduction of switching activity by introducing XOR gate

Figure 5 shows the reduction of the switching activity if the logic expression  is modified as    . The total switching activities for  and    are respectively 35 64 and 26 64 . Lemma 3. If a sum term in a logic expression contains an odd numbers of complemented terms, then the switching activity can be reduced gate. i.e. 2×by introducing      + +  = + =1   =  =1  ) . . . (   ) +  =  +  =    where (   +1 2 ×  - 1 2 ×        = =1  + ( +1 ) . . . (2×-1 2× ). From Lemma 2 and Lemma 3, it can be clearly seen that the introduction of  or   gate in a sum term or in a product term with an odd number of literals reduces the total switching activity and hence reduces the power dissipation of digital circuits. VI. A LGORITHM F OR M INIMIZATION O F T OTAL S WITCHING ACTIVITY The proposed algorithm for minimization of the switching activity in a circuit is shown in Figure 6. The algorithm takes the truth table of any switching function as input and gives the output as a logic expression of the switching function with minimized switching activity. The following Rules (i.e. transformations) are applied to the switching expression in order to reduce the switching activity. Rule 1: 1) . + . =.( +  ) 2) 1 .1 +2 .1 +1 .2 +2 .2 +. . . +1 . +2 . = (1 + 2 ).(1 + 2 + . . .  ) 3) 1 .2 . . .  .1 .2 . . .  +1 .2 . . .  .1 .2 . . .  =1 .2 . . . . . (1 .2 .3 . . .  +1 .2 . . .  ) 4) 1 .2 . . . . .1 .2 . . . . +1 .2 . . . . .1 .2 . . . .

Rule 9: 1)  +  =  +  . This is known as absorption rule [5]. 2)  +  = . . 3)  +  =  +  = ( ). The last two transformation can be defined as a modified absorption rule. Rule Rule Rule Rule 10: 11: 12: 13:  =    .  +  =    . Apply Lemma 2. Apply lemma 3.

Algorithm Minimize Switching Activity() Input: Truth Table Output: Function for minimal switching activity 1. Obtain the minimal Sum-of-Product ( ) or Product-ofSum (  ) expression using any standard method (such as Kmap, Quine- McCluskey method, Espresso)for the minimization of given switching function  . E-XOR() operations can also be considered to minimize the logic expression. 2. Apply Rule 1 and Rule 2 to reduce the switching activity. 3. If either the  or   does not contain any complemented literal then 4. Calculate the total switching activity of  or   . 5. Take the function with minimum switching activity. 6. Endif 7. If number of complemented literals of a product term in a  is even then 8. Replace a pair of complemented terms by   gate using De Morgan's theorem. (Rule 3). 9. Endif 10. If number of complemented literals of a sum term in a   is even then 11. Replace a pair of complemented terms by    gate using De Morgan's theorem. (Rule 3). 12. Endif 13. If applicable then 14. Apply Rule 4, Rule6, Rule 7, Rule 9 to reduce the switching activity. 15. Endif 16. If a  (  ) contains only one complemented literal  then 17. Apply Rule 8 or Rule 10 (in case of SOP). 18. Apply Rule 5 or Rule 11 (in case of POS). 19. Take the function with minimum switching activity. 20. Endif 21. If the product term in  (  ) contains odd number of complemented literals then 22. Apply rule 12 in case of  . 23. Or Apply Rule 13 in case of   24. Take the function with minimum switching activity. 25. Endif 26. end.
Fig. 6. Algorithm For Minimal Switching Activity

of the circuit may increase. Table II shows the decrease or the increase of area or delay due to each transformation. VII. E XPERIMENTAL R ESULTS The implementation and power estimation of the proposed rule-based algorithm has been done by using Synopsys EDA tool -DESIGN VISION version I-2013.12-SP1, 20, 2014 under CENT OS and TSMC 120 nm library. The experiment is done on some basic circuits and three other benchmarks circuit (alu1, cm138a, z9sym) [21]. The basic circuits and also the 2-adder and the 3-adder circuits are manually constructed. The switching activity of circuits and the associated dynamic power dissipation using conventional SOP (POS) method implemented with two-input gates with the help of chain structure and our proposed method are summarized in Table III. We observe that the total switching activity for our proposed method never exceeds, and is less in most of the cases than the one obtained using the traditional logic optimization. To calculate the area of a circuit it is assumed that the area of inverter is 1. The area of  , ,   ,  , and  gate are assumed to be 3, 3 2, 2 and 7 respectively. The delay of   ,  , ,   ,  , and  gate are assumed to be 1, 3, 3, 2, 2 and 7 respectively. A. Comparison of Our Proposed Method with the Existing Method of [12] In [12] the authors basically modify the Karnaugh maps to reduce the switching activity. Logic optimization using Karnaugh maps is limited to 6 variables switching functions. The reduction of the switching activity and hence the power dissipation of CMOS VLSI circuits by our proposed algorithm is not restricted to 6 variables switching functions and it is applicable for any kind of circuits. Moreover, the method of [12] is not applicable for all types of switching functions. For instance if the logic expression of a two-variable switching function is  , then the method in [12] cannot reduce the switching activity. On the other hand, our proposed method reduces the switching activity. B. Power-Delay Tradeoff Logic optimization using our proposed method surely minimizes the switching activity. But in order to minimize the switching activity by the proposed rules, it may also happen that a NOR gate is used instead of a single NOT gate (e.g. transformation of Rule 7, i.e., ( +  ) = (( +  )+  ) and so on). In this case, the total transistors count of the circuit increases. In our proposed method we do not consider the circuit delay. Logic optimization to minimize the switching activity and the delay as a joint objective will be the future direction of our work. VIII. C ONCLUSION In this paper we propose a rule-based approach to reduce the switching activity of combinational logic circuits. This would reduce the dynamic power and hence the total power

A. Effectiveness of the Proposed Algorithm The total switching activity of a logic expression calculated by the proposed algorithm is less than or equal to the switching activity of the equivalent logic expression by any standard logic method. In our proposed algorithm, first, any standard logic optimization method is used to find the minimized logic expression. Then a part of or the whole logic expression is replaced with an equivalent logic expression for which the switching activity is minimized. The transformations as given by Rule 1 to Rule 13 decrease the switching activity by i) reducing the number of NOT gates ii) increasing the number of inputs to AND or OR gates, iii) introducing XOR or XNOR gate (Lemma 2, Lemma 3) iv) a combination of these. For some specific logic expressions, it is not possible to replace the logic expression with an equivalent logic expression with less switching activity. Hence the total switching activity of a logic expression calculated by the proposed algorithm is less than or equal to the switching activity of the same logic expression calculated by any standard method. In order to reduce the switching activity, some times, the area or the delay

TABLE III C OMPARISON OF S WITCHING ACTIVITY AND DYNAMIC POWER OF DIFFERENT COMBINATIONAL CIRCUITS
TSA Conv 2.75 1.6875 0.4375 1.5469 0.6875 1.8125 5.9609 3.5392 4.2218 6.2285 10.034 8.0374 19.1958 our 1.8125 1.4375 0.4375 1.3594 0.625 1.4375 4.7891 2.9064 2.9710 3.7479 7.3476 5.3484 15.8665 Dynamic Power (micro watt) Conv Our 15.7121 14.3159 7.3628 5.2588 5.1288 5.1288 17.8317 11.1890 6.5248 5.8172 15.3178 14.7618 18.3713 15.4985 14.0391 9.0759 17.8394 14.7104 32.5028 28.608 21.5760 18.8346 7.1527 5.97 61.3653 58.35 Area Conv Our 39 50 35 42 10 10 29 26 11 12 30 33 82 94 52 50 59 51 91 89 113 96 92 77 231 210 Delay Conv Our 7 9 13 16 7 7 12 12 7 7 12 13 13 16 19 14 24 25 36 39 14 13 14 12 50 57

Circuits 3: 8 decoder 4:1 MUX Half Adder Full Adder Half Subtractor Full Subtractor 2 Bit Comparator Priority Encoder 2-Adder 3-Adder ALU1 cm18a z9sym

% Reduction of TSA 34.09 14.81 0 12.12 9.09 20.69 19.66 17.88 29.63 39.83 26.77 33.46 17.34

dissipation, enabling the design of power-efficient circuits with several useful applications. Experimental results show significant reduction of the switching activity. In this paper, together with all the limitations, SOP representations are used and switching possibility of input signal are assumed to be 50% . The method can be improved by making the computation scalable and precise. Introduction of signal probability to compute the switching activity for logic circuits can be used for this computation. The method presented in this paper can be further improved by considering both delay and power as objective functions and implementation and testing of the improved algorithm with standard benchmark circuits. ACKNOWLEDGMENT This research has been partially supported by a grant GA16-05179S of the Czech Grant Agency, "Fault-Tolerant and Attack-Resistant Architectures Based on Programmable Devices: Research of Interplay and Common Features" (20162018). R EFERENCES
[1] S. Chattopadhyay and N. Choudhary, "Genetic Algorithm based Approach for Low Power Combinational Circuit Testing",Procedings of the 16th International conference on VLSI Design (VLSI'03). [2] P.Girard, C.Landraut, S.Pravossoudovitch and D.Severac, "Reducing Power Consumption During Test Application by Test Vector Ordering", Int'l Symposium on Circuits and Systems (ISCAS 98), pp 296-299. [3] P. Ghosal, T. Samanta, H. Rahaman and P. Dasgupta, "Thermal-aware placement of standard cells and gate arrays: Studies and observations", IEEE Computer Society Annual Symposium on VLSI, April 2008. ISVLSI'08., pp 369-374. [4] K. Roy and S.C.Prasad, "Low Power CMOS VLSI Circuit Design", Wiley India Edition, Reprint 2011. [5] Z. Kohavi and N. K. Jha, "Switching and Finite Automata Theory", 3 Edition, Cambridge University Press, 2010. [6] N. Wehn and M. Munch, "Minimising power consumption in digital circuits and systems: An overview", Kleinheubacher Berichte, Band 43, pages pp.308-319, September, 1999, Kleinheubach, Germany, Invited Talk. [7] A. P. Chandrakasan and R. W. Brodersen, "Minimizing Power Consumption in Digital CMOS Circuits", Proceedings of the IEEE, Vol 83, No. 4, April 1995.

[8] K. Roy and S. C. Prasad, "Circuit Activity Based Logic Synthesis for Low Power Reliable Operations", IEEE Transactions on VLSI, Vol-1, No. 4, December 1993, pp 503-512. [9] S. Ahuja, A. Lakshminarayana and S. K. Shukla, "Low Power Design with High-Level Power Estimation and Power-Aware Synthesis", Springer Pub., 2011. [10] I. Brzozowski and A. Kos, "Minimization of Power Consumption in Digital Integrated Circuits of Reduction of Switching Activity", 25 Euromicro Conference, 1999, pp 376-380 Vol. 1. [11] R. V. Menon, S. Chennupati, N. K. Samala, D. Radhakrishnan and B. Izadi, "Power Optimized Combinational Logic Design", Proceeding of the International Conference on Embeded Systems and Applications, June 2003, pp. 223-227. [12] R. V. Menon, S. Chennupati, N. K. Samala, D. Radhakrishnan and B. Izadi, "Switching Activity Minimization in combinational Logic Design", Proceeding of the International Conference on Embeded System and Application, 2004, pp 47-53. [13] J. Monteiro, S. Devadas, A. Ghosh, K. Keutzer, and J. White, "Estimation of average Switching Activity in Combinational Logic Circuits Using Symbolic Simulation", IEEE Transaction on Computer Aided Design of Integrated Circuits and Systems, Vol. 16, NO. 1, 1997, pp 121-127. [14] M. Ghasemazar and M. Pedram, "Variation Aware Dynamic Power Management for Chip Multiprocessor Architectures", Design, Automation & Test in Europe Conference & Exhibition (DATE), 2011. [15] P. Dasgupta, P. Dasgupta, D. K. Das, "A Novel Algorithm for Interconnect-aware Two level Optimization of Multi-output SOP functions", Proceeding of the 11 International Workshop on Boolean Problems, Freiberg, September 2014, pp 219-226. [16] M. Alidina, J. Monterio, S. Davadas, A. Ghosh and M. Papaeftymiou, "Precomputation-Based Sequential Logic Optiomization for Low Power", IEEE International Conference on Computer Aided design, 1994, pp 74-81. [17] P. Parker and E. J. McCluskey, "Probabilistic treatment of General Combinatorial Networks", IEEE trans. Computers, Vol c-24, pp 668670, 1975 [18] J. A. Darringer, W. H. Joyner, Jr. C. Leonard Berman and L. Trevillyan, "Logic Synthesis Through Local Transformations", IBM J. RES. DEVELOP. Vol. 25 NO. 4 July 1981. [19] L. H. Goldstein, "Controllability / Observability Analysis of Digital Circuits", IEEE Transactions on Circuit and Systems, Vol. CAS-26, No. 9, 1979, pp 685-693. [20] S. C. Seth, V. D. Agrawal, "A new model for Computation of Probabilistic testability in combinational circuits", Integration, The VLSI Journal 7(1989) pp 49-75, 1989. [21] S. Yang, "Logic Synthesis and Optimization Benchmarks User Guide," Technical Report 1991 IWLS-UG-Saeyang, MCNC, Research Triangle Park, NC, January 1991, p. 45.

With extreme miniaturization of traditional CMOS devices in deep sub-micron design levels, the delay of a circuit, as well as power dissipation and area are dominated by interconnections between logic blocks. In an attempt to search for alternative materials, Graphene nanoribbons (GNRs) have been found to be potential for both transistors and interconnects due to its outstanding electrical and thermal properties. GNRs provide better options as materials used for global routing trees in VLSI circuits. However, certain special characteristics of GNRs prohibit direct application of existing VLSI routing tree construction methods for the GNR-based interconnection trees. In this paper, we address this issue possibly for the first time, and propose a heuristic method for construction of GNR-based minimum-delay Steiner trees based on linear-cum-bending hybrid delay model. Experimental results demonstrate the effectiveness of our proposed methods. We propose a novel technique for analyzing the relative accuracy of the delay estimates using rank correlation and statistical significance test. We also compute the delays for the trees generated by hybrid delay heuristic using Elmore delay approximation and use them for determining the relative accuracy of the hybrid delay estimate.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/303563207

Intelligentandadaptivetemperaturecontrolfor large-scalebuildingsandhomes
ConferencePaper·April2016
DOI:10.1109/ICNSC.2016.7478971

CITATIONS

READS

0
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

14

AllcontentfollowingthispagewasuploadedbyYuanWangon28August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Proceedings of 2016 IEEE 13th International Conference on Networking, Sensing, and Control Mexico City, Mexico, April 28-30, 2016

Intelligent and Adaptive Temperature Control for Large-Scale Buildings and Homes
Yuan Wang
Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu

Partha Dasgupta
Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract--Temperature control in smart buildings and homes can be automated by having computer controlled air-conditioning systems along with temperature sensors that are distributed in the controlled area. However, programming actuators in largescale buildings and homes can be time consuming and expensive. We present an approach that algorithmically sets up the control system that can generate optimal actuator settings for large-scale environments. This paper clearly describes how the temperature control problem is modeled using convex quadratic programming. The impact of every air conditioner(AC) on each sensor at a particular time is learnt using linear regression model. The resulting system controls air-conditioning equipments to ensure the maintenance of user comforts and low cost of energy consumptions. Our method works as generic control algorithms and are not preprogrammed for a particular place. The system can be deployed in large scale environments. It can accept multiple target setpoints at a time, which improves the flexibility and efficiency for temperature control. The feasibility, adaptivity and scalability features of the system have been validated through various actual and simulated experiments.

I. I NTRODUCTION About 70% of the electricity load is consumed by commercial and residential buildings in the US. Studies show by the year 2025, building electricity energy costs will be over 430 billion dollars. With the rapid increase of energy costs in the building sector, carbon dioxide(CO2 ) emissions are growing faster than before. The sector contributes around 39% CO2 emissions in the US per year, more than any other sector such as transportation, which results in threat of climate change. Heating, ventilation and air conditioning(HVAC) system is the largest energy consuming sector in buildings and homes. In the year 2009, about 48% energy costs are contributed by space heating and cooling. Most of traditional HVAC systems in buildings and homes are controlled by thermostats that are manually configured either by centralized control technicians or individual users, which requires a lot of human efforts. For maintaining user comfort levels in work places, these systems usually keep AC settings all the day, even during off hours when the places are not occupied, which causes a lot of unnecessary energy consumptions. Some of thermostats in office areas do not provide friendly interfaces and flexible functionalities for users to control. In a centralized air-conditioning control area, users can not customize the

temperature settings for a particular place, which causes a lot of inconvenience. Saving energy and satisfying user comforts are two main aims for HVAC control systems design. For saving energy, amount of total energy costs should be minimized during the control. For maintaining user comforts, inside room temperature(sensed by temperature motes) should be uniform and stay in a satisfied range. There always exist tradeoffs between these two goals. One of the popular products in current market for smart thermostat technology is the Nest Learning Thermostat [1]. It learns user preferences such as temperature levels during a day for about a week and then it can automatically generate AC control plans based on user input data. The drawback of this approach is it relies too much on user selected data. If users randomly provide some incorrect data to the thermostat or they disable it for a particular time, the effectiveness of the thermostat may drop down. [2] proposed an approach called smart thermostat that can automatically turn on/off air conditioning systems by sensing occupancy and sleep patterns. It can also generate an energy efficient plan for the preheating stage by looking at system configurations and analyzing historical occupancy patterns. One limitation of this work is it relies on a lot of information from the equipment itself and this paper only evaluates a single type. The scalability and adaptivity of the system need to be improved. Although there are a lot of great improvements for smart building technologies nowadays, current control plans for HVAC systems still have some drawbacks and limitations. First, in most of the commercial buildings, one thermostat is used for controlling multiple vents at same time. People in different working areas can not customize their preferences when they have conflicts on temperature settings. Second, current HVAC control approaches are not good for large scale environments where there are many standalone ACs in an area. Each AC is independently controlled by a single switch. A subset of ACs need to be turned on according to users' preference settings. The control plan should optimize user comfort levels without sacrificing too much energy. Last, in the current HVAC control system, temperature sensors are usually built in thermostats that are put on the wall. They should be placed at sitting areas in order to better capture surrounding temperature for users. In this paper, we design an air-conditioning control system

978-1-4673-9975-3/16/$31.00 c 2016 IEEE

for tackling the above challenges. Our system can be used for controlling large commercial and residential buildings where there exist multiple ACs, control switches and temperature sensors. The goal of the system is to optimize the user comfort levels and minimize energy consumptions. There are two main stages in this system: predictive stage and adjustive stage. In the predictive stage, mathematical models are used to formalize the problem and optimization algorithms are used to get a predicted solution. In the adjustive stage, feedback system are designed to reduce potential errors. Our system is good for providing fine-grained settings during precooling/preheating stage. Our approach is adaptive, which means it can be easily applied in any type of building environments. II. R ELATED W ORK Thermostats technologies have been widely used in HVAC systems for automatically controlling HVAC equipments in buildings and homes. The basic logic behind is HVAC equipments are turned on when its controlled area is occupied and turned off when occupants leave the area. The thermostats are preprogrammed and temperature setpoints are predefined according to the local environments such as static occupancy patterns. It relies on too much static information therefore it is hard to adapt to environmental changes. An alternative method called reactive thermostat is proposed for tackling the problem. It uses various sensors such as motion sensors or door sensors to detect user activities in real time so that the control system can adjust system settings when pattern changes. However, some studies found that this method didn't improve the efficiency as people expected since there usually exists long delay for the system reaction due to the hardware limitations which even save less energy than the previous programmable thermostat approach. [2] proposed an approach called smart thermostat that can automatically turn on/off air conditioning systems by sensing occupancy and sleep patterns in real time. It can also generate an energy efficient plan for the preheating stage by looking at system configurations and analyzing historical occupancy patterns. This approach dynamically controls HVAC systems based on occupancy status in a place, which can effectively adapt to environmental changes. One limitation of this work is it relies on a lot of information from the equipment itself and this paper only evaluates a single type. The scalability and adaptivity part of the system need to be improved. Nest Learning Thermostat [1] is a popular user-centric tool for HVAC system control, which is very easy to use compare to the traditional programmable thermostat. It automatically learns a user's preferences and behaviors based on some precollected data from the user. The drawback of this approach is it relies too much information on the user-input data. If users randomly provide some incorrect data to the thermostat or they disable it for a particular time, the effectiveness of the thermostat may drop down. Another drawback of this approach is it is usually set up in a single AC environment such as residential homes where only one target temperature value at a time is accepted. When two people have conflicts

on target temperature values in a place or the thermostat is set up in a large scale environment where multiple ACs exist, the thermostat can not give effective solutions. A fuzzy inference system for adaptively doing heating control is proposed in [3]. The main idea is it takes power profile of the previous day, adjusts the profile based on current conditions and then applies the latest profile to the current day. It uses Artificial Neural Network model to predict the future comfort levels and a fuzzy rule is designed for the setting adjustment step. The drawback of the system is it mainly focuses on maintaining user comforts during the control. The energy saving part is only considered when a place is not occupied. The fuzzy rule should be improved so that some fine-grained settings can be provided instead of using the words "large", "medium" and "small". Our approach is proposed for tackling the above problems. It can be deployed in a large scale environment without any static or customized configurations. If needed, users can set their own preferences on temperature setpoints in a workplace. The system can efficiently and effectively control HVAC equipments even when there exist multiple target values on sensors. It provides fine-grained pre-cooling plans based on occupancy schedules. When environment changes, the system can automatically update calibration data and adjust the settings accordingly. WSN technologies have been applied into various areas such as [4] [5][6]. It consists of portable wireless sensor motes such as Crossbow's TelosB or MICAz to monitor the values of physical conditions, such as temperature, light, humidity, and so on. WSN data collections use several specific protocols such as Collection Tree Protocol [7]. III. F ORMAL M ODEL This section describes how we use mathematical model to formulate the problem. For simplicity, we build the model for cooling strategy of air-conditioning control. Heating strategy can be modeled in a similar way. In this model we have a set of on/off switches that control arbitrary n ACs (one per switch) and a set of m temperature sensors. The sensors are connected to the control system via a wireless sensor network and the switches are activated via actuators connected to the system. Physical locations and correlations between them are initial unknown to the control system. The ultimate task is to compute the positions of switches over time, i.e. durations for each switch to be kept on. We assume an AC will keep running until its control switch is turned off. Let t = (t1 , ..., tn ) denote the assignment for AC switches where ti  0 and it denotes switch i is kept on for ti time units. The goal is to optimize the energy E (t) and the comfort C (t). E (t) is the total energy consumptions in the control period. We assume the power of each AC is constant during the control period. Let w = (w1 , ..., wn ) denote the electric power for each AC. Then E (t) = w × t. For C (t), it is satisfied when all sensor values approach their target values

and stay in a satisfied range. Let e = e1 , ..., em denote all sensor readings. Each sensor has a target value to be reached, for example, sensor j 's target value is tari . Let tar = tar1 , ..., tarm denote all target values. They can be set either by users or default standard values. Hence the problem can be stated as: minimize
t

T =kt+b where t stands for interval time and T stands for temperature level. In order to learn k and b, some experimental data points should be pre-collected. These coefficients should be updated when environment changes such as building patterns or weather conditions. There are multiple ACs and multiple sensors in the system. For each sensor, every AC has an impact factor on it, i.e. a pair of (k, b). Since sensor readings are additive, to a particular sensor j , its reading ej after an operation of n ACs on a vector t time is: ej = sj - mj  t where sj stands for the initial sensor j 's value and     t1 k1j      t2   k2j   t=. mj =  .  .  .  .  . tn knj sj can be collected from the calibration step and mj can be calculated based on linear regression model. B. Calibrating and Calculating Impact Factor In the last section, we discussed how we use a linear regression model to learn the relationship between time and temperature. Note that this model is effective when time vector is small. Assume all ACs are off at the beginning. Calibration steps involve the following: · Turn on one AC at a time for a period of time t, record the temperature changes on every sensor. Selection of t should be reasonable, so that a range from initial temperature to the target temperature can be covered for each sensor, if applicable. · Analyze the collected data using linear regression model. Perform linear regression analysis for different period of time, if necessary. Thus, a impact factor kij might have different values for different periods of time. · Repeat step 1 n times until all ACs are counted. For each sensor j , a vector mj is created. If more than one kij exist, then for each period of time, a mj is recorded. In each AC operation cycle, either the predictive stage or the adjustive stage, only one mj can be used for sensor j . The selection of mj is decided by the initial sensor value on j . Each AC is assigned an upperbound value for its operational time during every cycle, formulated as 0  ti  tmax . Value of tmax is associated with value of kij . For each AC i, from time 0 to time tmax , its impact factor on every sensor j kij should remain the same. Ideally, impact factors and room initial temperatures should be updated frequently to ensure the accuracy of the computation. However, in some cases, these values remain the same in a period of time. Thus in order to increase the efficiency, these values don't have to be re-calibrated or updated if the (3)

w × t, e - tar

2 2

subject to min  ej  max, j = 1, ..., m 0  ti  tmax , i = 1, ..., n

(1)

By applying the -constraint method [8] designed for solving multi-objective optimization problems, the new objective function can be defined as: minimize
t

e - tar w ×t

2 2

subject to min  ej  max, j = 1, ..., m 0  ti  tmax , i = 1, ..., n

(2)

We assume that the impact of ACs on sensors(number of sensor values decrease) over a short time period is additive, i.e., the impact of two ACs over a short time period on one sensor(total number of sensor values decrease during the period) is the sum of the individual impact for each AC. Since the time period is considered to be small for solving the above equation, the relationship between time and temperature can be learnt using linear regression model, which will be discussed in the next section. IV. C ONTROL A PPROACHES There are two main stages for solving the air-conditioning problem. The first stage is called predictive stage that happens before the room is occupied, which is used for predicting an approximate setting for air-conditioning control. The second stage is called adjustive stage that happens after room is occupied. It is used for eliminating the potential offsets from the predictive stage and providing fine-grained air-conditioning control for maintaining user comfort levels. In order to achieve max electricity savings, we decide to turn off all ACs when the controlled area is not occupied, and pre-cool the area in advance before users enter the space. By default we assume we have a list of occupancy schedules. The schedule can be updated by users in real time, for example, a user can send a notification at any time telling the system when he expects to get back. A. Predicting AC Impacts on Sensors Over time In the cooling stage, an AC continuously provides cooling air to reduce the inside temperature levels until it is turned off. Since the relationship between time and temperature in each cycle is investigated over a short time period, up to an upperbound tmax , a linear regression model can be identified with a high coefficient of determination [9]. It is defined in the form of:

change doesn't exceed a threshold. The threshold value can be set based on local environments. C. Solving the Equations to Compute Approximate AC Settings The result of equation 2 is a time vector t that indicates the working time interval for every AC in the room. The upperbound value tmax for each ti in vector t can be different. The largest value among all values of tmax is assigned to q . If users return at time p, then the system will start calculating t from time p - q in order to cool the room at time p. After time p, the system will go to the adjustive stage until the room is unoccupied again. Applying equation 3 to equation 2, we get:
m

The above transformation satisfies the format of quadratic
m

programming model [10] where H = 2 
j =1 m

mj  mj ,

f = -2 
j =1

xj  mj , lb is a vector of zeros, ub is a vector

minimize
t j =1

(mj  t - xj )2

subject to sj - max  mj  t  sj - min, j = 1, ..., m w ×t 0  ti  tmax , i = 1, ..., n (4) where xj = sj - tarj . The objective function of equation 4 is a quadratic function that can be re-written in the following form:
m m m

of all tmax values. Number of elements in lb and ub is both n. Thus our problem belongs to quadratic programming problem, which can be solved by some existing algorithms such as interior point method. In addition, the objective function appearing in equation 4 is a standard form of least-squares [11] that is convex, therefore our problem is a convex quadratic programming problem. The result set will be a set of global minimum. The weight factor (which appeared in equation 4) is used to balance the tradeoff between saving energy and maintaining user comforts. The assignment of will significantly affect the final result, therefore, it should stay in a reasonable range. In order to rationally set , we first study the approximate minimum value of , i.e.,  min , by solving the following equation: minimize
t

w t (6) 0  ti  tmax , i = 1, ..., n

subject to A  t  b

t (
j =1

mj  mj )  t - 2  (
j =1

xj  mj )  t +
j =1

x2 j (5)

The constraints of equation 4 can where  k11 k21  k k22  12  . .  . . .  .  k2m  k1m  Am,n =  -k11 -k21   -k12 -k22  . .  . .  . .  -k1m -k2m w1 w2 and

be re-written as A  t  b 

··· ··· .. .

      · · · knm   · · · -kn1   · · · -kn2   . ..  . . .   · · · -knm  ··· wn

kn1 kn2 . . .

where A and b can be obtained from A and b by removing their last rows(w and ) respectively. This equation is simpler than the previous one since we remove an optimized variable(approaching all sensor values to the targets) from the original equation. In other words, compared to the original problem, in this equation, we focus on minimizing the energy usage only, with all sensor readings requiring to stay within an accepted range. This equation satisfies the form of linear programming model, which can be solved by existing algorithms such as interior point or simplex algorithm. The value of can be set as min + a reasonable threshold that is assigned according to the real environments. D. Adjusting AC Settings for Maintaining User Comforts and Adapting Environmental Changes There might exist some errors in the predictive stage. In order to eliminate them, we add an adjustive stage in the system that can adjust the settings accordingly so that the user comfort levels can be maintained. Compared to the predictive stage, the adjustive stage has some similarities and some differences. The similarity in both stages rely on the proposed computational model to generate AC settings. Differences are: · Unlike the predictive stage, the adjustive stage can be done repeatedly until the controlled area is unoccupied. In other words, predictive stage is used for precooling the area before the room is occupied. Adjustive stage needs to be activated whenever users are in the area and comfort levels are not satisfied. · During the first adjustive cycle, impact factor mj is selected based on initial value on sensor j , as predictive

 s1 - min  s - min   2    .   . .      sm - min    b =  max - s1     max - s2    .   .   .   max - sm 



cycle does. Afterwards, in every adjustive cycle, the vector mj should be checked using the partial real data points collected from sensors. If there is any change, the vector should be updated accordingly. This update reflects self-calibrating and self-learning features of the system, which increases the accuracy of the computation. When environment changes such as location changes or season changes, impact factor mj need to be re-updated. Data for learning the factor need to be re-collected from the environments, but the general approaches remain the same. For the AC control at a particular day when calibration data for previous (similar) days have been recorded, impact vectors should be first chosen from similar categories and updated accordingly in the real time, similar to the update step in the adjustive stage. For example, we wish to predict the AC settings for today at 5pm. We first use yesterday's mj in the category of 5pm. After getting the results from the predictive stage, we turn on the ACs based on the predictive settings. If in the first 2 minutes, we detect the impact factor varies a lot compared with yesterday's one, we will update the mj accordingly based on today's data and recompute the settings. Thus a new dataset of mj for today's 5pm is obtained. It will be inserted into database for future computations. If allowed, the interval time for recalibration can be set longer for better accuracy. V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS A. Verification of Linear Regression Model We performed two calibration experiments in the test room with one AC(540 watt) and one sensor in a same day at 3pm and 4pm respectively. For each experiment, the AC is operated continuously for about 40 minutes and temperature changes are recorded. The calibration follows the steps described in section IV-B. Figure 1 shows the AC impacts on the sensor at different time points, and their corresponding fitted lines generated by MATLAB. From the figures, following things are observed:
·

B. Simulation Experiments and Results In section IV, the computational model is proved to be convex, which means the result set is guaranteed be global minimum. In this section, we use MATLAB to simulate a multi-AC environment and compute the optimal solution. The optimization toolbox is called Quadratic Programming. Simulated set-up is given based on some real experimental data, described as follows: Assume there are 3 ACs and 2 sensors in a room. Sensors are placed at user-sitting areas to capture inside temperature levels. Initial values(s1 and s2 ) are both 30. Target values tar1 and tar2 are both 26. Therefore, x1 and x2 are both 4. min is 25 and max is 28. w = (0.45, 0.46, 0.47) , m1 = (0.05, 0.06, 0.07) , m2 = (0.05, 0.06, 0.07) . Based on these settings, we can get H, f , Am,n , b defined in section IV-C. These factors are needed in the Quadratic Programming toolbox. In equation 4, weight factor affects the final result. It represents the tradeoff between energy savings and user satisfactions. In this simulation experiment, we set different values to and hope to learn the effect of on the final result set. In order to better assign the values, the approximate lowest is learnt by solving the equation 6 using interior-point method. For the particular example stated above, lowest is 14. From the graph, we can see that when is becoming larger, minimum value of the objective function is becoming smaller(sensor values are more converged to the targets) while the energy consumption is becoming larger. This matches theoretical analysis as well: when is larger(more flexibilities on energy consumption), user comforts can be better optimized(more candidates are counted), therefore sensor readings are more converged to the target values. We also scale our simulations and do simulated experiments on a larger number of ACs and sensors. We set n = 100 and m = 100. Min, target and max values of sensors do not change. Assume all ACs have same operational powers(0.46kw). Impact vectors are randomly given on a 0.005base increase from 0.05, for example, if k11 = 0.05, then k21 = 0.055 and so on. To a particular AC i, kip = kiq . We run the above set-up in MATLAB. It quickly gives us the result of M inV alue = 0(global minimum) and Energy = 10.72. It verifies the scalability of our approach. VI. C ONCLUSION AND F UTURE W ORK To enable automated temperature control in a multi-AC environment, under varying conditions of occupancy, weather, seasons and other influences it is essential to have a robust air-conditioning control system that is effective and adaptive. Such system must be deployable in a simple, cost effective way without the need for customizations and reprogramming as conditions change. It also needs to create a comfortable environments with energy savings as a goal. This paper presents such a complete core system that can pre-cool the room before users come back and maintain the comfort levels with a relative small energy cost. It also clearly shows how

· ·

Both two figure can be divided into 2 parts. The first part covers the temperature points from initial point to 24 C. The second part covers the temperature points from 23.5 C to 19 C. Both parts are fitted by linear regression models with different impact factors. The regressed line for the first part has a larger scope than the second one's. Figure 1(b) has similar slopes compared with Figure 1(a), for both lines. Two figures have similar initial sensor values. Temperature's range is approximately from 19 C to 33 C.

From the above findings, we verify the accuracy of linear regression model for learning AC's impacts on sensors during selected time intervals. The experiment also shows the calibration data can be reused for a period of time to increase efficiency. In this experiment, during the 40-minute interval, the AC has two impact factors on sensors. Every AC operation cycle should select appropriate impact value for computation.

34 32 30
Collected Data y=-0.0286t+32.6453 Collected Data y=-0.0023t+23.8684

34 32 30
Collected Data y=-0.0276t+32.5954 Collected Data y=-0.0023t+23.8801

Temperature (celsius)

28 26 24 22 20 18 0 500 1000 1500 2000 2500

Temperature (celsius)

28 26 24 22 20 18 0 500 1000 1500 2000 2500

Time (seconds)

Time (seconds)

(a) 3pm Fig. 1. AC Impacts on Sensors over Time

(b) 4pm

's Impact on Energy
28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 8 7 6

's Impact on MinValue

Energy Cost (kw s)

MinValue

5 4 3 2 1 0 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Value of 

Value of 

(a) Energy Fig. 2. Impact of on Energy and MinValue

(b) MinValue

the control problem can be written in convex quadratic programming form, which is guaranteed to get global minimum values. Impact factors of AC on sensors are learnt using linear regression model. Results have been validated in both experimental and simulated scenarios, which shows the feasibility and effectiveness of our system. Our current air-conditioning control system only considers temperature value as the main target for maintaining user comfort levels. Other factors such as humidity, air flow, etc might affect the user comforts as well. These factors should be added into the system for computation in future. Appropriate models need to be selected for solving the new problem. In this paper, we assume we have a list of occupancy schedules, which is used for the pre-cooling stage. This information needs to be further studied in future to increase the accuracy of our system control. With the use of the system, more data will be collected in the database. These data can be used to better predict the impact factors. The results will be applied into the current system control plan. When dataset is made larger, the system will become more robust and precise. R EFERENCES
[1] Nest thermostat. [Online]. Available: https://nest.com/thermostat/lifewith-nest-thermostat/

[2] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic, E. Field, and K. Whitehouse, "The smart thermostat: using occupancy sensors to save energy in homes," in Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems. ACM, 2010, pp. 211­224. [3] A. Guillemin and N. Morel, "An innovative lighting controller integrated in a self-adaptive building control system," Energy and Buildings, vol. 33, no. 5, pp. 477­487, 2001. [4] Y. Wang and P. Dasgupta, "Designing an adaptive lighting control system for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2015 IEEE 12th International Conference on. IEEE, 2015, pp. 450­455. [5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of the 1st ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88­97. [6] Y. Wang and P. Dasgupta, "Designing adaptive lighting control algorithms for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE, 2014, pp. 279­284. [7] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1­14. [8] G. Mavrotas, "Effective implementation of the -constraint method in multi-objective mathematical programming problems," Applied Mathematics and Computation, vol. 213, no. 2, pp. 455­465, 2009. [9] Wikipedia. Coefficient of determination. [Online]. Available: http://en.wikipedia.org/wiki/Coefficient of determination [10] MathWorks. Quadratic programming. [Online]. Available: http://www.mathworks.com/help/optim/ug/quadprog.html [11] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004.

Automotive software applications implement a variety of control algorithms, with many of them being safety-critical in nature. A typical design flow starts with modeling these control algorithms using tools like MATLAB/Simulink. However, at this stage, a number of assumptions, like negligible sensor-to-actuator delay and instantaneous computation of the controller software, are often made. In particular, the details of the software implementation and the computing platform, both eventually defining the timing properties of the applications, are not accounted for. Such idealistic assumptions can cause a significant deviation of the control performance compared to what was proven at the modeling stage. This is usually addressed with multiple design iterations, which are costly and may lead to over-provisioned and thus poorly designed systems. In this paper we attempt to address this problem by proposing a design-and tool flow that integrates software-and platform-level timing information into the high-level modeling stage. We outline our proposed flow using concrete, industry-strength design tools.Digital microfluidic biochip (DMFB), a latest invention in lab-on-a-chip devices integrates electronics with biology for development of customized miniature devices intended for new application areas namely clinical diagnostics and detection, DNA analysis, point-of-care applications and so on. All-Terrain droplet actuation (ATDA) based biochips have (has) recently been emerged as the new variant in DMFB devices for application in 3D domain. In this work, we explored the possibility of design optimization followed by development of design automation techniques specifically applicable in 3D biochips. We consider the advantages and issues involved in development and application of 3D based design. Accordingly we demonstrated few prescheduled bioassay execution in both 2D and 3D domain and the following enhancement in the route performance.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/280003515

DesigninganAdaptiveLightingControlSystem forSmartBuildingsandHomes
ConferencePaper·April2015
DOI:10.1109/ICNSC.2015.7116079

CITATIONS

READS

2
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

97

AllcontentfollowingthispagewasuploadedbyYuanWangon12July2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Designing an Adaptive Lighting Control System for Smart Buildings and Homes
Yuan Wang
Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu

Partha Dasgupta
Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract--Lighting control in smart buildings and homes can be automated by having computer controlled lights and blinds along with illumination sensors that are distributed in the building. However, programming a large building light switches and blind settings can be time consuming and expensive. We present an approach that algorithmically sets up the control system that can automate any building without custom programming. This is achieved by making the system self calibrating and self learning. This paper described how the problem is NP hard but can be resolved by heuristics. The resulting system controls blinds to ensure even lighting and also adds artificial illumination to ensure light coverage remains adequate at all times of the day, adjusting for weather and seasons. In the absence of daylight, the system resorts to artificial lighting. Our method works as generic control algorithms and are not preprogrammed for a particular place. The feasibility, adaptivity and scalability features of the system have been validated through various actual and simulated experiments.

I. I NTRODUCTION Work environments (and homes) benefit from having even and adequate lighting in spaces that are occupied. Lighting control in large buildings can be challenging to automate, specially as blinds and lights have to be custom programmed for building architecture, geography, weather conditions, seasons and so on. This paper presents an approach to self learning, adaptive lighting control that is not preprogrammed or having a-priori information about the building. Further, such systems can save energy by reducing the use of artificial lighting during daytime hours and unoccupied spaces. Integrating daylight and artificial lighting in automated system can be challenging. Natural lighting is not stable, even at a fixed location; sunlight's impact varies during times of a day, weather changes, seasons and so on. Our system harvests daylight and then fills in the deficiencies using artificial lights, with attention to provision of even lighting and avoiding light that is too bright (or has glare). A complete lighting control system contains two interacting modules: daylight control module and artificial lighting control module. The aims of these two modules are different. Daylight control module is mainly used for reducing energy costs while artificial lighting control module is good for providing a more comfortable working environment. Automating and balancing the lighting control system such that uniform and stable lighting is maintained at occupied locations while energy

consumptions are kept as low as possible turns out to be a hard problem. In this paper, a Wireless Sensor Network(WSN)-based lighting control system is introduced. We use static lights that can be turned on/off by the system and venetian blinds on windows, whose angles can be set by the system. The control system is not custom programmed for the environment, i.e. it does not know which light switch controls which light, which blind setting affects which window or even the physical location of rooms and walls. Thus the system is self calibrating, and adaptive to changes in outside lighting, weather, seasons and so on. The formal model of the problem leads to non-linear integer programming and proves to be NP-Hard. We use a heuristic lighting control algorithm and show that it solves the problem well and efficiently (and is competitive with the optimal solution). The rest of this paper is organized as follows. Section II introduces the related work of the problem. Section III formalizes the lighting control problem using mathematical model. Section IV presents the heuristic algorithms for lighting control. Experimental work and simulation results are discussed in section V. We conclude the paper in section VI and talk about some future improvements of the system as well. II. R ELATED W ORK To make use of natural sunlight is called daylight harvesting. Electrical blinds are set up at each window and are controlled by feedback systems that adjust blind angles based on daylight levels. It is used in some of the lighting control systems such as [1], [2], etc. for energy savings. Studies show that daylight harvesting can save lighting energy up to 77% [3]. The idea behind is to make use of the sunlight if applicable when light level is not sufficient [1]. The above method can adapt to environmental changes but it usually needs a long adjustment cycle until the blind settings are finally set up, which results in users not liking too frequent blind movements or hunting. [4] proposed a technique called SunCast that can better predict sunlight values by using historical data and approximate simulation results. The drawback of this system is that every time ambient environment or building patterns change, the system needs

to collect historical data and rebuild the mathematical model, leading to delays of many months. WSN technologies have been applied into various areas such as [5]. It consists of portable wireless sensor motes such as Crossbow's TelosB or MICAz to monitor the values of physical conditions, such as temperature, light, humidity, and so on. WSN data collections use several specific protocols such as Collection Tree Protocol [6]. Some customized lighting control systems are targeted for special cases. [7] is designed for theater arts area and [8] was mainly designed for entertainment and media production. Some systems use occupancy sensors [9] to switch off lights in unoccupied positions. [10] presented a mathematical model for lighting control problem in which a luminary impact is continuous such as light emitting diodes(LEDs) luminaries rather than discrete values, and the expected illumination level is given as a single value rather than a range. [11] used smart illuminance sensors with infrared ray communication technology to retrieve the lighting ID binding with each lighting fixture. Through analyzing lighting ID information, sensors can recognize nearby luminaries, which is helpful for systems to know the group information for each actuator.

minimize
x,b

E (x, b),  (L1 (x, b), ..., Lm (x, b)) min  Lj (x, b)  max, j = 1, ..., m xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n (1)

subject to

By applying the -constraint method designed for solving multi-objective optimization problems, the new objective function can be defined as: minimize
x,b

 (L1 (x, b), ..., Lm (x, b)) min  Lj (x, b)  max, j = 1, ..., m E (x, b)   xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n (2)

subject to

One important feature for lighting impacts is sensor readings are additive, i.e. let Impactij to be the impact of light i on k sensor j when only light i is on, and Impactb kj to be the impact of daylight on sensor j when only blind k is on and set to a particular setting bk , then we have
n  i=1 n  k=1


III. F ORMAL M ODEL Lj (x, b) = In this model we have a set of switches that control arbitrary lights (one per switch) and a set of blind control switches (each sets angle of one blind)and a set of light sensors. We have established that illumination is additive, that is, the impact of two light bulbs is the sum of the impact of each at a fixed point. Assume a place has n light switches, n blind switches and m light-level sensors, placed by the human designer of the place. The sensors are connected to the control system via a WSN and the switches are activated via actuators connected to the system. The physical locations of lights, blinds, sensors and their correlations are initially unknown to the control system. The ultimate task is to compute the positions of switches (lights and blinds). Let x = x1 , ..., xn  denote the assignment for light switches where xi  {0, 1} and 0 denotes off and 1 denotes on, b = b1 , ..., bn  denote the assignment for automatic blinds where each blind has finite settings and 0 denotes fully off, i.e., let B to be the set of discrete blind settings, then bi  B and bi = 0 indicates blind i is fully off. The goal is to optimize the energy E (x, b) and the comfort C (x, b). E (x, b) can be measured by the number of artificial lights on. For C (x, b), there are two criteria needed to satisfy, lighting level and lighting uniformity. The former is satisfied when every sensor reading stays in an accepted range i.e. if sensor j 's reading is Lj (x, b), then min  Lj (x, b)  max. The latter can be satisfied by minimizing the standard deviation(represented by  ) of the sensor readings. Hence the problem can be stated as: { f (bk ) =

Impactij · xi + bk =  0 bk = 0

k Impactb kj · f (bk )

(3)

1 0

Note for a specific light i, Impactij ( 0,  Z) is always a constant and for a specific short time period and a fixed blind k setting, we assume Impactb kj ( 0,  Z) also to be a constant. [12] and [13] gave a general definition of nonlinear integer programming problem. It can be stated as: max/min subject to f (x) hi (x) = 0, i  I = 1, ..., p gj (x)  0, j  J = 1, ..., q x  Zn

(4)

where x is a vector of decision variables, and some of the constraints hi , gj : Zn  R or the objective function f : Zn  R are non-linear functions. In equation (2), let g1 (x, b) = Lj (x, b) - max, g2 (x, b) = E (x, b) - , h1 (x, b, y ) = Lj (x, b) - min - y = 0, y  0, y  Z. It can be transformed to: minimize
x,b

 (L1 (x, b), ..., Lm (x, b)) g1 (x, b)  0 g2 (x, b)  0 h1 (x, b, y ) = 0 xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n y  0, y  Z (5)

subject to

Since the equivalent version of equation (2) equation (5) satisfies the format of equation (4) where the objective function and partial constrains(g1 (x, b) and h1 (x, b, y )) are nonlinear, our problem belongs to nonlinear integer programming problem. According to [12], it is NP-hard. Therefore, any polynomial time computed solution would be an approximation.  To find the best setting, a naive approach is to try all 2n+n positions for n + n switches, which is unacceptable due to the time complexity. Therefore, we propose a heuristic algorithm for computing an approximate optimal combination of blind and light settings. This paper focuses on the system control during the day. In the evening when sunlight doesn't exist, artificial lighting becomes the only lighting source and the detailed control approaches can be referred to the method described in [14]. Conceptually, for better saving energy, during the day, daylight will be considered first to provide illuminance. Artificial lights will be used to even and compensate the lighting when necessary. Detailed control approaches will be discussed in section IV. IV. C ONTROL A PPROACHES A. Calibration In an earlier publication[14] we described the calibration procedure for artificial lights. The calibration of the blinds follow a similar method and hence we outline it. While there is no daylight, artificial lights are calibrated by turning on one switch at a time and noting the sensor readings or sensors where this light has impact. From this data, we compute zones (or rooms where each light is located). From the impact measures, we are then, heuristically able to decide which light switches should be on for the lighting to be even, even when there is daylighting. In this section, we describe the calibration steps for eleci trical blinds, which is used to calculate Impactb ij (a.k.a. blind i's impact on sensor j at angle bi ). We assume there is no any other lighting source involved except daylight at this stage. Specifically, suppose there are n blinds in an area, each blind has k settings. The process of calibration is: 1) Turn off all blinds and check existing values on each sensor 2) Turn on one blind up to a particular angle at a time 3) Calculate the blind's impact on each sensor(sensor value - existing value) 4) Repeat step 2 n × k times until all settings are counted It is clear that the above calibration steps run in linear time(O(kn )) and the space complexity is (O(kmn )). Compared to the exhaustive search solution which runs and records all the combinations of blind settings(complexity is  O(k n )), our calibration largely saves both time and space complexities. All impact data will be stored in database for further processing. B. System Control Workflow The goal is to compute desired blind and light settings for a time point T . The proposed lighting control approach contains

three main stages: blind prediction stage, blind adjustment stage and artificial lighting control stage. Blind prediction will be initiated first to compute predicted blind settings based on predicted calibration datasets. Predicted settings will be passed into blind adjustment stage for adjusting electrical blinds in real time. Artificial lighting will be used when inside lighting level is either not sufficient or not distributed evenly. The system relies on the calibration datasets stored in database. Those datasets will be passed into data collection and modeling server for further processing. This server is doing some preliminary work, for example, classfication. In this paper, we assume that the historic data used for blind settings computation at particular day has to be picked up from the category of that day. For example, computations in sunny days use the historic data from sunny days. To build a more precise computational model, the system also collects some conditions from outside world such as comfort conditions. New generated results(settings) will be put into database, which are collected by the system itself for better learning scale factors and creating predicted datasets. The system therefore can be viewed as a learning-based closedloop control system, which will become more robust and precise when dataset is becoming larger. Figure 1 reflects the system control workflow. Detailed methodologies operated at three main stages are discussed below. C. Blind-Prediction Blind prediction relies on the predicted calibration datasets generated by data collection and modeling part. It applies a scale factor(offset) onto the historic calibrated data for similar days and produces the one for blind prediction use. 1) Count the minimum number of blinds needed to be bi turned on: suppose C = {Ci }, i  [1, n ] where
x bi = arg max Ci x Ci = m  j =1 x B

Impactx ij · f (x)

(6)

To reach this goal, simply find minimum number of elements in the sorted array C such that sum of them are greater or equal than lowerbound(m × min). 2) Base-level candidates are computed: From step 1, it is known at least w blinds is needed. Therefore, each base-level candidate setting should have at least w elements selected to be turned on, and the total contribution on sensors under each candidate should be greater or equal than lowerbound value. To solve the problem, it needs w iterations. At each iteration, an unselected blind is picked up whose setting(i, bi ) is generated based on: lowerbound - existing , iteration  [0, w - 1] w - iteration

bi Ci >=

Comfort Conditions User Wishes Energy Consumption

system inputs

Data Collection & Modeling

database
calibration datasets

predicted calibration datasets

Blind Prediction

predictive values
Fig. 1.

Blind Adjustment
System Control Workflow

approximate optimal values

Artificial Lighting Control

3) More candidates are generated from base-level ones: Since adding  blinds to compute needs O(n ) time, to ensure low response time, we set  = 2. Suppose each base-level candidate has w elements which have non-zero settings, there are n blinds in total, each blind has k settings. Then if each candidate wants to pick up an unselected blind, there would be k (n - w) choices. If there are s base-level candidates, finally there would be s × k × (n - w) new candidates that have w + 1 elements which have non-zero settings. Similarly when another unselected blind is trying to be picked up, total amount of candidates that have w + 2 elements which have non-zero settings is becoming s × k × (n - w) × k × (n - w - 1). Thus total number of candidates would be s + s × k × (n - w) + s × k × (n - w) × k × (n - w - 1). Note for each candidate  bi q, Cp  upperbound(m × max) where Cp = Ci and
pq,

expectation) only. Algorithm 1 describes the steps of blind adjustment. Specially, setting denotes the predicted blind setting generated by blind prediction stage, lur denotes the real sensor readings under setting , lup denotes the predicted sensor readings under setting . With the blind-adjustment step, the system is able to adapt the environmental changes more quickly and make corresponding adjustments more properly compared to a pure learning system. Algorithm 1 Blind Adjustment
Input: setting , lur , lup Output: new blind settings 1: scale =  × (lur / lup ) 2: for (i, bi )  setting do x x 3: max = maxxB Ci × scale, min = minxB Ci × scale bi bi 4: Ci = Ci × scale bi 5: dif fmax  |Ci - max| bi 6: dif fmin  |Ci - min| 7: end for 8: if lur < lowerbound then 9: choose values from dif fmax , sum of the values  offset 10: else 11: if lur > upperbound then 12: choose values from dif fmin , sum of the values  offset 13: end if 14: end if 15: adjust blind angles based on the chosen values

p = (i, bi ). 4) Standard deviation of sensor readings generated by each candidate is calculated: According to equation 3, for each candidate c, we are able to calculate its impact on sensor j Lj (c)(note there is no artificial lighting at this point). Then it is easy to know the standard deviation of all sensors' readings under c. The candidate that generates the lowest standard deviation of sensor readings would be selected as the final blind setting, at the blind prediction stage. Suppose there are n blinds in total. Since step 1 has a time complexity O(n ), step 2, step 3 and step 4 all have a time complexity O(n2 ), Blind-Prediction has a time complexity O(n2 ). Compared to feedback system, our approach predicts and finds the approximate blind settings in a very short time without having physical blind movements, which results in more satisfactory feelings to users. D. Blind-Adjustment Since the calibration data used in the prediction stage is predicted based on historical data, there exists errors at the prediction stage. The adjustment step is added into the control plan to reduce the offset in real time. To minimally reduce the number of blind movements, each blind(if selected) will be adjusted to maximum impact angle(when lighting is not sufficient) or minimum impact angle(when lighting is beyond

E. Artificial Lighting Involved into the System The biggest advantage of involving blind control into the system is saving lighting resources. However, artificial lighting cannot be fully ignored in some circumstances. When sunlight is not sufficient, artificial lighting is required for completing the inside lighting levels. When there are variations among sensor readings, artificial lighting is needed to reduce them. According to the objective function described in section III, the selection of value  introduced in equation 2 needs to be adjusted based on real environments. Since computational time is important and low time complexity is desired,  is always set to be minimum required lighting + 2 artificial lights(unless there is a user-specific requirement), and thus the total time complexity would be no more than O(N 2 )(N is number of

updated data

actuators). The extension work described in [14] detailedly describes how to adjust artificial lights to get a more even and balanced environment. V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS A. Implementation Details To demonstrate the feasibility and effectiveness of our approach, we used an experimental setup. A 8ft × 8ft test cell was instrumented with 9 lights (15W incandescent, 120V ), two automated blinds and 9 sensors connected via a WSN and actuators to a computer [14]. The blinds are placed on two windows and communicate with the control server through serial port. They can be controlled by slave commands sent from the server. Each blind can be adjusted from 0 degree(fully off) to 90 degree(fully on). Blind control programs are written in Java. B. Feasibility The experiment is run during the day. It is to verify the feasibility of our proposed lighting control algorithm. Control strategies are based on the contents introduced in section IV. One special sensor is put on the window to detect the offset (scale) between the experimental day's sunlight level and the previous days. Blinds' impacts on sensors at previous days are stored in the database. By applying the scale factor, a predicted calibration dataset is obtained for blind prediction. Min value is set 100. Max value is set 130. To check the difference between our computed results and optimal ones, we first run our proposed method at 8 am. Then with the same configurations, we run a brute force algorithm checking all combinations of all actuators using real data rather than predicted ones. Sensor readings for both experiments are recorded. We repeat this experiment at other time points like 10am, 12pm, 2pm and 4pm. The results are shown in Figure 2. From the results we can see compared to the brute force, the proposed approach has a similar light intensity performance and a very short increase on standard deviation. C. Adaptivity and Scalability Our previous paper [14] has shown the artificial lighting control part satisfies the adaptivity and scalability features, that is when either pattern changes or amount of lights is increased, the system is still able to compute settings in a reasonable time. Now we need to study these two features for the blind control part of the lighting control system. To this end, we perform simulations on a more complex, synthetic setup, with randomly generated impact values. Suppose there are n blinds, n lights, m sensors, each blind has k settings. To match the scenario in the previous real experiment, we set n = 9, m = 9 and k = 7. Historic calibration dataset A will be given in the following way: to each blind, every setting's impact on a specific sensor m is randomly given an integer value from 1 to 100. The scale factor  is set to be a random decimal value from 1 to 2, which matches the factor used in our real experiments. The

TABLE I L IGHTING L EVEL C OMPARISON BETWEEN B RUTE F ORCE M ETHOD O UR P ROPOSED M ETHOD (LUX)

AND

# of Blinds 8 9 10 11 12 13 14

Min 175 191 219 229 246 277 298

Max 306 298 350 333 369 383 426

Brute Force 181 260 292 309 296 298 365

Proposed Method 294 269 236 318 340 364 403

real data is also derived from the dataset A, with a scale value in the range of 1 to 2 as well. Existing illumination level on each sensor is randomly set from 1 to 50. If blind i's total impact on sensors is blindi , then the average impact of a blind n  blindi  is /n . lowerbound is selected as n /2 × average k i=1 impact, upperbound is selected as (n /2 + 3) × average impact. Each artificial light's impact on sensor is randomly set between 1 to 100, same to the configurations described in [14]. We did 7 groups of simulation experiments with an increased number of blinds. We compared the results getting from brute force with our approach. TABLE I describes the average lighting levels generated by brute force method and our proposed method for each simulation experiment. It indicates settings computed by our proposed lighting control algorithm can generate the impact values that fit into the acceptable range. Figure 3 shows the standard deviation and computational time results for brute force method and our proposed method respectively. From the results, we can see the standard deviation of the proposed method is nearly 1.5 - 2 times as compared with the optimal solutions while computational time is far less than the latter one. Compared to the brute force, the proposed approach has an extremely better time performance with a similar light intensity performance and a reasonable increase on standard deviation. In other words, the blind control part of the lighting control system satisfies the adaptivity and scalability features. VI. C ONCLUSION AND F UTURE W ORK To enable automated lighting control, under varying conditions of occupancy, weather, seasons and other lighting influences it is essential to have a complete system that is effective and adaptive. Such system must be deployable in a simple, cost effective system without the need for customizations and reprogramming as conditions change. It also needs to create a comfortable environments with energy savings as a goal. This paper presents such a complete core system that contains both daylight harvesting module and artificial lighting control module. It also tests the feasibility and effectiveness in both experimental and simulated scenarios. The underlying system can be deployed in buildings and homes without

130

40

Light Intensity (Lux)

Standard Deviation

125 120 115 110 105 100 95 90 8am 9am 10am 11am 12pm 1pm 2pm 3pm 4pm

35 30 25 20 15 10 5 0 8am 9am 10am 11am 12pm 1pm 2pm 3pm 4pm

brute force our approach

brute force our approach

Time
(a) Light Intensity Fig. 2.
30

Time
(b) Standard Deviation Evaluation of System Feasibility
5 4 3

Standard Deviation

25

brute force our approach

20

log10 Time
brute force our approach
8 9 10 11 12 13 14

2 1 0 -1 -2 8 9 10 11 12 13 14

15

10

5

0

# Of Blinds
(a) Standard Deviation Fig. 3.

# of Blinds
(b) Computational Time Evaluation of System Adaptivity and Scalability

excessive costs. The lighting control problem is built using non-linear integer programming model, which is NP-hard. A heuristic algorithm is proposed to solve the problem to compute approximate optimal solutions. Future improvements to the lighting control system include the investigation of data classification, sensor placement and scale factor learning. Current data classification only consider seasons (such as summer or winter) or weather (such as sunny or cloudy) conditions. More conditions such as daylight illumination level, geographical information need to be collected for better classification. With the use of the system, more data will be collected in the database. These data can be used to better predict the scale factors. The results will be applied into the current system control plan. When dataset is made larger, the system will become more robust and precise. R EFERENCES
[1] S. Matta and S. Mahmud, "An intelligent light control system for power saving," in IECON 2010-36th Annual Conference on IEEE Industrial Electronics Society. IEEE, 2010, pp. 3316­3321. [2] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews, "Intelligent light control using sensor networks," in Proceedings of the 3rd international conference on Embedded networked sensor systems. ACM, 2005, pp. 218­229. [3] P. Ihm, A. Nemri, and M. Krarti, "Estimation of lighting energy savings from daylighting," Building and Environment, vol. 44, no. 3, pp. 509­ 514, 2009. [4] J. Lu and K. Whitehouse, "Suncast: fine-grained prediction of natural sunlight levels for improved daylight harvesting," in Proceedings of the 11th international conference on Information Processing in Sensor Networks. ACM, 2012, pp. 245­256.

[5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of the 1st ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88­97. [6] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1­14. [7] C. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, "Design of a wireless sensor network based automatic light controller in theater arts," in Engineering of Computer-Based Systems, 2007. ECBS'07. 14th Annual IEEE International Conference and Workshops on the. IEEE, 2007, pp. 161­170. [8] H. Park, J. Burke, and M. B. Srivastava, "Design and implementation of a wireless sensor network for intelligent light control," in Proceedings of the 6th international conference on Information processing in sensor networks. ACM, 2007, pp. 370­379. [9] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic, E. Field, and K. Whitehouse, "The smart thermostat: using occupancy sensors to save energy in homes," in Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems. ACM, 2010, pp. 211­224. [10] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon, "Intelligent lighting control using sensor networks," in Networking, Sensing and Control (ICNSC), 2013 10th IEEE International Conference on. IEEE, 2013, pp. 170­175. [11] M. Miki, A. Amamiya, and T. Hiroyasu, "Distributed optimal control of lighting based on stochastic hill climbing method with variable neighborhood," in Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference on. IEEE, 2007, pp. 1676­1680. [12] R. Hemmecke, M. K¨ oppe, J. Lee, and R. Weismantel, "Nonlinear integer programming," arXiv preprint arXiv:0906.5171, 2009. [13] Wikipedia. Nonlinear programming. [14] Y. Wang and P. Dasgupta, "Designing adaptive lighting control algorithms for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE, 2014, pp. 279­284.

Digital microfluidic biochip's (DMFB's) have emerged as an alternative to various in-vitro diagnostic tests and are expected to be closely coupled with cyber physical systems. Efficient-error-free-routing and cross-contamination minimisation are needed during bioassay operations on DMFB. This study proposes a two phase heuristic technique for routing droplets on a two-dimensional DMFB. Initially it attempts to route maximum number of nets in a concurrent fashion depending on the evaluated value of a proposed function named interfering index (IInet). Then exact routing is attempted based on tabulation minimisation process. Remaining nets having interfering index values higher than threshold will be routed considering various constraints in DMFB framework. In second phase another metric named routable ratio (RR) is proposed and depending on RR metric, the routing order among conflicting paths are prioritised to avoid deadlock from there onwards till the droplet reaches its target location. Finally we formulate droplet movement problem as satisfiability problems and solve with SAT based solver engine if higher number of overlapping (â‰¥5) nets exist. Experimental results on benchmark suite I and III show our proposed technique significantly reduces latest arrival time, average assay execution time and number of used cells as compared with earlier methods.Fair Resource Allocation for Heterogeneous Tasks
Koyel Mukherjee, Partha Dutta, Gurulingesh Raravi, Thangaraj Balasubramaniam, Koustuv Dasgupta, Atul Singh Xerox Research Center India, Bangalore, India 560105 Email: {Koyel.Mukherjee, Partha.Dutta, Gurulingesh.Raravi}@xerox.com {Rajasubramaniam.T, Koustuv.Dasgupta, Atul.Singh}@xerox.com

Abstract--We consider the problem of fair allocation of resources to tasks where a resource has to be assigned to at most one task entirely without any fractional allocation. The system is heterogeneous in the sense that the cost may vary across resources, and different tasks may have different resource demand. Due to heterogeneity of resource costs, the cost of allocation for a task in isolation, without any other competing task, may differ significantly from its allocation cost when the task is allocated along with other tasks. In this context, we consider the problem of allocating resources to tasks, while ensuring that the cost is distributed fairly across the tasks, namely, the ratio of allocation cost of a task to its isolation cost is minimized over all tasks. We show that this fair resource allocation problem is strongly NP-Hard even when resources are of unit size by a reduction from 3-partition. Our main results are a 2+ O( ) approximation LP rounding based algorithm for the problem when resources are of unit capacity, and a near-optimal greedy algorithm for a more restricted version. The above fair allocation problem arises in various context, such as, allocating computing resources for reservation requests from clients in a data center, allocating resources to computing tasks in grid computing, or allocating personnel for projects in service delivery organizations. Keywords-Resource allocation, Approximation algorithm, LP rounding, Greedy algorithm

I. I NTRODUCTION In this work we consider a heterogeneous system where each resource has an associated cost and a capacity (or size), and a resource can be allocated entirely to at most one task, without any fractional allocation. Every task has a demand (or capacity requirement) and we would like to minimize the resource allocation cost for meeting this demand. In isolation, i.e., when there is exactly one task in the system, an ideal allocation would select a set of resources that would meet the task's demand while minimizing its total cost. This case of a task in isolation is identical to the minimization knapsack problem [6], a known NP-Hard problem, and we call the corresponding minimum cost of a task, its isolation cost. However, in presence of multiple tasks, due to heterogeneity of resources, the resource allocation

cost for a task may significantly differ from its isolation cost. In this scenario, we would like to minimize the cost for multiple tasks in a manner that is fair across tasks. In particular, when multiple tasks are present, we want to allocate the resources in a way that minimizes over all tasks, the ratio of the allocation cost of each task to its isolation cost. Fair resource allocation is one of the core problems in parallel and distributed computing which arises in multiple settings, as illustrated next. Consider a multitenant data center where multiple tenants request to reserve certain computing capacity over the set of available (physical or virtual) machines. Here the tasks are the reservation requests with their respective demands, and the resources are the machines, each with its computing capacity and cost. We would like to ensure a resource allocation that is fair across tenants, in terms of the costs of resources allocated. From a tenant's point of view, such a fair allocation is preferable than the case where the data center operator tries to minimize the total cost over all resources. A similar problem can be seen in a geographically distributed grid computing environment in which different users request computing resource reservations, where the resource costs can be the monetary cost, or the cost of network communication. Finally, the problem of allocating personnel to project in large service delivery organizations, such that any increase in project cost, due to the presence of other tasks, is fairly distributed, can also be modeled as the above fair resource allocation problem. A. Problem Definition and System Model We study the following problem. Given a set of tasks where each task has a certain resource requirement (also called demand), and a set of resources where each resource has a certain resource supply (also called capacity or size of the resource) and a cost, find a fair allocation of tasks to resources such that the resource requirements of all the tasks are met and each resource is allocated entirely to at most one task. The fairness is

defined with respect to the ratio of the cost of a task when allocated along with other tasks, to the optimal cost of the task, when allocated in isolation (i.e., when other tasks are not present). Our goal is to minimize the maximum of this ratio across all tasks. Henceforth, we refer to this problem as the fair allocation problem. Formally, we are given a set of tasks P , and we have access to a set of resources E . Each task j is characterized by its resource requirement denoted by Dj . Each resource i is characterized by two parameters: a resource supply si and a cost ci . Upon allocating a resource i to a task j , the task incurs a cost of ci and its resource requirement is reduced by si . In order for a task j to successfully execute, it must be ensured that the total size of resources allocated to it is at least Dj . We require all assignments to be integral (0 or 1), i.e., each resource must be entirely assigned to a task and hence cannot be fractionally assigned to multiple tasks. We denote by Ij the isolation cost of a task j , that is the cost that the task would incur if this is the only task that needs to be allocated resources and no other tasks were present in the system. We denote by Cj the cost incurred by the task j when resources are allocated to it along with other tasks in the system; we refer to this cost as the actual cost of the task. The problem objective i is to minimize  = maxiP C Ii . B. Related Work Fairness of resource allocation has been well-studied both from a theoretical and practical point of view. Fair sharing of network resources (such as a wired network links and wireless spectrum) has been extensively studied, and various indices and algorithms for fairness have been developed, such as [5], [7]. More recent work on sharing of system resources has focused on sharing resources in data centers [4], [9], and on a variant of max-min fairness. Our work differs from these [4], [5], [7], [9] in following two ways. Firstly, we consider a system where there are sufficient resources to satisfy requirements of all tasks, but the cost of the resources may vary, which in turn results in increase in cost of task in presence of other tasks. Most earlier work considers a system where the number of resources is limited and may not be able to fully satisfy the demands of all tasks. With the rapid increase in the computing capacity of data centers, our assumption of a large number of available resources from one or more data centers, albeit with different (rental) costs, is increasingly becoming more relevant. Secondly, we give a provable fairness guarantee for tasks, where fairness is quantified by the ratio of actual cost of a task to its isolation cost, which has not been investigated earlier. 2

The combinatorial optimization problem studied in this paper is a mixed packing and covering problem [11], where we need to cover the demand of task using resources subject to packing the selected resources within a certain cost budget. Young [11] studies the fractional version of the problem, which is not NP-hard, and provides efficient sequential and parallel algorithms for the problem. Chakaravarthy et al. [2] study a version of mixed packing and covering problems, where they solve a knapsack cover problem subject to cardinality constraints, and then extend it to multiple matroid constraints. In terms of the resource allocation problem that we study in this paper, the work of Chakaravarthy et al. [2] can be used to find the minimum cost resource allocation for one task, such that the total size of the resources allocated meets the total resource requirement of the task, and the number of resources allocated is at most a pre-specified number. This is slightly different compared to our problem where we try to ensure fairness of resource allocation costs across multiple tasks, such that the total resource requirement of every task is met. Escoffier et al. [3] study some multi-agent optimization problems where the goal is to maximize the satisfaction of the least satisfied agent, where the satisfaction of an agent is defined as the ratio between his utility for the given solution and his maximum possible utility. For some NP-hard problems, assuming a feasible solution exists, they give polynomial algorithms with approximation factors dependent on the number of agents and/or on other problem parameters. Though the objective of the problem studied is quite similar to ours, the underlying problem we study is different, and we provide constant approximations, not dependent on the problem size. Moreover, our techniques are fundamentally different from Escoffier et al. [3]. Online algorithms for the fractional mixed packing and covering problem are investigated by Azar et al. [1], where the packing constraints are known offline, while the covering constraints get revealed online. Their objective is to minimize the maximum multiplicative factor by which any constraint is getting violated, while meeting all the covering constraints. They give a polylogarithmic competitive ratio, and a nearly tight lower bound for the fractional problem. In contrast, we study an integral, offline version of the above problem, for which we give constant approximations. C. Our Contributions This paper makes the following contributions. 1) We first show that the fair allocation problem is NP-Hard in the strong sense even when the resources are of unit size.

2) We formulate the fair allocation problem as a mixed packing and covering problem and give a 2 + O( ) approximation algorithm, based on LP rounding when resources are unit sized. 3) We further show that the LP considered has an integrality gap of 2, hence the bound of 2 + O( ) is essentially tight for this LP. 4) Finally, for a restricted version of the problem, we give a near-optimal greedy algorithm. We would like to note that, although we assume that resources cannot be fractionally allocated to a task, it is straightforward to extend our methods to a setup where a resource can be allocated in multiples of a certain given 1 fraction k , by creating k copies of the resource each 1 with k of the cost and size of the original resource. D. Organization of the Paper The rest of the paper is organized as follows. In Section II, we show that the fair allocation problem is strongly NP-hard even for unit size resources. Then we formulate the problem as an integer linear program and show that a naive relaxation has an unbounded integrality gap in Section III, for unit sizes. We also show that when resources have arbitrary sizes and costs, any trivial modifications of the LP considered will still result in an unbounded integrality gap. Hence, we focus on the unit size resource problem the next section onwards. We give a 2 + O( ) approximate LP rounding algorithm for this problem in Section IV, following which we show that the LP considered has an integrality gap of 2 in Section V which essentially shows that our bound is tight. In Section VI, we present a greedy algorithm, that in a restricted scenario, achieves near optimal performance and finally Section VII concludes the paper. II. NP- HARDNESS In this section, we show that the problem of fair resource allocation to multiple tasks, is strongly NPhard even when the resources are of unit size and have different costs and the tasks are all identical in the sense that the resource requirement of each task is same. The reduction is from 3-Partition. Theorem 1: The feasibility problem of the fair resource allocation to multiple tasks is strongly NP-hard. Proof: Consider an instance of the 3-partition problem. There are n = 3m integers {ai |i  [1, . . . , n]} such that the sum of the integers i[1,...,n] ai = mB , B and each integer ai is strictly between B 4 and 2 , i.e., B B 4 < ai < 2 . The feasibility question is whether there exists a partition of the n integers into m partitions such that the sum of the integers in each partition is exactly equal to B . Note that this would require every partition 3

Minimize  subject to the following constraints: I1. j  P iE xi,j × ci   × Ij I2. j  P iE xi,j × si  Dj I3. i  E j P xi,j  1 I4. xi,j  {0, 1} i  E , j  P Fig. 1. An integer linear program for the fair allocation problem.

to have exactly 3 integers. Now, let us define a fair resource allocation problem, where we have m tasks, each requiring 3 units of resource. We create n = 3m resources, each of unit size, where the cost of each resource is ci = ai . Let us order the integers in the 3partition instance in non-decreasing order of their sizes. Let the sum of the 3 smallest integers be I . The isolation cost of every task is therefore equal to I . The feasibility question we ask is whether  = B I is feasible. If there exists a feasible solution to the 3-partition instance, that implies that  = B I is feasible, since this corresponds to 3 units of resources per task, and the cost incurred by every task is B . At the same time, if there exists a feasible  = B I for the fair resource allocation problem, then this implies that the 3-partition instance is feasible. Every task has received 3 resources, and the cost incurred by every task must be  I = B I I = B . Since we have allocated every resource, the total sum of the costs incurred by all m tasks is mB , hence no task can cost < B , as that would make another task cost > B , which cannot happen,  being feasible. As a result, this requires every task to cost exactly B . Therefore, the resource allocations to the m tasks corresponds to feasible m partitions of the integers in 3-partition, where the sum of the integers in every partition is exactly B . III. ILP FORMULATION , LP RELAXATION AND I NTEGRALITY GAP In this section, we formulate the problem as Integer Linear Program (ILP), then relax it to a Linear Program (LP) and show that the problem in the generic case where resources may have different costs and sizes has an unbounded integrality gap. Recall that the aim is to find an allocation of resources to tasks that minimizes the ratio of actual cost of allocation to the isolation cost, for all tasks. Formally, C we want to minimize  , where  = maxj P Ijj . This is subject to fulfilling the resource requirement Dj for all tasks j  P . We formulate this problem as an ILP, shown in Figure 1. In the formulation given in Figure 1,  represents an upper bound on the ratio of Cj to Ij across all tasks j  P , and since we are minimizing it,  is the smallest possible value for the required objective, i.e., C min maxj P Ijj , that can be achieved by any integral allocation. In the formulation , the inequality I 1 ensures that the cost incurred by a task j due to resource

allocation is at most Cj = Ij , inequality I 2 ensures that the total resource allocated to a task is no less than its resource requirement Dj , inequality I 3 ensures that no resource must be over allocated, and finally I 4 ensures that every resource must be integrally allocated to a task, if at all. A naive linear relaxation of this ILP formulation would relax I 4 to xi,j  0. However, such a relaxation has an unbounded integrality gap as illustrated next. Consider m tasks, each with a resource requirement of Dj = 1, and m resources of which m - 1 resources have a cost 1 and one resource has a cost m. All resources are of unit-size, i.e., si = 1. Any integral allocation would have to allocate the high cost resource to one of the tasks, hence incurring a  = m. However, the linear program 1 would allocate m of each resource to each task. This will meet the resource requirement of every task since 1 m· m = 1, while the cost incurred by every task is 1 -1 1 m m + mm = 2- m . Therefore, the integrality gap is m m  2- 1 > 2   as m  . m To overcome this, we use the parametric pruning technique, similar to the seminal work of Lenstra et al. [8]. We guess the optimal value of  , and solve a feasibility linear program. (Later in the section, we show that  can be guessed in a logarithmic number of iterations.) For each guess of  , we solve the feasibility linear program shown in Figure 2, where in order to avoid giving an unfair advantage to the linear program, we allow the LP to assign a resource i to a task j , only if ci   × Ij .
C1. C2. C3. C4.
iE|ci Ij iE|ci Ij j P xi,j xi,j  0

C1. C2. C3. C4.

iE|ci Ij  si Dj iE|ci Ij  si Dj j P xi,j  1

xi,j × ci  Ij xi,j × si  Dj

xi,j  0

j  P j  P i  E i  E , j  P

Fig. 3. Further restricted LP relaxation (still with an unbounded integrality gap for arbitrary sizes.

xi,j × ci  Ij xi,j × si  Dj 1

j  P j  P i  E i  E , j  P

Fig. 2.

An LP relaxation of ILP shown in Figure 1.

However, if the resources are of arbitrary sizes, then the linear program shown in Figure 2 still has an unbounded gap1 . Further, simple parametric pruning along the size dimension, specifically, allowing the LP to allocate a resource i to a task j , only if si  Dj , still cannot remove the gap. Consider the LP shown in Figure 3. This LP still has an unbounded integrality gap, and trivial modification of this LP cannot remove this gap. We show this with an example. Consider m tasks where each task has a resource requirement of m. Let there
1 Consider a system with 2 tasks, each with a resource requirement of Dj = 1, and 2 resources of which resource 1 has a size = 2 and a cost 1, and resource 2 has a size 1 and a cost L 1. It can be seen that the isolation cost of both tasks is 1. Any integral allocation would have to allocate the high cost resource to one of the tasks, resulting in a  = L. However, the linear program would return a feasible solution 1 for  = 1, by allocating fraction 2 of resource 1 to both tasks.

be 2m - 1 resources of which m - 1 resources have a cost of m and a size of m, and m resources have a cost of m and a size of 1. Any integral feasible solution would have to assign the m - 1 resources of size m to m - 1 tasks and the remaining m resources of unit size to another task. The task receiving the m unit size resources incurs a cost of m2 , whereas the isolation cost of every task is m, which means any integral solution would incur a  of m. Since the isolation cost of every task is m, and the resource requirement of every task is m, and further since i, j : si  Di , every resource is feasible to be allocated to every task by the LP, for every   1. A feasible LP solution therefore, allocates each of the m - 1 resources of size m to each of the 1 m tasks to an extent of m , and one unit size resource integrally to each task. The total resource received by 1 + 1 = m. The total every task is therefore (m - 1)m m 1 cost incurred by every task is (m - 1)m m + m = 2m - 1. Therefore, the LP will return a feasible solution for a -1  = 2m < 2, whereas any integral solution will incur m a  of m, giving an integrality gap of m 2  , as m  . Note that if we try to modify the LP by restricting the allocation of resources which have a large ratio of cost to size, compared to the ratio of isolation cost to the resource requirement of a task, may render the LP infeasible, when a feasible solution exists, or suboptimal. Hence, in the rest of the paper, we will only consider the case where all resources are of unit size, but may have different costs. Guessing  : We next show that we can guess a near optimal  in a logarithmic number of iterations. The minimum value of  is given by min = 1. For every guess of  , we allow a resource i to be allocated to a task j , only if ci  Ij , i.e., ci  Cj . Without loss of generality, we assume that the minimum cost of any resource is 1, since the costs are rational numbers, and can always be made integral by scaling the problem. Similarly, we assume that the sizes of the resources are 1, and the resource requirement for every task Dj is integral. Observation 2: The maximum value of  is max = cmax , where cmax = maxiE ci . Proof: Suppose that the resource requirement of a task j with the highest value of  is Dj . In the worst case, the task is allocated those resources that have the highest cost, hence Cj  Dj × cmax . However, the isola4

tion cost of the task can be expressed as Ij  Dj , since minimum cost of any resource is  1. Therefore, the D ×c maximum value of  is given by   j Djmax = cmax . With that, we can do a binary search for  in the range [min, max ] = [1, cmax ], using a resolution of and guess the optimal  in log2 ( cmax ) iterations. The next theorem shows that a near optimal value of  can be found by such a method. Theorem 3: Let OP T -IN T be the objective function value for an integral optimal solution (where the optimality is with respect to the fairness objective). Let OP T -LP be the smallest  for which the linear program LP is feasible. Then OP T -LP  OP T -IN T + . Proof: Let the smallest  for which the LP is feasible be OP T -LP . By definition of OP T -LP and by the property of binary search it must hold that for OP T -LP - , the LP must have been infeasible. Therefore, there exists no feasible resource allocation satisfying the specified constraints in the LP, such that the cost for every task j is  (OP T -LP - ) Ij and allocation is  Dj , and every resource is allocated to unity. This implies that in any feasible (integral or otherwise) allocation of resources satisfying the resource requirement Dj for every task j , there must exist a task j such that Cj > (OP T -LP - ) Ij . Since any the optimal integral allocation is a feasible allocation (satisfying the LP constraints), this implies that there exists a task j in the optimal integral allocation, such that the cost incurred by j is Cj > (OP T -LP - ) Ij , C therefore, OP T -IN T  I j > (OP T -LP - ). This j completes the proof. In the remainder of the paper, we assume that the resources are of unit size. Note that, with unit-sized resources, the isolation cost Ij of a task j , with demand Dj , is simply the sum of the cost of Dj lowest cost resources. We also assume that there are sufficient resources in the system to satisfy the demand of all tasks simultaneously, but possibly with different allocation costs. IV. LP ROUNDING A LGORITHM In this section, we present a polynomial LP rounding algorithm with a 2 + O( ) approximation to the fair resource allocation problem. The factor is due to the binary search performed over the space of  with a resolution . We later (in Section V) show that for unit size resources, even the LP has an integrality gap approaching 2, hence our rounding algorithm is essentially tight. The rounding proceeds as follows. First, we convert a feasible LP solution to another feasible solution where 5

every resource, if allocated, is allocated up to unity. In other words, no resource has an overall fractional allocation. Once we have done this conversion, we essentially have a perfect fractional matching, where every resource is allocated to unity, and every task's resource requirement is exactly satisfied, while the cost of every task j is  Ij . Now, we convert this to an integral, feasible, perfect matching, such that the total cost of the resources allocated to every task j is at most twice Ij , where the LP was feasible for  . This procedure is described in detail in Section IV-A. We now show how to convert a feasible LP solution to a perfect fractional matching, where every resource is allocated to unity, and every task's resource requirement is exactly satisfied, while the cost of every task j is  Ij . Lemma 4: Let LP -OP T be the smallest value for which the LP is feasible. Then there exists a feasible LP solution  for the same value of  , in which any resource that has a non-zero allocation, is allocated to an extent of 1; formally, if i  E , j  P such that xij > 0 then it holds that j P xij = 1. Proof: Informally, the proof is based on the following reasoning. A more formal proof follows the informal discussion. Depending on whether one or more resources are fractionally allocated, we need to consider two cases. If there exists one resource i that is fractionally allocated (i.e., xi = j P xi,j < 1) then in a feasible solution, some tasks must be over-allocated. This is because (i) total resource requirement D = j P Dj of tasks is integral since individual resource requests Dj of each task are integral, and (ii) resources are unit size, (iii) only one resource is fractionally assigned, therefore, any feasible solution must have assigned  D resources to unity. Hence, there must be over-allocation of resources and this over allocation sums to xi which can be eliminated by readjusting the resource allocation of tasks without violating the feasibility of the solution thereby making all the resource allocations integral. If there exists multiple resources that are fractionally assigned then considering a pair of such resources at a time and by readjusting their resource allocations (decreasing one of the fractional allocation and increasing the other fractional allocation) systematically converts at least one of the fractional resource allocations into an integral allocation without violating the feasibility of the solution. Repeating this process either eliminates all the fractional allocations or leaves one fractional allocation in which case we can use the procedure described for the first case. A formal proof follows now. We prove the claim by contradiction. Suppose that the claim is not true. Then

we need to consider two cases. Case 1: A single resource has an allocation xi = j P xi,j < 1. We have already argued that at least D resources must have been allocated to unity, otherwise, the total resource requirement of D cannot be met. For every task j for which resource i has a non-zero allocation to, we first reduce xi,j by kE xk,j - Dj , without any loss of feasibility. Now, let P be the set of tasks for which resource i has a non-zero allocation after this modification. All the tasks in P are now exactly satisfied in terms of their resource requirements, i.e., k xk,j = Dj j  P . Let us consider two scenarios depending on the cost ci of the fractional resource i. Scenario 1.a. First consider the scenario, where all of the resources allocated to unity, cost at most ci . Since k ,j P xk,j > j P Dj , there must exist a task j , where k xk,j > Dj . Let the highest cost resource allocated to task j be i . Since ci  ci and resource i has a non-zero allocation to task j , resource i is feasible to be allocated to j . Now, we decrease the allocation of resource i to task j by  = min xi ,j , xi,j , kE xk,j - Dj > 0 and increase the allocation of resource i to task j by  , and decrease the allocation of resource i allocation to task j by  . The resultant allocation is still feasible in terms of resource requirements to all tasks, and the cost of every task is at most the cost of the earlier allocation. Since by definition,  > 0, we have decremented xi,j . As long as xi,j > 0, there is always a task j that has been over allocated, and we repeat the above transformations, till we have removed the allocation of resource i to task j , or, xi,j = 0. If xi > 0, then there must exist another task p to which resource i has a non-zero allocation. We repeat the above steps for resource i and task p. We continue till we have removed the allocation of i to every project, without violating feasibility, and removing the fractional resource. This takes a polynomial number of transformations. Scenario 1.b. Now consider the scenario where there exists a resource that has been allocated to unity and whose cost is greater than ci . From the set of resources allocated to unity, let k be the highest cost resource. Clearly, ck > ci . Now consider a task j to which resource k is allocated. Increase the allocation of resource i to task j by  = min (xk,j , 1 - xi ), while decreasing allocation of resource k to task j by  . In the process, we have not violated feasibility since the resource requirement is still met every where, every resource is allocated at most to unity, and the costs of the tasks after this transformation is at most what they were earlier. After this transformation, if we have made resource i allocated up to unity, then resource k 6

becomes the new fractionally allocated resource having the highest cost of any non-zero allocated resource, and hence we can now apply the procedure of Scenario 1.a to reduce its allocation to 0. Otherwise, i.e., if resource i is not allocated to unity then xk,j = 0. Note that, even after this transformation, xk > 0 since  < 1. Therefore, there must exist another task j where resource k has a non-zero allocation. We repeat the above process, till we make resource i allocated to unity. This will always be possible since resource k is allocated to unity, and 1 - xi < 1. Now, resource k becomes the new fractionally allocated resource, and it costs the most among all the resources allocated, hence this reduces to Scenario 1.a. We repeat the transformations outlined in Scenario 1.a, till we reduce xk to 0. Case 2: More than one resource has an allocation < 1. Let us consider a pair of resources (i, i ) both of which are fractionally allocated. Without loss of generality, let ci  ci . Suppose xi + xi  1. In this case, for every task j that resource i has a non-zero allocation to, we make xi,j = xi ,j and reduce xi ,j to 0. In this way, we have reduced the number of resources with fractional allocation by at least 1. We repeat this for every pair of fractional resources, till we are left with at most one fractional resource. Then we perform the transformations described in Case 1. Now, suppose xi + xi > 1. Again, we repeat the above procedure for every task that resource i has a non-zero allocation to, till we come across a task, where xi ,j > 1 - xi , where xi refers to the current fractional allocation of resource i after the above transformations. In this case, we set xi,j = 1 - xi , and xi ,j = xi ,j - xi,j . Now, we have again reduced the number of fractional resources by 1. We repeat the above described procedures for every pair, till we are left with at most one fractional resource, which corresponds to Case 1. This completes the proof. We now describe the rounding algorithm. A. Description of the LP Rounding Algorithm The solution  is a set of connected components in a bipartite graph, with one vertex partition V1 = {i  E| j P xi,j > 0} being the set of resources with non-zero allocation. In fact, after the polynomial transformation described in Lemma 4, V1 = {i  E| j P xi,j = 1}. The other partition V2 is the set of tasks P . We have already argued that we have a perfect fractional matching in this bipartite graph, which we will now convert to a perfect integral matching, without violating the cost constraints too much. From  , we first remove all the edges that are integral, allocating the corresponding resource to the corresponding task, without any loss in feasibility. Let V1 = E be

  1    

2   
  

3   
  

the set of resources who are yet unallocated and V2 = P be the set of tasks, that have still not been fully allocated, after removing the integral edges. Since all the resource requirements are integral, and all resources are allocated to unity, the total number of resource available is no less than the total resource requirement over all tasks, i.e., |V1 |  pV Dp , i.e., we still have a perfect fractional 2 matching. We order the resource vertices in non-increasing order of their costs, and assume henceforth that they are numbered accordingly, namely e1 , e2 , . . . , e|V1 | , where ce1  ce2  . . . ce|V | . The tasks are ordered in 1 any arbitrary order, p1 , p2 , . . . , p|V2 | . Now, we use a procedure, which is somewhat inspired by the analysis of the generalized assignment problem, in the seminal work of Shmoys and Tardos [10] to transform the current fractional matching to an integral matching. The algorithm proceeds as follows. Start with the highest cost resource and allocate it integrally to any task that it has a non-zero allocation to. Now, adjust the allocation of the resources to maintain feasibility. Then, proceed to completely allocate this task. Once done, pick the next highest cost resource, that is not yet integrally allocated and repeat the procedure. We describe this in detail in the following paragraphs. Let pj be a task where resource e1 has a nonzero allocation, in other words, (e1 , pj ) is an edge in this connected component. We allocate resource e1 to resource pj integrally (i.e., xe1 ,pj = 1), and remove e1 from the set V1 . Now, we find the lowest resource index ei such that ek |k{1,...,i} xek ,pj  1, and ek |k{1,...,i-1} xek ,pj < 1. We remove the edge xei ,pj and split the node i into two nodes i1 and i2 , and add two edges xei1 ,pj = ek |k{1,...,i} xek ,pj - 1, and xei2 ,pj = xei ,pj - xei1 ,pj . We have made xe1 ,pj = 1. Therefore to maintain feasibility, we have to make xe1 ,pq = 0 pq  P , q = j . Note that all the resources ek , 1 < k  i, have costs  ce1 , and hence are feasible to be allocated to any task that resource e1 was allocated to. Now, consider a task j to which resource e1 had a nonzero allocation. We know that all of the resources ek , 2  k  i are feasible to be allocated to j . Hence, we add an edge from resource e2 to task j of value xe2 ,j = min (xe2 ,j , xe1 ,j ) to j and decrement xe2 ,j by xe2 ,j . Now, if xe2 ,j = xe1 ,j , we consider the next task j that resource e1 had an allocation to, and repeat the above process, with e2 if xe2 ,j > 0. Otherwise, if xe2 ,j = 0, and xe2 ,j < xe1 ,j , we pick e3 do the same procedure. We continue till either we have removed all the edges (ek , pj ) for all 1  k < i and xei1 ,pj , or we have have removed all edges xe1 ,pq for all q = j , in a 7

1 1

2

3 3  
 

1

2  
 

1

2

3 1  2    

Fig. 4.

Illustration of one iteration of the LP rounding algorithm.

polynomial number of transformations (O(|V1 | + |V2 |)). Note that xei1 ,pj + ek |k{2,...,i} xek ,pj = 1 - xe1 ,pj , therefore, both the events will happen simultaneously, after which we have a feasible solution, where every resource in V1 is allocated up to unity and every task in V2 is fully allocated. If task pj had only one resource requirement, then we have allocated task pj integrally, while not violating the cost constraint of pj , as ce1  Cj . Otherwise, if Dj > 1, then we still have unmet resource requirement in task pj . Now, we find the lowest resource index u > 1, that has a non-zero allocation to task pj , i.e., xeu ,pj > 0 and repeat the above. If xei2 ,pj > 0, then u = i, otherwise, u > i. We allocate resource u integrally to task pj , remove the resource from V1 , and repeat the above procedure. We continue till pj is allocated Dj integral resources. Once task pj is fully allocated integrally, we remove it from V2 , and find the next highest cost resource es remaining in V1 . Let pr be a task that es has a non-zero allocation to. We perform the same transformations on resource es and task pr and continue, till both V1 =  and V2 = . Figure 4 provides an example for one iteration of the rounding algorithm. Observe that we allocate only one resource integrally to a task at a time, and adequately remove some assignments to maintain feasibility of total allocation of any resource, and resource allocation of any task. Since, initially |V1 |  pk V2 Dpk , and we are not violating the size constraints on resource allocations at any iteration, we will always find a resource to allocate to a task yet unsatisfied, that is, if V2 = , that implies V1 = . Not only that, at any iteration, when we change the allocations, we maintain feasibility in terms of the assignment restrictions on the resources; in other words, we allow a resource to be allocated to a task, only if the cost of the resource is at most the total cost

allowed for the task. Moreover, at any iteration, when we are processing the resources allocated to a task q , we are removing Dq vertices from V1 and 1 vertex from V2 . Therefore, in at most |V1 | iterations, we will have allocated every resource and and every task integrally. B. Cost Incurred due to Rounding Algorithm In this section, we argue that the resultant integral allocation will be at most 2Cj for every task j . Note that the first resource e1 that is allocated to any task pj has a non-zero allocation to pj , hence, ce1  Cj . Let the next resource to be allocated be eu , where u > 1. Also, let i be the lowest index such that ek |1ki xek ,pj  1. Note that, u  i. According to the procedure outlined earlier, we must have added two vertices i1 and i2 to the graph, in place of i, and replaced the edge xei ,pj by xei1 ,pj and xei2 ,pj , such that ek |1k<i xek ,pj + xei1 ,pj = 1, and xei2 ,pj = xei ,pj - xei1 ,pj . In this case, C1 = ek |1k<i cek xek ,pj + cei xei1 ,pj . Clearly, C1  cei ek |1k<i xek ,pj + cei xei1 ,pj = cei as ek |1k<i xek ,pj + xei1 ,pj = 1. Therefore, cei  C1 , hence we charge its cost to C1 at no additional cost. Similarly, let the next resource to be integrally allocated to j is ev , v > u. This means ek |ukv xek ,pj > 1, and let ew be the lowest index such that ek |ukw xek ,pj  1. Note that v  w . Again, following the same of procedure of adding two vertices (say w1 and w2 ) and replacing the edges as described above, we obtain cew  C2 and hence we charge its cost to C2 at no additional cost where C2 = ek |uk<w cek xek ,pj + cew xew1 ,pj The total cost Cj = C1 + C2 + . . . CDj , where Ci s are defined above, and we have charged the cost of the k th resource allocated to pj , to Ck-1 , where k  {2, . . . , Dj }, therefore, the total cost of these resources is  Cj . The first resource to be allocated has a cost ce1  Cj . Therefore, the cost of the total integral allocation to task pj is at most 2Cj . We repeat the above argument for every task. C. Approximation Ratio of LP Rounding Algorithm Theorem 5: Given a  for which a feasible LP solution exists, there exists a polynomial time algorithm that gives an integral feasible resource allocation to all tasks, such that the cost of any task pj , is at most 2Ij , where Ij is its isolation cost. Proof: The proof follows from the discussion in Sections IV-A and IV-B. Theorem 6: There exists a polynomial 2 + O( ) approximation algorithm to the integral fair resource allocation problem. Proof: From Theorem 5, we know that given a  , for which a feasible LP solution exists, we can find 8

an integral feasible solution, where every task costs at most 2Cj = 2Ij . From Theorem 3, we know that the lowest value for which the LP is feasible is, LP -OP T  OP T -IN T + , and it can be found in at most log cmax iterations, for any fixed > 0. Hence the resultant value of  incurred by our algorithm is  2LP -OP T  2OP T -IN T + 2 . This proves the theorem. V. I NTEGRALITY G AP FOR U NIT S IZE R ESOURCES In this section we show that the LP has an integrality gap approaching 2. Theorem 7: The LP has an integrality gap  2, hence the LP rounding algorithm of Section IV is tight. Proof: Consider an instance with m tasks, each with a resource requirement of m. Let there be m2 resources of which m2 - 1 resources are of cost 1 and another resource is of cost m. Any feasibly integral solution would have to allocate the resource of cost m to one of the tasks thereby incurring a cost of 2m-1. However, the isolation cost I of every task is m. Therefore, any inte-1 1 gral feasible solution would incur a   2m = 2- m . m For the LP, every resource is feasible to be allocated to every task for every   1, since the isolation cost of every task is m and the resource requirement of every 1 . A feasible LP task is m. Let us consider  = 1 + m solution would allocate to every task, m2 - m unit cost resources integrally, m - 1 unit cost resources to an 1 1 extent of m , and m of the resource of cost m. With such an allocation, every task receives a resource allocation of m units, and every resource is allocated up to 1. Note that the overall number of resources allocated is m2 -m+m-1+1 = m2 . The cost incurred by every task 1 1 1 +m m = m+1- m < m+1, which is (m-1)+(m-1) m m+1 is feasible since  = m . Therefore, the integrality gap 2- 1 is at least 1+ m 1  2, when m  . Hence the proof.
m

VI. G REEDY ALGORITHM In this section, we consider a restricted version of the problem, where the costs of the resources, though arbitrary, vary smoothly across resources. The resources are unit sized as in the previous sections. We give a greedy algorithm which will give a near optimal fairness in resource allocation, when all the tasks have the same resource requirement. We first define some notations. Definition 8: A set of resources are called homogenous, if, when ordered in non-increasing order of their costs, the difference in costs between two consecutive resources is very small,  for some small fixed > 0. In other words, assuming the resources are numbered according to their position in the sorted order, for any i  {1, . . . , |E|}, ci+1 - ci  . We assume that ci  1.

Definition 9: A set of tasks are called identical in terms of their resource requirements, if their resource requirements are identical. In other words, for every pair of tasks j, j  P , Dj = Dj = D, where D  1. In this version of the problem, we assume the resources are homogeneous and give a greedy algorithm which allocates resources to the tasks in iterations, till every task is fully allocated its total resource requirement. In every iteration, the task to allocate a resource is chosen according to a rule which is a function of the total resource requirement of the task, remaining resource requirement and the cost incurred so far by the task due to allocated resources. To the chosen task, the algorithm allocates the next available resource in the sorted order. The choosing rule is as follows: pick the task j with the largest value of f (j ), i.e., j = arg maxkP f (k ), D +C where f (j ) is defined as: f (j ) = j Dj j , where Dj is the total resource requirement of task j , Dj is the remaining resource requirement or current deficit, Cj is the current cost incurred by j by the resources already allocated. Note that the isolation cost of a task depends on the total resource requirement of the task, and is higher for tasks with high resource requirements. The intuition behind choosing this function is that we want to favor the tasks that have already incurred a high cost relative to the total resource requirement (hence, the isolation cost), and also the tasks for which the remaining resource requirement is high relative to the total resource requirement. By favoring a task, we mean allocating it lower cost resources. This is done to balance the ratio of actual costs to isolation costs across resources. The value of  is chosen so that the greedy algorithm behaves in a certain manner to ensure fairness of cost across tasks. 1 . Specifically,  = iE ci The pseudo-code of the greedy algorithm is presented in Algorithm 1. We will prove that when the resources are homogeneous and tasks are identical, the greedy algorithm will give a near optimal fairness ratio. Let us denote the number of tasks |P| = n, in the following analysis for notational ease, and the resource requirement of each as D. Since we have n tasks, each requiring D resources, the greedy algorithm will require nD iterations. Let us define the set of consecutive n iterations [kn + 1, kn + 2, . . . , (k + 1)n] as the k th block iteration. Clearly we have D block iterations. We next prove that the greedy algorithm will allocate resources to the tasks in an alternating round-robin manner, such that in the ith block iteration, where i  {1, . . . , D}, every task will be allocated exactly one resource. Lemma 10: Every task is allocated a resource in every 9

Algorithm 1: Greedy algorithm for fair resource allocation.
Input : P : set of tasks; Dj : total resource requirement in task j  P Dj : current resource deficit in task j  P (initially set to the original resource requirement of j ; Cj : current cost incurred by task j  P (initially set to 0); E : set of resources; ci : cost of a resource i  E Output: xij : assignment of resources to tasks. Set Dj  Dj and Cj  0, j  P . Sort all resources in E such that ci  ci+1 i  {1, . . . , |E| - 1} Set xi,j  0, i  E , j  P . Set i  1; while P =  do Choose the task j , such that
k k j = arg maxkP Dk Set xi,j  1 and allocate resource i to task j . Set Dj  Dj - 1 and Cj  Cj + ci Set i  i + 1. if Dj = 0 then P  P \ j. end

1 2 3 4 5

D +C

6 7 8 9 10 11

end

block iteration (in other words, consecutive n iterations). Proof: Let us consider the first block iteration. Suppose a task j is not allocated at all in block iteration 1. Since a block iteration consists of n rounds, this implies that some task j is was allocated  2 resources in this iteration. Consider the round in the first block iteration, when j was chosen for a second allocation. According to the choosing rule, therefore, j = arg maxkP f (k ), Dj +Cj Dj +Cj  . But Dj = D which implies D D whereas Dj = D - 1, Cj = 0 and Cj = c, where c is the cost of the resource allocated to j . Clearly, D +C Dj +Cj 1 = 1. On the other hand, j D j = 1 - D + D c 1 < 1 . This gives a contradiction. Therefore, D kE ck every task gets an allocation in the first block iteration. Now, let us assume by induction hypothesis that this holds for the first i block iterations, where i < D. Now, all tasks have have i unit resources. If a task j does not get an allocation in the iteration i + 1, then this implies that some task j is was allocated  2 resources in this iteration. Consider the round in the i + 1 block iteration, when j was chosen for a second allocation. Since j = Dj +Cj Dj +Cj arg maxkP f (k ),  . Note that D D Dj = D - (i + 1), whereas Dj = D - i. Because of our choice of , 
Dj D

Also, =1 1 - i+1 D . Therefore, f (j ) - f (j ) < 0. This gives a contradiction. Hence, we have proved inductively the statement of the lemma. Observation 11: When all tasks have received the same number of resources, the next task to be allocated a resource is the task that has incurred the highest cost

Cj 1 D < D , therefore, Dj i -D , whereas D =



Cj D

-

Cj D

<

1 D.

so far. This follows from the function used by greedy as the choosing rule. Lemma 12: At the end of every block iteration i, the cost incurred by any task j , Cj is at most Ci,OP T + 1 n , where Ci,OP T = n e{1,...,i·n} ce is the optimal fair allocation cost when all the tasks are identical, each with a resource requirement of i, and the resources are homogeneous. Proof: At the end of any block iteration i, let the costs incurred by tasks be Cj j  [1, . . . , n]. Since any optimal solution would allocate the first i · n resources from the sorted list to the n tasks, the total cost incurred across all tasks in the greedy solution till iteration i is the same as that of any optimal solution. Therefore, 1 Ci,OP T = n j [1,...,n] Cj . Let Cmin,i denote the lowest cost incurred by any task at the end of block iteration i and Cmax,i denote the highest cost incurred by any task. Clearly, Ci,OP T  Cmin,i by definition. We will prove by induction that for any pair of tasks (j, j ). at the end of block iteration i, |Cj - Cj |  n . At the end of block iteration 1, Cmin,1 = c1 , and Cmax,1 = cn  c1 + n , (where cp is the cost of the pth resource in the sorted order), by our assumption of homogeneous resources. Hence, the base case holds. Now, we assume by induction hypothesis, that the above holds for all block iterations {1, . . . , i - 1}. In the block iteration i, according to Observation 11 and Lemma 10, the task with the highest cost is allocated first, followed by the next highest cost, and so on, till the lowest cost task gets allocated last. Suppose at the end of block iteration i, Cj,i  Cj ,i , where Cj,i denotes the cost incurred by j at the end of iteration i, hence j will allocated before j . But from induction hypothesis, Cj,i  Cj ,i + n . Therefore, Cj,i+1 - Cj ,i+1  c - c + n , where c is the cost of the resource allocated to j in block iteration i + 1, and c is the cost of the resource allocated to j in block iteration i +1. Note that c  c due to the sorted order and from Observation 11. Therefore, Cj,i+1 - Cj ,i+1  n . At the same time, Cj ,i+1 - Cj,i+1 = Cj ,i + c - Cj,i - c  c - c , since Cj ,i  Cj,i . But we know from homogeneous property of resources. c - c  n , therefore, Cj,i+1 - Cj ,i+1  n . This holds for any pair of resources (j, j ). This completes the proof by induction. We have proved that for any pair of tasks (j, j ). at the end of block iteration i, |Cj - Cj |  n . Therefore, Cmax,i  Cmin,i + n  Ci,OP T + n . Theorem 13: For homogeneous resources and identical tasks, the greedy algorithm gives a 1 + approximation to the fair resource allocation problem. 10

Proof: Let Cmax,D be the highest cost incurred by any task at the end of nD iterations, and COP T be the optimal fair allocation cost. For any optimal solution, T OP T = COP where I is the isolation cost of any I resource. From Lemma 12, Cmax,D  COP T + n . T +n Therefore, the resultant greedy  COP I . Hence, greedy n  1 + . We know that C = nDcavg OP T OP T COP T where cavg the average cost of the first nD resources.  n Therefore, OP  1 + nDc < 1 + , since D  1 avg T and cavg  c1  1. VII. C ONCLUDING R EMARKS This paper studied the problem of fair allocation of resources with heterogeneous costs to tasks with heterogeneous resource requirements. We show that the problem is strongly NP-Hard even with unit sized resources, and present an LP rounding approximation algorithm for this version of the problem and a near-optimal greedy algorithm for a special case in which costs of resources do not differ much and resource requirements of tasks are identical. As part of future work, we plan to study the problem where resources have limited heterogeneity in costs, as well as, the online version of the problem. R EFERENCES
[1] Yossi Azar, Umang Bhaskar, Lisa Fleischer, and Debmalya Panigrahi. Online mixed packing and covering. In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 85­100, 2013. [2] Venkatesan T Chakaravarthy, Anamitra Roy Choudhury, Sivaramakrishnan R Natarajan, and Sambuddha Roy. Knapsack cover subject to a matroid constraint. In LIPIcs-Leibniz International Proceedings in Informatics, volume 24. Schloss DagstuhlLeibniz-Zentrum fuer Informatik, 2013. [3] Bruno Escoffier, Laurent Gourv` es, and J´ er^ ome Monnot. Fair solutions for some multiagent optimization problems. Autonomous agents and multi-agent systems, 26(2):184­201, 2013. [4] Ali Ghodsi, Matei Zaharia, Scott Shenker, and Ion Stoica. Choosy: max-min fair sharing for datacenter jobs with constraints. In Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys), pages 365­378, 2013. [5] Raj Jain, Dah-Ming Chiu, and W Hawe. A quantitative measure of fairness and discrimination for resource allocation in shared computer systems. Technical Report TR-301, DEC, 1984. [6] Hans Kellerer, Ulrich Pferschy, and David Pisinger. Knapsack problems. Springer, 2004. [7] H. J. Kushner and P. A. Whiting. Convergence of proportionalfair sharing algorithms under general conditions. IEEE Transactions on Wireless Communications, 3(4):1250­1259, 2004. ´ Tardos. Approximation [8] J. K. Lenstra, D. B. Shmoys, and E. algorithms for scheduling unrelated parallel machines. Math. Program., 46(3):259­271, 1990. [9] Alan Shieh, Srikanth Kandula, Albert G. Greenberg, Changhoon Kim, and Bikas Saha. Sharing the data center network. In NSDI, 2011. ´ [10] David B. Shmoys and Eva Tardos. An approximation algorithm for the generalized assignment problem. Math. Program., 62(3):461­474, 1993. [11] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering. In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science (FOCS), pages 538­ 546, 2001.

Efficient routing and cross-contamination minimization are two interrelated challenging areas in Digital Microfluidic Biochip (DMFB). This paper proposes a two phase heuristic technique for routing droplets on a two-dimensional DMFB. Initially it attempts to route maximum number of nets in a concurrent fashion depending on the evaluated value of a proposed function named Interfering Index (IInet). Next the remaining nets having interfering index values higher than threshold will be routed considering various constraints in DMFB framework. Our method will check all the conflict arises between various routing paths from same assay or from different assays and also with the unsuccessful routing paths from the first phase (if any). If conflict occurs it determines the conflict zone on the chip as Critical Region(CR). Another metric named Routable Ratio (RR) is proposed and depending on RR metric, our method prioritized the routing order among conflicting paths(nets) to avoid deadlock from there onwards till the droplet reaches its target location. Experimental results on Benchmark suites III show our proposed technique reduces latest arrival time upto certain extent but significantly reduces average assay execution time and number of used cell compared to earlier routing methods for DMFB.Multi-mode Sampling Period Selection for Embedded Real Time Control
Rajorshee Raha, Soumyajit Dey, Partha Pratim Chakrabarti, Pallab Dasgupta Department of Computer Science & Engineering Indian Institute of Technology Kharagpur, INDIA {rajorshee.raha, soumya, ppchak, pallab}@cse.iitkgp.ernet.in

arXiv:1506.08538v1 [cs.SY] 29 Jun 2015

ABSTRACT
Recent studies have shown that adaptively regulating the sampling rate results in significant reduction in computational resources in embedded software based control. Selecting a uniform sampling rate for a control loop is robust, but overtly pessimistic for sharing processors among multiple control loops. Fine grained regulation of periodicity achieves better resource utilization, but is hard to implement online in a robust way. In this paper we propose multi-mode sampling period selection, derived from an offline control theoretic analysis of the system. We report significant gains in computational efficiency without trading off control performance.

General Terms
Real Time Control, Automotive Software

Keywords
Real Time Control, Automotive Software

1.

INTRODUCTION

Embedded software-based control systems have traditionally been implemented by assuming fixed sampling rates and fixed task periods [3]. The sampling rate is derived from a control theoretic analysis [9] of the system in a manner that guarantees desired level of control performance at all reachable states of the system. A uniform period can be implemented robustly, since we can analyze the sampling rate of all the control tasks and then choose an appropriate computational infrastructure that can statically schedule a periodic execution of the software components of these control tasks. The schedule does not change during execution and hence the control performance is deterministic. Recent studies confirm a widely accepted belief, namely that a uniform sampling rate is not a good choice when multiple control loops share a common computing resource such as an Electronic control unit (ECU). These studies establish

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

that the sampling period can be regulated to achieve significant benefits in computational performance without any trade off in control performance. In fact, it has also been shown that a non-uniform scheduling strategy can balance the sampling rates among the control loops sharing a ECU in such a way that the overall control performance improves [4, 5]. Uniform sampling rate is typically a pessimistic choice, since we need to choose a sampling rate that guarantees control performance at all reachable states of the system. It is often the case that the selected rate is necessary at only specific control states of the system, whereas at all other states a much lesser sampling rate suffices. Adaptive sampling derives its benefit from this fact by intelligently regulating the sampling period as needed to maintain the desired level of control performance [4]. Fine grained regulation of the sampling rate may theoretically determine the optimal balance between computational efficiency and control performance but such schemes are difficult to implement in practice due to non-determinism in timing introduced by the computational infrastructure (including message delays, execution time variations in different paths of the control software, etc). If the sampling rate is known a priori then it becomes possible to develop an appropriate schedule for all control tasks sharing a ECU with adequate consideration for these types of non-determinism. In this paper we profess the use of coarse grained regulation of the sampling rate. Specifically we propose an approach where for each control loop, a limited number of sampling rates are chosen and a control theoretic analysis is used to determine the switching criteria between these modes. Therefore, for each control loop we have a set of sampling states, and an automaton that captures the switching between these sampling states. The global sampling state of the system is a concatenation of the sampling states of the control loops in the system. For each global sampling state, the exact schedule for the control tasks is precomputed considering all types of non-determinism arising out of the execution of software tasks. The reachable global sampling states are chosen based on available computational bandwidth and the relative priorities of the control loops. The primary objective of this paper is to establish the benefit of using multiple discretely chosen sampling rates. In the process, we also present the following enabling contributions. 1. We outline the basis for choosing the various sampling rates based on use case analysis. 2. We present an analytical approach which determines

the criteria for switching between sampling rates so that control performance is not hampered. 3. We present the construction of an automaton based scheduler, which implements the switching between sampling rates of the controller. We present the necessary background for the work in Section 2. We outline the methodology of multi-mode sampling period selection in Section 3. Subsequently, we use the control theoretic model of an Anti-lock Braking System (ABS) as a running example to validate our approach.

3. METHODOLOGY OUTLINE
In this section, we outline our proposed methodology of multi-mode sampling period selection for embedded real time control. The main steps of this proposed multi-mode methodology are as follows, · Step I : Developing a control theoretic model of the corresponding system. · Step II : Classification of different modes based on different control parameters and selection of best possible sampling rate for the corresponding operating modes. · Step III : Construction of a supervisory automaton for controlling the mode switching.

2.

BACKGROUND STUDY

In this section, we outline the mathematical relation between the sampling period of a discrete time controller and its control stability. Any discrete time feedback control system can be represented as shown in Fig. 1 [2], here x(t) is the input to the system, e(t) is the error signal, u(t) is the controller output and y (t) is the plant output fed back to the controller using a sensor.

Figure 2: Methodology Outline In the following sections we demonstrate the proposed methodology with an extensive analysis of ABS as a running example. In Section 4, we present the control model of the ABS. In Section 5, we divide the driving pattern of a vehicle into multiple modes parameterized by vehicular characteristics like velocity, brake pedal pressure and slip. For each such mode, we choose a sampling frequency which ensures stability guarantee of the ABS. In Section 6, we outline our approach for guard condition selection for switching between different modes, and synthesize a scheduler automaton which may supervise the mode selection depending on vehicular dynamics. In Section 7, we provide experimental results supporting the proposed approach.

Figure 1: Discrete Time Control System Generally, the sampling of the continuous signal is done at a constant rate T , which is known as the sampling period or interval. The sampled signal ek = e(kT ), is the discretized signal with k  N. In general, for any transfer function, the control signal output depends on n previous control signal output instances and m previous error signal instances [2]. The situation may be represented as, uk = -a1 uk-1 - a2 uk-2 - · · · - an uk-n + b0 ek + (1) b 1 e k -1 + b 2 e k -2 + · · · + b m e k -m The discrete signals uk-1 ,uk-2 , .. are the delayed versions of uk by sampling period T, 2T, .. respectively. In Laplace domain, a time delay is introduced into a signal by multiplying its Laplace transform by the operator e-T s . Let the Laplace domain representation of uk and ek be U (s) and E (s) respectively [7]. Hence, uk-1 , uk-2 , .. can be represented in Laplace frequency domain as e-T s U (s), e-2T s U (s) , .. respectively and similarly for ek-1 , .. . Thus, the corresponding Laplace domain representation of Eq. 1 shall be, U (s) = -a1 e-T s U (s) - a2 e-2T s U (s) - · · · + b0 E (s)+ b1 e-T s E (s) + b2 e-2T s E (s) + . . . (2)

4. CONTROL MODEL
ABS is an automobile safety critical driver assistance system which prevents the wheels from locking and avoids uncontrolled skidding. An abstract block diagram of a vehicle with ABS is shown in Fig. 5. For designing the vehicle we used a simplified quarter car model as shown in Fig. 3. Here, m is the mass of the quarter vehicle, Vx is lateral speed of the vehicle,  is the angular speed of the wheel, FN is the vehicle vertical force, Fx is the frictional force transmitted to the road, Mb is the braking torque, R is the wheel radius and J is the wheel inertia. Wheel slip  is given as,  = 1 - R/Vx . The effective braking force is dependent on the frictional force [1] transmitted to the road which is related to FN as, Fx = -µFN , where µ is the frictional coefficient of the road

Substituting eT s with the discrete frequency domain operator z [2] and simplifying this further we get the discrete time transfer function of the controller C (z ) as, m b0 z n + b1 z n-1 + .. + bm z n-m j =1 (z - zj ) n-m = b z (3) 0 z n + a1 z n-1 + .. + an n i=1 (z - pi ) where zj are the zeros and pi are the poles of the transfer function. Behavior of any discrete time controller can be observed by analyzing the poles and zeros of the corresponding transfer function [2]. The positions of the poles and zeros differ for different sampling intervals (T ). Correspondingly, the control stability of the overall system gets effected. More related background is provided in Appendix A.

Figure 3: 1/4 Car Forces and Torques [1]

1

Frictional Co-efficient

design a discrete PID controller for this purpose as, de dt Ki T Kd (z - 1) Mb (z ) = [Kp + + ]E (z ) z-1 T Mb = Kp e + Ki edt + Kd
1

0.8 0.6 0.4 0.2 0 0 Dry Aspalt Gravel Loose Gravel Wet 0.2 0.4 0.6 0.8

(7)

Slip

Figure 4: µ -  Curve [10]

Here Kp , Ki , Kd are the proportional, integral, and derivative gain respectively of the PID controller. T is the sampling interval. Mb (z ), E (z ) are the discrete domain representation of the the braking torque, Mb and the error signal, e = d - , which is the difference between the desired slip (d ) and actual slip (). Observe that when  = d then  = 0. Substituting this in Eq. 5, Mb can be e = 0, i.e.  represented as, J Mb = [( - 1) - mR]Vx (8) R Hence, it is obvious from Eq. 7 & 8 that the control performance, i.e. stability of the ABS controller will vary for different values of T , Vx and .

Figure 5: ABS Overview [1]

surface. Thus, as evident from Fig. 4, the amount of slip will vary depending on road conditions. Relationship between wheel slip and frictional coefficient can be approximated, µ = f (), using a piecewise linear function [6] as, µ= , 1 -2 +   0.2  > 0.2 (4)

5. MODE AND PERIOD SELECTION
The candidate sampling modes and periods for a controller are determined by partitioning its input space based on the use-case scenarios and stability of the control law in those scenarios. For example, in ABS, the adequacy of a sampling rate in a given scenario depends on the urgency of braking (which is a function of vehicle speed and pedal pressure) and the slip ratio (which is a function of the vehicle speed and friction on the road). In general, we select the different possible sampling frequencies of the controller using the following approach. 1. Identify vehicular parameters which impact the controller output (i.e. braking torque in this case). 2. Identify multiple possible driving scenarios and corresponding ranges of vehicular parameters and also the probable driver response. 3. Perform stability analysis followed by identification of maximal acceptable sampling period in each driving scenario. Steps 2 and 3 may have to be iterated to arrive at a gainful combination of sampling modes. A sampling mode is effective towards gaining computational efficiency only if the controller stays in that mode for a non-trivial period of time. Therefore it is necessary to relate the sampling modes with different use-case scenarios. In ABS, we estimate different possible traffic scenarios (city traffic, suburban or medium traffic & highway traffic) and the variation in traffic density, traffic regulations and corresponding average cruising speed and driver reaction to arrive at the sampling modes. We categorize the brake pedal pressure range as low, mild, medium and high, and also consider various speed ranges and slip ratios. For each of the scenarios, we determine the sampling rate at which the controller is stable. Through this study we selected three sampling modes, as outlined below: · N0 Mode: This mode requires the least sampling rate among the three chosen mode, and targets scenarios where the vehicle is cruising at low to medium speeds (such as in city traffic). Considering average cruising speed and the driver reaction, we arrive at the operating sampling rate

3 4

+ ,

where,   [0, 8] and   [-0.1, 0.1]. The non-linear equations for designing a quarter car model can be given as,
1 Vx = - m FN µ R b   = J FN µ - M J 2  = - 1 [ 1 (1 - ) + R ]FN µ +  Vx m J

(5)
1 R Vx J

Mb

Using Taylor series expansion method [6, 8] for linearizing a nonlinear system we obtain a linear (affine) system description from Eq. 5 as, x  = Al x + El + Bl u y = Cl x + Dl u (6)

where, xT = [Vx , ], u = Mb · Vx , y = [], Al , Bl , Cl , Dl are system input and output matrices respectively. El , l = f (x) are the affine term and function telling the validation of linearizion. The formation of this state space equation from the nonlinear equations are described in details in Appendix B. The objective of ABS controller is to decelerate the vehicle as fast as possible, while maintaining its steer ability by minimizing wheel slip. The main components of ABS are the ABS-ECU, hydraulic modulator, and wheel speed sensor. The ECU constantly monitors the wheel rotational speed through the wheel speed sensors, and also measures the actual slip. The controller's task is to maintain the braking torque within a certain range. The braking force is applied to the wheels by the hydraulic modulator. It rapidly pulses the brakes to prevent wheel lock up, even during panic braking in extreme conditions and promises shortest possible distance under most conditions. We can

Ts = 0.2ms in which the ABS achieves satisfactory control performance as given by Fig. 6. For a given velocity (X axis) and slip (Y axis), we carry out the standard unit circle analysis and plot the maximum magnitude among the different pole positions (Z axis) of the transfer function corresponding to Ts = 0.2ms. The stability variation for different slip values is due to the piecewise linear function given in Eq. 4. It may be observed from Fig. 6, all the poles are of magnitude <= 1 for velocity range [0 . . . 85]km/h and slip range [0 . . . 0.65] thus ensuring stable vehicular dynamics. Thus correspondingly brake pedal pressure variation range is selected as [low, mild, medium] estimating the probability of the above mentioned slip range. We carry out similar analysis for the other cruising modes and derive satisfactory sampling intervals.

Unstable
2

2.5

|Pole|

1.5 1 0.5 0 1 0.8 0.6 200 0.4 0.2 150 100 0 50 0 250

Stable

Slip

Velocity (km/h)

Stable

Figure 8: E Mode Stability Guarantee

6. SUPERVISORY AUTOMATA
Our analysis of the different possible driving scenarios and the choice of sampling periods entails the creation of a scheduler which may dynamically switch the controller among different sampling modes. Such a supervisory automaton is shown in Fig. 9. The criteria for switching between the three

Unstable
2

2.5

|Pole|

1.5 1 0.5 0 1 0.8 0.6 100 120 140

Stable Slip

Figure 9: Scheduler Automata
0.4 0.2 0 0 20 40 60 80

Velocity(km/h)

Stable

Figure 6: N0 Mode Stability Guarantee · N1 Mode: This mode uses higher sampling rate than N0, and targets suburban traffic scenarios. The maximum sampling interval with stability guarantee is found to be Ts = 0.15ms as shown in Fig. 7.

modes of the automaton is chosen based on our observations about vehicular parameters vis-a-vis stability. Let "v " and "bpp" be the velocity and brake pedal pressure respectively. The brake pedal pressure is divided in to the ranges low, mild, medium and high. High brake pedal pressure represents the probability of larger value of slip. Medium, mild and low brake pedal pressure signifies lesser value of slip. The guard conditions for switching between states are given as,
n0 = = = (v  [0 . . . 85] & bpp  [low, mild, medium]) | (v  [85 . . . 140] & bpp  [low, mild]) (v  [0 . . . 80] & bpp  [low, mild, medium]) | (v  80 . . . 135] & bpp  [low, mild]) (v  [0 . . . 85] & bpp  [high]) | (v  [85 . . . 140] & bpp  [medium, high]) | (v  [> 140] & bpp  [low, mild]) (v  [0 . . . 80] & bpp  [high]) | (v  [80 . . . 135] & bpp  [medium, high]) | (v  [> 135] & bpp  [low, mild]) e = (v  [> 140] & bpp  [medium, high])

2.5

Unstable
2

 n 0

|Pole|

1.5 1 0.5

n1

Stable
0 1 0.8 0.6 140 160 180 120

Slip

0.4 0.2 0 0 20 40 60

80

100

 n 1

=

Velocity(km/h)

Stable

Figure 7: N1 Mode Stability Guarantee · E Mode: This mode uses a sampling rate that is adequate in all scenarios. Existing approaches for choosing a uniform sampling mode will choose this sampling rate. We choose the sampling frequency for which the vehicle remains stable considering all velocity and brake pedal pressure variations as shown in Fig. 8. The corresponding sampling period for our model is found to be Ts = 0.1ms. We designate this as emergency sampling mode, in case of any driving irregularity the controller switches to this mode, thus ensuring vehicular stability. Z-domain unit circle stability analysis is used to show the vehicular parameters and stability relationship graphically for extensive parameter ranges. Similar observations can be obtained mathematically using other nonlinear system stability [2] criteria like, Lyapunov, Nyquist, Routh-Hurwitz or Bode plot analysis. The effective braking pressure range is varied in each of these modes to achieve satisfactory performance, depending upon driving scenarios. The mode switching and detailed switching criterion are discussed in the next section.

The guard conditions for switching between `N 0' to `N 1'  and vice versa are selected to be `n1 ' and `n 0 ' respectively. Similarly, for `N 1' to `E ' and vice versa, switching condi tions are `e ' and `n 1 ' respectively. The guard conditions are selected ensuring some amount of hysteresis while mode switching. For example, the automaton switches from mode `N 0' to mode `N 1' in case bpp = high and v  [0 . . . 85]. However, the automaton switches from mode `N 1' to mode `N 0' when v  [0 . . . 80] and bpp is not high. Similarly, the automaton switches from mode "N 1" to E when bpp is medium or high and v  [> 140], however, the automaton switches from mode `E ' to mode `N 1' when v  [> 135] and bpp is low or mild. Whenever the automaton makes a transition from a mode with lower sampling period to a mode with higher sampling period (e.g. E to N 1), it ensures that the guard conditions are valid for a certain prefixed number of clock cycles. In that way, unwanted glitches due to faulty sensor readings are expected to be filtered out.

Magnitude (dB)

The main objective of synthesizing the scheduler automaton was to reduce ECU bandwidth requirement. Scheduling in E mode signifies a sampling periodicity of 0.1ms, while the sampling periodicity of N 0 and N 1 modes are 0.2ms and 0.15ms respectively. If we notice the mode switching scenarios, we observe that when the car is cruising at a certain speed and no brake pedal pressure is applied, the controller is scheduled using infrequent sampling periods (N 0 or N 1). Further, in scenarios when the car is cruising at a certain speed and brake pedal pressure is applied, the mode switching will be supervised by the respective scheduler automaton ensuring that it will switch to a more frequent sampling mode. Thereby, in a general cruising scenario, a multi-mode controller ensures nearly 30% - 50% ECU bandwidth saving as shown in our simulation results.

the stability of the system. We empirically observe the variation of stability with sampling rate. For stability analysis, we used unit circle as well as bode plot analysis [2]. We take a moderate velocity, say V = 60km/h and calculate the stability of the system for different sampling intervals. We show one such example Bode plot for stability analysis in Fig. 11. It may be observed that with the sampling interval Ts = 0.01s, the system is unstable. However, if we drastically reduce the sampling interval to 0.1ms, the system becomes stable. In Appendix C.1 and C.2 more such results are given.
Bode Diagram 1 0 -1 -2 -3 10 0.1ms 0.01s

7.

RESULTS

The initial part of this section is devoted towards establishing the motivation for multi-mode sampling through experimental results. The latter part of the section demonstrates the gain in computational bandwidth and the benefit of effective sharing of computational resources between multiple controllers. We analyze the performance of the ABS for different sampling intervals. Observing the µ vs  curve in Fig. 4 carefully, we notice that the peak point on most of the road scenarios belong to the range [0, 0.2]. Hence, for effective braking with maximum possible road friction, we set d = 0.2 as the desired slip. In our experimental setup, when brake force is applied with current velocity V = 100km/h, it is expected to gradually decrease until V = 0km/h and throughout this deceleration phase the slip value should be as close as possible to the desired slip (d =0.2) thus ensuring smooth braking. Finally the slip value should be 1 (normalized slip) when the vehicle comes to rest.
100 90 80 70 60 Speed Speed 50 40 30 20 10 0 0 2 4 6 8 Time 10 12 14 16 18 Vehicle Speed Wheel Speed 100 90 80 70 60 50 40 30 20 10 0 0 2 4 6 8 Time 10 12 14 16 Vehicle Speed Wheel Speed

Phase (deg)

5 0 -5

System: 0.01s Phase Margin (deg): -180 Delay Margin (samples): Inf At frequency (rad/s): 0 Closed loop stable? No System: 0.1ms Phase Margin (deg): -180 Delay Margin (samples): 2.62 At frequency (rad/s): 1.2e+04 Closed loop stable? Yes 10
-2

-10 -3 10

10 10 Frequency (rad/s)

-1

0

10

1

10

2

Figure 11: Stability Variation:Ts = 0.01s, Unstable & Ts = 0.1ms, Stable We have employed our methodology of multi-mode sampling period selection for the ABS running example. We consider a braking scenario with the initial and final speed being 200km/h and 0km/h respectively and compare the minimum stopping distance achieved by our `Multi-mode' ABS Controller with the existing Matlab model of ABS with fixed periodicity. The simulation results are provided in Table 1 considering different possible road surfaces. We obTable 1: Stopping Distance in Kilometer
ABS Controller Existing model Multi-mode Dry Asphalt 3.073 3.080 Road Surface Gravel Loose Gravel 3.424 3.771 3.433 3.795 Wet 4.269 4.289

Slip 1 Normalized Relative Slip 0.8 0.6 0.4 0.2 0 Normalized Relative Slip 1 0.8 0.6 0.4 0.2 0

Slip

0

5

10 Time

15

20

0

2

4

6

8 Time

10

12

14

16

Figure 10: Slip Variation: the Left & Right column figures correspond to sampling time Ts1 = 1s & sampling time Ts1 = 0.01s respectively. We observe that the slip varies significantly for two different sampling rates as given by Fig. 10. Observe from the left column in Fig. 10 that with a choice of moderate sampling rate, the slip varies drastically thus leading to undesired perturbations in vehicular speed while braking. However, as shown in the right column of the same figure, with the higher sampling rate, the slip exhibits a well damped trajectory around the desired value (d =0.2) leading to smoother vehicular deceleration. Our notion of control performance is based on ensuring

serve that the stopping distance is nearly same for both the controllers. For this `panic braking' scenario, the estimated percentage of time spent in each of E , N 1 and N 0 modes is highlighted in Fig. 12. It is evident from Fig. 12 that we can save significant amount of ECU bandwidth (30% - 50%) using a multi-mode controller as compared to the controller with fixed periodicity.

100 90

B/W Requirement (%)

80 70 60 50 40 30 20 10 0 0
Multi-mode (Wet Road) Multi-mode (Loose Gravel Road) Multi-mode (Gravel Road) Multi-mode (Dry Road) Existing (All Road)

200

400

600

800

1000

Time (sec)

Figure 12: ECU, Panic Braking Scenario

Further, we investigate the utility of our multi-mode ABS controller in a general cruising scenario where the car is being driven in a speed range of 0km/h to 200km/h, in various traffic densities. The ECU bandwidth requirement for this scenario is shown in Fig. 13. It is evident that the multimode controller requires much lesser bandwidth compared to the existing controller since the supervisory automaton schedules the controller in the infrequent sampling modes for most of the time as described in Section 6. In that way, we can guarantee a significant amount of bandwidth saving which may be utilized for scheduling other tasks.
100

100

B/W Requirement (%)

80 60 40 20 0 0

ACC ABS

200

400

600 800 Time (sec)

1000

1200

Figure 15: ECU Sharing: ACC-ABS

8. CONCLUSIONS
The present work provides a methodology for adaptively regulating the sampling rate of embedded software based controllers leading to significant reduction of computational resource requirement. Applying the methodology on single controller based systems like ABS has shown that 30% - 50% reduction in ECU bandwidth requirement is possible. Further, it was also shown that the method smoothly scales up for multiple controller based systems. Our future research shall focus on giving a sound formal underpinning to the method of creating multiple sampling modes for software based controllers and creating a tool flow which mechanizes the synthesis of such multi-mode controllers.

B/W Requirement (%)

80

60

40

20

Multi-mode Existing
500 1000 1500 2000

0 0

Time (sec)

Figure 13: ECU, General Cruising Scenario We further demonstrate the promise of multi-mode sampling for a multiple ECU scenario taking an Adaptive Cruise Control (ACC) system as a running example. ACC is an automobile safety critical driver assistance system which automatically adjusts the vehicle speed in order to maintain a safe distance from vehicles ahead. In case of an ACC system, the driver sets a safe cruising speed and a desired safe distance (from preceding vehicle) as controller inputs. The other inputs of an ACC controller which comes from the radar sensor are preceding vehicle speed and approximate distance from preceding vehicle as shown in Fig. 14.

9. REFERENCES
[1] Safety, Comfort and Convenience Systems. Robert Bosch GmbH, 2006. [2] K. ° Astr¨ om and B. Wittenmark. Computer controlled systems: theory and design. Prentice Hall, 1984. [3] G. Buttazzo. Research trends in real-time computing for embedded systems. SIGBED Rev., 3(3). [4] A. Cervin, M. Velasco, P. Marti, and A. Camacho. Optimal online sampling period assignment: Theory and experiments. IEEE Transactions on Control Systems Technology, 19(4), 2011. [5] D. Henriksson and A. Cervin. Optimal on-line sampling period assignment for real-time control tasks based on plant state information. In 44th IEEE CDC-ECC, 2005. [6] H. Khalil. Nonlinear systems. Macmillan Pub.Co., 1992. [7] B. Kuo. Automatic control systems. Prentice Hall, 1962. [8] M. Schinkel and K. Hunt. Anti-lock braking control using a sliding mode like approach. In Proceedings of the ACC,, volume 3, 2002. [9] D. Seto, J. Lehoczky, L. Sha, and K. Shin. On task schedulability in real-time control systems. In 17th IEEE RTSS, 1996. [10] J. Wong. Theory of Ground Vehicles. Wiley, 2001.

Figure 14: ACC System Overview [1] ACC is a drive assist system for highway cruising which monitors the inputs and decides cruising speed or distance to lead vehicle. When the applied brake pedal pressure is high, the ACC is overridden by the braking controller (ABS). The operating modes of ACC controller can be classified into `active', `suspended' and `idle' while our multi-mode ABS operates in three different modes as discussed previously. Generally, the ACC system and the braking controller are mapped to separate ECUs. We can achieve significant reduction in bandwidth requirement by sharing an ECU between these features. When the ACC is suspended, because the applied brake pedal pressure is high, we schedule the ABS controller with frequent sampling rate and the ACC controller with relatively infrequent sampling rates. On the other hand, when the ACC is active, i.e. applied brake pedal pressure is low or null, then the ACC controller is scheduled with frequent sampling rate and the ABS controller is scheduled with relatively infrequent sampling rate. In Fig. 15, we show the bandwidth requirement of both ACC and ABS controllers when scheduled in a single ECU while providing satisfactory control performance.

APPENDIX A. BASIC CONTROL SYSTEM
Any continuous feedback control system[7] can be represented as shown in Fig. 16, where x(t) is the input to the system, e(t) is the error signal, u(t) is the controller output and y (t) is the plant output fed back to the controller using a sensor(H). Correspondingly, a Discrete time Feedback

A.1 Stability: Discrete Time Control System
The stability property of this system can be defined from the impulse response of a system as · Asymptotic stable system: The steady state impulse response is zero. lim y (k) = 0
k

· Marginally stable system: The steady state impulse response is different from zero, but limited. lim 0 < y (k) < 
k

· Unstable system: The steady state impulse response is unlimited. lim y (k) = 
k

Figure 16: Continuous Time Control System Control System can be represented as shown in Fig. 17, where the dotted box highlights the discretized controller [2]. Generally, the sampling of the continuous signal is done

where y (k) is the impulse response of the corresponding system. The impulse response for different stability property is illustrated in Fig. 18. Let us assume a control system with
Stable System
2 Magnitude 1.5 1 0.5 0 0 50 100 150 Time 200 250 300

Marginally Stable System
1 Magnitude 0.5 0 -0.5 -1 0 x 10
25

Figure 17: Discrete Time Control System at a constant rate T , which is known as the sampling period or interval. The sampled signal yk = y (kT ) is the discretized signal with k  N. For understanding the design of a discretized control system, let us first consider a simple continuous domain transfer function for the controller C in Fig. 16 given as follows. C (s ) =
du dt U (s) E (s)

50

100 Time

150

200

250

Unstable System

1 Magnitude

0

-1

=

K (s+a) (s+b)

0

50

100

150 TIme

200

250

300

The corresponding time domain representation shall be, + bu = K ( de + ae) dt The Euler's approximation of the first order derivative is represented as: dx xk+1 - xk  (9) dt t Applying Euler's approximation of first order derivative [2] on the continuous differential equation, we get the following discrete difference equation.
uk+1 -uk t

Figure 18: Impulse Response and Stability input u and output y . The transfer function of any discrete time control system can be represented as C (z ) =
y (z ) u (z )

=

bz ( z - p)

where p is the pole which is in general a complex number and can be written in polar from as p = mej where m is the magnitude and  is the phase. The impulse response of the system can be given as } = b|m|k ejk y (k) = Z -1 { zbz -p Thus, it is the magnitude m which determines if the steady state impulse response converges towards zero or not. The relationship between stability and pole placement can be stated as follows. · Asymptotic stable system: All poles lie inside (none is on) the unit circle, or what is the same: all poles have magnitude less than 1. · Marginally stable system: One or more poles but no multiple poles are on the unit circle. · Unstable system: At least one pole is outside the unit circle. The situation is graphically shown in Fig. 19.

+ buk = K (

ek+1 -ek t

+ aek )

Generally, t, K, a and b are fixed. The digital controller updates the control signal every cycle as per the following equation, uk+1 = -a1 uk + b0 ek+1 + b1 ek (10) where, b0 = K , b1 = K (at - 1), a1 = (bt - 1). In general, for any transfer function, the control signal output depends on n previous control signal output instances and m previous error signal instances which can be represented as[2], uk = -a1 uk-1 - a2 uk-2 - · · · - an uk-n + b0 ek + b 1 e k -1 + b 2 e k -2 + · · · + b m e k -m (11)

The discrete signals uk-1 ,uk-2 , . . . are the delayed versions of uk by sampling period T, 2T,. . . respectively.

wheel slip and frictional coefficient can be approximated by using piecewise linear function as, µ() = , -1 + 2
3 4

+ ,

  0.2  > 0.2

(12)

where,   [4.8, 5.1, 5.46, 6.4] and   [-0.1, 0.1]. Al = 0
2  NR   FV 2 x J N - F m F N R2  V  J x

(13)

Figure 19: Unit Circle: Stability areas in Complex Plane

El = and for  > 0.2 Al = 0

0
R2   - F N J Vx 

(14)

B.

QUARTER VEHICLE MODELING

A quarter car model is shown in figure 20. Here, m is the

FN R +3 ) FN R ± 0.1 V (-  2 J 2 4 V 2 J 
x x



2

2

FN 4m F N R2 J 4Vx 

(15)

El =

(

 2

3 N ± 0.1) F (- 4 m 2 F R2 3 NR ± 0 .2 F -2 ) VN J V  J 
x x

(16)

The stability analysis of this system can be done using Lyapunov, Bode, Nyquist, Unit Circle or Hurwitz stability criteria [8].

C.
Figure 20: 1/4 car forces and torques mass of the quarter vehicle, Vx is lateral speed of the vehicle,  is the angular speed of the wheel, FN is the vehicle vertical force, Fx is tire frictional force, Mb is the braking torque, R is the wheel radius and J is the wheel inertia. Wheel slip  is represented as: =1-
R Vx

STABILITY V/S SAMPLING PERIOD

In this section we provide few examples relating to the variation in stability with respect to sampling periods.

C.1 Simple Examples:
Let the transfer function of any arbitrary system be, U (s ) =
s+0.5 ms2 +bs+u

The relationship between FN and Fx [1] is given as: Fx = -µ()FN where µ() is the frictional coefficient of the road surface. The non-linear equations for designing a quarter car model can be given as[8]:
1 Vx = - m FN µ() R b   = J FN µ() - M J x (1-)-R V   = 

Case 1: Let m=2, b=-0.5, u=1. For Ts =2s the system unstable but for Ts =1s the system is stable. The values of m, b, u are same for both of the sampling intervals. The corresponding stability response is shown in Fig. 21.
Bode Diagram 10 5 0 -5 -10 -15 -135 Phase (deg)
System: Ts_1 Gain Margin (dB): 11.8 At frequency (rad/s): 3.14 Closed loop stable? Yes

 = 

1 -V [ 1 (1 x m

- ) +

Vx R2 ]FN µ() J

+

1 R Vx J

Mb

Magnitude (dB)

Ts_2 Ts_1

-180 -225 -270 -315 -360 -2 10
-1 0 1

For linearizing the system approximation using the Taylor  series expansion can be expressed as [6]: f (, Vx )  f ( , Vx ) d f d f    ( -  ) + + d | ,Vx |   (V - Vx ). From this from dV  ,Vx this we can derive a linear (affine) system description as: x  = Al x + El + Bl u y = Cl x + Dl u l = f (x ) where, Al , Bl , Cl , Dl are system input and output matrices respectively. El are the affine terms and f (x) is the function telling the validity of linearizion and xT = [Vx , ] , u = Mb · Vx , y = []. From Fig. 4 the relationship between

System: Ts_1 Phase Margin (deg): -132 Delay Margin (samples): 9.04 At frequency (rad/s): 0.44 Closed loop stable? Yes

System: Ts_2 Phase Margin (deg): -12.2 Delay Margin (samples): 3.12 At frequency (rad/s): 0.973 Closed loop stable? No

10

10 Frequency (rad/s)

10

Figure 21: Ts = 2s, Unstable; Ts = 1s, Stable Case 2: Let m=5, b=1, u=10. In this case the system stable for both the sampling intervals Ts =2s and Ts =1s. The values of m, b, u are same for both of the sampling intervals. The corresponding stability response is shown in Fig. 22.

Bode Diagram 0 -5 Magnitude (dB) -10 -15 -20 -25 -30 45 0 Phase (deg) -45 -90 -135 -180 -2 10
-1 0 1

Ts_2 Ts_1

System: Ts_2 Gain Margin (dB): 2.43 At frequency (rad/s): 1.57 Closed loop stable? Yes

Case 3: For V = 40 Km/h,  = 0.2, the stability response is shown in Fig. 25.
Bode Diagram 0.5

Magnitude (dB)

System: Ts_1 Gain Margin (dB): 18.3 At frequency (rad/s): 3.14 Closed loop stable? Yes

0 -0.5 -1 -1.5 5

10

10 Frequency (rad/s)

10

Phase (deg)

System: T_N Phase Margin (deg): -179 Delay Margin (samples): 9.58 At frequency (rad/s): 33.1 Closed loop stable? No

0
System: T_E Phase Margin (deg): -180 Delay Margin (samples): 6.72 At frequency (rad/s): 9.38e+03 Closed loop stable? Yes

Figure 22: Ts = 2s, and Ts = 1s, Both Stable

C.2

ABS example

-5 -2 10

10

-1

10 10 Frequency (rad/s)

0

1

10

2

10

3

We provide more examples of stabiliy variation with sampling interval for the ABS controller model. Case 1: For V = 10 Km/h,  = 0.1, the stability response is shown in Fig. 23.
Bode Diagram 0.2

Figure 25: Ts = 0.6 ms, Unstable; Ts = 0.3 ms, Stable Case 4: For V = 15 Km/h,  = 0.3, the stability response is shown in Fig. 26.
Bode Diagram 0.4 T_E T_N

Magnitude (dB)

0.1 0 -0.1 -0.2 -0.3 1 Magnitude (dB) 0.2 0 -0.2 -0.4 -0.6 2 Phase (deg)
System: T_N Phase Margin (deg): -180 Delay Margin (samples): Inf At frequency (rad/s): 0 Closed loop stable? Yes

Phase (deg)

0.5 0 -0.5

1 0 -1 -2 -2 10

System: T_N Phase Margin (deg): -179 Delay Margin (samples): 1.39 At frequency (rad/s): 2.27e+03 Closed loop stable? Yes

T_N -1 -2 10 10
-1

System: T_E Phase Margin (deg): -180 Delay Margin (samples): 2.43 At frequency (rad/s): 2.6e+03 Closed loop stable? Yes

10

0

10 Frequency (rad/s)

1

10

2

10

3

10

4

10

-1

10

0

10 Frequency (rad/s)

1

10

2

10

3

10

4

Figure 23: Ts = 0.6 ms, Stable Case 2: For V = 100 Km/h,  = 0.6, the stability response is shown in Fig. 24.
Bode Diagram 0 -200 -400 -600 -800 180 Phase (deg) 0 -180 -360 -540 -3 10
System: T_E Phase Margin (deg): -180 Delay Margin (samples): 3.34 At frequency (rad/s): 1.88e+04 Closed loop stable? Yes System: T_N Gain Margin (dB): 635 At frequency (rad/s): 108 Closed loop stable? No

Figure 26: Ts = 0.6 ms, and Ts = 0.1 ms, Both Stable From these examples we can observe that sampling period has a major role to play in deciding the stability of a software based controller.

Magnitude (dB)

10

-2

10

-1

10 Frequency (rad/s)

0

10

1

10

2

10

3

Figure 24: Ts = 0.6 ms, Unstable; Ts = 0.1 ms, Stable

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/262602090

AlgorithmsforRotationSymmetricBoolean Functions
Article·September2012

CITATIONS

READS

0
4authors,including: SatrajitGhosh AcharyaPradullaChandraCollege,KOLKATA,I...
13PUBLICATIONS2CITATIONS
SEEPROFILE

79

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyParthasarathiDasguptaon28September2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

INDIAN INSTITUTE OF MANAGEMENT CALCUTTA

WORKING PAPER SERIES

WPS No. 713/ September 2012

Algorithms for Rotation Symmetric Boolean Functions

by

Subrata Das Assistant Professor, Department of Information Technology, Academy of Technology, West Bengal

Satrajit Ghosh Assistant Professor, Department of Computer Science, APC College, West Bengal

Parthasarathi Dasgupta Professor, IIM Calcutta, Diamond Harbour Road, Joka, Kolkata 700104, India

&

Samar Sensarma Professor, Department of Computer Science & Engineering University of Calcutta

Algorithms for Rotation Symmetric Boolean Functions
Subrata Das1 , Satrajit Ghosh2 , Parthasarathi Dasgupta3 , and Samar Sensarma4
Department of Information Technology Academy of Technology dsubrata.mt@gmail.com 2 Department of Computer Science APC College gsatrajit@gmail.com 3 Management Information Systems Group Indian Institute of Management Calcutta partha@iimcal.ac.in Department of Computer Science & Engineering University of Calcutta sssarma2010@gmail.com
1

4

Abstract. Rotation Symmetric Boolean functions (RSBF) are of immense importance as building blocks of cryptosystems. This class of Boolean functions are invariant under circular translation of indices. It is known that, for n-variable RSBF functions, the associated set of input bit strings can be divided into a number of subsets (called partitions or orbits), where every element of a subset can be obtained by simply rotating the string of bits of some other element of the same subset. In this paper we propose algorithms for the generation of these partitions of RSBF s and implement for up to 26 variables. Keywords: Algorithms,cryptography, Symmetric boolean functions,Rotation symmetric boolean functions

1

Introduction

n A Boolean function f n (xn-1 , xn-2 , ..., x0 ) of n variables is a mapping from F2 n to F2 ,where F2 is a n-dimensional vector space over the two-element field F2 . A Boolean function is symmetric if and only if it is invariant under any permutation of its variables [1]. Rotation Symmetric Boolean Functions(RSBFs) are a special class of Boolean function for which the function value will be same for any rotation of its variables [12]. For secret key cryptosystems, balancedness,nonlinearity,correlation immunity,algebraic degree [11] are the different criteria for choosing a Boolean Function for cryptographic applications. RSBFs have good combination of these properties [6],[4]. It is clear to see that there are n 22 boolean functions on n variables, and thus, searching for all these functions

2

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

exhaustively is exponential.Thus, in order to look for RSBFs, it is imperative to have an idea about the number of orbits(i.e. partitions) in rotation symmetric functions. In this paper, we propose three simple algorithms for generating RSBF s of a given number of variables and implement upto 26 variables. Rest of the paper is organized as follows.Section 2 reviews some related recent works. Section 3 introduces some terminologies to be used in subsequent discussions and Section 4 proposes three algorithms A,B and C for generating RSBF s of n variables. Section 5 briefly discusses the implementation of the algorithms A,B and C . Finally, Section 6 concludes the chapter and briefly states the future scopes of work.

2

Literature Review

An extensive study of symmetric Boolean functions, especially of their cryptographic properties has been done in [8]. In [2] Pieprzyk and Qu have studied a special type of symmetric Boolean functions, called rotation symmetric Boolean function.In that paper the authors have suggested that RSBF can be applied as round functions of a hashing algorithm such as MD5. Rotation symmetric Boolean functions (RSBF) are of great research interest for theoreticians as well as for practitioners in the field of cryptography [3]. Any symmetric boolean function (w.r.t. any permutation) is also rotation symmetric, but the converse is not always true [2]. In [4] the authors discuss some new results on rotation symmetric correlation immune(CI) and bent functions and important data structures for efficient search strategy of these functions. They also proved the non existence of the homogeneous rotation symmetric bent functions of degree  3 on a single cycle. In [5] the authors have proposed an efficient implementation of search strategy for rotation symmetric boolean function. In [6] theoretical construction of rotation symmetric boolean functions on odd number of variables with maximum possible algebraic immunity are discussed.The investigation of balanced RSBFs and 1st order correlation immune RSBFs and enumeration formula for n-variable balanced RSBFs where n is a power of prime reported in [7].

3

Preliminaries

A of n variables Boolean function f (xn-1 , xn-2 , ..., x0 ) is said to be Rotation Symmetric if f (xn-1 , xn-2 ,. . .,x0 )=f (xn-2 , xn-2 ,. . .,x0 ,xn-1 )=....=f (x0 , x1 ,. . .,xn-1 ). Thus, a Rotation Symmetric Boolean function remains invariant under cyclic permutation of its variables. The cyclic permutation may be in either left-to-right or in right-to-left order of the variables. Now, for a function f (xn-1 , xn-2 ,. . .,x0 ) of n variables, the number of possible strings of elements is 2n . From the above definition of RSBF, the value of f (xn-1 , xn-2 ,. . .,x0 ) will be the same for a set of input strings of elements,where any input string is obtained by cyclic permutation of some other input string. A set of strings of elements 0 and 1 of n variables which are rotation symmetric is

Algorithms for Rotation Symmetric Boolean Functions

3

said to form an orbit. Figure 1 illustrates the set of 14 orbits for a function of 6 variables.
{(000000)} {(000001) (000010) {(000011) (000110) {(000101) (001010) {(000111) (001110) {(001001) (010010) {(001011) (010110) {(001101) (011010) {(001111) (011110) {(010101) (101010)} {(010111) (101110) {(011011) (110110) {(011111) (111110) {(111111)} orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit orbit 0 1 2 3 4 5 6 7 8 9 10 11 12 13

(000100) (001100) (010100) (011100) (100100)} (101100) (110100) (111100)

(001000) (011000) (101000) (111000) (011001) (101001) (111001)

(010000) (110000) (010001) (110001) (110010) (010011) (110011) (110101) (110111)

(100000)} (100001)} (100010)} (100011)} (100101)} (100110)} (100111)} (101011)} (101111)}

(011101) (111010) (101101)} (111101) (111011)

Fig. 1. Orbits of RSBF for n = 6

Each orbit is also known as a partition. Number of partitions gn satisfies the inequality 2n - 2 gn  2 + (1) n where the equality holds when n is prime[3]. We now introduce a few definitions for the subsequent discussions. Definition 1. Hamming distance d(s1 , s2 ) between two binary strings s1 and s2 is the number of positions where the bit values of s1 and s2 differ. Definition 2. If a boolean function f (xn-1 , xn-2 ,. . .,x0 ) exhibits rotation symmetry, then the period over which it exhibits this property is defined to be the cycle length for the function. Consider a rotation symmetric boolean function whose associated binary strings are {(1011), (0111), (1110), (1101)} obtained through all possible cyclic permutation of the bits containing 3 ones and 1 zero.The RSBF f (x3 , x2 , x1 , x0 ) must have the same value for all the permutations. Cycle length of this function is then 4. Definition 3. For a n-variable RSBF containing gn orbits,the orbits which have maximum cycle length i.e. cycle length n are known as long cycles. If the cycle length is some factor of n then it is known as a short cycle. For instance, in Figure 1, the RSBFs corresponding to orbits 1, 2, 3, 4, 6, 7, 8, 10 and 12 are all long cycles, whereas those corresponding to orbits 0, 5, 9, 11, and 13 are all short cycles. The following observation is then clear:

4

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

Observation 1 For a RSBF of n variables, n > 1, the two trivial orbits containing all zeros and all ones respectively are always short cycles. Lemma 1 If n is prime, then n and an integer.
n r

can be expressed as the product of the number

Proof. Let us assume the result is wrong.So if n is prime, then n r can be exn-1)! n n! pressed as the product of the number n and a fraction. r = (n-r)!×r! =n × (n(- r )!×r ! . n n It is quite trivial that r yields an integer. So to get the value of r as integer (n-1)! n-1)! either the denominator of (n(- r )!×r ! is n or the value of (n-r )!×r ! is an integer.Since n is a prime number so it can not be expressed as product of two or more numbers other than 1 and the number n itself.So the denominator of (n-1)! (n-1)! (n-r )!×r ! i.e. (n - r)! × r! will never be n.Hence it is a contradiction. So (n-r )!×r ! is not a fraction. Lemma 2 For a RSBF of n variables, if n is prime, then there does not exist any orbit of cycle length less than n except for the two trivial orbits containing all zeros and all ones. Proof. Let r denotes the number of 1s in the binary string of n bits. Then, total number of binary strings that can be formed with n bits having r ones is n r . Since n is prime, n can be expressed as the product of the number n and an r integer (by Lemma 1). Moreover, the cycle length of the RSBF cannot be greater than n. Thus, every cycle that can be generated for the RSBF are of full cycle length except for the two trivial orbits containing all zeroes and all ones. Lemma 1 and Lemma 2 clearly indicates that for a composite value of n, the different strings generated for the RSBF will have some short cycles. A closer look at the Boolean strings corresponding to RSBF yields the following observation: Observation 2 If the cycle length of an orbit for a n-variable RSBF is less than n, then there always exists a substring within the corresponding binary strings which is also repeated within every string of the orbit. The length of this repeating substring is defined to be the internal period of the orbit. In Figure 1, for example, for orbit 5, cycle length is 3, and the substring {001} is repeated three times in every element of the orbit, and the internal period is also 3. A formal definition of internal period is given below: Definition 4. For a n-variable binary string corresponding to a RSBF,where n is not prime if d be a divisor of n, then a substring of d bits of the n- variable string is said to exhibit periodicity d if (an-1 , . . ., an-d ) = (an-d-1 , . . ., an-2d ) = . . . = (ad-1 , . . ., a0 ). d is defined to be the internal period of these substrings. For example in Figure 1 orbit 5 is {(001001)(010010)(100100)},where substring (001) has periodicity 3. The following observation follows from the Definition 4.

Algorithms for Rotation Symmetric Boolean Functions

5

Observation 3 Product of the internal period and the number of substrings within a string yields the number of variables of the string. For instance, in Figure 1, for Orbit 9, number of substrings is 3,internal period is 2 and the number of variables is 6.

3.1

Algebraic Normal forms and RSBF

The classical approach to the analysis, synthesis or testing of a switching circuit is based on the description by the Boolean algebra operators. A description of a switching circuit based on Modulo-2 arithmetic (the simplest case of the Galois field algebra [10]) is inherently redundancy-free, and is implemented as the multi-level tree of XOR (addition operator over GF (2)) gates. Definition 5. An n-variable Boolean function f (xn-1 , . . ., x1 , x0 ) can be expressed as a multivariate polynomial over GF (2). More precisely, f (xn-1 , . . ., n 1i<j n i x1 , x0 ) can be written as : f (xn-1 , . . ., x1 , x0 ) = a0 =1 ai xi aij xi xj . . . a1,2,...,n , where the coefficients a0 , ai , aij , . . ., a1,2,...,n  {0, 1} and xi  {0, 1}. This representation of f (xn-1 , . . ., x1 , x0 ) is known as its Algebraic normal form (ANF) or its Positive Polarity Reed-Muller form (PPRM). The number of variables in the highest order product term with non-zero coefficient of an AN F is its algebraic degree. A Boolean function is defined to be homogeneous if all terms of its AN F are of the same degree. Definition 6. A homogeneous function of degree one is called a linear function. Definition 7. A Boolean function of degree at most one is called an affine function. Definition 8. The non-linearity N Lf of a Boolean function f (xn-1 , xn-2 , .., x0 ) is the minimum number of truth table entries that must be changed in order to convert f (xn-1 , xn-2 , .., x0 ) to an affine function. Non-linearity of a Boolean function f (xn-1 , xn-2 , .., x0 ) may be measured as the minimum Hamming distance between truth tables of f (xn-1 , xn-2 , .., x0 ) and its corresponding (same degree) affine function [9]. For a Boolean function f (xn-1 , xn-2 , .., x0 ) of n variables, there are 2n different input values corresponding to the function. From the definition of RSBF, the function has the same value corresponding to each of the subsets generated from the rotational symmetry. Each of these subsets is called a partition. In the following Section, we propose three algorithms for the generation of partitions for rotational symmetry for a given number n variables. For each partition (orbit), we select the first element of the partition as its representative element.

6

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

4

Proposed Algorithms

The proposed algorithm starts with a string of n zeros, which forms the first orbit. Subsequent representative strings are formed by the odd numbers whose binary representation has r initial zeros starting with most significant bit, followed by (r + 1)th bit as 1 and least significant bit as 1. Left rotation of these starting (representative) strings by up to r bits yields an even number. Further left rotation may yield again an odd number. Lemma 3 If a n bit odd number (d) consists of 1st r bits as zeros ,starting with th most significant bit,the (r + 1) bit as one, and the least significant bit as one, then d × 2r+1 > 2n , where n > r  1. Proof. From mathematical inequality, we have 2n-r-1 < (2n-r -1). Now, (2n-r - 1) is the maximum odd number (dmax ) of (n - r) bits. Prefixing r zeros to the left of binary form of dmax also yields dmax . Thus, 2n-r-1 < dmax , i.e., dmax ×2r+1 > 2n . Moreover, the minimum odd number of (n - r) bits is (2n-r-1 + 1). Let dmin denotes this number. It can be easily shown that dmin × 2r+1 > 2n . Thus, all odd numbers between dmin and dmax satisfy the same inequality constraint. Consider an odd number whose binary representation has r initial zeros starting with most significant bit, followed by (r + 1)th bit as 1 and least significant bit as 1. Left rotation of these starting (representative) strings by up to r bits yields an even number. Thus, from Lemma 3, we find that for the representative string s of an orbit, beyond left rotation of a number by r bits starting with s, the number obtained is not necessarily an odd number. Value of this bit string is obtained from Lemma 3. An AV L tree is used to store the Decimal values of all odd numbers from this position up to n - 1 rotations for avoiding possible repetition in future orbit generations. A formal description of the proposed Algorithm A is given in Figure 2. The following result clearly follows from the description of the proposed Algorithm A. Lemma 4 Worst-case time complexity of Algorithm A is 2n . Lemma 5 The proposed Algorithm A requires a maximum space of 2n-1 - gn . Proof. For an n-variable string, the corresponding decimal numbers range ben tween 0 and 2n - 1. Of these, the number of odd decimal values is 22 . We store only odd numbers in the AV L tree. Thus, the total storage requirement n -2 is 2n-1 - gn . Total number of orbits for an n-variable RSBF is gn  2 + 2 n . Since nodes are gradually deleted from the AV L tree, we have space requirement  2n-1 - gn . The sequence of outputs generated by the proposed Algorithm A is illustrated with an example below. Consider n = 5. Thus, the maximum value of the corresponding decimal number is 31. Orbit 1: All zeros.

Algorithms for Rotation Symmetric Boolean Functions

7

Algorithm A Data structures: Cntr :# of orbits, Result[]: starting string of orbit, T : AVL tree Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2. Take the string of all zeros as Orbit 0; store its decimal form in Result[]; 3. Cntr = Cntr + 1; 4. Take the string of first n - 1 zero bits and last bit One as Orbit 1; store its decimal form in Result[]; 5. while last < 2n do 6. Take the binary string s corresponding to next odd number 7. if s  T then 8. store decimal form d of s in Result[]; 9. Cntr = Cntr + 1; 10. while s does not reappear do 11. bit-wise rotate the binary representation of s to generate a set of strings S = {sk |k = 1, n - 1} 12. if d × 2k > 2n then store decimal form of sk in T if sk is odd, where k = r + 1 (by Lemma 3) 13. endwhile 14. endwhile 15. end Fig. 2. Algorithm A for generating orbits for n-variable RSBF

Orbit 2: Starting string must have 4 zeros, and a one, which is decimal 1 {i.e.04 1}. Now, check for some k for which 1 × 2k > 31, k = 1, . . . 4. No such value of k exists. Orbit 3: Starting bit string of this orbit must be the next odd number. Since any rotation of 1 in the starting string of Orbit 2 yields an even number, the starting string of this orbit must have three consecutive zeros, followed by two ones (decimal 3). The condition 3 × 2k > 31, k = 1, . . . , 4 is satisfied for k = 4. Rotate the starting string four times to obtain 10001, having decimal value 17. Since decimal value of this string is odd, it is saved in the AV L tree ( Figure 3(a)). Orbit 4: Starting bit string of this orbit must be the next odd number (after starting string of previous orbit), having decimal value 5, i.e., two zeros followed by 101. For k = 3, 5 × 2k > 31. Rotate 00011 three and four times respectively to yield 01001 (=9) and 10010 (=18). Since the number 9 is odd, it is saved in the AV L tree (Figure 3(b)). Orbit 5: Starting bit string of this orbit must be the next odd number 7 (after 5), i.e., two zeros followed by 111. For k = 3, 7 × 2k > 31. Rotate 00111 three and four times respectively to yield 11001 (=25) and 10011 (=19). Since both these numbers are odd, they are saved in the AV L tree (see (Figure 3 (c)). Orbit 6: Starting bit string of this orbit must be the next odd number after 7 which is not already in the AV L tree. Since the next odd number 9 is already in the AV L tree, the starting string of this Orbit would be 01011 (=11). The

8

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

9 17

17

25 19

Delete 9 17 19

(a)

9 (b)

9

25

17

25

(d) (c) 19

21,13 Delete 13 19 19 (29) 17 25 17 13 21 (f ) (e) ( g) 21 21 29 25 17 25 19 29,27,23

19 25 (23) 17 25 19 29 19 29 25

21 (27) 27

29

17

21

27

17

21

27

(g)

(i)

23

Fig. 3. An example execution of Algorithm A

Algorithms for Rotation Symmetric Boolean Functions

9

number 9 is deleted from the AV L tree ( Figure 3(d)) For k = 2, 11 × 2k > 31. Rotate 01101 (=13) two to four times to yield respectively 01101 (=13), 11010 (=26), and 10101 (=21). The odd values 13 and 21 are stored in the AV L tree (Figure 3(e)). Orbit 7: Starting bit string of this orbit must be the next odd number after 11 which is not already in the AV L tree. Since the next odd number 13 is already in the AV L tree, the starting string of this Orbit would be 01111 (=15). The number 13 is deleted from the AV L tree ( Figure 3(f)) For k = 2, 15 × 2k > 31. Rotate 01111 (=15) two to four times to yield respectively 11101 (=29), 11011 (=27), and 10111 (=23). The odd values 29, 27 and 23 are stored in the AV L tree ( Figure 3(g)(h)(i)). Orbit 8: Starting bit string of this orbit must be the next odd number after 15 which is not already in the AV L tree. Since the next few odd numbers 17, 19, 21, 23, 25, 27, and 29 are already in the AV L tree, these numbers are successively deleted from the AV L tree (see Figure 3g). Thus, the string for this Orbit 8 is 31. The algorithm terminates successfully at this point. Figure 3 illustrates some of the steps using the AV L tree during execution of Algorithm A with an example. Let the symbols RR and LR respectively denote right and left rotations of a bit string by one bit position. For a string of bits s, we define its rotation cousin to be the bit string obtained on application of LR or RR on s. The following result is used to design an improved algorithm. Lemma 6 Right rotation of each of the bit strings corresponding to two consecutive odd integers by one bit yields two consecutive integers. Proof. Let two successive n-bit binary odd numbers be represented as {an-1 , an-2 , . . . , a1 , a0 }2 and {bn-1 , bn-2 , . . . , b1 , b0 }2 and {an-1 , an-2 , . . . , a1 , a0 } + 2 = {bn-1 , bn-2 , . . . , b1 , b0 }. Now RR {bn-1 , bn-2 , . . . , b1 , b0 } by 1 bit yields {b0 , bn-1 , bn-2 , . . . , b1 }. =b0 2n-1 + (bn-1 2n-2 + bn-2 2n-3 + ... + b1 20 ) n -1 n -2 ...+b1 21 ) 22 =b0 2n-1 + (bn-1 2 +bn- 2
n -1

+bn-2 2n-2 ...+b1 21 )+b0 -b0 2 n -1 +bn-2 2n-2 ...+b1 21 +b0 )-b0 n-1 (bn-1 2 =b0 2 + 2 n -1 n -2 1 =b0 2n-1 + (an-1 2 +an-2 2 2 ...+a1 2 +a0 )+2-b0 n -1 n -2 1 =a0 2n-1 + (an-1 2 +an-2 2 2 ...+a1 2 +a0 )+2-b0 [a0 n -2 n -3 ...+a1 20 )×2+2+a0 -b0 =a0 2n-1 + (an-1 2 +an-2 2 2 =a0 2n-1 +(an-1 2n-2 + an-2 2n-3 ... + a1 20 ) + 1

=b0 2n-1 + (bn-1 2

= b0 , since both are odd numbers]

={a0 an-1 , an-2 , . . . , a1 }2 + 1 . Let {an-1 , an-2 , . . . , a1 , a0 } represent a n-bit string, where ai represents a bit, i = 0, . . . , n - 1. Then Lemma 6 may be expressed as {{an-1 , an-2 , . . . , a1 , a0 } RR 1} = {{an-1 , an-2 , . . . , a1 , a0 } + 2 } RR 1.

10

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

4.1

An improved Algorithm

The proposed Algorithm A is improved with a minor modification. We note the following observation: Consider the bit string (for an odd number) having 1 at the right-most position{0n-1 1}. Let this bit string be right-rotated right by 1-bit (i.e., n-bits left-rotate) to form a new bit string P , say P = {11 0n-1 }. The starting bit-string of each orbit is an odd number, generated by simply adding 2 to the starting string of the previous orbit. In the previous algorithm, we had to check all these starting strings in an AV L tree to avoid repetitive occurrences of numbers. Lemma 7 If the starting string of an orbit is P + 1, then the numbers between P + 1 and the number generated by one-bit right-rotation of (P - 1) may be ignored for generating subsequent orbits. Proof. From Algorithm A it is clear that the starting string of the orbits must be odd number.From the previous lemma 6 it is clear that the last rotation cousins are the consecutive numbers if starting strings of the orbits are consecutive odd numbers.When some number misses in the orbit then its corresponding last rotation cousin do no appear.So as soon as the starting string of the orbit becomes P +1 which comes already as the last cousin of some orbit we do not cosier from this to the last cousin of P - 1. The above lemma shows that the n-bit left-rotation (= 1-bit right rotation) of successive odd numbers results in successive numbers. Thus, whenever the odd number becomes (P + 1), all the successive numbers up to the number which is RR of (P - 1) already appears. A formal description of the proposed Algorithm B is given in Figure 4. Following result is clear from the description Algorithm B . Lemma 8 Worst-case time complexity of Algorithm B is 2n . Lemma 9 The proposed Algorithm B requires a maximum space of 2n-1 -gn -a, where a = RR(2n-1 + 1) - (2n-1 + 1). Proof. Follows from Lemma 6. 4.2 A further improved Algorithm

In both the algorithms proposed above, an AV L tree is used to reduce certain iterations. The following observation helps in getting rid of this auxiliary data structure and its associated operations. Observation 4 If the rotation cousin of an odd starting number of an orbit is also odd, and is greater than the value of the next starting string of the next orbit, then this starting string may be discarded. A formal description of Algorithm C is given in Figure 5. Following result is clear from the description Algorithm C .

Algorithms for Rotation Symmetric Boolean Functions

11

Algorithm B Data structures: Cntr :# of orbits, Result[]: starting string of orbit, T : AV L tree Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2.Store the decimal value of 1 0n-1 to some variable P 3. Do not store the value from P to RR of P - 1 in T 4. Take the string of all zeros as Orbit 0; 5. store its decimal form in Result[]; 6. Cntr = Cntr + 1; 7. Take the string of first n - 1 zero bits and last bit One as Orbit 1; 8. store its decimal form in Result[]; 9. while last < 2n do 10. Take the binary string s corresponding to next odd number 11. if s  T then 12. store decimal form d of s in Result[]; 13. Cntr = Cntr + 1; 14. while s does not reappear do 15. bit-wise rotate the binary representation of s to generate a set of strings S = {sk |k = 1, n - 1} 16. if d × 2k > 2n then store decimal form of sk in T if it is odd, where k = r + 1 (by Lemma 3) 17. if d  P then d=RR of P - 1 18. endwhile 19. endwhile 20. end Fig. 4. Improved algorithm B for generating orbits for n-variable RSBF

Lemma 10 Worst-case time complexity of Algorithm C is 2n . The space requirement for Algorithm C is much less than those of the previous algorithms, as it does not use the AV L tree. The proposed Algorithm C is illustrated with an example below. Consider n = 7. Thus, the maximum value of the corresponding decimal number is 31. Orbit 1: All zeros. Orbit 2: Starting string will be {06 1} as none of the rotational cousins of 6 {0 1} is of smaller value. Orbit 3: Starting bit string of this orbit must be the next odd number, i.e., the bit-string {05 12 }, as none of the rotational cousins of {06 1} is smaller than {05 12 }. Orbit 4: Starting bit string of this orbit must be the next odd number (= 5), i.e., {04 101} as none of the rotational cousins of {05 12 } is smaller than {04 101}. Continuing in this manner, we find that up to Orbit 9 the consecutive odd numbers form the starting strings of the respective orbits. Thus, the starting string of Orbit 9 is {03 14 }. The next odd number is 17, i.e., {02 103 1}. We observe that 3rd left-rotate cousin of {02 103 1} is {03 102 1} (=9), which is less than 17. Thus, we discard the number 17, and do not consider it as the starting number of any orbit. For the next consecutive odd numbers between 19 nd 23, none of them

12

Subrata Das, Satrajit Ghosh, Parthasarathi Dasgupta, and Samar Sensarma

Algorithm C Data structures: Cntr :# of orbits, Result[]: starting string of orbit Input: Number of bits Output: Starting string of every orbit and total number of orbits 1. Initialize Cntr = 0; 2. Take the string of all zeros as Orbit 0; store its decimal form in Result[]; 3. Cntr = Cntr + 1; 4. Consider the bit-string s corresponding to the next odd number; store its decimal form in Result[]; 5. while bit-string corresponding to s = {1n } do 6. if a number p has any of its rotational cousins pc < p then goto Step 8 7. take bit-string corresponding to s as the starting string of the next orbit 8. consider the bit string corresponding to the next odd number 9. endwhile 10. end Fig. 5. Improved algorithm (without AV L tree) for generating orbits for n-variable RSBF

have any smaller left-rotate cousin, and hence, they form the starting numbers for the next three successive orbits. The algorithm continues in this manner, and terminates successfully at Orbit 20.

5

Implementation of the Proposed Algorithms

The proposed algorithms have the same worst-case time complexities, and exhibit drastic improvement in space requirements from Algorithm A through Algorithm C . Algorithms A and B use the AV L tree, while the other two do not require this data structure. Algorithm A was implemented in C language on a 32-bit PC running on Intel CPU under Windows environment having 2.27 GHz clock speed. It could generate RSBF s up to 26 bits in reasonable time.

6

Conclusion

In this work, we investigated rotation symmetric Boolean functions. Our major contributions include designing of certain efficient algorithms for generation of RSBF s. Future possibilities of work include (i) possible reducing of the time complexities, (ii) generating RSBF s with more than 30 variables, and (iii) generating cryptographically better RSBF s. The proposed algorithms are of great importance as they can help to construct new cryptographically very strong boolean functions.

References
1. Zvi Kohavi, Switching and Finite Automata Theory, 2nd Edition, Tata-McGraw Hill, 2008.

Algorithms for Rotation Symmetric Boolean Functions

13

2. J. Pieprzyk and C. X. Qu, Fast hashing and Rotatio-symmetric functions, Journal of Universal Computer Science, pp. 20-31, vol. 5, no. 1, 1999. 3. P. Stanica and S. Maitra, Rotation Symmetric Boolean functions - Count and Cryptographic properties, Discrete Applied Mathematics, vol. 156, no. 10, May, 2008. 4. P. Stanica and S. Maitra and J A Clark, Results on Rotation symmetric Bent and Correlation immune Boolean functions in B. Roy and W. Meier (Eds.), FSE 2004, LNCS 3017, pp. 161-177, 2004, International Assoc. for Cryptologic Research. 5. M.Hell, A. Maximov,and S. Maitra, On efficient implementation of search stratgey for RSBF, 9th International workshop on Algebraic and Combinatorila coding theory, ACCT 2004, June 19-25, 2004, Bulgaria. 6. P.Sarkar and S. Maitra, Construction of rotation symmetric boolean function with important cryptographic properties,Advances in cryptology,EUROCRYPT2000,Lecture notes in Computer Science,Vol-1807,Berlin 2000,PP 485-506. 7. ShaoJing Fu, Chao Li and LongJiang Qu, On the number of rotation symmetric Boolean functions, Science China Information Sciences March 2010, vol. 53, no 3, pp. 537?545. 8. A Canteaut and M Videau, Symmetric Boolean functions, IEEE Transactions on Information Theory, vol. 51, no. 8, pp. 2791-2811, Aug 2005. 9. J Butler and T Sasao, Logic functions for cryptography - A tutorial, Proceedings of Reed-Muller Workshop, Okinawa, Japan, 2010, pp. 127-136. 10. N. Deo, Graph Theory with Applications to Engineering and Computer Science, PHI Learning Pvt. Ltd., 2004. 11. A menezes, p Van Oorschot,S Vanstone, Handbook of applied cryptography,1997,CRC Press,Inc. 12. X Zhang, Hua Guo,Yifa Li,Proof of a Cojecture about Rotation Symmetric Functions.

View publication stats

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

6483

Filter Design of Direct Matrix Converter for Synchronous Applications
Anindya Dasgupta and Parthasarathi Sensarma, Member, IEEE
Abstract--Filters for switching ripple attenuation are essential at the input, and sometimes at the output for certain applications, for the deployment of matrix converters (MCs). Due to the absence of inertial elements in the MC structure and the consequent tight input­output coupling, the filter parameters significantly affect its dynamic behavior. This paper presents an exhaustive filter design method for synchronous applications of the MC in power systems. Apart from the usual considerations of ripple attenuation, voltage regulation, reactive current loading, and internal losses, this paper also addresses additional constraints which may be imposed by requirements of dynamic performance and reliable commutation. Rigorous analytical justification of each design step is provided and the sequential design process is summarized. Relevant experimental results are presented to validate the proposed design tool. Index Terms--Filter design, matrix converter (MC), synchronous applications.

I. I NTRODUCTION ECENT YEARS have witnessed a growing interest [1]­ [7] toward the deployment of the matrix converter (MC) in power systems, which are synchronous applications characterized by identical input and output frequencies. The advantage of reduced energy storage needs has inspired the use of MC as a FACTS device [1]. The use of MC for power quality improvement, reactive power control, and other power system applications has been reported in [2]­[4]. Benefits of using MC as a voltage regulator in a distribution system and as a high-performance power supply have been reported in [5]­[7], respectively. The input filter is an essential requirement of this topology for providing a local circulating path to the switching frequency current. Some of the synchronous applications require regulated output voltage and, hence, a second-order ripple filter at the output side, in addition to the input filter. The filter parameters have to be carefully selected such that their inclusion does not degrade voltage regulation, efficiency, and reactive current loading beyond an acceptable limit. Since the

R

Manuscript received April 19, 2013; revised August 19, 2013 and December 2, 2013; accepted February 14, 2014. Date of publication April 14, 2014; date of current version September 12, 2014. This work was supported in part by the NaMPET initiative of DeitY under Grant CDAC/EE/20050026 and in part by the Department of Science and Technology, Government of India, under sponsored project DST/EE/20110021. A. Dasgupta is with the Department of Avionics, Indian Institute of Space Science and Technology, Thiruvananthapuram 695-547, India (e-mail: anindyadgupta@iist.ac.in; anindyadgupta@gmail.com). P. Sensarma is with the Department of Electrical Engineering, Indian Institute of Technology Kanpur, Kanpur 208-016, India (e-mail: sensarma@ iitk.ac.in). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TIE.2014.2317134

only inertial elements in this topology are the filter components, the filter parameters significantly affect the system dynamics. Input filter design has been discussed in [8]­[13], where, from cost and weight considerations, the single-stage LC filter has been found to be the most appropriate topology. Although the set of filter elements (Lf , Cf ) for a particular resonant frequency is infinite, Klumpner et al. [9] advocate a maximum Cf to ensure a minimum input displacement factor (IDF) for low loads. However, in a distribution system, most of the loads are inductive, and this restriction may be relaxed at lighter loads. An exhaustive treatment of input/output filters with focus on reducing electromagnetic interference and common-mode voltages has been provided in [14] and [15]. However, most of these have not investigated the comprehensive design of filters in the context of dynamic performance improvement or reliability of commutation hardware. The only reported approach [13], which considers both steady-state and dynamic objectives, uses a genetic algorithm to derive the parameter values as well as the filter topology. Dynamic stability criteria are based on limits detailed in [16] and [17]. Digital filters are used for measuring input voltage which are purported to improve the system stability limit. It has, however, been shown in [18] that, with a proper choice of system input and output, the derived plant has minimum phase poles for all operating points, but nonminimum phase zeros could appear, depending on input filter parameters and system operating points. Since the plant is inherently stable, imposing a general stability limit on filter design for all operating conditions [13] is unnecessary. Also, analytical justification of the filter design method is not established with any degree of rigor. Output voltage regulation specifications restrict the inductive (series) component of the output filter. Hence, commutation based on output current direction becomes particularly difficult at zero crossing (ZC) due to the high ripple content. The detection of current direction based on switch voltage measurement introduces large delays because of the stray capacitances and large resistances used [19]­[21]. An analysis of the critical window area around the ZC of line voltages, for safe voltagebased commutation (VBC), has been provided in [22]. However, the effect of switching frequency ripple in input voltage on the demarcation of the critical areas has been ignored. In [21], it is shown that, with proper zero vector placement in space vector modulation (SVM), safe commutation can be achieved in spite of voltage measurement inaccuracies. This method is restricted to the operation with the input displacement angle within ± of the voltage ZC, which is to be decided on the basis of the specific input ripple voltage measurements for a given hardware. A closed-form expression of the ripple voltage

0278-0046 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

6484

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE I N OMINAL /R ATED PARAMETERS

Fig. 2.

Single-phase diagram including source inductance.

Fig. 1. Schematic of a 3 Ph MC.

Fig. 3. |iin (nb t)|/|iin (b t)| for (a) m = 1 and (b) m = 0.5.

would therefore provide a complete analytical design tool for this method. For wider control of IDF in applications [23], [24] requiring input reactive power control, the correct measurement of the ripple voltage is still difficult, particularly with high output current amplitude. This paper discusses a filter design approach for the MC which integrates the steady-state performance along with the constraints imposed by the requirements of closed-loop dynamic performance and safe commutation. In the first section, the design of filters to meet certain steady-state performance specifications is detailed. Since modulation strategies for MC do not provide any separate control on the input current amplitude, an external damping resistor is included. Power loss estimation due to this inclusion is presented. Input capacitor sizing for reliable commutation is analyzed in the context of the voltage ripple and consequent problems in VBC. Then, the effect of the grid inductance has been presented. Thereafter, the output filter design is presented in the second section. In the next section, the design guidelines that emerge from the analysis have been summarized to enable a sequential design process. Finally, relevant experimental results on a 6-kVA laboratory prototype are provided for validation. II. I NPUT F ILTER D ESIGN Filter parameters are chosen for the nominal/rated system parameters shown in Table I. In this analysis, an application requiring regulated 3 Ph sine-wave voltage supply has been considered which makes output ripple filters mandatory. Fig. 1 shows the schematic diagram of a 3 Ph MC. The single-phase equivalent of the overall system is shown in Fig. 2 where the Thevenin impedance of the source is modeled as an inductance Ls . The input filter damping resistor is placed across the filter inductor as the conflict between filter efficiency and damping requirements is least with this arrangement [25]. The power stage of MC appears as a current stiff sink to the supply side and as a voltage stiff supply to the load. A phase-locked loop is locked to phase a of the point of common coupling marked as P. Ls , which is usually small at the

distribution level, is initially neglected and would be introduced later to study its impact on filter performance. The selection of filter parameters to satisfy certain criteria are sequentially described in the following sections. A. Attenuation to Switching Ripple and Low-Order Harmonics For Ls = 0, the forward gain of the input filter is defined as Gf v (s) =


is ( s ) iin (s)

=
vs

vcf (s) v s (s )

=
iin

+1 s Rf d s2 Lf Cf + s Rf +1 d
L

L

.

(1)

Denoting the corner frequency as c : (2fc ), the normalized form of its frequency response magnitude is |Gf v (j )| = 1+
2 r Q2 2 )2 + (1 - r 2 r Q2

(2)

which is plotted in Fig. 4. The definitions r =  , c Q = Rd Cf , Lf c = 1 Lf C f (3)

enable generalized analysis. The ratio of the magnitude of the switching ripple components to the fundamental component in iin varies with modulation index m [26]. Fig. 3 shows the magnitude of these component as a fraction of the fundamental obtained from simulation. To restrict these switching ripple components from appearing in is , the first design criterion is set as Spec.1 |Gf v (j 2fs )|  Asw dB. Also, in the deployment site of the MC, the grid voltage spectrum may contain significant low-order (hv ) harmonics. To limit the appearance of these in the filter capacitor voltage vcf , the following restriction is introduced: Spec.2 |Gf v (j 2fb max .(hv ))|  Avh dB. Defining the following frequency ratios: mf hb = max .(hv ) and mf sh = s mf hb b (4)

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6485

TABLE III Lf,max AND Cf,max F ROM R EGULATION AND R EACTIVE L OADING

Fig. 4. |Gf v (j )|(dB) with Q = 1, 3, and 5. TABLE II Q AND fc F ROM F ORWARD G AIN Fig. 5. (a) |GR1 (j )|(dB) for Rd = 1 . (b) |GR2 (j )|(dB).

C. Selection of Damping Resistor Rd Since an external damping resistor Rd is indispensable in the input filter, minimum operating efficiency requires a limit on the maximum loss associated with Rd . Denoting the current through Rd as iRd , the corresponding design criterion is 2 Spec.5 3Rd  n=1 [iRd (jnb )] < 1% of rated load. iRd can be expressed as iRd (s) = GR1 (s)vs (s) + GR2 (s)iin (s) where GR1 (s) =


it is noted that, for given switching frequency and deployment site conditions, these ratios have unique values. These are depicted in Fig. 4 by the vertical lines which are mutually stationary but could slide horizontally. Also included are the criteria defined in Spec.1 and Spec.2, which are depicted as regions demarcated by stationary horizontal lines. The interval [rv , rs ] defines the initial choice of Q (Q(1) ) and, thereby, the (1) (1) acceptable range of fc ([fc,min , fc,max ]). For the experimental model and deployment site of MC, these selections, along with the numerical limits of Spec.1 and Spec.2, are listed in Table II. B. Voltage Regulation and Reactive Current Loading Referring to Fig. 2, the full load regulation of MC output voltage is decided by the fundamental voltage drop in Lf . Restriction on regulation leads to the following design criterion: Spec.3 VLf  b Lf
2 + I2 Icf in,rated  kR Vs .

(6)

iR d ( s ) v s (s )

=
s=j

1 · Rd

2 r 2 )2 + (1 - r
2 r Q2

GR2 (s) =



iR d ( s ) iin (s)

=
s=j

1 · Q

r
2 )2 + (1 - r
2 r Q2

.

(7)

A restriction on reactive current loading is necessary for satisfying stipulations on part-load power factor, which leads to the next criterion Spec.4 Icf  b Cf Vs  kPF Iin,rated . For deriving the boundary values at rated power level, analysis is done considering unity IDF operation with unity modulation index (m) [27] and resistive load unless stated otherwise. The specific inequalities in Spec.3 and Spec.4 obviously translate to (1) (1) maximum limits for Lf (Lf,max ) and Cf (Cf,max ). The rated input current is evaluated from [27] Iin,max = 0.866Io . (5)

The frequency responses of these transfer functions are plotted in Fig. 5(a) and (b), respectively, for Rd = 1 . High Q is particularly desirable in |GR2 | as it implies lesser ripple content in iRd and, therefore, a lower loss. However, high Q in |GR1 | implies a higher loss due to increased low-order harmonics in iRd . The plots reveal a response similar to that encountered with the normalized form of |Gf v | in Fig. 4. As can be expected from this similarity, Q(1) chosen in Section II-A is found to give an even balance between both the conflicting aspects. Therefore, Q(1) is finalized for filter design. The power loss associated with Rd is PR = 3Rd
n 2 2 + |GR2 (jnb )|2 Iin,n |GR1 (jnb )|2 Vs,n

For the rated values in Table I, numerical limits considered for Spec.3 and Spec.4, along with the maximum allowable values (1) (1) Lf,max and Cf,max , are listed in Table III.

(8) where Vs,n and Iin,n are the values corresponding to the nth harmonic order. The calculation of PR using (8) requires the

6486

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE IV H ARMONICS IN iin F ROM S IMULATION AT m = 1

TABLE V Lf,min AND Cf,min F ROM R EGULATION , R EACTIVE L OADING , AND F IXED fc

Fig. 6.

Commutation between input phases a and b.

Fig. 7.

Input current and voltage.

prior selection of fc for evaluating the gains |GR1 (jnb )| and |GR2 (jnb )|. For the experimental model, fc is selected from the allowable range provided in Table II as
(1) = 1 kHz. fc

of intersection, the numerical limits of all or some of Spec.3, Spec.4, and Spec.5 must be made less stringent. For the experimental model, these ranges of Rd are listed in Table V. Since these ranges intersect, (13) defines the final set for selecting Rd . D. Lower Limit of Cf : Maximum Ripple in Input Line­Line Voltage and Consequent Problem in Commutation Fig. 6(a) shows two input phases a and b, which are alternatively switched to an output phase. Commutation based on the polarity of input line­line voltage (vL = vab ) is difficult at its ZC, due to the presence of switching ripple component vrL , as shown in Fig. 6(b). In the following analysis, it is assumed that only fundamental and switching frequency components are present in the input current and filter capacitor voltages. vL can be represented as vL = (vf a - vf b ) + (vra - vrb )
vf L vrL

(9)

1) Rd From Loss Limit: With the harmonic analysis of MC being outside the scope of this paper, the expression for losses is derived assuming sinusoidal io . The harmonic components in (1) iin obtained from simulation are listed in Table IV. Using fc (1) and Q , the gains defined in (7) are evaluated. Subsequently, using these gains and the harmonics listed in Table IV in (8), for the nominal parameters in Table I, PR is obtained in the following form: PR = F1 + F2 R d Rd (10)

(14)

where F1 and F2 are functions of |GR1 (jb )|, Vs , |GR2 (jnb )|, and Iin,n . These are tabulated in Table V. Thereafter, applying the numerical limits of Spec.5, (10) yields two roots which define an allowable range of Rd as follows: Rd,min  Rd  Rd,max .
(1) (1)

(11)

where the subscripts f and r have been used to denote the fundamental and ripple components, respectively. Fig. 7 shows the input current and filter capacitors. It is assumed that the ripple component irx completely circulates through the filter capacitor while if x flows through the source. Hence, vrL = vra - vrb = - 1 Cf (ira - irb ) dt. (15)

This choice of Rd definitely satisfies Spec.1, Spec.2, and Spec.5. However, it is not apparent whether any value of Rd in this band and the corresponding Lf and Cf evaluated using (3) would also satisfy Spec.3 and Spec.4. 2) Rd From Voltage Regulation and Reactive Loading Lim(1) (1) its: The upper bounds Cf,max and Lf,max imply corresponding lower bounds Lf,min and Cf,min for the selected fc in (9). These are listed in Table V. Using (3), since
(1) (1) Q Rd = 2fc Lf . (1) (1) (1) (1)

(12)

Lf,max and Lf,min define a new set of bounds on Rd Rd,min  Rd  Rd,max .
(2) (2)

(13)

Hence, the allowable values of Rd will be those where the two ranges obtained from (11) and (13) intersect. In the absence

irx is synthesized from the output current envelope, which is a direct function of the load. Therefore, an arbitrarily small Cf , chosen to reduce reactive loading at low loads, leads to a corresponding high vrL at high loads. With the usual delays involved, the phase error in the measurement of vrL is significantly more than that of vf L . The consequent inaccuracy in detecting the polarity of vrL around the ZC of vf L increases the chance of a commutation failure. Therefore, the estimation of maximum vrL and analyzing its effect on commutation form a prerequisite for determining the lower limit of Cf . Deriving a closed-form expression for maximum vrL is described in Appendix A. Incorrectly sensing the polarity of vf L may lead to erroneous commutation in the following manner. Fig. 8 shows the plot of ripple current and vrL around ^rL,max . This occurs during the instant when v ^rL reaches v commutation from phase a to phase b. The capacitor voltages

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6487

Fig. 10. Path of short circuit current.

Fig. 8.

Error in measuring vrL around its maximum value.

[22]. The instantaneous values of the fundamental component of the voltages of phases a and b are   3^ 3^ vf a  Vcf , vf b  0 f L = Vcf . (17) 2 2 Since vf L has a substantially high magnitude and delay in sensing it is much lesser than the ripple component, it is justified to assume that its polarity is correctly detected. Also, since vrs is positive, polarity of vL is interpreted to be positive. Now, using (16) and (17)  ^o Ts I 3 ^ Vcf - vL = . (18) 2 4Cf

Fig. 9. Instantaneous positions of if a , if b , vf a , and vf b at beginning of commutation from phase a to phase b.

Therefore, vL becomes negative if ^ ^cf < Io Ts V 4Cf  Cf,min =
(2)

are sampled at frequency 4fs . vrs is the corresponding sensed ripple voltage available for sampling. Noise filtering in voltage measurement and the antialiasing filters invariably cause delays in the sensed signal. The presented plot of vrs is derived using a first-order low-pass approximation of the measurement circuit and filter. At the sampling instant just before commutation, vrs is positive, and vrL is negative with a magnitude Vm . Using (43)  3 ^ I o Ts . Vm  (16) 8Cf At the instant when commutation is about to begin, ¯ Iin is very close to the middle of sector II of the input current hexagon shown in Fig. 17(a) in Appendix A. The 3 Ph waveforms shown in Fig. 9 have been used to distinguish between the position of different variables on the basis of their instantaneous phases, at the start of commutation. The intersection of the vertical line L1 with the three waveforms therefore indicates the position of if a , if b , and if c , respectively, at the start of commutation. At the same instant, the relative position of the fundamental components of input voltages depends on the IDF. Let the intersection of the line L2 with the waveforms indicate the positions of vf a , vf b , and vf c , respectively. Therefore, the input currents lead the voltages, a situation which can arise in applications requiring input reactive power control [23], [24]. From Fig. 9, since vf a and vf b have clearly distinguished instantaneous values, the commutation between these two phases is to be interpreted as an "uncritical" one [22]. For an uncritical commutation, regular two- or four-step commutation methods are suggested [21],

^o Ts I I o Ts  . ^ 4Vs 4Vcf

(19)

Fig. 10 shows the condition that will arise in this situation. Short circuit current isc flows over an interval Tsc , which comprises two turn-on and turnoff times of the insulated gate bipolar transistor (IGBT). Lst in Fig. 10 represents the stray inductance of the circuit. Let vD be the total forward drop of the two IGBTs ^D as the maximum current and diodes. Thereby, by denoting I rating of the devices, the criterion for the safety of the devices is obtained as Isc,max 1 = Lst
Tsc

^D . (-vL - vD ) dt  I
0

(20)

Using (18) in (20), a lower limit of Cf is obtained as
(3) Cf,min

1^ = I o Ts 4

^cf V

^D Lst I + 1.15 vD + Tsc
(2)

-1

.

(21)

Cf,min is, of course, hardware specific. Cf,min may be used as a preliminary check before proceeding with the hardware details. If the converter is operated at unity IDF, then, for maximum vrL to occur at the instant of commutation, vf L has to be zero. The line voltage magnitude vL is then equal to Vm defined in (16). Substituting -vL accordingly in (20) yields another lower limit of Cf as
(4) Cf,min

(3)

3^ I o Ts = 8



^D Lst I vD + Tsc

-1

.

(22)

6488

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE VI Cf,min FOR R ELIABLE VOLTAGE C OMMUTATION

the harmonic performance, further complicates the controller design as discussed subsequently. F. Design Modifications for a Nonminimum Phase Plant The dynamic model of 3 Ph MC in the dq reference frame has been analyzed in [18] where a condition for avoiding right half zeros (RHZs) in the plant transfer function matrix has been described. For the per-phase system depicted in Fig. 2, the input filter output impedance Zso is defined as Zso (s) = - vcf (s) iin (s) .
vs (s)=0

(26)

This scalar transfer function is transformed to a transfer matrix Zso (s) in a synchronous dq reference frame as Zso (s) =
Fig. 11. Magnitude plot of Gf v (s) for Ls of 0.5 and 1 mH.

Zso11 (s) -Zso12 (s)

Zso12 (s) . Zso11 (s)

(27)

For avoiding RHZs, it is necessary that
(3)

Cf,min would be higher than both Cf,min and Cf,min . For unity IDF, the commutation through a third phase [21] would allow using a smaller capacitor. However, operating with a (4) value higher than Cf,min will ensure safe commutation for any operating condition. Moreover, this is ensured irrespective of any particular commutation method and is not dependent on the accurate ZC detection of ripple voltage. (2) (3) (4) Table VI shows Cf,min , Cf,min , and Cf,min , calculated using tabulated device ratings and measured Lst . E. Effect of Ls With the inclusion of Ls , Gf v (s) is obtained as Gf v (s) = +1 s Rf d s3
Ls Lf C f Rd L

(4)

(2)

|Zso11 (jM )| <

^ 2 cos2 i 3V cf . 2 Pin

(28)

+ s2 Cf (Ls + Lf ) + s Rf +1 d

L

.

(23)

Zso11 (j ) has multiple phase crossovers, and M is that phase crossover frequency where |Zso11 (j )| is maximum. Pin is the power at the input terminals of MC, and cos i is the IDF. Importantly, the condition defined in (28) is not confined to any specific modeling approach. The power stage of MC, along with input filters in the small signal/linearized model, is responsible for RHZs. From (28), input filter parameters are one of the factors deciding the emergence of RHZs. The violation of (28) causes four RHZs to emerge in the system, which severely complicates the controller design, particularly if the bandwidth (BW ) requirement is high. Then, either the dynamic specifications have to be relaxed or the input filter has to be designed such that (28) is not violated. With reference to (28) and [18] |Zso11 (jM )|  Zso11 (j )


Under the assumption of underdamped second-order filter Gf v (s)  +1 s Rf d
n s Rf +1 d n+1 L s2 2 c L

 Zso (j )

.

(29)

+

s  c Qc

(24) +1

Assuming underdamped second-order filter and ideal passive elements Zso (j )


where c  1 , Q  Rd (n +1)Lf Cf Cf Lf (1+ n)1.5 , n = Ls . Lf (25)

= |Zso (jc )|  Rd 1 +

Ls Lf

2

.

(30)

Fig. 11 shows the gain plot for varying Ls . Input filter parameters are used from Table VIII. Therefore, as Ls increases, c falls while Q increases. Thus, the gain to lower harmonics is bound to increase. However, the losses are going to decrease as fs components of iin now see a higher impedance parallel to Cf . Hence, knowledge of Ls is necessary before finalizing on the input filter parameters. If n defined is (25) is less than 0.5, then c does not get significantly affected, and neither does the filter performance. However, a high n, apart from deteriorating

So as Ls increases, chances of the plant migrating from minimum to nonminimum phase also increase. Under these circumstances a lower Rd or higher Lf can be chosen to ensure that (28) is satisfied and the plant remains minimum phase. Either of these options degrade the steady-state performance in different ways. Higher voltage drop across Lf lowers the maximum output voltage that can be obtained. A lower Rd on the other hand increases losses. So, for a weak grid, a tradeoff is necessary between the dynamic performance and either filter losses or full-load regulation. The inclusion of the internal resistances (ri ) of passives at M results in higher damping owing to the skin effect and, hence, lower |Zso (jM )| than what is represented by (30).

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6489

TABLE VII B OUNDARY VALUES FOR PARAMETERS OF O UTPUT F ILTER PARAMETERS

Therefore, to avoid a conservative design, using the measured value of ri or a good design estimate of it, to calculate |Zso (jM )|, is necessary. III. O UTPUT F ILTER The criteria defined in Spec.1 to Spec.4 are also followed for output filter design. The procedure, being very similar to the earlier exercise, is therefore briefly discussed. Denoting the resonant frequency of the output filter shown in Fig. 2 as co , the forward gain is obtained as Gvof (s) = 1 vco (s) = 2 von (s) (s/co ) + (s/Qo co ) + 1 (31)

conforming to Spec.1 and Spec.2. As discussed in Section II-C, Q(1) is the optimum value and, therefore, the chosen value for the design. Step 2) Selection of fc from the allowable range obtained using the forward gain plot, Q(1) , Spec.1, and Spec.2 (1) (1) Step 3) Computation of the upper limits Lf,max and Cf,max by using Spec.3 and Spec.4, respectively. Subsequently, these values, along with the selected fc , (1) are used to compute the corresponding Cf,min and Lf,min , respectively. Step 4) Computation of the first allowable set of Rd conforming to Spec.5 and the second set that conforms to Spec.3 and Spec.4 as discussed in Section II-C2. The intersection of these two sets forms the final allowable range of Rd . If they do not intersect, then either one or both of the sets has to be expanded by relaxing the corresponding Spec. to obtain a region of overlap. The first set can be expanded at the expense of allowing higher losses while relaxing the upper and lower bounds of the second set implies poor regulation and higher reactive loading, respectively, than the imposed limits. In this regard, the steady-state requirements for the target application help in deciding which Spec. should be relaxed. This implies starting again from the step related to the redefined Spec. (4) Step 5) Computation of Cf,min for reliable voltage commutation using (19). This has to be treated as the absolute lower limit for Cf if this value is greater (1) than Cf,min . Step 6) Selection of Lf from the interval [Lf,min , Lf,max ]. Choosing Cf from the interval [Cf,min , Lf,max ] or
(4) (1) (1) (1) (1)

where Qo = Ro (Co /Lo ). Since von can be controlled, a virtual damping resistor can be introduced through control to emulate Ro . The selection of the values of Qo , fco,max , fco,min , Co,max , and Lo,max is carried out in the same manner followed during the input filter design. For the input filter, the output current decides the ripple current rating of the filter capacitors. At the output side, however, the proper sizing of Lo allows an additional freedom of using capacitors with a lower ripple current rating. This is clarified using the expression of output admittance, which is obtained as Ymo (s) = 1 + s (Qo /co ) 1 io ( s ) = . von (s) Ro (s/co )2 + (s/Qo co ) + 1 (32)

Thus, for Qo greater than 1 1 |Ymo (js )|  .  s Lo (33)

In SVM [27], at any instant, three output phases are connected to any two input phases. Hence, from Fig. 1, for output phase A v ^on = 2 v ^ab = 1.63Vs . 3 (34)

The resulting peak switching ripple, found using (33) and (34), ^r,rated ) must be lower than the peak ripple current rating (I of Co . Hence ^r,rated ). Spec.6 Lo,min = (1.63Vs,rated )/(s I Table VII lists the described limits for output filter parameters calculated using the nominal values in Table I. IV. S ELECTION OF PARAMETER VALUES The design guidelines from the previous sections are summarized here to facilitate a step-by-step design process. With the nominal power, fundamental and switching frequency, and percentages of different harmonics in the source voltage as design inputs, parameters are selected in the following manner. Step 1) Selection of quality factor Q (= Q(1) ) from the normalized form of the input filter forward gain

from [Cf,min , Lf,max ] as discussed in Step 5. Also, finalizing the value of Rd from the range obtained from Step 4. Step 7) Detecting whether the chosen input filter parameters result in a nonminimum phase plant. If it does, then the feasibility of obtaining a stable closed-loop system in compliance to the dynamic specifications has to be investigated. If a minimum phase plant is desired, then either the full-load regulation (Spec.3) or filter loss (Spec.5) has to be relaxed, and parameters have to be redesigned repeating Steps 3 to 7. Step 8) Selecting Qo , fco , Co,max , and Lo,max following the same steps as their input-side counterparts. Step 9) Selection of Lo,min conforming to Spec.6. Subsequently choosing the values of Lo and Co , thereby completing the design process. The filter parameters are selected following the described steps and using the boundary values listed in Tables II, III, V­VII. The parameter values chosen in this paper are shown in Table VIII, which also lists the specific volume (cm3 /kW) and weight (kg/kW) of the inductors and capacitors. Experimental results are discussed next.

(1)

(1)

6490

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE VIII I NPUT AND O UTPUT F ILTER PARAMETERS

Fig. 13. Open-loop experimental results. vs : 100 V/div, is : 5 A/div, vco : 100 V/div, and io : 5 A/div. Time: 10 ms/div.

Fig. 14. Open-loop results with an Ls of 1 mH. vs : 100 V/div, is : 5 A/div, vco : 100 V/div, and io : 5 A/div. Time: 10 ms/div. TABLE IX M EASURED VALUES

A. Open-Loop Experimental Results MC was operated with m = 1 and unity IDF with io = 8.2 A (fundamental). Figs. 12­14 show the waveforms. Experimentally measured data are listed in Table IX. The voltage waveform across the damping resistor Rd is shown in Fig. 12(a). From the experimental data, its rms value was calculated, resulting in a power loss of 2 W which is much lower than 1% of the input power listed in Table IX. The analysis in Section II-C1 using fundamental vs and io suggests a value (2.1 W) very close to the experimentally obtained value. Referring to Appendix A and particularly (40), operating at unity IDF implies that v ^rL,max is expected almost at the ZC of input line­line voltage vL . Fig. 12(b) shows vL around ZC over 5 ms, and its zoomed view is presented in Fig. 12(c). It is evident that maximum ripple occurs around the ZC zone as expected. In the experimental setup, the measurement of the input filter capacitors with the RCL meter at 10 kHz revealed the values shown in Table IX. The values of Cf from Table IX are used to calculate vra and vrb at the maximum ripple condition as detailed in Section II-D. Subsequently,

Fig. 12. (a) Voltage across Rd : 10 V/div; time: 10 ms/div. (b) vL : 100 V/div; time: 500 s/div. (c) vL : 20 V/div; time: 100 s/div.

V. R ESULTS AND D ISCUSSION Experimental validation was carried out on a 6-kVA MC prototype with IGBT-based four-quadrant switches with the filter parameters listed in Table VIII. The entire control logic was realized on an FPGA platform using ALTERA EP1C12Q240C8 with a sampling frequency of 20 kHz. For commutation, the measured value of vcf was sampled at 40 kHz. A regular four-step commutation strategy, as reviewed in [22], was implemented.

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6491

TABLE X H ARMONICS IN O NE P HASE OF vs , is , AND vco

TABLE XI H ARMONICS vs , is , AND vco W ITH Ls = 1 mH

Fig. 15. Dynamic performance. (a) vco reference command of 140 V. (b) vco reference reset to 0 V. Upper trace: Error voltage in d-axis (116 V/div). Second trace: m (0.2/div). Bottom trace: vco (100 V/div). Time: 10 ms/div.

Fig. 16. Decoupled rectifier-inverter construct.

the v ^rL,max magnitude is found to be 30.8 V. Numerically extracting (filtering) the switching ripple component from the experimental data reveals a maximum ripple listed in Table IX. Thus, a very close agreement is observed between the analytical and experimental observations. Experimental data also revealed an IDF of 0.982 as listed in Table IX. Table X lists the harmonic components in phase a, where the supply voltage vs contains significant low-order harmonics, which, in turn, get transmitted to vco . Since is is a function of both vs and io , it has a higher harmonic content. From the harmonics of vco listed, it is evident that the gain criterion adopted in Sections II and III is effective in the minimal transmission of low-order harmonics from vs to vco . The switching frequency ripple in is was found to be slightly less than 1% of its fundamental component. 1) Performance With Ls Included: Fig. 14 shows the steady-state waveforms, at the same power level, with an externally added Ls of 1 mH. With this inclusion, using (25), c reduces from 1 kHz to 746 Hz, and Q increases from 3.1 to 7.6. The harmonic contents of vs , is , and vco are given in Table XI. Owing to the proximity of input and output corner frequencies (746 and 796 Hz), an increase in the harmonic content of is close to these frequencies is observed. At the instant that this experiment was performed, the total harmonic distortion (THD) of vs was slightly better than the previous situation. However, due to the proximity of the input and output filter corners, the harmonics of is around the output corner have increased, which marginally increased the THD of vco . The measured value of IDF was found to be 0.984. Fig. 15 shows the closed-loop dynamic response of one of the phases of output voltage. The error voltage in vcod , m, and vco are presented in the situation where, initially, a step command of 140 V is introduced and, subsequently, the reference command is reset to zero.

Fig. 17. (a) Input current hexagon. (b) Output voltage hexagon.

VI. C ONCLUSION An exhaustive filter design approach of 3 Ph direct MC (DMC) is presented which, in addition to meeting general requirements of a ripple filter, also addresses the design constraints imposed by dynamic specifications and commutation requirements. Ripple filter design aspects like attenuation and regulation and also MC-specific issues like the damping resistors at the input filter have been addressed. The detailed analytical derivation of the damping resistor losses and input voltage ripple is provided, and experimental results are presented to establish the validity of the analytical conclusions. The minimum input filter capacitor required for reliable VBC has been derived based on the maximum error in ripple voltage sensing. The effect of the grid inductance has been discussed where the possibility of a compromise between the filter and controller designs has been highlighted. The output ripple filter has also been discussed, and experimental waveforms are provided to demonstrate the close agreement of the filter performance with the input specifications. The necessary tradeoffs in filter design, which may be imposed by dynamic requirements, originate from the basic power stage of a 3 Ph DMC. Therefore, the observations are equally applicable to single-phase MCs or indirect MCs which are derived from the basic MC topology.

6492

IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS, VOL. 61, NO. 12, DECEMBER 2014

TABLE XII ¯in IN S ECTOR 2 AND V ¯OL IN S ECTOR 3 I NPUT ­O UTPUT C ONNECTIONS IN MC FOR I

Ts . The input currents are denoted as If a and If b , respectively. Output currents over the same period are represented as IC and IB . From Table XII and Figs. 9 and 19, If a and If b can be expressed as If a = (IC d d + IB d d ) = mIP sin(60 - SI )
Fig. 18. SI and SV .

If b = (IC d d + IB d d ) = mIP sin SI where IP = IC sin(60 - SV ) + IB sin SV .

(35)

(36)

^rL , is maximum From (15), the peak­peak magnitude of vrL , v when area AP , shown shaded in Fig. 19, is maximum. AP is evaluated as AP = {(IC - If a + If b )d d +(IB - If a + If b )d d }Ts . (37) Substituting (35) in (37), AP gets modified as AP = mIP Ts sin(60 - SI )k (SI , SV ) where  k (SI , SV ) = 1+ 3m sin(SI - 30 )cos(SV - 30 ) . (39)
Fig. 19. Fundamental (If a , If b ) and switching frequency (ira , irb ) components of input currents (ia , ib ).

(38)

A PPENDIX A M AXIMUM vrL B ETWEEN P HASES A AND B Fig. 16 shows the decoupled rectifier-inverter construct of MC. The input current and output voltage stationary vectors for realizing SVM [27] are shown in Fig. 17(a) and (b). I1 (a, c) implies that input phases "a" and "c" are connected to the positive and negative rails of the fictitious dc link, respectively. Also, V1 (+, -, -) indicates that the output phase A is connected to the positive rail and both B and C are connected to the negative rail. Considering an instant when ¯ Iin lies in sector II, requiring ¯ OL is commutation between input phases a and b, and V in sector III, the relevant switching vector combinations and input­output connections are listed in Table III. Fig. 18 shows the rotating space vectors of each hexagon and the two bordering stationary vectors. SI and SV are the angles between the rotating and the trailing stationary vectors. The expression for duty cycles [27] is also indicated in Table XII. Fig. 19 shows the fundamental and switching frequency components of the input currents. The fundamental components of both input and output currents are assumed to be constant over a switching period

Since SI and SV each vary in the closed interval [0 , 60 ] and m varies in [0, 1], it is derived that AP reaches its maximum for SI = 28 , SV = 60 & m = 1. (40)

Therefore, v ^rL is maximum at an instant when if a  if b . Using (40), (38), and (39) in (15), maximum v ^rL is ^in Ts . v ^rL,max  (0.5/Cf )I ^in in MC [27] is The amplitude of the input current I  ^in = mIP = ( 3/2)mI ^o cos(oL ) I (41)

(42)

impedance angle. From (42), the maxiwhere oL is the load  ^o . Substituting this in (41) ^in is ( 3/2)I mum possible I  3 ^ I 0 Ts . (43) v ^rL,max  4Cf

ACKNOWLEDGMENT The authors would like to thank A. Basu and Nandkishore for the support during the hardware fabrication.

DASGUPTA AND SENSARMA: FILTER DESIGN OF DIRECT MATRIX CONVERTER FOR SYNCHRONOUS APPLICATIONS

6493

R EFERENCES
[1] J. Monteiro, J. F. Silva, S. F. Pinto, and J. Palma, "Matrix converter-based unified power-flow controllers: Advanced direct power control method," IEEE Trans. Power Del., vol. 26, no. 1, pp. 420­430, Jan. 2011. [2] B. Wang and G. Venkataramanan, "Dynamic voltage restorer utilizing a matrix converter and flywheel energy storage," IEEE Trans. Ind. Appl., vol. 45, no. 1, pp. 222­231, Jan./Feb. 2009. [3] K. Mohapatra and N. Mohan, "Matrix converter fed open-ended power electronic transformer for power system application," in Proc. IEEE Power Energy Soc. Gen. Meet., Convers. Del. Elect. Energy 21st Century, Jul. 20­24, 2008, pp. 1­6. [4] H. Nikkhajoei and M. R. Iravani, "A matrix converter based micro-turbine distributed generation system," IEEE Trans. Power Del., vol. 20, no. 3, pp. 2182­2192, Jul. 2005. [5] A. Garces and A. Trejos, "A voltage regulator based on matrix converter for smart grid applications," in Proc. IEEE PES Conf. ISGT Latin America, Oct. 19­21, 2011, pp. 1­6. [6] P. M. Garcia-Vite, F. Mancilla-David, and J. M. Ramirez, "Per-sequence vector-switching matrix converter modules for voltage regulation," IEEE Trans. Ind. Electron., vol. 60, no. 12, pp. 5411­5421, Dec. 2013. [7] P. Zanchetta, P. Wheeler, L. Empringham, and J. Clare, "Design control and implementation of a three-phase utility power supply based on the matrix converter," IET Power Electron., vol. 2, no. 2, pp. 156­162, Mar. 2009. [8] P. Wheeler and D. Grant, "Optimised input filter design and low-loss switching techniques for a practical matrix converter," Proc. Inst. Elect. Eng.--Elect. Power Appl., vol. 144, no. 1, pp. 53­59, Jan. 1997. [9] C. Klumpner, P. Nielsen, I. Boldea, and F. Blaabjerg, "New Matrix Converter Motor (MCM) for industry applications," IEEE Trans. Ind. Electron., vol. 49, no. 2, pp. 325­335, Apr. 2002. [10] M. Hamouda, F. Fnaiech, and K. Al-Haddad, "Input filter design for SVM dual-bridge matrix converters," in Proc. IEEE Int. Symp. Ind. Electron., Jul. 9­13, 2006, vol. 2, pp. 797­802. [11] D. Gopinath, "Modeling, real-time simulation and design of matrix converters," Ph.D. dissertation, Dept. Elect. Eng., Indian Inst. Sci., Bangalore, India, Sep. 2009. [12] J. Andreu et al., "A step forward towards the development of reliable matrix converters," IEEE Trans. Ind. Electron., vol. 59, no. 1, pp. 167­183, Jan. 2012. [13] A. Trentin, P. Zanchetta, J. Clare, and P. Wheeler, "Automated optimal design of input filters for direct ac/ac matrix converters," IEEE Trans. Ind. Electron., vol. 59, no. 7, pp. 2811­2823, Jul. 2012. [14] T. Kume et al., "Integrated filters and their combined effects in matrix converter," IEEE Trans. Ind. Appl., vol. 43, no. 2, pp. 571­581, Mar./Apr. 2007. [15] T. Friedli, J. W. Kolar, J. Rodriguez, and P. W. Wheeler, "Comparative evaluation of three-phase ac-ac matrix converter and voltage dc-link backto-back converter systems," IEEE Trans. Ind. Electron., vol. 59, no. 12, pp. 4487­4510, Dec. 2012. [16] D. Casadei, G. Serra, A. Tani, A. Trentin, and L. Zarri, "Theoretical and experimental investigation on the stability of matrix converters," IEEE Trans. Ind. Electron., vol. 52, no. 5, pp. 1409­1417, Oct. 2005. [17] D. Casadei et al., "Large-signal model for the stability analysis of matrix converters," IEEE Trans. Ind. Electron., vol. 54, no. 2, pp. 939­950, Apr. 2007. [18] A. Dasgupta and P. Sensarma, "Low-frequency dynamic modelling and control of matrix converter for power system applications," IET Power Electron., vol. 5, no. 3, pp. 304­314, Mar. 2012. [19] K. Sun, D. Zhou, H. Lipei, K. Matsuse, and K. Sasagawa, "A novel commutation method of matrix converter fed induction motor drive using RB-IGBT," IEEE Trans. Ind. Appl., vol. 43, no. 3, pp. 777­786, May/Jun. 2007.

[20] D. Gopinath and V. Ramanarayanan, "Implementation of bi-directional switch commutation scheme for matrix converters," in Proc. Nat. Power Electron. Conf., Bangalore, India, 2007, [CD-ROM]. [21] H. She et al., "Implementation of voltage-based commutation in space vector modulated matrix converter," IEEE Trans. Ind. Electron., vol. 59, no. 1, pp. 154­166, Jan. 2012. [22] J. Mahlein, J. Igney, J. Weigold, M. Braun, and O. Simon, "Matrix converter commutation strategies with and without explicit input voltage sign measurement," IEEE Trans. Ind. Electron., vol. 49, no. 2, pp. 407­414, Apr. 2002. [23] R. Cardenas, R. Pena, P. Wheeler, J. Clare, and G. Asher, "Control of the reactive power supplied by a WECS based on an induction generator fed by a matrix converter," IEEE Trans. Ind. Electron., vol. 56, no. 2, pp. 429­ 438, Feb. 2009. [24] R. Vargas, U. Ammann, B. Hudoffsky, J. Rodriguez, and P. Wheeler, "Predictive torque control of an induction machine fed by a matrix converter with reactive input power control," IEEE Trans. Power Electron., vol. 25, no. 6, pp. 1426­1438, Jun. 2010. [25] R. Erickson and D. Maksimovic, Fundamentals of Power Electronics, 2nd ed. Berlin, Germany: Springer-Verlag, 2001. [26] N. Mohan, T. Underland, and W. Robbins, Power Electronics: Converters, Applications, Design, 2nd ed. Hoboken, NJ, USA: Wiley, 2001. [27] L. Huber and D. Borojevic, "Space vector modulated three phase to three phase matrix converter with input power factor correction," IEEE Trans. Ind. Appl., vol. 31, no. 6, pp. 1234­1246, Nov./Dec. 1995.

Anindya Dasgupta received the B.E.E. degree in electrical engineering from Jadavpur University, Calcutta, India, in 2000; the M.E. degree in electrical engineering from the Bengal Engineering and Science University, Shibpur, India, in 2006; and the Ph.D. degree in electrical engineering from the Indian Institute of Technology Kanpur, Kanpur, India, in 2013. He is currently a Faculty Member in the Department of Avionics, Indian Institute of Space Science and Technology, Thiruvananthapuram, India. His research interests include power converter topologies and their modeling and control.

Parthasarathi Sensarma (M'00) received the B.E.E. degree in electrical engineering from Jadavpur University, Calcutta, India, in 1990; the M.Tech. degree in electrical engineering from the Indian Institute of Technology (IIT) Kharagpur, Kharagpur, India, in 1992; and the Ph.D. degree in electrical engineering from the Indian Institute of Science, Bangalore, India, in 2001. He has held positions with Bharat Bijlee Ltd., Thane, India; CESC Ltd., Kolkata, India; and ABB Corporate Research, Baden-daettwil, Switzerland, where he was a Staff Scientist with the Power Electronics Department. Since 2002, he has been with the Department of Electrical Engineering, IIT, Kanpur, India, where he is currently an Associate Professor. His research interests include power quality, FACTS devices, power converters, and renewable energy integration.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Digital microfluidic biochips in recent years have been developed as a major alternative platform for conventional benchtop laboratory procedures. It offers better precision, scalability, higher sensitivity, lower cost due to smaller sample and reagent volumes. Testing of DMFBs is of major significance in terms of dependability and reliability issues for safety-critical applications. A series of complex microfluidic operations are executed in a compact 2D array within a DMFB. The layout engages a group of cells as transportation path as well as a specific cluster of cells as functional modules to perform basic operations of routing, mixing, splitting, merging, storage and detection. In order to determine the correctness and reliability of results testing of these prescheduled layout are necessary both for transportation (structural) as well as functionality (functional). In this paper we propose a technique for customized testing of a prescheduled layout within a microfluidic array. The test performs both structural as well as functional testing for specified cells that forms the layout. The simulations are carried out in testbenches of benchmark suite III and the results are compared with contemporary methods.Microfluidic based biochips as a composite microsystem offers an alternative platform for conventional laboratory procedures. In recent years a new generation of such lab on chip devices namely Digital Microfluidic Biochip is emerged as a suitable application for concurrent and scalable integration of multiple bioassay protocols. Dependability and accuracy are major issues for safety critical applications of DMFBs specifically in the areas of clinical diagnostics and other biochemical applications. Functional testing is targeted towards assessment of reliability of basic microfluidic operations namely merging, mixing, splitting and incubation. In this paper we proposed a layout specific functional testing method that detects faults in a prespecified group of cells already placed within a 2D array for execution of a given bioassay protocol. The objective is to minimize the test completion time, optimize test resources and customize the test for prespecified cluster of cells to be utilized as modules dedicated for microfluidic operations. The simulation is carried out on testbenches of Benchmark suite III and the results are found to be encouraging.Recent studies have shown that adaptively regulating the sampling rate results in significant reduction in the computational resources of embedded software based control. Selecting a uniform sampling rate for a control loop is robust, but pessimistic for sharing processors among multiple control loops. Fine-grained regulation of periodicity achieves better resource utilization, but is hard to implement online in a robust way. However, an offline control theoretic analysis of the system illustrates the benefits of proper period selection for different modes. Such analysis reveals the necessity to automatically derive a multi-mode scheduler and converge on suitable periods for each mode. This paper proposes a methodology to automatically generate such a scheduler for an embedded real-time control system leveraging its design attributes. The proposed method provides significant gains in computational efficiency without trading off the performance.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/268487117

Designingadaptivelightingcontrolalgorithms forsmartbuildingsandhomes
ConferencePaper·April2014
DOI:10.1109/ICNSC.2014.6819639

CITATIONS

READS

2
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

44

AllcontentfollowingthispagewasuploadedbyYuanWangon05March2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Designing Adaptive Lighting Control Algorithms for Smart Buildings and Homes
Yuan Wang Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu Partha Dasgupta Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract-Artificial lighting is often the main lighting provision for workplaces. This paper describes algorithms for optimizing lighting control in large (smart) buildings that are extensible to smart home use. Systems that provide uniform lighting, under varying outdoor light levels, at occupied locations turns out to be a hard problem. We present methods that work as generic control algorithms and are not preprogrammed for a particular building. Our system uses wireless sensors and wired actuators for the lighting control. But the system does not know about locations and correlations between lights and sensors. The model shows that the control problem is NP-Hard. A heuristic algorithm is proposed and validated to solve the problem to compute approximate optimal solution.

I.

INTRODU CTION

Artificial lighting is essential in large buildings and homes. Lighting has very important effect on people's health and pro ductivity as shown by recent studies [ 1]. Controlling lighting based on occupancy, daylight effects and energy costs can impact energy usage. Currently there are 1.5 million commercial buildings and 1 14 million homes in the US with a growth rate of about 3% each year. Smart buildings and homes need, among other features, the ability to control lighting. Deploying such systems in existing buildings is most feasible if the system uses wireless sensors, simple actuators and does not need custom programming. These goals have driven our research. Automating the lighting control systems such that uniform lighting, under varying outdoor light levels, is maintained at occupied locations turns out to be a hard problem. We are working on algorithms that can be used in generic systems that are not preprogrammed for a particular building. That is the system does not know about locations and correlations between lights and sensors. Several existing approaches are given to increase lighting comfort levels. Some of lighting control systems may be able to provide enough lighting for a workplace, but people may still feel uncomfortable when illumination level is sufficient but not uniform. According to [ 1], three problems are insuf ficient light or uneven light or lights too bright. Controlling lighting levels automatically need customized control systems that rely on extensive pre-programming involving detailed custom models andlor lengthy set-up. When the pattern in a workplace changes, it needs expensive customization and updates performed by trained personnel, which is costly and

time-consuming. The "sequential lighting changes" approach is sometimes used in lighting control system [2] for calculating light settings. This method adjusts light settings based on feedback data, which is able to generate a reasonable result, but the average computational time is high due to the steps of prediction and adjusting. It also increases the number of times lights are switched on/off (unnecessarily) due to its feedback design [2]. In this paper, a Wireless Sensor Network(WSN)-based light ing control system is introduced to efficiently and adaptively control artificial lights to provide a stable and uniform lighting environment. The system measures light at preset locations and turns on or off light switches to achieve the goal. The system is not pre-programmed and learns about effects light switches have on sensors via calibration. The work is formalized using non-linear integer programming model that proves to be NP Hard. A heuristic algorithm is proposed and validated to solve the problem to compute approximate optimal solution. The approach can be used in different places where uniform lighting environment is required. For the sake of brevity and simplicity we only describe the operation of the system while there is no outside lighting. Addition of varying levels of outside lights is an extension(see section V). Also, energy consumption minimization can be incorporated without major changes. The rest of this paper is organized as follows. Section II introduces the background and related work of the problem. Section III presents the heuristic algorithms for lighting con trol. Experimental work and simulation results are discussed in section IV. Section V talks about potential extensions of the algorithm. We conclude the paper in section VI. II.
B ACKGROUND AND RELATED WORK

A. Problem Description

Assume a place has n light switches and m light-level sensors, placed by the human designer of the place. The sensors are connected to the control system via a W SN and the switches are activated via actuators connected to the system. The physical location of lights, sensors and their correlations are initially unknown to the control system. The ultimate task is to compute the positions (on/off) of n light switches. Let x (Xl, ... ,xn ) denote the assignment for light switches where Xi E {O, I} and 0 denotes off and 1
=

978-1-4799-3 106-4114/$3 1.00 ©20 14 IEEE

279

denotes on. The goal is to optimize the comfort level. There are two criteria needed to satisfy, lighting level and lighting uniformity. The former is satisfied when every sensor reading stays in an accepted range i.e. if sensor j's reading is Lj(x), then min  Lj(x)  max. The latter can be satisfied by minimizing the standard deviation(represented by 0") of the sensor readings. Hence the problem can be stated as: minimize
x

TABLE I CALlBRATlON:LIGHTS' IMPACTS ON SENSORS(LUX)

II
81 82

12 230 10

13 350 0

20 680

B. Related Work

(O"(L1(x), ..., Lm(x))) min  Lj(x)  max, j= I, ..., m XiE {O, l}, i=l, ..., n
( 1)

subject to

For lighting impacts, one important feature is that sensor readings are additive [3], i.e. let Impacti j to be the impact of light i on sensor j when only light i is on and it is dark outside, then we have

n Lj(x)= L Impacti j . Xi
i =l

(2)

[4] and [5] gave a general definition of nonlinear integer programming problem. It can be stated as: max/min subject to

f(x) hi(x)=0, iE I= I, ... , p gj(x)  0, jE J =l, ... ,q xE Z n
(3)

where x is a vector of decision variables, and some of the constraints hi, gj : zn ---+ or the objective function f : zn ---+ are non-linear functions. In equation 1, let gl (X) = It can be transformed to: minimize
x

Lj(x)-max, h1(x, y)=Lj(x)-min-y=O, y;::: O, yE Z. (0"(L1(x), ..., Lm(x))) g l (X)  0 h1(x, y)=0 XiE {O, l}, i=l, ..., n y;::: O, yE Z
(4)

lR

lR

subject to

Since equation (4) satisfies the format of equation (3) where the objective function is nonlinear, our problem belongs to nonlinear integer programming problem. According to [4], it is NP-hard. Therefore, any polynomial time computed solution would be an approximation. To find the best setting, a naive approach is to try all 2n positions of n switches, which is un acceptable due to the time complexity. Therefore, we propose a heuristic algorithm for computing an approximate optimal light setting. The heuristic plan has three main steps. First, calibration, i.e. calculating Impacti ,(section III-A). Second, sensors and lights are partitioned into small zones based on calibration data(section III-B). Third, an approximate optimal light setting is generated for each zone (section III-C).

Recently W SN technologies have been applied into a lot of areas such as [6] [7]. It consists of distributed portable wireless sensors to monitor physical conditions, such as light, temperature, humidity, etc. By using W SN, people can easily detect the change of surrounding environments and make cor responding adjustments for actuators. In lighting control area, several existing work [3][8][9] applied W SN technologies into lighting control system for energy conservation purposes. It is involved with using some specific protocols in W SN area such as ZigBee [ 10]. [ 1 1] proposed an approach to integrate small wireless sensor or actuator nodes into an IP-based network so that it is possible to provide web services at each node. Several customized lighting control systems are designed for some specific areas. For example, [8] was mainly designed for entertainment and media production area while [ 12] was mainly designed for theater arts area. These systems generally have some specific requirements that normal lighting control systems do not have. For example, in theater arts area, it needs to capture the positions of actors in real time. Therefore, the objective functions for the lighting control in specific places need to be adjusted accordingly and some particular sensors may need to be enrolled. When more than one lighting re source are involved, such as whole lighting and local lighting, [9] proposed an approach that every user carries a sensor to detect the local intensity. [ 13] introduced an user-friendly interface for a networked lighting control system. [ 14] presented a mathematical model for lighting control system that could be applied into the case that a luminary impact is continuous such as light emitting diodes(LEDs) luminaries and the goal illumination level is given as a single value rather than a particular range. [ 15] used infrared ray communication technology for smart illuminance sensor to retrieve the lighting ID binding with each lighting fixture. Through analyzing lighting ID information, sensors can rec ognize nearby luminaries so that the control efficiency is improved. III.
A. Calibration
CONTROL ApPROACHES

Our system first determines the lighting levels, lighting locations and sensor correlations via calibration. We note that each light has an impact on a set of sensors. Calibration is used for calculating Impacti ;' Suppose there are n lights in an area. The process of calibration is: 1) Turn off all lights and turn on one light at a time 2) Record the light's impact on each sensor 3) Repeat step 1 n times until all lights are counted

280

The impact data will be inserted into a table as shown in Table I. Assume there are 3 lights(h, h and l3) and 2 sensors(sl and S2) in an area. Table I shows all lights' impacts on sensors in lux values. For example, when only h is on, its impact on Sl is 20 lux(Jmpach1) and on S2 is 680 lux(J mpach2).
B. Zoning/Partitioning

From the calibration data we can partition the whole area into smaller zones. The idea of zones is that some switches have no impact on some sensors as they are in different rooms or floors. We assume that every light and sensor can only belong to one zone and artificial lighting is the only lighting resource in the area. Assume there are A sensors and B lights. For each j, j 1,... , A, we define a function fj : {O,l}B -+ which is a mapping from all lights status(on/off) to a non-negative value registered by sensor Sj. It is clear that fJ(x) Lj(x). Using the same example as shown in Table I, if II is on, l2 is off, and l3 is on, h(l,O,1) L1(l,O,1) 1· 20+0·230+1·350 370. All fjs can be collected into F : {O,l}B -+ with for every choice of which lights are on/off gives the value of lux for all sensors. The goal is to partition the whole area for dropping down the computational size (number of lights in a zone). For each j, j I, ... , A, we define a vector gj of length B to indicate which lights are assigned to sensor S j. We also set up a threshold " which is used to decide whether a light is partitioned into a zone or not. When only li is on, if the value of fJ is smaller than " then the ith value of the vector gj is 0; otherwise it is 1. Proceeding with the example of Table I, we show the procedure of getting gl and g2. Let's set, 30. Then since when only h is on, the value of h is 20 which is smaller than,, 1st value of the vector gl is O. The value of 12 is 680 which is larger than ,, 1st value of the vector g2 is 1. Applying this method, in consequence, we have gl (0, I, 1) and g2 (1,0,0), i.e. lights hand b are assigned to sensor Sl while light h is assigned to S2.
=

the problem. If sum of all values in C is still smaller than lowerbound, simply turn on all lights. Part 2 and Part 3 compute candidates of final light setting. A candidate c is a set that only contains light numbers selected to be on, i.e. if i E c then Xi 1, otherwise Xi O. Part 2 computes the base-level candidates. From part 1, it is known at least w lights needed to be turned on to adjust light intensity greater or equal than lowerbound. Therefore, for each base-level candidate setting, it should contain w elements and sum of the w elements' contributions to the
= =

L Ck) should be slightly greater or equal kEsettin g than lowerbound. The proposed approach is based on the
whole zone( fact that lux level light intensity is additive[3] and greedy algorithm, which is shown in Algorithm 1.
Algorithm 1
Output: 2: 3: 4: 6: 7:

lR+,

Base-Level Candidates Selection
setting
Sl, S2 ,

Input: C, n, w, lowerbound

=

base-level candidate

1: searchvalue +-lowerbound/w,

s +- 0

=

=

=

lR

find index of the value that searchvalue in C and put it find index of the value that searchvalue in C and put it add Sl and S2 to s
Si

is the smallest value larger than in Sl is the largest value smaller than in S2

5: for

for

j

E s do = 1 --+ w

-

1 do

=

searchvalue +-(lowerbound

-

8:
9: 10:

change
end for

Sl

to

Si

in line2 and repeat

kEsi

L Ck)/(W - j)

add
Si

Si

to

setting

1 1: end for 12: for 13: 14: 15: 16: 17: for

=

j

E s do = 1 --+ w

-

1 do

searchvalue +-(lowerbound if

-

L Ck)/(W - j)

j is odd then change Sl to Si in line2 and repeat change
S2

=

else

=

18:
19: 20: 2 1:

to

Si

in line3 and repeat

end if end for

C. Final Computation

add

Si

to

setting

The last step is to compute desired light settings. Note that the computations in all zones are independent and can run concurrently. We describe the final computation for a zone, with lights and sensors belonging to that zone only. The proposed lighting control algorithm contains 4 parts. Part 1 counts the minimum number of lights needed to be turned on. Let m to be the number of sensors, C {C1,... ,Cn } to be the set of lights' contributions to the whole
=

22: end for 23: return setting

m

zone where Ci

=

to min x m and upperbound is equivalent to max x m. To count the minimum number of lights turned on, simply find minimum number of elements in the sorted array C such that sum of them are greater or equal than lowerbound. It is obvious that a linear search would be enough to solve

j=l

L Impactij,

lowerbound is equivalent

Part 3 generates more candidates from base-level candidates. Unlike light impacts, standard deviation is not additive, thus increasing number of lights may negatively impact the stan dard deviation. However, due to time and quality tradeoffs, computing more sets likely results in getting closer to the theoretical optimality. Adding <5 lights to compute needs O ( nO ) time. To ensure low response time a low complexity is desirable, and based on the theoretical analysis and simulation experiments, we set <5 2. It indicates that we will at most add 2 lights to the existing candidates. Suppose each base-level candidate has w elements, there are n lights in total. Then if each candidate wants to add an un selected light into itself, there would be n - w choices.
=

28 1

If there are k base-level candidates, finally there would be k x (n - w) new candidates that have w + 1 elements added. Similarly when another unselected light is trying to add into the candidates that have w + 1 elements, total amount of candidates that have w + 2 elements is becoming k x (n - w) x (n - w 1). Thus total number of candidates would be k + k x (n - w) + k x (n - w) x (n - w -1). Note
-

B. Feasibility

The experiment is run at night. Each sensor is placed under each light, at a distance of 60 inches, and they are numbered o to 8. Experimental design and elements are shown in Fig. 1. The steps are: 1) Calibrate lights' impacts on sensors using the approach described in section III-A. 2) Set expected lux level range from 49 to 5 1(Test 1). 3) Turn off all lights. Run control server to compute light settings. When selected lights are turned on, record the reading on each mote. 4) Change the step two's expected lux level range to be 99 to 10 1(Test2), 149 to 15 1(Test3) and 199 to 20 1(Test4) respectively. Rerun step three. The results are shown in Fig. 2. From the figure, we can see for each test, the computed light setting's impact on every mote slightly fluctuates at the middle point of the expected range. Take test 3 as an example. Lights 0, 1,2,3,6,7,8 are selected to be on. Every mote's reading is falling between 149 and 15 1. Average of mote readings for test 3 is 149.22, standard deviation is 3.80. All other tests show the similar trends, which shows the feasibility of our real system, that is using the results of our computation and then applying it to a real, calibrated room, it provides the results that we predicted. Prior to this test, we had already measured the additive property and had some experience with daylight measurements.
300 · Testl, Lights 5,7 on, Avg=49.89, Stdev-2.26 .Test2, Lights 0,1,3,6,7 on, Avg=l00.11, Stdev=3.48 ... Test3, Lights 0,1,2,3,6,7,8 ·
on,

for each candidate

c,

pEe Part 4 calculates the standard deviation of sensor readings generated by each candidate. According to equation 2, for each candidate c, we are able to calculate its impact on sensor j Lj ( c) . Then it is easy to know the standard deviation of all sensors' readings under c. The candidate that generates the lowest standard deviation of sensor readings would be selected as the final light setting.
D. Time Complexity of Heuristic Lighting Control Algorithm

L Cp :s; upperbound.

Suppose there are n lights, m sensors in a zone. Calibration has a time complexity O(n). Zoning/Partitioning has a time complexity O(m n). Final computation has 4 subparts. Part l's time complexity is O(n). The worst case for Part 2 is O(n2). As discussed earlier, the time complexity for Part 3 is nO . In our case, 0 2, which makes Part 3 time complexity to be O(n2). Obviously Part 4's time complexity is equivalent to Part 3, which is also O(n2). Therefore, for every zone, the total time complexity is O(n2). When considering the whole area, suppose there are z zones, and zone Zi(i E [1, z ]) has ni lights. If total number of lights
=

z

is N, we have N

=

L ni.
i=l

Since computations in zones
X
.... u

250

Avg=149.22, Stdev=3.80
on,

are running concurrently, the total complexity is equivalent to O( (max(nl, ..., nz)) 2). It is clear max(nl, ..., nz) :s; N, so the whole area's time complexity is O(N2). IV.
EX PERIMENTAL WORK AND SIMULATION RESULTS

eTest4, Lights 0,1,2,3,4,5,6,7,8 :::J 200 · ·

Avg=199.67, Stdev=4.30 · · · · ·

·

<::"

a. 150 .5 fA, 100
...

.a.

.a.

.a.

.a.

.a.

.a.

.a.

.a.

.a. ·

A. Implementation Details

:::;

·

·

·

·

·

·

·

· ·

To demonstrate the feasibility and effectiveness of our approach, we used an experimental setup. A 8ft x 8ft test cell was instrumented with 9 lights (l5W incandescent, 120V), and 9 sensors connected via a W SN and actuators to a computer. The computer is a Intel Core2 running Linux. The sensors use Crossbow's TelosB motes containing visible light sensors called Hamamatsu S 1087. The sensor network uses the Collection Tree Protocol [ 16] to send lighting data to a designated gateway sensor. Applications running on the motes are implemented in NesC and TinyOS environment. The computer runs a server called SenServer developed in Java and connects to the gateway sensor via serial port. A ProXR Relay Controller is connected to Control Server through USB port to control artificial lights. Since the official driver of the relay controller is designed for Windows platform, Control Server is developed in C# that runs in a Windows machine. It is implemented based on the contents discussed in section III. Eclipse Standard 4.3 and Microsoft Visual Studio 20 10 are used for Java and C# developments respectively.

50

·

·

·

·

·

·

·

·

Mote 10
Fig. 2. Experimental Results of 9 lights and 9 motes

C. Adaptivity and Scalability

The test cell experiments are limited, as the number of lights are low, the space is limited and the geometry is rather simple. To be able to study adaptivity and scalability we did simulations on a more complex, synthetic setup, with randomly generated impact values. Adaptivity and scalability are two important features of the system. Adaptivity means when room pattern changes such as adding or removing some lights, changing lights' posi tions, etc, the system can automatically detect environmental changes and make corresponding adjustments. In other words, when room pattern changes, the lighting control algorithm

282

(c) Testbed Layout Fig. 1. Experimental Design

should be able to adjust light settings accordingly in a short time without any modifications. Adaptivity is also necessary when there is additional illumination due to outside lights. Scalability means when amount of lights is growing large, the lighting control algorithm could still compute the light settings in a reasonable time. Apparently customized algorithms do not have the adaptivity feature and custom systems also use brute force methods to determine lighting levels and hence have exponential complexity, thus not being scalable. When pattern changes, the only part that might get changed in the objective function(Equation 1) is Impacti;' Therefore, to test if the heuristic algorithm is adaptive, Impacti; should be randomly assigned. In addition, expected lux level for this experiment is set between 320 and 500 lux to match normal office lighting range [l7]. The simulation experiment runs as follows: 1) Assume there are 30 lights and 25 sensors in a zone. Each light i has an impact value on sensor j(Impacti;)' There should be 750(30 x 25) impact values. 2) Randomly assign an integer value from 0 to 100(spec ification of 15W, 120V light bulb) to be one impact value. Repeat this step 750 times until all impact values are successfully assigned. 3) Run brute force algorithm(O(2n)), the proposed light ing control algorithm(O(n2)), and a revised O(n3) algorithm(o 3) that adds one more light to the candi date sets of O(n2), respectively. Record computational time, lux level light intensity and standard deviation for each method. 4) Repeat step 1-3 1000 times. Take an average of standard deviation, light intensity and computational time results respectively. 5) Increase light's amount and rerun steps 1-4.
=

to the revised O(n3) algorithm, but the computational time is far less than the latter one. Compared to the brute force, the proposed approach has an extremely better time performance with a similar light intensity performance and a reasonable increase on standard deviation. We also do some other evaluations to verify the system's scalability. For example, we run the above algorithms on very large dataset like 1000 number of lights. Result shows that the proposed O(n2) approach can still compute the light setting in a desired range while other methods are not applicable due to the time performance. When size is 1000, the O(n2) algorithm is running in around 106 time complexity which reduces substantial workloads from brute force's 1.07 x 10301 time complexity. V.
EXT ENDING T HE ALGORIT HM

The result is shown in Fig. 3. Fig. 3(c) describes time per formances of different methods. Since when amount of lights increases, the computational time of brute force method is increasing exponentially, which quickly arrives at a very large point of value while polynomial time algorithms have a pretty low computational time. Therefore, in order to put all data together, for Fig. 3(c), we take a common 10garithm(loglO) on the real computational time data. From the graph, we can see that our proposed 0 (n2) algorithm reaches a similar performance in light intensity and standard deviation compared

Our goal is to produce a stable and uniform lighting environment in workplaces by adjusting artificial lights. In this paper, we assume artificial lighting is the only lighting source in a workplace and there is no other lighting involved. However, when ambient light exists, the algorithm needs to be extended to allow for the impact produced by such light. Ambient light will produce a non-zero and varying impact on different sensors and is not controllable via light switches. This impact will have to be detected and used in the computation, in other words the computation needs to compute deltas between current conditions and desired levels and recomputed switch settings. The idea can be also applied when required illumination levels in an area are different. The selection of the final computation results can be done in an energy efficient manner by computing the energy costs of each light being turned on. From this we can select candidate setting that may not produce the best uniformity but its energy is consumed in significantly lower. In addition, the goal lighting level can be changed for energy savings especially at peak load hours. Placement of sensors is an important point we did not address in the paper. In the current work, we assume sensors in an area are distributed in a good way that can very well capture the light intensity. In the future work, we will do more investigations on sensor layouts, which will better capture the light intensity in an area. Although the brute force method can't be directly applied due to the time issue, it still might be worthy to add into the

283

70 c: 0 ',0:; 60

40 Qj Q 30 'l:I 0 'l:I c: 10

'>

"'

50
.... 0(2AN)
__ 0(NA2)

... 2 "'

...... 0(NA3)

\I')

"' ....

30

31

3

2

33

34

3

5

36

37

38

39

40

Amount of Lights
(a) Standard Deviation
600

in both experimental and simulated scenarios. The underlying system of wireless sensors and wired actuators can be de ployed at homes and in larger building without excessive costs. We build a formal model of the lighting control problem and show it is a hard problem (NP-Hard). A heuristic algorithm is proposed to solve the problem to compute approximate optimal solution. Experimental results show the efficiency and effectiveness of the heuristic algorithm. The proposed approach can be augmented to acconunodate day lighting, failures, manual overrides, and occupancy detectors.
REFERENCES [1] Lighting at work. Health and Safety Executive, 1997. [2] R. Mohamaddoust, A. T. Haghighat, M. J. Motahari Sharif, and N. Ca panni, "A novel design of an automatic lighting control system for a wireless sensor network with increased sensor lifetime and reduced sensor numbers," Sensors, vol. II, no. 9, pp. 8933-8952, 2011. [3] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews, "Intelligent light control using sensor networks," in Proceedings of the
3rd international conference on Embedded networked sensor systems.

.u; =
c:

 500 ..
c: Q) 400 300 200 100

--'"

.......

"I

.....

,.
.... 0(2AN) __ 0(NA2) ...... 0(NA3)

 )( .3

Qj >

30

31

32

33

34

35

36

37

38

39

40

Amout of Lights
(b) Light Intensity

ACM, 2005, pp. 218-229 . [4] R. Hemmecke, M. Koppe, J. Lee, and R. Weismantel, "Nonlinear integer programming," arXiv preprint arXiv:0906.5171, 2009. [5] Wikipedia. Nonlinear programming. [6] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of
the lst ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88-97. [7] Q. Li, M. De Rosa, and D. Rus, "Distributed algorithms for guiding navigation across a sensor network," in Proceedings of the 9th annual international conference on Mobile computing and networking. ACM, 2003, pp. 313-325. [8] H. Park, J. Burke, and M. B. Srivastava, "Design and implementation of a wireless sensor network for intelligent light control," in Proceedings

6.5 5.5 5  c: 4.5 8 4 Qj 3.5 11\ - 3 I- 2.5  2 1.5 0.5

--

----

....-

--

....-

....-

-0(2AN) -0(NA2) -0(NA3)

of the 6th international conference on Information processing in sensor

ACM, 2007, pp. 370-379. [9] M. Pan, L. Yeh, Y. Chen, Y. Lin, and Y. Tseng, "Design and implemen tation of a wsn-based intelligent light control system," in Distributed
networks. Computing Systems Workshops, 2008. 1CDCS'08. 28th 1nternational

Amount of Lights
(c) Computational Time Fig. 3. Comparison of Different Approaches

IEEE, 2008, pp. 321-326. [10] Y. Wang and Z. Wang, "Design of intelligent residential lighting control system based on zigbee wireless sensor network and fuzzy controller," in Machine Vision and Human-Machine Interface (MVHl), 2010 Inter national Conference on. IEEE, 2010, pp. 561-564. [11] L. Schor, P. Sommer, and R. Wattenhofer, "Towards a zero-configuration wireless sensor network architecture for smart buildings;' in Proceedings
Conference on. of the First ACM Workshop on Embedded Sensing Systems for Energy Efficiency in Buildings. ACM, 2009, pp. 31-36. [12] c. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, "Design of a wireless sensor network based automatic light controller in theater arts," in Engineering of Computer-Based Systems, 2007. ECBS'07. 14th Annual

system because it can produce the theoretical best result. One idea is to combine our current work with brute force method. The current work can be used during the initial step to quickly generate an accepted light setting. The brute force method can finely adjust the setting in the backend afterwards. Optimal results can be also stored for future use. VI.
CONCLUSION

IEEE, 2007, pp. 161-170. [13] T. Hiroyasu, A. Nakamura, S. Shinohara, M. Yoshimi, M. Miki, and H. Yokouchi, "Intelligent lighting control user interface through design of illuminance distribution," in Intelligent Systems Design and Applica tions, 2009. ISDA'09. Ninth International Conference on. IEEE, 2009, pp. 714-719. [14] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon, "Intelligent lighting control using sensor networks," in Networking,
IEEE International Conference and Workshops on the. Sensing and Control (lCNSC), 2013 10th 1EEE 1nternational Conference on. IEEE, 2013, pp. 170-175. [IS] M. Miki, A. Amamiya, and T. Hiroyasu, "Distributed optimal control of lighting based on stochastic hill climbing method with variable neighborhood," in Systems, Man and Cybernetics, 2007. ISle. IEEE International Conference on. IEEE, 2007, pp. 1676-1680. [16] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1-14. [17] Wikipedia. Lux. [Online]. Available: http: //en.wikipedia.orglwikilLux

To enable automated lighting control, under varying con ditions of occupancy, needs, outside lighting influences and other perturbations it is essential to have a core algorithm that is effective and adaptive. Such algorithm must be deploy able in a simple, cost effective system without the need for customizations and reprograrmning as conditions change. This paper presents such a core algorithm and tests its effectiveness

284
View publication stats

Digital Microfluidic Biochips (DMFB), a promising platform for Lab-on-chip systems are capable of automated biochemical analysis targeted for medical diagnostics and other biochemical applications. The inherent nature of reconfigurability and scalability enables the device to integrate multiple bioassay protocols within the same array for simultaneous execution. Each execution of Bioassay involves numerous microfluidic operations to be performed successfully within the DMFB. Optical Detection and analysis are one of the significant operations required to be performed in DMFB systems for conclusive diagnosis and testing of targeted parameters within the specified sample. In this paper we propose a new design for automated detection based analyzer system integrated with a multipartioned scalable DMFB for two stage confirmatory detection of multiple parameters for a given set of samples .Multiple bioassays are executed sequentially in two stages and the results are analyzed using an intelligent system with integrated memory containing precharacterized standard outputs. The design prototype is implemented on FPGA platform and the simulations and detection results for a set of specified bioassay protocols are found to be satisfactory and in conformance with conventional benchtop laboratory processes.Reliability has become an integral component of the design intent of embedded cyber-physical systems. Safety-critical embedded systems are designed with specific reliability targets, and design practices include the appropriate allocation of both spatial and temporal redundancies in the implementation to meet such requirements. With increasing complexity of such systems and considering the large number of components in such systems, redundancy allocation requires a formal scientific basis. In this work, we profess the analysis of the redundancy requirement upfront with the objective of making it an integral part of the specification. The underlying problem is one of synthesizing a formal specification with built-in redundancy artifacts, from the formal properties of the error-free system, the error probabilities of the control components, and the reliability target. We believe that upfront formal analysis of redundancy requirements is important in budgeting the resource requirements from a cost versus reliability perspective. Several case-studies from the automotive domain highlight the efficacy of our proposal.Abstract:
Droplet based microfluidic technology in recent years is reckoned as a major driving force for the development of new generation of Lab-on-chip devices. Such devices known as digital micro fluidic biochips are capable of manipulating discrete nanolitre volumes of droplets on a 2D planar array of electrodes. Due to their inherent nature of reconfigurability and scalability these devices are designed to be employed for large scale integration of multiple bioassays within the same grid. In order to enable such applications increasing number of control pin requirements together with high wire planning complexity becomes a major problem. In this paper we have proposed new techniques for interconnection wire routing for the control electrodes operating at identical time sequence. We have defined a double layer dual wire system running in parallel along two separate planes in mutually perpendicular directions. We further proposed an algorithm to develop a feasible wire plan for a given layout with an aim to optimize the overall number of pin count. Multiphasing on same pin has been proposed to resolve the issue of wire planning in cases of cross contamination at any particular site. The proposed technique has been employed in layouts using test benches for Benchmark suite III and selective test benches for benchmark suite I. The objective was to obtain a feasible wire plan with optimum pin utilization and enhanced route performance. The results obtained from simulation of the proposed algorithm on the test benches (mentioned earlier) are found to be encouraging.Ghosh, Sharma, Chakrabarti, & Dasgupta

are useful in practice. One example of such a problem is finding the secondary structure of RNA (Mathews & Zuker, 2004) which is an important problem in Bioinformatics. RNAs may be viewed as sequences of bases belonging to the set {Adenine(A), Cytocine(C), Guanine(G), Uracil(U) }. RNA molecules tend to loop back and form base pairs with itself and the resulting shape is called the secondary structure. The primary factor that influences the secondary structure of RNA is the number of base pairings (higher number of base pairings generally implies more stable secondary structure). Under the well established rules for base pairings, the problem of maximizing the number of base pairings has an interesting dynamic programming formulation. However, apart from the number of base pairings, there are other factors that influence the stability, but these factors are typically evaluated experimentally. Therefore, for a given RNA sequence, it is useful to compute a pool of candidate secondary structures (in decreasing order of the number of base pairings) that may be subjected to further experimental evaluation in order to determine the most stable secondary structure. The problem of generating ordered set of solutions is well studied in other domains. For discrete optimization problems, Lawler (1972) had proposed a general procedure for generating k-best solutions. A similar problem of finding k most probable configurations in probabilistic expert systems is addressed by Nilsson (1998). Fromer and Globerson (2009) have addressed the problem of finding k maximum probability assignments for probabilistic modeling using LP relaxation. In the context of ordinary graphs, Eppstein (1990) has studied the problem of finding k-smallest spanning trees. Subsequently, an algorithm for finding k-best shortest paths has been proposed in Eppstein's (1998) work. Hamacher and Queyranne (1985) have suggested an algorithm for k-best solutions to combinatorial optimization problems. Algorithms for generating k-best perfect matching are presented by Chegireddy and Hamacher (1987). Other researchers applied the k-shortest path problem to practical scenarios, such as, routing and transportation, and developed specific solutions (Takkala, Bornd¨ orfer, & L¨ obel, 2000; Subramanian, 1997; Topkis, 1988; Sugimoto & Katoh, 1985). However none of the approaches seems to be directly applicable for AND/OR structures. Recently some schemes related to ordered solutions to graphical models (Flerova & Dechter, 2011, 2010) and anytime AND/OR graph search (Otten & Dechter, 2011) have been proposed. Anytime algorithms for traditional OR search space (Hansen & Zhou, 2007) are well addressed by the research community. In this paper, we address the problem of generating ordered set of solutions for explicit AND/OR DAG structure and present new algorithms. The existing method, proposed by Elliott (2007), works bottom-up by computing k-best solutions for the current node from the k-best solutions of its children nodes. We present a best first search algorithm, named Alternative Solution Generation (ASG) for generating ordered set of solutions. The proposed algorithm maintains a list of candidate solutions, initially containing only the optimal solution, and iteratively generates the next solution in non-decreasing order of cost by selecting the minimum cost solution from the list. In each iteration, this minimum cost solution is used to construct another set of candidate solutions, which is again added to the current list. We present two versions of the algorithm ­ a. Basic ASG (will be referred to as ASG henceforth) : This version of the algorithm may construct a particular candidate solution more than once;

278

Generating Ordered Solutions for Explicit AND/OR Structures

b. Lazy ASG or LASG : Another version of ASG algorithm that constructs every candidate solution only once. In these algorithms, we use a compact representation, named signature, for storing the solutions. From the signature of a solution, the actual explicit form of that solution can be constructed through a top-down traversal of the given DAG. This representation allows the proposed algorithms to work in a top-down fashion starting from the initial optimal solution. Another salient feature of our proposed algorithms is that these algorithms work incrementally unlike the existing approach. Our proposed algorithms can be interrupted at any point of time during the execution and the set of ordered solutions obtained so far can be observed and subsequent solutions will be generated when the algorithms are resumed again. Moreover, if an upper limit estimate on the number of solutions required is known a priori, our algorithms can be further optimized using that estimate. The rest of the paper is organised as follows. The necessary formalisms and definitions are presented in Section 2. In Section 3, we address the problem of generating ordered set of solutions for trees. Subsequently in Section 4, we address the problem of finding alternative solutions of explicit acyclic AND/OR DAGs in non-decreasing order of cost. We present two different solution semantics for AND/OR DAGs and discuss the existing approach as well as our proposed approach, along with a comparative analysis. Detailed experimental results, including the comparison of the performance of the proposed algorithms with the existing algorithm (Elliott, 2007), are presented in Section 5. We have used randomly constructed trees and DAGs as well as some well-known problem domains including the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding the secondary structure of RNA as test domain. The time required and the memory used for generating a specific number of ordered solutions for different domains are reported in detail. In Section 6, we outline briefly about applying the proposed algorithms for implicitly specified AND/OR structures. Finally we present the concluding remarks in Section 7.

2. Definitions
In this section, we describe the terminology of AND/OR trees and DAGs followed by other definitions that are used in this paper. G = V, E is an AND/OR directed acyclic graph, where V is the set of nodes and E is the set of edges. Here  and  in G refer to the AND nodes and OR nodes in the DAG respectively. The direction of edges in G is from the parent node to the child node. The nodes of G with no successors are called terminal nodes. The non-terminal nodes of G are of two types ­ i) OR nodes and ii) AND nodes . V and V are the set of AND and OR nodes in G respectively, and n = |V |, n = |V |, and n = |V |. The start (or root) node of G is denoted by vR . OR edges and AND edges are the edges that emanate from OR nodes and AND nodes respectively. Definition 2.a [Solution Graph] A solution graph, S (vq ), rooted at any node vq  V , is a finite sub-graph of G defined as: a. vq is in S (vq );   is an OR node in G b. If vq  and vq is in S (vq ), then exactly one of its immediate successors in G is in S (vq );   is an AND node in G c. If vq  and vq is in S (vq ), then all its immediate successors in G are in S (vq );
279

Ghosh, Sharma, Chakrabarti, & Dasgupta

d. Every maximal (directed) path in S (vq ) ends in a terminal node; e. No node other than vq or its successors in G is in S (vq ). By a solution graph S of G we mean a solution graph with root vR .

 

Definition 2.b [Cost of a Solution Graph] In G , every edge eqr  E from node vq to node vr has a finite non-negative cost ce ( vq , vr ) or ce (eqr ). Similarly every node vq has a finite non-negative cost denoted by cv (vq ). The cost of a solution S is defined recursively as follows. For every node vq in S , the cost C (S, vq ) is:   cv (vq ), if vq is a terminal node;     cv (vq ) + C (S, vr ) + ce ( vq , vr ) , where vq is an OR node, and  C (S, vq ) = vr is the successor of vq in S ;    C (S, vj ) + ce ( vq , vj ) , where 1  j  k, vq is an AND node cv (vq ) +     with degree k, and v1 , . . . , vk are the immediate successors of vq in S. Therefore the cost of a solution S is C (S, vR ) which is also denoted by C (S ). We denote the optimal solution below every node vq as opt(vq ). Therefore, the optimal solution of the entire AND/OR DAG G , denoted by Sopt , is opt(vR ). The cost of the optimal solution rooted at every node vq in G is Copt (vq ), which is defined recursively (for minimum cost objective functions) as follows:   cv (vq ), if vq is a terminal node;      cv (vq ) + min Copt (vj ) + ce ( vq , vj ) , where 1  j  k, vq is an OR node Copt (vq ) = with degree k, and v1 , . . . , vk are the immediate successors of vq in G ;   cv (vq ) + Copt (vj ) + ce ( vq , vj ) , where 1  j  k, vq is an AND node     with degree k, and v1 , . . . , vk are the immediate successors of vq in G . The cost of the optimal solution Sopt of G is denoted by Copt (vR ) or, alternatively, by Copt (Sopt ). When the objective function needs to be maximized, instead of the min function, the max function is used in the definition of Copt (vq ).  

It may be noted that it is possible to have more than one solution below an OR node vq to qualify to be the optimal one, i.e., when they have the same cost, and that cost is the minimum. Ties for the optimal solution below any such OR node vq are resolved arbitrarily and only one among the qualifying solutions (determined after tie-breaking) is marked as opt(vq ). An AND/OR tree, T = V, E , is an AND/OR DAG and additionally satisfies the restrictions of a tree structure i.e., there can be at most one parent node for any node vq in T . In the context of AND/OR trees, we use eq to denote the edge that points to ^ = V, E , is an AND/OR tree with the the vertex vq . An alternating AND/OR tree, T restriction that there is an alternation between the AND nodes and the OR nodes. Every child of an AND node is either an OR node or a terminal node, and every children of an OR node is either an AND node or a terminal node. We use the term solution tree to denote the solutions of AND/OR trees. We also discuss a different solution semantics, namely tree based semantics, for AND/OR DAGs. Every AND/OR DAG can be converted to an equivalent AND/OR tree by traversing
280

Generating Ordered Solutions for Explicit AND/OR Structures

the intermediate nodes in reverse topological order and replicating the subtree rooted at every node whenever the in-degree of the traversed node is more than 1. The details are shown in Procedure ConvertDAG. Suppose an AND/OR DAG G is converted to an equivalent AND/OR tree T . We define the solutions of T as the solutions of G under tree based semantics. Procedure ConvertDAG(G ) input : An AND/OR DAG G output: An equivalent AND/OR tree T 1 Construct a list M , of non-terminal nodes of G , sorted in the reverse topological order; 2 while M is not empty do 3 vq  Remove the first element of M ; /* Suppose Ein (vq ) is the list of incoming edges of vq */ 4 if InDegree(vq ) > 1 then 5 for i  2 to InDegree(vq ) do 6 et  Ein (vq )[i];  as the root; Replicate the sub-tree rooted at vq with vq 7 ; Modify the target node of et from vq to vq 8 9 end 10 end 11 end In this paper we use the solution semantics defined in Definition 2.a as the default semantics for the solutions of AND/OR DAGs. When the tree based semantics is used, it is explicitly mentioned. 2.1 Example

2, 34

v1
2 1 1

v1

2, 89 2

3

3, 29

v2
1

v3
35 4

2, 37

v4
1 3

v2

3, 43 5 4

v3

2, 41 1

5

v4
40 4

v5

2, 35 3

v6
52

2, 8

v5
2

v6
12 2

3, 11

v7
3 1

4, 17

v8
1 2

v7
1

3, 9 2

v8
17

1

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

v9
5

v10
7

Figure 1: Alternating AND/OR Tree
281

Figure 2: AND/OR DAG

Ghosh, Sharma, Chakrabarti, & Dasgupta

We present an example of an alternating AND/OR tree in Figure 1. In the figure, the terminal nodes are represented by a circle with thick outline. AND nodes are shown in the figures with their outgoing edges connected by a semi-circular curve in all the examples. The edge costs are shown by the side of each edge within an angled bracket. The cost of the terminal nodes are shown inside a box. For every non-terminal node vq , the pair of costs, cv (vq ) and Copt (vq ), is shown inside a rectangle. In Figure 1 the optimal solution below every node is shown using by thick dashed edges with an arrow head. The optimal solution of the AND/OR tree can be traced by following these thick dashed edges from node v1 . The cost of the optimal solution tree is 34. Also, Figure 2 shows an example of a DAG; the cost of the optimal solution DAG is 89.

3. Generating Ordered Solutions for AND/OR Trees
In this section we address the problem of generating ordered solutions for trees. We use the notion of alternating AND/OR trees, defined in Section 2, to present our algorithms. An alternating AND/OR tree presents a succinct representation and so the correctness proofs are much simpler for alternating AND/OR trees. In Appendix C we show that every AND/OR tree can be converted to an equivalent alternating AND/OR tree with respect to the solution space. It is worth noting that the search space of some problems (e.g. the search space of multipeg Tower of Hanoi problem) exhibit the alternating AND/OR tree structure. Moreover, the algorithms that are presented for alternating AND/OR trees work without any modification for general AND/OR trees. In this section, first we present the existing algorithm (Elliott, 2007) briefly, and then we present our proposed algorithms in detail. 3.1 Existing Bottom-Up Evaluation Based Method for Computing Alternative Solutions We illustrate the working of the existing method that is proposed by Elliott (2007) for computing alternative solutions for trees using an example of an alternating AND/OR tree. This method (will be referred as BU henceforth) computes the k-best solutions in a bottomup fashion. At every node, vq , k-best solutions are computed from the k-best solutions of the children of vq . The overall idea is as follows. a. For an OR node vq , a solution rooted at vq is obtained by selecting a solution of a child. Therefore k-best solutions of vq are computed by selecting the top k solutions from the entire pool consisting of all solutions of all children. b. In the case of AND nodes, every child of an AND node vq will have at most k solutions. A solution rooted at an AND node vq is obtained by combining one solution from every child of vq . Different combinations of the solutions of the children nodes of vq generate different solutions rooted at vq . Among those combinations, top k combinations are stored for vq . In Figure 3 we show the working of the existing algorithm. At every intermediate node 2-best solutions are shown within rounded rectangle. At every OR node vq , the ith -best solution rooted at vq is shown as a triplet of the form ­ i : < child, solidx >, cost. For example, at node v1 the second best solution is shown as ­ 2 : v2 , 2 , 37; which means

282

Generating Ordered Solutions for Explicit AND/OR Structures

that the 2nd best solution rooted at v1 is obtained by selecting the 2nd best solution of v2 . Similarly, at every AND node vq , the ith solution rooted at vq is shown as a triplet of the form ­ i : |sol vec|, cost triplets. Here sol vec is a comma separated list of solution indices such that every element of sol vec corresponds to a child of vq . The j th element of sol vec shows the index of the solution of j th child. For example, the 2nd best solution rooted at v2 is shown as ­ 2 : |2, 1|, 32. This means the 2nd best solution rooted at v2 is computed using the 2nd best solution of the 1st child (which is v5 ) and the best solution (1st ) of the 2nd child (which is v6 ). Which index of sol vec corresponds to which child is shown by placing the child node name above every index position.
1 : v2 , 1 , 34 2 : v2 , 2 , 37

2, 34

v1

3

2

1

3, 29

v2

v5 v6 1 : |1, 1|, 29 2 : |2, 1|, 32
1

v3
35 4

2, 37

v4

v7 v8 1 : |1, 1|, 37 1 : |1, 2|, 40
3

5

2, 8

v5

1 : v9 , 1 , 8 2 : v10 , 1 , 11

v6
12 2

3, 11

v7

1 : v11 , 1 , 11 2 : v12 , 1 , 15

4, 17

v8

1 : v13 , 1 , 17 2 : v14 , 1 , 20

1

2

3

1

1

2

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 3: Example working of the existing algorithm The existing method works with the input parameter k, i.e., the number of solutions to be generated have to be known a priori. Also this method is not inherently incremental in nature, thus does not perform efficiently when the solutions are needed on demand, e.g., at first, top 20 solutions are needed, then the next 10 solutions are needed. In this case the top 20 solutions will have to be recomputed while computing next 10 solutions, i.e., from the 21st solution to the 30th solution. Next we present our proposed top-down approach which does not suffer from this limitation. 3.2 Top-Down Evaluation Algorithms for Generating Ordered Solutions So far we have discussed the existing approaches which primarily use bottom-up approach for computing ordered solutions. Now we propose a top-down approach for generating alternative solutions in the non-decreasing order of cost. It may be noted that the top-down
283

Ghosh, Sharma, Chakrabarti, & Dasgupta

approach is incremental in nature. We use an edge marking based algorithm, Alternative Solution Generation (ASG ), to generate the next best solutions from the previously generated solutions. In the initial phase of the ASG algorithm, we compute the optimal solution ^ and perform an initial marking of all OR edges. for a given alternating AND/OR tree T The following terminology and notions are used to describe the ASG algorithm. In the context of AND/OR trees, we use eq to denote the edge that points to the vertex vq . We will use the following definitions for describing our proposed top-down approaches. Definition 3.c [Aggregated Cost] In an AND/OR DAG G , the aggregated cost, ca , for an edge eij from node vi to node vj , is defined as : ca (eij ) = ce (eij ) + Copt (vj ).  
2, 34

v1

[e2 : 5] 3

2,3 : 5
[e3 : 1] 2

3,4 : 1

1

3, 29

v2

v3
35

2, 37

v4

5

1

4

3

2, 8

v5

v6
12 [e11 : 4]

3, 11

v7
[e13 : 3]

4, 17

v8

[e9 : 3] 1

9,10 : 3

2

2

11,12 : 4

3

1

[e14 : 6]

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 4: Example of OR-edge marking and swap option Marking of an OR edge : The notion of marking an OR edge is as follows. For an OR node vq , L(vq ) is the list of OR edges of vq sorted in non-decreasing order of the aggregated cost of the edges. We define (i,i+1) as the difference between the cost of OR edges, ei and ei+1 , such that ei and ei+1 emanate from the same OR node vq , and ei+1 is the edge next to ei in L(vq ). Procedure MarkOR describes the marking process for the OR edges of an OR node. Intuitively, a mark represents the cost increment incurred when the corresponding edge is replaced in a solution by its next best sibling. The OR edge having maximum aggregated cost is not marked. Consider a solution, Scur , containing the edge ei = (vq , vi ), where ei  Eopt (Scur ). We mark ei with the cost increment which will be incurred to construct the next best solution from Scur by choosing another child of vq . In Figure 4 the marks corresponding to OR edges e2 , e3 , e9 , e11 , e13 , and e14 are [e2 : 5], [e3 : 1], [e9 : 3], [e11 : 4], [e13 : 3], and [e14 : 6].
284

Generating Ordered Solutions for Explicit AND/OR Structures

Procedure MarkOR(vq )
1

2 3 4 5 6 7 8

Construct L(vq ) ; /* List of OR edges of vq sorted in the non-decreasing order of ca values */ count  number of elements in L(vq ) ; for i  1 to i = count - 1 do ec  L(vq )[i] ; en  L(vq )[i + 1] ; tmp = (ca (en ) - ca (ec )) ; Mark ec with the pair [en : tmp ] ; end

Definition 3.d [Swap Option] A swap option ij is defined as a three-tuple ei , ej , ij where ei and ej emanate from the same OR node vq , ej is the edge next to ei in L(vq ), and ij = ca (ej ) - ca (ei ). Also, we say that the swap option ij belongs to the OR node vq .  

Consider the OR node vq and the sorted list L(vq ). It may be observed that in L(vq ) every consecutive pair of edges forms a swap option. Therefore, if there are k edges in L(vq ), k -1 swap options will be formed. At node vq , these swap options are ranked according to the rank of their original edges in L(vq ). In Figure 4 the swap options are : (2,3) = e2 , e3 , 5 , (3,4) = e3 , e4 , 1 , (9,10) = e9 , e10 , 3 , (11,12) = e11 , e12 , 4 , (13,14) = e13 , e14 , 3 , and (14,15) = e14 , e15 , 6 . Consider the node v1 where L(v1 ) = e2 , e3 , e4 . Therefore, the swap options, (2,3) and (3,4) , belong to v1 . At node v1 , the rank of (2,3) and (3,4) are 1 and 2 respectively. Definition 3.e [Swap Operation] Swap operation is defined as the application of a swap option ij = ei , ej , ij to a solution Sm that contains the OR edge ei in the following way:  . Edge e is a. Remove the subtree rooted at vi from Sm . Let the modified tree be Sm i the original edge of ij .  , which is constructed at the previous step. Let the b. Add the subtree opt(vj ) to Sm  . Edge e is the swapped edge of  . newly constructed solution be Sm j ij  from S when Intuitively, a swap operation ij = ei , ej , ij constructs a new solution Sm m  Sm contains the OR edge ei . Moreover, the cost of Sm is increased by ij compared to cost of Sm if C (Sm , vi ) = Copt (vi ).  

Our proposed algorithms use a swap option based compact representation, named signature, for storing the solutions. Intuitively, any alternative solution can be described as a set of swap operations performed on the optimal solution Sopt . It is interesting to observe that while applying an ordered sequence of swap options, 1 , · · · , k , the application of each swap operation creates an intermediate alternative solution. For example, when the first swap option in the sequence, 1 , is applied to the optimal solution, Sopt , a new solution, say S1 , is constructed. Then, when the 2nd swap option, 2 , is applied to S1 , yet another solution S2 is constructed. Let Si denote the solution obtained by applying the swap options, 1 , · · · , i , on Sopt in this sequence. Although, an ordered sequence of swap options, like 1 , · · · , k , can itself be used as a compact representation of an alternative solution, the following key points are important to observe. A. Among all possible sequences that generate a particular solution, we need to preclude those sequences which contain redundant swap options (those swap options whose orig285

Ghosh, Sharma, Chakrabarti, & Dasgupta

inal edge is not present in the solution to which it is applied). This is formally defined later as superfluous swap options. Also the order of applying the swap options is another important aspect. There can be two swap options, i and j where 1  i < j  k such that the source edge of j belongs to the sub-tree which is included in the solution Si only after applying i to Si-1 . In this case, if we apply j at the place of i , i.e., apply j directly to Si-1 , it will have no effect as the source edge of j is not present in Si-1 , i.e., after swapping the location of i and j in the sequence, j becomes a redundant swap option and the solution constructed would be different for the swapped sequence from the original sequence. We formally define an order relation on a pair of swap options based on this observation in the later part of this section and formalize the compact representation of the solutions based on that order relation. B. Suppose the swap option j belongs to a node vpj . Now it is important to observe that the application of j on Sj -1 to construct Sj , invalidates the application of all other swap options that belong to an OR edge in the path from the root node to vpj in the solution Sj . This is because in Sj the application of any such swap option which belongs to an OR edge in the path from the root node to vpj would make the swap at vpj redundant. In fact, for each swap option i belonging to node vpi , where 1  i  j , the application of all other swap options that belong to an OR edge in the path from the root node to vpi is invalidated in the solution Sj for the same reason. This condition restricts the set of swap options that can be applied on a particular solution. C. Finally, there can be two swap options i and j for 1  i < j  k such that i and j are independent of each other, that is, (a) applying i to Si-1 and subsequently the application of j to Sj -1 , and (b) applying j to Si-1 and subsequently the application of i to Sj -1 , ultimately construct the same solution. This happens only when the original edges of both i and j are present in Si-1 , thus application of one swap option does not influence the application of the other. However, it is desirable to use only one way to generate solution Sj . In Section 3.3, we propose a variation of the top-down approach (called LASG) which resolves this issue. ^ ] We define an order relation, namely R ^ , between a pair Definition 3.f [Order Relation R of swap options as follows. ^ , where ei and er are OR edges, qi and rj are a. If there is a path from vi to vr in T ^ ^. swap options, then (qi , rj )  R. For example, in Figure 4 ((3,4) , (13,14) )  R b. If pq = ep , eq , pq and rt = er , et , rt are two swap options such that vq = vr , ^ . In Figure 4 ((2,3) , (3,4) )  R ^. then (pq , rt )  R   Implicit Representation of the Solutions : We use an implicit representation for storing every solution other than the optimal one. These other solutions can be constructed from the optimal solution by applying a set of swap options to the optimal solution in the ^ , i has to be applied before j . Therefore, every solution is following way. If (i , j )  R ^ ^ if (i , j )  R ^. represented as a sequence  of swap options, where i appears before j in  Intuitively the application of every swap option specifies that the swapped edge will be the ^ , it may so part of the solution. Since the swap options are applied in the specific order R happen that an OR edge which had become the part of solution due to the application of an earlier swap option and may get swapped out due to the application of a later swap option.
286

Generating Ordered Solutions for Explicit AND/OR Structures

^ = Definition 3.g [Superfluous Swap Option] Consider a sequence of swap options  1 , · · · , m corresponding to a solution Sm . Clearly it is possible for a swap option, i , where 1  i  m, to be present in the sequence such that the original edge of i is not present in the solution Si-1 which is constructed by the successive applications of swap options 1 , · · · , i-1 to solution Sopt . Now the application of i has no effect on Si-1 , i.e., solution Si is identical to solution Si-1 . Each such swap option i is a superfluous swap ^ of swap options corresponding to solution Sm . option with respect to the sequence    Property 3.1 The sequence of swap options corresponding to a solution is minimal, if it has no superfluous swap option. This property follows from the definition of superfluous swap options and the notion of the implicit representation of a solution. Definition 3.h [Signature of a Solution] The minimal sequence of swap options corresponding to a solution, Sm , is defined as the signature, Sig(Sm ), of that solution. It ^ , may be noted that for the optimal solution Sopt of any alternating AND/OR tree T Sig(Sopt ) = {}, i.e., an empty sequence. It is possible to construct more than one signature ^ is a partial order. It is important to observe that all different signatures for a solution, as R for a particular solution are of equal length and the sets of swap options corresponding to these different signatures are also equal. Therefore the set of swap options corresponding to a signature is a canonical representation of the signature. Henceforth we will use the set notation for describing the signature of a solution.
2, 39

v1

3

2,3 : 5
2

3,4 : 1

1

3, 29

v2

v3
35

2, 37

v4

5

1

4

3

2, 8

v5

v6
12

3, 11

v7

4, 17

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 5: A solution, S2 , of the AND/OR tree shown in Figure 4 In Figure 5 we show a solution, say S2 , of the AND/OR tree shown in Figure 4. The solution is highlighted using thick dashed lines with arrow head. The pair, cv (vq ), C (S2 , vq ),
287

Ghosh, Sharma, Chakrabarti, & Dasgupta

is shown within rectangles beside each node vq in solution S2 , and we have used the rectangles with rounded corner whenever C (S2 , vq ) = Copt (vq ). Since S2 is generated by applying the swap option (2,3) to solution Sopt , the signature of S2 , Sig(S2 ) = (2,3) . Consider ^ 2 = (2,3) , (9,10) , of swap options. It is worth noting that  ^ 2 also another sequence,  ^ 2 , namely 9,10 , can not be represents the solution S2 . Here the second swap option in  applied to the solution constructed by applying (2,3) to Sopt as the source edge of (9,10) , ^2 . e9 , is not present in that solution. Hence (9,10) is a superfluous swap option for  Definition 3.i [Vopt and Eopt ] For any solution graph Sm of an AND/OR DAG G , we define a set of nodes, Vopt (Sm ), and a set of OR edges, Eopt (Sm ), as: a. Vopt (Sm ) = vq vq in Sm and solution graph Sm (vq ) is identical to the solution graph opt(vq ) b. Eopt (Sm ) = epr OR edge epr in Sm , and vr  Vopt (Sm ) Clearly, for any node vq  Vopt (Sm ), if vq is present in Sopt , then ­ (a) the solution graph Sm (vq ) is identical to the solution graph Sopt (vq ), and (b) C (Sm , vq ) = Copt (vq )   Definition 3.j [Swap List] The swap list corresponding to a solution Sm , L(Sm ), is the list of swap options that are applicable to Sm . Let Sig(Sm ) = {1 , · · · , m } and i, 1  i  m, each swap option i belongs to node vpi . The application of all other swap options that belong to the OR edges in the path from the root node to vpi is invalidated in the solution Sm . Hence, only the remaining swap options that are not invalidated in Sm can be applied to Sm for constructing the successor solutions of Sm . It is important to observe that for a swap option i , if the source edge of i belongs to Eopt (Sm ), the application is not invalidated in Sm . Hence, for a solution Sm , we construct L(Sm ) by restricting the swap operations only on the edges belonging to Eopt (Sm ). Moreover, this condition also ensures that the cost of a newly constructed solution can be computed directly form the cost of the parent solution and the  value of the applied swap  is constructed form S option. To elaborate, suppose solution Sm m by applying jk . The   ) = C (S ) +  cost of Sm can be computed directly form C (Sm ) and jk as : C (Sm m jk if ej  Eopt (Sm ). Procedure ComputeSwapList(Sm ) describes the details of computing swap options for a given solution Sm .   Procedure ComputeSwapList(Sm)
1 2 3

4 5 6

L(Sm )  ; Compute Eopt (Sm ); foreach OR edge ec in Eopt (Sm ) do if there exists a swap option on edge ec then /* Suppose ec emanates from OR node vq such that ec = L(vq )[i]. Also ec is marked with the pair tmp , en , where en = L(vq )[i + 1] */ cn  ec , en , tmp ; Add cn to L(Sm ); end end

The swap list of the optimal solution, L(Sopt ), in Figure 4, is {(2,3) , (9,10) }. In the solution S1 , shown in Figure 6, Vopt = {v6 , v10 }, because except node v6 and v10 , for all other nodes vi in S1 , opt(vi ) = S1 (vi ). Here also rectangles with rounded corner are used when C (S1 , vq ) = Copt (vq ). Therefore, Eopt = {e6 , e10 }. Since there exists no swap option
288

Generating Ordered Solutions for Explicit AND/OR Structures

2, 37

v1

3

2,3 : 5
2

3,4 : 1

1

3, 32

v2

v3
35

2, 37

v4

5

1

4

3

2, 11

v5

v6
12

3, 11

v7

4, 17

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 6: A solution, S1 , of the AND/OR tree shown in Figure 4 on the OR edges, e6 and e10 , the swap list of solution S1 , L(S1 ) = . Hence, for a solution Sm , L(Sm ) may be empty, though Vopt (Sm ) can never be empty. Although we use the notation ij to denote a swap option with edge ei as the original edge and edge ej as the swapped edge, for succinct representation, we also use  with a single subscript, such as 3 , k , ij etc., to represent a swap option. This alternative representation of swap options does not relate to any edge. Definition 3.k [Successors and Predecessors of a Solution] The set of successors and predecessors of a solution Sm is defined as:   can be constructed from S a. Succ(Sm ) = {Sm Sm m by applying a swap option that belongs to the swap list of Sm }  )}    Sm  Succ(Sm b. P red(Sm ) = {Sm ^ the following stateProperty 3.2 For any solution Sm of an alternating AND/OR tree T   ment holds: Sm  P red(Sm ), C (Sm )  C (Sm ) The property follows from the definitions. One special case requires attention. Consider  ) = C (S ) and S   P red(S ). This case can only arise when a swap the case when C (Sm m m m option of cost 0 is applied to Sm . This occurs in the case of a tie. 3.2.1 ASG Algorithm We present ASG, a best first search algorithm, for generating solutions for an alternating AND/OR tree in non-decreasing order of costs. The overall idea of this algorithm is as follows. We maintain a list, Open, which initially contains only the optimal solution Sopt . At any point of time Open contains a set of candidate solutions from which the next best
289

Ghosh, Sharma, Chakrabarti, & Dasgupta

solution in the non-decreasing order of cost is selected. At each iteration the minimum cost solution (Smin ) in Open is removed from Open and added to another list, named, Closed. The Closed list contains the set of ordered solutions generated so far. Then the successor set of Smin is constructed and any successor solution which is not currently present in Open as well as is not already added to Closed is inserted to Open. However as a further optimization, we use a sublist of Closed, named TList, to store the relevant portion of Closed such that checking with respect to the solutions in TList is sufficient to figure out whether the successor solution is already added to Closed. It is interesting to observe that this algorithm can be interrupted at any time and the set of ordered solutions computed so far can be obtained. Also, the algorithm can be resumed if some more solutions are needed. The details of ASG algorithm are presented in Algorithm 4. Algorithm 4: Alternative Solution Generation (ASG) Algorithm ^ input : An alternating AND/OR tree T ^ in the non-decreasing order of cost output: Alternative solutions of T 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create three lists, Open, Closed, and TList, that are initially empty; 3 Put Sopt in Open; 4 lastSolCost  C (Sopt ); 5 while Open is not empty do 6 Smin  Remove the minimum cost solution from Open ; 7 if lastSolCost < C (Smin ) then 8 Remove all the elements of TList; 9 lastSolCost  C (Smin ); 10 end 11 Add Smin to Closed and TList; 12 Compute the swap list, L(Smin ), of Smin ; /* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open */ 13 foreach ij  L(Smin ) do 14 Construct Sm by applying ij to Smin ; 15 Construct the signature of Sm , Sig(Sm ), by concatenating ij after Sig(Smin ); /* Check whether Sm is already present in Open or in TList */ 16 if (Sm not in Open) and (Sm not in TList) then 17 Add Sm to Open; 18 end 19 end 20 Report the solutions in Closed; The pseudo-code from Line-1 to Line-4 computes the optimal solution Sopt , performs the marking of OR edges, populates the swap options, and initializes Open, Closed and TList. The loop in Line-10 is responsible for generating a new solution every time it is executed as long as Open is not empty. In Line-6 of the ASG algorithm, the solution that is the current minimum cost solution in Open (Smin ) is selected and removed from Open. The TList is populated and maintained from Line-7 to Line-10. The loop in Line-13 generates

290

Generating Ordered Solutions for Explicit AND/OR Structures

the successor solutions of Smin one by one and adds the newly constructed solutions to Open if the newly constructed solution is not already present in Open as well as not added to TList (Line-16 does the checking). The proof of correctness of Algorithm 4 is presented in Appendix A. We discuss the following issues related to Algorithm 4. Checking for Duplication : In order to check whether a particular solution Si is already present in Open or TList, the signature of Si is matched with the signatures of the solutions that are already present in Open and TList. It is sufficient to check the equality between the set of swap options in the respective signatures because that set is unique for a particular solution. It may be noted that TList is used as an optimization, which avoids searching the entire Closed list. Resolving Ties : While removing the minimum cost solution from the Open list, a tie may be encountered among a set of solutions. Suppose there is a tie among the set Stie = {S1 , · · · , Sk }. The ties are resolved in the favor of the predecessor solutions, that is, Si , Sj  Stie , (If Si is the predecessor of Sj )  (Si is removed before Sj ) For all other cases the ties are resolved arbitrarily in the favor of the solution which was added to Open first. 3.2.2 Working of ASG Algorithm We illustrate the working of the ASG algorithm on the example AND/OR tree shown in Figure 4. The contents of the different lists obtained after first few iterations of outermost while loop are shown in Table 1. We use the signature of a solution for representation purpose. The solutions that are already present in Open and also constructed by expanding the current Smin , are highlighted with under-braces.
It. 1 2 3 4 5 Smin {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } L(Smin ) (2,3) , (9,10)  (3,4) (11,12) , (13,14) (11,12) , (14,15) Open {(2,3) }, {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) } {(2,3) , (3,4) , (13,14) , (11,12) }, {(2,3) , (3,4) , (13,14) , (14,15) } Closed {} {}, {(9,10) } {}, {(9,10) }, {(2,3) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {}, {(9,10) }, {(2,3) }, TList {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4),

6

(13,14)

7

{(2,3) , (3,4) , (13,14) , (11,12) }

(14,15)

{(2,3) , (3,4) } (11,12) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) , {(2,3) , (3,4) , (13,14) , {(2,3) , (3,4) } (13,14) , (11,12) } (11,12) , (14,15) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (11,12) }

Table 1: Working of ASG Algorithm
291

Ghosh, Sharma, Chakrabarti, & Dasgupta

Before entering the outermost while loop (Line 5), ASG computes the optimal solution Sopt , populates the swap options, and inserts Sopt to Open. Thus, at this point of time, Open contains only the optimal solution Sopt ; Closed and TList are empty. In the first iteration Sopt (the signature of Sopt is {}) is selected and removed from Open. Then the swap list of Sopt , L(Sopt ), is computed. L(Sopt ), consists of two swap options, namely (2,3) and (9,10) . ASG adds two new solutions {(2,3) } and {(9,10) } to Open. Then solution Sopt is added to both Closed and TList. In the next iteration, solution {(9,10) } which has the minimum cost among the solutions currently in Open, is selected and removed from Open, the swap list {(9,10) } is computed and subsequently {(9,10) } is added to Open and TList. As it happens, L({(9,10) }) =  (owing to the fact that Eopt = {e6 , e10 } and there exists no swap option on the OR edges, e6 and e10 ), thus nothing else happens in this iteration. In the next iteration, solution {(2,3) } is removed from Open and ultimately solution {(2,3) , (3,4) } is added to Open after adding {(2,3) } to Closed as well as to TList. Next two iterations proceed in a similar fashion. Now, consider the 6th iteration. In this iteration, solution {(2,3) , (3,4) , (11,12) } is removed from Open, and its successor set has only one solution, {(2,3) , (3,4) , (11,12) , (13,14) }, which is already present in Open (inserted to Open in Iteration-5). Therefore, solution {(2,3) , (3,4) , (11,12) , (13,14) } is not inserted to Open again. We have shown up to Iteration-7 in Table 1. 3.3 Technique for Avoiding the Checking for Duplicates in Open In this section, we present a technique to avoid the checking done before adding a newly constructed solution Sm to Open to determine whether Sm is already present in Open. We first explain the scenario with an example, which is a portion of the previous example shown in Figure 4. In Figure 7-10, the solutions are shown using thick dashed line with arrow head. Also the rectangles with rounded corner are used to highlight the fact that the corresponding node in the marked solution does not belong to the Vopt set of that solution.
2, 37

v4
3 4

2, 44

v4
3

4

3, 11

v7
3 1

4, 17

v8
1 2

3, 15

v7
3 1

4, 20

v8
1

2 11,12 : 4

13,14 : 3

v11
6

v12
9

v13
12

v14
15

v11
6

v12
9

v13
12

v14
15

Figure 7: Running Example

Figure 8: Solution S3

Consider the solutions S1 , S2 and S3 (shown in Figure 9, Figure 10 and Figure 8). Here (a) L(Sopt ) = {(11,12) , (13,14) }, (b) Succ(Sopt ) = {S1 , S2 }, (c) Sig(S1 ) = {(13,14) }, (d) Sig(S2 ) = {(11,12) }, and (e) Sig(S3 ) = {(13,14) , (11,12) }. Algorithm 4 constructs the solution S3 (shown in Figure 8) for adding to Open twice ­ (i) as a part of adding Succ(S1 ) to Open, and (ii) while adding Succ(S2 ) to Open.
292

Generating Ordered Solutions for Explicit AND/OR Structures

2, 40

v4
3 4

2, 41

v4
3

4

3, 11

v7
3 1

4, 20

v8
1 2

3, 15

v7
3 1

4, 17

v8
1

2 11,12 : 4

13,14 : 3

v11
6

v12
9

v13
12

v14
15

v11
6

v12
9

v13
12

v14
15

Figure 9: Solution S1

Figure 10: Solution S2

Clearly Sopt is the root node of G s .

We use the following definitions to describe another version of the ASG algorithm, which constructs the solutions in such a way that the check to find out whether a solution is already added to Open is avoided. Definition 3.l [Solution Space DAG(SSDAG)] The solution space DAG of an alternating ^ is a directed acyclic graph (DAG), G s = V , E , where V is the set of all AND/OR tree T ^ , and E is the set of edges which is defined as: possible solutions of the AND/OR tree T   Sp , Sm  V , and   s E = es pm epm is a directed edge from node Sp to Sm , and   Sm  Succ(Sp )

 

Definition 3.m [Solution Space Tree and Completeness] A solution space tree of an ^ is a tree T s = V t , E t where V t  V , where V is the set of alternating AND/OR tree T ^ , and E t is the set of edges which is defined all possible solutions of the AND/OR tree T as:   Sp , Sm  V t , and       s e is a directed edge from node S to S , and p m s t pm E = epm Sp  P red(Sm ), and         P red(S ), (S = S  )  there is no edge between S  and S .  Sp m m p p p The sibling set for a solution Sm , is denoted using Sib(T s , Sm ). A solution space tree T s for an AND/OR tree is complete if V t = V .   It may be noted that the complete solution space tree of an alternating AND/OR tree is not necessarily unique. It is possible for an alternating AND/OR tree to have more than one complete solution space tree. However the solution space DAG for any AND/OR tree is unique. Definition 3.n [Native Swap Options of a Solution] Consider a solution Sm of an al^ . Suppose Sm is constructed by applying swap option ij to ternating AND/OR tree T solution Sp . Since swap option ij = ei , ej , ij is used to construct Sm , AND node vj is present in Sm . The native swap options of solution Sm with respect to swap option ij , N (Sm , ij ), is a subset of L(Sm ), and comprises of the following swap options :
293

Ghosh, Sharma, Chakrabarti, & Dasgupta

2, 49

v1

3

2,3 : 5
2

3,4 : 1

1

3, 32

v2

v3
35

2, 43

v4

5

1

4

3

2, 11

v5

v6
12

3, 11

v7

4, 23

v8

1

9,10 : 3

2

2

11,12 : 4

3

1

1

2

13,14 : 3 14,15 : 6

v9
5

v10
7

v11
6

v12
9

v13
12

v14
15

v15
20

Figure 11: A solution, S4 , of the AND/OR tree shown in Figure 4 a. jk , where jk is the swap option on the edge ej b. each t , if t belongs to an OR node vq where vq is a node in Sm (vj ) We use the term N (Sm ) to denote the native swap options when ij is understood from the context. Intuitively the native swap options for solution Sm are the swap options that become available immediately after applying ij , but were not available in the predecessor solution of Sm .  

Consider the solution S4 shown in Figure 11 where Sig(S4 ) = {(2,3) , (3,4) , (13,14) }. The solution is highlighted using thick dashed lines with arrow head. We have used the rectangles with rounded corner beside each node vq in solution S4 , where C (S4 , vq ) = Copt (vq ). Suppose S4 is constructed form solution S3 (where Sig(S3 ) = {(2,3) , (3,4) }) using swap option (13,14) . Here N (S4 , (13,14) ) = {(14,15) } whereas L(S4 ) = {(11,12) , (14,15) }. Now consider solution S6 where Sig(S6 ) = {(2,3) , (3,4) , (11,12) , (13,14) ). It is worth observing that applying only the native swap options to S4 instead of all swap options in L(S4 ) prevents the construction of solution S6 from solution S4 . S6 can also be constructed by applying (13,14) to solution S5 , where Sig(S5 ) = {(2,3) , (3,4) , (11,12) }. However, it may be noted that (13,14) is not a native swap option of solution S5 . 3.3.1 Lazy ASG Algorithm The intuition behind the other version of the ASG algorithm is as follows. For a newly constructed solution Sm , we need to check whether Sm is already present in Open because Sm can be constructed as a part of computing the successor set of multiple solutions. Instead of using the entire swap list of a solution to construct all successors at once and then add those solutions to Open, using the native swap options for constructing a subset of the successor set ensures the following. The subset constructed using native swap options
294

Generating Ordered Solutions for Explicit AND/OR Structures

consists of only those solutions that are currently not present in Open and thus can be added to Open without comparing with the existing entries in Open. The construction of  of S each remaining successor solution Sm m and then insertion to Open is delayed until  is added to Closed. every other predecessor solution of Sm Algorithm 5: Lazy ASG (LASG) Algorithm ^ input : An alternating AND/OR tree T ^ in the non-decreasing order of cost output: Alternative solutions of T 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create two lists, Open and Closed, that are initially empty; 3 Put Sopt in the Closed list; 4 Create a solution space tree T s with Sopt as root; 5 Compute the swap list, L(Sopt ), of Sopt ; 6 Construct Succ(Sopt ) using L(Sopt ); 7 forall Sm  Succ(Sopt ) do 8 Add Sm to Open; 9 end 10 while Open is not empty do 11 Smin  Remove the minimum cost solution from Open ; /* Suppose Smin is constructed from Sm applying swap option ij */ 12 Add a node corresponding to Smin in T s and connect that node using an edge from Sm ; 13 Compute the swap list L(Smin ) and the list of native swap options N (Smin , ij ); /* Expansion using native swap options */ 14 foreach tmp  N (Smin , ij ) do 15 Construct Stmp from Smin by applying tmp ; 16 Construct the signature of Stmp , Sig(Stmp ), by concatenating tmp after Sig(Smin ); 17 Add Stmp to Open; 18 end /* Lazy Expansion */ s 19 forall Sp  Sib(T , Smin ) do 20 if ij  L(Sp ) then  from S using  ; Construct Sp 21 p ij  , Sig (S  ), by concatenating  after Sig (S ); Construct the signature of Sp 22 ij p p  Add Sp to Open; 23 24 end 25 end 26 Add Smin to Closed; 27 end 28 Report the solutions in Closed; The solution space tree T s is maintained throughout the course of the algorithm to  is added to Closed. Based on this idea we determine when every other predecessor of Sm
295

Ghosh, Sharma, Chakrabarti, & Dasgupta

present a lazy version of ASG algorithm, named LASG. After selecting the minimum cost solution from Open, the algorithm explores the successor set of the current minimum cost solution in a lazy fashion. For a solution Sm , at first a subset of Succ(Sm ) is constructed using only the native swap options of Sm . The other solutions that belong to Succ(Sm ) are explored as late as possible as described above. For resolving ties, LASG algorithm uses the same strategy which is used by ASG algorithm. The details of LASG algorithm are presented in Algorithm 5. The proof of correctness of this algorithm is presented in Appendix B. Consider the example tree shown in Figure 7 and solutions S1 and S2 (shown in Figure 9 and Figure 10). Initially the Open will contain only Sopt and N (Sopt ) = {(11,12) , (13,14) }. When Sopt is selected from Open, both S1 and S2 is added to Open. Next S1 will be selected followed by S2 . Since, N (S1 ) =  and N (S2 ) = , after selecting S1 or S2 no successor solutions are constructed using the native swap list. Among the predecessors of S3 , S2 is added last to Closed. After selecting and removing S2 from Open, solution S3 is constructed from the previously selected predecessor S1 using the swap option (11,12) which is used to construct solution S2 from Sopt . 3.3.2 Working of LASG Algorithm (on AND/OR tree in Figure 4) Before entering the outermost while loop (Algorithm 5, Line 10), LASG computes the optimal solution Sopt and constructs Succ(Sopt ). Then the solutions in Succ(Sopt ) are added to Open and the contents of the Open becomes {(2,3) }, {(9,10) } . The contents of the different lists when a solution is added to Closed are shown in Table 2. The solutions are represented using their signatures. The solutions that are added to Open as a result of lazy expansion, are highlighted using under-brace.
Iteration 1 2 3 4 Smin {} {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } N (Smin ) (2,3) , (9,10)  (3,4) (11,12) , (13,14) (14,15) Closed {} {}, {(9,10) } {}, {(9,10) }, {(2,3) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) , (13,14) , (11,12) } {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) }, {(2,3) , (3,4) } {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) } {(2,3) , (3,4) , (13,14) , (11,12) } Open {(2,3) }, {(9,10) } {(2,3) } {(2,3) , (3,4) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) } {(2,3) , (3,4) , (11,12) }, {(2,3) , (3,4) , (13,14) , (14,15) }

5

{(2,3) , (3,4) , (11,12) }



6

{(2,3) , (3,4) , (13,14) , (11,12) }



Table 2: Working of LASG Algorithm While generating the first four solutions, the contents of the different lists for LASG are identical to the contents of the corresponding lists of ASG (shown in Table 1). For
296

Generating Ordered Solutions for Explicit AND/OR Structures

each of these soltuions, the native swap list is equal to the actual swap list of that solution. It is worth noting that, unlike ASG, for LASG the outermost while loop starts after generating the optimal solution Sopt , thus while generating the same solution the iteration number for LASG is less than that of ASG by 1. In the 4th iteration, for solution S4 = {(2,3) , (3,4) , (13,14) } the native swap list is not equal to the swap list as described previously. The same holds true for solution S5 = {(2,3) , (3,4) , (11,12) } and solution S6 = {(2,3) , (3,4) , (13,14) , (11,12) }. It is important to observe that LASG adds the solution S6 = {(2,3) , (3,4) , (13,14) , (11,12) } to Open after the generation of solution S5 = {(2,3) , (3,4) , (11,12) } as a part of lazy expansion (highlighted using under-brace in Table 2). Whereas, the ASG algorithm adds S6 to Open after generating solution S4 = {(2,3) , (3,4) , (13,14) }. 3.4 Complexity Analysis and Comparison among ASG, LASG and BU In this section we present a complexity analysis of ASG and LASG and compare them with BU. We will use the following parameters in the analysis. a. n and n denote the total number of nodes and the number of OR nodes in an alternating AND/OR tree. b. d denotes the out degree of the OR node having maximum number of children. c. m denotes the maximum number of OR edges in a solution. d. o denotes the maximum size of Open. We will present the complexity analysis for generating c solutions. Therefore the size of Closed is O(c). 3.4.1 Complexity of ASG Time Complexity : The time complexity of the major steps of Algorithm 4 are as follows. a. Computing the first solution can be done in bottom-up fashion, thus requiring O(n ) steps. The edges emanating from an OR node are sorted in the non-decreasing order of aggregated cost to compute the marks of the OR edges, the marking process takes O n .d. log d . Since the value of d is not very large in general (can be upper bounded by a constant), O n .d. log d = O(n ). b. The number of swap options available to a solution can be at most equal to the number of OR edges in that solution. Thus, the swap list for every solution can be built in O(m) time. For c solutions, generating swap options take O(c.m). c. Since the size of the successor set of a solution can be m at most, the size of Open, o can at most be c.m. Also the size of the TList can at most be equal to c (the size of Closed). d. The Open list can be implemented using Fibonacci heap. Individual insert and delete operation on Open take O(1)(amortized) and O(lg o) time respectively. Hence, for inserting in the Open and deleting from Open altogether takes O(o. lg o) time which is O(c.m. log(c.m)). e. The checking for duplicates requires scanning the entire Open and TList. Since the length of TList can be at most c, for a newly constructed solution this checking takes O(c + o) time and at most O(c + o) solutions are generated. Since O(c + o) is actually O(o), for generating c solutions, this step takes O(o)2 time. Also, the maximum value
297

Ghosh, Sharma, Chakrabarti, & Dasgupta

of o can be O(c.m). Thus, the time complexity of this step is O(c.m)2 . Clearly this step dominates O(o. lg o) which is the total time taken for all insertions into the Open and deletions from Open. However, this time bound can be further improved if we maintain a hash map of the solutions in the Open and TList, and in this case the checking for duplicates can be done in O(o) time. In that case O(o. lg o) (total time taken for all insertions into the Open and deletions from Open) becomes dominant over the time required for checking for duplicates. f. An upper limit estimate of m could be made by estimating the size of a solution tree  which is n for regular and complete alternating AND/OR trees. It is important to observe that the value of m is independent of the average out degree of a node in ^ . T Combining the above factors together we get the time complexity of ASG algorithm as : O n + o2 = O n + (c.m)2 = O n + c2 .n = O(c2 .n ) However if the additional hash map is used the time complexity is further reduced to :   O n + o. lg o = O n + c. n . lg(c.n ) = O n + n .(c. lg c + c. lg n ) Space Complexity: The following data-structures primarily contribute to the space complexity of ASG algorithm. a. Three lists, namely, Open, Closed, and TList are maintained throughout the course of the running ASG. This contributes a O(o + c) factor, which is O(o). b. Since the number of swap options is upper bounded by the total number of OR edges, constructing the swap list contributes the factor, O(n .d) to the space complexity. Also marking a solution requires putting a mark at every OR node of the AND/OR tree, thus adding another O(n ) space which is clearly dominated by the previous O(n .d) factor. c. Since the signature of a solution is essentially a set of swap options, the size of a signature is upper bounded by the total number of swap options available. Combining the Open and Closed list, altogether (c + o) solutions need to be stored. Since (c + o) is O(o), total space required for storing the solutions is O o.n .d . Combining the above factors together we get the space complexity of ASG algorithm as : O o + n .d + o.n .d = O(o.n .d) When an additional hash map is used to improve the time complexity, another additional O o.n .d space is required for maintaining the hash map. Although the exact space requirement is doubled, asymptotically the space complexity remains same. 3.4.2 Complexity of LASG Time Complexity : Compared to Algorithm 4, Algorithm 5 does not check for the duplicates and adds the solution to Open only when it is required. Therefore the other terms in the complexity remain the same except the term corresponding to the checking for duplicates. However, here T s is created and maintained during the course of Algorithm 5. Creating and maintaining the tree require O(c) time. Also during the lazy expansion the swap list of the previously generated sibling solutions are searched (Line 19 and Line 20 of Algorithm 5). The size of the swap list of any solution is O(m), where m is the maximum number of OR edges in a solution. Also there can be at most O(m) sibling solutions for a
298

Generating Ordered Solutions for Explicit AND/OR Structures

solution. Therefore the complexity of the lazy expansion is O(c.m2 ). Since O(c.m2 ) is the dominant factor, the time complexity of LASG is O(c.m2 ) = O(c.n ). Space Complexity : Compared to ASG algorithm, LASG algorithm does not maintain the TList. However LASG maintains the solution space tree T s whose size is equal to the Closed list, thus adding another O(c) factor to the space complexity incurred by ASG algorithm. It is interesting to observe that the worst case space complexity remains O(o + n .d + o.n .d) = O(o.n .d) which is equal to the space complexity of ASG algorithm. 3.4.3 Comparison with BU The time complexity of generating the c best solutions for an AND/OR tree is O(n .c. log c) and the space complexity is O(n .c). The detailed analysis can be found in the work of Elliott (2007). Since, n .d = O(n ), the space complexity of both ASG and LASG algorithm reduces to O(n .c) and the time complexity of LASG is log c factor better than BU whereas the time complexity of ASG is quadratic with respect to c compared to the (c. log c) factor of BU. When an additional hash-map is used to reduce the time overhead of duplicate checking, ASG beats both LASG and BU both in terms time complexity, as both  O(n ) and O n .(c. lg c + c. lg n ) is asymptotically lower than O(n .c. log c). However this worst case complexity is only possible for AND/OR trees where no duplicate solution is generated. Empirical results show that the length of Open, o hardly reaches O(c.m).

4. Ordered Solution Generation for AND/OR DAGs
In this section, we present the problem of generating solutions in non-decreasing order of cost for a given AND/OR DAG. We present the working of the existing algorithm for generating solution for both tree based semantics and default semantics. Next we present the modifications in ASG and LASG for handling DAG. 4.1 Existing Bottom-Up Algorithm Figure 12 shows an example working of the existing bottom-up approach, BU, on the AND/OR DAG in Figure 2. We use the notations that are used in Figure 3 to describe different solutions in Figure 12 and the generation of the top 2 solutions under tree-based semantics is shown. It is important to notice that although BU correctly generates alternative solutions of an AND/OR DAGs under tree based semantics, BU may generate some solutions which are invalid under default semantics. In Figure 13 we present a solution of the AND/OR DAG in Figure 2. This solution is an example of such a solution which is correct under tree-based semantics but is invalid under default semantics. The solution DAG (highlighted using thick dashed lines with arrow heads) in Figure 13 will be generated as the 3rd solution of the AND/OR DAG in Figure 2 while running BU. At every non-terminal node, the entry (within rectangle) corresponding to the 3rd solution is highlighted using bold face. It may be noted that the terminal nodes, v9 and v10 , are included in the solution DAG though both of them emanate from the same parent OR node. Therefore, this solution is not a valid one under default semantics.
299

Ghosh, Sharma, Chakrabarti, & Dasgupta

2, 89 1

v1

v2 v3 1 : |1, 1|, 89 2 : |2, 1|, 90 2 1 1 : v5 , 1 , 41 2 : v5 , 2 , 44 1 v7 v8 1 : |1, 1|, 35 2 : |2, 1|, 38 3 v8 17 1 3, 9 v7

2, 89

v1

v2 v3 1 : |1, 1|, 89 2 : |2, 1|, 90 3 : |1, 2|, 92 2

3, 43 1

v2

1 : v5 , 1 , 43 2 : v4 , 1 , 44 5 4

2, 41

v3

3, 43 1

v2

1 : v5 , 1 , 43 2 : v4 , 1 , 44 5 4

2, 41

v3

1 : v5 , 1 , 41 2 : v5 , 2 , 44 1

v4 40

2, 35

v5

v6 52

v4 40

2, 35

v5

v7 v8 1 : |1, 1|, 35 2 : |2, 1|, 38 3 v8 17

v6 52

4 3, 9 1 v7 1 : v9 , 1 , 9 2 : v10 , 1 , 12 2

4 1 : v2 , 1 , 34 2 : v2 , 2 , 37 2

v9 5

v10 7

v9 5

v10 7

Figure 12: BU approach for AND/OR DAG

Figure 13: A solution (tree based semantics)

Proposed Extension of BU to Generate Alternative Solutions under Default Semantics : We propose a simple top-down traversal and pruning based extension of BU to generate alternative solutions under default semantics. While generating the ordered solutions at any AND node vq by combining the solutions of the children, we do the following. For each newly constructed solution rooted at vq , a top-down traversal of that solution starting from vq is done to check whether more than two edges of an OR node are present in that particular solution (a violation of the default semantics ). If such a violation of the default semantics is detected, that solution is pruned from the list of alternative solutions rooted at vq . Therefore, at every AND node, when a new solution is constructed, an additional top-down traversal is used to detect the semantics violation. 4.2 Top-Down Method for DAGs The proposed top-down approaches (ASG and LASG) are also applicable for AND/OR DAGs to generate alternative solution DAGs under default semantics. Only the method of computing the cost increment after the application of a swap option needs to be modified to incorporate the fact that an OR node may be included in a solution DAG through multiple paths from the root node. We use the notion of participation count for computing the cost increment. Participation Count : The notion of participation count is applicable to the intermediate nodes of a solution DAG as follows. In a solution DAG, the participation count of an intermediate node, vq , is the total number of distinct paths connecting the root node, vR , and vq . For example, in Figure 14, the optimal solution DAG is shown using thick dashed lines with arrow heads, and the participation count for every intermediate OR nodes are shown within a circle beside the node.
300

Generating Ordered Solutions for Explicit AND/OR Structures

v1
1
1

2, 89 2
1

v1
1

2, 90

2
3, 44 1

v2

3, 43 5 4

v3

2, 41 1 1

1

v2

v3

2, 41 1

1

5
1

4

2,5,4 : 1

3,5,6 : 14
2

3,5,6 : 14

v4
40

v5

2, 35 3

v6
52

v4
40

v5

2, 35 3

v6
52

4
2

4
1

v7

3, 9 2

v8
17 1

v7

3, 9 2

v8

17

1

7,9,10 : 3

7,9,10 : 3

v9
5

v10
7

v9
5

v10
7

Figure 14: AND/OR DAG

Figure 15: Solution DAG S1

We use the notation ijk to denote a swap option in the context of AND/OR DAGs, where swap option ijk belongs to node vi , the source edge of the swap option is eij from node vi to node vj , and the destination edge is eik from node vi to node vk . 4.2.1 Modification in the Proposed Top-Down Approach The ASG algorithm is modified for handling AND/OR DAGs in the following way. The computation of the successor solution in Line 14 of Algorithm 4 is modified to incorporate the participation count of the OR node to which the applied swap option belongs. The overall method is shown in Algorithm 6(in the next page). In order to apply LASG on AND/OR DAGs, apart from using the above mentioned modification for computing the cost of a newly generated solution, another modification is needed for computing the native swap options for a given solution. The modification is explained with an example. Consider the solution, S1 , shown in Figure 15. S1 is highlighted using thick dashed lines with arrow heads. The pair, cv (vq ), C (S1 , vq ), is shown within rectangles beside each node vq ; rectangles with rounded corner are used when C (S1 , vq ) = Copt (vq ). Swap option (2,5,4) was applied to Sopt to generate S1 . After the application of swap option (2,5,4) , the participation count of node v5 is decremented to 1. Therefore in S1 there is a path from the root node to node v5 and so node v5 is still present in S1 . As a result, the swap option (7,9,10) is available to S1 with a participation count equal to 1 for node v7 , whereas (7,9,10) is available to its parent solution Sopt with participation count 2 for node v7 . In other words, (7,9,10) is not available to S1 and its parent solution Sopt with the same value of participation count for node v7 . Therefore (7,9,10) becomes the native swap option of S1 . The generalized definition of native swap options for a solution is presented below. Definition 4.o [Native Swap Options of a Solution] Consider a solution Sm of an AND/OR DAG G , where Sm is constructed by applying swap option hij to solution Sp . Since swap option hij = ehi , ehj , hij is used to construct Sm , AND node vj belongs
301

Ghosh, Sharma, Chakrabarti, & Dasgupta

to Sm . Similarly, if the participation count of node vi remains greater than zero after applying hij to Sm , node vi belongs to Sm . The native swap options of solution Sm with respect to swap option hij , N (Sm , hij ), a subset of L(Sm ), comprises of the following swap options : a. hjk , where hjk is the swap option on the edge ehj b. each t , if t belongs to an OR node vq where vq is a node in Sm (vj )  , if node v is present in S  c. each t i m and t belongs to an OR node vq where vq is a node in Sm (vi ). We use the term N (Sm ) to denote the native swap options when hij is understood from the context. Intuitively the native swap options for solution Sm are the swap options that become available immediately after applying hij , but were not available in the predecessor solution of Sm .   Algorithm 6: ASG Algorithm for AND/OR DAGs input : An AND/OR DAG G output: Alternative solutions of G in the non-decreasing order of cost 1 Compute the optimal solution Sopt , perform OR edge marking and populate the swap options; 2 Create three lists, Open, Closed, and TList, that are initially empty; 3 Put Sopt in Open; 4 lastSolCost  C (Sopt ); 5 while Open is not empty do 6 Smin  Remove the minimum cost solution from Open; 7 if lastSolCost < C (Smin ) then 8 Remove all the elements of TList; 9 lastSolCost  C (Smin ); 10 end 11 Add Smin to Closed and TList; 12 Compute the swap list, L(Smin ), of Smin ; /* Construct Succ(Smin ) using L(Smin ) and add new solutions to Open */ 13 foreach ij  L(Smin ) do 14 Construct Sm by applying ij to Smin ; 15 Construct the signature of Sm , Sig(Sm ), by concatenating ij after Sig(Smin ); 16 Let ij belongs to OR node vq , p is the participation count of vq , and  is the cost increment for ij ; 17 C (Sm ) = C (Sm ) + p × ; /* Check whether Sm is already present in Open or in TList */ 18 if (Sm not in Open) and (Sm not in TList) then 19 Add Sm to Open; 20 end 21 end 22 Report the solutions in Closed; It is worth noting that Definition 4.o of native swap option is a generalization of the earlier definition of native swap option (Definition 3.n), defined in the context of trees. In
302

Generating Ordered Solutions for Explicit AND/OR Structures

the case of trees, the participation count of any node can be at maximum 1. Therefore, after the application of a swap option to a solution, the participation count of the node, to which the original edge of the swap option points to, becomes 0. Therefore the third condition is never applicable for trees. LASG (Algo. 5) can be applied on AND/OR DAGs, with the mentioned modification for computing the cost of a newly generated solution and the general definition of native swap option to generate ordered solutions under default semantics. 4.2.2 Working of ASG and LASG Algorithm on AND/OR DAG We describe the working of ASG algorithm on the example DAG shown in Figure 2. Before entering the outermost while loop, TList and Closed are empty, and Open contains the optimal solution Sopt . The contents of the different lists obtained after first few cycles of outermost while loop are shown in Table 3. Each solution is represented by its signature. The solutions that are already present in Open and also constructed by expanding the current Smin , are highlighted with under-braces. For example, the solution {(2,5,4) , (3,5,6) } which is added to Open in Iteration 2 (while constructing the successor solutions of {(2,5,4) }) constructed again in Iteration 5 while expanding solution {(3,5,6) }.
It. 1 2 L(Smin ) Open (2,5,4) , (3,5,6) , (7,9,10) {(2,5,4) }, {(3,5,6) }, {(7,9,10) } (3,5,6) , (7,9,10) {(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) }, {(2,5,4) , (7,9,10) } 3 {(2,5,4) , (7,9,10) }  {(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) }, Smin {} {(2,5,4) } 4 {(7,9,10) }  {(3,5,6) }, {(2,5,4) , (3,5,6) }, Closed {} {}, {(2,5,4) } {}, {(2,5,4) } {(2,5,4) , (7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) }, {(3,5,6) }

5

{(3,5,6) }

(2,5,4) , (7,9,10)

{(2,5,4) , (3,5,6) }, {(3,5,6) , (7,9,10) }

Table 3: Example Working of ASG Algorithm on the DAG shown in Figure 2 Now we illustrate the working of LASG algorithm on the example DAG shown in Figure 2. The contents of the different lists when a solution is added to Closed are shown in Table 4. It is worth noting that for solution S1 = {2,5,4 }, the swap list L(S1 ) = {(3,5,6) , (7,9,10) } whereas the native swap list N (S1 ) = {(7,9,10) }. The solutions that are added to Open as a result of lazy expansion, are highlighted using under-brace. For example, in Iteration 7 LASG adds the solution S5 = {(2,5,4) , (3,5,6) } to Open after the generation of solution S4 = {3,5,6 } as a part of lazy expansion, whereas the ASG algorithm adds S5 to Open after generating solution S1 = {2,5,4 }. 4.2.3 Generating Solutions under Tree Based Semantics Unlike the default semantics, ASG or LASG does not have any straight forward extension for generating solutions under tree based semantics. In Figure 13 we show an example solution which is valid under tree based semantics, but invalid under default semantics, because both OR edges emanating form the OR node v7 , namely e(7,9) and e(7,10) , are
303

Ghosh, Sharma, Chakrabarti, & Dasgupta

It. 1

N (Smin ) Open (2,5,4) , (3,5,6) , (7,9,10) {(2,5,4) }, {(3,5,6) }, {(7,9,10) } (7,9,10) {(3,5,6) }, {(7,9,10) }, {(3,5,4) , (7,9,10) } 2 {(2,5,4) , (7,9,10) }  {(3,5,6) }, {(7,9,10) }, Smin {} {(2,5,4) } 3 {(7,9,10) }  {(3,5,6) }

Closed {} {}, {(2,5,4) } {}, {(2,5,4) } {(2,5,4) , (7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) } {}, {(2,5,4) }, {(2,5,4) , (7,9,10) }, {(7,9,10) }, {(3,5,6) }

4

{(3,5,6) }

(7,9,10)

{(3,5,6) , (7,9,10) }, {(2,5,4) , (3,5,6) }

Table 4: Example Working of LASG Algorithm on the DAG shown in Fugure 2

present in this solution. These two OR edges are included in the solution through two different paths emanating form the root node, v1 . As the existing bottom-up approach stores the alternative solutions at each node in terms of the solutions of the children of that node, this representation allows these different paths to be stored explicitly, thus making BU amenable for generating alternative solutions under tree-based semantics. On the contrary, our approach works top-down using a compact representation (signature) for storing the solutions. In this signature based representation, it is currently not possible to store the fact that a particular OR node is included in the solution through two different paths which may select different child of that OR node. If we use the equivalent tree constructed form the given graph, our compact representation will work correctly, because in that case, each node would be reachable from the root node through at most one path. An AND/OR DAG can be converted to its equivalent AND/OR tree representation using procedure ConvertDAG (described in Section 2) and then ASG or LASG can be applied on the equivalent tree representation in order to generate the alternative solutions correctly under tree-based semantics. However, in the worst case, procedure ConvertDAG incurs a space explosion which will blow up the worst case complexity of both ASG and LASG algorithms. Using our compact representations to generate the ordered solutions under tree-based semantics for a given AND/OR DAG while containing the space explosion such that the worst case complexity of our algorithms remain comparable with BU turns out to be an interesting open problem.

5. Experimental Results and Observations
To obtain an idea of the performance of the proposed algorithms and to compare with the existing approach, we have implemented the ASG, LASG and BU (existing bottom-up approach) and tested on the following test domains. a. A set of synthetically generated AND/OR trees; b. Tower of Hanoi (TOH) problem; c. A set of synthetically generated AND/OR DAGs; d. Matrix-chain multiplication problem; and e. The problem of determining the secondary structure of RNA sequences.
304

Generating Ordered Solutions for Explicit AND/OR Structures

It may noted that in our implementation of the ASG algorithm, we have implemented the more space efficient version of ASG algorithm (without a separate hash-map for storing the solutions in Open and Closed, thereby incurring an extra overhead in time for duplication checking). Another important point is that for every test case the reported running time of ASG and LASG for generating a particular number of solutions includes the time required for constructing the optimal solution graph. The details of the different test domains are as follows. 5.1 Complete Trees We have generated a set of complete d-ary alternating AND/OR trees by varying ­ (a) the degree of the non-terminal nodes (denoted by d), and (b) the height (denoted by h).
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions ASG LASG BU 0.027 0.005 0.004 0.216 0.010 0.015 1.170 0.031 0.068 6.072 0.124 0.257 30.434 0.517 1.180 130.746 2.265 4.952 0.046 0.006 0.005 0.528 0.017 0.037 5.812 0.106 0.343 66.313 1.552 3.973 636.822 12.363 31.043 0.144 0.011 0.033 2.916 0.056 0.573 58.756 1.266 7.698 0.334 0.012 0.081 12.227 0.177 2.066 0.699 0.022 0.161 32.620 0.654 7.464 1.306 0.030 0.287 81.197 1.786 15.892 300 solutions ASG LASG BU 0.086 0.014 0.009 1.448 0.035 0.046 10.098 0.094 0.184 57.757 0.348 0.777 278.453 1.433 3.917 T 6.443 13.277 0.196 0.015 0.018 4.764 0.060 0.153 55.170 0.290 1.733 620.996 3.712 14.323 T 34.150 128.314 1.041 0.025 0.092 25.341 0.181 1.561 544.989 3.327 27.063 2.792 0.036 0.400 102.577 0.443 11.717 5.384 0.071 1.418 288.257 1.566 37.758 12.006 0.092 1.833 785.160 4.284 102.431 500 solutions ASG LASG BU 0.186 0.023 0.020 4.137 0.060 0.097 27.354 0.216 0.407 158.520 0.524 1.641 766.201 2.806 7.257 T 10.306 29.703 0.459 0.026 0.042 10.345 0.088 0.457 156.158 0.494 4.913 T 6.607 33.923 T 55.510 303.785 2.610 0.042 0.123 69.596 0.264 2.107 T 5.172 38.606 7.374 0.062 0.930 283.689 0.827 26.994 15.133 0.134 2.235 832.235 2.594 90.465 29.870 0.179 4.322 T 6.890 241.064

Table 5: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees (T denotes the timeout after 15 minutes) These trees can be viewed as the search space for a gift packing problem, where (a) the terminal nodes represent the cost of elementary items, (b) the OR nodes model a choice among the items (elementary or composite in nature) represented by the children, and (c) the AND nodes model the repackaging of the items returned by each of the children. Every packaging incurs a cost which is modeled by the cost of the intermediate AND nodes. Here the objective is to find the alternative gifts in the order of non-decreasing cost. Table 5 shows the time required for generating 100, 300, and 500 solutions for various complete alternating AND/OR trees. We have implemented the ASG, LASG and the existing bottom-up algorithm and the corresponding running time is shown in the column with the heading ASG, LASG and BU, respectively. We have used a time limit of 15 minutes
305

Ghosh, Sharma, Chakrabarti, & Dasgupta

(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7)

ASG 12.633 52.770 116.582 287.898 664.789 1785.156 17.270 82.609 335.301 1474.477 9139.312 40.285 213.816 1563.770 64.879 529.738 97.703 1264.828 137.527 2628.461

100 solutions LASG BU 13.168 11.047 26.152 48.484 63.254 198.234 173.730 797.234 413.855 3193.234 1257.387 12777.234 17.258 11.688 48.086 111.438 184.375 1009.188 1071.352 9088.938 7872.055 81806.688 24.469 47.453 128.629 767.453 1158.582 12287.453 40.355 88.281 343.254 2217.188 58.191 151.047 862.332 5449.797 90.703 242.219 1995.195 11882.781

ASG 28.105 144.730 341.227 832.562 1767.867 T 47.531 235.855 926.004 3234.523 T 121.336 559.734 3209.145 182.270 1254.715 270.027 2747.238 369.086 4869.551

300 solutions LASG BU 32.293 14.266 75.355 69.953 165.824 292.703 399.445 1183.703 804.801 4747.703 2047.859 19003.703 49.230 14.812 134.102 152.062 376.766 1387.312 1656.844 12504.562 9565.598 112559.812 67.102 112.609 284.922 1826.359 1699.191 29246.359 110.480 225.781 596.957 5675.000 148.453 372.141 1273.641 13433.391 205.914 576.594 2627.211 28295.281

ASG 41.676 230.168 566.766 1396.758 2942.629 T 76.270 393.113 1507.973 T T 199.254 917.824 T 305.891 2008.344 443.656 4203.957 606.133 T

500 solutions LASG BU 49.832 16.609 128.934 87.922 269.766 373.172 612.184 1514.172 1197.266 6078.172 2849.617 24334.172 80.980 17.938 219.555 192.688 577.766 1765.438 2238.152 15920.188 11251.035 143312.938 116.535 129.016 451.223 2105.266 2240.012 33725.266 179.801 363.281 858.852 9132.812 245.227 593.234 1695.684 21416.984 317.492 910.969 3273.703 44707.781

Table 6: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees

and the entries marked with T denotes that the time-out occurred for those test cases. The space required for generating 100, 300, and 500 solutions is reported in Table 6. It can be observed that in terms of both time and space required, LASG outperforms both ASG and BU. Between ASG and BU, for most of the test cases BU performs better than ASG with respect to the time required for generating a specific number of solutions. The space requirement of ASG and BU for generating a specific number of solutions has an interesting correlation with the degree (d) and height (h) parameter of the tree. For low numerical values of the d and the h parameter, e.g., (d, h) combinations like (2, 7), (3, 5) etc., BU performs better than ASG. On the contrary, for the other combinations, where at least one of these d and h parameters has a high value, e.g., (d, h) combinations like (2, 17), (7, 5), (4, 9) etc., ASG outperforms BU. 5.1.1 Experimentation with Queue with Bounded Length Since the Open can grow very rapidly, both ASG and LASG incur a significant overhead in terms of time as well as space to maintain the Open list when the number of solutions to be generated is not known a priori. In fact, for ASG checking for duplicates in Open is actually the primary source of time complexity and storing the solutions in Open is a major contributing factor in space complexity. If the number of solutions that have to generated is known a priori, the proposed top-down approach can leverage the fact by using a bounded length queue for implementing Open. When a bounded length queue is used, the time requirement along with space requirement decreases significantly.
306

Generating Ordered Solutions for Explicit AND/OR Structures

(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7)

100 solutions ASG LASG BU 0.011 0.008 0.004 0.030 0.011 0.015 0.051 0.031 0.068 0.125 0.103 0.257 0.473 0.421 1.180 2.129 2.199 4.952 0.012 0.009 0.005 0.031 0.018 0.037 0.133 0.102 0.343 1.246 1.143 3.973 10.713 10.313 31.043 0.019 0.008 0.033 0.071 0.055 0.573 1.099 0.998 7.698 0.025 0.013 0.081 0.201 0.161 2.066 0.036 0.018 0.161 0.543 0.460 7.464 0.042 0.029 0.287 1.940 1.705 15.892

300 solutions ASG LASG BU 0.003 0.002 0.009 0.008 0.006 0.046 0.020 0.011 0.184 0.043 0.059 0.777 0.168 0.164 3.917 0.766 1.005 13.277 0.003 0.002 0.018 0.012 0.006 0.153 0.048 0.043 1.733 0.477 0.636 14.323 4.160 5.555 128.314 0.006 0.004 0.092 0.026 0.023 1.561 0.443 0.552 27.063 0.009 0.031 0.400 0.083 0.078 11.717 0.014 0.011 1.418 0.240 0.325 37.758 0.020 0.013 1.833 0.807 0.843 102.431

500 solutions ASG LASG BU 0.005 0.004 0.020 0.014 0.008 0.097 0.023 0.017 0.407 0.065 0.058 1.641 0.254 0.346 7.257 1.146 1.492 29.703 0.005 0.004 0.042 0.019 0.010 0.457 0.071 0.061 4.913 0.693 0.905 33.923 6.013 7.890 303.785 0.010 0.006 0.123 0.038 0.033 2.107 0.641 0.808 38.606 0.015 0.008 0.930 0.116 0.153 26.994 0.021 0.010 2.235 0.326 0.431 90.465 0.025 0.022 4.322 0.870 1.125 241.064

Table 7: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees with bounded length Open queue for ASG and LASG
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions LASG BU 2.383 11.047 4.883 48.484 14.883 198.234 54.883 797.234 214.883 3193.234 854.883 12777.234 2.617 11.688 11.160 111.438 88.047 1009.188 780.027 9088.938 7007.852 81806.688 5.016 47.453 57.016 767.453 889.016 12287.453 10.195 88.281 217.715 2217.188 19.773 151.047 657.648 5449.797 35.742 242.219 1677.051 11882.781 300 solutions LASG BU 5.508 14.266 8.008 69.953 18.008 292.703 58.008 1183.703 218.008 4747.703 858.008 19003.703 5.742 14.812 14.285 152.062 91.172 1387.312 783.152 12504.562 7010.977 112559.812 8.141 112.609 60.141 1826.359 892.141 29246.359 13.320 225.781 220.840 5675.000 22.898 372.141 660.773 13433.391 38.867 576.594 1680.176 28295.281 500 solutions LASG BU 8.633 16.609 11.133 87.922 21.133 373.172 61.133 1514.172 221.133 6078.172 861.133 24334.172 8.867 17.938 17.410 192.688 94.297 1765.438 786.277 15920.188 7014.102 143312.938 11.266 129.016 63.266 2105.266 895.266 33725.266 16.445 363.281 223.965 9132.812 26.023 593.234 663.898 21416.984 41.992 910.969 1683.301 44707.781

ASG 10.109 23.875 54.609 135.477 361.859 1071.258 12.008 39.469 169.469 971.930 7075.109 20.664 116.609 1082.633 33.344 324.258 51.742 825.859 78.141 1919.805

ASG 27.781 64.430 141.203 317.445 738.992 1845.562 34.609 101.320 353.477 1529.031 8763.023 56.703 247.320 1607.859 84.422 565.531 121.031 1227.742 169.297 2542.047

ASG 45.789 104.117 225.969 497.508 1114.422 2615.656 57.617 163.102 537.328 2085.367 10457.797 93.031 377.922 2132.516 135.812 806.797 190.758 1628.797 260.406 3163.438

Table 8: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for complete alternating AND/OR trees with bounded length Open queue for ASG and LASG

307

Ghosh, Sharma, Chakrabarti, & Dasgupta

We show the effect of using bounded length queue to implement Open in Table 7 (reporting the time requirement) and in Table 8 (reporting the memory usage) for generating 100, 300, and 500 solutions, where the number of solutions to be generated are known beforehand. Table 7 and Table 8 show that in this case both ASG and LASG outperforms BU in terms of time as well as space requirements. Particularly, ASG performs very well in this setting, outperforming LASG in some cases. 5.1.2 Experimentation to Compare the Incremental Nature The proposed top-down algorithms are incremental in nature whereas the existing bottomup approach is not incremental. After generating a specified number of ordered solutions, our methods can generate the next solution incrementally without needing to restart itself, whereas the existing approach needs to be restarted. For example, after generating the first 10 ordered solutions, ASG and LASG generate the 11th solution directly from the data structures maintained so far by these algorithms and perform necessary updates to these data structures. Whereas, BU needs to be restarted with input parameter 11 for generating the 11th solution. In Table 9 we compare the time needed to generate the subsequent 11th solution and 12th solution incrementally after generating first 10 solutions. In order to have more clarity in the comparison among the running times of the respective algorithms, we have used higher precision (upto the 6th decimal place) while reporting the running time in Table 9. Clearly, both ASG and LASG outperform BU for generating the 11th and 12th solution in terms of the time requirement.
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) first 10 0.002403 0.009111 0.028519 0.097281 0.396460 1.561020 0.001692 0.012097 0.097356 0.934389 7.898530 0.005833 0.051598 0.813028 0.051530 0.172475 0.053422 0.502939 0.033831 1.198354 ASG 11th 0.000201 0.001957 0.003311 0.014776 0.063641 0.251839 0.000158 0.001542 0.013046 0.127943 1.082319 0.000650 0.006956 0.110205 0.001327 0.024262 0.002701 0.061417 0.003706 0.156145 12th 0.000201 0.001302 0.003533 0.015929 0.059229 0.277763 0.000151 0.001572 0.014405 0.156579 1.194090 0.000671 0.007196 0.124750 0.001641 0.024438 0.003092 0.069727 0.003846 0.166501 first 10 0.001003 0.003023 0.006700 0.025877 0.102493 0.446899 0.000683 0.004084 0.031159 0.311128 2.811539 0.002143 0.017046 0.294561 0.004638 0.059751 0.005282 0.184584 0.012862 0.466560 LASG 11th 0.000240 0.000714 0.001250 0.004113 0.014490 0.061422 0.000176 0.000583 0.003948 0.033169 0.282836 0.000303 0.002209 0.027612 0.000753 0.006116 0.000636 0.017116 0.001266 0.038792 12th 0.000123 0.000629 0.001346 0.004918 0.020031 0.082366 0.000112 0.000959 0.004604 0.047594 0.387715 0.000582 0.003115 0.037281 0.000652 0.007197 0.001087 0.024042 0.001282 0.061305 first 10 0.001344 0.003596 0.014628 0.059326 0.238418 0.962635 0.001055 0.009507 0.085610 0.778298 7.037050 0.004181 0.044913 0.727766 0.005963 0.152285 0.010895 0.406947 0.018185 0.929941 BU 11th 0.001359 0.003696 0.015046 0.061393 0.246042 0.989777 0.001101 0.009931 0.089379 0.811176 7.313715 0.004434 0.047867 0.775950 0.006358 0.162527 0.011604 0.435398 0.019567 0.989326 12th 0.001397 0.003895 0.015521 0.062717 0.251746 1.015848 0.001133 0.010336 0.093419 0.846578 7.608619 0.004725 0.050940 0.823442 0.006782 0.173191 0.012556 0.465301 0.020896 1.052566

Table 9: Comparison of running time (in seconds) for generating for first 10 solutions and then the 11th solution and 12th solution incrementally for complete alternating AND/OR trees

308

Generating Ordered Solutions for Explicit AND/OR Structures

5.2 Multipeg Tower of Hanoi Problem Consider the problem of Multipeg Tower of Hanoi (Majumdar, 1996; Gupta, Chakrabarti, & Ghose, 1992). In this problem,  pegs are fastened to a stand. Initially  disks rest on the source peg A with small disk on large disk ordering. The objective is to transfer all  disks from A to the destination peg B with minimum legal moves. In a legal move, the topmost disk from any tower can be transferred to any other peg with a larger disk as the topmost disk. The problem of multi-peg tower of Hanoi can be solved recursively as follows. a. Move recursively the topmost k (k varies from 1 to  - 1) disks from A to some intermediate peg, I , using all the pegs. b. Transfer the remaining  - k disks from A to B recursively, using the ( - 1) pegs available. c. Recursively move k disks that were transferred to I previously, from the intermediate peg I to B , using all the  pegs. It may be noted that there is a choice for the value of k, which may take any value from 1 to  - 1. Solutions with different values of k may take different number of moves, and the solution which incurs minimum number of moves is the optimal solution. This choice of the value of k is modeled as an OR node, and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. Therefore, the search spaces of the multi-peg Tower of Hanoi problem correspond to alternating AND/OR trees.
#disks 8 9 10 11 12 13 100 solutions ASG LASG BU 0.034 0.030 0.069 0.119 0.116 0.264 0.479 0.635 1.310 2.421 2.178 3.171 7.453 7.448 11.437 25.379 25.115 38.458 300 solutions ASG LASG BU 0.104 0.084 0.252 0.314 0.289 0.942 1.706 1.658 3.305 6.573 6.161 12.998 21.232 21.081 43.358 68.574 67.170 140.392 500 solutions ASG LASG BU 0.200 0.138 0.577 0.590 0.458 2.183 2.303 2.829 7.592 10.651 9.678 29.242 35.825 35.663 99.593 112.411 112.470 332.113 #Opt. No. of Moves 23 27 31 39 47 55

Table 10: Comparison of running time (in seconds) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different number of disks
#disks 8 9 10 11 12 13 100 solutions ASG LASG BU ASG 36.664 43.008 416.312 64.516 96.211 111.320 1471.656 131.266 295.672 341.000 5074.219 326.352 957.336 1113.508 17197.312 999.602 3155.086 3664.117 57512.812 3198.156 10339.078 12022.883 190297.969 10412.242 300 solutions LASG BU ASG 80.734 660.062 105.039 154.266 2359.156 166.789 383.453 8161.719 373.453 1158.797 27728.562 1039.367 3719.352 92906.562 3247.547 12078.914 307872.969 10483.570 500 solutions LASG BU 117.008 903.812 197.859 3246.656 427.766 11249.219 1204.719 38259.812 3767.617 128300.312 12137.242 425447.969

Table 11: Comparison of space required (in KB) for alternating AND/OR trees corresponding to the search spaces of 5-peg Tower of Hanoi problem with different number of disks We have used the search space of 5 peg Tower of Hanoi problem with different number of disks,  , and generated alternative solutions in non-decreasing order of cost using ASG and
309

Ghosh, Sharma, Chakrabarti, & Dasgupta

LASG algorithms. Here the cost function expresses the number of legal moves. The value of  is varied from 8 to 13, and in Table 10 and in Table 11, we report the time required and space required, respectively, for generating 100, 300, and 500 solutions for every test cases. Experimental results show that the performance of ASG is similar to the performance of LASG with respect to both space and time. However ASG as well as LASG outperforms BU with respect to both time and space requirements. 5.3 Randomly Constructed AND/OR DAGs We have constructed a set of randomly generated AND/OR DAGs and evaluated the ASG, LASG, and BU algorithm for generating solutions under default semantics. We have used the proposed extension to the BU algorithm for generating solutions under default semantics.
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 100 solutions ASG LASG BU 0.027 0.006 0.039 0.060 0.009 0.096 0.363 0.020 0.106 0.020 0.006 0.019 0.203 0.018 0.067 3.550 0.045 0.730 26.659 0.201 14.620 0.065 0.008 0.034 0.877 0.025 0.400 7.422 0.160 26.683 T 1.972 T 300 solutions ASG LASG BU 0.089 0.021 0.158 0.281 0.030 1.100 2.485 0.059 0.266 0.123 0.021 0.098 1.483 0.048 0.257 30.302 0.126 1.681 257.605 0.612 33.382 0.348 0.027 0.217 6.910 0.069 0.994 69.097 0.449 66.558 T 5.819 T 500 solutions ASG LASG BU 0.172 0.033 0.282 0.594 0.051 3.665 6.163 0.100 0.528 0.280 0.032 0.245 4.043 0.083 0.541 85.863 0.215 2.766 710.708 1.194 52.406 0.817 0.049 2.251 18.823 0.118 1.365 194.452 0.927 109.076 T 9.426 T

Table 12: Comparison of running time (in seconds) for generating 100, 300, and 500 solutions for AND/OR DAGs (T denotes the timeout after 15 minutes)
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 ASG 11.609 23.141 74.082 13.914 48.867 229.820 772.441 30.648 121.535 471.625 2722.938 100 solutions LASG BU 8.875 8.125 16.219 31.312 39.000 106.875 10.492 8.172 35.445 66.938 118.707 389.844 339.676 1996.875 17.332 29.609 65.578 287.109 266.078 2729.297 1256.535 T ASG 32.852 62.516 220.648 46.117 151.363 705.809 2245.938 85.781 381.133 1183.379 T 300 solutions LASG BU 30.797 10.906 46.711 49.562 105.852 172.562 32.539 11.297 101.168 98.188 312.246 621.094 825.984 3321.875 53.961 73.359 168.305 737.891 550.477 6945.703 2353.562 T ASG 54.094 100.555 371.344 77.445 262.816 1200.336 3732.523 140.312 659.434 1927.961 T 500 solutions LASG BU 50.035 13.250 74.379 65.188 168.375 230.375 54.602 14.422 163.273 129.438 507.762 852.344 1327.406 4646.875 93.539 86.641 275.594 883.984 843.484 8419.922 3447.809 T

Table 13: Comparison of space required (in KB) for generating 100, 300, and 500 solutions for AND/OR DAGs

Table 12 and Table 13 compare the time required and space required for running ASG, LASG and BU for generating 100, 300, and 500 solutions for every test cases. The first and second columns of every row provide the size (n ) and the average out-degree (d) of the DAG. The results obtained for this test domain are similar to the results for randomly
310

Generating Ordered Solutions for Explicit AND/OR Structures

constructed AND/OR trees. It may be noted that in terms of both time and space required, LASG outperforms both ASG and BU. Between ASG and BU, for most of the test cases BU performs better than ASG with respect to the time required for generating a specific number of solutions. Whereas, the space requirement of ASG and BU for generating a specific number of solutions has an interesting co-relation with the average degree (d) and the size (n ) parameter of the DAG. For low numerical values of the d and the n parameter, e.g., (n , d) combinations like (60, 2), (33, 3) etc., BU performs better than ASG. On the contrary, for the other combinations, where at least one of these n and d parameter has a high value, e.g., (n , d) combinations like (920, 2), (9624, 3), (40884, 4) etc., ASG outperforms BU. 5.4 Matrix-Chain Multiplication Problem We have also used the well-known matrix-chain multiplication (Cormen, Stein, Rivest, & Leiserson, 2001) problem for experimentation. The search space of the popular dynamic programming formulation of this problem correspond to AND/OR DAG.
DAG Cnstr. #matrices Time (Sec) 20 0.033 30 0.200 40 0.898 50 3.033 60 8.335 70 19.591 80 41.960 90 82.578 100 151.814 Sopt Cnstr. Time (Sec) 0.001 0.003 0.008 0.016 0.029 0.046 0.071 0.101 0.143 10 solutions ASG 0.003 0.009 0.019 0.047 0.088 0.140 0.209 0.296 0.409 LASG 0.002 0.008 0.018 0.048 0.090 0.142 0.212 0.300 0.412 BU 0.206 2.785 15.580 93.267 342.212 862.387 T T T ASG 0.004 0.012 0.024 0.062 0.118 0.187 0.280 0.396 0.546 15 solutions LASG 0.003 0.010 0.024 0.065 0.120 0.190 0.282 0.398 0.548 BU 0.288 4.087 23.414 140.513 509.906 T T T T ASG 0.005 0.015 0.030 0.079 0.148 0.235 0.351 0.496 0.688 20 solutions LASG 0.004 0.012 0.030 0.081 0.151 0.238 0.354 0.499 0.683 BU 0.373 5.406 31.112 187.227 678.718 T T T T

Table 14: Comparison of time required (in seconds) for AND/OR DAGs corresponding to the search spaces of matrix-chain multiplication with different number of matrices, (T denotes the timeout after 15 minutes)
#matrices 20 30 40 50 60 70 80 90 100 ASG 19.641 66.367 156.559 308.984 537.383 859.844 1290.117 1843.828 2537.582 10 solutions LASG 20.203 69.273 160.227 315.012 545.117 869.160 1301.406 1857.480 2556.883 BU 160.918 555.684 1317.637 2563.965 4411.855 6978.496 T T T ASG 20.543 67.809 157.738 310.277 538.930 862.133 1293.148 1847.602 2542.746 15 solutions LASG 21.227 70.695 161.785 316.543 546.512 870.867 1303.426 1859.812 2560.043 BU 234.305 821.902 1960.281 3825.223 6592.508 T T T T ASG 21.914 69.516 158.758 311.551 539.914 863.977 1295.852 1851.164 2549.352 20 solutions LASG 22.773 72.523 162.852 318.145 547.551 872.219 1305.090 1861.789 2566.992 BU 303.973 1081.902 2594.207 5075.262 8759.441 T T T T

Table 15: Comparison of space required (in KB) for AND/OR DAGs corresponding to the search spaces of matrix-chain multiplication with different number of matrices

Given a sequence of matrices, A1 , A2 , · · · , An , of n matrices where matrix Ai has dimension pi-1 × pi , in this problem the objective is to find the most efficient way to multiply
311

Ghosh, Sharma, Chakrabarti, & Dasgupta

these matrices. The classical dynamic programming approach works as follows. Suppose A[i,j ] denotes matrix that results from evaluating the product, Ai Ai+1 · · · Aj , and m[i, j ] is the minimum number of scalar multiplications required for computing the matrix A[i,j ] . Therefore, the cost of optimal solution is denoted by m[i, j ] which can be recursively defined as :  0, if i = j ; m[i, k] + m[k + 1, j ] + pi-1 × pk × pj , if i < j .

m[i, j ] =

The choice of the value of k is modeled as OR node and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. It is worth noting that unlike the search space of 5-peg ToH problem, the search space of the matrix-chain multiplication problem corresponds to AND/OR DAG. We have used the search space for different matrix sequences having varying length and generated alternative solutions in the order of non-decreasing cost. In Table 14, we report the time required and in Table 15, we report the memory used for generating 10, 15, and 20 solutions for every test cases. In Table 14, for each test case, we also report the time required for constructing the explicit AND/OR DAG from the recursive formulation in the 2nd column, and the optimal solution construction time in the 3rd column. It is interesting to observe that the relative performance of ASG and LASG for this search space is very similar to that obtained for 5peg ToH search space though this search space for this domain is AND/OR DAG. Both ASG and LASG perform approximately the same with respect to time and space requirement. However, the advantage of ASG as well as LASG over BU with respect to both time and space requirement is more significant in this domain. 5.5 Generating Secondary Structure for RNA Another relevant problem where the alternative solutions play an important role is the computation of the secondary structure of RNA. RNA molecules can be viewed as strings of bases, where each base belongs to the set {Adenine, Cytocine, Guanine, U racil} (also denoted as {A, C, G, U }). RNA molecules tend to loop back and form base pairs with itself and the resulting shape is called secondary structure (Mathews & Zuker, 2004). The stability of the secondary structure largely depends on the number of base pairings (in general, larger number of base pairings implies more stable secondary structure). Although there are other factors that influence the secondary structure, it is often not possible to express these other factors using a cost function and they are typically evaluated empirically. Therefore, it is useful to generate a set of possible alternative secondary structures ordered by decreasing numbering of base pairings for a given RNA which can be further subjected to experimental evaluation. The computation of the optimal secondary structure considering the underlying principle of maximizing the number of base-pairings has a nice dynamic programming formulation (Kleinberg & Tardos, 2005). Given an RNA molecule B = b1 b2 · · · bn where each bi  {A, C, G, U }, the secondary structure on B is a set of base pairings, D = {(i, j )}, where i, j  {1, 2, · · · n}, that satisfies the following conditions:
312

 min

ik<j

Generating Ordered Solutions for Explicit AND/OR Structures

Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14

Organism Name Anaerorhabdus Furcosa Archaeoglobus Fulgidus Chlorobium Limicola Desulfurococcus Mobilis Haloarcula Japonica Halobacterium Sp. Mycoplasma Genitalium Mycoplasma Hyopneumoniae Mycoplasma Penetrans Pyrobaculum Aerophilum Pyrococcus Abyssi Spiroplasma Melliferum Sulfolobus Acidocaldarius Symbiobacterium Thermophilum

# Bases 114 124 111 129 122 120 104 105 103 131 118 107 126 110

Table 16: Details of the RNA sequences used for Experimentation a. if (i, j )  D , then i + 4 < j : This condition states that the ends of each pair in D are separated by at least four intermediate bases. b. The elements of any pair in D consists of either {A, U } or {C, G} (in either order). c. No base appears in more than one pairings, i.e., D is a matching. d. If (i, j ) and (k, l) are two pairs in D , then it is not possible to have i < k < l < j , i.e., no two pairings can cross each other.
Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 DAG Cnstr. Time (Sec) 34.464 57.999 26.423 83.943 51.290 46.508 16.766 22.775 18.831 91.419 47.660 22.649 67.913 28.911 Sopt Cnstr. Time (Sec) 0.042 0.057 0.038 0.065 0.051 0.047 0.029 0.033 0.031 0.073 0.047 0.034 0.061 0.038 ASG 0.094 0.126 0.084 0.144 0.114 0.107 0.068 0.077 0.068 0.167 0.111 0.078 0.140 0.087 5 solutions LASG BU 0.095 449.916 0.128 823.493 0.089 363.421 0.152 1089.462 0.116 681.429 0.108 598.419 0.069 210.806 0.078 284.455 0.072 233.999 0.170 T 0.109 627.744 0.079 288.520 0.141 962.641 0.085 366.693 ASG 0.145 0.193 0.135 0.230 0.176 0.166 0.101 0.120 0.109 0.249 0.173 0.116 0.206 0.134 10 solutions LASG BU 0.148 893.682 0.198 T 0.133 718.326 0.227 T 0.180 1349.181 0.175 T 0.103 410.817 0.122 559.318 0.111 458.290 0.263 T 0.171 1253.034 0.123 573.602 0.218 T 0.137 724.113 ASG 0.197 0.271 0.183 0.314 0.239 0.226 0.136 0.153 0.144 0.347 0.220 0.165 0.290 0.182 15 solutions LASG BU 0.202 1359.759 0.277 T 0.186 1077.094 0.317 T 0.245 T 0.238 T 0.144 621.792 0.165 836.359 0.148 683.411 0.355 T 0.240 T 0.167 849.134 0.288 T 0.186 1072.552

Table 17: Comparison of time required (in seconds) for AND/OR DAGs corresponding to the search spaces of RNA secondary structure with different number of bases (T denotes the timeout after 30 minutes)

Under the above mentioned conditions the dynamic programming formulation is as follows. Suppose P (i, j ) denotes the maximum number of base pairings in a secondary structure on bi · · · bj . P (i, j ) can be recursively defined as : P [i, j ] =  0, if i + 4  j ,

max P [i, j - 1], max 1 + P [i, k - 1] + P [k + 1, j - 1] ,
ik<j

if i + 4 < j .

313

Ghosh, Sharma, Chakrabarti, & Dasgupta

Here, a choice of the value of k is modeled as an OR node and for every such choice, the problem is divided into three sub-problems. This decomposition into sub-problems is modeled as an AND node. We have experimented with the search space of this problem for the set of RNA molecule sequences obtained from the test-cases developed by Szymanski, Barciszewska, Barciszewski, and Erdmann (2005). The details of the test cases are shown in Table 16. For every test cases, we report the time required in Table 17 for generating 5, 10, and 15 solutions. For the same setting, the space required is reported in Table 18. In Table 17, for each test case, we also report the time required for constructing the explicit AND/OR DAG from the recursive formulation in the 2nd column, and the time required for constructing the optimal solution time in the 3rd column. We use a high value of time-out (1800 seconds) in order to gather the running time required by BU. We limit the maximum solutions generated at 15 because for generating higher number of solutions, BU is timed out for most of the test cases. It is worth noting that the result obtained for this domain is very similar to the result obtained for the matrix-chain multiplication problem domain. Both space and time wise ASG and LASG perform similarly and they outperform BU significantly with respect to time as well as space requirement.
Test Case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 5 solutions LASG 1694.688 2310.008 1516.922 2665.820 2097.414 1963.367 1138.633 1333.336 1207.633 3047.539 2022.906 1335.883 2496.469 1517.828 10 solutions LASG 1697.797 2315.258 1521.750 2671.711 2101.836 1968.305 1142.023 1338.070 1211.523 3053.977 2030.922 1339.516 2502.625 1521.352 15 solutions LASG 1700.492 2318.008 1526.797 2675.633 2106.000 1972.172 1144.109 1342.484 1213.906 3059.781 2038.461 1341.914 2506.703 1525.344

ASG 1647.555 2254.531 1473.852 2606.242 2045.930 1912.227 1101.125 1293.812 1170.094 2984.773 1974.695 1295.141 2438.898 1475.477

BU 7409.336 9902.953 6629.891 11358.945 9021.273 8499.570 5087.680 5855.547 5352.477 T 8641.422 5924.664 10657.945 6627.844

ASG 1651.273 2258.773 1477.492 2610.875 2049.844 1916.422 1104.422 1297.750 1173.023 2990.211 1979.344 1297.273 2442.961 1478.555

BU 14656.469 T 13103.492 T 17875.430 T 10036.938 11560.203 10562.766 T 17119.820 11701.695 T 13099.055

ASG 1654.367 2262.492 1480.555 2615.719 2052.867 1921.117 1108.047 1302.242 1176.352 2994.773 1983.664 1299.805 2447.172 1482.234

BU 21846.156 T 19518.625 T T T 14924.820 17211.406 15718.617 T T 17420.719 T 19519.742

Table 18: Comparison of space required (in KB) for AND/OR DAGs corresponding to the search spaces of RNA secondary structure with different number of bases

5.6 Observations The experimental data shows that the LASG algorithm generally outperforms the ASG algorithm and the existing bottom-up approach in terms of the running time for complete alternating AND/OR trees and AND/OR DAGs. Whereas, for the other problem domains, i.e., the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem, and the problem of determining secondary structure of RNA sequences, the overall performance of the ASG algorithm is similar to the performance of the LASG algorithm. This behavior can be explained from the average and maximum length statistics of Open list, reported in Table 19 - Table 23, for these above mentioned test domains.
314

Generating Ordered Solutions for Explicit AND/OR Structures

In the case of complete trees and random DAGs, for ASG algorithm, the average as well as the maximum size of Open grows much faster than that of LASG algorithm (Table 19 and Table 20), with the increase in the size of the tree/DAG.
(d, h) (2, 7) (2, 9) (2, 11) (2, 13) (2, 15) (2, 17) (3, 5) (3, 7) (3, 9) (3, 11) (3, 13) (4, 5) (4, 7) (4, 9) (5, 5) (5, 7) (6, 5) (6, 7) (7, 5) (7, 7) 100 solutions ASG LASG avg. max. avg. max. 235 383 75 159 994 1894 73 120 2427 4709 156 306 5546 10947 524 1149 11744 23291 384 523 24264 48333 655 841 304 549 120 242 1561 3015 172 346 5496 10899 191 289 17336 34542 486 691 53139 106155 1138 1216 734 1427 103 176 3748 7383 194 381 16282 32451 422 488 1216 2352 146 307 7261 14446 249 335 1781 3489 141 276 12362 24651 297 342 2433 4765 261 508 19311 38435 450 529 300 solutions ASG LASG avg. max. avg. max. 435 629 179 289 2657 4931 220 528 6935 13537 483 1005 16266 32076 1550 2726 34836 69160 677 1121 T T 1087 1611 740 1323 341 652 4359 8400 579 1260 16272 32244 387 661 51954 103549 956 1754 T T 1267 1569 2062 4006 256 503 10928 21489 678 1467 48786 97196 687 1131 3407 6555 496 1053 21652 42972 470 888 5089 9911 507 1126 36868 73323 461 789 7072 13910 747 1483 57754 115116 687 961 500 solutions ASG LASG avg. max. avg. max. 545 792 236 372 4103 7569 449 1069 11251 21843 851 1771 26748 52724 2261 3844 57673 114367 983 1824 T T 1527 2819 1107 1972 539 1007 7026 13588 1012 2084 26904 53271 622 1368 T T 1460 2672 T T 1432 1776 3322 6375 452 1065 17932 35222 1265 2837 T T 1025 1807 5508 10694 852 1742 35850 71054 832 1781 8250 16035 971 2164 61221 121958 749 1573 11595 22809 1204 2273 T T 984 1922

Table 19: Average and maximum length of Open while generating 100, 300, and 500 solutions for complete alternating AND/OR trees
n 60 220 920 33 404 2124 9624 144 744 8844 40884 d 2 2 2 3 3 3 3 4 4 4 4 100 solutions ASG LASG avg. max. avg. max. 181 338 39 63 479 854 77 133 1530 2957 116 227 202 409 58 102 1001 1969 236 447 5008 9911 374 626 14422 28666 394 491 510 990 56 101 2407 4760 253 485 7522 14931 258 437 T T 749 804 300 solutions ASG LASG avg. max. avg. max. 428 768 131 282 1144 2058 210 417 4289 8278 332 639 604 1193 154 281 2958 5799 675 1256 14803 29314 851 1569 43087 85825 746 1339 1374 2563 187 458 7166 14204 590 1018 22254 44062 847 1831 T T 852 1004 500 solutions ASG LASG avg. max. avg. max. 643 1138 219 411 1721 3139 329 612 6902 13305 512 946 978 1875 234 422 4874 9781 1013 1810 24442 48357 1337 2527 71547 142327 1254 2756 2140 3996 376 868 11874 23558 885 1655 36743 72740 1565 3493 T T 961 1215

Table 20: Average and maximum length of Open while generating 100, 300, and 500 solutions for randomly constructed AND/OR DAGs

Since ASG algorithm checks for the presence of duplicates while expanding a solution, the time required for duplication checking grows rapidly for these test domains. Hence, the overall time required for generating a specific number of solutions also increases rapidly (faster than both BU and LASG) with the increase in the size of the tree/DAG. As a result, BU outperforms ASG with respect to the time requirement for trees and DAGs. However
315

Ghosh, Sharma, Chakrabarti, & Dasgupta

the memory used for generating a specific number of solutions increases moderately (slower than BU) with the increase in the size of the tree/DAG. Therefore with respect to space requirement, ASG outperforms BU for larger trees and DAGs. Between LASG and BU, the time as well as the memory requirement of BU increases faster than that of LASG when the degree of the AND/OR tree or DAG increases. This happens because, for BU, the time taken for merging the sub-solutions at the AND nodes and memory required for storing alternative solutions that are rooted at different nodes increases rapidly with the increase in the degree of that node. On the contrary, for the other test domains, 5-peg Tower of Hanoi problem, matrix-chain multiplication problem, and the probelm of finding secondary structure of RNA sequences, the average and the maximum size of Open for both ASG and LASG are comparable (Table 21, Table 22 and Table 23). Therefore, for the LASG algorithm, the time saved by avoiding the duplication checking is compensated by the extra overhead of maintaining the solution space tree and the checks required for lazy expansion. Hence the running time as well as the space requirement are almost same for both algorithms for these three above mentioned problem domains. Moreover, due to the low values of the average and the maximum size of Open, ASG outperforms BU with respect to both time requirement and memory used for these three test domains. For these three domains also, between LASG and BU, the time as well as the memory requirement of BU increases faster than that of LASG when the size of the search space (AND/OR tree or DAG) increases.

6. Ramifications on Implicitly Specified AND/OR Structures
In this section, we briefly discuss use of our proposed algorithms for generation of alternative solutions in the non-decreasing order of cost for implicit AND/OR search spaces. One possible way is to extend the standard AO for generating a given number of solutions, say k, as follows. Instead of keeping only one potential solution graph(psg), at any stage k psgs can be computed on the explicitly constructed search space and instead of expanding one node, k nodes, (that is, one node from each psg), can be expanded at once. After expanding the nodes, k psgs are recomputed once again. Since the cost of the nodes are often recomputed after expanding nodes, the swap options associated with any such node have to be updated after every such recomputation. Another possible approach could be to run AO until it generates the optimal solution. At this point of time the swap options can be computed on the explicit portion of the graph and swap option with minimum cost can be applied to the optimal solution. Then the resulting psg is again expanded further resulting in the expansion of the explicit graph. The swap options are re-evaluated to incorporate the cost update. Again the next best psg is computed. This process continues till the second best solution is derived. Now among the remaining successor psgs of the first solution and the successor psgs of second solution, the most promising psg is selected and expanded. This process continues till the third solution is found. Then the successor psgs are also added to the already existing pool of candidate psgs. These two broad steps, (a) selecting the next best psg from the pool of candidate psgs, and then (b) keeping on expanding the explicit graph till the next best solution is found, is continued till k solutions are found.
316

Generating Ordered Solutions for Explicit AND/OR Structures

# disks 8 9 10 11 12 13

100 solutions ASG LASG avg. max. avg. max. 55 92 41 68 66 122 42 71 109 183 53 79 132 218 76 140 219 385 85 147 259 482 118 200

300 solutions ASG LASG avg. max. avg. max. 111 186 91 174 163 331 119 252 216 367 142 283 296 611 177 373 473 776 234 492 675 1240 252 437

500 solutions ASG LASG avg. max. avg. max. 174 375 135 235 265 484 198 382 345 693 234 447 486 882 291 558 668 1200 404 724 1016 1828 377 697

Table 21: Average and maximum length of Open while generating 100, 300, and 500 solutions for 5-peg Tower of Hanoi problem with different number of disks
# matrices 20 30 40 50 60 70 80 90 100 10 solutions ASG LASG avg. max. avg. max. 46 87 25 39 84 162 71 126 73 123 58 90 86 151 75 126 91 144 76 112 136 234 85 122 181 324 94 132 226 414 103 142 307 576 167 259 15 solutions ASG LASG avg. max. avg. max. 68 121 34 59 123 230 94 157 98 182 73 129 120 211 100 169 118 189 94 137 188 329 103 147 258 469 112 157 328 609 122 167 445 823 216 337 20 solutions ASG LASG avg. max. avg. max. 90 176 46 95 160 293 116 192 125 226 90 152 151 266 123 205 151 267 108 160 243 437 117 170 335 607 127 180 427 777 136 190 583 1145 262 477

Table 22: Average and maximum length of Open while generating 10, 15, and 20 solutions for matrix-chain multiplication problems
Test case TC1 TC2 TC3 TC4 TC5 TC6 TC7 TC8 TC9 TC10 TC11 TC12 TC13 TC14 5 solutions ASG LASG avg. max. avg. max. 45 84 41 74 50 95 50 95 47 90 46 89 50 93 49 90 47 86 45 74 49 93 47 84 42 81 42 80 46 89 44 84 40 77 39 73 59 116 59 113 55 106 54 105 33 64 31 51 51 98 51 97 41 78 40 73 10 solutions ASG LASG avg. max. avg. max. 93 176 75 125 100 192 94 170 90 168 82 142 101 194 87 155 98 186 87 149 105 200 95 168 83 157 73 119 97 188 86 159 80 147 70 119 128 251 116 212 115 225 110 211 67 116 55 98 103 193 100 185 82 154 69 112 15 solutions ASG LASG avg. max. avg. max. 135 249 95 143 146 266 125 197 132 244 115 210 152 292 119 197 140 246 114 184 155 294 127 206 121 231 92 138 144 277 120 214 115 214 93 146 189 350 161 280 171 317 166 321 95 172 78 135 149 276 140 239 120 231 97 176

Table 23: Average and maximum length of Open while generating 5, 10, and 15 solutions for generating secondary structure of RNA sequences

317

Ghosh, Sharma, Chakrabarti, & Dasgupta

It is important to observe that both methods heavily depend on incorporating the updates in the explicit DAG like adding nodes, increase in the cost, etc., and recomputing the associated swap options along with the signatures that use those swap options. Handling dynamic updates in the DAG efficiently and its use in implicit AND/OR search spaces remains an interesting future direction.

7. Conclusion
In our work we have presented a top-down algorithm for generating solutions of a given weighted AND/OR structure (DAG) in non-decreasing order of cost. Ordered solutions for AND/OR DAGs are useful for a number of areas including model based programming, developing new variants of AO*, service composition based on user preferences, real life problems having dynamic programming formulation, etc. Our proposed algorithm has two advantages ­ (a) it works incrementally, i.e., after generating a specific number of solutions, the next solution is generated quickly, (b) if the number of solutions to be generated is known a priori, our algorithm can leverage that to generate solutions faster. Experimental results show the efficacy of our algorithm over the state-of-the-art. This also opens up several interesting research problems and development of applications.

8. Acknowledgments
We thank the anonymous reviewers and the editor, Prof. Hector Geffner, for their valuable comments which have enriched the presentation of the paper significantly. We also thank Prof. Abhijit Mitra, International Institute of Information Technology, Hyderabad, India, for his valuable inputs regarding the test domain involving secondary structure of RNA. We thank Aritra Hazra and Srobona Mitra, Research Scholar, Department of Comp. Sc. & Engg., Indian Institute of Technology Kharagpur, India, for proof reading the paper.

Appendix A. Proof of Correctness of Algorithm 4
Lemma A.1 Every solution other than the optimal solution Sopt can be constructed from ^. Sopt by applying a sequence of swap options according to the order R ^ Proof: [Lemma A.1] Every solution other than Sopt of an alternating AND/ OR tree T is constructed by choosing some non optimal edges at some OR nodes. Consider any other solution Sm , corresponding to which the set of non-optimal OR edges is S and suppose |S  | = m. We apply the relation R to S to obtain an ordered sequence  of OR edges where e1 , e2  , e1 appears before e2 in  if (e1 , e2 )  R. We show that there exists a ^ of swap options that can be constructed for S . For every OR edge ei of  sequence  j (here eij is the ith edge of  and 1  i  m), we append the subsequence of OR edges ei1 , . . . , eij -1 before eij , where ei1 , . . . , eij are the OR edges that emanate from the same parent vq , and ei1 , . . . , eij -1 are the first ij - 1 edges in L(vq ). We get a sequence of OR edges aug from  by the above mentioned augmentation. aug is basically a concatenation of subsequences 1 , . . . , m , where i is a sequence of edges ei1 , . . . , eij such that ei1 , . . . , eij are the OR edges that emanate from the same parent vq , ^ from aug as follows. From and ei1 , . . . , eij are the first ij edges in L(vq ). We construct 
318

Generating Ordered Solutions for Explicit AND/OR Structures

every i , we construct  ^i = i1 ,i2 , . . . , ij -1,ij , where ik ,ik +1 = eik , eik +1 , ik ,ik +1 and ^ i1  ik  (ij - 1).  is constructed by concatenating every individual  ^i . Hence there exists ^ corresponding to every other solution Sm . a sequence of swap options    Definition A.p [Default Path] From Lemma A.1, every non-optimal solution Sm can be constructed from the initial optimal solution by applying a sequence of swap options, ^ Sm ), according to the order R ^ . The sequence of solutions that is formed following ( ^ Sm ) ( s corresponds to a path from Sopt to Sm in SSDAG G . This path is defined as the default path, Pd (Sm ), for Sm . ^ contains every alternative Lemma A.2 The SSDAG of an alternating AND/OR tree T ^ . solution of T Proof: [Lemma A.2] We prove this by induction on the length of the default path Pd of the solutions. [Basis (n = 1) :] Consider the swap list of Sopt . The solutions whose default path length is equal to 1 form the Succ(Sopt ). Therefore these solutions are present in G . [Inductive Step :] Suppose the solutions whose default path length is less than or equal to n are present in G . We prove that the solutions having default path length equal to ^ Sm ) = n + 1 are also present in G . Consider any solution Sm where Pd (Sm ) = n + 1. Let (   ^ S ) = 1 , · · · , n . Since Pd (S  ) = 1 , · · · , n , n+1 . Consider the solution Sm where ( m m s     V , and swap option  n , Sm n+1  L(Sm ), there is a directed edge from Sm to Sm in G . Hence every solution having a default path length equal to n + 1 is also present in G .   ^ , Algorithm 4 adds solutions to Closed Lemma A.3 For any alternating AND/OR tree T (at Line 11) in non-decreasing order of cost. Proof: [Lemma A.3] Consider the following invariants of Algorithm 4 that follow from the description of Algorithm 4. a. The minimum cost solution from Open is always removed at Line 6 of Algorithm 4. b. The cost of the solutions that are added in Open, while exploring the successor set of a solution Sm (at Line 13 of Algorithm 4), are greater than or equal to C (Sm ). From these two invariants it follows that Algorithm 4 adds solutions to Closed (at Line 11) in non-decreasing order of cost. ^ , for every node of the SSDAG of T ^ , Lemma A.4 For any alternating AND/OR tree T Agorithm 4 generates the solution corresponding to that node. Proof: [Lemma A.4] From Lemma A.3 it follows that Algorithm 4 generates the solutions in the non-decreasing order of cost. By generating a solution Sm , we mean adding Sm to Closed (at line 11 of Algorithm 4). For the purpose of proof by contradiction, let us assume that Algorithm 4 does not generate solution Sm . Also let Sm be the first occurrence of this
319

Ghosh, Sharma, Chakrabarti, & Dasgupta

scenario while generating solutions in the mentioned order. According to Lemma A.1, there ^ = 1 , . . . , k corresponding to Sm . Also consider the exists a sequence of swap options   ^  = 1 , . . . , k-1 . According to Property 3.2, solution Sm whose sequence of swap options is   C (Sm )  C (Sm ). Consider the following two cases:
 ) < C (S ): Since S a. C (Sm m m is the first instance of the incorrect scenario, and Algo is generated rithm 4 generates the solutions in the non-decreasing order of cost, Sm prior to Sm .  ) = C (S ): Since Algorithm 4 resolves the tie in the favor of the parent solution, b. C (Sm m  will be and Sm is the first instance of the incorrect scenario ­ in this case also Sm generated prior to Sm .  . When S  was generated by Algorithm 4, The swap option k belongs to the swap list of Sm m   that is, when Sm was added to Closed, Sm was also expanded and the solutions which can  applying one swap option, were added to the Open list. Since S be constructed from Sm m  applying one swap option  , S was constructed from Sm was also added to the Open m k  . Therefore S while exploring the successors of Sm m will also be eventually generated by Algorithm 4 - a contradiction.  

^ , Algorithm 4 does not add any solution Lemma A.5 For any alternating AND/OR tree T to Closed (at Line 11 of Algorithm 4) more than once. Proof: [Lemma A.5] For the purpose of contradiction, let us assume that Sm is the first solution that is added to Closed twice. Therefore Sm must have been added to Open twice. Consider the following facts. a. When Sm was added to Closed for the first time, the value of lastSolCost was C (Sm ), and Sm was added to TList. b. From the description of Algorithm 4 it follows that the contents of TList are deleted only when the value of lastSolCost increases. c. From Lemma A.3 it follows that Algorithm 4 generates the solutions in non-decreasing order of cost. Hence, when Sm was generated for the second time, the value of lastSolCost did not change from C (Sm ). From the above facts it follow that Sm was present in TList when Sm was added to Open for the second time. Since, while adding a solution to Open, Algorithm 4 checks whether it is present in TList (at Line 16 of Algorithm 4); Algorithm 4 must had done the same while adding Sm to Open for the second time. Therefore Sm could not be added Open for the second time ­ a contradiction.   Theorem A.1 Sj  V , Sj is generated (at Line 11) by Algorithm 4 only once and in the non-decreasing order of costs while ties among the solutions having same costs are resolved as mentioned before. Proof: [Theorem A.1] Follows from Lemma A.2, Lemma A.3, Lemma A.4 and Lemma A.5.  
320

Generating Ordered Solutions for Explicit AND/OR Structures

Appendix B. Proof of Correctness of Algorithm 5
Definition B.q [Reconvergent Paths in Solution Space DAG] Two paths, (i) p1 = 1  · · ·  S 1 and (ii) p = S 2  · · ·  S 2 , in the SSDAG G s of an alternating Si 2 i1 im in 1 ^ are reconvergent if the following holds: AND/OR tree T
1 = S 2 , i.e. the paths start from the same node; a. Si i1 1 1 = S 1 , i.e. the paths ends at the same node; b. Si im n 1 = S 2 ; i.e. the paths do not have any common c. (j  [2, n - 1])(k  [2, m - 1]), Si ik j intermediate node.

Definition B.r [Order on Generation Time] In the context of Algorithm 5, we define an order relation, t  V × V , where (Sp , Sq ) t if Sp is generated by Algorithm 5 before Sq . ^ . Here V is set of vertices in SSDAG G s of an alternating AND/OR tree T Lemma B.1 Algorithm 5 adds the solutions to the Closed list in the non-decreasing order of costs. Proof: [Lemma B.1] Consider the following invariants of Algorithm 5 that follow from the description of Algorithm 5. a. The minimum cost solution from Open is always removed at line 11 of Algorithm 4. b. Algorithm 5 expands any solution, say Sp , in two phases. At the first phase Sp is expanded using the native swap options of Sp . The solutions that are added to Open as a result of the application of the native swap options, will have cost greater than or equal to C (Sp ). In the second phase, i.e., during lazy expansion, Sp is again expanded using a non native swap option. A solution Sp may undergo the second phase  times where 0    (|L(Sp )| - |N (Sp , k )|) and k is used to construct Sp . In every lazy expansion of Sp , a new solution is added to Open. Consider a solution Sm which is  using  by Algorithm 5 where S   P red(S ). Suppose swap constructed from Sm m j m option i  L(Sm ), and i  / N (Sm , j ), i.e., i is not a native swap option of Sm .  ). Suppose S and S  are the successors of S and S  respectively, Clearly i  L(Sm m c m c i i  -  , and S  constructed by the application of i , i.e., Sm  Sc -  S . m c Also let Sc is added to Closed after Sm . Consider the fact that Algorithm 5 does not apply swap option i to Sm , that is, Sc is  is added to Closed. Since C (S  )  C (S ), C (S  )  C (S ). not added to Open until Sc m c m c According to Algorithm 5, i is applied to Sm (during the lazy expansion), and Sc is  is added to Closed. Consider the time period between added to Open right after Sc  to Closed. During that period, every solution that is added adding Sm and adding Sc  ), i.e., the cost is less or equal to C (S ). In to Closed has cost between C (Sm ) and C (Sc c general, the application of a swap option to add a solution to Open is delayed by such an amount of time, say , so that all the solutions, which are added to Closed during this  time interval, have cost less than or equal to the solution under consideration.
321

Ghosh, Sharma, Chakrabarti, & Dasgupta

From the above facts it follow that Algorithm 5 adds the solutions to the Closed list in non-decreasing order of costs.   Lemma B.2 Any two reconvergent paths in the SSDAG G s of an alternating AND/OR ^ are of equal length. tree T Proof: [Lemma B.2] Consider the paths:
1 2 n 1 2 m  (i) p1 = S1 -  Sp -  ··· -  Sn , and (ii) p2 = S1 -  Sp -  ··· - -  Sn .













The edges in the paths represent the application of a swap option to a solution. Now p1 and p2 start from the same solution and also end at the same solution. Therefore the sets of swap options that are used in these paths are also same. Hence the lengths of those paths are equal, that is, in the context of p1 and p2 , n = m. Lemma B.3 For any set of reconvergent paths of any length n, Algorithm 5 generates at most one path. Proof: [Lemma B.3] The following cases are possible. [Case 1 (n = 2) :] Consider the following two paths:
1 2 1 2  (i) p1 = S1 -  S2 -  S3 , and (ii) p2 = S1 -  S2 -  S3 .









 and  =   . Suppose S  S  . Here Algorithm 5 does not It is obvious that 1 = 2 2 t 2 2 1  . Therefore p is not generated by Algorithm 5. apply the swap option 1 to S2 2

[Case 2 (Any other values of n) :] In this case, any path belonging to the set of reconvergent paths, consists of n different swap options, suppose 1 , · · · , n . Also the start node and the end node of the paths under consideration are Sp and Sm . Consider the nodes in the paths having length 1 from Sp . Clearly there can be n such nodes. Among those nodes, suppose Algorithm 5 adds Sp1 to Closed first, and Sp1 is constructed from Sp by applying swap option 1 . According to Algorithm 5, 1 will not be applied to any other node that is constructed from Sp and is added to Closed after Sp1 . Therefore, all those paths starting from Sp , whose second node is not Sp1 , will not be generated by Algorithm 5. We can use the similar argument on the paths from Sp1 to Sm of length n - 1 to determine the paths which will not be generated by Algorithm 5. At each stage, a set of paths will not be grown further, and at most one path towards Sm will continue to grow. After applying the previous argument n times, at most one path from Sp to Sm will be constructed. Therefore Algorithm 5 will generate at most one path from Sp to Sm .   ^ c ] We define connection relation, Rc , a Definition B.s [Connection Relation Rc and R symmetric order relation for a pair of OR nodes, vq and vr , belonging to an alternating ^ as: AND/OR tree T ^ there exists an AND node vp , from which (vq , vr )  Rc | if in T (ii) p2 = vp  . . .  vr
322

there exist two paths, (i) p1 = vp  . . .  vq , and

Generating Ordered Solutions for Explicit AND/OR Structures

^ c , is defined between two swap options as follows. ConSimilarly the connection relation, R sider two swap options iq and jr , where iq = ei , eq , iq and jr = ej , er , jr . Suppose OR edges ei and eq emanate from vp , and OR edges ej and er emanate from vt . Now ^ c if (vp , vt )  Rc . (iq , jr )  R Definition B.t [Mutually Connected Set] For a solution Sm , a set Vm of OR nodes is mutually connected, if v1 , v2  Vm , (v1 = v2 )  {(v1 , v2 )  Rc } Consider the set of OR nodes, Vm = {v1 , · · · , vk }, where swap option j belongs to vj and ^m = {1 , · · · , k } is mutually connected. 1  j  k. Here the set of swap options V ^ , P red(Sm ) = Lemma B.4 Suppose Sm is a solution of an alternating AND/OR tree T {S1 , · · · , Sk }, and swap option j is used to construct Sm from Sj where 1  j  k. The swap options 1 , · · · , k are mutually connected. Proof: [Lemma B.4] Since Sm is constructed from S1 , · · · , Sk by applying 1 , · · · , k respectively, 1 , · · · , k are present in the signature of Sm . Suppose set s = {1 , · · · , k }. We have to show that ^c a , b  s , (a , b )  R ^ c . Also Sm is conFor the purpose of proof by contradiction, let us assume (i1 , i2 )  / R structed by applying i1 and i2 to Si1 and Si2 respectively. Consider the path p1 in SSDAG ^ which starts from Sopt and ends at Sm , and along p1 , Si is the parent of Sm . Now of T 1 along this path, i2 is applied before the application of the swap option i1 . Similarly con^ which starts from Sopt and ends at Sm , and along p2 , sider the path p2 in SSDAG of T Si2 is the parent of Sm . Along this path, i1 is applied before the application of the swap option i2 . Suppose i1 and i2 belongs to OR node v1 and v2 respectively. Since along path p1 , i1 is the swap option which is applied last, Sm contains node v1 . Similarly along path p2 , i2 is the swap option which is applied last. Hence Sm contains node v2 . Therefore, there must ^ , from which there exist paths to node v1 and v2 ­ implies that be an AND node vr in T ^ c . We arrive at a contradiction that proves 1 , · · · , k are mutually connected. (i1 , i2 )  R   Definition B.u [Subgraph of SSDAG] Consider a solution Sp of an alternating AND/OR ^ and mutually connected set Vm of OR nodes in Sp , where vq  Vm , C (Sp , vq ) = tree T s (S , V ) = V Copt (vq ) . The subgraph Gsub p m sub , Esub of the SSDAG with respect to Sp and Vm is defined as follows. Vsub consists of only those solutions which can be constructed from Sp by applying a sequence of swap options belonging to Vm , and Esub is the set of edges corresponding to the swap options that belong to Vm .
s (S , V ) Lemma B.5 The number of total possible distinct solutions at each level d in Gsub p m d-2 , where | V | = n . is n+ m n -1

323

Ghosh, Sharma, Chakrabarti, & Dasgupta

Proof: [Lemma B.5] Consider the swap options that belong to the nodes in Vm . With s (S , V ) is represented by a sequence respect to these swap options, every solution Sr in Gsub p m of numbers of length n, Seq (Sr ), where every number corresponds to a distinct node in Vm . The numerical value of a number represent the rank of the swap option that is chosen for a node vq  Vm . According to the representation, at each level: i. the sum of numbers in Seq (Sr ) of a solution, Sr , is equal to the sum of numbers in  ) of any other solution, S  , in that same level; Seq (Sr r ii. the sum of numbers in Seq (Sr ) of a solution, Sr , is increased by 1 from the sum of  ) of any solution, S  , of the previous level. numbers in Seq (Sr p Hence, at the dth level, there are n slots and d - 1 increments that need to be made to Seq (Sr ). This is an instance of the well known combinatorial problem of packing n + d - 1 objects in n slots with the restriction of keeping at least one object per slot. This can be d-2 done in n+ ways.   n -1 Theorem B.1 The solution space tree constructed by Algorithm 5 is complete. Proof: [Theorem B.1] For the purpose of contradiction, suppose Sm is the first solution which is not generated by Algorithm 5. Also P red(Sm ) = {Spi } and Sm can be constructed from Spi by applying qi , where 1  i  k. From Lemma B.4 it follows that the set of swap options {qi | 1  i  k} is mutually connected. Therefore the set of OR nodes Vm to which the swap options belong is also mutually connected. Suppose |Vm | = n. Consider the solution Sq , where Vm is mutually connected, and for 1  i  k, every qi belongs to the set of native swap options of Sq with respect the swap option that is used to construct Sq . Clearly vt  Vm , C (Sq , vt ) = Copt (vt ) We argue that Sq is generated by Algorithm 5 because Sm is the first solution which is s of T s rooted at S , where only not generated by Algorithm 5. Consider the subtree Tsub q the edges corresponding to swap options that belong to Vm are considered. Now we prove s is equal to the that the number of solutions generated by Algorithm 5 at every level of Tsub s number of solutions at the same level in Gsub (Sq , Vm ). Consider the solution Sq and the set Succ(Sq ). Suppose Succ(Sq , Vm ) is the set of successor solutions that are constructed from Sq by applying the swap options belonging  is the minimum cost solution in Succ(Sq , Vm ). According to to the nodes in Vm , and Smin  Algorithm 5 initially Succ(Smin ) is partially explored by using the set of native swap options  of Smin . Any other non native swap option, b , that belongs to the nodes in Vm , is used to   explore Succ(Smin ), right after the sibling solution of Smin , constructed by applying b to Sq , is added to Closed. Consider the fact that for solution Sq , vt  Vm , C (Sq , vt ) = Copt (vt ) holds. Therefore all the swap options belonging to Vm will also be eventually used to explore  the successors of Smin . Similarly the second best successor of Sq will be able use all but  . one swap option, c , which is used to construct Smin  s The immediate children of Smin in Tsub will consist of all solutions, that can be obtained   by the application of one swap option in Vm to Smin . The native swap list of Smin contains the swap option ranking next to c . The swap options, that are used to construct the other
324

Generating Ordered Solutions for Explicit AND/OR Structures

 n - 1 sibling solutions of Smin , will be used again during lazy expansion, which accounts   for another n - 1 children of Smin . Hence there would be n children of Smin . s Similarly, the second best successor of Sq in Tsub will have n - 1 immediate children. s will have n - 2 children and so on. Now the children The third best successor of Sq in Tsub of these solutions will again have children solutions of their own, increasing the number of solutions at each level of the tree. This way, with each increasing level, the number of solutions present in the level keeps increasing. We prove the following proposition as a part of proving Theorem B.1. s ) is given by Proposition B.1 At any level d, the number of solutions N (d, n, Tsub n s )= N (d, n, Tsub k =1 s )= N (d - 1, k, Tsub

n+d-2 n-1
n

Proof: [Proposition B.1] At second level, there are n solutions. These give rise to solutions at third level. Similarly at fourth level we have
n n -1 n -2 k =1

k

k+
k =1 k =1

k+
k =1

s s ) + ... + 1 ) + N (3, n - 1, Tsub k.... + 1 = N (3, n, Tsub

We can extend this to any level d and the result is as follows.
s ) = 1 N (1, n, Tsub s ) = n N (2, n, Tsub

n

s N (3, n, Tsub )

=
k =1 n k =1

k=

n+1 2 n+2 3

s N (4, n, Tsub ) =

s N (3, k, Tsub )=

s by induction on the depth d. We determine the number of solutions at any level of Tsub

[Basis (d = 1) :]

s ) = n. Clearly, N (1, n, Tsub

[Inductive Step :] Suppose, at dth level the number of solutions is Therefore at d + 1th level,
n s )= N (d + 1, n, Tsub k =1 s )= N (d, k, Tsub

n+d-2 n -1

=

n+d-2 d-1

.

n+d-2 n+d-3 + + ··· + 1 = d-1 d-1

n+d-1 n-1

Since Algorithm 5 does not generate duplicate node, and from Proposition B.1 the s (S , V ) at any level is equal to the number of solutions in number of solutions in Gsub q m s (S , V ) is also generated by s that level of Tsub , at any level the set of solutions in Gsub q m s (S , V ), will s Algorithm 5 through Tsub . Therefore, the level, at which Sm belongs in Gsub q m also be generated by Algorithm 5. Therefore Sm will also be generated by Algorithm 5 ­ a contradiction which establishes the truth of the statement of Theorem B.1.  
325

Ghosh, Sharma, Chakrabarti, & Dasgupta

Appendix C. Conversion between AND/OR Tree and Alternating AND/OR Tree
An AND/OR tree is a generalization of alternating AND/OR tree where the restriction of strict alternation between AND and OR nodes are relaxed. In other words an intermediate OR node can be a child of another intermediate OR node and the similar parent child relation is also allowed for AND node. We present an algorithm to convert an AND/OR to an equivalent alternating AND/OR tree. We use two operations namely, folding and unfolding for the conversions. Corresponding to every edge, a stack, update-list, is used for the conversions. In an AND/OR tree, consider two nodes, vq and vr , of similar type (AND/OR) and they are connected by an edge er . Edges, e1 , · · · , ek emanate from er . [Folding OR Node :] Suppose vq and vr are OR nodes. The folding of vr is performed as follows. · The source of the edges e1 , · · · , ek are changed from vr to vq and the costs are updated as ce (ei )  ce (ei ) + ce (er ) + cv (vr ) where 1  i  k, that is the new cost is the sum of the old cost and the cost of the edge that points to the source of ei . The triplet vr , cv (vr ), ce (er ) is pushed into the update-list of ei , 1  i  k. · The edge er along with node vr is removed from vq . [Folding AND Node :] Suppose vq and vr are AND nodes. The folding of vr is performed as follows. · The source of the edges e1 , · · · , ek are changed from vr to vq . One of the edges among e1 , · · · , ek , suppose ei , is selected arbitrarily and the cost is updated as ce (ei )  ce (ei ) + ce (er ) + cv (vr ) where 1  i  k. The triplet vr , cv (vr ), ce (er ) is pushed into the update-list of ei , whereas the triplet vr , 0, 0 is pushed into the update-list of ej , where 1  j  k and j = i. · The edge er along with node vr is removed from vq . The unfolding operation is the reverse of the folding operation and it is same for both OR and AND nodes. It works on a node vq as follows. Procedure Unfold(node vq )
1 2 3 4 5 6 7 8 9 10 11

forall edge ei that emanate from vq do if the update list of ei is not empty then vt , c1 , c2  pop(update list of ei ); if there exists no edge et from vq that points to the node vt then Create a node vt , and connect vt using edge et from vq ; cv (vt )  c1 ; ce (et )  c2 ; else if c2 = 0 then ce (et )  c2 ; end end

326

Generating Ordered Solutions for Explicit AND/OR Structures

Function Convert takes the root node of AND/OR tree and transforms it to an equivalent alternating AND/OR tree recursively. Function Convert(vq )
1 2 3 4 5 6

7

if every child of vq is a terminal node then if vq and its parent vp are of same type then Apply f old operation to vq ; end else foreach child vr of vq , where vr is an intermediate AND/OR node do Convert(vr ); end

Function Revert takes the root node of an alternating AND/OR tree and converts it to the original AND/OR tree recursively. Function Revert(vq )
1 2 3 4 5 6

if every child of vq is a terminal node then return; Perform unf old operation to vq ; foreach child vr of vq do Revert(vr ); end

The overall process of generating alternative solutions of an AND/OR tree is as follows. The AND/OR tree is converted to an alternating AND/OR tree using Convert function, and the solutions are generated using ASG algorithm. The solutions are transformed back using the Revert function. The proof of correctness is presented below. C.1 Proof of Correctness Suppose in an AND/OR tree T two nodes, vq and vr , are of similar type (AND/OR) and they are connected by an edge er . Edges e1 , · · · , ek emanate from er . Now fold operation 1 is the AND/OR tree which is generated by the application is applied to vq and vr . Let T of the f old operation. Lemma C.1 In the context mentioned above, we present the claim of in the following two propositions. Proposition C.1 The set of solutions of T having node vq can be generated from the set 1 having node v by applying the unfold operation to v of the solutions of of solutions of T q q T .
1 of T 1 that contains node v , there exists a soluProposition C.2 For every solution Sm q  1 tion Sm of T that can be generated from Sm by applying unfold to vq .

Proof: [Proposition C.1] We present the proof for the following cases. Consider any solution of Sm of T that contains node vq .
327

Ghosh, Sharma, Chakrabarti, & Dasgupta

a. vq and vr are OR nodes: There are two cases possible. 1. vr is absent from Sm : Since the fold operation modifies the edge er only, all 1 . Therefore S the other edges from vq in T are also present in T m will also 1 be present in the solution set of T and it will remain unchanged after the application of unfold operation. 2. vr is present in Sm : Since there are k distinct OR edges emanating from vr , let any one of those OR edges, say ei , is present in Sm . We prove that there is 1 of T 1 , such that the application of unfold operation to S 1 will a solution Sm m  generate Sm . The application of fold operation to the node vr modifies the source 1 . and the cost of edge ei from vr to vq and ce (ei ) to ce (ei ) + ce (er ) + cv (vr ) in T 1 is a solution of T 1 , where the edge e is present in S 1 . Also other Suppose Sm i m  1 and S than the subtree rooted at vq , the remaining parts of Sm m are identical 1 1 with each other. Clearly Sm exists as a solution of T and the application of 1 generates S . unfold operation to vq in Sm m b. vq and vr are AND nodes: Since vq is an AND node Sm will contain all of the AND edges that emanate from vq . Therefore edge er and vr both will be present in 1 of T 1 , such that the following holds. Sm . Consider the solution Sm 
1. 1. vq is present in Sm

2. The subtrees rooted at the children of vq other than vr in Sm are identical with 1 . the subtrees rooted at those children of vq in Sm
1 and S are identical 3. Other than the subtree rooted at vq , remaining parts of Sm m with each other. 1 exists as a solution of T 1 and the application of unfold operation to v Clearly Sm q  1 in Sm generates Sm .  of T 1 Any other solution Sm  that does not contain node vq , is a valid solution for T as well.  

Proof: [Proposition C.2] We present the proof for the following cases. Consider any 1 of T 1 that contains node v . solution Sm q  a. vq and vr are OR nodes: Since vq is an OR node, exactly one OR edge ei of vq will 1 . There are two cases possible. belong to Sm
1 : Since the fold operation modifies 1. ei was not modified while folding vr in T the edge er and the OR edges of vr only, all the other edges from vq in T are 1 . Since e was not modified during folding, the same solution also present in T i 1 Sm is also a valid solution for T . 1 : Suppose e connects v and v in 2. ei was modified while folding vr in T i q i 1 and generate solution S . 1 Sm . Apply the unfold operation to the node vq in Sm m The edge ei will be replaced with edge er which connects vq and vr and then ei will connect vr and vi . We argue that Sm is a valid solution of T since the

328

Generating Ordered Solutions for Explicit AND/OR Structures

subtree rooted at vi is not modified by the sequence of ­ (a) the folding of vr to 1 from T , and (b) the unfolding of v to construct S from S 1 . construct T q m  m
1 will contain all of the b. vq and vr are AND nodes: Since vq is an AND node, Sm AND edges that emanate from vq . There are two types of AND edges emanating from 1 and they are (a) Type-1 : the edges from v that are also present in T vq in T q  from vq , (b) Type-2 : the edges that are added to vq by folding and these edges are from 1 and generate solution vr in T . Apply the unfold operation to the node vq in Sm Sm . Sm will contain Type-1 edges, and another edge er from vq . In Sm , vq and vr are connected by er and the Type-2 edges are originated from vr . We argue that Sm is a valid solution of T since the subtree rooted at nodes pointed by Type-2 edges 1 from T , are not modified by the sequence of ­ (a) the folding of vr to construct T  1. and (b) the unfolding of vq to construct Sm from Sm 1 of T 1 that does not contain node v is valid solution for T Clearly any solution Sm q  as  well.  


Lemma C.2 If function Convert is applied to the root node of any AND/OR tree T , an ^ is generated. alternating AND/OR tree T Proof: [Lemma C.2] Function Convert traverses every intermediate node in a depth first manner. Consider any sequence of nodes, vq1 , vq2 , · · · , vqn of same type, where vqi is the parent of vqi+1 in T and 1  i < n. Obviously, the fold operation is applied to vqi+1 before vqi , where 1  i < n. In other words, the fold operation applied to the sequence of nodes in the reverse order and after folding vqi+1 , all the edges of vqi+1 are modified and moved to vqi , where 1  i < n. When the function call Convert(vq2 ) returns, all the edges of vq2 , · · · , vqn are already moved to vq1 and the sequence of nodes, vq1 , vq2 , · · · , vqn are flattened. Therefore, every sequence of nodes of same type are flattened, when the function call Convert(vR ) returns, where vR is the root of T and an alternating AND/OR tree ^ is generated. T ^ , the updateLemma C.3 If function Revert is applied to an alternating AND/OR tree T ^ becomes empty. list of every edge in T Proof: [Lemma C.3] Follows from the description of Revert. Theorem C.1 For any AND/OR tree T , it is possible to construct an alternating AND/OR ^ using function Convert, where the set of all possible solutions of T is generated tree T ^ , and then converting in the order of their increasing cost by applying Algorithm 4 to T individual solutions using function Revert. Proof: [Theorem C.1] According to Lemma C.2, after the application of function Convert ^ is generated. Consider the intermediate AND/OR to T an alternating AND/OR tree T 0 , T 1 , · · · , T n are the trees that are the generated after folding every node in T . Let T   n . Since T i is generated from T i+1 0 = T ,T ^ = T sequence of AND/OR trees and T     
329

Ghosh, Sharma, Chakrabarti, & Dasgupta

i , where 0  i < n, according after folding exactly one node in T i can be generated from T i+1 by unfolding the same solutions of T  ^ , Revert unfolds every node vq in Lemma C.3, for any solution of T ^ . Therefore vq was folded by Convert while transforming T to T ^ . can be generated from the solutions of T

to Lemma C.1, the node. According to that solution, where the solutions of T

References
Bonet, B., & Geffner, H. (2005). An algorithm better than AO ?. In Proceedings of the 20th national conference on Artificial intelligence - Volume 3, pp. 1343­1347. AAAI Press. Chakrabarti, P. P. (1994). Algorithms for searching explicit AND/OR graphs and their applications to problem reduction search. Artif. Intell., 65 (2), 329­345. Chakrabarti, P. P., Ghose, S., Pandey, A., & DeSarkar, S. C. (1989). Increasing search efficiency using multiple heuristics. Inf. Process. Lett., 32 (5), 275­275. Chang, C. L., & Slagle, J. R. (1971). An admissible and optimal algorithm for searching AND/OR graphs. Artif. Intell., 2 (2), 117­128. Chegireddy, C. R., & Hamacher, H. W. (1987). Algorithms for finding k-best perfect matchings. Discrete Applied Mathematics, 18 (2), 155­165. Chen, H., Xu, Z. J., Liu, Z. Q., & Zhu, S. C. (2006). Composite templates for cloth modeling and sketching. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1, pp. 943­950. IEEE Computer Society. Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction to Algorithms (2nd edition). McGraw-Hill Higher Education. Darwiche, A. (1999). Compiling knowledge into decomposable negation normal form. In Proceedings of the 16th international joint conference on Artifical intelligence - Volume 1, pp. 284­289. Morgan Kaufmann Publishers Inc. Darwiche, A. (2001). Decomposable negation normal form. J. ACM, 48, 608­647. Dasgupta, P., Sur-Kolay, S., & Bhattacharya, B. (1995). VLSI floorplan generation and area optimization using and-or graph search. In VLSI Design, 1995., Proceedings of the 8th International Conference on, pp. 370 ­375. Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artif. Intell., 171 (2-3), 73­106. Ebendt, R., & Drechsler, R. (2009). Weighted A search - unifying view and application. Artificial Intelligence, 173 (14), 1310 ­ 1342. Elliott, P. (2007). Extracting the k best solutions from a valued And-Or acyclic graph. Master's thesis, Massachusetts Institute of Technology. Elliott, P., & Williams, B. (2006). DNNF-based belief state estimation. In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, pp. 36­41. AAAI Press.
330

Generating Ordered Solutions for Explicit AND/OR Structures

Eppstein, D. (1990). Finding the k smallest spanning trees. In Proc. 2nd Scandinavian Worksh. Algorithm Theory, No. 447 in Lecture Notes in Computer Science, pp. 38­ 47. Springer Verlag. Eppstein, D. (1998). Finding the k shortest paths. SIAM J. Comput., 28 (2), 652­673. Flerova, N., & Dechter, R. (2010). M best solutions over graphical models. In 1st Workshop on Constraint Reasoning and Graphical Structures. Flerova, N., & Dechter, R. (2011). Bucket and mini-bucket schemes for m best solutions over graphical models. In GKR 2011(a workshop of IJCAI 2011). Fromer, M., & Globerson, A. (2009). An LP view of the m-best MAP problem. In Advances in Neural Information Processing Systems (NIPS) 22, pp. 567­575. Fuxi, Z., Ming, T., & Yanxiang, H. (2003). A solution to billiard balls puzzle using ao algorithm and its application to product development. In Palade, V., Howlett, R., & Jain, L. (Eds.), Knowledge-Based Intelligent Information and Engineering Systems, Vol. 2774 of Lecture Notes in Computer Science, pp. 1015­1022. Springer Berlin / Heidelberg. Gogate, V., & Dechter, R. (2008). Approximate solution sampling (and counting) on AND/OR spaces. In CP, pp. 534­538. Gu, Z., Li, J., & Xu, B. (2008). Automatic service composition based on enhanced service dependency graph. In Web Services, 2008. ICWS '08. IEEE International Conference on, pp. 246 ­253. Gu, Z., Xu, B., & Li, J. (2010). Service data correlation modeling and its application in data-driven service composition. Services Computing, IEEE Transactions on, 3 (4), 279­291. Gupta, P., Chakrabarti, P. P., & Ghose, S. (1992). The Towers of Hanoi: generalizations, specializations and algorithms. International Journal of Computer Mathematics, 46, 149­161. Hamacher, H. W., & Queyranne, M. (1985). K best solutions to combinatorial optimization problems. Annals of Operations Research, 4, 123­143. Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. J. Artif. Intell. Res. (JAIR), 28, 267­297. Hansen, E. A., & Zilberstein, S. (2001). LAO  : A heuristic search algorithm that finds solutions with loops. Artificial Intelligence, 129 (1-2), 35 ­ 62. Homem de Mello, L., & Sanderson, A. (1990). AND/OR graph representation of assembly plans. Robotics and Automation, IEEE Transactions on, 6 (2), 188 ­199. Jim´ enez, P., & Torras, C. (2000). An efficient algorithm for searching implicit AND/OR graphs with cycles. Artif. Intell., 124, 1­30. Kleinberg, J., & Tardos, E. (2005). Algorithm Design. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA. Lang, Q. A., & Su, Y. (2005). AND/OR graph and search algorithm for discovering composite web services. International Journal of Web Services Research, 2 (4), 46­64.
331

Ghosh, Sharma, Chakrabarti, & Dasgupta

Lawler, E. L. (1972). A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem. Management Science, 18 (7), pp. 401­405. Ma, X., Dong, B., & He, M. (2008). AND/OR tree search algorithm in web service composition. In PACIIA '08: Proceedings of the 2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application, pp. 23­27, Washington, DC, USA. IEEE Computer Society. Majumdar, A. A. K. (1996). Generalized multi-peg Tower of Hanoi problem. The Journal of the Australian Mathematical Society. Series B. Applied Mathematics, 38, 201­208. Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for solving mixed integer linear programming problems. In CP, p. 857. Marinescu, R., & Dechter, R. (2006). Memory intensive branch-and-bound search for graphical models. In AAAI. Marinescu, R., & Dechter, R. (2007a). Best-first AND/OR search for 0/1 integer programming. In CPAIOR, pp. 171­185. Marinescu, R., & Dechter, R. (2007b). Best-first AND/OR search for graphical models. In AAAI, pp. 1171­1176. Marinescu, R., & Dechter, R. (2009a). AND/OR branch-and-bound search for combinatorial optimization in graphical models. Artif. Intell., 173 (16-17), 1457­1491. Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search for combinatorial optimization in graphical models. Artif. Intell., 173 (16-17), 1492­1524. Martelli, A., & Montanari, U. (1973). Additive AND/OR graphs. In Proceedings of the 3rd international joint conference on Artificial intelligence, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Martelli, A., & Montanari, U. (1978). Optimizing decision trees through heuristically guided search. Commun. ACM, 21, 1025­1039. Mateescu, R., & Dechter, R. (2008). AND/OR multi-valued decision diagrams for constraint networks. In Concurrency, Graphs and Models, pp. 238­257. Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR multi-valued decision diagrams (AOMDDs) for graphical models. J. Artif. Intell. Res. (JAIR), 33, 465­519. Mathews, D. H., & Zuker, M. (2004). RNA secondary structure prediction. In Encyclopedia of Genetics, Genomics, Proteomics and Bioinformatics. John Wiley & Sons, Ltd. Nilsson, D. (1998). An efficient algorithm for finding the m most probable configurations in probabilistic expert systems. Statistics and Computing, 8, 159­173. Nilsson, N. J. (1980). Principles of artificial intelligence. Tioga Publishing Co. Otten, L., & Dechter, R. (2011). Anytime AND/OR depth-first search for combinatorial optimization. In SoCS. Pearl, J. (1984). Heuristics: intelligent search strategies for computer problem solving. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
332

Generating Ordered Solutions for Explicit AND/OR Structures

Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (2nd edition edition)., chap. Planning, pp. 375­461. Prentice-Hall, Englewood Cliffs, NJ. Shiaa, M. M., Fladmark, J. O., & Thiell, B. (2008). An incremental graph-based approach to automatic service composition. IEEE International Conference on Services Computing, 4 (2), 46­64. Shin, D. H., Jeon, H. B., & Lee, K. H. (2010). A sophisticated approach to composing services based on action dominance relation. In Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific, pp. 164 ­170. Subramanian, S. (1997). Routing algorithms for dynamic, intelligent transportation networks. Master's thesis, Virginia Technical Univ., Dept. of Civil Engineering. Sugimoto, K., & Katoh, N. (1985). An algorithm for finding k shortest loopless paths in a directed network. Trans. Information Processing Soc. Japan, 26, 356­364. In Japanese. Szymanski, M., Barciszewska, M. Z., Barciszewski, J., & Erdmann, V. A. (2005). 5S Ribosomal RNA Database. http://biobases.ibch.poznan.pl/5SData/. Online Database. Takkala, T., Bornd¨ orfer, R., & L¨ obel, A. (2000). Dealing with additional constraints in the k-shortest path problem. In Proc. WM 2000. Topkis, D. M. (1988). A k-shortest path algorithm for adaptive routing in communications networks. Trans. Communications, 36 (7), 855­859. Yan, Y., Xu, B., & Gu, Z. (2008). Automatic service composition using AND/OR graph. In E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services, 2008 10th IEEE Conference on, pp. 335­338.

333

Digital microfluidic biochip revolutionizes the medical diagnosis process rendering multiple tasks executed on a single chip. Incorporation of multiple functionality makes the design process complex and costly for digital microfluidic biochip. Physical simulation for the device components in a biochip is essential in todays manufacturing industry. In this paradigm, design automation and development of computer-aided-design tool that can perform physical level simulation and testing becomes crucial for a successful biochip design. This paper presents a comprehensive survey on design automation for biochip. Initially, a brief description on popular optimization techniques and some heuristic algorithms to solve various optimization problems is presented, followed by a review on biochip design automation works. Generally, architectural and geometry level synthesis for biochip design is performed using optimization techniques. Hence, some recent works on bioassay analysis, resource binding, and scheduling in geometry level are discussed. Finally the survey concludes with some possible future research directions.We present formal methods for determining whether a set of components with given reliability certificates for specific functional properties are adequate to guarantee desired end-to-end properties with specified reliability requirements. We introduce a formal notion for the reliability gap in component-based designs and demonstrate the proposed approach for analyzing this gap using a case study developed around an Elevator Control System.Handling Fault Detection Latencies in Automata-based Scheduling for Embedded Control Software
Santhosh Prabhu M, Aritra Hazra, Pallab Dasgupta and P. P. Chakrabarti Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, West Bengal, India - 721302. Email: {santhosh.prabhu, aritrah, pallab, ppchak}@cse.iitkgp.ernet.in
Abstract--There has been recent interest in automata-based scheduling for dynamic adaptation of schedules for embedded control software. Recently we have shown how automata-based scheduling can be extended to account for the possibility of faults in application of control. In this paper, we address the problem of automata-based scheduling when latencies are associated with detection of faults. We show that the gametheoretic approach that is used for handling faults under full visibility may be suitably augmented so as to decide the scheduling strategy in the presence of latencies in fault detection. Index Terms--Embedded Control Systems, Automata-based Scheduling, Fault-tolerant System, Reliable Scheduling.

I. I NTRODUCTION Typically, in any software controlled embedded system, a control software is executed on a computational platform (often on ECUs/processors) and interacts with the plant through its sensors and actuators. It enacts a set of periodically scheduled control actions to perform a required activity and achieve the desired performance of the overall embedded system. The control components are scheduled in a way so that it meets some performance requirement like exponential stability. A typical software controller in an embedded system can have multiple software components that run on shared ECUs. Each of these components is usually responsible for some aspect of the control activity. Generally the task of scheduling the components so as to satisfy the control requirement is done by a scheduler. Every time a component gets scheduled, it controls the system by applying some transformation, which can be represented by a transformation matrix. Hence, scheduling a component effectively means determining the transformation matrix to be applied on the system. Alur and Wiess [1], [7] have described an automata based solution to the problem of generating schedules that satisfy the exponential stability [2], [3], [4] requirement. Their approach is based on the observation that the language of admissible schedules is  -regular. It involves using a generating automaton to dynamically generate admissible schedules with respect to the stability criterion. The scheduling problem gets more complicated when there exists a possibility of a fault. A fault causes the chosen control actions of a software component to be applied incorrectly, or not applied at all. When this happens, a mutated transformation matrix can be said to be applied on

the system. In a previous work [5], we have addressed the problem of automata based scheduling in the presence of faults, and shown that it is possible to construct an automaton for scheduling the components, even under the possibility of faults. The approach formulates the scheduling problem as a game, such that a winning strategy in the game denotes a safe scheduling strategy for the scheduler. In [5], we have assumed that the scheduler has full visibility of the faults, i.e, the scheduler knows about the fault as soon as it occurs. However, in many real world scenarios, the fault monitoring mechanism might be such, that the occurrence of the fault is not detected immediately by the scheduler. In this paper, we study the problem of scheduling software components in such situations. The primary contributions of this paper is as follows: · We study the problem of reliable control scheduling where the environment can introduce faults in the applied transformations scheduled by the controller and these faults are not immediately visible to the scheduler. · We formulate this problem as a partial observability game between environment and scheduler. · We derive an automaton which can generate schedules that adheres to the exponential stability criterion under partial observability of faults. · We present a set of experimental results showing the efficacy of our proposed method. This paper is organized as follows. In Section II, we describe the background for the problem of scheduling software components. The problem is presented formally in Section III. Section IV describes the mechanism for handling latencies in fault detection. We describe the implementation details and present the results in Section VI. Section VII concludes this paper. II. BACKGROUND Formally, the dynamics of the physical plant can be approximated by a given discrete-time linear time-invariant system, described as: xp (t + 1) y (t) = Ap xp (t) + Bp u(t) (1) (2)

= Cp xp (t)

The state of the plant is defined by the set of plant variables, xp , the output of the plant is y , and the control input of the

plant is u. The matrices Ap , Bp and Cp denotes the state transition matrix, the input map, and the output map for the plant model. Equation (1) defines the state transition function of the plant, and Equation (2) defines the plant output. The feedback control software reads the plant output, y , and adjusts the control variables, u. The controller can be designed as a linear time-invariant system such that the stability [3], [4] of the whole system (the controller and the plant together) is guaranteed when the output of the plant is fed to the input of the controller and vice versa. Formally, it is assumed that the controller of such an embedded system can be represented as a discrete-time linear time-invariant system: xc (t + 1) = Ac xc (t) + Bc u(t) (3)

At+1 . . . At+L of Z . We shall refer to sequences of transformations satisfying the exponential stability requirement as admissible sequences. It was shown in [7] that the language of admissible sequences is  -regular. The language can be expressed as: L(A) =  -    where  consists of the L-length sequences of transformations that violate the exponential stability criteria. In [1], it was shown that it is possible to construct a finite state automaton, such that any infinite random walk in it represents an admissible sequence of transformations and vice versa. III. P ROBLEM F ORMULATION

The role of the automata-based scheduler described in [1] is to ensure that the sequence of control components that u(t) = Cc xc (t) (4) are scheduled adheres to the admissible patterns with respect where, xc is the state of the controller and Ac , Bc and Cc are, to the stability criteria. In reality, the application of control respectively, the state transition matrix, the input map, and the can be non-ideal due to several other factors such as delay output map for the controller. Equation (3) and Equation (4) in receiving sensory inputs and electrical faults affecting define the controller state transition function and the output the sensors / actuators, even when the software components function, respectively. The dynamics of the composed system are scheduled correctly. Following the terminology of the (the controller and the plant together) can be described by previous section, a fault at a time step is manifested as a transformations of the form: mutation on the transformation scheduled in that time step. In other words, we consider scenarios where the scheduler selected some transformation Ai , but the actual execution Ap Bp C c x(t + 1) = x(t) (5) resulted in a transformation, A ^i , which differs from Ai in Bc C p Ac terms of the updation of the control variables that are affected T T where x = (xT by the fault. p , xc ) is the concatenation of the states of the plant and the controller. In general different types of faults may be manifested We can represent the sequence of control actions applied as different mutations on a transformation matrix, Ai . Our on the environment by one or more controllers, by a sequence approach is capable of handling multiple types of faults, but of transformation matrices, A1 , A2 . . ., where each Ai is we consider a single type of fault for ease of presentation. chosen from an alphabet  of transformation matrices. The We are given a set  of transformation matrices, which matrix in Equation 5 is one such transformation matrix. correspond to simultaneous control actions by one or more We denote by Ci a particular combination of controllers software components. We are also given f , the mutated that may execute together. A word  = 1 2 . . ., (where versions of the transformation matrices in . We assume each i  [1, m]) specifies a schedule, which denotes the that within any given window of length L, a maximum sequence of combinations C = C1 C2 . . .. The sequence of k faults can occur. Also, it is assumed that the fault of transformations corresponding to this schedule is given by is detected by the scheduler after m units of time, where A = A1 A2 . . .. m  L (We believe that our approach will hold even when One of the standard control requirements is that of expo- m > L, but such scenarios are realistically not possible). nential stability. A system (whose state is defined in terms of Such criteria indicating fault detection latencies introduces n variables) is said to be (L, )-exponentially stable, given the a new dimension to the problem formulation and proposed parameters L  N and  (0, 1], if ||x(t + L)||/||x(t)|| < approach in comparison to our previous work [5] where we for every t  N. It follows from control theory and the work provided reliable control schedules under faults with full presented by Weiss and Alur [1], [7] that the exponential visibility. stability requirement can be captured by the following lanHowever, in this work, our problem is to arrive at a strategy guage: for scheduling the transformation matrices from  such that, even in the presence of up to k faults in any window of length ExpStab(L, ) = {  I  : L, and the faults being detected only after m time instants, ||At+L · · · At+1 || < f or every t  N} the exponential stability requirement is not violated. This definition means that an infinite sequence, Z , of transExample 1: Consider a braking system, which reduces the formations satisfies the exponential stability requirement iff speed of a vehicle using a combination of brakes and throttle ||At+L · · · At+1 || < for every L length subsequence control. We are given two transformation matrices AR and

AS , corresponding to application of brakes and adjusting the throttle. Assume that a fault may cause the apply brake action to manifest incorrectly at most once (k = 1) in every 3 cycles ^S being (L = 3), resulting in the transformation matrix A applied. The fault detection latency is assumed to be 1. We are given that the stability requirement admits the following set of sequences: { AS , AR , AS , AS , AR , AR , AS , AS , AR , ^S , AR , AS , AS , AR , A ^S , AR , AS , AS , A ^S , AR , AR , AR , A ^S } AR , AS , AR , AR , A The scheduler must ensure that in every window of three steps, the transformation applied is confined to these sequences ­ even in the presence of faults. IV. S CHEDULING UNDER FAULTS WITH L ATENT V ISIBILITY We have formulated the problem of scheduling with fault detection latencies as a partial observation game between the scheduler, which is the protagonist and the environment, which is the antagonist1 . In every round of the game, the scheduler decides on a transformation matrix to be scheduled, and the environment chooses whether to mutate it or not, constrained by the assumptions in the fault model. The environment's objective is to force an inadmissible sequence to be scheduled, while the scheduler's is to prevent that from happening. Due to the latency in fault detection, the scheduler is unable to immediately determine which of the possible set of moves the environment has opted for. The state space for the game is defined by the set of L length sequences over   f , along with the current turn. We define the game graph as G = (V, E ) as follows: · The set V of vertices consists of two types of vertices. Those where the scheduler makes its move (VS ), and those where the environment makes its move (VE ). In addition to the turn, the vertex also stores the L length history of transformation matrices. The sequences in VS are the ones which are actually applied on the system. On the other hand, the last matrix in the sequences associated with vertices in VE may get mutated by the environment in its move. · The set E of edges contains edges between every vertex u to v from V such that, 1) the players getting the turn in u and v are different 2) If u  VS , sequ [2 · · · L] = seqv [1 · · · L - 1], and the edge is labeled by the matrix seqv [L]. 3) If u  VE , sequ [1 · · · L - 1] = seqv [1 · · · L - 1], the edge is labeled by the matrix seqv [L], and seqv [L] = sequ [L] or seqv [L] = sequ [L]. Here, sequ and seqv denote the sequences associated with u and v respectively.
1 We point out here that the adversarial modeling of the environment has not been made after a qualitative analysis. There may exist non adversarial models of the environment which may also be suitable.

Due to the partially visible nature of the game, the scheduler will not be able to recognize the mutations in the most recent window of length m. The intuitive idea for constructing a winning strategy for the scheduler is quite straightforward ­ we try to come up with a strategy such that the scheduler's response to every move of environment, will guarantee that the scheduler can force victory irrespective of what move the environment has chosen. The responses to the move begin to differ only when the actual nature of the environment's move becomes known to the scheduler. To obtain such a strategy for the scheduler, we first expand the graph as a tree and using the Min-max algorithm[6], we annotate each state with win/loss markings. In order to be able to perform such an annotation, it is necessary to first define the winning and losing leaf nodes. The definition of losing leaf nodes is straightforward. Any state whose L length sequence is present in the set of inadmissible sequences is a loss node. For defining the winning leaf nodes for the scheduler, we use the following lemma: Lemma 1: If the antagonist has a winning strategy from a node v in the game tree, and there is another occurrence of v on the path from the root, then the antagonist has a winning strategy from the previous occurrence of v that does not involve the latter occurrence of v . Proof: The antagonist wins if the state of the game reaches a loss node. Therefore, a winning strategy for the antagonist is a subtree, W , of the game tree, T , such that all leaf nodes of W are loss nodes and at each intermediate node of W , the following condition holds: each node u  VE in W has a single successor from T in W and each node u  VS has all successors from T in W . Consider the smallest subtree, W , which is a winning strategy from a node v . Then W cannot contain another occurrence of v , otherwise the subtree of W rooted at the second occurrence will be a smaller winning strategy. Since the subtrees rooted at each occurrence of v are isomorphic, the winning strategies from each occurrence of v are also isomorphic. So, the smallest winning strategy from the first occurrence of v does not involve the subsequent occurrences of v in the game tree. On the basis of this lemma, we mark as a winning leaf node any state where the protagonist(the controller) has the turn, and has already been seen in the path to it from the root. Once this is done, the Min-max algorithm may be applied directly. Figure 1 illustrates a portion of the Minmax tree, for the controller described in Example 1. The game is assumed to start from the state corresponding to the sequence AS , AR , AS . If the scheduler chooses AR in the first round, the environment will be unable to mutate it, and it will be applied correctly. This takes the game to the state corresponding to the sequence AR , AS , AR . Then, if in the second round, the scheduler chooses AS and the environment chooses not to mutate it, the game reaches the state with sequence AS , AR , AS , which is a repetition, and hence labeled as a winning state. That this tree gives all

possible winning strategies for the controller under the given fault model, when the faults are immediately visible, is stated by the following lemma. Lemma 2: The scheduler can guarantee a schedule satisfying the exponential stability criterion if and only if the protagonist has a winning strategy in our game formulation. Proof (Reproduced from [5]): We recall that a schedule satisfies the exponential stability criterion if the sequence of transformations applied in this schedule does not contain any inadmissible L-length subsequence. A winning strategy for the protagonist in the game is one that prevents the state of the game to reach any LOSS node, which represents a state of the system where the last L transformations constitute an inadmissible sequence. Therefore a winning strategy for the protagonist guarantees a schedule satisfying exponential stability. If the protagonist has no winning strategy, then the antagonist can lead the game to a LOSS node. In other words, the antagonist can choose a suitable sequence of mutations for each choice of transformations by the protagonist, such that an inadmissible sequence of transformations is guaranteed to be applied on the system. Now, it remains to be shown that the transformations that the scheduler can choose in order to satisfy exponential stability is no different from the choices available to it in the game as the protagonist, and that the environment as the antagonist in the game is only as powerful as it is in the real world, in terms of the mutations it can introduce. We make the assumption that any transformation matrix Ai   can be chosen by the protagonist at any instant, irrespective of the history of the game. So, the choices of transformation matrices available to the scheduler in the game is same as the choices available to it for scheduling. Given that only a maximum of k mutations are possible within any L-length window, and that the mutations are governed by no other constraint, whether the transformation matrix scheduled next can be mutated, is determined entirely by the number of mutations that have been introduced within the last L-length window. This restriction is placed on the antagonist of the game too. So, the antagonist is only as powerful as the environment in the real world. Once we obtain the game tree with win/loss markings on each state, we define an equivalence relation R on the set of states. Two states q and q are said to be equivalent with respect to R if they are indistinguishable for the controller. For example, in Figure 1, the states n4 and n5 are equivalent with respect to R, if the fault detection latency is not zero. We now construct a modified game tree, as follows: For every set S = {q1 , q2 . . . qn } of states which are equivalent with respect to R, we look for isomorphic winning strategies that are applicable from all qi  S . All other winning strategies are discarded, by marking the moves as losing moves. If there does not exist such an isomorphic winning strategy, we mark each qi  S as a loss node. After this step, we apply the Min-max algorithm again, and repeat the above procedure. The iteration stops when no more

A S ,AR ,AS AS n2 AS n4 L A R ,AS ,AS AS AR AS n7 SCHEDULER'S TURN ENVIRONMENT'S TURN W - WIN NODE L - LOSS NODE AS AS n9 W AR A S ,AR ,AS AR n8 Round-2 AS n5 n1 AR n3 STATE REPETITION n6 Round-1

AR

AS n10

AR

Fig. 1.
n1 AR n2 AR n3 W AS W n5 AR W n6 W

A section of a Game tree for Example 1
Winning Subtree W n1 AR n2 AR W AS L n7 AR W n8 AS W n5 n6 n4 W AR W AR L AS L n7 n8 AR W n4

W

(full visibility)

(A)

AR n3 W

W

n1 AR n2 W

Winning Subtree (latent visibility)

?

n1 AR L n2 AR n4 L

n3 W AS L n5

AR

AR W

(B)
n4 AR W n8 n3 L AS L n5

AR

AR W n6

AS L n7

AR L n6

AS L n7

AR L n8

Fig. 2. A snapshot showing the re-labeling of game trees (These trees are not necessarily subtrees of the game tree for the speed governor example).

changes are necessary to the game tree. Figure 2 illustrates how the markings on the game trees may change in each iteration of the above process. In the first game tree, nodes n3 and n4 are equivalent, and there exists an isomorphic winning strategy from both nodes, that of choosing AR . Choosing AS , however, is not a winning strategy from n4 , and hence, choosing AS is marked as a losing move from n3 , by relabeling n5 as a loss node. In the second game tree, the nodes n3 and n4 are again equivalent. On applying the same strategy, n5 and n8 gets relabeled as loss nodes, which in turn results in n3 , n4 and n2 being relabeled as loss nodes. Had there been no latency in fault detection, these nodes would have been winning nodes for the scheduler.

Theorem 1: The re-labeled game tree contains exactly those strategies that do not require the scheduler to behave differently in two states that appear similar to it due to fault detection latencies. Proof: Since the re-labeled subtree is obtained by dropping strategies from the original tree, and for no two equivalent states is the set of outgoing transitions different, it is clear that every strategy present in the tree is correct. We now prove that every correct strategy is present in the new tree. Suppose that there exists a subtree for a strategy that was present in the original game tree, but is not present in the new tree. We prove that such a subtree cannot exist. Consider a subtree of height zero. The root of such a tree will be marked as a losing state, even though it may not be among the inadmissible sequences. This is true because it is not possible for the scheduler to distinguish between this state and some state that is actually a losing state. Such states should be marked as loss, since the attempts by the scheduler to take the game to such a state might actually be leading the game to a losing state. Now assume that no strategy of upto h rounds is missed. Consider a strategy of h + 1 rounds. Since the strategy doesn't require the scheduler to behave differently in two states that appear similar, the first move of the strategy must be applicable in all states that are equivalent to the root of the subtree. But if that be the case, the remainder of the strategy would still be in the tree, by our assumption. So, no strategy of h + 1 rounds is missed. Thus, by induction, the re-labeled tree contains all strategies in which the scheduler behaves identically on the same information. V. AUTOMATON FOR GENERATING SCHEDULES From the marked Min-max tree for the game formulation, we construct the automaton for generating the schedules. The automaton is defined by a tuple Q, , , q0 , where:
·

VI. I MPLEMENTATION AND E XPERIMENTAL D ETAILS We have implemented an optimized version of the proposed approach, in which we intelligently prune subtrees based on equivalence even as the Min-max algorithm assigns the initial markings to the nodes. For doing this, we keep track of equivalence between the explored states, and whenever the strategy of a state is modified, the strategies of all states equivalent to it are also modified. Whenever a state is found to be losing, the subtrees rooted at that node and all other nodes equivalent to it are pruned. Table I presents the results obtained from the implementation. The results are obtained for different values of L, different alphabets, different sets of admissible sequences and different values of m and k . Columns 1-3 give the window length (L), maximum number of injected faults (k ) and the latency in fault visibility (m), respectively. The number of transformation matrices are reported in Column 4. Column 5 presents the size of the equivalence classes and the required time for the overall analysis (in seconds) is outlined in Column 6. The execution time was obtained on a 1.6GHz quad core Intel Core i5 with 4 GigaBytes of memory, running 32-bit Linux. The language used for the implementation was Java (OpenJDK 1.6.0 24).
L 10 10 9 9 8 8 8 8 8 8 7 7 6 6 5 5 k 1 1 1 2 4 4 3 3 2 2 1 1 1 1 1 1 m 4 3 3 2 3 4 2 3 2 3 2 3 2 2 2 2 || 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 Number of Equivalence Classes 4119 4637 2072 6303 3550 2346 4822 2736 2336 1501 918 395 248 2720 107 1007 Time (in sec) 16.801 17.681 4.072 46.479 220.706 49.867 60.752 33.95 7.608 5.056 1.512 0.484 0.3 9.621 0.092 1.576

· ·

·

Q, the set of states of the automaton consists of L length sequences from (  f )k , in which not more than k symbols are from f . Also, the last m symbols should necessarily be from .  is, as defined previously, the set of transformation matrices   Q ×  × Q is the transition relation, which is defined as follows. (u, Ai , v )   if Ai is part of a winning strategy from the state with sequence u where protagonist has the turn and, v satisfies the following conditions: ^i and, 1) v [L] = Ai or v [L] = A 2) v differs at most at one position from u, and if it does, the difference will be that the L-m+1th symbol in v is the mutated version of the corresponding symbol in u. q0 is the predefined initial sequence

TABLE I E XPERIMENTAL R ESULTS

The results show that the proposed method works in reasonable time. It can be seen that the time for computing the strategy increases with L, k and the size of . A higher value of L means that a longer history needs to be remembered, and hence, the state space will larger. k and || affect the execution time by determining the branching factor of the Min-max tree. As k and || increases, the branching factor increases, thereby increasing the execution time. On the other hand, the execution time decreases as the latency (m) increases. This is because more subtrees get pruned out, and thereby, less states need to be explored.

VII. C ONCLUSION We have described how latencies in fault detection may be handled in the automata-based scheduling framework. We have shown that the game theoretic approach can be used with certain modifications, in cases where faults are not immediately visible. Experimental results are found to be quite encouraging. ACKNOWLEDGEMENT Pallab Dasgupta and P. P. Chakrabarti acknowledge the support of IGSTC (Indo-German Science and Technology Center) Project. Aritra Hazra is supported by Microsoft Corporation and Microsoft Research India under the Microsoft Research India PhD Fellowship Award. R EFERENCES
[1] R. Alur and G. Weiss. Regular Specifications of Resource Requirements for Embedded Control Software. In the Symposium on Real-Time and Embedded Technology and Applications (RTAS), pages 159­168, 2008. [2] L. Gurvits. Stability of Discrete Linear Inclusion. Linear Algebra Applications, 231:47­85, 1995. [3] J. P. Hespanha and A. S. Morse. Stability of Switched Systems with Average Dwell-time. In Proceedings of the 38th IEEE Conference on Decision and Control, volume 3, pages 2655­2660, 1999. [4] D. Liberzon. Switching in Systems and Control. Birkh¨ auser, 2003. [5] S. P. M, A. Hazra, and P. Dasgupta. Reliability Guarantees in Automata Based Scheduling for Embedded Control Software. To appear in IEEE Embedded Systems Letters (An extended version of this is available at: http://facweb.iitkgp.ernet.in/pallab/TechRep/ESL2013TR.pdf), 2013. [6] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Pearson Education, 2003. [7] G. Weiss and R. Alur. Automata Based Interfaces for Control and Scheduling. In 10th International Workshop on Hybrid Systems: Computation and Control (HSCC), pages 601­613, 2007.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/256537661

ARankingAlgorithmforOnlineSocialNetwork Search
ConferencePaper·August2013
DOI:10.1145/2522548.2523134

CITATIONS

READS

2
4authors,including: DivyaSharma IndianInstituteofManagement
6PUBLICATIONS10CITATIONS
SEEPROFILE

539

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

DebashisSaha IndianInstituteofManagementCalcutta
206PUBLICATIONS1,582CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyParthasarathiDasguptaon16April2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

A Ranking Algorithm for Online Social Network Search
Divya Sharma
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

AKZ Alam
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

Parthasarathi Dasgupta
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

Debashis Saha
IIM Calcutta D.H. Road, Joka Kolkata 700104, India

divyas12@iimcal.ac.in

abula12@iimcal.ac.in

partha@iimcal.ac.in

ds@iimcal.ac.in

ABSTRACT
Online Social Networks have become the new arena for people to stay in touch, pursue their interests and collaborate. In October 2012, Facebook reported a whopping 1 billion users which is testimony of the fact how online social networks have proliferated and made inroads into the real world. Some of the obvious advantages of social networks are (1) 24X7 availability allowing users to stay in virtual touch at any time of the day, (2) get in touch with people who have similar interests and collaborate with them and (3) the ability to be able to search for users to add to one's friend circle. The third advantage is the focus of this paper. With the increasing number of users on online social networks, it is important that when a user searches for another user, appropriate results are returned. This paper identifies three criteria ­ proximity, similarity and interaction, which can be used to rank search results so that more appropriate results are presented to the searching user. Also, this algorithm allows the search ranking to be customized according to the nature of the online social network in question.

search for potential friends and in doing this, return relevant results. The need for such a search technique arises due to the inherent structure of social networks and the behavior of users on the network. Most searches on an online social network are queries containing names of users and a multitude of users may share the same name, which makes the trivial task of searching for friends very cumbersome. In order to overcome this challenge, it would make sense to rank the list of search results in order of decreasing relevance to the user searching for friends. Here we discuss social network search ranking by means of an algorithm which takes into account three important factors that make search results relevant to a user ­ proximity, similarity and interaction. Based on these three factors, search ranks are allotted to search results when one user searches for another user by name. Also this algorithm takes into account the fact that all social networks are not of the same type. When a user searches for friends on a network of classmates, it is more relevant to rank results, which are closer to the searching user in the virtual space higher in the results list. On the other hand, on a social network for football fans, ranking must be done on the basis of sharing common interests like being fans of the same football club or the same football players. Still another example is a social network of acquaintances, where ranking on the basis of the level of interaction among users might be a more useful criteria. Rest of the paper is organized as follows. Section 2 discusses some of the related recent works. Section 3 describes the proposed work including the definitions of the ranking metrics used, the proposed rank function, and the proposed method. Section 4 summarizes the empirical results and Section 5 highlights some of the real-life applications of the proposed method. Finally, Section 6 gives the concluding remarks and discusses scopes for future works.

Categories and Subject Descriptors
H.3.5[Information Storage and Retrieval]: Online Information Services - Web-based services

General Terms
Algorithms, Management

Keywords
Algorithm, Online Social Network, Search Ranking.

1. INTRODUCTION
Social networks consist of sites that allow people to interact and share social experiences by exchange of multimedia objects (text, audio, and video) associated with the people themselves and their actions [7]. Every user on a social network possesses a user profile that contains all the information about the user ranging from basic information like name, date of birth, gender, location to more detailed information capturing educational and professional information and areas of interest. Online social networks have become abstractions of the real world where users interact, exchange and keep in touch. A major challenge, therefore, is the ability of a social network site to allow users to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. Compute '13, August 22 - 24 2013, Vellore, Tamil Nadu, India Copyright 2013 ACM 978-1-4503-2545-5/13/08...$15.00. http://dx.doi.org/10.1145/2522548.2523134

2. RELATED WORK
Recently, there has been lots of interest in the field of online social network search and ranking. The work in [7] focuses on the problem of how to improve the search experience of the users. It suggests seed based ranking instead of text-based ranking by measuring shortest distances between the nodes in a friendship graph. This is the first work that makes use of the friendship graph in a big social network to improve the search experience. The paper in [1] presents a novel social search model for finding a friend with common interests in OSN (Online Social Network) with the introduction of trust value and popularity value. The trust value is calculated by the improved shortest path algorithm with a trust threshold, and the popularity value is obtained by the page rank algorithm iteratively. In order to ensure more accurate search results, [3] demonstrates an algorithm called E.L.I.T.E. which has five essential components - Engagement-U, Lifetime, Impression, Timeframe and Engagement-O. Engagement-U is the affinity between users which is measured by their relationships and other related interests between them, Lifetime is a trace of users' past based on their positive, neutral and even negative interactions and

actions with other users, Impression is the weight of each object determined by the number of positive responses from users, Timeframe is the timeline scoring technique in which an object naturally loses its value as time passes and Engagement-O is the attraction of users to objects which is measured between objects and associated interests of users. Emphasizing on tree based search techniques, [4]compares the efficiency of reliable searching between Maximum Reliable Tree (MRT) algorithm and Optimum Branching Tree (OBT) algorithm and proposes the use of the MRT algorithm that is newly developed based on a graph-based method, as a generic technique which facilitates effective social network search and that can be the most reliable social network search method for the promptly appearing smart phone technologies. [5] explores the correlation between preferences of web search results and similarities among users by presenting an efficient search system called SMART finder. The work provides more information about SMART finder and publishes a quantitative evaluation of how SMART finder improves web searching compared to a baseline ranking algorithm. The concept of user tag feedback scores is employed in [2]. Based on this concept, a tag-based feedback web ranking algorithm is designed. The algorithm can efficiently use t he user's feedback. Searching for another person is one of the most fundamental uses to which social networks are put. However, most of the research work until now does not specifically look into this area. A small quantum of research work which does delve into this topic either provides just a conceptual framework or does not discuss the implementation of the framework in detail. Considering that online social networks are an abstraction of realworld social networks, it is important to understand the factors that influence the associations between the users. From experience it can be said that one person's association with another person is a function of how closely linked the two users are, how many interests they share and how often they interact. We call these three factors proximity, similarity and interaction respectively. Though this is not an exhaustive list of factors that might influence association between two persons, it does give a fairly good idea about the same. This work tries to use this concept of association between users to rank search results when a particular user searches for another user on an online social network. When any framework is to be implemented, it is required that the various factors which are being considered are quantified. The proposed algorithm in this work provides such a means to quantify the factors like proximity, similarity and interaction and then combine them to give a value for association between two persons. Along with providing a conceptual framework, this work also details out the implementation of the framework in practice.

node. It is calculated by running a shortest distance algorithm and finding the minimum distance of the various nodes in the search result from the searching node. Let d ab be the shortest distances between nodes a and b, then proximity pab is calculated as

Proximity may also be calculated as (1 -dab/D), where D is the diameter of the social network, used to normalize dab. This would ensure the maximum value of proximity to be 1.

3.1.2 Similarity
Social networks provide users a platform for interacting with other users who share similar interests, listen to the same kind of music, read books from the same author, follow the same sport, share the same hobbies, etc. On a social network all these details are captured in the user profiles. When a user issues a search for another user, a user who is more similar to the searching user is likely to be a more relevant result in comparison to a user who is less similar. We define a user profile of user a as

where k, l, m, n ... are the interests. Similarity S, between two nodes a and b in a list of search results with n nodes is defined as

Alternatively, similarity may be defined in terms of the Jacquard similarity coefficient as

In this case, we have considered the former definition of in order to define similarity in the context of all possible users' profiles. Though our definition of the might yield very small values of the metric, this would not affect the relative ranking of the users.

3.1.3 Interaction
Social networks provide users different means for interacting with one another. Most online social networks allow users to interact via textual comments, exchange links through shares and like the posts of other users. When two users interact on a social network, there are two factors that can be used to gauge the closeness of these two users ­ frequency of interaction and recency of interaction. Frequency captures volume of interaction (for each of comment, share, like) between two users within a given span of time. This span of time is defined by window size w, defined further in the discussion. Recency captures the time gap between the time of issuance of the search query and the most recent interaction (for each of comment, share, like) between the searching user and the user being searched. Frequency of an interaction of type T between user a and b is defined as

3. THE PROPOSED WORK 3.1 Ranking Metrics
This paper defines three metrics for the purpose of ranking search results:

3.1.1 Proximity
On a social network, a user may be linked directly and indirectly to millions of users. A social network is a myriad web of interconnections and a node which is closer to the searching node in terms of number of hops is likely to be a better search result in comparison to a node which is several hops further away. Proximity measures the closeness of a node from the searching

where Ti is the type of interaction ( i = 1 for comment, i = 2 for share and i = 3 for like) and VabTi is the volume of interactions between users a and b of type Ti. Recency of interaction between users a and b is defined as

where to is the time instance at which the search query was issued, tabTi is the time instance of the most recent interaction between user a and b of type Ti and window size wTi is defined as where users b, c, d, ..., n are the search results of the query. The frequency and recency metrics are then used to define interaction i of type Ti between two users a and b as where 0    1.  is defined as the relative importance of recency in comparison to frequency when quantifying a particular interaction. The weighted interaction metric I between two users a and b is defined as the weighted sum of the three types of interactions (comment, share, like) as where +  +  = 1. Here ,  and  define the percentage importance of the three types of interaction i.e. comment, share and like in the overall interaction metric I.

association function is central to the calculation of the ranks of the search results. We consider that n potential search results are returned by the searching algorithm for a given query. Once this list is obtained, the next steps are to calculate the association and then rank the list on the basis of the association values. The pseudo code for the algorithm is presented below. The variables and functions used in the pseudo code are explained in Table 1. Table 1. Description of Variables/Functions Variable/Function search_results ranked_list window_sizes Description Array of unranked results of the search query Array of ranked results of the search query An array of window sizes for interactions of type comment (index 1), share (index 2) and like (index 3) Cardinality of the set of common interests of the searching user and the search results Array of the association values of the search results Window recency size for comment

common_interests_ cardinality association comment_recency_ window share_recency_window like_recency_window search() compute_ranks() get_window_sizes()

3.2 The Association Function
Once the three metrics proximity, similarity and interaction have been defined, the next step is to define the association between the searching user and the search results based on these three parameters. Association captures the effect of the three metrics in question and returns a composite value which describes how closely the user is associated with each of the search results. As discussed earlier, the importance of each of these metrics may vary according to the nature of the online social network. Therefore, weights are defined to give different weightage to each of these factors while calculating the rank of the search results. The weighted association of a search result, b, being searched by user a is defines as whereµ1, µ2and µ3 are the weights assigned to each of the metrics pab, Sab and Iab. It is important to note that Therefore, µ1, µ2 and µ3 can be defined as the percentage importance of proximity, similarity and interaction in calculating the association according to the nature of the social network. For example, in social network catering to football fans, where similarity is more important than proximity and interaction, µ2 may have a higher value, while on a social network for classmates, proximity is more important, so, µ1may have a higher value.

Window size for share recency Window size for like recency Function to search results of a query potential

Function to rank the search results Function to compute the window sizes for interactions of type comment, share and like Function to compute cardinality of the set of common interests Function to compute association values of the search results Function to sort the list of search results according to association values Function to calculate the value of proximity metric between two nodes Function to calculate the value of similarity metric between two nodes Function to calculate the value of interaction metric between two nodes Function to return the number of interactions of type j between two nodes Function to returns the recency of interaction between two nodes

get_common_interests_ cardinality() compute_association() sort()

get_proximity()

3.3 The Rank Function
The ranks of the search results are subsequently obtained by sorting the results by the weighted association values. The search with the highest weighted association value is given rank 1, the search result with the second highest weighted association value is give rank 2, and so on, until the search result with the lowest weighted association value is given rank n. get_similarity()

get_interaction()

3.4 Algorithm for Computing Search Ranks
The proposed algorithm is a ranking algorithm and, therefore, allows different search algorithms to be used to identify the unranked search results. Once the unranked search results are obtained, they are then ranked using the proposed algorithm. The

interaction_volumej()

recency_of_interaction()

Pseudo Code: 1. Initialize the network N, searching user s, search query q 2. rank_search_results(N, s, q) 3. begin 4. search_results[1 ... n] search(q, N) 5. ranked_results[1 ... n] compute_ranks(s, search_results) 6. end 7. compute_ranks(s, search_results[1 ... n]) 8. begin 9. window_sizes[1 ... 3]get_window_sizes(s, search_results[1 ... n]) 10. common_interests_cardinalityget_common_interests_ cardinality(s,search_results[1 ... n]) 11. association[1 ... n] compute_association(s, search_results, window_sizes, common_interests_cardinality) 12. ranked_results[1 ... n]  sort(association[1 ... n]) 13. end 14. get_window_sizes(s, search_results[1 ... n]) 15. begin 16. comment_recency_window0 17. share_recency_window0 18. like_recency_window0 19. for i = 1 to n 20. comment_recency_windowmax(comment_recency_ window, comment_recencysi) 21. share_recency_windowmax(share_recency_window, share_recencysi) 22. like_recency_windowmax(like_recency_window, like_recencysi) 23. end 24. window_sizes[comment_recency_window, share_recency_window, like_recency_window] 25. end 26. get_common_interests_cardinality(s, search_results[1 ... n]) 27. begin 28. common_interestsinterestss 29. for i = 1 to n 30. common_interestscommon_interests interestsi 31. end 32. common_interests_cardinalitycardinality(common_in terests) 33. end 34. compute_association(s, search_results[1 ... n], window_sizes, common_interests_cardinality) 35. begin 36. for i= 1 to n 37. proximitysiget_proximity(s, search_resultsi) 38. similaritysiget_similarity(s, search_resultsi)/common_interests_cardinality 39. interactionsi get_interaction(s, search_resultsi, window_sizes) 40. associationsi µ1proximitysi+ µ2 similaritysi+ µ3interactionsi 41. end 42. end 43. get_proximity(s,i) 44. begin 45. proximity 1/(1+distance(s,i)) 46. end 47. get_similarity(s, i)

48. begin 49. similarity interestss interestsi 50. end 51. get_interaction(s, i, window_sizes) 52. begin 53. for j = 1 to 3 54. frequencyj 1 ­ 1/interaction_volumej(s,i) 55. recencyj 1 ­ recency_of_interactionj(s,i)/ window_sizej 56. end 57. frequency frequency1 +  frequency2+  frequency3 58. recency recency1 + recency2+  recency3 59. interaction   recency + (1 ­ ) frequency 60. end The search and sort functions in the above pseudo code are place holders for any searching and sorting algorithms that returns the list of search results based on the searching user's query and sort the search results on the basis of calculated association values respectively. Further details regarding the algorithm and its complexity have been described in [6].

4. EMPIRICAL RESULTS 4.1 The Framework
We propose to model a social network in the form of a graph, where the nodes correspond to the users and the edges correspond to the friendship between them. A part of one of the author's online social network was considered and anonymized for the purpose of this paper. Figure 1 shows this network, where a user John issues a search for another user Maria. The search query will return Maria a, Maria b and Maria c. The task now is to rank these results according to relevance to user John.

4.2 Simulation and Results
We now return to the framework described in Figure 1. The profiles of users we are interested in i.e. John, Mariaa, Mariab and Mariac are defined as follows:

The interactions between John and Maria a, Mariab and Mariac are defined in Table 2.The ranking algorithm was then simulated by varying the weights of the different parameters , , , , µ1, µ2 and µ3.

Figure 1. Model Social Network

4.3 Results and Comparisons
Some illustrative results of the proposed ranking algorithm for different weights are mentioned along with a comparison of search results from other popular social network sites and recent research work in this area in Table 3. The small simulation network which was used for computing the results of the proposed algorithm exists on Facebook and Google+, in reality, for one of the authors. It was observed that the results obtained by issuing the same search on these networks can be replicated using the proposed algorithm by varying the various parameters, i.e., , , , , µ1, µ2 and µ3. Table 3 is illustrative of this observation, as results at serial numbers 2, 5 and 6 give the same as those for Facebook and the result at serial number 1 is same as that of Google +. It is, therefore, critical to note that there may not be a unique set of parameters for ranking the search results in a particular fashion and the same search result ranking can be obtained by tuning the various parameters according to the requirements of the social network in which the algorithm is used. Though, this algorithm is not aimed at conjecturing the logic that might have been used to obtain search results by a social network website, it can, however, be used to obtain search results that concur with the results of the website in question to a certain extent. The algorithm discussed in [2], based on trust and popularity was also implemented on the small simulation network. The result obtained was similar to that for Google+. Certain assumptions

were made while implementing the algorithm in [2]. While calculating the contribution of trust in the rank, trust values for two adjacent nodes was taken as 0.5. For the calculation of the contribution of popularity in the rank, the damping factor d was taken to be 0.8 and the initial values of popularity were taken to be 0. The final values of popularity for each node with respect to the various keywords in the profile were obtained by running the circular algorithm until the values converged to a precision of 10%.

4.4 Scalability
The proposed algorithm was tested for scalability by varying the number of potential search results and measuring the time required for calculation of association function for the same. Table 4 shows selected results of the simulations. The time for execution with different number of potential search results was found to increase almost linearly. Figure 2 shows the plot of execution time versus the number of potential search results. The important insight from this analysis is that calculation of the association function for as many as 1 million records is 3188 ms, which means that if we assume each user to have 1000 contacts in his/her network we can calculate the association of a user with other users up to two hops away in about 3 seconds. This can have important implications for the usefulness of the association function. Some likely usages to which the association function can be applied are discussed in a section below.

Table 2.Interaction between Nodes Comment User Volume Mariaa Mariab Mariac 3 9 Most Recent 05/08/2012 24/09/2012 Volume 1 10 Most Recent 05/08/2012 18/07/2012 Volume 12 11 Most Recent 18/09/2012 02/10/2012 Share Like

Table 3. Selected Observations from Simulation Proposed Algorithm S.No 1. 2. 3. 4. 5. 6. 7. 8. 9.  0.5 0.5 0 0 0.5 0  0.5 0.5 0 0 1 1  0.3 0.3 0 0 0 0  0.2 0.2 0 0 0 0 Facebook Google + Rank Algorithm Based on Trust and Popularity [2] µ1 0.34 0.5 0.5 0 0.5 0.5 µ2 0.33 0 0.5 1 0 0 µ3 0.33 0.5 0 0 0.5 0.5 Rank 1 Mariaa Mariac Mariaa Mariaa Mariac Mariac Mariac Mariaa Mariaa Rank results Rank 2 Mariac Mariaa Mariab Mariab Mariaa Mariaa Mariaa Mariac Mariac Rank 3 Mariab Mariab Mariac Mariac Mariab Mariab Mariab Mariab Mariab

Table 4. Selected Results of Simulation No. of Potential Search Results 100 500 1000 5000 10000 50000 100000 500000 1000000 Execution Time (ms) 0.586108 2.867683 6.002439 33.4847 61.12565 174.8376 323.0852 1584.719 3188.202 The results from the proposed algorithm show that depending on the values set for the different parameters namely, , , , , µ 1, µ2 and µ3, suitable results can be obtained. The nature of a particular social network will dictate what values must be set for each of these parameters. The advantages of the proposed algorithm include: a) Intuitiveness: The algorithm uses intuitive concepts like proximity, similarity and interaction to rank search results such that more relevant results may be ranked higher than less relevant ones. Adaptability: The algorithm is not confined to providing suitable search ranks for any particular kind of network and can be adapted for use in any kind of social network by defining the various parameters according to the nature of the network Flexibility: The algorithm can use the search results supplied by any search algorithm. Once the association values of the elements in the search list are obtained, subsequently, any algorithm can be used to sort the elements to obtain the ranked list. Therefore, the algorithm provides flexibility of implementation as it can be easily plugged between the search and sort algorithms in a social network. 4. Weighted association, A, is the weighted sum of proximity, similarity and interaction such that 3. Interaction is defined as the weighted sum of the weighted sum of recency and frequency of a particular type of interaction. It is assumed that the window size, w, will always be a non-zero number. Therefore,

b)

c) Figure 2. Execution Time vs. Potential Search Results

4.5 Observations and Discussion
The nature of the rank function defined by the proposed algorithm leads to some important observations: 1. Proximity by definition is the inversely related to the distance, d, between two nodes, where the distance means the number of hops to reach the destination node from the source node. Therefore, 2. Similarity is the ratio of the cardinality of the set of common interests between the searching user and a user in the search result and the cardinality of the set of the union of interests of the searching user and all the users in the search result. It is assumed that the profile of a user would contain at least one interest and a finite number of interests. Therefore,

5. APPLICATIONS OF THE PROPOSED WORK
Association, defined as a composition of proximity, similarity and interaction in this work, provides a means of quantifying the strength of linkage between two persons. This concept is not new, but, what is novel is how this can be used. Though this work is restricted to its use for search result ranking on social networks, it can be used in lots of other areas in the online space like marketing and advertising.

5.1 Use in Social Networks for Suggesting Friends
It is a common practice to suggest connections to a user on social network websites. The utility of this functionality in a social network is derived from its ability to suggest useful connections which might exist in a user's extended network and may have similar interests as those of the user. The association function can

be used effectively in this regard and can identify potential connections for a user. Depending on the kind of social network, weights can be allotted to the factors of proximity and similarity and the corresponding association function values can be computed. Thereafter, the connections with association function values exceeding a pre-defined threshold can be suggested to a user. In this application, the factor of interaction is not considered as new connections are being suggested to a user with whom s/he does not have any previous communication history.

result, an adaptive algorithm has been proposed which uses intuitive concepts like proximity, similarity and interaction to rank search results according to their relevance in a particular social network setting. The proposed method has ample scope of improvement and future work. The ranking function may consider more aspects apart from those considered here. Considering the huge data volume associated with search through social networks, the proposed algorithm may have to be tuned appropriately for time-efficiency.

5.2 Use in Targeted Advertising and Marketing
Online advertisers can use this framework to identify prospects and then pursue them. An advertiser would be interested in finding people who might have similar affinities as their existing customers. The first step, for a given customer base would be to identify other people in the online social circles of these customers who have similar interests as the customers themselves. This can be done by calculating association values using the factor of similarity alone. Once prospects with high degree of similarity in interests as the existing customers have been found, the next step would be to identify people who may be easily influenced by the existing customers. For this, association values using both the factors of proximity and interaction can be computed. A prospect is likely to convert into a customer if s/he knows that the people who s/he is close to or often interacts with also use the product or service in question. The factors of proximity and interaction quantify this behavior of prospects and, therefore, association values on the basis of these two factors would serve to identify potential customers from the pool of prospects. In this way, targeted marketing and advertising can be carried out and the positive effects of word-of-mouth can made use of. The association function can also be put to other innovative uses like identification of potential employees by job portals, recommendation for downloads of movies/songs, suggestions for online games, etc.

7. REFERENCES
[1] Huang, Chuan, Yinzi Chen, Wendong Wang, Yidong Cui, Hao Wang, and Nan Du. A novel social search model based on trust and popularity." In 3rd IEEE International Conference on Broadband Network and Multimedia Technology (IC-BNMT), (2010), 1030-1034. [2] Jiang, Zongli, and Jingsheng Li. A tag feedback based sorting algorithm for social search. In International Conference on Systems and Informatics (ICSAI), (2012), 1482-1485. [3] Lee, Khuan Yew, and Jer Lang Hong. ELITE--A novel ranking algorithm for social networking sites. In 9th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), (2012), 1009-1013. [4] Lee, Wookey, JJ-H. Lee, JJ-S. Song, and CS-H. Eom. Maximum reliable tree for social network search. In IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing (DASC), (2011),1243-1249. [5] Park, GunWoo, SooJin Lee, and SangHoon Lee. To Enhance Web Search Based on Topic Sensitive_Social Relationship Ranking Algorithm in Social Networks. In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT'09, (2009), 3, 445-448. [6] Sharma Divya, Dasgupta Parthasarathi, and Saha Debashis. A Ranking Algorithm for Online Social Network Search. In Working Paper Series of Indian Institute of Management Calcutta. (2013), WPS No. 724. [7] Vieira, Monique V., Bruno M. Fonseca, Rodrigo Damazio, Paulo B. Golgher, Davi de Castro Reis, and Berthier RibeiroNeto. Efficient search ranking in social networks. In Proceedings of the sixteenth ACM Conference on information and knowledge management, (2007), 563-572.

6. CONCLUSION
Online social networks have become an essential part of the lives of internet users. The importance of these networks will only grow over time and they are likely to become more and more complex and specialized as they evolve. The work presented in this paper tries to provide a generic solution for ranking of search results over social networks. Considering the large volume of searches being performed on social networks, the trivial function of providing relevant search results has become a differentiator for different social networks. This work takes into account the fact that all social networks are not similar and, hence, the same search result algorithm is not likely to be useful for all of them. As a

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/280313140

Anewcrosscontaminationawarerouting methodwithintelligentpathexplorationin DigitalMicrofluidicBiochips
ConferencePaper·March2013
CITATIONS READS

0
5authors,including: PampaHowladar IndianInstituteofEngineeringScienceandTec...
5PUBLICATIONS0CITATIONS
SEEPROFILE

41

PranabRoy IndianInstituteofEngineeringScienceandTec...
46PUBLICATIONS71CITATIONS
SEEPROFILE

HafizurRahaman IndianInstituteofEngineeringScienceandTec...
339PUBLICATIONS871CITATIONS
SEEPROFILE

ParthasarathiDasgupta IndianInstituteofManagementCalcutta
92PUBLICATIONS238CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

PerformanceOptimizationofDielectricallyModulatedBiosensorsViewproject

ResearchScholarViewproject

AllcontentfollowingthispagewasuploadedbyPampaHowladaron24July2015.

Theuserhasrequestedenhancementofthedownloadedfile.

$QHZFURVVFRQWDPLQDWLRQDZDUHURXWLQJPHWKRGZLWK LQWHOOLJHQWSDWKH[SORUDWLRQLQ'LJLWDO0LFURIOXLGLF%LRFKLSV
6FKRRORI9/6,7HFKQRORJ\(PDLO±SDUWKD#LLPFDODFLQ %HQJDO(QJLQHHULQJDQG6FLHQFH8QLYHUVLW\6KLESXU,1',$   (PDLO±URQPDULQH#\DKRRFRLQSKRZODGDU#JPDLOFRP WKBUXS#\DKRRFRLQUDKDPDQBK#\DKRRFRLQ  $EVWUDFW² 'LJLWDO PLFURIOXLGLF V\VWHPV LQ UHFHQW \HDUV KDYH EHHQ ELRPHGLFDO DSSOLFDWLRQV VXFK DV SRO\PHU FKDLQ UHDFWLRQ SFU >@ GHYHORSHG DV DQ DOWHUQDWLYH SODWIRUP IRU H[HFXWLRQ RI PXOWLSOH HQ]\PH DVVD\V >@ SURWHRPLFV >@ GQD K\EULGL]DWLRQ DQG VRIW FRQYHQWLRQDO ODERUDWRU\ PHWKRGV VLPXOWDQHRXVO\ RQ D VLQJOH SODQDU SULQWLQJ >@ *HQHUDOO\ WKH PHFKDQLVP RI HOHFWUR ZHWWLQJ ' DUUD\ RI HOHFWURGHV WDUJHWHG IRU ELRFKHPLFDO DQDO\VLV DQG SKHQRPHQRQ LV EDVHGRQ FRQWURO RI VXUIDFH WHQVLRQRI D OLTXLGVROLG ELRPHGLFDO DSSOLFDWLRQV 'XH WR LWV GLVFUHWH QDWXUH GURSOHWV FDQ EH LQWHUIDFH E\ DSSO\LQJ HOHFWULFDO SRWHQWLDO DW WKH LQWHUIDFH $W WKH QDQROLWUH VFDOH IRUFHV RI LQWHUIDFLDO WHQVLRQ EHWZHHQ WKUHH SKDVHV PDQLSXODWHG WKURXJK PXOWLSOH UHFRQILJXUDEOH SDWKV GHULYHG E\ GRPLQDWH D GURSOHW¶V K\GURG\QDPLF EHKDYLRU 7KHVH LQFOXGH D IRUFH SUHSURJUDPPHG HOHFWURGH DFWXDWLRQ VHTXHQFHV WKURXJK WKLV SODQDU RQ WKH LQWHUIDFH EHWZHHQ WKH GURSOHW DQG WKH DPELHQW IOXLG DQG D DUUD\ NQRZQ DV GLJLWDO PLFURIOXLGLF ELRFKLS V\VWHP &URVV IRUFH DFWLQJ RQ WKH WULSKDVH FRQWDFW OLQH ZKHUH WKH GURSOHW WKH FRQWDPLQDWLRQ EHWZHHQ KHWHURJHQHRXV VDPSOHV WXUQV RXW WR EH D DPELHQWIOXLGDQGWKHVROLGPHHW(OHFWURFDSLOODULW\WKDWSURYLGHVWKH PDMRULVVXHFRQFHUQHGZLWKWUDQVSRUWDWLRQRIGURSOHWVDQGFRUUHFWQHVV GLVWULEXWLRQ RI IUHH FKDUJHV RQ WKH LQWHUIDFHV EHWZHHQ GLIIHUHQW RI WKH GHWHFWLRQ UHVXOWV IRU WKH ELRDVVD\ SURWRFROV ±ZKLFK LV KLJKO\ SKDVHV PRGXODWHV LQWHUIDFLDO WHQVLRQ IRUFHV WR PDQLSXODWH GURSOHWV VLJQLILFDQW IRU FOLQLFDO GLDJQRVWLFV DQG WR[LFLW\ PRQLWRULQJ >@ 7KHUHE\ ZKHQHYHU D GURSOHW LV NHSW LQ FRQWDFW ZLWK D VROLG DSSOLFDWLRQV,QWKLVSDSHUZHKDYHSURSRVHGDQLQWHOOLJHQWURXWHSDWK HOHFWURGH D ZHWWLQJ IRUFH FDQ DULVH XSRQ DSSOLFDWLRQ RI DQ HOHFWULF H[SORUDWLRQ WHFKQLTXH WKDW DWWHPSWV SDUWLDOO\ RU FRPSOHWHO\ WR DYRLG WKH QXPEHU RI FURVV FRQWDPLQDWLRQ GHSHQGLQJ RQ WKH IOXLGLF ILHOG WKURXJK WKHVH HOHFWURGHV 7KLV ZHWWLQJ IRUFH DFWLQJ RQ WKH WULSKDVH FRQWDFW OLQH FDQ EH DOWHUHG E\ DSSO\LQJ YDULHG HOHFWULFDO FRQVWUDLQWV HPSOR\HG GXULQJ URXWLQJ 7KH SDWK  LV IXUWKHU UHILQHG SRWHQWLDO WR WKH GURSOHW WKURXJK WKH HOHFWURGHV >@ +HQFH E\ XVLQJLQWHOOLJHQWGHWRXUE\LGHQWLI\LQJ]RQHVRIIULFWLRQEHWZHHQWZR DSSOLFDWLRQ RI D SURJUDPPHG VHTXHQFH RI HOHFWULF SRWHQWLDO DW DGMDFHQWURXWHSDWKVWKDWRSWLPL]HVWKHRYHUDOOURXWHWLPHE\UHGXFLQJ FRQVHFXWLYH HOHFWURGHV D VWUHQJWK GLVSDULW\ RI WKLV ZHWWLQJ IRUFH LV WKH RYHUDOO WLPH IRU VWDOOLQJ ZKLOH URXWLQJ ±DV ZHOO DV IXUWKHU FUHDWHGDQGWKLVUHVXOWVLQ PRYHPHQWRIGURSOHWVIURPRQHHOHFWURGH RSWLPL]DWLRQRIUHVRXUFHVWREHXWLOL]HG7KHVLPXODWLRQLVFDUULHGRXW FRQWDFWWRDQDGMDFHQWRQH RQ WHVW EHQFKHV RI EHQFKPDUN VXLWH , DQG EHQFKPDUN VXLWH ,,, 7KH 7KLV PDQLSXODWLRQ RI GLVFUHWH GURSOHWV DOORZV WR FODVVLI\   WKH UHVXOWV VKRZ LPSURYHPHQW LQ RYHUDOO DV ZHOO DV DYHUDJH URXWH WLPH PLFURIOXLGLFRSHUDWLRQVLQWRDVHWRIEDVLFRSHUDWLRQVQDPHO\PL[LQJ DQGPDMRUUHGXFWLRQLQWKHQXPEHURIFURVVRYHUV PHUJLQJ VSOLWWLQJ VWRUDJH DQG WUDQVSRUWDWLRQ +HQFH WKH XVH RI XQLW .H\ZRUGV GLJLWDO PLFURIOXLGLFV &URVV FRQWDPLQDWLRQ URXWLQJ YROXPHV RI GURSOHWV FDQ EH DFFRPSOLVKHG LQ D ' SODQDU DUUD\ RI GHWRXUDOJRULWKPUHVRXUFHXWLOL]DWLRQ HOHFWURGHVFRQILJXUHGIRUDQ(OHFWURZHWWLQJRQGLHOHFWULFV\VWHP7KH ,,1752'8&7,21 PLFURIOXLGLFDUUD\FRQWDLQVDVHWRIEDVLFFHOOVWKDWLVPDGHXSRIWZR 7KH PLQLDWXUL]DWLRQ LQWHJUDWLRQ DQG SDUDOOHOL]DWLRQ RI FRPPRQ SDUDOOHOJODVVSODWHV)LJXUH7KHERWWRPSODWHFRQWDLQVDSDWWHUQHG ODERUDWRU\ SURFHVVHV LQ ODERQDFKLS V\VWHPV KDYH SRWHQWLDOO\ DUUD\ RI LQGLYLGXDOO\ FRQWUROODEOH HOHFWURGHV DQG WKH WRS SODWH LV WUDQVIRUPHG FRQYHQWLRQDO ODERUDWRU\ SURFHGXUHV XVXDOO\ FRDWHG ZLWK D FRQWLQXRXV JURXQG HOHFWURGH 7KH ILOOHU PHGLXP VXFK FXPEHUVRPHDQGH[SHQVLYHLQWRORZFRVWDQGV\QWKHVL]DEOHVHULHVRI DVVLOLFRQHRLODORQJZLWKWKHVDPSOHGURSOHWLVVDQGZLFKHGEHWZHHQ RSHUDWLRQVWKDWLQYROYHVPXOWLSOHFKHPLFDOPDQLSXODWLRQVRQDVLQJOH WKH WZR SODWHV %\ DVVLJQLQJ WLPHYDU\LQJ YROWDJH YDOXHV WR WXUQ PRQROLWKLF SODWIRUP 7KH PDMRU DGYDQWDJH RI VXFK GHYLFHV OLHV LQ RQRII WKH HOHFWURGHV RQ WKH GLJLWDO PLFURIOXLGLF ELRFKLS WKH KLJKHU VHQVLWLYLW\ EHWWHU DFFXUDF\ ORZHU FRVW DV ZHOO DV SRUWDELOLW\ LQWHUIDFLDO WHQVLRQ RI WKH GURSOHWV DUH PRGXODWHG UHVXOWLQJ LQ WKHLU DQG KLJKHU OHYHOV RI V\VWHP LQWHJUDWLRQ 7KH PLQLDWXUL]DWLRQ WUDQVSRUWDWLRQ DURXQG WKH HQWLUH ' DUUD\ DQG H[HFXWLRQ RI LQFRUSRUDWHG LQ WKHVH GHYLFHV OHDGV WR VKRUWHU GHWHFWLRQ WLPHV KLJK IXQGDPHQWDO PLFURIOXLGLF RSHUDWLRQV IRU GLIIHUHQW ELRDVVD\V 7KH WKURXJKSXWDQGUHGXFWLRQRQHQHUJ\DVZHOODVUHDJHQWVFRQVXPSWLRQ RSHUDWLRQV SHUIRUPHG E\ DFWXDWLQJ FRQWURO YROWDJHV WKURXJK WKH LQELRORJLFDOH[SHULPHQWV HOHFWURGHVDUHDOVRFDOOHG UHFRQILJXUDEOH RSHUDWLRQVEHFDXVHRIWKHLU $ SRSXODU FODVV RI FRPPHUFLDOO\ DYDLODEOH ELRFKLSV DUH EDVHG RQ IOH[LELOLW\ LQ ORFDWLRQ DQG LQ H[HFXWLRQ WLPH 6XFK UHFRQILJXUDEOH FRQWLQXRXVIOXLGIORZEHLQJFRQWUROOHGE\PLFURSXPSVPLFURYDOYHV RSHUDWLRQFDQEHFDUULHGRXWDWDQ\SUHVFKHGXOHGSODFHZLWKLQWKH' HOHFWURNLQHWLFVRUHOHFWURRVPRVLV,QODWHVDQDOWHUQDWLYHFODVVRI SODQH+HQFHWKH'0)6RIIHUVDSODWIRUPWKDWSURYLGHVWKHDGYDQWDJH ODERQFKLSV\VWHPZKLFKLVFDSDEOHRIPDQLSXODWLQJGLVFUHWHGURSOHWV RI G\QDPLF UHFRQILJXUDELOLW\ DQG VRIWZDUH EDVHG FRQWURO IRU LQ YROXPHV RI QDQROLWUHV KDV EHHQ HPHUJHG 7KHVH QHZ FODVV RI PXOWLIXQFWLRQDOELRFKLSV GHYLFHVWHUPHGDV'LJLWDOPLFURIOXLGLFELRFKLS'0)%SHUIRUPVWKH 7ZR PDMRU UHVRXUFH FRPSRQHQWV LQYROYHG LQ SHUIRUPLQJ SURFHVVRIGURSOHWV PDQLSXODWLRQWKURXJKGLIIHUHQW PHFKDQLVPV YL] IXQGDPHQWDORSHUDWLRQVLQD'0)%DUHWKH0L[HUVDQG6WRUDJHXQLWV HOHFWURZHWWLQJ>@GLHOHFWURSKRUHVLV>@WKHUPRFDSLOODU\WUDQVSRUW FRQVLVWLQJ RI HOHFWURGHV 0L[HUV DUH XVHG WR SHUIRUP PL[LQJ DQG >@ DQG VXUIDFH DFRXVWLF ZDYH WUDQVSRUW >@ ,Q WKH GLJLWDO VSOLWWLQJ RSHUDWLRQV ZKHUHDV VWRUDJH XQLWV DUH XVHG IRU VWRUDJH RI PLFURIOXLGLFDUFKLWHFWXUHWKHEDVLFOLTXLGXQLWYROXPHLVIL[HGE\WKH GURSOHWV JHQHUDWHG IRU VXEVHTXHQW PL[LQJV 7UDQVSRUWDWLRQ SDWKV DUH JHRPHWU\RIWKHV\VWHPIOXLGTXDQWL]DWLRQZKHUHDVYROXPHWULFIORZ XVHGWRPRYHGURSOHWVDPRQJGLIIHUHQWFRPSRQHQWVHJPL[HUVDQG UDWH LV GHWHUPLQHG E\ WKH UDWH DQG WKH QXPEHU RI GURSOHW WUDQVSRUW VWRUDJHXQLWVZLWKLQWKH'DUUD\,QDGGLWLRQWKHDUUD\PD\FRQWDLQ &RPSDUHGWRFRQWLQXRXVIORZEDVHGWHFKQLTXHVGLJLWDOPLFURIOXLGLFV FHOOV WKDW FDQ SHUIRUP VSHFLDOL]HG RSHUDWLRQV VXFK DV KHDWLQJ RU RIIHUV WKH DGYDQWDJH RI LQGLYLGXDO VDPSOH DGGUHVVLQJ UHDJHQW RSWLFDO VHQVLQJ+HQFH WUDQVSRUWDWLRQ RI GURSOHWV LQ '0)%V ILJXUHV LVRODWLRQ DQG FRPSDWLELOLW\ ZLWK DUUD\EDVHG WHFKQLTXHV XVHG IRU RXW WR EH D PDMRU VWHS UHJDUGLQJ WKH RYHUDOO RSHUDWLRQ DQG UHVXOWV ELRFKHPLFDODQGELRPHGLFDODSSOLFDWLRQ>@ REWDLQHG LQ GLJLWDO PLFURIOXLGLF V\VWHPV 7KH G\QDPLF (OHFWURZHWWLQJRQGLHOHFWULFHZRGLVFXUUHQWO\XVHGDVRQHRIWKH UHFRQILJXUDELOLW\ LQKHUHQW LQ '0)%V DOORZV IRU VKDULQJ RI FHOOV E\ EHVW DFWXDWLRQ PHWKRGV IRU GURSOHW PDQLSXODWLRQ LQ GLJLWDO PXOWLSOH GURSOHWV URXWHV LQ D WLPH PXOWLSOH[HG PDQQHU ,I WKH PLFURIOXLGLF V\VWHPV >@ LW KDV HPHUJHG DV DQ XVHIXO WRRO IRU PLFURIOXLGLFV\VWHPLVGHVLJQHGIRUH[HFXWLRQRIPXOWLSOHELRDVVD\

3UDQDE5R\3DPSD+RZODGDU5XSDP%KDWWDFKDUMHH3DUWKDVDUDWKL'DVJXSWD +DIL]XU5DKDPDQ,QGLDQ,QVWLWXWHRIPDQDJHPHQW&DOFXWWD,1',$

978-1-4673-6040-1/13/$31.00 c 2013 IEEE

50

 )LJXUH±6FKHPDWLFGLDJUDPIRUGURSOHWPRYHPHQWLQD'0)% SURWRFROV LQYROYLQJ KHWHURJHQHRXV VDPSOHV ± WKLV VKDULQJ SKHQRPHQRQ PD\ UHVXOW LQ D PDMRU SUREOHP WHUPHG DV FURVV FRQWDPLQDWLRQ 6XFK FRQWDPLQDWLRQV FDXVHG E\ EHDG UHWHQWLRQ DQG OLTXLG UHVLGXH EHWZHHQ VXFFHVVLYH GURSOHW URXWHV RI GLIIHUHQW GURSOHW VDPSOHV PD\ UHVXOW LQ LQHYLWDEOH HUURQHRXV UHDFWLRQ WKDW SURGXFHV LQFRUUHFW GHWHFWLRQ RXWFRPHV ,Q DGGLWLRQ H[FHVVLYH DQG SURORQJHG FRQWDPLQDWLRQ PD\ UHVXOW LQ EUHDNGRZQ RI WKH HOHFWURGHV DV ZHOO DV HOHFWURGHVKRUWSUREOHPV7KLVILQDOO\JHQHUDWHVSK\VLFDOGHIHFWVDQG SURGXFHLQFRUUHFWEHKDYLRXUVLQWKHHOHFWULFDOGRPDLQ>@ 7KHUHE\ LW EHFRPHV QHFHVVDU\ WR GHVLJQ DOJRULWKPV WKDW SODQ WKH GURSOHWVWREHURXWHGLQGLVMRLQWPDQQHU:LWKIXUWKHULQFUHDVHLQWKH OHYHOVRILQWHJUDWLRQDVZHOODVGHJUHHRIVFDOLQJ±ILQGLQJRIGLVMRLQW URXWH ZLWKLQ D SUHGHWHUPLQHG OD\RXW EHFRPHV LQFUHDVLQJO\ GLIILFXOW 7KLV LQ WXUQ UHGXFHV WKH DYDLODELOLW\ RI WKH QXPEHU RI VSDUH FHOOV ± WKDW FDQ EH XVHG WR UHSODFH WKH GHIHFWLYH SULPDU\ FHOOV ZKLOH UHURXWLQJWRDYRLGIDXOW\FHOOVKHQFHFRPSURPLVLQJZLWKWKHGHIHFW WROHUDQFHIRUWKHJLYHQELRDVVD\ ,QWKLVSDSHUZHKDYHSURSRVHGDFURVVFRQWDPLQDWLRQDZDUHURXWLQJ PHWKRG WKDW LQYROYHV LQWHOOLJHQW H[SORUDWLRQ EHWZHHQ VXLWDEOH URXWH SDWKIRUHDFKVDPSOHGURSOHWV±WKDWQRWRQO\PLQLPL]HVRUHOLPLQDWHV WKHQXPEHURIFURVVFRQWDPLQDWLRQVEXWDOVRZLWKIXUWKHUUHILQHPHQW LQWHUPVRIGHWRXURURSWLPL]HGDOWHUDWLRQWKHRYHUDOOURXWHWLPHDV ZHOODVFHOOXWLOL]DWLRQFDQEHRSWLPL]HG7KHRUJDQL]DWLRQRIWKHUHVW RI WKH SDSHU LV DV IROORZV 6HFWLRQ ,, GLVFXVVHV WKH UHODWHG ZRUNV UHJDUGLQJGURSOHWURXWLQJDQGFURVVFRQWDPLQDWLRQLVVXHV6HFWLRQ,,, IRUPXODWHVWKHGURSOHWURXWLQJSUREOHPVDQGWKHFRQVWUDLQWVLQYROYLQJ FURVV FRQWDPLQDWLRQ 6HFWLRQ ,9 VWDWHV WKH PRWLYDWLRQ DQG WKH FRQWULEXWLRQ RI RXU ZRUN 6HFWLRQ 9 HODERUDWHV WKH GHVLJQHG DOJRULWKP ZLWK DQ LOOXVWUDWLYH H[DPSOH VWDWHG LQ 6HFWLRQ 9, ,Q 6HFWLRQ9,,WKHH[SHULPHQWDOVLPXODWLRQUHVXOWVDUHGLVSOD\HGWRJHWKHU ZLWK WKH FRUUHVSRQGLQJ FRPSDULVRQV ZLWK UHVXOWV REWDLQHG IURP FRQWHPSRUDU\ PHWKRGV )LQDOO\ 6HFWLRQ 9,,, VWDWHV WKH FRQFOXGLQJ UHPDUNVDQGWKHIXWXUHVFRSHVIRUIXUWKHUHQKDQFHPHQW

7KH LVVXH RI FURVV FRQWDPLQDWLRQ ZDV LQLWLDOO\ DGGUHVVHG LQ >@DQG >@,Q>@ZDVKGURSOHWVDUHLQWURGXFHG±WRRYHUFRPHWKHSUREOHP RI FRQWDPLQDWLRQ DW DQ\ SDUWLFXODU VLWH DQG VSHFLILF VWUDWHJLHV DUH GHVLJQHG EDVHOLQH DQG EDVHOLQH  WR V\QFKURQL]HWKH ZDVK GURSOHW RSHUDWLRQZLWKIXQFWLRQDOGURSOHWURXWHRSHUDWLRQV>@DWWHPSWHGWR KDQGOHWKHLVVXHRIFURVVFRQWDPLQDWLRQE\ILUVWXVLQJNVKRUWHVWSDWK URXWLQJ WHFKQLTXHV WR PLQLPL]H  WKH URXWLQJ FRPSOH[LWLHV DV ZHOO DV FHOO XWLOL]DWLRQ DQG WKHUHE\ UHGXFLQJ WKH QXPEHU RI FRQWDPLQDWLRQ VSRWV LQ WKH SURFHVV 7KHQ WR WDNH DGYDQWDJH RI PXOWLSOH ZDVK GURSOHWV D PLQLPXP FRVW FLUFXODWLRQ DOJRULWKP 0&& IRU RSWLPDO ZDVKGURSOHWURXWLQJWRVLPXOWDQHRXVO\PLQLPL]HWKHQXPEHURIXVHG FHOOVDQGWKHFOHDQLQJWLPHKDVEHHQDGRSWHG0RUHRYHUDORRNDKHDG SUHGLFWLRQWHFKQLTXHKDVEHHQXVHGWRGHWHUPLQHWKHFRQWDPLQDWLRQV EHWZHHQ VXFFHVVLYH VXESUREOHPV ,W KDV EHHQ IRXQG WKDW UHVXOWV REWDLQHGLQ>@KDVEHHQYDVWO\LPSURYHGDV FRPSDUHGZLWK>@DQG >@ 7R PDNH WKH ELRFKLS IHDVLEOH IRU SUDFWLFDO DSSOLFDWLRQV SLQ FRXQW UHGXFWLRQ SRVHV DQRWKHU PDMRU GHVLJQ SUREOHP ± ZKLFK KDV EHHQ DGGUHVVHG ZLWK D SLQ FRXQW DZDUH VWUDWHJ\ XVLQJ D WZR VWDJHG ,/3 EDVHG PHWKRG LQWHJUDWLQJ WKH FRQWDPLQDWLRQ LVVXHV LQ >@ ,Q >@ D PHWKRG KDV EHHQ SURSRVHG WKDW XWLOL]HG D JUDSK EDVHG KHXULVWLFV WR VHOHFW WKH EHVW VXLWDEOH SDWK IRU URXWLQJ XVLQJ PXOWLSOH DOWHUQDWLYHURXWHVWRDYRLGFROOLVLRQRUYLRODWLRQRIIOXLGLFFRQVWUDLQWV +RZHYHU WKH DOJRULWKP ZDV IRFXVHG RQO\ RQ FROOLVLRQV WR PLQLPL]H VWDOOLQJUHVXOWLQJLQVXEVWDQWLDOLPSURYHPHQWLQWKHURXWHWLPH,Q>@ WKHFURVVFRQWDPLQDWLRQLVVXHVZHUHQRWDGGUHVVHG±WKLVUHQGHUVWKH PHWKRGWREHVXLWDEOHRQO\IRUKRPRJHQHRXVVDPSOHDSSOLFDWLRQV ,Q DOO RI WKH DERYH PHWKRGV GLIIHUHQW URXWLQJ VWUDWHJLHV DUH SUHVFULEHG IRU RSWLPL]HG FRQWDPLQDWLRQ +HUH ZH XVHG D PHWKRG WR LQWURGXFH LQWHQWLRQDO UHGXQGDQF\ IRU HVWLPDWLQJ DW OHDVW RQH DOWHUQDWLYH IRU HDFK SDWK ZLWK PLQLPXP GHYLDWLRQ IRU HDFK VDPSOH GXULQJ YLUWXDO URXWH HVWLPDWLRQ SKDVH :H WKHUHE\ DWWHPSWHG DQ LQWHOOLJHQW H[SORUDWLRQ VWUDWHJ\ WR VHOHFW LWHUDWLYHO\ PRVW VXLWDEOH PLQLPXP GHYLDWLRQ SDWKV DPRQJ DYDLODEOH DOWHUQDWLYHV WR RSWLPL]H FURVV FRQWDPLQDWLRQ WR D PLQLPXP RU ]HUR $Q HIILFLHQW WUDGH RII VWUDWHJ\ KDV EHHQ DGRSWHG WR DYRLG RU PLQLPL]H VWDOOLQJ XVLQJ DOWHUQDWLYH GHYLDWLRQ RU GHWRXU WR PLQLPL]H WKH ODWHVW DV ZHOO DV DYHUDJHDUULYDOWLPH7HVWEHQFKHVIURPEHQFKPDUNVXLWH,DQG,,,KDV EHHQ XVHG IRU LPSOHPHQWDWLRQ RI WKH SURSRVHG VWUDWHJ\ DQG WKH VLPXODWLRQUHVXOWVDUHGLVSOD\HGLQVHFWLRQ9,,

,,,'523/(75287,1*$1'&5266 &217$0,1$7,21,1'0)%
7KH REMHFWLYH RI GURSOHW URXWLQJ LV WR WUDQVPLW DOO WKH GURSOHWV IURP WKHLU UHVSHFWLYH VRXUFHV WR WDUJHWV ZLWKLQ D ' JULG DUUD\ ZKLOH IXOILOOLQJ DOO WKH FRQVWUDLQWV LPSRVHG IRU WKH WUDQVSRUWDWLRQ $Q HIILFLHQW URXWLQJ VFKHGXOH YLUWXDO URXWH LV RIWHQ UHTXLUHG WR EH GHYHORSHG WRSURYLGH DQ RSWLPDO URXWLQJ LQ WHUPV RI REMHFWLYHVVXFK DV ODWHVW DUULYDO WLPH DQG RYHUDOO FHOO XWLOL]DWLRQ 'URSOHW URXWLQJ SUREOHP LQ '0)%V LV W\SLFDOO\ PRGHOHG LQ WHUPV RI D 'JULG )LJXUH  )RU HDFK GURSOHW WKHUH H[LVWV D VHW RI VRXUFH JULG ORFDWLRQV D VHW RI WDUJHW JULG ORFDWLRQV DQG RSWLRQDOO\ D VHW RI PL[HUV (DFK VRXUFHWDUJHW FRPELQDWLRQ LV GHILQHG DV D QHW $ SLQ QHW KDV D VLQJOH VRXUFH DQG VLQJOH WDUJHW $ FRPELQDWLRQ RI WZR 6RXUFHVRQH0L[HUDQGRQH7DUJHWIRUPVDSLQQHW ,Q FDVHV RI SLQ QHWV GURSOHW PHUJLQJ LV GHVWLQHG DW SUHGHILQHG ORFDWLRQVFDOOHGPL[HUV6HYHUDOPLFURIOXLGLFPRGXOHVPD\EHSODFHG DV SHU VFKHGXOHV IRU PL[LQJ VSOLWWLQJ VWRUDJH GLVSHQVLQJ DQG RWKHU RSHUDWLRQV ZLWKLQ WKH DUUD\ 7KHVH DUH FRQVLGHUHG DV WKH +DUG %ORFNDJHV ,Q RUGHU WR DYRLG FRQIOLFWV EHWZHHQ GURSOHW URXWHV DQG DVVD\ RSHUDWLRQV D VHJUHJDWLRQ UHJLRQ LV GHILQHG DURXQG WKH IXQFWLRQDOUHJLRQRIHDFKVXFKPLFURIOXLGLFPRGXOHV0RUHRYHUWKHUH DUH SRVVLELOLWLHV RI LQWHUVHFWLRQ RU RYHUODSSLQJ RI GURSOHW URXWHV GXULQJWKHLUFRQFXUUHQWURXWLQJLQWLPHPXOWLSOH[HGPDQQHU7RDYRLG VXFK XQGHVLUDEOH EHKDYLRUV IROORZLQJ IOXLGLF FRQVWUDLQWV DUH LQWURGXFHG /HW GL DW [WL \WL  DQG GM DW  [WM \WM GHQRWH WZR LQGHSHQGHQW GURSOHWV DW DQ\ JLYHQ WLPHVWDPS W 7KHQ WKH IROORZLQJ FRQVWUDLQWV GHILQHG DV )OXLGLF &RQVWUDLQWV DUH UHTXLUHG WR EH VDWLVILHG IRU DQ\ WLPHWZKLOHURXWLQJ>@ 6WDWLFFRQVWUDLQW_[WL±[WM_!RU_\WL±\WM_!

,,5(/$7(':25.6
7KH LVVXH RI VLPXOWDQHRXV GURSOHW URXWLQJ ZLWKLQ D '0)% LQYROYHV PDMRUREMHFWLYHVQDPHO\WKHRSWLPL]DWLRQRIWKHODWHVWDUULYDOWLPHRI DOOGURSOHWVIURPVRXUFHWRWDUJHWDVZHOODVWKHRYHUDOOWUDQVSRUWWLPH IRUDOOWKHGURSOHWVDQGRSWLPL]DWLRQRIWKHFHOOHOHFWURGHXWLOL]DWLRQ LQWKHSURFHVV'LIIHUHQWDOJRULWKPVKDYHEHHQSURSRVHGLQDQDWWHPSW WR UHVROYH WKH GURSOHW URXWLQJ LVVXHV ZLWK QHFHVVDU\ RSWLPL]DWLRQ ZKLOH PDLQWDLQLQJ WKH UHTXLVLWH FRQVWUDLQWV WR DFKLHYH WKH REMHFWLYHV VWDWHG DERYH $ GLUHFW DGGUHVVLQJ PHWKRG LV XVHG LQ >@ ZKHUH WKH GURSOHW URXWLQJ SUREOHP LV PDSSHG LQWR D JUDSK FOLTXH PRGHO $Q LQWHJHU OLQHDU SURJUDPPLQJ ,/3 IRUPXODWLRQ EDVHG RQ GLUHFW DGGUHVVLQJ PRGH LV UHSRUWHG LQ >@ 7KH PRYHPHQW RI GURSOHWV DW HDFKWLPHVWHSLVGHWHUPLQHGE\VROYLQJDQ,/3$µGURSOHWPRYHPHQW FRVW¶ LV XVHG DV D KHXULVWLF WR HYDOXDWH FRQJHVWLRQ ZKHQ VROYLQJ WKH ,/3 ,Q >@ G\QDPLF UHFRQILJXUDELOLW\ RI WKH PLFURIOXLGLF DUUD\ LV H[SORLWHG GXULQJ UXQWLPH $ KLJK SHUIRUPDQFH GURSOHW URXWLQJ DOJRULWKP XVLQJ JULGEDVHG UHSUHVHQWDWLRQ LV UHSRUWHG LQ >@ $ SDUWLWLRQEDVHGDOJRULWKPIRUSLQFRQVWUDLQWEDVHGGHVLJQLVSURSRVHG LQ >@ $QRWKHU IDVW URXWDELOLW\ DQG SHUIRUPDQFHGULYHQ GURSOHW URXWHUIRU'0)%VKDVEHHQSURSRVHGLQ>@$UHFHQWZRUNEDVHGRQ 6RXNXS¶V URXWLQJ DOJRULWKP >@ IRU FRQFXUUHQW SDWK DOORFDWLRQ WR PXOWLSOH GURSOHWV DSSHDUV LQ >@)LQDOO\ D SUHSODFHG PRGXOH EDVHG URXWLQJ PHWKRG LV SURSRVHG LQ >@ ZKLFK GLVSOD\V PXFK LPSURYHG UHVXOWVIRUFURVVUHIHUHQFLQJ%LRFKLSV

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

51

'\QDPLFFRQVWUDLQW_[WL±[WM_! RU_\WL±\WM_! RU_[WM±[WL_!RU_\WM±\WL_! 7KLVLPSOLHVWKDWIRUDQ\GURSOHWDWORFDWLRQ[\WKHORFDWLRQV [\[\[\[\[\[\[\ DQG [ \ DUH SURKLELWHG IRU DQ\ RWKHU GURSOHW WR HQWHU DW WLPHVWDPSV W DQG W LQ RUGHU WR PDLQWDLQ WKHVH IOXLGLF FRQVWUDLQWV +HQFH DOO WKH ORFDWLRQV DGMDFHQW WR [ \ DV VWDWHG DERYH IRUP D &ULWLFDO=RQH)LJXUHIRUDQ\GURSOHWDW [\ DWWLPHVWDPS W$ SUHGHWHUPLQHG WLPH OLPLW FDOOHG WKH 7LPLQJ &RQVWUDLQW GHILQHV WKH PD[LPXPDOORZHGWUDQVSRUWDWLRQWLPHIRUDJLYHQVHWRIGURSOHWV 'XULQJ WKLV WUDQVSRUWDWLRQ SURFHVV LQWHUVHFWLRQ EHWZHHQ PXOWLSOH URXWHV PD\ RFFXU ± FDXVLQJ D GURSOHW  DUULYLQJ DW WKH LQWHUVHFWLRQ ORFDWLRQDWDODWHUFORFNF\FOHFDQEHFRQWDPLQDWHGE\WKHUHVLGXHOHIW EHKLQG E\ DQRWKHU GURSOHW DOUHDG\ FURVVLQJ WKH VDLG ORFDWLRQ DW DQ HDUOLHU FORFN F\FOH 7KLV SKHQRPHQRQ LV WHUPHG DV FURVV FRQWDPLQDWLRQ DQG WKH ORFDWLRQV VKDUHG E\ PXOWLSOH GURSOHWV ZKLOH URXWLQJDUHGHILQHGDVFURVVFRQWDPLQDWLRQVLWHV&URVVFRQWDPLQDWLRQ LVXQGHVLUDEOHGXULQJGURSOHWURXWLQJVLQFHLWOHDGVWRHUURQHRXVDVVD\ RXWFRPHV ± DV ZHOO DV   SK\VLFDO GHIHFWV  IRU SURORQJHG RSHUDWLRQ )LJXUH  VKRZV WKH LQVWDQFH RI FURVV FRQWDPLQDWLRQ DW DQ\ JLYHQ WLPHVWDPSZLWKLQD'0)%

· )RUHDFKQHWLIVRXUFHDQGWDUJHWDUHQRWLQWKHVDPHURZRU FROXPQZHKDYHHVWLPDWHGWZRDOWHUQDWLYH0DQKDWWDQSDWKV ZLWK PLQLPXP GHYLDWLRQ LQ RUGHU WR LQWURGXFH LQWHQWLRQDO UHGXQGDQF\ LJQRULQJ DQ\ PRGXOHV FRPLQJ LQ WKH ZD\ RI WKHVHVURXWHVDVREVWDFOHV · 8VLQJDJUDSKEDVHGKHXULVWLFVZHLWHUDWLYHO\HOLPLQDWHGRQH DOWHUQDWHURXWHZKLOHVHOHFWLQJWKHPRVWVXLWDEOHRQHLQWHUPV RIFRQWDPLQDWLRQDYRLGDQFH · 1RZ ZH FRQVLGHU DOO WKRVH SDWKV WKDW FURVV WKURXJK WKH PRGXOHV DQG GHWHUPLQH WZR DOWHUQDWH SRVVLEOH GHWRXUV FRQVWLWXWLQJDJDLQWZRDOWHUQDWLYHV · $JDLQXVLQJDJUDSKEDVHGKHXULVWLFVZHDWWHPSWHGWRVHOHFW WKH VXLWDEOH GHYLDWLRQ WKDW DJDLQ PLQLPL]HV WKH DGGLWLRQDO FRQWDPLQDWLRQLIDQ\FDXVHGE\WKHVHGHYLDWLRQVDVZHOODV WKHQXPEHURIFHOOVFRYHUHG · )LQDOO\ ZH GHILQH IULFWLRQ ]RQHV IRU DOO WKRVH SDUDOOHO URXWH SDWKWKDWFRYHUVFHUWDLQQHLJKERXULQJFHOOV±WKHUHE\LQYLWLQJ SRVVLELOLW\RIVWDOOLQJ · 2QHVWLPDWLRQRIWKHWLPHVWDPSVDWWKHVHQHLJKERXULQJFHOOV IRU WKH FRUUHVSRQGLQJ URXWHV ± LI SRVVLEOH VWDOOLQJ LV SUHGLFWHGZHH[SORUHGIXUWKHUSRVVLEOHGHWRXUWKDWWUDGHVRII ZLWK SRVVLEOH VWDOOLQJ LQ WHUPV RI UHGXFWLRQ LQ RYHUDOO URXWH WLPHWKDWUHVXOWVLQHQKDQFHGURXWHSHUIRUPDQFH

97+(352326('0(7+2'
+HUHZHVWDUWZLWKDJLYHQOD\RXWFRPSULVHGRIDJLYHQQXPEHUQRI SLQ DQG SLQ QHWV7KH REMHFWLYH LV WR GHWHUPLQH Q QXPEHU RI VXLWDEOHURXWHSDWKVWKDWUHVXOWVLQ&QXPEHURIFRQWDPLQDWLRQVZKHUH PLQLPXPQXPEHURIFRQWDPLQDWLRQV &DORQJZLWKODWHVWDUULYDOWLPH 7EHLQJWKHPLQLPXPDVZHOODVWRWDOFHOOFRYHUHG(EHLQJRSWLPXP

$3URSRVHG$OJRULWKP
)LJXUHSLQDQGSLQQHWVLQD'*ULG$UUD\&ULWLFDO]RQHIRU 6LVWKH*UHHQVKDGHGUHJLRQ$KDUG%ORFNDJHLVDOVRLOOXVWUDWHG



 )LJXUH,QVWDQFHRIFRQWDPLQDWLRQZLWKLQD'0)%ZKLOHURXWLQJ

,9027,9$7,21$1'285&2175,%87,21
$V HYLGHQW IURP WKH HDUOLHU GLVFXVVLRQV FURVV FRQWDPLQDWLRQ DYRLGDQFH KDV WXUQHG RXW WR EH D PDMRU LVVXH ERWK LQ FRQWH[W RI UHOLDELOLW\RIUHVXOWVDVZHOODVRSHUDWLRQRIWKHGHYLFH)HZPHWKRGV KDV DOUHDG\ EHHQ SURSRVHG WR WDFNOH WKHVH LVVXHV DV PHQWLRQHG LQ VHFWLRQ ,, ,QLWLDOO\ WKH IRFXV ZDV DW WKH HVWLPDWHG URXWH WUDFNV ± E\ FRQVFLRXVO\DOORZLQJGHWRXURIGURSOHWVZKLOHURXWLQJWRDYRLGFURVV RYHU DQG WKHUHE\ UHGXFLQJ WKH QXPEHU RI FURVV FRQWDPLQDWLRQ ORFDWLRQV)RUSLQGURSOHWVGLIIHUHQWVRXUFHVRIWKHVDPHQHWPD\EH DOORZHGWRVKDUHWKHVDPHSDWKDVPL[LQJPD\QRWLQWHUIHUHZLWKWKH DFFXUDF\RIUHVXOWVLQVXFKFDVHV7KHVHW\SHVRIFURVVRYHURUVKDULQJ RI FHOOV DUH WHUPHG DV DOORZDEOH FRQWDPLQDWLRQ +HQFH LW KDV EHHQ IRXQG WKDW ZKLOH RSWLPL]HG URXWLQJ UHGXFHV WKH QXPEHU RI FRQWDPLQDWLRQ±DWWKHVDPHWLPHWKHWZRPDMRUREMHFWLYHVIRUURXWLQJ KDVEHHQFRPSURPLVHG±E\DOORZLQJDQLQFUHDVHLQODWHVWDUULYDOWLPH DV ZHOO DV FHOO XWLOL]DWLRQ LQ RUGHU WR DFFRPPRGDWH WKH LQWHQWLRQDO GHWRXU ,QWKLVZRUNZHDWWHPSWHGWRUHVROYHWKHVHLVVXHVVLPXOWDQHRXVO\WR RSWLPL]H WKH URXWH SHUIRUPDQFH LQ WHUPV RI RYHUDOO URXWH WLPH WRJHWKHU ZLWK UHGXFWLRQ LQ WKH QXPEHU RI FRQWDPLQDWLRQV 7KH VXPPDU\RIWKHZRUNLVGHSLFWHGEHORZ

,QSXW $0[0'JULGFRQWDLQLQJQQXPEHURIVDPSOHVFRPSULVLQJ RI[QXPEHURISLQDQG\QXPEHURISLQGURSOHWV 3URFHGXUH 6WHS±+HUHHDFKSLQQHWKDVEHHQGHFRPSRVHGLQWRWKUHHSRVVLEOH QHWV QDPHO\ IURP VRXUFH WR PL[HUVRXUFH WR PL[HU DQG PL[HU WR WDUJHW HDFK  SLQ QHW ZH DVVXPH RQH VLQJOH QHW LH IURP VRXUFH WR WDUJHW +HQFHIRUQQXPEHURIVDPSOHVSRVVLEOHQHWVN [\ 6WHS±RXWRIWKHVHNQXPEHURIQHWVOHWSQXPEHURIQHWVVKDUHWKH VDPHURZVRUFROXPQV±WKHUHE\KDYLQJRQO\RQHHVWLPDWHGURXWHSDWK )RU WKH NS QXPEHU RI QHWV ZH HVWLPDWH WZR DOWHUQDWH PDQKDWWDQ SDWKV LJQRULQJ DQ\ SDVVDJH WKURXJK WKH PRGXOHV WHUPHG DV KDUG EORFNDJHV +HQFHWRWDOQXPEHURIHVWPDWHGURXWHSDWKV5 SNS 6WHS±:HILUVWGUDZDGURSOHWLQWHUVHFWLRQJUDSK*' 9'(' ZKHUHY9' UHSUHVHQWVHDFKGURSOHWDQGHYLYM ('H[LVWVLIWKHUH DUHFURVVRYHUVEHWZHHQSDWKVRIGURSOHWYLDQGYMHDFKVXFKHGJHHLV DVVLJQHG DQ ZHLJKW ZLM UHSUHVHQWLQJ WRWDO QXPEHU RI SDWKV FROOLGLQJ EHWZHHQYLDQGYM 6WHS  ± 7KHQ ZH GUDZ D SDWK LQWHUVHFWLRQ JUDSK *3  93  (3  ZKHUHY93UHSUHVHQWVHDFKSDWKDQGHY[Y\(3H[LVWVLIWKHUHDUH FURVVRYHUVEHWZHHQSDWKVY[DQGY\(DFKHGJHHKDVEHHQDVVLJQHG DQZHLJKWZ[\UHSUHVHQWLQJQXPEHURIFHOOVFRQWDPLQDWHGGXULQJHDFK FURVRYHU7KHUHZLOOEH5QXPEHURIYHUWLFHVLQ*3WKDWUHSUHVHQWVWRWDO QXPEHURIHVWLPDWHGSDWKVLQFOXVLYHRILQWHQWLRQDOUHGXQGDQF\ ,IWKHSDWKUHSUHVHQWHGE\YLFURVVHVDPRGXOH0[DWWDFKYL WRDQHZ EORFNYHUWH[0[WKURXJKDGLIIHUHQWFRORUHGJHHPL[ 6WHS±IRUPDWDEOH7SDWKWRPDSHDFKYHUWH[Y[ZLWKLWVDOWHUQDWH Y[LIDQ\RWKHUZLVHNHHSWKHDOWHUQDWHFROXPQEODQN 6WHS±LQ*UDSK*3 DUUDQJHDOOWKHYHUWLFHVLQGHVFHQGLQJRUGHURI GHJUHHVDQGQXPEHUWKHPIURPWR5LQWKHVDPHRUGHU )RUL WR5 )RUYHUWH[YL93LIYLH[LVWVDQGLVQRWGHOHWHGDOUHDG\ ,IGHJYL!GHJDOWHUQDWHYL>LIDOWHUQDWHRIYLH[LVWVLQ7SDWK@ 'HOHWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQYL ,IGHJYL GHJDOWHUQDWHYL>LIDOWHUQDWHRIYLH[LVWVLQ7SDWK@ &RPSXWHZHLJKWYL : Z[\VXPPDWLRQRIZHLJKWVRI DOOHGJHVRILQFLGHQWRQYL

52

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

 &RPSXWHZHLJKWDOWHUQDWHYL : Z[\VXPPDWLRQRI ZHLJKWVRIDOOHGJHVRILQFLGHQWRQDOWHUQDWHYL ,I:!: 'HOHWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQYL (OVHLI:!: 'HOHWHDOWHUQDWHYLZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHV LQFLGHQWRQDOWHUQDWHYL (OVH>IRUWKHFDVH: :DQGGHJYL GHJDOWHUQDWHYL@ ,IHLWKHURIYLRUDOWHUQDWHYLKDVHGJHHP[ERXQGWRPRGXOH0[ 'HOHWHWKHFRUUHVSRQGLQJYHUWH[DQGZLWKDOOWKHHGJHV LQFLGHQWRQLWLHYLRUDOWHUQDWHYLZKLFKHYHUDSSOLFDEOH 1H[WL 6WHS  ±  1RZ GUDZ WKH OD\RXW ZLWK UHPDLQLQJ SDWKV DIWHU GHOHWLRQ 7KHUH PD\ VWLOO UHPDLQ VRPH UHGXQGDQW SDWKV IRU FHUWDLQ QHWV1RZ DVVXPH WRWDO 6 QXPEHU RI SDWKV DUH OHIW ZLWKLQ WKH QHZ OD\RXW DIWHU GHOHWLRQ 6WHS  ±  /HW WKHUH EH / QXPEHU RI SDWKV WKDW SDVVHV WKURXJK KDUG EORFNDJHV )RU HDFK VXFK SDWK GHWHUPLQH WZR SRVVLEOH GHYLDWLRQV WR E\SDVV WKH FRUUHVSRQGLQJ EORFNDJHV +HQFH WRWDO QXPEHU RI SDWKV DYDLODEOH5 6// 6WHS  ± $JDLQ GUDZ D SDWK LQWHUVHFWLRQ JUDSK IURP WKLV UHPDLQLQJ OD\RXWDQGUHSHDWIURP6WHSWR6WHSXQWLODOOSDWKVDUHDGGUHVVHG 6WHS±2XWRIWKHVH6QXPEHURIUHPDLQLQJSDWKVILQDOO\REWDLQHG± LGHQWLI\ WKRVH SDWKV WKDW IRUPV ]RQHV RI IULFWLRQ =RQH RI IULFWLRQ LV GHILQHG DV ]RQHV FRPSULVLQJ RI FHOOV WKDW FRQVWLWXWH SDUW RI WZR QHLJKERXULQJ URZV RU FROXPQV   ZKLFK KDV EHHQ XWLOL]HG E\ WZR FRUUHVSRQGLQJURXWHSDWKV 6WHS  ± LQ WKLV ILQDO SDWK LQWHUVHFWLRQ JUDSK IRUP D K\SHUSDWK FRQQHFWLQJDQ\WZRSDWKVYHUWLFHVWKDWIRUPHDFKIULFWLRQ]RQHV (DFKIULFWLRQ]RQHK\SHUSDWKFRQQFWVRQO\WZRYHUWLFHVY[DQGY\ )RUHDFKIULFWLRQ]RQHK\SHUSDWK ,IDOWHUQDWHY[H[LVWVDQGGRHVQRWEHORQJWRDQ\RWKHUK\SHUSDWK 'HOHWHY[ZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQY[ (OVHLIDOWHUQDWHY\H[LVWVDQGGRHVQRWEHORQJWRDQ\RWKHUK\SHUSDWK  'HOHWHY\ZLWKDOOWKHWKHFRUUHVSRQGLQJHGJHVLQFLGHQWRQY\ (OVHLIQRDOWHUQDWHYHUWH[H[LVWVIRUHLWKHUY[RUY\ 0DUNWKH]RQHDVLQHYLWDEOHIULFWLRQ]RQHDQGH[SORUHDSRVVLEOH GHWRXUIRUHLWKHUY[RUY\ZKLFKHYHUVXLWDEOHDQGFRYHUVPLQLPXP QXPEHURIDGGLWLRQDOFHOOVZLWKQRFRQWDPLQDWLRQV 6WHS  ±  LQ WKH ILQDO SDWK LQWHUVHFWLRQ JUDSK OHW WKH QXPEHU RI UHPDLQLQJ YHUWLFHV EH 93 DQG HGJHV (3 7KHVH HGJHV UHSUHVHQW QXPEHU RI XQDYRLGDEOH FURVVRYHUV DQG VXPZHLJKWV RI HGJHV :3 UHSUHVHQWV WKH WRWDO QXPEHU RI XQDYRLGDEOH FROOLVLRQVUHDUUDQJH DOO YHUWLFHVDJDLQLQGHVFHQGLQJRUGHURIWKHLUGHJUHHV 6WHS± )RUL WR93 ,IIRUYLWKHUHH[LVWVDQDOWHUQDWHYL 'HOHWHDQ\RQHRIWKHVHWZRZLWKDOOWKHLQFLGHQWHGJHVLIDQ\ 1H[WL 6WHS±(VWLPDWHWKHWRWDOQXPEHURIFURVVRYHUVDQGIULFWLRQ]RQHV OHIWRYHUZLWKLQWKHILQDOSDWKLQWHUVHFWLRQJUDSK'UDZWKHILQDOOD\RXW DQGIRUPWKHILQDOGURSOHWLQWHUVHFWLRQJUDSKDVGUDZQLQVWHS 6WHS  ± (VWLPDWH WKH 0DQKDWWDQ GLVWDQFH 'L IRU HDFK QHW During concurrent routing of several droplets, there still remains a possibility of collision or overlap of paths between multiple droplets. In such a case, priorities are given in favour of the droplets with larger Di. For 2-pin nets, each droplet is routed directly from its source to its target. For 3-pin (multi-pin) nets, routing from each of the sources Sx and Sy to the mixer (M) is done concurrently. This part of routing is referred to as the First Generation route. The largest arrival time T between (SX ÆM) and (SY ÆM) is noted. The final mixed droplet from M is next routed to Target T. This is called the Second Generation route. nd For 2 generation route, the timestamp starts from TSM as determined earlier. - Start routing of each net starting with timestamp = 0 for Step 16 -each net. It is also assumed that transition time for a droplet between two adjacent cells to be of One unit.Compute the arrival time for each droplet -- mark the maximum of all arrival times as the latest arrival - it is possible to conclude that route is time T. If T = max (Di) -optimized- because max (Di) represents the critical path among all the
SM

nets. Finally count the total number of cells utilized -- E and the number of cells sharing multiple droplets representing total number of contaminations C. Output: The final layout representing optimized route paths .the overall route time, the latest arrival time and the average arrival time (overall route time/number of droplets) ,the total number of contaminations remaining and number of cells utilized.

9,$1,//8675$7,9((;$03/(
7KH LPSOHPHQWDWLRQ RI WKH DOJRULWKP LV GHPRQVWUDWHG XVLQJ DQ LOOXVWUDWLYHH[DPSOHDVVKRZQLQILJXUH,QILJXUHDDJLYHQOD\RXW KDV EHHQ GLVSOD\HG 7KH OD\RXW LV FRPSULVHG RI  7ZR SLQ DQG 2QH 7KUHHSLQQHWZLWK7ZRKDUGEORFNDJHVQDPHO\0DQG07KHSDWKV IRU HDFKGURSOHW ZLWK FRUUHVSRQGLQJ DOWHUQDWLYHV DUHGLVSOD\HG DV3QL ZKHUHQ DEFGRUHGURSOHWQDPHDQGLLQGLFDWHVWKHFRUUHVSRQGLQJ SDWK QXPEHU IRU WKH VDLG GURSOHW)RU 7KUHH SLQ GURSOHW SDWK QDPH LV LQGLFDWHG E\ 3Q[L DQG 3Q\L IRU VRXUFHV Q[ DQG Q\ WR WKH PL[HU UHVSHFWLYHO\7KH SDWK 3QWL UHSUHVHQWV WKH SDWK IURP PL[HU WR WKH UHVSHFWLYH WDUJHW)LJXUH  E GLVSOD\V WKH GURSOHW LQWHUVHFWLRQ JUDSK WRJHWKHU ZLWK OLQNDJHV WR WKH KDUG EORFNDJHV RU PRGXOHV LQGLFDWLQJ SDWKVIRUWKHVDLGQHWSDVVHVWKURXJKWKHPRGXOH7KHZHLJKWDJHWRWKH HGJHVUHSUHVHQWVWKHQXPEHURISDWKVIRUWKHQHWVDUHLQYROYHGLQWKH FURVV RYHU)LJXUH  F VKRZV WKH LQLWLDO SDWK LQWHUVHFWLRQ JUDSK 7KH YHUWLFHV ZLWK VFDU PDUNV DUH PDUNHG IRUGHOHWLRQ ZLWK FRUUHVSRQGLQJ LQFLGHQWHGJHVVKRZQE\SDWWHUQHGJUHHQOLQH7KHJUH\FRORUHGHGJH UHSUHVHQWV DOORZDEOH FURVVRYHU RU FRQWDPLQDWLRQ DV HYLGHQW IRU WKH VDPHPHPEHUVRIDWKUHHSLQQHW7KHUHGFRORUHGHGJHVFRQQHFWLQJWKH SDWKV ZLWK PRGXOHV UHSUHVHQW SDWKV WKDW FURVVHV RYHU WKH VDLG PRGXOHV)LJXUH  G VKRZV WKH ILQDO SDWK LQWHUVHFWLRQ JUDSK DIWHU LWHUDWLYHVHOHFWLRQRIVXLWDEOHSDWKV)LJXUHHUHSUHVHQWVWKHUHVXOWLQJ OD\RXW ZLWK FXUUHQWO\ VHOHFWHG SDWKV DIWHU RPLVVLRQ RI WKH UHGXQGDQW RQHV)LJXUH  I UHSUHVHQWV WKH DGGLWLRQDO SDWKV LQWURGXFHG DIWHU GHYLDWLRQ WR E\SDVV WKH KDUG EORFNDJHV ,Q ILJXUH  J WKH SDWK LQWHUVHFWLRQ JUDSK LV UHGUDZQ ± ZKHUH WKH IRXU IULFWLRQ ]RQHV DUH LQGLFDWHGE\K\SHUSDWKK\SHUSDWKK\SHUSDWKDQGK\SHUSDWK 7KHSDWKZLWKDYDLODEOHDOWHUQDWLYHLVRPLWWHGWRHOOLPLQDWHWKHIULFWLRQ ]RQHV DYRLGLQJ SRVVLEOH VWDOOLQJ 0RUHRYHU RWKHU SDWKV KDYH EHHQ SUHIHUDEO\VHOHFWHGXVLQJWKHWHFKQLTXHPHQWLRQHGLQ6WHSLQVHFWLRQ 97KHILQDOOD\RXWZLWKURXWHSDWKVDQGFRUUHVSRQGLQJURXWHUHVXOWVDUH VKRZQLQILJXUHK+RZHYHUWKHUHPDLQLQJIULFWLRQ]RQHVIRUPHGLQ WKH OD\RXW DUH IRXQG WR EH IUHH RI VWDOOLQJ ± DV GURSOHWV FOHDUV RI ZLWKRXWDQ\YLRODWLRQRIIOXLGLFFRQVWUDLQWV7KHODWHVWDUULYDOWLPHLV IRXQGWREHDYHUDJHDUULYDOWLPHLVFHOOXWLOL]DWLRQEHLQJ WKH QXPEHU RI XQDYRLGDEOH FRQWDPLQDWLRQV DUH OLPLWHG WR LQGLFDWHG E\ SLQN FHOOV ZLWK DOORZDEOH FRQWDPLQDWLRQ EHLQJ LQGLFDWHGE\JUHHQFHOO

 )LJXUHDLQLWLDOOD\RXWZLWKWZRSLQDQG7KUHHSLQQHW±DQGWZR PRGXOHV0DQG0$OOHVWLPDWHGURXWHSDWKVZLWKUHGXQGDQWDOWHUQD WLYHVDUHVKRZQ

 )LJXUH  E WKH GURSOHW LQWHUVHFWLRQ JUDSK ZLWK PRGXOH OLQNDJHV E\ SDWKVDUHVKRZQ

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

53

 )LJXUHK7KHILQDOOD\RXWZLWKURXWHSDWKVDQGFRUUHVSRQGLQJURXWH UHVXOWV  )LJXUHF,QLWLDOSDWKLQWHUVHFWLRQJUDSKLQGLFDWLQJUHGXQGDQWSDWKV DQGFURVVRYHUGHWHUPLQHGIRURPLVVLRQ

 )LJXUHL)LQDOGURSOHWLQWHUVHFWLRQJUDSKVKRZLQJ)RXUHGJHVZLWK 7KUHHXQDYRLGDEOHDQG2QHDOORZDEOHFRQWDPLQDWLRQ

9,,(;3(5,0(17$/5(68/76
 )LJXUH  G )LQDO SDWK LQWHUVHFWLRQ JUDSK DIWHU LWHUDWLYH VHOHFWLRQ RI WKHVXLWDEOHSDWKV 7KHSURSRVHGWHFKQLTXHIRUH[SORUDWLRQRIVXLWDEOHURXWHSDWKZLWKDQ REMHFWLYH RI PLQLPL]LQJ QXPEHU RI FRQWDPLQDWLRQV DV ZHOO DV HQKDQFLQJ WKH URXWLQJ SHUIRUPDQFH KDV EHHQ DSSOLHG RQ 7HVWEHQFKHV RI%HQFKPDUNVXLWH,DQG,,,7KHGHWDLOHGURXWLQJUHVXOWERWKZLWKDQG ZLWKRXWFURVVFRQWDPLQDWLRQIRUVXESUREOHPVRIWHVWEHQFK,QBYLWUR  LQ EHQFKPDUN VXLWH ,,, KDV EHHQ GLVSOD\HG LQ 7DEOH  7DEOH  GLVSOD\V VXPPDUL]HG UHVXOWV IRU WHVWEHQFKHV ,QB9LWUR ,QB9LWUR SURWHLQDQGSURWHLQRI%HQFKPDUNVXLWH,,,7KHUHVXOWVIRUIHZ WHVWEHQFKHVRIEHQFKPDUNVXLWH,KDYHEHHQJLYHQLQ7DEOH)LQDOO\ LQ 7DEOH  IHZ FRPSDUDWLYH UHVXOWV ZLWK SHUIRUPDQFHV RI FRQWHPSRUDU\ PHWKRGV LV GLVSOD\HG WR HVWDEOLVK WKH HIIHFWLYHQHVV RI WKHSURSRVHGWHFKQLTXH ,W KDV EHHQ IRXQG WKDW LQ PRVW FDVHV IRU EHQFKPDUN VXLWH ,,, FHOO XWLOL]DWLRQ KDV EHHQ LQFUHDVHG EXW 0D[LPXP DUULYDO WLPH DV ZHOO DV DYHUDJH DUULYDO WLPH LQFUHDVHG LQ VRPH FDVHV RQO\ PDUJLQDOO\ 7KLV LPSOLHV WKDW DV PRUH GHWRXUV DUH QHFHVVDU\ WR DYRLG FURVV FRQWDPLQDWLRQ WKDW FRQWULEXWHV WRZDUGV LQFUHDVH LQ FHOO XWLOL]DWLRQ RQ WKHRWKHUKDQG5RXWHWLPHKDVEHHQNHSWWREHRSWLPXPWKDWUHVXOWVLQ RQO\PDUJLQDOLQFUHDVHLQFHUWDLQFDVHVRIWKHDYHUDJHDUULYDOWLPHZLWK DOPRVWQRYDULDWLRQLQPD[LPXPDUULYDOWLPH7KHFRQWDPLQDWLRQVDUH IRXQGWREHUHGXFHGFRQVLGHUDEO\LQDOPRVWDOOWKHFDVHV +RZHYHUIRUWHVWEHQFKHVLQEHQFKPDUNVXLWH,DVQXPEHURIGURSOHWV DUH PRUH LQ FRPSDULVRQ WR JULG VL]H ± OLWWOH VSDFH DUH DYDLODEOH WR SURYLGHDOWHUQDWHSDWKRUQHFHVVDU\GHWRXUWRDYRLGFRQWDPLQDWLRQ6WLOO WKH SURSRVHG PHWKRG KDV DWWHPSWHG WR UHGXFH QXPEHU RI FRQWDPLQDWLRQZLWKRSWLPL]HGURXWHSHUIRUPDQFH 7DEOHGHWDLOHGURXWHSHUIRUPDQFHIRUWHVWEHQFK,Q9LWUR

 )LJXUHHWKHUHVXOWLQJOD\RXWZLWKFXUUHQWO\VHOHFWHGSDWKVDIWHU RPLVVLRQRIWKHUHGXQGDQWRQHV

 )LJXUHIOD\RXWZLWKWKHDGGLWLRQDOSDWKVEHLQJLQWURGXFHGDIWHU GHYLDWLRQWRE\SDVVWKHKDUGEORFNDJHVLQGLFDWLQJWKHIULFWLRQ]RQHV


)LJXUHJWKHQHZSDWKLQWHUVHFWLRQJUDSK±ZKHUHWKHIRXUIULFWLRQ ]RQHVDUHLQGLFDWHGE\K\SHUSDWKK\SHUSDWKK\SHUSDWKDQG K\SHUSDWK 

54

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

>@$ 5HQDXGLQ 3 7DERXULHU  9 =KDQJ & 'UXKRQ DQG -& &DPDUW 3ODWHIRUPH 6$: GpGLpH j OD PLFURIOXLGLTXH GLVFUqWH SRXU DSSOLFDWLRQV ,QWKLVZRUNZHSURSRVHGDVSHFLILFWHFKQLTXHWKDWSHUIRUPVLQWHOOLJHQW ELRORJLTXHV qPH &RQJUqV )UDQoDLV GH 0LFURIOXLGLTXH6RFLpWp H[SORUDWLRQRI FURVV FRQWDPLQDWLRQ DZDUH YLUWXDOURXWH SDWKV WKDWQRW +\GURWHFKQLTXHGH)UDQFH7RXORXVH)UDQFH >@ 6. &KR + 0RRQ DQG &- .LP ³&UHDWLQJ WUDQVSRUWLQJ FXWWLQJ DQG RQO\ PLQLPL]HV WKH RYHUDOO QXPEHU RI FRQWDPLQDWLRQV EXW DOVR WKH PHUJLQJ OLTXLG GURSOHWV E\ HOHFWURZHWWLQJEDVHG DFWXDWLRQ IRU GLJLWDO PLFUR RYHUDOOURXWHWLPHWRJHWKHUZLWKODWHVWDUULYDOWLPHIRUDOOWKHGURSOHWV IOXLGLF FLUFXLWV´ -RXUQDO RI 0LFURHOHFWURPHFKDQLFDO 6\VWHPV YRO   LQYROYHGLQWKH H[HFXWLRQRI WKHVSHFLILHG SURWRFRO,WKDV EHHQ IRXQG SS VR IDU WKDW RQ LPSOHPHQWDWLRQ RI WKH SURSRVHG DOJRULWKP ± DQ >@$OERU]$U]SH\PD6KDQ%KDVHHQ$OL'RODWDEDGL3DXOD:RRG$GDPV HQFRXUDJLQJ LPSURYHPHQW KDV EHHQ DFKLHYHG LQ FRPSDULVRQ WR $FRXSOHGHOHFWURK\GURG\QDPLFQXPHULFDOPRGHOLQJRIGURSOHWDFWXDWLRQE\ FRQWHPSRUDU\ PHWKRGV ,Q WKLV SDSHU ZH KDYH HPSOR\HG  D JUDSK HOHFWURZHWWLQJ &ROORLGV DQG 6XUIDFHV $ 3K\VLFRFKHP (QJ $VSHFWV YRO EDVHG KHXULVWLF WR LGHQWLI\ VXLWDEOH DOWHUQDWLYHV IRU LQWHOOLJHQW SS DYRLGDQFH RI FURVV FRQWDPLQDWLRQV ZKLOH PDLQWDLQLQJ RSWLPXP URXWH >@ <+ &KDQJ *% /HH )& +XDQJ << &KHQ -/ /LQ ,QWHJUDWHG SRO\PHUDVH FKDLQ UHDFWLRQ FKLSV XWLOL]LQJ GLJLWDO PLFURIOXLGLFV %LRPHG SHUIRUPDQFH LQ GLJLWDO PLFURIOXLGLF %LRFKLSV %XW WKHUH VWLOO UHPDLQ 0LFURGHYLFHVYROSS VFRSHV IRU IXUWKHU LPSURYHPHQW XVLQJ ILQHU DGMXVWPHQW RI WKH QHWV >@96ULQLYDVDQ9.3DPXOD5%)DLU$QLQWHJUDWHGGLJLWDOPLFURIOXLGLF XVLQJPHWLFXORXVSODFHPHQWDQGFRPSDFWLRQWHFKQLTXHV ODERQDFKLS IRU FOLQLFDO GLDJQRVWLFV RQ KXPDQ SK\VLRORJLFDO IOXLGV/DE 7DEOH&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWVIRUEHQFKPDUN &KLSYROSS VXLWH,,, >@+0RRQ$5:KHHOHU5/*DUUHOO-$/RR&-.LP$QLQWHJUDWHG GLJLWDOPLFURIOXLGLFFKLSIRUPXOWLSOH[HGSURWHRPLFVDPSOHSUHSDUDWLRQ DQGDQDO\VLVE\0$/'O06/DE&KLSYROSS >@0$EGHOJDZDG$DURQ5:KHHOHU/RZFRVWUDSLGSURWRW\SLQJRI GLJLWDOPLFURIOXLGLFVGHYLFHV0LFURIOXLG1DQRIOXLGYROSS  >@ - %HUWKLHU 0LFURGURSHV DQG 'LJLWDO 0LFURIOXLGLFV :LOOLDP $QGUHZ ,QF1RUZLFK1< >@)6X-XQ=HQJ³&RPSXWHUDLGHGGHVLJQDQGWHVWIRUGLJLWDOPLFURIOXLGLFV  ´,((('HVLJQDQGWHVWIRUFRPSXWHUVSS)HE >@7< +R - =HQJ DQG . &KDNUDEDUW\ ³'LJLWDO PLFURXLGLF ELRFKLSV $ 7DEOH&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWVIRUVHOHFWHGWHVW YLVLRQIRUIXQFWLRQDOGLYHUVLW\DQGPRUHWKDQ0RRUH3URF$&0,((( EHQFKHVLQEHQFKPDUNVXLWH, ,&&$'SS^ >@7VXQJ:HL+XDQJ&KXQ+VLHQ/LQDQG7VXQJ<L+R³$FRQWDPLQDWLRQ DZDUH GURSOHW URXWLQJ DOJRULWKP IRU GLJLWDO PLFURIOXLGLF ELRFKLSV ´ ,&&$'¶ 1RYHPEHU±6DQ-RVH&DOLIRUQLD86$ >@$NHOOD6*ULIILWK(-DQG*ROGEHUJ0.³3HUIRUPDQFHFKDUDFWHU L]DWLRQ RI D UHFRQILJXUDEOH SODQDUDUUD\ GLJLWDO PLFURIOXLGLF V\VWHP´ ,((( 7UDQVDFWLRQV &RPSXWHU$LGHG 'HVLJQ RI ,QWHJUDWHG &LUFXLWV DQG 6\VWHPV SS)HE >@3<XK66DSDWQHNDU&<DQJDQG<&KDQJ³$SURJUHVVLYHLOSEDVHG URXWLQJ DOJRULWKP IRU FURVV UHIHUHQFLQJ ELRFKLSV´ 'HVLJQ $XWRPDWLRQ &RQIHUHQFHSS± >@   + :LOOLDP 6X )HL DQG &KDNUDEDUW\ . ³'URSOHW URXWLQJ LQ WKH V\QWKHVLVRIGLJLWDOPLFURIOXLGLFELRFKLSV´ ,Q3URFRI'HVLJQ$XWRPDWLRQ  7HVWLQ(XURSH >@&KR0LQVLNDQG3DQ'DYLG=³$KLJKSHUIRUPDQFHGURSOHWURXWLQJ DOJRULWKPIRUGLJLWDOPLFURIOXLGLFELRFKLSV´,(((7UDQVDFWLRQV&RPSXWHU $LGHG'HVLJQRI,QWHJUDWHG&LUFXLWVDQG6\VWHPVSS >@7;XDQG.&KDNUDEDUW\³'URSOHWWUDFHEDVHGDUUD\SDUWLWLRQLQJDQGD  SLQ DVVLJQPHQW DOJRULWKP IRU WKH DXWRPDWHG GHVLJQ RI GLJLWDO PLFURIOXLGLF 7DEOHFRPSDULVRQRI&URVVFRQWDPLQDWLRQDZDUHURXWHUHVXOWV ELRFKLSV´,Q&2'(6,666SS±2FW >@7VXQJ:HL+XDQJ7VXQJ<L+R³$)DVW5RXWDELOLW\DQG3HUIRUPDQFH ZLWKFRQWHPSRUDU\PHWKRGVIRU%HQFKPDUNVXLWH,,, GULYHQGURSOHWURXWLQJDOJRULWKPIRUGLJLWDOPLFURIOXLGLFELRFKLSV´ &RPSXWHU 'HVLJQ,&&',(((,QWHUQDWLRQDO&RQIHUHQFH2FWSS /DNH7DKRH&$ >@ 6RXNXS - ³)DVW 0D]H 5RXWHU´ 3URFHHGLQJV RI WKH WK$&0 'HVLJQ $XWRPDWLRQ&RQIHUHQFHSS >@ 3 5R\ + 5DKDPDQ 3 'DVJXSWD ³$ 1RYHO 'URSOHW 5RXWLQJ $OJRULWKP IRU'LJLWDO0LFURIOOXLGLF%LRFKLSV´3URFRIWKH*/69/6,3URYLGDQFH 5KRGH,VODQG86$ >@=LJDQJ;LDR(YDQJHOLQH)<<RXQJ³'URSOHW5RXWLQJ$ZDUH0RGXOH 3ODFHPHQW IRU &URV5HIHUHQFLQJ %LRFKLSV´,63' 6DQ )UDQVLVFR &DOLIRUQLD86$WK0DUFK >@ <DQJ =KDR DQG .ULVKQHQGX &KDNUDEDUW\³6\QFKURQL]DWLRQ RI :DVKLQJ  2SHUDWLRQV ZLWK 'URSOHW 5RXWLQJ IRU &URVV&RQWDPLQDWLRQ $YRLGDQFH LQ 'LJLWDO0LFURIOXLGLF%LRFKLSV´3URF2I'$&
-XQH$QDKHLP 5()(5(1&(6 &DOLIRUQLD86$ >@ 0* 3ROODFN 5% )DLU DQG $' 6KHQGHURY ³(OHFWURZHWWLQJEDVHG >@ 7: +XDQJ DQG 7< +R ³$ 7ZR6WDJH ,/3%DVHG 'URSOHW 5RXWLQJ DFWXDWLRQ RI OLTXLG GURSOHWV IRU  PLFURIOXLGLF DSSOLFDWLRQV´ $SSO3K\V/HWW  $OJRULWKP IRU 3LQ&RQVWUDLQHG 'LJLWDO 0LFURXLGLF %LRFKLSV 3UR RI $&0  ,63'0DU >@-/HH+0RRQ-)RZOHU&-.LPDQG76FKRHOOKDPPHU³$GGUHVVDEOH >@<DQJ&/<XK3+DQG&KDQJ<:³%LRURXWH$QHWZRUNIORZEDVHG PLFUR OLTXLG KDQGOLQJ E\ HOHFWULF FRQWURO RI VXUIDFH WHQVLRQ´ 3URF RI  URXWLQJ DOJRULWKP IRU GLJLWDO PLFUR IOXLGLF ELRFKLSV´ ,Q 3URFHHGLQJV RI ,((( WK ,QWHUQDWLRQDO &RQIHUHQFH RQ 0(06 ,QWHUODNHQ6ZLW]HUODQG  ,((($&0 ,QWHUQDWLRQDO&RQIHUHQFH RQ &RPSXWHU$LGHG 'HVLJQ SDJHV    >@5&*DVFR\QH DQG -9 9\NRXNDO³'LHOHFWURSKRUHVLV EDVHG VDPSOH >@ 3 5R\ + 5DKDPDQ  5 %KDWWDFKDU\D 36 'DVJXSWD ³$ %HVW 3DWK KDQGOLQJ LQ JHQHUDOSXUSRVH SURJUDPPDEOH GLDJQRVWLF ,QVWUXPHQWV´ 3URF 6HOHFWLRQ %DVHG 3DUDOOHO 5RXWHU )RU '0)%V´3URF RI ,6(' ,((( ,((( ,QWHUQDWLRQDO&RQIHUHQFHSS'HFHPEHU.RFKL,QGLD >@'$$QWRQ-39DOHQWLQR607URMDQDQG6:DJQHU³7KHUPRFDSLOODU\ >@<=KDRDQG.&KDNUDEDUW\³&URVVFRQWDPLQDWLRQDYRLGDQFHIRUGURSOHW DFWXDWLRQ RI GURSOHWV RQ FKHPLFDOO\ SDWWHUQHG VXUIDFHV E\ SURJUDPPDEOH URXWLQJLQGLJLWDOPLFURIOXLGLFELRFKLSV´,((($&0'$7($SU PLFURKHDWHUDUUD\V´-0LFURHOHFWURPHFKDQLFDO6\V

9,,,&21&/86,21

2013 8th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)

55

View publication stats

Digital microfluidic biochip (DFMB) systems have been developed as a promising platform for Lab-on-chip systems that manipulate individual droplet of chemicals on a 2D planar array of electrodes. Because of the safety critical nature of the applications these devices are intended for high reliability and thereby dependability becomes a major issue for the design of DMFBs. Therefore, such devices are required to be tested frequently both off-line (e.g., post-manufacturing) and prior to each assay execution. Under both scenarios, testing is accomplished by routing one or more test droplets across the chip and recording their arrival at the scheduled destination. In this paper, we have proposed a new design of a droplet motion detector based on capacitive sensing, which can be manufactured with the cell electrodes for detection of the presence (arrival) of a droplet at a predetermined location. Using this sensor, we have further proposed a customized testing technique for a specified layout with an objective of 1) optimizing the total number of test droplets for testing a particular bioassay, 2) optimizing the number of dispensers, 3) minimizing the overall test completion time, 4) detection of a specific segment at fault within the given layout, and 5) optimizing the number of locations where the detectors are to be activated. The test simulation has been carried out on two testbenches of Benchmark suite III and the results are found to be encouraging compared to the existing methods.A second generation lab-on-chip device termed as digital microfluidic biochip is developed as a time-multiplexed reconfigurable device capable of executing multiple bioassay protocols simultaneously on a single 2D planar array. They combine biology with electronics and integrate various bioassay operations namely sample preparation, analysis, separation and detection. Optical detection, processing and analysis are considered to be of major significance as these may influence the decisions on diagnosis, detection and testing. In this paper we initially propose two separate circuit designs of digital detection analyzer to be coupled with a biochip for execution of prescheduled bioassay protocols. The two types of analyzers enable automated detailed analysis of the optical detection results based on the data acquired at the detection site through successful Biochip operation for a) homogeneous droplet samples and b) heterogeneous droplet samples. A third circuit that integrates the previous two designs for dual mode operation is also proposed. Synthesis and simulation of all the three proposed designs are carried out using pre-characterized reference data for measurement of different parameters of human blood samples and the corresponding final detection results are displayed and verified successfully.The need for training employees in new skills in an organization generally arises due to the changing skill requirements coming from the introduction of new products, technology and customers. Efficient assignment of employees to trainings so that the overall training cost is minimized while considering the career goals of employees is a challenging problem and to the best of our knowledge there is no existing work in literature that solves this problem. This paper presents TRaining AssigNment Service (TRANS) that minimizes an organizationâ€™s overall training costs while assigning employees to trainings that match their learning ability and career goals. TRANS uses an ORGanization and Skills ontology (ORGS) to calculate the cost for training each available employee for a potential role taking into account constructivist learning theory. TRANS uses TRaining assIgnMent algorithm (TRIM), based on Hungarian method for bipartite matching, for assigning employees to trainings. In our experiments with real-world data, proposed allocation algorithm performs better than the existing strategy of the organization.A Rule-Based Approach for Minimizing Power Dissipation of Digital Circuits
Subrata Das , Parthasarathi Dasgupta , Petr Fiser , Sudip Ghosh§ and Debesh Kumar Das
of Computer Science & Engineering, Jadavpur University, Kolkata, India Email: dsubrata.mt@gmail.com, debeshd@hotmail.com  MIS Group, Indian Institute of management Calcutta, Kolkata, India Email: partha@iimcal.ac.in  Faculty of Information Technology, Czech Technical University in Prague, Czech Republic Email:fiserp@fit.cvut.cz § School of VLSI Technology, IIEST, Shibpur, Howrah, India Email:sudipghosh2005@gmail.com
Abstract--Minimization of power dissipation of VLSI circuits is one of the major concerns of recent digital circuit design primarily due to the ever decreasing feature sizes of circuits, higher clock frequencies and larger die sizes. The primary contributors to power dissipation in digital circuits include leakage power, short-circuit power and switching power. Of these, power dissipation due to the circuit switching activity constitutes the major component. As such, an effective mechanism to minimize the power loss in such cases often involves the minimization of the switching activity. In this paper, we propose an intelligent rule-based algorithm for reducing the switching activity of the digital circuits at logic optimization stage. The proposed algorithm is empirically tested for several standard digital circuits with Synopsys EDA tool and the results obtained are quite encouraging. Index Terms--Switching activity, low-power VLSI circuits, CMOS, power dissipation, dynamic power, logic optimization.
 Department

I. I NTRODUCTION Traditionally, the major concerns of VLSI designers include minimization of the chip area, enhancement of performance, testability, reduction of manufacturing cost and the improvement of reliability. With increasing use of portable devices and wireless communication systems, the reduction of energy consumption and hence the reduction of power dissipation and optimization of chip temperature are current issues in recent VLSI design [1]. Power dissipated by a digital system increases the temperature of the chip and affects battery life of the digital devices [2]. Aggressive device scaling also causes excessive increase in power per unit area of the chip. As such, heat generation and its removal from a chip is a matter of serious concern [3]. In   circuits the three primary sources of power dissipation are [4] 1) The switching activity occurs due to logic transitions. When the nodes of a digital circuit make transition back and forth between two logic levels, parasitic capacitances are charged and discharged. Consequently current flows through the channel resistance of the transistors, and electrical energy is converted into heat [4].

2) The short-circuit current that flows from supply to ground when both the -subnetwork and -subnetwork of a   gate conduct [4]. 3) The leakage current [4] caused by substrate injection at - junctions and sub-threshold effects determined by the fabrication technology. The first two sources of power dissipation are known as dynamic power dissipation and the third one constitutes the static power dissipation. In the present-day technology about 80% of the total power loss occurs due to the switching activity [4]. Thus, minimization of the power dissipation of VLSI circuits necessitates minimization of the dynamic power and hence minimization of the switching activity. Minimization of switching activity can be done at the logic optimization stage. However, the focus of earlier works in logic optimization is primarily on reduction of the number of appearances of literals, minimum number of literals in a Sum of Products (SOP) or Product of Sums (POS) expression and minimum number of terms in a SOP expression [5]. In this paper, we propose an algorithm to obtain for a given input logic expression, an equivalent logic expression with minimized switching activity. II. L ITERATURE R EVIEW Minimization of power consumption of CMOS digital circuits is studied in the past, considering all levels of the design such as physical, circuit and logic level [6], [7]. In digital CMOS circuits the measure of power dissipation is the circuit switching activity or the average number of transitions. Minimization of the average number of transitions of CMOS digital circuits nodes is discussed in [8]. The work in [9] provides an interesting repository of recent techniques of power modeling and low-power design based on high-level synthesis. The evaluation and the reduction of the switching activity in combinational logic circuits considering both the transitions 1  0 and 0  1 at any output node is proposed in [10]. In order to satisfy the classical probabilistic approach that limits the maximum value of the switching activity to 1, the definition of the switching activity as proposed in [10]

was customized in [11]. An algorithmic approach at the gate level using Karnaugh maps for reducing the switching activity in combinational logic circuits is presented in [12]. However, the use of Karnaugh maps restricts the number of variables to around 6. Moreover, for the method proposed in [12], the switching activity can be minimized only for some specific switching functions. In [13] the authors proposed a method to estimate the switching activity using a variable delay model. The work of [14] has discussed the system level dynamic power management in chip multiprocessor (CMP) architectures. [15] proposed an algorithm to minimize logic functions with reduced area and interconnects that will improve the circuit performance. Pre-computation-based optimization for low power that computes the output logic values of the circuit in one clock cycle before they are computed, was discussed in [16]. III. P RELIMINARIES A definition of the switching activity based on the classical probabilistic approach is given in [11]. For a logic expression of a switching function for an output node , let    and    represent the number of 1's and the number of 0's. The probabilities of occurrence of a 0 and a 1 respectively at the output node  are given by the following equations: 0 = 1 =       +          +    (1) (2)

switching activity of the basic gates. For an  , ,    and   gate with  inputs, the output is 0 or 1 for exactly one input combination (input vector). Hence the value of    or    is 1 or 2 - 1. Hence 0 = 21  or 2 -1 2 -1 1 , respectively, and the corresponding  = or 1 2 2 2 . 2 -1 Thus, the switching activity is given by 22 . Hence, as the number of inputs to the above mentioned logic gates increases, the switching activity of these gates decreases. It is clear to see that the switching activity for the   gate is maximum and of value 1 4 . The switching activity for  and   gates is independent of the number of inputs to the gate and is equal to 1 4. B. Calculation of Switching Activity for a Logic Expression Without loss of generality, we assume the logic gates to have at most two inputs. Computation of the switching activity for a logic expression is illustrated by an example. Consider a logic expression  (, , , ) =  +  +  + . Consider Figure 1 and the following logic expressions, 1 = , 2 = , 3 = ., 4 =  + , 5 = , 6 =  +  = 3 + 5 , 7 .  = 6 +  +  = 6 + 4 . The switching activity for 5 is 64 This is due to the fact that the output is 1 only for the input vector 101. As such, the number of 1's and 0's in the output column are 1 and 7 respectively. For the implementation of , a   gate is required for 2 = , having  = 1 4, 3 . The an   gate is required for 1 =  with  = 16 outputs of these   gate and   gate are inputs to a 7 second   gate having   = 64 . 6 can be represented in sum of minterms as 0, 1, 2, 3, 4, 5. Hence the switching (since   = 6,   = 2). The total activity for 6 is 12 64 3 7 12 switching activity for 1 , 2 , 5 and 6 is thus 1 4 + 16 + 64 + 64 . Clearly, the switching activities for 3 =  and 4 =  +  3 each. When represented as a sum of minterms, the are 16 function  is given by  = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 and the switching 39 . (since   = 13,   = 3). Thus, the activity is 256 3 3 3 total switching activity of the circuit is 1 4 + 16 + 16 + 16 + 7 12 39 323 64 + 64 + 256 = 256 (Figure 1). Signal probability can also
a b c
3/16

In the computation of the switching activity, it is assumed that the distribution of 0's and 1's at the primary inputs (PIs) is uniform. Definition 1. For a given node of a circuit the probability of transition either from 0 to 1 or from 1 to 0 is known as switching activity of that node. Thus, the switching activity of node  is given by the composite probability  = 0 × 1 =    ×   (   +   )
2

(3)

f1
1/4

As already mentioned, power dissipation in digital circuits can be reduced by minimizing their total switching activity (TSA). It can be easily shown that the switching activity is maximum when the number of 1's (  ) and the number of 0's (  ) in the output column of the truth table are equal and the switching activity is minimum when the difference between the number of zeroes and the number of ones in the output column in the truth table is maximum. A. Calculation of Switching Activities of Logic Gates To calculate the switching activity of a logic circuit, it is important to determine the switching activity for the constituent logic gates. Based on the classical probabilistic definition of the switching activity [11], we can easily calculate the

7/64

f5
3/16

12/64

f2 f3
3/16

f6
39/256

f

d

f4

Fig. 1. Circuit schematic and switching activity for  +  +  + 

be used to estimate the switching activity. Figure 2 shows the propagation of signal probabilities through the gates. Here it is assumed that at the PIs the signal probabilities are equal. Hence in Figure 1,  () =  () =  () =  () = 1 2. The Algorithm, Compute Signal Probabilities [4], [17], can

be used to estimate signal probabilities at the internal nodes of a logic circuit. From Figure 1, using this algorithm and propagation of signal probabilities through gates, we can 3 obtain  1 =  (). () = 1 4 ,  = 16 . Similarly, the switching activities for nodes 2 to 6 and  can be computed.

A B C D E

3/16

7/64 15/256 31/1024 63/4096

A B C D E F

3/16 15/256 3/16 3/16
Total Switching Activity =2607/4096 Delay=3 SA*d=7821/4096

63/4096

F Total Switching Activity=1643/4096 Delay=5 SA*d=8215/4096

P1 P2

P1.P2 1-P1

P1 P2

1-P1.P2 (1-P1 )(1-P2 )

Fig. 4. Comparison of switching activity between ((((A*B)*C)*D)*E)*F and ((A*B)*(C*D))*(E*F) TABLE I S WITCHING ACTIVITY (SA) AND D ELAY ( D ) P RODUCT       ×  0.1875 1 0.1875 0.2461 2 0.4922 0.4336 2 0.8672 0.4639 3 1.3916 0.6365 3 1.9094 0.7382 3 2.2147 0.8711 3 2.6132 0.8884 4 3.5536 1.0596 4 4.2382   0.1875 0.2461 0.3555 0.3857 0.4011 0.4089 0.4128 0.4147 0.4157    ×  1 0.1875 2 0.4922 3 1.0664 4 1.5430 5 2.0056 6 2.4532 7 2.8894 8 3.3177 9 3.7412

P1 P1 P2

P1 P2 1-(1- P1 )(1-P2) =P1 +P2 -P1.P2

Fig. 2. Propagation of probabilities through gates

The controllability/observability concepts as described in [19] can also be used to calculate the switching activity. Unfortunately, both approaches give inaccurate results in presence of reconvergent paths. The approach presented in [20] tries to partially solve this problem. IV. M OTIVATION OF THE WORK In this paper, we attempt to design a generalized method to minimize the switching activity for logic expressions. The proposed method accepts any  /   expression as input and transforms it into a functionally equivalent multilevel expression with minimized switching activity. Figure 3 illustrates a motivating example. Consider the logic expression of a Full Subtractor with  , ,  as input variables and  and   as output variables. Here only the  is of interest. The logic expressions corresponding to  and   are respectively given by ( +  +  ) and (     ). With the discussions in next few sections, we will find that the total switching activity of this logic expression will be 83 64 . But the logic expression for borrow can also be written as (( +  )  ( +  )) +  and the switching activity for this expression will be 70 64 .
bout=xy+xb in +yb in 15/64 3/16 y 1/4 x 3/16 b in 3/16 y Total Switching Activity =3*(3/16)+2*(1/4)+15/64=83/64
bout=xy+xb in +yb in =(x(y+b in ) + (y+b in ))+yb in

  2 3 4 5 6 7 8 9 10

by the corresponding logic expression. Here, we consider a straightforward realization of the expression. Lemma 1. If a product (or sum) term contains more than two literals (..  = -1  -2  -3  -4  . . .  1  0 ), the function can be implemented as ((-1  -2 )  -3 )  . . .)  1  0 ) (chain representation) instead of ((-1  -2 )  (-3  -4 )  . . .  (1  0 )) if  is even or ((-1  -2 )  (-3  -4 )  . . .  (2  1 )  0 ) (tree representation) if  is odd. The switching activity is less in the chain representation [7]. Figure 4 shows the implementation of a 6 variable (A, B, C, D, E, F) product term. The first implementation switching 2607 activity ( 1643 4096 ) is less in comparison to the second one ( 4096 ). A similar type of operation can be done for a sum term as well. Due to delay in different gates there may be power loss due to glitches. The width of the glitch depends on the delay of logic gates and interconnections [4]. It is clear that glitches can occur in a chain structure and in the tree structure the occurrence of glitches is much less. Hence, minimization of the product of the switching activity and the delay may be a good design parameter. Table I shows the product of the switching activity and the delay for AND gate in the Tree structure and the Chain structure. Here, it is assumed that for each gate the delay is 1. Observation 1. if the switching then for   9 chain structure inputs. For AND and OR gates, it can be shown that activity and the delay product is considered, the tree structure is better, and if  > 9 the is better. Here  is the number of primary

x 1/4 y b in 3/16

15/64

15/64 1/4

y b in 3/16 Total switching activity =2*(3/16)+2*(15/64)+1/4=70/64

Fig. 3. Reduction of switching activity of Full Subtractor

A Rule-based systems for logic synthesis through local transformations has been proposed in [18]. In our paper different rules are used to minimize the switching activity. V. D ESIGN FOR M INIMAL S WITCHING ACTIVITY Based on the discussions in Section III-A and Section III-B, it is observed that the switching activity of a circuit realization depends on its implementation, which can be described

For XOR and XNOR gates the tree structure is better since the switching activities for these gates are independent of the number of inputs. Lemma 2. Let an  variables product term contains an odd number of complemented literals, say, 0 to  literals -1 literals are in are in complemented form and +1 to =   =-1 prime form, k is even, i.e., ( =0  ).(  =+1  ) then the following transformations will reduce the switching activity:  =-1 = ( =0  ).(  =+1  ) =-1  =-1 =( =0  ).(  =+1  ).  =-1 =(0 + 1 . . . -2 + -1 ).(  =+1  ).   =-1 (0 + 1 . . . -2 + -1 ).(  =+1  )
7/64 3/16 7/64

TABLE II I NCREMENT / D ECREMENT OF DELAY AND AREA DUE TO EACH RULE  1 2 3 4 5 6 7 8 9 10 11 12 13                               ( = 2)( > 2)        

z y x

3/16 1/4 7/64

x y z

a) Switching Activity for

zy x

b) Switching Activity for

zyx

zy

=1 . . . . .(1 . . . . .+1 . . .  +1 .2 . . .  ) Here ,  and  are in general different and    . Rule 2: Apply the transformation of Lemma 1 and Observation 1. Rule 3: If a product (sum) term contains even number of complemented literals, then replacing each pair of complemented literals with their   (  ) combination by application of De Morgan's theorem will reduce the switching activity of the entire term. Rule 4: Applying Consensus Theorem [5] i.e. ( +  +  ) = ( +  ). Rule 5: If (  2) and  is even then apply 1 + 2 .3 . . . . . = 1 + 1 + 2 + . . .  Rule 6: If (  3) then apply 1 + 2 .3 . . . . .-1 . = 1 + 1 + 2 + . . . -1 . . Rule 7: For two or more than two variables the Boolean expression 1) ( +  )=(( +  ) +  ). 2) 1 +2 + . . . -1 +1 .2 . . . . . = 1 .2 . . . . .-1 +  +  Rule 8: 1) 2) 3)  = ( ) .   = 1 2 . . .  2 3 . . .  . 1 2 3 . . .  =  =2× ( =0  ).(  =1  ).( ) = =( =0  ).(1 + 2 ) . . . (2×-1 + 2× ). = ( =0  ).(1 + 2 ) . . . (2×-1 + 2× )( )

Fig. 5. Reduction of switching activity by introducing XOR gate

Figure 5 shows the reduction of the switching activity if the logic expression  is modified as    . The total switching activities for  and    are respectively 35 64 and 26 64 . Lemma 3. If a sum term in a logic expression contains an odd numbers of complemented terms, then the switching activity can be reduced gate. i.e. 2×by introducing      + +  = + =1   =  =1  ) . . . (   ) +  =  +  =    where (   +1 2 ×  - 1 2 ×        = =1  + ( +1 ) . . . (2×-1 2× ). From Lemma 2 and Lemma 3, it can be clearly seen that the introduction of  or   gate in a sum term or in a product term with an odd number of literals reduces the total switching activity and hence reduces the power dissipation of digital circuits. VI. A LGORITHM F OR M INIMIZATION O F T OTAL S WITCHING ACTIVITY The proposed algorithm for minimization of the switching activity in a circuit is shown in Figure 6. The algorithm takes the truth table of any switching function as input and gives the output as a logic expression of the switching function with minimized switching activity. The following Rules (i.e. transformations) are applied to the switching expression in order to reduce the switching activity. Rule 1: 1) . + . =.( +  ) 2) 1 .1 +2 .1 +1 .2 +2 .2 +. . . +1 . +2 . = (1 + 2 ).(1 + 2 + . . .  ) 3) 1 .2 . . .  .1 .2 . . .  +1 .2 . . .  .1 .2 . . .  =1 .2 . . . . . (1 .2 .3 . . .  +1 .2 . . .  ) 4) 1 .2 . . . . .1 .2 . . . . +1 .2 . . . . .1 .2 . . . .

Rule 9: 1)  +  =  +  . This is known as absorption rule [5]. 2)  +  = . . 3)  +  =  +  = ( ). The last two transformation can be defined as a modified absorption rule. Rule Rule Rule Rule 10: 11: 12: 13:  =    .  +  =    . Apply Lemma 2. Apply lemma 3.

Algorithm Minimize Switching Activity() Input: Truth Table Output: Function for minimal switching activity 1. Obtain the minimal Sum-of-Product ( ) or Product-ofSum (  ) expression using any standard method (such as Kmap, Quine- McCluskey method, Espresso)for the minimization of given switching function  . E-XOR() operations can also be considered to minimize the logic expression. 2. Apply Rule 1 and Rule 2 to reduce the switching activity. 3. If either the  or   does not contain any complemented literal then 4. Calculate the total switching activity of  or   . 5. Take the function with minimum switching activity. 6. Endif 7. If number of complemented literals of a product term in a  is even then 8. Replace a pair of complemented terms by   gate using De Morgan's theorem. (Rule 3). 9. Endif 10. If number of complemented literals of a sum term in a   is even then 11. Replace a pair of complemented terms by    gate using De Morgan's theorem. (Rule 3). 12. Endif 13. If applicable then 14. Apply Rule 4, Rule6, Rule 7, Rule 9 to reduce the switching activity. 15. Endif 16. If a  (  ) contains only one complemented literal  then 17. Apply Rule 8 or Rule 10 (in case of SOP). 18. Apply Rule 5 or Rule 11 (in case of POS). 19. Take the function with minimum switching activity. 20. Endif 21. If the product term in  (  ) contains odd number of complemented literals then 22. Apply rule 12 in case of  . 23. Or Apply Rule 13 in case of   24. Take the function with minimum switching activity. 25. Endif 26. end.
Fig. 6. Algorithm For Minimal Switching Activity

of the circuit may increase. Table II shows the decrease or the increase of area or delay due to each transformation. VII. E XPERIMENTAL R ESULTS The implementation and power estimation of the proposed rule-based algorithm has been done by using Synopsys EDA tool -DESIGN VISION version I-2013.12-SP1, 20, 2014 under CENT OS and TSMC 120 nm library. The experiment is done on some basic circuits and three other benchmarks circuit (alu1, cm138a, z9sym) [21]. The basic circuits and also the 2-adder and the 3-adder circuits are manually constructed. The switching activity of circuits and the associated dynamic power dissipation using conventional SOP (POS) method implemented with two-input gates with the help of chain structure and our proposed method are summarized in Table III. We observe that the total switching activity for our proposed method never exceeds, and is less in most of the cases than the one obtained using the traditional logic optimization. To calculate the area of a circuit it is assumed that the area of inverter is 1. The area of  , ,   ,  , and  gate are assumed to be 3, 3 2, 2 and 7 respectively. The delay of   ,  , ,   ,  , and  gate are assumed to be 1, 3, 3, 2, 2 and 7 respectively. A. Comparison of Our Proposed Method with the Existing Method of [12] In [12] the authors basically modify the Karnaugh maps to reduce the switching activity. Logic optimization using Karnaugh maps is limited to 6 variables switching functions. The reduction of the switching activity and hence the power dissipation of CMOS VLSI circuits by our proposed algorithm is not restricted to 6 variables switching functions and it is applicable for any kind of circuits. Moreover, the method of [12] is not applicable for all types of switching functions. For instance if the logic expression of a two-variable switching function is  , then the method in [12] cannot reduce the switching activity. On the other hand, our proposed method reduces the switching activity. B. Power-Delay Tradeoff Logic optimization using our proposed method surely minimizes the switching activity. But in order to minimize the switching activity by the proposed rules, it may also happen that a NOR gate is used instead of a single NOT gate (e.g. transformation of Rule 7, i.e., ( +  ) = (( +  )+  ) and so on). In this case, the total transistors count of the circuit increases. In our proposed method we do not consider the circuit delay. Logic optimization to minimize the switching activity and the delay as a joint objective will be the future direction of our work. VIII. C ONCLUSION In this paper we propose a rule-based approach to reduce the switching activity of combinational logic circuits. This would reduce the dynamic power and hence the total power

A. Effectiveness of the Proposed Algorithm The total switching activity of a logic expression calculated by the proposed algorithm is less than or equal to the switching activity of the equivalent logic expression by any standard logic method. In our proposed algorithm, first, any standard logic optimization method is used to find the minimized logic expression. Then a part of or the whole logic expression is replaced with an equivalent logic expression for which the switching activity is minimized. The transformations as given by Rule 1 to Rule 13 decrease the switching activity by i) reducing the number of NOT gates ii) increasing the number of inputs to AND or OR gates, iii) introducing XOR or XNOR gate (Lemma 2, Lemma 3) iv) a combination of these. For some specific logic expressions, it is not possible to replace the logic expression with an equivalent logic expression with less switching activity. Hence the total switching activity of a logic expression calculated by the proposed algorithm is less than or equal to the switching activity of the same logic expression calculated by any standard method. In order to reduce the switching activity, some times, the area or the delay

TABLE III C OMPARISON OF S WITCHING ACTIVITY AND DYNAMIC POWER OF DIFFERENT COMBINATIONAL CIRCUITS
TSA Conv 2.75 1.6875 0.4375 1.5469 0.6875 1.8125 5.9609 3.5392 4.2218 6.2285 10.034 8.0374 19.1958 our 1.8125 1.4375 0.4375 1.3594 0.625 1.4375 4.7891 2.9064 2.9710 3.7479 7.3476 5.3484 15.8665 Dynamic Power (micro watt) Conv Our 15.7121 14.3159 7.3628 5.2588 5.1288 5.1288 17.8317 11.1890 6.5248 5.8172 15.3178 14.7618 18.3713 15.4985 14.0391 9.0759 17.8394 14.7104 32.5028 28.608 21.5760 18.8346 7.1527 5.97 61.3653 58.35 Area Conv Our 39 50 35 42 10 10 29 26 11 12 30 33 82 94 52 50 59 51 91 89 113 96 92 77 231 210 Delay Conv Our 7 9 13 16 7 7 12 12 7 7 12 13 13 16 19 14 24 25 36 39 14 13 14 12 50 57

Circuits 3: 8 decoder 4:1 MUX Half Adder Full Adder Half Subtractor Full Subtractor 2 Bit Comparator Priority Encoder 2-Adder 3-Adder ALU1 cm18a z9sym

% Reduction of TSA 34.09 14.81 0 12.12 9.09 20.69 19.66 17.88 29.63 39.83 26.77 33.46 17.34

dissipation, enabling the design of power-efficient circuits with several useful applications. Experimental results show significant reduction of the switching activity. In this paper, together with all the limitations, SOP representations are used and switching possibility of input signal are assumed to be 50% . The method can be improved by making the computation scalable and precise. Introduction of signal probability to compute the switching activity for logic circuits can be used for this computation. The method presented in this paper can be further improved by considering both delay and power as objective functions and implementation and testing of the improved algorithm with standard benchmark circuits. ACKNOWLEDGMENT This research has been partially supported by a grant GA16-05179S of the Czech Grant Agency, "Fault-Tolerant and Attack-Resistant Architectures Based on Programmable Devices: Research of Interplay and Common Features" (20162018). R EFERENCES
[1] S. Chattopadhyay and N. Choudhary, "Genetic Algorithm based Approach for Low Power Combinational Circuit Testing",Procedings of the 16th International conference on VLSI Design (VLSI'03). [2] P.Girard, C.Landraut, S.Pravossoudovitch and D.Severac, "Reducing Power Consumption During Test Application by Test Vector Ordering", Int'l Symposium on Circuits and Systems (ISCAS 98), pp 296-299. [3] P. Ghosal, T. Samanta, H. Rahaman and P. Dasgupta, "Thermal-aware placement of standard cells and gate arrays: Studies and observations", IEEE Computer Society Annual Symposium on VLSI, April 2008. ISVLSI'08., pp 369-374. [4] K. Roy and S.C.Prasad, "Low Power CMOS VLSI Circuit Design", Wiley India Edition, Reprint 2011. [5] Z. Kohavi and N. K. Jha, "Switching and Finite Automata Theory", 3 Edition, Cambridge University Press, 2010. [6] N. Wehn and M. Munch, "Minimising power consumption in digital circuits and systems: An overview", Kleinheubacher Berichte, Band 43, pages pp.308-319, September, 1999, Kleinheubach, Germany, Invited Talk. [7] A. P. Chandrakasan and R. W. Brodersen, "Minimizing Power Consumption in Digital CMOS Circuits", Proceedings of the IEEE, Vol 83, No. 4, April 1995.

[8] K. Roy and S. C. Prasad, "Circuit Activity Based Logic Synthesis for Low Power Reliable Operations", IEEE Transactions on VLSI, Vol-1, No. 4, December 1993, pp 503-512. [9] S. Ahuja, A. Lakshminarayana and S. K. Shukla, "Low Power Design with High-Level Power Estimation and Power-Aware Synthesis", Springer Pub., 2011. [10] I. Brzozowski and A. Kos, "Minimization of Power Consumption in Digital Integrated Circuits of Reduction of Switching Activity", 25 Euromicro Conference, 1999, pp 376-380 Vol. 1. [11] R. V. Menon, S. Chennupati, N. K. Samala, D. Radhakrishnan and B. Izadi, "Power Optimized Combinational Logic Design", Proceeding of the International Conference on Embeded Systems and Applications, June 2003, pp. 223-227. [12] R. V. Menon, S. Chennupati, N. K. Samala, D. Radhakrishnan and B. Izadi, "Switching Activity Minimization in combinational Logic Design", Proceeding of the International Conference on Embeded System and Application, 2004, pp 47-53. [13] J. Monteiro, S. Devadas, A. Ghosh, K. Keutzer, and J. White, "Estimation of average Switching Activity in Combinational Logic Circuits Using Symbolic Simulation", IEEE Transaction on Computer Aided Design of Integrated Circuits and Systems, Vol. 16, NO. 1, 1997, pp 121-127. [14] M. Ghasemazar and M. Pedram, "Variation Aware Dynamic Power Management for Chip Multiprocessor Architectures", Design, Automation & Test in Europe Conference & Exhibition (DATE), 2011. [15] P. Dasgupta, P. Dasgupta, D. K. Das, "A Novel Algorithm for Interconnect-aware Two level Optimization of Multi-output SOP functions", Proceeding of the 11 International Workshop on Boolean Problems, Freiberg, September 2014, pp 219-226. [16] M. Alidina, J. Monterio, S. Davadas, A. Ghosh and M. Papaeftymiou, "Precomputation-Based Sequential Logic Optiomization for Low Power", IEEE International Conference on Computer Aided design, 1994, pp 74-81. [17] P. Parker and E. J. McCluskey, "Probabilistic treatment of General Combinatorial Networks", IEEE trans. Computers, Vol c-24, pp 668670, 1975 [18] J. A. Darringer, W. H. Joyner, Jr. C. Leonard Berman and L. Trevillyan, "Logic Synthesis Through Local Transformations", IBM J. RES. DEVELOP. Vol. 25 NO. 4 July 1981. [19] L. H. Goldstein, "Controllability / Observability Analysis of Digital Circuits", IEEE Transactions on Circuit and Systems, Vol. CAS-26, No. 9, 1979, pp 685-693. [20] S. C. Seth, V. D. Agrawal, "A new model for Computation of Probabilistic testability in combinational circuits", Integration, The VLSI Journal 7(1989) pp 49-75, 1989. [21] S. Yang, "Logic Synthesis and Optimization Benchmarks User Guide," Technical Report 1991 IWLS-UG-Saeyang, MCNC, Research Triangle Park, NC, January 1991, p. 45.

With extreme miniaturization of traditional CMOS devices in deep sub-micron design levels, the delay of a circuit, as well as power dissipation and area are dominated by interconnections between logic blocks. In an attempt to search for alternative materials, Graphene nanoribbons (GNRs) have been found to be potential for both transistors and interconnects due to its outstanding electrical and thermal properties. GNRs provide better options as materials used for global routing trees in VLSI circuits. However, certain special characteristics of GNRs prohibit direct application of existing VLSI routing tree construction methods for the GNR-based interconnection trees. In this paper, we address this issue possibly for the first time, and propose a heuristic method for construction of GNR-based minimum-delay Steiner trees based on linear-cum-bending hybrid delay model. Experimental results demonstrate the effectiveness of our proposed methods. We propose a novel technique for analyzing the relative accuracy of the delay estimates using rank correlation and statistical significance test. We also compute the delays for the trees generated by hybrid delay heuristic using Elmore delay approximation and use them for determining the relative accuracy of the hybrid delay estimate.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/303563207

Intelligentandadaptivetemperaturecontrolfor large-scalebuildingsandhomes
ConferencePaper·April2016
DOI:10.1109/ICNSC.2016.7478971

CITATIONS

READS

0
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

14

AllcontentfollowingthispagewasuploadedbyYuanWangon28August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Proceedings of 2016 IEEE 13th International Conference on Networking, Sensing, and Control Mexico City, Mexico, April 28-30, 2016

Intelligent and Adaptive Temperature Control for Large-Scale Buildings and Homes
Yuan Wang
Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu

Partha Dasgupta
Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract--Temperature control in smart buildings and homes can be automated by having computer controlled air-conditioning systems along with temperature sensors that are distributed in the controlled area. However, programming actuators in largescale buildings and homes can be time consuming and expensive. We present an approach that algorithmically sets up the control system that can generate optimal actuator settings for large-scale environments. This paper clearly describes how the temperature control problem is modeled using convex quadratic programming. The impact of every air conditioner(AC) on each sensor at a particular time is learnt using linear regression model. The resulting system controls air-conditioning equipments to ensure the maintenance of user comforts and low cost of energy consumptions. Our method works as generic control algorithms and are not preprogrammed for a particular place. The system can be deployed in large scale environments. It can accept multiple target setpoints at a time, which improves the flexibility and efficiency for temperature control. The feasibility, adaptivity and scalability features of the system have been validated through various actual and simulated experiments.

I. I NTRODUCTION About 70% of the electricity load is consumed by commercial and residential buildings in the US. Studies show by the year 2025, building electricity energy costs will be over 430 billion dollars. With the rapid increase of energy costs in the building sector, carbon dioxide(CO2 ) emissions are growing faster than before. The sector contributes around 39% CO2 emissions in the US per year, more than any other sector such as transportation, which results in threat of climate change. Heating, ventilation and air conditioning(HVAC) system is the largest energy consuming sector in buildings and homes. In the year 2009, about 48% energy costs are contributed by space heating and cooling. Most of traditional HVAC systems in buildings and homes are controlled by thermostats that are manually configured either by centralized control technicians or individual users, which requires a lot of human efforts. For maintaining user comfort levels in work places, these systems usually keep AC settings all the day, even during off hours when the places are not occupied, which causes a lot of unnecessary energy consumptions. Some of thermostats in office areas do not provide friendly interfaces and flexible functionalities for users to control. In a centralized air-conditioning control area, users can not customize the

temperature settings for a particular place, which causes a lot of inconvenience. Saving energy and satisfying user comforts are two main aims for HVAC control systems design. For saving energy, amount of total energy costs should be minimized during the control. For maintaining user comforts, inside room temperature(sensed by temperature motes) should be uniform and stay in a satisfied range. There always exist tradeoffs between these two goals. One of the popular products in current market for smart thermostat technology is the Nest Learning Thermostat [1]. It learns user preferences such as temperature levels during a day for about a week and then it can automatically generate AC control plans based on user input data. The drawback of this approach is it relies too much on user selected data. If users randomly provide some incorrect data to the thermostat or they disable it for a particular time, the effectiveness of the thermostat may drop down. [2] proposed an approach called smart thermostat that can automatically turn on/off air conditioning systems by sensing occupancy and sleep patterns. It can also generate an energy efficient plan for the preheating stage by looking at system configurations and analyzing historical occupancy patterns. One limitation of this work is it relies on a lot of information from the equipment itself and this paper only evaluates a single type. The scalability and adaptivity of the system need to be improved. Although there are a lot of great improvements for smart building technologies nowadays, current control plans for HVAC systems still have some drawbacks and limitations. First, in most of the commercial buildings, one thermostat is used for controlling multiple vents at same time. People in different working areas can not customize their preferences when they have conflicts on temperature settings. Second, current HVAC control approaches are not good for large scale environments where there are many standalone ACs in an area. Each AC is independently controlled by a single switch. A subset of ACs need to be turned on according to users' preference settings. The control plan should optimize user comfort levels without sacrificing too much energy. Last, in the current HVAC control system, temperature sensors are usually built in thermostats that are put on the wall. They should be placed at sitting areas in order to better capture surrounding temperature for users. In this paper, we design an air-conditioning control system

978-1-4673-9975-3/16/$31.00 c 2016 IEEE

for tackling the above challenges. Our system can be used for controlling large commercial and residential buildings where there exist multiple ACs, control switches and temperature sensors. The goal of the system is to optimize the user comfort levels and minimize energy consumptions. There are two main stages in this system: predictive stage and adjustive stage. In the predictive stage, mathematical models are used to formalize the problem and optimization algorithms are used to get a predicted solution. In the adjustive stage, feedback system are designed to reduce potential errors. Our system is good for providing fine-grained settings during precooling/preheating stage. Our approach is adaptive, which means it can be easily applied in any type of building environments. II. R ELATED W ORK Thermostats technologies have been widely used in HVAC systems for automatically controlling HVAC equipments in buildings and homes. The basic logic behind is HVAC equipments are turned on when its controlled area is occupied and turned off when occupants leave the area. The thermostats are preprogrammed and temperature setpoints are predefined according to the local environments such as static occupancy patterns. It relies on too much static information therefore it is hard to adapt to environmental changes. An alternative method called reactive thermostat is proposed for tackling the problem. It uses various sensors such as motion sensors or door sensors to detect user activities in real time so that the control system can adjust system settings when pattern changes. However, some studies found that this method didn't improve the efficiency as people expected since there usually exists long delay for the system reaction due to the hardware limitations which even save less energy than the previous programmable thermostat approach. [2] proposed an approach called smart thermostat that can automatically turn on/off air conditioning systems by sensing occupancy and sleep patterns in real time. It can also generate an energy efficient plan for the preheating stage by looking at system configurations and analyzing historical occupancy patterns. This approach dynamically controls HVAC systems based on occupancy status in a place, which can effectively adapt to environmental changes. One limitation of this work is it relies on a lot of information from the equipment itself and this paper only evaluates a single type. The scalability and adaptivity part of the system need to be improved. Nest Learning Thermostat [1] is a popular user-centric tool for HVAC system control, which is very easy to use compare to the traditional programmable thermostat. It automatically learns a user's preferences and behaviors based on some precollected data from the user. The drawback of this approach is it relies too much information on the user-input data. If users randomly provide some incorrect data to the thermostat or they disable it for a particular time, the effectiveness of the thermostat may drop down. Another drawback of this approach is it is usually set up in a single AC environment such as residential homes where only one target temperature value at a time is accepted. When two people have conflicts

on target temperature values in a place or the thermostat is set up in a large scale environment where multiple ACs exist, the thermostat can not give effective solutions. A fuzzy inference system for adaptively doing heating control is proposed in [3]. The main idea is it takes power profile of the previous day, adjusts the profile based on current conditions and then applies the latest profile to the current day. It uses Artificial Neural Network model to predict the future comfort levels and a fuzzy rule is designed for the setting adjustment step. The drawback of the system is it mainly focuses on maintaining user comforts during the control. The energy saving part is only considered when a place is not occupied. The fuzzy rule should be improved so that some fine-grained settings can be provided instead of using the words "large", "medium" and "small". Our approach is proposed for tackling the above problems. It can be deployed in a large scale environment without any static or customized configurations. If needed, users can set their own preferences on temperature setpoints in a workplace. The system can efficiently and effectively control HVAC equipments even when there exist multiple target values on sensors. It provides fine-grained pre-cooling plans based on occupancy schedules. When environment changes, the system can automatically update calibration data and adjust the settings accordingly. WSN technologies have been applied into various areas such as [4] [5][6]. It consists of portable wireless sensor motes such as Crossbow's TelosB or MICAz to monitor the values of physical conditions, such as temperature, light, humidity, and so on. WSN data collections use several specific protocols such as Collection Tree Protocol [7]. III. F ORMAL M ODEL This section describes how we use mathematical model to formulate the problem. For simplicity, we build the model for cooling strategy of air-conditioning control. Heating strategy can be modeled in a similar way. In this model we have a set of on/off switches that control arbitrary n ACs (one per switch) and a set of m temperature sensors. The sensors are connected to the control system via a wireless sensor network and the switches are activated via actuators connected to the system. Physical locations and correlations between them are initial unknown to the control system. The ultimate task is to compute the positions of switches over time, i.e. durations for each switch to be kept on. We assume an AC will keep running until its control switch is turned off. Let t = (t1 , ..., tn ) denote the assignment for AC switches where ti  0 and it denotes switch i is kept on for ti time units. The goal is to optimize the energy E (t) and the comfort C (t). E (t) is the total energy consumptions in the control period. We assume the power of each AC is constant during the control period. Let w = (w1 , ..., wn ) denote the electric power for each AC. Then E (t) = w × t. For C (t), it is satisfied when all sensor values approach their target values

and stay in a satisfied range. Let e = e1 , ..., em denote all sensor readings. Each sensor has a target value to be reached, for example, sensor j 's target value is tari . Let tar = tar1 , ..., tarm denote all target values. They can be set either by users or default standard values. Hence the problem can be stated as: minimize
t

T =kt+b where t stands for interval time and T stands for temperature level. In order to learn k and b, some experimental data points should be pre-collected. These coefficients should be updated when environment changes such as building patterns or weather conditions. There are multiple ACs and multiple sensors in the system. For each sensor, every AC has an impact factor on it, i.e. a pair of (k, b). Since sensor readings are additive, to a particular sensor j , its reading ej after an operation of n ACs on a vector t time is: ej = sj - mj  t where sj stands for the initial sensor j 's value and     t1 k1j      t2   k2j   t=. mj =  .  .  .  .  . tn knj sj can be collected from the calibration step and mj can be calculated based on linear regression model. B. Calibrating and Calculating Impact Factor In the last section, we discussed how we use a linear regression model to learn the relationship between time and temperature. Note that this model is effective when time vector is small. Assume all ACs are off at the beginning. Calibration steps involve the following: · Turn on one AC at a time for a period of time t, record the temperature changes on every sensor. Selection of t should be reasonable, so that a range from initial temperature to the target temperature can be covered for each sensor, if applicable. · Analyze the collected data using linear regression model. Perform linear regression analysis for different period of time, if necessary. Thus, a impact factor kij might have different values for different periods of time. · Repeat step 1 n times until all ACs are counted. For each sensor j , a vector mj is created. If more than one kij exist, then for each period of time, a mj is recorded. In each AC operation cycle, either the predictive stage or the adjustive stage, only one mj can be used for sensor j . The selection of mj is decided by the initial sensor value on j . Each AC is assigned an upperbound value for its operational time during every cycle, formulated as 0  ti  tmax . Value of tmax is associated with value of kij . For each AC i, from time 0 to time tmax , its impact factor on every sensor j kij should remain the same. Ideally, impact factors and room initial temperatures should be updated frequently to ensure the accuracy of the computation. However, in some cases, these values remain the same in a period of time. Thus in order to increase the efficiency, these values don't have to be re-calibrated or updated if the (3)

w × t, e - tar

2 2

subject to min  ej  max, j = 1, ..., m 0  ti  tmax , i = 1, ..., n

(1)

By applying the -constraint method [8] designed for solving multi-objective optimization problems, the new objective function can be defined as: minimize
t

e - tar w ×t

2 2

subject to min  ej  max, j = 1, ..., m 0  ti  tmax , i = 1, ..., n

(2)

We assume that the impact of ACs on sensors(number of sensor values decrease) over a short time period is additive, i.e., the impact of two ACs over a short time period on one sensor(total number of sensor values decrease during the period) is the sum of the individual impact for each AC. Since the time period is considered to be small for solving the above equation, the relationship between time and temperature can be learnt using linear regression model, which will be discussed in the next section. IV. C ONTROL A PPROACHES There are two main stages for solving the air-conditioning problem. The first stage is called predictive stage that happens before the room is occupied, which is used for predicting an approximate setting for air-conditioning control. The second stage is called adjustive stage that happens after room is occupied. It is used for eliminating the potential offsets from the predictive stage and providing fine-grained air-conditioning control for maintaining user comfort levels. In order to achieve max electricity savings, we decide to turn off all ACs when the controlled area is not occupied, and pre-cool the area in advance before users enter the space. By default we assume we have a list of occupancy schedules. The schedule can be updated by users in real time, for example, a user can send a notification at any time telling the system when he expects to get back. A. Predicting AC Impacts on Sensors Over time In the cooling stage, an AC continuously provides cooling air to reduce the inside temperature levels until it is turned off. Since the relationship between time and temperature in each cycle is investigated over a short time period, up to an upperbound tmax , a linear regression model can be identified with a high coefficient of determination [9]. It is defined in the form of:

change doesn't exceed a threshold. The threshold value can be set based on local environments. C. Solving the Equations to Compute Approximate AC Settings The result of equation 2 is a time vector t that indicates the working time interval for every AC in the room. The upperbound value tmax for each ti in vector t can be different. The largest value among all values of tmax is assigned to q . If users return at time p, then the system will start calculating t from time p - q in order to cool the room at time p. After time p, the system will go to the adjustive stage until the room is unoccupied again. Applying equation 3 to equation 2, we get:
m

The above transformation satisfies the format of quadratic
m

programming model [10] where H = 2 
j =1 m

mj  mj ,

f = -2 
j =1

xj  mj , lb is a vector of zeros, ub is a vector

minimize
t j =1

(mj  t - xj )2

subject to sj - max  mj  t  sj - min, j = 1, ..., m w ×t 0  ti  tmax , i = 1, ..., n (4) where xj = sj - tarj . The objective function of equation 4 is a quadratic function that can be re-written in the following form:
m m m

of all tmax values. Number of elements in lb and ub is both n. Thus our problem belongs to quadratic programming problem, which can be solved by some existing algorithms such as interior point method. In addition, the objective function appearing in equation 4 is a standard form of least-squares [11] that is convex, therefore our problem is a convex quadratic programming problem. The result set will be a set of global minimum. The weight factor (which appeared in equation 4) is used to balance the tradeoff between saving energy and maintaining user comforts. The assignment of will significantly affect the final result, therefore, it should stay in a reasonable range. In order to rationally set , we first study the approximate minimum value of , i.e.,  min , by solving the following equation: minimize
t

w t (6) 0  ti  tmax , i = 1, ..., n

subject to A  t  b

t (
j =1

mj  mj )  t - 2  (
j =1

xj  mj )  t +
j =1

x2 j (5)

The constraints of equation 4 can where  k11 k21  k k22  12  . .  . . .  .  k2m  k1m  Am,n =  -k11 -k21   -k12 -k22  . .  . .  . .  -k1m -k2m w1 w2 and

be re-written as A  t  b 

··· ··· .. .

      · · · knm   · · · -kn1   · · · -kn2   . ..  . . .   · · · -knm  ··· wn

kn1 kn2 . . .

where A and b can be obtained from A and b by removing their last rows(w and ) respectively. This equation is simpler than the previous one since we remove an optimized variable(approaching all sensor values to the targets) from the original equation. In other words, compared to the original problem, in this equation, we focus on minimizing the energy usage only, with all sensor readings requiring to stay within an accepted range. This equation satisfies the form of linear programming model, which can be solved by existing algorithms such as interior point or simplex algorithm. The value of can be set as min + a reasonable threshold that is assigned according to the real environments. D. Adjusting AC Settings for Maintaining User Comforts and Adapting Environmental Changes There might exist some errors in the predictive stage. In order to eliminate them, we add an adjustive stage in the system that can adjust the settings accordingly so that the user comfort levels can be maintained. Compared to the predictive stage, the adjustive stage has some similarities and some differences. The similarity in both stages rely on the proposed computational model to generate AC settings. Differences are: · Unlike the predictive stage, the adjustive stage can be done repeatedly until the controlled area is unoccupied. In other words, predictive stage is used for precooling the area before the room is occupied. Adjustive stage needs to be activated whenever users are in the area and comfort levels are not satisfied. · During the first adjustive cycle, impact factor mj is selected based on initial value on sensor j , as predictive

 s1 - min  s - min   2    .   . .      sm - min    b =  max - s1     max - s2    .   .   .   max - sm 



cycle does. Afterwards, in every adjustive cycle, the vector mj should be checked using the partial real data points collected from sensors. If there is any change, the vector should be updated accordingly. This update reflects self-calibrating and self-learning features of the system, which increases the accuracy of the computation. When environment changes such as location changes or season changes, impact factor mj need to be re-updated. Data for learning the factor need to be re-collected from the environments, but the general approaches remain the same. For the AC control at a particular day when calibration data for previous (similar) days have been recorded, impact vectors should be first chosen from similar categories and updated accordingly in the real time, similar to the update step in the adjustive stage. For example, we wish to predict the AC settings for today at 5pm. We first use yesterday's mj in the category of 5pm. After getting the results from the predictive stage, we turn on the ACs based on the predictive settings. If in the first 2 minutes, we detect the impact factor varies a lot compared with yesterday's one, we will update the mj accordingly based on today's data and recompute the settings. Thus a new dataset of mj for today's 5pm is obtained. It will be inserted into database for future computations. If allowed, the interval time for recalibration can be set longer for better accuracy. V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS A. Verification of Linear Regression Model We performed two calibration experiments in the test room with one AC(540 watt) and one sensor in a same day at 3pm and 4pm respectively. For each experiment, the AC is operated continuously for about 40 minutes and temperature changes are recorded. The calibration follows the steps described in section IV-B. Figure 1 shows the AC impacts on the sensor at different time points, and their corresponding fitted lines generated by MATLAB. From the figures, following things are observed:
·

B. Simulation Experiments and Results In section IV, the computational model is proved to be convex, which means the result set is guaranteed be global minimum. In this section, we use MATLAB to simulate a multi-AC environment and compute the optimal solution. The optimization toolbox is called Quadratic Programming. Simulated set-up is given based on some real experimental data, described as follows: Assume there are 3 ACs and 2 sensors in a room. Sensors are placed at user-sitting areas to capture inside temperature levels. Initial values(s1 and s2 ) are both 30. Target values tar1 and tar2 are both 26. Therefore, x1 and x2 are both 4. min is 25 and max is 28. w = (0.45, 0.46, 0.47) , m1 = (0.05, 0.06, 0.07) , m2 = (0.05, 0.06, 0.07) . Based on these settings, we can get H, f , Am,n , b defined in section IV-C. These factors are needed in the Quadratic Programming toolbox. In equation 4, weight factor affects the final result. It represents the tradeoff between energy savings and user satisfactions. In this simulation experiment, we set different values to and hope to learn the effect of on the final result set. In order to better assign the values, the approximate lowest is learnt by solving the equation 6 using interior-point method. For the particular example stated above, lowest is 14. From the graph, we can see that when is becoming larger, minimum value of the objective function is becoming smaller(sensor values are more converged to the targets) while the energy consumption is becoming larger. This matches theoretical analysis as well: when is larger(more flexibilities on energy consumption), user comforts can be better optimized(more candidates are counted), therefore sensor readings are more converged to the target values. We also scale our simulations and do simulated experiments on a larger number of ACs and sensors. We set n = 100 and m = 100. Min, target and max values of sensors do not change. Assume all ACs have same operational powers(0.46kw). Impact vectors are randomly given on a 0.005base increase from 0.05, for example, if k11 = 0.05, then k21 = 0.055 and so on. To a particular AC i, kip = kiq . We run the above set-up in MATLAB. It quickly gives us the result of M inV alue = 0(global minimum) and Energy = 10.72. It verifies the scalability of our approach. VI. C ONCLUSION AND F UTURE W ORK To enable automated temperature control in a multi-AC environment, under varying conditions of occupancy, weather, seasons and other influences it is essential to have a robust air-conditioning control system that is effective and adaptive. Such system must be deployable in a simple, cost effective way without the need for customizations and reprogramming as conditions change. It also needs to create a comfortable environments with energy savings as a goal. This paper presents such a complete core system that can pre-cool the room before users come back and maintain the comfort levels with a relative small energy cost. It also clearly shows how

· ·

Both two figure can be divided into 2 parts. The first part covers the temperature points from initial point to 24 C. The second part covers the temperature points from 23.5 C to 19 C. Both parts are fitted by linear regression models with different impact factors. The regressed line for the first part has a larger scope than the second one's. Figure 1(b) has similar slopes compared with Figure 1(a), for both lines. Two figures have similar initial sensor values. Temperature's range is approximately from 19 C to 33 C.

From the above findings, we verify the accuracy of linear regression model for learning AC's impacts on sensors during selected time intervals. The experiment also shows the calibration data can be reused for a period of time to increase efficiency. In this experiment, during the 40-minute interval, the AC has two impact factors on sensors. Every AC operation cycle should select appropriate impact value for computation.

34 32 30
Collected Data y=-0.0286t+32.6453 Collected Data y=-0.0023t+23.8684

34 32 30
Collected Data y=-0.0276t+32.5954 Collected Data y=-0.0023t+23.8801

Temperature (celsius)

28 26 24 22 20 18 0 500 1000 1500 2000 2500

Temperature (celsius)

28 26 24 22 20 18 0 500 1000 1500 2000 2500

Time (seconds)

Time (seconds)

(a) 3pm Fig. 1. AC Impacts on Sensors over Time

(b) 4pm

's Impact on Energy
28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 8 7 6

's Impact on MinValue

Energy Cost (kw s)

MinValue

5 4 3 2 1 0 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Value of 

Value of 

(a) Energy Fig. 2. Impact of on Energy and MinValue

(b) MinValue

the control problem can be written in convex quadratic programming form, which is guaranteed to get global minimum values. Impact factors of AC on sensors are learnt using linear regression model. Results have been validated in both experimental and simulated scenarios, which shows the feasibility and effectiveness of our system. Our current air-conditioning control system only considers temperature value as the main target for maintaining user comfort levels. Other factors such as humidity, air flow, etc might affect the user comforts as well. These factors should be added into the system for computation in future. Appropriate models need to be selected for solving the new problem. In this paper, we assume we have a list of occupancy schedules, which is used for the pre-cooling stage. This information needs to be further studied in future to increase the accuracy of our system control. With the use of the system, more data will be collected in the database. These data can be used to better predict the impact factors. The results will be applied into the current system control plan. When dataset is made larger, the system will become more robust and precise. R EFERENCES
[1] Nest thermostat. [Online]. Available: https://nest.com/thermostat/lifewith-nest-thermostat/

[2] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic, E. Field, and K. Whitehouse, "The smart thermostat: using occupancy sensors to save energy in homes," in Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems. ACM, 2010, pp. 211­224. [3] A. Guillemin and N. Morel, "An innovative lighting controller integrated in a self-adaptive building control system," Energy and Buildings, vol. 33, no. 5, pp. 477­487, 2001. [4] Y. Wang and P. Dasgupta, "Designing an adaptive lighting control system for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2015 IEEE 12th International Conference on. IEEE, 2015, pp. 450­455. [5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of the 1st ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88­97. [6] Y. Wang and P. Dasgupta, "Designing adaptive lighting control algorithms for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE, 2014, pp. 279­284. [7] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1­14. [8] G. Mavrotas, "Effective implementation of the -constraint method in multi-objective mathematical programming problems," Applied Mathematics and Computation, vol. 213, no. 2, pp. 455­465, 2009. [9] Wikipedia. Coefficient of determination. [Online]. Available: http://en.wikipedia.org/wiki/Coefficient of determination [10] MathWorks. Quadratic programming. [Online]. Available: http://www.mathworks.com/help/optim/ug/quadprog.html [11] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004.

Automotive software applications implement a variety of control algorithms, with many of them being safety-critical in nature. A typical design flow starts with modeling these control algorithms using tools like MATLAB/Simulink. However, at this stage, a number of assumptions, like negligible sensor-to-actuator delay and instantaneous computation of the controller software, are often made. In particular, the details of the software implementation and the computing platform, both eventually defining the timing properties of the applications, are not accounted for. Such idealistic assumptions can cause a significant deviation of the control performance compared to what was proven at the modeling stage. This is usually addressed with multiple design iterations, which are costly and may lead to over-provisioned and thus poorly designed systems. In this paper we attempt to address this problem by proposing a design-and tool flow that integrates software-and platform-level timing information into the high-level modeling stage. We outline our proposed flow using concrete, industry-strength design tools.Digital microfluidic biochip (DMFB), a latest invention in lab-on-a-chip devices integrates electronics with biology for development of customized miniature devices intended for new application areas namely clinical diagnostics and detection, DNA analysis, point-of-care applications and so on. All-Terrain droplet actuation (ATDA) based biochips have (has) recently been emerged as the new variant in DMFB devices for application in 3D domain. In this work, we explored the possibility of design optimization followed by development of design automation techniques specifically applicable in 3D biochips. We consider the advantages and issues involved in development and application of 3D based design. Accordingly we demonstrated few prescheduled bioassay execution in both 2D and 3D domain and the following enhancement in the route performance.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/280003515

DesigninganAdaptiveLightingControlSystem forSmartBuildingsandHomes
ConferencePaper·April2015
DOI:10.1109/ICNSC.2015.7116079

CITATIONS

READS

2
2authors,including: YuanWang ArizonaStateUniversity
3PUBLICATIONS4CITATIONS
SEEPROFILE

97

AllcontentfollowingthispagewasuploadedbyYuanWangon12July2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Designing an Adaptive Lighting Control System for Smart Buildings and Homes
Yuan Wang
Arizona State University Tempe, AZ, USA Email: Yuan.Wang.4@asu.edu

Partha Dasgupta
Arizona State University Tempe, AZ, USA Email: partha@asu.edu

Abstract--Lighting control in smart buildings and homes can be automated by having computer controlled lights and blinds along with illumination sensors that are distributed in the building. However, programming a large building light switches and blind settings can be time consuming and expensive. We present an approach that algorithmically sets up the control system that can automate any building without custom programming. This is achieved by making the system self calibrating and self learning. This paper described how the problem is NP hard but can be resolved by heuristics. The resulting system controls blinds to ensure even lighting and also adds artificial illumination to ensure light coverage remains adequate at all times of the day, adjusting for weather and seasons. In the absence of daylight, the system resorts to artificial lighting. Our method works as generic control algorithms and are not preprogrammed for a particular place. The feasibility, adaptivity and scalability features of the system have been validated through various actual and simulated experiments.

I. I NTRODUCTION Work environments (and homes) benefit from having even and adequate lighting in spaces that are occupied. Lighting control in large buildings can be challenging to automate, specially as blinds and lights have to be custom programmed for building architecture, geography, weather conditions, seasons and so on. This paper presents an approach to self learning, adaptive lighting control that is not preprogrammed or having a-priori information about the building. Further, such systems can save energy by reducing the use of artificial lighting during daytime hours and unoccupied spaces. Integrating daylight and artificial lighting in automated system can be challenging. Natural lighting is not stable, even at a fixed location; sunlight's impact varies during times of a day, weather changes, seasons and so on. Our system harvests daylight and then fills in the deficiencies using artificial lights, with attention to provision of even lighting and avoiding light that is too bright (or has glare). A complete lighting control system contains two interacting modules: daylight control module and artificial lighting control module. The aims of these two modules are different. Daylight control module is mainly used for reducing energy costs while artificial lighting control module is good for providing a more comfortable working environment. Automating and balancing the lighting control system such that uniform and stable lighting is maintained at occupied locations while energy

consumptions are kept as low as possible turns out to be a hard problem. In this paper, a Wireless Sensor Network(WSN)-based lighting control system is introduced. We use static lights that can be turned on/off by the system and venetian blinds on windows, whose angles can be set by the system. The control system is not custom programmed for the environment, i.e. it does not know which light switch controls which light, which blind setting affects which window or even the physical location of rooms and walls. Thus the system is self calibrating, and adaptive to changes in outside lighting, weather, seasons and so on. The formal model of the problem leads to non-linear integer programming and proves to be NP-Hard. We use a heuristic lighting control algorithm and show that it solves the problem well and efficiently (and is competitive with the optimal solution). The rest of this paper is organized as follows. Section II introduces the related work of the problem. Section III formalizes the lighting control problem using mathematical model. Section IV presents the heuristic algorithms for lighting control. Experimental work and simulation results are discussed in section V. We conclude the paper in section VI and talk about some future improvements of the system as well. II. R ELATED W ORK To make use of natural sunlight is called daylight harvesting. Electrical blinds are set up at each window and are controlled by feedback systems that adjust blind angles based on daylight levels. It is used in some of the lighting control systems such as [1], [2], etc. for energy savings. Studies show that daylight harvesting can save lighting energy up to 77% [3]. The idea behind is to make use of the sunlight if applicable when light level is not sufficient [1]. The above method can adapt to environmental changes but it usually needs a long adjustment cycle until the blind settings are finally set up, which results in users not liking too frequent blind movements or hunting. [4] proposed a technique called SunCast that can better predict sunlight values by using historical data and approximate simulation results. The drawback of this system is that every time ambient environment or building patterns change, the system needs

to collect historical data and rebuild the mathematical model, leading to delays of many months. WSN technologies have been applied into various areas such as [5]. It consists of portable wireless sensor motes such as Crossbow's TelosB or MICAz to monitor the values of physical conditions, such as temperature, light, humidity, and so on. WSN data collections use several specific protocols such as Collection Tree Protocol [6]. Some customized lighting control systems are targeted for special cases. [7] is designed for theater arts area and [8] was mainly designed for entertainment and media production. Some systems use occupancy sensors [9] to switch off lights in unoccupied positions. [10] presented a mathematical model for lighting control problem in which a luminary impact is continuous such as light emitting diodes(LEDs) luminaries rather than discrete values, and the expected illumination level is given as a single value rather than a range. [11] used smart illuminance sensors with infrared ray communication technology to retrieve the lighting ID binding with each lighting fixture. Through analyzing lighting ID information, sensors can recognize nearby luminaries, which is helpful for systems to know the group information for each actuator.

minimize
x,b

E (x, b),  (L1 (x, b), ..., Lm (x, b)) min  Lj (x, b)  max, j = 1, ..., m xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n (1)

subject to

By applying the -constraint method designed for solving multi-objective optimization problems, the new objective function can be defined as: minimize
x,b

 (L1 (x, b), ..., Lm (x, b)) min  Lj (x, b)  max, j = 1, ..., m E (x, b)   xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n (2)

subject to

One important feature for lighting impacts is sensor readings are additive, i.e. let Impactij to be the impact of light i on k sensor j when only light i is on, and Impactb kj to be the impact of daylight on sensor j when only blind k is on and set to a particular setting bk , then we have
n  i=1 n  k=1


III. F ORMAL M ODEL Lj (x, b) = In this model we have a set of switches that control arbitrary lights (one per switch) and a set of blind control switches (each sets angle of one blind)and a set of light sensors. We have established that illumination is additive, that is, the impact of two light bulbs is the sum of the impact of each at a fixed point. Assume a place has n light switches, n blind switches and m light-level sensors, placed by the human designer of the place. The sensors are connected to the control system via a WSN and the switches are activated via actuators connected to the system. The physical locations of lights, blinds, sensors and their correlations are initially unknown to the control system. The ultimate task is to compute the positions of switches (lights and blinds). Let x = x1 , ..., xn  denote the assignment for light switches where xi  {0, 1} and 0 denotes off and 1 denotes on, b = b1 , ..., bn  denote the assignment for automatic blinds where each blind has finite settings and 0 denotes fully off, i.e., let B to be the set of discrete blind settings, then bi  B and bi = 0 indicates blind i is fully off. The goal is to optimize the energy E (x, b) and the comfort C (x, b). E (x, b) can be measured by the number of artificial lights on. For C (x, b), there are two criteria needed to satisfy, lighting level and lighting uniformity. The former is satisfied when every sensor reading stays in an accepted range i.e. if sensor j 's reading is Lj (x, b), then min  Lj (x, b)  max. The latter can be satisfied by minimizing the standard deviation(represented by  ) of the sensor readings. Hence the problem can be stated as: { f (bk ) =

Impactij · xi + bk =  0 bk = 0

k Impactb kj · f (bk )

(3)

1 0

Note for a specific light i, Impactij ( 0,  Z) is always a constant and for a specific short time period and a fixed blind k setting, we assume Impactb kj ( 0,  Z) also to be a constant. [12] and [13] gave a general definition of nonlinear integer programming problem. It can be stated as: max/min subject to f (x) hi (x) = 0, i  I = 1, ..., p gj (x)  0, j  J = 1, ..., q x  Zn

(4)

where x is a vector of decision variables, and some of the constraints hi , gj : Zn  R or the objective function f : Zn  R are non-linear functions. In equation (2), let g1 (x, b) = Lj (x, b) - max, g2 (x, b) = E (x, b) - , h1 (x, b, y ) = Lj (x, b) - min - y = 0, y  0, y  Z. It can be transformed to: minimize
x,b

 (L1 (x, b), ..., Lm (x, b)) g1 (x, b)  0 g2 (x, b)  0 h1 (x, b, y ) = 0 xi  {0, 1}, i = 1, ..., n bi  B, i = 1, ..., n y  0, y  Z (5)

subject to

Since the equivalent version of equation (2) equation (5) satisfies the format of equation (4) where the objective function and partial constrains(g1 (x, b) and h1 (x, b, y )) are nonlinear, our problem belongs to nonlinear integer programming problem. According to [12], it is NP-hard. Therefore, any polynomial time computed solution would be an approximation.  To find the best setting, a naive approach is to try all 2n+n positions for n + n switches, which is unacceptable due to the time complexity. Therefore, we propose a heuristic algorithm for computing an approximate optimal combination of blind and light settings. This paper focuses on the system control during the day. In the evening when sunlight doesn't exist, artificial lighting becomes the only lighting source and the detailed control approaches can be referred to the method described in [14]. Conceptually, for better saving energy, during the day, daylight will be considered first to provide illuminance. Artificial lights will be used to even and compensate the lighting when necessary. Detailed control approaches will be discussed in section IV. IV. C ONTROL A PPROACHES A. Calibration In an earlier publication[14] we described the calibration procedure for artificial lights. The calibration of the blinds follow a similar method and hence we outline it. While there is no daylight, artificial lights are calibrated by turning on one switch at a time and noting the sensor readings or sensors where this light has impact. From this data, we compute zones (or rooms where each light is located). From the impact measures, we are then, heuristically able to decide which light switches should be on for the lighting to be even, even when there is daylighting. In this section, we describe the calibration steps for eleci trical blinds, which is used to calculate Impactb ij (a.k.a. blind i's impact on sensor j at angle bi ). We assume there is no any other lighting source involved except daylight at this stage. Specifically, suppose there are n blinds in an area, each blind has k settings. The process of calibration is: 1) Turn off all blinds and check existing values on each sensor 2) Turn on one blind up to a particular angle at a time 3) Calculate the blind's impact on each sensor(sensor value - existing value) 4) Repeat step 2 n × k times until all settings are counted It is clear that the above calibration steps run in linear time(O(kn )) and the space complexity is (O(kmn )). Compared to the exhaustive search solution which runs and records all the combinations of blind settings(complexity is  O(k n )), our calibration largely saves both time and space complexities. All impact data will be stored in database for further processing. B. System Control Workflow The goal is to compute desired blind and light settings for a time point T . The proposed lighting control approach contains

three main stages: blind prediction stage, blind adjustment stage and artificial lighting control stage. Blind prediction will be initiated first to compute predicted blind settings based on predicted calibration datasets. Predicted settings will be passed into blind adjustment stage for adjusting electrical blinds in real time. Artificial lighting will be used when inside lighting level is either not sufficient or not distributed evenly. The system relies on the calibration datasets stored in database. Those datasets will be passed into data collection and modeling server for further processing. This server is doing some preliminary work, for example, classfication. In this paper, we assume that the historic data used for blind settings computation at particular day has to be picked up from the category of that day. For example, computations in sunny days use the historic data from sunny days. To build a more precise computational model, the system also collects some conditions from outside world such as comfort conditions. New generated results(settings) will be put into database, which are collected by the system itself for better learning scale factors and creating predicted datasets. The system therefore can be viewed as a learning-based closedloop control system, which will become more robust and precise when dataset is becoming larger. Figure 1 reflects the system control workflow. Detailed methodologies operated at three main stages are discussed below. C. Blind-Prediction Blind prediction relies on the predicted calibration datasets generated by data collection and modeling part. It applies a scale factor(offset) onto the historic calibrated data for similar days and produces the one for blind prediction use. 1) Count the minimum number of blinds needed to be bi turned on: suppose C = {Ci }, i  [1, n ] where
x bi = arg max Ci x Ci = m  j =1 x B

Impactx ij · f (x)

(6)

To reach this goal, simply find minimum number of elements in the sorted array C such that sum of them are greater or equal than lowerbound(m × min). 2) Base-level candidates are computed: From step 1, it is known at least w blinds is needed. Therefore, each base-level candidate setting should have at least w elements selected to be turned on, and the total contribution on sensors under each candidate should be greater or equal than lowerbound value. To solve the problem, it needs w iterations. At each iteration, an unselected blind is picked up whose setting(i, bi ) is generated based on: lowerbound - existing , iteration  [0, w - 1] w - iteration

bi Ci >=

Comfort Conditions User Wishes Energy Consumption

system inputs

Data Collection & Modeling

database
calibration datasets

predicted calibration datasets

Blind Prediction

predictive values
Fig. 1.

Blind Adjustment
System Control Workflow

approximate optimal values

Artificial Lighting Control

3) More candidates are generated from base-level ones: Since adding  blinds to compute needs O(n ) time, to ensure low response time, we set  = 2. Suppose each base-level candidate has w elements which have non-zero settings, there are n blinds in total, each blind has k settings. Then if each candidate wants to pick up an unselected blind, there would be k (n - w) choices. If there are s base-level candidates, finally there would be s × k × (n - w) new candidates that have w + 1 elements which have non-zero settings. Similarly when another unselected blind is trying to be picked up, total amount of candidates that have w + 2 elements which have non-zero settings is becoming s × k × (n - w) × k × (n - w - 1). Thus total number of candidates would be s + s × k × (n - w) + s × k × (n - w) × k × (n - w - 1). Note for each candidate  bi q, Cp  upperbound(m × max) where Cp = Ci and
pq,

expectation) only. Algorithm 1 describes the steps of blind adjustment. Specially, setting denotes the predicted blind setting generated by blind prediction stage, lur denotes the real sensor readings under setting , lup denotes the predicted sensor readings under setting . With the blind-adjustment step, the system is able to adapt the environmental changes more quickly and make corresponding adjustments more properly compared to a pure learning system. Algorithm 1 Blind Adjustment
Input: setting , lur , lup Output: new blind settings 1: scale =  × (lur / lup ) 2: for (i, bi )  setting do x x 3: max = maxxB Ci × scale, min = minxB Ci × scale bi bi 4: Ci = Ci × scale bi 5: dif fmax  |Ci - max| bi 6: dif fmin  |Ci - min| 7: end for 8: if lur < lowerbound then 9: choose values from dif fmax , sum of the values  offset 10: else 11: if lur > upperbound then 12: choose values from dif fmin , sum of the values  offset 13: end if 14: end if 15: adjust blind angles based on the chosen values

p = (i, bi ). 4) Standard deviation of sensor readings generated by each candidate is calculated: According to equation 3, for each candidate c, we are able to calculate its impact on sensor j Lj (c)(note there is no artificial lighting at this point). Then it is easy to know the standard deviation of all sensors' readings under c. The candidate that generates the lowest standard deviation of sensor readings would be selected as the final blind setting, at the blind prediction stage. Suppose there are n blinds in total. Since step 1 has a time complexity O(n ), step 2, step 3 and step 4 all have a time complexity O(n2 ), Blind-Prediction has a time complexity O(n2 ). Compared to feedback system, our approach predicts and finds the approximate blind settings in a very short time without having physical blind movements, which results in more satisfactory feelings to users. D. Blind-Adjustment Since the calibration data used in the prediction stage is predicted based on historical data, there exists errors at the prediction stage. The adjustment step is added into the control plan to reduce the offset in real time. To minimally reduce the number of blind movements, each blind(if selected) will be adjusted to maximum impact angle(when lighting is not sufficient) or minimum impact angle(when lighting is beyond

E. Artificial Lighting Involved into the System The biggest advantage of involving blind control into the system is saving lighting resources. However, artificial lighting cannot be fully ignored in some circumstances. When sunlight is not sufficient, artificial lighting is required for completing the inside lighting levels. When there are variations among sensor readings, artificial lighting is needed to reduce them. According to the objective function described in section III, the selection of value  introduced in equation 2 needs to be adjusted based on real environments. Since computational time is important and low time complexity is desired,  is always set to be minimum required lighting + 2 artificial lights(unless there is a user-specific requirement), and thus the total time complexity would be no more than O(N 2 )(N is number of

updated data

actuators). The extension work described in [14] detailedly describes how to adjust artificial lights to get a more even and balanced environment. V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS A. Implementation Details To demonstrate the feasibility and effectiveness of our approach, we used an experimental setup. A 8ft × 8ft test cell was instrumented with 9 lights (15W incandescent, 120V ), two automated blinds and 9 sensors connected via a WSN and actuators to a computer [14]. The blinds are placed on two windows and communicate with the control server through serial port. They can be controlled by slave commands sent from the server. Each blind can be adjusted from 0 degree(fully off) to 90 degree(fully on). Blind control programs are written in Java. B. Feasibility The experiment is run during the day. It is to verify the feasibility of our proposed lighting control algorithm. Control strategies are based on the contents introduced in section IV. One special sensor is put on the window to detect the offset (scale) between the experimental day's sunlight level and the previous days. Blinds' impacts on sensors at previous days are stored in the database. By applying the scale factor, a predicted calibration dataset is obtained for blind prediction. Min value is set 100. Max value is set 130. To check the difference between our computed results and optimal ones, we first run our proposed method at 8 am. Then with the same configurations, we run a brute force algorithm checking all combinations of all actuators using real data rather than predicted ones. Sensor readings for both experiments are recorded. We repeat this experiment at other time points like 10am, 12pm, 2pm and 4pm. The results are shown in Figure 2. From the results we can see compared to the brute force, the proposed approach has a similar light intensity performance and a very short increase on standard deviation. C. Adaptivity and Scalability Our previous paper [14] has shown the artificial lighting control part satisfies the adaptivity and scalability features, that is when either pattern changes or amount of lights is increased, the system is still able to compute settings in a reasonable time. Now we need to study these two features for the blind control part of the lighting control system. To this end, we perform simulations on a more complex, synthetic setup, with randomly generated impact values. Suppose there are n blinds, n lights, m sensors, each blind has k settings. To match the scenario in the previous real experiment, we set n = 9, m = 9 and k = 7. Historic calibration dataset A will be given in the following way: to each blind, every setting's impact on a specific sensor m is randomly given an integer value from 1 to 100. The scale factor  is set to be a random decimal value from 1 to 2, which matches the factor used in our real experiments. The

TABLE I L IGHTING L EVEL C OMPARISON BETWEEN B RUTE F ORCE M ETHOD O UR P ROPOSED M ETHOD (LUX)

AND

# of Blinds 8 9 10 11 12 13 14

Min 175 191 219 229 246 277 298

Max 306 298 350 333 369 383 426

Brute Force 181 260 292 309 296 298 365

Proposed Method 294 269 236 318 340 364 403

real data is also derived from the dataset A, with a scale value in the range of 1 to 2 as well. Existing illumination level on each sensor is randomly set from 1 to 50. If blind i's total impact on sensors is blindi , then the average impact of a blind n  blindi  is /n . lowerbound is selected as n /2 × average k i=1 impact, upperbound is selected as (n /2 + 3) × average impact. Each artificial light's impact on sensor is randomly set between 1 to 100, same to the configurations described in [14]. We did 7 groups of simulation experiments with an increased number of blinds. We compared the results getting from brute force with our approach. TABLE I describes the average lighting levels generated by brute force method and our proposed method for each simulation experiment. It indicates settings computed by our proposed lighting control algorithm can generate the impact values that fit into the acceptable range. Figure 3 shows the standard deviation and computational time results for brute force method and our proposed method respectively. From the results, we can see the standard deviation of the proposed method is nearly 1.5 - 2 times as compared with the optimal solutions while computational time is far less than the latter one. Compared to the brute force, the proposed approach has an extremely better time performance with a similar light intensity performance and a reasonable increase on standard deviation. In other words, the blind control part of the lighting control system satisfies the adaptivity and scalability features. VI. C ONCLUSION AND F UTURE W ORK To enable automated lighting control, under varying conditions of occupancy, weather, seasons and other lighting influences it is essential to have a complete system that is effective and adaptive. Such system must be deployable in a simple, cost effective system without the need for customizations and reprogramming as conditions change. It also needs to create a comfortable environments with energy savings as a goal. This paper presents such a complete core system that contains both daylight harvesting module and artificial lighting control module. It also tests the feasibility and effectiveness in both experimental and simulated scenarios. The underlying system can be deployed in buildings and homes without

130

40

Light Intensity (Lux)

Standard Deviation

125 120 115 110 105 100 95 90 8am 9am 10am 11am 12pm 1pm 2pm 3pm 4pm

35 30 25 20 15 10 5 0 8am 9am 10am 11am 12pm 1pm 2pm 3pm 4pm

brute force our approach

brute force our approach

Time
(a) Light Intensity Fig. 2.
30

Time
(b) Standard Deviation Evaluation of System Feasibility
5 4 3

Standard Deviation

25

brute force our approach

20

log10 Time
brute force our approach
8 9 10 11 12 13 14

2 1 0 -1 -2 8 9 10 11 12 13 14

15

10

5

0

# Of Blinds
(a) Standard Deviation Fig. 3.

# of Blinds
(b) Computational Time Evaluation of System Adaptivity and Scalability

excessive costs. The lighting control problem is built using non-linear integer programming model, which is NP-hard. A heuristic algorithm is proposed to solve the problem to compute approximate optimal solutions. Future improvements to the lighting control system include the investigation of data classification, sensor placement and scale factor learning. Current data classification only consider seasons (such as summer or winter) or weather (such as sunny or cloudy) conditions. More conditions such as daylight illumination level, geographical information need to be collected for better classification. With the use of the system, more data will be collected in the database. These data can be used to better predict the scale factors. The results will be applied into the current system control plan. When dataset is made larger, the system will become more robust and precise. R EFERENCES
[1] S. Matta and S. Mahmud, "An intelligent light control system for power saving," in IECON 2010-36th Annual Conference on IEEE Industrial Electronics Society. IEEE, 2010, pp. 3316­3321. [2] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews, "Intelligent light control using sensor networks," in Proceedings of the 3rd international conference on Embedded networked sensor systems. ACM, 2005, pp. 218­229. [3] P. Ihm, A. Nemri, and M. Krarti, "Estimation of lighting energy savings from daylighting," Building and Environment, vol. 44, no. 3, pp. 509­ 514, 2009. [4] J. Lu and K. Whitehouse, "Suncast: fine-grained prediction of natural sunlight levels for improved daylight harvesting," in Proceedings of the 11th international conference on Information Processing in Sensor Networks. ACM, 2012, pp. 245­256.

[5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, "Wireless sensor networks for habitat monitoring," in Proceedings of the 1st ACM international workshop on Wireless sensor networks and applications. ACM, 2002, pp. 88­97. [6] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection tree protocol," in Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM, 2009, pp. 1­14. [7] C. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, "Design of a wireless sensor network based automatic light controller in theater arts," in Engineering of Computer-Based Systems, 2007. ECBS'07. 14th Annual IEEE International Conference and Workshops on the. IEEE, 2007, pp. 161­170. [8] H. Park, J. Burke, and M. B. Srivastava, "Design and implementation of a wireless sensor network for intelligent light control," in Proceedings of the 6th international conference on Information processing in sensor networks. ACM, 2007, pp. 370­379. [9] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic, E. Field, and K. Whitehouse, "The smart thermostat: using occupancy sensors to save energy in homes," in Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems. ACM, 2010, pp. 211­224. [10] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon, "Intelligent lighting control using sensor networks," in Networking, Sensing and Control (ICNSC), 2013 10th IEEE International Conference on. IEEE, 2013, pp. 170­175. [11] M. Miki, A. Amamiya, and T. Hiroyasu, "Distributed optimal control of lighting based on stochastic hill climbing method with variable neighborhood," in Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference on. IEEE, 2007, pp. 1676­1680. [12] R. Hemmecke, M. K¨ oppe, J. Lee, and R. Weismantel, "Nonlinear integer programming," arXiv preprint arXiv:0906.5171, 2009. [13] Wikipedia. Nonlinear programming. [14] Y. Wang and P. Dasgupta, "Designing adaptive lighting control algorithms for smart buildings and homes," in Networking, Sensing and Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE, 2014, pp. 279­284.

