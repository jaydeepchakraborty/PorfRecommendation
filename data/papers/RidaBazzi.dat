REDUCED-DELAY SELECTIVE ARQ FOR LOW BIT-RATE IMAGE AND MULTIMEDIA
DATA TRANSMISSION
Tuyet-Trang Lam1 , Lina J. Karam1 , Rida A. Bazzi2, Glen P. Abousleman3
1

Electrical Engineering Dept., Arizona State University, Tempe, AZ 85287-5706.
Computer Science & Engineering Dept., Arizona State University, Tempe, AZ 85287-5406.
3
Compression, Comm. & Intelligence Lab, General Dynamics C4 Systems, Scottsdale, AZ 85257.
lamsnow@asu.edu, karam@asu.edu, bazzi@asu.edu, glen.abousleman@gdds.com
2

ABSTRACT
This paper presents a reduced-delay selective-ARQ scheme
based on a similarity check function that measures the degree of corruption in transmitted multimedia data packets.
Accordingly, if the packet is found to be badly corrupted
based on a speciﬁed similarity criteria, it can be considered
as a lost packet and retransmitted. The degree of corruption contributed by a particular packet is measured by the
proposed similarity check function at the receiver without
explicit knowledge of the original source data. Simulation
results and comparisons with a conventional ARQ scheme
are provided to illustrate the performance of the proposed
method.
1. INTRODUCTION
In typical data transmission systems, the original source
ﬁle is compressed, packetized, and transmitted through a
noisy channel. Because bit corruption can cause a catastrophic loss of information, the packets are protected with
channel codes, and error detection is performed on each
packet at the receiver. If a feedback channel is available
at the decoder, retransmission requests can be provided by
automatic-repeat request (ARQ) and hybrid-ARQ protocols
(H-ARQ) [1] at the cost of a reduction in throughput. However, existing error detection schemes do not provide information about the effects of a corrupted packet on the quality of the reconstructed data. Bit errors on signiﬁcant bits
may propagate throughout the reconstructed data, while bit
errors on less signiﬁcant bits may not present visible distortions.
A scheme has been proposed in [2] for selective error
recovery of DCT block-based image compression. A ”side
match test” consisting of computing the mean squared error between pixels in adjacent blocks has been proposed to
detect block errors that causes serious image degradation.
This test combined with sending the block length shows an
increase of channel throughput compared to a conventional

0-7803-8874-7/05/$20.00 ©2005 IEEE

ARQ scheme. However, the method of [2] is not suitable
for wavelet-based and other non-block based coders.
In this paper, a more general framework based on similarity check functions is presented. In our proposed selectiveARQ (S-ARQ) scheme, rather than directly retransmitting
all erroneous packets, the degree of corruption is measured
using a similarity check function that does not require explicit knowledge of the original data at the receiver. Accordingly, if the packet is found to be badly corrupted based
on a speciﬁed similarity criteria, it can be considered as a
lost packet and recovered using ARQ or hybrid-ARQ. If the
applied similarity check function indicates that the degree of
corruption of the considered packet is acceptable, the packet
is still usable and no retransmission is needed. In this way,
the total amount of information needed to be retransmitted
can be decreased and, consequently the delay due to retransmission is decreased and more bits can be allocated to the
source coding for enhanced quality.
This paper is organized as follows. Section 2 describes
the new concept of the similarity check function. This function can be used to intelligently retransmit data for packets that most need to be corrected rather than retransmitting data for all corrupted packets. Section 3 illustrates the
proposed selective-ARQ scheme using several examples of
similarity check function designs applied with the waveletbased TCQ image coder presented in [3]. A conclusion is
given in Section 4.
2. SIMILARITY CHECK FUNCTION
The proposed concept of a similarity check function is motivated by the use of hash functions to provide fault tolerance and security in a distributed storage environment [4].
A hash is computed to uniquely represent a ﬁle (or a piece
of a ﬁle) using only a few bits. It can be thought of as a
digital ﬁngerprint of a larger document [5]. Because the
hash is much smaller than the original ﬁle, it is more efﬁcient to fully protect the hash than the corresponding ﬁle.

II - 309

ICASSP 2005

(a) No noise, PSNR = 29.88 dB

(b) BSC with BER= 0.001, PSNR= 27.34 dB

Fig. 1. 512 × 512 Lena image coded at 0.246 bpp.
The protected hash is sent along with the ﬁle. When the ﬁle
is received, a new hash is computed from that ﬁle. If the
received ﬁle is identical to the original ﬁle, the two hashes
will match. If the hashes do not match, the received ﬁle
is discarded. Similarly, our similarity check function is designed such that, when applied to a piece of data (or packet),
it results in an output (i.e., an index) that can be represented with an insigniﬁcant number of bits relative to the
original piece of data, without the need to have the original piece of data. However, while a hash can only indicate
whether there is an exact match between the received and
transmitted pieces of data, and thus, cannot provide information about the amount of corruption introduced in a ﬁle,
the proposed similarity check function is designed to measure the degree of corruption introduced. For multimedia
applications, a ﬁle does not need to be exactly identical to
the original data to be usable, which motivates the proposed
similarity check function general framework. Note that a
CRC code or a hash function can be considered as a special
similarity check function with the similarity criteria set to
be the identity function.
Let the original data be transmitted (or stored) as N packets (or pieces), p1 , · · · , pN , each of size L. Let p1 · · · , pN
be the corresponding received packets. Let P denote the set
containing all packets (data pieces) of size L. The similarity
check function, S, is deﬁned as a mapping from the set P to
the set I = {I1 , · · · , Ir } as follows:
S(p ∈ P ) = Il , l ∈ {1, · · · , r},

512 × 512 Lena image compressed with the wavelet-based
TCQ image coder of [3] at 0.25 bits per pixel (bpp) and
transmitted through an ideal channel with no noise (Fig. 1a),
and through the binary symmetric channel (BSC) with bit
error rate (BER), Pb = 0.001 (Fig. 1b). For this image, 71
bits have been corrupted, resulting in 65 out of 256 (25.4%)
corrupted packets. Despite the fact that the PSNR for the
image in Fig. 1b is 2 dB lower than the image in Fig. 1a,
both images are very similar. Thus, for multimedia applications, it would be wasteful to discard the image in Fig. 1b
because it is not identical to the image in Fig. 1a.
The similarity check function can be used to estimate
the amount of protection needed to receive acceptable data.
In the proposed selective-ARQ scheme, the similarity check
function is used to evaluate the number of packets to be protected. Then, rather than correcting all corrupted packets,
only packets that result in a high degree of degradation will
be corrected using corresponding retransmitted data. In this
S-ARQ method, the decoded data will not be exactly the
same as the transmitted data, but annoying noise artifacts
can be corrected. Note that, if not enough bits are assigned
to fully recover all severely corrupted or lost packets, the
similarity check function can also be designed to provide
usable information to correct a portion of the lost packets.
3. CODING EXAMPLE USING A SIMILARITY
CHECK FUNCTION AND SELECTIVE ARQ

(1)

where Il in (1) is referred to as the similarity check index.
Note that only c = log2 (r) bits are needed to represent elements in the set I, and c is selected to be very small compared to the packet size. Two packets, pk and pk , are considered similar if and only if S(pk ) = S(pk ).
The similarity check function can be designed based on
MSE or perceptual criteria. For example, Fig. 1 shows the

In the following coding example, the wavelet-based TCQ
coder of [3] is used to code 512 × 512 images, which are
then transmitted through the BSC with a bit error rate, Pb
= 0.001. The coder in [3] has been modiﬁed to perform a
4-level dyadic wavelet decomposition to simplify the packetization.
Let P = {pi }, i = 1, · · · , N , be the set of all packets that represent the data. As shown in Fig. 2, each packet

II - 310

LL−band

2. Send the resulting k-bit similarity check indices, Ipi ,
to the receiver along with the data packets. Note that
k is selected to be very small compared to the packet
size. So, these constitute an insigniﬁcant portion of
the bit budget and can be protected fully without signiﬁcant added overhead.

(l=2,o = 2)

(l=3,o =1)
(l=2,o =1)

(l=1,o =2)

(l =1,o =1)

3. At the receiver, for each received packet, pi , compute
the similarity check indices Sk (pi ) = Ipi as in (2),
and compare with the corresponding transmitted similarity check index, Sk (pi ) = Ipi . If Sk (pi ) = Ipi ,
then pi is requested for retransmission using ARQ or
H-ARQ; otherwise, pi is kept and used to reconstruct
the image.

(l =1,o =3)

Fig. 2. Packetization of wavelet coefﬁcients for a 13 subband decomposition. (l,o) denotes the level and orientation
of a child subband.
pi consists of 4 quantized wavelet coefﬁcients from the LL
subband (bi,4,0 ), and its corresponding children subbands
bi,l,o , where l = 1, · · · , 4, and o = 1, 2, 3, denote, respectively, the level and orientation of a child subband. To limit
the propagation of bit errors across packets, the TCQ quantizer is re-initialized after coding the LL subband of each
packet. The above packetization results in N = 256 packets of length L = 250 bits for a 512 × 512 image.
A similarity check function is designed to measure the
degree of corruption in perceptually signiﬁcant subbands including the LL subband and k of its corresponding children
subbands bi,l,o , where l = (4 − (k div 3) + 1) to 4, and
o = 1, 2, 3, for all levels l except for the lowest l for which
o = 1 to ((k−1) mod 3)+1). The parameter k can be varied depending on the noisy conditions of the channel and the
available bit budget. The similarity check function Sk (.)
evaluated for a packet pi ∈ P can be deﬁned as a vector of
indices Sk (pi ) = Ipi = {Ipi ,0 , Ipi ,1 · · · , Ipi ,k }, where Ipi ,0
represents the variability of the 4 LL-coefﬁcients coded in
packet pi forming the block bi,4,0 and Ipi ,j , 1 ≤ j ≤ k,
represents the variability of the wavelet coefﬁcients in the
corresponding children subband bi,l,o . Each Ipi ,j is computed as follows:

1, if maxx∈bi,l,o (|x − µi,l,o |) > T
Ipi ,j =
(2)
0, otherwise,
where T represents a threshold, µi,l,o is the mean of the
wavelet coefﬁcients in the child subband bi,l,o corresponding to packet pi , and j = 3(4 − l) + o . In our implementation, T = 1 since, in the coder [3], all coefﬁcients are
normalized to have a variance of 1.
The proposed S-ARQ-based joint source-channel coding scheme proceeds as follows:
1. At the transmitter side, for each packet, pi ,
compute the similarity check function Sk (pi ) =
{Ipi ,0 , · · · , Ipi ,k }, Ipi ,j ∈ {0, 1}, as in (2). This will
result in a k-bit output Ipi .

In addition, the minimum and maximum values of the
entire LL-subband are sent along with the data using few
bits and can be used to detect coefﬁcients lying outside of
the acceptable range.
Table 1 presents the performance results averaged over
50 simulations using the proposed S-ARQ scheme with the
similarity check function Sk for different values of k =
0, 3, 6. For comparison, Table 1 also shows results using
a conventional ARQ scheme with a 16-bit CRC (CRC16 ARQ) that is applied to fully protect the entire transmitted
data, and also with an 8-bit CRC (CRC8 -ARQ-LL) that is
applied to fully protect only the most signiﬁcant LL subband. In Table 1, the number of corrected packets (column
3) is the average number of packets that were detected to
be unusable and are retransmitted. The number of parity
bits (column 2) is the number of bits that are appended to
each packet by the corresponding similarity check function.
Fig. 3 shows that a very good image quality can be obtained
by only retransmitting a fraction of the corrupted packets.
S0 was able to detect the most annoying artifacts due to
errors occurring in the LL-subband, whereas using CRC8 ARQ-LL doubles the number of packets requested for retransmission without improving signiﬁcantly the quality of
the corrected image (Fig. 3c). In comparison, Fig. 3d shows
that, for the same number of retransmitted packets as in
Fig. 3c, the proposed S-ARQ scheme results in an improved
performance in terms of both PSNR and visual quality by
protecting perceptually relevant data in higher subbands.
Figs. 3d and 3e show that the high frequency noise artifacts
decrease as more perceptually signiﬁcant subbands are protected by the proposed similarity check function, while the
number of retransmission is still signiﬁcantly lower than the
conventional ARQ scheme (Fig. 3f).
4. CONCLUSION
In this paper, we have presented a selective-ARQ scheme
based on the concept of a similarity check function that
measures the amount of corruption introduced in an image

II - 311

(a) Not protected, PSNR = 26.93
check bits per packet : 0 (0%)
Retransmitted packet: 0 (0%)

(b)S0 -ARQ, PSNR = 29.58
check bits per packet : 1 (0.4%)
Retransmitted packets: 4 (1.6%)

(c)CRC8 -ARQ-LL, PSNR= 29.61
check bits per packet: 8 (3.2%)
Retransmitted packets: 10 (3.9%)

(d) S3 -ARQ, PSNR = 29.63
check bits per packet: 4 (1.6%)
Retransmitted packets: 10 (3.9%)

(e) S6 -ARQ, PSNR = 29.69
check bits per packet: 7 (2.8%)
Retransmitted packets: 28 (10.9%)

(f) CRC16 -ARQ, PSNR = 29.82
check bits per packet: 16 (6.4%)
Retransmitted packets: 65 (25.4%)

Fig. 3. PSNR (in dB) of 512 × 512 Lena source coded at 0.242 bpp over the BSC channel with a bit error rate, Pb = 0.001.

Table 1. 512 × 512 Lena image coded at 0.242 bpp using
the wavelet-based TCQ coder in [3], ber=0.001.
similarity parity
packets
PSNR
function
bits
corrected in dB
None
0
0
28.12
S0
1
2.0
29.31
CRC8
8
6.97
29.41
S3
4
6.08
29.47
S6
7
18.32
29.51
CRC16
16
58.1
29.82

without explicit knowledge of the original data at the receiver. In the proposed low-delay S-ARQ scheme, not all
corrupted packets need to be corrected and retransmitted;
only those corrupted packets that results in perceptually annoying artifacts are detected by the similarity check function and retransmitted. Compared to the conventional ARQ

scheme, the proposed method results in a reduced delay and
improved performance due to a lower retransmission rate.
5. REFERENCES
[1] S. Wicker, Error control systems for digital communication
and storage, Prentice Hall PTR, 1995.
[2] N. Matoba, Y. Kondo, and T. Tanaka, “Still image transmission using ARQ over Rayleigh fading channels,” IEE Electronics Letters, vol. 32, no. 9, pp. 803–804, Apr. 1996.
[3] T.-T. Lam, G.P. Abousleman, and L.J. Karam, “Image coding with robust channel-optimized trellis-coded quantization,”
IEEE Journal on Selected Areas in Communications, vol. 18,
pp. 940–951, June 2000.
[4] D. MacKay, Information theory, Inference and learning algorithms, chapter 12, Cambridge University Press, 2003.
[5] R. Rivest, “The MD5 message digest algorithm,” Internet RFC
1321, Apr. 1992.

II - 312

Distrib. Comput. (1997) 10: 117—127

On the use of registers in achieving wait-free consensus*
Rida A. Bazzi1, Gil Neiger2, Gary L. Peterson3
1 Computer Science and Engineering Department, College of Engineering and Applied Sciences, Arizona State University,
P.O. Box 875406, Tempe, AZ 85287-5406, USA
2 Intel Corporation, JF3-359, 2111 N.E. 25th Avenue, Hillsboro, OR 97124-5964, USA
3 Computer and Information Science Program, Spelman College, 350 Spelman Lane SW, P.O. Box 333, Atlanta, GA 30314-0339, USA

Summary. The computational power of concurrent data
types has been the focus of much recent research. Herlihy
showed that such power may be measured by the type’s
ability to implement wait-free consensus. Jayanti argued
that this ability could be measured in different ways, depending, for example, on whether or not read/write registers could be used in an implementation. He demonstrated
the significance of this distinction by exhibiting a nondeterministic type whose ability to implement consensus
was increased with the availability of registers. We show
that registers cannot increase the ability to implement
wait-free consensus of any deterministic type or of any
type that can, without them, implement consensus for at
least two processes. These results significantly impact the
study of the wait-free hierarchies of concurrent data types.
In particular, the combination of these results with other
recent work suggests that Jayanti’s h hierarchy is robust
.
for certain classes of deterministic types.
Key words: Registers — Consensus — Wait-free computation — Wait-free hierarchies — Robustness

1 Introduction
Achieving consensus in the presence of process failures is
of fundamental importance in distributed computing.
A large body of research has studied algorithms for achieving consensus in three domains: (1) synchronous messagepassing systems, (2) asynchronous message-passing systems, and (3) asynchronous read/write memory systems.
While the first domain has produced a large number of
deterministic algorithms, it has been shown that such

* The first two authors were supported in part by the National
Science Foundation under grants CCR-9106627 and CCR-9301454.
The second author was supported in part by a scholarship from the
Hariri Foundation. The third author was supported in part by the
W.F. Kellogg Foundation under a grant to the Center for Scientific
Applications of Mathematics at Spelman College
Correspondence to: G. Neiger

algorithms do not exist in the other two [6, 8, 10, 11, 20].
Because of these results, researchers also consider algorithms for consensus in asynchronous shared-object systems with primitives more powerful than simple reads and
writes [1, 2, 7, 11, 14—18, 20, 24, 26].
Another reason for taking this approach stems from
the study of wait-free implementations of concurrent data
types. Here, researchers ask questions such as the following: ‘‘is there a wait-free implementation of type T using
1
objects of type T ?’’ A concurrent implementation of
2
a data type is wait-free if any process can complete any
operation of the implementation in a finite number of its
own steps regardless of the behavior and speed of other
processes. Wait-free implementations are desirable in
asynchronous systems because they prevent slow processes from slowing down faster ones. In addition, they can
tolerate any number of stopping failures. Herlihy [11]
showed a direct connection between a type’s ability to
implement wait-free consensus (i.e., provide an implementation of consensus that is wait-free) and its ability to
provide wait-free implementations of other types. In particular, he showed that consensus is universal: for any
n'0, if type T can implement wait-free consensus in
systems with n processes, then T can provide a wait-free
implementation of any type in such systems. In light of this
result, Herlihy evaluated the power of a data type by
assigning it a consensus number; this is the maximum
number of processes for which the type can be used to
implement wait-free consensus. He thus cast the universe
of concurrent data types into a hierarchy, each level of
which contains types with a particular consensus number.
Jayanti [14] refined this study by asking the following
question: what does it mean to say that a type can implement wait-free consensus? He argued that an answer required addressing the following questions.
1. Can more than one object of the type be used in the
implementation?
2. Can read/write registers (also called read/write
memory) also be used in the implementation?
Because these questions can be answered together in four
different ways, Jayanti identified four possible hierarchies
of types, one of which corresponds to Herlihy’s assignment

118

of consensus numbers (answering ‘‘no’’ to question 1 and
‘‘yes’’ to 2). He called these h , h3 , h , and h3 . A subscript
1 1 .
.
‘‘1’’ indicates that only one object of a type can be used,
while a subscript ‘‘m’’ indicates that many can be used.
A superscript ‘‘r’’ indicates that registers may be used,
while its absence indicates that they may not. Jayanti
indicated that Herlihy’s hierarchy is h3 .1
1
Given these four hierarchies, Jayanti naturally asked if
they were distinct and, if so, which best measured the
computational power of different data types. He argued
that a hierarchy does not properly measure this power if it
is not robust. Informally, a hierarchy is robust if no collection of types at low levels can implement a type at a higher
level. Jayanti showed that none of h , h3 , and h could be
1 1
.
robust if it were not equal to h3 . He then showed that both
.
h3 and h were different from h3 , proving that only
1
.
.
h3 might be robust (h cannot equal h3 if either h or
.
1
.
.
h3 does not). Jayanti left the robustness of h3 as an open
1
.
question.
Recall that Jayanti’s hierarchy h was defined by an.
swering ‘‘yes’’ to question 1 above and ‘‘no’’ to question 2;
3
it differs from h on whether or not registers may be used
.
in implementations of consensus. Jayanti proved h 9h3
.
.
(and h to not be robust) by exhibiting a type that was at
.
different levels in the two hierarchies. This type was specified nondeterministically; that is, there is (at least) one
sequence of operations on the type for which more than
one behavior is possible. This raises an obvious question:
can the same result be shown with a deterministic type?
Since most commonly used concurrent data types are
deterministic, a positive answer to this question would
imply that the non-robustess of h would hold even for the
.
restricted class of deterministic types.
We answer this question negatively. That is, we show
that the two hierarchies give equal values for any deterministic type. Thus, the nondeterminism used by Jayanti
is necessary. We also demonstrate other results relevant
to the use of registers in implementing wait-free consensus.
For all types (even nondeterministic ones), the two
hierarchies can differ only at the first level: if h assigns
.
a type a value greater than 1, then h3 assigns it the same
.
value.
These results confirm that, in most cases, registers do
not play a special role in achieving wait-free consensus.
Other papers [3, 24] have claimed h3 to be robust for
.
certain classes of deterministic types. Combined with the
results of this paper, those results would also imply that
h is also robust for these types.
.
Our results are proven through the introduction of
a new concurrent data type called the one-use bit.A
n
object of this type is a bit that can be read at most once and
written at most once. Our main results stem from the
following facts.
— A finite number of one-use bits can implement
a read/write register in a wait-free implementation of
consensus (this is shown in Sect. 4).
1 The reader should note that, while h3 matches Herlihy’s definitions,
1
his impossibility proofs apply to h3 . Because Jayanti’s work was the
.
first to demonstrate types requiring multiple objects to solve consensus, the distinction was not made earlier

— Almost any type can be used to implement a one-use bit
(this is shown in Sect. 5).
These results show that almost any type can be used to
implement read/write registers in a wait-free implementation of consensus. Thus, the availability of registers does
not increase the ability of such a type to implement consensus if one is allowed multiple objects of the type. The
types that cannot implement one-use bits are so weak that
they cannot implement consensus with or without the aid
of registers.

2 Background
This section presents the definitions and background material necessary to present and interpret the results of this
paper.
2.1 Types
We define a concurrent data type with an automata-based
definition. Processes interact with an object of a type by
invoking accesses on the object’s ports and receiving responses on those ports. This section presents formal definitions of these concepts.
A type is a 5-tuple T"Sn, Q, I, R, dT. The components
are n, the number of ports the type has (this limits the
number of processes that may access the type); Q, a (possibly infinite) set of states; I, a set of access invocations; R,
a set of access responses; and d, a transition function. Such
a type is called an n-ported type.An object of type T (also
called a T-object) is an instance of T that specifies, for each
port, which processes (if any) access the object through
that port. Let N be M1, 2,2, nN. Type T may be either
n
deterministic, in which case d : Q]N ]I > Q]R,or nonn
deterministic, in which case d : Q]N ]I > 2 Q]R. In keepn
ing with traditional automata theory [13], we assume that,
for any nondeterministic type T, d(q, j, i) is finite for all q, j,
and i. This specification of a type indicates how processes
may access an object of type T (using invocations in I),
how the object communicates to processes (using responses in R), and what are the legal sequential histories of
the type (specified by d). If an object of type T is in state
q when invocation i 3 I appears on port j 3 N , then the
n
object changes to state q@ and returns response r over port
j if and only if Sq@, rT"d(q, j, i) (if T is deterministic) or
Sq@, rT 3 d(q, j, i) (if T is nondeterministic).
A type is oblivious if, for all q 3 Q, j , j 3 N , and i 3 I,
1 2
n
d(q, j , i)"d(q, j , i). An oblivious type does not distin1
2
guish identical accesses on different ports. For oblivious
types, we often abuse notation and omit the second (port
number) input to the transition function. For non-oblivious types, we require that at most one process is allowed to
access each port of an object; other researchers [3] have
made other assumptions. It is also traditional to require
that each process is allowed to access at most one port of
an object [5]; we do not require this.
Invocation i on port j is useless if there is some response
r such that, for all states q, d(q, j, i)"Sq, rT (if T is deterministic) or d(q, j, i)"MS q, rTN (if T is nondeterministic).
Useless invocations are sometimes needed to specify some

119

types completely, and we do not provide implementations
for them in the sequel.
An operation on type T"Sn, Q, I, R, dT is an element
of N ]I]R, operation S j, i, rT representing the execution
n
of invocation i on port j with response r being returned.
A sequential history of T from a state q is a sequence of
0
alternating states and operations (starting with q ) meet0
ing certain conditions. In particular, consider the sequence
H"q ; S j , i , r T; q ; S j , i , r T; q ;2.
0
1 1 1
1
2 2 2
2
It must be that, for all k, Sq , r T"dSq
, j , i T (if T is
k k
k~1 k k
deterministic) or Sq , r T 3 d(q , j , i ) (if T is nondeterk k
k~1 k k
ministic). We say that state q@ is reachable from q if q@
appears in some sequential history from q. If H contains
k operations, then the length of H, denoted D HD, is k. We
define invs(H, j ) to be the sequence of invocations in H on
port j and resps(H, j ) to be the sequence of responses
returned to port j.
2.2 Implementations
This section defines what it means for one type to be
implemented by others. Informally, an implementation is
a set of objects (appropriately initialized) and deterministic
programs that operate on these objects. There is one
program for each process and for each invocation for
the type being implemented. More formally, let T"
Sn, Q, I, R, dT and let S"MO , O ,2, O N be a set of
1 2
m
objects such that O is of type T "Sn , Q , I , R , d T. An
j
j
j j j j j
implementation of T from state q 3 Q from S is a tuple of
initial states Sq , q ,2, q T (q 3 Q ) and a deterministic
1 2
m
j
j
program P for each i 3 I and each l 3 N . Each program
k, l
k
n
of the implementation specifies how the implementing
objects are to be accessed and what response should be
returned to the invocation associated with that program.
The implementation also specifies, for each port of each
implementing object O , the corresponding port number of
j
T. If port l of T corresponds to port l of O , this means
j
j
that, when a program P (i 3 I) accesses O , it does so
k, l k
j
through port l . We require that each port of each
j
O correspond to at most one port of T. It is also tradij
tional to require that each port of T correspond to at most
one port of each O , although we do not require this.
j
There is an implementation of T from S if implementations exist from all states of T. Such an implementation is
correct if all resulting histories are wait-free [11] and
linearizable [12]. By wait-free, we mean that, in all histories of the implementation, any process performing an
infinite number of steps completes every implementing
program that it begins. By linearizable, we mean that each
execution of the implementation must be equivalent to
a sequential history of the type. There must be a linear
ordering of the implemented operations (i.e., port-invocation-response triples) in the execution such that (1) the
ordering is that of a sequential history from the appropriate state and (2) if the execution of two implemented
operations does not overlap in real time, then they appear
in the sequential history in their real-time order. (For
further details of these definitions, consult Herlihy [11],
Herlihy and Wing [12], or Jayanti [14].)

2.3 Some specific types
This section defines some specific types that are used in the
sequel.
2.3.1 Consensus types
The ability of a type to solve consensus is central to this
paper. We define consensus as a type and consider the
ability of different types to implement a consensus object
(see Sect. 2.2 below). The n-process binary consensus type
cons is an oblivious type defined to be Sn, Q, I, R, dT,
n
where Q"Mo, 0, 1N, I"Mi , i N, R"M0, 1N, and d is de0 1
fined as follows:
d(o, i )"S0, 0T
0
d(a, i )"Sa, aT for any a, b 3 M0, 1N
b
d(o, i )"S1, 1T
1
(We can specify d without regard to port numbers as cons
n
is oblivious.) Usually, consensus objects are chosen to have
state o initially. A process proposes 0 (respectively, 1) to
a cons -object by invoking i (respectively, i ). Note that
n
0
1
the first invocation on the object determines all future
responses, which are identical. This response is sometimes
called the consensus value of the object.
If there is an implementation of cons from S, we say
n
that S implements n-process consensus. Note that it is trivial
to implement cons from any state except o so, in the
n
sequel, we will equate an implementation of cons with an
n
implementation from o.
2.3.2 Registers
Another type important in this paper is the single-reader,
single-writer bit. Note that, because such a bit has only one
writing process, that process always knows the value of the
bit and thus needs to write to it only to change it from 0 to
1 or from 1 to 0. We call this action a ‘‘flip’’, and it will be
used in place of ‘‘write’’ below. Formally, the single-reader,
single-writer bit is a 2-ported type called bit (the ‘‘mu’’
.6
indicates that it can be used a multiple number of times
and distinguishes it from one-use bits defined in Sect. 3
below) and is defined to be S2, Q , I , R , d T, where
.6 .6 .6 .6
Q "M0, 1N, I "Mread, flipN, R "M0, 1, okN and
.6
.6
.6
d is defined as follows, where v 3 M0, 1N.
.6
d (v, 1, read)"Sv, vT d (v, 2, read)"Sv, okT
.6
.6
d (v, 1, flip)"Sv, okT d (v, z, flip)"S1!v, okT
.6
.6
The process connected to port 1 (the reader) of a bit .6
object can discover the state of the object using a read
invocation, while the process connected to port 2 (the
writer) can change the state using a flip invocation. Note
that flip invocations on port 1 and read invocations on
port 2 are useless.
Many papers (including this one) use the term ‘‘register’’ or ‘‘read/write memory’’ to refer to a similar type that
is multi-reader, multi-writer, and multi-valued. We call
such a type a general register and denote it by reg. We do
not provide a formal description of this extension; it is
similar to bit , except that it is oblivious (any port can be
.6
used either to read or to write the register) and can hold

120

m different values instead of just 2. Instead of flip, regobjects support a set of m different write invocations, each
of which allows a process to set the object to a specified
value. Implementations of reg from bit are discussed in
.6
Sect. 4.1.
2.4 ¹he universality of consensus and wait-free hierarchies
Herlihy [11] demonstrated that the consensus types cons
n
are universal in the following sense: there is a wait-free
implementation of any n-ported type T from some set of
general registers (reg-objects) and cons -objects. Because
n
of this, Herlihy proposed evaluating different types by
assigning them consensus numbers. The consensus number
of type T is the largest integer n for which some set of
reg-objects and a single T-object can implement cons .
n
Jayanti [14] questioned two of Herlihy’s assumptions
in assigning a consensus number to a type T: whether or
not reg-objects should be used in an implementation of
cons and whether or not multiple T-objects can be used.2
n
To explore the impact of different choices here, he defined
four wait-free hierarchies:
— h (T)7n if and only if one T-object can implement n1
process consensus.
— h3 (T)7n if and only if some set of reg-objects and one
1
T-object can implement n-process consensus.
— h (T)7n if and only if some set of T-objects can imple.
ment n-process consensus.
— h3 (T)7n if and only if some set of reg- and T-objects
.
can implement n-process consensus.
(Thus, for example, h (T) is the largest n such that a single
1
T-object can implement n-process consensus.) Herlihy’s
assignment of consensus number corresponds to Jayanti’s
hierarchy h3 . It is clear from these definitions that, for all
1
types T, 16h (T)6h3 (T)6h3 (T) and h (T)6h (T)6
1
1
.
1
.
h3 (T). In addition, standard techniques can be used to
.
show that, if T is n-ported, then h(T)6n (where h is any of
the hierarchies given above).
Ideally, the assignment of a consensus (or hierarchy)
number to a type should be a good measure of the type’s
computational power. The larger the number assigned, the
more power the type has to implement other types. Indeed,
Herlihy’s result on the universality of consensus shows
that, if h(T)"n (where h is any of the hierarchies given
above) and T@ has at most n ports, then there is an implementation of T@ using some number of reg- and T-objects.
Given four different ways of assigning these values, it
makes sense to consider which is best. Jayanti identified
a desirable property of hierarchies that he called robustness. Hierarchy h is robust if, for every choice of n, T, and
T , T ,2, T , the relations h(T)7n and h(T )(n (for all
1 2
m
j
16j6m) imply that there is no implementation of T from
any set of objects of types T , T ,2, T . Robustness im1 2
m
plies that there can be no ‘‘synergistic’’ effect that would
allow ‘‘weak’’ types to implement a ‘‘strong’’ one.

2 The reader should note that, while Herlihy’s model allowed only
one T-object to be used, all the impossibility results that he showed
apply for any number of objects

Jayanti showed that none of h , h3 , and h could be
1 1
.
robust if it were not equal to h3 . He then showed that both
.
3
3
h and h were different from h , proving that only
1
.
.
h3 might be robust (h cannot equal h3 if either h or
.
1
.
.
h3 does not). Jayanti left the robustness of h3 as an open
1
.
question. Recent papers [3, 24] have claimed that h3 is
.
robust for certain classes of deterministic types. However,
Moran and Rappoport [21] exhibited a class of deterministic types for which h3 is not robust.3
.
Jayanti’s proof that h differs from h3 exhibited a type
.
.
T with h (T)"1 and h3 (T)72. This type is nondetermin.
.
istic. The remainder of this paper considers restricted
classes of types for which h is shown equal to h3 . For
.
.
these classes, h is robust if and only if h3 is.
.
.
3 One-use bits
The main results of this paper stem from the implementation and use of a new concurrent data type called the
one-use bit. Objects of this type are one-bit registers that
can be read only once and written only once. Section 4
shows that objects of this type can be used to implement
general registers (reg-objects) in the context of wait-free
implementations of consensus, while Section 5 shows that
it is easy to implement this type.
The one-use bit type bit
is defined to be
16
S2, Q , I , R , d T, where Q "Moff, on, deadN, I "
16 16 16 16
16
16
Mlook, setN, R "Moff, on, okN, and d is defined as follows.
16
16
d (off, 1, look)"MSdead, off TN
(1)
16
d (on, 1, look)"MSdead, onTN
(2)
16
d (dead, 1, look)"MSdead, off T, Sdead, onTN
(3)
16
d (q, 1, set)"MSq, okTN
(4)
16
d (off, 2, set)"MSon, okTN
(5)
16
d (on, 2, set)"MSdead, okTN
(6)
16
d (dead, 2, set)"MSdead, okTN
(7)
16
d (q, 2, look)"MSq, okTN
(8)
16
(Lines 4 and 8 above hold for any q 3 Q and indicate that
16
the specified invocations are useless.)
State off is usually chosen as an initial state. The
process connected to port 2 can write the bit -object once
16
by invoking set. This moves the object from state off to
state on. The process connected to port 1 can read the
object once by invoking look. If the object is in state off or
the state on, that state is returned to process. After two set
invocations or one look invocation, the object enters the
3 Moran and Rappoport defined a deterministic 4-ported type
W with the property that h3 (W)(3 and that there is a wait-free
.
implementation of 4-process consensus using a single W-object, four
cons -objects, and read/write memory. Since h3 (cons )"3, this
3
.
3
implies that h3 is not robust for any class of types that includes both
.
W and cons . However, the proof by Moran and Rappoport of
3
h3 (W)(3 required the restriction that, in a 3-process system,
.
a single process could not access a W-object (which is 4-ported) by
more than one port. The model of Borowsky, Gafni, and Afek [3]
does not allow this restriction

121

state dead. At this point, no further information can be
derived from the object from port 1 because of the type’s
nondeterminism. Note that this nondeterminism will play
no role in our use of the type (Sect. 4); a look will never be
invoked when the object is in state dead.

4 Using one-use bits
Although one-use bits (bit -objects) are apparently
16
weaker than general registers (reg-objects, defined in
Sect. 2.3.2), we can show that, within the context of waitfree implementations of consensus, they are equally
powerful. This is shown through the following three
observations.
1. General registers can be implemented using singlereader, single-writer multi-use bits (bit -objects, defined
.6
in Sect. 2.3.2).
2. For any n, any wait-free implementation of nprocess consensus, and any bit -object b used by the
.6
implementation, there are bounds on the number of times
that b is read and flipped in any execution of the
implementation.
3. If there are bounds on the number of times that
a bit -object b can be read and flipped, then b can be
.6
implemented by a finite number of single-use bits (bit 16
objects).
These are shown in Sects. 4.1— 4.3 below.
4.1 Implementing general registers
A large body of literature has considered the definition and
implementation of a variety of different kinds of read/write
registers (or memory) and the relationships between these
kinds. Most recent research in wait-free computation has
considered registers that are atomic (linearizable), multireader, multi-writer, and multi-valued (reg-objects).
Earlier research considered weaker kinds of registers.
Figure 1 summarizes a sequence of constructions that
allow bit -objects to be used to implement reg; the
.6
bracketed notes are references to the bibliography.
All the constructions are wait-free and exist for any
number of processes. The following paragraph details
these constructions. Note that this is a very incomplete
account of the large volume of results that have been
produced, mentioning only those that are necessary for the
results of this paper.

Lamport [19] showed that there is an implementation
of multi-reader, single-writer, regular bits from singlereader, single-writer, regular bits. Since regular bits are
weaker than atomic bits, this implementation can also use
bit -objects. Burns and Peterson [4], Newman-Wolfe
.6
[22], and Singh, Anderson, and Gouda [27] all showed
that there is an implementation of multi-reader, singlewriter, atomic bits from multi-reader, single-writer, regular
bits. Peterson [23] showed that there is an implementation
of multi-reader, single-writer, atomic, multi-valued registers from multi-reader, single-writer, atomic bits. Peterson
and Burns [25] showed that there is an implementation of
multi-reader, multi-writer, atomic, multi-valued registers
(reg-objects) from multi-reader, single-writer, atomic,
multi-valued registers. It follows from all these results that
there are implementations of reg from bit -objects.
.6
4.2 Access bounds in wait-free consensus
Suppose that there is a wait-free implementation of nprocess consensus that uses some number of registers and
T-objects. The observations of the previous section allow
us to assume that the registers are bit -objects. We show
.6
that, for each bit -object b, there exist constants r and
.6
b
f such that in no execution of the implementation is b read
b
more than r times or flipped more than f times.
b
b
Consider the executions of the implementation of
cons as a collection of trees. Each vertex of a tree corresn
ponds to some configuration of the implementing objects
(of types bit and T) and the ‘‘program counters’’ of the
.6
n processes in their implementing functions. The roots of
the trees correspond to possible initial configurations: the
initial states of the implementing objects and the vector of
invocations that the n processes will first apply to the
cons -object (each may be i or i ); that is, each process is
n
0
1
at the ‘‘entry point’’ of one of its two implementing functions. A configuration C is the parent of C if C results
1
2
2
from C through the execution of one low-level operation
1
(on a bitmu-object or a T-object) by one process in its first
invocation on the cons -object. (If a configuration can be
n
reached via multiple paths, it appears multiple times.) Any
configuration in which some process accesses the cons n
object a second time does not appear in a tree. Thus,
a configuration in which all n processes have completed
their first invocations is a leaf vertex.
We consider only first invocations because any later
invocations by a process must return the same response as
first (see Sect. 2.3.1). We assume that each process stores
the first response locally and does not access any of the
implementing objects after its first invocation.
Consider any one of these trees. We show by contradiction that it is finite. Assume that it is not. This means that
a form of König’s Infinity Lemma [9; Theorem 2.8, page
32] applies:
Lemma 1 (Ko~ nig). If G is an infinite digraph, with root
r and finite out-degree for all its vertices, then G has an
infinite directed path, starting in r.

Fig. 1. Implementations of registers

The out-degree of each vertex in our trees is finite. If T is
deterministic, it is bounded by n. Any vertex has at most

122

n children, one for each process. This is because the
processes are deterministic, as are bit and T. If T"
.6
Sn, Q, I, R, dT is nondeterministic, the out-degree of
a vertex is bounded by n times the size of the largest set
d(q, j, i) (recall that these sets are finite for nondeterministic
types).
König’s Lemma now implies that there is an infinite
path from the root of the tree. This path corresponds to
some execution of the implementation. This means that
there is an execution in which some process executes an
infinite number of steps but never completes its first invocation on cons . This contradicts the fact that the implen
mentation is wait-free.
The tree described is thus finite; let h be its height, the
maximum length of a path from the root. There are 2n such
trees. This is because the initial states of the implementing
objects are the same in all trees. (the implementation must
specify a unique initial state for each such object), and only
the choice of the entry points of the n processes can vary.
Let h
be the maximum h over all the trees; since there
.!9
are finitely many trees, h is finite. This means that in no
.!9
execution are more than h steps executed. Thus, at most
.!9
h
accesses are invoked on any implementing object in
.!9
any execution. By choosing r "f "h , we know that
b
b
.!9
in no execution of the implementation does any process
read a bit -object b more than r times or flip it more
.6
b
than f times.
b
4.3 Implementing multi-use bits
This section shows how any bit -object that is accessed
.6
a bounded number of times can be implemented with
a finite number of bit -objects. Suppose that bit -object
16
.6
b is initialized to v, read at most r times, and flipped at
b
most f times.
b
The implementation uses r · f -objects of type bit .
b b
16
These form an r ]f array bits[12r , 12 f ], all eleb b
b
b
ments of which are initially in state off. A read on port 1 is
implemented by P
and a flip on port 2 is implemented
1,read
by P
(see below). (Recall that the other invocations are
2, flip
useless and trivially return ok.) Each row of the array
corresponds to an execution of P
and each column to
1, read
an execution of P
. Intuitively, an execution of
2, flip
P
and one of P
‘‘communicate’’ through one
1, read
2, flip
element of the array. If P
invokes set on that element
2, flip
before P
invokes look on it, the flip will be ‘‘seen’’ by
1, read
the look; otherwise, it will not. P
invokes look on
1,read
bit -objects in its corresponding row until it finds one
16
that has not been set. P
invokes set on all bit 2, flip
16
objects in its corresponding column. Port 1 (respectively,
port 2) of b corresponds to port 1 (respectively, port 2) of
each of the bit -objects. The reading process maintains
16
two local integer variables Reads and Col, while the writing process maintains local Row and Flips; these are all
initially 1.
The implementing programs use the following notation. If i is an invocation on some type T and O is a Tobject, i(O) (called an O-access) is used to indicate that the
invocation is performed and the result returned. The following are the implementing programs (recall that v is the
initial value of the bit -object being implemented):
.6

P
:: while Col6f and look(bits[Reads, Col ])"on do
1, read
b
Col "Col#1
:
Reads "Reads#1
:
return((v#(Col!1)) mod 2)
P
:: for Row "1
:
to r do
2, flip
b
set (bits[Row, Flips])
Flips "Flips#1
:
return(ok)
Note that an execution of P
does not invoke look on
1, read
every bit -object in its row. Instead, it starts in the col16
umn where the previous execution ended and proceeds
only until it finds a column in which its access returns off.
The on-column of an execution of P
is one less than the
1, read
value of Col at the end of that execution. It is the total
number of columns in which any execution of P
has
1, read
seen a bit -object with state on. The index of an execution
16
of P
is the value of Flips at the beginning of that
2, flip
execution. This reflects the execution’s ordinal position
among all executions of P
; it is also the column of the
2, flip
array to whose entries the execution applies set.
To prove the correctness of the implementation, we
need to prove it linearizable and wait-free. Wait-freedom is
obvious: no execution of P
requires more than
1, read
f operations and all executions of P
use exactly
b
2, flip
r operations. To show linearizability, we must show that,
b
for each execution of the implementation from a state,
there is a sequential history from that state that preserves
the real-time ordering of the operations in the execution.
Consider an execution of the implementation from state
v in which P
is executed at most r times and P
at
1,read
b
2, flip
most f times. We now describe a linear ordering of the
b
corresponding operations. The relative order of the read
operations is that in which they were invoked, as is the
relative order of the flip operations. A read operation is
ordered before a flip if its execution’s on-column is less
than the index of the execution of the flip. It is easy to see
that, if an execution of P
has on-column c, then the
1,read
corresponding read is preceded by c flip’s in the linear
ordering.
We first show that the resulting linear ordering respects the real-time ordering of the programs’ executions.
This is obvious for any pair of operations on the same
port; it remains only to show it for the ordering of a read
operation and a flip operation. Suppose that the read’s
execution has on-column c and the flip’s has index i. We
must consider two cases:
— The execution of P
completes before that of
1,read
P
begins. In this case, the ith column of the array
2, flip
bits is completely off when the P
executes (the same
1,read
is true for all previous executions of P
as well). This
1,read
means that the while loop will terminate with Col less
than equal to i; c is one less than this value of Col. Thus,
c(i and the two operations are ordered correctly.
— The execution of P
completes before that of
2, flip
P
begins. In this case, the ith column of the array
1, read
executes, as are all
bits is completely on when P
1,read
previous columns. When P
executes, it will find all
1, read
these bit -objects on and advance Col to be at least
16
i#1; since c is one less than this value of Col, c7i.

123

Thus, 2 (c(i) and the two operations are ordered
correctly.

q, and i can be chosen such that p is reachable from q in
one operation.

Finally, we need to show that this linear ordering is
indeed a sequential history from v. All flip invocations
return ok, as desired. Consider some read operation that is
preceded by f flip operations. This means that the onis f, and
column of the corresponding execution of P
1, read
this execution ended with Col"f#1. The execution thus
returns (v#(Col!1)) mod 2"(v#f ) mod 2. Since the
bit -object was initially v and was then flipped f times,
16
this is the correct value.

Lemma 2. ¸et T"Sn, Q, I, R, dT be non-trivial, oblivious,
and deterministic. ¹hen there are states q and p and invocation i that witness T’s non-triviality such that p is reachable
from q in one operation.

5 Implementing one-use bits
This section illustrates two cases in which one-use bits
(bit -objects) can be implemented. These are non-trivial
16
deterministic types and types above level 1 in the hierarchy
h .
.
5.1 Non-trivial deterministic types
This section shows that an object of any non-trivial deterministic type T can implement bit . Informally, T is
16
non-trivial if a T-object, suitably initialized, is capable of
providing processes with some information about how it
has been accessed. Deriving an implementation of bit is
16
much simpler for oblivious types, and this case is presented
in Sect. 5.1.1. The general case is presented in Sect. 5.1.2.
5.1.1 Oblivious types
Most, but not all, deterministic oblivious types can implement bit . Some types, however, are so weak as to be
16
incapable of implementing any interesting type. Consider,
for example, a type T"Sn, Q, I, R, dT such that D R D"1.
Because the type must return the same response to every
invocation, there is no way that it can supply any useful
information. Formally, an oblivious type T"Sn, Q, I, R, dT
is trivial if, for every state q 3 Q and every invocation i 3 I,
there is a response r 3 R such that, for each state p reachqi
able from q (including q itself ), there is a state p@ such that
d( p, i)"Sp@, r T.4 A trivial oblivious type, once inqi
itialized, returns the same response to each occurrence of
a given invocation; processes can gain no information by
accessing an object of the type. An oblivious type that is
not trivial is non-trivial. We now show that an object of
any non-trivial oblivious deterministic type can implement
bit .
16
Let T"Sn, Q, I, R, dT be a non-trivial oblivious deterministic type. This means that there are states q and p,
invocation i, and responses r and r such that r 9r , p is
q
p
q
p
reachable from q, d(q, i)"Sq@, r T (for some state q@), and
q
d(p, i)"S p@, r T (for some state p@). In this case, q, p, and
p
i are said to witness T’s non-triviality. We first show that p,

4 Jayanti, Chandra, and Toueg [15, Sect. 5.1.2] give a slightly
stronger definition of a trivial oblivious type. Their definition requires that, for all i, p, and q, r "r
qi
pi

Proof. Let q and p be the states and i the invocation that
witness T’s non-triviality. Let l71 be the number of
operations between q and p in some sequential history of
T (such a history must exist since p is reachable from q) and
suppose that p, q, and i were chosen to minimize l. If l"1,
we are done. Otherwise, let s be the state reachable from
q by the first l!1 operations that lead from q to p. Let
d(q, i)"Sq@, r T, d( p, i)"S p@, r T, and d(s, i)"Ss@, r T.
q
p
s
Since r 9r , r must be different from one of them. Note
q
p s
that s is reachable from q and p is reachable from s; thus,
either q, s and i or s, p, and i witness T’s non-triviality. In
either case, fewer than l operations are needed for the
reachability. This contradicts the minimality of l. K
We now give an implementation of bit from one T16
object. Let q, p, and i witness T’s non-triviality such that
there is an invocation i whose operation from q leads to p;
4
Lemma 2 guarantees the existence of such an invocation.
We use one T-object O, initialized to state q. A look on
port 1 of the bit -object is performed as follows:
16
P
:: if i(O)"r
then
1, look
q
/* O was still in state q */
return (off )
else
/* O was not in state q */
return (on)
A set on port 2 is performed as follows:
P
::
2, set

i (O)
s
return(ok)

(Recall that set on port 1 and look on port 2 are useless.)
Intuitively, state q corresponds to off, p to on, and any
other state to dead.
To prove the correctness of the implementation, we
need to prove it linearizable and wait-free. Wait-freedom is
obvious: each invocation uses exactly one operation on
T-object O. To show linearizability, we must show that, for
each execution of the implementation, there is a sequential
history that preserves the real-time ordering of the operations in the execution. Consider an execution of the implementation. We now describe a linear ordering of the
corresponding operations. Since each invocation contains
exactly one O-access, order the corresponding operations
according to the order of these accesses. It is easy to see
that the resulting linear ordering respects the real-time
ordering of the programs’ executions. If the execution of
two invocations does not overlap in real time, then the
O-access of the first must precede that of the second, and
the two operations are ordered correctly.
Finally, we need to show that the linear ordering of
operations specified above is indeed a sequential history
from off. All set invocations return ok, as desired. Because
of the nondeterminism in the specification of bit , all look
16

124

invocations besides the first can correctly return either off
or on, and they do so. Consider the first look invocation
and the following three cases:
— The look is first in the linear ordering. This means that
the O-access of the first execution of P
preceded all
1,look
others and thus occurred when O was in state q. Therefore, this access returned r and P
returns off, as
q
1, look
desired.
— The look follows exactly one set in the linear ordering.
This means that the first execution of P
first invoked
2, set
i on O in state q. After this, O was in state p. The first
4
execution of P
then applied i to O and received
1, look
response r , which is different from r . Therefore,
p
q
P
returned on, as desired.
1, look
— The look follows two or more set operations in the linear
ordering. In this case, it can correctly return either off or
on, and it does so.
5.1.2 General types
The previous section showed that any non-trivial oblivious
deterministic type can implement one-use bits. The definition of triviality and the proof depended on the obliviousness of the type being used. This section generalizes that
result to general types that are not necessarily oblivious.
A deterministic type is trivial if, for all ports, all finite
sequences of invocations on that port always return the
same finite sequence of responses regardless of any invocations performed (and the order in which they are performed) on other ports. In other words, T"Sn, Q, I, R, dT
is trivial if, for all states q 3 Q, all finite histories H and
1
H from q, and all ports j 3 N , invs(H , j )"invs(H , j )
2
n
1
2
implies resps(H , j )"resps(H , j ). A type is non-trivial if it
1
2
is not trivial. Thus, for any non-trivial T, there is a state q,
finite histories H and H from q, and port j such that
1
2
invs(H , j )"invs(H , j ) and resps(H , j )9resps(H , j ).
1
2
1
2
Call H and H a non-trivial pair from q on port j. Note that
1
2
different sequences of operations may be invoked on ports
other than port j in H and H .
1
2
For the remainder of this section, we will assume that
q, H , H , and j are chosen such that DH D#DH D is mini1 2
1
2
mal among all non-trivial pairs. Let ıl "invs(H , j ) (which
1
is the same as invs(H , j )). Since ıl is finite, suppose that
2
ıl "Si , i ,2, i T and thus has length k. The following
1 2
k
sequence of lemmas demonstrate certain properties of
H and H . These properties allow one T-object to imple1
2
ment bit .
16
Lemma 3. ¹he last operation in each of H and H is on
1
2
port j.
Proof. Without loss of generality, assume that H ends
1
with an operation on a port other than j. Let H@ be the
1
prefix of H up to but not including this last operation.
1
Since that operation is not on port j, invs(H @ , j )"ıl and
1
resps(H , j )"resps(H @ , j ). This means that H @ and H are
1
1@
1
2
a minimal pair and DH D#DH D"DH D#DH D!1. This
1
2
1
2
contradicts the minimality of H and H . K
1
2
Lemma 4. One of H and H has length k; that is, it consists
1
2
only of operations on port j.

Proof. Let H be the history from q consisting only of the
0
invocations in ıl on port j. Because resps(H , j )9
1
resps(H , j ), resps(H , j ) must differ from at least one of
2
0
them. Without loss of generality, assume that it differs
from resps(H , j ). In this case, H and H are also a non2
0
2
trivial pair. Since DH D#DH D is minimal, DH D#
1
2
0
DH D"k#DH D7DH D#DH D, so DH D6k. Since
2
2
1
2
1
H must contain at least the k operations on port j,
1
DH D"k.5 K
1
Lemma 4 allows us to assume, without loss of generality,
that H contains only the k invocations on port j and that
1
H contains at least one invocation on some other port
2
(otherwise, H "H and they are not a non-trivial pair).
1
2
Lemma 5. H consists of one operation on some port other
2
than j followed by k operations on port j.
Proof. We begin by proving that the last k invocations in
H are all on port j. Let DH D"l'k. Suppose that H is
2
2
2
chosen so that the last operation o on some port other
than j is as late as possible and suppose that this operation
is followed by m operations on port j; that is, we are
minimizing m over all possible choices for H . Since there
2
are only k operations on port j, 06m6k. We wish to
prove m"k. Lemma 3 implies m'0, so o is immediately
followed by at least one operation on port j. Let H be
4
a sequential history from q with the same invocations as
H in the same order except that the order of o’s invocation
2
and that of the immediately following invocation on port
j are reversed. In H , the last operation on a port other
4
than j is followed by m!1 operations on port j. Since
H was chosen to minimize m, H and H cannot form
2
4
1
a nontrivial pair. Since invs(H , j )"ıl (the order of
4
invocations on port j did not change), it must be that
resps(H , j )"resps(H , j ). Since resps(H , j )9resps(H , j ),
4
1
1
2
resps(H , j )9resps(H , j ).
4
2
Note that H and H are identical through their first
2
4
l!(m#1) operations. Let q@ be the state of each of these
histories after these operations and let H @ and H @ be the
2
4
suffixes of H and H , respectively, of length m#1. These
2
4
are both sequential histories from q@ containing o and the
last m operations on port j. Thus, invs(H @ , j )"invs(H @ , j ).
2
4
Because H and H are identical before these suffixes and
2
4
because resps(H , j )9resps(H , j ), it must be that
2
4
resps(H @ , j )9resps(H @ , j ). This means that H @ and
2
4
2
@
@
H form a nontrivial pair. D H D#D H @ D"2(m#1). By the
4
2
4
minimality of H and H , DH @ D#DH@ D"2(m#1)7DH D#
4
1
1
2
2
DH D"k#l. Since l'k, we have 2(m#1)7k#l'2k,
2
so m#1'k. Because m6k by definition, we have m"k,
as desired.
We need now to show that D H D"l"k#1; this will
2
imply that H is a single operation on a port other than
2
j followed by k operations on port j. Recall that q@ is the
state of H after its first l!(m#1)"l!(k#1) opera2
tions and that o is an operation on a port other than j that
is executed from state q@. Let H be a history from q gener3
ated by the first l!(k#1) operations in H (which lead
2
to q@) followed by the k invocations in ıl ; thus, H does
3
not include o. Since D H D(D H D, H and H cannot
3
2
1
3
5 This implies that H "H
0
1

125

form a nontrivial pair; since invs(H , j )"ıl , it must be
3
that resps(H , j )"resps(H , j ). Thus, resps(H , j )9
3
1
3
@
resps(H , j ). Let H be the suffix of H from q@ and
2
3
3
recall that H @ is the suffix of H from q@. Clearly,
2
2
invs(H @ , j )"invs(H @ , j )"ıl , resps(H @ , j )"resps(H , j ),
3
2
3
3
and resps(H @ , j )"resps(H , j ). Thus, H @ and H @ form
2
2
3
2
a nontrivial pair. Since H and H are the shortest such
1
2
pair, we have D H @ D#D H @ D"k#(k#1)"2k#17
3
2
DH D#DH D" k#l. Thus, l6k#1. But l'k by defini1
2
tion, so l"DH D"k#1.6 K
2
We now know, by Lemma 4, that H consists of k op1
erations on port j and, by Lemma 5, that H consists of one
2
invocation, say i , on some other port, say j , followed by
4
4
the same k invocations on port j. Let q be the state the
4
results from applying i to O in state q. We can now show
4
that an object O of any non-trivial deterministic type can
be used by two processes to implement bit . Initialize
16
O to the state q associated with the shortest nontrivial pair
(see above). Port 1 of the bit -object (the reading port) is
16
connected to port j of O and a look on that port is
performed as follows:
P
:: for l "1
:
to k
1, look
r[l ] "i
:
(O)
l
If rl "resps(H , j ) then
1
/* writer has not written */
return(off )
else
/* writer has written */
return(on)
The process performs the invocations in ıl and checks to see
whether or not resps(H , j ) is returned.7 Port 2 of the
1
bit -object (the writing port) is connected to port j of
16
4
O and a set on that port is performed simply with the one
invocation i from H on port j :
4
2
4
P
:: i (O)
2, 4et
4
return(ok)
Note that the reader may receive a response that is neither
H ’s nor H ’s. However, this still indicates that the writer
1
2
has written, so on can be returned if rl 9resps(H , j ).
1
To prove the correctness of the implementation, we need
to prove it linearizable and wait-free. Wait-freedom is
obvious: each invocation of P
used exactly k opera1, look
tions on shared object O, and each invocation of P
uses
2,4et
one. To show linearizability, we must show that, for each
execution of the implementation, there is a sequential
history that preserves the real-time ordering of the operations in the execution. Consider an execution of the implementation. Order the corresponding operations linearly in
any way that is consistent with their real-time ordering
except for the following. If the first execution of
P
overlaps with the second execution of P
, order
1, look
2,4et

the corresponding look after the corresponding (second)
set. If the first execution of P
overlaps with the first
1, look
execution of P
and completely precedes the second,
2,4et
order the corresponding look before the corresponding
(first) set if and only if P
returns off. Note that, by
1, look
definition, the resulting linear order respects the real-time
ordering of the programs’ executions.
We need to show that the linear ordering of operations
specified above is indeed a sequential history from off. As
in the proof in Sect. 5.1.1, all set invocations and all look
invocations besides the first return correct values. Consider the first look invocation and the following three cases:
— The look is first in the linear ordering. This means that
the first execution of P
precedes all executions of
1,look
P
or it overlapped with the first such execution and
2, set
returned off. In the first case, the invocations on O from
ıl on port j preceded all other O-accesses. This means
that rl as computed by P
was resps(H , j ) and
1, look
1
P
returned off. In both cases, the correct value (off )
1, look
is returned.
— The look follows exactly one set in the linear ordering.
This means that the first execution of P
completely
1,look
preceded the second execution of P
. Also, either the
2,set
first execution of P
took place between the first and
1, look
second executions of P
or it overlapped with the first
2,set
execution and P
returned on. In the first case, O was
1, look
in state q when P
began and the k invocations in
4
1,look
ıl took place consecutively. Thus, rl as computed by
P
equals resps(H , j ). Since, by definition, this is
1, look
2
different from resps(H , j ), P
returned on. In both
1
1,look
cases, the correct value (on) is returned.
— The look follows two or more set operations in the linear
ordering. In this case, it can correctly return either off or
on, and it does so.
5.2 High-level types in h

.
Let T be any type such that h (T)72. This means that
.
there is an implementation of cons using only T-objects
2
(without registers). We now show that, even if T is
nondeterministic, T can implement bit . We do this by
16
exhibiting an implementation of bit from cons . Since
16
2
T-objects can implement cons , then they can implement
2
bit .
16
Let O be a cons -object, initialized to state o. Port
2
1 (respectively, port 2) of bit corresponds to port 1 (re16
spectively, port 2) of O. A look on port 1 of bit is
16
performed as follows:
P
:: if i (O)"0 then
1, look
0
return(off )
else
return(on)
A set on port 2 is performed as follows:
P
::
2, set

6 This implies that the prefix of H consisting of its first l!(k#1)
2
operations is empty. This means that q@"q, H @ "H , and
2
2
H @"H "H
3
3
1
7 One can show that it is sufficient for the process to check only the
last response in rl to see if it matches that of resps(H , j )
1

i (O)
1
return(ok)

Basically, the reader proposes 0, meaning ‘‘look precedes
set,’’ while the writer proposes 1, meaning ‘‘set precedes
look.’’ The ‘‘winner’’ of O determines the consensus value
and thus the ordering of the first look and the first set. Note

126

that this implementation returns the same response to all
invocations of look on port 1; this is permitted by the
nondeterministic specification of bit .
16
To prove the correctness of the implementation, we
need to prove it linearizable and wait-free. Wait-freedom is
obvious: each invocation uses exactly one operation on
shared object O. To show linearizability, we must show
that, for each execution of the implementation, there is
a sequential history that preserves the real-time ordering
of the operations in the execution. Consider an execution
of the implementation. We now describe a linear ordering
of the corresponding operations. Since each invocation
contains exactly one access to O, order the corresponding
operations according to the order of these accesses. As
in Sect. 5.1.1, it is easy to see that the resulting linear
ordering respects the real-time ordering of the programs’
executions.
Finally, we need to show that the linear ordering of
operations specified above is indeed a sequential history
from off. As in the proofs in Sect. 5.1, all set invocations
and all look invocations besides the first return correct
values. Consider the first look invocation and the following
three cases:
— The look is first in the linear ordering. This means that
its O-access preceded all others, which was thus in state
o when the corresponding execution of P
proposed
1,look
0 (invoked i ). By the specification of cons , O returned
0
2
0, so P
returned off, as desired.
1,look
— The look follows exactly one set in the linear ordering.
This means that an execution of P
invoked i on O in
2,set
1
state o. After this, O was in state 1. The corresponding
execution of P
then applied i to O and, by the
1, look
0
specification of cons , received response 1. Therefore,
2
the P
returned on, as desired.
1, look
— The look follows two or more set operations in the linear
ordering. In this case, it can correctly return either off or
on, and it does so.
6 Applications to wait-free hierarchies
The above results have two important applications to
wait-free hierarchies:
Theorem 6. Suppose that one of the following holds of
type T:
— T is deterministic; or
— h (T)72.
.
¹hen h (T)"h3 (T).
.
.
Proof. Let T be a type with one of the above properties.
Recall that 16h (T)6h3 (T) for all types T. It thus suffi.
.
ces to show that h3 (T)6h (T). The proof is divided into
.
.
three cases:
— T is deterministic and trivial. This means that, no matter
how a T-object is initialized, any sequence of invocations on a port always returns the same sequence of
responses. The object can thus be trivially implemented
locally (this conclusion requires our assumption that no
more than one process can access a particular port).
This means that, if h3 (T)7n, then registers alone can
.

implement n-process consensus. Since registers cannot
implement 2-process consensus [6, 11, 20], this implies
that h3 (T)"1. Since h (T)71 for any T, h3 (T)6h (T)
.
.
.
.
as desired.
— T is deterministic and non-trivial. We show that, for all
n, h3 (T)"n implies h (T)7n. If h3 (T)"n, then regis.
.
.
ters and T-objects can implement n-process consensus.
As noted in Sect. 4.1, the registers can be bit -objects.
.6
Section 4.2 showed that there is bound on the number of
times each bit -object may be used and Sect. 4.3
.6
showed that, if this is the case, each such bit -object
.6
may be implemented by a finite number of bit -objects.
16
Section 5.1 showed that an object of any non-trivial
deterministic type can implement bit . Thus, T-objects
16
can implement n-process consensus (without registers).
This implies that h (T)7n, as desired.
.
— h (T)72. Again, we show that, for all n, h3 (T)"n
.
.
implies h (T)7n. As noted above, if h3 (T)"n, then
.
.
some set of T and bit -objects can implement n-process
16
consensus. Section 5.2 showed that one T-object can
implement bit . Thus, some set of T-objects can imple16
ment n-process consensus without using registers. This
implies that h (T)7n, as desired.
.
In all cases, h3 (T)6h (T). This implies h (T)
.
.
.
"h3 (T). K
.
Theorem 6 shows that Jayanti’s choice of a type T to
distinguish h and h3 was not accidental: it had to be
.
.
a nondeterministic type with h (T)"1 and h3 (T)72.8
.
.
7 Conclusions
The results of this paper show that, in most cases of
interest, registers are not ‘‘special’’ when it comes to implementing wait-free consensus. This can simplify the reasoning process: various arguments made with the assumptions
that registers are available (e.g., about the hierarchy h3 )
.
apply when they are not (e.g., to the hierarchy h ); the
.
converse is also true.
Theorem 6 shows that, for two large classes of concurrent data types, Jayanti’s wait-free hierarchies h and
.
h3 are equal. One of these is the class of deterministic
.
types, which is of considerable interest. Furthermore, these
results pertain to Jayanti’s robustness property. His proof
that h is not robust does not apply, for example, to
.
deterministic types. Recent papers [3, 24] have claimed
that h3 is robust for certain classes of deterministic types.
.
The results of the current paper would then imply that
h is also robust for these types.
.
Although this paper has shown how most interesting
types can implement registers in the context of a wait-free
consensus algorithm, one should note that this context
was required only in Sect. 4.2. Since results similar to that
section can be shown for wait-free implementations of any
bounded-use type, our implementations of registers are
thus applicable also to these implementations.

8 In fact, Jayanti exhibited the following: for each k'1, a type
T such that h3 (T )"k and h (T )"1
k
. k
. k

127
Acknowledgements. We are grateful to Scott McCrickard for discussing this work with us. In addition, we thank the anonymous referees
for many useful commments.

References
1. Afek Y, Greenberg DS, Merritt M, Taubenfeld G: Computing
with faulty shared memory. ACM 42(6): 1231—1274 (1995)
2. Afek Y, Weisberger E, Weisman H: A completeness theorem for
a class of synchronization objects. In: Proceedings of the 12th
ACM Symposium on Principles of Distributed Computing,
pp 159—170. ACM Press, August 1993
3. Borowsky E, Gafni E, Afek Y: Consensus power makes (some)
sense! In: Proceedings of the 13th ACM Symposium on
Principles of Distributed Computing, pp 363—372. ACM Press,
August 1994
4. Burns JE, Peterson GL: Constructing multi-reader atomic values
from non-atomic values. In: Proceedings of the 6th ACM Symposium on Principles of Distributed Computing, pp 222—231.
ACM Press, August 1987
5. Chandra T, Hadzilacos V, Jayanti P, Toueg S: Wait-freedom
versus t-resiliency and the robustness of wait-free hierarchies. In:
Proceedings of the 13th ACM Symposium on Principles of
Distributed Computing, pp 334—343. ACM Press, August 1994
6. Chor B, Israeli A, Li M: Wait-free consensus using asynchronous
hardware. SIAM J Comput 23(4): 701—712 (1994)
7. Cori R, Moran S: Exotic behaviour of consensus numbers. In:
Tel G, Vitányi P (eds) Proceedings of the 8th International
Workshop on Distributed Algorithms, Lect Notes Comput Sci,
vol 857, pp 101—115. Springer, Berlin Heidelberg New York 1994
8. Dolev D, Dwork C, Stockmeyer L: On the minimal synchronism
needed for distributed consensus. J ACM 34(1): 77— 97 (1987)
9. Even S: Graph algorithms. Computer Science Press, 1979
10. Fischer MJ, Lynch NA, Paterson MS: Impossibility of distributed consensus with one faulty process. J ACM 32(2): 374—382
(1985)
11. Herlihy M: Wait-free synchronization. ACM Trans Program
Lang Syst 13(1): 124 —149 (1991)
12. Herlihy MP, Wing JM: Linearizability: a correctness condition
for concurrent objects. ACM Trans Program Lang Syst 12(3):
463— 492 (1990)
13. Hopcroft JE, Ullman JD: Introduction to automata theory,
languages, and computation. Addison-Wesley, 1979
14. Jayanti P: On the robustness of Herlihy’s hierarchy. In: Proceedings of the 12th ACM Symposium on Principles of Distributed
Computing, pp 145—158. ACM Press, August 1993
15. Jayanti P, Chandra TD, Toueg S: Fault-tolerant wait-free shared
objects. In: Proceedings of the 33rd Symposium on Foundations
of Computer Science, pp 157—166. IEEE Computer Society
Press, October 1992 (A revised and expanded version exists [16])
16. Jayanti P, Chandra TD, Toueg S: Fault-tolerant wait-free shared
objects. Technical Report 96-1565, Department of Computer
Science, Cornell University, January 1996
17. Jayanti P, Toueg S: Some results on the impossibility, universality, and decidability of consensus. In: Segall A, Zaks S (eds)
Proceedings of the 6th International Workshop on Distributed
Algorithms. Lect Notes Comput Sci, vol 647, pp 69 —84.
Springer, Berlin Heidelberg New York 1992
18. Kleinberg J, Mullainathan S: Resource bounds and combinations of consensus objects. In: Proceedings of the 12th ACM
Symposium on Principles of Distributed Computing, pp
133—144. ACM Press, August 1993
19. Lamport L: On interprocess communication. Part II: Algorithms. Distrib Comput 1(2): 86—101 (1986)
20. Loui MC, Abu-Amara HH: Memory requirements for agreement among unreliable asynchronous processors. In: Preparata
FP (ed) Advances in Computing Research, vol 4, pp 163—183.
JAI Press, 1987

21. Moran S, Rappoport L: On the robustness of h3 . To appear in:
.
Babaog\ lu O®, Marzullo K (eds) Proceedings of the 10th International Workshop on Distributed Algorithms. Lect Notes
Comput Sci. Springer, Berlin Heidelberg New York 1996
22. Newman-Wolfe R: A protocol for wait-free, atomic, multi-reader
shared variables. In: Proceedings of the 6th ACM Symposium on
Principles of Distributed Computing, pp 232—248. ACM Press,
August 1987
23. Peterson GL: Concurrent reading while writing. ACM Trans
Program Lang Syst 5(1): 46 —55 (1983)
24. Peterson GL, Bazzi RA, Neiger G: A gap theorem for consensus
types. In: Proceedings of the 13th ACM Symposium on
Principles of Distributed Computing, pp 344 —353. ACM Press,
August 1994.
25. Peterson GL, Burns JE: Concurrent reading while writing II: the
multi-writer case. In: Proceedings of the 28th Symposium on
Foundations of Computer Science, pp 383—392. IEEE Computer
Society Press, October 1987
26. Plotkin S: Sticky bits and the universality of consensus. In:
Proceedings of the 8th ACM Symposium on Principles of
Distributed Computing, pp 159—175. ACM Press, August 1989
27. Singh AK, Anderson JH, Gouda MG: The elusive atomic register. J ACM 41(2): 311—339 (1994)

Rida A. Bazzi received his B.E. in Computer and Communications
Engineering from the American University of Beirut in 1989, and an
M.Sc. and a Ph.D. in Computer Science from Georgia Institute of
Technology in 1994. After working as a senior consultant at International Integration Inc. in Cambridge, Massachusetts, he joined the
School of Computer Science at Florida International University in
August 1995. Currently he is an assistant professor in the Computer
Science and Engineering Department at Arizona State University.
His major research interests are distributed computing, fault tolerance, and computer vision.

Gil Neiger was born on February 19, 1957 in New York, New
York. In June 1979, he received the A.B. in Mathematics and Psycholinguistics from Brown University in Providence, Rhode Island. In
February 1985, he spent two weeks picking cotton in Nicaragua in
a brigade of international volunteers. In January 1986, he received
the M.S. in Computer Science from Cornell University in Ithaca,
New York and, in August 1988, he received the Ph.D. in Computer
Science, also from Cornell University. Dr. Neiger is currently a
Research Scientist at Intel’s MicroComputer Research Labs in
Hillsboro, Oregon. He is a member of the editorial boards of the
Chicago Journal of ¹heoretical Computer Science and the Journal of
Parallel and Distributed Computing.

Gary L. Peterson received a B.S. degree in Mathematics: Computer Science and a M.S. degree in Mathematics from Portland State
University. He has a Ph.D. degree in Computer Science from the
University of Washington. He has been on the faculty at the University of Rochester and Georgia Institute of Technology and is currently on the faculty in the Department of Computer and Information Science at Spelman College. His research interests include theory of concurrent control, algorithms and data structures, and complexity theory.

On the Establishment of Distinct Identities in Overlay
Networks
Rida A. Bazzi

Goran Konjevod

∗

Computer Science and Engineering
Arizona State University
Tempe, Arizona

Computer Science and Engineering
Arizona State University
Tempe, Arizona

bazzi@asu.edu

goran@asu.edu

ABSTRACT

1. INTRODUCTION

We study ways to restrict or prevent the damage that can be
caused in a peer-to-peer network by corrupt entities creating multiple pseudonyms. We show that it is possible to remotely issue certificates that can be used to test the distinctness of identities. To our knowledge, this is the first work
that shows that remote anonymous certification of identity
is possible under adversarial conditions. Our certification
protocols are based on geometric techniques that establish
location information in a fault-tolerant and distributed fashion. They do not rely on a centralized certifying authority
or infrastructure that has direct knowledge of entities in the
system, and work in Euclidean or spherical geometry of arbitrary dimension. Our protocols tolerate corrupt entities,
including corrupt certifiers as well as collusion by certification applicants and certifiers. We consider both broadcast
and point-to-point message passing models.

In a large scale peer-to-peer overlay network, physical entities that reside on different physical nodes communicate
with each other using pseudonyms or logical identities. In
the absence of direct physical knowledge of a remote entity
or a certification by a central authority that a particular
identity resides in a particular node, an entity can appear
in the system under different names or counterfeit identities. Counterfeit identities are problematic in a peer-to-peer
system because they can prevent entities from performing a
remote operation, such as saving a file, multiple times to increase availability. An entity might select different identities
to perform an operation, but these identities can all reside
on the same corrupt entity resulting in a loss of redundancy.
Counterfeit identities can also prevent the formation of reliable reputation-based recommendation systems. An entity
that can create counterfeit identities can also create identities with fake reputations which makes reputations meaningless. Douceur [4] calls the forging of multiple identities a
Sybil attack.
In this paper we study ways to restrict or prevent the damage that can result from corrupt entities performing Sybil
attacks. In other words, we are interested in mechanisms
to restrict the damage due to the creation of pseudonyms,
while not relying on a centralized certifying authority or infrastructure with direct knowledge of entities in the system.
While standard authentication techniques work well to prevent impersonation of existing identitities, they do not address the issues arising from the proliferation of pseudonyms.
To our knowledge, the first work that studies counterfeit
identities is the paper by Douceur [4]. He argues that under the strictest requirements, that is, in a fully distributed
system without a central authority and in which entities
communicate by broadcasting messages, the only means to
limit the generation of multiple identities is by exploiting
the fact that resources of individual entities are bounded.1
Douceur argues that, by requiring any entity trying to establish an identity to dedicate a significant portion of its
resources to this purpose, one could, at least theoretically,
limit the number of identities that are forged by a corrupt
entity. The three types of resources he considers are: computation, communication and storage.

Categories and Subject Descriptors
C.2.4 [Computer-communication networks]: Distributed
systems; H.3.4 [Information storage and retrieval]: Distributed systems; C.2.2 [Network protocols]: Applications; K.6.5 [Computing milieux]: Manatement of computing and information systems—security and protection

General Terms
algorithms, security, theory, verification

Keywords
sybil attack, identity verification, security, overlay networks,
peer-to-peer systems, fault-tolerance, distance geometry

∗Supported in part by the NSF Grant CCR-0209138

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’05, July 17–20, 2005, Las Vegas, Nevada, USA.
Copyright 2005 ACM 1-58113-994-2/05/0007 ...$5.00.

1
The absence of a central authority precludes the use of IP
addresses to identify entities because they rely on the authority of ICANN. Furthermore, in real systems IP addresses
can be spoofed and a host might be provided dynamic IP
addresses by its ISP.

312

Our main observation is that the damage caused by Sybil
attacks in Douceur’s model is not only due to the fact that
corrupt entities can forge multiple identities, but is also due
to the fact that, in the absence of additional information,
and for any two identities one of which is corrupt and has
unbounded resources, one cannot conduct a test to determine that their entities are distinct. So, our goal need not
necessarily be to test that any two particular identities are
distinct, but rather to test that amongst a group of identities, a large enough subset of them resides on a set of distinct
entities. Realizing such a test would allow the remote execution of remote operations and therefore circumvent the harm
done by Sybil attacks. An example illustrates this point.
Assume that one can divide identities into separate groups
such that two identities in different groups are distinct, but
two identities in the same group are not necessarily distinct.
For concreteness, also assume that there is only one corrupt
entity in the system. Under these assumptions, if an entity
asks n entities in one group to perform an operation and
another n entities in another distinct group to perform the
same operation, it can be guaranteed that at least n distinct
entities performed the operation even though it cannot tell
which ones they are. If the operation consists of saving a
file, the entity can be assured that there are enough correct
replicas of the file in the system. The goal of this paper
it to show that such a test is possible in real systems and
to explore conditions under which such a test can be made
accurately.
We develop our work by exploiting an ingredient that was
eliminated by the strong assumptions of Douceur, namely
that entities have certain physical properties, in particular, locations. Most of the real distributed systems we can
imagine are in some way embedded in space with geometric
properties. Moreover, the entities in the system have (at
any moment in time) their own physical locations and no
two entities share the exact same location at any moment.
In general, we can assume that the underlying space (whose
points include all the participants in the protocol) has a geometric structure of standard d-dimensional Euclidean space
d
or sphere d. For sound or radio communication these assumptions are quite realistic (even though accuracy of measurement is always an issue) and they have already been
exploited for secure location verification [6], while in the
case of Internet-based overlay networks they are justified by
recent work on estimating network distances [7] (we further
discuss these assumptions as they relate to the Internet in
Section 6). We distill our assumptions into the following:

the time in which a message is transmitted from point x to
point y gives an upper bound on the distance d(x, y) between
the two points.

1.1 An example
To illustrate how physical locations can be used to provide a test of distinctness, consider two correct entities A
and B at a distance d from each other. Assume that there
is only one corrupt entity C in the system, that C has unbounded resources, and that C is within a radius of d/2
from A. Under these conditions, C can forge an unbounded
number of identities, but all of these forged identities cannot pretend to be at a distance less than d/2 from B. In
fact, for each identity c of C, one can request from A and
B an upper bound on their distances to c. These distances
can be obtained by having A and B broadcast probe messages to c and measure the time it takes to receive a reply
from c. Since the distance from C to A is less than d/2 and
the distance from A to B is d, it follows from the triangle
inequality that the roundtrip time of probes sent from B
to C (under any of its pseudonyms) will always indicate a
distance that is larger than d/2 and therefore none of C’s
identities can prove that they are within radius d/2 from B.
Using this test of distinctness, an entity can require that a
remote operation be executed by n identities that can prove
that they are within a radius of d/2 from A and another
n identities that can prove that they are within a radius of
d/2 from B and therefore be guaranteed that enough correct
entities executed the operation. One can use the distance
between A and B and their distance from c as a certificate
of identity of c. Such a certificate allows one to determine
that two identities are distinct if one certificate shows a distance smaller than d/2 to A and the other shows a distance
smaller than d/2 from B. These certificates can be signed
by A and B and later be presented to a different entity to
prove distinctness. Note that such certificates give sufficient
but not necessary conditions for distinguishing identities.
We should emphasize that A and B in the example above
are not the same as a centralized certifying authority (we
discuss this point further in Section 6). In fact, A and B’s
knowledge of C or its forged identities is obtained solely
through remote interaction with C’s various identities and
with each other and the assumption that they are both honest (which we will not require in general), whereas a centralized certifying authority requires some form of direct knowledge of C. Also, note that A and B need not know each
other’s location, they only need to know the distance between them and that they are both honest.



1. the actual distances between pairs of entities at least
approximately satisfy the metric properties (symmetry, definiteness, triangle inequality), and

1.2 Paper Outline
The goal of this paper is to study various scenarios under which entities such as A and B in the example above
can be used to significantly restrict the types of counterfeit identities by a corrupt entity and therefore eliminate
the harm caused by Sybil attacks. We show that one can
construct certificates that are much more powerful than the
one suggested above and that can be used under stronger
adversarial conditions. The rest of the paper is organized
as follows. Section 2 defines our system models, with additional discussion on the models available in Section 6. Section 3 summarizes our results and contributions, Section 4
discusses related work, and Section 5 present details of our
results for the various models.

2. the transfer of a message back and forth between two
identities takes time that is lower-bounded by a (nondecreasing) function of the distance between the two
entities on which they reside.
We do not assume that logical identities are always honest,
and we place no bounds on the computational resources of
corrupt entities. However, since each entity is located at a
point in a geometric space, its communication with the rest
of the identities in the protocol is restricted by the geometry
of the space. In particular, a simple assumption of finite
message propagation implies our second assumption above:

313

2.

2.5 Synchrony and Reliability

SYSTEM MODEL

We consider a system consisting of a set B of n beacons
and a set of A of applicants. The set A ∪ B is the set of
participants. We assume the participants are points in either
the standard d-dimensional Euclidean space d or the ddimensional unit sphere d. In making statements that hold
for both d and d, we refer to the space as X. We denote
by ρ the metric in X, that is, if x, y ∈ X, then ρ(x, y) is the
distance between x and y. We assume that participants are
not mobile and that their locations are fixed.

We assume that the system is asynchronous and that message transmission is not reliable, but that there are periods
of time during which message transmission is synchronous
and reliable. We assume that for large enough time intervals,
the system will enter a synchronous period. While these assumptions do not really simplify online communication between peers in an overlay network, because peers cannot
wait for the periods of synchrony, they are necessary for establishing geometric certificates. The idea is to have participants probe each other for a somewhat long period of time
in order to get an accurate measure of distance. In fact, we
expect that the measurements during periods of synchrony
(low congestion periods) accurately reflect the distances between correct participants. Once certificates are obtained,
we do not require any synchrony assumptions for communications between applicants. Beacons have local clocks and
the rate of drift of these clocks is small enough so that clock
drift is negligible during the time it takes to establish a certificate.





2.1 Communication
Beacons communicate with each other and with applicants
by exchanging messages. We distinguish two models for the
messages transmitted by the participants: broadcast and
point-to-point.

2.2 Failures
Some applicants and some beacons might be faulty (or
corrupt). We assume that no more than f beacons are corrupt. The remaining beacons are correct (or honest). A
corrupt beacon can report fake distances to any participant,
and these distances can be smaller or larger than the actual distance to the participant. An applicant can also be
corrupt and it can delay its responses for probes from the
beacons therefore making it appear farther away than it really is. We consider cases in which applicants might collude
and cases in which corrupt applicants do not collude. To
strengthen our adversarial model, we assume that corrupt
applicants and beacons know the locations of all beacons
and that correct beacons do not know each other’s locations
other than what can be implied from the time it takes to
receive replies from probes. We assume that the distance
between two correct entities is a non-decreasing function of
the roundtrip delay between them (we discuss this and our
other assumptions in detail in Section 6). We use µ for the
distances as measured by exchanging messages between the
points. Thus µ(A, B) is the distance A can deduce from the
roundtrip time of a message transmitted from A to B and
back. For correct participants A and B, we assume that
µ(A, B) = ρ(A, B) that is, the distance can be accurately
measured by observing the roundtrip delay. Note that in the
presence of faulty participants µ is not necessarily symmetric. For a participant A in the system, we denote by x(A)
the location of A in the underlying geometric space.

3. CONTRIBUTIONS AND SUMMARY OF
RESULTS
The main contribution of this work is to show that it is
possible to remotely issue certificates that can be used to test
the distinctness of identities. To our knowledge this is the
first work that shows that remote anonymous certification
of identity is possible under adversarial conditions.
The following is the summary of our results. We present
geometric certification protocols, which issue compact and
easily-checkable certificates to applicants. Given two certified entities, a distinctness test may be performed, and if the
two entities’ geometric locations are distinguishable from the
point of view of the beacons that participated in the certification protocol, the distinctness test will succeed and certify
that the two entities are indeed distinct. The certification
protocols we present work for several different settings, including the following (in all cases, we assume the number
of beacons is at least d + 1, where d is the dimension of
the space; also, unless otherwise specified, the applicant entity or entities should be in the convex hull of the certifying
beacon set; finally, beacons’ messages are always broadcast):
(1) honest participants: no restrictions;
(2) corrupt applicant: either applicant in the convex hull of
beacons (in d ), or a sufficient set of beacons (in d) without
restriction on the applicant’s location (for an exact definition, see Section 5.1.2);
(3) multiple colluding entities: broadcast message model;
(4) multiple colluding entities (at most d of them): pointto-point (arbitrary) applicant message model;
(5) up to f corrupt beacons, at least f + d + 1 correct: single corrupt applicant entity, or multiple colluding applicant
entities.

2.3 Geometric Certificates



An applicant can request a geometric certificate from a set
of beacons. When an applicant requests a geometric certificate, the beacons and the applicant execute a protocol that
might require the applicant (as well as other beacons) to respond to probe messages. The protocol might also require
the applicant to send probe messages to the beacons and
report distances to various beacons. The result of these exchanges will be a geometric certificate: a set of distance values between the beacons and the applicants that are signed
by the beacons as well as the applicant.

4. RELATED WORK

Ng and Zhang [7] model the Internet as a geometric space
by using measurements of round-trip delay for ICMP ping
A distinctness test is a function D : C×C 7→ {true, unknown} messages between a set of known hosts probes and several
that assigns to a pair of geometric certificates a value in the
sets of targets. They assign the targets to points in a coset {true, unknown}. If D(c1 , c2 ) = true, then the entities
ordinate system, by defining each coordinate of a node as
that obtained these certificates are distinct.
its distance from one of the probes. This embedding into a

2.4 Distinctness Test

314

In our model, we assume only that the distances between
beacons can be calculated, while the locations of beacons are
unknown. Given a distance matrix Md whose entries are the
pairwise distances between points in a geometric space, it is
possible to find a set of points expressed in an orthonormal
coordinate system and whose distance matrix is identical to
Md [2, 3]. If all beacons are correct, these methods can be
used to transform a distance matrix representation into a
coordinate system representation. In the presence of faulty
beacons, the calculated distance matrix might not be realizable in a geometric space and a coordinate representation
consistent with all the beacons might not be possible. Still,
in the presence of faulty beacons, the distance matrix is realizable if it is restricted to the set of correct beacons. So,
our goal would be to find a realization that is guaranteed to
be consistent with the set of correct beacons. Assuming that
the set of correct beacons is in general position (that is, no
(d + 1)-subset is contained in a d-dimensional hyperplane,
and no (d + 2)-subset is contained in a d-sphere), this can
be easily achieved by considering either all sets of d + 1 or
all sets of d + f + 1 beacons (depending on which of the two
families is smaller). In the first case, we use each (d + 1)-set
to build a coordinate representation and then check if there
are another f beacons consistent with this representation.
In the second case, we look for a consistent (d + f + 1)-set of
beacons. In case such a set is found, it must contain at least
d+1 correct beacons, therefore the coordinate representation
defined by this set is consistent with all the correct beacons
and every beacon inconsistent with this representation can
be discarded as faulty.
It is important to note that, while the procedure described
above is expensive—being exponential in the (usually small
constant) d, and including a verification of the positivesemidefinitness of a matrix—it is only performed once for
an applicant to establish the certificate. The size of the certificate itself is small, and the test of distinctness efficient.

low-dimensional Euclidean space allows them to derive simple lower and upper bounds on distances between targets
from their probe-target measurements by using the triangle
inequality.
Kleinberg et al. [5] design algorithms that try to infer a
complete distance matrix of a finite set of points, given only
the distances from a small number of selected points (beacons in their terminology) to every other point. They show
that most of the distances can actually be reconstructed even
from such limited data, but also that arbitrary distortion of
a certain fraction of all distances is unavoidable. They use
some of the powerful recent results on metric embeddings
and provide very general algorithms, however their results
do not seem to have immediate applications to the Sybil
attack problem.
Newsome et al. [6] study the Sybil attack in the context of sensor networks, and so their approach relies heavily on strong restrictions of computational, communication
and memory resources available to the nodes. While these
restrictions are realistic for certain sensor networks, their
techniques do not exploit locational properties of the nodes.
The closest to our approach in the existing literature seems
to be the paper of Sastry et al. [8]. They do study protocols for establishing identity of nodes based on their location. Their methods only use single beacons (verifiers in
their terminology). This doesn’t allow them to determine
exact locations of nodes and reduces most of the problems
to simple covering problems. Also, they do not consider adversarial conditions such as faulty beacons or collusions by
applicants.

5.

GEOMETRIC CERTIFICATION
PROTOCOLS

In this section we present our results under various system
assumptions. For each set of assumptions, we state our results in the form of a theorem that specifies conditions under
which a participant (or group of participants) is incapable
of pretending to be in a location other than the real location
of the participant or one of the group members. We say that
a participant (or a group of participants) simulates a point,
if it can make all its communications appear to come from
the point.
These results can be readily used to construct geometric certificates for the applicant. In each case, a certificate
would consist of the set of measurements that is sufficient to
uniquely identify the location of an entity, and a test of distinctness is simply a comparison between the two locations
defined by two certificates.
All our results are stated assuming the distance between
correct participants is accurately measured using roundtrip
delays (as explained in Section 2). These results can be extended to the case in which measurements are not accurate.
For that case, the statements of the theorems will change to
specify conditions under which an applicant is incapable of
pretending to be outside of a well-defined neighborhood of
its actual location. In the presence of inaccuracies, a certificate consists of the measurements that establish a neighborhood of the applicant’s location, and a test of distinctness
is simply the test of disjointness of two such neighborhoods.
While we do not describe such protocols here, our results can
be generalized to account for small inaccuracies (as outlined
in Section 5.4).

5.1 Honest beacons with known locations
5.1.1 Trilateration in an honest world
If all participants in the protocol are honest, then the
problem is easy. To determine the exact location of a point
x(A) in d-dimensional space, it is enough to know all the distances ρ(x(A), x(Bi )) between x(A) and d + 1 other affinely
independent points x(B1 ), . . . x(Bd+1 ). With this information, the point x(A) can be reconstructed as follows: let Si
be the sphere of diameter ρ(x(A), x(Bi )) around x(Bi ). The
point x(A) belongs to Si for every i. A sphere with center
c = (c1 , . . . , cd ) and radius r is the set of all points x =
(x1 , . . . , xd ) that satisfy the equation i (xi − ci )2 − r2 = 0.
Equating the left-hand sides of the equations for Si and Sj
gives a linear equation in xi , thus the intersection of two
spheres belongs to a hyperplane. Since we assume general
position, each pair S1 , Si defines a hyperplane, which we denote by Hi . Since S1 ∩ Si ⊆ Hi , it follows that x(A) ∈ ∩i Hi ,
and we can determine x(A) by solving a linear system.

5.1.2 Trilateration against cheaters
In the situation where the applicant may cheat by pretending to be at a different location, the protocol should
compute the applicant’s position or detect the cheating. We
first discuss the case where a single point attempts to cheat
without colluding with other entities.

315

unit vectors +e1 , −e1 , +e2 , −e2 , . . . , +ed , −ed in both orientations (that is, if d−1 is considered as a subset of d , the
beacons are at the vertices of the polar of the inscribed ddimensional cube). Then no point x0 ∈ d−1 can be simulated by any other point.

Consider an applicant at A that attempts to impersonate a point x0 6= x(A). The applicant contacts d + 1 beacons B1 , . . . , Bd+1 and exchanges a message with each of
them. Let µi = µ(Bi , A). If A can successfully impersonate x0 , then µ(Bi , A) = ρ(x(Bi ), x0 ) for every i. Since
µ(Bi , A) ≥ ρ(x(Bi ), x(A)), it follows that ρ(x(Bi ), x(A)) ≤
ρ(x(Bi ), x0 ) for every i, that is, x0 ∈ D1 ∩ · · · ∩ Dk , where
Di = (x(Bi ), µ(Bi , A)), the ball of radius µ(Bi , A) around
Bi . For a set Z, we use int Z to denote its interior, and
convZ its convex hull.





Proof. (Sketch.) First, assume without loss of generality that x0 is in the interior of the positive orthant. (By
symmetry x0 can be assumed to be in the positive orthant,
and if it is on the boundary, then we can restrict the discussion to d−2 and continue the proof there using only the
appropriate subset of beacons.) We can now use only the
d beacons located at +e1 , . . . , +ed , We claim that for any
x∗ 6= x0 , there exists an i such that ρ(bi , x∗ ) > ρ(bi , x0 ).
For the most interesting case, where the simulating point is
also located in the positive orthant, the proof is the same
as that for Euclidean space in Theorem 1 with the definitions of convexity, distance and hyperplane modified to fit
the spherical geometry of d−1.


Theorem 1. Let X = d . Let Bi be a beacon with x(Bi ) =
bi for each i = 1, . . . , d + 1. If {b1 , . . . , bd+1 } is affinely independent and x0 ∈ int conv{b1 , . . . , bd+1 }, then x0 cannot be
simulated by any other point.
Proof. First, the set S of all points that can simulate x0
can be written as S = {x | ∀i ρ(bi , x) ≤ ρ(bi , x0 )}. If S 6= ∅,
take x∗ ∈ S. Since S is an intersection of closed balls, it is
convex and so xλ = λx∗ + (1 − λ)x0 ∈ S for all 0 ≤ λ ≤ 1.
Take λ > 0 small enough that xλ ∈ conv{b1 , . . . , bd+1 }. If
we can show that for some i∗ , ρ(bi∗ , xλ ) > ρ(bi∗ , x0 ), it will
follow that xλ 6∈ S, and the proof by contradiction will be
complete. So assume that ρ(bi , xλ ) ≤ ρ(bi , x0 ) for all i. Let
H be the hyperplane through 21 (x0 + xλ) with normal vector
xλ − x0 . H is exactly the set of points at equal distance
to x0 and xλ . This implies that for every i, bi is on the
same side of H as xλ , in other words, the hyperplane H
separates x0 from bi for every i. This contradicts the fact
that x0 ∈ conv{b1 , . . . , bd+1 }.



The boundedness of the sphere makes it possible to use
a single “universal” set of beacons to distinguish any point
from any other. We do not claim that we can always place
beacons exactly at the locations used in Theorem 2. However, this theorem gives a sufficient condition to ensure that
every point in the space is contained in the convex hull of
some set of beacons, and therefore cannot be simulated by
any other point. (As long as the number of beacons is finite,
this cannot be done in Euclidean space because the applicant can always be far enough to be outside of the convex
hull of the beacons.) In practice, we may use any appropriate beacon set, but distinguishability may be more difficult
to guarantee if the beacons used are not all within a single half-sphere because then we must be very careful about
convexity.

If the underlying geometric space X is d , then the theorem above gives necessary and sufficient conditions for a set
of beacons to be able to detect a cheating point. If x0 is not
in the convex hull of the beacon set, it may be impossible
to detect cheating.
However, on the sphere d, the situation is different, and
in fact much better. Note first that on a sphere, the distance between a pair of points is measured along a geodesic
curve (great circle) that connects the pair. The notion of
convexity can then also be defined for the sphere. Given
two points x, y ∈ d, we would like to define their convex
hull as the set of all points on the shorter segment of the
geodesic between x and y. This works because there is a
unique x-y geodesic as long as x and y are not antipodal
points. (If x and y are antipodal, then there are infinitely
many geodesics between x and y, and in fact every point
on the sphere lies on one of them.) Now consider a finite set of points X = {x1 , . . . , xk } on the d-dimensional
sphere, such that X is contained in some open half-sphere
(this ensures that no two points of X are antipodal, and
more generally that our definition is useful). We define
the convex hull conv(X) inductively: conv({x1 , x2 }) is the
(shorter) segment of the geodesic between x1 and x2 . Then
suppose we have defined Ci =conv({x1 , . . . , xi }). We define
Ci+1 =conv({x1 , . . . , xi , xi+1 }) to be the union of segments
between xi+1 and y, where y ranges over all the points in Ci .
(The same definition can be stated for the Euclidean space
and results in the standard notion of convexity there.)
As an example of our claim that the situation on the
sphere is even better than in the Euclidean space, consider
the following theorem.

Theorem 3. Let x0 be a point surrounded by an independent set of beacons {B1 , . . . , Bd+1 }, either in the sense
of Theorem 1 in d or in the sense of Theorem 2 in d.
In the broadcast model, x0 cannot be simulated by any set
{A1 , . . . , Ak } of entities unless x(Ai ) = x0 for some i.

Theorem 2. Let X = d−1. Let b1 , . . . , b2d (the locations
of beacons B1 , . . . B2d ) be the points defined by the coordinate

In the proof of the theorem, we simply use A to denote
the entity of the applicant. The first reason for this is that

5.2 Multiple colluding entities



We have seen that it is impossible for a single applicant
at point x∗ to impersonate any other point x0 in the convex
hull of a sufficiently large set of active beacons. However,
the proof was based on the fact that the distance from x∗ to
some beacon would have to be greater than from x0 to the
same beacon and so x∗ couldn’t return messages in time. If
several entities located at different points collude to jointly
impersonate another point, our protocols from Section 5.1.2
don’t work anymore. In fact, in this setting there is a significant difference between the problem under the broadcast
and point-to-point models.



5.2.1 Broadcast messages
In the broadcast model the applicant cannot send a message to a single recipient. Instead, every message sent is
broadcast and thus received by every other entity (or at
least every entity expecting a message). More precisely, every message sent by an applicant A at time t is received by
every beacon Bi at time t + ρ(x(A), x(Bi )).





316

no beacon can tell which Ai sends the message just by the
content of the message since A1 , . . . , Ak are in collusion. The
second reason is that the broadcast model ensures that each
message sent by any of the entities A1 , . . . , Ak is received by
all beacons.
Proof. The protocol begins by the beacon B1 sending a
probe at time 0. The applicant responds (broadcasting to all
the beacons). Upon receiving the response, each beacon immediately forwards it to B1 . Now B1 records µ(B1 , A) and
the times at which it receives the responses from the other
beacons. Given the distances ρ(x(B1 ), x(Bi )), the beacon
can deduce the time it took the message from A to arrive
to each beacon by subracting µ(B1 , A)/2 + ρ(x(B1 ), x(Bi ))
from the time ti at which the response from Bi arrived.
Therefore, the beacon B1 can calculate the distance µ(A, Bi )
for each i. Now by Theorem 1 for d or by Theorem 2 for
d
, the point x0 cannot be simulated by any other point.

B3

3
x0
A1

2

1
B2

B1

(a) A1 cannot cheat B3 because A1
doesn’t lie in region 3

C



5.2.2 Point-to-point messages

D

The case in which an applicant can send a message to
any single beacon appears to be substantially more difficult
in the case of colluding entities. For example, our protocols
from the previous section fail because the simulating entities
A1 , . . . , Ak can reply to the beacons selectively, so that Ai
sends a message to Bj only if Ai is closer to Bj than x0 . Thus
a point x0 can be simulated whenever for each beacon Bi
there exists an Aj such that ρ(x(Aj ), x(Bi )) ≤ ρ(x0 , x(Bi )).

E
B

A
(b) |AE| + |EB| ≤ |AC| + |CB|

Figure 1: Proof that two entities cannot cheat.

Theorem 4. Let X = d and let x0 be a point in the
convex hull of d + 1 affinely independent beacons. Then no d
entities can simulate x0 unless one of them is located at x0 .

hyand all the beacons except for Bi and Bj . The d+1
2
perplanes Hi,j all meet at x0 , and the intersection of Hi,j
with S can be written as a union of d subsets, all defined by
the intersections of Hi,j with the other hyperplanes. (For
an illustration in the case d = 3, consider Figure 5.2.2.) Of
the d parts into which Hi,j is broken up by the boundary of
S and the other hyperplanes, d − 1 contain a beacon point
(recall that Hi,j passes through d − 1 of the beacon points).
The d-th one does not and this is the one we focus on. Denote by Fi,j this subset of Hi,j bounded by the intersections
of Hi,j with the other hyperplanes and by the boundary of
S. Then the d+1
hyperplane segments Fi,j break up S
2
into d + 1 pieces, each containing exactly one of the beacon
points. Call these sets Ti .
Since there are only d colluding points and d + 1 beacons,


Proof. (Sketch.) We describe the protocol and give the
proof for d = 2, because it is easy to visualize and contains
all the necessary ideas.
The protocol consists of two rounds. In the first round,
the beacons compute x0 from the values of µ(Bi , A), i =
1, . . . , d + 1. In the second round, the beacons synchronize
clocks and broadcast messages M1 , . . . , Md+1 . The message
Mi is sent by beacon Bi , so that at time t0 all d+1 messages
simultaneously reach x0 . The messages should be impossible to forge. A simple solution is that each beacon sends
a random message to the applicant. The applicant must
then immediately forward all three messages to each beacon. (The beacons later verify that the forwarded messages
are indeed the messages they sent to the applicant.)
Consider the situation for d = 2, where there are three
beacons (B1 , B2 and B3 ) and two entities, (A1 and A2 ). In
order that A1 and A2 successfully simulate x0 , each beacon
Bj must receive a message forwarded from either A1 or A2
on time (no later than by time t0 + ρ(x0 , x(Bj ))). From the
definition of the protocol, if Ai can forward the required
message to Bj on time, it must be true that



B3

ρ(x(Bk ), x0 )+ρ(x0 , x(Bj )) ≤ ρ(x(Bk ), x(Ai ))+ρ(x(Ai), x(Bj ))
for all k 6= j. A necessary condition for this is (see Figure 1(a)) that Ai lies on the same side as Bj of the two lines
defined by (x0 , Bk ) for k 6= j. (For a proof of this, consider
Figure 1(b)).
This proof generalizes quite directly to the case of arbitrary dimension d. Let S =conv({x(B1 ), . . . , x(Bd+1 )}) be
the convex hull of an affinely independent set of d + 1 beacon points. Notice that S is a d-simplex. For each pair of
beacons Bi , Bj , let Hi,j be the hyperplane spanned by x0

B4

x0

B2

B1
Figure 2: In three dimensions, three entities cannot
cheat.

317

B3

there exists an i such that the interior of Si contains no
colluding points. Now we can proceed to the second part of
the proof and consider the simplex Si0 spanned by x0 and all
the beacons except for Bi :
Si0 = conv({x0 , x(B1 ), . . . , x(Bd+1 )} \ {x(Bi )}).
For any any colluding entity A` , there exist two vertices of
Si0 , say, x(Bj ) and x(Bk ), such that A` is neither in Hi,j
nor in Hi,k . This then implies ρ(x(Bj ), x0 ) + ρ(x0 , x(Bk )) <
ρ(x(Bj ), x(A` )) + ρ(x(A`), x(Bk )), that is, A` cannot simulate the message sent from Bi to Bj .

x0

5.3 Trilateration with corrupt beacons
In the presence of faults, we cannot rely on the operation
of any single beacon to work as specified by the protocols.
We now show how to tolerate beacon failures.

B1

Theorem 5. Let x0 be the location claimed by the applicant A. Consider a set of d + 1 + 2f affinely independent beacons surrounding x0 , at most f of which are faulty.
Then no applicant A can simulate x0 unless x(A) = x0 .
A certificate for the applicant can be constructed in time
min{ d+1+2f
, d+1+2f
}.
d+1+f
d+1


B2

Figure 3: The small angle case.

restrictive as it appears, especially in view of our suggestion
in Section 2.5 that each distance be measured more than
once over a period of time to ensure accuracy (for example,
if the measurements deviate from the true value according
to a reasonable probability distribution, the smallest of the
multiple measurements will generally tend to the true value
fairly quickly).



A set of d + 1 + 2f beacons contains at most f faulty, and
therefore at least d+1+f correct beacons. The f faulty beacons may, together with at most d correct beacons determine
an incorrect value for x(A). However, there are more correct
beacons and so the maximum subset of beacons agreeing on
a location for x0 is correct. Unfortunately, finding a maximum consistent subset of a set of n linear equalities is not
only NP-hard, but also hard to approximate within an n
factor for some  > 0 [1].

5.4.1 Inaccurate distance measurements
To account for the inaccuracies in measuring the distance,
we consider the variant of the problem where the measured
distance µ(Bi , A) is allowed to exceed the actual reported
distance by a small multiplicative factor: as long as there
exists a point x0 that satisfies ρ(x(Bi ), x0 ) ≤ µ(Bi , A) ≤
(1 + )ρ(x(Bi ), x0 ) for each i, the protocol should not reject
the applicant.
The problem of validating an applicant becomes equivalent to identifying an intersection of thin (because we assume
 to be small relative to the measured distances) spherical
shells (one around each of the beacons).
The goal of all our protocols is to distinguish between
different applicants. Therefore a natural measure of how
badly a protocol fails might be the smallest distance between points that cannot be reliably distinguished using the
protocol. Now imagine a region E such that no two points
in E can be reliably distinguished. Since E is an intersection
of shells around beacons and the thickness of each shell is
small compared to its radius, we may think of E as bounded
by almost straight planar surfaces. Consider as an example
the two dimensional case, and focus first on just two of the
beacons, B1 and B2 . The straight line segments from their
locations to the applicant’s location meet at an angle, say θ.
If θ is close to the right angle, then the indistinguishability
region is close to a square (actually, two disjoint squares, because two circles around B1 and B2 intersect in two points).
If θ is very small, the indistinguishability region looks more
like a thin parallelogram, and in such a case it can happen that two points a large distance apart cannot be distinguished. When the third beacon is included, it may still
be the case that the indistinguishability region has a large
diameter.
For this case, if the shell boundaries are replaced by straight

Proof. We check either all sets of d + 1 or all sets of
d + f + 1 beacons (depending on which of the two families is
smaller). In the first case, for every (d + 1)-set of beacons,
we find a candidate point for x0 (by solving a linear equality
system as in Section 5.1.1). Then for each beacon B, we
check if µ(B, A) = ρ(x(B), x0 ). If there are at least d + 1 + f
such beacons, then x(A) = x0 .
In the second case, if each beacon in a set of d + f +
1 beacons is consistent with x0 , then this set of beacons
defines a unique point, which must be the location of the
applicant, because a (d + 1)-subset of this set consists of
correct beacons, and so also defines the actual location of
A.
Theorem 6. Let x0 be the location claimed by the applicant A. Consider a set of d + 1 + 2f beacons surrounding
x0 , at most f of which are faulty. Then no set of entities
{A1 , . . . , Ak } can simulate x0 in the broadcast model unless
x(Ai ) = x0 for some i.
Proof. As in the proof of Theorem 5, if A is honest,
the set S will have a unique element, and more beacons
will agree on this point than on any other. The required
distances are computed as in the proof of Theorem 3.

5.4 Inaccuracies
In this section we very briefly describe a generalization of
the problem that allows inaccuracies in measured distances.
We only consider the simplest case, where measured distance
may vary by a small constant fraction (known in advance)
from the actual distance. This assumption may not be as

318

6.2 Limitation of distinctness tests

lines, we see that the distance between the two points farthest apart in the indistinguishability region is at most D =
2d cos(θ/2)
, where d is the distance between B1 and x0 . In
sin θ
other words, the diameter of the indistinguishability region
may increase proportionally to 1/ sin θ. We defer a more
detailed analysis of this problem to the full version of this
paper.

6.

Our approach is conservative. The distinctness tests we
propose are sufficient but not necessary to establish distinctness of two identities. For example, different machines that
reside on the same LAN would appear to be at the same location to remote beacons (assuming these machines do not
route through each other). This does not mean that we can
only use one machine from a set of machines that appear
to be at the same location. As our introductory example
shows, to execute a remote operation, one can choose multiple groups of machines such that machines in each group
appear to be at the same location. If the number of groups
exceed the assumed upper bound on faulty entities, then the
collection of groups would be guaranteed to contain at least
as many entities as in the smallest amongst them.
The distinctness tests we presented assume that the entity
under consideration is in the convex hull of the beacons in
the system. If an entity is outside the convex hull of the
beacons, then most of the theorems we prove do not hold. It
is reasonable to question whether the convex hull condition
is only of theoretical interest and if anything can be done
if an entity is not in the convex hull of the beacons. The
answer to the first part of the question would depend on
the actual network under consideration. We can expect,
and that needs to be verified by further study, that if the
network is dense and beacons are evenly distributed in the
network, then it should not be hard to find a set of beacons
that surround and entity. Even if the points are not in the
convex hull of the beacons, it seems possible that one can
generalize the introductory example

DISCUSSION OF MODEL

6.1 Certificates
It is important to realize that the set of beacons in our
model is not the same as a central certifying authority. In
fact, all we need to assume about the beacons is that they
are distinct and that a certain number of them are correct.
In principle, a given entity can establish the distinctness
of an initial set of beacons by using some of the resourceconsuming challenge-response described in [4] without requiring any certifying authority. The assumption that a
certain proportion of beacons chosen at random is correct is
a system assumption for we cannot expect a system with an
arbitrary number of faulty entities to be able to function.
Once an initial set of beacons is established, our results
show that they can be used remotely to establish the distinctness of identities created by entities with unbounded
computing power. This shows that the following lemma
from [4] (his notation is different from ours, but should be
clear from the context) does not hold once the geometric
properties of communication are considered.

6.3 Communication

Lemma 7. Lemma 4 [4] If the correct entities in set C
do not coordinate time intervals during which they accept
identities, and if local entity l accepts any identity vouched
for by q accepted identities, then even a minimally capable
faulty entity f can present g = b|C|/qc distinct identities to
l.

A number of our results assume that the only mode of
communication available to applicants is the broadcast of a
message to all other participants. These results have limited applicability because in actual overlay networks peers
communicate by a combination of point-to-point communication and broadcast. It is interesting to note, though,
that restricting communication to broadcast makes it harder
for faulty applicants to benefit from colluding, whereas in
Douceur’s argument broadcast makes is harder to prevent
Sybil attacks. The reason is that we assume that our broadcast primitives is an indivisible operation that cannot be
split into multiple point-to-point operations. If the broadcast operation can be implemented using multiple pointto-point operations, then our result for tolerating colluding
entities in the broadcast mode would no longer hold.

This lemma basically says that accepted identities cannot be used to accept further entities. We showed that, if
we take the geometric properties of communication into account, we can use accepted identities to accept additional
entities. In fact, in one of our results we showed that a set
of d + 1 + 2f , at most f of which are faulty, can prevent
one faulty entity ef aulty in their convex hull from presenting
distinct identities even if ef aulty has unbounded resources.
This result is achieved without assuming a central authority.
In practice, the beacons can be certified by a central certifying authority to bootstrap the system. Once a set of
beacons is certified, it can be used to provide certificates
remotely. In that case, an applicant that wants to obtain
a certificate from the set of beacons would identify beacons
that have valid public certificates obtained from the central
authority. Then, the applicant can initiate a geometric certificate request which will result in the beacons probing the
applicant as explained in the various protocols we presented.
These probes will be started by multiple beacons to obtain
the distances as required by the protocols. At the end of the
probing period, the beacons will present the applicant with
pieces of the geometric certificate (distances from beacon to
applicant or location of applicant as calculated by a beacon)
that the applicant can put together to obtain the geometric
certificate.

6.4 Accuracy of measured distances
Ng and Zhang [7] show that on the Internet the roundtrip
delays can be used to measure distances between entities if
enough measurements are taken and the minimum amongst
the measured delay is used as the distance measure. These
measurements were done using ICMP ping messages. In
our model, communication is done in an overlay network
that does not necessarily exhibit the same delay characteristics as those of the Internet. Nonetheless, we can expect that in periods of low congestion, the distances will
reflect the underlying network distances. In our work, establishing a geometric certificate can be done over a period
of time and multiple measurements can be taken and the
smallest times be included in the certificates. If the participants belong to common congestion zones (which can be

319

7. CONCLUSION

corollated with time zones), then we can expect that the
minimal delay measured by participants will exhibit metric
characteristics. Nevertheless, studying delay characteristics
in Internet-based overlay networks is a subject that needs
further study and our work is based on the assumption that
these characteristics are similar to those of the Internet.

We have shown that it is possible to exploit the geometric
properties of message transmission delay in order to reduce
the effects of Sybil attacks. We believe that a lot more
work is still needed to make this work of more practical
value. In particular, we believe that a generalization of the
introductory example that directly exploits the triangular
inequality has a good chance of leading to solutions that
can be used in practice.

6.5 Adversary model
Throughout, we assumed that the corrupt entities have
unbounded computation power. Our protocols implicitly
assume that an entity cannot anticipate probe messages and
send replies before the receipt of the actual probes. This
assumption can be easily enforced by using the standard
technique in which each probe message includes a randomly
generated string that only the sender knows and that must
also be included in the reply. This way, an entity would
have only a very small probability of successfully being able
to reply before receiving a probe.
The adversary model is particularly important for the accuracy of measured distances. In fact, a corrupt entity that
is used to route messages between non-corrupt beacons can
artificially increase the distance between them as well as between corrupt entities and beacons. Later, the calculated
distance could be shortened which violates a main assumption of our model. We do not have a fully satisfactory solution to this problem, but we have two approaches to deal
with it. The first approach applies to peer-to-peer systems
that exhibit locality characteristics. In such systems, the
distance between nodes is proportional to the actual network distance between the nodes. If the overlay network exhibits locality characteristics, we can calculate the networkdistances between beacons directly without going through
the overlay network and therefore without risking that the
routing is compromised (assuming the routing on the underlying network cannot be easily compromised). These
distances will be smaller than the distances on the overlay
network, but one could then use solutions that tolerate inaccuracies in the measured distances. The second approach
makes limiting assumptions on the disruption power of the
adversary. If we assume that any two nodes are connected
by a path that does not go through a corrupt node, then
we can use multiple paths to calculate the distance between
two nodes; The shortest amongst the calculated distances
would be chosen as the distance between two nodes.
Another potential difficulty can be cause by corrupt nodes
trying to flood the network with message in order to prevent
accurate measurements of distances. Dealing with such denial of service attacks is beyond the scope of this paper.

8. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their comments.

9. REFERENCES
[1] E. Amaldi and V. Kann. The complexity and
approximability of finding maximum feasible subsets of
linear relations. Theoretical Computer Science,
147:181–210, 1995.
[2] L. Blumenthal. Theory and applications of distance
geometry. Clarendon Press, 1953.
[3] M. Deza and M. Laurent. Geometry of cuts and
metrics. Springer, 1997.
[4] J. Douceur. The Sybil attack. In Proc. of IPTPS, pages
251–260, 2002.
[5] J. Kleinberg, A. Slivkins, and T. Wexler. Triangulation
and embedding using small sets of beacons. In Proc. of
the IEEE FOCS, pages 444–453, 2004.
[6] J. Newsome, E. Shi, D. Song, and A. Perrig. The Sybil
attack in sensor networks: analysis and defenses. In
Proc. of IPSN, 2004.
[7] T. Ng and H. Zhang. Predicting Internet network
distance with coordinates-based approaches. In Proc. of
INFOCOM, 2002.
[8] N. Sastry, U. Shankar, and D. Wagner. Secure
verification of location claims. In Proc. of ACM WiSe,
2003.

320

arXiv:1503.07235v2 [cs.SE] 10 Sep 2015

A Formal Study on Backward Compatible
Dynamic Software Updates
Jun Shen

Rida A. Bazzi

Arizona State University
jun.shen.1@asu.edu

Arizona State University
bazzi@asu.edu

Abstract

4.1
4.2

We study the dynamic software update problem for programs interacting with an environment that is not necessarily updated. We
argue that such updates should be backward compatible. We propose a general definition of backward compatibility and cases of
backward compatible program update. Based on our detailed study
of real world program evolution, we propose classes of backward
compatible update for interactive programs, which are included at
an average of 32% of all studied program changes. The definitions
of update classes are parameterized by our novel framework of program equivalence, which generalizes existing results on program
equivalence to non-terminating executions. Our study of backward
compatible updates is based on a typed extension of W language.

4.3

5 Program equivalence
5.1 Definitions of execution . . . . . . . . . . . . . .
5.2 Equivalent computation for terminating programs .
5.2.1 Proof rule for equivalent computation for
terminating programs . . . . . . . . . . .
5.2.2 Soundness of the proof rule for equivalent
computation for terminating programs . .
5.2.3 Supporting lemmas for the soundness proof
of equivalent computation for terminating
programs . . . . . . . . . . . . . . . . .
5.3 Termination in the same way . . . . . . . . . . .
5.3.1 Proof rule for termination in the same way
5.3.2 Soundness of the proof rule for termination
in the same way . . . . . . . . . . . . . .
5.3.3 Supporting lemmas for the soundness proof
of termination in the same way . . . . . .
5.4 Behavioral equivalence . . . . . . . . . . . . . .
5.4.1 Proof rule for behavioral equivalence . . .
5.4.2 Soundness of the proof rule for behavioral
equivalence . . . . . . . . . . . . . . . .
5.4.3 Supporting lemmas for the soundness proof
of behavioral equivalence . . . . . . . . .
5.5 Backward compatible DSU based on program
equivalence . . . . . . . . . . . . . . . . . . . .

Categories and Subject Descriptors D.3.1 [Formal Definitions
and Theory]: Semantics, Syntax; D.2.4 [Software/Program Verification]: Correctness Proof, Formal Methods; F.3.2 [Semantics of
Programming Languages]: Operational Semantics, Program Analysis; D.3.3 [Language Constructs and Features]: Input/output,
Procedures, functions, and subroutines
General Terms

Theory

Keywords dynamic software update, backward compatibility,
program equivalence, proof rule, operational semantic

Contents
1 Introduction
2 Backward compatibility
2.1 Programs and Specifications . . . . . . . . . . .
2.2 Hybrid executions and state mapping . . . . . .
2.3 Backward compatibility . . . . . . . . . . . . .
2.4 Backward compatible program behavior changes

2
.
.
.
.

3 Real world backward compatible update classes: brief
description
3.1 Observational equivalence: the old behavior . . . .
3.2 Enum. type extension: old behavior for old input
and allowing new input . . . . . . . . . . . . . .
3.3 Variable type weakening: more output when the old
program terminates . . . . . . . . . . . . . . . .
3.4 Exit on errors: stopping execution while the old
program produces more output . . . . . . . . . .
3.5 Improved prompt messages: functionally equivalent outputs . . . . . . . . . . . . . . . . . . . .
3.6 Missing variable initialization: enforcing restrictions on program states . . . . . . . . . . . . . .
4 Formal programming language

Syntax of the formal language . . . . . . . . . . .
Small-step operational semantics of the formal language . . . . . . . . . . . . . . . . . . . . . . .
Preliminary terms and notations . . . . . . . . . .

2
2
3
3
4

7
7
9
9
9
9
10
10
14
18
18
19
26
37
37
38
40
45

4
5

6 Real world backward compatible update classes: proof
rules
6.1 Proof rule for specializing new configuration variables . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Proof rule for enumeration type extension . . . . .
6.3 Proof rule for variable type weakening . . . . . .
6.4 Proof rule for exit on errors . . . . . . . . . . . .
6.5 Proof rule for improved prompt message . . . . .
6.6 Proof rule for missing variable initializations . . .

45
50
55
56
61
65

6

7 Related Work

66

6

8 Conclusion

66

6

A Type system

67

6

B Syntactic definitions

67

6

C Properties of imported variables

68

7

D Properties of expression evaluation

68

1

45

2015/9/14

E Properties of remaining execution

alence is enforced on corresponding functions severely limits the
applicability of the work to general transformations affecting loops
such as loop-invariant code motion, loop fission/fusion. So, as a
major component of our formal treatment of backward compatible updates, we set out to develop sufficient conditions for semantic equivalence for programs in a typed extension of the W languages [11] with small-step operational semantics. The syntax of
language is extended with arrays and enumeration types and the
semantics take into consideration the execution environment to allow various classes of updates.
In summary, the paper makes the following contributions:

69

1. Introduction
Dynamic software update (DSU) allows programs to be updated in
the middle of their execution by mapping a state of an old version
of the program to that of a newer version. The ability to update programs without having to restart them is useful for high-availability
applications that cannot afford the downtime incurred by offline updates [16]. DSU has been an active area of research[5, 16, 22, 25]
with much of the published work emphasizing the update mechanism that implements a state mapping which maps the execution
state of an old version of the program to that of a new version.
DSU safety has not yet been successfully studied. Existing studies on DSU safety are lacking in one way or another: high-level
studies are concerned with change management for system components [9, 19] and lower-level studies typically require significant
programmer annotations [15, 24, 32] or have a restricted class of
applications to which they apply (e.g., controller systems [28]).
In this paper, we consider the safety of DSU when applied to
possibly non-terminating programs interacting with an environment that is not necessarily updated. For such updates, the new
version of the program must be able to interact with the old environment, which means that it should be, in some sense, backward
compatible with the old version. A strict definition of backward
compatibility would require the new version to exhibit the same I/O
behavior as the old version; in other words the two programs are observationally equivalent. It should be immediately clear that a more
nuanced definition is needed because observational-equivalence
does not allow changes which one would want to allow as backward compatible such as bug fixes, new functionalities, or usability improvement (e.g., improved user messages). Allowing for such
differences would be needed in any practical definition of backward
compatibility. One contribution of this work is a general definition
of backward compatibility, a classification of common backward
compatible program behavior changes, as well as classes of program change from real world program evolution.
Determining backward compatibility, which allows for differences between two program versions, is requiring one to solve the
semantic equivalence problem which has been extensively studied [6, 13, 17, 18, 20, 21, 23, 31]. Unfortunately, existing results
turned out to be lacking in one or more aspects which rules out
retrofitting them for our setting. In fact, existing work on program
equivalence typically guarantees equivalence at the end of an execution. Such equivalence is not adequate for our purposes because
it does not allow us to express that a point in the middle of a loop
execution of one program corresponds (in a well defined sense) to
a point in the middle of a loop execution of another program. The
ability to express such correspondences is desirable for dynamic
software update. Besides, existing formulations of the program
equivalence problem either do not use formal semantics [7, 17, 18],
only apply to terminating programs [6, 20], severely restrict the
programming model [13, 18, 31], or rely on some form of model
checking [21, 23] (which is not appropriate for non-terminating
programs with infinite states). Our goal for program equivalence
is to establish compile-time conditions ensuring that two programs
have the same I/O behavior in all executions. In particular, if one
program enters an infinite loop and does not produce a certain output, the other program should not produce that output either. This is
different from much of the literature on program equivalence which
only guarantees same behavior in terminating executions.
The closest work that aims to establish program equivalence for
nonterminating programs is that of Godlin and Strichman [13] who
give sufficient conditions for semantic equivalence for a language
that includes recursive functions, but does not allow loops (loops
are extracted as recursive functions). That and the fact that equiv-

1. We formally define backward compatibility and identify cases
of backward compatible program behavior for typical program
update motivation.
2. We identify and formally define classes of program changes
that result in backward compatible program update based on
empirical study of real world program evolution.
3. We give a formal treatment of the semantic equivalence for
nonterminating imperative programs.
The rest of the paper is organized as follows. Section 2 proposes
the general backward compatibility and cases of backward compatible new program behavior. Then we describe real world update
classes that result in backward compatible update in Section 3. Section 4 formally defines our extension of the W language to study
backward compatible updates. Section 4.3 shows terms, notations
and definitions (e.g., execution) heavily used in the technical result. The technical results on semantic equivalence are presented in
Section 5. We propose our formal treatment of real world update
classes in Section 6. A more detailed comparison to related work is
given in Section 7 . Section 8 concludes the paper.

2. Backward compatibility
2.1 Programs and Specifications
Programs are designed to satisfy specifications. Specification can
be explicitly provided or implicitly defined by the behavior of a
program. Programs interact with their environment by receiving
inputs and producing outputs. In this section we introduce enough
of a computing model to describe the input/output behavior of
programs; In the next section we introduce a specific programming
language to reason about specific software updates.
An execution of a program consists of a sequence of steps from
a finite set of steps, S = Sin ∪ Sinternal ∪ Sout ∪ {halt}. A step
of a program can either be an input step in which input is received,
an internal step in which the state of the program is modified, an
output step in which output is produced, or a halt.
We make a distinction between internal state of a program and
external state (e.g., application settings) of the local environment
in which the program executes. Such external state can include the
state of a file system that program can access; we include both as
part of the program state. The state of a program is an element
Q intof a
set M×I, where the set M = Mint ×Mext , Mint = n
k=0 Vk
is a cartesian product of nint sets
of
values,
one
for
each
internal
Q ext
memory location, and Mext = n
k=0 Vk is a cartesian product of
next sets of values, one for each external location. The input value
last received is an element of the set I of input values.
A program executes in an execution environment. An execution
environment (Mext0 , I) specifies an initial value for the external
program state Mext0 and a possibly infinite sequence of input
values I. The input sequence is assumed to be produced by users
that we do not model explicitly.
A step of a program P is a mapping that specifies the next
program state and the next step to execute. For an internal step
sinternal ∈ Sinternal , the mapping is sinternal : M × I 7→
2

2015/9/14

S × M × {⊥}, which specifies the next step and how the state
is modified. The internal steps clear input in the state if any. For an
output step sout ∈ Sout , the mapping sout : M 7→ S × O which
specifies the next step to execute and the output value produced. O
is the set of output values produced by the program. An input step
sin ∈ Sin is simply an element of S × I and specifies the next step
to execute and the input obtained from the environment. (We simply
write sin () to denote the next step and the input received.) Because
the input value is received by the program, we do not restrict the
next step to execute. We allow the input value to be ignored by the
program by two consecutive input steps. When the step is halt,
there is no further action as if halt were mapped to itself.

guishes executions into those that satisfy the specification and those
that do not.
A specification defines the external behavior of a program that
is observed by a user. The input sequence and I/O sequence are
obviously part of external behavior. We also include Mext in
specification domain because a user can have information about the
external state. For example, a user who has data stored in the file
system considers the program’s refusal to access the stored data a
violation of the service specification; this is not the case if the user
has no stored data.

Definition 1. (Program) A program P is a tuple (S, M, Mint0 , s0 ,
I, O), where S is the set of steps as defined above, M is the set of
program states, Mint0 is the initial internal state, s0 is the initial
step, and I and O are disjoint sets of input and output values.

DSU is a process of updating software while it is running. This
results in a hybrid execution in which part of the execution is that
of the old program and part of the execution is for the new program.
State mapping is a function δ mapping an internal state and a
non-halt step of one program P to an internal state and a step of
P′
P
P′
.
another program P ′ , δ : MP
int × (S \ {halt}) 7→ Mint × S
The external state is not mapped because the environment is not
necessarily updated. In addition, we cannot change input and output
that already occurred and that I/O must be part of the hybrid
execution.

2.2 Hybrid executions and state mapping

We do not include the initial external state Mext0 in the program
definition; we include it in the execution environment of P .
Definition 2. (Execution) An execution of a program P =
(S, M, Mint0 , s0 , I, O) in execution environment (Mext0 , I),
where I is a possibly infinite sequence of input values from I, is a
sequence of configurations C from the infinite set {(M, s, i, Ir , IO)}.
A configuration c has the form c = (M, s, i, Ir , IO), where M is
a state, s is a step, i is the last input received, Ir is a sequence of
remaining input values and IO is the input/output sequence produced so far. The kth configuration ck in an execution is obtained
from the (k − 1)th configuration ck−1 = (M, s, i, Ir , IO) where
s 6= halt in one of the following cases:

Definition 4. (Hybrid execution) A hybrid execution (Mext0 , I,
CP ; CP ′ ), produced by DSU using state mapping δ from program P to program P ′ , is an execution (Mext0 , I, CP ) of P
′
concatenated with an execution (Mext
, Ir′ , CP ′ ) of P ′ where the
′
′
, Mext
), s′ , i′ , Ir′ , IO′ ) in CP ′ is
first configuration cP ′ = ((Mint
obtained by applying the state mapping to the last configuration
cP = ((Mint , Mext ), s(6= halt), i, Ir , IO) in CP as follows:
′
• (Mint
, s′ ) = δ(Mint , s);
′
′
• (i = i) ∧ (Ir′ = Ir ) ∧ (IO′ = IO) ∧ (Mext ⊆ Mext
).

1. The first configuration c0 is of the form (M0 , s0 , ⊥, I, ∅),
where M0 = (Mint0 , Mext0 );
2. s ∈ Sinternal : ck = (M ′ , s′ , ⊥, Ir , IO), where (s′ , M ′ , ⊥) =
s(M, i);
3. s ∈ Sin and the remaining inputs Ir is not empty: ck =
(M, s′ , head(Ir ), tail(Ir ), IO·head(Ir )) where (s′ , head(Ir ))
= s(Ir );
4. s ∈ Sin and the remaining inputs Ir is empty: ck = ck−1 ;
5. s ∈ Sout : ck = (M, s′ , i, Ir , IO · o′ ), where (s′ , o′ ) = s(M );

2.3 Backward compatibility
In this paper, we consider updates in which the environment is not
necessarily updated. It follows that in order for the hybrid execution
to be meaningful, the new program should provide functionality
expected by both old and new users of the system.
In practice, specifications are not explicitly available. Instead,
the program is its own specification. This means that the specification that the program satisfies can only be inferred by the external
behavior of the program. Bug fixes create a dilemma for dynamic
software updates. When a program has a bug, its external behavior does not captures its implicit specification and the update will
change the behavior of the program. In what follows, we first discuss what flexibility we can be afforded for a backward compatible
update and then we give formal definitions of backward compatibility and state our assumptions for allowing bug fixes.
We consider a hybrid execution starting from a program P =
(S, Mint × Mext , Mint0 , s0 , I, O) and being updated to a pro′
gram P ′ = (S ′ , M′int × M′ext , Mint
, s′0 , I ′ , O′ ). We examine
0
how the two programs should be related for a meaningful hybrid
execution.

In the definition, head(I) denotes the head (leftmost) element
in the sequence I and tail(I) denotes the remaining sequence
without the head. The input value in i is either consumed by the
next internal step or updated by another input from the next input
step. Execution is stuck if an input step is attempted in state in
which there are no remaining inputs. In what follows, we include
the execution environment in the execution and we abuse notation
to say (Mext0 , I, C) is an execution of a program P .
Specifications We consider specifications that define the input/output behavior of programs. Specifications are not concerned
with how fast an output is produced or about the internal state of
the program.
Definition 3. (Specification) Given a set Mext of external states,
a set seq(I) of input sequences, and a set seq(I ∪ O) of I/O
sequences, specification Σ is a predicate: Mext ×seq(I)×seq(I∪
O)× 7→ {true, f alse}.

1. (Inputs) Input set I ′ of P ′ should be a superset of that I of P
to allow for old users to interact with P ′ after the update. It is
possible to allow for new input values in I ′ to accommodate
new functionality under the assumption that old users do not
generate new input values. Such new input values should be
expected to produce erroneous output by old users as they are
not part of P ’s specification.

We define the I/O sequence of a sequence of configurations C
to be a sequence IO(C) of values from I ∪ O such that every finite
prefix of IO(C) is the IO sequence of some configuration c ∈ C
and every I/O sequence of a configuration c ∈ C is a finite prefix
of IO(C).
An execution (Mext0 , I, C) of program P satisfies a specification Σ if Σ(Mext0 , I, IO(C)) = true. A specification distin-

2. (Outputs) Output produced by P ′ should be identical to output
produced by P if all the input in an execution comes from
the input set of P . This is needed to ensure that interactions
between old users and the program P ′ can make sense from the

3

2015/9/14

• Inputs/outputs/external states of P are a subset of those of

perspective of old users. This is true in the case that the update
does not involve a bug fix, but what should be done if the update
indeed involves a bug fix and the output produced by the old
program was not correct to start with? As far as syntax, a bug fix
should not introduce new output values. As far as semantics, we
should allow the bug fix to change what output is produced for
a given input. We discuss this further under the bug fix heading.
In summary, if we ignore bug fixes, the new program should
behave as the old program when provided with input meant for
the old program.

P ′ : I ⊆ I ′ , O ⊆ O′ and Mext ⊆ M′ext ;

If there is bug fix between programs P ′ and P , we need to adapt
Definition 5 to allow for some executions on input sequences from
I to violate the specification of P . Above we identified two cases in
which bug fixes are safe (replacing a response with no response or
replacing a no response with a correct response without introducing
new output values). We omit the definition.
We have the backward compatible updates by extending the
definition of a backward compatible hybrid execution to all possible
hybrid executions.

3. (Bugfix) Handling bug fixes is problematic. If the produced output already violates the fix, then there is no way for the hybrid
execution to satisfy the implicit semantics of the program or the
semantics of the new program. Some bug fixes can be handled.
For example, a bug that causes a program to crash for some
input can be fixed to allow the program to continue executing.
Applying the fix to a program that has not encountered the bug
should not be problematic. Another case is when the program
should terminate for some input sequence, but the old program
does not terminate. A bug fix that allows the program to terminate should not present a semantic difficulty for old users.
In general, we assume that there are valid executions and invalid executions of the old program. I/O sequences produced in
invalid executions are not in specification of the program. We
assume that an invalid execution will lead to an error configuration not explicitly handled by the program developers. We do
not expect the state mapping to change an error configuration
into an non-error configuration just as static updating does not
fix occurred errors. Besides, we do not attempt to determine if a
particular configuration is an error configuration. Such determination is not possible in general and very hard in practice. We
simply assume that the configuration at the time of the update
is not an error configuration. (which is equivalent to assuming
the existence of an oracle JP to determine if a particular configuration is erroneous, JP (CP ) = true if the configuration CP
is not erroneous).

Definition 6. (Backward compatible updates) We say an updated
program P ′ is backward compatible with a program P in configuration C if there is hybrid execution, from configuration C of P to
P ′ that is backward compatible with specification of P .
2.4 Backward compatible program behavior changes
With the formal definition of backward compatibility, it is desirable to check what behavior changes of an updated program help
ensure a safe update. Backward compatibility is essentially a relation between I/O sequences produced by an old program and those
produced by an updated program. We summarized typical possibilities of the relation into six cases in Figure 1 by considering consequence of major update motivation (i.e., new functionality, bug fix
and program perfective/preventive needs [1]). According to David
Parnas [29], a program is updated to adapt to changing needs. In
other words, program changes are to produce more or less or different output according to changing needs. These changes are captured by case 2, 3, 4, 5 and 6 in Figure 1. We also capture outputpreserving changes which are most likely motivated by the program developer’s own needs (e.g., software maintainability), which
is case 1 in Figure 1.
Furthermore, we find that an update is backward compatible
if in every execution the new program behavior is one of the six
cases in Figure 1. Cases 1 and 2 are obviously backward compatible
because an old client is guaranteed to get old responses. Cases 3,
4, 5, and 6 are not obviously backward compatible. Unlike case
1 and 2, case 3, 4 and 5 are backward compatible under specific
assumptions on program semantics while case 6 is different. Case
3 is backward compatible because we assume the change is either
adding new functionality, or fixing a bug in which the old program
hanged or crashed. Similarly, case 4 is backward compatible. Case
5 is backward compatible because different I/O interaction could
express the same application semantics. For example, a greeting
message could be changed from “hi” to “hello”. Case 6 is backward
compatible in that the new program makes implicit specification of
the program explicit by enforcing restrictions on program state and
therefore eliminating undesired I/O sequence.
The six cases in Fig. 1 have covered the changes of output, including more or less or different output. There exists more specific
cases of backward compatible program behavior changes under
various specific assumptions. However, these more specific cases
could be attributed to one of the six cases as far as the changes of
output are concerned. In conclusion, it is not possible to go much
beyond the six cases of backward compatibility in Fig. 1.

4. (New functionality) New functionality is usually accompanied
by new inputs/outputs and the expansion of external state. We
assume that new functionality is independent of existing functionality in the sense that programs P and P ′ produce the same
I/O sequence when receiving inputs in I only. We therefore assume all new inputs I ′ \ I are introduced by new functionality.
Every external state of P is part of some external state of program P ′ because of the definition of the specification of P . We
only consider expansion of the external state of P for new functionalities in P ′ where the expansion of external state is independent of values in existing external state. One of the motivating examples is to add application settings for new program
feature.
In light of the discussion above we give the following definition
of backward compatibility in the absence of bug fixes.
Definition 5. (Backward compatible hybrid executions) Let
P = (S, Mint × Mext , Mint0 , s0 , I, O) be a program satisfying
a specification Σ. We say that a hybrid execution (Mext , I, CP ; CP′ )
′
from P to a program P ′ = (S ′ , M′int × M′ext , Mint
, s′0 , I ′ , O′ )
0
is backward compatible with implicit specification of P if all of the
following hold:

3. Real world backward compatible update
classes: brief description
We have studied evolution of three real world programs (i.e., vsftpd, sshd and icecast) to identify real world changes that are backward compatible. We chose these three programs because the programs are widely used in practice [2, 3] and are widely studied in
the DSU community [26, 27]. We have studied several years of re-

• The last configuration in CP is not an error configuration,

CP = “C ′ ; (M, s′ , i, Ir , IO)” : JP (CP )=true.
• The hybrid execution satisfies the specification Σ of P ,
Σ(Mext , I, IO(CP ; CP′ )) = true;

4

2015/9/14

Case
1

2

3

4

5
6

Update date Total Class
Software version
Formal new program behavior
vsftpd 1.1.0 –1.1.1 2002-10-07
16
8
the old behavior including external state extension:
vsftpd 1.1.1 –1.1.2 2002-10-16
8
1
ΣP ⊆ ΣP ′ , or ΣP ′ = {(Mext′ , oneseq(I), oneseq(I ∪ O)) → val
8
4
vsftpd 1.1.2 –1.1.3 2002-11-09
|∃(Mext , oneseq(I), oneseq(I ∪ O)) → val in ΣP
61
9
vsftpd 1.1.3 –1.2.0 2003-05-29
and Mext ⊆ Mext′ } where I = I ′ , O = O′ and Mext ⊆ Mext′
vsftpd 1.2.0 –1.2.1 2003-11-13
33
11
the old behavior for old input and consuming inputs
10
6
vsftpd 1.2.1 –1.2.2 2004-04-26
that are only from new clients:
52
13
vsftpd 1.2.2 –2.0.0 2004-07-01
ΣP ⊆ ΣP ′ ∧
vsftpd 2.0.0 –2.0.1 2004-07-02
7
4
ΣP ′ \ ΣP = {(Mext , oneseq(I ′ ), oneseq(I ′ ∪ O′ )) → true
23
4
vsftpd 2.0.1 –2.0.2 2005-03-03
| oneseq(I ′ ∪ O′ ) includes at least one input in (I ′ \ I)} =
6 ∅
vsftpd 2.0.2 –2.0.3 2005-03-19
18
8
where I ⊂ I ′ , O ⊆ O′ and Mext = Mext′
14
9
vsftpd 2.0.3 –2.0.4 2006-01-09
producing more output while the old program terminates:
21
15
vsftpd 2.0.4 –2.0.5 2006-07-03
ΣP ′ \ ΣP = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ false
vsftpd 2.0.5 –2.0.6 2008-02-13
20
9
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆f 6= ∅}
16
8
vsftpd 2.0.6 –2.0.7 2008-07-30
∪ {(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) 7→ true
vsftpd 2.0.7 –2.1.0 2009-02-19
53
11
|(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) ∈ ∆t 6= ∅}
21
9
vsftpd 2.1.0 –2.1.2 2009-05-29
ΣP \ ΣP ′ = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ false
vsftpd 2.1.2 –2.2.0 2009-08-13
34
14
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆t 6= ∅}
Update date Total Class
Software version
∪ {(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) 7→ true
2009-10-19
21
5
|(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) ∈ ∆f 6= ∅} vsftpd 2.2.0 –2.2.2
vsftpd 2.2.2 –2.3.0
2010-08-06
13
3
where I = I ′ , O = O′ and Mext = Mext′
2010-08-19
5
0
vsftpd 2.3.0 –2.3.2
termination while the old program produces erroneous output:
2011-03-12
7
0
vsftpd 2.3.2 –2.3.4
ΣP ′ \ ΣP = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ true
vsftpd 2.3.4 –2.3.5
2011-12-19
14
6
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆t 6= ∅}
2012-04-10
23
4
∪ {(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) 7→ false vsftpd 2.3.5 –3.0.0
2012-09-19
40
2
|(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) ∈ ∆f 6= ∅} vsftpd 3.0.0 –3.0.2
sshd 3.5p1 –3.6p1
2003-03-31
95
34
ΣP \ ΣP ′ = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ true
sshd 3.6p1 –3.6.1p1
2003-04-01
13
12
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆f 6= ∅}
16
12
∪ {(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) 7→ false sshd 3.6.1p1 –3.6.1p2 2003-04-29
2007-03-07
48
13
|(Mext , oneseq(I), oneseq(I ∪ O)  oneseq’(I ∪ O)) ∈ ∆t 6= ∅} sshd 4.5p1 –4.6p1
sshd 6.6p1 –6.7p1
2014-10-06
283
51
where I = I ′ , O = O′ and Mext = Mext′
icecast 0.8.0 –0.8.1
2004-08-04
4
3
different output that is functionally equivalent to old output:
icecast 0.8.1 –0.8.2
2004-08-04
2
0
(ΣP 6= ΣP ′ ) ∧ (ΣP ≡ ΣP ′ )
2005-11-30
47
10
icecast 2.3.0 –2.3.1
where I = I ′ , (O 6= O′ ) ∧ (O ≡ O′ ) and Mext = Mext′
icecast 2.3.1 –2.3.2
2008-06-02
250
28
enforcing restrictions on program state:
2014-11-19
178
154
icecast 2.4.0 –2.4.1
ΣP ′ \ ΣP = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ false
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆arbi 6= ∅}
ΣP \ ΣP ′ = {(Mext , oneseq(I), oneseq(I ∪ O)) 7→ true
Figure 2: Statistics of classified real world software update
|(Mext , oneseq(I), oneseq(I ∪ O)) ∈ ∆arbi 6= ∅}
′
′
where I = I , O = O and Mext = Mext′
that these update classes are also widespread in other program evolution. Each of the six real world update classes falls in one of the
five cases of backward compatibility in Fig. 1. We present informal descriptions of all update classes including required assumptions for the two programs to produce same or equivalent output
sequence which guarantees backward compatible DSU.

Figure 1: Six cases of formalized general new program behavior

leases of vsftpd and consecutive updates of sshd and icecast. This is
because vsftpd is more widely studied by the DSU community [25–
27].
Our study of real world program evolution is carried out as follows. We examined every changed function manually to classify
updates. For every individual change, we first identified the motivation of the change, then the assumptions under which the change
could be considered backward compatible. If the assumption under
which the change is considered backward compatible is reasonable,
we recorded the change into one particular update class. Finally we
summarized common update classes observed in the evolution of
studied programs.
Fig. 2 shows the statistics from our study of real world program evolution where “total” refers to the number of all updated
functions, “class” refers to the number of updated functions with
at least one classified update pattern. In summary, 32% of all updated functions include at least one classified program update; the
unclassified updates are mostly bug fix that are related to specific
program logic. We summarized seven most common real world update classes from all the studied updates in Fig. 3 and we believe

3.1 Observational equivalence: the old behavior
In case 1 in Fig. 1, two programs are backward compatible because
the new program keeps all old behaviors (“observational equivalence”). In our study, we differentiate two types of “observational
equivalence” based on if assumptions are required.
Program equivalence We consider several types of program
changes that are allowed by “observational equivalence” without
user assumptions. These changes include: loop fission or fusion,
statement reordering or duplication, and extra statements unrelated to output(e.g., logging related changes). We incorporate these
changes in our framework of program equivalence which ensures
two programs produce the same output regardless of whether the
programs terminate or not. The details of the formal treatment is in
Section 5.
Specializing new configuration variables Another update class
of “observational equivalence” is “specializing new configuration
variables”, which is backward compatible under user assumptions.

5

2015/9/14

Update class (Case)
program equivalence (1)
new config. variables (1)
enum type extension (2)
var. type weakening (3)
exit on error (4)
improved prompt msgs (5)
missing var. init. (6)

1:
2:
3: output a

Required assumptions for backward
compatible update
none
no redefinitions of new config
variables after initialization
no inputs from old clients match
the extended enum labels
no intentional use of value type
mismatch and array out of bound
correct error check before exit
changing prompt messages for
more effective communication
no intentional use of undefined
variables

old

new
Figure 6: Exit-on-error

3.3 Variable type weakening: more output when the old
program terminates
In program updates, variable types are changed either to allow for
larger ranges (weakening) or smaller ranges to save space (strengthening). For example, an integer variable might be changed to become a long variable to avoid integer overflow or a long variable
might be changed to an integer variable because the larger range
of long is not needed. Type weakening also includes adding a new
enumeration value and increasing array size. The kinds of strengthening or weakening that should be allowed are application dependent and would need to be defined by the user in general. The type
weakening considered is either changes from type int to long or
increase of array size. These updates fix integer overflow or array
index out of bound respectively, the case 3 of backward compatibility. Implicitly, we assume that there is no intentional use of integer
overflow and array out of bound as program semantics.

Figure 3: Required assumptions for real world backward compatible update classes

1:
2:
3:
4: output a + 2
old

1’: If (1/(a − 5)) then
2’:
skip
3’: output a

1’: If (b) then
2’:
output a ∗ 2
3’: else
4’:
output a + 2
new

Figure 4: Specializing new configuration variables

3.4 Exit on errors: stopping execution while the old program
produces more output

1: enum id {o1 }
2: a : enum id
3: If (a == o1 ) then
4:
output 2 + c
5:
6:
old

One kind of bug fix, which we call exit on error, causes a program
to exit in observation of errors that depend on application semantic.
Fig. 17 shows an example of exit-on-error update. In the example,
the fixed bugs refer to the program semantic error that a = 5. Instead of using an “exit” statement, we rely on the crash from expression evaluations to model the “exit”. When errors do not occur,
the two programs in Fig. 17 produce the same output sequence.
This is case 4 of backward compatibility. Naturally, we assume that
all error checks are correct.

1’:
2’:
3’:
4’:
5’:
6’:

enum id {o1 , o2 }
a : enum id
If (a == o1 ) then
output 2 + c
If (a == o2 ) then
output 3 + c
new

Figure 5: Enumeration type extension

3.5 Improved prompt messages: functionally equivalent
outputs
In this update class, new configuration variables are introduced to
generalize functionality. For example, in Fig. 15, a new configuration variable b is used to introduce new code. The two statement
sequences in Fig. 15 are equivalent when the new variable b is specialized to 0. In general, if all new code is introduced in a way
that is similar to that in Fig. 15 where there is a valuation of new
configuration variables under which new code is not executed, and
new configuration variables are not redefined after initialization,
then the new program and the old program produce the same output sequence. The point is that new functionality is not introduced
abruptly in interaction with an old client. Instead new functionality could be enabled for a new client when old clients are not a
concern.

In practice, outputs could be classified into prompt outputs and
actual outputs. Prompt outputs are those asking clients for inputs,
which are constants hardcoded in output statements. Actual outputs
are dynamic messages produced by evaluation of non-constant
expressions in execution. If the differences between two programs
are only the prompt messages that a client receives, we consider
that the two programs are equivalent. The prompt messages are
the replaceable part of program semantics. We observe cases of
improving prompt messages in program evolution for effective
communication. The changes of prompt outputs do not matter only
for human clients. This is case 5 of backward compatibility.

3.2 Enum. type extension: old behavior for old input and
allowing new input

Another kind of bug fix, which we call missing variable initialization, includes initializations for variables whose arbitrary initial
values can affect the output sequence in the old program. Fig. 18
shows an example of missing variable initialization. The initialization b := 2 ensures the value used in “output b + c” not to be undefined. Despite of initialization statements, the two programs are
same. In general, initializations of variables only affect rare buggy
executions of the old program, where undefined variables affect the
output sequence. This update class is case 6 of backward compatibility and we assume that there is no intentional use of undefined
variable in the program. When there are no uses of variables with

3.6 Missing variable initialization: enforcing restrictions on
program states

Enumeration types allow developers to list similar items. New code
is usually accompanied with the introduction of new enumeration
labels. Fig. 16 shows an example of the update. The new enum label o2 gives a new option for matching the value of the variable
a, which introduces the new code “output 3 + c”. To show enumeration type extensions to be backward compatible, we assume
that values of enum variables, used in the If-predicate introducing
the new code, are only from inputs that cannot be translated to new
enum labels. This is case 2 of the backward compatibility.

6

2015/9/14

1:
2: If (a > 0) then
3:
b := c + 1
4: output b + c
old

1’: b := 2
2’: If (a > 0) then
3’:
b := c + 1
4’: output b + c
new

Figure 7: Missing initialization

Identifier
id
Enum Items
Enumeration
Prompt Msg
Prompts
Base type
Variables
Left value
Expression
Statement
Stmt Seq.
Program

Values

v

∈

ZL ∪ L

I/O values
Inputs
Eval. values

vio
vi
verr

∈
::=
::=

ZL
v io
v | error

Param. types

τ⊤

::=

τ | array(τ, n)

Loop Labels

looplbl ∈

integer values in type long and
enum/prompt labels
tagged input values
values and the runtime error

N

Figure 9: Values, types and domains

Constant
n
Label
l
el
::= l | el1 , el2
EN ::= ∅ | enum id {el} | EN 1 , EN 2
msg ::= l : n | msg 1 , msg 2
P mpt::= ∅ | {msg}
τ
::= Int | Long | pmpt | enum id
V
::= ∅ | τ id | τ id[n] | V1 , V2
lval ::= id | id1 [id2 ] | id[n]
e
::= id == l | lval | other
s
::= lval := e | input id | output e | skip
| while (e) {S}
S
::= s1 ; ...; sk for k ≥ 1
P
::= P mpt; EN ; V ; Sentry

Crash flag
Overflow flag
Type Env.
Loop counter
Value store

f
::=
of
::=
Γ
::=
loopc::=
σ
::=
|
|
|

0|1
0|1
∅ | id : τ⊤ | id : {l1 , ..., lk } | Γ1 , Γ2
(looplbl 7→ (n | ⊥))
id 7→ (v | ⊥)
values of scalar variables
id 7→ (n 7→ (v | ⊥))
values of array elements
∗
idI 7→ vio
input sequence
idIO 7→ (vi | vo )∗
I/O sequence

State

m

(f, of, Γ, loopc , σ)

::=

Figure 8: Abstract syntax

Figure 10: Elements of an execution state

undefined variables in executions of the old program, the two programs produce the same output sequence.

statements Sentry . Finally, we have a standard type system based
on our syntax.
4.2 Small-step operational semantics of the formal language

4. Formal programming language

Figure 9 shows semantic categories of our language. We consider
values to be either labels L or integer numbers ZL defined in type
Long. The integer numbers defined ZI of type Int are a proper
subset of those in type Long, ZI ⊂ ZL . We use the notation ZL+
for the positive integers defined in type Long. We use the notation
udfJτ K for an undefined value of type τ . Unlike the “undef” in
Clight [8], we need to parameterize the undefined value with a
type τ because we do not have an underlining memory model that
can interpret any block content according to a type. An individual
value in I/O sequence is an integer number with tag differentiating
inputs and outputs, our tags for inputs and outputs are standard
notations [14]. The value from expression evaluation is a pair. One
of the pair is either a value v or “error” for runtime errors(e.g.,
division by zero); the other is the overflow flag (i.e., 0 for no
overflow).
We use notation τ⊤ for all types that are defined in syntax,
including array types.
Every loop statement in a program is with a unique label looplbl
of a natural number in order to differentiate their executions.
The composition of an execution state is in Figure 10.

We present the formal programming language based on which
we prove our semantic equivalence results and describe categories
of backward compatible changes. We first explain the language
syntax, then the language semantics.
4.1 Syntax of the formal language
The language syntax is in Figure. 8. We use id to range over the
set of identifiers, n to range over integers, l to range over labels.
We assume unique identifiers across all syntactic categories, unique
labels across all enumeration types and the prompt type. We have
base type Int and Long for integer values. The integers defined in
type Int are also defined in type Long. Every label defined in the
prompt type is related with an integer constant as the actual value
used in output statement. We differentiate type Long and Int to
define the bug fix of type relaxation from Int to Long to prevent
overflow in calculation (e.g., a + b can cause an error with Int but
not with Long). The type Int is necessary reflecting the concern of
space and time efficiency in practical computation. We also have
user-defined enumeration type, prompt type and array type.
We explicitly have “id == l” and lval as expressions for convenience of the definition of specific updates. To make our programming language general and to separate the concern of expression evaluation, we parameterize the language by “other” expressions which are unspecified.
We have explicit input and output statement because we model
the program behavior as the I/O sequence which is the observational behavior of a program. The I/O statement makes it convenient for the argument of program behavior correspondence. In this
paper, every I/O value is an integer value which is a common I/O
representation [14]. A Statement sequence is defined as s1 ; ...; sk
where k > 0 for the convenience of syntax-direct definition from
both ends of the sequence.
A program is composed of a possibly empty prompt type P mpt,
a possibly empty sequence of enumeration types EN , a possibly
empty sequence of global variables V and a sequence of entry

1. The crash flag f is initially zero and is set to one whenever an
exception occurs. Once the crash flag is set, it is not cleared. We
only consider unrecoverable crashes. The crash flag is used to
make sure that updates do not occur in error states.
2. The overflow flag of is initially zero and is set to one whenever
an integer overflow in expression evaluation occurs. Overflow
flag is sticky in the sense that once it is set, the flag is not
cleared. According to [12], integer overflows are common in
mature programs.
3. Γ is the type environment mapping enumeration type identifers
and variable identifiers to their types. Type environment is necessary for checking array index out of bound or checking value
mismatch in execution of input/assignment statement.
4. Loop counters loopc are to record the number of iterations for
one instance of a loop statement. The loop counters loopc is
7

2015/9/14

(S, m) → (S ′ , m′ )

(r, m) → (r ′ , m′ )

(r, m) → (r ′ , m′ )

(E[r], m) → (E[r ′ ], m′ )
Eval. Context E ::= | id[E] | E == l
| id := E | id[E] := e | id[v] := E | output E
| while (E){S} | If (E) then {St } else {Sf } | E; S

As-Scl
As-Arr

Figure 11: Contextual semantic rule

As-Err1

f=0

f=0

E : other → σ → (verr × {0, 1})
Err : other → {id} (unspecified)

Var

f=0

σ(id) = v

As-Err3

(id, m(f, σ)) → (v, m)
f=0

Arr-1

f=0

f=0

Arr-2
Eq-T

Eq-F

(Γ ⊢ id : array(τ, n)) ∧ ¬(1 ≤ v1 ≤ n)

If-F

(id[v1 ], m(f, Γ)) → (id[v1 ], m(1/f))

f=0
σ(id, v1 ) 6= ⊥
(Γ ⊢ id : array(Int, n)) ∧ (v2 ∈ (ZL \ ZI ))
(id[v1 ] := v2 , m(f, Γ, σ)) → (id[v1 ] := v2 , m(1/f))

f=0
l1 6= l2
(l1 == l2 , m(f)) → (0, m)

Wh-F

f = 0 e = other
(e, m(f, σ)) → (EJeKσ, m)

(v ∈ ZL ) ∧ (v 6= 0)

f=0
(If (0) then {St } else {Sf }, m(f)) → (Sf , m)

Wh-T

ECrash

(id := v, m(f, Γ, σ)) → (id := v, m(1/f))

(If (v) then {St } else {Sf }, m(f)) → (St , m)

f=0
(l == l, m(f)) → (1, m)

EEval

f=0
σ(id) 6= ⊥
(Γ ⊢ id : Int) ∧ (v ∈ (ZL \ ZI ))

f=0

If-T

(Γ ⊢ id : array(τ, n)) ∧ ¬(1 ≤ v1 ≤ n)

(id[v1 ] := v2 , m(f, Γ)) → (id[v1 ] := v2 , m(1/f))

σ(id, v1 ) = v2

(id[v1 ], m(f, σ)) → (v2 , m)

σ(id, v1 ) 6= ⊥

(id[v1 ] := v2 , m(f, σ)) → (skip, m(σ[v2 /(id, v1 )]))

(r, m) → (r ′ , m′ )
As-Err2

σ(id) 6= ⊥

(id := v, m(f, σ)) → (skip, m(σ[v/id]))

Seq

f=0

(v ∈ ZL ) ∧ (v 6= 0)

loopc (n) = k

(whilehni (v) {S}, m(f, loopc )) →
(S; whilehni (e) {S}, m(loopc [(k + 1)/n])
f=0

loopc (n) 6= ⊥

(whilehni (0) {S}, m(f, loopc )) → (skip, m(loopc [0/n]))
f=0
(skip; S, m(f)) → (S, m)

Crash

f=1
(s, m(f)) → (s, m)

f=0
((error, vof ), m(f)) → (0, m(1/f))

EOflow-1

f = 0 of = 0
((v, vof ), m(f, of)) → (v, m(vof /of))

EOflow-2

f = 0 of = 1
((v, vof ), m(f, of)) → (v, m)

Figure 13: SOS rules for Assignment, If, and While statements

other → {id} maps an “other” expression to a set of variables
used in the expression; there is a function Err : other → {id}
maps an expression to a set of variables whose values decide if the
evaluation of expression leads to crash. We assume function Use
and Err available. The value returned by the expression meaning
function only depends on the values of variables in the use set of the
expression and the error evaluation only depends on the variables
in the error set.
As to integer overflow, there are two ways of handling overflow in practice one is to wrap around overflow using twoscomplement representation (e.g., the gcc option -fwrapv); the other
is to generates traps for overflow (e.g., the gcc option -ftrapv). We
adopt a combination of the two handling of overflow: the meaning function E wraps the overflow in some representation (e.g.,
two-complement) and notifies the overflow in return value. Rule
EOflow-1 and EOflow-2 update the sticky overflow flag. The evaluation of lval or id == l is shown by respective rules in Figure 12.
Figure 13 shows SOS rules for assignment, If, while statements,
statement sequence, and crash, which are almost standard. There
are four particular crash in execution of assignment statements.
One is array out of bound for array access for l-value (e.g., rule
As-Err1); the second is assigning a value defined in type Long but
not type Int to an Int-typed variable (e.g., rule As-Err2); the third is
value mismatch in input statement; the last is expression evaluation
exception. As to loop statement, if the predicate expression evaluates to a nonzero integer, corresponding loop counter value increments by one; otherwise, the loop counter value is reset to zero. We

Figure 12: SOS rules for expressions

not necessary for program executions but are needed for our
reasoning of the execution of loops. When a counter entry for
loop label n is not defined in loop counters loopc , we write
loopc (n) = ⊥. Otherwise, we write loopc (n) 6= ⊥.

5. The value store σ is a valuation for scalar variables, array
elements, the input sequence variable, and the I/O sequence
variable.

Execution state m is a composition of elements discussed
above. In our SOS rules, we only show components of a state
m when necessary (e.g., m(Γ, σ)).
Figure 11 shows typical contextual rule and Figure 12, 13 and
14 show all SOS rules.
Figure 12 shows rules for expression evaluation. We use the
expression meaning function E : other → σ → (verr × {0, 1})
to evaluate “other” expressions. In evaluation of expression “other”
against a value store σ, the expression meaning function E returns a
pair (verr , of) where the value verr is either a value v or an “error”, of
is a flag indicating if there is integer overflow in the evaluation (e.g.,
1 if there is overflow). The meaning function E interprets “other”
expressions deterministically. In addition, there is a function Use :

8

2015/9/14

to be in a statement sequence. We use the symbol ⊂ to refer to
proper subset relation.
We call an “If” statement or a “while” statement as a compound statement; all other statements are simple statements. We
introduce terms referring to a part of a compound statement. Let
s = “If(e) then{St } else{Sf }” be an “If” statement, we call e in s
the predicate expression, St /Sf the true/false branch of s.

(r, m) → (r ′ , m′ )
In-1

In-2

In-3

In-4

In-5
In-6
Out-1
Out-2

f=0

f=0
hd(σ(idI )) = vio

hd(σ(idI )) = vio

Γ ⊢ id : Long

σ(id) 6= ⊥
(Γ ⊢ id : Int) ∧ (vio ∈ ZI )

(input id, m(f, Γ, σ)) → (skip,
m(σ[vio /id][tl(σ(idI ))/idI ][“σ(idIO ) · v io ”/idIO ])
f=0
hd(σ(idI )) = vio

5. Program equivalence
We consider several types of program changes that are allowed
by “observational equivalence” without user assumptions. These
changes include: statement reordering or duplication, extra statements unrelated to output(e.g., logging related changes), loop fission or fusion, and extra statements unrelated to output. Our program equivalence ensures two programs produce the same output,
which means two programs produce same I/O sequence till any output. The program equivalence is established upon two other kinds
of equivalence, namely equivalent terminating computation of a
variable and equivalent termination behavior.
We first define terminating and nonterminating execution. Then
we present the framework of program equivalence in three steps in
which every later step relies on prior ones. We first propose a proof
rule ensuring two programs to compute a variable in the same way.
We then suggest a condition ensuring two programs to either both
terminate or both do not terminate. Finally we describe a condition
ensuring two programs to produce the same output sequence. Our
proof rule of program equivalence gives program point mapping as
well as program state mapping. Though we express the program
equivalence as a whole program relation, it is easy to apply the
equivalence check for local changes using our framework under
user’s various assumptions for equivalence.

σ(id) 6= ⊥
(Γ ⊢ id : Int) ∧ (vio ∈
/ ZI )

(input id, m(f, Γ, σ)) → (input id, m(1/f))
f=0
σ(id) 6= ⊥
hd(σ(idI )) = vio
(Γ ⊢ id : enum id′ ) ∧ (Γ ⊢ id′ : {l1 , ..., lk }) ∧ (1 ≤ vio ≤ k)
(input id, m(f, Γ, σ)) → (skip,
m(σ[lvio /id, tl(σ(idI ))/idI ][“σ(idIO ) · v io ”/idIO ])
f=0
σ(id) 6= ⊥
hd(σ(idI )) = vio
(Γ ⊢ id : enum id′ ) ∧ (Γ ⊢ id′ : {l1 , ..., lk }) ∧ ¬(1 ≤ vio ≤ k)
(input id, m(f, Γ, σ)) → (input id, m(1/f))
f=0

σ(idI ) = ∅

(input id, m(f, σ)) → (input id, m(1/f))
f=0

v ∈ ZL

(output v, m(f, σ)) → (skip, m(σ[“σ(idIO ) · v”/idIO ]))
f=0

Γ ⊢ id : {l1 , ..., lk } ∧ v = li ∈ {l1 , ..., lk }

(output v, m(f, Γ, σ)) → (skip, m(σ[“σ(idIO ) · i”/idIO ]))
f=0

Out-3

σ(id) 6= ⊥

(input id, m(f, Γ, σ)) →
(skip, m(σ[vio /id][tl(σ(idI ))/idI ][“σ(idIO ) · v io ”/idIO ])

Γ ⊢ pmpt : {l1 : n1 , ..., lk : nk }
“l : n” ∈ {l1 : n1 , ..., lk : nk }

(output l, m(f, Γ)) → (output n, m)

5.1 Definitions of execution
We define an execution to be a sequence of configurations which
are pairs (S, m) where S is a statement sequence and m is a execution state shown in Figure 10. Let (S1 , m1 ), (S2 , m2 ) be two
consecutive configurations in an execution, the later configuration
(S2 , m2 ) is obtained by applying one semantic rule w.r.t to the
configuration (S1 , m1 ), denoted (S1 , m1 ) → (S2 , m2 ), called
one step (of execution). For our convenience, we use the notation
k
(S, m) → (S ′ , m′ ) for k steps execution where k > 0. When we
do not care the exact (finite) number of steps, we write the exe∗
cution as (S, m) → (S ′ , m′ ). We express terminating executions,
nonterminating executions including crash in Definition 7 and 8.

Figure 14: SOS rules for input/output statements

use rule Crash to treat crash as non-terminating execution, telling
apart normally terminating executions and others.
Figure 14 shows rules for the execution of input/output statements. As to input, there are conversion from values of type Long
to those of Int or enumeration types but not the prompt type. For an
enumeration type, the Long-typed value is transformed to the label
with index of that value if possible. There is crash when value conversion is impossible. Besides, there is crash when executing input
statement with empty input sequence. We use standard list operation hd and tl for fetching the list head(leftmost element) or the list
tail(the list by removing its head) respectively [30].
Last, we construct initial state in following steps: First, crash
flag f, overflow flag of are zero. Second, type environment is obtained after parsing of the program. Third, every loop counter value
in loopc is initially zero. Fourth, every scalar variable or array element has an entry in value store with some initial value if specified.
Last, there is initial input sequence and empty I/O sequence.

Definition 7. (Termination) A statement sequence S normally
∗
terminates when started in a state m iff (S, m) → (skip, m′ (f))
where f = 0.
Definition 8. (Nontermination) A statement sequence S does not
k
terminate when started in a state m iff, ∀k > 0 : (S, m) →
(Sk , mk ) where Sk 6= skip.
5.2 Equivalent computation for terminating programs
We propose a proof rule under which two terminating programs
are computing a variable in the same way. We start by giving the
definition of equivalent computation for terminating programs right
after this paragraph. Then we present the proof rule of equivalent
computation in the same way. We prove that the proof rule ensures
equivalent computation for terminating programs by induction on
the program size of the two programs in the proof rule. We also
list auxiliary lemmas required by the soundness proof for the proof
rule for equivalent computation for terminating programs.

4.3 Preliminary terms and notations
We present terms, notations and definitions for program equivalence and backward compatible update classes.
We use Use(e) or Use(S) to denote used variables in an expression e or a statement sequence S; Def(S) denotes the set of defined
variables in a statement sequence S. The full definitions of Use and
Def are in appendix B.
We use symbol ∈ for two different purposes: x ∈ X denotes
one variable to be in a set of variables, s ∈ S denotes a statement
9

2015/9/14

(d) S1 and S2 do not define the variable x: x ∈
/ Def(S1 ) ∪
Def(S2 ).
2. S1 and S2 are not both one statement and one of the following
holds:
(a) S1 = S1′ ; s1 , S2 = S2′ ; s2 and last statements both define
the variable x such that both of the following hold:
′
• ∀y ∈ Imp(s1 , {x}) ∪ Imp(s2 , {x}) : S1′ ≡S
y S2 ;
S
• s1 ≡x s2 where x ∈ Def(s1 ) ∩ Def(s2 );
(b) Last statement in S1 or S2 does
 not define the variable′ x: S
S
′
x∈
/ Def(s1 ) ∧ (S1 ≡x
/ Def(s2 ) ∧ (S1 ≡x S2 ) ∨ x ∈
S2 ) ;
(c) S1 = S1′ ; s1 , S2 = S2′ ; s2 and there are statements moving
in/out of If statement: s1 = “If (e) then {S1t } else {S1f }”,
s2 = “If (e) then {S2t } else {S2f }” such that none of the
above cases hold and all of the following hold:
′
• ∀y ∈ Use(e) : S1′ ≡S
y S2 ;
f
′
t
S
′
t
′
′
• (S1 ; S1 ≡x S2 ; S2 ) ∧ (S1 ; S1f ≡S
x S2 ; S2 );

Definition 9. (Equivalent computation for terminating programs) Two statement sequences S1 and S2 compute a variable x
equivalently when started in states m1 and m2 respectively, writ∗
ten (S1 , m1 ) ≡x (S2 , m2 ), iff (S1 , m1 ) → (skip, m′1 (σ1′ )) and
∗
′
(S2 , m2 ) → (skip, m2 (σ2′ )) imply σ1′ (x) = σ2′ (x).
5.2.1 Proof rule for equivalent computation for terminating
programs
We define a proof rule under which (S1 , m1 ) ≡x (S2 , m2 ) holds
for generally constructed initial states m1 and m2 , written S1 ≡S
x
S2 . Our proof rule for equivalent computation for terminating programs allows updates including statement reordering or duplication, loop fission or fusion, additional statements unrelated to the
computation and statements movement across if-branch.
Definition 12 includes the recursive proof rule of equivalent
computing for terminating programs. The base case is the condition for two simple statements in Definition 11. Definition 10 of
imported variables captures the variable def-use chain which is the
essence of our equivalence. In Definition 10, the Def and Use refer
to variables defined or used in a statement (sequence) or an expression similar to those in the optimization chapter in the dragon
book [4]; S i refers to i consecutive copies of a statement sequence
S.

The generalization of definition S1 ≡S
x S2 to a set of variables
is as follows.

Definition 13. Two statement sequences S1 and S2 have equivalent computation of variables X, written S1 ≡S
X S2 , iff ∀x ∈ X :
S1 ≡ S
x S2 .

Definition 10. (Imported variables) The imported variables
in a sequence of statements S relative to variables X, written
Imp(S, X), are defined in one of the following cases:

5.2.2 Soundness of the proof rule for equivalent computation
for terminating programs

1. Def (S) ∩ X = ∅: Imp (S, X) = X;
2. S = “id := e” or “input id” or “output e” and Def(S) ∩ X 6=
∅:
Imp(S, X) = Use(S) ∪ (X \ Def(S));
3. S = “If (e) then {St } else
S {Sf }” and Def(S) ∩ X 6= ∅: 
Imp(S, X) = Use(e)∪ y∈X Imp(St , {y})∪Imp(Sf , {y}) ;
4. S = “while(e) {S ′ }” where (Def(S ′ ) ∩ X) 6= ∅): Imp (S, X)
S
i
= i≥0 Imp (S ′ , Use(e) ∪ X);
5. For k > 0, S = s1 ; ...; sk+1 :
Imp(S, X) = Imp(s1 ; ...; sk , Imp(sk+1 , X))

We show that if two programs satisfy the proof rule of equivalent
computation of a variable x (Definition 12) and their value stores
in initial states agree on values of the imported variables relative
to x, then the two programs compute the same value of x if they
terminate. We start by proving the theorem for the base cases of
terminating computation equivalently.
Theorem 1. If s1 and s2 are simple statements that satisfy the
proof rule for equivalent computation of x, s1 ≡S
x s2 , and their initial states m1 (σ1 ) and m2 (σ2 ) agree on the values of the imported
variables relative to x, ∀y ∈ Imp(s1 , {x}) ∪ Imp(s2 , {x}) :
σs1 (y) = σs2 (y), then s1 and s2 equivalently compute x when
started in states m1 and m2 respectively, (s1 , m1 ) ≡x (s2 , m2 ).

Definition 11. (Base cases of the proof rule for equivalent computation for terminating programs) Two simple statements s1
and s2 satisfy the proof rule of equivalent computation of a variable
x, written s1 ≡S
x s2 , iff one of the following holds:

Proof. The proof is a case analysis according to the cases in the
definition of the proof rule for equivalent computation (i.e., Definition 11).

1. s1 = s2 ;
2. s1 =
6 s2 and one of the following holds:
(a) s1 = “input id1 ”, s2 = “input id2 ”, x ∈
/ {id1 , id2 };
(b) Case a) does not hold and x ∈
/ Def(s1 ) ∪ Def(s2 );

1. s1 = s2
Since the two statements are identical, they have the same
imported variables. By assumption, the imported variables of
s1 and s2 have the same initial values, so it is enough to show
Definition 12. (Proof rule of equivalent computation for terthat the value of x at the end of the computation only depends
minating programs) Two statement sequences S1 and S2 satisfy
on the initial values of the imported variables.
the proof rule of equivalent computation of a variable x, written
(a) s1 = s2 = “skip”. In this case, the states before and after
S1 ≡ S
x S2 , iff one of the following holds:
the execution of skip are the same and Imp(skip, {x}) =
1. S1 and S2 are one statement and one of the following holds:
{x}.
(a) S1 and S2 are simple statement: s1 ≡S
(b) s1 = s2 = “lval := e”.
x s2 ;
i. lval = x.
(b) S1 = “If (e) then {S1t } else {S1f }”, S2 = “If (e) then {S2t } else
s1 = s2 = “x := e”. By the definition of imported
{S2f }” such that all of the following hold:
variables, Imp(x := e, {x}) = Use(e). The execution
• x ∈ Def(S1 ) ∩ Def(S2 );
of s1 proceeds as follows.
f
f
t
S
• (S1t ≡S
x S2 ) ∧ (S1 ≡x S2 );
(x := e, m(σ))
(c) S1 = “whilehn1 i (e) {S1′′ }”, S2 = “whilehn2 i (e) {S2′′ }”
→(x := E ′ JeKσ, m(σ)) by the EEval’ rule
such that both of the following hold:
→(skip, m(σ[E ′JeKσ/x])) by the Assign rule.
• x ∈ Def(S1 ) ∩ Def(S2 );
The value of x after the full execution is σ[(E JeKσ)/x](x)
′′
• ∀y ∈ Imp(S1 , {x}) ∪ Imp(S2 , {x}) : S1′′ ≡S
y S2 ;
which only depend on the initial values of the imported
10

2015/9/14

Similarly, (s2 , m2 ) → (skip, m2 (σ2 [tl(σ2 (idI ))/id2 ]
[“σ2 (idIO ) · hd(σ2 (idI ))”/idIO ][hd(σ2 (idI ))/id2 ])).
Let σ2′ = σ2 [tl(σ2 (idI ))/idI ][“σ2 (idIO )·hd(σ2 (idI ))”/idIO ]
[hd(σ2 (idI ))/id2 ]. Then the value of x after the execution of s2 is one of the following:
A. σ2′ (x) = tl(σ2 (idI )) if x = idI
B. σ2′ (x) = σ2 (idIO ) · hd(σ2 (idI )) if x = idIO
Repeatedly, σ2 (idI ) = σ1 (idI ) and σ2 (idIO ) =
σ1 (idIO ). Therefore, the theorem holds.
ii. x ∈
/ {idI , idIO }
Repeatedly, x ∈
/ {id1 , id2 }. By same argument in the
subcase id 6= x of case s1 = s2 = “id := e”, the
theorem holds.
(b) all the above cases do not hold and x ∈
/ Def(s1 ) ∪ Def(s2 )
By same argument in the subcase id 6= x of case s1 = s2 =
“id := e”, the theorem holds.

variables by the property of the expression meaning
function.
ii. lval 6= id.
By the definition of imported variables, Imp(s1 , {x}) =
Imp(s2 , {x}) = {x}. It follows, by assumption, that
∗
σ1 (x) = σ2 (x) and also s1 terminate, (s1 , m1 (σ1 )) →
′
′
′
(skip, m1 (σ1 )). Hence, σ1 (x) = σ1 (x) by Corol∗
lary E.2. Similarly, s2 terminates, (s2 , m2 (σ2 )) →
′
′
′
′
(skip, m2 (σ2 )) and σ2 (x) = σ2 (x). Therefore, σ2 (x) =
σ2 (x) = σ1 (x) = σ1′ (x) and the theorem holds.
(c) s1 = s2 = “input id”.
i. x ∈ Def(input id) = {id, idI , idIO }.
By the In rule, the execution of input id is the following.
(input id, m(σ))
→(skip, m(σ[tl(σ(idI ))/idI ]
[“σ(idIO ) · hd(σ(idI ))”/idIO ][hd(σ(idI ))/id])).
The value of x after the execution of “input id” is one of
the following:
A. tl(σ(idI )) if x = idI .
B. σ1 (idIO ) · hd(σ(idI )) if x = idIO .
C. hd(σ(idI )) if x = id.
By the definition of imported variables, Imp(input id, {x})
= {idIO , idI }. So, in all cases, the value of x only depends on the initial values of the imported variables idI
and idIO .
ii. x ∈
/ Def(input id) = {id, idI , idIO }.
By same argument in the subcase id 6= x of case s1 =
s2 = “id := e”, the theorem holds.
(d) s1 = s2 = “output e”.
i. x = idIO
By the definition of imported variables, Imp(output e, {x}) =
{idIO } ∪ Use(e). The execution of s1 proceeds as follows.
(output e, m(σ))
→(output E JeKσ, m(σ))
¯
→(skip, m(σ[“σ(idIO ) · E JeKσ”/id
IO ])).
¯
The value of x after the execution is “σ(idIO ) · E JeKσ”,
which only depends on the initial value of the imported
variables of the statement “output e” by the expression
meaning function.
ii. x 6= idIO
By same argument in the subcase id 6= x of case s1 =
s2 = “id := e”, the theorem holds.

Theorem 2. If statement sequence S1 and S2 satisfy the proof
rule of equivalent computation of a variable x, S1 ≡S
x S2 , and
their initial states m1 (σ1 ) and m2 (σ2 ) agree on the initial values of the imported variables relative to x, ∀y ∈ Imp(S1 , {x}) ∪
Imp(S2 , {x}) : σ1 (y) = σ2 (y), then S1 and S2 equivalently compute the variable x when started in state m1 and m2 respectively,
(S1 , m1 ) ≡x (S2 , m2 ).
Proof. By induction on size(S1 )+size(S2 ), the sum of the program
size of S1 and S2 .
Base case.
S1 ≡ S
x S2 where S1 and S2 are two simple statements. This
theorem holds by theorem 1.
Induction step
The hypothesis IH is that Theorem 2 holds when size(S1 ) +
size(S2 ) = k ≥ 2.
Then we show that the Theorem holds when size(S1 )+size(S2 ) =
k + 1. The proof is a case analysis according to the cases in the
definition of the proof rule of terminating computation of statement
sequence. the two big categories enum

1. S1 and S2 are one statement such that one of the following
holds:
(a) S1 and S2 are If statement that define the variable x:
S1 = “If (e) then {S1t } else {S1f }”, S2 = “If (e) then {S2t }
else {S2f }” such that all of the following hold:
• x ∈ Def(S1 ) ∩ Def(S2 );
2. s1 6= s2
t
• S1t ≡S
x S2 ;
f
S
(a) s1 = “input id1 ”, s2 = “input id2 ”, x ∈
/ {id1 , id2 }.
• S1 ≡x S2f ;
i. x ∈ {idI , idIO }.
We first show that the evaluations of the predicate expresBy the definition of imported variables, Imp(s1 , {x}) =
sion of S1 and S2 produce the same value when started from
Imp(s2 , {x}) = {idIO , idI }. It follows, by assumption,
state m1 (σ1 ) and m2 (σ2 ), w.l.o.g. say zero. Next, we show
that σ1 (y) = σ2 (y), ∀y ∈ {idIO , idI }. The execution
that S1f started in the state m1 and S2f in the state m2 equivof s1 proceeds as follows.
alently compute the variable x.
(s1 , m1 )
In order to show that the evaluations of predicate ex= (input id1 , m1 (σ1 ))
pression of S1 and S2 produce same value when started
→(skip, m1 (σ1 [tl(σ1 (idI ))/idI ]
from state m1 (σ1 ) and m2 (σ2 ), we show that the vari[“σ1 (idIO ) · hd(σ1 (idI ))”/idIO ][hd(σ1 (idI ))/id1 ]))
ables used in predicate expression of S1 and S2 are a
subset of imported variables in S1 and S2 relative to
′
Let σ1 = σ1 [tl(~v )/idI , “σ1 (idIO )·hd(~v)”/idIO , hd(~v )/id1 ].
x. This is true by the definition of imported variables,
The value of x after the execution of s1 is one of the folUse(e) ⊆ Imp(S1 , {x}), Use(e) ⊆ Imp(S2 , {x}). By aslowing:
sumption, the value stores σ1 and σ2 agree on the values
A. σ1′ (x) = tl(σ1 (idI )) if x = idI .
of the variables used in predicate expression of S1 and
B. σ1′ (x) = σ1 (idIO ) · hd(σ1 (idI )) if x = idIO .
S2 , σ1 (y) = σ2 (y), ∀y ∈ Use(e). By the property of
11

2015/9/14

the values of the imported variables in S1′′ and S2′′ relative to the variables Imp(∆).
By Lemma 5.2, we show S1 and S2 compute the variable
x equivalently when started from state m1 (m1c , σ1 ) and
m2 (m2c , σ2 ) respectively. The theorem holds.
(c) S1 and S2 do not define the variable x: x ∈
/ Def(S1 ) ∪
(S1 , m1 (σ1 ))
Def(S2 ).
f
t
= (If (e) then {S1 } else {S1 }, m1 (σ1 ))
By the definition of imported variable, the imported vari→(If ((0, vof )) then {S1t } else {S1f }, m1 (σ1 ))
ables in S1 and S2 relative to x are both x, Imp(S1 , {x}) =
by the EEval’ rule
Imp(S2 , {x}) = {x}. By assumption, the initial values
σ1 and σ2 agree on the value of the variable x, σ1 (x) =
→(If (0) then {S1t } else {S1f }, m1 (σ1 ))
σ2 (x). In addition, by assumption, execution of S1 and
by the E-Oflow1 or E-Oflow2 rule
S2 when started in state m1 (σ1 ) and m2 (σ2 ) terminate,
→(S1f , m1 (σ1 )) by the If-F rule.
∗
∗
(S1 , m1 (σ1 )) → (skip, m′1 (σ1′ )), (S2 , m2 (σ2 )) → (skip, m′2 (σ2′ )).
f
Finally,
by
Corollary
E.2,
the
value
of
x
is
not
changed
in
Similarly, the execution from (s2 , m2 (σ2 )) gets to (S2 , m2 (σ2 )).
′
′
f
f
execution
of
S
1 and S2 , σ1 (x) = σ1 (x) = σ2 (x) = σ2 (x).
By the hypothesis IH, we show that S1 and S2 compute the
The theorem holds.
variable x equivalently when started in state m1 (σ1 ) and
2.
S
1 and S2 are not both one statement such that one of the
m2 (σ2 ) respectively. To do that, we show that all required
following holds:
conditions are satisfied for the application of hypothesis IH.
(a) Last statements both define the variable x such that all of
• size(S1f ) + size(S2f ) < k.
the following hold:
Because size(S1 ) = 1+size(S1t )+size(S1f ), size(S2 ) =
′
• S1′ ≡S
y S2 , ∀y ∈ Imp(s1 , {x}) ∪ Imp(s2 , {x});
1 + size(S2t ) + size(S2f ).
• x ∈ Def(s1 ) ∩ Def(s2 );
• the value stores σ1 and σ2 agree on the values of the
• s1 ≡S
imported variables in S1f and S2f relative to x, σ1 (y) =
x s2 ;
We show that S1′ and S2′ compute the imported variables in
σ2 (y), ∀y ∈ Imp(S1f , {x}) ∪ Imp(S2f , {x}).
s1 and s2 relative to the variable x equivalently when started
By the definition of imported variables,
in state m1 (σ1 ) and m2 (σ2 ) respectively by the hypothesis
Imp(S1f , {x}) ⊆ Imp(S1 , {x}), Imp(S2f , {x}) ⊆ Imp(S2 , {x}).
IH. To do that, we show the required conditions are satisfied
By the hypothesis IH, S1f and S2f compute the variable
for applying the hypothesis IH.
x equivalently when started in state m1 (σ1 ) and m2 (σ2 )
• size(S1′ ) + size(S2′ ) < k.
respectively. Therefore, the theorem holds.
By the definition of program size, size(s1 ) ≥ 1,
(b) S1 and S2 are while statement that define the variable x:
size(s2 ) ≥ 1. Hence, size(S1′ ) + size(S2′ ) < k.
′′
′′
S1 = “whilehn1 i (e) {S1 }”, S2 = “whilehn2 i (e) {S2 }”
• the executions from (S1 , m1 (σ1 )) and (S2 , m2 (σ2 ))
such that both of the following hold:
terminate respectively,
• x ∈ Def(S1 ) ∩ Def(S2 );
∗
∗
(S1′ , m1 (σ1 )) → (skip, m′′1 (σ1′′ )), (S2′ , m2 (σ2 )) →
′′
S
′′
• S1 ≡y S2 for ∀y ∈ Imp(S1 , {x}) ∪ Imp(S2 , {x});
′′
′′
(skip, m2 (σ2 )).
By Lemma 5.2, we show S1 and S2 compute the variBy assumption, the execution from (S1 , m1 (σ1 )) and
1
able x equivalently when started from state m1 (mc , σ1 )
(S2 , m2 (σ2 )) terminate, then the execution of S1′ and
and m2 (m2c , σ2 ) respectively. The point is to show that
∗
S2′ from state m1 (σ1 ) and m2 (σ2 ) terminate, (S1′ , m1 (σ1 )) →
all required conditions are satisfied for the application of
∗
′′
′′
′
′′
′′
(skip, m1 (σ1 )), (S2 , m2 (σ2 )) → (skip, m2 (σ2 )).
lemma 5.2.
• the initial value stores agree on the values of the vari• loop counter value of S1 and S2 are zero.
ables:
By our assumption, the loop counter value of S1 and S2
Imp(S1′ , Imp(s1 , {x})) ∪ Imp(S2′ , Imp(s2 , {x})).
are initially zero.
By
Lemma 5.3, s1 and s2 have the same imported
• S1 and S2 have same imported variables relative to x,
variables
relative to x, Imp(s1 , {x}) = Imp(s2 , {x})
Imp(S1 , {x}) = Imp (S2 , {x}) = Imp(∆).
= Imp(x). By the definition of imported variables, imThis is obtained by Lemma 5.3.
ported variables in S1′ relative to Imp(x) are same as the
• the initial value store σ1 and σ2 agree on the values
imported variables in S1 relative to x, Imp(S1′ , Imp(s1 , {x})) =
of the imported variables in S1 and S2 relative to x,
Imp(S1 , {x}). Similarly, Imp(S2′ , Imp(s2 , {x})) =
σ1 (y) = σ2 (y), ∀y ∈ Imp(S1 , {x}) ∪ Imp(S2 , {x}).
Imp(S2 , {x}). Then, by assumption, the initial value
By assumption, this holds.
stores agree on the values of the variables Imp(S1′ , Imp(s1 , {x}))
• S1′′ and S2′′ compute the imported variables in S1 and
and
∀y ∈ Imp(S1′ , Imp(s1 , {x})) ∪ Imp(S2′ , Imp(s2 , {x})),
S2 relative to x equivalently, (S1′′ , mS1′′ (σS1′′ )) ≡y
′
Imp(S
2 , Imp(s2 , {x})), σ1 (y) = σ2 (y).
(S2′′ , mS2′′ (σS2′′ )), ∀y ∈ Imp(∆) with value stores σS1′′
By the hypothesis IH, after the full execution of S1′ from
and σS2′′ agreeing on the values of the imported varistate m1 (σ1 ) and the execution of S2′ from state m2 (σ2 ),
ables in S1′′ and S2′′ relative to Imp(∆), σS1′′ (z) =
the value stores agree on the values of the imported variables
in s1 and s2 relative to x, σ1′′ (y) = σ2′′ (y), ∀y ∈ Imp (x) =
σS2′′ (z), ∀z ∈ Imp(S1′′ , Imp(∆)) ∪ Imp(S2′′ , Imp(∆)).
Imp (s1 , {x}) = Imp (s2 , {x}).
By the definition of program size, the sum of the proThen, we show s1 and s2 compute x equivalently. By Corolgram size of S1′ and S2′ is less than k, size(S1′′ ) +
lary E.1, s1 and s2 continue execution after the full exsize(S2′′ ) < k. By the hypothesis IH, S1′′ and S2′′ com∗
ecution of S1′ and S2′ respectively, (S1′ ; s1 , m1 (σ1 )) →
pute the imported variables in S1 and S2 relative to
∗
′′
′′
′
′′
′′
′′
′′
x equivalently when started in states mS1 (σS1 ) and
(s1 , m1 (σ1 )), (S2 ; s2 , m2 (σ2 )) → (s2 , m2 (σ2 )). When
s1 and s2 are while statements, by our assumption of unique
mS2′′ (σS2′′ ) with value store σS1′′ and σS2′′ agreeing on
expression meaning function E , the predicate expression
of S1 and S2 evaluate to the same value when started in
states m1 (σ1 ) and m2 (σ2 ), E JeKσ1 = E JeKσ2 , w.l.o.g,
E JeKσ1 = E JeKσ2 = (0, vof ). Then the execution of S1
proceeds as follows.

12

2015/9/14

loop labels, s1 is not in S1′ . By Corollary E.4, the loop
counter value of s1 is not redefined in the execution of S1′ .
Similarly, the loop counter value of s2 is not redefined in the
execution of S2′ . By the hypothesis IH again, after the full
execution of s1 and s2 , the value stores agree on the value
∗
∗
of x, (s1 , m′′1 (σ1′′ )) → (skip, m′1 (σ1′ )), (s2 , m′′2 (σ2′′ )) →
′
′
′
′
(skip, m2 (σ2 )) such that σ1 (x) = σ2 (x). The theorem
holds.
(b) One last statement does not define the variable x: W.l.o.g.,
′
(x ∈
/ Def(s2 )) ∧ (S1 ≡S
x S2 ).
We show that S1 and S2′ compute the variable x equivalently when started from state m1 (σ1 ) and m2 (σ2 ) by
the hypothesis IH. First, by the definition of program size,
size(s2 ) ≥ 1. Hence, size(S1 ) + size(S2′ ) ≤ k . Next,
by the definition of imported variables, Imp (S2′ , {x}) ⊆
Imp (S2 , {x}). By assumption, σ1 (y) = σ2 (y) for ∀y ∈
Imp (S2′ , {x}) ∪ Imp (S1 , {x}). By the hypothesis IH, S1
and S2′ compute the variable x equivalently when started in
∗
state m1 (σ1 ) and m2 (σ2 ) respectively, (S2′ , m2 (σ2 )) →
∗
′′
′′
′
′
(skip, m2 (σ2 )), (S1 , m1 (σ1 )) → (skip, m1 (σ1 )) such
that σ1′ (x) = σ2′′ (x).
Then, we show that S1 and S2 compute the variable x equivalently after the full execution of s2 . By Corollary E.1,
s2 continues execution immediately after the full execu∗
tion of S2′ , (S2′ ; s2 , m2 ) → (s2 , m′′2 ). By assumption, the
∗
′
execution from (S2 ; s2 , m2 ) terminates, (s2 , m′′2 (σ2′′ )) →
(skip, m′2 (σ2′ )). By Corollary E.2, the value of x is not
changed in the execution of s2 , σ2′ (x) = σ2′′ (x). Hence,
σ1′ (x) = σ2′ (x). The theorem holds.
(c) There are statements moving in/out of If statement:
s1 = “If (e) then {S1t } else {S1f }”, s2 = “If (e) then {S2t }
else {S2f }” such that none of the above cases hold and all
of the following hold:
′
• S1′ ≡S
y S2 for ∀y ∈ Use(e);
′
t
S
• S1 ; S1 ≡x S2′ ; S2t ;
f
′
• S1′ ; S1f ≡S
x S2 ; S2 ;
• x ∈ Def(s1 ) ∩ Def(s2 );
Repeatedly S1 = S1′ ; s1 , S2 = S2′ ; s2 . We first show that,
after the full execution of S1′ and S2′ started in state m1 and
m2 , the predicate expression of s1 and s2 evaluate to the
same value, w.l.o.g, zero. Next we show that S1 and S1′ ; S1f
compute the variable x equivalently when (1) both started
in state m1 and (2) the predicate expression of s1 evaluates
to zero after the full execution of S1′ started in state m1 ,
similarly S2 and S2′ ; S2f compute the variable x equivalently
when (1) both started in the state m2 and (2) the predicate
expression of s2 evaluates to zero after the full execution of
S1′ when started in state m2 . Last we prove the theorem by
showing that S1′ ; S1f started in state m1 and S2′ ; S2f started
in state m2 compute the variable x equivalently.

cution of S1′ and S2′ terminate when started in state m1
and m2 respectively.
• the initial value stores σ1 and σ2 agree on the values
of the imported variables in S1′ and S2′ relative to the
variables used in the predicate expression of s1 and s2 .
By Lemma 5.3, the imported variables in S1′ and S2′ relative to the variables used in predicate expression of s1
and s2 are same, Imp(S1′ , Use(e)) = Imp(S2′ , Use(e)) =
Imp(e). By the definition of imported variable, the imported variables in S1′ relative to the variables used in
predicate expression of s1 are a subset of the imported
variables in S1 relative to x respectively, Imp(S1′ , Use(e)) ⊆
Imp(S1′ , Imp(s1 , {x})) = Imp(S1 , {x}). Similarly
Imp(S2′ , Use(e)) ⊆ Imp(S2 , {x}). Then, by assumption, the initial value stores agree on the values of the
imported variables in S1′ and S2′ relative to the variables used in the predicate expression of s1 and s2 ,
σ1 (y) = σ2 (y), ∀y ∈ Imp(e) = Imp(S1′ , Use(e)) =
Imp(S2′ , Use(e)).
By the hypothesis IH, after the full execution of S1′ and
S2′ , the value stores agree on the values of the variables
used in the predicate expression of s1 and s2 , σ1′′ (y) =
σ2′′ (y), ∀y ∈ Use(e). By Corollary E.1, s1 and s2 continue execution after the full execution of S1′ and S2′ respec∗
∗
tively, (S1′ ; s1 , m1 ) → (s1 , m′′1 (σ1′′ )), and (S2′ ; s2 , m2 ) →
′′
′′
(s2 , m2 (σ2 )).
By the property of expression meaning function E , expression e evaluates to the same value w.r.t value stores σ1′′ and
σ2′′ , w.l.o.g., zero, E JeKσ1′′ = E JeKσ2′′ = 0. Then the execution of s1 proceeds as follows.
(s1 , m′′1 (σ1′′ ))
= (If (e) then {S1t } else {S1f }, m′′1 (σ1′′ ))
→(If (0) then {S1t } else {S1f }, m′′1 (σ1′′ )) by the EEval rule.
→(S1f , m′′1 (σ1′′ )) by the If-F rule.

Similarly, the execution from (s2 , m′′2 (σ2′′ )) gets to (S2f , m′′2 (σ2′′ )).
Then, we show that S1 and S1′ ; S1f compute the variable
x equivalently when both started from state m1 (σ1 ). The
execution of S1′ ; S1f started from state m1 also gets to configuration (S1f , m′′1 (σ1′′ )) because execution of S1 = S1′ ; s1
∗
and S1′ ; S1f share the common execution (S1′ , m1 ) →
f
(skip, m′′1 (σ1′′ )). By Corollary E.1, S1 continues execution
∗
after the full execution of S1′ , (S1′ ; S1f , m1 ) → (S1f , m′′1 ).
f
′
Therefore, the execution of S1 and S1 ; S1 from state m1
compute the variable x equivalently because both executions get to same intermediate configuration. Similarly, S2
and S2′ ; S2f compute the variable x equivalently when both
started from state m2 (σ2 ).
Lastly, we show that S1′ ; S1f and S2′ ; S2f compute the variable x equivalently when started in states m1 (σ1 ) and
m2 (σ2 ) respectively by the hypothesis IH. To do that, we
show that all required conditions are satisfied for the application of hypothesis IH.
• size(S1′ ; S1f ) + size(S2′ ; S2f ) < k.
This is obtained by the definition of program size.
• execution of S1′ ; S1f and S2′ ; S2f terminate when started
in state m1 (σ1 ) and m2 (σ2 ) respectively.
This is obtained by above argument.
• σ1 (y) = σ2 (y), ∀y ∈ Imp(S1′ ; S1f , {x})∪Imp(S2′ ; S2f , {x}).
We show that Imp(S1′ ; S1f , {x}) ⊆ Imp(S1 , {x}) as
follows.
Imp(S1f , {x})

In order to show that S1′ and S2′ compute the variables
used in predicate expression of s1 and s2 equivalently by
the hypothesis IH, we show that all required conditions are
satisfied for the application of hypothesis IH.
• size(S1′ ) + size(S2′ ) < k.
The sum of program size of S1′ and S2′ are less than k by
the definition of program size for s1 and s2 , size(S1′ ) +
size(S2′ ) < k.
∗
• the execution of S1′ and S2′ terminate, (S1′ , m1 ) →
∗
′′
′′
′
′′
′′
(skip, m1 (σ1 )), and (S2 , m2 ) → (skip, m2 (σ2 )).
By assumption, the execution of S1 and S2 from the
state m1 and m2 respectively terminate, then the exe-

13

2015/9/14

⊆ Imp(s1 , {x}) (1) by the definition of imported variables.
Imp(S1′ ; S1f , {x})
= Imp(S1′ , Imp(S1f , {x})) by Lemma
⊆ Imp(S1′ , Imp(s1 , {x})) by (1)

C.1

= Imp(S1 , {x}) by the definition of imported variables.
Imp(S2′ ; S2f , {x})

Similarly,
⊆ Imp(S2 , {x}). Then, by
assumption, the initial value stores agree on the values
of the imported variables in S1′ ; S1f and S2′ ; S2f relative
to x.
Then, by the hypothesis IH, after the full execution of
S1′ ; S1f and S2′ ; S2f , the value stores agree on the value
∗
∗
of x, (S1′ ; S1f , m1 ) → (skip, m′1 (σ1′ )), (S2′ ; S2f , m2 ) →
′
′
′
′
(skip, m2 (σ2 )) such that σ1 (x) = σ2 (x).
In conclusion, after execution of S1 and S2 , the value stores
agree on the value of x. Therefore, the theorem holds.

′

∗

∗

• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) →
′

(s1 , m1i (loop1ci , σ1i )), loop1c (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
′
2
2i
(s2 , m2i (loopc , σ2i )), loopc (n2 ) ≤ i;
Proof. By induction on i.
Base case. i = 1.
By assumption, initial loop counters of s1 and s2 are of value
zero. Initial value stores σ1 and σ2 agree on the values of the
variables in Imp(x). Then we show one of the following cases hold:
1. The loop counters for s1 and s2 are always less than 1:
′
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 )
′
∗
′
′
2′
1′
→ (S2 , m2 (loopc )), loopc (n1 ) < 1 and loop2c (n2 ) < 1;
2. There are two configurations (s1 , m11 ) and (s2 , m21 ) reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in which the
loop counters of s1 and s2 are equal to 1 and value stores agree
on the values of imported variables relative to x and, for ev∗
∗
ery state in execution, (s1 , m1 ) → (s1 , m11 ) or (s2 , m2 ) →
(s2 , m21 ) the loop counters for s1 and s2 are less than or equal
to one respectively:
∗
∃(s1 , m11 ), (s2 , m21 ) : (s1 , m1 ) → (s1 , m11 (loop1c1 , σ11 ))∧
∗
21
(s2 , m2 ) → (s2 , m21 (loopc , σ21 )) where
• loop1c 1 (n1 ) = loop2c 1 (n2 ) = 1; and
• ∀y ∈ Imp(x) : σ11 (y) = σ21 (y); and
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m11 (loop1c 1 , σ11 )),
′
1
loopc (n1 ) ≤ 1; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m21 (loop2c 1 , σ21 )),
2′
loopc (n2 ) ≤ 1.

5.2.3 Supporting lemmas for the soundness proof of
equivalent computation for terminating programs
The lemmas include the proof of two while statements computing a
variable equivalently used in the proof of Theorem 2 and the property that two programs have same imported variables relative to a
variable x if the two programs satisfy the proof rule of equivalent
computation of the variable x. From the proof rule of terminating
computation of a variable x equivalently, we have the two programs
either both define x or both do not.
Lemma 5.1. Let s1 = “whilehn1 i (e) {S1 }” and s2 = “whilehn2 i (e)
{S2 }” be two while statements with the same set of imported variables relative to a variable x (defined in s1 and s2 ), Imp(x), and
whose loop bodies S1 and S2 terminatingly compute the variables
in Imp(x) equivalently when started in states that agree on the
values of the variables imported by S1 or S2 relative to Imp(x):

We show evaluations of the predicate expression of s1 and s2
w.r.t value stores σ1 and σ2 produce same S
value. By the definition of imported variables, Imp(s1 , {x}) = j≥0 Imp(S1j , {x} ∪
Use(e)). By our notation of S 0 , S10 = skip. By the definition of
imported variables, Imp(S10 , {x} ∪ Use(e)) = {x} ∪ Use(e). Then
Use(e) ⊆ Imp(x). By assumption, value stores σ1 and σ2 agree
on the values of the variables in Use(e). By Lemma D.1, the predicate expression e of s1 and s2 evaluates to same value v w.r.t value
stores σ1 , σ2 , E ′ JeKσ1 = E ′ JeKσ2 = v. Then there are two possibilities to consider.

• x ∈ Def(s1 ) ∩ Def(s2 );
• Imp(s1 , {x}) = Imp(s2 , {x}) = Imp(x);
• ∀y ∈ Imp(x), ∀mS1 (σS1 ), mS2 (σS2 ) :

((∀z ∈ Imp(S1 , Imp(x)) ∪ Imp(S2 , Imp(x)) : σS1 (z) =
σS2 (z)) ⇒ (S1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).

If the executions of s1 and s2 terminate when started in
states m1 (loop1c , σ1 ) and m2 (loop2c , σ2 ) in which s1 and s2
have not already executed (loop counter initially 0: loop1c (n1 ) =
loop2c (n2 ) = 0), and whose value stores σ1 and σ2 agree on the
values of the variables in Imp(x), ∀y ∈ Imp(x), σ1 (y) = σ2 (y),
then, for any positive integer i, one of the following holds:

1. E ′ JeKσ1 = E ′ JeKσ2 = v = 0
The execution from (s1 , m1 (loop1c , σ1 )) proceeds as follows.
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c ))
→(whilehn1 i (0) {S1 }, m1 (loop1c )) by the EEval’ rule
→(skip, m1 (loop1c [0/n1 ])) by the Wh-F rule.

1. The loop counters for s1 and s2 are always less than i:
′
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c ) and (s2 , m2 )
′
∗
→ (S2′ , m′2 (loop2c )),
′
′
loop1c (n1 ) < i and loop2c (n2 ) < i;
2. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in which the
loop counters of s1 and s2 are equal to i and value stores agree
on the values of imported variables relative to x and, for ev∗
∗
ery state in execution, (s1 , m1 ) → (s1 , m1i ) or (s2 , m2 ) →
(s2 , m2i ) the loop counters for s1 and s2 are less than or equal
to i respectively:
∗
∃(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (loop1ci , σ1i )) ∧
∗
2i
(s2 , m2 ) → (s2 , m2i (loopc , σ2i )) where
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀y ∈ Imp(x) : σ1i (y) = σ2i (y) and

2

Similarly, (s2 , m2 (loop2c , σ2 )) → (skip, m2 (loop2c [0/n2 ])).
In conclusion, the loop counters of s1 and s2 in any states of
the execution from (s1 , m1 ) and (s2 , m2 ) respectively are less
′
∗
than 1, ∀m′1 , m′2 such that (s1 , m1 ) → (S1 ′ , m′1 (loop1c ) and
′
′
′
∗
(s2 , m2 ) → (S2 ′ , m′2 (loop2c )), loop1c (n1 ) < 1, loop2c (n2 ) <
1.
2. E ′ JeKσ1 = E ′ JeKσ2 = v 6= 0
The execution from (s1 , m1 (loop1c , σ1 )) proceeds as follows.
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c , σ1 ))
→(whilehn1 i (v) {S1 }, m1 (loop1c , σ1 )) by the EEval’ rule
→(S1 ; whilehn1 i (e) {S1 }, m1 (loop1c [1/(n1 )], σ1 ))
by the Wh-T rule.

14

2015/9/14

2

The induction hypothesis IH is that, for a positive integer i, one of
Similarly, (s2 , m2 (loop2c , σ2 )) → (S2 ; whilehn2 i (e){S2 }, m2
2
the
following holds:
(loopc [1/(n2 )], σ2 )). Then, the loop counters of s1 and s2 are
1, value stores σ11 and σ21 agree on values of variables in
1. The loop counters for s1 and s2 are always less than i:
′
Imp(x): loop1c [1/n1 ](n1 ) =
∗
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c ) and (s2 , m2 ) →
loop2c [1/(n2 )](n2 ) = 1; and ∀y ∈ Imp(x), σ11 (y) = σ21 (y).
′
′
2′
(S2 , m2 (loopc )),
By assumption, the execution of s1 terminates when started in
′
′
loop1c (n1 ) < i and loop2c (n2 ) < i;
the state m1 (, σ1 ), then the execution of S1 terminates when
∗
1
2. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable
started in the state m1 (loopc [1/(n1 )], σ1 ), (s1 , m1 ) →
∗
from (s1 , m1 ) and (s2 , m2 ), respectively, in which the loop
(S1 ; whilehn1 i (e){S1 }, m1 (loop1c [1/(n1 )], σ1 )) → (skip, m′1 ) ⇒
counters of s1 and s2 are equal to i and value stores agree on the
∗
(S1 , m1 (loop1c [1/(n1 )], σ1 )) → (skip, m11 (loop1c1 , σ11 )).
values of imported variables relative to x and, for every state in
Similarly, the execution of S2 terminates when started in the
∗
∗
execution, (s1 , m1 ) → (s1 , m1i ) and (s2 , m2 ) → (s2 , m2i )
state m2 (loop2c [1/(n2 )], σ2 ),
the loop counters for s1 and s2 are less than or equal to i
∗
(S2 , m2 (loop2c [1/(n2 )], σ2 )) → (skip, m21 (loop2c1 , σ21 )).
respectively:
We show that, after the full execution of S1 and S2 , the follow∗
∃(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (loop1ci , σ1i )) ∧
ing four properties hold.
∗
2i
(s2 , m2 ) → (s2 , m2i (loopc , σ2i )) where
• The loop counters of s1 and s2 are of value 1, loop1c 1 (n1 ) =
21
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
loopc (n2 ) = 1.
• ∀y ∈ Imp(x), σ1i (y) = σ2i (y); and
By assumption of unique loop labels, s1 ∈
/ S1 . Then, the
′
∗
∗
loop counter value of n1 is not redefined in the execution of
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c ) →
11
1
′
1
1i
S1 by corollary E.4, loopc [1/(n1 )](n1 ) = loopc (n1 ) = 1.
(s1 , m1i (loopc , σ1i )), loopc (n1 ) ≤ i; and
′
Similarly, the loop counter value of n2 is not redefined in
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
2
21
the execution of S2 , loopc [1/(n2 )](n2 ) = loopc (n2 ) = 1.
2′
2i
∗
(s2 , m2i (loopc , σ2i )), loopc (n2 ) ≤ i.
• In any state in the execution (s1 , m1 ) → (s1 , m11 (loop1c 1 , σ11 )),
the loop counter of s1 is less than or equal to 1.
Then we show that, for the positive integer i + 1, one of the
The loop counter of s1 is zero in any of the two states in
following holds:
the one step execution (s1 , m1 ) → (whilehn1 i (v) {S1 },
1. The loop counters for s1 and s2 are always less than i + 1:
m1 (loop1c , , σ1 )), and the loop counter of s1 is 1 in any
′
∗
states in the execution
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 )
∗
1
′
∗
(S1 ; whilehn1 i (e) {S1 }, m1 (loopc [i/(n1 )], σ1 )) →
→ (S2′ , m′2 (loop2c )),
′
′
(s1 , m11 (loop1c1 , σ11 )).
loop1c (n1 ) < i + 1 and loop2c (n2 ) < i + 1;
∗
• In any state in the executions (s2 , m2 ) → (s2 , m21 (loop2c 1 , σ21 )), 2. There are two configurations (s , m
1
1i+1 ) and (s2 , m2i+1 )
the loop counter of s2 is less than or equal to 1.
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in which
By similar argument for the loop counter of s1 .
the loop counters of s1 and s2 are equal to i + 1 and value
• The value stores σ11 and σ21 agree on the values of the
stores agree on the values of imported variables relative to x
∗
imported variables in s1 and s2 relative to the variable x:
and, for every state in executions (s1 , m1 ) → (s1 , m1i+1 ) and
∀y ∈ Imp(x), σ11 (y) = σ21 (y).
∗
(s2 , m2 ) → (s2 , m2i+1 ) the loop counters for s1 and s2 are
We show that the imported variables in S1 relative to those
less
than
or
equal to i + 1 respectively:
in Imp(x) are a subset of Imp(x).
∗
∃(s1 , m1i+1 ), (s2 , m2i+1 ) : (s1 , m1 ) → (s1 , m1i+1 (loop1ci+1 ,
∗
Imp(S1 , Imp(x))
2i+1
σ1i+1 )) ∧ (s2 , m2 ) → (s2 , m2i+1 (loopc , σ2i+1 )) where
= Imp(S1 , Imp(s
1 , {x} ∪ Use(e))) by the definition of Imp(x)
S
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
j
= Imp(S1 , j≥0 Imp(S1 , {x} ∪ Use(e)))
• ∀y ∈ Imp(x), σ1i+1 (y) = σ2i+1 (y); and
by the definition of imported variables
′
∗
∗
S
j
•
∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) →
= j≥0 Imp(S1 , Imp(S1 , {x} ∪ Use(e))) by Lemma C.2
′
S
(s1 , m1i+1 (loop1ci+1 , σ1i+1 )), loop1c (n1 ) ≤ i + 1; and
= j>0 Imp(S1j , {x} ∪ Use(e)) by Lemma C.1
S
′
∗
∗
j
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
⊆ j≥0 Imp(S1 , {x} ∪ Use(e))
2′
2i+1
= Imp(s1 , {x} ∪ Use(e)) = Imp(x).
(s2 , m2i+1 (loopc , σ2i+1 )), loopc (n2 ) ≤ i + 1.
Similarly, Imp(S2 , Imp(x)) ⊆ Imp(x). Consequently,
the value stores σ11 and σ21 agree on the values of the
imported variables in S1 and S2 relative to those in Imp(x),
∀y ∈ Imp(S1 , Imp(x)) ∪ Imp(S2 , Imp(x)), σ1 (y) = σ2 (y).
Because S1 and S2 have computation of every variable in
Imp(x) equivalently when started in states agreeing on the
values of the imported variables relative to Imp(x), then
value store σ11 and σ21 agree on the values of the variables
Imp(x), ∀y ∈ Imp(x), σ11 (y) = σ21 (y).
It follows that, by corollary E.1,
∗
(S1 ; s1 , m1 (loop1c [1/n1 ], σ1 )) → (s1 , m11 (loop1c1 , σ11 )) and
∗
2
(S2 ; s2 , m2 (loopc [1/n2 ], σ2 )) → (s2 , m21 (loop2c1 , σ21 )).

By the hypothesis IH, one of the following holds:
1. The loop counters for s1 and s2 are always less than i:
′
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c ) and (s2 , m2 )
′
∗
→ (S2′ , m′2 (loop2c )),
′
′
loop1c (n1 ) < i and loop2c (n2 ) < i;
When this case holds, then we have the loop counters for s1 and
s2 are always less than i + 1:
′
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c ) and (s2 , m2 )
′
∗
→ (S2′ , m′2 (loop2c )),
′
′
loop1c (n1 ) < i + 1 and loop2c (n2 ) < i + 1.
2. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable
from (s1 , m1 ) and (s2 , m2 ), respectively, in which the loop
counters of s1 and s2 are equal to i and value stores agree on the

Induction Step.

15

2015/9/14

• in any state in the executions
values of imported variables relative to x and, for every state in
∗
∗
∗
(s2 , m2i ) → (s2 , m2i+1 (loop2ci+1 , σ2i+1 )), the loop
executions (s1 , m1 ) → (s1 , m1i ) and (s2 , m2 ) → (s2 , m2i )
counter of s2 is less than or equal to i + 1.
the loop counters for s1 and s2 are less than or equal to i
respectively:
With the hypothesis IH, there are two configurations (s1 , m1i+1 )
∗
and (s2 , m2i+1 ) reachable from (s1 , m1 ) and (s2 , m2 ), re∃(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (loop1ci , σ1i )) ∧
∗
2i
spectively, in which the loop counters of s1 and s2 are equal
(s2 , m2 ) → (s2 , m2i (loopc , σ2i )) where
to i + 1 and value stores agree on the values of imported
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
variables
relative to x and, for every state in executions
• ∀y ∈ Imp(x), σ1i (y) = σ2i (y); and
∗
∗
(s1 , m1 ) → (s1 , m1i+1 ) and (s2 , m2 ) → (s2 , m2i+1 ) the
∗
∗
′
′
′
1′
1i
• ∀m1 : (s1 , m1 ) → (S1 , m1 (loopc ) → (s1 , m1i (loopc ,
loop counters for s1 and s2 are less than or equal to i + 1
′
, σ1i )), loop1c (n1 ) ≤ i; and
respectively:
′
∗
∗
∃(s1 , m1i+1 ), (s2 , m2i+1 ) :
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i (loop2c i ,
′
∗
(s1 , m1 ) → (s1 , m1i+1 (loop1ci+1 , σ1i+1 ))∧
), σ2i )), loop2c (n2 ) ≤ i.
∗
By similar argument in base case, evaluations of the predicate
(s2 , m2 ) → (s2 , m2i+1 (loop2ci+1 , σ2i+1 )) where
expression of s1 and s2 w.r.t value stores σ1i and σ2i produce
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
same value. Then there are two possibilities:
• ∀y ∈ Imp(x), σ1i+1 (y) = σ2i+1 (y); and
(a) E ′ JeKσ1i = E ′ JeKσ2i = (0, vof )
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) →
1
The execution from (s1 , m1 (loopc , σ1 )) proceeds as fol1′
1i+1
(s1 , m1i+1 (loopc , σ1i+1 )), loopc (n1 ) ≤ i+1; and
lows.
′
∗
∗
1i
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
(s1 , m1i (loopc , σ1i ))
1i
2′
2i+1
= (whilehn1 i (e) {S1 }, m1i (loopc , σ1i ))
(s2 , m2i+1 (loopc , σ2i+1 )), loopc (n2 ) ≤ i + 1.
→(whilehn1 i ((0, vof )) {S1 }, m1i (loop1ci , σ1i )) by the EEval’ rule
→(whilehn1 i (0) {S1 }, m1i (loop1ci , σ1i ))
by the E-Oflow1 and E-Oflow2 rule
→(skip, m1i (loop1ci [0/n1 ], σ1i )) by the Wh-F rule.
Lemma 5.2. Let s1 = “whilehn1 i (e) {S1 }” and s2 = “whilehn2 i (e)
{S2 }” be two while statements with the same set of imported variBy the hypothesis IH, the loop counter of s1 and s2 in any
∗
configuration in executions (s1 , m1 ) → (s1 , m1i (loop1ci , σ1i )) ables relative to a variable x (defined in s1 and s2 ), and whose loop
∗
bodies S1 and S2 terminatingly compute the variables in Imp(x)
and (s2 , m2 ) → (s2 , m2i (loop2ci , σ2i )) respectively are
equivalently when started in states that agree on the values of the
less than or equal to i,
variables imported by S1 or S2 relative to Imp(x):
∗
∗
1′
′
′
′
1i
∀m1 : (s1 , m1 ) → (S1 , m1 (loopc ) → (s1 , m1i (loopc ,
′
• x ∈ Def(s1 ) ∩ Def(s2 );
, σ1i )), loop1c (n1 ) ≤ i; and
∗
∗
2′
′
′
′
2i
• Imp(s1 , {x}) = Imp(s2 , {x}) = Imp(x);
∀m2 : (s2 , m2 ) → (S2 , m2 (loopc )) → (s2 , m2i (loopc ,
• ∀y ∈ Imp(x) ∀mS1 (σS1 ) mS2 (σS2 ) :
2′
), σ2i )), loopc (n2 ) ≤ i.
((∀z ∈ Imp(S1 , Imp(x))∪Imp(S2 , Imp(x)), σS1 (z) = σS2 (z)) ⇒
Therefore, the loop counter of s1 and s2 in any configura((S
1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).
tion in executions
∗
(s1 , m1 ) → (skip, m1i (loop1ci \ {(n1 )}, σ1i )) and
If the executions of s1 and s2 terminate when started in
∗
(s2 , m2 ) → (skip, m2i (loop2ci [0/n2 ], σ2i )) respectively
states m1 (loop1c , σ1 ) and m2 (loop2c , σ2 ) in which s1 and s2
are less than i + 1.
have not already executed (loop counter initially 0: loop1c (n1 ) =
(b) E ′ JeKσ1i = E ′ JeKσ2i = v 6= 0
loop2c (n2 ) = 0), and whose value stores σ1 and σ2 agree on
the values of the variables in Imp(x), ∀y ∈ Imp(x) σ1 (y) =
The execution from (s1 , m1i (loop1ci , σ1i )) proceeds as fol∗
lows.
σ2 (y), when s1 and s2 terminate, (s1 , m1 ) → (skip, m1i (σ1′ ))
∗
(s1 , m1i (loop1ci , σ1i ))
and (s2 , m2 ) → (skip, m2i (σ2′ )), value stores σ1′ and σ2′ agree on
the value of x, σ1′ (x) = σ2′ (x).
= (whilehn1 i (e) {S1 }, m1i (loop1ci , σ1i ))
1i
→(whilehn1 i (v) {S1 }, m1i (loopc , σ1i )) by the EEval’ rule
→(S1 ; whilehn1 i (e) {S1 }, m1i (loop1ci [i + 1/(n1 )]
Proof. We show that there must exist a finite integer k such that the
, σ1i )) by the Wh-T rule.
loop counters of s1 and s2 in executions started in states m1 and
2
m2 is always less than k. By the definition of terminating execuSimilarly, (s2 , m2i (loop2ci , σ2i )) → (S2 ; whilehn2 i (e){S2 },
tion, there are only finite number of steps in executions of s1 and
2i
m2i (loopc [i + 1/(n2 )], σ2i )).
s2 started in states m1 and m2 respectively. Then, by Lemma E.8,
By similar argument in base case, the executions of S1
there must be a finite integer k such that the loop counter of s1 and
1i
and S2 terminate when started in states m1i (loopc [i +
s2 is always less than k. In the following, we consider k be the
1/(n1 )], , σ1i ) and m2i (loop2ci [i + 1/(n2 )], σ2i ) respecsmallest positive integer such that the loop counter of s1 and s2 in
∗
tively, (S1 ; s1 , m1i (loop1ci [i + 1/(n1 )], σ1i )) →
executions started in states m1 and m2 is always less than k.
(s1 , m1i+1 (loop1ci+1 , σ1i+1 )) and (S2 ; s1 , m2i (loop2ci [i +
By Lemma 5.1, there are two possibilities:
∗
1/(n2 )], σ2i )) → (s2 , m2i+1 (loop2ci+1 , σ2i+1 )) such that
1. The loop counters for s1 and s2 are always less than 1 (k = 1):
all of the following holds:
′
∗
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 )
′
∗
′
′
2
• ∀y ∈ Imp(x), σ1i+1 (y) = σ2i+1 (y), and
→ (S2 , m2 (loopc )),
′
′
• in any state in the execution
loop1c (n1 ) < 1 and loop2c (n2 ) < 1;
∗
1i+1
(s1 , m1i ) → (s1 , m1i+1 (loopc , σ1i+1 )), the loop
By the proof in base case of Lemma 5.1, the execution of s1
counter of s1 is less than or equal to i + 1.
proceeds as follows:

16

2015/9/14

(s1 , m1 )
= (whilehn1 i (e) {S1 }, m1 (loop1c , σ1 ))
→(whilehn1 i (0) {S1 }, m1 (loop1c , σ1 )) by the EEval’ rule
→(skip, m1 (loop1c [0/(n1 )], σ1 )) by the Wh-F rule.

Proof. By induction on size(S1 )+size(S2 ), the sum of the program
size of S1 and S2 .
Base case.
S1 and S2 are simple statement. Then the proof is a case analysis
according to the cases in the definition of the proof rule of computation equivalently for simple statements.

Similarly, the execution of s2 proceeds to
(skip, m2 (loop2c [0/n2 ], σ2 )). Therefore, σ1′ = σ1 and σ2′ =
σ2 .
By the definition of imported variables, x ∈ Imp(s1 , {x}). By
assumption, value stores σ1 and σ2 agree on the value of x,
σ1′ (x) = σ1 (x) = σ2 (x) = σ2′ (x). The lemma holds.
2. For some finite positive k(> 1), both of the following hold:
• The loop counters for s1 and s2 are always less than k:
′
∗
∀m′1 , m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c ) and
′
∗
(s2 , m2 ) → (S2′ , m′2 (loop2c )),
′
′
loop1c (n1 ) < k and loop2c (n2 ) < k;
• There are two configuration (s1 , m1k−1 ) and (s2 , m2k−1 )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which the loop counters of s1 and s2 are equal to k − 1
and value stores agree on the values of imported variables
∗
relative to x and, for every state in execution, (s1 , m1 ) →
∗
(s1 , m1k−1 ) or (s2 , m2 ) → (s2 , m2k−1 ) the loop counters
for s1 and s2 are less than or equal to k − 1 respectively:
∃(s1 , m1k−1 ), (s2 , m2k−1 ) :
1
∗
(s1 , m1 ) → (s1 , m1k−1 (loopck−1 , σ1k−1 ))∧
2k−1
∗
, σ2k−1 )) where
(s2 , m2 ) → (s2 , m2k−1 (loopc

1. S1 = S2
By the definition of imported variables, same statement have
same imported variables relative to same x.
2. S1 6= S2
There are two further cases:
• S1 = “input id1 ”, S2 = “input id2 ” and x ∈
/ {id1 , id2 }.
When x = idI , by the definition of imported variables,
Imp(S1 , {idI }) = Imp(S2 , {idI }) = {idI }. When x =
idIO , by the definition of imported variables, Imp(S1 , {idIO }) =
Imp(S2 , {idIO }) = {idI , idIO }.
When x ∈
/ {idI , idIO }, by the definition of imported variables, Imp(S1 , {x}) = Imp(S2 , {x}) = {x}.
• the above cases do not hold and x ∈
/ Def(S1 ) ∪ Def(S2 ).
By the definition of imported variables, Imp(S1 , {x}) =
Imp(S2 , {x}) = {x}.
Induction Step.
The hypothesis IH is that the lemma holds when size(S1 ) +
size(S2 ) = k ≥ 2.
Then we show the lemma holds when size(S1 )+size(S2 ) = k +1.
The proof is a case analysis based on the cases in the definition of
the proof rule of computation equivalently for statement sequence,
S1 ≡ S
x S2 :

2

1

loopck−1 (n1 ) = loopck−1 (n2 ) = k − 1; and
∀y ∈ Imp(x) : σ1k−1 (y) = σ2k−1 (y); and
′

∗

∗

∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c ) →
′
1
(s1 , m1k−1 (loopck−1 , σ1k−1 )), loop1c (n1 ) ≤ k − 1;
and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
2k−1
2′
, σ2k−1 )), loopc (n2 ) ≤ k − 1;
(s2 , m2k−1 (loopc
By proof of Lemma 5.1, value stores σ1k−1 and σ2k−1 agree
on the values of the variables in Use(e). By Lemma D.1,
E ′ JeKσ1k−1 = E ′ JeKσ2k−1 = v. Because the loop counter of
s1 and s2 is less than k in executions of s1 and s2 when started
in states m1 and m2 , then by our semantic rules, the predicate
expression of s1 and s2 must evaluate to zero w.r.t value stores
σ1k−1 and σ2k−1 , E ′ JeKσ1k−1 = E ′ JeKσ2k−1 = (0, vof ). Then
the execution of s1 proceeds as follows.

1. S1 and S2 are one statement such that one of the following
holds:
(a) S1 and S2 are If statement that define the variable x:
S1 = “If (e) then {S1t }else {S1f }”, S2 = “If (e) then {S2t }
else {S2f }” such that all of the following hold:
• x ∈ Def(S1 ) ∩ Def(S2 );
t
• S1t ≡S
x S2 ;
f
S
• S1 ≡x S2f ;
By the hypothesis IH, the imported variables in S1t and
S2t relative to x are same, Imp(S1t , {x}) = Imp(S2t , {x}).
Similarly, Imp(S1f , {x}) = Imp(S2f , {x}). By the definition of imported variables, Imp(S1 , {x}) = Use(e) ∪
Imp(S1t , {x}) ∪ Imp(S1f , {x}). Similarly, Imp(S2 , {x})
= Use(e)∪Imp(S2t , {x})∪Imp(S2f , {x}). Then, the lemma
holds.
(b) S1 and S2 are while statement that define the variable x:
S1 = “whilehn1 i (e) {S1′′ }”, S2 = “whilehn2 i (e) {S2′′ }”
such that both of the following hold:
• x ∈ Def(S1 ) ∩ Def(S2 );
′′
• ∀y ∈ Imp(S1 , {x}) ∪ Imp(S2 , {x}), S1′′ ≡S
y S2 ;
By the definition of imported variables, Imp(S1 , {x}) =
S
′′
Imp(S1 i , {x} ∪ Use(e)). Similarly, Imp(S2 , {x}) =
Si≥0
′′ i
′′ i
i≥0 Imp(S2 , {x}∪Use(e)). Then, we show that Imp(S1 , {x} ∪

1

(whilehn1 i (e) {S1 }, m1k−1 (loopck−1 , σ1k−1 ))
1
→(whilehn1 i ((0, vof )) {S1 }, m1k−1 (loopck−1 , σ1k−1 ))
by the EEval’ rule
1
→(whilehn1 i (0) {S1 }, m1k−1 (loopck−1 , σ1k−1 ))
by the E-Oflow1 or E-Oflow2 rule
1
→(skip, m1k−1 (loopck−1 [0/n1 ], σ1k−1 ))
by the Wh-F rule.
Similarly, the execution of s2 proceeds to
2
(skip, m2k−1 (loopck−1 [0/n2 ], σ2k−1 )). Therefore, σ2′ = σ2k−1 ,
′
σ1 = σ1k−1 . By the definition of imported variables, x ∈
Imp(x). In conclusion, σ2′ (x) = σ2k−1 (x) = σ1k−1 (x) =
σ1′ (x).

′′

Use(e)) = Imp(S2 i , {x} ∪ Use(e)) by induction on i.
Base case.
′′
′′
By our assumption of the notation S 0 , S1 0 = skip, S2 0 =
skip.
′′
′′
Then, Imp(S1 0 , {x}∪Use(e)) = {x}∪Use(e), Imp(S2 0 , {x}∪
Use(e)) = {x} ∪ Use(e).
′′
′′
Hence, Imp(S1 0 , {x}∪Use(e)) = Imp(S2 0 , {x}∪Use(e)).
Induction step.

Lemma 5.3. If two statement sequences S1 and S2 satisfy the proof
rule of terminating computation of a variable x equivalently, then
S1 and S2 have same imported variables relative to x: (S1 ≡S
x
S2 ) ⇒ (Imp(S1 , {x}) = Imp(S2 , {x})).
17

2015/9/14

′′

The hypothesis IH2 is that Imp(S1 i , {x} ∪ Use(e)) =
′′
Imp(S2 i , {x} ∪ Use(e)) for i ≥ 0.
′′
′′
Then we show that Imp(S1 i+1 , {x}∪Use(e)) = Imp(S2 i+1 , {x}∪
Use(e)).
′′

Imp(S1 i+1 , {x} ∪ Use(e))
′′
′′
= Imp(S1 , Imp(S1 i , {x} ∪ Use(e))) (1) by corollary C.1
′′

Imp(S2 i+1 , {x} ∪ Use(e))
′′
′′
= Imp(S2 , Imp(S2 i , {x} ∪ Use(e))) (2) by corollary C.1
′′

′′

Imp(S1 i , {x} ∪ Use(e)) = Imp(S2 i , {x} ∪ Use(e))
by the hypothesis IH2
′′
′′
Imp(S1 , Imp(S1 i , {x} ∪ Use(e)))
′′
′′
= Imp(S2 , Imp(S2 i , {x} ∪ Use(e)))
by the hypothesis IH
′′
Imp(S1 i+1 , {x} ∪ Use(e))
′′
= Imp(S2 i+1 , {x} ∪ Use(e)) by (1),(2)
′′
′′
Therefore, Imp(S1 i+1 , {x}∪Use(e)) = Imp(S2 i+1 , {x}∪
Use(e)).
In conclusion, Imp(S1 , {x}) = Imp(S2 , {x}). The lemma
holds.
(c) S1 and S2 do not define the variable x: x ∈
/ Def(S1 ) ∪
Def(S2 ).
By the definition of imported variable, the imported variables in S1 and S2 relative to x is x, Imp(S1 , {x}) =
Imp(S2 , {x}) = {x}. The lemma holds.

2. S1 and S2 are not both one statement such that one of the
following holds:
(a) Last statements both define the variable x such that all of
the following hold:
′
• ∀y ∈ Imp(s1 , {x}) ∪ Imp(s2 , {x}), S1′ ≡S
y S2 ;
• x ∈ Def(s1 ) ∩ Def(s2 );
• s1 ≡S
x s2 ;
By the hypothesis IH, we have Imp(s1 , {x}) = Imp(s2 , {x}) =
Imp(∆). Then, by the hypothesis IH again, we have that
∀y ∈ Imp(∆) = Imp(s1 , {x}) = Imp(s2 , {x}), Imp(S1′ , {y})
= Imp(S2′ , {y}). By taking the union of all ∀y ∈ Imp(∆),
Imp(S1′ , {y}) and Imp(S2′ , {y}), by the Lemma C.2, Imp(S1′ ,
Imp(∆)) = Imp(S2′ , Imp(∆)). By the definition of imported variables,
Imp(S1 , {x}) = Imp(S1′ , Imp(s1 , {x})), Imp(S2 , {x}) =
Imp(S2′ , Imp(s2 , {x})). Therefore, the lemma holds.
(b) One last statement does not define
the variable x: w.l.o.g.,

x∈
/ Def(s1 ) ∧ (S1′ ≡S
x S2 ) ;
By the definition of imported variables, we have Imp(s1 , {x})
= {x}. By the hypothesis IH, Imp(S1′ , {x}) = Imp(S2 , {x})
Therefore, by the definition of imported variables, Imp(S1 , {x})
= Imp(S1′ , Imp(s1 , {x})) = Imp(S1′ , {x}) = Imp(S2 , {x}).
(c) There are statements moving in/out of If statement:
s1 = “If (e) then {S1t } else {S1f }”, s2 = “If (e) then {S2t }
else {S2f }” such that none of the above cases hold and all
of the following hold:
′
• ∀y ∈ Use(e), S1′ ≡S
y S2 ;
′
t
• S1′ ; S1t ≡S
x S2 ; S2 ;
f
′
• S1′ ; S1f ≡S
x S2 ; S2 ;
• x ∈ Def(s1 ) ∩ Def(s2 );
We show all of the following hold.
i. Imp(S1′ , Use(e)) = Imp(S2′ , Use(e)).
By the hypothesis IH and the assumption that ∀y ∈
′
Use(e), S1′ ≡S
y S2 . Then, by Lemma C.2,
Imp(S1′ , Use(e)) = Imp(S2′ , Use(e)).
18

ii. Imp(S1′ , Imp(S1t , {x})) = Imp(S2′ , Imp(S2t , {x})).
Because size(“If(e) then {St } else {Sf }”) = 1 +
size(St ) + size(Sf ), then
size(S1′ ; S1t ) + size(S2′ ; S2t ) < k. By the hypothesis IH,
Imp(S1′ ; S1t , {x}) = Imp(S2′ ; S2t , {x}).
Besides, by Lemma C.1,
Imp(S1′ , Imp(S1t , {x})) = Imp(S1′ ; S1t , {x})
= Imp(S2′ ; S2t , {x}) = Imp(S2′ , Imp(S2t , {x})).
iii. Imp(S1′ , Imp(S1f , {x})) = Imp(S2′ , Imp(S2f , {x})).
By similar argument in the case that
Imp(S1′ , Imp(S1t , {x})) = Imp(S2′ , Imp(S2t , {x})).
Then, by combining things together,
Imp(S1 , {x})
= Imp(S1′ , Imp(s1 , {x}))
by the definition of imported variables
= Imp(S1′ , Imp(S1t , {x}) ∪ Imp(S1f , {x}) ∪ Use(e))
by the definition of imported variables
= Imp(S1′ , Imp(S1t , {x})) ∪ Imp(S1′ , Imp(S1f , {x}))
∪Imp(S1′ , Use(e)) by Lemma C.2
= Imp(S2′ , Imp(S2t , {x})) ∪ Imp(S2′ , Imp(S2f , {x}))
∪Imp(S2′ , Use(e)) by i, ii, iii
= Imp(S2′ , Imp(S2t , {x}) ∪ Imp(S2f , {x}) ∪ Use(e))
by Lemma C.2
= Imp(S2′ , Imp(S2t , {x}) ∪ Imp(S2f , {x}) ∪ Use(e))
by the definition of imported variables
= Imp(S2′ , Imp(s2 , {x}))
by the definition of imported variables
= Imp(S2 , {x}).
Hence, the lemma holds.

5.3 Termination in the same way
We proceed to propose a proof rule under which two statement
sequences either both terminate or both do not terminate. We start
by giving the definition of termination in the same way. Then
we present the proof rule of termination in the same way. Our
proof rule of termination in the same way allows updates such
as statement duplication or reordering, loop fission or fusion and
additional terminating statements. We prove that the proof rule
ensures terminating in the same way by induction on the program
size of the two programs in the proof rule. We also list auxiliary
lemmas required by the proof of termination in the same way.
Definition 14. (Termination in the same way) Two statement
sequences S1 and S2 terminate in the same way when started in
states m1 and m2 respectively, written (S1 , m1 ) ≡H (S2 , m2 ), iff
one of the following holds:
∗

∗

1. (S1 , m1 ) → (skip, m′1 ) and (S2 , m2 ) → (skip, m′2 );
i

(S1i , mi1 )

2. ∀i ≥ 0, (S1 , m1 ) →
where S1i 6= skip, S2i 6= skip.

i

and (S2 , m2 ) → (S2i , mi2 )

5.3.1 Proof rule for termination in the same way
We define proof rules under which two statement sequences S1 and
S2 terminate in the same way. We summarize the cause of nonterminating execution and then give the proof rule.
We consider two causes of nonterminating executions: crash
and infinite iterations of loop statements. As to crash [? ], we consider four common causes based on our language: expression evaluation exceptions, the lack of input value, input/assignment value
type mismatch and array index out of bound. In essence, the causes
of nontermination are partly due to the values of some particular
variables during executions. We capture variables affecting each

2015/9/14

i. S1t , S1f , S2t , S2f are all sequences of “skip”;
ii. At least one of S1t , S1f , S2t , S2f is not a sequence of
f
f
t
S
“skip” such that: (S1t ≡S
H S2 ) ∧ (S1 ≡H S2 );
(c) S1 = “whilehn1 i (e){S1′′ }”, S2 = “whilehn2 i (e){S2′′ }” and
both of the following hold:

source of nontermination; loop deciding variables LVar(S) are variables affecting the evaluation of a loop statements in the statement sequence S, crash deciding variables CVar(S) are variables
whose values decide if a crash occurs in S. We list the definitions
of LVar(S) and CVar(S) in Definition 15 and 16. Definition 17
summarizes the variables whose values decide if one program terminates.

′′
• S1′′ ≡S
H S2 ;
• S1′′ and S2′′ have equivalent computation of TVar(S1 ) ∪

Definition 15. (Loop deciding variables) The loop deciding variables of a statement sequence S, written LVar(S), are defined as
follows:

TVar(S2 );
2. S1 and S2 are not both one statement and one of the following
holds:
(a) S1 = S1′ ; s1 and S2 = S2′ ; s2 and all of the following hold:

1. LVar(S) = ∅ if ∄s = “while(e) {S ′ }” and s ∈ S;
2. LVar(“If (e) then {St } else {Sf }”) = Use(e) ∪ LVar(St ) ∪
LVar(Sf ) if “while(e){S ′ }” ∈ S;
3. LVar(“while(e){S ′ }”) = Imp(S, Use(e) ∪ LVar(S ′ ));
4. For k > 0, LVar(s1 ; ...; sk ; sk+1 ) = LVar(s1 ; ...; sk ) ∪
Imp(s1 ; ...; sk , LVar(sk+1 ));

′
• S1′ ≡S
H S2 ;
• S1′ and S2′ have equivalent computation of TVar(s1 ) ∪

TVar(s2 );

• s1 ≡S
H s2 where s1 and s2 are not “skip”;

Definition 16. (Crash deciding variables) The crash deciding
variables of a statement sequence S, written CVar(S), are defined
as follows:

(b) One last statement is “skip”:

′
S
′
(S1 = S1′ ; “skip”)∧(S
1 ≡H S2 ) ∨ (S2 = S2 ; “skip”)∧

′
(S1 ≡S
S
)
.
2
H
(c) One last statement is a “duplicate” statement and one of the
following holds:
i. S1 = S1′ ; s′1 ; S1′′ ; s1 and all of the following hold:
• S1′ ; s′1 ; S1′′ ≡S
H S2 ;
• (s′1 ≡S
H s1 ) ∧ (s1 6= “skip”);
• Def(s′1 ; S1′′ ) ∩ TVar(s1 ) = ∅;
ii. S2 = S2′ ; s′2 ; S2′′ ; s2 and all of the following hold:
′
′
′′
• S1 ≡ S
H S2 ; s 2 ; S2 ;
′
S
• (s2 ≡H s2 ) ∧ (s2 6= “skip”);
• Def(s′2 ; S2′′ ) ∩ TVar(s2 ) = ∅;
(d) S1 = S1′ ; s1 ; s′1 and S2 = S2′ ; s2 ; s′2 where s1 and s2 are
reordered and all of the following hold:

1. CVar(skip) = ∅;
2. CVar(lval := e) = Idx(lval) ∪ Use(e) if (Γ ⊢ lval :
Int) ∧ (Γ ⊢ e : Long);
3. CVar(lval := e) = Idx(lval)∪Err(e) if (Γ ⊢ lval : Int)∧(Γ ⊢
e : Long) does not hold;
4. CVar(input id) = {idI };
5. CVar(output e) = Err(e);
6. CVar(“If (e) then {St } else {Sf }”) = Err(e), if CVar(St )
∪ CVar(Sf ) = ∅;
7. CVar(“If (e) then {St } else {Sf }”) = Use(e) ∪ CVar(St )
∪ CVar(Sf ), if CVar(St ) ∪ CVar(Sf ) 6= ∅;
8. CVar(“whilehni (e){S ′ }”) = Imp(“whilehni (e){S ′ }”, Use(e) ∪
CVar(S ′ ));
9. For k > 0, CVar(s1 ; ...; sk+1 ) = CVar(s1 ; ...; sk )
∪ Imp(s1 ; ...; sk , CVar(sk+1 ));

′
• S1′ ≡S
H S2 ;
• S1′ and S2′ have equivalent computation of TVar(s1 ; s′1 )∪

Definition 17. (Termination deciding variables) The termination
deciding variables of statement sequence S are CVar(S)∪LVar(S),
written TVar(S).

•
•
•
•

Definition 18. (Base cases of the proof rule of termination in
the same way) Two simple statements s1 and s2 satisfy the proof
rule of termination in the same way, written s1 ≡S
H s2 , iff one of
the following holds:

TVar(s2 ; s′2 ).
′
s1 ≡S
H s2 ;
′
S
s1 ≡H s2 ;
Def(s1 ) ∩ TVar(s′1 ) = ∅;
Def(s2 ) ∩ TVar(s′2 ) = ∅;

5.3.2 Soundness of the proof rule for termination in the same
way

1. s1 and s2 are same, s1 = s2 ;
2. s1 and s2 are input statement with same type variable: s1 =
“input id1 ”, s2 = “input id2 ” where (Γs1 ⊢ id1 : τ ) ∧ (Γs2 ⊢
id2 : τ );
3. s1 = “output e” or “id1 := e”, s2 = “output e” or “id2 := e”
where both of the following hold:
• There is no possible value mismatch in “id1 := e”,
¬(Γs1 ⊢ id1 : Int) ∨ ¬(Γs1 ⊢ e : Long) ∨ (Γs1 ⊢ e : Int).
• There is no possible value mismatch in “id2 := e”,
¬(Γs2 ⊢ id2 : Int) ∨ ¬(Γs2 ⊢ e : Long) ∨ (Γs2 ⊢ e : Int).

We show that two statement sequences satisfy the proof rules of
termination in the same way, and their initial states agree on the
values of their termination deciding variables, then they either both
terminate or both do not terminate.
Theorem 3. If two simple statements s1 and s2 satisfy the proof
rule of termination in the same way, s1 ≡sH s2 , and their initial
states m1 (f1 , σ1 ) and m2 (f2 , σ2 ) with crash flags not set, f1 =
f2 = 0, and whose value stores agree on values of the termination
deciding variables of s1 and s2 , ∀x ∈ TVar(s1 ) ∪ TVar(s2 ) :
σ1 (x) = σ2 (x), when executions of s1 and s2 start in states m1
and m2 respectively, then s1 and s2 terminate in the same way
when started in states m1 and m2 respectively: (s1 , m1 ) ≡H
(s2 , m2 ).

Definition 19. (proof rule of termination in the same way) Two
statement sequences S1 and S2 satisfy the proof rule of termination
in the same way, written S1 ≡S
H S2 , iff one of the following holds:

1. S1 and S2 are both one statement and one of the following
holds.
(a) S1 and S2 are simple statements: s1 ≡S
H s2 ;
(b) S1 = “If(e) then {S1t } else {S1f }”, S2 = “If(e) then {S2t }
else {S2f }” and one of the following holds:

Proof. The proof is a case analysis of those cases in the definition
of s1 ≡sH s2 . Because s1 is a simple statement and s1 ’s execution
is without function call, we only care the crash variables of s1 in
the termination deciding variables of s1 , CVar(s1 ). Similarly, we
only care CVar(s2 ).

19

2015/9/14

√

First s1 and s2 are same: s1 = s2 ;
We show the theorem by induction on abstract syntax of s1 and
s2 .
1. s1 = s2 = skip.
By definition of termination in the same way, both s1 and
s2 terminate. The theorem holds.
2. s1 = s2 = “lval := e”.
There are further cases regarding what lval is.
(a) lval = id.
By definition, CVar(s1 ) = CVar(s2 ) = Err(e) or
Use(e) based on if there is possible value mismatch
(e.g., assigning value defined only in type Long to a
variable of type Int). There are two subcases.
• Left value id is of type Int and expression e is of
type Long but not type Int, (Γ ⊢ id : Int) ∧ (Γ ⊢ e :
Long) ∧ ¬(Γ ⊢ e : Int).
By definition, CVar(s1 ) = CVar(s2 ) = Use(e).
By assumption, ∀x ∈ Use(e), σ1 (x) = σ2 (x). By
Lemma D.1, the expression evaluates to the same
value w.r.t two pairs of value stores σ1 and σ2 respectively,
Both evaluations of expression lead to crash,
E JeKσ1 = E JeKσ2 = (error, vof ).
Then the execution of s1 is as follows:
(s1 , m1 )
= (id := e, m1 (σ1 ))
→(id := (error, ∗), m1 (σ1 )) by rule EEval’
→(id := 0, m1 (1/f)) by rule ECrash.
i
→(id := 0, m1 (1/f)) for any i > 0 by rule Crash.
Similarly, s2 does not terminate. The theorem
holds.
Both evaluations of expression lead to no crash,
E JeKσ1 = E JeKσ2 = (v, vof ).
Then there are cases regarding if value mismatch
occurs.
√
The value v is only defined in type Long, (Γ ⊢
v : Long) ∧ ¬(Γ ⊢ v : Int).
The execution of s1 is as follows:
(s1 , m1 )
= (id := e, m1 (σ1 ))
→(id := (v, vof ), m1 (σ1 )) by rule EEval
→(id := v, m1 (σ1 )) by rule EOflow-1 or EOflow-2.
→(id := v, m1 (1/f)) by rule Assign-Err.
i
→(id := v, m1 (1/f)) for any i > 0 by rule Crash.
Similarly, s2 does not terminate. The theorem
holds.
√
The value v is defined in type Int, Γ ⊢ v : Int.
Assuming that the variable id is a global one, the
execution of s1 is as follows:
(s1 , m1 )
= (id := e, m1 (σ1 ))
→(id := (v, vof ), m1 (σ1 )) by rule EEval
→(id := v, m1 (σ1 )) by rule EOflow-1 or EOflow-2.
→(skip, m1 (σ1 (σ1 [v/id]))) by rule Assign.
Similarly, s2 terminate. The theorem holds.
When the variable id is a local variable, by similar
argument for the global variable, we can show that
s1 and s2 terminate. Then the theorem holds.
• It is not the case that left value id is of type Int and
the expression e is of type Long only,

¬ (Γ ⊢ id : Int)∧(Γ ⊢ e : Long)∧¬(Γ ⊢ e : Int) .
There are two cases based on if there is crash in
evaluation of expression e.

Both evaluations of expression lead to crash,
E JeKσ1 = E JeKσ2 = (error, vof ).
By the same argument in case where left value id is
of type Int and the expression e is of type Long only,
this
√ theorem holds.
Both evaluations of expression lead to no crash,
E JeKσ1 = E JeKσ2 = (v, vof ).
By the same argument in subcase of no value mismatch in case where left value id is of type Int and
the expression e is of type Long only, this theorem
holds.
(b) lval = id[n].
There are two subcases based on if n is within the array
bound of id. By our assumption, array variable id is of
the same bound in two programs. W.l.o.g., we assume
id is local variable.
i. n is out of bound of array variable id, ((id, n) 7→
v1 ) ∈
/ σ1 and ((id, n) 7→ v2 ) ∈
/ σ2 ;
Then the execution of s1 continues as follows:
(s1 , m1 )
= (id[n] := e, m1 (σ1 ))
→(id[n] := e, m1 (1/f) by rule Arr-3
i
→(id[n] := e, m1 (1/f)) by rule Crash.
Similarly, s2 does not terminate. The theorem holds.
ii. n is within the bound of array variable id, ((id, n) 7→
v1 ) ∈ σ1 and ((id, n) 7→ v2 ) ∈ σ2 ;
There are cases of CVar(s1 ) and CVar(s2 ) based on
if there is possible value mismatch exception in s1
and s2 .
• Left value id[n] is of type Int and expression e is of
type Long but not type Int, (Γ ⊢ id[n] : Int) ∧ (Γ ⊢
e : Long) ∧ ¬(Γ ⊢ e : Int).
By definition, CVar(s1 ) = CVar(s2 ) = Use(e).
By assumption, ∀x ∈ Use(e), σ1 (x) = σ2 (x). By
Lemma D.1, the expression evaluates to the same
value w.r.t two value stores σ1 and σ2 respectively,
Both evaluations of expression lead to crash,
E JeKσ1 = E JeKσ2 = (error, vof ).
Then the execution of s1 is as follows:
(s1 , m1 )
= (id[n] := e, m1 (σ1 ))
→(id[n] := (error, ∗), m1 (σ1 )) by rule EEval’
→(id[n] := 0, m1 (1/f)) by rule ECrash.
i
→(id[n] := 0, m1 (1/f)) for any i > 0
by rule Crash.
Similarly, s2 does not terminate. The theorem
holds.
Both evaluations of expression lead to no crash,
E JeKσ1 = E JeKσ2 = (v, vof ).
Then there are cases regarding if value mismatch
occurs.
√
The value v is only defined in type Long, (Γ ⊢
v : Long) ∧ ¬(Γ ⊢ v : Int).
The execution of s1 is as follows:
(s1 , m1 )
= (id[n] := e, m1 (σ1 ))
→(id[n] := (v, vof ), m1 (σ1 )) by rule EEval’
→(id[n] := v, m1 (σ1 ))
by rule EOflow-1 or EOflow-2.
→(id[n] := v, m1 (1/f)) by rule Assign-Err.
i
→(id[n] := v, m1 (1/f)) for any i > 0
by rule Crash.

20

2015/9/14

When the variable id is a global variable, by similar
argument, the theorem holds.
ii. id is of type Int or enumeration, Γ ⊢ id : Int or
enum id′ ;
There are cases regarding if the head of input sequence can be transformed to type of id. Let vio =
hd(σ1 (idI )).
• id is of type Int.
If vio is not of type Int, Γ ⊢ vio : Long and ¬(Γ ⊢
vio : Int), then the execution of s1 continues as
follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(input id, m1 (1/f)) by Rule In-4.
i
→(input id, m1 (1/f)) by Rule crash.
Similarly, s2 does not terminate. The theorem
holds.
If vio is of type Int, Γ ⊢ vio : Long and Γ ⊢
vio : Int, assuming id is a local variable, then the
execution of s1 continues as follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(skip, m1 (σ1 [vio /id, tl(σ1 (idI ))/idI ,
“σ1 (idIO ) · v io ”/idIO ])) by Rule In-8.
Similarly, s2 terminates. The theorem holds.
When id is a global variable, by similar argument,
the theorem holds.
• If id is of type enum id′ = {l1 , ..., lk }.
If (vio < 1) ∨ (vio > k), then the execution of s1
continues as follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(input id, m1 (1/f)) by Rule In-6.
i
→(input id, m1 (1/f)) by Rule crash.
Similarly, s2 does not terminate. The theorem
holds. When id is a global variable, by similar argument, the theorem holds.
If 1 ≤ vio ≤ k, assuming id is a local variable,
then the execution of s1 continues as follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(skip, m1 (σ1 [lvio /id, tl(σ1 (idI ))/idI ,
“σ1 (idIO ) · v io ”/idIO ])) by Rule In-5.
Similarly, s2 terminates. The theorem holds. When
id is a global variable, by similar argument, the
theorem holds.
(c) s1 = s2 = “output e”;
There are two cases based on if evaluation of expression e crashes. By definition, CVar(s1 ) = CVar(s2 ) =
Err(e). By assumption, ∀x ∈ Err(e), σ1 (x) = σ2 (x).
By Lemma D.2, evaluation of the expression e w.r.t two
value stores σ1 and σ2 either both crash or both do not
crash.
i. There is crash in evaluation of the expression e w.r.t
1
two value stores σ1 and σ2 , E JeKσ1 = (error, vof
)
2
and E JeKσ2 = (error, vof ).
The execution of s1 continues as follows:
(s1 , m1 )
= (output e, m1 (σ1 ))
1
→(output (error, vof
), m1 (1/f)) by Rule EEval’
→(output 0, m1 (1/f)) by Rule ECrash.
i
→(output 0, m1 (1/f)) by Rule crash.

Similarly, s2 does not terminate. The theorem
holds.
√
The value v is defined in type Int, Γ ⊢ v : Int.
The execution of s1 is as follows:
(s1 , m1 )
= (id[n] := e, m1 (σ1 ))
→(id[n] := (v, vof ), m1 (σ1 )) by rule EEval’
→(id[n] := v, m1 (σ1 ))
by rule EOflow-1 or EOflow-2.
→(skip, m1 (σ1 (σ1 [v/(id, n)])))
by rule Assign-A.
Similarly, s2 terminate. The theorem holds.
When the variable id is a global variable, by similar argument for the global variable, we can show
that s1 and s2 terminate. Then the theorem holds.
• It is not the case that left value id is of type Int and
the expression e is of type Long only,
¬ (Γ
 ⊢ id : Int) ∧ (Γ ⊢ e : Long) ∧ ¬(Γ ⊢ e :
Int) .
There are two cases based on if there is crash in
evaluation of expression e.
Both evaluations of expression lead to crash,
E JeKσ1 = E JeKσ2 = (error, vof ).
By the same argument in case where left value id
is of type Int and the expression e is of type Long
only, this theorem holds.
Both evaluations of expression lead to no crash,
E JeKσ1 = E JeKσ2 = (v, vof ).
By the same argument in subcase of no value
mismatch in case where left value id is of type
Int and the expression e is of type Long only, this
theorem holds.
If array variable id is a global variable, by similar argument above, the theorem holds.
(c) lval = id1 [id2 ].
By definition, Idx(s1 ) = Idx(s2 ) = {id2 } ⊆ CVar(s1 ) =
CVar(s2 ). By assumption, σ1 (id2 ) = σ2 (id2 ) = n. By
the same argument in the case where lval = id[n], the
theorem holds.
3. s1 = s2 = “input id”,
By definition, CVar(s1 ) = CVar(s2 ) = {idI }. By assumption σ1 (idI ) = σ2 (idI ). There are cases regarding if input
sequence is empty or not.
(a) There is empty input sequence, σ1 (idI ) = σ2 (idI ) =
∅.
Then the execution of s1 continues as follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(input id, m1 (1/f)) by rule In-7
i
→(input id, m1 (1/f)) by rule Crash.
Similarly, s2 does not terminate. The theorem holds.
(b) There is nonempty input sequence, σ1 (idI ) = σ2 (idI ) 6=
∅.
There are cases regarding if type of the variable id is
Long or not.
i. id is of type Long, Γ ⊢ id : Long;
Assuming id is a local variable, then the execution
of s1 continues as follows:
(s1 , m1 )
= (input id, m1 (σ1 ))
→(skip, m1 (σ1 [vio /id, tl(σ1 (idI ))/idI ,
“σ1 (idIO ) · v io ”/idIO ])) by rule In-3.
Similarly, s2 terminates. The theorem holds.

21

2015/9/14

Similarly, s2 does not terminate. The theorem holds.
ii. There is no crash in evaluation of the expression e
1
w.r.t two value stores σ1 and σ2 , E JeKσ1 = (v1 , vof
)
2
and E JeKσ2 = (v2 , vof ).
According to rule Out-1 and Out-2, there is no exception in transformation of different typed output
value. We therefore only show the execution for output value of Int type. The execution of s1 continues
as follows:
(s1 , m1 )
= (output e, m1 (σ1 ))
1
→(output (v1 , vof
), m1 (1/f)) by Rule EEval’
1
→(output v1 , m1 (vof
/of))
by Rule EOflow-1 or EOflow-2.
→(skip, m1 (σ1 [“σ(idIO ) · v 1 ”/idIO ]))
by Rule Out-1.
Similarly, s2 terminates. Theorem holds.
Second s1 and s2 are input statement with same type variable:
s1 = “input id1 ”, s2 = “input id2 ” where (Γs1 ⊢ id1 :
t) ∧ (Γs2 ⊢ id2 : t);
The theorem holds by similar argument for the case s1 = s2 =
input id.
Third s1 = “output e” or “id1 := e”, s2 = “output e” or “id2 :=
e” where both of the following hold:
• There is no possible value mismatch in “id1 := e”,
¬(Γs1 ⊢ id1 : Int) ∨ ¬(Γs1 ⊢ e : Long) ∨ (Γs1 ⊢ e : Int).
• There is no possible value mismatch in “id2 := e”,
¬(Γs2 ⊢ id2 : Int) ∨ ¬(Γs2 ⊢ e : Long) ∨ (Γs2 ⊢ e : Int).
We show that the evaluations of the expression e w.r.t the value
stores σ1 and σ2 either both raise an exception or both do not.
By the definition of crash variables, the crash variables of s1
are those obtained by the function Err(e), CVar(s1 ) = Err(e).
Similarly, the termination deciding variables of s2 are Err(e).
By assumption, the initial value stores σ1 and σ2 agree on
values of those in CVar(s1 ) and CVar(s2 ), ∀x ∈ Err(e) =
(CVar(s1 ) ∪ CVar(s2 )) : σ1 (x) = σ2 (x). By Lemma D.2, the
evaluations of expression e w.r.t two value stores, σ1 and σ2 ,
either both raise an exception or both do not raise an exception.
1. The evaluations of the expression e raise an exception w.r.t
1
two value stores σ1 and σ2 , E ′ JeKσ1 = (error, vof
), E ′ JeKσ2 =
2
(error, vof ):
We show the execution of s1 proceeds to an configuration
where the crash flag is set and then does not terminate.
When s1 = “output e”, the execution of “output e” proceeds as follows.
(output e, m1 (σ1 ))
1
→(output (error, vof
), m1 (σ1 )) by rule EEval’
→(output 0, m1 (1/f)) by rule ECrash
i
→(output 0, m1 (1/f)) for any i ≥ 0, by rule Crash.
When s1 = “id1 := e”, the execution of “id1 := e”
proceeds as follows.
(id1 := e, m1 (σ1 ))
1
), m1 (σ1 )) by rule EEval’
→(id1 := (error, vof
→(id1 := 0, m1 (1/f)) by rule ECrash
i
→(id1 := 0, m1 (1/f)) for any i ≥ 0, by rule Crash.
Similarly, the execution of s2 proceeds to a configuration
where the crash flag is set. Then, by the crash rule, the
execution of s2 does not terminate. The theorem 3 holds.
2. the evaluations of expression e do not raise an exception
1
), E ′ JeKσ2 =
w.r.t two value stores, σ1 and σ2 , E ′ JeKσ1 = (v1 , vof
2
(v2 , vof ):
We show the execution of s1 terminates.

22

When s1 = output (e), the execution of output (e) proceeds
as follows. W.l.o.g, we assume expression e is of type Int.
This is allowed by the condition that it does not hold that
(Γs1 ⊢ e : Long) ∧ ¬(Γs1 ⊢ e : Int).
(output e, m1 (σ1 ))
1
→(output (v1 , vof
), m1 (σ1 )) by rule EEval’
1
→(output v1 , m1 (vof
/of, σ1 ))
by rule E-Oflow1 or E-Oflow2
→(skip, m1 (σ1 [“σ1 (idIO ) · v¯1 ”/idIO ])) by rule Out.
When s1 = “id1 := e”, by assumption, the expression e is
of type Int, there is no possible value mismatch in execution
of “id1 := e” because the only possible value mismatch
occurs when assigning a value of type Long but not Int to
a variable of type Int. By the condition ¬(Γs1 ⊢ id1 :
Int)∨¬(Γs1 ⊢ e : Long)∨(Γs2 ⊢ e : Int), when expression
e is of type Long, then the variable id1 is not of type Int. In
summary, there is no value mismatch.
The execution of “id1 := e” proceeds as follows.
(id1 := e, m1 (σ1 ))
1
→(id1 := (v1 , vof
), m1 (σ1 )) by rule EEval’
1
→(id1 := v1 , m1 (vof
/of, σ1 )) by rule EEval’
→(skip, m1 (σ1 [v1 /id1 ])) by the rule Assign.
When id1 is a variable of enumeration or Long type, by
similar argument, the theorem still holds.
Similarly, the execution of s2 terminates when started in the
state m2 (σ2 ). Theorem 3 holds.

Theorem 4. If two statement sequences S1 and S2 satisfy the
proof rule of termination in the same way, S1 ≡S
H S2 , and their
respective initial states m1 (f1 , σ1 ) and m2 (f2 , σ2 ) with crash flags
not set, f1 = f2 = 0, and whose value stores agree on values of the
termination deciding variables of S1 and S2 , ∀x ∈ TVar(S1 ) ∪
TVar(S2 ) : σ1 (x) = σ2 (x), then S1 and S2 terminate in the same
way when started in states m1 and m2 respectively: (S1 , m1 ) ≡H
(S2 , m2 ).
Proof. The proof is by induction on size(S1 ) + size(S2 ), the sum
of program size of S1 and S2 .
Base case. S1 and S2 are simple statement. By Theorem 3, Theorem 4 holds.
Induction step.
There are two hypotheses. The hypothesis IH is that Theorem 4
holds when size(S1 ) + size(S2 ) = k ≥ 2.
We show Theorem 4 holds when size(S1 ) + size(S2 ) = k + 1.
The proof of Theorem 4 is a case analysis according to the cases
in the definition of the proof rule of termination in the same way,
S1 ≡ S
H S2 .
1. S1 and S2 are one statement and one of the following holds.
(a) S1 = “If(e) then {S1t } else {S1f }”, S2 = “If(e) then {S2t }
else {S2f }” such that one of the following holds:

i. S1t , S1f , S2t , S2f are all sequences of “skip”;
We show that the evaluation of expression e w.r.t the
value store σ1 and σ2 either both raise an exception or
both do not. By the definition of crash/loop variables,
CVar(S1t ) = CVar(S1f ) = ∅, LVar(S1 ) = ∅. By the
definition of termination deciding variables, the termination deciding variables of S1 is the crash variables
of S1 , TVar(S1 ) = CVar(S1 ) = Err(e). By assumption, the value stores σ1 and σ2 agree on the values
of those in the crash variables of S1 and S2 , ∀x ∈
Err(e) = TVar(S1 ) = TVar(S2 ), σ1 (x) = σ2 (x). By

2015/9/14

the property of the expression meaning function E , the
evaluation of predicate expression e of S1 and S2 w.r.t
value store σ1 and σ2 either both crash or both do not
crash, (E JeKσ1 = E JeKσ2 = error) ∨ (E JeKσ1 6=
error) ∧ (E JeKσ2 6= error) . Then we show that Theorem 4 holds in either of the two possibilities.
A. E JeKσ1 = E JeKσ2 = error.
The execution of S1 proceeds as follows:
(If(e) then {S1t } else {S1f }, m1 (σ1 ))
→(If(error) then {S1t } else {S1f }, m1 (σ1 )) by rule EEval
→(If(0) then {S1t } else {S1f }, m1 (1/f, σ1 )) by rule ECrash
i
→(If(0) then {S1t } else {S1f }, m1 (1/f, σ1 )) for any i ≥ 0,
by rule Crash.
Similarly, the execution of S2 started in the state
m2 (σ2 ) does not terminate. The theorem 4 holds.
B. (E JeKσ1 6= error) ∧ (E JeKσ2 6= error).
W.l.o.g, E JeKσ1 = v1 6= 0, E JeKσ2 = 0. Then the
execution of S1 proceeds as follows.
(If(e) then {S1t } else {S1f }, m1 (σ1 ))
→(If(v1 ) then {S1t } else {S1f }, m1 (σ1 )) by rule EEval
→(S1t , m1 (σ1 )) by rule If-T
∗
→(skip, m′1 ) by rule Skip.
Similarly, the execution of S2 started in the state
m2 (σ2 ) terminates. The theorem 4 holds.

i

→(If(0) then {S1t } else {S1f }, m1 (1/f, σ1 )) for any i ≥ 0,
by rule Crash.
Similarly, the execution of S2 started from state
m2 (σ2 ) does not terminate. The theorem 4 holds.
B. E JeKσ1 = E JeKσ2 = v 6= error, w.l.o.g., v = 0.
Then the execution of S1 proceeds as follows:
(If(e) then {S1t } else {S1f }, m1 (σ1 ))
→(If(0) then {S1t } else {S1f }, m1 (σ1 )) by rule EEval
→(S1f , m1 (σ1 )) by rule If-F.
Similarly, after two steps of execution, S2 gets to the
configuration (S2f , m2 (σ2 )).

We show that S1f and S2f terminate in the same way
when started in the state m1 (σ1 ) and m2 (σ2 ) ref
spectively. Because S1f ≡S
H S2 , by Corollary 5.1,
the termination deciding variables of S1f and S2f
are same, TVar(S1f ) = TVar(S2f ). By the definition of crash/loop variables, CVar(S1f ) ⊆ CVar(S1 )
and LVar(S1f ) ⊆ LVar(S1 ). Hence, the termination deciding variables of S1f are a subset of the
termination deciding variables of S1 , TVar(S1f ) ⊆
TVar(S1 ). Similarly, TVar(S2f ) ⊆ TVar(S2 ). Therefore, the value store σ1 and σ2 agree on the values of those in the termination deciding variables
of S1f and S2f , ∀y ∈ TVar(S1f ) ∪ TVar(S2f ) :
ii. At least one of S1t , S1f , S2t , S2f is not a sequence of
σ
f
f
1 (y) = σ2 (y). In addition, the sum of program size
t
S
“skip” and (S1t ≡S
H S2 ) ∧ (S1 ≡H S2 );
of
S1f and S2f is less than k because program size
t
W.l.o.g., S1 is not of “skip” only. We show that the
of each of S1t and S2t is great than or equal to one,
evaluation of the expression e w.r.t the value stores σ1
f
f
size(S
1 ) + size(S2 ) < k. As is shown, crash flags
and σ2 either both raise an exception or both produce
are not set. Therefore, by the hypothesis IH, S1f and
the same integer value. Then there is either some loop
t
t
S2f terminate in the same way when started in state
statement in S1 or the crash variables of S1 are not ∅ or
both.
m1 (f1 , σ1 ) and m2 (f2 , σ2 ), (S1f , m1 (f1 , σ1 )) ≡H
A. When there is some loop statement in S1t , then, by
(S2f , m2 (f2 , σ2 )). Hence, Theorem 4 holds.
the definition of loop variables, the loop variables
(b) S1 = “whilehn1 i (e) {S1′′ }”, S2 = “whilehn2 i (e) {S2′′ }”
of S1 include all variables used in the predicate exsuch that both of the following hold:
pression of S1 , LVar(S1 ) = Use(e) ∪ LVar(S1t ) ∪
′′
• S1′′ ≡S
H S2 ;
LVar(S1f ).
′′
′′
• S1 and S2 have equivalent computation of TVar(S1 ) ∪
B. When the crash variables of S1t are not ∅, then, by
TVar(S2 );
the definition of crash variables, the crash variables
By Corollary 5.3, we show S1 and S2 terminate in the
of S1 include all variables used in the predicate exsame way when started from state m1 (f1 , m1c , σ1 ) and
pression of S1 , CVar(S1 ) = Use(e) ∪ CVar(S1t ) ∪
2
m
2 (f2 , mc , σ2 ) respectively. We need to show that all reCVar(S1f ).
quired conditions are satisfied.
In summary, all variables used in predicate expression
• The crash flags are not set, f1 = f2 = 0.
of S1 is a subset of termination deciding variables of
• The loop counter value of S1 and S2 are zero: m1c (n1 ) =
S1 , Use(e) ⊆ TVar(S1 ). By assumption, the value
m2c (n2 ) = 0.
store σ1 and σ2 agree on the values of those in the
• The value stores σ1 and σ2 agree on the values of those
termination deciding variables of S1 and S2 . It follows,
in the termination deciding variables of S1 and S2 , ∀x ∈
by the property of expression meaning function E , the
TVar(S1 ) ∩ TVar(S2 ) : σ1 (x) = σ2 (x).
evaluation of the predicate expression e of S1 and S2
The three above conditions are from assumption.
produce the same value w.r.t the value store σ1 and σ2 ,
• S1 and S2 have same set of termination deciding variE JeKσ1 = E JeKσ2 . Then either the evaluations of the
ables, TVar(S1 ) = TVar(S2 ).
predicate expression e of S1 and S2 both crash w.r.t
By Corollary 5.1.
the value store σ1 and σ2 , or both evaluations produce
• The loop body S1′′ of S1 and S2′′ of S2 terminate in
the same integer value, (E JeKσ1 = E JeKσ2 = error) ∨
the same way when started in state mS1 (fS1 , σS1 ) and
(E JeKσ1 = E JeKσ2 = v 6= error). We show Theorem 4
mS2 (fS2 , σS2 ) with crash flags not set and in which
holds in either of the two possibilities.
value
stores agree on the values of those in the terA. E JeKσ1 = E JeKσ2 = error.
mination deciding variables of S1′′ and S2′′ : ((∀x ∈
The execution of S1 proceeds as follows:
TVar(S1′′ ) ∪ TVar(S2′′ ) : σS1 (x) = σS2 (x)) ∧ (fS1 =
(If(e) then {S1t } else {S1f }, m1 (σ1 ))
fS2 = 0)) ⇒ (S1′′ , mS1 (fS1 , σS1 )) ≡H (S2′′ , mS2 (fS2 , σS2 )).
f
t
→(If(error) then {S1 } else {S1 }, m1 (σ1 )) by rule EEval
By the definition of program size, size(S1 ) = size(S1′′ )+
→(If(0) then {S1t } else {S1f }, m1 (1/f, σ1 )) by rule ECrash
1, size(S2 ) = size(S2′′ )+1. Then, size(S1′′ )+size(S2′′ ) <
23

2015/9/14

k. Then, by the hypothesis IH, the loop body S1′′ of S1
and S2′′ of S2 terminate in the same way when started in
state mS1 (σS1 ) and mS2 (σS2 ) with crash flags not set
and whose value stores agree on values of the termination deciding variables of S1′′ and S2′′ .
Then, by Corollary 5.3, S1 and S2 terminate in the same
way when started in the states m1 (m1c , σ1 ) and m2 (m2c , σ2 )
respectively. The theorem 4 holds.
2. S1 and S2 are not both one statement and one of the following
holds:
(a) S1 = S1′ ; s1 , S2 = S2′ ; s2 and all of the following hold:
′
• S1′ ≡S
H S2 ;
′
′
• S1 and S2 have equivalent computation of TVar(s1 ) ∪
TVar(s2 );
• s1 ≡S
H s2 where s1 and s2 are not sequences of “skip”;
By the hypothesis IH, we show that S1′ and S2′ terminate in
the same way when started in the states m1 (f1 , σ1 ), m2 (f2 , σ2 )
respectively, (S1′ , m1 (f1 , σ1 )) ≡H (S2′ , m2 (f2 , σ2 )). We
need to show all required conditions are satisfied.
• Crash flags are not set, f1 = f2 = 0;
By assumption.
• size(S1′ ) + size(S2′ ) < k.
By the definition, size(s1 ) ≥ 1, size(s2 ) ≥ 1. Hence
size(S1′ ) + size(S2′ ) < k.
• Value stores σ1 and σ2 agree on values of the termination deciding variables of S1′ and S2′ .
Besides, by the definition of loop/crash variables, LVar(S1′ ) ⊆
LVar(S1 ) and CVar(S1′ ) ⊆ CVar(S1 ). Hence, TVar(S1′ ) ⊆
TVar(S1 ). Similarly, TVar(S2′ ) ⊆ TVar(S2 ). Then,
value stores σ1 and σ2 agree on the values of those
in the termination deciding variables of S1′ and S2′ ,
∀x ∈ TVar(S1′ ) ∪ TVar(S2′ ) : σ1 (x) = σ2 (x).
Then, by the hypothesis IH, S1′ and S2′ terminate in the
same way when started in the states m1 (f1 , σ1 ), m2 (f2 , σ2 )
respectively, (S1′ , m1 (f1 , σ1 )) ≡H (S2′ , m2 (f2 , σ2 )).
If the execution of S1′ and S2′ terminate when started in the
states m1 (f1 , σ1 ) and m2 (f2 , σ2 ) respectively, we show that
s1 and s2 terminate in the same way. We prove that S1′ and
S2′ equivalently compute the termination deciding variables
of s1 and s2 by Theorem 2.
• Crash flags are not set, f1 = f2 = 0;
By definition of terminating execution of S1′ and S2′
when started in states m1 and m2 respectively.
• The executions of S1′ and S2′ terminate when started in
the states m1 (σ1 ) and m2 (σ2 ).
∗
By assumption, (S1′ , m1 (σ1 )) → (skip, m′1 (σ1′ )) and
∗
′
′
′
(S2 , m2 (σ2 )) → (skip, m2 (σ2 )).
• s1 and s2 have same termination deciding variables.
By Corollary 5.1, s1 and s2 have same termination deciding variables, TVar(s1 ) = TVar(s2 ) = TVar(s).
• Value stores σ1 and σ2 agree on the values of variables
in Imp(S1′ , TVar(s)) ∪ Imp(S2′ , TVar(s)).
By the definition of loop/crash variables, Imp(S1′ , LVar(s1 )) ⊆
LVar(S1 ) and Imp(S1′ , CVar(s1 )) ⊆ CVar(S1 ). Hence,
by Lemma C.2, the imported variables in S1′ relative to
the termination deciding variables of s1 is a subset of the
termination deciding variables of S1 , Imp(S1′ , TVar(s))
⊆ TVar(S1 ). Similarly, Imp(S2′ , TVar(s)) ⊆ TVar(S2 ).
Thus, by assumption, the value stores σ1 and σ2 agree
on the values of the variables in Imp(S1′ , TVar(s)) ∪
Imp(S2′ , TVar(s)).
By Theorem 2, ∀x ∈ TVar(s) : σ1′ (x) = σ2′ (x).
24

∗

By Corollary E.1, (S1′ ; s1 , m1 (σ1 )) → (s1 , m′1 (f1 , σ1′ ))
∗
and (S2′ ; s2 , m2 (σ2 )) → (s2 , m′2 (f2 , σ2′ )). Then, by the hypothesis IH, we show that s1 and s2 terminate in the same
way when started in the states m′1 (σ1′ ) and m′2 (σ2′ ). We
show that all required conditions are satisfied. size(s1 ) +
size(s2 ) < k because size(S1′ ) ≥ 1, size(S2′ ) ≥ 1 by the
definition of program size. If s1 , s2 are loop statement, then,
by the assumption of unique loop labels, s1 ∈
/ S1′ , s2 ∈
/
S2′ . Then, by Corollary E.4, the loop counter value of s1
and s2 is not redefined in the execution of S1′ and S2′ respectively. By the hypothesis IH, s1 and s2 terminate in
the same way when started in the states m′1 (f1 , σ1′ ) and
m′2 (f2 , σ2′ ), (s1 , m′1 (f1 , σ1′ )) ≡H (s2 , m′2 (f2 , σ2′ )). The
theorem 4 holds.
(b) One last statement is “skip”: w.l.o.g., (s2 = “skip”) ∧
′
(S1 ≡S
H S2 ).
We show that S1 and S2′ terminate in the same way when
started in the states m1 (σ1 ) and m2 (σ2 ) respectively by
the hypothesis IH. By the definition of crash/loop variables,
CVar(S2′ ) ⊆ CVar(S2 ), LVar(S2′ ) ⊆ LVar(S2 ). Then, by
assumption, ∀x ∈ TVar(S2′ ) ∪ TVar(S1 ) : σ1 (x) =
σ2 (x). Besides, size (s2 ) ≥ 1 by the definition of program
size. Then size (S1 ) + size (S2′ ) ≤ k. By the hypothesis
IH, S1 and S2′ terminate in the same way when started
in the states m1 (f1 , σ1 ), m2 (f2 , σ2 ), (S1 , m1 (f1 , σ1 )) ≡H
(S2′ , m2 (f2 , σ2 )).
When the execution of S1 and S2′ terminate when started in
the states m1 (σ1 ) and m2 (σ2 ) respectively, s2 terminates
after the execution of S2′ by the definition of terminating
execution.
(c) One last statement is a “duplicate” statement such that one
of the following holds:
W.l.o.g., S2 = S2′ ; s′2 ; S2′′ ; s2 and all of the following hold:
′
′
′′
• S1 ≡ S
H S2 ; s 2 ; S2 ;
′
S
• s2 ≡H s2 ;
• Def(s′2 ; S2′′ ) ∩ TVar(s2 ) = ∅;
• s2 6= “skip”;
We show that S1 and S2′ ; s′2 ; S2′′ terminate in the same way
when started in the states m1 (f1 , σ1 ),
m2 (f2 , σ2 ) respectively by the hypothesis IH. The proof is
same as that in case a).
We show that s2 terminates if the execution of S2′ ; s′2 ; S2′′
terminates. We need to prove that s′2 and s2 start in the states
agreeing on the values of variables in TVar(s2 ). By assump∗
tion, S2′ ; s′2 ; S2′′ terminates, (S2′ ; s′2 ; S2′′ , m2 (f2 , σ2 )) →
′
′
′
′
′′
(skip, m2 (f2 , σ2 )). Then, by Corollary E.1, (S2 ; s2 ; S2 ; s2 , m2 (f2 , σ2 ))
∗
→ (s2 , m′2 (f2 , σ2′ )). In addition, the execution of S2′ and s′2
must terminate because the execution of S2′ ; s′2 ; S2′′ terminates,
∗
∗
(S2′ ; s′2 ; S2′′ ; s2 , m2 (f2 , σ2 )) → (s′2 ; S2′′ ; s2 , m′′2 (f2 , σ2′′ )) →
′
′
(s2 , m2 (f2 , σ2 )).
By assumptin, Def(s′2 ; S2′′ ) ∩ TVar(s2 ) = ∅. Then, by
Corollary E.2, the value store σ2′′ and σ2′ agree on values of
the termination deciding variables of s2 , ∀x ∈ TVar(s2 ) :
σ2′′ (x) = σ2′ (x). By Corollary 5.1, TVar(s′2 ) = TVar(s2 ).
Because the execution of s′2 terminates, then the execution
of s2 terminates when started in the state m′2 (f2 , σ2′ ) by the
∗
hypothesis IH, (s2 , m′2 (f2 , σ2′ )) → (skip, m′′2 ).
In addition, we show that there is no input statement in
s2 by contradiction. Suppose there is input statement in
s2 . By Lemma 5.11, idI ∈ CVar(s2 ). Hence, the input
sequence variable is in the termination deciding variables
of s2 , idI ∈ TVar(s2 ). By Corollary 5.1, TVar(s2 ) =

2015/9/14

TVar(s′2 ). Then, there must be one input statement in s′2 .
Imp(S2′ , TVar(s2 ; s′2 )) ⊆ TVar(S2 ).
Otherwise, by Lemma 5.2, the input sequence variable is not
W.l.o.g, we show that Imp(S1′ , TVar(s1 ; s′1 )) ⊆ TVar(S1 ).
in the termination deciding variables of s′2 . A contradiction
Specifically, we show Imp(S1′ , CVar(s1 ; s′1 )) ⊆ CVar(S1 ).
against the result that idI ∈ TVar(s′2 ). Since there is one
CVar(s1 ; s′1 )
input statement in s′2 , by Lemma 5.11, idI ∈ Def(s′2 ).
= CVar(s1 ) ∪ Imp(s1 , CVar(s′1 )) (1)
Thus, by defintion, idI ∈ Def(s′2 ; S2′′ ). Then, Def(s′2 ; S2′′ )∩
by the defintion of CVar(s1 ; s′1 )
TVar(s2 ) 6= ∅. A contradiction. Therefore, there is no input
statement in s2 .
Imp(S1′ , CVar(s1 ; s′1 ))
(d) S1 = S1′ ; s1 ; s′1 ; and S2 = S2′ ; s2 ; s′2 where s1 and s2 are
= Imp(S1′ , CVar(s1 ) ∪ Imp(s1 , CVar(s′1 ))) by (1)
reordered and all of the following hold:
= Imp(S1′ , CVar(s1 )) ∪ Imp(S1′ , Imp(s1 , CVar(s′1 ))) (2)
′
• S1′ ≡S
H S2 ;
by Lemma C.2
• S1′ and S2′ have equivalent computation of TVar(s1 ; s′1 )∪
′
TVar(s2 ; s2 ).
Imp(S1′ , CVar(s1 ))
′
• s1 ≡S
s
;
H 2
⊆ CVar(S1′ ; s1 ) by the defintion of CVar(·)
• s′1 ≡S
⊆ CVar(S1′ ; s1 ; s′1 ) by the defintion of CVar(·)
H s2 ;
• Def(s1 ) ∩ TVar(s′1 ) = ∅;
• Def(s2 ) ∩ TVar(s′2 ) = ∅;
Imp(S1′ , Imp(s1 , CVar(s′1 )))
= Imp(S1′ ; s1 , CVar(s′1 )) by Lemma C.1
The proof is to show that if S1 terminates when started in
⊆
CVar(S1′ ; s1 ; s′1 ) by the defintion of CVar(·).
the state m1 , the S2 terminates when started in the state
m2 , and vice versa. Due to the symmetric conditions, it
∗
is suffice to show one direction that, w.l.o.g., (S1 , m1 ) →
Imp(S1′ , CVar(s1 )) ∪ Imp(S1′ , Imp(s1 , CVar(s′1 )))
∗
′
′
⊆ CVar(S1′ ; s1 ; s′1 ) by (3) and (4).
(skip, m1 ) ⇒ (S2 , m2 ) → (skip, m2 ).
We show that the execution of S2′ terminates by the hypothIn conclusion, Imp(S1′ , CVar(s1 ; s′1 )) ⊆ CVar(S1 ).
esis IH. We need to show that all required conditions are
Similarly, Imp(S1′ , LVar(s1 ; s′1 )) ⊆ LVar(S1 ). Thus,
satisfied.
Imp(S1′ , TVar(s1 ; s′1 )) ⊆ TVar(S1 ). Similarly,
• size(S1′ ) + size(S2′ ) < k.
Imp(S2′ , TVar(s2 ; s′2 )) ⊆ TVar(S2 ).
This is because size(s1 ; s′1 ) > 1, size(s2 ; s′2 ) > 1.
Then, by Theorem 2, after terminating execution of S1′
• Initial value stores σ1 and σ2 agree on values of the
and S2′ , value stores σ1′ and σ2′ agree on values of the
termination deciding variables of S1′ and S2′ , ∀x ∈
termination deciding variables of s1 ; s′1 and s2 ; s′2 ,
TVar(S1′ ) ∪ TVar(S2′ ) : σ1 (x) = σ2 (x).
∀x ∈ TVar(s1 ; s′1 ) ∪ TVar(s2 ; s′2 ) : σ1′ (x) = σ2′ (x).
We show that TVar(S1′ ) ⊆ TVar(S1 ). In the following,
We show that the execution of s2 terminates by the hypothwe prove that CVar(S1′ ) ⊆ CVar(S1 ).
esis IH. By Corollary E.1,
∗
CVar(S1′ )
(S1′ ; s1 ; s′1 , m1 (σ1 )) → (s1 ; s′1 , m′1 (σ1′ )) and (S2′ ; s2 ; s′2 ,
′
′
∗
′
⊆ CVar(S1 ; s1 ) by the defintion of CVar(S1 ; s1 )
m2 (σ2 )) → (s2 ; s2 , m′2 (σ2′ )). By assumption, S1 termi∗
⊆ CVar(S1′ ; s1 ; s′1 ) by the defintion of CVar(S1′ ; s1 ; s′1 )
nates, then s1 terminates, (s1 , m′1 (σ1′ )) → (skip, m′′1 (σ1′′ )).
Similarly, LVar(S1′ ) ⊆ LVar(S1 ). Hence, TVar(S1′ ) ⊆
S
′
Because s1 ≡H s2 , to apply the induction hypothesis, we
TVar(S1 ). Similarly, TVar(S2′ ) ⊆ TVar(S2 ). By asneed to show that all required conditions hold.
sumption, initial value stores σ1 and σ2 agree on values
• size(s2 ) + size(s′1 ) < k.
′
′
of the termination deciding variables of S1 and S2 .
By definition, size(S2′ ) > 1, size(S1′ ) > 1, size(s1 ) >
By the hypothesis IH, (S1′ , m1 (σ1 )) ≡H (S2′ , m2 (σ2 )).
1, size(s′2 ) > 1.
Because the execution of S1 terminates, then S1′ termi• Value stores σ1′′ and σ2′ agree on values of the termina∗
nates when started in the state m1 (σ1 ), (S1′ , m1 (σ1 )) →
tion deciding variables of s′1 and s2 . ∀x ∈ TVar(s′1 ) ∪
(skip, m′1 (σ1′ )). Therefore, S2′ termiantes when started in
TVar(s2 ) : σ1′′ (x) = σ2′ (x).
∗
the state m2 (σ2 ), (S2′ , m2 (σ2 )) → (skip, m′2 (σ2′ )).
By Corollary 5.1, TVar(s′1 ) = TVar(s2 ). Because of the
We show that after the execution of S1′ and S2′ , value stores
condition Def(s1 ) ∩ TVar(s′1 ) = ∅, by Corollary E.2,
agree on values of the termination deciding variables of
value stores σ1′′ and σ1′ agree on values of the termis1 ; s′1 and s2 ; s′2 , ∀x ∈ TVar(s1 ; s′1 )∪TVar(s2 ; s′2 ), σ1′ (x) =
nation deciding variables of s′1 , ∀x ∈ TVar(s′1 ) :
σ2′ (x). We split the argument into two steps.
σ1′′ (x) = σ1′ (x). By the argument above, ∀x ∈ TVar(s2 ) :
i. We show that TVar(s1 ; s′1 ) = TVar(s2 ; s′2 ).
σ1′ (x) = σ2′ (x). Thus, the condition holds.
By Corollary 5.1, TVar(s1 ) = TVar(s′2 ) and TVar(s′1 ) =
By the induction hypothesis IH, (s′1 , m′′1 (σ1′′ )) ≡H (s2 , m′2 (σ2′ )).
TVar(s2 ). Then we show that TVar(s1 ; s′1 ) = TVar(s1 )∪
Because the execution of s′1 terminates, then the exeucTVar(s′1 ). To do that, we show that CVar(s1 ; s′1 ) =
tion of s2 terminates when started in the state m′2 (σ2′ ),
∗
CVar(s1 ) ∪ CVar(s′1 ).
′′
′′
′
′
(s
2 , m2 (σ2 )) → (skip, m2 (σ2 )).
CVar(s1 ; s′1 )
We
show
that
the
execution
of
s′2 terminates. This is by the
= CVar(s1 ) ∪ Imp(s1 , CVar(s′1 )) by the defintion of CVar(s1 ; s′1 ) similar argument that s2 terminates.
= CVar(s1 ) ∪ CVar(s′1 ) by Def(s1 ) ∩ TVar(s′1 ) = ∅ and
In conclusion, S2 terminates when started in the state
the defintion of Imp(·).
m2 (σ2 ). The theorem holds.
Similarly, LVar(s1 ; s′1 ) = LVar(s1 ) ∪ LVar(s′1 ). Thus,
In addition, we show that it is impossible that s1 and s′1 both
TVar(s1 ; s′1 ) = TVar(s1 )∪TVar(s′1 ). Similarly, TVar(s2 ; s′2 ) =
include input statements by contradiction. Suppose there
TVar(s2 ) ∪ TVar(s′2 ). In summary, TVar(s1 ; s′1 ) =
are input statements in both s1 and s′1 . By Lemma 5.11,
TVar(s2 ; s′2 ).
idI ∈ Def(s1 ) ∩ TVar(s′1 ). A contradiction against the
ii. We show that Imp(S1′ , TVar(s1 ; s′1 )) ⊆ TVar(S1 ) and
condition that Def(s1 ) ∩ TVar(s′1 ) = ∅. Similarly, there
are no input statements in both s2 and s′2 .
25

2015/9/14

• There is no possible value mismatch in “id1

:= e”,
¬(ΓS1 ⊢ id1 : Int) ∨ ¬(ΓS1 ⊢ e : Long) ∨ (ΓS1 ⊢ e : Int).
• There is no possible value mismatch in “id2 := e”,
¬(ΓS2 ⊢ id2 : Int) ∨ ¬(ΓS2 ⊢ e : Long) ∨ (ΓS2 ⊢ e : Int).

5.3.3 Supporting lemmas for the soundness proof of
termination in the same way
The supporting lemmas include various properties of TVar(S),
two statement sequences satisfying the proof rule of termination
in the same way consume the same number of input values when
both terminate, and the proof for the case of while statement of
theorem 4.

By the definition of loop variables, LVar(S1 ) = LVar(S2 ) = ∅ in
both base cases. Therefore, Lemma 5.6 holds.
Induction Step.
The hypothesis IH is that Lemma 5.6 holds when size(S1 ) +
size(S2 ) = k ≥ 2.
We show that Lemma 5.6 holds when size(S1 ) + size(S2 ) =
k + 1.
The proof is a case analysis according to the cases in the definition of (S1 ≡S
H S2 ):

The properties of the termination deciding variables
Lemma 5.4. The crash variables of S1 ; S1′ is same as the union
of the crash variables of S1 and the imported variables in S1
relative to the crash variables of S1′ , CVar(S1 ; S1′ ) = CVar(S1 ) ∪
Imp(S1 , CVar (S1′ )).

1. S1 and S2 are one statement and one of the following holds.
(a) S1 = “If(e) then {S1t } else {S1f }”, S2 = “If(e) then {S2t }
else {S2f }” such that one of the following holds:

S1′

Proof. Let
= s1 ; ...; sk for k > 0. We show the lemma by
induction on k.
Base case.
By the definition of CVar(S), the lemma holds.
Induction step.
The hypothesis IH is that CVar(S1 ; s1 ; ...; sk ) = CVar(S1 ) ∪
Imp(S1 , CVar(s1 ; ...; sk )) for k ≥ 1.
Then we show that the lemma holds when S1′ = s1 ; ...; sk+1 .

i. S1t , S1f , S2t , S2f are all sequences of “skip”;
By the definition of loop variables, LVar(S1t ) = LVar(S1f ) =
LVar(S2t ) = LVar(S2f ) = ∅. Therefore, by the definition of loop variables, LVar(S1 ) = LVar(S2 ) = ∅. The
lemma 5.6 holds.
ii. At least one of S1t , S1f , S2t , S2f is not a sequence of
f
f
t
S
“skip” such that: (S1t ≡S
H S2 ) ∧ (S1 ≡H S2 );
f
t
t
size(S1 ) + size(S2 ) < k, size(S1 ) + size(S2f ) < k.
Then, by the hypothesis IH1, LVar(S1t ) = LVar(S2t ), LVar(S1f ) =
LVar(S2f ). Consequently, (LVar(S1t ) ∪ LVar(S1f )) =
(LVar(S2t ) ∪ LVar(S2f )) = LVar(∆). When LVar(∆) =
∅, then LVar(S1 ) = LVar(S2 ) = ∅ by the definition of loop variables. When LVar(∆) 6= ∅, then
LVar(S1 ) = LVar(S2 ) = LVar(∆) ∪ Use(e) by the
definition of loop variables. The lemma 5.6 holds.
(b) S1 = “whilehn1 i (e){S1′′ }”, S2 = “whilehn2 i (e){S2′′ }” such
that both of the following hold:
′′
• S1′′ ≡S
H S2 ;
′′
′′
• S1 and S2 have equivalent computation of TVar(S1 ) ∪
TVar(S2 );
By the hypothesis IH1, LVar(S1′′ ) = LVar(S2′′ ). Then
Use(e) ∪ LVar(S1′′ ) = Use(e) ∪ LVar(S2′′ ). Then, we show
that:
i
i
∀i ≥ 0, Imp(S1′′ , Use(e)∪LVar(S1′′ )) = Imp(S2′′ , Use(e)∪
′′
LVar(S2 )) by induction on i.
Base case
0
0
By our notation S 0 , S1′′ = skip, S2′′ = skip. Then, by the
definition of imported variables,
0
Imp(S1′′ , Use(e) ∪ LVar(S1′′ )) = Use(e) ∪ LVar(S1′′ ),
′′ 0
Imp(S2 , Use(e) ∪ LVar(S2′′ )) = Use(e) ∪ LVar(S2′′ ).
0
0
Then, Imp(S1′′ , Use(e)∪LVar(S1′′ )) = Imp(S2′′ , Use(e)∪
′′
LVar(S2 )).
Induction step
i
The hypothesis IH3 is that, ∀i ≥ 0, Imp(S1′′ , Use(e) ∪
′′
′′ i
′′
LVar(S1 )) = Imp(S2 , Use(e) ∪ LVar(S2 )).
i+1
Then we show that Imp(S1′′ , Use(e) ∪ LVar(S1′′ )) =
′′ i+1
Imp(S2
, Use(e) ∪ LVar(S2′′ )).
By Corollary C.1,
i+1
i
Imp(S1′′ , Use(e)∪LVar(S1′′ )) = Imp(S1′′ , Imp(S1′′ , Use(e)∪
LVar(S1′′ ))),
i+1
i
Imp(S2′′ , Use(e)∪LVar(S2′′ )) = Imp(S2′′ , Imp(S2′′ , Use(e)∪
′′
LVar(S2 ))).
i
By the hypothesis IH3, Imp(S1′′ , Use(e) ∪ LVar(S1′′ )) =
′′ i
′′
Imp(S2 , Use(e) ∪ LVar(S2 )) = LVar(∆).

CVar(S1 ; s1 ; ...; sk+1 )
= CVar(S1 ; s1 ) ∪ Imp(S1 ; s1 , CVar(s2 ; ...; sk+1 )) by IH
CVar(S1 ; s1 )
= CVar(S1 ) ∪ Imp(S1 , CVar(s1 )) (1)
Imp(S1 ; s1 , CVar (s2 ; ...; sk+1 ))
= Imp(S1 , Imp(s1 ; CVar (s2 ; ...; sk+1 ))) (2)
Combining (1) and (2), we have
CVar(S1 ; s1 ) ∪ Imp(S1 ; s1 , CVar (s2 ; ...; sk+1 ))
= CVar(S1 ) ∪ Imp (S1 , CVar (s1 ))
∪ Imp (S1 , Imp(s1 ; CVar (s2 ; ...; sk+1 )))
= CVar(S1 ) ∪ Imp(S1 , CVar(s1 ) ∪ Imp(s1 ; CVar(s2 ; ...; sk+1 )))
by Lemma C.2
= CVar(S1 ) ∪ Imp(S1 , CVar(s1 ; ...; sk+1 )).
Lemma 5.5. The loop deciding variables of S1 ; S1′ is same as
the union of the loop deciding variables of S1 and the imported
variables in S1 relative to the loop deciding variables of S1′ ,
LVar(S1 ; S1′ ) = LVar(S1 ) ∪ Imp(S1 , LVar (S1′ )).
By proof of Lemma 5.5 similar to that of lemma 5.4 above.
Lemma 5.6. If two statement sequences S1 and S2 satisfy the proof
rule of termination in the same way, then S1 and S2 have same loop
variables, (S1 ≡S
H S2 ) ⇒ (LVar(S1 ) = LVar(S2 )).
Proof. By induction on size(S1 )+size(S2 ), the sum of the program
size of S1 and S2 .
Base case.
S1 and S2 are simple statement. There are three base cases
according to the definition of s1 ≡S
H s2 .
1. two same simple statements, S1 = S2 ;
2. S1 and S2 are input statement with same type variable: S1 =
“input id1 ”, S2 = “input id2 ” where (ΓS1 ⊢ id1 : t) ∧ (ΓS2 ⊢
id2 : t);.
3. S1 = “output e” or “id1 := e”, S2 = “output e” or “id2 := e”
where both of the following hold:

26

2015/9/14

Besides, by the definition of loop variables, LVar(∆) ⊆
LVar(S1 ), LVar(∆) ⊆ LVar(S2 ).
Then,
i
Imp(S1′′ , Imp(S1′′ , Use(e) ∪ LVar(S1′′ )))
′′
= Imp(S1 , LVar(∆))
= Imp(S2′′ , LVar(∆)) by Lemma 5.3
i
= Imp(S2′′ , Imp(S2′′ , Use(e) ∪ LVar(S2′′ )))
In summary, LVar(S1 )) = LVar(S2 ). The lemma 5.6 holds.

In the following, we show LVar(S1 ) = LVar(S2 ) in three
steps.
i. We show LVar(S1′ ; s1 ; s′1 ) = LVar(S1′ )
∪Imp(S1′ , LVar(s1 )) ∪ Imp(S1′ , LVar(s′1 )).
LVar(S1′ ; s1 ; s′1 )
= LVar(S1′ ; s1 ) ∪ Imp(S1′ ; s1 , LVar(s′1 ))
by the definition of loop variables

LVar(S1′ ; s1 )
= LVar(S1′ ) ∪ Imp(S1′ , LVar(s1 )) (2)
by the definition of loop variables

2. S1 and S2 are not both one statement and one of the following
holds:
(a) S1 = S1′ ; s1 and S2 = S2′ ; s2 such that all of the following
hold:

Imp(S1′ ; s1 , LVar(s′1 ))
= Imp(S1′ , Imp(s1 , LVar(s′1 ))) by Lemma C.1
= Imp(S1′ , LVar(s′1 )) (3)
by the condition Def(s1 ) ∩ TVar(s′1 ) = ∅

′
• S1′ ≡S
H S2 ;
′
′
• S1 and S2 have equivalent computation of TVar(s1 ) ∪

TVar(s2 );

• s1 ≡S
H s2 where s1 and s2 are not “skip”;

LVar(S1′ )

According to (2) and (3), LVar(S1′ ; s1 ; s′1 ) = LVar(S1′ )∪
Imp(S1′ , LVar(s1 )) ∪ Imp(S1′ , LVar(s′1 )).
Similarly, LVar(S2′ ; s2 ; s′2 ) = LVar(S2′ )∪
Imp(S2′ , LVar(s2 )) ∪ Imp(S2′ , LVar(s′2 )).
ii. We show that Imp(S1′ , LVar(s1 )) = Imp(S2′ , LVar(s′2 )).
Imp(S2′ , LVar(s′2 )).
We need to show that LVar(s1 ) ⊆ LVar(s1 ; s′1 ) and
LVar(s′2 ) ⊆ LVar(s2 ; s′2 ). By the definition of loop
variables, LVar(s1 ) ⊆ LVar(s1 ; s′1 ). By the definition
of loop variables again, LVar(s2 ; s′2 ) = LVar(s2 ) ∪
Imp(s2 , LVar(s′2 )). Because Def(s2 ) ∩ TVar(s′2 ) = ∅,
Imp(s2 , LVar(s′2 )) = LVar(s′2 ).
By the induction hypothesis IH, LVar(s1 ) = LVar(s′2 ).
By Lemma 5.3, ∀x ∈ LVar(s1 ) = LVar(s′2 ), Imp(S1′ , {x}) =
Imp(S2′ , {x}). By Lemma C.2, Imp(S1′ , LVar(s1 )) =
Imp(S2′ , LVar(s′2 )).
iii. We show that Imp(S1′ , LVar(s′1 )) = Imp(S2′ , LVar(s2 )).
By the similar argument that Imp(S1′ , LVar(s1 )) =
Imp(S2′ , LVar(s′2 )).
In conclusion, LVar(S1′ ; s1 ; s′1 ) = LVar(S2′ ; s2 ; s′2 ).

LVar(S2′ ),

By the hypothesis IH1,
=
LVar(s1 ) =
LVar(s2 ) = LVar(∆). Besides,
Imp(S1′ , LVar(s1 )) = LVar(S2′ , LVar(s2 )) by Lemma 5.3.
Therefore, LVar(S1 ) = LVar(S2 ) by the definition of loop
variables. The lemma 5.6 holds.
(b) One last statement is “skip”: w.l.o.g.,

′
(S1 ≡S
H S2 ) ∧ (s2 = “skip”) .
By the hypothesis IH1, LVar(S1 ) = LVar(S2′ ) . Besides,
LVar(s2 ) = ∅ by the definition of loop variables. Therefore,
LVar(S1 ) = LVar(S2 ) = LVar(S2′ ) ∪ Imp(S2′ , ∅). The
lemma 5.6 holds.
(c) One last statement is a “duplicate” statement such that one
of the following holds:
W.l.o.g. S1 = S1′ ; s′1 ; S1′′ ; s1 and all of the following hold:
• S1′ ; s′1 ; S1′′ ≡S
H S2 ;
• s′1 ≡S
H s1 ;
• Def(s′1 ; S1′′ ) ∩ TVar(s1 ) = ∅;
• s2 6= “skip”;
By the hypothesis IH, LVar(S1′ ; s′1 ; S1′′ ) = LVar(S2 ).
Then, we show that LVar(S1′ ; s′1 ; S1′′ ) = LVar(S1′ ; s′1 ; S1′′ ; s1 ).
By the induction hypothesis IH, LVar(s′1 ) = LVar(s1 ).
LVar(S1′ ; s′1 ; S1′′ ; s1 )
= LVar(S1′ ; s′1 ; S1′′ ) ∪ Imp(S1′ ; s′1 ; S1′′ , LVar(s1 ))
by the definition of loop variables

Lemma 5.7. If two statement sequences S1 and S2 satisfy the proof
rule of termination in the same way, then S1 and S2 have same
crash variables, (S1 ≡S
H S2 ) ⇒ (CVar(S1 ) = CVar(S2 )).
By proof similar to those for Lemma 5.6.

Imp(S1′ ; s′1 ; S1′′ , LVar(s1 ))
= Imp(S1′ , Imp(s′1 ; S1′′ , LVar(s1 ))) by Lemma C.1
= Imp(S1′ , LVar(s1 )) by Def(s′1 ; S1′′ ) ∩ TVar(s1 ) = ∅
= Imp(S1′ , LVar(s′1 )) by LVar(s′1 ) = LVar(s1 )
⊆ LVar(S1′ ; s′1 ) by the definition of loop variables
⊆ LVar(S1′ ; s′1 ; S1′′ ) by Lemma 5.5.

Corollary 5.1. If two statement sequences S1 and S2 satisfy
the proof rule of termination in the same way, then S1 and S2
have same termination deciding variables, (S1 ≡S
H S2 ) ⇒
(TVar(S1 ) = TVar(S2 )).
By Lemma 5.6, and 5.7.

In conclusion, LVar(S1′ ; s′1 ; S1′′ ; s1 ) = LVar(S1′ ; s′1 ; S1′′ ).
The lemma holds.
(d) S1 = S1′ ; s1 ; s′1 ; and S2 = S2′ ; s2 ; s′2 where s1 and s2 are
reordered and all of the following hold:

Properties of the input sequence variable
Lemma 5.8. If there is no input statement in a statement sequence
S, then the input sequence variable is not in the defined variables
of S, (∄“input x” ∈ S) ⇒ idI ∈
/ Def(S).

′
• S1′ ≡S
H S2 ;
′
′
• S1 and S2 have equivalent computation of TVar(s1 ; s′1 )∪

•
•
•
•

TVar(s2 ; s′2 ).
′
s1 ≡S
H s2 ;
′
S
s1 ≡H s2 ;
Def(s1 ) ∩ TVar(s′1 ) = ∅;
Def(s2 ) ∩ TVar(s′2 ) = ∅;

Proof. By induction on abstract syntax of S.
Lemma 5.9. If there is no input statement in a statement sequence
S, then the input sequence variable is not in the crash variables of
S, (∄“input x” ∈ S) ⇒ (idI ∈
/ CVar(S)).

By the hypothesis IH, LVar(S1′ ) = LVar(S2′ ), LVar(s1 ) =
LVar(s′2 ), LVar(s′1 ) = LVar(s2 ).

Proof. By induction on abstract syntax of S.

27

2015/9/14

i

Imp(S ′ , {idI } ∪ Use(e))
i
⊆ Imp(S ′ , CVar(S ′ ) ∪ Use(e)) (1) by the hypothesis IH1

Lemma 5.10. If there is no input statement in a statement sequence
S, then the input sequence variable is in the loop variables of S,
(∄“input x” ∈ S) ⇒ (idI ∈
/ LVar(S)).

i+1

Imp(S ′ , {idI } ∪ Use(e))
i
= Imp(S ′ , Imp(S ′ , {idI } ∪ Use(e))) (2) by Corollary C.1

Proof. By induction on abstract syntax of S.
Corollary 5.2. If there is no input statement in a statement sequence S, then the input sequence variable is in the termination
deciding variables of S, (∄“input x” ∈ S) ⇒ (idI ∈
/ TVar(S)).

i+1

Imp(S ′ , CVar(S ′ ) ∪ Use(e))
i
= Imp(S ′ , Imp(S ′ , CVar(S ′ ) ∪ Use(e))) (3) by Corollary C.1.

By Lemma 5.9 and 5.10.
Lemma 5.11. If there is one input statement in a statement sequence S, then the input sequence variable is in the crash variables and defined variables of S, (∃“input x” ∈ S) ⇒ (idI ∈
CVar(S)) ∧ (idI ∈ Def(S)).

Combining (1), (2) and (3):
i

Imp(S ′ , Imp(S ′ , {idI } ∪ Use(e)))
i
⊆ Imp(S ′ , Imp(S ′ , CVar(S ′ ) ∪ Use(e))) by Lemma C.2.

Proof. By induction on abstract syntax of S.

i+1

i+1

Therefore, Imp(S ′ , {idI }∪Use(e)) ⊆ Imp(S ′ , CVar(S ′ )∪
Use(e)).
In conclusion, Imp(S, {idI }) ⊆ CVar(S).
4. S = s1 ; ...; sk , for k > 0.
By induction on k.
Base case. k = 1.
Proof. By induction on abstract syntax of S.
By above cases, the lemma holds.
1. S = “input x”.
Induction step.
By the definition of CVar(·) and Imp(·), CVar(S) = Imp(S, {idI }) =
The induction hypothesis IH2 is that the lemma holds when
{idI }.
k > 0. We show that the lemma holds when S = s1 ; ...; sk+1 .
2. S = “If(e) then {St } else {Sf }”.
By the definition of crash variables, CVar(s1 ; ...; sk+1 ) =
W.l.o.g., there is input statement in St , by the induction hyCVar(s1 ; ...; sk ) ∪ Imp(s1 ; ...; sk , CVar(sk+1 )).There are two
pothesis, Imp(St , {idI }) ⊆ CVar(St ). There are two subcases
possibilities.
regarding if input statement is in Sf .
(a) ∄“input x” ∈ sk+1 .
(a) There is input statement in Sf .
By Lemma 5.8, idI ∈
/ Def(S).
By the induction hypothesis, Imp(Sf , {idI }) ⊆ CVar(Sf ).
Imp(s1 ; ...; sk+1 , {idI })
Hence, the lemma holds.
= Imp(s1 ; ...; sk , {idI }) by idI ∈
/ Def(S) and
(b) There is no input statement in Sf .
the definition of imported variables
By the definition of imported variables, Imp(Sf , {idI }) =
⊆ CVar(s1 ; ...; sk ) by the hypothesis IH2
{idI }. By Lemma 5.11, idI ∈ CVar(St ). Therefore, the
⊆ CVar(s1 ; ...; sk+1 ) by the definition of crash variables
lemma holds.
3. S = “whilehni (e){S ′ }”.
(b) ∃“input x” ∈ sk+1 .
By the induction hypothesis, Imp(S ′ , {idI }) ⊆ CVar(S ′ ).
S
Imp(s1 ; ...; sk+1 , {idI })
i
By the definition of Imp(·), Imp(S ′ , {idI }) = i≥0 Imp(S ′ , {idI }∪
= Imp(s1 ; ...; sk , Imp(sk+1 , {idI }))
′
Use(e)). By the definition of CVar(·), CVar(S ) =
by the definition of imported variables
S
′i
′
i≥0 Imp(S , CVar(S ) ∪ Use(e)).
i
Imp(sk+1 , {idI })
By induction on i, we show that, ∀i ≥ 0, Imp(S ′ , {idI } ∪
′i
′
⊆ CVar(sk+1 ) by the hypothesis IH2
Use(e)) ⊆ Imp(S , CVar(S ) ∪ Use(e)).
Base case i = 0.
0
Imp(s1 ; ...; sk , Imp(sk+1 , {idI }))
By notation S ′ = skip.
⊆ Imp(s1 ; ...; sk , CVar(sk+1 )) by Lemma C.2
0
⊆ CVar(s1 ; ...; sk+1 ) by the definition of crash variables.
Imp(S ′ , {idI } ∪ Use(e))
= {idI } ∪ Use(e) by the definition of imported variables

Lemma 5.12. If there is one input statement in a statement sequence S, then the imported variables in S relative to the input sequence variable are a subset of the crash variables of S,
(∃“input x” ∈ S) ⇒ (Imp(S, {idI }) ⊆ CVar(S)).

0

Imp(S ′ , CVar(S ′ ) ∪ Use(e))
= CVar(S ′ ) ∪ Use(e) by the definition of imported variables

Lemma 5.13. If two programs S1 and S2 satisfy the proof rule of
termination in the same way, then S1 and S2 satisfy the proof rule
of terminating computation in the same way of the input sequence,
S
(S1 ≡S
H S2 ) ⇒ (S1 ≡idI S2 ).

′

idI ⊆ CVar(S ) (1) by Lemma 5.11
0

Imp(S ′ , {idI } ∪ Use(e))
0
⊆ Imp(S ′ , CVar(S ′ ) ∪ Use(e)) by Lemma C.2.

Proof. By induction on size(S1 ) + size(S2 ).
Base case. S1 and S2 are simple statements.
There are three cases.

Induction step.
i
The hypothesis IH1 is that Imp(S ′ , {idI } ∪ Use(e)) ⊆
′i
′
Imp(S , CVar(S ) ∪ Use(e)) for i > 0.
i+1
Then we show that Imp(S ′ , {idI } ∪ Use(e))
′ i+1
′
⊆ Imp(S
, CVar(S ) ∪ Use(e))

1. S1 and S2 are “skip”: S1 = S2 = “skip”;
2. S1 and S2 are input statement: S1 = “input id1 ”, S2 =
“input id2 ”;

28

2015/9/14

i. idI ∈ TVar(s1 ) = TVar(s2 )
Then there is input statement in s1 and s2 . Otherwise, by
Lemma 5.2, idI ∈
/ TVar(s1 ) = TVar(s2 ). A contradiction. Then, by Lemma 5.11, idI ∈ Def(s1 ) ∩ Def(s2 ).
By Lemma 5.3, Imp(s1 , {idI }) = Imp(s2 , {idI }).
By Lemma 5.12, Imp(s1 , {idI }) ⊆ CVar(s1 , {idI }).
Therefore, S1′ and S2′ equivalently compute Imp(s1 , {idI })∪
Imp(s2 , {idI }). The lemma holds.
ii. idI ∈
/ TVar(s1 ) = TVar(s2 ).
Then, there is no input statement in s1 and s2 . Otherwise, by Lemma 5.11, idI ∈ CVar(s1 ) = CVar(s2 ). A
contradiction. Then, by Lemma 5.8, idI ∈
/ Def(s1 ) ∪
′
Def(s2 ). By the induction hypothesis IH, S1′ ≡S
idI S2 .
The lemma holds.
(b) One laststatement is “skip”: W.l.o.g., (S1′ ≡S
H S2 )∧(s1 =
“skip”)
By the induction hypothesis, S1′ ≡S
idI S2 . By definition,
idI ∈
/ Def(s1 ). The lemma holds.
(c) One last statement is a “duplicate” statement such that one
of the followings holds:
W.l.o.g., S1 = S1′ ; s′1 ; S1′′ ; s1 and all of the followings hold:
• S1′ ; s′1 ; S1′′ ≡S
H S2 ;
• s′1 ≡S
H s1 ;
• Def(s′1 ; S1′′ ) ∩ TVar(s1 ) = ∅;
• s2 6= “skip”.
By the induction hypothesis, S1′ ; s′1 ; S1′′ ≡S
idI S2 . In the
proof of Theorem 3, there is no input statement in s2 . Because ∀x : “input x” ∈
/ s2 , by Lemma 5.8, idI ∈
/ Def(s1 ).
The lemma holds.
(d) S1 = S1′ ; s1 ; s′1 ; and S2 = S2′ ; s2 ; s′2 where s1 and s2 are
reordered and all of the followings hold:

3. s1 and s2 are with the same expression: s1 = “output e” or
“id1 := e”, s2 = “output e” or “id2 := e”.
By definition of the proof rule of equivalent computation, the
lemma holds in above three cases.
Induction step.
The hypothesis IH is that the lemma holds when size(S1 ) +
size(S2 ) = k ≥ 2.
Then, we show that the lemma holds when size(S1 )+size(S2 ) =
k + 1. The proof is a case analysis of the cases in the proof rule of
termination in the same way.
1. S1 and S2 are one statement and one of the followings holds.
(a) S1 = “If(e) then {S1t } else {S1f }”, S2 = “If(e) then {S2t }
else {S2f }” and one of the followings holds:

i. S1t , S1f , S2t , S2f are all sequences of “skip”;
By Lemma 5.8, idI ∈
/ Def(S1 ) ∩ DefS2 . The lemma
holds.
ii. At least one of S1t , S1f , S2t , S2f is not a sequence of
“skip” such that:
f
f
t
S
(S1t ≡S
H S2 ) ∧ (S1 ≡H S2 );
Because size(S1 ) = 1+size(S1t )+size(S1f ), size(S2 ) =
1+size(S2t )+size(S2f ). Therefore, size(S1t )+size(S2t ) <
k, size(S1f ) + size(S2f ) < k. By the induction hypothef
f
t
S
sis IH, (S1t ≡S
idI S2 ) ∧ (S1 ≡idI S2 ). Then, the lemma
S
holds by the definition of S1 ≡idI S2 .

(b) S1 = “whilehn1 i (e){S1′′ }”, S2 = “whilehn2 i (e){S2′′ }” and
both of the followings hold:
′′
• S1′′ ≡S
H S2 ;
′′
′′
• S1 and S2 have equivalent computation of TVar(S1 ) ∪

TVar(S2 );

′
• S1′ ≡S
H S2 ;
• S1′ and S2′ have equivalent computation of TVar(s1 ; s′1 )∪

′′
By the induction hypothesis IH, S1′′ ≡S
idI S2 . In addition,
by Corollary 5.1, TVar(S1 ) = TVar(S2 ). There are two
cases.
i. idI ∈ TVar(S1 ) = TVar(S2 ).
We show that idI ∈ Def(S1 ) ∩ Def(S2 ). If there is
no input statement in S1 or S2 , then, by Corollary 5.2,
idI ∈
/ TVar(S1 ) ∩ TVar(S2 ). A contradiction. Thus,
there is input statement in S1 and S2 , by Lemma 5.11,
idI ∈ Def(S1 ) ∩ Def(S2 ).
By Lemma 5.12, Imp(S1 , {idI }) ⊆ CVar(S1 ). Similarly, Imp(S2 , {idI }) ⊆ TVar(S2 ). Hence, loop bodies
of S1 and S2 equivalently compute every of the imported variables in S1 and S2 relative to the input sequence variable, ∀x ∈ Imp(S1 , {idI })∪Imp(S2 , {idI }),
′′
S1′′ ≡S
x S2 . Thus, the lemma holds.
ii. idI ∈
/ TVar(S1 ) = TVar(S2 ).
Then there is no input statement in S1 and S2 . Otherwise, by Lemma 5.11, (idI ∈ CVar(S1 )) ∨ (idI ∈
CVar(S2 )). A contradiction. Then by Lemma 5.8, idI ∈
/
(Def(S1 ) ∩ Def(S2 )). Hence, the lemma holds.

•
•
•
•

TVar(s2 ; s′2 ).
′
s1 ≡S
H s2 ;
′
S
s1 ≡H s2 ;
Def(s1 ) ∩ TVar(s′1 ) = ∅;
Def(s2 ) ∩ TVar(s′2 ) = ∅;

In the proof of Theorem 4, we showed that s1 and s2 do not
both include input statement, s2 and s′2 do not both include
input statement. There are two subcases.
i. There are no input statements in both s1 and s′1 .
We show that there are no input statements in both
s2 and s′2 . By Corollary 5.1, TVar(s1 ) = TVar(s′2 )
and TVar(s′1 ) = TVar(s2 ). By Corollary 5.2, idI ∈
/
TVar(s1 )∪TVar(s′1 ). Thus, idI ∈
/ TVar(s2 )∪TVar(s′2 ).
If there is input statement in s2 or s′2 , then, by Lemma 5.11,
idI ∈
/ TVar(s2 ) ∪ TVar(s′2 ). A contradiction. In summary, there are no input statements in both s2 and s′2 .
By Lemma 5.8, idI ∈
/ Def(s1 ; s′1 ) and idI ∈
/ Def(s2 ; s′2 ).
′
By the induction hypothesis, S1′ ≡S
S
.
Therefore,
the
2
idI
lemma holds.
ii. W.l.o.g, there are input statements in s1 only.
By similar argument in the proof of Theorem 4 that s1
and s2 do not both include input statements, we can
show that there is no input statement in s2 and there is
input statement in s′2 .
In the following, the proof is of two steps.
′
A. We show that s1 ; s′1 ≡S
idI s2 .
′
By the induction hypothesis IH, s1 ≡S
idI s2 . Because there is no input statement in s′1 , then by

2. S1 and S2 are not both one statement and one of the followings
holds:
(a) S1 = S1′ ; s1 and S2 = S2′ ; s2 and all of the followings hold:

′
• S1′ ≡S
H S2 ;
′
′
• S1 and S2 have equivalent computation of TVar(s1 ) ∪

TVar(s2 );

• s1 ≡S
H s2 where s1 and s2 are not “skip”;

By the induction hypothesis IH, s1 ≡S
idI s2 . By Corollary 5.1, TVar(s1 ) = TVar(s2 ). There are two cases.

29

2015/9/14

′
Lemma 5.8, idI ∈
/ Def(s′1 ). Thus, s1 ; s′1 ≡S
idI s2
by definition.
B. We show that S1′ and S2′ ; s2 equivalently compute
Imp(s1 ; s′1 , {idI }) ∪ Imp(s′2 , {idI }).
The argument is of two parts. First, we need to
show that Def(s2 ) ∩ Imp(s′2 , {idI }) = ∅. By
Lemma 5.12, Imp(s′2 , {idI }) ⊆ CVar(s′2 ). Thus,
Imp(s′2 , {idI }) ⊆ TVar(s′2 ). By assumption, Def(s2 )∩
TVar(s′2 ) = ∅. Then, Def(s2 )∩Imp(s′2 , {idI }) = ∅.
By Lemma 5.3, Imp(s1 ; s′1 , {idI }) = Imp(s′2 , {idI }).
Thus, Def(s2 ) ∩ Imp(s1 ; s′1 , {idI }) = ∅.
Second, we show that Imp(s′2 , {idI }) ⊆ TVar(s2 ; s′2 )
and Imp(s1 ; s′1 , {idI }) ⊆ TVar(s1 ; s′1 ). By Lemma 5.12,
Imp(s1 ; s′1 , {idI }) ⊆ TVar(s1 ; s′1 ) and Imp(s′2 , {idI }) ⊆
TVar(s′2 ). Then we show that TVar(s′2 ) ⊆ TVar(s2 ; s′2 ).
We need to show that CVar(s′2 ) ⊆ CVar(s2 ; s′2 ) and
LVar(s′2 ) ⊆ LVar(s2 ; s′2 ).

CVar(s2 ; s′2 )
= CVar(s2 ) ∪ Imp(s2 , CVar(s′2 ))
by the definition of crash variables
Imp(s2 , CVar(s′2 ))
= CVar(s′2 ) (1) by the assumption
Def(s2 ) ∩ TVar(s′2 ) = ∅
CVar(s2 ) ∪ Imp(s2 , CVar(s′2 ))
= CVar(s2 ) ∪ CVar(s′2 ) by (1)

• ∀mS1 (fS1 , σS1 ) mS2 (fS2 , σS2 ) :

(((∀z ∈ TVar(S1 ) ∪ TVar(S2 )), σS1 (z) = σS2 (z)) ∧ (fS1 =
fS2 = 0)) ⇒
(S1 , mS1 (fS1 , σS1 )) ≡H (S2 , mS2 (fS2 , σS2 )).

If s1 and s2 start in the state m1 (f1 , loop1c , σ1 ) and
m2 (f2 , loop2c , σ2 ) respectively in which crash flags are not set,
f1 = f2 = 0, s1 and s2 have not already executed, loop1c (n1 ) =
loop2c (n2 ) = 0, value stores σ1 and σ2 agree on values of variables
in TVar(s), ∀x ∈ TVar(s) : σ1 (x) = σ2 (x), then, for any positive
integer i, one of the following holds:
1. The loop counters for s1 and s2 are less than i where s1 and s2
terminate in the same way:
′
∗
∗
∀m′1 m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
′
′
2
1
2′
(S2 , m2 (loopc )), loopc (n1 ) < i and mc (n2 ) < i and one
of the following holds:
(a) s1 and s2 both terminate:
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
(b) s1 and s2 both do not terminate:
k
k
∀k > 0 : (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) →
(S2k , m2k ) in which S1k 6= skip, S2k 6= skip.
2. The loop counters for s1 and s2 are less than or equal to i
where s1 and s2 do not terminate such that there are no configurations (s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and
(s2 , m2 ), respectively, in which crash flags are not set, the loop
counters of s1 and s2 are equal to i, and value stores agree on
the values of variables in TVar(s):

Similarly, LVar(s′2 ) ⊆ LVar(s2 ; s′2 ). Thus, TVar(s′2 ) ⊆
TVar(s2 ; s′2 ).
By assumption, ∀x ∈ Imp(s1 ; s′1 , {idI })∪Imp(s′2 , {idI }) :
′
′
S1′ ≡S
x S2 . In addition, Def(s2 )∩(Imp(s1 ; s1 , {idI })∪
Imp(s′2 , {idI })) = ∅. Thus, ∀x ∈ Imp(s1 ; s′1 , {idI })∪
′
Imp(s′2 , {idI }) : S1′ ≡S
x S2 ; s2 . The lemma holds.

Lemma 5.14. If two programs S1 and S2 satisfy the proof rule
of termination in the same way, and S1 and S2 both terminate
when started in their initial states with crash flags not set, f1 =
f2 = 0, whose value stores agree on values of variables of the
termination deciding variables of S1 and S2 , ∀x ∈ TVar(S1 ) ∪
TVar(S2 ), σ1 (x) = σ2 (x), and S1 and S2 are fed with the same
∗
infinite input sequence, σ1 (idI ) = σ2 (idI ), (S1 , m1 (f1 , σ1 ) →
∗
′
′
′
′
(skip, m1 (σ1 )) and (S2 , m2 (f2 , σ2 ) → (skip, m2 (σ2 )), then the
execution of S1 and S2 consume the same number of input values,
σ1′ (idI ) = σ2′ (idI ).
Proof. By Lemma 5.13, S1 ≡S
idI S2 . By Lemma 5.12, Imp(S1 , idI ) ⊆
CVar(S1 ) and Imp(S2 , idI ) ⊆ CVar(S2 ). By assumption, ∀x ∈
Imp(S1 , idI ) ∪ Imp(S2 , idI ) : σ1 (x) = σ2 (x). By Theorem 2,
σ1′ (idI ) = σ2′ (idI ).
Theorem of two loop statements terminating in the same way
Lemma 5.15. Let s1 = “whilehn1 i (e){S1 }” and
s2 = “whilehn2 i (e){S2 }” be two while statements with the same
set of termination deciding variables in program P1 and P2 respectively, whose bodies S1 and S2 satisfy the proof rule of equivalently
computation of variables in TVar(s), and S1 and S2 terminate in
the same way when started in states with crash flags not set and
agreeing on values of variables in TVar(S1 ) ∪ TVar(S2 ):
• TVar(s1 ) = TVar(s2 ) = TVar(s);
• ∀x ∈ TVar(s) : S1 ≡S
x S2 ;

∗

′

∗

• ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )), (s2 , m2 ) →
′

(S2 , m′2 (loop2c )) where
′
′
loop1c (n1 ) ≤ i, loop2c (n2 ) ≤ i;
• ∀k > 0 :
k
k
(s1 , m1 ) → (S1k , m1k ), (s2 , m2 ) → (S2k , m2k ) where
S1k 6= skip, S2k 6= skip; and
• ∄(s1 , m1i ) (s2 , m2i ) :
∗
(s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
(s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
f1 = f2 = 0; and
loop1ci (n1 ) = loop2ci (n2 ) = i; and
∀x ∈ TVar(s) : σ1i (x) = σ2i (x).

3. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in which both
crash flags are not set, the loop counters of s1 and s2 are
equal to i and value stores agree on the values of variables in
∗
TVar(s), and for every state in execution (s1 , m1 ) → (s1 , m1i )
∗
or (s2 , m2 ) → (s2 , m2i ), the loop counters for s1 and s2 are
less than or equal to i respectively:
∗
∃(s1 , m1i ) (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
2i
(s2 , m2 ) → (s2 , m2i (f2 , loopc , σ2i )) where
• f1 = f2 = 0; and
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ TVar(s) : σ1i (x) = σ2i (x); and
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (m1c )) → (s1 , m1i ),
1′
loopc (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (m2c )) → (s2 , m2i ),
2′
loopc (n2 ) ≤ i;
Proof. By induction on i.
Base case. i = 1.

30

2015/9/14

By assumption, initial loop counters of s1 and s2 are of value
zero. Initial value stores σ1 and σ2 agree on the values of variables
in TVar(s). Then we show one of the following cases hold:
1. The loop counters for s1 and s2 are less than 1, s1 and s2
terminate in the same way:
′
∗
∗
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
′
2
(S2 , m2 (loopc )),
′
′
m1c (n1 ) < 1 and m2c (n2 ) < 1 and one of the following holds:
(a) s1 and s2 both terminate:
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
(b) s1 and s2 both do not terminate:
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
in which S1k 6= skip, S2k 6= skip.
2. The loop counters for s1 and s2 are less than or equal to 1,
and s1 and s2 do not terminate such that there are no configurations (s1 , m11 ) and (s2 , m21 ) reachable from (s1 , m1 ) and
(s2 , m2 ), respectively, in which crash flags are not set, the loop
counters of s1 and s2 are equal to 1 and value stores agree on
the values of variables in TVar(s):
′

∗

∗

• ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )), (s2 , m2 ) →
′

′

′

(S2 , m′2 (loopc2 )) where loop1c (n1 ) ≤ i, loop2c (n2 ) ≤ i;

→(whilehn1 i (0) {S1 }, m1 (1/f1 )) by the ECrash rule
k
→(whilehn1 i (0) {S1 }, m1 (1/f1 , loop1c , σ1 )),
for any k ≥ 0, by the Crash rule.
Similarly, the execution of s2 started in the state m2 (f2 , loop2c , σ2 )
does not terminate.
The loop counters of s1 and s2 are less than 1:
′
∗
∗
∀m′1 m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
2′
1′
2′
(S2 , m2 (loopc )) where loopc (n1 ) < 1 and loopc (n2 ) < 1.
Besides, s1 and s2 both do not terminate when started in states
k
m1 and m2 , ∀k > 0 : (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 )
k
→ (S2k , m2k ) in which S1k 6= skip, S2k 6= skip.
2. E ′ JeKσ1 = E ′ JeKσ2 = (0, vof )
The execution of s1 proceeds as follows.
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c ))
→(whilehn1 i ((0, vof )) {S1 }, m1 (loop1c )) by the EEval’ rule
→(whilehn1 i (0) {S1 }, m1 (loop1c )) by the E-Oflow1 or E-Oflow2 rule
→(skip, m1 ) by the Wh-F1 rule.
2

Similarly, (s2 , m2 (loop2c , σ2 )) → (skip, m2 ).
The loop counters for s1 and s2 are less than 1:
• ∀k > 0 : (s1 , m1 ) → (S1k , m1k ), (s2 , m2 ) → (S2k , m2k )
′
∗
∗
where S1k 6= skip, S2k 6= skip; and
∀m′1 m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
′
∗
∄(s1 , m11 ), (s2 , m21 ) : (s1 , m1 ) → (s1 , m11 (f1 , loop1c1 , σ11 ))∧ (S2′ , m′2 (loop2c )) where loop1c (n1 ) < 1 and loop2c (n2 ) < 1.
∗
Besides, s1 and s2 both terminate when started in states m1 and
(s2 , m2 ) → (s2 , m21 (f2 , loop2c1 , σ21 )) where
m2 :
f1 = f2 = 0; and
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
loop1c1 (n1 ) = loop2c1 (n2 ) = 1; and
′
′
3. E JeKσ1 = E JeKσ2 = (v, vof ) where v ∈
/ {0, error};
∀x ∈ TVar(s) : σ11 (x) = σ21 (x).
The execution from (s1 , m1 (loop1c , σ1 )) proceeds as follows.
3. There are two configurations (s1 , m11 ) and (s2 , m21 ) reachable from (s1 , m1 ) and (s2 , m2 ) respectively, in which the loop
(s1 , m1 (loop1c , σ1 ))
counters of s1 and s2 are equal to 1 and value stores agree on
= (whilehn1 i (e) {S1 }, m1 (loop1c , σ1 ))
the values of variables in TVar(s) and, for every state in execu→(whilehn1 i ((v, vof )) {S1 }, m1 (loop1c , σ1 )) by the EEval’ rule
∗
∗
tion, (s1 , m1 ) → (s1 , m11 ) or (s2 , m2 ) → (s2 , m21 ) the loop
→(whilehn1 i (v) {S1 }, m1 (loop1c , σ1 ))
counters for s1 and s2 are less than or equal to 1 respectively:
by rule E-Oflow1 or E-Oflow2
∗
1
∃(s1 , m11 ) (s2 , m21 ) : (s1 , m1 ) → (s1 , m11 (f1 , loop1c1 , σ11 ))∧
→(S
1 ; whilehn1 i (e) {S1 }, m1 (loopc [1/(n1 )], σ1 ))
∗
(s2 , m2 ) → (s2 , m21 (f2 , loop2c1 , σ21 )) where
by the Wh-T1 rule.
• f1 = f2 = 0; and
2
• loop1c 1 (n1 ) = loop2c 1 (n1 ) = 1; and
Similarly, (s2 , m2 (loop2c , σ2 )) → (S2 ; whilehn2 i (e){S2 },
2
• ∀x ∈ TVar(s) : σ11 (x) = σ21 (x); and
m2 (loopc [1/(n2 )], σ2 )). After two steps of executions of s1
′
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m11 ), loop1c (n1 ) ≤ and s2 , crash flags are not set, the loop counter value of s1 and
s2 are 1, value stores σ1 and σ2 agree on values of variables in
1; and
′
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m21 ), loop2c (n2 ) ≤ TVar(s).
We show that TVar(SS
1 ) ⊆ TVar(s). By definition of loop
1.
variables, LVar(s1 ) = j≥0 Imp(S1j , LVar(S1 ) ∪ Use(e)). By
We show evaluations of the predicate expression of s1 and s2
notation of S 0 , S 0 = skip. By definition of imported variw.r.t value stores σ1 and σ2 produce
same value. By the definition
S
ables, Imp(S10 , LVar(S1 ) ∪ Use(e)) = LVar(S1 ) ∪ Use(e).
of loop variables, LVar(s1 ) = j≥0 Imp(S1j , LVar(S1 ) ∪ Use(e)).
Then
LVar(S1 ) ⊆ LVar(s). By similar argument, we have
By our notation of S 0 , S10 = skip. By the definition of loop
CVar(S1 ) ⊆ CVar(s). Hence, TVar(S1 ) ⊆ TVar(s). Simivariables, Use(e) ⊆ LVar(s) = LVar(s1 ). By assumption, value
larly, TVar(S2 ) ⊆ TVar(s). By assumption, S1 and S2 either
stores σ1 and σ2 agree on the values of the variables in Use(e). By
both terminate or both do not terminate when started in state
Lemma D.1, the predicate expression e of s1 and s2 evaluates to
m1 (loop1c [1/(n1 )], σ1 ) and m2 (loop2c [1/(n2 )], σ2 ) in which
′
′
same value v w.r.t value stores σ1 , σ2 , E JeKσ1 = E JeKσ2 . Then
∀y ∈ TVar(S1 ) ∪ TVar(S2 ), σ1 (y) = σ2 (y) and crash flags
there are three possibilities.
are not set. Then there are two possibilities:
(a) S1 and S2 both terminate when started in states m1 (
1. E ′ JeKσ1 = E ′ JeKσ2 = (error, ∗)
loop1c [1/(n1 )], σ1 ) and m2 (loop2c [1/(n2 )], σ2 ) respectively:
The execution from (s1 , m1 (f1 , loop1c , σ1 )) proceeds as fol∗
1
11
(S
lows.
1 , m1 (loopc [1/(n1 )], σ1 )) → (skip, m11 (f1 , loopc , σ11 ))
and
∗
(s1 , m1 (f1 , loop1c , σ1 ))
(S2 , m2 (loop2c [1/(n2 )], σ2 )) → (skip, m21 (f2 , loop2c1 , σ21 )).
1
= (whilehn1 i (e) {S1 }, m1 (f1 , loopc , σ1 ))
We show that, after the full execution of S1 and S2 , the
following five properties hold.
→(whilehn1 i ((error, ∗)) {S1 }, m1 (f1 , loop1c , σ1 )) by the EEval’ rule
k

k

31

2015/9/14

m1 (loop1c [1/(n1 )], σ1 ) and
By the definition of terminating execution, crash flags
m2 (loop2c [1/(n2 )], σ2 ) respectively:
k
are not set, f1 = f2 = 0.
∀k > 0, (S1 , m1 (loop1c [1/(n1 )], σ1 )) →
• The loop counter of s1 and s2 are of value 1, loop1c 1 (n1 ) =
11
(S1k , m11k (loopc k , σ11k )) and
loop2c1 (n2 ) = 1.
k
(S2 , m2 (loop2c [1/(n2 )], σ2 )) →
By the assumption of unique loop labels, s1 ∈
/ S1 . Then,
21
the loop counter value of n1 is not redefined in the
(S2k , m21k (loopc k , σ21k )) in which S1k 6= skip, S2k 6=
execution of S1 by corollary E.2, loop1c [1/n1 ](n1 ) =
skip.
loop1c1 (n1 ) = 1. Similarly, the loop counter value of n2
By our assumption of unique loop labels, s1 ∈
/ S1 . Then,
11
is not redefined in the execution of S2 , loop2c [1/(n2 )](n2 )
∀k > 0, loopc k (n1 ) = loop1c [1/(n1 )](n1 ) = 1. Similarly,
21
= loopc (n2 ) = 1.
21
∀k > 0, loopc k (n2 )
∗
• In any state in the execution (s1 , m1 ) → (s1 , m11 (loop1c 1 , σ11 )),
= loop2c [1/(n2 )](n2 ) = 1. In addition, by Lemma E.2,
the loop counter of s1 is less than or equal to 1.
k
∀k > 0, (S1 ; s1 , m1 (loop1c [1/(n1 )], σ1 )) →
As is shown above, the loop counter of s1 is zero
1k
(Sk ; s1 , m1k (loopc , σ1k )) and
in any of the two states in the one step execution
k
1
(s1 , m1 ) → (whilehn1 i (v) {S1 }, m1 (loopc , σ1 )), and
(S2 ; s2 , m2 (loop2c [1/(n2 )], σ2 )) → (S2k ; s2 , m2k (loop2ck , σ2k ))
the loop counter of s1 is 1 in any states in the execution
in which S1k 6= skip, S2k 6= skip.
∗
In summary, loop counters of s1 and s2 are less than or
(S1 ; whilehn1 i (e) {S1 }, m1 (loop1c [i/(n1 )], σ1 )) →
equal to 1, and s1 and s2 do not terminate such that there are
(s1 , m11 (loop1c1 , σ11 )).
∗
no configurations (s1 , m11 ) and (s2 , m21 ) reachable from
21
• In any state in the executions (s2 , m2 ) → (s2 , m21 (loopc , σ21 )),
(s1 , m1 ) and (s2 , m2 ), respectively, in which crash flags
the loop counter of s2 is less than or equal to 1.
are not set, the loop counters of s1 and s2 are equal to 1 and
By similar argument above.
value stores agree on the values of variables in TVar(s).
• The value stores σ11 and σ21 agree on values of the
termination deciding variables in s1 and s2 : ∀x ∈
Induction Step.
TVar(s), σ11 (x) = σ21 (x).
The induction hypothesis IH is that, for a positive integer i, one of
We show that the imported variables in S1 relative to
the following holds:
those in LVar(s) are a subset of LVar(s) and the imported variables in S1 relative to those in CVar(s) are
1. The loop counters for s1 and s2 are less than i, and s1 and s2
a subset of CVar(s).
both terminate in the same way:
′
∗
∗
LVar(s1 )
S
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
j
′
= j≥0 Imp(S1 , LVar(S1 ) ∪ Use(e)) (1)
(S2′ , m′2 (loop2c )),
by the definition of loop variables.
′
′
loop1c (n1 ) < i and loop2c (n2 ) < i and one of the following
holds:
Imp(S1 , LVar(s)) = Imp(S1 , LVar(s1 ))
(a)
s1 and s2 both terminate:
= Imp(S1 , Imp(s1 , Use(e) ∪ LVar(S1 )))
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
by the definition
of
LVar(s)
S
(b) s1 and s2 both do not terminate:
= Imp(S1 , j≥0 Imp(S1j , LVar(S1 ) ∪ Use(e))) by (1)
k
k
S
j
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
= j≥0 Imp(S1 , Imp(S1 , LVar(S1 ) ∪ Use(e)))
in which S1k 6= skip, S2k 6= skip.
by
S Lemma C.2
2.
The
loop counters for s1 and s2 are less than or equal to i, and
j
= j>0 Imp(S1 , LVar(S1 ) ∪ Use(e)) by Lemma C.1
S
s1 and s2 do not terminate such that there are no configurations
j
⊆ j≥0 Imp(S1 , LVar(S1 ) ∪ Use(e))
(s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and (s2 , m2 ),
= Imp(s1 , LVar(S1 ) ∪ Use(e)) = LVar(s1 ) = LVar(s).
respectively, in which crash flags are not set, the loop counters
Similarly, Imp(S1 , CVar(s)) ⊆ CVar(s). Hence,
of s1 and s2 are equal to i and value stores agree on the values
Imp(S1 , TVar(s)) ⊆ TVar(s). In the same way, we
of variables in TVar(s):
′
∗
∗
can show that Imp(S2 , TVar(s)) ⊆ TVar(s). Con• ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )), (s2 , m2 ) →
sequently, the value stores σ11 and σ21 agree on the
′
′
′
2
1
2′
(S2 , m2 (loopc )) where loopc (n1 ) ≤ i, loopc (n2 ) ≤ i;
values of the imported variables in S1 and S2 relak
k
tive to those in TVar(s), ∀x ∈ Imp(S1 , TVar(s)) ∪
• ∀k > 0 : (s1 , m1 ) → (S1k , m1k ), (s2 , m2 ) → (S2k , m2k )
Imp(S2 , TVar(s)), σ1 (x, ) = σ2 (x). Because S1 and
where S1k 6= skip, S2k 6= skip; and
∗
S2 have equivalent computation of every variable in
• ∄(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1c i , σ1i ))∧
TVar(s) when started in states agreeing on the values
∗
2i
(s2 , m2 ) → (s2 , m2i (f2 , loopc , σ2i )) where
of the imported variables relative to TVar(s), by Thef1 = f2 = 0; and
orem 1, value stores σ11 and σ21 agree on the values
loop1ci (n1 ) = loop2ci (n2 ) = i; and
of the variables TVar(s), ∀x ∈ TVar(s), σ11 (x) =
∀x ∈ TVar(s) : σ1i (x) = σ2i (x).
σ21 (x).
It follows that, by Corollary E.1,
3. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable
∗
from (s1 , m1 ) and (s2 , m2 ), respectively, in which crash flags
(S1 ; whilehn1 i (e){S1 }, m1 (loop1c [1/(n1 )], σ1 )) →
(whilehn1 i (e) {S1 }, m11 (loop1c1 , σ11 )) = (s1 , m11 (loop1c1 , σ11 )) are not set, the loop counters of s1 and s2 are equal to i
and value stores agree on the values of variables in TVar(s)
and
∗
∗
(S2 ; whilehn2 i (e) {S2 }, m2 (loop2c [1/(n2 )], σ2 )) →
and, for every state in execution, (s1 , m1 ) → (s1 , m1i ) or
∗
21
21
(whilehn2 i (e) {S2 }, m21 (loopc , σ21 )) = (s2 , m21 (loopc , σ21 )). (s2 , m2 ) → (s2 , m2i ) the loop counters for s1 and s2 are less
(b) S1 and S2 do not terminate when started in states
than or equal to i respectively:
• The crash flags are not set.

32

2015/9/14

∗

∃(s1 , m1i ) (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
(s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
• f1 = f2 = 0; and
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ TVar(s) : σ1i (x) = σ2i (x,; and
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i ),
1′
loopc (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i ),
2′
loopc (n2 ) ≤ i;

1. The loop counters for s1 and s2 are less than i:
′
∗
∗
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
2′
′
(S2 , m2 (loopc )),
′
′
loop1c (n1 ) < i and loop2c (n2 ) < i and one of the following
holds:
(a) s1 and s2 both terminate:
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
(b) s1 and s2 both do not terminate:
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
in which S1k 6= skip, S2k 6= skip.

Then we show that, for the positive integer i + 1, one of the
following holds:
1. The loop counters for s1 and s2 are less than i + 1, and s1 and
s2 both terminate in the same way:
′
∗
∗
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
′
2
(S2 , m2 (loopc )),
′
′
loop1c (n1 ) < i + 1 and loop2c (n2 ) < i + 1 and one of the
following holds:
(a) s1 and s2 both terminate:
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
(b) s1 and s2 both do not terminate:
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
in which S1k 6= skip, S2k 6= skip.
2. The loop counters for s1 and s2 are less than or equal to
i + 1, and s1 and s2 do not terminate such that there are
no configurations (s1 , m1i+1 ) and (s2 , m2i+1 ) reachable from
(s1 , m1 ) and (s2 , m2 ), respectively, in which crash flags are
set, the loop counters of s1 and s2 are equal to i + 1 and value
stores agree on the values of variables in TVar(s):
′

∗

∗

• ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )), (s2 , m2 ) →

k

k

∗

∗

(s1 , m1 ) → (s1 , m1i+1 (f1 , loopc1i+1 , σ1i+1 ))∧(s2 , m2 ) →
(s2 , m2i+1 (f2 , loop2ci+1 , σ2i+1 )) where
f1 = f2 = 0; and
loop1ci+1 (n1 ) = loop2ci+1 (n2 ) = i + 1; and
∀x ∈ TVar(s) : σ1i+1 (x) = σ2i+1 (x).

When this case holds, we have the loop counter of s1 and s2 are
less than i + 1, and s1 and s2 both do not terminate:
′
∗
∗
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
2′
(S2 , m2 (loopc )),
′
′
loop1c (n1 ) < i + 1 and loop2c (n2 ) < i + 1 and s1 and s2 both
do not terminate:
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
in which S1k 6= skip, S2k 6= skip.
3. There are two configurations (s1 , m1i ) and (s2 , m2i ) reachable
from (s1 , m1 ) and (s2 , m2 ), respectively, in which crash flags
are not set, the loop counters of s1 and s2 are equal to i
and value stores agree on the values of variables in TVar(s)
∗
and, for every state in executions (s1 , m1 ) → (s1 , m1i ) and
∗
(s2 , m2 ) → (s2 , m2i ) the loop counters for s1 and s2 are less
than or equal to i respectively:
∗
∃(s1 , m1i ) (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
2i
(s2 , m2 ) → (s2 , m2i (f2 , loopc , σ2i )) where
• f1 = f2 = 0; and
• loopc1i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ TVar(s), σ1i (x) = σ2i (x); and

3. There are two configurations (s1 , m1i+1 ) and (s2 , m2i+1 )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in which
the loop counters of s1 and s2 are equal to i + 1 and value
stores agree on the values of variables in TVar(s) and, for every
∗
∗
state in execution, (s1 , m1 ) → (s1 , m1i+1 ) or (s2 , m2 ) →
(s2 , m2i+1 ) the loop counters for s1 and s2 are less than or
equal to i + 1 respectively:
∃(s1 , m1i+1 ) (s2 , m2i+1 ) :
∗
∗
(s1 , m1 ) → (s1 , m1i+1 (loop1ci+1 , σ1i+1 )) ∧ (s2 , m2 ) →
(s2 , m2i+1 (loop2ci+1 , σ2i+1 )) where
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
• ∀x ∈ TVar(s) : σ1i+1 (x) = σ2i+1 (x); and
∗

• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i ),
′

•

k

where S1k 6= skip, S2k 6= skip; and
∗
∄(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
(s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
f1 = f2 = 0; and
loop1ci (n1 ) = loop2ci (n2 ) = i; and
∀x ∈ TVar(s) : σ1i (x) = σ2i (x).

k

where S1k 6= skip, S2k 6= skip; and

loop1c (n1 ) ≤ i + 1; and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
′
2
loopc (n2 ) ≤ i + 1;

∗

′

• ∀k > 0 : (s1 , m1 ) → (S1k , m1k ), (s2 , m2 ) → (S2k , m2k )

• ∄(s1 , m1i+1 ), (s2 , m2i+1 ) :

′

′

(S2 , m′2 (loop2c )) where loop1c (n1 ) ≤ i, loop2c (n2 ) ≤ i;

• ∀k > 0 : (s1 , m1 ) → (S1k , m1k ), (s2 , m2 ) → (S2k , m2k )

∗

′

∗

• ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )), (s2 , m2 ) →
′

′

(S2 , m′2 (loop2c )) where
′
′
loop1c (n1 ) ≤ i + 1, loop2c (n2 ) ≤ i + 1;

When this case holds, then we have the loop counters for s1 and
s2 are less than i + 1, and s1 and s2 both terminate in the same
way:
′
∗
∗
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and (s2 , m2 ) →
′
′
2′
(S2 , m2 (loopc )),
′
′
loop1c (n1 ) < i + 1 and loop2c (n2 ) < i + 1 and one of the
following holds:
(a) s1 and s2 both terminate:
∗
∗
(s1 , m1 ) → (skip, m′′1 ) and (s2 , m2 ) → (skip, m′′2 ).
(b) s1 and s2 both do not terminate:
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
in which S1k 6= skip, S2k 6= skip.
2. The loop counters for s1 and s2 are less than or equal to i, and
s1 and s2 both do not terminate such that there are no configurations (s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and
(s2 , m2 ), respectively, in which the loop counters of s1 and s2
are equal to i and value stores agree on the values of variables
in TVar(s):

(s2 , m2i ),

By the hypothesis IH, one of the following holds:

33

2015/9/14

∗

′

∗

m1i (loop1ci [(i+1)/(n1 )], σ1i ) and m2i (loop2ci [(i+1)/(n2 )], σ2i )
respectively. Then there are two possibilities.
i. S1 and S2 terminate when started in states m1i (loop1ci [(i+
•
(s2 , m2i ),
1)/(n1 )], σ1i ) and
m2i (loop2ci [(i + 1)/(n2 )], σ2i ) respectively
∗
By similar argument in base case, evaluations of the predicate
(S1 ; s1 , m1i (loop1ci [(i + 1)/(n1 )], σ1i )) →
expression of s1 and s2 w.r.t value stores σ1i and σ2i produce
(s1 , m1i+1 (f1 , loop1ci+1 , σ1i+1 )) and
same value. Then there are three possibilities:
∗
(S2 ; s1 , m2i (loop2ci [(i + 1)/(n2 )], σ2i )) →
(a) E ′ JeKσ1i = E ′ JeKσ2i = (error, ∗).
2i+1
(s2 , m2i+1 (f2 , loopc , σ2i+1 )) such that all of the folThen the execution of s1 proceeds as follows.
lowing
holds:
(s1 , m1i (f1 , σ1i ))
• f1 = f2 = 0; and
= (whilehn1 i (e) {S1 }, m1i (f1 , σ1i ))
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
→(whilehn1 i ((error, ∗)) {S1 }, m1i (f1 , σ1i )) by the EEval’ rule
• ∀y ∈ TVar(s), σ1i+1 (y) = σ2i+1 (y), and
→(whilehn1 i (0) {S1 }, m1i (1/f1 )) by the ECrash rule
k
• in any state in the execution
→(whilehn1 i (0) {S1 }, m1i (1/f1 )), for any k ≥ 0, by the Crash rule.
∗
(s1 , m1i ) → (s1 , m1i+1 (loop1ci+1 , σ1i+1 )), the
Similarly, the execution of s2 started in the state m2i (σ2i )
loop counter of s1 is less than or equal to i + 1.
does not terminate.
• in any state in the executions
The loop counters for s1 and s2 are less than i + 1:
∗
′
∗
(s2 , m2i ) → (s2 , m2i+1 (loop2ci+1 , σ2i+1 )), the
∀m′1 m′2 such that (s1 , m1 ) → (S1′ , m′1 (loop1c )) and
′
∗
loop counter of s2 is less than or equal to i + 1.
(s2 , m2 ) → (S2′ , m′2 (loop2c )),
With the hypothesis IH, there are two configurations
1′
2′
loopc (n1 ) < i + 1 and loopc (n2 ) < i + 1.
(s1 , m1i+1 ) and (s2 , m2i+1 ) reachable from (s1 , m1 )
Besides, s1 and s2 both do not terminate when started in
and (s2 , m2 ), respectively, in which crash flags are not
states m1 and m2 ,
set, the loop counters of s1 and s2 are equal to i + 1
k
k
∀k > 0, (s1 , m1 ) → (S1k , m1k ) and (s2 , m2 ) → (S2k , m2k )
and value stores agree on the values of TVar(s) and, for
∗
in which S1k 6= skip, S2k 6= skip.
every state in executions (s1 , m1 ) → (s1 , m1i+1 ) and
′
′
(b) E JeKσ1i = E JeKσ2i = (0, vof )
∗
(s2 , m2 ) → (s2 , m2i+1 ) the loop counters for s1 and
The execution from (s1 , m1i (loop1ci , σ1i )) proceeds as fols
2 are less than or equal to i + 1 respectively:
lows.
∃(s1 , m1i+1 ) (s2 , m2i+1 ) :
∗
∗
(s1 , m1i (loop1ci , σ1i ))
(s1 , m1 ) → (s1 , m1i+1 (f1 , loop1ci+1 , σ1i+1 ))∧(s2 , m2 ) →
= (whilehn1 i (e) {S1 }, m1i (loop1ci , σ1i ))
2i+1
(s2 , m2i+1 (f2 , loopc , σ2i+1 )) where
→(whilehn1 i ((0, vof )) {S1 }, m1i (loop1ci , σ1i )) by rule EEval’
• f1 = f2 = 0; and
1i
→(whilehn1 i (0) {S1 }, m1i (loopc , σ1i ))
• loop1c i+1 (n1 ) = loop2c i+1 (n2 ) = i + 1; and
by rule E-Oflow1 or E-Oflow2
• ∀x ∈ TVar(s), σ1i+1 (x) = σ2i+1 (x); and
→(skip, m1i (loop1ci [0/(n1 )], σ1i )) by the Wh-F2 rule.
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) →
By the hypothesis IH, the loop counter of s1 and s2 in any
′
∗
(s1 , m1i+1 (loop1ci+1 , σ1i+1 )), loop1c (n1 ) ≤ i + 1;
configuration in executions (s1 , m1 ) → (s1 , m1i (loop1ci , σ1i ))
and
∗
′
∗
∗
and (s2 , m2 ) → (s2 , m2i (loop2ci , σ2i )) respectively are
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
less than or equal to i,
′
′
∗
∗
(s2 , m2i+1 (loop2ci+1 , σ2i+1 )), loop2c (n2 ) ≤ i + 1.
∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i (loop1ci ,
′
ii.
S
and
S2 do not terminate when started in states
1
σ1i )), loop1c (n1 ) ≤ i; and
2i
1i
(loop
m
1
′
i
c [(i + 1)/(n1 )], σ1i ) and m2i (loopc [(i +
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i (loop2ci ,
)
respectively:
1)/(n
)],
σ
2
2i
′
k
σ2i )), loop2c (n2 ) ≤ i.
∀k > 0, (S1 , m1i (loop1ci [(i + 1)/(n1 )], σ1i )) →
1
Therefore, s1 and s2 both terminate and the loop counter
1
(S1k , m11k (loopc k , σ11k )) and
of s1 and s2 in any state in executions respectively are less
k
than i + 1.
(S2 , m2i (loop2ci [(i + 1)/(n2 )], σ2i )) →
21
′
′
/ {0, error};
(c) E JeKσ1i = E JeKσ2i = (v, vof ) where v ∈
(S2k , m21k (loopc k , σ21k )) in which S1k 6= skip, S2k 6=
The execution from (s1 , m1i (loop1ci , σ1i )) proceeds as folskip.
lows.
By our assumption of unique loop labels, s1 ∈
/ S1 .
11
(s1 , m1i (loop1ci , σ1i ))
Then, ∀k > 0, loopc k (n1 ) =
loop1ci [(i + 1)/(n1 )](n1 ) = i + 1. Similarly, ∀k >
= (whilehn1 i (e) {S1 }, m1i (loop1ci , σ1i ))
21
→(whilehn1 i ((v, vof )) {S1 }, m1i (loop1ci , σ1i )) by rule EEval’
0, loopc k (n2 ) =
1i
2
→(whilehn1 i ((v, vof )) {S1 }, m1i (loopc , σ1i )) by rule EEval’
loopci [(i + 1)/(n2 )](n2 ) = i + 1. In addition, by
→(whilehn1 i (v) {S1 }, m1i (loop1ci , σ1i ))
Lemma E.2,
k
by rule E-Oflow1 or E-Oflow2
∀k > 0, (S1 ; s1 , m1i (loop1ci [(i + 1)/(n1 )], σ1i )) →
11
→(S1 ; whilehn1 i (e) {S1 }, m1i (loop1ci [(i + 1)/(n1 )],
k
(S1k ; s1 , m11k (loopc , σ11k )) and (S2 ; s2 , m2 (loop2c [(i+
σ1i )) by rule Wh-T.
21
k
1)/(n2 )], σ2 )) → (S2k ; s2 , m21k (loopc k , σ21k )) in
2
2i
Similarly, (s2 , m2i (loopc , σ2i )) → (S2 ; whilehn2 i (e){S2 },
which S1k 6= skip, S2k 6= skip.
m2i (loop2ci [(i + 1)/(n2 )], σ2i )).
In summary, the loop counter of s1 and s2 are less than
By similar argument in base case, the executions of S1
equal to i + 1, and s1 and s2 do not terminate such that
and S2 terminate in the same way when started in states
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i ),
′

loop1c (n1 ) ≤ i; and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) →
′
2
loopc (n2 ) ≤ i.

34

2015/9/14

there are no configurations (s1 , m1i+1 ) and (s2 , m2i+1 )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which crash flags are not set, the loop counters of s1 and
s2 are equal to i + 1 and value stores agree on values of
variables in TVar(s).

tively, in which both crash flags are not set, the loop counters of s1 and s2 are equal to j and value stores agree on
the values of variables in TVar(s), and for every state in ex∗
∗
ecution (s1 , m1 ) → (s1 , m1j ) or (s2 , m2 ) → (s2 , m2j ),
the loop counters for s1 and s2 are less than or equal to j
respectively:
∃(s1 , m1j ) (s2 , m2j ) :
1
∗
(s1 , m1 ) → (s1 , m1j (f1 , loopcj , σ1j ))∧
2j
∗
(s2 , m2 ) → (s2 , m2j (f2 , loopc , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ TVar(s) : σ1j (x) = σ2j (x); and

Corollary 5.3. Let s1 = “whilehn1 i (e){S1 }” and
s2 = “whilehn2 i (e){S2 }” be two while statements respectively,
with the same set of the termination deciding variables, TVar(s1 ) =
TVar(s2 ) = TVar(s), whose bodies S1 and S2 satisfy the proof
rule of equivalently computation of variables in TVar(s), ∀x ∈
TVar(s) : (S1 ) ≡S
x (S2 ), and whose bodies S1 and S2 terminate
in the same way when started in states with crash flags not set and
agreeing on values of variables in TVar(S1 ) ∪ TVar(S2 ):
∀mS1 (fS1 , σS1 ), mS2 (fS2 , σS2 ) :
(((∀z ∈ TVar(S1 )∪TVar(S2 )), σS1 (z) = σS2 (z))∧(fS1 = fS2 =
0)) ⇒ (S1 , mS1 (fS1 , σS1 )) ≡H (S2 , mS2 (fS2 , σS2 )).
If s1 and s2 start in the state m1 (f1 , loop1c , σ1 ) and m2 (f2 , loop2c ,
, σ2 ) respectively in which crash flags are not set, f1 =
f2 = 0, s1 and s2 have not already executed, loop1c (n1 ) =
loop2c (n2 ) = 0, value stores σ1 and σ2 agree on values of variables
in TVar(s), ∀x ∈ TVar(s), σ1 (x) = σ2 (x), then s1 and s2
terminate in the same way:
∗

′

∗

∗

∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1j ),
′
loop1c (n1 ) ≤ j; and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2j ),
′
loop2c (n2 ) ≤ j.

k

2. s1 and s2 both do not terminate, ∀k > 0, (s1 , m1 ) →
k
(S1k , m1k ), (s2 , m2 ) → (S2k , m2k ) where S1k 6= skip,
S2k 6= skip such that one of the following holds:
(a) For any positive integer i, there are two configurations
(s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and
(s2 , m2 ), respectively, in which both crash flags are not
set, the loop counters of s1 and s2 are equal to i and
value stores agree on the values of variables in TVar(s),
∗
and for every state in execution (s1 , m1 ) → (s1 , m1i ) or
∗
(s2 , m2 ) → (s2 , m2i ), the loop counters for s1 and s2 are
less than or equal to i respectively:
∀i > 0 ∃(s1 , m1i ) (s2 , m2i ) :
∗
(s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
(s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
• f1 = f2 = 0; and
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ TVar(s) : σ1i (x) = σ2i (x); and
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i ),
1′
loopc (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i ),
2′
loopc (n2 ) ≤ i;
(b) The loop counters for s1 and s2 are less than a positive
integer i and less than or equal to i − 1 such that all of the
following hold:

∗

1. s1 and s2 both terminate, (s1 , m1 ) → (skip, m′1 ), (s2 , m2 ) →
(skip, m′2 ).
k

2. s1 and s2 both do not terminate, ∀k > 0, (s1 , m1 ) →
k
(S1k , m1k ), (s2 , m2 ) → (S2k , m2k ) where S1k 6= skip,
S2k 6= skip.
This is from Lemma 5.15 immediately.
Lemma5.16 is necessary only for showing the same I/O sequence in the next section.
Lemma 5.16. Let s1 = “whilehn1 i (e){S1 }” and
s2 = “whilehn2 i (e){S2 }” be two while statements in program
P1 and P2 respectively with the same set of termination deciding
variables, TVar(s1 ) = TVar(s2 ) = TVar(s), whose bodies S1 and
S2 satisfy the proof rule of equivalently computation of variables
in TVar(s), ∀x ∈ TVar(s) : S1 ≡S
x S2 and whose bodies S1
and S2 terminate in the same way in executions when started in
states with crash flags not set and agreeing on values of variables
in TVar(S1 ) ∪ TVar(S2 ):
∀mS1 (fS1 , σS1 ) mS2 (fS2 , σS2 ) :
(((∀z ∈ TVar(S1 )∪TVar(S2 )), σS1 (z) = σS2 (z))∧(fS1 = fS2 =
0)
⇒ (S1 , mS1 (fS1 , σS1 )) ≡H (S2 , mS2 (fS2 , σS2 )).
If s1 and s2 start in the state m1 (f1 , loop1c , σ1 ) and m2 (f2 , loop2c ,
, σ2 ) respectively in which crash flags are not set, f1 = f2 = 0, s1
and s2 have not already executed, loop1c (n1 ) = loop2c (n2 ) = 0,
value stores σ1 and σ2 agree on values of variables in TVar(s),
∀x ∈ TVar(s), σ1 (x) = σ2 (x), one of the following holds:

′

∗

• ∃i > 0, ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )),
∗

′

′

(s2 , m2 ) → (S2 , m′2 (loop2c )) where loop1c (n1 ) < i,
′
loop2c (n2 ) < i;
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which both crash flags are not set, the
loop counters of s1 and s2 are equal to j and value
stores agree on the values of variables in TVar(s), and
∗
for every state in execution (s1 , m1 ) → (s1 , m1j ) or
∗
(s2 , m2 ) → (s2 , m2j ), the loop counters for s1 and s2
are less than or equal to j respectively:
∃(s1 , m1j ) (s2 , m2j ) :
1
∗
(s1 , m1 ) → (s1 , m1j (f1 , loopcj , σ1j ))∧
2
∗
(s2 , m2 ) → (s2 , m2j (f2 , loopcj , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ TVar(s) : σ1j (x) = σ2j (x); and

1. s1 and s2 both terminate and the loop counters of s1 and s2
are less than a positive integer i and less than or equal to i − 1:
∗
∗
(s1 , m1 ) → (skip, m′1 ), (s2 , m2 ) → (skip, m′2 ) where both of
the following hold:
• The loop counters of s1 and s2 are less than a positive
integer i:
∃i > 0 ∀m′1 m′2 :
′
′
∗
(s1 , m1 ) → (S1′ , m′1 (loop1c )), loop1c (n1 ) < i and
′
′
∗
(s2 , m2 ) → (S2′ , m′2 (loop2c )), loop2c (n2 ) < i.
• ∀0 < j < i, there are two configurations (s1 , m1j ) and
(s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ), respec-

35

2015/9/14

′

∗

∗

2

∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1j ),
′
loop1c (n1 ) ≤ j; and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2j ),
2′
loopc (n2 ) ≤ j;

Similarly, (s2 , m2 (loop2c , σ2 )) → (skip, m2 (loop2c \ {(n2 ,
, ∗)})). Then the lemma holds because of the initial configuration (s1 , m1 (loop1c , σ1 )) and (s2 , m2 (loop2c , σ2 )).
(b) i > 1.
Because s1 and s2 terminate, i is the smallest positive integer such that the loop counters of s1 and s2 are less than
i, by Lemma 5.15, ∀0 < j < i, there are two configurations (s1 , m1j ) and (s2 , m2j ) reachable from (s1 , m1 )
and (s2 , m2 ), respectively, in which both crash flags are
not set, the loop counters of s1 and s2 are equal to j and
value stores agree on the values of variables in TVar(s),
∗
and for every state in execution, (s1 , m1 ) → (s1 , m1j ) or
∗
(s2 , m2 ) → (s2 , m2j ) the loop counters for s1 and s2 are
less than or equal to j respectively. With the initial configuration (s1 , m1 ) and (s2 , m2 ), the lemma holds.
2. s1 and s2 both do not terminate. There are three possibilities.
(a) ∀i > 0, there are two configurations (s1 , m1i ) and (s2 , m2i )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which both crash flags are not set, the loop counters of
s1 and s2 are equal to i and value stores agree on the values
of variables in TVar(s), and for every state in execution,
∗
∗
(s1 , m1 ) → (s1 , m1i ) or (s2 , m2 ) → (s2 , m2i ) the loop
counters for s1 and s2 are less than or equal to i respectively.
(b) The loop counters of s1 and s2 are less than a positive
integer i.
Let i be the smallest positive integer such that there is no
positive integer j < i that the loop counters of s1 and s2 are
less than the positive integer j. This case occurs when s1
and s2 finish the full (i−1)th iterations and both executions
raise an exception in the evaluation of the loop predicate of
s1 and s2 for the ith time. There are further two possibilities.
i. i = 1.
In proof of Lemma 5.15, evaluations of the predicate
expression of s1 and s2 raise an exception w.r.t value
stores σ1 and σ2 . The lemma holds.
ii. i > 1. By the assumption of initial states (s1 , m1 )
and (s2 , m2 ), when j = 0, initial states (s1 , m1 ) and
(s2 , m2 ) have crash flags not set, the loop counters of
s1 and s2 are zero and value stores agree on values of
variables of s1 and s2 .
By Lemma 5.15, ∀0 < j < i, there are two configurations (s1 , m1j ) and (s2 , m2j ) reachable from (s1 , m1 )
and (s2 , m2 ), respectively, in which both crash flags are
not set, the loop counters of s1 and s2 are equal to j and
value stores agree on the values of variables in TVar(s),
∗
and for every state in execution, (s1 , m1 ) → (s1 , m1j )
∗
or (s2 , m2 ) → (s2 , m2j ) the loop counters for s1 and
s2 are less than or equal to j respectively. With the initial
configuration (s1 , m1 ) and (s2 , m2 ), the lemma holds.
(c) The loop counters of s1 and s2 are less than or equal to a
positive integer i.
Let i be the smallest positive integer such that the loop
counters of s1 and s2 are less than or equal to the positive
integer i. There are two possibilities.
i. i = 1.
In the proof of Lemma 5.15, the execution of s1 proceeds as follows:
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c , σ1 ))
→(whilehn1 i ((v, vof )) {S1 }, m1 (loop1c , σ1 )) where v 6= 0
by rule EEval’
→(whilehn1 i (v) {S1 }, m1 (loop1c , σ1 ))
by rule E-Oflow1 or E-Oflow2

(c) The loop counters for s1 and s2 are less than or equal to
some positive integer i such that all of the following hold:
′

∗

• ∃i > 0 ∀m′1 m′2 : (s1 , m1 ) → (S1 , m′1 (loop1c )),
∗

′
(S2 , m′2 (loop2c ))

′
loop1c (n1 )

where
≤ i,
(s2 , m2 ) →
′
loop2c (n2 ) ≤ i;
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which both crash flags are not set, the
loop counters of s1 and s2 are equal to j and value
stores agree on the values of variables in TVar(s), and
∗
for every state in execution (s1 , m1 ) → (s1 , m1j ) or
∗
(s2 , m2 ) → (s2 , m2j ), the loop counters for s1 and s2
are less than or equal to j respectively:
∃(s1 , m1j ) (s2 , m2j ) :
1
∗
(s1 , m1 ) → (s1 , m1j (f1 , loopcj , σ1j ))∧
2j
∗
(s2 , m2 ) → (s2 , m2j (f2 , loopc , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ TVar(s) : σ1j (x) = σ2j (x); and
∗

′

∗

∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1j ),
′
loop1c (n1 ) ≤ j; and
′
∗
∗
∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2j ),
2′
loopc (n2 ) ≤ j;
• There are no configurations (s1 , m1i ) and (s2 , m2i )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which crash flags are not set, the loop counters of s1 and
s2 are equal to i, and value stores agree on the values of
variables in TVar(s):
∄(s1 , m1i ) (s2 , m2i ) :
∗
(s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
(s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
f1 = f2 = 0; and
loop1ci (n1 ) = loop2ci (n2 ) = i; and
∀x ∈ TVar(s) : σ1i (x) = σ2i (x).
Proof. From lemma 5.15, we have s1 and s2 terminate in the same
way when started in states m1 and m2 respectively. Then there are
two big cases.
1. s1 and s2 both terminate.
Let i be the smallest integer such that the loop counters of s1
and s2 are less than i in the executions. Then there are two
possibilities.
(a) i = 1.
In the proof of Lemma 5.15, the evaluation of the loop
predicate of s1 and s2 produce zero w.r.t value stores, σ1
and σ2 . Then the execution of s1 proceeds as follows:
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c ))
→(whilehn1 i ((0, vof )) {S1 }, m1 (loop1c )) by rule EEval’
→(whilehn1 i (0) {S1 }, m1 (loop1c ))
by rule E-Oflow1 or E-Oflow2
→(skip, m1 (loop1c \ {(n1 , ∗)}))
by rule Wh-F1 or Wh-F2.

36

2015/9/14

→(S1 ; whilehn1 i (e) {S1 }, m1 (loop1c [1/(n1 )], σ1 ))
by rule Wh-T .
The execution of s2 proceeds to
(S2 ; whilehn2 i (e) {S2 }, m2 (loop2c [1/(n2 )], σ2 )). In
addition, executions of S1 and S2 do not terminate when
started in states m1 (loop1c [1/(n1 )], σ1 ) and
m2 (loop2c [1/(n2 )], σ2 ). By Lemma E.2, executions of
s1 and s2 do not terminate.
ii. i > 1.
In the proof of Lemma 5.15, when started in the state
(s1 , m1i−1 (f1 , loop1ci−1 , σ1i−1 )) the execution of s1
proceeds as follows:
(s1 , m1i−1 (f1 , loop1ci−1 , σ1i−1 ))
= (whilehn1 i (e) {S1 }, m1i−1 (f1 , loop1ci−1 , σ1i−1 ))
→(whilehn1 i ((v, vof )) {S1 }, m1i−1 (f1 , loop1ci−1 , σ1i−1 ))
by rule EEval’
→(whilehn1 i (v) {S1 }, m1i−1 (f1 , loop1ci−1 , σ1i−1 ))
by rule E-Oflow1 or E-Oflow2
→(S1 ; whilehn1 i (e) {S1 },
m1i−1 (f1 , loop1ci−1 [i/(n1 )], σ1i−1 ))
by rule Wh-T1 or Wh-T2.
The execution of s2 proceeds to (S2 ; whilehn2 i (e) {S2 },
m2i−1 (f2 , loop2ci−1 [i/(n2 )], σ2i−1 )). In addition, executions of S1 and S2 do not terminate when started in
states m1i−1 (f1 , loop1ci−1 [i/(n1 )], σ1i−1 ) and
m2i−1 (f2 , loop2ci−1 [i/(n2 )], σ2i−1 ). By Lemma E.2,
executions of s1 and s2 do not terminate.

1. Impo (S) = {idIO }, if (∀e : “output e” ∈
/ S);
2. Impo (“output e”) = {idIO } ∪ Use(e);
3. Impo (“If (e) then {St } else {Sf }”) = Use(e) ∪ Impo (St ) ∪
Impo (Sf ) if (∃e : “output e” ∈ S);
4. Impo (“whilehni (e){S ′′ }”) = Imp(“whilehni (e){S ′′ }”, {idIO })
if (∃e : “output e” ∈ S ′′ );
5. For k > 0, Impo (s1 ; ...; sk ; sk+1 ) = Imp(s1 ; ...; sk , Impo (sk+1 ))
if (∃e : “output e” ∈ sk+1 );
6. For k > 0, Impo (s1 ; ...; sk ; sk+1 ) = Impo (s1 ; ...; sk ) if (∀e :
“output e” ∈
/ sk+1 );
Definition 22. (Termination deciding variables relative to output) The termination deciding variables in a statement sequence S
relative to output, written TVaro (S), are listed as follows:
1. TVaro (S) = ∅ if (∀e : “output e” ∈
/ S);
2. TVaro (“output e”) = Err(e);
3. TVaro (“If (e) then {St } else {Sf }”) = Use(e) ∪ TVaro (St ) ∪
TVaro (Sf ) if (∃e : “output e” ∈ S);
4. TVaro (“whilehni (e){S ′′ }”) = TVar(“whilehni (e){S ′′ }”) if
(∃e : “output e” ∈ S ′′ );
5. For k > 0, TVaro (s1 ; ...; sk ; sk+1 ) = TVar(s1 ; ...; sk )
∪Imp(s1 ; ...; sk , TVaro (sk+1 )) if (∃e : “output e” ∈ sk+1 );
6. For k > 0, TVaro (s1 ; ...; sk ; sk+1 ) = TVaro (s1 ; ...; sk ) if
(∀e : “output e” ∈
/ sk+1 );
Definition 23. (Output deciding variables) The output deciding
variables in a statement sequence S are Impo (S) ∪ TVaro (S),
written OVar(S).
The condition of the behavioral equivalence is defined recursively. The base case is for two same output statements or two
statements where the output sequence variable is not defined. The
inductive cases are syntax directed considering the syntax of compound statements and statement sequences.

5.4 Behavioral equivalence
We now propose a proof rule under which two programs produce
the same output sequence, namely the same I/O sequence till any
ith output value. We care about the I/O sequence due to the possible
crash from the lack of input. We start by giving the definition of the
same output sequence, then we describe the proof rule under which
two programs produce the same output sequence, finally we show
that our proof rule ensures same output together with the necessary
auxiliary lemmas. We use the notation “Out(σ)” to represent the
output sequence in value store σ, the I/O sequence σ(idIO ) till the
rightmost output value. Particularly, when there is no output value
in the I/O sequence σ(idIO ), Out(σ) = ∅.

Definition 24. (proof rule of behavioral equivalence) Two statement sequences S1 and S2 satisfy the proof rule of behavioral
equivalence, written S1 ≡S
O S2 , iff one of the following holds:

1. S1 and S2 are one statement and one of the following holds:
(a) S1 and S2 are simple statement and one of the following
holds:
i. S1 and S2 are not output statement, ∀e1 e2 :
(“output e1 ” 6= S1 ) ∧ (“output e2 ” 6= S2 ); or
ii. S1 = S2 = “output e”.
(b) S1 = “If (e) then {S1t } else {S1f }”, S2 = “If (e) then {S2t } else
{S2f }” and all of the following hold:
• There is an output statement in S1 and S2 ,
∃e1 e2 : (“output e1 ” ∈ S1 ) ∧ (“output e2 ” ∈ S2 );
f
f
t
S
• (S1t ≡S
O S2 ) ∧ (S1 ≡O S2 );
′′
(c) S1 = “whilehn1 i (e) {S1 }” and S2 = “whilehn2 i (e) {S2′′ }”
and all of the following hold:
• There is an output statement in S1 and S2 ,
∃e1 e2 : (“output e1 ” ∈ S1 ) ∧ (“output e2 ” ∈ S2 );
′′
• S1′′ ≡S
O S2 ;
′′
′′
• S1 and S2 have equivalent computation of OVar(S1 ) ∪
OVar(S2 );
• S1′′ and S2′′ satisfy the proof rule of termination in the
′′
same way, S1′′ ≡S
H S2 ;
(d) Output statements are not in both S1 and S2 ,
∀e1 e2 : (“output e1 ” ∈
/ S1 ) ∧ (“output e2 ” ∈
/ S2 ).
2. S1 and S2 are not both one statement and one of the following
holds:

Definition 20. (Same output sequence) Two statement sequences
S1 and S2 produce the same output sequence when started in
states m1 and m2 respectively, written (S1 , m1 ) ≡O (S2 , m2 ), iff
∗
∗
∀m′1 m′2 such that (S1 , m1 ) → (S1′ , m′1 (σ1′ )) and (S2 , m2 ) →
′
′
′
′′
′′
(S2 , m2 (σ2 )), there are states m1 m2 reachable from initial
∗
∗
states m1 and m2 , (S1 , m1 ) → (S1′′ , m′′1 (σ1′′ )) and (S2 , m2 ) →
′′
′′
′′
′′
′
′′
(S2 , m2 (σ2 )) so that Out(σ2 ) = Out(σ1 ) and Out(σ1 ) = Out(σ2′ ).
5.4.1 Proof rule for behavioral equivalence
We show the proof rules of the behavioral equivalence. The output sequence produced in executions of a statement sequence S
depends on values of a set of variables in the program, the output
deciding variables OVar(S). The output deciding variables are of
two parts: TVaro (S) are variables affecting the termination of executions of a statement sequence; Impo (S) are variables affecting
values of the I/O sequence produced in executions of a statement
sequence. The definitions of TVaro (S) and Impo (S) are shown in
Definition 21 and 22.
Definition 21. (Imported variables relative to output) The imported variables in one program S relative to output, written
Impo (S), are listed as follows:
37

2015/9/14

(a) S1 = S1′ ; s1 and S2 = S2′ ; s2 , and all of the following hold:
′
• S1′ ≡S
O S2 ;
′
• S1 and S2′ have equivalent computation of OVar(s1 ) ∪
OVar(s2 );
• S1′ and S2′ satisfy the proof rule of termination in the
′
same way: S1′ ≡S
H S2 ;
• There is an output statement in both s1 and s2 ,
∃e1 e2 : (“output e1 ” ∈ s1 ) ∧ (“output e2 ” ∈ s2 );
• s1 ≡S
O s2 ;
(b) There is no output statement in the last statement in S1 or
S2 :

(S1 = S1′ ; s1 ) ∧ (S1′ ≡S
/ s1 )
O S2 ) ∧ (∀e : “output e” ∈
′
∨ (S2 = S2′ ; s2 )∧(S1 ≡S
/ s2 ) ;
O S2 )∧(∀e : “output e” ∈

→(output 0, m1 (1/f)) by the ECrash rule
i
→(output 0, m1 (1/f)) for any i > 0 by the Crash rule.
Similarly, the execution of S2 does not terminate and
there is no change to I/O sequence in execution. Because
σ1 (idIO ) = σ2 (idIO ), then the output sequence in value
stores σ1 and σ2 are same, Out(σ1 ) = Out(σ2 ), the theorem holds.
(b) E JeKσ1 = E JeKσ2 6= (error, vof )
S1 and S2 satisfy the proof rule of equivalent computation
of I/O sequence variable and their initial states agree on the
values of the imported variables relative to I/O sequence
variable. By Theorem 2, S1 and S2 produce the same output
sequence after terminating execution when started in state
m1 (σ1 ) and m2 (σ2 ) respectively. The theorem holds.

5.4.2 Soundness of the proof rule for behavioral equivalence

Induction step.
The hypothesis IH is that Theorem 5 holds when size(S1 ) +
size(S2 ) = k ≥ 2.
We show Theorem 5 holds when size(S1 ) + size(S2 ) = k + 1.
The proof is a case analysis according to the cases in the definition
of the proof rule of behavioral equivalence.

We show that two statement sequences satisfy the proof rule of the
behavioral equivalence and their initial states agree on values of
their output deciding variables, then the two statement sequences
produce the same output sequence when started in their initial
states.
Theorem 5. Two statement sequences S1 and S2 satisfy the proof
rule of the behavioral equivalence, S1 ≡S
O S2 . If S1 and S2 start in
states m1 (f1 , σ1 ) and m2 (f2 , σ2 ) where both of the following hold:

1. S1 and S2 are one statement and one of the following holds:
(a) S1 = “If(e) then {S1t } else {S1f }” and S2 = “If(e) then
{S2t } else {S2f }” and all of the following hold:

• Crash flags are not set, f1 = f2 = 0;
• Value stores σ1 and σ2 agree on values of the output decid-

• There is an output statement in S1 and S2 : ∃e1 e2 :

ing variables of S1 and S2 , ∀id ∈ OVar(S1 ) ∪ OVar(S2 ) :
σ1 (id) = σ2 (id);

(“output e1 ” ∈ S1 ) ∧ (“output e2 ” ∈ S2 );

t
• S1t ≡S
O S2 ;
f
S
• S1 ≡O S2f ;

then S1 and S2 produce the same output sequence,
(S1 , m1 ) ≡O (S2 , m2 ).

By Lemma 5.17, {idIO } ∈ Impo (S1 ). By assumption,
value stores σ1 and σ2 agree on the value of the I/O sequence variable and the I/O sequence variable, σ1 (idIO ) =
σ2 (idIO ).
We show that the evaluations of the predicate expression of
S1 and S2 w.r.t. initial value store σ1 and σ2 produce the
same value. We need to show that value stores σ1 and σ2
agree on values of variables used in the predicate expression
e of S1 and S2 . Because the output sequence is defined
in S1 , by the definition of imported variables relative to
output, Impo (S1 ) = Use(e) ∪ Impo (S1t ) ∪ Impo (S1f ). Thus,
Use(e) ⊆ OVar(S1 ). By assumption, value stores σ1 and σ2
agree on values of variables used in the predicate expression
e of S1 and S2 , ∀x ∈ Use(e) : σ1 (x) = σ2 (x). By
Lemma D.1, the evaluations of the predicate expression of
S1 and S2 w.r.t. pairs value stores, σ1 and σ2 generate
the same value, E ′ JeKσ1 = E ′ JeKσ2 . Then there are two
possibilities.
i. E ′ JeKσ1 = E ′ JeKσ2 = (error, vof ).
Then the execution of S1 proceeds as follows:

The proof is by induction on the sum of program size of S1 and
S2 , size(S1 ) + size(S2 ) and is a case analysis based on S1 ≡S
O S2 .

Proof. The proof is by induction on the sum of program size of
S1 and S2 , size(S1 ) + size(S2 ) and is a case analysis based on
S1 ≡S
O S2 .
Base case.
S1 and S2 are simple statement. There are two cases according
to the proof rule of behavioral equivalence because stacks are not
changed in executions of S1 and S2 .
1. S1 and S2 are not output statement, ∀e1 e2 : (“output e1 ” 6=
S1 ) ∧ (“output e2 ” 6= S2 );
By the definition of imported variables relative to output,
Impo (S1 ) = Impo (S2 ) = {idIO }. By assumption, initial value
stores σ1 and σ2 agree on the value of the I/O sequence variable,
σ1 (idIO ) = σ2 (idIO ). By definition, Out(σ1 ) = Out(σ2 ).
By Lemma 5.23, in any state m′1 reachable from m1 , the output
∗
sequence in m′1 is same as that in m1 , ∀m′1 : ((S1 , m1 (σ1 )) →
′
′
′
′
(S1 , m1 (σ1 ))) ⇒ (Out(σ1 ) = Out(σ1 )). Similarly, for any
state m′2 reachable from m2 , the output sequence in m′2 is
same as that in m2 . The theorem holds.
2. S1 = S2 = “output e”.
We show that the expression e evaluates to the same value w.r.t
value stores, σ1 , σ2 . By the definition of imported variables relative to output, Impo (S1 ) = Impo (S2 ) = Use(e) ∪ {idIO }.
Then ∀x ∈ Use(e) ∪ {idIO } : σ1 (x) = σ2 (x) by assumption. By Lemma D.1, E JeKσ1 = E JeKσ2 . Then, there are two
possibilities.
(a) E JeKσ1 = E JeKσ2 = (error, vof ).
The execution of S1 proceeds as follows.
(output e, m1 (σ1 ))
= (output (error, vof ), m1 (σ1 )) by the rule EEval’

(If(e) then {S1t } else {S1f }, m1 (σ1 ))
→(If((error, vof )) then {S1t } else {S1f }, m1 (σ1 ))
by the EEval’ rule
→(If(0) then {S1t } else {S1f }, m1 (1/f))
by the ECrash rule
i
→(If(0) then {S1t } else {S1f }, m1 (1/f))
for any i > 0, by the Crash rule.
Similarly, the execution of S2 does not terminate and
does not redefine I/O sequence. Because σ1 (idIO ) =
σ2 (idIO ), the theorem holds.
ii. E ′ JeKσ1 = E ′ JeKσ2 6= (error, vof ).
W.l.o.g., E ′ JeKσ1 = E ′ JeKσ2 = (0, vof ). The execution
of S1 proceeds as follows.

38

2015/9/14

(If(e) then {S1t } else {S1f }, m1 (σ1 ))
→(If((0, vof )) then {S1t } else {S1f }, m1 (σ1 ))
by the EEval rule
→(If(0) then {S1t } else {S1f }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1f , m1 (σ1 )) by the If-F rule.

By Lemma 5.19, Impo (S1 ) = Impo (S2 ). By definition, Impo (S1 ) = Imp(S1 , {idIO }) and Impo (S2 ) =
Imp(S2 , {idIO }). Thus, Imp(S1 , {idIO }) = Imp(S2 , {idIO }).
• The loop body of S1 and S2 produce the same output sequence when started in states with crash flags not set and
whose value stores agree on values of the out-deciding
variables of S1′′ and S2′′ , ∀mS1′′ (f′′1 , σ1′′ ) mS2′′ (f′′2 , σ2′′ ) :
((∀x ∈ OVar(S1′′ ) ∪ OVar(S2′′ ) : σ1′′ (x) = σ2′′ (x)) ∧
(f′′1 = f′′2 = 0)) ⇒ (S1′′ , mS1′′ (f′′1 , σ1′′ )) ≡O (S2′′ , mS2′′ (f′′2 , σ2′′ )).
Because size(S1 ) = 1 + size(S1′′ ), size(S2 ) = 1 +
size(S2′′ ), then size(S1′′ ) + size(S2′′ ) < k. By the hypothesis IH, the condition is satisfied.
By Corollary 5.5, S1 and S2 produce the same output sequence when started in states m1 (σ1 ) and m2 (σ2 ) respectively. The theorem holds.
(c) Output statements are not in both S1 and S2 , ∀e1 e2 :
(“output e1 ” ∈
/ S1 ) ∧ (“output e2 ” ∈
/ S2 ).
By Lemma 5.17, {idIO } ⊆ Impo (S1 ). By assumption,
value stores in initial states m1 , m2 agree on values of
the I/O sequence variable, σ1 (idIO ) = σ2 (idIO ). By
Lemma 5.23, the value of output sequence is same in m1
∗
and any state reachable from m1 , ∀m′1 m′2 : (S1 , m1 (σ1 )) →
∗
(S1′ , m′1 (σ1′ )) and (S2 , m2 (σ2 )) → (S2′ , m′2 (σ2′ )), Out(σ1′ ) =
Out(σ1 ) = Out(σ2 ) = Out(σ2′ ). The theorem holds.

Similarly, the execution of S2 proceeds to (S2f , m2 (σ2 ))
after two steps. By the hypothesis IH, we show that S1f
and S2f produce the same output sequence when started
in states m1 (σ1 ) and m2 (σ2 ). We need to show that all
required conditions are satisfied.
• size(S1f ) + size(S2f ) ≤ k.
By definition, size(S1 ) = 1 + size(S1t ) + size(S1f ).
Therefore, size(S1f ) + size(S2f ) < k.
• Value stores σ1 and σ2 agree on values of the outdeciding variables of S1f and S2f , ∀x ∈ OVar(S1f ) ∪
OVar(S2f ) : σ1 (x) = σ2 (x).
By the definition of imported variables relative to
output, Impo (S1f ) ⊆ Impo (S1 ). Besides, by the
definition of TVaro (S1 ), TVaro (S1f ) ⊆ TVaro (S1 ).
Then OVar(S1f ) ⊆ OVar(S1 ). Similarly, OVar(S2f ) ⊆
OVar(S2 ).By assumption, the value stores σ1 and σ2
agree on the values of the out-deciding variables of
S1f and S2f .
By the hypothesis IH, S1f and S2f produce the same
output sequence when started from state m1 (σ1 ) and
m2 (σ2 ) respectively. The theorem holds.
(b) S1 = “whilehn1 i (e) {S1′′ }” and S2 = “whilehn2 i (e) {S2′′ }”
and all of the following hold:

2. S1 = S1′ ; s1 and S2 = S2′ ; s2 are not both one statement and
one of the following holds:
(a) There is an output statement in both s1 and s2 , ∃e1 e2 :
(“output e1 ” ∈ s1 ) ∧ (“output e2 ” ∈ s2 ), and all of the
following hold:
′
• S1′ ≡S
O S2 ;
′
• S1 and S2′ satisfy the proof rule of termination in the

• There is an output statement in S1 and S2 : ∃e1 e2 :

(“output e1 ” ∈ S1 ) ∧ (“output e2 ” ∈ S2 );

′
same way: S1′ ≡S
H S2 ;
′
′
• S1 and S2 have equivalent computation of OVar(s1 ) ∪
OVar(s2 );
• s1 ≡S
O s2 ;

′′
• S1′′ ≡S
O S2 ;
• Both loop bodies satisfy the proof rule of termination in
′′
the same way: S1′′ ≡S
H S2 ;

• S1′′ and S2′′ have equivalent computation of OVar(S1 ) ∪

OVar(S2 );

By Corollary 5.5, we show that S1 and S2 produce the same
output sequence when started in states m1 (σ1 ) and m2 (σ2 )
respectively. We need to show that the required conditions
are satisfied.
• Crash flags are not set, f1 = f2 = 0.
• Value stores σ1 and σ2 agree on the values of the outdeciding variables of S1 and S2 , ∀x ∈ OVar(S1 ) ∪
OVar(S2 ) : σ1 (x) = σ2 (x).
• The loop counter value of S1 and S2 are zero in initial
loop counter, loop1c (n1 ) = loop2c (n2 ) = 0.
• The loop body of S1 and S2 satisfy the proof rule of
′′
termination in the same way, S1′′ ≡S
H S2 .
• The loop body of S1 and S2 satisfy the proof rule of
equivalent computation of OVar(S1 ) ∪ OVar(S2 ).
The above five conditions are from assumption.
• S1 and S2 have same set of termination deciding variables, TVar(S1 ) = TVar(S2 ).
By the definition of TVaro (S1 ), TVaro (S1 ) = TVar(S1 )
and TVaro (S2 ) = TVar(S2 ). By Lemma 5.21, TVaro (S1 ) =
TVaro (S2 ). Thus, TVar(S1 ) = TVar(S2 ).
• S1 and S2 have same set of imported variables relative
to the I/O sequence variable,
Imp(S1 , {idIO }) = Imp(S2 , {idIO }).
39

By the hypothesis IH, we show S1′ and S2′ produce the same
output sequence when started in states m1 (σ1 ) and m2 (σ2 )
respectively. We need to show that all required conditions
are satisfied.
• size(S1′ ) + size(S2′ ) < k.
By the definition of program size, size(s1 ) ≥ 1, size(s2 ) ≥
1. Then size(S1′ ) + size(S2′ ) < k.
• Value stores σ1 and σ2 agree on values of the outdeciding variables of S1′ and S2′ , ∀x ∈ OVar(S1′ ) ∪
OVar(S2′ ) : σ1 (x) = σ2 (x).
We show that TVaro (S1′ ) ⊆ TVaro (S1 ).
TVaro (S1′ )
⊆ TVar(S1′ ) by Lemma 5.20
⊆ TVaro (S1 ) by the definition of TVaro (S1 )
We show that Impo (S1′ ) ⊆ Impo (S1 ).
Impo (S1′ )
⊆ Imp(S1′ , {idIO }) (1) by Lemma 5.18
{idIO } ⊆ Impo (sk+1 ) (2) by Lemma 5.17
Combining (1) + (2)
Imp(S1′ , {idIO })
⊆ Imp(S1′ , Impo (s1 )) by Lemma C.2
= Impo (S1 ) by the definition of Impo (S).
2015/9/14

Similarly, OVar(S2′ ) ⊆ OVar(S2 ). By assumption, value
stores σ1 and σ2 agree on values of out-deciding variables of S1′ and S2′ .
By the hypothesis IH, S1′ and S2′ produce the same output
sequence when started in state m1 (σ1 ) and m2 (σ2 ) respectively.
We show that S1 and S2 produce the same output sequence
if s1 and s2 execute. We need to show that S1′ and S2′ terminate in the same way when started in states m1 (σ1 ) and
m2 (σ2 ) respectively. Specifically, we prove that the value
stores σ1 and σ2 agree on the values of termination deciding
variables of S1′ and S2′ . By definition, the termination deciding variables in S1′ are a subset of the termination deciding
variables relative to output, TVar(S1′ ) ⊆ TVaro (S1 ). Similarly, TVar(S2′ ) ⊆ TVaro (S2 ). By assumption, the value
stores σ1 and σ2 agree on the values of the termination deciding variables of S1′ and S2′ , ∀x ∈ TVar(S1′ )∪TVar(S2′ ) :
σ1 (x) = σ2 (x). By Theorem 4, S1′ and S2′ terminate in the
same way when started in state m1 (σ1 ) and m2 (σ2 ) respectively.
If S1′ and S2′ terminate when started in states m1 (σ1 ) and
m2 (σ2 ), by Lemma 5.14, S1′ and S2′ consume same amount
of input values. In addition, we show that value stores agree
on values of the out-deciding variables of s1 and s2 by
Theorem 2. We need to show that S1′ and S2′ start execution
in states agreeing on values of the imported variables in S1′
and S2′ relative to the out-deciding variables of s1 and s2 .
• Imp(TVaro (s1 ), S1′ ) ⊆ TVaro (S1 ).
This is by the definition of TVaro (S1 ).
• Imp(Impo (s1 ), S1′ ) = Impo (S1 ).
This is by the definition of Impo (S1 ).
Thus, the imported variables in S1′ relative to the outdeciding variables of s1 are a subset of the out-deciding
variables of S1 , Imp(S1′ , OVar(s1 )) ⊆ OVar(S1 ). Similarly, Imp(S2′ , OVar(s2 )) ⊆ OVar(S2 ). By Corollary 5.4,
s1 and s2 have same out-deciding variables, OVar(s1 ) =
OVar(s2 ). By assumption, S1′ and S2′ terminate when
∗
started in states m1 (σ1 ) and m2 (σ2 ), (S1′ , m1 (σ1 )) →
∗
′
′
′
′
′
(skip, m1 (σ1 )), (S2 , m1 (σ2 )) → (skip, m2 (σ2 )). By Theorem 2, value stores σ1′ and σ2′ agree on values of the outdeciding variables of s1 and s2 .
By the hypothesis IH again, s1 and s2 produce the same
output sequence when started in states m′1 (σ1′ ) and m′2 (σ2′ )
respectively. The theorem holds.
(b) There is no output statement in the last statement in S1 or
S2 : W.l.o.g., (∀e : “output e” ∈
/ s1 ) ∧ ((S1′ ) ≡S
O (S2 )).
By the hypothesis IH, we show that S1′ and S2 produce the
same output sequence when started in states m1 (σ1 ) and
m2 (σ2 ) respectively. We need to show that two required
conditions are satisfied.
• size(S1′ ) + size(S2 ) ≤ k.
size(s1 ) ≥ 1 by definition. Then size(S1′ ) + size(S2 ) ≤
k.
• ∀x ∈ OVar(S1′ ) ∪ OVar(S2 ) : σ1 (x) = σ2 (x).
By definition of TVaro (S)/Impo (S), TVaro (S1′ ) =
TVaro (S1 ), and Impo (S1′ ) = Impo (S1 ) Hence, ∀x ∈
OVar(S1′ ) ∪ OVar(S2 ) : σ1 (x) = σ2 (x).
Therefore, S1′ and S2 produce the same output sequence
when started in state m1 (σ1 ) and m2 (σ2 ) respectively,
(S1′ , m1 ) ≡O (S2 , m2 ) by the hypothesis IH.
When the execution of S1′ terminates, then the output sequence is not changed in the execution of s1 by Lemma 5.23.
The theorem holds.

5.4.3 Supporting lemmas for the soundness proof of
behavioral equivalence
We listed the lemmas and corollaries used in the proof of Theorem 5 below. The supporting lemmas are of two parts. One part is
various properties related to the out-deciding variables. The other
part is the proof that two loop statements produce the same output
sequence.
Lemma 5.17. For any statement sequence S, the I/O sequence
variable is in imported variable in S relative to output, idIO ∈
Impo (S).
Proof. By structure induction on abstract syntax of S.
Lemma 5.18. For any statement sequence S, the imported variables in S relative to output are a subset of the imported variables in S relative to the I/O sequence variable, Impo (S) ⊆
Imp(S, {idIO }).
Proof. By induction on abstract syntax of S. In every case, there
are subcases based on if there is output statement in the statement
sequence S or not if necessary.
Lemma 5.19. If two statement sequences S1 and S2 satisfy the
proof rule of behavioral equivalence, then S1 and S2 have the
same set of imported variables relative to output, (S1 ≡S
O S2 ) ⇒
(Impo (S1 ) = Impo (S2 )).
Proof. By induction on size(S1 ) + size(S2 ).
Lemma 5.20. For any statement sequence S and any variable x,
the termination deciding variables in S relative to output is a subset
of the termination deciding variables in S, TVaro (S) ⊆ TVar(S).
Proof. By induction on abstract syntax of S. In every case, there
are subcases based on if there is output statement in the statement
sequence S or not if necessary.
Lemma 5.21. If two statement sequences S1 and S2 satisfy the
proof rule of behavioral equivalence, then S1 and S2 have the same
set of termination deciding variables relative to output, (S1 ≡S
O
S2 ) ⇒ (TVaro (S1 ) = TVaro (S2 )).
Proof. By induction on size(S1 ) + size(S2 ).
Corollary 5.4. If two statement sequences S1 and S2 satisfy the
proof rule of behavioral equivalence, then S1 and S2 have the
same set of out-deciding variables, (S1 ≡S
O S2 ) ⇒ OVar(S1 ) =
OVar(S2 ).
Proof. By Lemma 5.19, Impo (S1 ) = Impo (S2 ). By Lemma 5.21,
TVaro (S1 ) = TVaro (S2 ).
Lemma 5.22. In one step execution (S, m(σ)) → (S ′ , m′ (σ ′ )), if
there is no output statement in S, then the output sequence is same
in value store σ and σ ′ , Out(σ ′ ) = Out(σ).
Proof. By induction on abstract syntax of S and crash flag f in state
m.
Lemma 5.23. If there is no output statement in S, then, after the
∗
execution (S, m(σ)) → (S ′ , m′ (σ ′ )), the output sequence is same
in value store σ and σ ′ , Out(σ ′ ) = Out(σ).

40

2015/9/14

the configuration (S; s, mi (f, loopic , σi )) reachable from (s, m) in
which the loop counter is i, loopic (n) = i, and the crash flag is not
set, f = 0. By the assumption of unique loop labels, s ∈
/ S. Then
the loop counter of s is not redefined in the execution of S started
in state mi (f, loopic , σi ). Because there is a configuration in which
the loop counter of s is i + 1, then the execution of S when started
∗
in the state mi (f, loopic , σi ) terminates, (S, mi (f, loopic , σi )) →
i+1
i+1
(skip, mi+1 (f, loopc , σi+1 )) where f = 0 and loopc (n) = i.
∗
, σi+1 )).
By Corollary E.1, (S; s, mi (f, loopic , σi )) → (s, mi+1 (f, loopi+1
c
By similar argument in base case, the evaluation of the predicate
expression w.r.t the value store σi+1 produce nonzero integer value.
The execution of s proceeds as follows:

Proof. By induction on number k of execution steps in the execuk
tion (S, m(σ)) → (S ′ , m′ (σ ′ )). The proof also relies on the fact
that if s ∈
/ S, then s ∈
/ S′.
Lemma 5.24. One while statement s = “whilehni (e){S}” starts
in a state m(f, loopc ) in which the loop counter of s is zero,
loopc (n) = 0 and the crash flag is not set, f = 0. For any positive
integer i, if there is a state m′ (m′c ) reachable from m in which
the loop counter is i, loop′c (n) = i, then there is a configuration
(S; s, m′′ (f′′ , loop′′c )) reachable from the configuration (s, m) in
which loop counter of s is i, loop′′c (n) = i and the crash flag is not
set, f′′ = 0:
∗
∀i > 0 : (((s, m(f, mc )) → (S ′ , m′ (f′ , loop′c ))) ∧ (loopc (n) =
0) ∧ (f = 0) ∧ (loop′c (n) = i)) ⇒
∗
(s, m(f, loopc )) → (S; s, m′′ (f, loop′′c )) where f = 0 and
′′
loopc (n) = i.

(s, mi+1 (f, loopi+1
, σi+1 ))
c
= (whilehni (e) {S}, mi+1 (f, loopi+1
, σi+1 ))
c
→(whilehni (E JeKσi+1 ) {S}, mi+1 (f, loopi+1
, σi+1 ))
c
by the EEval rule
→(S; whilehni (e) {S}, mi+1 (f, loopi+1
[(i + 1)/n], σi+1 ))
c
by the Wh-T rule.

Proof. The proof is by induction on i.
Base case i = 1.
We show that the evaluation of the loop predicate of s w.r.t the
value store σ in the state m(loopc , σ) produces an nonzero integer
value. By our semantic rule, if the evaluation of the predicate
expression of s raises an exception, the execution of s proceeds
as follows:

The lemma holds.
Lemma 5.25. Let s1 = “whilehn1 i (e) {S1 }” and s2 = “whilehn2 i (e)
{S2 }” be two while statements and all of the followings hold:

• There are output statements in s1 and s2 , ∃e1 e2 : (“output e1 ” ∈
(s, m(f, loopc , σ))
s1 ) ∧ (“output e2 ” ∈ s2 );
= (whilehni (e) {S}, m(loopc , σ))
•
s1 and s2 have the same set of termination deciding vari→(whilehni (error) {S}, m(loopc , σ)) by the EEval rule
ables relative to output, and the same set of imported variables
→(whilehni (0) {S}, m(1/f, loopc , σ)) by the ECrash rule
relative to output, (TVaro (s1 ) = TVaro (s2 ) = TVar(s)) ∧
k
→(whilehni (0) {S}, m(1/f, loopc )) for any k > 0, by the Crash rule.
(Impo (s1 ) = Impo (s2 ) = Imp(io));
• Loop bodies S1 and S2 satisfy the proof rule of equivalent
Hence, we have a contradiction that there is no configuration in
computation of the out-deciding variables of s1 and s2 , ∀x ∈
which the loop counter of s is 1.
OVar(s) = TVar(s) ∪ Imp(io) : S1 ≡S
x S2 ;
When the evaluation of the loop predicate expression of s pro•
Loop
bodies
S
and
S
satisfy
the
proof
rule of termination in
1
2
duce zero, the execution of s proceeds as follows:
the same way, S1 ≡S
H S2 ;
(s, m(f, loopc , σ))
• Loop bodies S1 and S2 produce the same output sequence
= (whilehni (e) {S}, m(loopc , σ))
when started in states with crash flags not set and whose value
→(whilehni (0) {S}, m(loopc , σ)) by the EEval rule
stores agree on values of variables in OVar(S1 ) ∪ OVar(S2 ),
→(skip, m(loopc [0/n1 ])) by the Wh-F rule.
∀mS1 (f1 , σS1 ) mS2 (f2 , σS2 ) :
((f1 = f2 = 0) ∧ (∀x ∈ OVar(S1 ) ∪ OVar(S2 ) : σS1 (x) =
Hence, we have a contradiction that there is no configuration in
σS2 (x))) ⇒
which the loop counter of s is 1. The evaluation of the predicate
((S1 , mS1 (f1 , σS1 )) ≡O (S2 , mS2 (f2 , σS2 ))).
expression of s w.r.t value store σ produce nonzero value. The
If s1 and s2 start in states m1 (f1 , loop1c , σ1 ), m2 (f2 , loop2c , σ2 )
execution of s proceeds as follows:
respectively with crash flags not set f1 = f2 = 0 and in which s1
(s, m(f, loopc , σ))
and s2 have not started execution (loop1c (n1 ) = loop2c (n2 ) = 0),
= (whilehni (e) {S}, m(f, loopc , σ))
value stores σ1 and σ2 agree on values of variables in OVar(s),
→(whilehni (E JeKσ) {S}, m(f, loopc , σ)) by the EEval rule
∀x ∈ OVar(s) : σ1 (x) = σ2 (x), then one of the followings holds:
→(S; whilehni (e) {S}, m(f, loopc [1/n1 ], σ)) by the Wh-T rule.
1. s1 and s2 both terminate and produce the same output sequence:
The lemma holds.
∗
∗
(s1 , m1 ) → (skip, m′1 (σ1′ )), (s2 , m2 ) → (skip, m′2 (σ2′ ))
Induction step.
′
′
′
i
where σ1 (idIO ) = σ2 (idIO ).
The hypothesis IH is that, if there is a configuration (S , mi (loopc ))
k
reachable from (s, m) in which the loop counter of s is i, loopic (n) =
2. s1 and s2 both do not terminate, ∀k > 0, (s1 , m1 ) →
k
i > 0, then there is a reachable configuration (S; s, mi (f, loopic ))
(S1k , m1k ), (s2 , m2 ) → (S2k , m2k ) where S1k 6= skip,
from (s, m) where the loop counter of s is i and the crash flag is
S2k 6= skip and one of the followings holds:
not set.
(a) For any positive integer i, there are two configurations
Then we show that, if there is a configuration (S ′ , mi+1 (loopi+1
))
c
(s1 , m1i ) and (s2 , m2i ) reachable from (s1 , m1 ) and
reachable from (s, m) in which the loop counter of s is i + 1, then
(s2 , m2 ), respectively, in which both crash flags are not
there is a reachable configuration (S; s, mi+1 (f, loopi+1
))
from
c
set, the loop counters of s1 and s2 are equal to i and
(s, m) where the loop counter of s is i + 1 and the crash flag f is
value stores agree on values of variables in OVar(s), and
not set.
∗
for every state in execution, (s1 , m1 ) → (s1 , m1i ) or
By Lemma E.8, the loop counter of s is increasing by one in
∗
one step. Hence, there must be one configuration reachable from
(s2 , m2 ) → (s2 , m2i ), loop counters for s1 and s2 are
(s, m) in which the loop counter of s is i. By hypothesis, there is
less than or equal to i respectively:

41

2015/9/14

∗

∀i > 0 ∃(s1 , m1i ) (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 ,
∗
loop1ci , σ1i )) ∧ (s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i ))
where
• f1 = f2 = 0; and
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ OVar(s) : σ1i (x) = σ2i (x).
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i (loop1c i ,
′
σ1i )), loop1c (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i (loop2c i ,
′
σ2i )), loop2c (n2 ) ≤ i;
(b) The loop counters for s1 and s2 are less than a smallest
positive integer i and all of the followings hold:
′

∗

• ∃i > 0 ∀m′1 , m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )),
′

∗

′

(s2 , m2 ) → (S2′ , m′2 (loop2c )) where loop1c (n1 ) < i,
′
loop2c (n2 ) < i;
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which both crash flags are not set, loop
counters of s1 and s2 are equal to j and value stores
agree on values of variables in OVar(s):
1
∗
∃(s1 , m1j ), (s2 , m2j ) : (s1 , m1 ) → (s1 , m1j (f1 , loopcj ,
2
∗
σ1j )) ∧ (s2 , m2 ) → (s2 , m2j (f2 , loopcj , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ OVar(s) : σ1j (x) = σ2j (x).
• If i = 1, then the I/O sequence is not redefined in any
states reachable from (s1 , m1 ) and (s2 , m2 ).
∗

∀m′′1 : (s1 , m1 (loop1c , σ1 )) → (S1′′ , m′′1 (σ1′′ ))
where σ1′′ (idIO ) = σ1 (idIO ).
∗
∀m′′2 : (s2 , m2 (loop2c , σ2 )) → (S2′′ , m′′2 (σ2′′ ))
′′
where σ2 (idIO ) = σ2 (idIO ).

∗

∀m′′1 : (s1 , m1i−1 (loopci−1 , σ1i−1 )) → (S1′′ , m′′1 (σ1′′ ))
where σ1′′ (idIO ) = σ1i−1 (idIO ).
∗

∀m′′2 : (s2 , m2i−1 (loopci−1 , σ2i−1 )) → (S2′′ , m′′2 (σ2′′ ))
where σ2′′ (idIO ) = σ2i−1 (idIO ).
(c) The loop counters for s1 and s2 are less than or equal to a
smallest positive integer i and all of the followings hold:
′

∗

• ∃i > 0 ∀m′1 , m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )), (s2 ,
∗

′

• Loop bodies S1 and S2 satisfy the proof rule of termination in

the same way;
By assumption.
• Loop bodies S1 and S2 satisfy the proof rule of equivalent
computation of those in the termination deciding variables of
s1 and s2 , ∀x ∈ TVar(s1 ) ∪ TVar(s2 ) : S1 ≡S
x S2 ;
By the definition of OVar(s), TVaro (s1 ) ⊆ OVar(s1 ) and
TVaro (s2 ) ⊆ OVar(s2 ). By the definition of TVaro , TVaro (s1 ) =
TVar(s1 ) and TVaro (s2 ) = TVar(s2 ).

• Crash flags are not set, f1 = f2 = 0;
• Loop counters of s1 and s2 are initially zero, loop1c (n1 ) =

states reachable from (s1 , m1i−1 ) and (s2 , m2i−1 ).

2

Proof. We show that s1 and s2 terminate in the same way when
started in states m1 (f1 , loop1c , σ1 ) and m2 (f2 , loop2c , σ2 ) respectively, (s1 , m1 ) ≡H (s2 , m2 ). In addition, we show that s1 and s2
produce the same output sequence in every possibilities of termination in the same way, (s1 , m1 ) ≡O (s2 , m2 ).
By definition, s1 and s2 satisfy the proof rule of termination in
the same way because

By Lemma 5.16, we show s1 and s2 terminate in the same way
when started in states m1 (f1 , loop1c , σ1 ) and m2 (f2 , loop2c , σ2 ). We
need to show that all the required conditions are satisfied.

• If i > 1, then the I/O sequence is not redefined in any
1

1

∗

∃(s1 , m1j ), (s2 , m2j ) : (s1 , m1 ) → (s1 , m1j (f1 , loopcj ,
2
∗
σ1j )) ∧ (s2 , m2 ) → (s2 , m2j (f2 , loopcj , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ OVar(s) : σ1j (x) = σ2j (x).
• If i = 1, then executions from (s1 , m1 ) and (s2 , m2 )
produce the same output sequence:
(s1 , m1 (loop1c , σ1 )) ≡O (s2 , m2 (loop2c , σ2 )).
• If i > 1, then executions from (s1 , m1i−1 ) and (s2 , m2i−1 )
produce the same output sequence:
2
1
(s1 , m1i−1 (loopci−1 , σ1i−1 )) ≡O (s2 , m2i−1 (loopci−1 ,
σ2i−1 )).

′

m2 ) → (S2′ , m′2 (loop2c )) where loop1c (n1 ) ≤ i,
′
loop2c (n2 ) ≤ i;
• There are no configurations (s1 , m1i ) and (s2 , m2i )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which crash flags are not set, the loop counters of s1
and s2 are equal to i, and value stores agree on values
of variables in OVar(s):
∗
∄(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci ,
∗
σ1i )) ∧ (s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i )) where
f1 = f2 = 0; and
loop1ci (n1 ) = loop2ci (n2 ) = i; and
∀x ∈ OVar(s) : σ1i (x) = σ2i (x).
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which crash flags are not set, the loop
counters of s1 and s2 are equal to j and value stores
agree on values of variables in OVar(s):

loop2c (n2 ) = 0;

• s1 and s2 have same set of termination deciding variables,

TVar(s1 ) = TVar(s2 ) = TVar(s);
• Value stores σ1 and σ2 agree on values of variables in TVar(s1 ) =

TVar(s2 ), ∀x ∈ TVar(s) : σ1 (x) = σ2 (x);
The above four conditions are from assumption.
• Loop bodies S1 and S2 terminate in the same way when started
in states with crash flags not set and whose value stores agree
on values of variables in TVar(S1 ) ∪ TVar(S2 );
By Theorem 4.
Therefore, by Lemma 5.16, we have one of the followings holds:
1. s1 and s2 both terminate and the loop counters of s1 and s2 are
less than a positive integer i such that the loop counters of s1
and s2 are less than or equal to i − 1:
∗
∗
(s1 , m1 ) → (skip, m′1 ), (s2 , m2 ) → (skip, m′2 ).
We show that, when s1 and s2 terminate, value stores of s1
and s2 agree on the value of the I/O sequence variable by
Lemma 5.2. We need to show all the required conditions hold.
• ∀x ∈ Imp(io) : σ1 (x) = σ2 (x);
• loop1c (n1 ) = loop2c (n2 ) = 0;
The above two conditions are from assumption.
• idIO ∈ Def(s1 ) ∩ Def(s2 );
Because there are output statements in s1 and s2 . By the
definition of Def(·), the I/O sequence variable is defined in
s1 and s2 .

42

2015/9/14

• Imp(s1 , {idIO }) = Imp(s2 , {idIO }) = Imp(io);

loop counters of s1 and s2 are equal to j and value stores
agree on the values of variables in TVar(s):
1
∗
∃(s1 , m1j ), (s2 , m2j ) : (s1 , m1 ) → (s1 , m1j (f1 , loopcj ,
2j
∗
σ1j )) ∧ (s2 , m2 ) → (s2 , m2j (f2 , loopc , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ TVar(s) : σ1j (x) = σ2j (x).

By the definition of Impo (·), Impo (s1 ) = Imp(s1 , {idIO }),
Impo (s2 ) = Imp(s2 , {idIO }).
• ∀y ∈ Imp(io), ∀mS1 (σS1 ) mS2 (σS2 ) :
((∀z ∈ Imp(S1 , Imp(io)) ∪ Imp(S2 , Imp(io)), σS1 (z) =
σS2 (z)) ⇒ (S1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).
By Theorem 2.
In addition, by the semantic rules, the I/O sequence is appended
at most by one value in one step. Hence, s1 and s2 produce
the same output sequence when started in states m1 and m2
respectively.
k

2. s1 and s2 both do not terminate, ∀k > 0, (s1 , m1 ) →
k
(S1k , m1k ), (s2 , m2 ) → (S2k , m2k ) where S1k 6= skip,
S2k 6= skip and one of the followings holds:
(a) ∀i > 0, there are two configurations (s1 , m1i ) and (s2 , m2i )
reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which both crash flags are not set, the loop counters of
s1 and s2 are equal to i and value stores agree on the values
of variables in TVar(s):
∗
∀i > 0 ∃(s1 , m1i ) (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 ,
∗
1i
loopc , σ1i )) ∧ (s2 , m2 ) → (s2 , m2i (f2 , loop2ci , σ2i ))
where
• f1 = f2 = 0; and
• loop1c i (n1 ) = loop2c i (n2 ) = i; and
• ∀x ∈ TVar(s) : σ1i (x) = σ2i (x).
′
∗
∗
• ∀m′1 : (s1 , m1 ) → (S1′ , m′1 (loop1c )) → (s1 , m1i (loop1c i ,
′
1
σ1i )), loopc (n1 ) ≤ i; and
′
∗
∗
• ∀m′2 : (s2 , m2 ) → (S2′ , m′2 (loop2c )) → (s2 , m2i (loop2c i ,
2′
σ2i )), loopc (n2 ) ≤ i;
We show that, for any positive integer i, value stores σ1i and
σ2i agree on values of variables in Imp(io) by the proof of
Lemma 5.1. We need to show that all the required conditions
are satisfied.
• ∀x ∈ Imp(io) : σ1 (x) = σ2 (x);
• loop1c (n1 ) = loop2c (n2 ) = 0;
The above two conditions are by assumption.
• idIO ∈ Def(s1 ) ∩ Def(s2 );
• Imp(s1 , {idIO }) = Imp(s2 , {idIO }) = Imp(io);
The above two conditions are obtained by similar argument in the case that s1 and s2 both terminate.
• ∀y ∈ Imp(io), ∀mS1 (σS1 ) mS2 (σS2 ) :
((∀z ∈ Imp(S1 , Imp(io))∪Imp(S2 , Imp(io)), σS1 (z) =
σS2 (z)) ⇒
(S1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).
By Theorem 2.
We cannot apply Lemma 5.1 directly because s1 and s2 do
not terminate. But we can still have the proof closely similar
to that of Lemma 5.1 by using the fact that there exists a
configuration of arbitrarily large loop counters of s1 and s2
and in which crash flags are not set.
Then, ∀i > 0, ∀x ∈ Imp(io) : σ1i (x) = σ2i (x). In addition, by the semantic rules, the I/O sequence is appended at
most by one value in one step. The lemma holds.
(b) The loop counters for s1 and s2 are less than a smallest
positive integer i and all of the followings hold:
′

∗

• ∃i > 0 ∀m′1 , m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )), (s2 ,
∗

′

′

m2 ) → (S2′ , m′2 (loop2c )) where loop1c (n1 ) < i,
′
loop2c (n2 ) < i;
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which both crash flags are not set, the

43

This case corresponds to the situation that the ith evaluations of the predicate expression of s1 and s2 raise an exception. There are two possibilities regarding the value of
i.
i. i = 1.
s1 and s2 raise an exception in the 1st evaluation of
their predicate expression because loop counters of s1
and s2 are less than 1. In the proof of Lemma 5.16,
value stores σ1 and σ2 in states m1 and m2 respectively
are not modified in the 1st evaluation of the predicate
expression of s1 and s2 . In addition, value stores σ1 and
σ2 are not modified after s1 and s2 both crash according
to the rule Crash. We have the corresponding initial state
in which value stores σ1 and σ2 agree on values of
variables in OVar(s). Thus, σ1 (idIO ) = σ2 (idIO ). The
lemma holds.
ii. i > 1.
We show that, for any positive integer 0 < j < i,
value stores σ1j and σ2j agree on values of variables
in Imp(s) by the proof of Lemma 5.1. We need to show
that all the required conditions are satisfied.
• ∀x ∈ Imp(io) : σ1 (x) = σ2 (x);
• loop1c (n1 ) = loop2c (n2 ) = 0;
The above two conditions are from assumption.
• idIO ∈ Def(s1 ) ∩ Def(s2 );
• Imp(s1 , {idIO }) = Imp(s2 , {idIO }) = Imp(io);
The above two conditions are obtained by the same
argument in the case that s1 and s2 terminate.
• ∀y ∈ Imp(io), ∀mS1 (σS1 ) mS2 (σS2 ) :
((∀z ∈ Imp(S1 , Imp(io))∪Imp(S2 , Imp(io)), σS1 (z) =
σS2 (z)) ⇒
(S1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).
By Theorem 2.
We cannot apply Lemma 5.1 directly because s1 and
s2 do not terminate. But we can still have the proof
closely similar to that of Lemma 5.1 by using the fact
that there are reachable configurations (s1 , m1i−1 ) and
(s2 , m2i−1 ) with the loop counters of s1 and s2 of value
i − 1 and crash flags not set.
By assumption, there is configuration (s1 , m1i−1 (f1 ,
loop1ci−1 , σ1i−1 )) reachable from (s1 , m1 ) in which the
loop counter of s1 is i − 1 and the crash flag is not set;
there is also a configuration (s2 , m2i−1 (f2 , loop2ci−1 , σ2i−1 ))
of s2 reachable from (s2 , m2 ) in which the loop counter
is i − 1 and the crash flag is not set. In addition, value
stores σ1i−1 and σ2i−1 agree on values of variables in
Imp(io). In the proof of Lemma 5.15, the ith evaluations
of the predicate expression of s1 and s2 must raise an
exception because loop counters of s1 and s2 are less
than i. Then the I/O sequence is not redefined in any
state reachable from (s1 , m1i−1 (f1 , loop1ci−1 , σ1i−1 ))
and (s2 , m2i−1 (f2 , loop2ci−1 , σ2i−1 )) respectively. In
addition, by the semantic rules, the I/O sequence is appended at most by one value in one step. The lemma
holds.

2015/9/14

(c) The loop counters for s1 and s2 are less than or equal to a
smallest positive integer i and all of the followings hold:
′

∗

∗

• ∃i > 0 ∀m′1 , m′2 : (s1 , m1 ) → (S1′ , m′1 (loop1c )), (s2 , m2 ) →
′

(S2′ , m′2 (loop2c ))
′
loop2c (n2 ) ≤ i;

where

′

loop1c

(n1 ) ≤ i,

• There are no configurations (s1 , m1i ) and (s2 , m2i )

reachable from (s1 , m1 ) and (s2 , m2 ), respectively, in
which crash flags are not set, the loop counters of s1 and
s2 are equal to i, and value stores agree on values of
variables in TVar(s):
∗
∄(s1 , m1i ), (s2 , m2i ) : (s1 , m1 ) → (s1 , m1i (f1 , loop1ci , σ1i ))∧
∗
2i
(s2 , m2 ) → (s2 , m2i (f2 , loopc , σ2i )) where
f1 = f2 = 0; and
loop1ci (n1 ) = loop2ci (n2 ) = i; and
∀x ∈ TVar(s) : σ1i (x) = σ2i (x).
• ∀0 < j < i, there are two configurations (s1 , m1j )
and (s2 , m2j ) reachable from (s1 , m1 ) and (s2 , m2 ),
respectively, in which both crash flags are not set, the
loop counters of s1 and s2 are equal to j and value stores
agree on values of variables in TVar(s):
1
∗
∃(s1 , m1j ), (s2 , m2j ) : (s1 , m1 ) → (s1 , m1j (f1 , loopcj , σ1j ))∧
2j
∗
(s2 , m2 ) → (s2 , m2j (f2 , loopc , σ2j )) where
f1 = f2 = 0; and
2
1
loopcj (n1 ) = loopcj (n2 ) = j; and
∀x ∈ TVar(s) : σ1j (x) = σ2j (x).

This case corresponds to the situation that, the ith evaluation
of the predicate expression of s1 and s2 produce same
nonzero integer value and loop bodies S1 and S2 do not
terminate after the ith evaluation of the predicate expression
of s1 and s2 . There are two possibilities regarding the value
of i.
i. i = 1.
By assumption, we have the initial value stores σ1 and
σ2 agree on values of variables in OVar(s). In the proof
of Lemma 5.15, the execution of s1 proceeds as follows:
(s1 , m1 (loop1c , σ1 ))
= (whilehn1 i (e) {S1 }, m1 (loop1c , σ1 ))
→(whilehn1 i (v) {S1 }, m1 (loop1c , σ1 )) by the EEval rule
→(S1 ; whilehn1 i (e) {S1 }, m1 (loop1c [1/n1 ], σ1 ))
by the Wh-T rule.

We show that, for any positive integer 0 < j < i,
value stores σ1j and σ2j agree on values of variables
in Imp(io) by the proof of Lemma 5.1. We need to show
that all the required conditions are satisfied.
• ∀x ∈ Imp(io) : σ1 (x) = σ2 (x);
• loop1c (n1 ) = loop2c (n2 ) = 0;
The above two conditions are from assumption.
• idIO ∈ Def(s1 ) ∩ Def(s2 );
• Imp(s1 , {idIO }) = Imp(s2 , {idIO }) = Imp(io);
The above two conditions are obtained by the same
argument in the case that s1 and s2 terminate.
• ∀y ∈ Imp(io), ∀mS1 (σS1 ) mS2 (σS2 ) :
((∀z ∈ Imp(S1 , Imp(io))∪Imp(S2 , Imp(io)), σS1 (z) =
σS2 (z)) ⇒
(S1 , mS1 (σS1 )) ≡y (S2 , mS2 (σS2 ))).
By Theorem 2.
We cannot apply Lemma 5.1 directly because s1 and s2
do not terminate. But we can still have the proof closely
similar to that of Lemma 5.1. The reason is that there are
reachable configurations (S1 ; s1 , m′1 ) and (S2 ; s2 , m′2 )
with loop counters of s1 and s2 of value i and crash
flags not set. This is by Lemma 5.24 because there are
configurations reachable from (s1 , m1 ) and (s2 , m2 )
respectively with loop counters of s1 and s2 of i.
There are configurations (s1 , m1i−1 ) reachable from
(s1 , m1 ) and (s2 , m2i−1 ) reachable from (s2 , m2 ) in
which loop counters of s1 and s2 are i−1 and crash flags
are not set and value stores agree on values of variables
in Imp(io). Because loop counters of s1 and s2 are less
than or equal to i. Then the execution of s1 proceeds as
follows:
(s1 , m1i−1 (loop1ci−1 , σ1i−1 ))
= (whilehn1 i (e) {S1 }, m1i−1 (loop1ci−1 , σ1i−1 ))
→(whilehn1 i (v) {S1 }, m1i−1 (loop1ci−1 , σ1i−1 ))
by the EEval rule
→(S1 ; whilehn1 i (e) {S1 }, m1i−1 (loop1ci−1 [i/n1 ],
σ1i−1 )) by the Wh-T rule.
The execution of s2 proceeds to (S2 ; whilehn2 i (e) {S2 },
m2i−1 (loop2ci−1 [i/n2 ], σ2i−1 )). By similar argument in
the case i = 1, S2 and S1 produce the same output sequence when started in states m1i−1 (loop1ci−1 [i/n1 ], σ1i−1 )
and m2i−1 (loop2ci−1 [i/n2 ], σ2i−1 ) respectively. In addition, by the semantic rules, the I/O sequence is appended at most by one value in one step. The lemma
holds.

The execution of s2 proceeds to (S2 ; whilehn2 i (e) {S2 },
m2 (loop2c [1/n2 ], σ2 )). Then the execution of S1 and S2
do not terminate when started in states m1 (loop1c [1/n1 ], σ1 )
and m2 (loop2c [1/n2 ], σ2 ). By assumption, value stores
σ1 and σ2 agree on values of the out-deciding variables of s1 and s2 , ∀x ∈ OVar(s1 ) ∪ OVar(s2 ) :
σ1 (x) = σ2 (x). By definition, Imp(S1 , {idIO }) ⊆
Corollary 5.5. Let s1 = “whilehn1 i (e) {S1 }” and s2 =
Imp(s1 , {idIO }), CVar(S1 ) ⊆ CVar(s1 ) and LVar(S1 ) ⊆
“whilehn2 i (e) {S2 }” be two while statements such that all of the
LVar(s1 ). Thus, TVar(S1 ) ⊆ TVar(s1 ). By Lemma 5.20,
followings hold
TVaro (S1 ) ⊆ TVar(S1 ). By Lemma 5.18, Impo (S1 ) ⊆
• There are output statements in s1 and s2 , ∃e1 e2 : (“output e1 ” ∈
Imp(S1 , idIO ). In conclusion, OVar(S1 ) ⊆ OVar(s1 ).
s1 ) ∧ (“output e2 ” ∈ s2 );
Similarly, OVar(S2 ) ⊆ OVar(s2 ). Thus, ∀x ∈ OVar(S1 )∪
• s1 and s2 have same set of termination deciding variables
OVar(S2 ) : σ1 (x) = σ2 (x). Then executions of S1 and
and same set of imported variables relative to the I/O seS2 when started from states
quence variable, (TVar(s1 ) = TVar(s2 ) = TVar(s)) ∧
m1 (loop1c [1/n1 ], σ1 ) and m2 (loop2c [1/n2 ], σ2 ) pro(Imp(s1 , {idIO }) = Imp(s2 , {idIO }) = Imp(io));
duce the same output sequence:
(S1 , m1 (loop1c [1/n1 ], σ1 )) ≡O (S2 , m2 (loop2c [1/n2 ], σ2 )). • Loop bodies S1 and S2 satisfy the proof rule of equivalent
computation of those in out-deciding variables of s1 and s2 ,
In addition, by the semantic rules, the I/O sequence is
∀x ∈ OVar(s) = TVar(s) ∪ Imp(io) : S1 ≡S
appended at most by one value in one step. The lemma
x S2 ;
• Loop bodies S1 and S2 satisfy the proof rule of termination in
holds.
the same way, S1 ≡S
ii. i > 1.
H S2 ;
44

2015/9/14

1:
2:
3:
4: output a + 2

• Loop bodies S1 and S2 produce the same output sequence when

started in states with crash flags not set and agreeing on values
of variables in OVar(S1 ) ∪ OVar(S2 ), ∀mS1 (f1 , σS1 ) mS2 (f2 ,
σS 2 ) :
((f1 = f2 = 0) ∧ (∀x ∈ OVar(S1 ) ∪ OVar(S2 ) : σS1 (x) =
σS2 (x))) ⇒ ((S1 , mS1 (f1 , σS1 )) ≡O (S2 , mS2 (f2 , σS2 ))).

old

If s1 and s2 start in states m1 (f1 , loop1c , σ1 ), m2 (f2 , loop2c , σ2 )
respectively with crash flags not set f1 = f2 = 0 and in which s1
and s2 have not started execution (loop1c (n1 ) = loop2c (n2 ) = 0),
value stores σ1 and σ2 agree on values of variables in OVar(s),
∀x ∈ OVar(s) : σ1 (x) = σ2 (x), then s1 and s2 produce the same
output sequence: (s1 , m1 ) ≡O (s2 , m2 ).

1’: If (b) then
2’:
output a ∗ 2
3’: else
4’:
output a + 2
new

Figure 15: Specializing new configuration variables

Definition 25. (Specializing new configuration variables) A
statement sequence S2 includes updates of specializing new configuration variables compared with S1 w.r.t a mapping ρ of new configuration variables in S2 , ρ : {id} 7→ {0, 1}, denoted S2 ≈S
ρ S1 ,
iff one of the following holds:

This is from lemma 5.25.
5.5 Backward compatible DSU based on program
equivalence

1. S2 = “If(id) then{S2t } else{S2f }” where one of the following
holds:
(a) (ρ(id) = 0) ∧ (S2f ≈S
ρ S1 );
(b) (ρ(id) = 1) ∧ (S2t ≈S
ρ S1 );
2. S1 and S2 produce the same output sequence, S1 ≈S
O S2 ;
3. S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
f
t
S f
where (S2t ≈S
ρ S1 ) ∧ (S2 ≈ρ S1 );
′
4. S1 = “whilehn1 i (e) {S1 }”, S2 = “whilehn2 i (e) {S2′ }” where
′
S2′ ≈S
ρ S1 ;
S
′
′
′
5. S1 = S1 ; s1 and S2 = S2′ ; s2 where (S2′ ≈S
ρ S1 ) ∧ (S2 ≈H
S1′ ) ∧ ∀x ∈ Imp(s1 , idIO ) ∪ Imp(s1 , idIO ) : (S2′ ≈S
x
S1′ ) ∧ (s2 ≈S
ρ s1 ).

Based on the equivalence result above, we show that there exists backward compatible DSU. We need to show there exists a
mapping of old program configurations and new program configurations and the hybrid execution obtained from the configuration
mapping is backward compatible. We do not provide a practical algorithm to calculate the state mapping. Instead we only show that
there exists new program configurations corresponding to some old
program configurations via a simulation. The treatment in this section is informal.
The idea is to map a configuration just before an output is produced to a corresponding configuration. Based on the proof rule
of same output sequences, not every statement of the old program
can correspond to a statement of the new program, but every output
statemet of the old program should correspond to an output statement of the new program. Consider configuration C1 of the old
program where the leftmost statement (next statement to execute)
is an output statement. We can define a corresponding statement of
the new program by simulating the execution of the new program
on the input consumed so far in C1 . There are two cases. When the
leftmost statement in C1 is not included in a loop statement, then
it is easy to know when to stop simulation. Otherwise, we have
the bijection of loop statements including output statements based
on the condition of same output sequences. Therefore, it is easy to
know how many iterations of the loop statements including the output statement shall be carried out based on the loop counters in the
old program configuration C1 . Based on Theorem 5, there must be
a configuration C2 corresponding to C1 . Moreover, the executions
starting from configurations C1 and C2 produce the same output
sequence based on Theorem 5. In conclusion, we obtain a backward compatible hybrid execution where the state mapping is from
C1 to C2 .

Then we show that executions of two statement sequences produce the same I/O sequence if there are updates of specializing new
configuration variables between the two.
Lemma 6.1. Let S1 and S2 be two different statement sequences
where there are updates of “specializing new configuration variables” in S2 compared with S1 w.r.t a mapping of new configuration variables ρ, S2 ≈S
ρ S1 . If executions of S2 and S1 start in
states m2 (f2 , σ2 ) and m1 (f1 , σ1 ) respectively where all of the following hold:
• Crash flags f2 , f1 are not set, f2 = f1 = 0;
• Value stores σ1 and σ2 agree on output deciding variables in

both S1 and S2 including the input and I/O sequence variable,
∀id ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : σ1 (id) =
σ2 (id);
• Values of new configuration variables in the value store σ2 are
matching those in ρ, ∀id ∈ Dom(ρ) : ρ(id) = σ2 (id);
• Values of new configuration variables are not defined in the
statement sequence S2 , Dom(ρ) ∩ Def(S2 ) = ∅;

6. Real world backward compatible update
classes: proof rules

then S2 and S1 satisfy all of the following:
• (S1 , m1 ) ≡H (S2 , m2 );
• (S1 , m1 ) ≡O (S2 , m2 );
• ∀x ∈ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );

We propose our formal treatment for real world update classes. For
each update class, we show how the old program and new program
produce the same I/O sequence which guarantees backward compatible DSU.

Proof. The proof of Lemma 6.1 is by induction on the sum of program sizes of S1 and S2 and is a case analysis based on Definition 25.
Base case.
S1 is a simple statement s, S2 = “If(id) then{st2 } else{sf2 }”
where st2 , sf2 are simple statement and one of the following holds:

6.1 Proof rule for specializing new configuration variables
New configuration variables can be introduced to generalize functionality. Figure 15 shows an example of how a new configuration
variable introduces new code. The two statement sequences in Figure 15 are equivalent when the new variable b is specialized to 0.
Our generalized formal definition of “specializing new configuration variables” is defined as follows.

1. (ρ(id) = 0) ∧ (sf2 = s);
2. (ρ(id) = 1) ∧ (st2 = s);
45

2015/9/14

• Value stores σ1 and σ2 agree on values of used variables

W.l.o.g., we assume that ρ(id) = 0. By assumption, σ2 (id) =
ρ(id) = 0. Then the execution of S2 proceeds as follows:

in S1f and S2f as well as the input, I/O sequence variable.
By definition, Use(S1f ) ⊆ Use(S1 ). So are the cases to
(If(id) then{st2 } else{sf2 }, m2 (σ2 ))
S2f and S2 . In addition, value stores σ1 and σ2 are not
→(If(0) then{st2 } else{sf2 }, m2 (σ2 ))
changed in the evaluation of the predicate expression e.
by the rule Var
The condition holds.
• Values of new configuration variables are consistent
→(sf2 , m2 (σ2 )) by the If-F rule.
in the value store σ2 and the specialization ρ, ∀id ∈
Dom(ρ) : σ2 (id) = ρ(id).
By Theorem 5 and Theorem 4, this lemma holds.
By assumption.
Induction step.
The induction hypothesis (IH) is that Lemma 6.1 holds when
By the hypothesis IH, the lemma holds.
the sum of the program size of S1 and S2 is at least 4, size(S1 ) +
(c) The evaluation of e reduces to the same nonzero integer
size(S2 ) = k ≥ 4.
value, E ′ JeKσ1 = E ′ JeKσ2 = (v, vof ) where v 6= 0.
Then we show that the lemma holds when size(S1 )+size(S2 ) =
By arguments similar to the second subcase above.
k + 1. There are cases to consider.
3. S1 and S2 are both “while” statements:
S1 = “whilehni (e) {S1′ }”, S2 = “whilehni (e) {S2′ }” where
1. S1 and S2 satisfy the condition of same output sequence,
′
S2′ ≈S
ρ S1 ;
S1 ≡ S
O S2 .
By Lemma 6.3, we show this lemma holds. We need to show
By Theorem 5, the lemma 6.1 holds.
that all required conditions are satisfied for the application of
2. S1 and S2 are both “If” statement:
Lemma 6.3.
S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
• S1 and S2 have same set of output deciding variables,
where both of the following hold
OVar(S1 ) = OVar(S2 ) = OVar(S);
t
• S2t ≈S
ρ S1 ;
By Lemma 6.2 and Corollary 5.1.
f
• S2f ≈S
• When started in states m′1 (σ1′ ), m′2 (σ1′ ) where value stores
ρ S1 ;
σ1′ and σ2′ agree on values of output deciding variables in
By the definition of Use(S1 ), variables used in the predicate exboth S1 and S2 as well as the input sequence variable and
pression e are a subset of used variables in S1 and S2 , Use(e) ⊆
the I/O sequence variable, then S1′ and S2′ terminate in the
Use(S1 ) ∩ Use(S2 ). By assumption, corresponding variables
same way, produce the same output sequence, and have
used in e are of same value in value stores σ1 and σ2 . By
equivalent computation of defined variables in both S1 and
Lemma D.1, the expression evaluates to the same value w.r.t
S2 .
value stores σ1 and σ2 . There are three possibilities.
By the induction hypothesis IH. This is because the sum of
(a) The evaluation of e crashes, E ′ JeKσ1 = E ′ JeKσ2 =
the program size of S1′ and S2′ is less than k. By definition,
(error, vof ).
size(S1 ) = 1 + size(S1′ ).
The execution of S1 continues as follows:
f
t
By Lemma 6.3, this lemma holds.
(If(e) then{S1 } else{S1 }, m1 (σ1 ))
4. S2 = “If(id) then {S2t } else {S2f }” where one of the following
→(If((error, vof )) then{S1t } else{S1f }, m1 (σ1 ))
holds:
by the rule EEval’
(a) (ρ(id) = 0) ∧ (S2f ≈S
→(If(0) then{S1t } else{S1f }, m1 (1/f))
ρ S1 );
by the ECrash rule
(b) (ρ(id) = 1) ∧ (S2t ≈S
ρ S1 );
i
→(If(0) then{S1t } else{S1f }, m1 (1/f)) for any i > 0
W.l.o.g, we assume (ρ(id) = 0) ∧ (S2f ≈S
ρ S1 );
by the Crash rule.
Then the execution of S2 proceeds as follows:
Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. The lemma holds.
(If(id) then{S2t } else{S2f }, m2 (σ2 ))
(b) The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
= (If(0) then{S2t } else{S2f }, m2 (σ2 ))
(0, vof ).
by the Var rule
The execution of S1 continues as follows.
→(S2f , m2 (σ2 ))
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
by the If-F rule
= (If((0, vof )) then{S1t } else{S1f }, m1 (σ1 ))
by the rule EEval’
By the induction hypothesis, we show that the lemma holds.
We need to show the required conditions are satisfied for the
→(If(0) then{S1t } else{S1f }, m1 (σ1 ))
application of the hypothesis.
by the E-Oflow1 or E-Oflow2 rule
• S2f ≈S
→(S1f , m1 (σ1 )) by the If-F rule.
ρ S1
By assumption.
Similarly, the execution of S2 gets to the configuration
• The sum of the program size of S1f and S2f is less than k,
(S2f , m2 (σ2 )).
By the hypothesis IH, we show the lemma holds. We need
size(S1f ) + size(S2f ) < k.
to show that all conditions are satisfied for the application
By definition, size(S2 ) = 1 + size(S2t ) + size(S2f ). Then,
f
t
of the hypothesis IH.
size(S
2 ) + size(S1 ) < k + 1 − 1 − size(S2 ) < k.
f
• (S2f ≈S
ρ S1 )
• Value stores σ1 and σ2 agree on values of used variables in
By assumption.
S2f and S1 as well as the input, I/O sequence variable.
• The sum of the program size of S1f and S2f is less than
By definition, Use(S2f ) ⊆ Use(S2 ). In addition, the value
k, size(S1f ) + size(S2f ) < k.
store σ2 is not changed in the evaluation of the predicate
By definition, size(S1 ) = 1+size(S1t )+size(S1f ). Then,
expression e. The condition holds.
size(S1f ) + size(S2f ) < k + 1 − 2 = k − 1.
By the hypothesis IH, the lemma holds.
46

2015/9/14

σ1 (id) = σ2 (id) = σ2′ (id). Otherwise, the variable id
is defined in the execution of S1′ and S2′ , by assumption,
σ1′ (id) = σ2′ (id). The condition holds.
• Values of new configuration variables are consistent
in the value store σ2′ and the specialization ρ, ∀id ∈
Dom(ρ) : σ2′ (id) = ρ(id).
By assumption, Dom(ρ) ∩ Def(S2 ). By Corollary E.2,
values of new configuration variables are not changed
in the execution of S2′ , ∀id ∈ Dom(ρ) : σ2′ (id) =
σ2 (id) = ρ(id).
By the hypothesis IH, the lemma holds.

5. S1 and S2 are same, S1 = S2 ;
By definition, used variables in S1 and S2 are same; defined
variables in S1 and S2 are same. By semantic rules, S1 and S2
terminate in the same way, produce the same output sequence
and have equivalent computation of defined variables in S1 and
S2 . This lemma holds.
6. S1 = S1′ ; s1 and S2 = S2′ ; s2 where both of the following hold:
′
• S2′ ≈S
ρ S1 ;
S
• s2 ≈ρ s1 ;
By Theorem 4 and the hypothesis IH, we show S2′ and S1′ terminate in the same way and produce the same output sequence
and when S2′ and S1′ both terminate, S2′ and S1′ have equivalent
terminating computation of variables used or defined in S2′ and
S1′ .
We show all the required conditions are satisfied for the application of the hypothesis IH.
′
• S2′ ≈S
ρ S1 .
By assumption.
• The sum of the program size of S1′ and S2′ is less than k,
size(S1′ ) + size(S2′ ) < k.
By definition, size(S2 ) = size(s2 ) + size(S2′ ) where
size(s2 ) < 1. Then, size(S2′ ) + size(S1′ ) < k + 1 −
size(s2 ) − size(s1 ) < k.
• Value stores σ1 and σ2 agree on values of output deciding
variables in S2′ and S1′ including the input, I/O sequence
variable.
By definition of TVaro and Impo , OVar(S2′ ) ⊆ OVar(S2 ).
The condition holds.
• Values of new configuration variables are consistent in the
value store σ2 and the specialization ρ, ∀id ∈ Dom(ρ) :
σ2 (id) = ρ(id).
By assumption.
By the hypothesis IH, one of the following holds:
(a) S1′ and S2′ both do not terminate.
By Lemma E.2, executions of S1 = S1′ ; s1 and S2 =
S2′ ; s2 both do not terminate and produce the same output
sequence.
(b) S1′ and S2′ both terminate.
∗
By assumption, (S2′ , m2 (σ2 )) → (skip, m′2 (σ2′ )),
∗
(S1′ , m1 (σ1 )) → (skip, m′1 (σ1′ )).
∗
By Corollary E.1, (S2′ ; s2 , m2 (σ2 )) → (s2 , m′2 (σ2′ )),
∗
′
′
′
(S1 ; s1 , m1 (σ1 )) → (s1 , m1 (σ1 )).
By the hypothesis IH, we show that s2 and s1 terminate
in the same way, produce the same output sequence and
when s2 and s1 both terminate, s2 and s1 have equivalent
computation of variables used or defined in s1 and s2 and
the input, and I/O sequence variables.
We need to show that all conditions are satisfied for the
application of the hypothesis IH.
• There are updates of “new configuration variables” between s2 and s1 ;
By assumption, s2 ≈S
ρ s1 .
• The sum of the program size s2 and s1 is less than or
equals to k;
By definition, size(S2′ ) ≥ 1, size(S1′ ) ≥ 1. Therefore,
size(s2 ) + size(s1 ) < k + 1 − size(S2′ ) − size(S1′ ) ≤ k.
• Value stores σ1′ and σ2′ agree on values of output deciding variables in s2 and s1 as well as the input, I/O
sequence variable.
By induction hypothesis IH, OVar(s1 ) ⊆ OVar(s2 ),
then Use(s2 ) ∩ Use(s1 ) = Use(s1 ). For any variable
id in OVar(s1 ), if id is in OVar(S1′ ), then the value of
id is same after the execution of S1′ and S2′ , σ1′ (id) =

We list properties of the update of new configuration variables
and the proof of backward compatibility for the case of loop statement as follows. We present one auxiliary lemma used in the proof
of Lemma 6.1.
Lemma 6.2. Let S2 be a statement sequence and S1 where there
are updates of “specializing new configuration variables” w.r.t
a mapping of new configuration variables ρ, S2 ≈S
ρ S1 . Then the
output deciding variables in S1 are a subset of the union of those
in S2 , OVar(S1 ) ⊆ OVar(S2 ).
Proof. By induction on the sum of the program size of S1 and
S2 .
Lemma 6.3. Let S1 = whilehn1 i (e) {S1′ } and S2 = whilehn2 i (e)
{S2′ } be two loop statements where all of the following hold:

• S2′ includes updates of “specializing new configuration vari-

′
′
ables” compared to S1′ , S2′ ≈S
ρ S1 where Dom(ρ) ∩ Def(S2 ) =
∅.
• the output deciding variables in S1 are a subset of those in S2 ,
OVar(S1 ) ⊆ OVar(S2 );
• When started in states agreeing on values of output deciding
variables in S1 and S2 including the input sequence variable
and the I/O sequence variable, ∀x ∈ OVar(S1 ) ∪ OVar(S2 ) ∪
{idI , idIO } ∀m′1 (σ1′ ) m′2 (σ2′ ) : (σ1′ (x) = σ2′ (x)), S1′ and S2′
terminate in the same way, produce the same output sequence,
and have equivalent computation of defined variables in S1′
and S2′ as well as the input sequence variable and the I/O
sequence variable ((S1′ , m1 ) ≡H (S2′ , m2 )) ∧ ((S1′ , m1 ) ≡O
(S2′ , m2 )) ∧ (∀x ∈ OVar(S1 ) ∪ OVar(S2 ) ∪ {idI , idIO } :
(S1′ , m1 ) ≡x (S2′ , m2 ));

If S1 and S2 start in states m1 (loop1c , σ1 ), m2 (loop2c , σ2 ) respectively, with loop counters of S1 and S2 not initialized (S1 , S2
have not executed yet), value stores agree on values of output deciding variables in S1 and S2 , then, for any positive integer i, one
of the following holds:

1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′′
′
2′
2′
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of output deciding variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal
to i, and there are no reachable configurations (S1 , m1 (loop1ci ,
σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 ,
m2 (σ2 )) where all of the following hold:

47

2015/9/14

• The loop counters of S1 and S2 are of value i, loop1c i (n1 )

i

→(whilehn1 i (0) {S1′ }, m1 (1/f)) for any i > 0
by the Crash rule.

= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in both S1 and S2 as well as the input sequence
variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in both S1 and S2 including the input sequence
variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).

Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. Therefore S1 and S2 terminate in the same way when
started from m1 and m2 respectively. Because σ1 (idIO ) =
σ2 (idIO ), the lemma holds.
2. The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
(0, vof ).
The execution of S1 continues as follows.
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((0, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(skip, m1 (σ1 )) by the Wh-F rule.

Proof. By induction on i.
Base case.
We show that, when i = 1, one of the following holds:

Similarly, the execution of S2 gets to the configuration (skip, m2 (σ2 )).
Loop counters of S1 and S2 are less than 1 and value stores
agree on values of used/defined variables in both S1 and S2
as well as the input sequence variable and the I/O sequence
variable.
3. The evaluation of e reduces to the same nonzero integer value,
E ′ JeKσ1 = E ′ JeKσ2 = (0, vof ).
Then the execution of S1 proceeds as follows:

1. Loop counters for S1 and S2 are always less than 1 if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1
and S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. Loop counters of S1 and S2 are of value less than or equal to 1
but there are no reachable configurations (S1 , m1 (loop1c1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ11 (x) = σ21 (x).
3. There are reachable configuration (S1 , m1 (loop1c1 , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ11 (x) = σ21 (x).

(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((v, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (v) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1′ ; whilehn1 i (e) {S1′ }, m1 (
loop1c ∪ {(n1 ) 7→ 1}, σ1 )) by the Wh-T rule.
Similarly, the execution of S2 proceeds to the configuration
(S2′ ; whilehn2 i (e) {S2′ }, m2 (loop2c ∪ {n2 7→ 1}, σ2 )).
By the hypothesis IH, we show that S1′ and S2′ terminate in
the same way and produce the same output sequence when
started in the state m1 (loop1c1 , σ1 ) and m2 (loop2c1 , σ2 ), and
S1′ and S2′ have equivalent computation of variables used or
defined in both statement sequences if both terminate. We need
to show that all conditions are satisfied for the application of the
hypothesis IH.
• variables in the domain of ρ are not redefined in the execution of S2′ .
The above three conditions are by assumption.
By definition, size(S1 ) = 1 + size(S1′ ). Then, size(S1′ ) +
size(S2′ ) = k + 1 − 2 = k − 1.
• Value stores σ1 and σ2 agree on values of used variables in
S1′ and S2′ as well as the input, I/O sequence variable.
By definition, OVar(S1′ ) ⊆ OVar(S1 ). So are the cases
to S2′ and S2 . In addition, value stores σ1 and σ2 are not
changed in the evaluation of the predicate expression e. The
condition holds.
• Values of new configuration variables are consistent in the
value store σ2 and the specialization ρ, ∀id ∈ Dom(ρ) :
σ2 (id) = ρ(id).
By assumption.
By assumption, S1′ and S2′ terminate in the same way and
produce the same output sequence when started in states
m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ). In addition, S1′ and S2′ have
equivalent computation of variables used or defined in S1′ and
S2′ when started in states m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ).
Then there are two cases.

By definition, variables used in the predicate expression e of S1
and S2 are used in S1 and S2 , Use(e) ⊆ OVar(S1 ) ∩ OVar(S2 ). By
assumption, value stores σ1 and σ2 agree on values of variables in
Use(e), the predicate expression e evaluates to the same value w.r.t
value stores σ1 and σ2 . There are three possibilities.
1. The evaluation of e crashes,
E ′ JeKσ1 = E ′ JeKσ2 = (error, vof ).
The execution of S1 continues as follows:
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
→(whilehn1 i ((error, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (1/f))
by the ECrash rule

48

2015/9/14

(a) S1′ and S2′ both do not terminate and produce the same
output sequence.
By Lemma E.2, S1′ ; S1 and S2′ ; S2 both do not terminate
and produce the same output sequence.
(b) S1′ and S2′ both terminate and have equivalent computation
of variables used or defined in S1′ and S2′ .
∗
By assumption, (S1′ , m1 (loop′c , σ1 )) → (skip, m′1 (loop′′c , σ1′ ));
∗
′
′
′
(S2 , m2 (loopc , σ2 )) → (skip, m2 (loop′′c , σ2′ )) where ∀x ∈
(OVar(S1′ ) ∩ OVar(S2′ )) ∪ {idI , idIO }, σ1′ (x) = σ2′ (x).
Because S1 and S2 have the same predicate expression,
variables used in the predicate expression of S1 and S2
are not in the domain of ρ. By assumption, OVar(S1′ ) ⊆
OVar(S2′ ) ⊆ OVar(S2′ ) ∪ Dom(ρ) and OVar(S1′ ) ⊆
OVar(S2′ ). Then variables used in the predicate expression
of S1 and S2 are either in variables used or defined in both
S1′ and S2′ or not. Therefore value stores σ2′ and σ1′ agree
on values of variables used in the expression e and even
variables used or defined in S1 and S2 .
Induction step on iterations
The induction hypothesis (IH) is that, when i ≥ 1, one of the
following holds:
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1
and S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci+1 , σ1i+1 )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i+1 ))
from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i + 1,
loop1ci+1 (n1 ) = loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
By hypothesis IH, there is no configuration where loop counters
of S1 and S2 are of value i + 1 when any of the following holds:
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
′′
′
1
1
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′′
′
2′
2′
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1
and S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables in
both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).
When there are reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )

= loop2ci (n2 ) = i.
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables in
both S1 and S2 as well as the input sequence variable and
the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

Then we show that, when i + 1, one of the following holds: The
induction hypothesis (IH) is that, when i ≥ 1, one of the following
holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
′′
′
1
1
2
(S1 , m1 (loopc )), loopc (n1 ) < i+1, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i + 1, S1 and S2 terminate
in the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,

By similar argument in base case, we have one of the following
holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
49

2015/9/14

1: enum id {o1 }
2: a : enum id
3: If (a == o1 ) then
4:
output 2 + c
5:
6:

1’:
2’:
3’:
4’:
5’:
6’:

old

enum id {o1 , o2 }
a : enum id
If (a == o1 ) then
output 2 + c
If (a == o2 ) then
output 3 + c

2. EN1 , EN2 include more than one enumeration type definitions
EN1 = “enum id {el1 }, EN′1 ”, EN2 = “enum id {el2 }, EN′2 ”
where one of the following holds:
(a) (EN′1 ⊂ EN′2 ) and (el1 = el2 ) ∨ (el2 = el1 , el);
(b) (EN′1 ⊂ EN′2 ) ∨ (EN′1 = EN′2 ) and “enum id {el1 }” ⊂
“enum id {el2 }”.

new

Figure 16: Enumeration type extension
′

′

∗

(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i+1, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i + 1, S1 and S2 terminate
in the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∩ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci+1 , σ1i+1 )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 ,
σ2i+1 )) from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI+1 , idIO } : σ1i+1 (x) = σ2i+1 (x).
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i+1 ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i+1 )) from (S2 ,
m2 (σ2 )) where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI+1 , idIO } : σ1i+1 (x) = σ2i+1 (x).

6.2 Proof rule for enumeration type extension
Enumeration types allow developers to list similar items. New code
is usually accompanied with the introduction of new enumeration
labels. Figure 16 shows an example of the update. The new enum
label o2 gives a new option for matching the value of the variable a,
which introduce the new code b := 3 + c. To show updates “enumeration type extension” to be backward compatible, we assume
that values of enum variables, used in the If-predicate introducing
the new code, are only from inputs that cannot be translated to new
enum labels.
In order to have a general definition of the update class, we show
a relation between two sequences of enumeration type definitions,
called proper subset.

Definition 27. (Enumeration type extension) Let P1 , P2 be two
programs where enumeration type definitions EN1 in P1 are a
subset of EN2 in P2 , EN1 ⊂ EN2 and E are new enum labels
in P2 . A statement sequence S2 in a program P2 includes updates
of enumeration type extension compared with a statement sequence
S1 in P1 , written S2 ≈S
E S1 , iff one of the following holds:

1. S2 = “If(id==l) then{S2t } else{S2f }” and all of the following
hold:
• l ∈ E;
• The variable id is not lvalue in an assignment statement,
“id := e” ∈
/ P2 ;
• S2f ≈S
E S1 ;
2. S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
f
f
t
S
where (S2t ≈S
E S1 ) ∧ (S2 ≈E S1 );
′
3. S1 = “whilehn1 i (e) {S1 }”, S2 = “whilehn2 i (e) {S2′ }” where
′
S2′ ≈S
E S1 ;
S
4. S1 ≈O S2 ;
′
′
S
5. S1 = S1′ ; s1 and S2 = S2′ ; s2 where (S2′ ≈S
E S1 ) ∧ (S2 ≈H
′
′
S1 ) ∧ ∀x ∈ Imp(s1 , idIO ) ∪ Imp(s1 , idIO ) : (S2 ≈S
x
S1′ ) ∧ (s2 ≈S
E s1 ).

We show that two programs terminate in the same way, produce
the same output sequence, and have equivalent computation of
variables defined in both of them in executions if there are updates
of enumeration type extension between them.
Lemma 6.4. Let S1 and S2 be two statement sequences in programs P1 and P2 respectively where there are updates of enumeration type extensions in S2 of P2 compared with S1 of P1 , S2 ≈S
E S1 .
If S1 and S2 start in states m1 (σ1 ) and m2 (σ2 ) such that both of
the following hold:
• Value stores σ1 and σ2 agree on values of output deciding vari-

ables in both S1 and S2 including the input sequence variable
and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪
{idI , idIO } : σ1 (x) = σ2 (x);
• No variables used in S2 are of initial value of enum labels in
E, ∀x ∈ Use(S2 ) : (σ2 (x) ∈
/ E);
• No inputs are translated to any label in E during the execution
of S2 ;
then S1 and S2 terminate in the same way, produce the same output
sequence, and when S1 and S2 both terminate, they have equivalent
computation of used variables and defined variables,
• (S1 , m1 ) ≡H (S2 , m2 );
• (S1 , m1 ) ≡O (S2 , m2 );
• ∀x ∈ OVar(S1 ) ∪ OVar(S2 ) : (S1 , m1 ) ≡x (S2 , m2 );

Proof. By induction on the sum of the program size of S1 and S2 ,
size(S1 ) + size(S1 ).
Base case. S1 is a simple statement s, and S2 = “If(id==l) then{st2 }
else{sf2 }” where all of the following hold:

Definition 26. (Extension relation of enumeration types) Let
EN1 , EN2 be two different sequences of enumeration type definitions. EN1 is a subset of EN2 , written EN1 ⊂ EN2 , iff one of the
following holds:

• l ∈ E;
• st2 , sf2 are two simple statements;
• sf2 = s;

1. EN1 = “enum id {el1 }”, EN2 = “enum id {el2 }” where labels in type “enum id” in EN1 are a subset of those in EN2 ,
el2 = el1 , el and el 6= ∅;
50

2015/9/14

We informally argue that the value of the variable id in the
predicate expression of S2 only coming from an input value or
the initial value. There are three ways a scalar variable is defined:
the execution of an assignment statement, the execution of an
input statement or the initial value. Because id is not lvalue in
an assignment statement, then the value of id is only from the
execution of an input statement or the initial value.
In addition, by assumption, any output deciding variable is not
of the initial value of enum label in E; no input values are translated
into an enum label in E. Then the execution of S2 proceeds as
follows:

By the hypothesis IH, we show the lemma holds. We need
to show that all conditions are satisfied for the application
of the hypothesis IH.
f
• S2f ≈S
E S1
By assumption.
• The sum of the program size of S1f and S2f is less than
k, size(S1f ) + size(S2f ) < k.
By definition, size(S1 ) = 1+size(S1t )+size(S1f ). Then,
size(S1f ) + size(S2f ) < k + 1 − 2 = k − 1.
• Value stores σ1 and σ2 agree on values of output deciding variables in S1f and S2f including the input, I/O
(If(id==l) then{st2 } else{sf2 }, m2 (σ2 ))
sequence variable.
→(If(0) then{st2 } else{sf2 }, m2 (σ2 ))
By definition, OVar(S1f ) ⊆ OVar(S1 ). So are the cases
by the rule Eq-F
to S2f and S2 . In addition, value stores σ1 and σ2 are not
→(sf2 , m2 (σ2 )) by the If-F rule.
changed in the evaluation of the predicate expression e.
The condition holds.
The value store σ2 is not updated in the execution of S2 so far.
• There are no inputs translated to enum labels in E in
By assumption, value stores σ1 and σ2 agree on values of output
S2f ’s execution.
deciding variables in both S1 and S2 .
By assumption.
By Theorem 2 and 4, S1 and S2 terminate in the same way,
By the hypothesis IH, the lemma holds.
produce the same I/O sequence. The lemma holds.
(c) The evaluation of e reduces to the same nonzero integer
Induction step.
value, E ′ JeKσ1 = E ′ JeKσ2 = (0, vof ).
The hypothesis is that this lemma holds when the sum k of the
By
similar to the second subcase above.
program size of S1 and S2 are great than or equal to 4, k ≥ 4.
2. S1 and S2 are both “while” statements:
We then show that this lemma holds when the sum of the
S1 = “whilehn1 i (e) {S1′ }”, S2 = “whilehn2 i (e) {S2′ }” where
program size of S1 and S2 is k + 1. There are cases regarding
S
′
S2 ≈E S1 .
S2′ ≈S
E S1 ;
By Lemma 6.6, we show the lemma holds. We need to show all
1. S1 and S2 are both “If” statement:
the required conditions for the application of Lemma 6.6 holds
f
f
t
t
S1 = “If(e) then{S1 } else{S1 }”, S2 = “If(e) then{S2 } else{S2 }”
(a) No variables are of initial values as new enum labels in E;
where both of the following hold
(b) Value stores σ1 and σ2 agree on values of variables used in
t
• S2t ≈S
E S1 ;
both S1 and S2 ;
f S f
• S2 ≈ E S1 ;
(c) Enumeration types in P1 are a subset of those in P2 ;
By the definition of Impo (S1 ), variables used in the predicate
The above three conditions are by assumption.
expression e are a subset of output deciding variables in S1
(d) The output deciding variables in S1′ are a subset of those in
and S2 , Use(e) ⊆ OVar(S1 ) ∩ OVar(S2 ). By assumption,
S2′ ;
corresponding variables used in e are of same value in value
The above condition is by Lemma 6.5.
stores σ1 and σ2 . By Lemma D.1, the expression evaluates to
(e) S1′ and S2′ produce the same output sequence, terminate in
the same value w.r.t value stores σ1 and σ2 . There are three
the same way and have equivalent computation of defined
possibilities.
variables in both S1′ and S2′ when started in states agreeing
(a) The evaluation of e crashes, E ′ JeKσ1 = E ′ JeKσ2 =
on values of variables used in both S1′ and S2′ ;
(error, vof ).
Because size(S1 ) = size(S1′ ) + 1, then the condition holds
The execution of S1 continues as follows:
by
the induction hypothesis.
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
By
Lemma
6.6, the lemma holds.
→(If((error, vof )) then{S1t } else{S1f }, m1 (σ1 ))
3. S2 = “If(id==l) then {S2t } else {S2f }” such that both of the
by the rule EEval’
following hold:
→(If(0) then{S1t } else{S1f }, m1 (1/f))
• The label l is in E, l ∈ E;
by the ECrash rule
• The variable id is not lvalue in an assignment statement,
i
→(If(0) then{S1t } else{S1f }, m1 (1/f)) for any i > 0
∄“id := e” in S2 ;
by the Crash rule.
• There are updates of enumeration type extension from S2f
Similarly, the execution of S2 started from the state m2 (σ2 )
to S1 , S2f ≈S
E S1 ;
crashes. The lemma holds.
By
Lemma
6.6,
we show this lemma holds. We need to show all
(b) The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
the conditions are satisfied for the application of Lemma 6.6.
(0, vof ).
• S1′ and S2′ have same set of output deciding variables,
The execution of S1 continues as follows.
OVar(S1′ ) = OVar(S2′ ) = OVar(S);
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
• The output deciding variables in S1′ are a subset of those in
= (If((0, vof )) then{S1t } else{S1f }, m1 (σ1 ))
S2′ , OVar(S1′ ) ⊆ OVar(S2′ );
by the rule EEval’
By Lemma 6.5.
→(If(0) then{S1t } else{S1f }, m1 (σ1 ))
• There are no inputs translated to enum labels in the set E.
by the E-Oflow1 or E-Oflow2 rule
By assumption.
→(S1f , m1 (σ1 )) by the If-F rule.
• When started in states m′1 (σ1′ ), m′2 (σ1′ ) where value stores
σ1′ and σ2′ agree on values of output deciding variables in
Similarly, the execution of S2 gets to the configuration
both S1′ and S2′ as well as the input sequence variable and
(S2f , m2 (σ2 )).
51

2015/9/14

• There are no inputs translated to enum labels in E in S2′ ’s

the I/O sequence variable, and there are no inputs translated
to enum labels in E, then S1′ and S2′ produce the same
output sequence.
By the induction hypothesis IH. This is because the sum of
the program size of S1′ and S2′ is less than k. By definition,
size(S1 ) = 1 + size(S1′ ).
4. S2 = “If(id==l) then {S2t } else {S2f }” such that both of the
following hold:
• The label l is in E, l ∈ E;
• The variable id is not lvalue in an assignment statement,
“id := e” ∈
/ S2 ;
• There are updates of enumeration type extension from S2f
to S1 , S2f ≈S
E S1 ;
We informally argue that the value of the variable id in the predicate expression of S2 only coming from an input value or the
initial value. There are several ways a scalar variable is defined:
the execution of an assignment statement, the execution of an
input statement or the initial value. Because id is not lvalue in
an assignment statement, then the value of id is only from the
execution of an input statement or initial value. In addition, by
assumption, any used variable is not of initial value of enum
label in E; no input values are translated into an enum label in
E.
Then the expression id==l evaluates to 0. The execution of S2
proceeds as follows.

execution.
• Initial values of used variables in S2′ are not enum labels in

E.
The above three conditions are by assumption.
• The sum of the program size of S1′ and S2′ is less than k,
size(S1′ ) + size(S2′ ) < k.
By definition, size(S1 ) = size(S1′ ) + size(s1 ). Then,
size(S1′ ) + size(S2′ ) < k + 1 − size(s2 ) − size(s1 ) < k.
• Value stores σ1 and σ2 agree on values of used variables in
both S1′ and S2′ as well as the input, output, I/O sequence
variable.
By definition, OVar(S2′ ) ⊆ OVar(S2 ), OVar(S1′ ) ⊆ OVar(S1 ).
In addition, value stores σ1 and σ2 are not changed in
the evaluation of the predicate expression e. The condition
holds.
By the hypothesis IH, one of the following holds:
(a) S1′ and S2′ both do not terminate.
By Lemma E.2, executions of S1 = S1′ ; s1 and S2 =
S2′ ; s2 both do not terminate and produce the same output
sequence.
(b) S1′ and S2′ both terminate.
∗
∗
By assumption, (S2′ , m2 (σ2 )) → (skip, m′2 (σ2′ )), (S1′ , m1 (σ1 )) →
′
′
(skip, m1 (σ1 )).
∗
By Corollary E.1, (S2′ ; s2 , m2 (σ2 )) → (s2 , m′2 (σ2′ )),
∗
′
′
′
(S1 ; s1 , m1 (σ1 )) → (s1 , m1 (σ1 )).
By the hypothesis IH, we show that s2 and s1 terminate
in the same way, produce the same output sequence and
when s2 and s1 both terminate, s2 and s1 have equivalent
computation of variables used or defined in s1 and s2 and
the input, output, and I/O sequence variables.
We need to show that all conditions are satisfied for the
application of the hypothesis IH.
• There are updates of “enumeration type extension” between s2 and s1 ;
• There are no input values translated into enum labels in
E in the execution of s2 ;
The above two conditions are by assumption.
• The sum of the program size s2 and s1 is less than or
equals to k;
By definition, size(S2′ ) ≥ 1, size(S1′ ) ≥ 1. Therefore,
size(s2 ) + size(s1 ) < k + 1 − size(S2′ ) − size(S1′ ) ≤ k.
• Value stores σ1′ and σ2′ agree on values of used variables
in s2 and s1 as well as the input, output, I/O sequence
variable.
By Lemma 6.5, OVar(s1 ) ⊆ OVar(s2 ), then OVar(s2 )
∩ OVar(s1 ) = OVar(s1 ). Similarly, by Lemma 6.5,
OVar(S1′ ) ⊆ OVar(S2′ ). For any variable id in OVar(s1 ),
if id is not in OVar(S1′ ), then the value of id is not
changed in the execution of S1′ and S2′ , σ1′ (id) =
σ1 (id) = σ2 (id) = σ2′ (id). Otherwise, the variable id
is defined in the execution of S1′ and S2′ , by assumption,
σ1′ (id) = σ2′ (id). The condition holds.
• Values of used variables in s2 are not of value as enum
labels in E, ∀id ∈ OVar(s2 ) : σ2′ (id) ∈ E.
By assumption, initial values of used variables in s2
are not of values as enum labels in E. S2′ and S1′ have
equivalent computation of defined variables in S2′ and
S1′ . Because enum labels are not defined in P1 , defined
variables in the execution of S2′ and S1′ are not of values
as enum labels in E.
By the hypothesis IH, the lemma holds.

(If(id==l) then{S2t } else{S2f }, m2 (σ2 ))
→(If(v==l) then{S2t } else{S2f }, m2 (σ2 )) where v 6= l
by the rule Var
→(If(0) then{S2t } else{S2f }, m1 (σ2 ))
by the Eq-F rule
→(S2f , m2 (σ2 )) by the If-F rule.
By the hypothesis IH, we show the lemma holds. We need
to show the conditions are satisfied for the application of the
hypothesis IH.
• S2f ≈S
E S1
• There are no inputs translated to enum labels in E in S2f ’s
execution.
• Initial values of used variables in S2 are not enum labels in
E.
The above three conditions are by assumption.
• The sum of the program size of S1 and S2f is less than k,
size(S1 ) + size(S2f ) < k.
By definition, size(S1 ) = 1 + size(S1t ) + size(S1f ). Then,
size(S1 ) + size(S2f ) < k + 1 − 1 − size(S2t ) < k.
• Value stores σ1 and σ2 agree on values of used variables in
both S1 and S2f as well as the input, I/O sequence variable.
By definition, OVar(S2f ) ⊆ OVar(S2 ). In addition, value
stores σ1 and σ2 are not changed in the evaluation of the
predicate expression e. The condition holds.
By the hypothesis IH, this lemma holds.
5. S1 = S1′ ; s1 and S2 = S2′ ; s2 such that both of the following
hold:
′
• S2′ ≈S
E S1 ;
S
• s2 ≈E s1 ;

By the hypothesis IH, we show S2′ and S1′ terminate in the same
way, produce the same output sequence, and have equivalent
computation of defined variables in S1 and S2 . We need to
show that all the conditions are satisfied for the application of
the hypothesis IH.
′
• S2′ ≈S
E S1 ;
52

2015/9/14

• Value stores σ1i and σ2i agree on values of output deciding

We show a auxiliary lemma telling that the two programs with
updates of enumeration type extension have same set of used variables and the same set of defined variables.

variables in both S1 and S2 as well as the input sequence
variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).

Lemma 6.5. If there are updates of enumeration type extension
in a statement sequence S2 against a statement sequence S1 ,
S2 ≈S
E S1 , then the output deciding variables in S1 are a subset
of those in S2 , OVar(S1 ) ⊆ OVar(S2 ).

Proof. By induction on i.
Base case.
We show that, when i = 1, one of the following holds:

Proof. By induction on the sum of the program size of S1 and
S2 .
Lemma 6.6. Let S1 = whilehn1 i (e) {S1′ } and S2 = whilehn2 i (e)
{S2′ } be two loop statements in programs P1 and P2 respectively
where all of the following hold:
• Enumeration types EN 1 in P1 are a proper subset of EN 2 in

P2 , EN 1 ⊂ EN 2 , such that there are a set of enum labels E
only defined in P2 ;
• When started in states agreeing on values of output deciding
variables in both S1′ and S2′ as well as the input sequence
variable and the I/O sequence variable, initial values of used
variables in S2′ are not enum labels in E, and there are no
inputs in S2 ’s execution translated into any label in E, ∀x ∈
OVar(S1′ ) ∪ {idI , idIO } ∀m1 (σ1 ) m2 (σ2 ) : σ1 (x) = σ2 (x),
and S1′ and S2′ terminate in the same way, produce the same
output sequence, and have equivalent computation of defined
variables in S1′ and S2′ as well as the input sequence variable
and the I/O sequence variable ((S1′ , m1 ) ≡H (S2′ , m2 )) ∧
((S1′ , m1 ) ≡O (S2′ , m2 )) ∧ (∀x ∈ OVar(S1 ) ∪ OVar(S2 ) ∪
{idI , idIO } :
(S1′ , m1 ) ≡x (S2′ , m2 ));

If S1 and S2 start in states m1 (loop1c , σ1 ), m2 (loop2c , σ2 ), with
loop counters of S1 and S2 not initialized (S1 , S2 have not executed
yet), value stores agree on values of output deciding variables in
S1 and S2 as well as the input sequence variable, the I/O sequence
variable, initial values of used variables in S2 are not of values as
enum labels in E, no inputs are translated into enum labels in E,
then, for any positive integer i, one of the following holds:

1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of output deciding variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in both S1 and S2 as well as the input sequence
variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∩
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
53

1. Loop counters for S1 and S2 are always less than 1 if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′′
′
2′
2′
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈
(Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );
2. Loop counters of S1 and S2 are of value less than or equal to 1
but there are no reachable configurations (S1 , m1 (loop1c1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ11 (x) = σ21 (x).
3. There are reachable configuration (S1 , m1 (loop1c1 , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ11 (x) = σ21 (x).
By definition, variables used in the predicate expression e of S1
and S2 are used in S1 and S2 , Use(e) ⊆ Use(S1 ) ∩ Use(S2 ). By
assumption, value stores σ1 and σ2 agree on values of variables in
Use(e), the predicate expression e evaluates to the same value w.r.t
value stores σ1 and σ2 by Lemma D.2. There are three possibilities.
1. The evaluation of e crashes,
E ′ JeKσ1 = E ′ JeKσ2 = (error, vof ).
The execution of S1 continues as follows:
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
→(whilehn1 i ((error, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (1/f))
by the ECrash rule
i
→(whilehn1 i (0) {S1′ }, m1 (1/f)) for any i > 0
by the Crash rule.
Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. Therefore S1 and S2 terminate in the same way when
started from m1 and m2 respectively. Because σ1 (idIO ) =
σ2 (idIO ), the lemma holds.
2. The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
(0, vof ).
The execution of S1 continues as follows.

2015/9/14

(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((0, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(skip, m1 (σ1 )) by the Wh-F rule.

of S1 and S2 are either in variables used or defined in both
S1′ and S2′ or not. Therefore value stores σ2′ and σ1′ agree
on values of variables used in the expression e and even
variables used or defined in S1 and S2 .
Induction step on iterations
The induction hypothesis (IH) is that, when i ≥ 1, one of the
following holds:

Similarly, the execution of S2 gets to the configuration (skip,
m2 (σ2 )). Loop counters of S1 and S2 are less than 1 and value
stores agree on values of used/defined variables in both S1 and
S2 as well as the input sequence variable, and the I/O sequence
variable.
3. The evaluation of e reduces to the same nonzero integer value,
E ′ JeKσ1 = E ′ JeKσ2 = (0, vof ).
Then the execution of S1 proceeds as follows:

1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of used/defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈
(Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables in
both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables in
both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((v, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (v) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1′ ; whilehn1 i (e) {S1′ }, m1 (
loop1c [1/n1 ], σ1 )) by the Wh-T rule.

Similarly, the execution of S2 proceeds to the configuration
(S2′ ; whilehn2 i (e) {S2′ }, m2 (loop2c [1/n1 ], σ2 )).
By the assumption, we show that S1′ and S2′ terminate in the
same way and produce the same output sequence when started
in the state m1 (loop1c1 , σ1 ) and m2 (loop2c1 , σ2 ) respectively,
and S1′ and S2′ have equivalent computation of variables defined
in both statement sequences if both terminate. We need to
show that all conditions are satisfied for the application of the
assumption.
• There are no inputs translated into enum labels in E in the
execution of S2′ .
The above condition is by assumption.
• Initial values of used variables in S2′ are not enum labels in
E.
By the definition of used variables, Use(S2′ ) ⊆ Use(S2 ).
By assumption, initial values of used variables in S2 are not
enum labels in E. The condition holds.
Then we show that, when i + 1, one of the following holds: The
• Value stores σ1 and σ2 agree on values of used variables
induction hypothesis (IH) is that, when i ≥ 1, one of the following
in S1′ and S2′ as well as the input, output, I/O sequence
holds:
variable.
By definition, Use(S1′ ) ⊆ Use(S1 ). So are the cases to
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
S2′ and S2 . In addition, value stores σ1 and σ2 are not
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
changed in the evaluation of the predicate expression e. The
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i+1, (S2 , m2 (loop2c , σ2 )) →
condition holds.
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i + 1, S1 and S2 terminate
By assumption, S1′ and S2′ terminate in the same way and
in
the same way, produce the same output sequence, and have
produce the same output sequence when started in states
equivalent computation of used/defined variables in both S1 and
m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ). In addition, S1′ and S2′ have
S2 and the input sequence variable, the I/O sequence variable,
equivalent computation of variables used or defined in S1′ and
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈
S2′ when started in states m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ).
(Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );
Then there are two cases.
2. The loop counter of S1 and S2 are of value less than or
(a) S1′ and S2′ both do not terminate and produce the same
equal to i + 1, and there are no reachable configurations
output sequence.
(S1 , m1 (loop1ci+1 , σ1i+1 )) from (S1 , m1 (σ1 )),
By Lemma E.2, S1′ ; S1 and S2′ ; S2 both do not terminate
(S2 , m2 (loop2ci+1 , σ2i+1 )) from (S2 , m2 (σ2 )) where all of the
and produce the same output sequence.
′
′
following hold:
(b) S1 and S2 both terminate and have equivalent computation
• The loop counters of S1 and S2 are of value i + 1,
of variables defined in S1′ and S2′ .
∗
loop1ci+1 (n1 ) = loop2ci+1 (n2 ) = i + 1.
By assumption, (S1′ , m1 (loop′c , σ1 )) → (skip, m′1 (loop′′c , σ1′ ));
∗
• Value stores σ1i+1 and σ2i+1 agree on values of used vari(S2′ , m2 (loop′c , σ2 )) → (skip, m′2 (loop′′c , σ2′ )) where ∀x ∈
ables in both S1 and S2 as well as the input sequence vari(Def(S1′ ) ∩ Def(S2′ )) ∪ {idI , idIO }, σ1′ (x) = σ2′ (x).
able, and the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩
By assumption,Use(S1′ ) ⊆ Use(S2′ ) and Def(S1′ ) =
Use(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
Def(S2′ ). Then variables used in the predicate expression
54

2015/9/14

3. There are reachable configurations (S1 , m1 (loopc1i+1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loopc1i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable, and the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩
Use(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).

• Value stores σ1i+1 and σ2i+1 agree on values of used vari-

ables in both S1 and S2 as well as the input sequence variable, and the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩
Use(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i+1 ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i+1 )) from (S2 ,
m2 (σ2 )) where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i.
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable and the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩
Use(S2 )) ∪ {idI+1 , idIO } : σ1i+1 (x) = σ2i+1 (x).

By hypothesis IH, there is no configuration where loop counters
of S1 and S2 are of value i + 1 when any of the following holds:
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of used/defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈
(Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables in
both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

6.3 Proof rule for variable type weakening
In programs, variable types are changed either to allow for larger
ranges (weakening). For example, an integer variable might be
changed to become a long variable to avoid integer overflow.
Adding a new enumeration value can is also type weakening. Increasing array size is another example of weakening. Allowing for
type weakening is essentially an assumption about the intent behind the update. The kinds of weakening that should be allowed
are application dependent and would need to be defined by the user
in general. The type weakening considered are either changes of
type Int to Long or increase of array size. These updates fix integer overflow or array index out of bound. In order to prove the
update of variable type weakening to be backward compatible, we
assume that there are no integer overflow and array index out of
bound in execution of the old program and the updated program.
In conclusion, the old program and the new program produce the
same output sequence because the integer overflow and index out
of bound errors fixed by the new program do not occur.
We formalize the update of variable type weakening, then we
show that the updated program produce the same output sequence
as the old program in executions if there are no integer overflow
or index out of bound exceptions related to variables with type
changes. First, we define a relation between variable definitions
showing the type weakening.

When there are reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )

= loop2ci (n2 ) = i.

• Value stores σ1i and σ2i agree on values of used variables in

both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

Definition 28. (Cases of type weakening) We say there is type
weakening from a sequence of variable definitions V1 to V2 , written
V1 րτ V2 , iff one of the following holds:

By similar argument in base case, we have one of the following
holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i+1, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of used/defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable,
(S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈
(Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x (S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci , σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i ))
from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
55

1. V1 = “Int id”, V2 = “Long id”;
2. V1 = “τ id[n2 ]”, V2 = “τ id[n1 ]” where n2 > n1 ;
3. V1 = V1′ , “τ1 id1 ”, V2 = V2′ , “τ2 id2 ” where (V1′ րτ V2′ ) ∧
(“τ1 id1 ” րτ “τ2 id2 ”);
4. V1 = V1′ , “τ1 id1 [n1 ]”, V2 = V2′ , “τ2 id2 [n2 ]” where (V1′ րτ
V2′ ) ∧ (“τ1 id1 [n1 ]” րτ “τ2 id2 [n2 ]”);
The following is the generalized definition of variable type
weakening.
Definition 29. (Variable type weakening) We say that there
are updates of variable type weakening in the program P2 =
P mpt; EN ; V2 ; Sentry compared with the program P1 = P mpt;
EN ; V1 ; Sentry , written P2 ≈S
τ P1 , iff V1 րτ V2 .
We show that two programs terminate in the same way, produce
the same output sequence, and have equivalent computation of
defined variables in both programs in valid executions if there are
updates of variable type weakening between them.

2015/9/14

1’: If (1/(a − 5)) then
2’:
skip
3’: output a

1:
2:
3: output a

Lemma 6.7. Let P1 = EN ; V1 ; Sentry and P2 = EN ; V2 ; Sentry
be two programs where there are updates of variable type weakening, P2 ≈S
τ P1 . If the programs P1 and P2 start in states m1 (σ1 )
and m2 (σ2 ) such that both of the following hold:

old

new

• Value stores σ1 and σ2 agree on values of variables used in

Sentry as well as the input sequence variable, the I/O sequence
variable, ∀x ∈ Use(Sentry ) ∪ {idI , idIO } : σ1 (x) = σ2 (x);
• There is no integer overflow or index out of bound exceptions
related to variables of type change;

Figure 17: Exit-on-error
6.4 Proof rule for exit on errors

then Sentry in the program P1 and P2 terminate in the same
way, produce the same output sequence, and when Sentry both
terminate, they have equivalent computation of defined variables
in Sentry in both programs as well as the input sequence variable,
the I/O sequence variable,

Another bugfix is called “exit-on-error”, which causes the program to exit in observation of application-semantic-dependent errors. Figure 17 shows an example of exit-on-error update. In the
example, the fixed bugs refer to the program semantic error that
a = 5. Instead of using an “exit” statement, we rely on the crash
from expression evaluations to formalize the update class. In order
to prove the update of exit-on-error to be backward compatible, we
assume that there are no application related error in executions of
the old program. Therefore, the two programs produce the same
output sequence because the extra check does not cause the new
program’s execution to crash.
The following is the generalized definition of the update class
“exit-on-error”.

• (Sentry , m1 ) ≡H (Sentry , m2 );
• (Sentry , m1 ) ≡O (Sentry , m2 );
• ∀x ∈ Def(S) ∪ {idI , idIO } :

(Sentry , m1 ) ≡x (Sentry , m2 );

Because Sentry are the exactly same in both programs P1 and
P2 , we omit the straightforward proof. Instead, we show that,
if there is no array index out of bound and integer overflow in
executions of the old program, then there is no array index out of
bound or integer overflow in executions of updated program due to
the increase of array index and change of type Int to Long.

Definition 30. (Exit on error) We say a statement sequence S2 includes updates of exit-on-err from a statement sequence S1 , written
S2 ≈ S
Exit S1 , iff one of the following holds:
1. S2 = “If(e) then{skip} else{skip}”; S1 ;
2. S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
where both of the following hold
t
• S2t ≈S
Exit S1 ;
f
• S2f ≈S
S
Exit 1 ;
3. S1 = “while hn1 i (e) {S1′ }”, S2 = “whilehn2 i (e) {S2′ }” where
′
S2′ ≈S
Exit S1 ;
4. S1 ≈S
S
O 2;
5. S1 = S1′ ; s1 and S2 = S2′ ; s2 such that both of the following hold:
′
• S2′ ≈S
Exit S1 ;
′;
• S2′ ≈S
S
H 1
′
• ∀x ∈ Imp(s1 , idIO ) ∪ Imp(s1 , idIO ) : S2′ ≈S
x S1 ;
• s 2 ≈S
s
;
1
Exit

Proof. The proof is straightforward because the statement sequence
S is same in programs P1 and P2 . The only point is that if there is
array index out of bound or integer overflow in execution of S in
P1 , then there is no array index out of bound or integer overflow in
execution of S in P2 . To show the point, we present the argument
for the array index out of bound and integer overflow separately.
1. We show that, as to one expression id1 [id2 ], there is no array
index out of bound in P2 if there is no array index out of bound
in P1 when P1 and P2 are in states agreeing on values of used
variables in P1 and P2 ;
(id1 [id2 ], m1 (σ1 ))
→(id1 [v], m1 (σ1 )) by the rule Var

Though the bugfix in Definition 30 is not in rare execution in
the first case, the definition shows the basic form of bugfix clearly.
We show that two programs terminate in the same way, produce
the same output sequence, and have equivalent computation of
defined variables in both programs in valid executions if there are
updates of exit-on-error between them.

Similarly, (id1 [id2 ], m2 (σ2 )) → (id1 [v], m2 (σ2 )). By Definition 28, the array bound of id1 in P2 is no less than that in P1 ,
then there is no array out of bound exception in evaluation of
id1 [id2 ] in P2 if there is no array out of bound exception in
evaluation of id1 [id2 ] in P1 .
2. We show that, as to one expression e, there is no integer overflow in evaluation of e in P2 if there is no integer overflow in
evaluation of e in P1 ;

Lemma 6.8. Let S1 and S2 be two statement sequences respectively where there are updates of exit-on-error in S2 against S1 ,
S2 ≈ S
Exit S1 . If S1 and S2 start in states m1 (σ1 ) and m2 (σ2 ) such
that both of the following hold:
• Value stores σ1 and σ2 agree on values of variables used in both

(e, m1 (σ1 ))
→((ve , vof ), m1 (σ1 )) by the rule EEval’

S1 and S2 as well as the input sequence variable and the I/O
sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI , idIO } :
σ1 (x) = σ2 (x);
• There are no program semantic errors related to the extra check
in the update of exit-on-error in the execution of S1 ;

When every used variable in the expression e is of same type
in P1 and P2 , then the evaluation of the expression e in P2 is
of the same result (ve , vof ) in P1 . When every used variable in
the expression e is of type Int in P1 and of type Long P2 , then
there is no integer overflow in the evaluation of the expression
e in P2 if there is no integer overflow in the evaluation of the
expression e in P1 . This is because the values of type Long are
a superset of those of type Int.

then S1 and S2 terminate in the same way, produce the same output
sequence, and when S1 and S2 both terminate, they have equivalent
computation of defined variables in both S1 and S2 as well as the
input sequence variable and the I/O sequence variable,
• (S1 , m1 ) ≡H (S2 , m2 );
• (S1 , m1 ) ≡O (S2 , m2 );
56

2015/9/14

→(If(0) then{S1t } else{S1f }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1f , m1 (σ1 )) by the If-F rule.
Proof. By induction on the sum of the program size of S1 and S2 ,
Similarly, the execution of S2 gets to the configuration
size(S1 ) + size(S2 ).
(S2f , m2 (σ2 )).
Base case. S1 = s and S2 = “If(e) then{skip} else{skip}”; s;
By the hypothesis IH, we show the lemma holds. We need
By assumption, there is no program semantic error related to
to show that all conditions are satisfied for the application
the update of exit-on-error. Then the evaluation of the predicate
of the hypothesis IH.
expression e in the first statement of S2 does not crash. W.l.o.g., the
f
• (S2f ≈S
Exit S1 )
expression e evaluates to zero. Then the execution of S2 proceeds
By assumption.
as follows.
• The sum of the program size of S1f and S2f is less than
k, size(S1f ) + size(S2f ) < k.
(If(e) then{skip} else{skip}; s, m2 (σ2 ))
→(If((0, vof )) then{skip} else{skip}; s, m2 (σ2 ))
By definition, size(S1 ) = 1+size(S1t )+size(S1f ). Then,
by the rule EEval’
size(S1f ) + size(S2f ) < k + 1 − 2 = k − 1.
→(If(0) then{skip} else{skip}; s, m2 (σ2 ))
• Value stores σ1 and σ2 agree on values of used variables
by the rule E-Oflow1 or E-Oflow2.
in S1f and S2f as well as the input, I/O sequence variable.
→(skip; s, m2 (σ2 )) by the rule If-F
By definition, Use(S1f ) ⊆ Use(S1 ). So are the cases to
→(s, m2 (σ2 )) by the rule Seq.
S2f and S2 . In addition, value stores σ1 and σ2 are not
∗
changed in the evaluation of the predicate expression e.
Value stores σ2 are not changed in the execution of (S2 , m2 (σ2 )) →
The condition holds.
(s, m2 (σ2 )). By assumption, σ1 and σ2 agree on values of used
• There are no program semantic error related to the extra
variables as well as the input sequence variable, and the I/O secheck in the update of exit-on-error in the execution of
quence variable, ∀x ∈ Use(S2 ) ∩ Use(S1 ) ∪ {idI , idIO } :
S2 .
σ1 (id) = σ2 (id). By semantics, S1 and S2 terminate in the same
By assumption.
way, produce the same output sequence, and have equivalent comBy the hypothesis IH, the lemma holds.
putation of defined variables in both S1 and S2 as well as the input
(c) The evaluation of e reduces to the same nonzero integer
sequence variable, and the I/O sequence variable. Then this lemma
value, E ′ JeKσ1 = E ′ JeKσ2 = (v, vof ) where v 6= 0.
holds.
By
similar to the second subcase above.
Induction step.
2. S1 and S2 are both “while” statements:
The hypothesis is that this lemma holds when the sum k of the
S1 = “whilehni (e) {S1′ }”, S2 = “whilehni (e) {S2′ }” where
program size of S1 and S2 are great than or equal to 4, k ≥ 4.
′
We then show that this lemma holds when the sum of the
(S2′ ≈S
exit S1 );
program size of S1 and S2 is k + 1. There are cases to consider.
By Lemma 6.10, we show this lemma holds. We need to show
that all required conditions are satisfied for the application of
1. S1 and S2 are both “If” statement:
Lemma 6.10.
S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
• The output deciding variables in S1′ are a subset of those in
where both of the following hold
S2′ , OVar(S1′ ) = OVar(S2′ );
t
• (S2t ≈S
S
);
By Lemma 6.9.
Exit 1
f
• (S2f ≈S
• When started in states m′1 (σ1′ ), m′2 (σ1′ ) where value stores
Exit S1 );
σ1′ and σ2′ agree on values of used variables in both S1′
By the definition of Use(S1 ), variables used in the predicate exand S2′ as well as the input sequence variable, and the I/O
pression e are a subset of used variables in S1 and S2 , Use(e) ⊆
sequence variable, then S1′ and S2′ terminate in the same
Use(S1 ) ∩ Use(S2 ). By assumption, corresponding variables
way, produce the same output sequence, and have equivaused in e are of same value in value stores σ1 and σ2 . By
lent computation of defined variables in both S1 and S2 as
Lemma D.1, the expression evaluates to the same value w.r.t
well as the input sequence variable, and the I/O sequence
value stores σ1 and σ2 . There are three possibilities.
variable.
(a) The evaluation of e crashes, E ′ JeKσ1 = E ′ JeKσ2 =
By the induction hypothesis IH. This is because the sum of
(error, vof ).
the program size of S1′ and S2′ is less than k. By definition,
The execution of S1 continues as follows:
′
f
t
size(S
1 ) = 1 + size(S1 ).
(If(e) then{S1 } else{S1 }, m1 (σ1 ))
By Lemma 6.10, this lemma holds.
→(If((error, vof )) then{S1t } else{S1f }, m1 (σ1 ))
3. S1 = S1′ ; s1 and S2 = S2′ ; s2 where both of the following hold:
by the rule EEval’
→(If(0) then{S1t } else{S1f }, m1 (1/f))
′
• (S2′ ≈S
Exit S1 );
by the ECrash rule
S
•
(s
≈
s
i
2
Exit 1 );
→(If(0) then{S1t } else{S1f }, m1 (1/f)) for any i > 0
by the Crash rule.
By the hypothesis IH, we show S2′ and S1′ terminate in the
Similarly, the execution of S2 started from the state m2 (σ2 )
same way and produce the same output sequence and when S2′
crashes. The lemma holds.
and S1′ both terminate, S2′ and S1′ have equivalent terminating
(b) The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
computation of variables used or defined in S2′ and S1′ as well
(0, vof ).
as the input sequence variable, and the I/O sequence variable.
The execution of S1 continues as follows.
We show all the required conditions are satisfied for the application of the hypothesis IH.
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
′
• (S2′ ≈S
= (If((0, vof )) then{S1t } else{S1f }, m1 (σ1 ))
Exit S1 ).
By assumption.
by the rule EEval’
• ∀x ∈ (Def(S1 ) ∩ Def(S2 )) ∪ {idI , idIO } :

(S1 , m1 ) ≡x (S2 , m2 );

57

2015/9/14

• The sum of the program size of S1′ and S2′ is less than k,

Lemma 6.10. Let S1 = whilehn1 i (e) {S1′ } and S2 = whilehn2 i (e)
size(S1′ ) + size(S2′ ) < k.
{S2′ } be two loop statements where all of the following hold:
By definition, size(S2 ) = size(s2 ) + size(S2′ ) where
• the output deciding variables in S1′ are a subset of those in S2′ ,
size(s2 ) < 1. Then, size(S2′ ) + size(S1′ ) < k + 1 −
OVar(S1′ ) ⊆ OVar(S2′ ) = OVar(S);
size(s2 ) − size(s1 ) < k.
• When started in states m′1 (σ1′ ), m′2 (σ2′ ) where
• Value stores σ1 and σ2 agree on values of used variables in
Value stores agree on values of output deciding variables
both S2′ and S1′ as well as the input, I/O sequence variable.
in both S1′ and S2′ as well as the input sequence variBy definition, Use(S2′ ) ⊆ Use(S2 ), Use(S1′ ) ⊆ Use(S1 ).
able, and the I/O sequence variable, ∀x ∈ OVar(S2′ ) ∪
The condition holds.
′
′
′
′
′
′
{id
I , idIO } ∀m1 (σ1 ) m2 (σ2 ) : σ1 (x) = σ2 (x);
By the hypothesis IH, one of the following holds:
There are no program semantic errors related to the extra
(a) S1′ and S2′ both do not terminate.
check in the update of exit-on-error in executions of S1′ and
′
By Lemma E.2, executions of S1 = S1 ; s1 and S2 =
S2′ ;
′
S2 ; s2 both do not terminate and produce the same output
then S1′ and S2′ terminate in the same way, produce the same
sequence.
output sequence, and have equivalent computation of defined
′
′
(b) S1 and S2 both terminate.
variables in S1′ and S2′ as well as the input sequence variable,
∗
∗
′
′
′
′
By assumption, (S2 , m2 (σ2 )) → (skip, m2 (σ2 )), (S1 , m1 (σ1 )) →
and the I/O sequence variable ((S1′ , m1 ) ≡H (S2′ , m2 )) ∧
(skip, m′1 (σ1′ )).
((S1′ , m1 ) ≡O (S2′ , m2 )) ∧ (∀x ∈ OVar(S) ∪ {idI , idIO } :
∗
By Corollary E.1, (S2′ ; s2 , m2 (σ2 )) → (s2 , m′2 (σ2′ )),
(S1′ , m1 ) ≡x (S2′ , m2 ));
∗
(S1′ ; s1 , m1 (σ1 )) → (s1 , m′1 (σ1′ )).
If S1 and S2 start in states m1 (loop1c , σ1 ), m2 (loop2c , σ2 ) reBy the hypothesis IH, we show that s2 and s1 terminate
spectively, with loop counters of S1 and S2 not initialized (S1 , S2
in the same way, produce the same output sequence and
have not executed yet), value stores agree on values of used variwhen s2 and s1 both terminate, s2 and s1 have equivalent
ables in S1 and S2 , and there are no program semantic errors recomputation of variables defined in both s1 and s2 and the
lated to the extra check in the update of exit-on-error, then, for any
input, and I/O sequence variables.
positive integer i, one of the following holds:
We need to show that all conditions are satisfied for the
application of the hypothesis IH.
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
• There are updates of “exit-on-error” between s2 and s1 ,
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
s2 ≈S
s
;
2
1
′′
′
1
1
Exit
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
By assumption, s2 ≈S
′′
′
2′
2′
Exit s1 .
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in
• The sum of the program size s2 and s1 is less than or
the same way, produce the same output sequence, and have
equals to k;
equivalent computation of output deciding variables in S1 and
By definition, size(S2′ ) ≥ 1, size(S1′ ) ≥ 1. Therefore,
S2 and the input sequence variable, the I/O sequence vari′
′
size(s2 ) + size(s1 ) < k + 1 − size(S2 ) − size(S1 ) ≤ k.
able, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
• Value stores σ1′ and σ2′ agree on values of output de∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } :
ciding variables in s2 and s1 as well as the input, I/O
(S1 , m1 ) ≡x (S2 , m2 );
sequence variable.
2. The loop counter of S1 and S2 are of value less than or equal
By Lemma 6.9, OVar(s1 ) ⊆ OVar(s2 ), then OVar(s2 )
to i, and there are no reachable configurations (S1 , m1 (loop1ci ,
∩OVar(s1 ) = OVar(s1 ). For any variable id in Use(s1 ),
σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 ,
if id is not in OVar(S1′ ), then the value of id is not
m
2 (σ2 )) where all of the following hold:
changed in the execution of S1′ and S2′ , σ1′ (id) =
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
′
σ1 (id) = σ2 (id) = σ2 (id). Otherwise, the variable id
= loop2ci (n2 ) = i.
is defined in the execution of S1′ and S2′ , by assumption,
′
′
•
Value stores σ1i and σ2i agree on values of output deciding
σ1 (id) = σ2 (id). The condition holds.
variables in S1 and S2 as well as the input sequence vari• There are no program semantic errors related to the extra
able, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
check in the update of exit-on-error in the execution of
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
S2 .
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
By assumption.
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
By the hypothesis IH, the lemma holds.
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
We list the auxiliary lemmas below. One lemma shows that,
variables in S1 and S2 including the input sequence variif there are updates of exit-on-error between two statement seable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
quences, then there are same set of defined variables in the two
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
statement sequences, and the used variables in the update program
are the superset of those in the old program.
Proof. By induction on i.
Lemma 6.9. Let S2 be a statement sequence and S1 where there
Base case.
are updates of exit-on-error, S2 ≈S
Exit S1 . Then output deciding variWe show that, when i = 1, one of the following holds:
ables in S1 are a subset of those in S2 , OVar(S1 ) ⊆ OVar(S2 ).
1. Loop counters for S1 and S2 are always less than 1 if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
Proof. By induction on the sum of the program size of S1 and
′
′
∗
1
′
′′
1
2
S2 .
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′′
′
2′
2′
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in the
58

2015/9/14

3. The evaluation of e reduces to the same nonzero integer value,
E ′ JeKσ1 = E ′ JeKσ2 = (0, vof ).
Then the execution of S1 proceeds as follows:

same way, produce the same output sequence, and have equivalent computation of output deciding variables in S1 and S2
including the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. Loop counters of S1 and S2 are of values less than or equal to 1
but there are no reachable configurations (S1 , m1 (loop1c1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of output deciding
variables in S1 and S2 including the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ11 (x) = σ21 (x).
3. There are reachable configuration (S1 , m1 (loop1c1 , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of output deciding
variables in S1 and S2 including the input sequence variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ11 (x) = σ21 (x).

(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((v, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (v) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1′ ; whilehn1 i (e) {S1′ }, m1 (
loop1c ∪ {(n1 ) 7→ 1}, σ1 )) by the Wh-T rule.
Similarly, the execution of S2 proceeds to the configuration
(S2′ ; whilehn2 i (e) {S2′ }, m2 (loop2c ∪ {(n2 ) 7→ 1}, σ2 )).
By the assumption, we show that S1′ and S2′ terminate in the
same way and produce the same output sequence when started
in the state m1 (loop1c1 , σ1 ) and m2 (loop2c1 , σ2 ) respectively,
and S1′ and S2′ have equivalent computation of variables defined
in both statement sequences if both terminate. We need to
show that all conditions are satisfied for the application of the
assumption.
• There are no program semantic errors related to the extra
check in the update of exit-on-error in executions of S2′ and
S1′ .
The above two conditions are by assumption.
• Value stores σ1 and σ2 agree on values of output deciding
variables in S1′ and S2′ including the input, I/O sequence
variable.
By definition, OVar(S1′ ) ⊆ OVar(S1 ). So are the cases
to S2′ and S2 . In addition, value stores σ1 and σ2 are not
changed in the evaluation of the predicate expression e. The
condition holds.
By assumption, S1′ and S2′ terminate in the same way and
produce the same output sequence when started in states
m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ). In addition, S1′ and S2′ have
equivalent computation of output deciding variables in S1′ and
S2′ when started in states m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ).
Then there are two cases.
(a) S1′ and S2′ both do not terminate and produce the same
output sequence.
By Lemma E.2, S1′ ; S1 and S2′ ; S2 both do not terminate
and produce the same output sequence.
(b) S1′ and S2′ both terminate and have equivalent computation
of output deciding variables in S1′ and S2′ .
∗
By assumption, (S1′ , m1 (loop′c , σ1 )) → (skip, m′1 (loop′′c , σ1′ ));
∗
′
′
′
(S2 , m2 (loopc , σ2 )) → (skip, m2 (loop′′c , σ2′ )) where ∀x ∈
(OVar(S1′ ) ∪ OVar(S2′ )) ∪ {idI , idIO }, σ1′ (x) = σ2′ (x).
By Lemma 6.9, OVar(S1′ ) ⊆ OVar(S2′ ). Then variables
used in the predicate expression of S1 and S2 are either in
output deciding variables in both S1′ and S2′ or not. Therefore value stores σ2′ and σ1′ agree on values of variables used
in the expression e and even output deciding variables in S1
and S2 .

By definition, variables used in the predicate expression e of S1
and S2 are in output deciding variables in S1 and S2 , Use(e) ⊆
OVar(S1 ) ∪ OVar(S2 ). By assumption, value stores σ1 and σ2
agree on values of variables in Use(e), the predicate expression
e evaluates to the same value w.r.t value stores σ1 and σ2 by
Lemma D.2. There are three possibilities.
1. The evaluation of e crashes,
E ′ JeKσ1 = E ′ JeKσ2 = (error, vof ).
The execution of S1 continues as follows:
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
→(whilehn1 i ((error, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (1/f))
by the ECrash rule
i
→(whilehn1 i (0) {S1′ }, m1 (1/f)) for any i > 0
by the Crash rule.
Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. Therefore S1 and S2 terminate in the same way when
started from m1 and m2 respectively. Because σ1 (idIO ) =
σ2 (idIO ), the lemma holds.
2. The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
(0, vof ).
The execution of S1 continues as follows.
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((0, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(skip, m1 (σ1 )) by the Wh-F rule.

Induction step on iterations
The induction hypothesis (IH) is that, when i ≥ 1, one of the
following holds:

Similarly, the execution of S2 gets to the configuration (skip, m2 (σ2 )).
Loop counters of S1 and S2 are less than 1 and value stores
agree on values of output deciding variables in S1 and S2 including the input sequence variable and the I/O sequence variable.

59

1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′′
′
2′
2′
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of output deciding variables in both S1 and S2

2015/9/14

as well as the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the followings hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in S1 and S2 including the input sequence variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in S1 and S2 including the input sequence variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x)).

same way, produce the same output sequence, and have equivalent computation of output deciding variables in S1 and S2
including the input sequence variable and the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in both S1 and S2 as well as the input sequence
variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i (x) = σ2i (x).
When there are reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )

Then we show that, when i + 1, one of the following holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i+1, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i + 1, S1 and S2 terminate in
the same way, produce the same output sequence, and have
equivalent computation of output deciding variables in S1
and S2 including the input sequence variable and the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O
(S2 , m2 ) and ∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } :
(S1 , m1 ) ≡x (S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci+1 , σ1i+1 )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 ,
σ2i+1 )) from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i + 1,
loop1ci+1 (n1 ) = loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of output deciding variables in S1 and S2 including the input sequence
variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of output deciding variables in S1 and S2 including the input sequence
variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
By hypothesis IH and theorem 4 and 5, there is no configuration
where loop counters of S1 and S2 are of value i + 1 when any of
the following holds:
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
1′
′
′′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
60

= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of output deciding
variables in S1 and S2 including the input sequence variable
and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪
{idI , idIO } : σ1i (x) = σ2i (x).

By similar argument in base case, we have one of the following
holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i+1, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
same way, produce the same output sequence, and have equivalent computation of output deciding variables in S1 and S2
including the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (OVar(S1 ) ∪ OVar(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x
(S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci , σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i ))
from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of output deciding variables in S1 and S2 including the input sequence
variable and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI , idIO } : σ1i+1 (x) = σ2i+1 (x).
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i+1 ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i+1 )) from (S2 ,
m2 (σ2 )) where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i.
• Value stores σ1i+1 and σ2i+1 agree on values of output deciding variables in S1 and S2 including the input sequence
variable, and the I/O sequence variable, ∀x ∈ (OVar(S1 ) ∪
OVar(S2 )) ∪ {idI+1 , idIO } : σ1i+1 (x) = σ2i+1 (x).

2015/9/14

programs P1 and P2 have the same entry statement sequence and
we have the assumption that different prompt outputs due to the
difference of the prompt type are equivalent.

6.5 Proof rule for improved prompt message
If the only difference between two programs are the constant messages that the user receives, we consider that the two programs to be
equivalent. We realize that in general it is possible to introduce new
semantics even by changing constant strings. An old version might
have incorrectly labeled output: “median value = 5” instead of “average value = 5, for example. We rule out such possibilities because
all non-constant values are guaranteed to be exactly same. In practice, outputs could be classified into prompt outputs and actual outputs. Prompt outputs are those asking clients for inputs, which are
constants hardcoded in the output statement. Actual outputs are dynamic messages produced by evaluation of non-constant expression
in execution. The changes of prompt outputs are equivalent only for
interactions with human clients. In order to prove the update of improved prompt messages to be backward compatible, we assume
that the different prompt outputs produced in executions of the old
program and the updated program, due to the different constants in
output statements, are equivalent. Because the old program and the
new program are exactly same except some output statements with
different constants as expression e, we could show two programs
produce the “equivalent” output sequence under the assumption of
equivalent prompt outputs.
We formalize the generalized update of improved prompt messages, then we show that the updated program produce the same
I/O sequence as the old program in executions without program semantic errors. The following is the definition of the update class of
improved prompt messages.

Proof. By induction on the sum of the program size of S1 and S2 ,
size(S1 ) + size(S2 ).
Base case. S1 = “output v1 ” and S2 = “output v2 ”;
Then the execution of S2 proceeds as follows.
(output v2 , m2 (σ2 ))
→(skip, m2 (σ2 [“σ2 (idIO ) · v¯2 ”/idIO ]))
by the rule Out-1 or Out-2
∗

Similarly, (output v1 , m1 (σ1 )) → (skip, m1 (σ1 [“σ1 (idIO )·v¯1 ”/idIO ])).
By assumption, σ2 (idIO ) ≡ σ1 (idIO ). In addition, by assumption, v¯2 ≡ v¯1 . Therefore, S1 and S2 terminate in the same way,
produce the same output sequence and have equivalent computation of defined variables in S1 and S2 . This lemma holds.
Induction step.
The hypothesis is that this lemma holds when the sum k of the
program size of S1 and S2 are great than or equal to 2, k ≥ 2.
We then show that this lemma holds when the sum of the
program size of S1 and S2 is k + 1. There are cases to consider.
1. S1 and S2 are both “If” statement:
S1 = “If(e) then{S1t } else{S1f }”, S2 = “If(e) then{S2t } else{S2f }”
where both of the following hold
t
• S2t ≈S
Out S1 ;
f
• S2f ≈S
Out S1 ;
By the definition of Use(S1 ), variables used in the predicate expression e are a subset of used variables in S1 and S2 , Use(e) ⊆
Use(S1 ) ∩ Use(S2 ). By assumption, corresponding variables
used in e are of same value in value stores σ1 and σ2 . By
Lemma D.1, the expression evaluates to the same value w.r.t
value stores (σ1 and σ2 . There are three possibilities.
(a) The evaluation of e crashes, E ′ JeKσ1 = E ′ JeKσ2 =
(error, vof ).
The execution of S1 continues as follows:
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
→(If((error, vof )) then{S1t } else{S1f }, m1 (σ1 ))
by the rule EEval’
→(If(0) then{S1t } else{S1f }, m1 (1/f))
by the ECrash rule
i
→(If(0) then{S1t } else{S1f }, m1 (1/f)) for any i > 0
by the Crash rule.
Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. The lemma holds.
(b) The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
(0, vof ).
The execution of S1 continues as follows.
(If(e) then{S1t } else{S1f }, m1 (σ1 ))
= (If((0, vof )) then{S1t } else{S1f }, m1 (σ1 ))
by the rule EEval’
→(If(0) then{S1t } else{S1f }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1f , m1 (σ1 )) by the If-F rule.
Similarly, the execution of S2 gets to the configuration
(S2f , m2 (σ2 )).
By the hypothesis IH, we show the lemma holds. We need
to show that all conditions are satisfied for the application
of the hypothesis IH.
f
• S2f ≈S
Out S1
By assumption.

Definition 31. (Improved user messages) A program P2 =
P mpt2 ; EN ; V ; Sentry includes updates of improved prompt messages compared with a program P1 = P mpt1 ; EN ; V ; Sentry ,
written P2 ≈S
Out P1 , iff P mpt2 6= P mpt1 .
We give the lemma that two programs terminate in the same
way, produce the equivalent output sequence, and have equivalent
computation of defined variables in both programs in valid executions if there are updates of improved prompt messages between
them.
Lemma 6.11. Let P1 = P mpt1 ; EN ; V ; Sentry and P2 =
P mpt2 ; EN ; V ; Sentry be two programs where there are updates
of improved prompt messages in P2 compared with P1 . If S1 and
S2 start in states m1 (σ1 ) and m2 (σ2 ) such that both of the following hold:
• Value stores σ1 and σ2 agree on values of variables used in

Sentry in both programs as well as the input sequence variable,
∀x ∈ Use(Sentry ) ∪ {idI } : σ1 (x) = σ2 (x);
• Value stores σ1 and σ2 have “equivalent” I/O sequence,
σ1 (idIO ) ≡ σ2 (idIO );
• The different prompt outputs in the update of improved prompt
messages are equivalent;
then S1 and S2 terminate in the same way, produce the equivalent
output sequence, and when S1 and S2 both terminate, they have
equivalent computation of defined variables in Sentry in both programs as well as the input sequence variable, Sentry in the two
programs produce the equivalent I/O sequence variable,
• (Sentry , m1 ) ≡H (Sentry , m2 );
• ∀x ∈ (Def(S1 ) ∩ Def(S2 )) ∪ {idI } : (Sentry , m1 ) ≡x

(Sentry , m2 );

• The produced output sequences in executions of Sentry in both

programs are “equivalent”, σ1 (idIO ) ≡ σ2 (idIO ).
The difference between prompt types in P1 and P2 can be either addition/removal of labels as well as the change of the mapping of labels with constants. The proof is straightforward because

61

2015/9/14

By definition, Use(S2′ ) ⊆ Use(S2 ), Use(S1′ ) ⊆ Use(S1 ).
The condition holds.
By the hypothesis IH, one of the following holds:
(a) S1′ and S2′ both do not terminate.
By Lemma E.2, executions of S1 = S1′ ; s1 and S2 =
S2′ ; s2 both do not terminate and produce the same output
sequence.
(b) S1′ and S2′ both terminate.
∗
∗
By assumption, (S2′ , m2 (σ2 )) → (skip, m′2 (σ2′ )), (S1′ , m1 (σ1 )) →
′
′
(skip, m1 (σ1 )).
∗
By Corollary E.1, (S2′ ; s2 , m2 (σ2 )) → (s2 , m′2 (σ2′ )),
∗
′
′
′
(S1 ; s1 , m1 (σ1 )) → (s1 , m1 (σ1 )).
By the hypothesis IH, we show that s2 and s1 terminate in
the same way, produce the “equivalent” output sequence and
when s2 and s1 both terminate, s2 and s1 have equivalent
computation of variables defined in both s1 and s2 and the
input sequence variable; s2 and s1 produce “equivalent”
output sequence.
We need to show that all conditions are satisfied for the
application of the hypothesis IH.
• There are updates of “improved prompt messages” in s2
compared with s1 , s2 ≈S
Out s1 ;
By assumption, s2 ≈S
Out s1 .
• The sum of the program size s2 and s1 is less than or
equals to k;
By definition, size(S2′ ) ≥ 1, size(S1′ ) ≥ 1. Therefore,
size(s2 ) + size(s1 ) < k + 1 − size(S2′ ) − size(S1′ ) ≤ k.
• Value stores σ1′ and σ2′ agree on values of used variables
in s2 and s1 as well as the input sequence variable;
By Lemma 6.9, Use(s1 ) = Use(s2 ), then Use(s2 )
= Use(s1 ) = Use(s). Similarly, by Lemma 6.9,
Def(S1′ ) = Def(S2′ ). For any variable id in Use(s1 ), if
id is not in Def(S1′ ), then the value of id is not changed
in the execution of S1′ and S2′ , σ1′ (id) = σ1 (id) =
σ2 (id) = σ2′ (id). Otherwise, the variable id is defined in the execution of S1′ and S2′ , by assumption,
σ1′ (id) = σ2′ (id). The condition holds.
• Values of , the I/O sequence variable in value stores σ1′
and σ2′ are equivalent.
By assumption.
By the hypothesis IH, the lemma holds.

• The sum of the program size of S1f and S2f is less than

k, size(S1f ) + size(S2f ) < k.
By definition, size(S1 ) = 1+size(S1t )+size(S1f ). Then,
size(S1f ) + size(S2f ) < k + 1 − 2 = k − 1.
• Value stores σ1 and σ2 agree on values of used variables
in S1f and S2f as well as the input, I/O sequence variable.
By definition, Use(S1f ) ⊆ Use(S1 ). So are the cases to
S2f and S2 . In addition, value stores σ1 and σ2 are not
changed in the evaluation of the predicate expression e.
The condition holds.
• Different constants used in output statements are equivalent as output values.
By assumption.
By the hypothesis IH, the lemma holds.
(c) The evaluation of e reduces to the same nonzero integer
value, E ′ JeKσ1 = E ′ JeKσ2 = (v, vof ) where v 6= 0.
By argument similar to the second subcase above.
2. S1 and S2 are both “while” statements:
S1 = “whilehni (e) {S1′ }”, S2 = “whilehni (e) {S2′ }” where
′
S2′ ≈S
Out S1 ;
By Lemma 6.13, we show this lemma holds. We need to show
that all required conditions are satisfied for the application of
Lemma 6.13.
• S1′ and S2′ have same set of defined variables, Def(S1′ ) =
Def(S2′ ) = Def(S);
• The used variables in S1′ are a subset of those in S2′ ,
Use(S1′ ) = Use(S2′ );
By Lemma 6.12.
• When started in states m′1 (σ1′ ), m′2 (σ1′ ) where value stores
σ1′ and σ2′ agree on values of used variables in both S1′ and
S2′ as well as the input sequence variable, , and the I/O
sequence variable, then S1′ and S2′ terminate in the same
way, produce the same output sequence, and have equivalent computation of defined variables in both S1 and S2 as
well as the input sequence variable and the I/O sequence
variable.
By the induction hypothesis IH. This is because the sum of
the program size of S1′ and S2′ is less than k. By definition,
size(S1 ) = 1 + size(S1′ ).
By Lemma 6.13, this lemma holds.
3. S1 = S1′ ; s1 and S2 = S2′ ; s2 where both of the following hold:
′
• S2′ ≈S
Out S1 ;
• s2 ≈S
Out s1 ;

We list the auxiliary lemmas below. One lemma shows that, if
there are updates of improved prompt messages between two statement sequences, then there are same set of defined variables and
used variables in the two statement sequences. The second lemma
shows that, if there are updates of improved prompt messages between two loop statements, then the two loop statement terminate
in the same way, produce the equivalent output sequence, and have
equivalent computation of defined variables in both the old and updated programs as well as the input sequence variable.

By the hypothesis IH, we show S2′ and S1′ terminate in the same
way and produce the equivalent output sequence and when S2′
and S1′ both terminate, S2′ and S1′ have equivalent terminating
computation of variables defined in S2′ and S1′ as well as the
input sequence variable. By assumption, the different value of
the I/O sequence in executions of S1 and S2 are equivalent.
We show all the required conditions are satisfied for the application of the hypothesis IH.
′
• S2′ ≈S
Out S1 ;
• The I/O sequence variable in executions of S1 and S2 are
equivalent, σ1 (idIO ) ≡ σ2 (idIO );
By assumption.
• The sum of the program size of S1′ and S2′ is less than k,
size(S1′ ) + size(S2′ ) < k.
By definition, size(S2 ) = size(s2 ) + size(S2′ ) where
size(s2 ) < 1. Then, size(S2′ ) + size(S1′ ) < k + 1 −
size(s2 ) − size(s1 ) < k.
• Value stores σ1 and σ2 agree on values of used variables in
both S2′ and S1′ as well as the input sequence variable.

Lemma 6.12. Let S2 be a statement sequence and S1 where there
are updates of “improved prompt messages”, S2 ≈S
Out S1 . Then used
variables in S2 are the same of used variables in S1 , Use(S1 ) =
Use(S2 ), defined variables in S2 are the same as used variables in
S1 , Def(S1 ) = Def(S2 ).
Proof. By induction on the sum of the program size of S1 and
S2 .
Lemma 6.13. Let S1 = whilehn1 i (e) {S1′ } and S2 = whilehn2 i (e)
{S2′ } be two loop statements where all of the following hold:
62

2015/9/14

• There are updates of improved prompt messages in S2′ com-

′
pared with S1′ , S2′ ≈S
Out S1 ;
′
′
• S1 and S2 have same set of defined variables,
Def(S1′ ) = Def(S2′ ) = Def(S);
• S1′ and S2′ have same set of used variables, Use(S1′ ) =
Use(S2′ );
• When started in states m′1 (σ1′ ), m′2 (σ2′ ) where
Value stores agree on values of used variables in both
S1′ and S2′ as well as the input sequence variable, ∀x ∈
Use(S1′ ) ∪ {idI } ∀m′1 (σ1′ ) m′2 (σ2′ ) : σ1′ (x) = σ2′ (x);
Values of the I/O sequence variable in value stores σ1′ , σ2′
are equivalent, σ1′ (idIO ) ≡ σ2′ (idIO ));
then S1′ and S2′ terminate in the same way, produce the “equivalent” output sequence, and have equivalent computation of
defined variables in S1′ and S2′ as well as the input sequence
variable, ((S1′ , m1 ) ≡H (S2′ , m2 )) ∧ (∀x ∈ Def(S) ∪ {idI } :
(S1′ , m1 ) ≡x (S2′ , m2 ));

If S1 and S2 start in states m1 (loop1c , σ1 ), m2 (loop2c , σ2 ) respectively, with loop counters of S1 and S2 not initialized (S1 , S2
have not executed yet), value stores agree on values of used variables in S1 and S2 , and there are no program semantic errors, then,
for any positive integer i, one of the following holds:

1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in the
same way, produce the equivalent output sequence, and have
equivalent computation of defined variables in both S1 and
S2 and the input sequence variable, (S1 , m1 ) ≡H (S2 , m2 )
and ∀x ∈ (Def(S1 ) ∩ Def(S2 )) ∪ {idI } : (S1 , m1 ) ≡x
(S2 , m2 );S1 and S2 produce the “equivalent” I/O sequence;
2. The loop counter of S1 and S2 are of value less than or equal
to i, and there are no reachable configurations (S1 , m1 (loop1ci ,
σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 ,
m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i (x) = σ2i (x).
• Values of the I/O sequence variable in value stores σ1i (idIO ) ≡
σ2i (idIO );
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i (x) = σ2i (x).
• Values of the I/O sequence variable in value stores σ1i , σ2i
are equivalent, σ1i (idIO ) ≡ σ2i (idIO );
Proof. By induction on i.
Base case.
We show that, when i = 1, one of the followings holds:
1. Loop counters for S1 and S2 are always less than 1 if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
′′
′
1
1
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the equivalent I/O sequence, and have

equivalent computation of defined variables in both S1 and
S2 and the input sequence variable, the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and ∀x ∈ Def(S) ∪ {idI } :
(S1 , m1 ) ≡x (S2 , m2 );
2. S1 and S2 produce the equivalent output sequence and the
equivalent I/O sequence;
3. Loop counters of S1 and S2 are of values less than or equal to 1
but there are no reachable configurations (S1 , m1 (loop1c1 , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI } : σ11 (x) = σ21 (x).
• Values of the I/O sequence variable in value stores σ11 and
σ21 are equivalent, σ11 (idIO ) ≡ σ21 (idIO );
4. There are reachable configuration (S1 , m1 (loop1c1 , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2c1 , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value 1, loop1c 1 (n1 )
= loop2c1 (n2 ) = 1.
• Value stores σ11 and σ21 agree on values of used variables
in both S1 and S2 as well as the input sequence variable, and
the I/O sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪
{idI } : σ11 (x) = σ21 (x).
• Values of the I/O sequence variable in value stores σ11 and
σ21 are equivalent, σ11 (idIO ) ≡ σ21 (idIO );
By definition, variables used in the predicate expression e of S1
and S2 are used in S1 and S2 , Use(e) ⊆ Use(S1 ) ∩ Use(S2 ). By
assumption, value stores σ1 and σ2 agree on values of variables in
Use(e), the predicate expression e evaluates to the same value w.r.t
value stores σ1 and σ2 by Lemma D.2. There are three possibilities.
1. The evaluation of e crashes,
E ′ JeKσ1 = E ′ JeKσ2 = (error, vof ).
The execution of S1 continues as follows:
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
→(whilehn1 i ((error, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (1/f))
by the ECrash rule
i
→(whilehn1 i (0) {S1′ }, m1 (1/f)) for any i > 0
by the Crash rule.
Similarly, the execution of S2 started from the state m2 (σ2 )
crashes. Therefore S1 and S2 terminate in the same way when
started from m1 and m2 respectively. Because σ1 (idIO ) ≡
σ2 (idIO ), the lemma holds.
2. The evaluation of e reduces to zero, E ′ JeKσ1 = E ′ JeKσ2 =
(0, vof ).
The execution of S1 continues as follows.
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((0, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (0) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(skip, m1 (σ1 )) by the Wh-F rule.
Similarly, the execution of S2 gets to the configuration (skip, m2 (σ2 )).
Loop counters of S1 and S2 are less than 1 and value stores
agree on values of used/defined variables in both S1 and S2

63

2015/9/14

in both S1 and S2 and the input sequence variable, (S1 , m1 )
≡H (S2 , m2 ) and ∀x ∈ Def(S) ∪ {idI } : (S1 , m1 ) ≡x
(S2 , m2 ); S1 and S2 produce the equivalent I/O sequence;
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i (x) = σ2i (x).
• Values of the I/O sequence variable in value stores σ1i and
σ2i , σ1i (idIO ) ≡ σ2i (idIO );
3. There are reachable configurations (S1 , m1 (loop1ci , σ1i )) from
(S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 )) where
all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i (x) = σ2i (x).
• Values of the I/O sequence variable in value stores σ1i and
σ2i are equivalent, σ1i (idIO ) ≡ σ2i (idIO );

as well as the input sequence variable and the I/O sequence
variable.
3. The evaluation of e reduces to the same nonzero integer value,
E ′ JeKσ1 = E ′ JeKσ2 = (v, vof ) where v 6= 0.
Then the execution of S1 proceeds as follows:
(whilehn1 i (e) {S1′ }, m1 (σ1 ))
= (whilehn1 i ((v, vof )) {S1′ }, m1 (σ1 ))
by the rule EEval’
→(whilehn1 i (v) {S1′ }, m1 (σ1 ))
by the E-Oflow1 or E-Oflow2 rule
→(S1′ ; whilehn1 i (e) {S1′ }, m1 (loop1c [1/n1 ], σ1 ))
by the Wh-T rule.

Similarly, the execution of S2 proceeds to the configuration
(S2′ ; whilehn2 i (e) {S2′ }, m2 (loop2c [1/n2 ], σ2 )).
By the assumption, we show that S1′ and S2′ terminate in
the same way and produce the equivalent I/O sequence when
started in the state m1 (loop1c1 , σ1 ) and m2 (loop2c1 , σ2 ) respectively, and S1′ and S2′ have equivalent computation of variables
defined in both statement sequences if both terminate. We need
to show that all conditions are satisfied for the application of
the assumption.
• Values of the I/O sequence variable in value stores σ1 and
σ2 are equivalent, σ1 (idIO ) ≡ σ2 (idIO );
The above two conditions are by assumption.
• Value stores σ1 and σ2 agree on values of used variables in
Then we show that, when i + 1, one of the following holds: The
S1′ and S2′ as well as the input sequence variable.
induction hypothesis (IH) is that, when i ≥ 1, one of the following
′
By definition, Use(S1 ) ⊆ Use(S1 ). So are the cases to
holds:
S2′ and S2 . In addition, value stores σ1 and σ2 are not
changed in the evaluation of the predicate expression e. The
1. Loop counters for S1 and S2 are always less than i + 1 if any
condition holds.
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
By assumption, S1 and S2 terminate in the same way and
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i+1, (S2 , m2 (loopc , σ2 )) →
produce the equivalent output sequence when started in states
′′
′
2′
2′
′
′
′
′
(S2 , m2 (loopc )), loopc (n2 ) < i + 1, S1 and S2 terminate in
m1 (loopc , σ1 ) and m2 (loopc , σ2 ). In addition, S1 and S2 have
the same way, produce the equivalent I/O sequence, and have
equivalent computation of variables used or defined in S1′ and
equivalent computation of defined variables in both S1 and
S2′ when started in states m1 (loop′c , σ1 ) and m2 (loop′c , σ2 ).
S2 and the input sequence variable, (S1 , m1 ) ≡H (S2 , m2 )
Then there are two cases.
and (S1 , m1 ) ≡O (S2 , m2 ) and ∀x ∈ (Def(S) ∪ {idI } :
(a) S1′ and S2′ both do not terminate and produce the equivalent
(S1 , m1 ) ≡x (S2 , m2 ); S1 and S2 produce the “equivalent”
I/O sequence.
I/O sequence variable;
By Lemma E.2, S1′ ; S1 and S2′ ; S2 both do not terminate
2. The loop counter of S1 and S2 are of value less than or
and produce the equivalent I/O sequence.
equal to i + 1, and there are no reachable configurations
(b) S1′ and S2′ both terminate and have equivalent computation
(S1 , m1 (loop1ci+1 , σ1i+1 )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 ,
of variables defined in S1′ and S2′ .
∗
′′
′
′
′
′
σ
2i+1 )) from (S2 , m2 (σ2 )) where all of the following hold:
By assumption, (S1 , m1 (loopc , σ1 )) → (skip, m1 (loopc , σ1 ));
∗
′′
′
′
′
′
•
The loop counters of S1 and S2 are of value i + 1,
(S2 , m2 (loopc , σ2 )) → (skip, m2 (loopc , σ2 )) where ∀x ∈
loop1ci+1 (n1 ) = loop2ci+1 (n2 ) = i + 1.
(Def(S1′ ) ∩ Def(S2′ )) ∪ {idI }, σ1′ (x) = σ2′ (x).
′
′
′
•
Value stores σ1i+1 and σ2i+1 agree on values of used variBy assumption,Use(S1 ) = Use(S2 ) and Def(S1 ) =
ables in both S1 and S2 as well as the input sequence variDef(S2′ ). Then variables used in the predicate expression
able, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i+1 (x) =
of S1 and S2 are either in variables used or defined in both
σ2i+1 (x).
S1′ and S2′ or not. Therefore value stores σ2′ and σ1′ agree
• Values of the I/O sequence variable in value stores σ1i+1
on values of variables used in the expression e and even
and σ2i+1 are equivalent, σ1i+1 (idIO ) ≡ σ2i+1 (idIO );
variables used or defined in S1 and S2 .
By assumption, S1 and S2 produce the equivalent output
3. There are reachable configurations (S1 , m1 (loop1ci+1 , σ1i ))
sequence.
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci+1 , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
Induction step on iterations
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )
The induction hypothesis (IH) is that, when i ≥ 1, one of the
= loop2ci+1 (n2 ) = i + 1.
following holds:
• Value stores σ1i+1 and σ2i+1 agree on values of used vari1. Loop counters for S1 and S2 are always less than i if any is
ables in both S1 and S2 as well as the input sequence vari′
′
∗
able, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i+1 (x) =
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
σ2i+1 (x);
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i, (S2 , m2 (loop2c , σ2 )) →
′
′
• Values of the I/O sequence variable in value stores σ1i+1
′′
′
2
2
(S2 , m2 (loopc )), loopc (n2 ) < i, S1 and S2 terminate in the
and σ2i+1 are equivalent, σ1i+1 (idIO ) ≡ σ2i+1 (idIO );
same way, and have equivalent computation of defined variables
64

2015/9/14

1:
2: If (a > 0) then
3:
b := c + 1
4: output b + c

By hypothesis IH, there is no configuration where loop counters
of S1 and S2 are of value i + 1 when any of the following holds:
1. Loop counters for S1 and S2 are always less than i if any is
′
′
∗
present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
∗
′′
′
1′
1′
2
(S1 , m1 (loopc )), loopc (n1 ) < i, (S2 , m2 (loopc , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the equivalent I/O sequence, and have
equivalent computation of used/defined variables in both S1
and S2 and the input sequence variable, , the I/O sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and (S1 , m1 ) ≡O (S2 , m2 ) and
∀x ∈ (Def(S1 ) ∩ Def(S2 )) ∪ {idI } : (S1 , m1 ) ≡x (S2 , m2 );
S1 and S2 produce , and the I/O sequence variable;
2. The loop counter of S1 and S2 are of value less than or equal to
i, and there are no reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i (n1 )
= loop2ci (n2 ) = i.
• Value stores σ1i and σ2i agree on values of used variables
in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI , , idIO } : σ1i (x) =
σ2i (x).
• Values of the I/O sequence variable in value stores σ1i and
σ2i are equivalent, σ1i (idIO ) ≡ σ2i (idIO );

old

1’: b := 2
2’: If (a > 0) then
3’:
b := c + 1
4’: output b + c
new

Figure 18: Missing initialization
• The loop counter of S1 and S2 are of value i, loop1c i+1 (n1 )

= loop2ci+1 (n2 ) = i.

• The loop counter of S1 and S2 are of value i, loop1c i+1 (, n1 )

= loop2ci+1 (, n2 ) = i.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i+1 (x) =
σ2i+1 (x);
• Values of the I/O sequence variable in value stores σ1i+1
and σ2i+1 are equivalent, σ1i+1 (idIO ) ≡ σ2i+1 (idIO );
6.6 Proof rule for missing variable initializations
A kind of bugfix we call missing-initialization includes variable
initialization for those in the imported variables relative to the I/O
sequence variable in the old program. Figure 18 shows an example of missing-initializations. The initialization b := 2 ensures the
value used in “output b + c” is not to be undefined. In general, new
variable initializations only affect rare buggy executions of the old
program, where there are uses of undefined imported variables relative to the I/O sequence variable in the program. Because DSU
is not starting in error state, we assume that, in the proof of backward compatibility, there are no uses of variables with undefined
variables in executions of the old program.
The following is the definition of the update class “missing
initializations”.

When there are reachable configurations (S1 , m1 (loop1ci , σ1i ))
from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i )) from (S2 , m2 (σ2 ))
where all of the following hold:
• The loop counter of S1 and S2 are of value i, loop1c i (n1 )

= loop2ci (n2 ) = i.

• Value stores σ1i and σ2i agree on values of used variables

in both S1 and S2 as well as the input sequence variable,
∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i (x) = σ2i (x).
• Values of the I/O sequence variable in value stores σ1i and σ2i
are equivalent, σ1i (idIO ) ≡ σ2i (idIO );

By similar argument in base case, we have one of the following
holds:
1. Loop counters for S1 and S2 are always less than i + 1 if any
′
′
∗
is present, ∀m′1 (loop1c ) m′2 (loop2c ) : (S1 , m1 (loop1c , σ1 )) →
′
′
∗
(S1′′ , m′1 (loop1c )), loop1c (n1 ) < i+1, (S2 , m2 (loop2c , σ2 )) →
′
′
(S2′′ , m′2 (loop2c )), loop2c (n2 ) < i, S1 and S2 terminate in
the same way, produce the equivalent I/O sequence, and have
equivalent computation of defined variables in both S1 and S2
and the input sequence variable, (S1 , m1 ) ≡H (S2 , m2 ) and
∀x ∈ (Def(S)) ∪ {idI } : (S1 , m1 ) ≡x (S2 , m2 );
2. The loop counter of S1 and S2 are of value less than or
equal to i + 1, and there are no reachable configurations
(S1 , m1 (loop1ci , σ1i )) from (S1 , m1 (σ1 )), (S2 , m2 (loop2ci , σ2i ))
from (S2 , m2 (σ2 )) where all of the following hold:
• The loop counters of S1 and S2 are of value i, loop1c i+1 (n1 )
= loop2ci+1 (n2 ) = i + 1.
• Value stores σ1i+1 and σ2i+1 agree on values of used variables in both S1 and S2 as well as the input sequence variable, ∀x ∈ (Use(S1 ) ∩ Use(S2 )) ∪ {idI } : σ1i+1 (x) =
σ2i+1 (x).
• Values of the I/O sequence variable in value stores σ1i+1
and σ2i+1 are equivalent, σ1i+1 (idIO ) ≡ σ2i+1 (idIO );
3. There are reachable configurations (S1 , m1 (loopc1i+1 , σ1i+1 ))
from (S1 , m1 (σ1 )), (S2 , m2 (loopc2i+1 , σ2i+1 )) from (S2 ,
m2 (σ2 )) where all of the following hold:

65

Definition 32. (Missing initializations) A statement sequence S2
includes updates of missing initializations compared with a statement sequence S1 , written S2 ≈S
Init S1 , iff S2 = SInit ; S1 where SInit
is a sequence of assignment statements of form “lval := v” and
Def(SInit ) ⊆ Imp(S1 , {idIO });
Though the bugfix in the update of missing initializations are
not in rare execution in the first case in Definition 32, the definition
shows the basic form of bugfix clearly.
We show that two statement sequences terminate in the same
way, produce the same output sequence, and have equivalent computation of defined variables in both programs in valid executions
if there are updates of missing initializations between them.
Lemma 6.14. Let S1 and S2 be two statement sequences respectively where there are updates of “missing initializations” in S2
compared with S1 , S2 ≈S
Init S1 . If S1 and S2 start in states m1 (σ1 )
and m2 (σ2 ) respectively such that both of the following hold:
• Value stores σ1 and σ2 agree on values of variables used in both

S1 and S2 as well as the input sequence variable and the I/O
sequence variable, ∀id ∈ (Use(S1 )∩Use(S2 ))∪{idI , idIO } :
σ1 (id) = σ2 (id);
• defined variables in SInit are of undefined value in value stores
σ1 , σ2 , ∀id ∈ Def(SInit ) : σ1 (id) = σ2 (id) = UdfJτ K where
τ is the type of the variable id;
• There are no use of variables with undefined values in the
execution of S1 ;
• There are no crash in execution of SInit ;

2015/9/14

7. Related Work

then S1 and S2 terminate in the same way, produce the same output
sequence, and when S1 and S2 both terminate, they have equivalent
computation of used variables and defined variables in both S1 and
S2 as well as the input sequence variable and the I/O sequence
variable,

We discuss related work on DSU safety and program equivalence
in order.
Existing studies on DSU safety could be roughly divided into
high level studies and low level ones. There are a few studies on
high level DSU safety. In [19], Kramer and Magee defined the DSU
correctness that the updated system shall “operate as normal instead
of progressing to an error state”. This is covered by our requirement that hybrid executions conform to the old program’s specification and our accommodation for bug fixes. Moreover, our backward compatibility includes I/O behavior, which is more concrete
than the behavior in [19]. In [9], Bloom and Day proposed a DSU
correctness which allows functionality extension that could not produce past behavior. This is probably because Bloom and Day considered updated environment. On the contrary, we assume that the
environment is not updated. In addition, we explicitly present the
error state, which is not mentioned in [9]. Panzica La Manna [28]
presented a high level correctness only considering scenario-based
specifications for controller systems instead of general programs.
There are also studies on low level DSU safety. Hayden et
al. [15] discussed DSU correctness and concluded that there is only
client-oriented correctness. Zhang et al. [32] asked the developers
to ensure DSU correctness. Magill et al. [24] did ad-hoc program
correlation without definitions of any correctness. We consider
that there is general principle of DSU safety. The difference lies
at the abstraction of the program behavior. We model program
behavior by concrete I/O while others [15, 24, 32] consider a
general program behavior.
We next discuss existing work on program equivalence. There is
a rich literature on program equivalence and we compare our work
only with most related work. Our study of program equivalence is
inspired by original work of Horwitz et al. [17] on program dependence graphs, but we take a much more formal approach and we
consider terminating as well as non-terminating programs with recurring I/O. In [13], Godlin and Strichman have a structured study
of program equivalence similar to that of ours. Godlin and Strichman [13] restricted the equivalence to corresponding functions and
therefore weakens the applicability to general transformations affecting loops such as loop fission, loop fusion and loop invariant
code motion. However, our program equivalence allows loop optimizations such as loop fusion and loop fission. Furthermore, our
syntactic conditions imply more program point mapping because
we allow corresponding program point in arbitrary nested statements and in the middle of program that does not include function
call.

• (S1 , m1 ) ≡H (S2 , m2 );
• (S1 , m1 ) ≡O (S2 , m2 );
• ∀x ∈ (Def(S1 ) ∪ Def(S2 )) ∪ {idI , idIO } : (S1 , m1 ) ≡x

(S2 , m2 );

Proof. By induction on the sum of the program size of S1 and S2 ,
size(S1 ) + size(S2 ).
Base case. S1 = s and S2 = SInit ; s where SInit = “lval := v”
and Def(lval) ∈ Use(s);
There are cases regarding lval in SInit .
1. lval = id.
Then the execution of S2 proceeds as follows.
(id := v; s, m2 (σ2 ))
→(skip; s, m2 (σ2 [v/(id)]))
by the rule As-Scl
→(s, m2 (σ2 [v/(id)])) by the rule Seq.
By assumption, id ∈ Use(e). By assumption, the value of id
is undefined in value store σ1 . Then there is no valid execution
of S1 . Then it holds that, in valid executions of S1 , S1 and S2
terminate in the same way, produce the same output sequence,
and have equivalent computation of defined variables in both
S1 and S2 . Then this lemma holds.
2. lval = id[n].
Then the execution of S2 proceeds as follows.
(id[n] := v; s, m2 (σ2 ))
→(skip; s, m2 (σ2 [v/(id, n)]))
by the rule As-Err
→(s, m2 (σ2 [v/(id, n)])) by the rule Seq.
By similar argument above, this lemma holds.
3. lval = id1 [id2 ].
Then the execution of S2 proceeds as follows.
(id1 [id2 ] := v; s, m2 (σ2 ))
→(id1 [v1 ] := v; s, m2 (σ2 [v/(id, n)]))
by the rule Var
→(skip; s, m2 (σ2 [v/(id, v1 )])) by the rule As-Arr.
→(s, m2 (σ2 [v/(id, v1 )])) by the rule Seq.

8. Conclusion
In this paper, we propose a formal and practical general definition
of DSU correction based on I/O sequences, backward compatibility. We devised a formal language and adapt the general definition of DSU correctness for executable programs based on our language. Based on the adapted backward compatibility, we proposed
syntactic conditions that help guarantee correct DSUs for both terminating and nonterminating executions. In addition, we formalize
typical program updates that are provably backward compatible,
covering both new feature and bugfix.
In the future, we plan to identify more backward compatible update patterns by studying more open source programs. Though it is
dubious if open source programs’ evolution history includes typical update patterns, open source programs are the most important
source of widely-used programs for our study of DSU. In addition,
we plan to develop an algorithm for automatic state mapping based
on our syntactic condition of program equivalence and definition of
update classes.

By similar argument above, this lemma holds.
Induction step.
The hypothesis is that this lemma holds when the sum k of the
program size of S1 and S2 are great than or equal to 3, k ≥ 3.
We then show that this lemma holds when the sum of the
program size of S1 and S2 is k + 1.
S2 = SInit ; S1 where SInit is a sequence of assignment statements and Def(SInit ) ∈ Imp(S1 , {idIO });
The proof is similar to that in the base case. By assump∗
tion, the execution of SInit does not crash, (SInit , m2 (σ2 )) →
(skip, m2 (σ2′ )) where σ2′ = σ2 [v1 /x1 ]...[vk /xk ] and ∀1 ≤ i ≤
k : xk ∈ Def(SInit ).
By assumption, there are no use of variables with undefined
values in the execution of S1 by Theorem 5 and Theorem 4, this
lemma holds.

66

2015/9/14

References

[26] K. Makris and R. A. Bazzi. Immediate Multi-Threaded Dynamic
Software Updates Using Stack Reconstruction. In Proceedings of the
USENIX ’09 Annual Technical Conference, June 2009. 3

[1] Software Engineering - Software Life Cycle Processes - Maintenance.
Technical Report ISO/IEC 14764:2006(E). 2.4

[27] I. Neamtiu. Practical Dynamic Software Updating.
University of Maryland, August 2008. 3

[2] Openssh users. http://www.openssh.com/users.html, 2015.
[Online; accessed 15-Jan-2015]. 3

PhD thesis,

[3] Vsftpd wikipage.
http://en.wikipedia.org/wiki/Vsftpd,
2015. [Online; accessed 15-Jan-2015]. 3

[28] V. Panzica La Manna, J. Greenyer, C. Ghezzi, and C. Brenner. Formalizing correctness criteria of dynamic updates derived from specification changes. SEAMS ’13, pages 63–72, 2013. 1, 7

[4] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: Principles, Techniques, and Tools. Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 1986. 5.2.1

[29] D. L. Parnas. Software aging. ICSE ’94, Los Alamitos, CA, USA.
IEEE Computer Society Press. 2.4
[30] B. C. Pierce. Types and programming languages. MIT press, 2002.
4.2

[5] J. Arnold and M. F. Kaashoek. KSplice: Automatic Rebootless Kernel
Updates. In EuroSys 2009, April 2009. 1

[31] S. Verdoolaege, G. Janssens, and M. Bruynooghe. Equivalence checking of static affine programs using widening to handle recurrences.
ACM Trans. Program. Lang. Syst., 34(3):11:1–11:35, Nov. 2012. 1

[6] N. Benton. Simple relational correctness proofs for static analyses and
program transformations. SIGPLAN Not., 39(1):14–25, Jan. 2004. 1
[7] D. Binkley, S. Horwitz, and T. Reps. The multi-procedure equivalence
theorem. Technical report, 1989. 1

[32] M. Zhang, K. Ogata, and K. Futatsugi. Formalization and verification
of behavioral correctness of dynamic software updates. Electronic
Notes in Theoretical Computer Science, 294:12 – 23, 2013. 1, 7

[8] S. Blazy and X. Leroy. Mechanized semantics for the clight subset of
the c language. Journal of Automated Reasoning, 43(3), 2009. 4.2
[9] T. Bloom and M. Day. Reconfiguration and module replacement in
argus: theory and practice. Software Engineering Journal, Mar 1993.
1, 7

A. Type system
Figure 19 shows an almost standard unsound and incomplete type
system. The type system is unsound because of three reasons, (a)
the possible value mismatch due to the subtype rule from hte type
Int to Long, (b) the implicit subtype between enumeration types
and the type Long allowed by our semantics and (c) the possible
array index out of bound. The type system is incomplete due to the
parameterized “other” expressions. The notation Dom(Γ) borrowed
from Cardelli [10] in rules Tvar1, Tvar2, Tlabels an Tfundecl refers
to the domain of the typing environment Γ, which are identifiers
bound to a type in Γ.

[10] L. Cardelli. Type systems. ACM Computing Surveys, 28(1):263–264,
1996. A
[11] R. Cartwright and M. Felleisen. The semantics of program dependence. SIGPLAN Not., 24(7):13–27, 1989. 1
[12] W. Dietz, P. Li, J. Regehr, and V. Adve. Understanding integer
overflow in c/c++. ICSE 2012, pages 760–770, 2012. 2
[13] B. Godlin and O. Strichman. Inference rules for proving the equivalence of recursive procedures. Acta Informatica, 45(6):403–439, 2008.
1, 7
[14] A. D. Gordon. Functional programming and input/output. Number 8.
Cambridge University Press, 1994. 4.1, 4.2

B. Syntactic definitions

[15] C. M. Hayden, S. Magill, M. Hicks, N. Foster, and J. S. Foster.
Specifying and verifying the correctness of dynamic software updates.
VSTTE ’12, pages 278–293, Jan. 2012. 1, 7

The syntax-directed definitions listed below make our argument
independent of existing program analysis partially.
Definition 33. (Idx(lval)) The used variables in index of a left
value lval, written Idx(lval), are listed as follows:

[16] M. Hicks. Dynamic Software Updating. PhD thesis, University of
Pennsylvania, August 2001. 1

1. Idx(id) = ∅;
2. Idx(id[n]) = ∅;
3. Idx(id1 [id2 ]) = {id2 };

[17] S. Horwitz, J. Prins, and T. Reps. On the adequacy of program
dependence graphs for representing programs. POPL ’88, pages 146–
157. ACM, 1988. 1, 7
[18] C. Karfa, K. Banerjee, D. Sarkar, and C. Mandal. Verification
of loop and arithmetic transformations of array-intensive behaviors.
Computer-Aided Design of Integrated Circuits and Systems, IEEE
Transactions on, 32(11):1787–1800, Nov 2013. 1

Definition 34. (Base(lval)) The base of a left value lval, written
Base(lval), is listed as follows:
1. Base(id) = {id};
2. Base(id[n]) = {id};
3. Base(id1 [id2 ]) = {id1 };

[19] J. Kramer and J. Magee. The evolving philosophers problem: dynamic
change management. Software Engineering, IEEE Transactions on,
16, Nov 1990. 1, 7

Definition 35. (Use(e)) The set of used variables in an expression
e, written Use(e), are listed as follows:

[20] S. Kundu, Z. Tatlock, and S. Lerner. Proving optimizations correct
using parameterized program equivalence. SIGPLAN Not., 44(6). 1

1. Use(lval) = Base(lval) ∪ Idx(lval);
2. Use(id == l) = {id};
3. Use(other) = Use(other) where function Use : other → {id}
is parameterized;

[21] D. Lacey, N. D. Jones, E. Van Wyk, and C. C. Frederiksen. Proving
correctness of compiler optimizations by temporal logic. SIGPLAN
Not., 37(1):283–294, Jan. 2002. ISSN 0362-1340. 1
[22] Y.-F. Lee and R.-C. Chang. Hotswapping linux kernel modules.
Journal of Systems and Software, 79(2):163–175, February 2006. 1

Definition 36. (Use(S)) The used variables in a sequence of statements S, written Use(S), are listed as follows:

[23] D. Lucanu and V. Rusu. Program equivalence by circular reasoning.
In Integrated Formal Methods, volume 7940 of Lecture Notes in Computer Science, pages 362–377. Springer Berlin Heidelberg, 2013. 1

1.
2.
3.
4.
5.
6.

[24] S. Magill, M. Hicks, S. Subramanian, and K. S. McKinley. Automating object transformations for dynamic software updating. SIGPLAN
Not., 47(10):265–280, Oct. 2012. 1, 7
[25] K. Makris. Whole-Program Dynamic Software Updating. PhD thesis,
Arizona State University, December 2009. 1, 3

67

Use(skip) = ∅;
Use(lval := e) = Use(e) ∪ Idx(lval);
Use(output e) = Use(e) ∪ {idIO };
Use(input id) = {idI , idIO };
Use(If (e) then {St } else {Sf }) = Use(e)∪Use(St )∪Use(Sf );
Use(whilehni (e){S ′ }) = Use(e) ∪ Use(S ′ );
2015/9/14

Definition 38. (s ∈ S) We say a statement s is in a sequence of
statements S of a program P , written s ∈ S, if one of the following
holds:

Γ⊢⋄

TInit

Γ⊢⋄

Γ → Γ′
(Tvar1)
Γ⊢⋄
V = V ′ , τ id
id ∈
/ Dom(Γ)
Γ, id : τ ⊢ ⋄
(Tprompt)
Γ⊢⋄
P mpt = {l1 : n1 , . . . , lk : nk }

1.
2.
3.
4.

(Tlabels)
Γ⊢⋄
k≥1
id ∈
/ Dom(Γ)
EN = EN ′ , enum id{l1 , ..., lk }
Γ, id : {l1 , ..., lk } ⊢ ⋄

We write s ∈
/ S if s ∈ S does not hold.
We show the definition of program size, which is based of our
induction proof.

pmpt ∈
/ Dom(Γ)

Γ, pmpt : {l1 : n1 , . . . , lk : nk } ⊢ ⋄
(Tvar2)
Γ⊢⋄
id ∈
/ Dom(Γ)
V = V ′ , τ id[n]
n>0

Definition 39. (size(S)) The program size of a statement sequence
S, written size(S), is listed as follows:

Γ, id : array(τ, n) ⊢ ⋄

Γ⊢τ

(Tint)
Γ⊢⋄
Γ ⊢ Int

(Tlong)
Γ⊢⋄
Γ ⊢ Long

1. size(“skip”) = size(“id := e”) = size(“id1 := call id2 (e∗ )”)
= size(“input id”) = size(“output e”) = 1;
2. size(“If(e) then {St } else {Sf }”) = 1 + size(St ) + size(Sf );
3. size(“while(e) {S ′ }”) = 1 + size(S ′ );
k
P
4. For k > 0, size(s1 ; ...; sk =
size(si );

(Tenum)
Γ ⊢ id : {l1 , ..., lk }
Γ ⊢ enum id

i=1

Γ⊢e:τ

(Topnd)

Γ, id : τ ⊢ id : τ
(Tarray1)
Γ ⊢ id : array(τ, n)
Γ ⊢ id′ : Long
Γ ⊢ id[id′ ] : τ
Γ⊢S
(Tassign)
Γ ⊢ lval : τ
Γ⊢e:τ
Γ ⊢ lval := e

(Tequiv)
Γ ⊢ id′ : {l1 , ..., l, ..., lk }
Γ ⊢ id : enum id′
Γ ⊢ (id == l) : Long

C. Properties of imported variables

(TSub)

Lemma C.1. Imp(S1 ; S2 , X) = Imp(S1 , Imp(S2 , X)).

Γ ⊢ e : Int
Γ ⊢ e : Long

Proof. Let statement sequence S2 = s1 ; s2 ; ...; sk for some k > 0.
The proof is by induction on k.

(Tarray2)
Γ ⊢ id : array(τ, n)
1≤k≤n
Γ ⊢ id[k] : τ

(Tinput)
Γ ⊢ id : τ
τ 6= pmpt
Γ ⊢ input id

(Tif)

(Toutput)
Γ⊢e:τ
Γ ⊢ output e

Corollary C.1. ∀i ∈ Z+ , Imp(S i+1 , X) = Imp(S, Imp(S i , X)).
This is by lemma C.1.
Lemma C.2. Imp(S, A ∪ B) = Imp(S, A) ∪ Imp(S, B).

(Tseq)
Γ ⊢ S1
Γ ⊢ S2
Γ ⊢ S1 ; S2

Proof. By structural induction on abstract syntax of statement sequence S.
Lemma C.3. For statement s = “while(e){S}” and a set of finite
number ofSvariables X such that X ∩ Def
S (s) 6= ∅, there is β > 0
such that 0≤i≤(β+1) Imp (S i , X) ⊆ 1≤j≤β Imp(S j , X).

(Twhile)

Γ ⊢ e : Long
Γ ⊢ S1
Γ ⊢ S2
Γ ⊢ If(e) then {S1 } else {S2 }

Γ ⊢ e : Long, Γ ⊢ S
Γ ⊢ while(e){S}

Proof. By contradiction against the fact that is finite number of
variables redefined in statement s.

Γ⊢P
(Tprog)
P mpt = {l1 : n1 , ..., lk : nk }
EN = enum id1 {l1 , ..., lr }, ..., enum idk {l′1 , ..., l′r }
Γ ⊢ enum idi , 1 ≤ i ≤ k
V = τ1′ id′1 , ..., τk′ id′k [n]
Γ ⊢ id′j : τj′ , 1 ≤ j ≤ k′ − 1
Γ ⊢ id′k : array(τk′ , n)
Γ ⊢ Sentry

D. Properties of expression evaluation
We wrap the two properties of expression evaluation, which is
based on the two properties of “other” expression evaluation. In
the following, we use the notation E ′ to expand the domain of the
expression meaning function E ′ : e → σ → (verror , {0, 1}).

Γ ⊢ P mpt; EN ; V ; Sentry

Lemma D.1. If every variable in Use(e) of an expression e has
the same value w.r.t two value stores, the expression e evaluates to
same value against the two value stores, (∀x ∈ Use(e) : σ1 (x) =
σ2 (x)) ⇒ (E ′ JeKσ1 = E ′ JeKσ2 ).

Figure 19: Typing rules

7. For k > 0, Use(s1 ; ...; sk+1 ) = Use(s1 ; ...; sk ) ∪ Use(sk+1 );

Proof. The proof is a case analysis of the expression e.

Definition 37. (Def(S)) The defined variables in a sequence of
statements S, written Def(S), are listed as follows:
1.
2.
3.
4.
5.
6.
7.

S = s;
If S = “If(e) then {St } else {Sf }”, (s ∈ St ) ∨ (s ∈ Sf );
If S = “while(e) {S ′ }”, s ∈ S ′ ;
For k > 0, if S = s1 ; ...; sk ; sk+1 , (s ∈ sk+1 ) ∨ (s ∈
s1 ; ...; sk );

1. e = lval;
There are further cases regarding lval.
(a) lval = id;
By definition, Use(e) = {id}. Besides, there is no integer
overflow in both evaluations. The lemma holds trivially.
(b) lval = id[n];
By definition, Use(e) = {id}. Because the array has fixed
size, by assumption, σ1 (id, n) = σ2 (id, n) or (id, n, ∗) ∈
/
σ1 , (id, n, ∗) ∈
/ σ2 . Besides, there is no integer overflow in
both evaluations. The lemma holds.

Def(skip) = ∅;
Def(id := e) = {id};
Def(input id) = {idI , idIO , id};
Def(output e) = {idIO };
Def(If (e) then {St } else {Sf }) = Def(St ) ∪ Def(Sf );
Def(whilehni (e){S}) = Def(S);
For k > 0, Def(s1 ; ...; sk+1 ) = Def(s1 ; ...; sk ) ∪ Def(sk+1 );

68

2015/9/14

(c) lval = id1 [id2 ];
By definition, Use(e) = {id1 , id2 }. By assumption, σ1 (id2 ) =
σ2 (id2 ) = n By similar argument to the case lval = id[n],
the lemma holds.
2. e = “id == l”;
By definition, Use(e) = {id}. W.l.o.g, id is a global variable.
By assumption, σ1 (id) = σ2 (id) = l′ . If l′ = l, by rule
Eq-T, (l′ == l, m(σ)) → (1, m). If l′ 6= l, by rule EqF, (l′ == l, m(σ)) → (0, m). Besides, there is no integer
overflow in both evaluations. The lemma holds.
3. e = other;
By definition, Use(e) = Use(e). By assumption, ∀x ∈
Use(e) : σ1 (x) = σ2 (x) ∨ σ1 (x) = σ2 (x). The lemma holds
by parameterized expression meaning function for “other” expression.

Lemma D.2. If every variable in Err(e) of an expression e has
same value w.r.t two pairs of (block, value store), ∀x ∈ Err(e) :
σ1 (x) = σ2 (x) then one of the following holds:
1. the expression evaluates to crash against the two value stores,
(E ′ JeKσ1 = (error, vof )) ∧ (E ′ JeKσ2 = (error, vof ));
2. the expression evaluates to no crash against the two pairs of
1
(block, value store) (E ′ JeKσ1 6= (error, vof
)) ∧ (E ′ JeKσ2 6=
2
(error, vof )).
Proof. The proof is a case analysis of the expression e.
1. e = lval;
There are further cases regarding lval.
(a) lval = id;
By definition, Err(e) = Idx(e) = ∅. By our semantic, the
evaluation of id never crash. Besides, there is no integer
overflow in both evaluations. The lemma holds.
(b) lval = id[n];
By definition, Err(e) = Idx(e) = ∅. Because the array id1
has a fixed array size, by assumption, either ((id1 , n, v1 ) ∈
σ1 ) ∧ ((id1 , n, v2 ) ∈ σ2 ) or ((id1 , n, v1 ) ∈
/ σ1 ) ∧
((id1 , n, v2 ) ∈
/ σ2 ). Besides, there is no integer overflow in
both evaluations. The lemma holds.
(c) lval = id1 [id2 ]
By definition, Err(e) = Idx(e) = {id2 }. By assumption,
σ1 (id2 ) = σ2 (id2 ) = n or σ1 (id2 ) = σ2 (id2 ) = n. By
similar argument to the case lval = id[n], the lemma holds.
2. e = “id == l”;
By definition, Err(e) = ∅. W.l.o.g, id is a global variable.
Let σ1 (id) = l1 , σ2 (id) = l2 . W.l.o.g., l1 = l and l2 6= l,
by rule Eq-T, (l1 == l, m(σ)) → (1, m) and, by rule EqF, (l2 == l, m(σ)) → (0, m). Besides, there is no integer
overflow in both evaluations. The lemma holds.
3. e = other;
By definition, Err(e) = Err(e). By assumption, ∀x ∈ Err(e) :
σ1 (x) = σ2 (x). The lemma holds by the property of parameterized expression meaning function for “other” expression.

With respect to Lemma D.1 and Lemma D.2, we extend semantic rule for expression evaluation as follows.

E. Properties of remaining execution
We assume that crash flag f = 0 in given execution state m(f).
Lemma E.1. (S1 , m) → (S1′ , m′ ) ⇒ (S1 ; S2 , m) → (S1′ ; S2 , m′ ).
69

(r, m) → (r ′ , m′ )
E ′ : e → σ → (verror × {0, 1})

EEval’

f=0
(e, m(f, σ)) → (E ′ JeKσ, m)

Figure 20: An extended SOS rule for expressions

Proof. The proof is by structural induction on abstract syntax of
S1 .
Case 1. S1 = “skip”.
By rule Seq, (skip; S2 , m) → (S2 , m) where m = m′ .
Case 2. S1 = “id := e”.
There are two subcases.
∗
Case 2.1. (e, m) → (v, m) for some value v.
By rule Assign,
(id := v, m) → (skip, m(σ[v/x])).
Then, by contextual (semantic) rule,
(id := v; S2 , m) → (skip; S2 , m(σ[v/x])).
∗
Case 2.2. (e, m) → (e′ , m(1/f)) for some expression e′ .
Then, by rule crash,
(id := e′ , m(1/f)) → (id := e′ , m(1/f)).
Then, by contextual rule,
(id := e′ ; S2 , m(1/f)) → (id := e′ ; S2 , m(1/f)).
Case 3. S1 = “output e”
Case 4. S1 = “input id”
By similar argument in Case 2, the lemma holds for case 3 and
4.
Case 5. S1 = “If (e) then {St } else {Sf }”.
Case 5.1. W.l.o.g., expression e in predicate of S1 evaluates to
∗
nonzero in state m, written (e, m) → (0, m).
By rule If-T, (If (0) then {St } else {Sf }, m) → (St , m).
By contextual (semantic) rule,
(If (0) then {St } else {Sf }; S2 , m) → (St ; S2 , m).
Case 5.2. Evaluation of expression e in predicate of S1 crashes,
∗
written (e, m) → (e′ , m(1/f)).
Then, by rule crash,
(If (e′ ) then {St } else {Sf }, m(1/f)) →
(If (e′ ) then {St } else {Sf }, m(1/f)).
Then, by contextual rule,
(If (e′ ) then {St } else {Sf }; S2 , m(1/f)) →
(If (e′ ) then {St } else {Sf }; S2 , m(1/f)).
Case 6. S1 = “whilehni (e) {S}”.
Case 6.1 When expression e in predicate of S1 evaluates to
∗
nonzero value, written (e, m) → (v, m) for some v 6= 0, then, by
rule Wh-T,
(whilehni (e){S}, m) → (S; whilehni (e){S}, m(mc [(k +
1)/n])) for some nonnegative integer k.
Then, by contextual rule,
(whilehni (e) {S}; S2 , m) →
(S; while hni (e) {S}; S2 , m(mc [(k + 1)/n])).
Case 6.2 When expression e in predicate of S1 evaluates to
∗
zero, written (e, m) → (0, m), then, by rule Wh-F,
(whilehni (e){S}, m) → (skip, m(mc [0/n])).
By contextual rule,
(whilehni (e) {S}; S2 , m) → (skip; S2 , m(mc [0/n])).
Case 6.3 Evaluation of expression e in predicate of S1 crashes,
∗
written (e, m) → (e′ , m).
By rule crash,
(whilehni (e′ ){S}, m(1/f)) → (whilehni (e′ ){S}, m(1/f)).
Then, by contextual rule,

2015/9/14

(whilehni (e′ ){S}; S2 , m(1/f)) →
(whilehni (e′ ){S}; S2 , m(1/f)).
∗

(id := e′ , m(1/f, σ)) → (id := e′ , m(1/f, σ)) where m′ =
m(1/f, σ).
Hence, σ ′ (x) = σ(x). Besides, x ∈
/ Def(id := e′ ) by definition.
Case 2. S = “ output e”.
Case 3. S = “ input id”.
By similar argument in case 1.
Case 4. S= “If (e) then {St } else {Sf }”.
Def (S) = Def (Sf ) ∪ Def (St ) by definition. Then x ∈
/
Def (Sf ) ∪ Def (St ).
There are two subcases.
Case 4.1 W.l.o.g., expression e in predicate of S evaluates to
∗
nonzero value, written (e, m) → (v, m) where v 6= 0.
Then by rule If-T, (If (v) then {St } else {Sf }, m(σ)) → (St , m(σ))
where m′ = m.
Therefore, σ ′ (x) = σ(x). By argument above, x ∈
/ Def (St ).
Case 4.2 Evaluation of expression e in predicate of S crashes,
∗
written (e, m) → (e′ , m(1/f)).
Then, by rule crash,
(If (e′ ) then {St } else {Sf }, m(1/f, σ)) →
(If (e′ ) then {St } else {Sf }, m(1/f, σ)) where m′ = m(1/f, σ).
Therefore, σ ′ (x) = σ(x).
Besides, x ∈
/ Def (If (e′ ) then {St } else {Sf }) = Def (Sf ) ∪
Def (St ).
Case 5. S = “whilehni (e) {S ′ }”.
Def (S) = Def (S ′ ) by definition. Then x ∈
/ Def (S ′ ) by
condition x ∈
/ Def (S).
There are subcases.
Case 5.1 Expression e evaluates to nonzero value, written
∗
(e, m) → (v, m) where v 6= 0.
By rule Wh-T, (whilehni (v) {S ′ }, m(σ)) →
(S ′ ; whilehni (e) {S ′ }, m(mc [(k + 1)/n]), σ) for some nonnegative integer k.
Let m′ = m(mc [(k + 1)/n], σ). Then σ ′ (x) = σ(x).
Besides, x ∈
/ Def (S ′ ; whilehni (e) {S ′ }) = Def (S ′ ) ∪
Def (S), because x ∈
/ Def (S ′ ).
Case 5.2 Expression e evaluates to zero in state m, written
∗
(e, m) → (0, m).
By rule Wh-F,
(whilehni (0) {S ′ }, m(σ)) → (skip, m(mc [0/n], σ)) where
′
m = m(mc [0/n], σ).
Therefore, σ ′ (x) = σ(x). Besides, x ∈
/ Def (skip).
∗
Case 5.3 Evaluation of expression e crashes, written (e, m) →
′
(e , m(1/f)).
By rule crash
(whilehni (e′ ) {S ′ }, m(1/f, σ)) →
(whilehni (e′ ) {S ′ }, m(1/f, σ)) where m′ = m(1/f, σ).
Therefore, σ ′ (x) = σ(x). Besides, x ∈
/ Def(whilehni (e′ ) {S ′ })
′
= Def (S ) by definition.
Case 6. S = S1 ; S2 .
By argument in Case 1 to 5, after one step execution ((S1 , m(σ)) →
(S ′ , m′ (σ))), σ ′ (x) = σ(x).
By contextual rule, the lemma holds.

∗

Lemma E.2. (S1 , m) → (S1′ , m′ ) ⇒ (S1 ; S2 , m) → (S1′ ; S2 , m′ ).
k

Proof. By induction on number of steps k in execution (S1 , m) →
(S1′ , m′ ).
Base case. k = 0 and 1.
By definition,
0
0
(S1 , m) → (S1 , m), and (S1 ; S2 , m) → (S1 ; S2 , m).
By lemma E.1,
(S1 , m) → (S1′ , m′ ) ⇒ (S1 ; S2 , m) → (S1′ ; S2 , m′ ).
Induction step.
The induction hypothesis IH is that, for k ≥ 1,
k
k
(S1 , m) → (S1′ , m′ ) ⇒ (S1 ; S2 , m) → (S1′ ; S2 , m′ ).
Then we show that,
k+1
k+1
(S1 , m) → (S1′ , m′ ) ⇒ (S1 ; S2 , m) → (S1′ ; S2 , m′ ).
We decompose the k+1 step execution into
k
(S1 , m) → (S1′′ , m′′ ) → (S1′ , m′ ).
By lemma E.1,
(S1 ; S2 , m) → (S1′′ ; S2 , m′′ ).
Next, by IH,
k
(S1′′ ; S2 , m′′ ) → (S1′ ; S2 , m′ ).
∗

∗

Corollary E.1. (S1 , m) → (skip, m′ ) ⇒ (S1 ; S2 , m) →
(S2 , m′ ).
Proof. By lemma E.2,
∗
∗
(S1 , m) → (skip, m′ ) ⇒ (S1 ; S2 , m) → (skip; S2 , m′ ).
Then, by rule Seq,
(skip; S2 , m′ ) → (S2 , m′ ).
Lemma E.3. If one statement s is not in S, then, after one step
of execution (S, m) → (S ′ , m′ ), s is not in the S ′ , (s ∈
/ S) ∧
((S, m) → (S ′ , m′ )) ⇒ (s ∈
/ S ′ ).
Proof. By induction on abstract syntax of S.
Lemma E.4. If one statement s is not in S, then, after the execution
∗
∗
(S, m) → (S ′ , m′ ), s is not in the S ′ , (s ∈
/ S) ∧ ((S, m) →
′
′
′
(S , m )) ⇒ (s ∈
/ S ).
Proof. By induction on the number k of the steps in the execution
k
(S, m) → (S ′ , m′ ).
Lemma E.5. If a variable x is not defined in a statement sequence
S, then, after one step execution of S, the value of x is not redefined,
(x ∈
/ Def(S)) ∧ ((S, m(σ)) → (S ′ , m′ (σ ′ ))) ⇒ (x ∈
/ Def(S ′ )) ∧
′
(σ (x) = σ(x))
Proof. By structural induction on abstract syntax of statement sequence S, we show the lemma holds.
Case 1. S = “id := e”.
By definition, Def(S) = {id}. Then id 6= x by condition that
x∈
/ Def (S).
Then there are two subcases.
Case 1.1 Expression e evaluates to some value v, written
∗
(e, m) → (v, m).
Then, by rule Assign, (S, m(σ)) → (skip, m(σ[v/id])) where
m′ = m(σ[v/id]).
Hence, σ ′ (x) = σ(x). Besides, x ∈
/ Def (skip) by definition.
∗
Case 1.2 Evaluation of expression e crashes, written (e, m) →
′
(e , m(1/f)).
Then, by rule crash,

Corollary E.2. If a variable x is not defined in a statement sequence S, then, after an execution of S, the value of x is not re∗
defined, (x ∈
/ Def(S) ∧ (S, m(σ)) → (S ′ , m′ (σ ′ )) ⇒ (x ∈
/
′
′
Def(S )) ∧ σ (x) = σ(x)).
k

Proof. Let (S, m) → (S ′ , m′ ). The proof is by induction on k
using lemma E.5.

70

2015/9/14

(If (e′ ) then {St } else {Sf }, m(1/f)) →
(If (e′ ) then {St } else {Sf }, m(1/f)).
By parameterized type rule TExpr, Γ 6⊢ e′ . By type rule Tif,
Γ 6⊢ If (e′ ) then {St } else {Sf }.
Because Γ ⊢ s, then s 6= If (e′ ) then {St } else {Sf }.
Besides, s ∈
/ St , s ∈
/ Sf by condition. Therefore, s ∈
/
If (e′ ) then {St } else {Sf }.
Case 5. S = “whilehni (e) {S ′ }”.
s∈
/ S ′ by definition. There are subcases.
Case 5.1 Expression e evaluates to nonzero value, written
∗
(e, m) → (v, m) where v 6= 0.
By rule Wh-T, (whilehni (v) {S ′ }, m) →
(S ′ ; whilehni (e) {S ′ }, m(mc [(k + 1)/n])) for some nonnegative integer k.
Then s ∈
/ S ′ ; whilehni (e) {S ′ } by definition.
Case 5.2 Expression e evaluates to zero in state m, written
∗
(e, m) → (0, m).
By rule Wh-F,
(whilehni (0) {S ′ }, m) → (skip, m(mc [0/n])).
Therefore, s ∈
/ skip.
∗
Case 5.3 Evaluation of expression e crashes, written (e, m) →
′
(e , m(1/f)).
By rule crash,
(whilehni (e′ ) {S ′ }, m(1/f)) →
(whilehni (e′ ) {S ′ }, m(1/f)).
Then, by type rule Twhile, Γ 6⊢ whilehni (e′ ) {S ′ }. Because
Γ ⊢ s, then s 6= whilehni (e′ ) {S ′ }.
Besides s ∈
/ S ′ , then s ∈
/ whilehni (e′ ) {S ′ } by definition.
Case 6. S = S1 ; S2 .
By argument in Case 1 to 5, after one step execution (S1 , m) →
(S ′ , m′ ), s ∈
/ S′.
By contextual rule, (S1 ; S2 , m) → (S ′ ; S2 , m′ ).
By definition, s ∈
/ S2 .
Then, by definition, s ∈
/ S ′ ; S2

Based on Corollary E.2, we extend the result to array variable
elements.
Corollary E.3. If an element in an array variable x[i] is not defined in a statement sequence S in a program P = EN ; V ; Sentry ,
then, after an execution of S, the value of x[i] is not redefined,
∗
(x ∈
/ Def(S)) ∧ ((x, i, ∗) ∈ σ) ∧ (S, m(σ)) → (S ′ , m′ (σ ′ )) ⇒
′
′
(x ∈
/ Def(S )) ∧ σ (x, i) = σ(x, i)).
Lemma E.6. If all of the following hold:
1. There is no loop of label n in statements S, “whilehni (e){S ′ }” ∈
/
S;
2. The crash flag is not set, f = 0;
3. There is an entry n in loop counter, (n, ∗) ∈ loopc ;
4. There is one step execution, (S, m(f, loopc )) → (S ′ , m′ (loop′c ));

then, loop′c (n) = loopc (n).

Proof. The proof is by induction on abstract syntax of S, similar to
that for lemma E.5.
Corollary E.4. If all of the following hold:
1. There is no loop of label n in statements S, “whilehni (e){S ′ }” ∈
/
S;
2. The crash flag is not set, f = 0;
3. There is an entry n in loop counter, (n, ∗) ∈ loopc ;
4. There is multiple steps execution of stack depth d = 0,
∗
(S, m(f, loopc )) → (S ′ , m′ (loop′c ));

then, loop′c (n) = loopc (n).

Lemma E.7. If all of the following hold:
1. A non-skip statement s is not in S, (s 6= skip) ∧ (s ∈
/ S);
2. There is one step execution of stack depth d = 0, (S, m) →
(S ′ , m′ ),
then, s ∈
/ S′.

Lemma E.8. Let s = “whilehni (e) {S ′′ }”. If both of the following
hold:

Proof. By structural induction on abstract syntax of statement sequence S, we show the lemma holds.
Case 1. S = “id := e”.
Then there are two subcases.
Case 1.1 Expression e evaluates to some value v, written
∗
(e, m) → (v, m).
Then, by rule Assign, (S, m) → (skip, m(σ[v/id])).
Hence, s ∈
/ skip by definition.
∗
Case 1.2 Evaluation of expression e crashes, written (e, m) →
′
(e , m(1/f)).
By parameterized type rule TExpr, Γ 6⊢ e′ . Then, by type rule
TAssign, Γ 6⊢ id := e′ .
Then, by rule crash,
(id := e′ , m(1/f)) → (id := e′ , m(1/f)).
Because Γ ⊢ s, then s 6= id := e′ . Hence, s ∈
/ id := e′ by
definition.
Case 2. S = “ output e”.
Case 3. S = “ input id”.
By similar argument in case 1.
Case 4. S= “If (e) then {St } else {Sf }”.
s∈
/ Sf , s ∈
/ St by definition. There are two subcases.
Case 4.1 W.l.o.g., expression e in predicate of S evaluates to
∗
nonzero value, written (e, m) → (v, m) where v 6= 0.
Then by rule If-T, (If (v) then {St } else {Sf }, m) → (St , m).
Therefore, s ∈
/ St .
Case 4.2 Evaluation of expression e in predicate of S crashes,
∗
written (e, m) → (e′ , m(1/f)).
Then, by rule crash,

• s ∈ S;
• (S, m(loopc )) → (S ′ , m′ (loop′c ));

then one of the following holds:
1. The loop counter of label n is incremented by one, loop′c (n) −
loopc (n) = 1;
2. There is no entry for label n in loop counter, (n, v) ∈
/ loop′c ;
′
3. The loop counter of label n is not changed, loopc (n)−loopc (n)
= 0;
Proof. Let S = s′ ; S ′′ . The proof is by induction on abstract syntax
of s′ .

71

2015/9/14

Compiler-Assisted Heterogeneous Checkpointing 
Feras Karablieh Rida A. Bazzi
Computer Science and Engineering
Arizona State University
Tempe, AZ 85287
fferas.karablieh,bazzig@asu.edu

Abstract
We consider the problem of heterogeneous checkpointing
in distributed systems. We propose a new solution to the
problem that is truly heterogeneous in that it can support
new architectures without any information about the architecture. The ability to support new architectures without additional knowledge or custom configuration is an important
contribution of this work. This ability is particularly useful
in mobile settings in which there is no a priori knowledge
of the potential machines on which the application might
execute. Our solution supports execution in unknown settings as long as there is compiler support for the high-level
language in which the application is written. We precisely
define what it means for a particular solution to be heterogeneous and discuss the heterogeneity of our solution and
other solutions. We use code instrumentation at the source
code level to provide heterogeneous checkpointing and recovery.

1 Introduction
In this paper, we propose a solution to the problem of heterogeneous checkpointing in distributed systems. The aim
is to provide transparent checkpointing and recovery mechanisms and tools that work across different architectures
and operating systems and that can support new architectures without modification. Solutions to the problem of heterogeneous checkpointing or process migration have been
proposed [2, 3, 8, 9, 10, 11, 12, 13]. However, existing
solutions require knowledge of the underlying architecture,
architecture-specific tailoring, and make some assumptions
(detailed below) that limit their flexibility. In this paper, we
propose a new solution that is truly heterogeneous in that it
can support new architectures without additional information about the architecture. The ability to support new architectures without additional knowledge or custom configuration is an important contribution of this work. This ability
is particularly useful in mobile settings in which there is no
a priori knowledge of the potential machines on which the
application might execute. Our solution supports execution
in unknown settings as long as there is compiler support for
 This work is supported in part by National Science Foundation CARRER award CCR-9876052.

Margaret Hicks
Agilent Technologies
815 14th St SW
Loveland, CO 80537
m.hicks@computer.org
the high-level language in which the application is written.
We successfully tested our implementation on Sun Solaris,
HP-UX, and Linux as well as on Windows 98, 2000, and
NT operating systems. We precisely define what it means
for a particular solution to be heterogeneous and discuss the
heterogeneity of our solution and other solutions.
We use code instrumentation at the source code level to
provide heterogeneous checkpointing and recovery. In this
approach, checkpointing is done at the application level and
the necessary code to affect the checkpointing and recovery is automatically inserted by a preprocessor. Other researchers use code instrumentation for heterogeneous checkpointing [2, 3, 8, 12], but, unlike our solution, their solutions are not truly heterogeneous and require knowledge
about the underlying architecture. To support portability,
we introduce the concept of Memory Refractor to separate
physical memory from the high-level logical memory that
is represented by high-level program variables.
The rest of the paper is organized as follows. Section 2
describes related work and summarizes our contributions.
Section 3 presents an execution model and precisely defines
heterogeneity. Section 4 presents the overall design of the
preprocessor. Section 5 presents the design of the memory
refractor. Section 6 presents the performance results.

2

Contributions

Many researchers use recompilation and/or code instrumentation for heterogeneous checkpointing or migration in heterogeneous systems. Theimer and Hayes [13] propose recompilation as a way to achieve portability in heterogeneous systems. Silva [9] provides a high level checkpointing technique that only works for programs of a given structure. The DOME project [2] uses high level checkpointing to achieve portability, but this is done in the DOME
environment and it has no support for dynamically allocated memory. The Arachne project [3] uses preprocessing to save the state of a thread, but there are restrictions on
when the state can be saved, and there is no support for handling dynamic structures. Ramkumar and Strumpen [8] developed a general heterogeneous checkpointing system using compiler techniques which are similar in many ways to
our approach. Unlike our solution, their solution requires
knowledge of the potential machines on which the checkpoint is to be taken and requires a configuration file to be

made for every new machine (this would include information about data representation and memory alignment requirements among other things). In addition their system
makes some restricting assumptions on the way the compiler generates code. For instance, it assumes that variables
appear in physical memory in the order they are declared.
While this may be true for most compilers, and is indeed
required for compilers of the C language, it is not required
for other programming languages and is not part of the specification of most high-level languages. Also, due to the way
checkpointing is done, recovery may result in a loss of accuracy in the representation of floating point numbers even if
checkpointing and recovery are done on the same machine.
In our approach, loss of accuracy occurs only when recovery is done on a machine whose accuracy is smaller than
that of the machine on which checkpointing is done. Ssu
and Fuchs [12] developed a general heterogeneous checkpointing system that overlaps checkpoint saving with the
application execution by sending checkpoints to a slave process that maintains calling sequence information. Similarly
to [8], their work requires knowledge of the underlying architecture in order to translate pointer values between two
architectures. The address translation is achieved through
address space tables that contain information about variables and their addresses. The address space table is sorted
according to the address field. Their paper does not describe
in detail how dynamic memory allocation is supported, but
it seems that allowing memory de-allocation would make
the address space table expensive to keep sorted. The performance examples they provide are for programs that do
not de-allocate memory. Unlike our work and that of [8],
they did not implement a preprocessor, but rather coded
some examples by hand.
The code generated by our preprocessor does not use any
features of the C language that are not commonly available
in other imperative programming languages. This makes
our approach applicable to imperative programming languages other than C. This is in contrast to the solution of [8]
which produces checkpointing code that uses pointer arithmetic and other peculiar C features, which restricts its applicability. The preprocessor generatres code with goto statements. This does not restrict the applicability of our results
because it is possible to automatically transform goto statements into while loops [1].
The main advantage of the approaches of [8] and [12]
over our approach is efficiency. In our approach, we sacrifice some efficiency for a level of heterogeneity that is not
supported by any other checkpointing system. While we
did not develop our system with a strong emphasis on efficiency, the experimental results we conducted so far are
encouraging.
Finally, we precisely define what it means for a checkpointing and recovery mechanism to be truly heterogeneous.
Other works do not attempt to define heterogeneity. We believe that a precise definition is needed for any fundamental study of the overhead incurred by heterogeneous checkpointing and recovery (such a study is beyond the scope of
this paper).

3

Heterogeneity and Transparency

3.1 Heterogeneity
A program can have various representations. For instance, a
program can be expressed in a high-level specification language, a high-level programming language, or object code.
Each representation can be executed either directly on the
hardware or indirectly through a software environment available for the particular architecture to translate the representation. For instance, a program written in the C programming language can be executed on a given architecture by
first compiling it with a compiler for that architecture. In
the following, we attempt to give a precise definition to the
notions of heterogeneity and hardware independence.
Definition 1: A language L is heterogeneous with respect
to a set of hardware platforms A if L has an execution environment on each element of A.
We should emphasize here that in our view, heterogeneity is a relative notion and that is why we define it relative
to a set of hardware platforms.
Definition 2: We say that a program P written in language
L is hardware independent with respect to a set of hardware
platforms A if L is heterogeneous with respect to A and the
execution of P is correct on all elements of A.
In this work, we assume that all programs under consideration are hardware independent with respect to all platforms on which they might execute. We call such programs
hardware independent programs. It should be clear that requiring programs to be hardware independent is not a limitation of our approach, but rather is a requirement for any
solution that does not use any knowledge of the underlying
hardware.
Definition 3: We say that a mechanism M is heterogeneous
with respect to a set of hardware platforms A relative to
language L if L is heterogeneous with respect to A and M
is written in L.
It should be clear that the availability of execution environments for L on each element of A is not a fixed property. In fact, some mechanisms that are claimed to be heterogeneous achieve heterogeneity by creating the execution
environment (as in JAVA) or by extending the execution environment to support particular details of the mechanism.

3.2 Transparency
We seek checkpointing and recovery mechanisms that require the least effort from the user. Ideally, a mechanism is
transparent if the user interface is the same with or without
the mechanism. In our implementation, there is no modification to the way the program is normally executed. To
recover, the user has to execute the program with a R option. Also, before producing the executable, the user has to
run the source code through the preprocessor first.

4 Preprocessor
The preprocessor takes an architecture-independent C program, source, as input and transforms it into a semantically
equivalent C program, target. The target is capable of saving its state at runtime, and can be executed in recovery
mode to restart from a saved checkpoint. The preprocessor assumes that source is error-free and syntactically correct. There are many difficulties when attempting to save
the state of the program in a machine-independent form.
We need to be able to save the global variables, the stack
variables, the heap variables and the program counter. In
what follows we discuss how each of these components can
be saved at the application level.

4.1 Checkpointing and Recovery
Checkpointing is the process by which the program state
is saved to stable storage. Checkpointing is done by the
checkpoint function that contains code that saves all global
variables and then traverses the State Stack and saves the
local variables and the label for every function on the call
stack.
To keep track of the mode of execution of a program, a
Boolean variable mode is added to main. The mode variable can have one of two values: NORMAL or RECOVERY. This variable is initialized to NORMAL and is set to
RECOVERY if the program is restarted after a fault (this
is indicated by an extra argument that is passed to the program). The mode variable is passed as a parameter to every
function in the program (in the declaration, definition, and
invocation). At the start of each function (as modified by the
preprocessor), there is code that checks the execution mode.
This check determines how the function will be executed.
In the NORMAL mode, the program executes its instructions sequentially and skips no part of its code. In addition,
the code generated by the preprocessor to maintain the state
stack is executed. In the RECOVERY mode, the program
bypasses code that has been executed when the checkpoint
was done. This is achieved as follows. Starting with the
outer scope, each called function recovers first its variables
and then a jump is executed to the label of the next function on the call stack (This discussion omits describing handling of heap variables. This will be done in more detail in
later sections). This transfers control to the next function
on the call stack and skips code between the beginning of
the current function and the next function on the call stack.
The same process is repeated until the innermost scope is
reached. In the innermost scope, the local variables are first
recovered then the recovery code jumps to the location after the checkpoint call (the call that resulted in the checkpointed file). At that point, NORMAL execution resumes.

4.2 Saving Global Variables
The global variables are relatively easy to save at the application level. For each global variable the preprocessor creates a function (macro) that knows how to save the variable.
The data type information of the variable is used in creating

the macro. Basic data types are handled directly, whereas
nested data types are handled recursively until a basic data
type is encountered. Saving pointer data types is especially
problematic and is described below.

4.3 Saving Stack Variables
To save the state of the stack at the source level, we need to
keep track at run time of the call sequence and the variables
in every scope of the call sequence.
To facilitate keeping track of stack variables, the preprocessor modifies the source code so that all variables in a
given scope are encapsulated inside a structure.
A data structure called the state stack is used to keep
track of the execution stack. The state stack and the code
that manipulates it is generated by the preprocessor. The
state stack is a stack that is maintained at runtime and simulates the execution stack in memory. As functions are called
and exited, nodes are pushed and popped from the state
stack. The code to push and pop nodes in the state stack
is generated by the preprocessor.
Each node in the state stack corresponds to a scope and
contains three fields to capture the state in that scope. The
first field of a node in the state stack is a pointer to the
structure encapsulating local variables. The second field is
a pointer to a function that is generated by the preprocessor and knows how to save the local variables in the current
scope. The third field is a label that indicates the call out
position in the current scope. The call out position is the
position in the current scope where a call to the next function in the current stack has been made. For example, if the
stack contains main, f , and g , in that order, then the node
entry for main will contain a label to the statement in main
in which f is called and the node entry for f contains a label to the statement in f in which g is called. The sequence
of labels in the state stack is used to recover the program
counter as explained below. 1

4.4 Saving the Program Counter
At the machine level, the program counter is the address of
the next instruction to execute. Saving the program counter
can be achieved by saving the pc register. At the source
level, saving the program counter is more involved and is
closely linked to saving the stack. To save the program
counter at the source level we use labels (which are available in C and most high-level languages). A similar approach is used in [8].
A label is inserted before each function call other than
checkpoint. A label is also inserted after each checkpoint
call. Before a function (other than checkpoint) is called,
the label associated with the call is inserted in the state stack
(in the same node that contains pointers to the local structure and associated saving function). We can think of the
1 We assume that the program is written in such a way that only one
function on a checkpoint path can appear in an expression and that no
function on a checkpoint path appears in argument list. This restriction
can be eased by transforming non-conforming programs by introducing
temporary variables.

sequence of labels from the main scope to the checkpoint
scope as representing the program counter and stack status.
During recovery, the saved state stack is used to restore
the state of the program. In each scope, local variables are
first restored, then the a jump is performed to the label indicated in the state stack entry of the current scope. This way,
the recovery procedure skips code that had been executed
when the last checkpoint was taken.

4.5 Heterogeneity
In order to provide heterogeneous checkpointing, we need
to save and restore basic data types in a machine-independent
format and without knowledge of the underlying architecture. We explain how saving and restoring is achieved, how
data is saved in the correct accuracy without knowledge of
the underlying architecture, and how our solution is unaffected by the endianess of the machines on which saving
and restoring is done.
4.5.1 Saving
All data is saved using the standard fprintf C library function. The resulting checkpoint file is an ASCII file that can
be read using the standard fscanf library function in any environment with standard compiler support for C. This guarantees that saving and recovery of basic data types are heterogeneous. While more efficient ways to save data exist,
they would require a knowledge of data representation that
we specifically avoid using. The experimental results also
show that the overhead of saving and restoring is not high.
4.5.2 Accuracy
Saving character and various integer types is straightforward. Saving floating point types is problematic because
of the need to determine the accuracy of the saving (the accuracy is a paramter to fprintf). One approach would be to
save all floating point data in an accuracy higher than the
accuracy on any of the platforms on which the application
can recover. This approach has the disadvantage of producing larger than needed checkpoint files, but it requires very
limited knowledge about the underlying architecture. We
take a different approach that saves floating point data in
the correct accuracy without knowledge of the underlying
architecture. We use techniques developed by others to calculate the accuracy of the environment [5]. The idea is to
calculate the accuracy of the environment (compiler and architecture) by exploiting properties of computer arithmetic.
Unlike [8], our approach has the advantage that accuracy is not lost if saving and recovery is done on the same
machine. In [8], saving data is done in the least accurate
representation amongst all potential architectures on which
recovery can take place. In our approach, saving data is
done in the accuracy of the machine on which saving is taking place.

4.5.3 Endianess
Different architectures can have different endianesses. Data
represented in an architecture with small endian would have
to be converted to be meaningful in architectures with big
endian. Other systems that support heterogeneous checkpointing have to provide the conversions routines. In our
system, we do not do any explicit conversion because all
data is saved in ASCII using standard C library functions as
explained above. We already tested our systems on architectures with different endianesses and different operating
systems without any problems.

5

Memory Refractor

Since pointers are addresses of memory locations, they are
inherently nonportable. Pointer values do not preserve their
meaning across executions whether on the same machine
or on different machines. We propose the Memory Refractor, MR for short, as a mechanism to achieve portability
of pointer variables. The MR achieves portability by separating the variable values from the values of the memory
addresses through the use of indirection. The MR is also
used to keep track of the data types of heap variables at
runtime. When dynamically allocated variables are checkpointed, the checkpointing functions needs to know their
size and type. The MR provides support for dynamically
allocated variables by storing their size and type. The MR
stores the types of every variable created, thereby enabling
the support of runtime structures such as ”unions” and runtime operations such as ”casting”. Functions that know how
to save all the data types of a given program are generated
by the preprocessor and the type and size information in the
MR is used along with these functions to checkpoint heap
data. In the next section we describe MR in detail.

5.1 Design of the Memory Refractor
The MR is an array of data structures. It is divided into
three parts: global variables, stack, and heap. The size of
the global variables section is fixed based on the number of
global variables declared in the program. Every global variable is assigned a unique entry in the MR. The stack section
is used for stack variables (or local variables). Every local variable has an entry in the MR. This entry is only valid
when the variable is active, i.e. when the function it belongs
to is currently executing. Once the function returns, the entries corresponding to its local variables are removed from
the stack. The entries in the stack section are reused much
like activation records in the processor stack. A base pointer
is used to indicate the start of a function’s local variables in
the MR stack.
In the heap section, each array element points to dynamically allocated memory. When memory is allocated, an element in the heap section is initialized to point to the newly
allocated memory. When memory is freed, the corresponding heap element becomes available. So in effect, the heap
section of the MR is managed as dynamic memory with

fixed partitions by the preprocessor generated code. Each
entry in the MR heap stores sufficient information to support portability. These are: the size of the memory allocated, the type of data stored in it, and a pointer to its starting location so data may be accessed and saved.

p = (int*)malloc(10*sizeof(int));
p[3] = 20;
p++;
*p = 15;

5.1.1 Fields of the Memory Refractor

int x; long int p; long int q;
MR[_offset+0].ptr=&x;
MR[_offset+0].MR_index=-1;
p = 1;
MR[p].offset = 0;
q = 2;
MR[q].offset = 0;
MR[p].index = 0;
MR[q].index = p;
*((int*)MR[MR[p].MR\_index].ptr) = 5;
*((int*)MR[MR[MR[q].index].index].ptr) = 7;
MR[p].MR_index =
(long int) MR_malloc(10*1,"int",sizeof(int));
((int*)MR[MR[p].MR_index].ptr)[3] = 20;
(MR[p].offset)++;
*((int*) MR[MR[p].MR_index].ptr+MR[p].offset) = 15;

Each element of the MR is a structure of five fields, these
fields are used differently in each part of the MR.
ptr The first field ptr is a pointer. In the global and stack
sections, this pointer is used to point to non dynamically
allocated memory. For global variables the value of the
pointer can be easily calculated. For stack variables the
pointer value is calculated as follows. The beginning of
each section the number of variables is calculated, and each
variable is assigned an entry in the MR. The entry corresponding to a variable n has the ptr field set to the address
of n after variable declaration and before the function body,
in both execution modes. In the heap section, this variable
is used to point to the dynamically allocated memory.
index The second field is the index. This field is the portable
value of pointers. Instead of pointing to memory location
directly, pointers point to other entries in the MR. For pointer
entries, index is set to the index of the MR entry corresponding to the variable pointed to by the pointer. So, instead of
saving memory pointers, our checkpointing function saves
indices in the MR. This makes pointers portable. When recovery is done, pointer values are retrieved like those of
other variables and the index values in the the MR entries
are assigned to be the actual location where variables reside
at the time of recovery. This is explained in more detail below. For entries corresponding to non-pointer variables and
for all heap entries, the index field is set to -1.
type The type field has two main functions. In the global
and stack sections, type is used to keep track of types at
runtime, thus facilitating the support of dynamic structures
such as unions. In the heap section this variable is used to
store the type of the dynamically allocated data.
size The size field is used for heap variables and is used
to store the size of the dynamically allocated memory. Together with the type information, size information is used
for checkpointing and recovery of dynamically allocated
memory.
offset The offset field is used to facilitate the access to
arrays and dynamically allocated memory at runtime. It is
needed to support pointer portability.
To explain how the MR supports pointer portability and
dynamic structures, we use the following code segment as a
running example.
int
x; int *p;
int **q;
p = &x; q = &p; *p = 5; **q = 7;

The code segment is transformed to the following segment.

In the following section, we explain each part of the transformation.

5.2 Supporting Portable Pointers
The MR redirects (refracts), pointer references to the correct memory locations. When a pointer is assigned the address of a variable, the index field of the pointer’s MR entry is set to the index of the variable (here variable means
any part of memory directly referenced by name or obtained
through memory allocation). For example, the two assignment statements
p = &x;
q = &p;

are transformed to
MR[p].index = 0;
MR[q].index = p;

In the assignments, variables p and q are pointers. When
p is assigned the address of variable x, the statement is
transformed such that the MR entry of pointer p points to
the MR entry of variable x. Similarly when p is assigned
to q , the index of q is set to p. Also, the pointer variable
p is changed to store its index in the MR (not shown). So,

the pointer is actually pointing to the MR entry instead of
pointing to the variable itself. By doing this we separate
pointer values from the addresses of the variables to which
they point. Instead of using a non-portable address value,
pointer values are integers that are meaningful across executions and can be checkpointed and restored. When the
checkpoint function is called, all entries of the MR are
saved (except for ptr). When the program is restarted, the
entries would be restored and the ptr values would be initialized before the program resumes execution (this is explained in more details below).
When dereferencing a pointer p, the ptr field in the entry corresponding to the variable pointed to by p is dereferenced. For example, the following code segment

*p = 5;
**q = 7;

is transformed to
*((int*)MR[MR[p].MR_index].ptr) = 5;
*((int*)MR[MR[MR[q].index].index].ptr) = 7;

When pointer p in the code segment above is dereferenced, the code is changed such as its accesses the portion
of memory pointed to by the MR entry. Thus, we will need
two memory accesses, the first to access the entry of pointer
p in the MR and the second to access the physical memory
pointed to by that entry. This will result in a loss of efficiency, but will ensure pointer portability. In the case of
variable q , four memory accesses are needed because q is a
double pointer.
The value of a pointer variable can be the address of a
global, stack, or heap variable. We explain how all these
values can be made portable.
5.2.1 Addresses of Global Variables
Each global variable has associated with it an entry in the
MR whose location is fixed at compile time. In each entry,
the ptr field is equal to the variable address. The pointer
field is initialized whenever the program is called whether
in NORMAL or RECOVERY mode. This way, these entries
always correctly point to the global variables. Also, pointers
that point to these variables will have their value (the index
of the entry) meaningful across different architectures and
executions. For example, the following declaration
int x;
int *p;

is transformed to
int
x; long int p;
MR[_offset+0].ptr=&x;
MR[_offset+0].MR_index=-1;
p = 1;
MR[p].offset = 0;

In the example, for every variable declared, the preprocessor assigns an entry in the MR. Pointer variables are
transformed into long int variables as they now are used to
store the index of their own entry in the MR. Non pointer
variable x has its ptr fields initialized after declaration so
that it points to the physical memory address of x. While
this is not needed to access x, it is needed to support the
assigning of the address of x to a pointer variable.
5.2.2 Addresses of Stack Variables
Stack variables addresses are more complicated to handle
than those of global variables. This is due to the fact that
these addresses are not fixed at compile time. Nevertheless,
the number of local variables is fixed at compile time. So,
whenever a new scope is entered, the MR stack section is increased by a number of entries equal to the number of variables in the scope (it is decreased by the same number when
the scope is exited). There is a correspondence between the

MR entries for a given scope and the local variables of the
scope. This correspondence can be set at compile time the
same way it is done for global variables.
One more complication has to do with the initialization
of these values when a scope is entered. Two approaches are
possible. In one approach, all entries are initialized at the
beginning of the scope. In another approach, the value of
an entry is initialized whenever an assignment of the form
p = &x, where x is a local variable of the scope, is executed. In the first approach, initialization is done regardless
of whether local addresses are assigned to local variables.
This could be costly if there are few pointer assignments
and many local variables. In the second approach, initialization is done every time there is an assignment of the address of a local variable to a pointer. The tradeoff between
the two approaches should be clear.
5.2.3 Addresses of Heap Variables
Heap variables are handled through the heap section of the
MR. When a new dynamically allocated memory is created
using malloc, an entry in the heap section of the MR is also
created. The ptr field of the newly created entries points to
the dynamically allocated memory, and the size and the type
fields store the size and the type of the allocated memory
(type information is obtained from the casting information
preceding the malloc call). This is achieved by wrapping
the C library function malloc and replacing it with our own
MR malloc function. So, the following code segment
int *p;
p = (int *)malloc(10*sizeof(int));

is transformed to
long int p;
MR[p].MR_index=
(long int)MR_malloc(10*1,"int",sizeof(int));

MR malloc works as follows:



allocate new entry on the MR heap section and let i
be its index.






MR[i]:type = type
MR[i]:size = size

Mr[i]:ptr = (type)malloc(size  sizeof (type))
return i

Thus pointer p will now be pointing to the MR entry associated with the allocated memory rather than the allocated
memory itself. Dereferencing a pointer p to access a memory location i where 0  i  size, will now happen through
the MR. The preprocessor is responsible for changing the
code for all dynamic memory accesses to include the necessary indirection. For example, the assignment
p[3] = 20;

becomes

Memory Refractor

Variables in Memory

(program view is architecture independent)

(representation is architecture dependent)
&x
Integer Variable

MR[i]

index = -1
ptr = &x

MR[p]

index = j
ptr = null

x

&a
Array Variable

offset = m

MR[q]

index = p
ptr = null

MR[j]

index = -1
ptr = &a
size = s
type = INT

m

s
(m’th entry)

stack portion
heap portion

Figure 1: Memory Refractor Layout. The dashed line in the middle illustrates the separation that the MR provides. Dashed arrows
illustrates pointer relationship inside the MR. Solid arrows illustrate direct pointers to memory. a is a dynamically allocated array
of size s. x is an integer variable. Pointer q points to pointer p in
the figure.
((int*)MR[MR[p].MR_index].ptr)[3] = 20;

The offset field of the entry associated with p is used in the
indirection process. For example, the assignment
p++;
*p = 15;

becomes
MR[p].offset ++; /* for ptr arithmetic */
*((int*)MR[MR[p].MR_index].ptr+MR[p].offset) = 15;

When checkpointing, and after the contents of the MR
are saved, the heap is traversed and, using the information
in the MR, all the heap variables are saved. This is done
according to type and size information specified in the MR.
When restarting, and after restoring the contents of the MR,
memory is reallocated for every entry in the heap section
and the saved data is copied back to the newly allocated
memory, thereby restoring the heap.
Figure 1 illustrates the layout of the MR.
Dynamically deallocating heap variables is also supported.
This is done by replacing each occurrence of the free library
function with our own MR free. The MR free function takes
as an input a pointer value, which, after preprocessing, is the
is the index of an entry in the heap section of the MR and
frees both the MR entry as well as the memory pointed to
by the MR entry. Managing the MR memory is a standard
fixed-size partition memory management problem and we
implemented our solution using free and used lists.

5.3 Supporting Unions
Supporting unions is a challenge one faces when providing
portable heterogeneous checkpointing. The main problem

of variables of union type is that they have a dynamic type
that change at runtime. A variable x of type union may assume any of a number of data types during execution. The
problem is to determine the type of the variable when checkpointing is done. The only way to do this is to keep track,
at runtime, of the variable’s type.
The MR was designed to provide the ability to track variable types at runtime. Every element in the stack and global
sections of the MR has a type field associated with the variable (the type field is only used in the heap section). The
type field is used to store the current type of the variable
corresponding to the current MR entry (only for variables
of union type). The preprocessor is responsible for inserting code that updates the type field as the variable changes
its type at runtime. As an example, the following code segment
union unionex
{
int Integer;
float Float;
};
union unionex unionv;
unionv.Integer = 3;
unionv.Float = 3.3;
printf("%d %f\n",unionv.Integer,unionv.Float);

is transformed to
union unionex
{
int Integer;
float Float;
};
union unionex unionv;
unionv.Integer = 3;
MR[3].type = 0;//where 0 stands for type int
unionv.Float = 3.3;
MR[3].type = 1;//where 1 stands for type float
printf("%d %f\n",unionv.Integer,unionv.Float);

In the example, the type of the global variable unionv
can be either integer or float. After every statement that
changes the type of unionv , the preprocessor inserts statements that update the corresponding type field in the MR.
Note that if in the same statement d assumes more than one
type, the last used type is the one stored in type. During
checkpointing, the variable type is read from the MR, and
the appropriate save function is called. A similar process
occurs when restoring the variable, as it is restored to its last
saved type. This process involves the overhead of runtime
checking. This is unavoidable when supporting structures
with dynamic properties.

5.4 Supporting Dynamic Pointers
The description of how heap variables are handled does not
cover dynamic pointers. These are pointers that do not correspond to stack or global variables. They are created when
a heap variable containing pointers is created (this covers
pointers that are elements of heap variables directly or recursively). To support dynamic pointers, when a heap variable h is created, MR entries are also created for each element of h that is a pointer. Also, code is added to link

the pointers in h with their heap entries. By creating heap
entries at runtime, these pointers can be supported as regular stack or global pointers. When a dynamically allocated
structure is freed, the heap entries for all its pointers are also
freed.
To test our support for dynamically allocated pointers,
we used a program that builds a binary tree with 5000 nodes.
Check pointing and recovery worked as expected. The following piece of code illustrates the transformation.
struct Tree
{
int value;
struct Tree * left;
struct Tree * right;
};
struct Tree * root;
root = (struct Tree *)malloc(1*sizeof(struct Tree));

The transformed code for malloc becomes
MR[root].MR_index =
(long int)MR_malloc(1*1,"Tree",sizeof(struct Tree));
(*((struct Tree*)MR[MR[root].MR_index].ptr + 0)).left =
get_heap_entry();
MR[(*((struct Tree*)
MR[MR[root].MR_index].ptr + 0)).left].offset = 0;
(*((struct Tree*)MR[MR[root].MR_index].ptr + 0)).right =
get_heap_entry();
MR[(*((struct Tree*)
MR[MR[root].MR_index].ptr + 0)).right].offset = 0;

In the example, the Tree structure has two pointers: left
and right. When an object of type Tree is created, code
is inserted after the malloc statement to assign heap entries
for the pointers. These pointers can be used to create other
nodes and to access node values.

CPU
Sun Sparc
PA-10
Dual P2-450
P3-600

Texc

Ovh

Ovhc

Ovhr

30.02
41.81
5.06
2.98

0.1%
4%
0.4%
0.7%

0.3%
4.2%
0.8%
10%

2.8%
1.6%
2.9%
20%

Figure 2:  Program

Calls to the checkpoint function (macro) are inserted at
various locations of the program (potential checkpoints locations). On the Solaris, HP/UX, and Linux operating systems, we controlled the activation of the checkpoint using a
timer variable. The timer is used to control how often the
checkpoint operation is performed. It is implemented as a
task that periodically interrupts the application process and
sets the timer variable. This part of the implementation is
operating systemi-dependent and we did not implement it
for Windows 98, 2000 or NT. When the checkpoint function is called, a check is made to make sure that enough
time has elapsed since the last checkpoint (this is indicated
by a boolean variable that is set by the timer and reset by the
checkpoint function). If enough time has elapsed since the
last checkpoint, the execution of the program is suspended
while data is being written to file, thus adding an overhead
that is proportional to the data size. If not enough time
has elapsed since the last checkpoint, the program immediately resumes normal execution. In the following section,
we present the results for both execution and memory overhead.

6 Experimental Results

6.1 Execution Overhead

We tested two scientific applications on a variety of hardware and OS platforms. To test the correctness of the implementation, we tested checkpointing and recovery on Sun
Ultra 1/Solaris 5.7, Sun Server 2000/Solaris, HP 9000 K210/
HP-UX 10.20, Intel Pentium 3/Windows 2000, Intel Pentium 2/Windows 98, and Intel Pentium 2/Windows NT. Checkpoints produced on one architecture were used to recover
on a different architecture. It is interesting to note here
that we did not need to tailor our implementation to any
of the operating systems even though the development was
done on Solaris 5.7 using gcc. To our knowledge no other
system that provides heterogeneous checkpointing supports
both different architectures and different operating system
platforms. We present experimental results for four of the
systems.
The first program implements an algorithm to calculate
 to a 1000 digits accuracy. This program uses large arrays
(10000) and has 8 functions. The second program implements an algorithm to calculate heat diffusion as a function
of time. This program was chosen because of its heavy use
of pointers and therefore is a good candidate to test the efficiency and the overhead induced by the MR. Both programs include intensive data calculations, and operate on
large data sets, thus are good for observing the checkpoint
file sizes.

6.1.1  Calculation
We use an implementation based on [7]. The program produces the first 1000 digits of  , using only integer arithmetic. It uses an array of 10000 integers and iterates 2500
times to calculate successive digits of  . We had two potential checkpoint locations, one at the start of the loop, and
another in the function that performs the main calculation.
We first measured the execution time of the untransformed
program. Then we measured the execution of the transformed program but with setting the timer to a value larger
than the execution time of the non transformed program.
This ensured that the checkpoint function is not executed.
These measurements are used to record the overhead induced by the code transformation rather than the checkpointing process itself. In the remaining runs, we measure
the overhead for various values of timer.
Our performance measurements are summarized in the
tables below. In the tables, Texc is the execution time without preprocessing and Ovh is the percentage overhead of
the preprocessed due to code instrumentation, but not checkpointing. Ovhc is the percentage overhead due to instrumentation and checkpointing. Ovhr is the percentage overhead due to recovery. Ttot is the execution time with the
number of checkpoints as indicated in the tables.

Timer (sec)
8
4
2
1

Nckpt

Ttot (sec)

Ovh

3
7
16
34

30.19
30.47
31.05
32.23

0.6%
1.5%
3.4%
7.4%

Figure 3:  Program on Sun Ultra 1/SunOS 5.7
Timer (sec)
16
8
4
2
1

Nckpt

Ttot (sec)

Ovh

2
5
11
22
47

43.6
43.75
44.08
44.88
46.65

4.5%
4.6%
5.6%
7.3%
11.6%

Figure 4:  Program on HP 9000 K210/HP-UX 10.20
Timer (sec)
4
2
1

Nckpt

Ttot (sec)

Ovh

11
22
47

5.10
5.11
5.15

0.8%
1.2%
1.8%

Figure 5:  Program on Dual P2-450/Linux

The Ovhr values are obtained by killing the process and
restarting it from the checkpoint file. This value measures
the time to read the values from the checkpoint file and reconstruct it stack and resume execution as a percentage of
the execution time of the program.
We can notice from the results that the overhead of our
methods for supporting data types, arrays, and functions is
small. The overhead of checkpointing is directly proportional to the number of times the checkpoint function is executed. The time taken by the checkpoint function to execute
is proportional to the size of the variables in the program.
The position of the checkpoint function affects the overall
performance; the larger the size of the stack, the longer the
checkpointing time.
6.1.2 Heat Equation
The Heat Equation program provides a solution to the heat
diffusion problem. It uses two dynamically allocated two
dimensional arrays of values of type double and of size
256*256. It employs intensive floating-point calculations
in its main loop. The main loop of the program iterates for
256*256*1000 times, in each iteration a value in the arrays
is updated using floating-point operations. This program is
a good illustration of the overhead due to the indirection
introduced by the MR for supporting pointers and dynamically allocated memory. On the Sun Ultra 1 machine the
overhead without checkpointing was calculated to be 76.6%
which is high, but it was 39% on the P3-600 machine. It is
important to realize in evaluating this overhead that checkpointing of pointer variables is being supported without any

CPU
Sun Sparc
PA-10
Dual P2-450
P3-600

Texc (sec)

Ovh

Ovhc

Ovhr

65.05
66.72
14.21
10.23

76.6%
99.6%
116%
39%

81.4%
103%
134%
11.3%

6.7%
4.9%
6.8 %
23.7%

Figure 6: Heat Program with pointer operations
CPU
Sun Sparc
PA-10
Dual P2-450
P3-600

Texc (sec)

Ovh

69.44
71.8
8.58
4.62

1%
3%
3%
7%

Figure 7: Heat Program without pointer operations

knowledge of the underlying architecture and that the program tested is an extreme case of the use of pointers. Also,
the development was done without a strong emphasis on optimization. The main aim of this part of the development is
to prove the feasibility of truly heterogeneous checkpointing and recovery. Furthermore, it should be expected that
a checkpointing scheme that is truly hardware independent
would require introducing some form of indirection similar
to the one we introduce with the MR; the use of indirection
for pointers would make it unavoidable to introduce overhead similar to ours.
We rewrote the same program to use statically allocated
arrays instead of dynamically allocated arrays. The overhead during execution dropped dramatically. The results
are shown in Table 7. We are currently studying ways to
treat dynamically allocated arrays similar to the way we
treat statically allocated arrays. This would reduce the overhead greatly and make the results comparable to those for
the  application.
The overhead of checkpointing alone is low on all machines as can be concluded from the table (compare Ovh
with Ovhc ). Note that for the faster machines the overhead of checkpointing alone or recovery is relatively high
because the execution time is much smaller so disk access
becomes more significant. The overhead due to the writing the checkpoint to file is proportional to size of the data
used in the program. Techniques to minimize the size of the
checkpoint have been proposed by many researchers [4, 6].
The use of such techniques is beyond the scope of this paper. We omit providing the results for multiple checkpoints
due to lack of space, but they are consistent with those of
the  program.

6.2 Optimized Design
The execution overhead is high for pointer accesses and
is due to the indirection introduced by the MR. To access
a heap memory location through a stack pointer (pointer
variable declared as a local variable), the transformed program has to go through two indirections. One indirection

is through the stack MR entry corresponding to that pointer
and another indirection is through the heap MR entry. We
need two levels of indirection to be able to support all types
of pointers and pointer operations.
For some programs, not all pointer operations are supported. We found that for some types of programs, it is possible to reduce the indirection to only one level by having
a stack pointer indexing a MR heap entry directly without
a MR stack entry. The cost to pay for this optimization is
a loss of generality. The resulting system loses the ability
to support pointers to pointers and to support pointer arithmetic.
We implemented an optimized version of the preprocessor and tested it on the heat program on different architecutres.
On all architectures, the resulting overhead was half the
overhead introduced by the non optimized preprocessor.

7 Memory Overhead
The difference in memory usage between the instrumented
code and the source code is application dependent. There is
sizeof(MR struct) bytes of overhead per variable and that’s
about the size of five integers. This overhead is proportional
to the number of variables in the program and not to their
size. So, if the number of variables is much smaller than
their size, the overhead is small. This is the case for scientific application that use large arrays.
The overhead can be large in two cases. If the program
creates dynamic structure with a large number of small nodes,
the overhead can be large. Also, the overhead can be large
if the program has small variables and deep stack during
execution.
The checkpointing overhead also depends on the application. The MR entries need to be saved only for pointers.
So, if a program contains few pointers, the main overhead
is due to the ASCII representation of the variables. It turns
out that in such a case the size of the checkpoint file can
be three times larger than the size of memory used by the
program. Surprisingly, the checkpoint file can be smaller
than the size of memory used by the program if the values
of the variables saved are small. For example, a variable of
type ”long int” whose value is 10 would only need 3 bytes
to save - 2 bytes to save 10 and one byte for space.

8 Conclusion
We have shown that it is possible to provide heterogeneous
checkpointing and recovery in environments in which the
target architectures are unknown. The only requirement is
the presence, on each architecture, of compiler support for
the high-level language in which the application is written.
We have shown that this can be achieved while introducing acceptable overhead, especially if the checkpointing frequency is not high.

References
[1] E. A. Ashcroft and Z. Manna. The Translation of ’Go To’
Programs to ’While’ Programs. In proceedings of IFIP
Congress, volume 1, pp. 250–255, Ljubljana, Yugoslavia,
August, 1971.
[2] A. Beguelin, E. Seligman, and P. Stephan Application Level
Fault Tolerance in Heterogeneous Networks of Workstations.
Journal of Parallel and Distributed Computing, v. 43, no. 2,
pp. 147–155, June 1997
[3] B. Dimitrov. Arachne: A Compiler-Based Portable Threads
Architecture Supporting Migration on Heterogeneous Networked Systems. Master Thesis, Purdue University, May
1996.
[4] E. N. Elnozahy, D. B. Johnson, and W. Zwaenepoel. The
performance of consistent checkpointing. In Proceedings of
the 11th IEEE Symposium on Reliable Distributed Systems,
pages 39–47, 1992.
[5] S. Pemberton. The Ergonomics of Software Porting: Automatically Configuring Software to the Runtime Environment. http://www.cwi.nl/ftp/steven/enquire/enquire.html.
[6] J. S. Planck, J. Xu, and R. H. B. Netzer. Compressed Differences: An Algorithm for Fast Incremental Checkpointing.
University of Tennessee, Technical Report, UT-CS-95-302,
1995
[7] S. Rabinowitz and S. Wagon. A spigot algorithm for  .
American Mathematical Monthly Volume 102, Number 3,
March 1995.
[8] B. Ramkumar and V. Strumpen. Portable Checkpointing for
Heterogeneous Architectures. In Proceedings of 27th International Symposium on Fault-Tolerant Computing Systems,
pages 58–67, June 1997.
[9] L. Silva. Portable Checkpointing and Recovery. In Proceedings of the Fourth IEEE International Symposium on High
Performance Distributed Computing, IEEE Computer Society Press, pp. 188-195, August 1995.
[10] P. Smith and N. Hutchinson. Heterogeneous Process Migration: The Tui System. Technical Report 96-04, Department of
Computer Science, University of British Columbia, February
1996.
[11] B. Steensgaard and E. Jul. Object and Native Code Thread
Mobility Among Heterogeneous Computers. Proceedings of
the Fifteenth ACM Symposium on Operating Systems Principles, 1995.
[12] K. F. Ssu and W. K. Fuchs. PREACHES - Portable Recovery and Checkpointing in Heterogeneous Systems. In Proceedings of IEEE Fault-Tolerant Computing Symposium, pp.
38–47, June 1998.
[13] M. M. Theimer and B. Hayes. Heterogeneous Process Migration by Recompilation. In Proceedings IEEE 11th International Conference on Distributed Computing Systems,
pages 18-25, 1991.

How to Have Your Cake and Eat It Too: Dynamic Software Updating with
Just-in-Time Overhead
Iulian Neamtiu
Department of Computer Science and Engineering
University of California, Riverside
Riverside, CA 92521
neamtiu@cs.ucr.edu

Rida A. Bazzi
Bryan Topp
School of Comp. Inf. and Dec. Sys. Engineering
Arizona State University
Tempe, AZ 85287
{bazzi, betopp}@asu.edu

changes in cache locality due to instrumentation by DSU
compiler [4], [2]. While some existing low-level mechanisms
can avoid that overhead, they are architecture dependent
and harder to maintain, but more importantly they are not
general mechanisms for dynamic software update. In this
paper, we argue that any general mechanism for dynamic
software update must suffer some unavoidable overhead
unless knowledge and control of the compiler is assumed
by the mechanism. This overhead must be introduced by
dynamic software update mechanisms even for update systems such as Ekiden [5] which aim to reduce the overhead
by restricting the generality of the mechanism and relying
on the programmer to make up for the loss of generality.
By analyzing the overhead that must be introduced by
current dynamic software update systems, we realized that
we could develop general update mechanisms that incur high
overhead only after an update has been initiated and have
little to no overhead during normal execution (i.e., before the
update is initiated and after the update has completed) The
solution uses a novel shifting gears approach. The idea is to
run in high gear (low overhead) during normal execution
and only shift to low gear (high overhead) just before
the update. Once the update is done, the execution shifts
up to high gear as the execution progresses. To evaluate
the overhead introduced by our approach, we performed a
preliminary implementation of this scheme where we added
the instrumentation manually, rather than automatically by a
compiler, on an example program to show that the overhead
can be greatly reduced during normal execution. We use
as an example, the KissFFT program and show that the
overhead due to instrumentation can be reduced to less
than 10% during normal execution. This is significantly
lower than the overhead introduced by the instrumentation
of Upstare and Ginseng which can range from 40% to more
than 100% depending on the platform and compiler used.
In summary, the paper has two main contributions:

Abstract—We consider the overhead incurred by programs
that can be updated dynamically and argue that, in general,
and regardless of the mechanism used, the program must
incur an overhead during normal execution. We argue that the
overhead during normal execution of the updateable program
need not be as high as the overhead for the updated program.
In light of the fundamental limitations and the differences in
the overhead that must be incurred by the updateable and
updated programs, we propose a new mechanism for dynamic
software update based on a new shifting gears approach.
The mechanism attempts to incur just the required overhead
depending on the stage of update the application is in. Before an
update the execution incurs low overhead and when an update
occurs the execution incurs higher overhead which reverts to
low overhead as the execution progresses. We evaluate the
mechanism by modifying an application by hand. Preliminary
performance numbers show that the mechanism performs
better than existing mechanisms for dynamic software update.

I. I NTRODUCTION
Upgrading deployed software, whether for adding functionality or fixing bugs, is a significant part of the software
lifecycle. Upgrading software typically results in substantial
downtime needed to stop the old application and load and
start the upgraded (new) application. For applications that
cannot tolerate the interruption associated with traditional
software upgrades, dynamic software update (DSU) offers
the possibility of replacing the running application inmemory without the need for relinquishing system resources
or terminating application processes and threads. Existing
works on DSU concentrate on providing the system mechanisms to induce the update: suspending execution without
stopping the application processes, copying the state from
the old to the new version, and starting the new version [1],
[2], [3].
Existing application-level mechanisms for dynamic
sofware update use DSU compilers which instrument the
application source code so that it is updateable at runtime.
These mechanisms introduce substantial overhead due to
indirection introduced by the mechanism, the loss of optimization opportunities by the compiler, as well as the

c 2012 IEEE
978-1-4673-1764-1/12/$31.00 

1) An analysis of the fundamental limitations on the
overhead introduced by general mechanisms dynamic
software updates.

1

HotSWUp 2012, Zurich, Switzerland

saving the state of the old version and constructing a state
of the new version. We discuss the effects of each on
performance.
A. Saving the State
Mapping the state of the application requires that, at the
time an update is effected, the application-level variables be
accessible in order to be able to use their values to construct
a state of the new version of the application. So, any update
mechanism whether applied at the application level or at the
executable level must have access to the application-level
variables. This rules out the possibility for the compiler to
optimize-out application level variables that might be needed
to construct the state of the new version. Also, at the update
point, the source level code that appears before the update
point should be fully executed. Otherwise, we cannot have
guaranteed semantics for the program state at the update
point. This rules out any compiler optimizations that move
code across the update point. Therefore, saving the state puts
the following two restrictions on the compiler:
1) Application-level variables must be accessible and
may not be optimized out at the update points
2) No code motion is allowed across the update point.
Otherwise, the compiler can optimize across an update
point as show in Figure 1 (a). In the figure, the red colored
rectangles represent update points. As shown in the figure,
the compiler can optimize the expression e = b + c to e = a
because that does not affect the saved values. The compiler
cannot move a = b + c to after the update point because
the value of a captured at the update point should correctly
reflect the semantics of the program.
The first execution, (a), shows only the effect of update
point and assumes that these update points do not add
to the size of the code. In the second execution, (b), the
update points are shown to be larger because in general code
needs to be added to the application in order to save the
state. Adding that code can affect the cache locality of the
application.

Figure 1. Effects of DSU on optimization of code: (a) code motion not
allowed across update point, but optimizations across update point are
allowed; (b) same as (a) but instrumented code is larger which affects
instruction cache locality; (c) optimizations not allowed across update
point. This is especially problematic inside loops.

2) A new dynamic software update approach that introduces less overhead than other existing approaches.
The rest of the paper is organized as follows. In Section II,
we study the fundamental limitations on the performance of
dynamically updateable programs. In Section III, we propose
a new DSU mechanism that minimizes the overhead through
a novel shifting gears approach. In Section IV we present
preliminary performance results that show that our approach
can reduce the overhead of dynamic software update. In
Section V we compare our approach to related work.
II. E FFECTS OF DSU ON P ERFORMANCE
In this section, we argue that in general, providing
dynamic software update must incur an overhead during
the normal execution of the updateable program as well as
the updated program. Interestingly, the overhead must exist
independently of the type of mechanism used, so it applies
to general systems such as Upstare and Ginseng [2], [4] as
well as systems that explictly try to avoid the overhead such
as Ekiden [5].
To simplify the discussion, we assume that the program
consists of only one function, possibly with local variables.
In application-level dynamic software update, a number of
points in the old version are marked as potential update
points—we call these points “potential” because, depending
on which functions are on the stack at the time the point
is reached, the update could be unsafe. Without loss of
generality, we assume that the update is safe at the next
update point.
When the update point is reached, dynamic software
update requires that the execution state of the old version
be mapped to a state of execution of the new version of the
program (or to a hybrid version consistent with both the old
and the new version). A state mapping has two components:

B. Constructing a New State
To construct the state, the programmer must be able to
associate a point in the execution of the old version with
a point in the execution of the new version. This requires
that the point in the execution of the new version, which
is specified statically, must be a barrier to any code motion
so that update does not alter the application-level semantics.
Also, since the state of the new version will be constructed
at that point and code that reconstructs the state needs to be
part of the new version, the compiler cannot tell whether the
values before the barriers are the same as after the barriers.
This would rule out any optimizations across the barrier.
This situation is shown in Figure 1 (c). In the figure, the
compiler cannot tell that e = a after the barrier because the
values of a, b, and c are read-in at the point of update.

2

a = b * c + d;

t = b * c;
a = t + d;

t = b * c;
a = t + d;

// update

// update

// update

e = b * c + f;

e = t + f;

t = b * c;
e = t + f;

(a)

(b)

an update must be performed, the state of the old version
must be saved and the state of the new version must be
constructed. The new version will be instrumented so that
it can construct its state from the saved state of the old
version. Finally, since the new version itself should be able
to save its state when an update is required, the new version
will first be loaded with ability to both save and restore
its state. Note that a fully instrumented new version will
incur high overhead in the worst case due to the presence
of instrumentation to restore the state. So, there is a need
for a way to eliminate the instrumentation that restores
the state and replace the fully instrumented new version
with a version that only contains instruentation to save the
execution state. We will argue below that, in general, it is
not possible to remove full instrumentation while a function
is active. Instead we propose that as functions exit, they are
replaced with versions that are instrumented only to save the
state. As a consequence, all but the longest running functions
(e.g., main()) can be replaced not long after an update is
completed. To summarize, the stages for an update are:
1) Initially, code is instrumented to save the execution
state.
2) When an update occurs, the execution state is saved.
3) A new version with the ability to both save and restore
the execution state is loaded.
4) As functions exit, they are replaced with versions that
are less heavily instrumented, that can only save the
execution state.
5) When a subsequent update is required, the state of
execution is saved, and go to stage 3.

(c)

Figure 2. Code example showing that the compiler cannot ignore the
presence of code that reconstructs application-level state of the program:
(a) source code; (b) incorrectly compiled code with optimization; and (c)
compiled code with no optimization.

This overhead is worst case overhead, but it must be
incurred at update points. Here, it might be tempting to think
that it would be enough to “instruct” the compiler to ignore
the code that constructs the new state when optimizing
in order to reduce the overhead. Unfortunately, that can
lead to incorrect code as we show below. So, the loss of
optimization is a fundamental cost that cannot be avoided.
To understand why instructing the compiler to ignore the
code that reconstructs the state is not a viable solution,
consider the code snippet shown in Figure 2. The source
code is shown in (a). In the compiled code (b) the value
of e is calculated using a temporary variable t and program
variable f . The cause of the difficulty is the fact that t is
not visible at the source level. If the state is reconstructed
at the update point, the values of a, b, c, d, and f before the
update point would be correct but e would not be correctly
calculated because t is not part of the program-visible state.
The compiler would have to compile the code so that t is
recalculated as shown in (c). So, constructing the state puts
the following three restrictions on the compiler:
1) Application-level variables must be accessible and
may not be optimized out at the update points
2) No code motion is allowed across the update point.
3) No optimization can be made across update points.
In summary, dynamic updates at the application level
would require a loss of optimization opportunities regardless
of the mechanism being used. Our study of the fundamental
constraints that dynamic software update introduces led us
to a new approach for dynamic software update that incurs
just the right amount of overhead. This is discussed next.

A. Compilation and Gear Shifting
Normal Operation (Pre-Update, High Gear): During
normal operation, the instrumented code must ensure that all
application-level variables are available at an update point
and that the update point be a barrier against code motion.
Also, the instrumentation should increase the code size as
little as possible.
To achieve these goals, we use the instrumentations illustrated in Figure 3 (which is an example of the instrumentation that we hand coded). All local variables of the function
are encapsulated inside a structure and all variable accesses
are transformed to accesses of the corresponding fields in
the structure. For each function there is a corresponding
struct type for the structure that holds the local variables
of that function.
At the update point, the local variables are saved
in a global stack variable. Each entry in stack
(stacknode) will contain the local variables of a function
that is active on the execution stack. A stacknode is large
enough to hold any set of local variables. To save space, it
is declared as a union of all possible local variables struct
types. The copying of the local variables is achieved with a
simple statement which assigns the local structure to the

III. S HIFTING G EARS : A N EW A PPROACH FOR
DYNAMIC S OFTWARE U PDATE
In this section, we present our approach to reducing the
overhead of dynamic software update. The approach relies
on the observation that we can run the system in high gear
in the common case (pre-update and post-update) and only
switch to low gear for a brief period around the update.
Essentially, the system operates in high gear, with the
only overhead being mainly due to instrumentation used to
save the execution state when an update is required. When

3

typedef struct {
int _t;
int _r;
int p;
int floor_sqrt;
} loc9t;

During Update Operation (Low Gear): When an update occurs, the saved state needs to be mapped to a state
of the new version. This requires a new version that can
restore and save the state be loaded, and the state mapping
be applied. This can be done in any way required (similar
to the way it is done in UpStare for example) as long as
it is done outside the main execution path. Naturally, code
introduced by the instrumentation (in the main execution
path) should be kept at a minimum. The instrumentation is
illustrated in the hand-coded example of Figure 4.
Normal Operation (Post-Update, High Gear): After the
state of the new version is retored (or constructed), we would
like to get rid of the instrumentation that restores the state
because that instrumentation introduces a lot of overhead.
We propose that such an elimination can be achieved as
functions exit. When a function is called again, a non-fully
instrumented version is used.

typedef struct {
int t;
union {
loc1t loc1;
loc2t loc2;
...
loc9t loc9;};
} stacknode;

void kf_factor(int n,int * facbuf)
{
loc9t loc;
loc._t = 9; // tag
loc.p=4;
loc.floor_sqrt = (int)floor (sqrt (n));
/*factor out ... */
do {
if(__update_req) {
loc._r = 91;
stack[sp++].loc9 = loc;
return;
}
...
} while (n > 1);

Figure 3.

B. Reducing the Overhead Further
Instrumentation to save state.

One might hope to further reduce the overhead by eliminating the code that restores the state while a function is
active. We argue that in general such an elimination is not
possible. Essentially, we would like to be able to map the
state of execution of a function with full instrumentation to
the state of execution of a function with less instrumentation
(and hence more optimization).
The difficulty of mapping the state of a fully instrumented
to that of a less instrumented function (that only saves state)
has to do with the differences in optimizations in general.
Revisting the example in Figure 2, we see that in general
replacing an active fully instrumnented function with a less
instrumented function can result in the loss of temporary
variables that are not visible at the application-level. Since
it is not possible to assign values to these temporaries at
the application level, we cannot expect an application-level
mechanism to be able to achieve the replacement.

void kf_factor(int n,int * facbuf)
{
loc9t loc;
loc._t = 9;
if (__recover) {
switch (stack[sp].loc6._r)
{ case 91: goto recovery91;
case 92: goto recovery92;}
}
loc.p=4;
loc.floor_sqrt = (int)floor (sqrt (n));
/* factor out ... */
do {
if(__update_req) {
loc._r = 91;
stack[sp++].loc9 = loc;
return;
}
recovery91:
if(__recover) {
loc = stack[sp--].loc9;
}
...

IV. E XPERIMENTS AND C ONCLUSION

} while (n > 1);

Figure 4.

To test our approach, we coded the instrumentation by
hand. We tested the instrumentation on Kiss FFT [6], the
bête noire of dynamic software update systems due to its
dependence on compiler optimizations for performance. We
had two versions of the instrumentation; one version could
save the state of execution and one version could both save
and restore the state.
We did the testing on two machines: (1) Intel DUO 2Gb,
1.6 Ghz machine and (2) Xenon 4Gb, 2.8 Ghz. The results
(additional overhead compared to the uninstrumented code)
are shown in Table I. The results show a wide gap between
the ovehead of instrumentation that saves the state and
instrumentation that saves and restore the state. At around
10% overhead for this CPU-bound application, one can
expect most applications to have considerably less overhead

Instrumentation to save and restore state.

corresponding field in the union. In addition, the update
point is stored in that structure as an integer identifier. It can
be seen that the amount of instumented code is minimal: (1)
store update point; (2) copy local variables to stack; and, (3)
return to caller.
When an update is invoked, one by one, the functions will
store the local variables in stack and when the main()
function is reached, stack contains a copy of the execution
stack (minus function parameters) and at that point a state
mapping can be done but that does not interfere with the
normal execution of the program.

4

whole state can saved and restored in any loop in the
program. In contrast, for the updates applied using Kitsune,
not all variables are saved and restored and stack reconstruction is done selectively which reduces the overhead. Also,
Kitsune manages to reduce the size of instrumented code
which reduces the effects of loss of cache locality.
Lucos [8] is an approach for applying dynamic updates
to the Linux kernel using Xen virtualization. Lucos employs
binary rewriting and paging to update functions and types.
The performance overhead is less than 1% when dynamic
updates consist of applying small patches to the Linux
kernel. Polus [1] is a user-space follow-up to Lucos: calls
to updated functions go to the latest version, but active
functions continue to execute at the old version. Old data and
new data may coexist, and maintaining coherence between
them is relegated to the programmer. The performance
overhead ranges from 5% (VsFTPd) to 30% (Apache).
UpStare [2] uses stack reconstruction to allow an actively
running function to transition to a corresponding point in
the new version of the same function when an update is
applied. This technique has the same effect as Ginseng’s
code extraction, but is more flexible, as transition points can
be specified at patch time, not deployment time.

Table I
OVERHEAD FOR K ISS FFT.
Pentium Duo
Xeon

Instrumented to Save
12.68%
8.65%

Instumented to Save/Restore
40%
39.42%

when executing in high gear. The result for the save-andrestore instrumentation are also competitive with those of
Ginseng and Upstare.
The experiments do not measure the overhead for an application after some, but not all, fully instrumented functions
are replaced with non-fully instrumented versions. We argue
that such functions will not be a source of high overhead
for well-written applications, since they will normally be
dispatch functions that do not have much computation.
Also, our discussion in the previous section shows that it
is not possible to eliminate such full instrumentation with
an application-level update mechanism.
In summary, we have studied fundamental limitations on
the overhead introduced by dynamic software update and
proposed a new scheme that preliminary experiments show
to be a viable approach for low overhead dynamic update.

ACKNOWLEDGMENTS
This research was supported in part by NSF grants CSR0849980 and CCF-0963996. We thank the anonymous referees for their helpful comments on this paper.

V. R ELATED W ORK
We discuss the most relevant related works. Ginseng [4]
transforms off-the-shelf C programs into C programs that
can be updated on-the-fly via two main techniques: type
wrapping and function indirection. Ginseng allows a wide
range of updates to C programs, however, indirection and
type wrapping impose a permanent performance overhead.
This overhead ranges from 32% for I/O bound programs to
129% for the KissFFT CPU-bound program.
Kitsune [7] is a whole program-replacement DSU systems
for C, i.e., it updates a program by starting the new version
from scratch and transferring the state from the running
version. In Kitsune state migration is automatic by default
for global variables, but other variables must be marked
manually for migration. Control migration means indicating
update points, just like in our approach. Some form of stack
reconstruction is manually achieved by the programmer who
“must write code to direct execution back to the equivalent
spot in the new program”. The low overhead achieved by
Kitsune (around 2% for the applications considered) does
not contradict the results of this paper. We argued that
for variables that need to be saved or restored, compiler
optimizations will be inhibited. Variables that do not need
to be saved or restored can still be optimized out. General
update mechanisms like Ginseng and Upstare are overly
conservative and instrument the code under the assumption
that all variables need to be saved and restored. The numbers
we provided for the KissFFT application can be considered
as extreme because the application is highly sensistive to
compiler optimizations and we instrumented it so that the

R EFERENCES
[1] H. Chen, J. Yu, R. Chen, B. Zang, and P. Yew, “POLUS: A
POwerful Live Updating System,” in ICSE’07, pp. 271–281.
[2] K. Makris and R. Bazzi, “Immediate multi-threaded dynamic
software updates using stack reconstruction,” in Proceedings
of USENIX Annual Technical Conference, 2009.
[3] G. Stoyle, M. Hicks, G. Bierman, P. Sewell, and I. Neamtiu,
“Mutatis mutandis: Safe and predictable dynamic software
updating,” ACM Trans. Program. Lang. Syst., vol. 29, no. 4,
p. 22, 2007.
[4] I. Neamtiu, M. Hicks, G. Stoyle, and M. Oriol, “Practical
dynamic software updating for C,” in PLDI’06, pp. 72–83.
[5] C. M. Hayden, E. K. Smith, M. Hicks, and J. S. Foster, “State
transfer for clear and efficient runtime upgrades,” in Proceedings of the Workshop on Hot Topics in Software Upgrades
(HotSWUp), Apr. 2011.
[6] M. Borgerding, “Kiss FFT,” http://sourceforge.net/projects/
kissfft/.
[7] C. M. Hayden, E. K. Smith, M. Denchev, M. Hicks, and J. S.
Foster, “Kitsune: Efficient, general-purpose dynamic software
updating for C,” Jan. 2012, http://www.cs.umd.edu/∼mwh/
papers/hayden11kitsune.html.
[8] H. Chen, R. Chen, F. Zhang, B. Zang, and P.-C. Yew, “Live
updating operating systems using virtualization,” in VEE ’06,
2006, pp. 35–44.

5

Distrib. Comput. (2007) 19:267–287
DOI 10.1007/s00446-006-0012-y

SPECIAL ISSUE PODC 05

On the establishment of distinct identities in overlay networks
Rida A. Bazzi · Goran Konjevod

Received: 6 September 2005 / Accepted: 5 July 2006 / Published online: 8 September 2006
© Springer-Verlag 2007

Abstract We study ways to restrict or prevent the
damage that can be caused in a peer-to-peer network
by corrupt entities creating multiple pseudonyms. We
show that it is possible to remotely issue certificates that
can be used to test the distinctness of identities. Our certification protocols are based on geometric techniques
that establish location information in a fault-tolerant
and distributed fashion. They do not rely on a centralized certifying authority or infrastructure that has direct
knowledge of entities in the system, and work in Euclidean or spherical geometry of arbitrary dimension. They
tolerate corrupt entities, including corrupt certifiers, collusion by either certification applicants or certifiers, and
either a broadcast or point-to-point message model.
Keywords Sybil attack · Identity verification ·
Overlay networks · Peer-to-peer systems · Distance
geometry

1 Introduction
In a large scale peer-to-peer overlay network, physical
entities that reside on different physical nodes commuThe second author was supported in part by the NSF Grant
CCR-0209138.
R. A. Bazzi (B) · G. Konjevod
Computer Science and Engineering Department,
Arizona State University, Tempe, AZ 85287, USA
e-mail: bazzi@asu.edu
G. Konjevod
e-mail: goran@asu.edu

nicate with each other using pseudonyms or logical identities. In the absence of direct physical knowledge of a
remote entity or a certification by a central authority
that a particular identity resides in a particular node, an
entity can appear in the system under different names
or counterfeit identities. Counterfeit identities are problematic in a peer-to-peer system because they can prevent entities from performing a remote operation, such
as saving a file, multiple times to increase availability.
An entity might select different identities to perform an
operation, but these identities might all reside on the
same corrupt entity, resulting in a loss of redundancy.
Counterfeit identities can also prevent the formation
of reliable reputation-based recommendation systems.
An entity that can create counterfeit identities can also
create identities with fake reputations, thus making reputations meaningless. Douceur [10] calls the forging of
multiple identities a Sybil attack.
In this paper we study ways to restrict or prevent the
damage that can result from corrupt entities performing Sybil attacks. We are interested in mechanisms to
restrict the damage due to the creation of pseudonyms,
while not relying on a centralized certifying authority
or infrastructure with direct knowledge of entities in
the system. While standard authentication techniques
work well to prevent impersonation of existing identities, they do not address the issues arising from proliferation of pseudonyms. The first work that studies counterfeit identities that we are aware of is the paper by
Douceur [10]. He argues that under the strictest requirements, that is, in a fully distributed system without a
central authority and in which entities communicate by
broadcasting messages, the only means to limit the generation of multiple identities is by exploiting the fact that

268

resources of individual entities are bounded.1 Douceur
argues that, by requiring entities to dedicate significant
portions of their resources to establish their identities,
one could, at least theoretically, limit the number of
identities that are forged by a corrupt entity. The three
types of resources he considers are: computation, communication and storage.
Our main observation is that the damage caused by
Sybil attacks in Douceur’s model is not only due to the
fact that corrupt entities can forge multiple identities.
The damage caused by Sybil attacks is also due to the
fact that, in the absence of additional information, for
any two identities one of which resides on a corrupt
entity with unbounded resources, one cannot conduct a
test to determine that their entities are distinct. So, our
goal need not necessarily be to test that any two particular identities reside on distinct entities, but rather to
test that amongst a group of identities, a large enough
subset of them resides on a set of distinct entities. We
call the problem of determining the number of distinct
entities on which a group of identities reside the group
distinctness problem. When determining the exact number of distinct entities is not possible, we determine a
lower bound on that number. We call a test to solve
the group distinctness problem a group-distinctness test.
Realizing such a test would allow the remote execution
of remote operations and therefore circumvent the harm
done by Sybil attacks. An example illustrates this point.
Assume that one can divide identities into two separate groups such that any two identities chosen from
different groups are distinct, but two identities from the
same group are not necessarily distinct (we will abuse
terminology and say that the identities are distinct when
they reside on distinct entities). For concreteness, also
assume that there is only one corrupt entity in the system. Under these assumptions, if an entity asks n entities
in one group to perform an operation and another n entities in the other group to perform the same operation,
it can be guaranteed that at least n distinct entities performed the operation even though it cannot tell which
ones they are. If the operation consists of saving a file,
the entity can be assured that there are enough correct
replicas of the file in the system. The goal of this paper
is to show group-distinctness tests are possible and to
explore conditions under which such tests can be made
accurate.

1

The absence of a central authority precludes the use of IP
addresses to identify entities because they rely on the authority of
the Internet Corporation for Assigned Names and Numbers (ICANN). Furthermore, in real systems IP addresses can be spoofed
and a host might be provided dynamic IP addresses by its ISP.

R. A. Bazzi, G. Konjevod

We develop our work by exploiting an ingredient that
was eliminated by the strong assumptions of Douceur,
namely that entities have locations. Most of the real
distributed systems we can imagine are in some way
embedded in space with geometric properties. Moreover, entities in the system have (at any moment in
time) their own physical locations and no two entities
share the exact same location at any moment (our model
will allow entities to share locations if their locations
cannot be distinguished by remote entities). In general,
we can assume that the underlying space (whose points
include all the participants in the protocol) has a geometric structure of standard d-dimensional Euclidean
space Rd or sphere Sd . For sound or radio communication these assumptions are quite realistic (even though
accuracy of measurement is always an issue) and they
have already been exploited for secure location verification [18], while in the case of Internet-based overlay
networks they are justified by recent work on estimating
network distances [19] (we further discuss these assumptions as they relate to the Internet in Sect. 2) . We distill
our assumptions into the following:
1.

2.

the actual distances between pairs of entities at least
approximately satisfy two of the three metric properties: symmetry and triangle inequality; and
the transfer of a message back and forth between
two identities takes time that is lower-bounded by
a (nondecreasing) function of the distance between
the two entities on which they reside.

We do not assume that logical identities are always
honest, and we place no bounds on the computational
resources of corrupt entities. However, since each entity
is located at a point in a geometric space, its communication with the rest of the identities in the system
is restricted by the geometry of the space. In particular, a simple assumption of finite message propagation
implies our second assumption above: the time in which
a message is transmitted from point x to point y gives
an upper bound on the distance d(x, y) between the two
points. Note that our model allows multiple entities to
reside at the same point in the geometric space.
1.1 An example
To illustrate how physical locations can be used to provide a test of distinctness, consider two correct entities
A and B at a distance d from each other. Assume that
there is only one corrupt entity C in the system, that C
has unbounded resources, and that C is within a radius
of d/2 from A. Under these conditions, C can forge

Distinct identities

an unbounded number of identities, but none of these
forged identities can pretend to be at a distance less
than d/2 from B. In fact, for each identity c of C, one
can request from A and B an upper bound on their distance to c. This bound can be obtained by having A and
B broadcast probe messages to c and measure the time
it takes to receive a reply from c. Since the distance
from C to A is less than d/2 and the distance from A
to B is d, it follows from the triangle inequality that the
roundtrip time of probes sent from B to C (under any
of its pseudonyms) will always indicate a distance that is
larger than d/2 and therefore none of C’s identities can
prove that they are within radius d/2 from B. Using this
test of distinctness, an entity can require that a remote
operation be executed by n identities that can prove
that they are within a radius of d/2 from A and another
n identities that can prove that they are within a radius
of d/2 from B and therefore be guaranteed that enough
correct entities executed the operation. One can use the
distance between A and B and their distance from c as a
certificate of identity of c as follows. For a group of identities, whose certificates are all computed with respect
to the same A and B and such that m of the certificates
have distances from A that are less than d/2, n certificates have distances from B that are less than d/2, and l
certificates that have distances from A and B that are at
least d/2, then at least min{m, n, l} of the identities are
distinct. Note that for such a group of certificates one
can give a lower bound on the number of distinct entities in the groups while in many cases it is not possible
to determine the exact number of distinct identities in
the group. Our goal is to get as high a lower bound as
possible for such a group-distinctness test.
We should emphasize that A and B in the example
above are not the same as a centralized certifying authority (we discuss this point further in Sect. 10). In fact,
A and B’s knowledge of C or its forged identities is
obtained solely through remote interaction with C’s various identities and with each other and the assumption
that they are both honest (which we will not require
in general), whereas a centralized certifying authority
requires some form of direct knowledge of C. Also, note
that A and B need not know each other’s location, they
only need to know the distance between them and that
they are both correct.

1.2 Paper outline
The goal of this paper is to study various scenarios under
which entities such as A and B in the example above
can be used to significantly restrict the types of counterfeit identities by corrupt entities and therefore eliminate

269

the harm caused by Sybil attacks. We show that one can
construct certificates that are much more powerful than
the one suggested above and that can be used under
stronger adversarial conditions. The rest of the paper is
organized as follows. Section 2 defines our system model.
Additional discussion of the model, mostly dealing with
issues that need to be resolved in a practical application
of our work, is presented in Sect. 10. Section 3 explains
how our model can be applied to a wireless network. Section 4 introduces group-distinctness tests and geometric
certificates. Section 5 summarizes our results and contributions and discuss related work. Section 6 collects
in one place the notation and terminology of the paper.
Finally, in Sect. 7 we begin the technical development of
our results.

2 System model
We consider a system consisting of a set B of n beacons
and a set of A of applicants. The set A∪B is the set of participants. We assume the participants are points in either
the standard d-dimensional Euclidean space Rd or the
d-dimensional unit sphere Sd . In making statements that
hold for both Rd and Sd , we refer to the space as X. We
denote by ρ the metric in X, that is, if x, y ∈ X, then
ρ(x, y) is the distance between x and y. We assume that
participants are not mobile and that their locations are
fixed.
Our model is not tied to any particular underlying
system. It can be applied to overlay as well as wireless
or other networks as long as the basic assumptions are
satisfied. A particular system might satisfy the assumptions only for a subset of its points. For instance, a system might satisfy the metric assumptions only for sets of
points that are a minimum distance θ of each other. We
discuss how our assumptions map to a particular system
in Sect. 3.
In what follows we list our remaining assumptions
about various aspects of the system, namely: communication, synchrony, and failures.

2.1 Communication
Beacons communicate with each other and with applicants by exchanging messages. We assume that beacons
communicate always using broadcast messages. This is in
keeping with the beacons’ a priori ignorance of the locations of applicants. In what follows we detail our assumptions about communication between beacons and
applicants.

270

2.1.1 Broadcast and point-to-point
We distinguish two models for the messages transmitted
by the participants: broadcast and point-to-point. We
assume that our broadcast (or multicast) primitive is an
indivisible operation that cannot be split into multiple
point-to-point operations. In other words, a beacon will
be able to tell if a message it receives is a broadcast
message and an applicant cannot make a point-to-point
message look like a broadcast message. A number of
our results assume that applicants must communicate
using broadcast; these results do not hold if the broadcast operation cannot be distinguished from multiple
point-to-point operations. By restricting applicants to
broadcast communication, we are able to tolerate more
adversarial conditions. It is interesting to note, though,
that restricting communication to broadcast makes it
harder for faulty applicants to benefit from colluding,
whereas in Douceur’s argument, broadcast makes it
harder to prevent Sybil attacks. The broadcast model
is meaningful in general overlay networks in which a
message flooding primitive is available for communication. Certificates can then be established using flooding
exclusively. The broadcast model is also meaningful if
the applicants cannot modify the underlying communication interface and a broadcast or multicast primitive
is available for communication. Finally, the broadcast
model is natural for wireless networks in which nodes
do not have directional antennas [15].

2.2 Bounded-range broadcast
We also consider a model in which beacons can bound
the range of a broadcast. In such a model, a broadcast
will only reach applicants that are within a given radius
from the broadcaster. We distinguish two variants on
this model. In the basic model, a recipient of a broadcast message with bounded range can determine the
range of the broadcast (note that the distance between
a recipient and a broadcaster might be smaller than the
range of the broadcast). In another model, we assume
than an applicant that receives a bounded-range broadcast message can only tell that the range of the broadcast
is at least equal to the distance between itself and the
broadcaster. In other words, an applicant cannot tell
the ultimate reach of a signal it receives. These models of communication are motivated by wireless sensor
networks. While the main thrust of our work emphasizes overlay networks, our techniques are also applicable to other settings in which communication delay has
geometric properties. As we noted in the introduction,
wireless communication systems present such a setting

R. A. Bazzi, G. Konjevod

and our techniques are applicable to such systems. In
such systems, there exists the additional capability of
limiting the range of communication by reducing the
transmission power. This corresponds to our idealized
model in which messages are received by all applicants
within a given radius from a broadcaster and no applicants beyond that radius. In practice, the range does not
have the shape of a step function, but rather the signal strength tapers off exponentially with distance. The
main model we are interested in is one in which applicants are not able to use measurements of the power of
a signal to determine the ultimate range of the signal
(we still assume though that the broadcasting beacon’s
location is known to all applicants, which only makes it
harder for beacons to tolerate faulty applicants).
This model can also be meaningful in computer networks if corrupt applicants do not have access to the
routing functionality. In that case, a bounded-range
broadcast can be implemented with a time-to-live (TTL)
field in the message header. That field gets decremented
as the message is forwarded from one node to the other
in order to reflect the elapsed time since the message was
sent. This idea is similar to that of packet leashes [13]
which have been proposed to handle wormhole attacks.

2.3 Failures
Some applicants and some beacons might be faulty (or
corrupt). When we consider corrupt beacons, we assume
that no more than f of them are corrupt and the remaining are correct (honest). A corrupt beacon can report
fake distances to any participant, and these distances
can be smaller or larger than the actual distance to
the participant. An applicant can also be corrupt and
it can delay its responses for probes from the beacons
therefore making it appear farther away than it really
is. We consider cases in which applicants might collude
and cases in which corrupt applicants do not collude. To
strengthen our adversarial model, and unless otherwise
noted, we assume that corrupt applicants and beacons
know the locations of all beacons and that correct beacons know about each other’s locations only what can
be inferred from the time it takes to receive replies from
probes. We assume that the distance between two correct entities is a non-decreasing function of the roundtrip
message delay between them (we discuss this and our
other assumptions in detail in Sect. 10). We use μ for the
distance as measured by exchanging messages between
points. Thus μ(A, B) is the distance A can deduce from
the roundtrip time of a message transmitted from A
to B and back. For correct participants A and B, we
assume that μ(A, B) = ρ(A, B) that is, the distance

Distinct identities

can be accurately measured by observing the roundtrip
delay. Note that in the presence of faulty participants
μ is not necessarily symmetric. For a participant A in
the system, we denote by x(A) the location of A in the
underlying geometric space.
Throughout, we assume that corrupt entities have
unbounded computation power. Our protocols implicitly assume that an entity cannot anticipate probe messages and send replies before the receipt of the actual
probes. This assumption can be easily enforced by using
the standard technique in which each probe message
includes a randomly generated string that only the
sender knows and that must also be included in the reply.
This way, an entity would have only a very small probability of successfully being able to reply before receiving
a probe.

2.4 Synchrony and reliability
We assume that the system is asynchronous and that
message transmission is not reliable, but that there are
periods of time during which message transmission is
synchronous and reliable. We assume that for large
enough time intervals, the system will enter a synchronous period. While these assumptions do not really simplify on-line communication between peers in an overlay
network, because peers cannot wait for the periods of
synchrony, they are necessary for establishing geometric
certificates. The idea is to have participants probe each
other for a somewhat long period of time in order to
get an accurate measure of distance. In fact, we expect
that the measurements during periods of synchrony (low
congestion periods) accurately reflect the distances
between correct participants. Once certificates are obtained, we do not require any synchrony assumptions
for communications between participants. Beacons have
local clocks and the rate of drift of these clocks is small
enough so that clock drift is negligible during the time it
takes to establish a certificate.

3 Specific system model
We illustrate the applicability of our model by explicitly mapping our assumptions to wireless networks. Section 10 further discusses the applicability of our results
to overlay networks in the Internet. For this section, we
use terminology from [16].
Metric Assumptions In wireless sensor networks
there are multiple possible candidates for the distance
measure. The time of flight (ToF) or time of arrival
(TOA) is the equivalent of our roundtrip delay and

271

it readily satisfies the metric assumptions. Accurately
measuring the time of flight is possible [16] and it requires synchronization between applicants and beacons.
An alternative measure used in wireless networks is time
difference of arrival (TDOA) which, instead of measuring ToF, measures the difference between the times
of arrivals of signals from a given sensor to beacons.
The advantage of using TDOA is that it only requires
synchronization between beacons, but it still assumes
that ToF defines a metric. The Receiver Signal Strength
Indicator (RSSI) is another possibility for measuring
distance. It is based on a model of signal fading with
distance, but it is not as accurate as TDOA.
As long as the overhead imposed by the protocols is
not large, the time of flight satisfies metric assumptions.
Communication In wireless networks, both broadcast and point to point communication is possible.
Broadcast requires no special hardware support, while
point to point communication is possible with the use of
directional antennas [15].
Synchronization Synchronization is possible in
wireless networks. Synchronization between applicants
and beacons is more difficult to achieve, especially if
applicants have limited resources. Synchronizations
between beacons is easier to achieve.
Failures Our model of failures encompasses all
kinds of malfunctions that can occur in a wireless sensor network, including sensor nodes and beacons being
taken over by an adversary.

4 Geometric certificates
4.1 Geometric certificates
An applicant can request a geometric certificate from a
set of beacons. When an applicant requests a geometric
certificate, the beacons and the applicant execute a protocol that might require the applicant (as well as other
beacons) to respond to probe messages. The protocol
might also require the applicant to send probe messages
to the beacons and report distances to various beacons.
The result of these exchanges will be a geometric certificate: a set of distance values between the beacons and
the applicants that are signed by the beacons as well as
the applicant. We assume that the beacons’ public keys
are known to applicants. An applicant’s public key is
provided by the applicant. Its function is to simply tie
the applicant’s identity to a key. In Sect. 10 we explain
why our assumption about the beacons’ public keys does
not imply that we need a central certification authority
to verify the identities of applicants.

272

4.2 Group-distinctness test
A group-distinctness test is a function D : 2C → N that
assigns to a group of certificates a number that is a lower
bound on the number of entities that were involved in
obtaining these certificates. A trivial lower bound for
any group is 1 and our goal is to get as high a lower
bound as possible.
A special case of a group-distinctness test is a test in
which the group is restricted to two elements. In that
case, we define a two-distinctness test as a function D :
C × C → {true, unknown} that assigns to a pair (c1 , c2 ) of
geometric certificates a value in the set {true, unknown}.
If D(c1 , c2 ) = true, then the entities that obtained these
certificates are distinct.
Our approach is conservative. The distinctness tests
we propose are sufficient but not necessary to establish
distinctness of identities. For example, different
machines that reside on the same LAN would likely
appear to be at the same location to remote beacons.
In fact, such machines would appear to be at the same
location in the geometric space. his does not mean that
we can only use one machine from a set of machines that
appear to be at the same location. As our introductory
example shows, to execute a remote operation, one can
choose multiple groups of machines such that machines
in each group appear to be at the same location. If the
number of corrupt entities in the system is smaller than
the number of groups of entities that are collocated, then
a group-distinctness test applied to certificates of entities
from these groups would reveal that there are multiple
entities in the groups without necessarily identifying the
entities that are different and the collection of groups
would be guaranteed to contain at least as many entities
as in the smallest amongst them.

R. A. Bazzi, G. Konjevod

should not confuse the location verification problem
with the group-distinctness problem. These two problems are somewhat related, but are fundamentally different. Verifying the location of identities is one way to
establish their distinctness, but the converse is not true
and distinctness tests are a more general approach to
dealing with Sybil attacks because in many instances it
is possible to come up with distinctness tests, even when
it is not possible to do location verification; the introductory example is a case in point.
The following is a summary of our results. We present
geometric certification protocols, which issue compact
and easily-checkable certificates to applicants. Some
protocols issue certificates that can be used in twodistinctness tests and others issue certificates that can
be used in group distinctness tests for groups of more
than two elements. Protocols that issue certificate for
two-distinctness tests are basically location verification
protocols and they outperform existing location verification protocols presented in the literature (we expand
on this in the next section). Given two certified entities, a
distinctness test may be performed, and if the two entities’ geometric locations are distinguishable from the
point of view of the beacons that participated in the certification protocol, the distinctness test will succeed and
certify that the two entities are indeed distinct. Protocols that issue certificates to be used in group distinctness
tests have no counterpart in the literature. The certification protocols we present work for several different
settings. In all cases, we assume the number of beacons is
at least d + 1, where d is the dimension of the space; also,
unless otherwise specified, the applicant entity or entities should be in the convex hull of the certifying beacon
set; finally, beacons’ messages are always broadcast.
1.

5 Contributions and summary of results

Two-distinctness tests
(a)

5.1 Contributions
The main contribution of this work is the identification
and introduction of the group-distinctness problem as
a problem whose solution can mitigate or eliminate the
effects of Sybil attacks and to show that it is possible to
remotely issue certificates that can be used to test the distinctness of identities. To our knowledge this is the first
work that shows that remote anonymous certification of
identity is possible under adversarial conditions.
For various settings, we exhibit two-distinctness tests.
For some settings we only provide general group-distinctness tests. Typically, we implement two-distinctness
tests by verifying the locations of identities, but one

(b)

(c)

Correct participants. The applicant’s identity
can be established with no restriction on the
applicant’s location or additional restrictions
on the number of beacons.
Corrupt applicants that do not collude and
correct beacons. The applicant’s identity can
be established if it is in the convex hull of beacons (in Rd ), or the set of beacons is
“sufficient” (in Sd ) without restriction on the
applicant’s location (for an exact definition,
see Sect. 7.1.2).
Multiple colluding applicants. The applicants’
identities can be established if they are
restricted to the broadcast message passing
model. In the point-to-point message passing model, the applicant’s identities can be

Distinct identities

(d)

2.

established in two dimensions if there are no
more than two corrupt applicants in collusion.
We also consider the case of multiple corrupt
applicants in two dimensions.
Up to f corrupt beacons. We require at least
f + d + 1 correct beacons in order to identify
applicants; this is in addition to the requirements for the correct beacons case (single
corrupt applicant or multiple colluding applicants).

273

for ICMP ping messages between a set of known hosts
probes and several sets of targets. They assign the targets
to points in a coordinate system, by defining each coordinate of a node as its distance from one of the probes.
This embedding into a low-dimensional Euclidean space
allows them to derive simple lower and upper bounds on
distances between targets from their probe-target measurements by using the triangle inequality. Our work is
motivated by these results in considering embeddings of
overlay networks in Euclidean space.

Group distinctness tests
(a)

(b)

(c)

Multiple colluding applicants in the point to
point model. We present a protocol that establishes a lower bound on the number of distinct
applicants amongst a group of applicants and
in the presence of multiple beacons.
Multiple colluding applicants in the pointto-point model and with bounded-range broadcasts used by beacons. In a system in which
beacons can limit the range of their broadcasts and applicants cannot determine the
reach of a message they receive, we show that
in R2 , and in the presence of three correct beacons, k faulty entities cannot simulate more
than k2 distinct points.
Multiple colluding applicants with a grid of
beacons in the point-to-point model (no
bounded-range broadcast). We present a protocol for the setting where a set of beacons
are equally spaced around the perimeter of
a square and we give a lower bound on the
number of entities corresponding to a group
of certificates. The protocol has the desirable feature that the number of corrupt applicants needed to simulate any point inside the
square is quadratic in the number of beacons.

5.2 Related work
There is a substantial body of literature that is related to
the results in this paper. Giving an overview of related
work is difficult and presents many subtle problems,
because of the differences in terminology and assumptions made in various works. We do not aim to give an
exhaustive overview of related work and we only present work that is most closely related to the results of this
paper.
5.2.1 Coordinate-based network distance prediction
Ng and Zhang [19] model the Internet as a geometric space by using measurements of roundtrip delay

5.2.2 Triangulation and embeddings
Kleinberg et al. [14] design algorithms that try to infer a
complete distance matrix of a finite set of points, given
only the distances from a small number of selected
points (beacons in their terminology) to every other
point. They show that it is possible to reconstruct most
of the distances, but also that arbitrary distortion of
a certain fraction of all distances is unavoidable. They
use some of the powerful recent results on metric embeddings and provide very general algorithms, however
their results do not seem to have immediate applications
to the Sybil attack problem.

5.2.3 Sybil attack
The Sybil attack was introduced by Douceur [10]. We
already discussed that work in the introduction and we
further discuss it in Sect. 10. After Douceur’s paper there
were a few attempts to deal with the Sybil attack.
Newsome et al. [18] study the Sybil attack in the context of sensor networks. They describe several approaches to Sybil attack prevention, designed to cope with
the limited resources of sensor nodes. For example, one
of their approaches relies on radio resource testing
(assuming no node can listen simultaneously on several
frequencies). Another is random key predistribution,
where neighboring nodes establish secure links, which
is more useful for maintaining a Sybil-attack-resistant
infrastructure, than for building one. They describe location verification as an open problem.
Sastry et al. [21] describe a protocol that uses node
location verification to establish node identities. However, their methods only use single beacons (verifiers in
their terminology). More precisely, the verifiers in their
approach can only test whether a given node is within
a given region that surrounds the verifier. They do not
describe ways to determine the exact location of a node.
Also, they do not consider adversarial conditions such
as faulty verifiers or collusion by applicants.

274

5.2.4 Node replication prevention
Parno et al. [20] study protocols for prevention of node
replication. In their model, an adversary can take control
of a node and any private keys it might have and then
attempt to clone it. Surprisingly, their model restricts
the clones to follow the original protocol and their work
does not tolerate corrupt nodes. More importantly, the
work assumes that “the adversary cannot readily create
new IDs for nodes”, so in effect it does not deal with
Sybil attack.
5.2.5 Beacon-based location verification
Independently of our work, Čapkun and Hubaux [7, 8]
consider the problem of establishing the location of a
sensor in a wireless network through the measurement
of distances from multiple verifiers. They show how in
two-dimensional (respectively, three-dimensional)
space having upper bounds from three (respectively,
four) verifiers suffices to establish the location of a node
or detect cheating. This is a special case of our Theorem 1. Most of their work focuses on the case where
beacons (verifiers) are correct and honest, and sensors
do not collude. They also give an example of a collusion attack where three colluding nodes, positioned
so that one is very close to each of three verifiers, can
appear to these verifiers as a single node whose position may be anywhere within the triangle spanned by
the three colluding nodes. This is the situation we discuss in Sect. 7.2.2. In Theorem 4 we show that fewer
than three colluding nodes cannot achieve the same by
giving a protocol to deal with the problem. Further, we
show in Sect. 8.2 that in the bounded-range broadcast
model where applicants cannot determine the range of
the broadcast, it is possible to tolerate any number of
corrupt beacons inside the triangle formed by three beacons in two-dimensional space.
At first, our work might seem to be closely related
to theirs. The common idea motivating both papers is
the observation that when message roundtrip time is
used to estimate the distance of a remote entity, that
entity can only cheat by pretending to be farther from
the verifier than its real distance, not closer. In fact, this
idea appears even earlier, in the report by Waters and
Felten [22]. What distinguishes our work is the ability
to deal with multiple colluding entities controlled by
an adversary. Čapkun and Hubaux do consider collusion by corrupt sensors, and the approaches they propose to solve the problem are (1) tamper-proofness
of authentication information within each device, and
(2) frequency fingerprinting—both reliable methods
for proving uniqueness. However, both of these

R. A. Bazzi, G. Konjevod

assumptions really claim that each device is uniquely
identifiable by some “black-box” method. In the presence of such an assumption, again, as in the case of
Parno et al. [20], Sybil attack prevention becomes trivial.
Their work considers only location verification and does
not consider the more general problem of group-distinctness testing that we introduce in this paper. Finally,
Čapkun and Hubaux do not deal with corrupt verifiers,
nor consider the general Sybil attack problem.
Thus, despite certain similarities, our results are much
more general, especially as we consider corrupt beacons
as well as colluding participants. Furthermore, instead
of focusing on specific technologies and making detailed
assumptions (that may or may not persist as new technological developments are made), we study the fundamental limitations of localization protocols in a more
abstract setting. The one basic assumption that we make
is that the participants lie in a metric or almost-metric
space in which communication delays depend on the
geometry of the space. We do focus on a few specific
classes of metric spaces (Euclidean, spherical geometry) because they seem most relevant to real-life applications.

6 Notation and terminology summary
For easy reference, in Table 1 we summarize most of the
notation used in the statements and proofs of Theorems

Table 1 Notation summary
Symbol

Meaning

B
A

Set of beacon entities (verifiers)
Set of applicant entities
The number of beacons (usually); n = |B|
Upper bound on the number of faulty beacons
Dimension of underlying geometric space
(most proofs)
Distance between two objects (early
discussions)
d-dimensional Euclidean space
d-sphere (boundary of unit ball in Rd+1 )
Distance between points x and y
Observed distance between entities A and B
The location of participant A in the space
Beacon (usually)
Applicant (usually)
Ball around x of radius r
The host metric space (in most proofs), usually
Euclidean or spherical
The convex hull of set S
The standard ith coordinate vector (in
Euclidean space)

n
f
d
d
Rd
Sd

ρ(x, y)
μ(A, B)
x(A)
Bi For various i
Ai For various i
B(x, r)
X
conv(S)
ei

Distinct identities

and Lemmas, as well as in some discussions throughout
the paper.

7 Geometric certification protocols: two-distinctness
tests
In this section we present our results under various system assumptions. For each set of assumptions, we state
our results in the form of a theorem that specifies conditions under which a participant (or group of participants) is incapable of pretending to be in a location
other than the real location of the participant or one
of the group members. We say that a participant (or a
group of participants) simulates a point, if it can make
all its communications appear to come from the point.
These results can be readily used to construct geometric certificates for the applicant. In each case, a certificate
would consist of the set of measurements that is sufficient to uniquely identify the location of an entity, and a
test of distinctness is simply a comparison between the
two locations defined by two certificates.
All our results are stated assuming the distance
between correct participants is accurately measured
using roundtrip delays (as explained in Sect. 2). These
results can be extended to the case in which measurements are not accurate. For that case, the statements
of the theorems will change to specify conditions under
which an applicant is incapable of pretending to be outside of a well-defined neighborhood of its actual location. In the presence of inaccuracies, a certificate consists
of the measurements that establish a neighborhood of
the applicant’s location, and a test of distinctness is simply the test of disjointness of two such neighborhoods.
While we do not describe such protocols here, our results
can be generalized to account for small inaccuracies (as
outlined in Sect. 9).
In our model, we assume only that the distances
between beacons can be calculated, while the locations
of beacons are unknown. Given a distance matrix Md
whose entries are the pairwise distances between points
in a geometric space, it is possible to find a set of points
expressed in an orthonormal coordinate system and
whose distance matrix is identical to M|rmd [4, 9]. If
all beacons are correct, these methods can be used to
transform a distance matrix representation into a coordinate system representation. In the presence of faulty
beacons, the computed distance matrix might not be
realizable in a geometric space and a coordinate representation consistent with all the beacons might not
be possible. Still, even in the presence of faulty beacons, the distance matrix is realizable if it is restricted
to the set of correct beacons. Our goal is then to find a

275

realization that is guaranteed to be consistent with the
set of correct beacons. Assuming that the set of correct
beacons is in general position (that is, no (d + 1)-subset is contained in a d-dimensional hyperplane, and no
(d + 2)-subset is contained in a d-sphere), this can be
easily achieved by considering either all sets of d + 1
or all sets of d + f + 1 beacons (depending on which of
the two families is smaller). In the first case, we use each
(d + 1)-set to build a coordinate representation and then
check if there are another f beacons consistent with this
representation. In the second case, we look for a consistent (d+f +1)-set of beacons. In case such a set is found,
it must contain at least d + 1 correct beacons, therefore
the coordinate representation defined by this set is consistent with all the correct beacons and every beacon
inconsistent with this representation can be discarded
as faulty.
It is important to note that, while the procedure
described above is expensive—being exponential in the
(usually small constant) d, and including a verification
of the positive-semidefiniteness of a matrix—it is only
performed once for an applicant to establish the certificate. The size of the certificate itself is small, and the test
of distinctness efficient.
7.1 Honest beacons with known locations
7.1.1 Trilateration in an honest world
If all participants in the protocol are honest, then the
problem is easy. To determine the exact location of a
point x(A) in d-dimensional space, it is enough to know
all the distances ρ(x(A), x(Bi )) between x(A) and d + 1
other affinely independent points x(B1 ), . . . , x(Bd+1 ).
With this information, the point x(A) can be reconstructed as follows: let Si be the sphere of diameter
ρ(x(A), x(Bi )) around x(Bi ). The point x(A) belongs to
Si for every i. A sphere with center c = (c1 , . . . , cd ) and
radius r is the
set of all points x = (x1 , . . . , xd ) that satisfy
the equation i (xi −ci )2 −r2 = 0. Equating the left-hand
sides of the equations for Si and Sj gives a linear equation in xi , thus the intersection of two spheres belongs
to a hyperplane. Since we assume general position, each
pair S1 , Si defines a hyperplane, which we denote by Hi .

Since S1 ∩ Si ⊆ Hi , it follows that x(A) ∈ i Hi , and we
can determine x(A) by solving a linear system.
7.1.2 Trilateration against cheaters
In the situation where the applicant may cheat by pretending to be at a different location, the protocol should
compute the applicant’s position or detect the cheating.

276

We first discuss the case where a single point attempts
to cheat without colluding with other entities.
Consider an applicant at A that attempts to impersonate a point x = x(A). The applicant contacts d + 1
beacons B1 , . . . , Bd+1 and exchanges a message with
each of them. Let μi = μ(Bi , A). If A can successfully impersonate x , then μ(Bi , A) = ρ(x(Bi ), x ) for
every i. Since μ(Bi , A) ≥ ρ(x(Bi ), x(A)), it follows that
ρ(x(Bi ), x(A)) ≤ ρ(x(Bi ), x ) for every i, that is, x ∈
D1 ∩ · · · ∩ Dk , where Di = B(x(Bi ), μ(Bi , A)), the ball of
radius μ(Bi , A) around Bi . For a set Z, we use int Z to
denote its interior, and convZ its convex hull.
Theorem 1 Let X = Rd . Let Bi be a beacon with x(Bi ) =
bi for each i = 1, . . . , d + 1. If {b1 , . . . , bd+1 } is affinely
independent and x ∈ int conv{b1 , . . . , bd+1 }, then x cannot be simulated by any other point.
Proof First, the set S of all points that can simulate x can
be written as S = {x | ∀i ρ(bi , x) ≤ ρ(bi , x )}. If S = ∅,
take x∗ ∈ S. Since S is an intersection of closed balls, it is
convex and so xλ = λx∗ + (1 − λ)x ∈ S for all 0 ≤ λ ≤ 1.
Take λ > 0 small enough that xλ ∈ conv{b1 , . . . , bd+1 }.
If we can show that for some i∗ , ρ(bi∗ , xλ ) > ρ(bi∗ , x ), it
will follow that xλ ∈ S, and the proof by contradiction
will be complete. So assume that ρ(bi , xλ ) ≤ ρ(bi , x ) for
all i. Let H be the hyperplane through 12 (x + xλ ) with
normal vector xλ − x . H is exactly the set of points at
equal distance to x and xλ . This implies that for every
i, bi is on the same side of H as xλ , in other words,
the hyperplane H separates x from bi for every i. This
contradicts the fact that x ∈ conv{b1 , . . . , bd+1 }.
If the underlying geometric space X is Rd , then the
theorem above gives necessary and sufficient conditions
for a set of beacons to be able to detect a cheating point.
If x is not in the convex hull of the beacon set, it may
be impossible to detect cheating.
However, on the sphere Sd , the situation is different,
and in fact much better. Note first that on a sphere, the
distance between a pair of points is measured along a
geodesic curve (great circle) that connects the pair. The
notion of convexity can then also be defined for the
sphere. Given two points x, y ∈ Sd , we would like to
define their convex hull as the set of all points on the
shorter segment of the geodesic between x and y. We
will refer to the shorter of the two segments of the geodesic between two points as the segment between these
points, and we will write [x, y].
Note that there is a unique x-y geodesic as long as x
and y are not antipodal points. (If x and y are antipodal,
then there are infinitely many geodesics between x and
y, and in fact every point on the sphere lies on one of
them.)

R. A. Bazzi, G. Konjevod

Now consider a set of points X ⊂ Sd . We say that X
is convex, if
1.
2.

X is contained in some half-sphere, and
For every a, b ∈ X, the segment [a, b] is a subset of
X.

It is easy to see that convex sets under this definition
satisfy some of the same properties as standard convex
sets in Euclidean space.
Lemma 1 Let {Xα | α ∈ I} be a family of convex subsets
of Sd . Then the intersection ∩α∈I Xα is also convex.
Let x, y ∈ Sd be such that  x0y ≤ π/2, where  x0y is
the angle formed by x, the origin 0 and y. We use this
angle as the measure of distance between x and y, that
is, ρ(x, y) :=  x0y.
Let B(x, r) = {y | ρ(x, y) ≤ r}. We call B(x, r) the ball
around x with radius r.
Lemma 2 If r is smaller than a quarter of the length of a
great circle, then B(x, r) is a convex set.
As a justification for our claim that the situation on
the sphere is even better than in the Euclidean space,
consider the following theorem.
Theorem 2 Let X = Sd−1 . Let b1 , . . . , b2d (the locations of beacons B1 , . . . , B2d ) be the points defined by the
coordinate unit vectors +e1 , −e1 , +e2 , −e2 , . . . , +ed , −ed
in both orientations (that is, if Sd−1 is considered as a subset of Rd , the beacons are at the vertices of the polar of the
inscribed d-dimensional cube). Then no point x ∈ Sd−1
can be simulated by any other point.
Proof First, we can assume without loss of generality
that x is in the interior of the positive orthant.
Indeed, suppose this is not true. Then we first project
the whole space to the subspace spanned by coordinate
axes in which x is zero. After this step, all of the components of the projected x are nonzero. Then we rename
the coordinate axes so that x belongs to (the interior of)
the positive orthant. Now x satisfies our assumptions.
We next prove that such an x cannot be simulated. In
order to then generalize this result to unrestricted x ,
one need only observe that the only effect of the projection is that we restrict ourselves to using even fewer
beacons.
We will use only the d beacons located at +e1 , . . . , +
ed , We claim that for any x∗ = x , there exists an i such
that ρ(bi , x∗ ) > ρ(bi , x ). For the most interesting case,
where the simulating point is also located in the positive
orthant, the proof is completely analogous to the proof
of Theorem 1 for Euclidean space.

Distinct identities

First, the set S of all points that can simulate x can
be written as S = {x | ∀i ρ(bi , x) ≤ ρ(bi , x )}. If S = ∅,
take x∗ ∈ S. Since S is an intersection of closed balls, it is
convex and so contains the whole segment [x , x]. Since
x is in the interior of conv{b1 , . . . , bd }, we can take a
point xλ close enough to x that xλ ∈ conv{b1 , . . . , bd+1 }.
If we can show that for some i∗ , ρ(bi∗ , xλ ) > ρ(bi∗ , x ), it
will follow that xλ ∈ S, and the proof by contradiction
will be complete. So assume that ρ(bi , xλ ) ≤ ρ(bi , x ) for
all i.
Let H be the hyperplane through the midpoint of the
spherical segment [x + xλ ] with normal vector xλ − x .
H ∩ Sd−1 is exactly the set of points at equal distance
to x and xλ . This implies that for every i, bi is on the
same side of H as xλ , in other words, the hyperplane H
separates x from bi for every i. This contradicts the fact
that x ∈ conv{b1 , . . . , bd }.
As we saw from the proof of Theorem 2, the boundedness of the sphere makes it possible to use a single
“universal” set of beacons of fixed size (depending linearly on the dimension of the space) to distinguish any
point from any other. We do not claim that we can always
place beacons exactly at the locations used in Theorem 2.
However, this theorem gives a sufficient condition to
ensure that every point in the space is contained in the
convex hull of some set of beacons, and therefore cannot
be simulated by any other point. (As long as the number of beacons is finite, this cannot be done in Euclidean
space because the applicant can always be far enough to
be outside of the convex hull of the beacons.) In practice,
we may use any appropriate beacon set, but distinguishability may be more difficult to guarantee if the beacons
used are not all within a single half-sphere because then
we must be very careful about convexity.

277

7.2.1 Broadcast messages
In the broadcast model the applicant cannot send a
message to a single recipient. Instead, every message
sent is broadcast and thus received by every other entity
(or at least every entity expecting a message). More precisely, every message sent by an applicant A at time t is
received by every beacon Bi at time t + ρ(x(A), x(Bi )).
Theorem 3 Let x be a point surrounded by an independent set of beacons {B1 , . . . , Bd+1 }, either in the sense of
Theorem 1 in Rd or in the sense of Theorem 2 in Sd . In
the broadcast model, x cannot be simulated by any set
{A1 , . . . , Ak } of entities unless x(Ai ) = x for some i.
In the proof of the theorem, we simply use A to denote
the entity of the applicant. The first reason for this is that
no beacon can tell which Ai sends the message just by
the content of the message since A1 , . . . , Ak are in collusion. The second reason is that the broadcast model
ensures that each message sent by any of the entities
A1 , . . . , Ak is received by all beacons.
Proof The protocol begins by beacon B1 sending a
probe at time 0. The applicant responds (broadcasting to all the beacons). Upon receiving the response,
each beacon immediately forwards it to B1 . Now B1
records μ(B1 , A) and the times at which it receives the
responses from the other beacons. Given the distances
ρ(x(B1 ), x(Bi )), the beacon can deduce the time it took
the message from A to arrive to each beacon by subtracting μ(B1 , A)/2 + ρ(x(B1 ), x(Bi )) from the time ti
at which the response from Bi arrived. Therefore, the
beacon B1 can calculate the distance μ(A, Bi ) for each
i. Now by Theorem 1 for Rd or by Theorem 2 for Sd , the
point x cannot be simulated by any other point.
7.2.2 Point-to-point messages

7.2 Multiple colluding entities
We have seen that it is impossible for a single applicant at
point x∗ to impersonate any other point x in the convex
hull of a sufficiently large set of active beacons. However, the proof was based on the fact that the distance
from x∗ to some beacon would have to be greater than
from x to the same beacon and so x∗ could not return
messages in time. If several entities located at different points collude to jointly impersonate another point,
our protocols from Sect. 7.1.2 do not work anymore.
In fact, in this setting there is a significant difference
between the broadcast and the point-to-point communication models.

The case in which an applicant can send a message to
any single beacon appears to be substantially more difficult in the case of colluding entities. For example, our
protocols from the previous section fail because the
simulating entities A1 , . . . , Ak can reply to the beacons
selectively, so that Ai sends a message to Bj only if Ai
is closer to Bj than x . Thus a point x can be simulated whenever for each beacon Bi there exists an Aj
such that ρ(x(Aj ), x(Bi )) ≤ ρ(x , x(Bi )). Intuitively, the
communication to each beacon may be controlled by
a single adversary. We present a protocol that forces
the adversary to do just this, namely, a protocol that
can only be cheated with the number of entities equal
or greater than the number of beacons, and located in
small bounded regions, one per beacon.

278

R. A. Bazzi, G. Konjevod

The protocol consists of two rounds. In the first round,
the beacons exchange the initial messages with the applicant and each other, accumulating enough information
to compute x from the values of μ(Bi , A), i = 1, . . . ,
d + 1, and to compute their own locations in some
fixed frame of reference. In the second round, the beacons synchronize clocks and broadcast special messages
M1 , . . . , Md+1 . For each i, the message Mi is sent by beacon Bi , so that at time t0 all d+1 messages simultaneously
reach x . Each of these messages should be impossible
to forge for an entity who has not received it. A simple
way to achieve this is for each beacon to send a random
message to the applicant. The applicant is then required
to immediately combine the d + 1 messages into a single
one, and forward this to each beacon. This message is
constructed so that it is difficult to forge it without having received all d + 1 of the beacons’ messages. Finally,
the beacons verify that from the forwarded message they
can indeed reconstruct the original messages that were
sent to the applicant.
The critical observation is that unless the applicant
receives all d + 1 messages, it cannot forge the combination message.
If the protocol is completed and the beacons decide
to accept the participant, then it must be true that every
beacon receives the combination message on time. Since
we assume that if a beacon receives a single message,
the message must have been sent by a unique entity
(multiple messages cannot combine themselves on the
fly into a single message), we may denote by Ai the participant in charge of forwarding the combination message to Bj . From the definition of the protocol, if Ai can
forward the required message to Bj on time, it must be
true that
ρ(x(Bk ), x ) + ρ(x , x(Bj ))
≤ ρ(x(Bk ), x(Ai )) + ρ(x(Ai ), x(Bj ))

(1)

for all k = j.
We claim that if the protocol is completed successfully from the beacons’ point of view, then either one of
the entities is at the claimed applicant location, or there
are at least d + 1 entities controlled by the adversary.
Before we discuss the protocol in general, we take
a look at the two-dimensional case and prove that no
two entities can simulate a third point in the convex
hull of three beacons. This implies that if the protocol is
completed successfully from the beacons’ point of view,
then either one of the entities is at the claimed applicant
location, or there are at least 3 entities controlled by the
adversary.

Theorem 4 Let X = R2 and let x be a point in the
convex hull of 3 affinely independent beacons B1 , B2 ,
B3 . Then no two entities can simulate x unless one of
them is located at x .
Note that, just as in the broadcast model, we do not
assume the entities controlled by the adversary are in
the convex hull of the beacons.
Proof Denote the two entities under adversary’s control
by A1 and A2 and assume that the protocol is completed
successfully. Since A1 and A2 successfully simulate x ,
it follows that for every j, the beacon Bj receives the
combination message by time t0 + ρ(x , x(Bj )). Let Ai be
the participant in charge of forwarding the combination
message to Bj . From the definition of the protocol, if Ai
can forward the required message to Bj on time, Eq. (1)
must be satisfied.
For convenience, for all j = k define Ejk to be the
ellipse with focal points Bj and Bk that passes through

x , and for all i, define Ei = j=i Eij .
The Eq. (1) is satisfied by Ai exactly if Ai can forward
the combination message to beacon Bj on time. On the
other hand, the set of points that satisfy this equation
is exactly the set Ej . In other words, Ai can forward the
message to Bj on time, if and only if Ai ∈ Ejk for each
k = j.
Let hjk be the line tangent to the ellipse Ejk at the
point x . Note that, since x is in the convex hull of
{B1 , B2 , B3 }, for any j, k, the beacons Bj and Bk are on
the same side of hjk , but Bi is on the other side. Denote
by Hjk the halfplane bounded by hjk that contains the
two points Bj and Bk . The discussion in this paragraph
can now be summarized by writing
Bi = Hjk
for all i = j, k.
In order to show that at the adversary needs at least
three participants to simulate a point distinct from each
of the participants, we will now argue that each beacon
requires a distinct participant. This will follow directly
if we can show that the set of points that can reach Bi
on time and the set of points that can reach Bj on time
have only x in common. Indeed, we will now show that
Ei ∩ Ej = {x } for all i = j.
First, instead of Ei we consider the set Hi = ∩j=i Hij
for each i. Since Eij ⊂ Hij , it is clear that Ei ⊂ Hi and it
will suffice to show that Hi ∩ Hj = {x } for all i = j. We
show this stronger statement by simple enumeration of
all possible cases. Let B1 , B2 , B3 , x be four points in the
plane with x ∈ conv{B1 , B2 , B3 }. Draw any line through
x that has B1 on one side and B2 and B3 on the other.
Then draw a line (again through x ) that has only B2
on one side and the other two beacons on the other.

Distinct identities

Finally, draw a third line that has B3 on one side and B1
and B2 on the other. The properties just stated related
to the separation of beacons by these three lines are certainly true of the lines h12 , h13 and h23 . Therefore, we
may safely infer properties of the sets H1 , H2 , H3 from
considering the halfspaces bounded by these three lines.
The three lines drawn as specified in the previous
paragraph all pass through x . Hence they divide the
plane into six wedges. Label these in clockwise order,
starting with the one that contains B1 , by W1 , W2 , . . . ,
W6 . First, B1 ∈ W1 and this is the only beacon in W1 ,
for otherwise two beacons would not be separated by
any of the three lines. More generally, any one wedge
Wj may contain at most one beacon. Consider now the
wedge W2 . If there exists an i such that Bi ∈ W2 , then B1
and Bi are together on the same side of two of the three
lines. However, this too is impossible by the separation
requirements. Therefore, W2 contains no beacons. Similarly, W6 contains no beacons and, more generally, for
any j, at most one of Wi , Wi+1 may contain a beacon.
From all this, we may conclude that exactly W1 , W3 and
W5 contain one beacon each. If the three lines were
really drawn in the same position as h12 , h13 and h23 ,
then three of the wedges would be exactly H1 , H2 and
H3 , namely we would have W1 = H1 for B1 ∈ W1 , and so
on. Now clearly, W1 ∩ W3 = W1 ∩ W5 = W3 ∩ W5 = {x },
and thus our proof is complete.
In the conference version of this paper, we claimed
the analogous result for arbitrary dimension. While this
still appears true, we have not been able to formalize a
proof for this general case.
Conjecture 1 Let X = Rd and let x be a point in the
convex hull of d + 1 affinely independent beacons B1 ,
B2 , . . . , Bd+1 . Then no d entities can simulate x unless
one of them is located at x .
The reason for the difficulty here seems to be that the
statement relies on more than just affine (projective)
properties of d-dimensional space, and so polyhedral
arguments (such as the one in our proof of the twodimensional case) will not suffice. On the other hand,
some special cases of the conjecture are relatively easy
to prove—such as when the beacons are equidistant.

279

Our goal is to show that Ei ∩ Ej = {x } for i = j.
Since Ei must contain an Aj for every i, this will imply
that, unless one of the Ai is at x , at least one will be
needed for every Ej , and thus at least d + 1 entities will
be necessary to complete the protocol.
By symmetry, it is enough to show that Ed+1 ∩ Ei =
{x } for i ≤ d. Since d > 3, we can find j = i and k = d+1
such that j = k. For such a choice of j and k, consider
Ei,j and Ek,d+1 . Since Ei ⊆ Ei,j and Ed+1 ⊆ Ek,d+1 , we
have Ei ∩ Ed+1 ⊆ Ei,j ∩ Ek,d+1 . Thus, if we can show that
Ei,j ∩ Ek,d+1 = {x }, the theorem will follow.
Project the simplex to a two-dimensional plane
spanned by the segment [Bk , Bd+1 ] and the point x . In
this projection, Ek,d+1 is mapped to an ellipse, and Ei,j
to a circle, as in Fig. 1. In the projection, the intersection
of Ei,j and Ek,d+1 is exactly {x }. If Ei,j ∩Ek,d+1 contained
an open ball, then this ball would project to either an
open disk or an open segment in any two-dimensional
projection. Since this is not the case, the intersection
Ei,j ∩Ek,d+1 does not contain an open ball. Suppose there
was another point x = x contained in Ei,j ∩Ek,d+1 . Then
also the segment [x , x ] must be contained in the intersection. However, since every point on the boundary of
an ellipse is an extremal point (cannot be written as a
convex combination of other points within the ellipse),
any point in the interior of the segment [x , x ] is also in
the interior of both Ei,j and Ek,d+1 . This contradicts our
previous conclusion that Ei,j ∩ Ek,d+1 doesn’t contain an
open ball.
Hence, x is the only point in Ei,j ∩ Ek,d+1 , thus also
the only point in Ei ∩ Ed+1 , which proves the theorem.
7.2.3 Point-to-point messages, the case of many beacons
In the previous section we showed that the adversary
needs at least three participants to simulate any point
in the convex hull of three beacons. Intuitively, what
happens is that every beacon must be cheated by a
distinct point in order to avoid detection. This intuition is justified by our next result, that shows that if a

Theorem 5 Let X = Rd and let x be at the center of the
regular simplex spanned by d + 1 beacons B1 ,
B2 , . . . , Bd+1 . Then no d entities can simulate x unless
one of them is located at x .
Proof Let A1 , . . . , Ad be d entities that can successfully
simulate x . As in the proof of Theorem 4, for all j = k
define Ejk to be the ellipsoid with focal points Bj and Bk
that passes through x , and for all i, define Ei = ∩j=i Eij .

Fig. 1 The projection of two Ei,j and Ek,d+1 is exactly {x }

280

larger number n of beacons is available, it is possible to
formulate a protocol that cannot be cheated by fewer
than n participants.
Theorem 6 Let X = R2 , and let B = {B1 , . . . , Bn } be a set
of n beacons in convex position (that is, no beacon is in the
convex hull of the others). Let x be a point in the convex
hull of these n beacons. If for any pair of beacons Bi , Bj
there exists a third beacon Bk with x ∈ conv{Bi , Bj , Bk },
then no n − 1 entities can simulate x unless one of them
is located at x .
Notice that this condition is generally not satisfied
when n is even. Also, in most cases the region in which
the applicant point can be decreases as the number of
beacons grows. For some examples, consider Fig. 2.
Proof First, let us describe the protocol. For this theorem, we use a straightforward generalization of the
three-point protocol from Theorem 4. In the first phase
of the protocol, the beacons determine the unique
claimed location x of the applicant. In the second phase,
each beacon Bi sends a message mi at a time determined
so that all the messages arrive at x simultaneously. The
applicant is then required to combine all the messages
received and immediately forward the resulting combination message to all the beacons.
To analyze the protocol, we imagine the beacons as
grouped into triples. Each triple contains the applicant
point in its convex hull, and each triple is considered as
if it were running the three-point version of the protocol. For any of these triples, the combination message
contains all the information in the three messages of the
triple, so any conclusions derived about the triple from
the proof of Theorem 4 hold for the n-beacon protocol as well. In particular, if the beacon Bi receives the
correct combination message on time, then the participant who sent it to Bi is contained within Ei , where Ei
is the set of all points x such that d(x, Bi ) + d(x, Bj ) ≤
d(x , Bi ) + d(x , Bj ), for all Bj that are contained in any
triple together with Bi . For any i and j, there exists a triple of beacons Bi , Bj , Bk such that x ∈ conv{Bi , Bj , Bk }.
Therefore, Ei ∩ Ej = {x }. If the protocol is completed,
then each beacon receives the combination message
on time, and so either a participant in the protocol is

Fig. 2 The points within the
inner pentagon satisfy the
condition of Theorem 6

R. A. Bazzi, G. Konjevod

located at x or each beacon receives the message from
a different location. In the former case, no cheating is
going on and the protocol should succeed; in the latter,
at least n participants are required.
7.3 Trilateration with corrupt beacons
In the presence of faults, we cannot rely on the operation of any single beacon to work as specified by the
protocols. For example, a corrupt beacon may report an
applicant as being further away than the actual distance,
violating one of our basic assumptions and creating a
situation where μ(x(B), x ) < ρ(x(B), x ). Note that an
honest applicant may recognize this situation and we
assume that a correct applicant will probe a beacon to
establish its distance to the beacon and compare it to
the distance reported by the beacon to the applicant.
If the applicant is correct, then it will accept the value
reported by the beacon if it matches its own value. This
will have the effect of preventing a beacon from making
a correct applicant look like it is closer to the beacon
than it really is.
We now show how to tolerate beacon failures.
Theorem 7 Let x be the location claimed by the applicant A. Consider a set S of n ≥ d + 1 + 2f beacons
arranged so that either: (first case) for every (n − f )-element subset S ⊂ S, x ∈ convS , or (second case), for
every (d + 1)-element subset S ⊂ S, x ∈ convS . If at
most f of the beacons in S are faulty, then no applicant
A can simulate x unless x(A) = x . Acertificate for the
applicant can be constructed in time nf (in the first case)
 n 
or d+1
(in the second case).
The convex hull condition may appear quite restrictive, but if n is large compared to f , then the intersection
of convex hulls of all the (n − f )-subsets will still be
considerably large. A condition such as this is needed
because of examples such as the one in Fig. 3. Here,
the top beacon (B5 ) is faulty and it may collude with
the applicant. Since the applicant is not in the convex
hull of the set of correct beacons, they may be fooled by
the collusion of the applicant and the faulty beacon into
believing the applicant is farther away from the square
than it really is.
Note that a set of d + 1 + 2f beacons contains at most
f faulty, and therefore at least d + 1 + f correct beacons.
The f faulty beacons may, together with at most d correct
ones, determine an incorrect value for x(A). However,
there are more correct beacons and so the maximum
subset of beacons agreeing on a location for x is correct. Unfortunately, finding a maximum consistent subset of a set of n linear equalities is not only NP-hard, but

Distinct identities
Fig. 3 Beacon B5 is faulty
and, together with A, may
create an entity at x

281

entities. If all are correct, they will also issue a certificate
to an honest applicant. Thus, we need only argue that
an (n − f )-subset containing some faulty beacons cannot
agree on an incorrect location for the applicant. But this
is true, since in the first phase we determined the unique
location claimed by the applicant.

8 Certification protocols: groups-distinctness tests
8.1 Group distinctness: general results

also hard to approximate within an n factor for some
 > 0 [2].
Proof We check either all sets of d + 1 or all sets of n − f
beacons (depending on which of the two conditions in
the theorem is satisfied, or if both are, which of the two
families is smaller).
In the first case, for every (d + 1)-set of beacons, we
find a candidate point for x (by solving a linear equality system as in Sect. 7.1.1). Then for each beacon B,
we check if μ(B, A) = ρ(x(B), x ). If there are at least
n − f such beacons (and x is in their convex hull), then
x(A) = x and the certificate is issued.
In the second case, if each beacon in a set of n − f
beacons is consistent with x (and x is in their convex
hull), then this set of beacons defines a unique point,
which must be the location of the applicant, because a
(d + 1)-subset of this set consists of correct beacons, and
so also defines the actual location of A.
Theorem 8 Let d = 2. Let x be the location claimed by
the applicant A. Consider a set of n ≥ d + 1 + 2f beacons
surrounding x , such that for every (n − f )-element subset
S ⊂ S, x ∈ convS .
If at most f of the beacons in S are faulty, then no set A
of at most d entities can simulate x in the point-to-point
model unless x(Ai ) = x for some applicant Ai .
Proof We first determine a unique claimed location for
the applicant, by running the protocol from (case 1 of)
the proof of Theorem 7.
In the second phase, we run the relay protocol from
Theorem 4. After the messages are all received, we
examine the (n − f )-subsets of beacons and their conclusions about the location of the applicant. Consider a set
of n − f beacons. If all are correct, then they will detect
an adversary trying to simulate x with fewer than d + 1

In this section, we consider a general group-distinctness
test using a protocol in which applicants are required
to relay messages between pairs of beacons. We show
how to find a lower bound on the number of applicants
needed to simulate a number of observed relay times by
reducing the problem to a set cover problem.
In the relay approach, beacons do not attempt to
determine the locations of the applicants directly, rather
they determine the time it takes for a message sent by
one beacon to be received by the applicant and then
relayed to another beacon. For a pair of beacons, the
time to relay a message defines an ellipse whose foci
are the two beacons and whose diameter is equal to the
relay time. An applicant that is on or inside the ellipse
can simulate the relay time observed by the two foci by
introducing appropriate delay. Applicants outside the
ellipse cannot simulate the observed relay time.
For the distinctness test, we assume we have as input
a set of relay times between applicants and pairs of beacons, and the goal is to determine the minimum number
of applicants that could have produced this set of relay
times.
Given two ellipses that have a nonempty intersection,
there could be an applicant in their intersection that can
simulate the observed relay times between their foci. We
will take a conservative approach and assume that for a
set of ellipses that have a common non-empty intersection there is a point in the intersection that can simulate
the relay times for all the ellipses in the set. We call this
assumption the conservative assumption. By abuse of
terminology, we identify such a point with the intersection itself, and say that an intersection of a set of ellipses
can simulate the relay times for the ellipses in the set.
Given a set of ellipses, we say that an intersection
of a subset (of ellipses) is minimal if the intersection
is not empty and does not have a proper subset that
is a nonempty intersection of ellipses (equivalently, no
proper superset of that set of ellipses has a nonempty
intersection). For example, if an ellipse does not intersect any other, then the whole ellipse is itself a minimal

282

intersection. We will show that our problem reduces
then to determining a set of minimal intersections that
are enough to simulate the observed relay times. In fact,
assume that a number of applicants can simulate the
observed relay times and that some applicants are not
in minimal intersections. For every applicant that is not
in a minimal intersection, the applicant must be in some
ellipse and possibly in the intersection of a number of
ellipses. Let S be the smallest intersection of ellipses
to which the applicant belongs. Since S is not minimal,
we can replace the applicant with another applicant in
the minimal intersection contained in S (such a minimal
intersection exists because the number of ellipses and
therefore of their intersections is finite). The replacement does not affect the ability of the set of applicants
to simulate or achieve the observed relay times because,
by our conservative assumption, some applicant in the
minimal intersection in S can simulate any relay time for
all ellipses to which S belongs, which is at least as much
as can be simulated by an applicant in S that is not in the
minimal intersection.
So, our problem reduces to finding a set of minimal
intersections that can simulate all the observed relay
times.
This problem is an instance of set cover [12]. To see
this, define a set X = {x1 , . . . , xn }, each of whose elements corresponds to a unique ellipse, and for each minimal intersection of ellipses from X, define a set {xi1 , . . .,
xit } containing exactly the elements corresponding to the
ellipses that form the intersection. Denote these sets by
S1 , . . . , Sm . Now our problem is exactly that of finding a
minimum-size family of sets {Si1 , . . . , Sic } such that the
union of all the sets in this family contains every element
of X.
In general, set cover is NP-hard, and even hard to
approximate to a factor better than log n [11]. However, our problem is restricted by the fact that ellipses
are not arbitrary subsets of the plane, and also by the
fact that we only consider minimal intersections. In fact,
if we had to consider arbitrary intersections of ellipses, we might have to create exponentially many sets.
Since ellipses are convex and we only consider minimal
intersections, the number of sets is bounded by O(n2 ),
where n is the number of ellipses in the instance. Furthermore, since all the sets have a special geometric structure (they are “pseudo-disks” in terminology of [17]),
the results of Matoušek et al. [17] imply that the set system {S1 , . . . , Sm } allows -nets of size only O(1/). For
such set systems, Brönnimann and Goodrich [6] give a
constant-ratio approximation algorithm for the set cover
problem (for a discussion of these results, see also the
survey by Bern and Eppstein [3] and the Thesis of Brönnimann [5]).

R. A. Bazzi, G. Konjevod

8.2 Group-distinctness with point-to-point messages
and bounded-range broadcast
In this section we consider a system in which applicants
can communicate with point-to-point messages and beacons can communicate with bounded-range broadcast
and applicants cannot determine the range of a broadcast message they receive. We show that in R2 , and in the
presence of three correct beacons, k faulty entities cannot simulate more than k2 distinct points. This result can
be used in the way suggested in the introductory example (Sect. 1.1) to mitigate the damage of Sybil attacks.
If there are no more than k faulty entities in the system, then from a set of applicants that appear to be at m
different locations, at least m−k2 applicants must reside
on correct entities.
To achieve our result, we modify the second phase
of the protocol for the point-to-point case. The second phase is replaced with m phases, where m is a
security parameter (the protocol will fail with probability inversely proportional to an exponential in m). We
describe one of the m phases. First, each beacon sends
a message that reaches x with probability 1/2 and does
not reach x , but reaches all points that are closer to the
beacon than x , with probability 1/2. As in the protocol
for the point-to-point case, all messages that reach x
arrive at the same time, say t0 .
Consider the message sent by the beacon Bi . If there
is no applicant whose distance from Bi is equal to the
distance between x and Bi , it follows that no applicant knows whether the message of Bi reaches xi . It
also follows that the colluding applicants, as a group,
do not know which messages x must combine and forward to all beacons. So, regardless of the times at which
the applicants receive the messages sent by the beacons,
any combination of messages they forward might be the
wrong combination with probability 1/23 . By repeating
the phase m times, the probability of forwarding the
correct messages m times is 1/8m . This is only true if
there are no applicants on the three circles centered at
the beacons and passing through x . If there are applicants on these three circles, then those applicants would
know if a particular message reaches x because they are
at the same distance (separately) from the beacons. This
situation is illustrated in Fig. 4.
So, in order for a location to be simulated by faulty
entities, there must be three entities, each of which
resides on a circle going through the simulated point
and centered at the beacons. The question we are interested in becomes the following: given a set of entities,
how many points can be simulated by them? We give a
loose upper bound on this number. Let k be the total
number of corrupt entities in the system. A point x can

Distinct identities

283

Fig. 4 Three applicants can
potentially simulate x

be simulated by these k entities if there are three entities
A1 , A2 , and A3 such that ρ(x(Ai ), x(Bi )) = ρ(x , x(Bi )).
In other words, x is at the intersection of three circles
centered around the beacons and each containing one
of the k entities. The set of points that can be simulated
by the k entities are points that are at the intersection of
such three circles. This set is a subset of the set of points
that belong to the intersection of the sets of circles centered at B1 and each containing an entity in the system
and set the of the circles centered around B2 and each
containing an entity in the system. Those circles have at
most 2k2 points in common (any two circles with different centers intersect at most in two points) and at most
k2 of these points are inside the triangle formed by the
beacons.
In the foregoing discussion we assume that the
bounded range broadcast can be made to reach only
points whose distance to a beacon is strictly less than a
given value. A more practical assumption would require
that a message reaches no point that is more than  + R
from a beacon for some  and some R. This will modify the result to the following: k entities cannot simulate
points in more than k2 disjoint small neighborhoods centered around the k2 points defined by the intersection
of circles, and where the area of a neighborhood is in
O( 2 ).
8.3 Group-distinctness with a grid of beacons
In this section, we show how the result of Sect. 8.1 can
be applied in a specific setting. We show how applicants
can be severely limited in the number of identities they
can simulate. We consider a system in which beacons are
evenly spaced on the perimeter of a square as shown in
Fig. 5. There are 4k beacons, k beacons on each edge.
Without loss of generality, let k be the length of an
edge of the square (in other words, beacons that are
adjacent are at a distance 1 apart). Let BLi , BRi , BUi
and BDi be the ith beacons on the left edge, right edge,
upper edge, and lower (down) edge respectively. We
use a coordinate system with center at the top left corner of the circle with the coordinates of the bottom right

Fig. 5 Beacons around the perimeter of a square and k applicants
that can appear to be anywhere

corner being (k, k) and the horizontal axis being the x
axis. For two beacons BLi and BRi , we can define a horizontal ellipse E√
Hi with foci BLi and BRi and with diameter just under k2 + 1. Similarly we define a vertical
ellipse √
EVi with foci BUi and BDi and with diameter just
under k2 + 1. Every point inside the square belongs
to at least one, but no more than two horizontal ellipses
and at least one, but no more than two vertical ellipses.
In fact, a horizontal band of width one around the line
segment joining BLi and BRi is completely contained in
EHi . A similar statement is true for vertical ellipses.
If we only consider vertical and horizontal ellipses,
then k applicants are sufficient to cover all the ellipses.
For example, place an applicant at (i, i) for each i. This is
illustrated in Fig. 5. The situation can be improved drastically, though, if we modify the protocol. In the modified
protocol, after the apparent location of an applicant is
determined, say (x, y), the applicant receives two nonces
(random messages) from BUx/l and BLy/l . These nonces arrive at the apparent location at the same time as
in the protocol of Sect. 8.1 and the applicant is required
to combine then and forward the combined message to
BDx/l and BRx/l (Fig. 6). In order for an applicant to
successfully forward the messages on time, it has to be
in the intersection of EHy/l and EVx/l . It follows that to
cover all the ellipses, we need (k/2)2 applicants. Indeed,
only adjacent ellipses have non empty intersections and
we need one applicant to cover the intersection of two
adjacent horizontal ellipses and two vertical adjacent
ellipses. Since there are k/2 disjoint pairs of adjacent
horizontal ellipses and an equal number of disjoint pairs
of vertical adjacent ellipses, we need at least (k/2)2 applicants to cover all of them.
With this result, we can define the following group
distinctness test when the maximum number of faulty

284

R. A. Bazzi, G. Konjevod

9.1 Inaccurate distance measurements

Fig. 6 Two-dimensional protocol: A must forward the combination of messages from BU and BL to both BD and BR

applicants is f and the total number of applicants is
m > f . For every applicant, determine an intersection
of a vertical ellipse and a horizontal ellipse in which
the applicant is located. Each intersection would have
a group of applicants, possibly empty. Let m(f ) be the
number of applicants in the f largest groups (assuming there are at least f groups). Then, there must be at
least m − (m(f ) − f ) distinct applicants amongst the m
applicants.
This result is interesting because it confines corrupt
applicants to simulating points in a small region. It is a
significant improvement over the introductory example,
where we needed one beacon per corrupt applicant. In
this example, the number of corrupt applicants needed
to cover the whole space of interest is quadratic in the
number of beacons.

9 Inaccuracies
In this section we very briefly describe a generalization of the problem that allows inaccuracies in measured
distances. We only consider the simplest case, where
measured distance may vary by a small constant fraction (known in advance) from the actual distance. This
assumption may not be as restrictive as it appears, especially in view of our suggestion in Sect. 2.4 that each
distance be measured more than once over a period
of time to ensure accuracy (for example, if the measurements deviate from the true value according to a
reasonable probability distribution, the smallest of the
multiple measurements will generally tend to the true
value fairly quickly).

To account for the inaccuracies in measuring the
distance, we consider the variant of the problem where
the measured distance μ(Bi , A) is allowed to exceed
the actual reported distance by a small multiplicative
factor: as long as there exists a point x that satisfies
ρ(x(Bi ), x ) ≤ μ(Bi , A) ≤ (1 + )ρ(x(Bi ), x ) for each i,
the protocol should not reject the applicant.
The problem of validating an applicant becomes
equivalent to identifying an intersection of thin (because
we assume  to be small relative to the measured distances) spherical shells (one around each of the
beacons).
The goal of all our protocols is to distinguish between
different applicants. Therefore a natural measure of how
badly a protocol fails might be the smallest distance
between points that cannot be reliably distinguished
using the protocol. Now imagine a region E such that
no two points in E can be reliably distinguished. Since
E is an intersection of shells around beacons and the
thickness of each shell is small compared to its radius,
we may think of E as bounded by almost straight planar
surfaces. Consider as an example the two dimensional
case, and focus first on just two of the beacons, B1 and
B2 . The straight line segments from their locations to the
applicant’s location meet at an angle, say θ . If θ is close
to the right angle, then the indistinguishability region is
close to a square (actually, two disjoint squares, because
two circles around B1 and B2 intersect in two points).
If θ is very small, the indistinguishability region looks
more like a thin parallelogram, and in such a case it can
happen that two points a large distance apart cannot be
distinguished. When the third beacon is included, it may
still be the case that the indistinguishability region has a
large diameter.
For this case, if the shell boundaries are replaced by
straight lines, we see that the distance between the two
points farthest apart in the indistinguishability region
is at most D = 2d cos(θ/2)
sin θ , where d is the distance
between B1 and x . In other words, the diameter of the
indistinguishability region may increase proportionally
to 1/ sin θ .
9.2 Inaccurate clocks in beacons
Some of our protocols depend on quite accurate clocks.
For example, in the proof of Theorem 4 we describe a
protocol that requires beacons to send messages independently, but at precisely timed moments, in order
to prevent several colluding points from simulating a
nonexistent applicant. Clock inaccuracy in such a protocol translates directly into increased inaccuracy of

Distinct identities
Fig. 7 The small angle case

distance measurement (because the beacon that sends
the message is not the same as the beacon that measures
the arrival time). A very similar problem is solved in
the existing Global Positioning System (GPS) [1] using
redundant information. A GPS receiver reads the timestamps in signals sent by several satellites to measure its
distance from each, and given a table of ephemerides
deduces its own geographical location. The satellite signals travel at the speed of light, and the Earth is small
enough that even slightly inaccurate clocks may lead to
inaccurate measurements. (Highly accurate clocks are
built into the satellites, but are too expensive for massproduced GPS receivers.) Therefore, instead of four signals (which would be enough to identify the receiver’s
location), five or more are used to allow for correction
of clock drift.

10 Discussion of model
10.1 Adversary model
The adversary model is particularly important for the
accuracy of measured distances. In fact, a corrupt entity
that is used to route messages between non-corrupt beacons can artificially increase the distance between them
as well as between corrupt entities and beacons. Later,
the calculated distance could be shortened which violates a main assumption of our model. We do not have a
fully satisfactory solution to this problem, but we have
two approaches to deal with it. The first approach applies
to peer-to-peer systems that exhibit locality characteristics. In such systems, the distance between nodes is

285

proportional to the actual network distance between the
nodes. If the overlay network exhibits locality characteristics, we can calculate the network-distances between
beacons directly without going through the overlay network and therefore without risking that the routing is
compromised (assuming the routing on the underlying network cannot be easily compromised). These distances will be smaller than the distances on the overlay
network, but one could then use solutions that tolerate inaccuracies in the measured distances. The second
approach makes limiting assumptions on the disruption
power of the adversary. If we assume that any two nodes
are connected by a path that does not go through a corrupt node, then we can use multiple paths to calculate
the distance between two nodes. The shortest among
the calculated distances would be chosen as the distance
between two nodes.
Another potential difficulty can be cause by corrupt
nodes trying to flood the network with message in order
to prevent accurate measurements of distances. Dealing
with such denial of service attacks is beyond the scope
of this paper.
10.2 Certificates
It is important to realize that the set of beacons in our
model is not the same as a central certifying authority.
In fact, all we need to assume about the beacons is that
they are distinct and that a certain number of them are
correct. In principle, a given entity can establish the distinctness of an initial set of beacons by using some of
the resource-consuming challenge-response described
in [10] without requiring any certifying authority. The
assumption that a certain proportion of beacons chosen
at random is correct is a system assumption for we cannot expect a system with an arbitrary number of faulty
entities to be able to function.
Once an initial set of beacons is established, our
results show that they can be used remotely to establish the distinctness of identities created by entities with
unbounded computing power. This shows that the following lemma from [10] (his notation is different from
ours, but should be clear from the context) does not
hold once the geometric properties of communication
are considered.
Lemma 3 Lemma 4 [10] If the correct entities in set C
do not coordinate time intervals during which they accept
identities, and if local entity l accepts any identity vouched
for by q accepted identities, then even a minimally capable
faulty entity f can present g = |C|/q distinct identities
to l.

286

This lemma basically says that accepted identities
cannot be used to accept further entities. We showed
that, if we take the geometric properties of communication into account, we can use accepted identities to
accept additional entities. In fact, in one of our results
we showed that a set of d + 1 + 2f , at most f of which
are faulty, can prevent one faulty entity efaulty in their
convex hull from presenting distinct identities even if
efaulty has unbounded resources. This result is achieved
without assuming a central authority.
In practice, the beacons can be certified by a central
certifying authority to bootstrap the system. Once a set
of beacons is certified, it can be used to provide certificates remotely. In that case, an applicant that wants to
obtain a certificate from the set of beacons would identify beacons that have valid public certificates obtained
from the central authority. Then, the applicant can initiate a geometric certificate request which will result
in the beacons probing the applicant as explained in
the various protocols we presented. These probes will
be started by multiple beacons to obtain the distances
as required by the protocols. At the end of the probing period, the beacons will present the applicant with
pieces of the geometric certificate (distances from beacon to applicant or location of applicant as calculated by
a beacon) that the applicant can put together to obtain
the geometric certificate.
10.3 Limitation of distinctness tests
Some of the distinctness tests we presented assume that
the entity under consideration is in the convex hull of
the beacons in the system. If an entity is outside the convex hull of the beacons, then some of the theorems we
prove do not hold. It is reasonable to question whether
the convex hull condition is only of theoretical interest
and if anything can be done if an entity is not in the
convex hull of the beacons. The answer to the first part
of the question would depend on the actual network
under consideration. We have answered the second part
when we presented group-distinctness tests for various
scenarios and under different system assumptions. We
believe that more work is needed in this direction to
further generalize the results.
10.4 Accuracy of measured distances
Ng and Zhang [19] show that on the Internet the
roundtrip delays can be used to measure distances
between entities if enough measurements are taken and
the minimum amongst the measured delay is used as the
distance measure. These measurements were done using
ICMP ping messages. In our model, communication is

R. A. Bazzi, G. Konjevod

done in an overlay network that does not necessarily
exhibit the same delay characteristics as those of the
Internet. Nonetheless, we can expect that in periods of
low congestion, the distances will reflect the underlying
network distances. In our work, establishing a geometric
certificate can be done over a period of time and multiple
measurements can be taken and the smallest times be
included in the certificates. If the participants belong to
common congestion zones (which can be correlated with
time zones), then we can expect that the minimal delay
measured by participants will exhibit metric characteristics. Nevertheless, studying delay characteristics in Internet-based overlay networks is a subject that needs
further study and our work is based on the assumption
that these characteristics are similar to those of the Internet.

11 Conclusion
We have shown that it is possible to exploit the geometric properties of message transmission delay in order to
reduce the effects of Sybil attacks. We believe that a lot
more work is still needed to make this work of more
practical value. In particular, we believe that extensions
of the protocols in Sect. 8 can have a good chance of
leading to solutions that can be used in practice.
Acknowledgements We would like to thank Roger Wattenhofer
for pointing out the work of Čapkun and Hubaux [7].

References
1. Agarwal, N., Basch, J., Beckmann, P., Bharti, P., Bloebaum,
S., Casadei, S., Chou, A., Enge, P., Fong, W., Hathi, N., Mann,
W., Sahai, A., Stone, J., Tsitsiklis, J., Roy, B.V.: Algorithms for
GPS operation indoors and downtown. GPS Solut. 6, 149–160
(2002)
2. Amaldi, E., Kann, V.: The complexity and approximability of
finding maximum feasible subsets of linear relations. Theoret.
Comput. Sci. 147, 181–210 (1995)
3. Bern, M., Eppstein, D.: Approximation algorithms for geometric problems. In: Approximation Algorithms for NP-hard
Problems, pp. 396–345. PWS (1997)
4. Blumenthal, L.: Theory and Applications of Distance Geometry. Clarendon Press, Oxford (1953)
5. Brönnimann, H.: Derandomization of geometric algorithms.
Ph.D. thesis, Princeton University (1995)
6. Brönnimann, H., Goodrich, M.: Almost optimal set covers
in finite VC dimension. In: Proceedings of the 10th Annual
ACM Symposium on Computational Geometry, pp. 292–302
(1994)
7. Čapkun, S., Hubaux, J.P.: Secure positioning of wireless
devices with applications to sensor networks. In: Proceedings
of INFOCOM (2005)

Distinct identities
8. Čapkun, S., Hubaux, J.P.: Secure positioning in wireless networks. IEEE J. Sel. Areas Commun. 24(2), 221–232 (2006)
9. Deza, M., Laurent, M.: Geometry of Cuts and Metrics.
Springer, Berlin Heidelberg New York (1997)
10. Douceur, J.: The Sybil attack. In: Proceedings of IPTPS, pp.
251–260 (2002)
11. Feige, U.: A threshold of ln n for approximating set cover. J.
ACM 45, 634–652 (1998)
12. Hochbaum, D.S.: Approximating covering and packing problems: set cover, vertex cover, independent set, and related
problems. In: Approximation Algorithms for NP-hard Problems, pp. 94–143. PWS (1997)
13. Hu, Y.C., Perrig, A., Johnson, D.B.: Packet leashes: a defense
against wormhole attacks in wireless networks. In: Proceedings of INFOCOM (2003)
14. Kleinberg, J., Slivkins, A., Wexler, T.: Triangulation and
embedding using small sets of beacons. In: Proceedings of
the IEEE FOCS, pp. 444–453 (2004)
15. Lazos, L., Poovendran, R.: SeRLoc: secure range-independent localization for wireless networks. In: Proceedings of
WISE 2004 (2004)

287
16. Luo, J., Shukla, H.V., Hubaux, J.P.: Non-interactive location
surveying for sensor networks with mobility-differentiated
ToA. In: Proceedings of INFOCOM (2006)
17. Matoušek, J., Seidel, R., Welzl, E.: How to net a lot with little:
small -nets for disks and halfspaces. In: Proceedings of the
6th Annual ACM Symposium on Computational Geometry,
pp. 16–22 (1990)
18. Newsome, J., Shi, E., Song, D., Perrig, A.: The Sybil attack
in sensor networks: analysis and defenses. In: Proceedings of
IPSN (2004)
19. Ng, T., Zhang, H.: Predicting Internet network distance with
coordinates-based approaches. In: Proceedings of INFOCOM
(2002)
20. Parno, B., Perrig, A., Gligor, V.: Distributed detection of node
replication attacks in sensor networks. In: Proceedings of the
IEEE Symposium on Security and Privacy, pp. 49–63 (2005)
21. Sastry, N., Shankar, U., Wagner, D.: Secure verification of
location claims. In: Proceedings of ACM WiSe (2003)
22. Waters, B.R., Felten, E.W.: Secure, private proofs of location.
Technical Report TR-667-03, Princeton (2003)

On the Use of Registers
Achieving
Rida

A.

Gil
Georgia

Wait-Free

Consensus
Gary

Bazzi”t

L. Peterson~

Spelman

Neiger*

Institute

in

College

of Technology

Abstract

1

Introduction

The computational
power of concurrent
data types has
been the focus of much recent research. Herlihy showed
that such power may be measured by examining
the
type’s ability to implement
wait-free consensus. Jayanti

is of fundamental
importance
in distributed
computing. A large body of research has studied algorithms
for

argued

passing systems, and (3) asynchronous

Achieving

that

this

“ability”

could

be measured

in differ-

ent ways, depending,
for example, on whether or not
read/write
registers could be used in an implementation. He demonstrated
the significance
of this distinc-

consensus in the presence of processor failures

achieving consensus in three domains:
(1) synchronous
message-passing
systems,
(2) asynchronous
messageory systems.
large number

While the first
of deterministic

read/write

mem-

domain has produced
a
algorithms,
it has been

shown that such algorithms
do not exist in the other
two [4–7,14]. Because of these results, researchers also

tion by exhibiting
a nondeterministic
type whose ability
to implement
consensus was increased with the availability of registers.
We show that registers cannot increase the computational
power (to implement
consensus) of any deterministic
type or of any type that can

consider algorithms
for consensus in asynchronous
systems with primitives
more powerful than simple reads
and writes [1,2,7,9–12,14,17,19].

implement

signifi-

the

hierar-

data

cantly

2-process

impact

consensus.

upon

chies of concurrent

the study

These

results

of the wait-free

data types.

In particular,

the com-

Another

reason for taldng

study

of wait-free

types.

the following:

bination
of these results with other recent works shows
that Jayanti’s
hm hierarchy
is robust for deterministic
types.

type

this approach

implementations

stems from

of concurrent

Here, researchers ask questions such as
“is there a wait-free implementation
of

T1 using

objects

of type

T2 ?“

A concurrent

plementation

of a data type is wait-free

can complete

any operation

im-

if any process

on the type in a finite

num-

ber of its own steps, regardless of the behavior of other
implementations
are desirable in
processes. Wait-free
asynchronous
systems because they prevent slow pro*This
author
was supported
in part
Foundation
under grants
CCR-9106627
thor’s address:
nology, Atlanta,
f T&
author

College of Computing,
Georgia Institute
Georgia
30332-0280.
was supported
in part by a scholarship

Hariri
Foundation.
t ‘ThIs
author was supported
dation

under

Mathematics
SW,

to the

at Spelman

and Information
Lane

a grant

Post

Science
Office

in part
Center

College.
Program,

Box

333,

Scientific

Author’s
Spelman

Atlanta

of Techfrom

by the W. F. Kellogg

for

cesses from slowing down faster ones; in addition,
they
tolerate any number of stopping failures.
Herlihy
[7]
showed a direct connection between a type’s ability to
implement
wait-free consensus and its ability to provide
wait-free implementations
of other types. In particular,

by the National
Science
and CCR-9301454.
Au-

Foun-

Applications

address:
College,

Georgia

the

he showed that

of

Computer

consensus is universal:

if type T can implement

wait-free

for any n >0,

consensus in systems

with n processes, then T can provide a wait-free
implementation
of any type in such systems. In light of
this result, Herlihy evaluated the power of a data type

35o Spelman

30314-0339.

Permission to copy without fee all or part of this material is
granted provided that the copies are not made or distributed for
direct commercial advantage, the ACM copyright notice and the
title of the publication and its date appear, and notice is given
that copying is by permission of the Association of Computing
Machinery. To copy otherwise, or to republish, requires a fee
and/or specific permission.
PODC 94- 8/94 Los Angeles CA USA
@ 1994 ACM 0-89791 -654-9/94/0008.$3.50

by assigning it a consensus number; this is the maximum number of processes for which the type can implement wait-free consensus. Herlihy thus cast the universe
of concurrent
data types into a hierarchy, each level of
which cent ains types with a particular
consensus number.
Jayanti

3.54

[91 refined

this study

by asking the following

question:
plement
required

what

does it mean to say that

a type can im-

wait-free consensus? He argued that
addressing the following
questions:

1. Can more than

one object

not oblivious;
such types are aware of (and may use)
the identities
of processes accessing them.
We also demonstrate
other results relevant to the use
of registers in implementing
wait-free consensus. For all

an answer

of the type

be used in

types (even nondeterministic

the implementation?
2. Can read/write

registers

also be used in the imple-

value.

ment ation?

These results confirm
Because these questions can be answered together
in
four different ways, Jayanti identified four possible hierarchies of types, one of which corresponds to Herlihy’s
assignment
of consensus numbers
(answering
“no” to
question 1 and “yes” to 2). He called these hl, hi, ~m,
and h~. A subscript
“l” indicates that only one object
of a type can be used, while a subscript
‘(m” indicates
that many can be used. A superscript
registers

may be used, while

they may not.

ones), the two hierarchies

can differ only at the first level: if either assigns a type a
value great er than 1, then the other assigns it the same

Using

that,

in most cases, registers

do

not play a special role in achieving wait-free consensus.
In another paper [17], we show (as a corollary)
that
Jayanti’s hierarchy ha is robust for
Combined
with the results of thk
that hm is robust for these types.
Our results are proven through
a new concurrent
data type called

deterministic
types.
paper, we conclude
the introduction
the one-use bit.

of
An

“r” indicates

that

object of this type is a bit, initially

its absence indicates

that

most once and set to 1 at most once. Our main results

this notation,

Herlihy’s

hierarchy

O, that can be read at

stem from the following:

is hi.
1. A finite

Given these four Klerartiles,
Jayanti naturally
asked
if they were distinct
and, if so, which best measured
the computational
power of different
data types.
He
argued that it is desirable for a hierarchy to be robust.
Informally,
a hierarchy is robust if no collection of types
at low levels can implement
a type at a higher level.
Jayanti showed that none of hl,
robust if it were not equal to h:.
both

h; and h~ were different

only ha might be robust (hl
h~ or hi does not).
Jayanti

h~,

proving

2. Almost

that

This

implement

that

that is, there is (at least)

deterministic
Jayanti

the two hierarchies
type.

is necessary.

Thus,

2

the nondeterminism

for the class of deterministic

types.

This

a one-

5).

any type can be used to

Thus,

the availability

the ability

of regis-

of such a type to do
objects of the type).

section

presents

the

definitions

necessary to present

and background

and interpret

the results

of this paper.

on

2.1

Types

A type is a 5-tuple T = (n, Q, I, R, b). The components
are (1) n, the number of “port s“ the type has (this limits the number of processes that may access the type),
(2) Q, a (possibly infinite)
set of states; (3) 1, a set
of access invocations;
(4) R, a set of access responses;
and (5) 6, a transition
terministic,

used by

in which

nondeterministic,

h~ is robust

The above result

4).

Background

material

give equal values for any

It is thus possible that

a

implementation

cause, as will be seen, they are too weak to implement
2-process consensus with or without
registers.

the type for which more than one behavior is possible.
This raises an obvious question:
can the same result
be shown with a deterministic
type? Since most commonly used concurrent
data types are deterministic,
a
positive answer to this question would imply that the
non-robustness
of hm would hold even for the class of
deterministic
types.
We answer this question in the negative. That is, we
show that

can implement

The existence of types that cannot implement
one-use
bits do not invalidate
the results mentioned
above be-

nondeterministically;

one sequence of operations

almost

consensus (if one is allowed multiple

levels in the two hier-

was specified

one-use bits.

ters does not increase

cannot equal h; if either
left the robustness of h~

was at different
type

is shown in Section

These results show that

Recall that Jayanti’s hierarchy h~ was defined by answering “yes” to question 1 above and “no” to question 2; it differs from ha on whether or not registers
may be used in implementations
of consensus. Jayanti
proved hm # h~ (and hm to not be robust) by exhibitarchies.

in a wait-free

any type can be used to implement

use bit (thk

as an open question.

ing a type

of one-use bits

register

of consensus (this is shown in Section

h;, and hm could be
He then showed that
from

number

read/write

Thk

is

first proven for types that are oblivious; objects of such
types are not aware of the identities
of processes accessing them. Most research in this area has concentrated
on such t yp es. Our results also hold for types that are

T may be either
x I +

of a type indicates

cZe-

Q x R, or

in which case 6: Q x Nn x I +

specification

access an object

function.

case 6: Q x N.

2Q x ‘.l

how processes may

of type T (using invocations

in I), how

the object communicates
to processes (using responses
in R), and what are the legal sequential histories of the
lIvn={l,2,

355

. . ..n}.

type (specified

by d). If an object

of type T is in state q

i e 1, d(q, jl, i) = 6(q, jz, i). An oblivious
distinguish
identical accesses by different
oblivious

types,

second (port
A

we often

number)

sequential

sequence

of

history

response triples
ditions

are met.

T

(i)il,

a state

and

q.

~l);

consider

a;(j2,

i27r2);

is a

ends after
length

k port-invocation-response

the sequence

An instantiation

triples,

””-

then

in the

system

that

accesses Oj;

processes can do so. Each program
tation specifies how the implementing
accessed and what

response

should

at most

mj

of the implemenobjects are to be
be returned

[9].) We say that there is an implementation
S for n processes if such an implementation
all q c Q.

We say that state q’ is reachable
in some serial history from q. If H

of H, denoted

of T in state q G Q from S for
. . .
of nntml states (ql, qz, . . . >flm)

w a tuple

to the

invocation
associated with that program.
An implementation
is correct if all resulting histories
are linearizable
with respect to the specification
of T
starting from state q [8]. 2 It is wait-free if, in all fair
histories, all invocations
of the deterministic
programs
terminate
in a finite number of steps.3
(For further
details of these definitions,
consult Herlihy [7] or Jayanti

con-

(for all k, qk C Q, ik C N~, ik G 1, and Tk E l?). It
must be the case that, for all k, (qk, ?’k) = d(qk,jk,ik)
(if T
(if T is deterministic)
and (qk, ?’k) E d(qk, ~k,ik)
is nondeterministic).
from q if q’ appears

implementation

process

the

port-invocation-

ff2;

let T =

(% ~ Qj) and a deterministicprogramPjk for ea~
The implementation
should
ij c I and each k c N..
specify, for each object Oj, the port number of each

function.

with qo) such that certain

In particular,

~=qO;

from

states

(starting

and omit

to the transition

of

alternating

An

More formally,

and let S = {01,02,...,0m~
be a set of
that Oj is of type Tj = (nj, Qj, Ij, Rj, ~j).

n processes

type does not
processes. For

abuse notation

input

for the type being implemented.
(n, Q, I,R,6)
objects such

when invocation
i E I appears on port j c Nn, then
the object changes to state q’ and returns response r
over port j if and only if (q’, r) = d(g, j, i) (if T is deterministic)
or (q’, T) c ~(q, j, i) (if T is nondeterministic).
A type is oblivious if, for all q c Q, jl, jz c Nn, and

the

If there is an implementation

1111,is k.

that

S implements

n-process

of T from
exists for

of TC,n from

S, we say

consensus.

or object of type T in a system must

specify, for each port, which
the object through that port.

process (if any) accesses
At most one process may

2.3

use a port.
If T is oblivious
and there are at most n
processes in the system, then the assignment
of “port
numb ers” is irrelevant.
The ability
of a type to solve consensus is central

The Universality
of Consensus
Wait-Free
Hierarchies

and

We define consensus as a type and
to this paper,
then consider the ability
of different
types to imple-

Herlihy [7] demonstrated
that the consensus types TC,n
are universal in the following sense. There is a wait-free
implementation
of any type T using registers and objects of type TC,n for systems of n processes. Because of
thk, Herlihy proposed evaluating
different types by as-

ment an object of the consensus type.
The n-process
binary consensus type Tc,n is an oblivious type defined

signing them consensus numbers. The consensus
ber of type T is the largest number n for which

to be (n, Q, I,.R,6),

number of registers
implement
TC,n.

where

Q = {1,0,

1}, 1 = {O, 1},

R = {O, 1},and

C$(l,o) =
6(1,1) =
d(a, b)

(a, a)

for any a, b C {O, 1}

all future responses. This response
the consensus value of the object.

This

is sometimes

●

defines

called

●

what

it means for one type
Informally,

is a set of objects

(appropriate

ministic

that

programs

is one program

operate

of different

hl (T)

> n if one object

and deter-

on these objects.

choices here, he de-

of type T can implement

h: (T) > n if some

number

of registers

and

n-process

h~ (T) z n if some number of objects
implement
n-process consensus.

one

ob-

consensus.

of type T can

to be

an implementation

ely initialized)

assumptions
or not regis-

consensus.

ject oft ype T can implement
●

by others.

the impact

n-process

Implementations
section

T can

fined four wait-j%ee hierarchies:

Usually, objects of the type are chosen to have state 1
initially.
If a process’s initial
value is O (or 1, respectively),
it performs
invocation
O (or 1, respectively).
Note that the first invocation
on the object determines

implemented

of type

ters should be used in the implementations
of TC,n and
whether or not multiple
objects of a type can be used.

(1,1)

To explore

2.2

object

Jayanti [9] questioned two of Herlihy’s
in assigning consensus numbers:
whether

(0,0)

=

and a single

numsome

2A recent

paper

is appropriate

for

computability
3A ~l~torY

There

for each process and for each invocation

forms

356

[15].
is &

an infinite

considers
certain

an alternative

aspects

if each process

number

to linearizability

of the

study

either

halts

of operations.

that

of asynchronous
explicitly

or Per-

hi(T)

●

> n if some number

of type T can implement

of registers
n-process

and objects

&u(uNSET,

Herlihy’s
assignment of consensus number corresponds
to Jayanti’s
hierarchy
h;.
It is clear from these definitions
that, for all types T, 1 < hl (T) < h; (T) <
h~(T)
and 1 s hl (T) < hm (T) < h~ (T).
In addi-

C$IU(SET,
61U (DEAD,

tion, standard techniques can be used to show that, if
T = (n, Q, 1, R, 6), then h(T) < n (where h. is any of
the hierarchies
Ideally,

&u(uNSET,

given above).

the assignment

RI.

=

{0,1,

read)

=

{(DEAD,

O)}

read)

=

{(DEAD,

1)}

read)

=

{(DEAD,

O),

write)

=

{(SET,

write)

=

{(DEAD,

ok)}

write)

=

{(DEAD,

ok)}.

ok}

consensus.

C!IU(SET,

of a consensus

(or hierarchy)

JIU(DEAD,

(DEAD,

1)}

ok)}

number to a type should be a good measure of the type’s
computational
power. The larger the number assigned,
the more power the type has to implement
other types.

Only read operations
return informational
responses;
any such operation sends the object to the state DEAD.

Indeed,

The object

Herlihy’s

result

sus shows that,

if h(T)

of the hierarchies
ports,

given

on the universality
= n > h(T’)
above)

of regist ers and objects

h is any

and T’ has at most

then there is an implementation

number

of consen-

(where

n

of T’ using some

can never leave this state

the nondeterminism,

no further

and, because

information

bit can be obtained

at this point.

erations,

also goes to the state

of type T.

the object

Note that,

although

After

the specification

of

about

the

two write

op-

DEAD.

of this type

is

Given four different ways of assigning these values, it
makes sense to consider which is best. Jayanti identified
a desirable property of hierarchies that he called robustness. Hierarchy
h is robust if, for every choice of n, T,
and S=
{T1, T2, ..., Tin}, the relations
h(T) > n and

nondeterministic,
this nondeterminism
will play no role
in our use of the type (Section 4); a read will never be
Note also
invoked when the object is in state DEAD.
that, as specified, this type is oblivious.
In all our uses
of the type, only one process performs write and only

h(ff”) < n (for 1< j < m) imply that there is no implement at ion of T using objects of types in S. Robustness
implies that there is no way to combine “weak” types

one performs read. Thus, the object could have been
specified as a 2-port, non-oblivious
type without
loss of

of implement

a “strong”

applicability.

type.

Jayanti showed that none of hl, h:, and hm could be
robust if it were not equal to h&. He then showed that

4

both

Although

h; and hm were different

only h& might be robust (hl
hm or hi does not). Jayanti
as an open question.

from

ha,

proving

that

Using

cannot equal h: if either
left the robustness of h:

1. General read/write
single-reader
2. For

Bits

The
defined

n,

any

consensus,

while

Section

5 shows that

registers

weaker than

gen-

we can show that,

wait-free
and

can be simulated

er multi-use

using

bits.

implement
any

bit b, there are bounds

respectively,

at ion

single-reader

of nsingle-

rb and ‘Wbsuch that

no more than

in any execution

b

rb and wb times,

of the implement

a-

tion.

the one-use bit. Objects of this type are one-bit registers that can be read only once and written
only once.
Section 4 shows that objects of this type can be used
to implement
general multi-reader,
multi-writer,
multiimplement

registers,

single-writ

is read and written

results of this paper stem from the impleand use of a new concurrent
date type called

value registers,

any

process
writer

The main
mentation

multi-value

within the context of wait-free implementations
of consensus, they are equally powerful.
This can be shown
through three observations:

and only if h& is.
One-Use

Bits

one-use bits are apparently

eral multi-use

Jayanti’s
proof that hm # h~ was based on the ex~ 2.
istence of a type T with hm (T) = 1 and ha(T)
This type is nondeterministic.
The remainder
of this
paper considers restricted
classes of types for which hm
is shown equal to h~. For these classes, hm is robust if

3

One-Use

it is easy to

3. If there are bounds
multi-use

on the number

of times

that

a

bit b can be read and writ t en, then b can

be simulated

by a finite

number

These facts are shown in Sections

of single-use

bits.

4.1–4.3 below.

this type.

one-use

bit

type

Tlu

= (2, Q1., 11., RI.,

all.)

is

as follows:
Ql~

=

{UNSET,

Il.

=

{read,

SET,

4.1

Simulating

General

Read/Write

Registers

has considered
the definition
A large body of literature
and implementation
of a variety of different
kinds of
read/write
registers and the relationships
between these

DEAD}

write}

357

kinds.

The registers

required

by Herlihy

Cl through

[7] and Jayanti

[9] are atomic,
multi-reader,
multi-writer,
and multivalue. Researchers have also considered registers that
are regular (weaker than atomic), single-reader,
singlewrit er, and one-bit.
The following
paragraph
gives a
very incomplete
account of the large volume of results
that
that

that have been produced,
mentioning
only
are necessary for the results of this paper.

Lamport

[13] showed

plementation

that

of multi-reader,

there

multi-value

stronger

than

registers.

We consider

im-

regular

regular

Since atomic

registers,

it follows

invocations

bits

registers
from

are

all these

mentioned

4.2

Bounds

Access

above exist for any number

in Wait-Free

by

(If a config-

their

first invocations

is a leaf

as first

invocations

because any later

by a process must return

the same response

only first

(see Section

2.1).

Thus,

the process

can store

the first response locally and need not access any of the
implementing
objects after its first invocation
on the
T.,n object.
Consider any one of these trees.
We will show by
cent radiction
that it is finite.
Assume that it is not.
This means that a form of K6nig’s Lemma applies:
Lemma
1 (Kiinig):
bound on the fan-out
nite path from
The fan-out

results that there is a wait-free implementation
of multireader, multi-writer,
atomic, multi-value
registers from
single-reader,
single-writer,
atomic bits.
(All the implementations
processes.)

operation

on Tc,~.

node.

from single-reader,
single-writer,
regular bits.
Burns
and Peterson [3] showed that there is a wait-free
implementation
of multi-reader,
single-writer,
atomic bits
from multi-reader,
single-writer,
regular bits.
Peterson [16] showed that there is a wait-free implementation
of mult i-reader, single-writer,
at omit, multi-value
registers from multi-reader,
single-writer,
atomic bits. Peterson and Burns [18] showed that there is a wait-free
implement at ion of multi-reader,
multi-writer,
at omit,
multi-value
registers from multi-reader,
single-writer,
atomic,

of one low-level

uration
can be reached via multiple
paths, it appears
multiple
times.) Any configuration
in which some process accesses the Tc,n object a second time does not
appear in a tree. Thus, a configuration
in whk.h all n
processes have completed

those

is a wait-free

single-writer,

the execution

invocation
one process in itsfirst

If an infinite
rooted tree has a
of its nodes, then there is an infi-

the root.
of our trees is bounded

by n.

Any

node

has at most n children,
one for each process. This is
because, as noted above, the processes are deterministic,

of

Consensus

as are registers and the type T.
Konig’s Lemma now implies that there is an infinite
path from the root.
This path corresponds
to some
execution of the implementation.
This means that there
is an execution in which some process never completes
its first

invocation

on Tc,n.

This

contradicts

the fact

Consider some deterministic
type T such that h~(T) z
n. This means that there is a wait-free implementation

that the implementation
is wait-free.
The tree described is thus finite; let d be its depth,

of TC,~ in a system with n processes that uses some
number of registers and objects of type T. As noted in
the previous section, we can assume that these registers

the maximum
length of a path from the root. Consider
now all the trees defined above. There are 2“ such trees.

for

This is because the initial
states of the implementing
objects are the same in all trees, and only the choice of

each bit b, there exist constants ?’b and wb such that in
no execution of the implementation
is the bit read more
than I’b times or written more than wb times.

the entry points of the n processes can vary. Let D be
the maximum
d over all the trees. This means that, in
each execution in which each process accesses the Tc,n

are single-reader

single-writer

We can consider
tion

(from

state

bits.

the executions

1) as a collection

of a tree corresponds

We show that,

of the implementa-

object

of trees.

Each node

the argument

of the im-

accesses are invoked on any implementing
object in any
execution of the implementation.
This means that, by
choosing rb = U)b = D, we know that, in no execution
of the implementation,
does any process read bit b more

to some configuration

at most once, at most D steps are executed.

plementing
objects (the bits and the objects of type T)
and the “program
counters”
of the n processes in their
The roots of the trees correimplementing
functions.
spond to possible initial configurations:
the initial states
of the implementing
objects and the vector of invocations that the n processes will use to first access the
Tc,n object (each may be O or 1); that is, each process
is at the “entry point”
of one of its two implementing

than

functions.

bit that

(Notice

that

two root

configurations

Tb tim(%

4.3

implemented
pose that

states in all such configurations,

times,

must

specify

configuration

a unique

initial

Cl is the parent

state for each object.)
of C’2 if C’2 results

A
from

358

Write

it

Multi-Use

a finite

and is written

at most

D

Bits

number

number

b is initialized

By

at most

wb times.

shows how any single-reader,
with

bit

means that

Wore than

is accessed a bounded

fer only with respect to the “entry points” selected by
the processes; the implementing
objects have the same
as the implementation

or

Implementing

This section

can dif-

given above, thh

single-writer
of times

can be

of one-use bits.

Sup-

to v, is read at most
wb times.

Because

written

by only one process, we assume that

written

when its value is being changed.

rb
b is

it is only

The

implementation

These form

uses ~b(7Ub + 1) one-use

. ..wb+l.

bits[l
all elements
sponds

bits.

i E I, there is a response rqi E R and state q’ such that

an (w~ + 1) x rb array

of which

to a write

d(q, i) = (q’, rq~) and, for every state p reachable from q,
there is a state p’ such that c$(p,i) = (p’, rqi ). A trivial

l...

rb],

are initially

type, once initialized,
occurrence

O. Each row corre-

and each column

to a read.

information

(This

The reader

maintains

two local

integer

vari-

for

in one step, that

for a row that

bits [i., jr]

= 1 do

a different

iv contains

column

Implementing

the index
flipped.

One-Use

illustrates

of the first

row that
bit b

A write

value is
value).

return(ok)

are non-trivial
level

returned

det ermin-

1 in hierarchy

shows how non-trivial

(note that,

one-use bits

oblivious

Deterministic

correctly

implement

a one-use bit

after the first read, any value can be properly
by subsequent

reads).

hm.
deter-

5.2

ministic
types can implement
one-use bits. Section 5.2
extends this to the general case of any type. Section 5.3
shows how higher-level
types in hm can implement
oneuse bits.
Oblivious

p to SET, and
state q corresponds to UNSET,
It is not hard to see that
state to DEAD.

the above procedures

two cases in which

as follows:

invoke i’ on O

Bits

above

Non-Trivial

was in state p *J

is performed

Intuitively,
any other

These

5.1

to state q. A

ret urn(1)

of this implementa-

istic

types

/“O

an execution

This means that

can be implemented.
5.1 first

of a one-use bit.

else

to ensure that no

once. After

A formal proof of the correctness
tion is deferred to the full paper.

and

i’

return(0)

has been written
ir – 1 times, so the returned
(v+ (ir - 1)) mod 2 (recall that v is b’s initial

Section

there is some invocation

d(q, i’) = (p, r’).

/* O is still in state q*/
+ (i. – 1)) mod 2)

has not been completely

types

(q’, rq),

if response is rq then

one-use bit is read more than

section

=

invoke i on O

+l

Each read examines

This

q, d(q, i)

We use one object O of type T, initialized
read of the bit is performed
as follows:

contains

bit:

return((w

5

from

We can now give an implementation
by looking

ir :=ir+l

of a read,

is, so that

and response r’ such that

return(ok)

jr:=j.

p is reachable

d(p, i) = (P’, rp), and rp # rq. It k not hard to see that
p and q can be chosen such that p is reachable from q

bits[iw, jw] := 1

A read is performed

A type

Q, invocationi GI, andresponses
rq,rp G

q,p, q’, p’ E
R such that

to the write:

iW:=iW+l

while

of the type.

terministic

jW := 1 to ?’b do

an unflipped

processes can gain no

by accessing an object

Let T = (n, Q, 1, R, d) be a non-trivial
oblivious deThis means that there are states
type.

ables ir and jr, while the writer maintains
local iW and
jW; these are all initially
1. A write is performed by flipping all the bits in the row corresponding

the same response to each

that is not trivial is non-trivial.
We now show that any
non-trivial
oblivious deterministic
type can implement
a one-use bit.

means that the last row is not actually necessary. It is
included here to simplify
the presentation
of the read
routine.)

returns

of a given invocation;

Non-Trivial

Deterministic

Types

in

General
The previous section showed that any non-trivial
oblivious deterministic
type can implement
one-use bits. The
proof given there depended on the obliviousness
of the
type being used. This section generalizes that result for

Types

types that are not necessarily oblivious.
A type is trivial if, for all start states and all ports,

Most, but not all, deterministic
oblivious
types can
implement
one-use bits.
Some types, however, are so
weak as to be triviaJ and are incapable of implement-

all sequences of invocations
on that port always return
the same sequence of results regardless of any invoca-

ing any interesting
type. Consider, for example, a type
T = (n, Q, 1, R, 6) such that IRI = 1. Because the
type must return the same response to every invocation,
there is no way that it can supply any useful information. Formally,
an oblivious
type 2’ = (n, Q, 1, R, 6) is

tions performed
(and the order in which they are performed) on other ports. Thus, for any non-trivial
type
T = (n, Q, 1, R, d), there is at least one start state q, a
sequence of invocations on one port (without
loss of generality port 1), and two sequential histories 171 and Hz

trivial

from q that contain

if, for every

state

q E Q and every

invocation
359

the same sequence of invocations

on

port 1 such that
returns dfierent

one of the invocations
in the sequence
values in the two schedules.
Call HI

of H2(notethat

m-j-l>

Let H{

and Hj

be the last j + 1 operations

Ht+l,

Then

Hi

k-j-l

and Hj

and Hz a non-trivial
pair. Without
10SSof generality,
we assume that the invocation
returning
ditTerent values

from

is the last invocation
on port
be the sequence of invocations

lH~l+[H~]=2(j+l)~2k
contradicts
the minimality

tories.

Note that

different

1. Let ~ = (il, iz,... , ik)
on port 1 in the two his-

sequences of operations

that

may

be invoked cm ports other than port 1 in HI and Hz.
Consider any sequential
history H from q in whkh
the invocations
on port 1 are i’. The history’s
mtum
vcdue is the resdt returned by ih.
For the remainder
Hl,

Hz,

among

of this section,

and g are such that

the non-triviai

iemmas

pairs.

demonstrate

These properties

certain

allow

followed

properties

sequence of

invocations

of H1 and Hz.

T to be used to implement

Let H be the history
Proof :
of the invocations
in 3 on port

now

on assume

invocations

on port

invocation

on another

fkom q consisting
1.

Because

that

1 and that
port

H1 contains
H2 contains

(the latter

oniy

the return

3:

The last

1.

of the iirst

m – (k+l)>l

1) followed

by the last

1); [Hl = m – 1. If the return

k invocations

only

the k

to simuiate

at least one

are all

The return

vaiue of H; is the same

as that of H1 and the return value of Hi is the same as
that of Hz. These differ, so Hi, and Hi are a non-triviai
pair. But lHjl+lH~l
= k+(k+l)
< k+m = IHII+IHz.1,
again contradicting
minimality.
Thus, IH21 = k +1.
❑
Lemma 4 establishes that an object O of any nontrivial deterministic
type can be used by two processes
a one-use

bit.

(Recall

that,

in Section

oniy two processes ever access such a bit in my
uses of the type,

vaiue).

in Hz

a contradiction.
Thus,
tlom that of H2. Let

one writing

reading process is connected
read as follows:

on

and one reading.)
to port

4,

of our
The

1 and performs

a

invoke 2 on O
Let m = IH2 I (m > k) and assume that

port 1; otherwise,
suiting in a shorter

if return

one

pair.

that

the last j invocations

return(0)
/* writer

Ht

and H1+l

values (if no such 4 exists, then Hi
on port

has written*/

return(1)

in

The writing
process is connected to port 2 and simply
performs the one invocation
iW from H2 on that port:
invoke iW on O
return(
ok)

resuits from inverting
the order of (m - (j - i))th and
(m – (~ – i) + l)st operations
in Hi. That is, each history successively moves the operation on port 2 (which
is (m — j)th in @) one step later.
Let 4 ~ O be least such that

of H1 then

has not written*/

else

this invocation
can eliminated,
reH;, with H1 and H; being a shorter

Suppose

value is that

/“ writer

Hz are on port 1 (k > j > 1) and that the (m – j)th
is on some other port} say 2. Detine a sequence of histories, H~, H~,...
,H~ as foliows: ~
= Hz, and W2+1

vocation

(all on port

and H2, respectively.

of the last k invocations
in Hz in on some port other
than port 1. The last invocation
in Hz must be ik on

ent return

that

q’ be the state of the object after the tirst m – (k+ 1)
invocations
of Hz. Let Hi and Hi be histories from q’
containing
the last k and last k + 1 invocations
in H1

because otherwise

H2 = .HI and they have the same return

non-trivial

Note

IH21 = m > k + 1. Let H be a

q consisting

a shorter non-triviai
pair, giving
the return value of H is dit&ent

vaiues of H1 and Hz difIer, the return vaiue of H must
differ from at least one of them, say H2 ‘s. In this case,
H and Hz are also a non-triviai
pair. Since IHI I + ]Hz I
is rninimai,
IH] + IH2 I = k+ IHzI ~ IHII + IH21, SO
IHI [ ~ k. Since HI must contain the k operations
on
c1
port 1, IE71I = k.

Proof:

pair.

and

values

value of H is the same as that of H2, then H1 and H are

Lemma 2: (he of HI and Hz has length k; that is, it
consists only of invocations on port 1.

Lemma
port 1.

that

on port

of H2 (none on port

k invocations

one-

use bits.

From

Assume
from

return

of H2 is k +1; that is, H2 con(say iW) on some port (say 2)

by k invocations

Proof :
history

20),
of Ht

=k+k<lHll+[H21.
This
of H1 and Hz. We conclude
•1
in Hz are on port 1.

the last k invocations

4: The length
sists of one invocation

IH1 I + [H21 is miniiai

have different

a non-trivial

Lemma

we will assume that

The following

q’ and thus form

=(k-l)-j

Note that the reader
neither H1’s nor H2’s.

have ciMer-

the writer

has written,

may get a return value that is
However, this still indicates that
so 1 can be returned.

has an in-

2 as its last and forms a non-triviai

pair

5.S

with HI, contradicting
the above observation
that both
histories must end with invocations
on port 1). Let q’ be
the state of the object after the fist m- j– 1 invocations

High-Level

Types

in &

Let T be any type such that &(T)
>2.
This means
that there is an implementation
of 2-process consensus

360

using

only

objects

of type

T (without

registers).

now show that, even if T is nondeterministic,
implement
one-use bits.
Let O bean

object

as implemented

by objects

use bit is performed
invoke

of type TC,2, initialized

We

T is deterministic

●

then

T can

of type T. A read of a one-

consensus.

showed that,
simulated
tion

by a finite

5.2 showed that

terministic
objects

as follows:

hm(T)

1 on O

reader

number

of one-use bits.

objects

of any non-trivial

type can imp~ment
of type

T can implement

z n and, therefore,

proposes

O, meaning

“read

pre-

●

preceded

the read,

so the read can be linearized b efore the write and return
O. If the consensus value is 1, then the read could not
have completely
preceded the write, so the read can be
linearized
after the write and return 1. (Note that this

of type T and one-use bits can implement

objects

above,

to Wait-FYee

●

hm(T)

~ 2.

<

hm (T).

= h~(T).

into three

and trivial.

how an object

sequence of invocations
the same sequence
thus be trivially
that,

5 shows that Jayanti’s

of type

This means that,
T is initialized,

type with

of responses.

implemented

The

locally.

no
any

object
This

= 1. Since hm(T)

that
❑
to

it had to a

h~ (T) = 1 and h:(T)

z 2.

for two large classes

in most cases of interest,

curiosities.
registers

are

can

means

hi)

apply

made with
(e.g., about

when they are not (e.g., to the

hierarchy h~); the converse is also true.
In particular,
these facts pertain to Jayanti’s robustness property.
Hks proof that hn is not robust does not
apply,

for example,

to deterministic

have shown in another
terministic
that

alone can imple-

z 1, h&(T)

show that,

the hierarchy

by a process always returns

if h~ (T) ~ n, then registers

h~(Z’)

implies

choice of a type T

was not accidental:

the reasoning process: various arguments
the assumptions that registers are available

ment n-process consensus.
Since registers cannot
implement
2-pro cess consensus [4,7,14], this implies
that

that
as de-

not “special” when it comes to implementing
wait-free
consensus. If convenient,
they can be used to simplify

cases:

is deterministic

matter

This

These results are of more than theoretical

Proof:
Let T be a type with one of the above properties. Recall that 1 s h~ (T) ~ h; (T) for all types T. It
thus suffices to show that h; (2’) s hm (T). The proof

T

s hm (T)

Conclusions

They

is divided

con-

implies

of concurrent data types, Jayanti’s wait-free hierarchies
h~ and h; are equal. One of these is the class of deterministic types, which is of considerable int crest.

deterministic;

Then h~(T)

n-process
This

h~ (T)

The results of this paper show that,
is

ob-

Thus,

holds of

type T:

●

T can implement

h~ and h:

nondeterministic
7

5: Suppose that one of the following

one-use bits.

= h~(T).

distinguish
to

~ n, then

5.3 showed that

using registers.

all cases, h%(T)

Theorem

Hierarchies

of type

Section

if h~(T)

sired.
In

of one-use bits.)

consensus.

hm (T) ~ n and, therefore,

hm(T)

The above results have two important
applications
wait-free hierarchies,
given below in Theorem 5.

T

as de-

~ 2. As noted

sensus without

returns the same response to all reads
this is permitted
by the nondeterministic

Applications

●

~ hm(T)

that

hm(T)

n-process

could not have completely

Theorem

con-

implies

objects

jects of type T can implement

6

deThus,

n-process
This

lz~ (T)

the write

specification

4.3
Sec-

one-use bits.

using registers.

while the writer
proposes 1, meaning
cedes write,”
“write precedes read.” If the consensus value is O, then

implementation
by the reader;

on the num-

sired.

ok)
the

bits.

if this is the case, each such bit maybe

sensus without

Basically,

4.1,

single-writer

ber of times each bit may be used and Section

return(r)

return(

> n,

can imple-

As noted in Section

Section 4.2 showed that there is bound

O on O

invoke

If h&(T)

T and registers

the registers can be single-reader,

as follows:

is performed

and non-trivial.

of type

ment n-process
to state 1,

let r be response

A write

objects

types

[17].

hm is also robust

paper

that

The results

types.

In fact,

h~ is robust

we

for de-

of this paper

show

for these types.

Acknowledgments

~ hm(T)

We are grateful
work with us.

as desired.

361

to Scott McCrickard

for discussing

this

[11]Prasad

References
Michael
[1] Yehuda Afek, David S. Greenberg,
Computing
ritt,
and Gadi Taubenfeld.
faulty

shared

Eleventh

ACM

tributed

In

memory.
Symposium

Computing,

Merwith

Proceedings

of

on Principles

pages 47–58. ACM

Afek, Eytan
A

man.

completeness

synchronization
Twelfth

the

Symposium

Computing,

August

In

for

on Distributed
Verlag,

Proceedings

on Principles

pages

of Dis-

159–1 70. ACM

Press,

1993.

[3] James E. Burns
ing

and Gary L. Peterson.

multi-reader

values.

In

posium

atomic

values

Proceedings

ACM

of Distributed

ACM

Lamport.

Sym-

[14] Michael

Chor,

Amos

cessor coordination
In

Proceedings

Principles
ACM

Israeli,

1987.

using

Dolev,

97, January

Computing,

On

interprocess

for agreement

processors.

Advances

In Franco

in Computing
JAI

among

on

[15] Gil

Neiger.

and Larry

of the

Symposium

Stock-

ing. ACM

needed for dis-

of the ACM,

34(1):77-

[16] Gary

study

on Principles
Press, August

L. Peterson.

ing.

Fischer,

consensus
ACM,

with

one faulty

Herlihy.

Transactions

[8] Maurice

earizability:

April

ACM

and

ACM

Transactions

[17] Gary L. Peterson,

of the

gap theorem
Distributed

com-

of the Thirteenth

ACM

of Distributed

reading

A CM

on Principles

M. Wing.

reading

Lin-

for concurrent

on Programming
July

pages 145-158.

[10] Prasad
Toueg.

Jayanti,

Press, August

Tushar

Deepak

Fault-tolerant

In Proceedings
Foundations
IEEE

ACM

of Distributed

wait-free

Lan-

of the Thirty-Third
of Computer

Computer

Society

ACM

Lan-

1983.

ACM

A

In Proceedings

of

on Principles

of

Press, August

objects.
on

Science, pages 157–166.
1992.

362

and James E. Burns.

while writing

II: The multi-writer

of the

Foundations

of Computer

Computer

Symposium

and Sam

Symposium

Press, October

Symposium

Computing.

[19] Serge Plotkln.
of consensus.

1993.

shared

January

Rida A. Bazzi, and Gil Neiger.

Proceedings
IEEE

1990.

Computing,

Chandra,

writ-

1994.

This volume.

On the robustness of Herlihy’s hi[9] Prasad Jayanti.
erarchy. In Proceedings of the Twelfth ACM Symposium

while

on Programming

for consensus types.

[18] Gary L. Peterson

12(3):463-492,

Comput-

1994. This volume,

and Sys-

1991.

condition

obliviousness:

of asynchronous

Concurrent

Transactions

the Thirteenth

Languages

and Jeannette

A correctness

guages and Systems,

Journal

synchronization.

January

4,

and

1985.

Wait-free

P. Herlihy

Lynch,

of dktributed

process.

on Programming

tems, 13(1):124–149,

A.

Impossibility

32(2):374–382,

[7] Maurice

objects.

Nancy

S. Paterson.

P. Preparata,

Press, 1987.

Set-linearizability

Foundations

1987.

J.

Michael

Mem-

unreliable

Research, volume

guages and Systems, 5(1):46-55,
[6] Michael

Computing,

hardware.
pages 86-97.

synchronism

Journal

communica-

Distributed

putability y. In Proceedings

Dwork,

on

pages 133-

C. Loui and Hosame H. Abu-Amara.

pages 163-183.

1987.

Cynthia

consensus.

On pro-

Symposium

Computing,

meyer. On the minimal
tributed

ACM

In

Symposium

1986.

asynchronous

Li.

asynchronous

of Distributed

Press, August

[5] Danny

and Ming

of the Sixth

ACM

1993.

II: Algorithms.

ory requirements
editor,

[4] Benny

Resource

of consensus objects.

Twelfth

of Distributed

1(2):86-101,

Computing,

Press, August

of the

Press, August

part

647 in Lecture

non-atomic

of the Sizth

on Principles

pages 222-231.

Construct-

from

and combinations

144. ACM

tion;

number

editors,

Workshop

1992.

Principles

[13] Leslie

results

and S. Zaks,

International

and Sendhil Mullainathan.

Proceedings

of the

Some

and decidability

Science, pages 69–84. Springer-

November

bounds

of

Toueg.

Algorithms,

[12] Jon Kleinberg

Weis-

a class

of the Sixth

Notes on Computer

Press, Au-

Sam

universality,

In A. Segall

Proceedings

of Dis-

and Hanan

theorem

objects.

ACM

tributed

Weisberger,

and

of consensus.

gust 1992.
[2] Yehuda

Jayanti

on the impossibility,

Twenty-Eighth

Society

Concurrent
case. In

Symposium

on

Science, pages 383-392.
Press, October

1987.

Sticky bits and the universality
In Proceedings of the Eighth ACM

on Principles

of Distributed

ing, pages 159–175. ACM

Press, August

Comput1989.

Synchronous Byzantine Quorum Systems
Rida

A. Bazzi

Department

o,j Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5~06
U.S.A.
bazzi@asu. edu

Abstract
Quorum systems have been used to implement many
coordination problems in d~tributed systems such as
mutual exclusion, data replication, distributed consensus, and commit protocols.
Malkhi and Reiter
recently proposed quorum systems that can tolerate
Byzantine failures; they called these systems Byzantine quorum systems and gave some examples of
such quorum systems.
In this paper, we argue that
the proposed definition of Byzantine quorums is too
strong fors ynchronous systems and we propose a definition of synchronous Byzantine quorums. We show
that the new definition is more appropriate for synchronous systems. We prove tight lower bounds on
the load of synchronous Byzantine quorum systems
for various failure assumptions and we present synchronous Byzantine quorums that have optimal loads
that match the lower bounds for two failure assumptions.
Keywords:
quorum systems,
fault tolerance,
Byzantine failures, load, dktributed systems.

1

processors request access to the critical section from all members of a quorum. A processor can
enter its critical section only if it receives permission
from all processors in a quorum.1 Quorum systems
are usually evaluated according to the following criteria:
system,

1. Quorum Size. The size of a quorum determines
the number of messages needed to access it.
2. Load. Informally, the load of a processor measures the share it has in in handling requests to
the quorum system. For a given probabilistic access rule of a quorum system, the load of a quorum system is the probability of accessing the
busiest processor. The load of a quorum system
is the minimum load over all access probability
distributions.
A quorum
3. Availability and Failure Probability.
system is said to be available if one of its quorums
consists of correct processors.
If failures occur
randomIy according to some probability distribution, the failure probability is the probability
that the quorum is not available.
The probabilistic model is not appropriate for Byzantine
failures.

Introduction

A quorum system is a collection of sets (quorums)
that mutually intersect.
Quorum systems have been
used to implement mutual exclusion [1, 9], replicated
data systems [8], commit protocols [19], and distributed consensus [13]. For example, in a typical
implementation of mutual exclusion using a quorum

Other criteria are also considered in the literature,
but we do not study them in this paper.
Work on quorum systems traditionally considered
crash failures [1, 2, 3, 4, 7, 8, 9, 18, 17, 16]. Recently,
Malkhi and Reiter [10] proposed the interesting notion of Byzantine quorums - quorum systems that
can tolerate Byzantine failures.
They showed that
the traditional definition of quorums is not adequate
for Byzantine quorums: in Byzantine quorum systems, the intersection of two quorums should contain
enough correct processors so that correct processors

Pennision to make digitaUhard copies ofoll or pmi ol’IhIs mo[crml Lx
persona! or cksroom use is granled withou[ fee provided IIM the copies
are not mode or distributed for profit or commetxinl mlvwIIJgt, the copyright notice, the title of the publimlicm and i[s date appear. wd notiw is
given that cop~ight is by permission oflk AC!tl, inc. To copy ntkrwisc.
to repuhl ish, 10 post on servers or to rediswilwk 10 lists, rqII ircs spccilic
permission andlor fee

1AdditiO~~l~essures are needed

1997 POtX- 97 Santa Bm+oro ( M [I*Y4
Copyrigkt

1997 ACM 0-8979 1-952 -1/97/8 ..$3.50

mentation

259

is fair and deadlock

free.

to insure that the imple-

2

Definitions
Model

3

can unambiguously access the quorums. They called
these quorums masking quorums because they hide
the failures from processors using them. They also
proposed variations of the basic Byzantine quorum
system for the case in which responses by faulty processors can be authenticated;
they called these systems dissemination quorum systems.
Finally, they
proposed the notion of opaque Byzantine quorums
and proved lower and upper bounds on their load.
They presented a number of masking Byzantine and
dissemination quorum systems for different failure assumptions and calculated their load. The quorum
systems they presented assume that the system is
asynchronous.

3.1

System

and

System

Model

We assume that the system consists of a set P of n
server processors and a number of client processors
that are distinct from the servers. All processors can
communicate using reliable message passing. We also
assume that there is a bound on message delivery time
that can be used to timeout failed processors that do
not respond to requests.

Failure Model

3.2

Contributions

Server processors can fail. 2 The assumptions about
failures affect the way a quorum can be used. In
studying quorum systems, it is usually assumed that
processors fail randomly according to some probability distribution and that failures are temporary.
Assuming that processors fail randomly is appropriate for crash failures or for failures that can be detected. A different model of failures is usually used
for Byzantine (arbitrary) failures. In the Byzantine
failure model, it is usually assumed that there is a
known bound on the number of failures that can occur in the system and that failed processors do not
recover. This is the model we adopt in this paper.
The set faulty denotes the set of faulty processors in
the system. A failure pattern 3 identifies the possible sets of faulty processors in the system. We write
F={
F1, F.2,. ... Fm }. There exists an element F of
7 such that at any given instant the faulty processors
belong to F. The processors do not necessarily know
F. A common example of a failure pattern is the ~threshold pattern in which 7 = {F E P : IFI = j}.
Another interesting failure pattern is the dis~oint pattern in which all elements of ~ are disjoint [10].

In this paper we carefully study the definitions of
Byzantine quorums as proposed in [10]. Specifically,
we study how the synchrony assumptions
affect the
definition of Byzantine quorums. This leads us to revisit, the traditional
definition of availability of quorum systems and the way quorums are accessed. We
find that the traditional definition of availability is intended to guarantee the correctness of quorum-based
protocols in a model in which failed processors can recover and, therefore, allows for a probabilistic model
of failures. We note that the probabilistic model is
not, adequate for Byzantine failures and conclude that
a new definition of availability and quorum access is
needed in the presence of Byzantine failures in a synchronous system. We revise the definition of masking Byzantine quorums proposed in [10] and propose
to relax it in a way that allows for more failures
in synchronous systems. We develop necessary and
sufficient conditions for the existence of synchronous
Byzantine quorums that can tolerate a general failure
pattern. We develop tight lower bounds for the load
of synchronous Byzantine quorums for various failure
patterns and, for each lower bound, we present a quorum system whose load is optimal and matches the
lower bound.
The rest of the paper is organized as follows. Section 3 presents some basic definitions and introduces
the notions of quorum access and availability.
Section 4 argues that, in synchronous systems, the definition of Byzantine quorum systems should be relaxed
and presents a relaxed definition.
Section 5 proves
lower bounds on the load of synchronous Byzantine
quorum systems for various failure assumptions. Section 6 presents synchronous Byzantine quorum systems that have optimal loads. Section 7 presents a
general construction of Byzantine quorum systems.

3.3

Coteries

and Quorums

The standard definition of a quorum is the following,

Definition 1: A quorum system Q over ‘F is a set
of subsets (called quorums) of P such that any two
quorums have a non-empty intersection.
The intersection property of quorums is essential for
their use in coordination problems.
Definition
2: A coterie C over P is a quorum system over S which is minimal under set inclusion.
2We do not consider

260

client failures

in this paPer

In this paper, we only consider quorum systems
are coteries.

quorum then the access is considered successful.
If
one of the servers failed, then the client attempts to
access another quorum that does not have any faulty
processor (the question of finding a quorum with no
fault y processors has been addressed in [2, 16]). Since
processors access a quorum only if all its members are
correct, two clients are always guaranteed to receive
a response from a common correct server which belongs to a non-empty intersection of two quorums.
The correctness of quorum-based protocols rely on
this intersection property. Traditionally, a quorum
is said to be available if all its elements are correct
processors [15]. One might think that it is sufficient
to have a non-empty intersection between quorums
to have availability. The following is an alternative
definition of availability.

that

Definition 3: A quorum sgstem Q dominates a quorum system Q’ if Q # Q’ and for every Q’ ~ Q’ there
exists Q c Q such that Q S Q’.
This suggests the definition of a non-dominated
rum systems, or NDQ for short.

quo-

Definition 4: A quowm
if

system is non-dominated
there is no other quorum system that dominates it.

3.4

Strategies

and Load

This section presents the formal definitions of strategy and load [15].
A protocol using a quorum system chooses a quorum for access according to some rules. A strategy is
a probabilistic rule to choose a quorum. Formally, a
strategy is defined as follows.

c Availability
y (alternative). A quorum system
is available if one of its quorums intersects
other quorum in a correct processor.

every

Definition 5: Let Q = {Q,,...,

Q&} be a quorum
w for Q is a probability distribu-

s~stem. A strategy
tion over Q.

This definition seems to guarantee
the non-empty
intersection
property of quorum systems.
Unfortunately, this definition is not sufficient if processors
can recover from failures.
The following example
clarifies this point.
Consider the quorum system

The probability of accessing Qi is w(Qi). For every
processor q 6 P, a strategy w induces a probability that q is accessed. This probability is called the
load on q. The system load is the load on the busiest
element induced by the best possible strategy.

Definition 6: Let w be a strategy for a quorum system Q = {Ql,..
induced

., Qm}.

For any q

by w on q is Jw(q) =

~Qj

●
~q

P, the load
W(Qj).

The.

load induced by w on Q is
JCW(Q)= rnEa&(q)
The system load on Q is
1(Q) = m~{ZW(Q)},
where the minimum

3.5

Quorum

is taken over all strategies

Access and Availability

Processors access a quorum to coordinate
their actions. The following is the typical way to access a
quorum.
A processor, the client, sends a request to
every processor, server, in a quorum set. Upon receiving a request, a correct processor updates its state and
sends a reply. The client waits until it receives a reply from every processor in the quorum. The replies
typically reflect requests made by other processors.
If the client receives replies from all processors in a

Q = {{1,2,3,4},
{1,2,5,6},
{2,3,5,6}}.
This system
is such that any two quorums have two processors in
common. Assume that initially processors 1, 3, and 5
are faulty. Trying to get permission to enter its critical section, a processor accessing quorum {1,2,3,4}
would get permission from {2, 4}, a set that intersects
every other quorum in a correct processor. Assume
that after this request is granted processors 2, 4, and
6 become faulty and that processors 1, 3, and 5 recover. Now, another processor trying to access its
critical section might be granted permission by the
set {1, 3], a set that intersects every other quorum.
If the alternative definition of availability is adopted,
we will have two processors in the critical section.
Nevertheless, the alternative definition can be used if
failures are not temporary. To access a quorum in a
synchronous system, a processor sends a request and
waits to receive replies or timeouts for all elements of
a quorum set3. If the set of processors that replied
intersects every other quorum, then the replies can be
used to coordinate.
This is because processors that
have failed will not recover later and put the system
in an inconsistent state. This is the case if failures
are arbitrary.

s~mmut~

261

are not pOSSible in

asynchronous sYsterns

Byzantine Quorums

4

Malkhi and Reiter [10] gave a general definition of
quorum systems that can tolerate Byzantine failures
in asynchronous
systems. We will first discuss their
definition, then we propose a less restrictive definition
that works in synchronous systems.

that some response is received. Sending requests to
every processor limits the efficiency of the use of quorums in asynchronous systems. Sending requests to
every processor is not needed in synchronous systems
if no failures occur. In what follows, we will refer
to Byzantine quorums for asynchronous systems as
asynchronous Byzantine quorum systems.

4.1

4.2

Byzantine
chronous

Quorums
Systems

in

Asyn-

If failures are arbitrary, a processor might receive conflicting replies from faulty and correct processors. It
follows that a processor must base its coordination
decisions on replies that it knows to be from correct
Motivated by this requirement,
Malkhi
processors.
and Reiter gave the following definition [10]:
Definition
ter-m .F if

2. VFe

F3Qe

Q VI’1, F2 EF:(Qln

Qz)–

Quorums
Systems

in

Syn-

We should first point to an important distinction between quorum systems that tolerate Byzantine failures and those that tolerate crash failures. If failures are benign, then the availability property can
be checked a posterior:
a client can determine if a
quorum system is available or not. This ability of
determining
the availability of the quorum system is
not possible if failures are Byzantine.
If many failures occur and the system is not available, then a
client cannot determine that fact because, in general,
there is no way to distinguish a correct processor from
a faulty one. It follows that the availability condition of a Byzantine quorum should hold a priori. A
Byzantine quorum system designed for a given failure pattern cannot dynamically
adapt if the failures
are different from those specified by the pattern it is
designed to tolerate.
From the discussion above and in the previous section, we conclude that only the first of the two conditions proposed by [10] is needed for Byzantine quorums in synchronous systems.

7: A quorum system tolerates failure pat-

1. VQ1, Q2C

Byzantine
chronous

F’l ZF2.

Q: FnQ=fl

The first condition requires that the intersection of
two quorums is not contained in the intersection of
two sets in 7. This will guarantee that the replies of
correct processors can be identified . To see why this
is the case, consider, as an example, the ~-threshold
failure pattern. For that pattern, the condition reduces to a familiar-looking condition:
QI n Q2 ~
2~ + 1. So, it is always possible to have ~ + 1 identical replies in the intersection of two quorums. One
of these replies must be the reply of a correct processor. It should not be surprising that, even though
the failures are arbitrary, only a simple majority of
correct processors (as opposed to a two-third majority) is required in the intersection. This is due to the
fact that the clients are assumed to be correct. The
second condition is based on the traditional definition of availability; it requires that some quorum consists of correct processors. As we mentioned above,
this definition of availability is designed for a failure
model in which faulty processor can recover, which
is not the case for arbitrary failures. Nevertheless, in
asynchronous systems, the second condition is needed
because there is no way to differentiate a slow processor from a faulty one. To access a quorum in
an asynchronous system, a processor cannot simply
send requests to all processors in a quorum set and
waits for replies. In the worst case, in a failure-free
execution, a processor might have to send requests
to every processor in the system and then waits for
replies from a quorum that consists of correct processor. The availability condition is needed to guarantee

8: For Bgzantine failures,
tem tolerates failure pattern .F if:

Definition

a quorum sys-

Note that this definition, which is identical to the first
condition of Definition 7, requires that the failure pattern be known a priori. It is enough to guarantee that
the quorum system can handle the worst-case failure
scenario. That’s why we do not have to require an extra condition for availability. Note that a synchronous
Byzantine quorum system is not necessarily an asynchronous Byzantine quorums system. On the other
hand, an asynchronous Byzantine quorum system is
also a synchronous Byzantine quorum system.
To see how a quorum satisfying Definition 8 can
be used to coordinate in the presence of Byzantine
failures, we consider the implementation of a sequentially consistent replicated single bit register that can
be set to 1 only once and whose initial value is O. To
set the register, a client sends a wn”te message to every processor in a quorum. On receipt of the wm”te

262

Proposition

message. a server will change its local copy of the register to 1. To read the value of the register, a client
sends a message requesting the value from every processor in a quorum.
If the client receives identical
replies from a set of processors that is not a subset
of a faulty set, then it reads the value in the identical
replies, otherwise it nmds O. For a sketch of the proof
of correctness, we note that if a write precedes a read,
then the read will return 1 because the intersection
of two quorums cannot be covered by two faulty set.
Also, there are not enough faulty processors in the
intersection of two quorums to make a read return 1
if it is not preceded by a write. Note that the implementation still works even if every quorum has some
faulty processors (system is not available according
to the traditional definition of availability).
This is
due to the fact that Definition 8 a prioti handles the
worst failure scenario assumed in the system.
Given a failure pattern, we are interested in deciding whether there exists a quorum system that tolerates the failure pattern. The following two propositions give necessary and sufficient conditions.

5.1

that
F if and only if Q = {P}

of Proposition

9.

}.

Case
a new form for Definition

VQ1, Q2E

(Ql n(~

8.

Q VF1,FZEF:

- (Fl uW)n(Q2n(p

- (FI uF2))

Uses standard manipulations

Proof:
tors.

#

0

on set opera■

In other words, if Q tolerates 3, then the projection
of Q into P - (Fl U F2) is a quorum system on p –
(Flu F2), where F, and F’z are elements of F.

Let Q-( F1u F2)={Q-(F1u

F2):

QE Q}.

be the projection of Q on P – (Fl U 1’2). The following two propositions follow from the application of
Proposition 11 to Q – (Fa U Fj ) for any two elements
Fi and Fj of F.

●

It follows that there exists a quorum system that
tolerates the f-threshold
failure pattern if and only if
n ~ 2$ + 1. This necessary and sufficient condition
is to be contrasted
with the requirement
that n z
4f + 1 for the existence of a Byzantine quorum for
asynchronous
systems [10],

Proposition 13: Let
tem

that

Z(Q)

~ max{c(~-(~u~’)~,

tolerates

Q

be

failure

a quorum
pattern
F,

,[~_(~iuFj)J}

systhen
for any Fi

and Fj.

Proposition 14: Let

5

&

Proposition
12: Let Q be a quorum s~stem that tolcrates failure pattern 3, then

Proposition
10: There exists a quorum s~stem that
tolemte failure pattern .F if and onl~ if VFl, F2 E .F :
P#Flu
F2.
Direct application

General

We first present

Proof :
If Q = {p} tolerates
F, then there
exists a quorum system that tolerates 7.
If there
exists a quorum system Q that tolerates
3, then
there exists a quorum in Q that cannot be contained in the union of any two elements
of f,
It follows that P cannot is not equal to the union
of any two elements in F and that {P} tolerates 7. ■

Proof:

~ max{~,

This lower bound obviously applies to Byzantine quorum systems because they are also quorum systems.
Malkhi and Reiter [10] proved the same lower bounds
with more direct methods, but did not prove any
new lower bounds for Byzantine quorums,4 In this
section, we prove new lower bounds that are specific
to Byzantine quorums in synchronous systems. The
lower bounds we prove also apply to asynchronous
Byzantine quorums [10]. We prove lower bounds for
a general failure pattern, the j-threshold
failure pattern, and the disjoint failure pattern.
The bounds
we prove for the ~-threshold
failure pattern and the
disjoint failure pattern
are tight.
In Section 6, we
present quorum constructions
whose loads match the
bounds for the ~-threshold
failure pattern
and the
disjoint failure pattern.

Proposition 9: There exists a quorum system
tolerates failure pattern
tolerates F.

11: L(Q)

Load Lower Bounds

tem
L(Q)

Let C(Q) be the size of the smallest quorum of Q.
Naor and WO01[15] proved the following lower bound
on the load of a quorum system.

that
~

tolemtes

Q

be

failure

a
quorum
pattern
3,

systhen

1
~–m=i,j=l,...,m

lFiuFjl “

41n ill], M~khi et ~, prove some lower bounds similar to
oum.

263

5.2

Disjoint

Applying Proposition
F~, we obtain:

Load Upper Bounds

6

Failure Pattern
13 to a disjoint failure pattern

In this section, we present synchronous quorum constructions that have loads that match the lower
bounds proved in the previous section.

Proposition
15: Let Q be a quorum system that
tolerates ~d = {Fl,. . . . Fm } and assume that the
Fi’s are sorted in a decreasing order of size, then
.C(Q) >

6.1

-“

Threshold

Failure

Pattern

In this section, we prove that the lower bound developed for the threshold failure pattern is tight.

Proof:
Since the sets are disjoint, the cardinality
of the union of any two distinct sets is equal to the
■
sum of the cardinalities of the two sets.

Proposition
rums system

There

18:

exists

whose load is

r

a synchronous

quo-

w~

In this section, we assume that up to f processors
can be faulty. The following lower bound holds for
dissemination quorum systems.

We construct a system whose load matches
Proof:
the lower bound.
Consider the following system,
which we call the parallel finite projective planes. For
simplicity, we assume that n is divisible by 2j + 1
and that ~
=k2+k+l
forsomek=
piand

Proposition 16: Let Q be a B~zantine

some prime number p.

5.3

Threshold

spstem

Proof:

Failure Pattern

that tolerates

the f-threshold

By Proposition

only need to prove that

13, L(Q)
L(Q)

quorum

pattern,

z

~.

2 ~.

So, we

Let w be any

strategy for the quorum system Q. Fix Q1 E Q such
that IQ1 I = C(Q). Summing the loads induced by w
on all the elements of Q1, we obtain:

each quorum is N
UEQ,

u~Q1 Q<3u

Proof:

We have

two

C(Q) ~ i--,

L(Q)

C(Q) < i-,

L(Q)

w.
T n

cases

to

> ~
z

= /m

~
T
the lower bound for the ~-threshold
we obtain a load of ~

derstood as follows. Each quorum Qi puts a load of
WI on each element it has in common with Q1. Since
it has 2f + 1 elements in common with Q1, it follows
that it contributes a load of (2f + l)~i to the sum of
the loads of all elements of Q1. The contributions of
all quorums to the sum of the loads of elements in Q1
= ‘2f + 1.
is ~QifW1
●
Note that for ~ = O, Proposition
16 reduces to Proposition 13. The following lower bound is a corollary of
Proposition 16.
17: L(Q) ~ ~~.

(2f + l)-

and

each processor belongs to exactly k + 1 quorums.
If
we choose quorums with probability
~
= ~Z+k+l
1
t

Q, ueQ, nQ,
Qi
It follows that there exists an element in QI that suffers a load of ~.
The calculation above can be un-

Corollary

It follows that k N

r *“
Divide the processors into 2 f + 1 identical sets.
On each of the sets, define a finite projective plane
system consisting of ~
quorums numbered from
ltok+l
[9]. Each quorum consists of k + 1
processors and each processor belongs to exactly
k + 1 quorums [9]. A quorum of the parallel finite
projective planes is the union of 2f + 1 identically
numbered quorums, one from each set. The size of

then

~

consider.

If

z

If

z

r ~.

Corollary

19:

which matches

z

pattern.

The bound of Corolla~

■

17 is tight.

Note that the parallel finite projective plane system
is also an asynchronous Byzantine quorum system. It
follows that the bound of Corollary 17 is also tight for
asynchronous Byzantine quorum systems.

6.2

Disjoint

Failure Pattern

In this section, we prove that the lower bound developed for the disjoint failure pattern is tight by
constructing
a system whose load matches the lower
F~ }
bound of Proposition
15. Let f = {Fl, Fz,...,
be the set of failure sets ordered in decreasing size.
Let a = n – (IF1 [+ IF21). By Proposition 15, the load
of a Byzantine quorum that tolerates F is greater
than -& .

*=

So, in all cases, L(Q) z ~.

Define ml and m2 as follows:

264

●

ml = min{’j : 1~1 U U3<~<j+1 ~kl ~ ~}
--

Note that j and k are bound variables in the definitions of ml and m2. Also, ml and m2 are always
guaranteed to exist.
Now, define the three sets So, S1 and S2 as follows:

● S2 =

Uma<k

F~ = P – (SI

US2)

Note that if F1 z ~, then So = F1. Also, if F2 z ~,
then S1 = F2

Proposition 20: SO,S1 and Sz are disjoint.
Prwof :
Follows immediately
from the definition of
SO, S1 and Sz and the fact that 3 consists of disjoint
●
sets.

to be the union of three sets: one from the triangle
lattice system on So, another from the triangle lattice
system on S1 and the third from the triangle lattice
system on Sz.
Proposition

22:

system tolerates F.

Proof: In fact, any two quorums intersect in three
points that belong to three distinct elements of 7
and, therefore, the intersection of two quorums does
not, belong to the union of two faulty sets.
■
Proposition
constructed

23: The load of the quorum
above is 0(~).

system

Proof:
On each Si, we can choose m
quorums
such that each processor
belongs to exactly two
quorums.
It follows that there are ~81SollSlllS21
quorums
on P corresponding
to all combinations
of these quorums.
Each processor in Si belongs to
2 ~41S[i_ ~)mOd31lS(i+l)mOd3T. It follows that the load
on a processor

Proposition 21:

The nmdting

in S~ is

‘2

41s(i-l)mods

lls(i+l)modSl

~“
S~ 2 ~ @

i = 0,1,2.
fi’G=e=@

Proof :
There are three cases to consider depending
on the size of F1 and F2.

Corollary

24:

The bound

of Proposition

15 is tight.

Note that the system presented in this section
not an asynchronous
Byzantine quorum system.

•F1~~andF2<f.
We first note that in this
case, SO = F1 and ISOI = IF11. We know that
ISI U Fm,+ll

2 ; and ISII <$.

AIso,

Wl

2

[F~,+l I because the q’s are sorted in decreasing
size. It follows that 21Sl I z
Finally, ISZI = IP–(SO+S1)I

~ and IS1 I > %.

2 n–(lFIl+~)

7

2

a–;>;.
.Fl<fmdFz<~.

The proof is similar to the
proof of the second case and is omitted.

Note that we do not consider F1 < $ and F2 ~ ~
■
because IF1 I ~ IF21. This completes the proof.

General
Construction
Byzantine Quorums

is

of

The results of the previous section suggest a simple
general way to construct
Byzantine
quorums.
The
construction
yields asynchronous
Byzantine quorum
systems.
Let Q be a quorum system on a set P of n processors. A Byzantine quorum on a set P’ of n(2~+l)
processors can be obtained as follows. Partition P’ into
2~ + 1 sets of n processors each. For each set Pi of the
partition, let pi be a bijection between P and Pi. Define Qi = LJ (Q). It is clear that Qt is a quorum sys-

u

Now, we describe the quorum system. On S;, i E
{O, 1, 2}, define a quorum system with load 0(~).

tem on P1. Let Q’ = {QIQ =

Many such systems exist.
One such system \S the
triangle lattice system [2]. In the triangle lattice system over Si, we can choose m
quorums such that
each processor belongs to exactly two quorums. If we
choose each quorum with a probability ~,
it fol\,
lows that the load of the triangle lattic~ system on

It is easy to show that Q’ is a Byzantine quorum system on P’ that can tolerate up to j arbitrary failures.
Also, it is easy to show that the resulting quorum haa
a load equal to the load of Q (note that the resulting quorum is on a set whose size is 2j + 1 times the
size of Q). Even though the method is simple, we
have seen that it can be used to construct Byzantine
quorum systems with optimal load.

‘i ‘s at ‘ost

&

= r

~.

Define a quorum

on P

Qji }.

Qji CQi,i=l,...’2f+l

265

8

Acknowledgements

[13] M. L. Neilsen Quorum Structures in Distributed Systems. Ph.D. Thesis, Department of Computer and
Information Sciences, Kansas State University, 1992.

I would like to thank Dahlia Malkhi for many clarifications and helpful discussions. I would also like to
thank the reviewers for many useful suggestions.

[14] M. L. Neilsen and M. Mizuno. Decentralized Consensus Protocols In Proceedings of 10th International
Phoenix Conference on Computing and Communtea-

tions, pages 257-262, 1991.

References

[15] M. Naor and A. Wool The Load, capacity and availability of quorum systems. In Proceedings of the 35th
IEEE Symposium on Foundations of Computer Science, pages 214–225. 1994.

[1] D. Agrawal and A. E1-Abbadi. An efficient and faulttolerant solution for distributed mutual exclusion.
ACM fiansactions
on Computer
Systems,
9(l): 120,1991.

[16] D. Peleg and A. Wool. How to bean
[2] R. A. Bazzi.

Planar Quorums
In Proceedings of
the 10th International Workshop on Distributed Algorithms, pages 251-268, October 1996.

or the

Probe

Complexity

of Quorum

Efficient

Snoop,

Systems.

In

Proceedings of the 15th ACM Symposium on Principles of Distributed Computing, pages 290-299, 1996.

[3] H. Garcia-Molina

[17] D. Peleg and A. Wool. The availabilityy of quorum
systems. Information and Computation, 123(2):210223, 1995.

[4] D. K. Gifford. Weighted Voting for Replicated

[18] D. Peleg and A. Wool. Crumbling Walls: A class of
high availability quorum systems. In Processedings

and D. Barbara. How to assign
votes in a distributed system. Journal of the ACM,
32(4):481-860, 1985.
Data
Proceeding of 7th ACM Symposium on Operating
Systems Principles, pages 150-162, December 1979.

l~th ACM Symposium on Principles of Distributed

Computing, pages 120-129, 1995.

[5] M. P. Herlihy.

Replication Methods for Abstract
Data fipes. Ph.D. Thesis, Massachusetts Institute
of Technology, 1984.

[19] D. Skeen.

A quorum-based

commit protocol.

In

Proceedings of 6th Berkeley Workshop on Distributed
Data Management and Computer Networks, pages

69-80, 1982.

[6] T. Ibaraki

and T. Kameda. A theory of Coteries:
Mutual Exclusion in Distributed Systems.
IEEE
Transactions on Parallel and Distributed Systems,
4(7):779-749,1993.

[7] A. Kumar.

Hierarchical quorum consensus:
A
new algorithm for managing replicated data. IEEE
Transactions of Computers, 40(9):996-1004,1991.

[8] A. Kumar.,

M. Rabinovich, and R. Sinha. A performance study of general grid structures for replicated data. In Proceedings of International Conference on Distributed Computing Systems, pages 178185, May, 1993.

[9] M. Maekawa.

A @ algorithm for mutual exclusion in decentralized systems. ACM Transactions
on Computer Systems, 3(2):145–159,1985.

[10] D. Malkhi and M. Reiter. Byzantine Quorum Systems. In Proceedings of ACM 29th Symposium on
Theory of Computing, May, 1997.
[11] D. Malkhi, M. Reiter, and A. Wool. The load and
availability of Byzantine quorum systems. In Proceedings of the Sizteenth ACM Symposium on Pn”nciples of Distributed Computing, August, 1997.
[12] S. J. Mullender and P. M. B. Vitanyi. Distributed
Match Making. Aigorithmica, 3:367--391, 1992,

266

Brief Announcement: On the Feasibility of Leader Election
and Shape Formation with Self-Organizing Programmable
Matter
∗

Zahra Derakhshandeh†

Robert Gmyr‡

Thim Strothmann‡

Arizona State University, USA

University of Paderborn,
Germany

University of Paderborn,
Germany

Rida Bazzi

gmyr@mail.upb.de
Andréa W. Richa†

thim@mail.upb.de
Christian Scheideler‡

Arizona State University, USA

Arizona State University, USA

bazzi@asu.edu

aricha@asu.edu

University of Paderborn,
Germany

zderakhs@asu.edu

ABSTRACT

scheideler@.upb.de

We also consider a geometric variant of the amoebot model by
restricting the particle structures to form a connected subset on a
triangular grid. For these structures we can show that there are
local-control protocols for the leader election problem and the line
formation problem. The protocols can also be adapted to other regular geometric structures demonstrating that it is advisable to restrict particle structures to such structures.

Imagine that we had a piece of matter that can change its physical properties like shape, density, conductivity, or color in a programmable fashion based on either user input or autonomous sensing. This is the vision behind what is commonly known as programmable matter. Many proposals have already been made for
realizing programmable matter, ranging from DNA tiles, shapechanging molecules, and cells created via synthetic biology to reconfigurable modular robotics. We are particularly interested in
programmable matter consisting of simple elements called particles that can compute, bond, and move, and the feasibility of solving fundamental problems relevant for programmable matter with
these particles. As a model for that programmable matter, we will
use a general form of the amoebot model first proposed in SPAA
2014, and as examples of fundamental problems we will focus on
leader election and shape formation. For shape formation, we investigate the line formation problem, i.e. we are searching for a
local-control protocol so that for any connected structure of particles, the particles will eventually form a line.
Prior results on leader election imply that in the general amoebot
model there are instances in which leader election cannot be solved
by local-control protocols. Additionally, we can show that if there
is a local-control protocol that solves the line formation problem,
then there is also a protocol that solves the leader election problem, which implies that in the general amoebot model also the line
formation problem cannot be solved by a local-control protocol.

Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed Systems; F.2.0 [Analysis of Algorithms and Problem Complexity]:
General

1.

INTRODUCTION

There have been many conceptions of programmable matter, and
thus many avenues of research using that name, each pursuing solutions for specific application scenarios with their own, special
capabilities and constraints. However, most of the research in this
area has been done by scientists from the natural sciences and the
robotics area and not so much from theoretical computer science,
so not much is known yet about which assumptions or primitives
allow the development of distributed algorithms that can solve relevant problems for programmable matter in a self-organizing way.
Notable exceptions are DNA tile assembly and population protocols. However, most of these approaches are passive in one or
more aspects or require powerful entities while we are interested
in studying the feasibility of solving problems with simple entities
that can not just compute but also control their movements. For that
we use a general form of the amoebot model first proposed in [2].

∗
A full paper presenting the results outlined in this brief announcement will appear at the 21st International Conference on DNA
Computing and Molecular Programming (DNA21).
†
This work was supported in part by the NSF under Awards CCF1353089 and CCF-1422603.
‡
Supported in part by DFG grant SCHE 1592/3-1

1.1

The general amoebot model

In the general amoebot model, programmable matter consists of
a set of simple uniform computational units called particles that can
move and bond to other particles and use these bonds to exchange
information. The particles have very limited memory, act asynchronously, and they achieve locomotion by expanding and contracting, which resembles the behavior of amoeba.
As a base for our amoebot model, we assume that we have a
set of particles that aim at maintaining a connected structure at all
times. This is needed to prevent the particles from drifting apart in
an uncontrolled manner like in fluids and because in our case particles communicate only via bonds. The shape and positions of the

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
PODC’15, July 21–23, 2015, Donostia-San Sebastián, Spain.
c 2015 ACM 978-1-4503-3617-8 /15/07.
Copyright 
http://dx.doi.org/10.1145/2767386.2767451.

67

2.

bonds of the particles mandate that they can only assume discrete
positions in the particle structure. This justifies the use of a possibly infinite, undirected graph G = (V, E), where V represents all
possible positions of a particle (relative to the other particles in their
structure) and E equals all possible transitions between positions.
Each particle occupies either a single node or a pair of adjacent
nodes in G, and every node can be occupied by at most one particle. Two particles occupying adjacent nodes are connected, and we
refer to such particles as neighbors. Particles are anonymous but
the bonds of each particle have unique labels, which implies that
a particle can uniquely identify each of its outgoing edges. Each
particle has a local memory in which it can store some bounded
amount of information, and any pair of connected particles has a
bounded shared memory that can be read and written by both of
them and that can be accessed using the edge label associated with
that connection.
Particles move through expansions and contractions: If a particle occupies one node (i.e., it is contracted), it can expand to an
unoccupied adjacent node to occupy two nodes. If a particle occupies two nodes (i.e., it is expanded), it can contract to one of
these nodes to occupy only a single node. Performing movements
via expansions and contractions has various advantages. For example, it would easily allow a particle to abort a movement if its
movement is in conflict with other movements. A particle always
knows whether it is contracted or expanded — in the latter case,
it also knows along which edge it is expanded — and this information will be available to neighboring particles. In a handover,
two scenarios are possible: a) a contracted particle p can "push"
a neighboring expanded particle q and expand into the neighboring node previously occupied by q, forcing q to contract, or b) an
expanded particle p can "pull" a neighboring contracted particle q
to a cell occupied by it thereby expanding that particle to that cell,
which allows p to contract to its other cell. The ability to use a
handover allows the system to stay connected while particles move
(e.g., for particles moving in a worm-like fashion). Note that while
expansions and contractions may represent the way particles physically move in space (resembling loosely the movement of amoeba),
they can also be interpreted as a particle "looking ahead" and establishing new connections (by expanding) before it fully moves to a
new position and severs the old connections it had (by contracting).
In Figure 1, we illustrate a set of particles (some contracted, some
expanded) using the infinite regular triangular grid graph Geqt as
the underlying graph G .

LEADER ELECTION AND LINE FORMATION IN THE GENERAL AMOEBOT
MODEL

Here we focus on the feasibility of leader election and shape formation for particles with constant memory. For the shape formation, we just focus on forming a line of particles, that is, we are
searching for a local-control algorithm such that for any initial set
A ⊆ V of positions occupied by particles where G|A (i.e., the
subgraph of G induced by A) is connected, the particles will eventually rearrange themselves into a line without losing connectivity.
Given that we do not assume any underlying geometric information regarding G, by forming a line, we mean that for the final set
of occupied positions A0 , G|A0 forms an arbitrary simple path.
Suppose that there is a protocol for the line formation problem in
the general amoebot model. Then it is easy to extend it to a leader
election protocol: once the line has been formed, its two endpoints
contend for leadership using tokens with random bits sent back and
forth. On the other hand, one can deduce from [3] that in the general amoebot model there is no local-control protocol for leader
election. Hence, also line formation cannot be solved. So additional assumptions are needed to solve these.

3.

LEADER ELECTION IN THE GEOMETRIC AMOEBOT MODEL

In this section we assume that G is equal to Geqt (see Figure 1)
and that the edges are labeled in a consecutive way in clockwise
direction so that every particle has the same sense of clockwise
direction, but the labelings do not have to be consistent. So the
particles may not have a common sense of orientation. However,
our model allows a particle to maintain a fixed orientation as it
moves because in an expanded state it knows along which edge it
expands from both sides.
Let A ⊆ V be any initial distribution of contracted particles such
that Geqt |A is connected. Consider the graph Geqt |V \A induced by
the unoccupied nodes in Geqt . We call a connected component of
Geqt |V \A an empty region. Let N (R) be the neighborhood of an
empty region R in Geqt . Then all nodes in N (R) are occupied and
we call the graph Geqt |N (R) a border. Since Geqt |A is a connected
finite graph, exactly one empty region has infinite size while the
remaining empty regions have finite size. We define the border corresponding to the infinite empty region to be the unique outer border and refer to a border that corresponds to a finite empty region
as an inner border, see Figure 1. The common sense of clockwise
rotation can be used to give each border a unique orientation.
The leader election algorithm operates independently on each
border. Due to the nature of Geqt a particle can be adjacent to at
most three different empty regions for each region a particle simulates one agent. For simplicity and ease of presentation we assume
for now that agents have a global view of the border they are part
of and that agents act synchronously. At any given time, some subset of agents on a border will consider themselves candidates, i.e.
potential leaders of the system. Initially, every agent considers itself a candidate. Between any two candidates on a border there is
a (possibly empty) sequence of non-candidate agents. We call such
a sequence a segment and specifically refer to the segment coming
after (before) a candidate c in the direction of the border as the front
segment (back segment) of c. We denote the lengths of those segments as |f s(c)| and |bs(c)|. We use front candidate (f c(c)) and
back candidate (bc(c)) to denote the candidates at the end of these
segments. We drop the c in parentheses if it is clear from the context. We define the distance d(c1 , c2 ) between candidates c1 and

Figure 1: The left part shows an example of a particle structure
in Geqt . A contracted particle is depicted as a black dot, and an
expanded particle is depicted as two black dots connected by an
edge. The right part shows a particle structure with 3 borders.
The outer border is solid and the two inner borders are dashed.
We assume the standard asynchronous computation model, i.e.,
only one particle can be active at a time. Whenever a particle is
active, it can perform an arbitrary bounded amount of computation
(involving its local memory as well as the shared memories with
its neighboring particles) followed by no or a single movement. A
round is over once every particle has been activated at least once.

68

c2 as the number of agents between c1 and c2 when going from c1
to c2 in the direction of the border. We say a candidate c1 covers a
candidate c2 (or c2 is covered by c1 ) if |f s(c1 )| > d(c2 , c1 ). The
leader election progresses in phases. In each phase, each candidate
executes Algorithm 1. A phase consists of three synchronized subphases, i.e., agents can only progress to the next subphase once all
agents have finished the current subphase.

ticle simply joins the line. Whenever the line cannot be extended
any more because of an empty spot, particles are drawn to that spot
using the following idea:
All particles connected to the line that are not yet part of it, and
have an empty spot in the direction in which the line needs to grow,
form the root of a tree of particles so that all particles belong to
one of these trees. Every root tries to travel along the line in the
direction in which it is grown. If this is not possible any more
because the next spot is already occupied by an particle, it joins
the tree of that particle. Otherwise, it moves forward on top of
the line and drags the particles of its tree with it in a worm-like
fashion (which can be realized with the handover operation). This
guarantees that eventually all particles end up being part of the line.

Algorithm 1 Leader Election in the Geometric Amoebot Model
Subphase 1:
pos ← position of f c
if covered by any candidate or |f s| < |bs| then
return not leader

5.

Subphase 2:
if coin flip results in heads then
transfer candidacy to agent at pos
Subphase 3:
if only candidate on border then
if outside border then
return leader
else
return not leader
In the first subphase candidates are eliminated deterministically.
In the second subphase candidates that survived the first subphase
are eliminated in a randomized fashion by transferring candidacy
to another agent. This transferral of candidacy means that c withdraws its own candidacy but at the same time promotes the agent
at position pos (i.e., the front candidate of c in subphase 1) to be
a candidate. Subphase 1 and 2 make sure that eventually there is
only one leader on each border. Since we assume that each particle
has a global view, it can detect whether it is on the outer or an inner
border. Hence, the algorithm will elect a unique leader.
Algorithm 1 can be implemented entirely as a local-control protocol. This implementation heavily relies on token passing. Since a
detailed description of the protocol cannot be presented in this brief
announcement, we focus on the local-control protocol for one part
of the algorithm: the test whether the last remaining candidate of
a border is on the outer border or an inner border. We can achieve
this as follows. The candidate sends a token along its border that
keeps track of the sum of angles the path takes with respect to the
orientation of the candidate. Once the token manages to return to
its origin (the candidate itself), the sum of angles stored in the token is inspected. It turns out that it is always 360◦ for the outer
border and −360◦ for an inner border. Since we work on Geqt , we
just need to count integer multiples of 60◦ so that instead of the
actual angle we can simply store an integer k in the token such that
the angle is k · 60◦ . Note that the range of k during the traversal
of the path is unbounded. However, since we are only interested in
whether the value of k is 6 or −6 once the token returns to its origin, we can use a counter modulo 5 so that for the outer border we
get k = 1 and for an inner border k = 4. This way, the token only
needs 3 bits of memory independently of the shape of the border.

4.

SELF-STABILIZING ALGORITHMS

The leader election algorithm for Geqt can be extended to a selfstabilizing leader election algorithm with O(log∗ n) memory using
the results of [1, 4] (i.e., we use their self-stabilizing reset algorithm
on every border in order to recover from failure states). However,
it is not possible to design a self-stabilizing algorithm for the line
formation problem. The reason for this is that even a much simpler
problem we call the movement problem cannot be solved in a selfstabilizing manner.
In the movement problem we are given an initial distribution A
of particles that can be in a contracted as well as expanded state,
and the goal is to change the set of nodes occupied by the particles
without causing disconnectivity. For the ring of expanded particles
it holds that for any protocol P there is an initial state so that P
does not solve the movement problem. To show this we essentially
consider two cases: suppose that there is any state s for some particle in the ring that would cause that particle to contract. In this
case set two particles on opposite sides of the ring to that state, and
the ring will break due to their contractions. Otherwise, P would
not move any particle of the ring, so also in this case it would not
solve the movement problem. It is easy to see that if the movement
problem cannot be solved in a self-stabilizing manner, then also
the line formation problem cannot be solved in a self-stabilizing
manner. Hence, while self-stabilizing leader election is possible,
self-stabilizing line formation is not possible, which also implies
that it is not possible for the particles to decide in this setting when
a leader has been successfully chosen.

6.

REFERENCES

[1] Baruch Awerbuch and Rafail Ostrovsky. Memory-efficient and
self-stabilizing network RESET (extended abstract). In 13th
Annual ACM Symposium on Principles of Distributed
Computing, Los Angeles, California, USA, August 14-17,
1994, pages 254–263, 1994.
[2] Zahra Derakhshandeh, Shlomi Dolev, Robert Gmyr,
Andréa W. Richa, Christian Scheideler, and Thim Strothmann.
Brief announcement: amoebot - a new model for
programmable matter. In 26th ACM Symposium on
Parallelism in Algorithms and Architectures, Prague, Czech
Republic - June 23 - 25, 2014, pages 220–222, 2014.
[3] Alon Itai and Michael Rodeh. Symmetry breaking in
distributive networks. In 22nd Annual Symposium on
Foundations of Computer Science, Nashville, Tennessee, USA,
28-30 October 1981, pages 150–158, 1981.
[4] Gene Itkis and Leonid A. Levin. Fast and lean self-stabilizing
asynchronous protocols. In 35th Annual Symposium on
Foundations of Computer Science, Santa Fe, New Mexico,
USA, 20-22 November 1994, pages 226–239, 1994.

LINE FORMATION IN THE GEOMETRIC AMOEBOT MODEL

Our line formation algorithm is based on the leader election strategy above. Once a leader has been determined, the leader is used
as a seed to arrange the particles in a straight line starting from that
leader. Every time the line can be extended by a particle, that par-

69

Heterogeneous Checkpointing for Multithreaded Applications *
Feras Karablieh and Rida A. Bazzi
Computer Science and Engineering
Arizona State University
Tempe, AZ 85287
{ feras.karablieh.bazzi} @asu.edu

Abstract
We present the first heterogeneous checkpointing scheme
for applications using POSlX threads. The scheme relies
on Source code instrumentation to achieve heterogeneity.
It supports various types of synchronization primitives.
such as locks. semaphores, condition variables, as well as
join operations. Unlike other non-heterogeneous checkpointing schemes proposed in the literature, our scheme
supports both kernel-level and application-level threads
executing as pan of the same application under various
scheduling policies. Also, unlike other non-heterogeneous
checkpointing mechanisms proposed in the literature, our
solution does not interfere with the semantics of the application and does not use signals. Test results on various
hardware platforms running Solaris. Linux. and Windows
NT show that the overhead of our scheme is low.

1 Introduction
Many researchers developed techniques and systems for
heterogeneous checkpointing [I,3.5.9, IO. 12. 131. Unfortunately. all previous systems for heterogeneous checkpointing, including our own system [ 5 ] , can only provide
checkpointing for single threaded programs. In this paper, we present the first system for heterogeneouscheckpointing of multithreaded programs. Our system builds
on our previous work and provides checkpointing support for C programs using POSlX threads [ 6 ] .but our approach is general enough to support heterogeneouscheckpointing for programs using other multithreading standards. As in our previous work on heterogeneous checkpointing, we rely on code instrumentation to achieveponability. In this approach, the sourre code is instrumented
to obtain a target code with functionality for high-level
'This m a t e d is bared upon work rupponed by the National Science Fovndvtran under Gnnt No. CCR-9876052.Any opinions. findings. and onc cl us ion^ or recommendations expmred in this malerial
are those of h e authors and do not necerranly reflect the views of the

..

2 Heterogeneous Checkpointing Issues
An essential requirement for checkpointingis to correctly
capture the program's state during execution. The state
of a single-threaded process consists of the global van-

Nutiam1 Science Foundadon.

1060-9857/02$17.00 Q 2002 IEEE

checkpointing and recovery. A checkpoint file produced
by targel on one platform can he used to restart target
on a different platform (target must be precompiled on
both platforms). We chose to support poslx because
the standard is widely implementedon UNIX, Linux. and
Microsoft Windows platforms.
In addition to being the first heterogeneouscheckpointing system for multithreaded programs. our system provides more functionalitv than other svstems that onlv movide homogeneous checkpointing for programs using POSlX
threads [Z, 141. Our system covers all the major categories of POSlX thread API including: thread management (creation and deletion). synchronization objects (locks,
semaphores, and condition variables), thread priorities.
and joining (one thread waiting for another thread to terminate)
Our system is truly heterogeneous because it does not
require any knowledge about the underlying architecture.
As long as the program being checkpointed is portable
across the platforms of interest. a checkpoint Ale produced on one machine can be used to restart the program
on a machine of a different architecture. We have successfully tested our system on Solaris running on Sun
Sparc architecture, as well as on Windows NT and Linux
running on Intel Pentium Architecture.
The rest of the paper is organized as follows. Section 2 discusses issues in checkpointing multithreaded
applications. Section 3 presents related work and summarizes our contributions. Section 4 presents the specific
details of checkpointing the various categories of POSlX
Threads API calls. Section 5 present our performance
results.

140

ables, stack, heap, register values, program counter and
stack pointer. In high-level checkpointing, the system
state consists of the values of the global, stack and heap
variables, the call sequence and the label of the next statement to execute ' .
Checkpointing multithreaded processes is more complicated than checkpointing single-threaded processes. The
state of the process at a given point in time is equal to the
local states of each thread (local variables and stack) as
well as the states of all synchronization variables such as
locks and semaphores and the states of direct synchronization such as the join operation. Capturing the state at
a given point in time is not straightforward because the
execution of one thread might overlap with the saving of
the local data of another thread and the set of all saved
local states might not constitute a consistent state. To
get around this difficulty. one can coordinate the saving
of all the states. In coordinated, or synchronous checkpointing, all threads are suspended before the checkpoint
is taken. This ensures that no computation overlaps with
data saving.' So. the mechanism to suspend the execution of threads is an important aspect of checkpointing
multithreaded process
In low-level checkpointing schemes, the thread that
initiates checkpointing does not have access to the local stacks of other threads and each thread needs to save
its own stack. In order to allow each thread to save its
own stack. all threads that are hlockedon synchronization
variables must be released when a checkpoint is taken. In
the approach of [21, the thread that initiates a checkpoint
sends a SlGUSRl signal to other threads to force them
to stop execution and participate in checkpointing. When
a thread receives a signal, it executes a signal handler to
save its local stack. Signaling threads that are blocked
on synchronization variables can lead to errors because
a thread is not guaranteed to remain locked after it returns from the signal handler. To prevent such behavior.
all threads mask SlGUSRl before attempting to aquire a
lock and unmask it if they are successful in acquiring the
lock. In [21, the thread that initiates a checkpoint sets a
Rag to prevent locking during a checkpoint. All threads
check the flag before attempting to lock a mutex. When a
thread that is not blocked receives a signal. i t executes a
signal handler that releases all the locks (if any) that the
thread owns. This enables other threads blocked on these
locks to he freed and participate in checkpointing.
In the absence of code instrumentation, signals seem
'This mles OUL 11a~emen15hac include muliiple functios cnlli.
While we do not handle such statements. they c m be handled by rplitling them in10 mulliplr s~oterncnls.rnch with no more than one funclion
call
Whdc there arc techniques for asynchronous checkpoinring. we do
no! discuss them in this paper.

141

necessary to interrupt the execution of running threads.
The approach of [21 cannot he used in the context of heterogeneous checkpointing because signals are not available on all opsrating systems and they cannot be used to
support checkpointing across different operating system
platforms. Also, the approach of [2] cannot be used to
checkpoint a thread joining on another thread. Since the
only way to release a joining thread is to terminate the
joined thread (assuming the joining thread is blocked),
checkpointing might never terminate if the joined thread
is a continuously running thread. Also, attempting to
signal the joiiling thread is not guaranteed to work; in
some implementations signals are not delivered to joining threads.
High-level checkpointing and code instrumentation allow us to have a different solution that does not suffer
from the drawbacks of a solution that require each thread
to participate in checkpointing. In our approach, the thread
that initiates the checkpoint is responsible for saving the
stacks of all the threads in the application. Before checkpointing, the thread that initiates the checkpoint (initiai o r for shon) must ensure that no data is modified during checkpointing. To ensure that threads d o not execute
during checkpointing. the preprocessor generates cooperation code segments to force a thread to stop during execution. A cooperation code segment causes a thread to
stop its execuiion only if a checkpointing Rag is set: otherwise, the cooperation code has no effect. The cooperation code segments are inserted throughout the program.
one in each basic block of the program. The overhead
of the cooperation code during normal execution is minimal. Unfonunately,not all threads are guaranteed to execute the cooperation code because some threads might be
blocked on synchronization objects or waiting for other
threads to terminate. So, the initiator must determine
which threads are blocked. Determining which threads
are blocked is not straightforward because a thread that
is blocked when the initiator checks it state. might be unblocked just after the initiator finishes recording its state.
We distinguish between threads that are really blocked
and threads that are temporarily blocked during a checkpoint. To determine the threads that are really blocked,
the preprocessor adds code before and after any potentially blocking API call. The added code declares (using global flags) that the thread is tentatively blocked.
After the call to the API. the thread is declared as not
blocked. We devised an algorithm, described in detail
below, which allows a checkpoint coordinating thread to
determineforevery thread whether(l)it is really blocked
on an API and has no possibility of executing while checkpointing is done or (2) is blocked because it is caught in
coopermion code.
Both the approach of 121 and our approach, hut to a

-

uses UNIX signals to initiate checkpointing,

much lesser extent, suffer from one further drawback. In
the approach of 121, all blocked threads are released when
a checkpoint is taken. If the implementationof a synchronization variable is required to be fair or FIFO. releasing the blocked threads in order to participate in checkpointing can change the FIFO semantics of a synchronization object or can lead to starvation. While. synchronization objects are not guaranteed to have fair semantics,
an application on a given platform might take advantage
of the specific semantics of Pthreads for that platform.
Checkpointing should not affect those semantics. Unfortunately. in the approach of I21 the order of threads
waiting on a given object can be changed every time a
checkpoint is taken. In our approach, the semantics are
not affected when a checkpoint is taken because blocked
threads remain blocked during a checkpoint. Nevenheless. if an application is recovered from a checkpoint file.
there are no guarantees that the threads will wait on synchronization objects in the same order they had when the
checkpoint was taken.

requires that all threads he scheduled with systemwide contention scope.

does not suppon checkpointing for semaphores, or
joining threads. and
assumes that all threads execute with the same priority.
The system and techniques we present in this paper
provide heterogeneous checkpointing for multithreaded
applications and has the following advantages:

No signals are used to initiate checkpointing,
checkpointing is done without interrupting blocked
threads and thereby the semantics of the application are not affected due to checkpointing.
s suppons both system-wide and local contention thread

scheduling,

3 Contributions and Related Work

suppons various scheduling schemes and priorities.

Many researchers use recompilation andlor code instrumentation for heterogeneous checkpointing or migration
in heterogeneous systems [I, 3. 5 , 9. IO, 12. 131. The
systems that provide the most functionality are those described i n (5, 91,and [121. Ramkumar and Strumpen [9]
developed a preprocessor lo suppon heterogeneous checkpointing. Their system requires some knowledge about
the underlying architecture amongst some other limitations. Ssu and Fuchs [I21 developed a general heterogeneous checkpointing system that overlaps writing checkpoint files to disk with the application execution by sending checkpoints to a slave process that maintains calling
sequence information. Similarly to [9], their work requires knowledge of the underlying architecture in order
to translate pointer values between two architectures. In
our previous work (51, we designed a preprocessor thar
adds checkpointingand recovery functionality to portable
C programs. Unlike other work, our preprocessor does
not require any knowledge about the underlying architecture.
Researchers have studied the problem of chrckpointing multithreaded applications in homogeneous systems [Z,
141 In [Z].the authors present a system for homogeneous
checkpointing for programs using POSIX threads on UNlX
platforms. ,To our knowledge. that system provides the

suppons various synchronization variables.
In summary. the system we present has the most functionality amongst all systems for checkpointing multithreaded application, both homogeneous and heterogeneous. Finally. the performanceof our system is the same
or better than that of 121 for a range of applications.

4

System Design

The preprocessor generates various data structures to keep
track ofthe state of the different executing threads. These
structures are updated wheneverthe user accesses the POSIX
API. A large subset of the API calls is wrapped and calls
to the original API are replaced by calls to the corresponding wrappers. These wrappers are responsible for
updatingdifferent structures beforecalling the actual API.
We start by describing the various classes of API calls in
Pthreads then we describe how each class of API calls is
handled.

4.1

Pthreads API

~.

The POSIX thread librarv orovides the oroerammer
with
.~
a set of function calls used to simplify writing MultimostfunctionalityamongstcheckpointingsystemsforPOSlX threaded programs based on the lEEE POSIX 1003.1cthreads. Nevertheless, it has the following drawbacks.
1995 standard. The library is rich enough to cover most
of the issues related to threaded programming. The Pthreads
requires each thread to checkpoint its own stack,
API can he divided into the following categories:

142

Thread Management.
Mutex Synchronization,
Condition Variable Synchronization,
Thread Specific Storage,
Thread Cancellation, and
Signals.
Our work provides support for API calls in the first
three categories in addition to the semaphore synchronization API calls which are not part of the Fthreads library. Thread specific storage provides the programmer
with the ability to allocate global variables within athread.
We do not currently support checkpointing of thread specific data, but we note that they can be handled without
introducing any fundamental difficulty. Thread cancellation allows one thread to kill another thread. Providing
support for implementing cancellation involves wrapping
various cancellation API calls to clean several data structures to reflect the current system state. Finally the calls
in the signals category are not handled because programs
that use signals are not portable.

4.2

Thread Attributes

The main structure added by the preprocessor is the Mainruinertable. This table isimplementedasan array, which,
for simplicity and efficiency, is limited to Nrhreuds entries. Every entry in the table stores information about
the thread's state at time it is created. Such information
is needed for correct thread recovery. When the user creates a new thread using
Pthreads-createlpthread-t

'thread.

const pthread-attr-t
'attr,
v o i d * ( * s t a r t - r o u t i n e l , void 'argl;
The call is replaced by:
pt-createlint *thread,
const pthread-attr-t
'attr,
v o i d * ( * s t a r t - r o u t i n e l , v o i d 'argl

;

The function pr-creare() allocates an entry in Muinrainer and its index is used to reference the thread. The
.real thread identifier returned byprhrrudxreure(J is stored
in Mainrainer an used when needed as an argument to
POSIX API calls. During checkpointing, the index of a
thread is saved but its real identifier is not saved because
its value can change when the thread is restarted during
recovery. The attributes of a thread are read using the
POSIX ger.' API call and stored in Muinruiner.

Since the value ofarrr may change between the time a
thread is created and the time checkpointing is done. we
save the affr value as a part of the thread creation state.
The arrr parameter governs the execution behavior of the
created thread (scheduling attributes, detach state, stack
size, etc ...)
The address of srun.rourine is also saved. The same
start routine used during the creation of the thread must
be used during recovery. Unfortunately, the thread creation API is passed the physical addressof the srurr.rourine(J.
The physical address of the smrr.rourine is not portable
and cannot be used to restart a thread. We use indirection
to get around this problem.
The preprocessor generates code that creates a global
function pointer array fprr. The entries in the global array are initialized at the beginning of the the execution
to store the addresses of all the functions of the program.
When the address of a function is passed lo the thread
creation routine, the index of the function is stored in
Muinminer. During recovery, fprr is initialized with the
(new) addresses of all the functions and the index of the
srurr.rourine is used to get its physical address.
In addition to the start routine, a thread is passed a
number of arguments when it is created. These argument are encapsulated in the arg parameter that is passed
to pthreudrreare(). The address of arg is also kept in
Maintainer. During recovery, a thread is restarted with
the same set of arguments, but not necessarily having the
same set of values. In facr. after a thread is created and
passed arg, another thread may change urg. Thus when
the thread is restarted it would be passed a different set of
values. Restarting a thread with a different value for arg
is correct because when a thread is restarted, it should resume execution from the point where saving took place.
At that point the value of arg must be the value it had at
the time checkpointing occurred not the value it had at
the time the thread was created.
Thus Maintainer always contains the variables and attributes needed to restart all running threads. Thread attributes that are set at creation time may be changed during execution either by the thread itself or by other threads.
Examples of these attributes are scheduling parameters
and execution state (detached/undetached). All API calls
that modify these attributes are wrapped by the preprocessor so that whenever a change occurs, the new values
are saved in Mainruiner.
A thread can terminate by either explicitly calling the
prhreaddxir() function or implicitly exit when the function it is executing returns. In both cases we need to free
the memory allocated for the thread in Maintainer. We
do this by wrapping theprhreadxxir(J function to do the
necessary memory deallocation. Also, the preprocessor
insert the wrapped version ofprhreadxxir(J on all thread

143

termination paths where prhrendmxiri) is not explicitly
called.’
During checkpointing Maintainer is saved to stable
storage. During recovery, Maintainer is restored from
stable storage and is used to recreate the threads with the
same attributes it had when the checkpoint file was created.

4.3 System State
To capture the state of a multi-threaded process. we need
to be able to capture ( I ) variables global to all threads
(shared memory), (2) variables local to each thread. (3)
variables introduced by the preprocessor to maintain the
state, and (4) the execution state associated with each
thread (Program Counter and Calling Stack).
The preprocessor generates functions that save basic
types. These functions print the value of a variable of a
specific type in ASCII furmat to an output file. An output
file is associated with each thread in which it saves its
local variables and execution state; global variables are
saved in the output file corresponding to thread 0 or the
thread containing the main(J function.
Saving floating-point numbers is done by reading the
accuracy of the platform and saving with that accuracy
(see [ 5 ] for details). Compound types are handled by
functions that save the individual field of the type. These
fields can beeitherof basic types. in which case. the functions that save basic types are called, or can be of compound types, in which case, function (generated by the
preprocessor) that know how to save these other compound types are called. In our previous work [51 we give
a detailed description of how saving and restoring variables is done.
Types introduced by the POSIX library are handled
differently. Objects of these types govern the behavior
of the thread’s execution and they need to be saved for
correct recovery. Examples of these types are: plhread2
representing the thread’s ID pllireadxmrib2 representing
attributes passed to a thread when a thread is created. and
srrucrschedqarani representing scheduling related parameters. The values of such variables is accessed using
the interface provided by the POSIX library.
The ser.*(J and ger.*O groups of Pthread functions
can be used to recover and checkpoint these attributes.
During checkpointing the gel-*() functions are used to
read the attributes values. These values can be easily
saved because they are of basic types. During recovery,
a default object is created; the attributes values are read

from the checkpoint file and then set as the object’s new
attributes using the set-*() functions.
The preprocessor is responsible for generating functions that know how to save and restore the local variables of every function. The local variables of all functions on the call stack of each thread are saved during
checkpointing. An essential pan of the execution state is
the program counter and stack status. In a multithreaded
program, we have a program counter and a stack for each
of the executing threads. All these counters and stacks
need to be saved.
The stack status is maintained by associating with every running thread a stack data structure mimicking the
stack. This stack is called the rhread state srock. The
thread stack grows whenever a new function is called
(new scope), and shrinks when the function is exited. The
top node of a thread’s stack represents the current scope.
Every node in the state stack consists of three objects: a
pointer to a function that knows how to save the functions
local variables, a pointer to a structure wrapping the local
variables, and a label used to capture the call sequence
(see [SI for more details)
The program counter is the address of the next instruction to execute. Saving the program counter at the
machine level can be achieved by saving the pc register. At the application level. saving the program counter
is closely related saving the stack. To save the program
counter at the source level we use labels. A label is insetted before each function call other than checkpoint ().
A label is also inserted after each checkpoinrO call. Before a function (other than checkpoint()) is called, the label associated with the call is inserted in the state stack
(in the same node that contains pointers to the local structure and associated saving function).
We can think of the sequence of labels from the main
scope to the checkpoinr(J scope as representing the program counter and stack status. During recovery, the saved
state stack is used to restore the state of the program. In
each scope, local variables are first restored, and then a
jump is performed to the label indicated in the state stack
entry of the current scope. This way, the recovery procedure skips code that has been executed when the last
checkpoint was taken (see [ 5 ] for more details).

4.4 Basic Checkpointing
In this section, we describe checkpointing for a multithreaded program using no blocking API calls. Blocking
calls are calls that can block the execution of a thread
like waiting on a semaphore or waiting for a thread to
exit. Checkpointing for programs that use blocking APls
is described later in the paper.
Checkpointing is done periodically every T sec. This

3 0 u r implernen~tiondoes no8 handle l e case where a lhread exi&
because it is killed by a n o l e r thread. That case can be handled by
having l e killer bread do LhC necessary deallacation.

144

4.5 Basic Recovery

is established by creating a timer thread at the beginning
of execution. This thread is assigned a priority higher
than that of all executing threads to ensure that it can always run when it needs to. The timer thread sleeps for
T sec then it wakes up to raise a checkpointing Rag cfra,q
and the sleeps again. At various locations in the code,
calls to the checkpoint function are added by the preprocessor to force the executing threads to read cfrq. When
a checkpoint function call is encountered by any of the
executing threads, cflag is checked to determine if checkpointing is required. If the Rag is set. the checkpointing
process starts. otherwise normal execution is resumed.
The first thread to call checkpoinrO and find cfrag Rag
raised is called the checkpointing thread or cpd. This
threadis assigned thejobofcoordinatingthe wholecheckpointing process. In what follows we assume that cfrag is
set when a thread executes checkpoinr().
To take a consistent snapshot of the process state. we
require that all threads stop their execution during checkpointing. Thus, c:pd will first wait for a11 other threads
to stop executing and this happcns whrn they encounter a
clieckpoinrf) call or exit. This is guaranteed to happen because rheckpoinr() calls are insened by the preprocessor.
So a thread can either hit a checkpoint call and stops. or
exits, in which case cpd does not wait for that thread anymore. Complications arise when a thread stops execution
due to a blocking API; that case is described in a separate
section. Each thread has a cooperarion Rag associated
with it to indicate that it is blocked on a rheckpuinr0 call.
When a thread other than cpd executes checkpoinr(). it
sets its rooperorion flag and then block on a condition
variable. The cpd sets its priority to 0 and constantly
yields to make sure every other thread has the opportunity to either exit or raise its coopemrion flag. When the
number of flags raised is equal to the number of threads
executing threads (as indicated by Moinminer). the second stage of checkpointing starts. In the second stage.
rpd saves the state of every thread including itself. This
is done by saving the slate stack of each thread starting
with the bottom node (representing the entry function of
the thread) to the top node representing the current scope.
For each function on the stack. cpd saves the local variables of the function. I n our implementation. the state
stack of each thread is saved in a separate file. Global
dataand the Mainrainerare saved in the checkpoint file of
thread 0. After saving all the data. rpdrestores its priority
to its original value. lowers the check pointing flag raised
by the timer thread, and executes a condition broadcast.
Upon receipt of the broadcast. every thread blocked in a
checkpoint call can proceed with normal execution.

When a program is restarted, thread 0 is the first thread
to exist. It star& by restoring the global variables. and
the Mainrainer table. The function and variable pointer
arrays (this is an array that contains the addresses of all
pointer variables in the program: it is described in details
in [ 5 ] , where it is called the memov refracror) are initialized to store the addresses of the program functions
and variables. After the Mainrainer table is restored. a
function is called to restart the threads.
For each entry in the Mainrainer, the attributes are
read and a thread with those attributes is created. If a
thread has function and argument parameters, their (new)
addresses are fetched from the function pointer arrays
and passed to the thread creating function.
After being created. a thread opens its checkpointing
file and restores its local variables. rebuild its function
call chain jumps to the label following the call to the
checkpoint() function, and waits on a condition variable.
After all threads restore their states. thread 0 execute a
broadcast so that threads can proceed with normal execution.

4.6 Checkpointing the SynchronizationState
A subset of t h t POSlX threads API may cause the calling
thread to block for an indefinite amount of time. A thread
may want to d o that if it wants to wait on a synchronizing
variable or wait for another thread to exit (joining). In
both cases. this may cause our simple checkpointing procedure described above to fail. In fact, a blocked thread
will not cooperarim in checkpointing and the cpd will
wait forever.
While checkpointing, it is important that no thread executes so that rhe saved state is consistent. Therefore. cpd
has to determine for each thread whether it is blocked or
not.
There are three approaches to solve this problem. In
the first approach, timeout is use. In the second approach.
all threads, including blocked threads are forced to rooperare. In the third approach, and this is the approach
we take i n this paper, the coordinator determines which
threads are blocked without a timeout.
In the first approach, cpdwill timeout threads so that a
thread that is not collaboraring by the end of the timeout
period is assumed to be blocked. The disadvantage of
this approach is that the duration of the timeout period is
not easy to set and a timeout period that is guaranteed to
work will he inefficient.
In the second approach. a11 blocked threads are forced
to execute (this is the approach taken by [2]. but that paper does not suppor! semaphores or the join operation).

145

To support saving and restoring of a mutex variable
(setting aside its blocking behavior for now), a Murex array is created by the preprocessor (similar to Muinruiner)
to keep track of mutexes and their attributes.
All the variables of type prhreudmurex.f are replaced
by integer data types that index the Murex array (that in
turn stores the prlmudmure.ri value). When a mutex
is initialized, an entry in the Murex array is allocated for
storing its value and its attributes: these values are used to
reinitialize the mutexes during recovery and are removed
when the mutex is destroyed.
Assuming the only synchronization variables are mu[exes, the following describes the checkpointing procedure:

For example, for mutex locks. the system keeps track of
the owner of the lock and the threads blocked trying to
aquire the lock. When checkpointing occurs, the cpd is
responsible for releasing all blocked threads in a specific
order so that all threads unblock and cooperare in checkpointing. After checkpointing is complete the cpd is responsible for letting other threads resume execution in.a
particular order so that all owners aquire their locks and
all threads that were blocked are again blocked. As we
explained earlier, the disadvantage of this approach is that
it change the semantics of the application. Nevertheless,
we have extended and implemented checkpointing using this approach to suppon semaphores. condition variables, different priorities for threads. system-wide and local contention. and the join operation. We do not report
on these extensions in this paper.
In the third approach the cpd can correctly recognize
when a thread is blocked on a resource and will not be executing before checkpointing ends. We call such threads
reully blocked. When the rpd decides that all threads are
threads are blocked (either cooperaring or at a blocking
API), the checkpointing process can safely proceed. In
what follows we discuss how the cpd determines that a
thread is reullj blocked.

4.6.1

I . Labels are inserted before calls to the
prlrreudmurexlock() function (this is done at preprocessing time).

2. When a thread tries to lock a mutex. it adds the
mutex number to its WANT variable.
3. When a thread acquires a mutex. it adds the mutex number to its HAVE list and clears the WANT
variable. The mutex adds the thread number to its
WHOHASME variable.

Mutex Locks

4. The cpd waits for all threads to be reolls blocked.
A thread is really blocked if one of the following is

Mutex locks are used by threads to coordinated access
to shared data. A mutex can always he in either of two
states: ACQUIRED or RELEASED. A thread can acquire a RELEASED mutex by executing a lock call. Any
other thread trying to acquire the mutex will end up hlocking on it until it is released. Multiple threads can block
on the same mutex and only one thread can acquire it at
a given time; that thread is called the owner of the mutex. A mutex can be released only its owner by executing an unlock call. After a mutex is released. one of the
threads blocked on it will succeed in acquiring it: all other
threads blocked on it will stay blocked. The POSIX interface provides a set of functions and types for handling
mutex locks:

true:
The thread hits the checkpoint function, but
is not the &(the thread is coopemring).
The thread wants a mutex whose owner is
blocked.
The cpd keeps trick of the blocked threads using a
blocked variable.

5. When the qddetermines that all other threads are
reullj blocked it saves the system state to stable
storage.

prhreudmure.rJ: a type representing the mutex vari-

able.
prhreudmure.rarrri: a type representing an attribute

structure used when creating the mutex.
prhreodmurexinir: for mutex initialization.
prhreudmure.rlock for locking a mutex variable.
prhreudmure.r~nlockfor unlocking a mutex vari-

able.

146

6 . After saving the system state, the cpd lets cooperuring threads resume execution.

This approach assumes that the program does not deadlock due to mutexes; otherwise, two threads that are each
waiting on a mutex owned by the other will not be declared really blocked by the cpd. While it is relatively
easy to detect deadlocks due to mutexes. we decided to
not suppon deadlocks in our current implementation. Nevertheless. our implementation will work in the presence
of deadlocks due to semaphores.
For recovery. the owners of mutexes are first allowed
to acquire their mutexes, then the remaining threads race

to try to aquire the mutexes. hut they will be guaranteed
to block. Here we note that the order of threads waiting
on a mutex can change during recovery. This should he

To determine if a thread is really blocked the cpd does
the following by repeatedly examining the states of all
threads.

acceptable as it only occurs during recovery.
4:6.2

If a thread reaches a checkpoinr(J function and is
cooperating, then it is really blocked.

Semaphore Variables

If a thread is blocked on a mutex whose owner is
really blocked, then the thread is really blocked.

Semaphore variables are used in multithreaded programs
to coordinate access to resources. A semaphore value
is .an integer initialized at the semaphore creation time
and can he read, incremented, or decremented by running threads. If a thread tries to decrement (wait) on a
semaphore whose value is 0, it gets blocked. Threads
blocked on a semaphore will remain blocked until the
semaphore is signaled, at which time one of the blocked
threads is released. A crucial difference between mutexes
and semaphores is that there is no sense of ownership for
semaphores and threads can signal any semaphore they
want. The POSlX semaphore related functions and data
structures that are supported by our system are the following:

If a thread is waiting on a semaphore whose value
is 0, then the thread might be really blocked.
If in a first pass over all threads the cpd determines
that all threads are either really blocked or waiting on
semaphores whose values are 0, then the cpd starts a second pass to check that all threads are either really blocked
as determined in the first pass or are still blocked on the
same semaphores they were blocked on in the first pass.
This might appear to he sufficient to declare all threads
really blocked. Unfortunately, this is not the case. Consider the following scenario with two threads t , and t 2
in addition to cpd. In the first pass, cpd finds that t ,
is blocked on a semaphore whose value is 0. Before
checking tg, t2 releases tl then blocks on a semaphore
whose value is 0. Then cpd checks tg and finds that it
is blocked. So in the first pass, cpd finds that all threads
are blocked on semaphores whose values are 0. Before
cpd starts the second path, tl releases tg and blocks on a
semaphore whose value is 0. cpd checks t, and finds that
it is blocked, hut before it checks t g , t g releases t , and
then blocks on a semaphore whose value is 0. Then cpd
checks t g . So in both rounds. all threads are seen to be
blocked on semaphore whose values are 0. To get around
this problem, we introduce an execution counter which is
initialized to 0 and incremented every time a semaphore
is accessed.
Our test for really blocking becomes the following. If
in a first pass over all threads the cpd determines that all
threadsareeitherreallyblockedorwaitingon semaphores
whose values are 0, then the cpd start a second pass to
check that all threads are either really blocked as determined in the first pass or are still blocked on the same
semaphores they were blocked on in the first pass and
their execution counter did not change. If that is the case.
all threads are determined to he really blocked.
After all the data is saved by cpd, all non-blocked
threads are allowed to resume execution and the appropriate Rags are reset.
Recovery is similar to recovery for mutexes.

sem-r: a type representing the semaphore variable.
sem-inir: for initializing the semaphore value.
sem-wait: for decrementing the semaphore value
an blocking if the value is not positive.
e sempost:

for incrementing the semaphore value.

a sem.gemalue:

for reading the semaphores value.

semdesrmy: for deleting the semaphore

All semaphore functions are wrapped by the preprocessor to add the functionality needed for checkpointing. Similar to the mutex case, data structures are added
to store information about the semaphores' states. Such
data structures are updated by the initialization and destruction wrappers and are saved and restored at checkpointing and recovery times.
In our approach. threads blocked on semaphores remain blocked while the cpd saves their data. As for mutexes. the cpd needs to know when all the threads are
really blocked. The problem with semaphores is that a
semaphore does not have an owner. Even if the cpd deXermines that a thread is blocked on a semaphore whose
ivalue is 0, there is no guarantee that the thread will not
ibe released by another thread who is not cooperating. To
solve this problem, each thread sets a flag before exe-cuting a sem.wair(J function and resets the Rag after it
finishes executing the sem.wair(J function. The code for
setting and reseting the Rag is added by the preprocessor
in the wrapped sem.wair() function.

4.6.3

Condition Variables

The handling of condition variables is similar to the handling of mutexes and semaphores. We omit the descrip-

147

tion.
4.6.4

Joining

The join operation allows a (joining) thread to wait on
another (joined) thread until the joined threads exits, at
which point the joining thread resumes execution and can
check, amongst other information. the exit status of the
joined thread. While joining is not done using explicit
synchronization objects, it presents problems similar to
those presented by them. In our approach, we provide
checkpointing for joining threads in a way similar to that
for threads waiting on synchronization variables. They
key information is to determine if the joined thread will
stay joined (blocked) during checkpointing. For that. we
need to determine if the joined thread is either blocked
or cooperating on checkpointing. The joining thread is
reo//yblocked if the joined thread is really blocked. So.
the problem reduces to that of determining if the joined
thread is really blocked.
Recovery for threads involved in ajoin operation should
be done in a specific order. After recreating all the threads.
the joining threads are allowed to restore their state and
re-execute their join operation. At that time, the joined
threads are allowed to restore their states and resume execution (or block if they were blocked when the checkpoint was taken). This order is necessary to make sure
that the joined thread does not exit before the join operation is executed and data about the joined thread is lost.

I"

5 Performance Results
We tested the system for correctness and performance on
the following 4 platforms: a Sun Ultra IO, I ULTRASPARC-IIi 440Mhz. 512 MB running Sun OS 5.8 operating system, a Sun SPARC 60.2 ULTRA-SPARC-II 450
Mhz, 1024 MB, running Sun OS 5.7.an Intel Dual Xeon
400Mhz. 5 12 MB running LINUX operating system, and
an Intel Pentium 111 I Ghz. 256 MB, running Windows
2000. We tested two programs: a multithreaded version
of a heat-diffusion program and a multithreaded matrix
multiplication program.
To test correctness, we created on each platform a checkpoint file for each of the two test programs and we used
the checkpoint files to restan the test programs on every other platform. The checkpoint files were created by
inserting a call to the checkpoint function in the source
code. The two programs restarted and finished execution
correctly for all 16 combinations of checkpointing and
recovery platforms.
The number of threads in each program can be easily
modified. We tested performance by executing different

148

Figure 1: Overhead for Matrix Multiplication
lo"

Figure 2: Overhead for Heat Diffusion
versions of the two programs on different input sizes. For
the matrix multiplication program we increased the input
size as we increased the number of threads. The program
calculates the product of a 1000 x i and i x 1000matrices,
where i is the number of threads.
The heat-diffusion program relies heavily on mutex
locks. The matrix multiplication program relies heavily
on semaphore synchronization. Therefore, the two program illustrate the overhead of our mechanisms for these
two synchronization primitives. The figures above depict
our results.
Figures I and 2 show the overhead introduced by the
instrumentation. This is the overhead due to the code
added by the preprocessor during normal execution. In
the figure, the source program is denoted S a n d the preprocessed program is denoted T - target. No checkpoints
were taken in these measurements, so that the times measured represent the overhead of our instrumentation. The
results show that the overhead introduced by the instrumentation is in the range of 2%-8%.
Figures 3 and 4 depict the times needed for checkpointing. C. and recovery. R, for each of the programs
tested. The checkpointing time is the time from the point
a checkpoint is initiated to the point all threads resume
execution after the checkpoint file is saved to disk. The
recovery time is the time elapsed from the time the program is restarted to the time all threads are ready to continue execution at the point after the checkpoint file was
created.

neous Newonled Swrems. Master Thesis, Purdur University. May 1996.

141 E. N. Elnozahy, D. B. Johnson. and W. Zwaeneporl. The
performance of consistent checkpinling. In Pmceedings
<$the 11th IEEE S?mporium on Reliable DirrriburedSw
rems. pages 3947. 1992.

151 F. Karablieh, R. A. Bazzi. and M. Hicks CompilerAssisted Heterogeneous Checkpointing In Proceedings
of the 20th Symposium on Reliable Distnbuted System,
New Orleans, Louisiana. October 2WI.

Figure 3: Checkpointing and Recovery times for Matrix
Multiplication

161 B. Lewis and D. J. Berg. Mulrirhreaded Programminji
with PTHREADS Sun Microsystems, 1998.

The Ergonomics of Soft171 S . Pernberton.
ware
Porting:
Automatically
Contiguring Software to the Runtime Environment.
http://www.cwi.n~ftplste~e~~~q~i~~l~~quire.html.
[8] J. S . Plank. J. Xu, and R. H. B. Netzr. Compresscd Differences: An Algorithm for Fast Incremental Checkpointing. University of Tennessee, Technical Repon. UT-CS95.302, 1995.

I

I

Figure 4: Checkpointing and Recovery times for Heat
Diffusion

191 B. Ramkumar and V. Strumpen. Portable Checkpointing for Hrterogrnrous Architectures. In Pmceedingr ,$
27rh Inrernori,,ml Sympoiium <m fitslr-E,lerrml Cumpuring Swrems, pages 5 8 6 7 , June 1997.

[IO] L. Silva. Ponablr Checkpointing and Recovery. In Pmc e d i n g s ofrhe Wur-rh IEEE lnrernurioml Symporiurn cm

Using the ASCII data format for the checkpoint file
can result in a file that is smaller ur larger than the data
in memory depending on the application. For instance. if
the data consist of integer many of which happen to have
small values. then the checkpoint file will be smaller than
the data in memory. In our application, the checkpoint
file is about 2.8 times bigger than the actual data. The
added overhead allows us to achieve portability without
information about the source and target platforms.
Checkpoints were taken after killing the program on
a specific machine, and were used to restart the target
on other machines with different executing environments.
programs restarted and finished executing successfully.

High Pe,jirmunce Dirrribured Computing. IEEE Computer Society Press. pp. 188-195,August 1995.
[ I I] P. Smith and N. Hutchinson. Hererofieneour Pmcess
Migration: The Tui Sisrern. Technical Repon 96-04.
Dcpanment of Computer Science. University of British
Columbia. February 1996.
I121 K. F. Ssu and W. K . Fuchs. PREACHES - Ponablr Rrcovery and Checkpointinp i n H~trrogrneourSystems. In
Proceedings .f IEEE Fad-Tolerant Comparing Symposium. pp. 38-47. June 1998.

1131 M. M. Theimer and B. Hayes. Heterogeneous Process
Migration by Recompilation. In Proceedings IEEE l l r h
lnrernutionul Cmjerencr on Dirrribured Compurinji S~.Yrern.~.pages 18-25,1991

References
[I] A. Beguelin. E. Ssligman. and P. Stephan. Applicution Level hurlr Ederurice in Here,rrge,ieoss N e ~ o r k o
rf
W,~vk.wrion.f.
Journal of Parallel and Distributed Computing, v. 41.no. 2.pp. 147-155. June 1997

121 W. Dieter and J.Lumpp Jr. A User-level Checkpainting
Library for POSlX Threads Programs In Pmceedinp ,f
the 29th Internotimu1 Synpurbm on Fuslr~E~ler-mr
Cornpnrinx. pp. 226227 , June 1999.
131 B. Dimitrov. A,nch!ie: A C~npiIer-BoredPorroble
Threud.7 A,rhirmrure Supporrinji Mi,qrorion on Hererojie-

149

1141 B. Yao. 1. SeNice, W. K. Fuchs. Checkpointing Multithreaded Windows NT Applications, In Sspplemenrol
Procerdingr ./the lnrernorioml ConfPrence on Dependable S?.~temsotrd Newwk.Gotrbarg, Sweden. 2001.

Simplifying Fault-Tolerance: Providing the Abstraction
of Crash Failures
RIDA A. BAZZI
Arizona State University, Tempe, Arizona

AND
GIL NEIGER
Intel Corporation, Hillsboro, Oregon

Abstract. The difficulty of designing fault-tolerant distributed algorithms increases with the severity
of failures that an algorithm must tolerate, especially for systems with synchronous message passing.
This paper considers methods that automatically translate algorithms tolerant of simple crash failures
into ones tolerant of more severe failures. These translations simplify the design task by allowing
algorithm designers to assume that processors fail only by stopping. Such translations can be
quantified by two measures: fault-tolerance, which is a measure of how many processors must remain
correct for the translation to be correct, and round-complexity, which is a measure of how the
translation increases the running time of an algorithm. Understanding these translations and their
limitations with respect to these measures can provide insight into the relative impact of different
models of faulty behavior on the ability to provide fault-tolerant applications for systems with
synchronous message passing.
This paper considers translations from crash failures to each of the following types of more severe
failures: omission to send messages; omission to send and receive messages; and totally arbitrary
behavior. It shows that previously developed translations to send-omission failures are optimal with
respect to both fault-tolerance and round-complexity. It exhibits a hierarchy of translations to general
(send/receive) omission failures that improves upon the fault-tolerance of previously developed
translations. These translations are optimal in that they cannot be improved with respect to one

Partial support for this work was provided by the National Science Foundation under grants
CCR-8909663, CCR-9106627, CCR-9301454, and CCR-9876052.
Earlier versions of parts of this paper appeared as “Optimally Simulating Crash Failures in a
Byzantine Environment” in S. Toueg, P. G. Spirakis, and L. Kirousis, editors, Proceedings of the 5th
International Workshop on Distributed Algorithms, volume 579 of Lecture Notes on Computer Science,
Springer-Verlag, New York, 1991, pp. 108 –128, and as “Simulating Crash Failures with Many Faulty
Processors” in Proceedings of the 6th International Workshop on Distributed Algorithms, A. Segall and
S. Zaks, eds. Lecture Notes on Computer Science, vol. 647. Springer-Verlag, New York, Nov. 1992,
pp. 164 –184.
Authors’ addresses: R. A. Bazzi, Computer Science and Engineering Department, College of
Engineering and Applied Sciences, Arizona State University, Box 875406, Tempe, AZ 85287-5406; G.
Neiger, EY2-09, Intel Corporation, 5350 N. E. Elam Young Parkway, Hillsboro, OR 97124-6461.
Permission to make digital / hard copy of part or all of this work for personal or classroom use is
granted without fee provided that the copies are not made or distributed for profit or commercial
advantage, the copyright notice, the title of the publication, and its date appear, and notice is given
that copying is by permission of the Association for Computing Machinery (ACM), Inc. To copy
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission
and / or a fee.
© 2001 ACM 0004-5411/01/0500-0499 $05.00
Journal of the ACM, Vol. 48, No. 3, May 2001, pp. 499 –554.

500

R. A. BAZZI AND G. NEIGER

measure without negatively affecting the other; that is, the hierarchy of translations is matched by
corresponding hierarchy of impossibility results. The paper also gives a hierarchy of translations to
arbitrary failures that improves upon the round-complexity of previously developed translations.
These translations are near-optimal; the hierarchy is matched by a hierarchy of impossibility results
whose fault-tolerances differ from those of the translations only by small constants.
Categories and Subject Descriptors: C.2.4 [Computer-Communication Networks]: Distributed Systems—distributed applications; D.1.3 [Programming Techniques]: Concurrent Programming—distributed programming; D.4.5 [Operating Systems]: Reliability—fault-tolerance
General Terms: Algorithms, Reliability, Theory
Additional Key Words and Phrases: Crash failures, fault-tolerance, translations.

1. Introduction
Distributed computing systems give algorithm designers the ability to write
fault-tolerant applications in which correctly functioning processors can complete
a computation despite the failure of other processors. To assist the designers of
such applications, researchers have developed translations that automatically
increase the fault-tolerance of distributed algorithms. Such translations automatically convert algorithms tolerant of relatively benign types of failure into ones
that tolerate more severe faulty behavior. They simplify the design task: an
algorithm can be designed with the assumption that faulty behavior is benign; the
translated algorithm can then be run correctly in a system with more severe
failures.
The following failures, which include those most frequently studied, form a
hierarchy from most benign to most severe:
(1) Crash Failures. Processors subject to crash failures fail by stopping prematurely [Hadzilacos 1983]. Before they stop, they behave correctly. After they
stop, they take no further actions.
(2) Send-Omission Failures. Processors subject to send-omission failures may
stop or they may fail by intermittently omitting to send some of the messages
that they should send [Hadzilacos 1983]. The messages they send and receive
are always correct.
(3) General Omission Failures. Processors subject to general omission failures
may stop, or they may intermittently fail to send and receive messages [Perry
and Toueg 1986]. The messages they send and receive are always correct.
(4) Arbitrary Failures. Processors subject to arbitrary failures can make arbitrary
state transitions and send arbitrary messages [Lamport et al. 1982]. Such
failures are also called malicious or Byzantine.
Researchers have developed translations for failures within this hierarchy.1
Coan [1988] considered systems with asynchronous message-passing (henceforth
called asynchronous systems) and developed a “compiler” that converts algorithms tolerant of crash failures into ones that tolerate arbitrary failures. Thus,
his translation spans this hierarchy of failures. Coan’s translation does not apply
to systems with synchronous message-passing (synchronous systems).
1
Technically, these translations translate algorithms tolerant of one type of failure into ones that
tolerate another; we will abuse terminology slightly and say that a translation translates from one type
of failure to another or is between two types of failures.

Simplifying Fault-Tolerance

501

Other researchers have considered synchronous systems. In these systems,
computation proceeds in synchronous rounds of communication. Hadzilacos
[1983] developed a technique to translate a restricted class of algorithms tolerant
of crash failures into ones that tolerate send-omission failures; Neiger and Toueg
[1990] gave a general translation for those failure types. They also developed
translations for other failure types; some translate from crash to general
omission failures, while others translate from general omission failures to
arbitrary failures. They showed that their translations could be composed,
yielding translations from crash to arbitrary failures. In related work, Srikanth
and Toueg [1987] showed how algorithms that use message authentication to
mitigate arbitrary failures can be transformed into ones that do not require
message authentication.
All these translations require simulating one round of communication of the
original untranslated algorithm by some number of rounds in the new translated
protocol. (This paper also limits itself to these round-based translations.) The
number of translating rounds is the translation’s round-complexity. Some translations require that no more than a certain fraction of processors fail. The
fault-tolerance of an algorithm or a translation is measured by comparing n, the
total number of processors in the system, to t, the number of failures tolerated. If
the requirements for a translation between two types of failures (with respect to
round-complexity or fault-tolerance) are necessary, this indicates that there is a
certain “separation” between these two types of failures. For example, Neiger
and Toueg showed that any uniform translation (defined below) from crash to
general omission failures requires that a majority of processors remain correct
(n ⬎ 2t); this indicated a fundamental difference between these two systems
that has since been explored elsewhere [Bazzi and Neiger 1992; Neiger and
Tuttle 1993].
This paper provides a comprehensive study of translations from crash failures
to the more severe types of failures in synchronous systems. In doing so, it
defines two types of translations:
—Uniform Translations. Such translations correctly simulate the behavior of all
processors.
—Nonuniform Translations. These translations do not guarantee correct simulation of the behavior of faulty processors. Such translations are adequate for
many applications.
While the applicability of nonuniform translations is less general than that of
uniform translations, their study is motivated by the following observations.
There are some systems that admit nonuniform translations but no uniform
translations; in some cases, higher fault-tolerance is possible with a nonuniform
translation; and uniform translations may allow lower round-complexity for a
fixed fault-tolerance (the last is not realized in this paper).
The following summarizes our results and observations regarding translations
from crash failures for synchronous systems.
—Translations to Send-Omission Failures. Neiger and Toueg gave a uniform
2-round translation that requires n ⬎ t. This is optimal with respect to both
round-complexity and fault-tolerance. Furthermore, no nonuniform translation
can do better.

502

R. A. BAZZI AND G. NEIGER

—Translations to General Omission Failures. Neiger and Toueg gave a uniform
2-round translation that requires n ⬎ 2t. They showed that, for uniform
translations, this is optimal with respect to both round-complexity and faulttolerance. For nonuniform translations, better fault-tolerance can be obtained.
Specifically, we exhibit a series of nonuniform translations: as the roundcomplexity of a translation increases, so does its fault-tolerance. (Specifically,
we provide, for any n and t, a translation that requires t/(n ⫺ t) ⫹
t/(n ⫺ t) ⫹ 1 rounds.) We show that each of these is optimal with respect to
round-complexity for the given fault-tolerance.
—Translations to Arbitrary Failures. Because of the nature of arbitrary failures, it
is impossible to provide uniform translations. Neiger and Toueg composed
their translation from crash to general omission failures with others from
general omission failures to arbitrary failures. This yielded two different
nonuniform translations. Because composing translations multiplies their
round-complexities, these have high round-complexity: one was a 4-round
translation that required n ⬎ 4t and the other was a 6-round translation
requiring n ⬎ 3t. In contrast, we give a hierarchy of nonuniform translations
with round-complexities 2, 3, and 4, requiring n ⬎ 6t ⫺ 3, n ⬎ 4t ⫺ 2, and
n ⬎ 3t, respectively. We show that each of these is near-optimal with respect
to round-complexity for the given fault-tolerance. (Specifically, translations
using 2, 3, and 4 rounds, respectively, are impossible for n ⱕ 6t ⫺ 5, n ⱕ
4t ⫺ 3, and n ⱕ 3t, respectively. The corresponding translations have
fault-tolerances that are within small constants of those of any optimal
translations.)
The balance of the paper is organized as follows: Section 2 presents basic
definitions, notation, and terminology. Section 3 defines correct behavior and the
types of failures that we consider in this paper. Section 4 gives the definitions of
protocol implementations, of translations between systems, and of problem
specifications; it shows that translations preserve the correctness of solutions to
problem specifications. Section 5 presents lower bounds on the round-complexity
of translations. Section 6 discusses the results on translations from crash to send
omission failures. Section 7 presents translations from crash to general omission
failures, and Section 8 presents those from crash to arbitrary failures. Section 9
contains a discussion and some concluding remarks.
2. Definitions, Assumptions, and Notation
This paper considers synchronous distributed systems. These are systems in
which computation proceeds in synchronous rounds. This section defines a
formal model of such systems. This model is an adaptation of that used by Neiger
and Toueg [1990].
2.1. DISTRIBUTED SYSTEMS. A distributed system is a set ᏼ ⫽ { p 1 , p 2 , . . . ,
p n } of n processors fully connected by bidirectional communication links.
Processors share no memory; they communicate only by sending messages along
the communication links. Each processor has a local state.
Processors communicate with each other in synchronous rounds. In each
round, a processor sends messages, receives messages, and then changes its state.

Simplifying Fault-Tolerance

FIG. 1.

503

The code for protocol ⌸ run by processor p.

Any message received in a round was sent in the same round. Let ⬜ be a special
value that indicates “no message.”2
2.2. PROTOCOLS. Processors run a protocol ⌸. A protocol is a set of programs, one for each processor, that specifies the messages to be sent and the
state transitions for each processor in each round. In addition, each protocol is
associated with a set ᏽ of states and, for each processor p, a subset ᏽ p 債 ᏽ,
which indicates the set of initial states for that processor. Each protocol also has
an associated set ᏹ of messages (⬜ 僆
兾 ᏹ). Let ᏹ⬘ ⫽ ᏹ 艛 {⬜}.
(A processor’s initial state can be used to model any input that it might receive
before a protocol is run. The model of a distributed system presented here could
be extended to allow processors to receive such inputs in every round. Doing so
would be straightforward but tedious and would add little insight to the
presentation of the paper’s results. Nevertheless, the ability to capture inputs
received in any round is a motivation for studying the round-based translations
mentioned in Section 1 and defined below in Section 4. The sequel will note
places in which this ability has impact on our results.)
Formally, a protocol consists of two functions, a message function and statetransition function. The message function ␮: Z ⫻ ᏼ ⫻ ᏽ 哫 ᏹ⬘ (where Z is the set
of positive integers) identifies the messages that must be sent by each processor
in each round. If processor p begins round i in state s, then ⌸ specifies that it
send ␮ (i, p, s) to all processors in that round. The state-transition function
␦: Z ⫻ ᏼ ⫻ (ᏹ⬘) n 哫 ᏽ identifies each processor’s state transition at the end of
each round. If, in round i, processor p receives the messages m 1 , . . . , m n (m j
from processor p j ), then ⌸ specifies that it change its state to ␦ (i, p, m 1 , . . . ,
m n ) at the end of round i. Figure 1 illustrates a protocol ⌸.
Our definition of protocols may appear restrictive. For example, every processor is required to broadcast a message in every round; a protocol’s statetransition function depends solely on the messages that it just received and not
on its previous state. These restrictions are made only to simplify the exposition
and do not restrict the applicability of the results. For example, any protocol in a
less-restricted model can be modified so that processors send the same message
to all processors in every round; moreover, the state of a processor can always be
included in the messages it sends to itself (remember that a processor sends a
message to itself in every round).
2

Thus, if p omits to send a message to q in a round, we say that p “sends” ⬜ to q, although no
message is actually sent.

504

R. A. BAZZI AND G. NEIGER

2.3. HISTORIES. A history describes the execution of a distributed system.
This description should include the states through which each processor passes,
the messages sent and received in the system, and the protocol being run by the
processors.3 Formally, a history consists of a protocol and the following three
functions:
—a state-sequence function,
—a message-sending function, and
—a message-receiving function.
The state-sequence function Q: Z ⫻ ᏼ 哫 ᏽ identifies the states of processors at
the beginning of each round; Q(i, p) is the state in which processor p begins
round i.
The message-sending function S: Z ⫻ ᏼ ⫻ ᏼ 哫 ᏹ⬘ identifies the messages sent
in each round; S(i, p, q) is the message that p sends to q in round i or ⬜ if p
sends no message to q in round i.
The message-receiving function R: Z ⫻ ᏼ ⫻ ᏼ 哫 ᏹ⬘ identifies the messages
received in each round; R(i, p, q) is the message that p receives from q in round
i or ⬜ if p does not receive a message from q in round i. 4 Let R(i, p) denote the
vector 具R(i, p, p 1 ), . . . , R(i, p, p n )典 of messages received by p in round i.
H ⫽ 具⌸, Q, S, R典 is a history of protocol ⌸. A system is identified with the set of
all histories (of all protocols) in that system. A system can be defined by giving
the properties that its histories must satisfy. The balance of this paper uses the
terms “execution” and “history” interchangeably.
3. Correctness and Failures
Individual processors may exhibit failures, thereby deviating from correct behavior. They may do so by failing to send or receive messages correctly or by
otherwise not following their protocol. This section formally defines correct and
faulty behavior.
3.1. CORRECTNESS. A processor executes correctly if its actions are always
those specified by its protocol. Consider a history H ⫽ 具⌸, Q, S, R典. Processor p
initializes correctly in H if Q(1, p) 僆 ᏽ p . It sends correctly in round i of H if

共 @q 僆 ᏼ兲共S共i, p, q兲 ⫽ ␮共i, p, Q共i, p兲兲兲.
It receives correctly in round i of H if

共 @q 僆 ᏼ兲共R共i, p, q兲 ⫽ S共i, q, p兲兲.
It makes a correct state transition in round i of H if
Q共i

3

⫹ 1, p兲 ⫽ ␦共i, p, R共i, p兲兲.

Including the processors’ protocol ⌸ in H allows identification of the processors faulty in H; see
Section 3.
4
Because of failures, the states and messages specified by ⌸ may be different from those indicated by
Q and S, and R(i, p, q) need not equal S(i, q, p); see Section 3.

Simplifying Fault-Tolerance

505

Processor p is correct through round i of H if it initializes, sends, receives, and
makes correct state transitions, up to and including round i of H. Let

Correct共H, i兲 ⫽ 兵 p 僆 ᏼ兩 p is correct through round i of H}.
Assume that all processors are initially correct, so Correct(H, 0) ⫽ ᏼ. Then let
Correct(H), the set of all processors correct throughout history H, be
艚 i僆Z Correct(H, i). If a processor is not correct, it is faulty. Formally,
Faulty(H, i) ⫽ ᏼ ⶿ Correct(H, i) for any i 僆 Z and Faulty(H) ⫽ ᏼ ⶿ Correct(H).
3.2. CRASH FAILURES. A crash failure [Hadzilacos 1983] is the most benign
type of failure that this paper considers. A processor commits a crash failure by
prematurely halting in some round. Formally, p commits a crash failure in round
i p 僆 Z of H ⫽ 具⌸, Q, S, R典 if p 僆 Faulty(H, i p ), p 僆 Correct(H, i) for all i ⬍ i p ,
and if
—if i p ⫽ 1, p initializes correctly: i p ⫽ 1 f Q(i p , p) 僆 ᏽ p ;
—p sends to each processor q in round i p either what the protocol specifies, or
nothing at all:

共 @q 僆 ᏼ兲共S共ip , p, q兲 ⫽ ␮共ip , p, Q共ip , p兲兲 ⵪ S共ip , p, q兲 ⫽ ⬜兲;

and

—afterwards, it sends and receives no messages and makes no state transitions:

共 @i ⱖ ip兲共Q共i ⫹ 1, p兲 ⫽ Q共ip , p兲 ⵩ 共@q 僆 ᏼ兲共S共i ⫹ 1, p, q兲 ⫽ R共i, p, q兲 ⫽ ⬜兲兲.
(If p does not crash, then i p ⫽ ⬁.)
The system C(n, t) corresponds to the set of histories in which up to t
processors commit only crash failures and all other processors are correct. That
is, H 僆 C(n, t) if and only 兩Faulty(H)兩 ⱕ t and

共 @p 僆 Faulty共H兲兲共?ip 僆 Z兲共 p commits a crash failure in round ip of H兲.
3.3. SEND-OMISSION FAILURES. Another type of failure, called a send-omission failure, occurs if a processor omits to send messages [Hadzilacos 1983].
Processor p may commit such failures in history H if it is subject to crash failures
and, before it crashes (if it does), it initializes correctly, makes correct state
transitions, receives correctly, sends correctly to itself, and sends to each other
processor what its protocol specifies or nothing at all:

共 @i ⬍ ip兲共@q ⫽ p兲共S共i, p, q兲 ⫽ ␮共i, p, Q共i, p兲兲 ⵪ S共i, p, q兲 ⫽ ⬜兲.
If faulty processor p does crash, then its behavior in and after round i p is as
described in Section 3.2. If it does not crash (i.e., if i p ⫽ ⬁), then its behavior is
correct except as allowed in formula above.
The system O(n, t) corresponds to the set of histories in which up to t
processors are subject to crash and send-omission failures and all other processors are correct. Note that C(n, t) 債 O(n, t).
3.4. GENERAL OMISSION FAILURES. A more complex type of failure, called a
general-omission failure [Perry and Toueg 1986], occurs if a processor intermittently fails to send and receive messages. Processor p may commit such failures
in history H if it is subject to crash failures and, before it crashes (if it does), it

506

R. A. BAZZI AND G. NEIGER

initializes correctly, makes correct state transitions, sends correctly to itself,
sends to each other processor what its protocol specifies or nothing at all, and
may omit to receive messages that are sent to it by other processors:
—(@i ⬍ i p )(@q ⫽ p)( S(i, p, q) ⫽ ␮ (i, p, Q(i, p)) ⵪ S(i, p, q) ⫽ ⬜); and
—(@i ⬍ i p )(@q ⫽ p)( R(i, p, q) ⫽ S(i, q, p) ⵪ R(i, p, q) ⫽ ⬜).
If faulty processor p does crash, then its behavior in and after round i p is as
described in Section 3.2. If it does not crash (i.e., if i p ⫽ ⬁), then its behavior is
correct except as allowed in formulas above.
The system G(n, t) corresponds to the set of histories in which up to t
processors are subject to general-omission failures and all other processors are
correct. Note that O(n, t) 債 G(n, t).
3.5. ARBITRARY FAILURES. Crash failures considerably restrict the behavior
of faulty processors. Omission failures place fewer restrictions on this behavior.
In the worst case, faulty behavior may be completely arbitrary [Lamport et al.
1982]. Processor p is subject to arbitrary failures in history H ⫽ 具⌸, Q, S, R典 if it
may deviate from ⌸ in any way by arbitrarily changing its state and by sending
and receiving arbitrary messages.
The system A(n, t) corresponds to the set of histories in which up to t
processors commit arbitrary failures and all other processors are correct. Note
that G(n, t) 債 A(n, t).
4. Implementations, Translations, and Specifications
This section defines many of the concepts central to the results of this paper.
Section 4.1 defines implementations, which are protocols designed to simulate the
behavior of other protocols. Section 4.2 defines translations, which are techniques
that automatically generate protocol implementations. Section 4.3 defines classes
of problem specifications and shows that solutions to problems in these classes
are preserved by the translations defined in Section 4.2.
4.1. IMPLEMENTATIONS. Often a protocol is designed with the assumption
that it will be run in a particular system. While it may be desired to run the
protocol in another system, it may not always be possible. For example, a
protocol might be designed with the assumption that processors fail by crashing.
Such a protocol would not necessarily perform as expected in a system in which
processors are subject to general-omission failures. In general, a protocol is not
guaranteed to behave correctly if it is run in a system other than the one for
which it was designed.
Given the many variations that can exist between systems, it may be desirable
that the protocol designer work at a level of abstraction that hides some of the
details of a system. To run a protocol in a particular system, one may need to
provide an implementation of the protocol for that system. Informally, an
implementation of a protocol ⌸ is a protocol ⌸imp (possibly identical to ⌸) that
handles the details of the system that ⌸ might not, while providing the high-level
behavior of ⌸. One way to provide implementations is by using additional
communication: each round of ⌸ is implemented using z rounds of ⌸imp. These z
rounds hide the details of the system that ⌸ does not handle. As noted in Section

Simplifying Fault-Tolerance

507

1, this is the approach taken by earlier work in this field, and it is the approach
followed by this paper.
Let S 1 (n) be the system for which ⌸ is designed, and let S 2 (n) be a system in
which we would like to run ⌸.5 For integer z ⱖ 1, a z-round implementation of ⌸
in S 2 (n) that provides the abstraction of S 1 (n) is a protocol ⌸imp, appropriate for
S 2 (n), with certain properties. Associated with ⌸imp are a state set ᏽimp and, for
each processor p, a subset ᏽimp;p 債 ᏽimp. In addition, there are (1) a statesimulation function ᏿⌸ mapping every implementing state s of a processor running
⌸imp into a simulated state ᏿ ⌸ (s) of a processor running ⌸; and (2) a
history-simulation function Ᏼ⌸ mapping every implementing history Himp of ⌸imp in
S 2 (n) to Ᏼ⌸(Himp), a simulated history of ⌸. The image of ⌸imp’s initial state sets
under the state-simulation function must correspond to and cover ⌸’s initial state
sets:
(a) (@p 僆 ᏼ)(@s imp 僆 ᏽimp;p )(?s 僆 ᏽ p )(᏿ ⌸ (s imp) ⫽ s); and
(b) (@p 僆 ᏼ)(@s 僆 ᏽ p )(?s imp 僆 ᏽimp;p )(᏿ ⌸ (s imp) ⫽ s).
Furthermore, the following must hold for every implementing history Himp ⫽
具⌸imp, Qimp, Simp, Rimp典 of ⌸imp in S 2 (n):
(c) Ᏼ⌸(Himp) ⫽ H ⫽ 具⌸, Q, S, R典 is a history of ⌸ running in S 1 (n),
(d) (@i 僆 Z)(@p 僆 ᏼ)(᏿ ⌸ ( Qimp( z(i ⫺ 1) ⫹ 1, p)) ⫽ Q(i, p)), and
(e) Correct(Himp) 債 Correct(H).
Each round of ⌸ in S 1 (n) is simulated by z rounds of ⌸imp in S 2 (n). Specifically,
round i of ⌸ is simulated by rounds z(i ⫺ 1) ⫹ 1 through zi of ⌸imp. We call
rounds z(i ⫺ 1) ⫹ 1 through round zi the ith phase of the implementation. The
round-complexity of the implementation is z. (Note that, if S 1 (n) ⫽ S 2 (n), a
protocol is a 1-round implementation of itself for which the history-simulation
and state-simulation functions each is the identity function.)
Condition (c) captures the idea that the implementation provides abstraction
of S 1 (n) for ⌸. Condition (d) ensures that processor states are correctly
implemented at the beginning of each phase of the implementation; that is, the
implementing states in Himp correspond to the simulated states in H. Condition
(e) (which implies Faulty(H) 債 Faulty(Himp)) states that an implementation
preserves the correctness of processors. That is, any processor correct in Himp is
also correct in the simulated history H. However, processors faulty in Himp may
appear to be correct in H.
As defined above, implementations require that the states of all processors
(correct or faulty) be correctly implemented (this is specified by condition (d)
above). This is desirable but, unfortunately, it is not always possible to achieve.
In fact, if failures are arbitrary, there is no way to require faulty processors to set
their states to any particular value. Nevertheless, it is possible to provide the
abstraction of S 1 (n) in a slightly weaker way. The behavior of the processors
faulty in the simulated history will be consistent with the behavior of (possibly
faulty) processors in S 1 (n); however, the states of processors faulty in the
implementing history (which include all those faulty in the simulated history)
5

We consider implementations only for systems that have the same number of processors as that for
which ⌸ was designed.

508

R. A. BAZZI AND G. NEIGER

may not correspond to the states of those processors in the simulated history. In
other words, the behavior of faulty processors as seen by the correct processors
in a simulated history is indistinguishable from the behavior of processors (faulty
or correct) in some history in S 1 (n). It follows that one can distinguish two types
of implementations that we shall call uniform implementations and nonuniform
implementations.
Uniform implementations are those defined above. Formally, we define a
nonuniform implementation that provides the abstraction of S 1 (n) as one that
provides the abstraction of S 1 (n) (as defined above) but with condition (d) above
replaced by the following:
(d) (@i 僆 Z)(@p 僆 Correct(Himp))(᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)) ⫽

Q(i,

p)).

For (d⬘), it is not necessary for the states of processors in Faulty(Himp) to be
implemented properly. Since nonuniform implementations satisfy weaker conditions than uniform ones, every uniform implementation is also a nonuniform one.
4.2. TRANSLATIONS. Every implementation is associated with a particular
protocol. We are interested in mechanisms that generates implementations. We
call such a mechanism a translation. For integer z ⱖ 1, a z-round translation ᐀
from system S 1 (n) to system S 2 (n) maps a protocol ⌸ designed to run S 1 (n) into
a z-round implementation ᐀(⌸) ⫽ ⌸imp of ⌸ in S 2 (n) that provides the
abstraction of S 1 (n). Like any implementation (see Section 4.1), ⌸imp ⫽ ᐀(⌸)
has an associated state set ᏽimp with subsets ᏽimp;p ; a state-simulation function
᏿⌸; and a history-simulation function Ᏼ⌸. A translation ᐀ is called a uniform
translation if, for every protocol ⌸, ᐀(⌸) is a uniform implementation of ⌸. A
translation is called a nonuniform translation if, for each ⌸, ᐀(⌸) is a nonuniform
implementation of ⌸. (Obviously, every uniform translation is also nonuniform.)
The round-complexity of a z-round translation is z.
While the translations defined in the previous paragraphs are general mechanisms for generating an implementation of any protocol, the implementations
that they produce may be quite diverse. This is because each implementation
⌸imp has its own simulation functions ᏿⌸ and Ᏼ⌸, and there need not be any
relationship between the functions of one implementation and those of another.
There may be value to having translations that produce implementations that are
somewhat homogeneous in that their simulation functions are related. We call
such translations canonical. A translation ᐀ from S 1 (n) to S 2 (n) is canonical if it
has associated functions ᏿ and Ᏼ with the following properties. Let ⌸imp ⫽ ᐀(⌸)
be an implementation produced by ᐀ with state set ᏽimp, state-simulation
function ᏿⌸, and history-simulation function Ᏼ⌸. Then ᏿⌸ is the restriction of ᏿
to ᏽimp and Ᏼ⌸ is the restriction of Ᏼ to the subset of S 2 (n) of executions of
⌸imp. The functions ᏿ and Ᏼ are ᐀’s canonical state-simulation function and
history-simulation function, respectively. All the translations demonstrated or
referenced in Sections 6 – 8 are canonical translations. The lower-bound results of
Section 5 apply to all translations (not just to canonical ones).
The translations (canonical or not) defined above are general in that they
place no restrictions on the relationship between S 1 (n) and S 2 (n). This paper
considers the case in which S 1 (n) and S 2 (n) differ only in the types of failures
that may occur. We use B(n, t) for S 1 (n) and S(n, t) for S 2 (n) to emphasize the

Simplifying Fault-Tolerance

509

fact that failures are benign in S 1 (n) and severe in S 2 (n) and that both systems
have the same upper bound on the number of faulty processors.
4.3. PROBLEM SPECIFICATIONS. In general, a problem can be defined by
stating the properties that need to be satisfied by the histories of protocols that
solve the problem. These properties can be expressed by a predicate that
distinguishes histories that solve the problem from those that do not. Formally,
we define a problem specification to be a mapping from histories to {true, false}.
Protocol ⌸ solves specification ⌺ in system S if ⌺(H) for every history H of ⌸ in
S.
4.3.1. Classes of Specifications. This paper concentrates on protocol implementations and the translations that produce them. In general, it is not concerned with the low-level details of the implementing histories, such as specific
message exchanges needed by the implementation. For this reason, the paper
focuses on what we call state specifications. A state specification ⌺ is one that is
sensitive only to histories’ Correct sets and their state-sequence functions. That is,
if H1 ⫽ 具⌸1, Q1, S1, R1典 and H2 ⫽ 具⌸2, Q2, S2, R2典 are such that Correct(H1) ⫽
Correct(H2) and Q1 ⫽ Q2, then ⌺(H1) if and only if ⌺(H2). (In the sequel, state
specifications will sometimes be considered predicates on correct-processor sets
and functions from Z ⫻ ᏼ to ᏽ. That is, we will abuse notation and write ⌺(C, Q)
instead of ⌺(H), where H ⫽ 具⌸, Q, S, R典 and C ⫽ Correct(H).) In addition, the
satisfaction of a state specification is not sensitive to additional failures. Specifically, if ⌺ is a state specification and ⌺(C, Q) is true, then ⌺(C⬘, Q) is true for
any C⬘ 債 C. The generality of this definition is discussed in Section 4.3.2 below.
When determining if a protocol implementation ⌸imp solves state specification
⌺, the implementing states (those through which ⌸imp has processors pass) are
not used; these states may include implementation details that are not relevant to
the protocol being implemented. Instead, the solution of ⌺ by ⌸imp is determined
using the simulated states corresponding to these implementing states (as
computed by ⌸imp’s state-simulation function). Specifically, history Himp ⫽ 具⌸imp,
Qimp, Simp, Rimp典 of z-round implementation ⌸imp with state-simulation function
᏿⌸ satisfies ⌺ if ⌺(Correct(Himp), f Q) is true, where f Q(i, p) ⫽
᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)).
The balance of this section will show that the translations defined in Section
4.2 preserve the correctness of solutions to certain classes of state specifications.
From the definitions of state specifications and uniform translations, we obtain
the following theorem:
THEOREM 4.1. Let ⌸ be a protocol, ᐀ a uniform z-round translation from
B(n, t) to S(n, t), and ⌺ a state specification. If ⌸ solves ⌺ in B(n, t), then
implementation ᐀(⌸) solves ⌺ in S(n, t).
PROOF. Suppose that ⌸imp ⫽ ᐀(⌸) has state-simulation function ᏿⌸ and
history-simulation function Ᏼ⌸. Let Himp ⫽ 具⌸imp, Qimp, Simp, Rimp典 be any history
of ⌸imp in S(n, t). Let C imp ⫽ Correct(Himp) and define f Q(i, p) ⫽
᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)). If ⌺(C imp, f Q) is true, then ⌸imp solves ⌺ in S(n, t).
By condition (c) of implementations, Hb ⫽ Ᏼ⌸(Himp) ⫽ 具⌸b, Qb, Sb, Rb典 is a
history of ⌸ in B(n, t). Let C ⫽ Correct(Hb). By condition (e) of implementations, C imp 債 C. Since ⌸ solves ⌺ in B(n, t) and ⌺ is a state specification,
⌺(C, Qb) is true. Since C imp 債 C and ⌺ is a state specification, ⌺(C imp, Qb) is

510

R. A. BAZZI AND G. NEIGER

also true. By condition (d) of uniform implementations, f Q(i, p) ⫽
᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)) ⫽ Qb(i, p) for each i 僆 Z and p 僆 ᏼ; that is, fQ ⫽ Qb.
Thus, ⌺(Cimp, fQ) is true (as desired), and ᐀(⌸) ⫽ ⌸imp solves ⌺ in S(n, t). e
The specifications of many problems do not depend on the behavior of the
faulty processors. Such specifications are faulty-state insensitive (or FS-insensitive). An FS-insensitive specification ⌺ is oblivious to the states of faulty
processors. Specifically, state specification ⌺ is FS-insensitive if the following is
true. If ⌺(C, Q) is true, then ⌺(C, Q⬘) is true if (@i 僆 Z)(@p 僆 C)(Q⬘(i, p) ⫽
Q(i, p)) (⌺(C, Q⬘) must be true even if Q and Q⬘ disagree on the states of faulty
processors). A state specification that is not FS-insensitive is FS-sensitive.
From the above definition and that of nonuniform translations, we obtain the
following theorem:
THEOREM 4.2. Let ⌸ be a protocol, ᐀ a nonuniform z-round translation from
B(n, t) to S(n, t), and ⌺ an FS-insensitive state specification. If ⌸ solves ⌺ in B(n, t),
then implementation ᐀(⌸) solves ⌺ in S(n, t).
PROOF. Suppose that ⌸imp ⫽ ᐀(⌸) has state-simulation function ᏿⌸ and
history-simulation function Ᏼ⌸. Let Himp ⫽ 具⌸imp, Qimp, Simp, Rimp典 be any history
of ⌸imp in S(n, t). Let C imp ⫽ Correct(Himp) and define f Q(i, p) ⫽
᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)). If ⌺(C imp, f Q) is true, then ⌸imp solves ⌺ in S(n, t).
By condition (c) of implementations, Hb ⫽ Ᏼ⌸(Himp) ⫽ 具⌸b, Qb, Sb, Rb典 is a
history of ⌸ in B(n, t). Let C ⫽ Correct(Hb). Since ⌸ solves ⌺ in B(n, t) and ⌺
is a state specification, ⌺(C, Qb) is true. By condition (e) of implementations,
C imp 債 C. Since ⌺ is a state specification, ⌺(C imp, Qb) is true. By condition (d⬘)
of nonuniform implementations, f Q(i, p) ⫽ ᏿⌸(Qimp( z(i ⫺ 1) ⫹ 1, p)) ⫽
Qb(i, p) for each i 僆 Z and p 僆 Correct(Himp) ⫽ C imp. Since ⌺ is FS-insensitive,
⌺(C imp, f Q) is true. Thus, ⌸imp ⫽ ᐀(⌸) solves ⌺ in S(n, t). e
4.3.2. Examples. One feature of our definition of state specifications—their
insensitivity to additional failures— has not been considered explicitly by other
researchers. For this reason, we consider some standard problems in distributed
computing and show that they are indeed state specifications by our definition.
We give both failure-insensitive and failure-sensitive examples.
An example of a failure-insensitive specification is that of Byzantine Agreement
[Lamport et al. 1982]. Solutions to this problem have a distinguished processor,
called the broadcaster, attempt to transmit a value to the other processors. The
specification ⌺BA of Byzantine Agreement maintains that every history of any
solution has the following properties:
—Termination. Each correct processor eventually enters a state corresponding to
a decision value. Once entering such a state, it enters only states corresponding
to the same value.
—Agreement. If two correct processors enter states corresponding to decision
values, they choose the same value. The value chosen by the correct processors
in a history is denoted v d .
—Broadcast Validity. The broadcaster’s initial state corresponds to an initial
value, denoted v i . If the broadcaster is correct, then v d ⫽ v i .

Simplifying Fault-Tolerance

511

It should be clear that ⌺BA refers only to processor states and the identities of the
correct processors. In addition, we can show that it is not sensitive to additional
failures as required for a state specification. Specifically, if ⌺BA(C, Q) and C⬘ 債
C, then ⌺BA(C⬘, Q); that is, 具C⬘, Q典 satisfies the three conditions of Byzantine
Agreement:
—Since ⌺BA(C, Q), Q has all processors in C reach irrevocable decision values.
Since C⬘ 債 C, the same is true for processors in C⬘: 具C⬘, Q典 satisfies
Termination.
—⌺BA(C, Q) implies that Q has all processors in C agree on their decision values.
Since C⬘ 債 C, processors in C⬘ agree: 具C⬘, Q典 satisfies Agreement.
—To show that 具C⬘, Q典 satisfies Broadcast Validity, assume that the broadcaster
is in C⬘. Since C⬘ 債 C, the broadcaster is in C. Since ⌺BA(C, Q), Q has v d ⫽
v i . Thus, 具C⬘, Q典 satisfies Broadcast Validity.
Thus, ⌺BA is a state specification. It should also be clear that it is FS-insensitive;
none of the three conditions refer in any way to the states of the faulty
processors. Thus, Theorem 4.2 implies that that nonuniform translations preserve the correctness of solutions to Byzantine Agreement.
Uniform Agreement [Neiger and Toueg 1990] is a strengthening of Byzantine
Agreement that is FS-sensitive. Its specification ⌺UA replaces the Agreement
condition of ⌺BA with the following:
—Uniformity. If any two processors enter states corresponding to decision
values, they choose the same value (v d ).
Uniformity does not require faulty processors to reach decision values but, if they
do, they must be consistent with those of the correct processors. While ⌺UA is
clearly also a state specification (this follows from reasoning similar to that used
above for ⌺BA), Uniformity is an FS-sensitive condition. Thus, while solutions to
Uniform Agreement might not be preserved by nonuniform translations, Theorem
4.1 implies that they are preserved by uniform translations. (Neiger and Toueg
used the fact that Uniform Agreement cannot be solved in G(n, t) to show that
uniform translations—the only type they considered— could not exist from
C(n, t) to G(n, t).)
One can also use Theorem 4.2 to show that solutions to Distributed Consensus
are also preserved by nonuniform translations. The specification of Distributed
Consensus is similar to that of Byzantine Agreement, but with Broadcast Validity
replaced by a unanimity condition that specifies that, if processors begin in
agreement, they end by agreeing on the originally agreed-upon value. The
following paragraphs discuss three different ways to specify such unanimity. They
argue that these ways are, in a certain sense, equivalent and show that one of
them results in a failure-insensitive state specification.
We call these three conditions Total Unanimity, Correct Unanimity, and Weak
Unanimity, and we denote the three derived specifications by ⌺TU, ⌺CU, and
⌺WU. The specifications are not equivalent. In particular, for every execution H,
⌺CU(H) f ⌺TU(H) and ⌺TU(H) f ⌺WU(H), but there are executions H such that
⌺TU(H) ⵩ ¬ ⌺CU(H) and those such that ⌺WU(H) ⵩ ¬ ⌺TU(H). The three conditions
are specified as follows:

512

R. A. BAZZI AND G. NEIGER

—Correct Unanimity. If all correct processors begin in initial states corresponding to the same initial value (v i ), then v d ⫽ v i .
—Total Unanimity. If all processors begin in initial states corresponding to the
same initial value (v i ), then v d ⫽ v i .
—Weak Unanimity. If all processors are correct and begin in initial states
corresponding to the same initial value (v i ), then v d ⫽ v i .
To see ⌺TU(H) f ⌺WU(H), consider a history H that satisfies Total Unanimity and
assume that, in H, all processors are correct and begin with v i . Since the
hypothesis of Total Unanimity is met, all processors decide on v i , and Weak
Unanimity is satisfied. In contrast, consider an execution in which there is at
least one faulty processor, all processors begin with the same value, but in which
the correct processors decide on a different value. Weak Unanimity is vacuously
satisfied while Total Unanimity fails to hold.
Another important observation regarding the relationships between these
specifications of Distributed Consensus is the following: While their specifications
are not equivalent, their solutions are equivalent with regard to the systems
defined in Section 3. Clearly, any solution to either ⌺TU or ⌺CU is also a solution
to ⌺WU (in any system). Standard techniques based on execution chains [Fischer
and Lynch 1982] can be used to show that a solution to ⌺WU in any of the systems
defined in Section 3 also solves ⌺CU and ⌺TU. Doing so is beyond the scope of this
paper.
We make the following observations about these three specifications as they
relate to the results of this paper.
—⌺CU is not a state specification as defined in Section 4.3.1 above. To see this, let
C and Q be such that ⌺CU(C, Q) and 兩C兩 ⱖ 2. Suppose that p 僆 C is such that
Q has all processors in C⶿{ p} begin with the same initial value v i but that p
begins with a different value. Suppose further that Q has processors in C
choose decision value v d ⫽ v i ; this is allowed as processors in C do not
initially agree. Correct Unanimity does not hold for 具C⶿{ p}, Q典 because all
processors in C⶿{ p} begin with v i but they do not choose v i as their decision
value. Thus, ⌺CU(C⶿{ p}, Q) is false and ⌺CU is not a state specification.
—⌺TU is a state specification but is not FS-insensitive; Unanimity refers to “all
processors,” including those that are faulty. Thus, Theorem 4.2 does not apply
directly to solutions to ⌺TU.
—⌺WU is a state specification for the following reason. If ⌺WU(C, Q) and C⬘ 債 C,
then either 具C⬘, Q典 trivially satisfies Weak Unanimity (because C⬘ does not
contain all processors) or C⬘ ⫽ C, in which case 具C⬘, Q典 satisfies Weak
Unanimity because ⌺WU(C, Q). It is not hard to see that ⌺WU is FS-insensitive.
Theorem 4.2 thus implies that solutions to ⌺WU are preserved by nonuniform
translations. We now claim that solutions to both ⌺TU and ⌺CU are also so
preserved. Consider, for example, ⌺CU. Let ᐀ be a nonuniform translation from
B(n, t) to S(n, t) and let ⌸ be a solution to ⌺CU in B(n, t). Since ⌺CU(H) f
⌺WU(H) for all H, ⌸ also solves ⌺WU in B(n, t). By Theorem 4.2, ⌸imp ⫽ ᐀(⌸)
solves ⌺WU in S(n, t). As noted earlier, if S(n, t) is any of the systems defined in
Section 3, any solution to ⌺WU in S(n, t) also solves ⌺CU in S(n, t). Thus, ⌸imp
solves ⌺CU in S(n, t), and the correctness of ⌸ is preserved.

Simplifying Fault-Tolerance

513

5. Lower Bounds
This section shows lower bounds on the worst-case round-complexity of some
translations from crash failures. Sections 5.1, 5.2, and 5.3 prove these bounds for
translations to send-omission, general omission, and arbitrary failures, respectively. The proofs in Section 5.2 and 5.3 are achieved by considering fullinformation translations. These are translations that produce implementations
that are full-information protocols. Full-information protocols require a processor, in each round, to send its current state to all protocols and to set its state to
the vector of messages received in that round. Arguments by Coan [1986] can be
used to show that, if there exists a translation from B(n, t) to S(n, t), then there
exists a full-information translation from B(n, t) to S(n, t).
Before proceeding, we note the following: Depending on the type of translation desired, the solution to an agreement problem can be used to do round-byround simulations. Such translations, however, have high round-complexity
(specifically, t ⫹ 1). Specifically, there is a nonuniform translation from C(n, t)
to S(n, t) with round-complexity f(n, t) if Byzantine Agreement (see Section
4.3.2) can be solved in S(n, t) in f(n, t) rounds. Byzantine Agreement can be
solved in t ⫹ 1 rounds in O(n, t) and G(n, t) regardless of the values of n and
t. It can be solved in t ⫹ 1 rounds in A(n, t) if and only if n ⬎ 3t. There is a
uniform translation from C(n, t) to S(n, t) with round-complexity f(n, t) if
Uniform Agreement (see Section 4.3.2) can be solved in S(n, t) in f(n, t) rounds.
Uniform Agreement can be solved in O(n, t) in t ⫹ 1 rounds for any n and t. It
can be solved in G(n, t) in t ⫹ 1 rounds if n ⬎ 2t. It cannot be solved in A(n, t)
if t ⬎ 0. Byzantine Agreement cannot be solved in any of these systems in fewer
than t ⫹ 1 rounds [Fischer and Lynch 1982]; since Uniform Agreement is a harder
problem, the same is true for it.
5.1. LOWER BOUNDS FOR CRASH-TO-SEND-OMISSION TRANSLATIONS. Neiger
and Toueg [1990] showed that there can be no uniform 1-round translation from
crash to send-omission failures. Their arguments are readily applicable to show
that there can be no nonuniform 1-round translation from crash to send-omission
failures.
5.2. LOWER BOUNDS FOR CRASH-TO-GENERAL-OMISSION TRANSLATIONS.
Neiger and Toueg [1990] showed that there can be no uniform translation from
crash to general omission failures if n ⱕ 2t. This section proves lower bounds on
the round-complexity of nonuniform translations from crash to general omission
failures. It shows that there can be no y-round translation from C(n, t) to
G(n, t) if y ⱕ t/(n ⫺ t) ⫹ t/(n ⫺ t) and t ⬍ n ⫺ 1. 6
The proof considers the executions of a simple protocol ⌸ and shows that it
cannot be translated in the specified number of rounds. It first shows a set of
constraints satisfied by any history of ⌸ in C(n, t). It then shows that these
constraints cannot all be satisfied by the simulated histories of a y-round
implementation of ⌸ in G(n, t) if y ⫽ t/(n ⫺ t) ⫹ t/(n ⫺ t) and t ⬍ n ⫺
1. (Obviously, implementations with fewer implementing rounds will similarly
fail.)
6
Note that t ⫽ 0 implies y ⫽ 0, so the result is not interesting in that case. If t ⱖ n ⫺ 1, then y ⱖ
2t ⱖ t ⫹ 1 and the result does not hold. As noted earlier, there are (t ⫹ 1)-round translations based
on Byzantine Agreement.

514

R. A. BAZZI AND G. NEIGER

Protocol ⌸ operates as follows: Let ᏽ ⫽ ᏹ ⫽ {0, 1, f}; ᏽ p ⫽ {0, 1} for p ⫽
p 1 and { f} for every other processor (recall that ᏽ p are the possible initial states
of p). In round 1, p 1 (the broadcaster) sends its initial state to all processors. At
the end of that round, every processor that receives a message from p 1 sets its
state to the value received. All other processors set their states to f. In each of
the following rounds, each processor sends its state to all other processors. If a
processor receives f from any processor, it sets its own state to f; otherwise, it
does not change its state. Formally, the protocol is specified as follows:

␮ 共 i, p, s 兲 ⫽ s

冦

r 关 p 1 兴 if

␦ 共 i, p, r 兲 ⫽ f

再

i ⫽ 1 and r关 p1兴 ⫽ ⬜

if

i ⫽ 1 and r关 p1兴 ⫽ ⬜ or

if

i ⬎ 1 and r关q兴 ⫽ f for some q

r关 p兴 otherwise, (i.e., if i ⬎ 1 and r关q兴 ⫽ f for all q).

⌸ simply has processors relay information about the broadcaster’s initial state or
its failure, a structure common to many distributed protocols.
As mentioned above, any history H of ⌸ in a system with crash failures must
satisfy a number of constraints. These are captured in the following lemma.
LEMMA 5.1. Any history H ⫽ 具⌸,
must satisfy the following constraints:

Q, S, R典

of ⌸ in a system with crash failures

(1) If p 僆 Correct(H, i) and Q(i, p) ⫽ f for some i ⬎ 1, then Q( j, q) ⫽ f for all
j ⬎ i and q 僆 Correct(H, j).
(2) If p 1 僆 Correct(H, 1), then Q(i ⫹ 1, p) ⫽ f for all i ⱖ 1 and p 僆
Correct(H, i).
(3) For all i ⱖ 1 and p 僆 Correct(H, i), Q(i ⫹ 1, p) ⫽ f or Q(i ⫹ 1, p) ⫽
Q(1, p 1 ).
PROOF. We provide a proof only for (3) above. The proofs of the other two
parts are similar and are omitted. The proof is by induction on i. The base case is
i ⫽ 1. By inspection of the protocol, each correct processor’s state at the end of
round 1 is either f or a message m ⫽ ⬜ received from p 1 in that round. If the
state is not f, then an inspection of the protocol also shows m ⫽ Q(1, p 1 ). For
the induction step, assume that, for some i ⱖ 1 and all p 僆 Correct(H, i),
Q(i ⫹ 1, p) ⫽ f or Q(i ⫹ 1, p) ⫽ Q(1, p 1 ). Because only crash failures can
occur, each processor q 僆 Correct(H, i ⫹ 1) will receive, in round i ⫹ 1, an
array of values whose entries are each either f, Q(1, p 1 ), or ⬜. Inspection of the
protocol shows that Q(i ⫹ 2, q) ⫽ f or Q(i ⫹ 2a, q) ⫽ Q(1, p 1 ). This completes
the inductive step and the proof. e
As stated earlier, the sequel will show that the above constraints cannot be
satisfied by a y-round implementation of ⌸ if y ⫽ t/(n ⫺ t) ⫹ t/(n ⫺ t) and
t ⬍ n ⫺ 1. Assume for a contradiction that such an implementation ⌸imp exists
with state-simulation function ᏿⌸ and history-simulation function Ᏼ⌸. We will
describe an execution of ⌸imp and show that it cannot implement a history of ⌸

Simplifying Fault-Tolerance

515

that meets all three of the constraints of Lemma 5.1. This will contradict the
existence of ⌸imp.
In order to describe the execution, we introduce some notation. Let ᐉ ⫽
t/(n ⫺ t). It follows that y ⫽ 2ᐉ ⫹ b, where b is 0 if t is a multiple of n ⫺ t
and is 1 otherwise. Partition ᏼ into the sets L 0 , L 1 , . . . , L y⫹1 as follows: For
each i, 0 ⱕ i ⱕ ᐉ,

L 2i ⫽ 兵 p i(n⫺t)⫹1 其 and
L 2i⫹1 ⫽ 兵 p i(n⫺t)⫹2 , . . . , p (i⫹1)(n⫺t) 其 .
(Notice that L 0 ⫽ { p 1 } and that t ⬍ n ⫺ 1 implies that none of these sets are
empty.) If b ⫽ 0 (t divides n ⫺ t), 2ᐉ ⫹ 1 ⫽ y ⫹ 1, and we have the desired
y ⫹ 2 sets comprising all ((t/n ⫺ t) ⫹ 1)(n ⫺ t) ⫽ t ⫹ (n ⫺ t) ⫽ n
processors. If b ⫽ 1, 2ᐉ ⫹ 1 ⫽ y, so we have only y ⫹ 1 sets comprising
(t/(n ⫺ t) ⫹ 1)(n ⫺ t) ⬍ t ⫹ (n ⫺ t) ⫽ n processors. In this case, define
L y⫹1 to be { p (ᐉ⫹1)(n⫺t)⫹1 , . . . , p n }. In either case, note that L i 艚 L j ⫽  if
i ⫽ j and 兩L i 艛 L i⫹1 兩 ⱖ n ⫺ t for all i, 0 ⱕ i ⱕ y.
Consider now two sequences of histories of ⌸imp, each containing y ⫹ 1
executions. The sequences are H00, H01, . . . , H0y and H10, H11, . . . , H1y and they
differ in that p 1 begins each H0k in simulated state 0 and each H1k in simulated
state 1. (Simulated states are determined by the state-simulation function ᏿⌸.)
The histories within a given sequence differ only in the identities of the correct
processors and are otherwise identical.7 They are such that communication is
perfect (there are no omissions) between processors in any single set L i and
between processors in “adjacent” sets (i.e., processors in L i communicate
perfectly with those in L i⫺1 and those in L i⫹1 , when these sets exist). No other
communication takes place successfully. In other words, no communication takes
place between processors in L i and those in L j , for any j ⬎ i ⫹ 1, at any time.
(Note that this implies that processors outside L 0 艛 L 1 never receive any
message from p 1 .) All processors behave correctly otherwise.
In each Hsk (s 僆 {0, 1} and 0 ⱕ k ⱕ y), the set of correct processors is L k 艛
L k⫹1 . The fact that 兩L k 艛 L k⫹1 兩 ⱖ n ⫺ t for all k implies that the described
histories are valid histories of ⌸imp in G(n, t) (no more than t processors fail).
This is because all failures to communicate can be ascribed to omission failures
(send or receive) by processors in ᏼ ⶿ (L k 艛 L k⫹1 ). For example, in Hs0 , the
n ⫺ t processors in L 0 艛 L 1 are correct. Processors in L 2 always omit to receive
from and to send to p 1 (the only processor in L 0 ). Processors in L i (i ⬎ 2)
similarly omit with respect to all processors in L 0 艛 L 1 . The omitted messages
among processors in ᏼ ⶿ (L 0 艛 L 1 ) (e.g., between L 2 and L 4 ) may be either the
result of either send or receive omissions.
The histories Hsk (for fixed s) are indistinguishable to all processors and, in
particular, no processor can determine the identity of the correct processors. It is
this inability that leads to the impossibility result. The patterns of failures in
7

“Otherwise identical” means that their state-sequence functions Q and message-receiving functions
are identical. The state-sending functions S may vary. For example, in one history H, we may have
S(i, p, q) ⫽ R(i, q, p) ⫽ ⬜, indicating a failure by p to send a message to q in round i. In another
history H⬘, we may have R⬘(i, q, p) ⫽ ⬜ and S⬘(i, p, q) ⫽ ⬜, indicating a failure of q to receive a
message from p.
R

516

R. A. BAZZI AND G. NEIGER

these histories is such that knowledge of p 1 ’s initial simulated state (i.e., that it is
s) expands by one set in each round. At the beginning of round 1, only p 1 , the
one processor in L 0 , knows. After round 1, only processors in L 0 艛 L 1 know
(because L 0 communicates only with L 1 ). After round 2, only processors in
L 0 艛 L 1 艛 L 2 know (because L 1 communicates only with L 0 and L 2 ). In
general, in each Hsk , processors in L i first learn of p 1 ’s initial simulated state at
the end of round i. In particular, processors in L y⫹1 do not do so until the end of
round y ⫹ 1 (the first round of phase 2) and thus do not know p 1 ’s initial
simulated state at the end of phase 1. This fact will be critical to the proof
because it will contradict the following lemma.
LEMMA 5.2. For each s 僆 {0, 1} and k (0 ⱕ k ⱕ y), each processor’s state is s
at the end of every round of Ᏼ⌸(Hsk ).
PROOF. Recall that s is p 1 ’s initial simulated state in each Hsk ; that is, s is p 1 ’s
initial state in each Ᏼ⌸(Hsk ). We will prove by induction on i (0 ⱕ i ⱕ y ⫹ 1)
that the state of each processor in L i is s at the end of every round of each
Ᏼ⌸(Hsk ).
For the base case, the only processor in L 0 is p 1 . In Hs0 , p 1 is correct. By
condition (e) of implementations, p 1 is also correct in Ᏼ⌸(Hs0 ). Condition (c) of
implementations implies that Ᏼ⌸(Hs0 ) is an execution of ⌸ in C(n, t), so
constraint (3) of Lemma 5.1 implies that p 1 ’s state is either s or f at the end of
every round of Ᏼ⌸(Hs0 ). Because p 1 is correct in that history, constraint (2)
implies its state cannot be f at the end of any round. Thus, its state is s at the end
of each round of Ᏼ⌸(Hs0 ). Condition (d⬘) of nonuniform implementations
implies that, at the end of each phase j of Hs0 , p 1 is in a state s j such that
᏿⌸(s j ) ⫽ s. Because the histories Hsk (1 ⱕ k ⱕ y) are indistinguishable to p 1 , it
is in state s j at the end of phase j of each Hsk . Thus, it is in state s ⫽ ᏿ ⌸ (s j ) at
the end of each round j of Ᏼ⌸(Hsk ).
For the induction step, assume that each processor in L i , 0 ⱕ i ⬍ y ⫹ 1, is in
state s at the end of every round of each Ᏼ⌸(Hsk ). We seek to prove the same for
processors in L i⫹1 . Let us consider specifically the history Hsi . Since each
processor in L i 艛 L i⫹1 is correct in Hsi , condition (e) of implementations
implies that it is also correct in Ᏼ⌸(Hsi ). Condition (c) of implementations
implies that Ᏼ⌸(Hsi ) is an execution of ⌸ in C(n, t), so constraint (3) of Lemma
5.1 implies that the state of each processor in L i⫹1 must be either s or f at the
end of each round of Ᏼ⌸(Hsi ). Suppose for a contradiction that the state of some
processor p 僆 L i⫹1 is f at the end of some round j of Ᏼ⌸(Hsi ). Because
processors in L i 艛 L i⫹1 are all correct in Ᏼ⌸(Hsi ), constraint (1) of Lemma 5.1
implies that the states of processors in L i must be f at the end of round j ⫹ 1 of
Ᏼ⌸(Hsi ). But this contradicts the inductive hypothesis. Thus, the state of each
processor in L i⫹1 must be s at the end of every round of each Ᏼ⌸(Hsi ).
Let p i be any processor in L i . Condition (d⬘) of nonuniform implementations
implies that, at the end of each phase j of Hsi , p i is in a state s j such that
᏿ ⌸ (s j ) ⫽ s. Because the histories Hsk (1 ⱕ k ⱕ y) are indistinguishable to p i , it
is in state s j at the end of phase j of each Hsk . Thus, p i is in state s ⫽ ᏿ ⌸ (s j ) at
the end of each round j of Ᏼ⌸(Hsk ). Since p i was chosen arbitrarily from L i , the
same is true for each processor in L i , completing the proof. e

Simplifying Fault-Tolerance

517

Lemma 5.2 implies that, at the end of phase 1 of H00, the simulated states of all
processors in L y⫹1 must be 0 ( p 1 ’s initial simulated state in that history). H00 and
H10 differ only with respect to p 1 ’s initial state and, as noted above, processors in
L y⫹1 cannot know p 1 ’s initial state until phase 2. Thus, their (nonsimulated)
states at the end of phase 1 of each of H00 and H10 are identical, and their
simulated states are thus 0 at the end of phase 1 of H10. This contradicts Lemma
5.2, which implies that, at the end of phase 1 of H10, the simulated states of all
processors in L y⫹1 must be 1 ( p 1 ’s initial simulated state in that history). Thus,
the existence of the implementation ⌸imp is contradicted, giving us the following
theorem:
THEOREM 5.3. There is no nonuniform y-round translation from C(n, t) to
G(n, t) if y ⱕ t/(n ⫺ t) ⫹ t/(n ⫺ t) and t ⬍ n ⫺ 1.
PROOF. Suppose that such a translation ᐀ exists. Then ⌸imp ⫽ ᐀(⌸) is a
y-round implementation of ⌸ in G(n, t) with state-simulation function ᏿⌸ and
history-simulation function Ᏼ⌸. This section has proven that such an implementation cannot exist. Thus, the translation cannot exist. e
Recall that Neiger and Toueg [1990] showed that there can be no uniform
translation from C(n, t) to G(n, t) if n ⱕ 2t and, if n ⬎ 2t, only 2-round
translations exist. For the case of n ⬎ 2t, t/(n ⫺ t) ⫹ t/(n ⫺ t) ⫽ 1, and
our results show that there can be no nonuniform 1-round translation from
C(n, t) to G(n, t). (Neiger and Toueg gave a 2-round uniform translation for
this case.) Thus, for the cases in which uniform and nonuniform translations are
both possible (n ⬎ 2t), the weaker requirements of nonuniform translations do
not admit translations that are more efficient with respect to round-complexity.
5.3. LOWER BOUNDS FOR CRASH-TO-ARBITRARY TRANSLATIONS. This section
shows that, for certain fault-tolerances, no translation of a specified roundcomplexity exists from C(n, t) to A(n, t). We begin by noting a result that
follows from considering the classical problem of Byzantine Agreement [Lamport
et al. 1982]. Hadzilacos [1983] solved this problem in systems with crash failures
for any n ⱖ t, but Lamport et al. showed that it cannot be solved in a system
with arbitrary failures if n ⱕ 3t; thus, the minimum fault-tolerance of a
translation from C(n, t) to A(n, t) is n ⬎ 3t.
The balance of this section deals with systems for which translations are
possible (i.e., when n ⬎ 3t) and explores their round-complexity. It proves
lower-bound results for 1-, 2-, and 3-phase translations: there can be no 1-round
translation if t ⬎ 0; there can be no 2-round translation if t ⬎ 1 and n ⱕ 6t ⫺
5; and there can be no 3-round translation if t ⬎ 2 and n ⱕ 4t ⫺ 3.
5.3.1. An Untranslatable Protocol. As in Section 5.2, we give a simple protocol
⌸ and note some properties of its executions in systems with crash failures. We
then show, in Sections 5.3.2–5.3.4, that these properties cannot be maintained for
certain combinations of round-complexity and fault-tolerance.
⌸ operates as follows. ᏽ ⫽ ᏹ ⫽ {0, 1, f, s ⬜ } 艛 {0, 1, f} n . For initial states,
let ᏽ p ⫽ ᏽ for all p except a distinguished processor b (the broadcaster); ᏽ b ⫽
{0, 1}. Each processor always sends their current state as a message. At the end
of round 1, each processor sets its state to be either the message it received from
b (if any) or f (if it received ⬜). At the end of round 2, it sets its state to be the

518

R. A. BAZZI AND G. NEIGER

vector of messages it received in that round. At the end of every subsequent
round, each processor sets its state to s ⬜ . Here is a formal specification of the
protocol:

␮ 共 i, p, s 兲 ⫽
␦ 共 1, p, r 兲 ⫽
␦共2, p, r兲 ⫽
␦共i, p, r兲 ⫽

s

再

if r关b兴 ⫽ ⬜
otherwise

f
r关b兴

r
s⬜

if

i⬎2

Any history H of ⌸ in a system with crash failures must satisfy a number of
constraints. These are captured in the following lemma.
LEMMA 5.4. Any history H ⫽ 具⌸, Q, S, R典 of ⌸ in a system with crash failures in
which b’s initial state is 0 or 1 must satisfy the following constraints:
(1) If b, p 僆 Correct(H, 1), then Q(2, p) ⫽ Q(1, b).
(2) If p, q 僆 Correct(H, 1) and Q(2, p) ⫽ Q(2, q), then Q(2, p) ⫽ f or
Q(2, q) ⫽ f.
(3) If p 僆 Correct(H, 1), q 僆 Correct(H, 2), and Q(2, p) ⫽ f, then Q(3, q) ⫽ s,
where s[b] ⫽ ⬜.
(4) If p, q 僆 Correct(H, 2), then Q(3, q) ⫽ s, where s[ p] ⫽ Q(2, p).
(5) If p 僆 Correct(H, 2) and Q(3, p) ⫽ s with s[q] ⫽ ⬜, then q 僆 Correct(H, 1)
and s[q] ⫽ Q(2, q).
PROOF. These constraints follow from an inspection of ⌸. We include here a
proof only of (3). Assume p 僆 Correct(H, 1), q 僆 Correct(H, 2), and Q(2, p) ⫽ f.
An inspection of the protocol implies R(1, p, b) ⫽ ⬜. Since b must send either
0 or 1 in round 1, this implies that b crashes in that round and thus sends no
messages in round 2. Thus, R(2, q, b) ⫽ ⬜ and, since Q(3, q) ⫽ ␦ (2, q,
8
R(2, q)) ⫽ R(2, q), Q(3, q) ⫽ s with s[b] ⫽ ⬜.
e
These constraints will be central to the impossibility proofs below. Most of
these proofs exploit the fact that, in many cases, the correct processors are
uncertain as to which processors are faulty. To maintain as much uncertainty for
as long as possible, the constructions of all histories given below assume that
faulty processors behave correctly unless otherwise noted.
5.3.2. 1-Round Translations. This section shows that there can be no 1-round
translation from C(n, t) to A(n, t) if t ⬎ 0. (Since C(n, 0) ⫽ A(n, 0) for all n,
a 1-round translation is trivial if t ⫽ 0.) Assume for a contradiction that a
1-round full-information translation ᐀1 from C(n, t) to A(n, t) exists for t ⬎ 0.
Let ⌸imp ⫽ ᐀1(⌸) be the resulting implementation of ⌸ with state-simulation
function ᏿⌸ and history-simulation function Ᏼ⌸. Since ⌸imp is full-information, b
should send its initial state to all processors in round 1. The initial states of the
other processors are irrelevant. (We have already observed that there can be no
translation at all if n ⱕ 3t, so it must be the case that n ⬎ 3t. It follows that, if
t ⬎ 0, n ⬎ 3t ⱖ 3.)
8

Recall that R(i, q) denotes the vector 具R(i, q, p 1 ), . . . ,

R(i,

q, p n )典.

Simplifying Fault-Tolerance

519

Fix two processors p 0 and p 1 other than b. Consider now the following three
executions of ⌸imp:
—Hf ⫽ 具⌸imp, Qf, Sf, Rf典. In this execution, b is faulty and sends, in round 1, 0 to
p 0 and 1 to p 1 ; p 0 , p 1 僆 Correct(Hf).
—H0 ⫽ 具⌸imp, Q0, S0, R0典. In this execution, the initial state of b is 0 and b, p 0 僆
Correct(H0).
—H1 ⫽ 具⌸imp, Q1, S1, R1典. In this execution, the initial state of b is 1 and b, p 1 僆
Correct(H1).
Note that p 0 cannot distinguish the first phases of Hf and H0; similarly, p 1 cannot
distinguish the first phases of Hf and H1.
Because b, p 0 僆 Correct(H0), condition (e) of implementations implies b and
p 0 are both correct in Ᏼ⌸(H0), which by condition (c) of implementations, is an
execution of ⌸. Constraint (1) of Lemma 5.4 then implies that p 0 ’s state must be
0 at the end of round 1 of Ᏼ⌸(H0). Similarly, p 1 ’s state must be 1 at the end of
round 1 of Ᏼ⌸(H1). Because (1) p 0 cannot distinguish the first phases of Hf and
H0; (2) p 1 cannot distinguish the first phases of Hf and H1; and (3) condition (d⬘)
of nonuniform implementations implies that the processors’ states in Hf functionally determine their states in Ᏼ⌸(Hf); it must be that, for i 僆 {0, 1}, each p i is in
state i at the end of round 1 of Ᏼ⌸(Hf). This contradicts constraint (2) of Lemma
5.4. We conclude that ᐀1 cannot exist.
5.3.3. 2-Round Translations. This section shows that there can be no 2-round
translation from C(n, t) to A(n, t) if t ⬎ 1 and 3t ⬍ n ⱕ 6t ⫺ 5. (Recall that
there is 2-round translation based on Byzantine Agreement if t ⱕ 1 and n ⬎ 3t,
and no translation at all if n ⱕ 3t.)
Assume for a contradiction that there is a 2-round full-information translation
᐀2 from C(n, t) to A(n, t) that is correct with t ⬎ 1 and 3t ⬍ n ⱕ 6t ⫺ 5. Let
⌸imp ⫽ ᐀2(⌸) be the resulting implementation of ⌸ with state-simulation
function ᏿⌸ and history-simulation function Ᏼ⌸. Since ⌸imp is a full-information
protocol, every processor (including b) sends its initial state to all processors in
the first round of phase 1. In round 2, each correct processor echoes the states it
received to all others. Since only the initial simulated state of b is relevant to the
constraints above, we will refer to message exchanges only as they relate to b’s
initial simulated state. Let V p be the vector of echoed messages (reporting b’s
initial simulated state) that p receives in round 2, where V p [q] is that received
from q. If V p [q] ⫽ s, then, in round 2, p receives a message from q claiming its
receipt of s from b in round 1.
We begin the proof with two lemmas that hold for any execution of ⌸imp.
LEMMA 5.5. Consider any set of processors G such that b 僆 G and 兩G兩 ⫽ n ⫺ t,
and let m 僆 {0, 1}. If correct processor p 僆 G receives m from b in round 1 and has
Vp[q] ⫽ m for all q 僆 G at the end of round 2, then p’s simulated state must be m
at the end of phase 1.
PROOF. Let H be the history under consideration. Consider another execution
H⬘ in which all processors in G are correct and the t processors in B ⫽ ᏼ ⶿ G are
faulty. In H⬘, b’s initial simulated state is m and processors in B send the same
messages to p that they did in H. Thus, p cannot distinguish the first two rounds
of H and H⬘: in each, it receives m from b in round 1, m is relayed by all

520

R. A. BAZZI AND G. NEIGER

processors in G in round 2, and processors in B send the same messages. Since b
and p are correct in H⬘, condition (e) of implementations imply that they are also
correct in Ᏼ⌸(H⬘); condition (c) of implementations implies that that history is
one of ⌸ in C(n, t). Constraint (1) of Lemma 5.4 then implies that p’s state after
round 1 of Ᏼ⌸(H⬘) must be m. Let s be p’s state after phase 1 of H⬘. Because p is
correct in H⬘, condition (d⬘) of nonuniform implementations implies that m ⫽
᏿ ⌸ (s). Since p is correct in H and cannot distinguish H from H⬘, it is also in state
s at the end of phase 1 of H, so its simulated state is m at that time. e
LEMMA 5.6. Consider any set G⬘ such that b 僆
兾 G⬘ and 兩G⬘兩 ⫽ n ⫺ 2t, and let
m 僆 {0, 1}. Suppose that, for some correct processor p 僆
兾 G⬘, Vp[q] ⫽ m for all q 僆
G⬘; then p’s simulated state must be m at the end of phase 1.
PROOF. Let H be the execution under consideration. Partition ᏼ into the
following sets: {b}, { p}, G⬘, A, and B. The partition into A and B is arbitrary
as long as 兩A兩 ⫽ 兩B兩 ⫽ t ⫺ 1.
We will construct executions H1 and H2 with the following properties:
—{ p} 艛 G⬘ 債 Correct(H1) and {b} 艛 G⬘ 債 Correct(H2).
—Processor p cannot distinguish the first two rounds of H and H1. Thus, p will
set its simulated state in the same way at the end of phase 1 of each execution.
—Processors in G⬘ cannot distinguish the first four rounds of H1 and H2. They
will set their simulated states identically at the end of the first two phases of
each execution.
In addition, H1 and H2 will be constructed so that these simulated states are
further constrained by Lemmas 5.4 and 5.5.
H1 is defined as follows: The t processors in A 艛 {b} are faulty while those in
G⬘ 艛 B 艛 { p} are correct. In round 1, b sends the following messages: to p,
the same message that it sent in round 1 of H; to processors in G⬘, m; to each
processor in B, whatever message that processor echoed to p in round 2 of H.9 In
round 2, b and processors in A echo to p what they did in H and m to all other
processors. In phase 2, they behave exactly as processors in G⬘ do.
H1 is defined so that the processors in A 艛 {b} send exactly the same
messages to p in phase 1 that they do in H. Processors in G⬘ echo m to p in
round 2 in H1, as this is what they received from b in round 1; by assumption,
they echoed m to p in round 2 of H as well. In round 1 of H1, b sends messages
to processors in B so that they echo to p the same messages that they did in
round 2 of H. Thus, p cannot distinguish the first two rounds of H and H1.
Note that each processor in G⬘ receives m from b in round 1 of H1. In
addition, it receives echoes of m from each processor in G⬘ 艛 A 艛 {b}. This
set has size (n ⫺ 2t) ⫹ (t ⫺ 1) ⫹ 1 ⫽ n ⫺ t. Using G⬘ 艛 A 艛 {b} for G,
Lemma 5.5 implies that each processor in G⬘ sets its simulated state to m at the
end of phase 1 of H1. By condition (c) of implementations, Ᏼ⌸(H1) is an
execution of ⌸ in C(n, t) and, by condition (e), p is correct in Ᏼ⌸(H1). By
constraint (2) of Lemma 5.4, p’s state at the end of round 1 of Ᏼ⌸(H1) is either
m or f.
H2 is defined as follows: The t processors in B 艛 { p} are faulty while those in
G⬘ 艛 A 艛 {b} are correct; b starts with initial simulated state m. In phases 1
9

It is not necessary to specify the messages faulty processors send to each other.

Simplifying Fault-Tolerance

FIG. 2.

521

Vector received by processor p.

and 2, each faulty processor sends exactly the same messages that it did in H1 (in
which it was correct).
The message passing in H2 is not identical to that of H1; for example, all
processors in A echo m to p in round 2 of H2, while this might not be the case in
H1. However, a simple inductive argument shows that processors correct in both
executions (i.e., those in G⬘) receive the same messages the two histories. Thus,
processors in G⬘ cannot distinguish them and set their simulated states identically in the two histories.
Since processors in {b} 艛 G⬘ are correct in H2, condition (e) of implementation implies that they are also correct in Ᏼ⌸(H2). By condition (c) of implementations, Ᏼ⌸(H2) is an execution of ⌸ in C(n, t). Thus, constraint (1) of Lemma
5.4 implies that b sets its state to m at the end of round 1 of Ᏼ⌸(H2). By
constraint (4), each processor in G⬘ sets its state at the end of round 2 of Ᏼ⌸(H2)
to s such that s[b] is b’s state at the end of round 1, that is, m.
Consider some processor q 僆 G⬘ and let s q be q’s state at the end of phase 2
of H2. Because q is correct in H2, condition (d⬘) of nonuniform implementations
implies ᏿ ⌸ (s q ) ⫽ s, where s[b] ⫽ m. Since processors in G⬘ cannot distinguish
H1 and H2 and are correct in both executions, q’s state at the end of phase 2 of
H1 is also s q . Thus, q’s state at the end of round 2 of Ᏼ⌸(H1) is s with s[b] ⫽ m.
Since p is correct in H1, constraint (3) of Lemma 5.4 implies that p’s state at the
end of round 1 of Ᏼ⌸(H1) cannot be f. We observed earlier that this state is
either m or f, so it must be m.
Let s p be p’s state at the end of the first phase of H1. Since p is correct in H1,
condition (d⬘) of nonuniform implementations implies ᏿ ⌸ (s p ) ⫽ m. As noted
above, p cannot distinguish the first phases of H and H1, so it must be in state s p
at the end of the first phase of H. Thus, p sets its simulated state to m at the end
of phase 1 of H, as desired. e
We now prove that a 2-round translation cannot exist. Let A 0 , A 1 , C 0 , and
C 1 be a partition of ᏼ ⶿ {b} such that 兩A 0 兩 ⫽ 兩A 1 兩 ⫽ t ⫺ 1, 兩C 0 兩 ⫽
(n ⫺ 2t ⫹ 1)/ 2, and 兩C 1 兩 ⫽ (n ⫺ 2t ⫹ 1)/ 2. Note that, since t ⬎ 1 and
3t ⬍ n ⱕ 6t ⫺ 5, it follows that 2 ⱕ 兩C i 兩 ⱕ 2t ⫺ 2, 兩A i 兩 ⱖ 1, and 兩A i 艛 C i 兩
⬎ t, for i 僆 {0, 1}. Suppose that, at the end of round 2 of execution H, correct
processor p 僆 C 0 艛 C 1 receives messages composing the array shown in Figure
2. It is clear to p that b is faulty: in round 2, too many processors (兩A 0 艛 C 0 兩 ⬎
t) give too much support to 0 for b to have correctly sent 1, and too many (兩A 1
艛 C 1 兩 ⬎ t) give too much support to 1 for it to have correctly sent 0. We will
show that p’s simulated state at the end of phase 2 cannot have a value that
satisfies all the requirements on histories of ⌸.
Suppose that, in round 3 (the first round of phase 2), all processors but two
send messages, each indicating that they too had this array after phase 1. The two
remaining processors, q 僆 A 0 and r 僆 A 1 send different messages. In round 3, q
claims that it had the array in Figure 3. There are n ⫺ 2 processors other than q
and b; q claims to have received 0 from all of these but C 1 . Since 兩C 1 兩 ⱕ 2t ⫺ 2,

522

R. A. BAZZI AND G. NEIGER

FIG. 3.

Vector received by processor q 僆 A 0 , forcing it to receive 0.

there at least (n ⫺ 2) ⫺ (2t ⫺ 2) ⫽ n ⫺ 2t of these. Thus, Lemma 5.6 implies
that, if q is correct in H, its simulated state was 0 at the end of phase 1.
Processor r claims in round 3 that it received the array in Figure 4. If r is
correct, Lemma 5.6 implies that it set its simulated state to 1 at the end of phase
1 (it claims to have received 1 from at least n ⫺ 2t processors not including itself
or b).
In round 4, all processors echo to p messages indicating that they received, in
round 3, the same messages that p did. The messages received by p are consistent
with two different executions H0 and H1:
—In H0, the faulty processors are those in {b} 艛 A 1 . Lemma 5.6 implies that q’s
simulated state is 0 at the end of phase 1. Condition (c) of implementations
implies that Ᏼ⌸(H0) is an execution of ⌸ in C(n, t) and condition (e) implies
that q is correct in that execution (because it is correct in H0). Condition (d⬘)
of nonuniform implementations then implies that q is in state 0 at the end of
round 1 of Ᏼ⌸(H0). By constraint (4) of Lemma 5.4, p’s state at the end of
round 2 of Ᏼ⌸(H0) is s, where s[q] ⫽ 0.
—In H1, the faulty processors are those in {b} 艛 A 0 . Here, r’s simulated state is
1 at the end of phase 1 and, by arguments similar to those given in the previous
item, p’s state at the end of round 2 of Ᏼ⌸(H1) is s, where s[r] ⫽ 1.
Recall that p’s behavior in H must be consistent with its behavior in both H0 and
H1 (it cannot distinguish them; in fact, H must be either H0 or H1). The fact that
p is correct in each of H, H0, and H1, together with condition (d⬘) of nonuniform
implementations, implies that, at the end of round 2 of Ᏼ⌸(H), p must set its
state to s such that s[q] ⫽ 0 and s[r] ⫽ 1. Constraint (5) of Lemma 5.4 implies
that q and r both behave correctly in the first round of Ᏼ⌸(H) and must be in
states 0 and 1, respectively, at the end of that round. This contradicts constraint
(2) of Lemma 5.4. This contradiction implies that the translation ᐀2 cannot exist,
giving us the following theorem:
THEOREM 5.7. If t ⬎ 1 and n ⱕ 6t ⫺ 5, there can be no 2-round translation
from C(n, t) to A(n, t).
5.3.4. 3-Round Translations. This section shows that there can be no 3-round
translation from C(n, t) to A(n, t) if t ⬎ 2 and 3t ⬍ n ⱕ 4t ⫺ 3. (Recall that
there is 3-round translation based on Byzantine Agreement if t ⱕ 2 and n ⬎ 3t,
and no translation at all if n ⱕ 3t.)
Assume for a contradiction that there is a 3-round full-information translation
᐀3 from C(n, t) to A(n, t) that is correct with t ⬎ 2 and 3t ⬍ n ⱕ 4t ⫺ 3.
Consider any execution of ⌸imp ⫽ ᐀3(⌸) (where ⌸ was defined in Section
5.3.1).10 As in Section 5.3.3, ⌸imp is full-information, and we focus on message
10

The proof in this section omits details of ⌸imp’s functions ᏿⌸ and Ᏼ⌸ as well as appeals to
conditions (c), (d⬘), and (e) of nonuniform implementations. These functions and conditions would
be used here just as they were in Section 5.3.3.

Simplifying Fault-Tolerance

FIG. 4.

523

Vector received by processor r 僆 A 1 , forcing it to receive 1.

exchanges as they relate to b’s initial simulated state. For each p 僆 ᏼ, let v p be
the message received from b in round 1. Let V p be the vector of echoed messages
that p receives in round 2, where V p [q] is the message received from processor q
(note that V p [ p] ⫽ v p if p is correct). Let M p be the matrix of messages that p
receives in round 3, where M p [q] is the vector that p receives from q in round 3;
M p [q][r] is the component of this vector corresponding to processor r. (Note
that M p [ p] ⫽ V p if p is correct.) Before proceeding with the proof, we prove two
lemmas that hold for any history of ⌸imp in A(n, t). These lemmas will be used
in the proof below. Their proofs are similar to those of Lemma 5.5 and 5.6 but
are included here because of their subtlety. Lemma 5.8 shows that p must
simulate the receipt of m if it is possible that b is correct and its simulated state
was m at the beginning of round 1.
LEMMA 5.8. Consider any set of processors G such that b 僆 G and 兩G兩 ⫽ n ⫺ t,
and let m 僆 {0, 1}. If a correct processor p 僆 G has Mp[q][r] ⫽ m for all q, r 僆 G,
then p must set its simulated state to m at the end of round 3.
PROOF. Note that, since p is correct and in G and V p ⫽ M p [ p], V p [q] ⫽ m
for all q 僆 G; since v p ⫽ V p [ p], v p ⫽ m. Let H be the history under
consideration. Consider another execution H⬘ in which all processors in G are
correct and the t processors in B ⫽ ᏼ ⶿ G are faulty. In H⬘, b’s initial simulated
state is m. In round 2, each processor q 僆 B sends to each processor r 僆 G the
value M p [r][q] (specifically, q sends M p [ p][q] ⫽ V p [q] to p, as it did in H). In
round 3, each processor q 僆 B sends the vector M p [q] to p. By definition, the
messages p received from processors in B are exactly those received in H. A
simple inductive argument shows that the messages received from processors in
G are also same in the first phase of the two histories. Thus, p cannot distinguish
the first three rounds of H and H⬘. Since b and p are correct in H⬘, constraint (1)
of Lemma 5.4 implies that p’s simulated state after phase 1 of H⬘ must be m.
Since p is correct in H and cannot distinguish H from H⬘, its simulated state is m
at the end of phase 1 of H. e
Lemma 5.9 considers cases in which the support for m may not be not strong
enough for p to believe that b is correct. In these cases, however, the support is
strong enough that p would not be able to convince other processors that b is
faulty. For this reason, p must simulate the receipt of m because to do otherwise
might lead to behavior inconsistent with crash failures. The general idea is that, if
the processors in G ⶿ G⬘ (including b) are faulty, then it is possible that some
other correct processor might receive messages such as those described in the
hypothesis of Lemma 5.8 and never believe that b is faulty.
LEMMA 5.9. Let G be a set containing b such that 兩G兩 ⫽ n ⫺ t, let G⬘ 債 G⶿{b}
be such 兩G⬘兩 ⫽ n ⫺ 2t, and let m 僆 {0, 1}. Let p 僆 G be a correct processor such
that the following facts hold of the matrix Mp at the end of round 3:
(1) (@q 僆 G⬘)(@r 僆 G)(M p [q][r] ⫽ m);

524

R. A. BAZZI AND G. NEIGER

(2) (@q 僆 G⬘)(M p [ p][q] ⫽ m); and
(3) (@q, r 僆 ᏼ)(@s 僆 G)(M p [q][s] ⫽ M p [r][s]).
Then p’s simulated state must be m at the end of round 3.
PROOF. Let H be the execution under consideration. We will construct
executions H1 and H2 with the same properties as in the proof of Lemma 5.6: p
cannot distinguish the first phase of H and H1 and processors in G⬘ cannot
distinguish the first two phases of H1 and H2.
H1 is defined as follows: The t processors in G ⶿ G⬘ (including b) are faulty
while those in G⬘ 艛 G (including p) are correct. The following describes the
behavior of the faulty processors in the first two phases of H1:
—In round 1, b sends to each correct processor q the message M p [q][q]
(messages sent by the other faulty processors are irrelevant in round 1).
—In round 2, faulty processor q sends to correct processor r the message
M p [r][q].
—In round 3, faulty processor q sends the vector M p [q] to p. To all other correct
processors it sends the vector V such that V[r] ⫽ M p [r][r] if r is correct and
V[r] ⫽ m otherwise.
—In each round of phase 2, each faulty processor sends all correct processors
exactly the same messages that are sent by processors in G⬘.
We confirm first that p cannot distinguish the first three rounds of H and H1 by
considering, round by round, the messages received by p:
—In round 1 of H1, p receives M p [ p][ p] from b; as noted earlier, M p [ p][ p] ⫽
V p [ p] ⫽ v p , which is the value received by p from b in round 1 of H. The
messages sent by other processors in round 1 are irrelevant.
—In round 2 of H1, p receives a relayed message from each processor q; this
should be V p [q] ⫽ M p [ p][q]. Consider separately the following two cases:
—q is correct (i.e., q 僆 G⬘ 艛 G). In this case, q relays the value that it
received from b, which is M p [q][q]. If q 僆 G⬘, then M p [q][q] ⫽ m ⫽
M p [ p][q] by hypotheses (1) and (2). If q 僆 G, then M p [q][q] ⫽ M p [ p][q]
by hypothesis (3).
—q is faulty. In this case, q sends M p [ p][q] to p, as desired.
—In round 3 of H1, p receives a relayed vector from each q; this should be
M p [q]. Again, consider these cases:
—q is correct. Here, p receives from q the vector that q received in round 2;
call this V q . We need to verify that V q [r] ⫽ M p [q][r] for all r. Consider two
subcases:
—r is correct. In this case, V q [r] ⫽ V p [r] (r sends the same to all in round
2), which is M p [ p][r]. If r 僆 G⬘, then M p [q][r] ⫽ m ⫽ M p [ p][r] by
hypotheses (1) and (2). If r 僆 G, then M p [ p][r] ⫽ M p [q][r] by
hypothesis (3).
—r is faulty. In this case, r sent M p [q][r] to q, so the requirement is met.
—q is faulty. In this case, q sends M p [q] to p, as desired.

Simplifying Fault-Tolerance

525

Thus, p cannot distinguish phase 1 of H and H1; it will set its simulated state at
the end of that phase identically in both executions.
We will next observe that, in H1, each processor q 僆 G⬘ (which is correct)
receives messages satisfying the hypothesis of Lemma 5.8 (with set G), implying
that it will set its simulated state to m at the end of phase 1. Specifically, we need
to verify that, in round 3, q receives from each processor r 僆 G the vector V
such that V[s] ⫽ m for all s 僆 G. Consider the following two cases:
—r 僆 G⬘. In this case, r is correct in H1 and sends q the vector that it received
in round 2. Since r is correct, it sends q the same vector that it sent p and this
is M p [r]. By hypothesis (1), M p [r][s] ⫽ m for all s 僆 G, as desired.
—r 僆 G ⶿ G⬘. In this case, r is faulty in H1. By definition, it sends the vector V
such that V[s] ⫽ M p [s][s] if s is correct and V[s] ⫽ m otherwise. If s 僆 G is
correct, then s 僆 G⬘ and, by hypothesis (1), M p [s][s] ⫽ m. Thus, V[s] ⫽ m
for all s 僆 G, as desired.
As noted above, Lemma 5.8 implies that each processor in G⬘ sets its simulated
state to m at the end of phase 1 of H1. By constraint (2) of Lemma 5.4, p’s
simulated state at the end of phase 1 of H1 (and of H) is either m or f.
H2 is defined as follows: Processors in G (including p) are faulty while those in
G (including b) are correct; b begins in simulated state m. In the first two phases
of H2, the faulty processors send exactly the same messages that they do in H1.
Arguments similar to those in the proof of Lemma 5.8 can be used to show that
processors in G⬘ cannot distinguish phase 1 of H1 and H2 (recall that H1 satisfies
the hypotheses of that lemma for any processor in G⬘). Now consider phase 2 of
H2. Processors in G send the same messages in H1 and H2 because they are faulty
in H2 (the definition of H2 specifies that faulty processors behave identically in
both executions). In H1, processors in G ⶿ G⬘ (which are faulty in that execution)
send, in phase 2, the same messages sent by those in G⬘. A simple inductive
proof shows that, in rounds 3–5 (and all subsequent rounds) of H2, they receive
the same messages as those in G⬘. Thus, processors in G⬘ will also receive (and
send) the same messages in phase 2 of H2 and H1. Thus, processors in G⬘ cannot
distinguish the first two phases of those executions. (The message passing in H2 is
not identical to that of H1; however, the executions differ only on the messages
that are sent to processors faulty in H2.)
By constraint (1) of Lemma 5.4, b must set its simulated state at the end of
phase 1 of H2 to m. By constraint (4) of that lemma, each processor in G⬘ must
set its simulated state at the end of phase 2 of H2 to s such that s[b] is b’s
simulated state at the end of phase 1, that is, m. Since processors in G⬘ cannot
distinguish H1 and H2, each sets is simulated state to s with s[b] ⫽ m ⫽ ⬜ at the
end of phase 2 of H1. Since p is correct in H1, constraint (3) of Lemma 5.4
implies that p’s simulated state at the end of phase 1 of H1 cannot be f. We
observed earlier that this state is either m or f, so it must be m. As noted above,
p cannot distinguish the first phase of H and H1. Thus, p sets its simulated state
to m at the end of phase 1 of H, as desired. e
We now prove that a 3-round translation cannot exist. Let A 0 , A 1 , C 0 ,
and C 1 be a partition of ᏼ ⶿ {b} such that 兩A 0 兩 ⫽ 兩A 1 兩 ⫽ t ⫺ 1, 兩C 0 兩 ⫽
(n ⫺ 2t ⫹ 1)/ 2, and 兩C 1 兩 ⫽ (n ⫺ 2t ⫹ 1)/ 2. Note that, since t ⬎ 2 and
3t ⬍ n ⱕ 4t ⫺ 3, 2 ⬍ 兩Ci兩 ⱕ t ⫺ 1, 兩Ai兩 ⱖ 2, and 兩A i 艛 C i 兩 ⬎ t, for i 僆 {0, 1}.

526

R. A. BAZZI AND G. NEIGER

FIG. 5.

Matrix M f received by processor p.

Suppose that, at the end of round 3, some correct processor p 僆 C 0 艛 C 1
receives vectors composing the matrix M f shown in Figure 5.11 It is clear to p that
b is faulty: in round 2, too many processors (兩A 0 艛 C 0 兩 ⬎ t) give too much
support to 0 for b to have correctly sent 1, and too many (兩A 1 艛 C 1 兩 ⬎ t) give
too much support to 1 for it to have correctly sent 0. We will show that p’s
simulated state at the end of phase 2 cannot have a value that satisfies the
requirements on histories of ⌸.
Suppose that, in round 4 (the first round of phase 2), all processors but two
send messages each indicating that they too had this array after phase 1. The two
remaining processors, q 僆 A 0 and r 僆 A 1 send different messages. In round 3, q
claims that it received the matrix M 0 in Figure 6, while r claims that it received
M 1 as given in Figure 7. We will use Lemma 5.9 to show that, if q (respectively,
r) is correct, then q (respectively, r) should set its simulated state to 0
(respectively, 1) at the end of phase 1. While p can determine that q and r cannot
both be correct, it cannot determine which, and the other’s being correct remains
a possibility. We will argue that this prevents p from properly setting its
simulated state.
We now show that, if q is correct in H, Lemma 5.9 implies that q’s simulated
state was 0 at the end of phase 1. We define sets G and G⬘ so that the
preconditions of Lemma 5.9 hold. Recall that 兩C 0 艛 C 1 兩 ⫽ n ⫺ 2t ⫹ 1 and
兩C 1 兩 ⱕ t ⫺ 1; thus, 兩C 0 兩 ⱖ n ⫺ 3t ⫹ 2. Let D 0 債 C 0 be such that 兩D 0 兩 ⫽ n ⫺
3t ⫹ 2, and let G ⫽ {b} 艛 ( A 0 ⶿ {q}) 艛 A 1 艛 D 0 (note that b 僆 G and q 僆
G). Note that 兩G兩 ⫽ 1 ⫹ (t ⫺ 2) ⫹ (t ⫺ 1) ⫹ (n ⫺ 3t ⫹ 2) ⫽ n ⫺ t, as
desired.
Since n ⱕ 4t ⫺ 3, t ⫺ 2 ⱖ n ⫺ 3t ⫹ 1; thus, 兩A 0 ⶿ {q}兩 ⱖ n ⫺ 3t ⫹ 1. Let
B 0 債 A 0 ⶿ {q} be such that 兩B 0 兩 ⫽ n ⫺ 3t ⫹ 1, and let G⬘ ⫽ B 0 艛 A 1 . Note
that G⬘ 債 G ⶿ {b} and 兩G⬘兩 ⫽ (n ⫺ 3t ⫹ 1) ⫹ (t ⫺ 1) ⫽ n ⫺ 2t, as desired.
Notice now that the following hold for M 0 :
(1) (@s 僆 A 0
11

艛 A 1 )(@t 僆 {b} 艛 A 0 艛 A 1 艛 C 0 )(M 0 [s][t] ⫽ 0);

In Figures 5, 6, and 7, each row corresponds to a message received in the third round of phase 1.

Simplifying Fault-Tolerance

FIG. 6.

Matrix M 0 received by processor q 僆 A 0 , forcing it to receive 0.

FIG. 7.

Matrix M 1 received by processor r 僆 A 1 , forcing it to receive 1.

527

(2) (@s 僆 A 0 艛 A 1 )(M 0 [q][s] ⫽ 0) (recall q 僆 A 0 );
(3) (@s, t 僆 ᏼ)(@u 僆 A 0 艛 C 0 艛 C 1 )(M 0 [s][u] ⫽ M 0 [t][u]).
Because G⬘ 債 A 0 艛 A 1 , G 債 {b} 艛 A 0 艛 A 1 艛 C 0 , and G 債 A 0 艛 C 0 艛
C 1 , the three hypotheses of Lemma 5.9 hold for M 0 and q. If q is correct, the
lemma implies that q set its simulated state to 0 at the end of phase 1. By similar
arguments, Lemma 5.9 implies that, if r is correct, it set its simulated state to 1 at
the end of phase 1.
In round 5, all processors echo to p messages indicating that they received, in
round 3, the same messages that p did, and they do this again in round 6. The
messages received by p are consistent with two different executions H0 and H1:
—In H0, the faulty processors are those in {b} 艛 A 1 (and q, which is in A 0 , is
correct). Here, q’s simulated state is 0 at the end of phase 1 by Lemma 5.9.

528

R. A. BAZZI AND G. NEIGER

Constraint (4) of Lemma 5.4 implies that, at the end of phase 2, p’s simulated
state is s such that s[q] ⫽ 0.
—In H1, the faulty processors are those in {b} 艛 A 0 , while r is correct. Here, r’s
simulated state is 1 at the end of phase 1 and, at the end of phase 2, p’s state
is s such that s[r] ⫽ 1.
The remainder of the argument is as in Section 5.3.3; p knows that exactly one of
q and r is correct, but it cannot tell which. One being correct requires p to
behave in a way inconsistent with the other being correct; thus, no translation
can function properly in all executions. This proves that the translation ᐀3
cannot exist, giving us the following theorem:
THEOREM 5.10. If t ⬎ 2 and n ⱕ 4t ⫺ 3, there can be no 3-round translation
from C(n, t) to A(n, t).
5.4. DISCUSSION. As with the rest of this paper, this section has considered
only translations that use a fixed number of rounds to translate every round of
the original protocol. If that restriction were relaxed, the results proven here
would apply only to the first one or two rounds. In fact, it is easy to show that
one can use an appropriate agreement algorithm to produce a translation that
simulates the first round with t ⫹ 1 and has a round-complexity of 0 for
subsequent rounds. Correct processors can use the algorithm to agree on all
processors’ initial simulated states (even those of the faulty processors) and then
simulate a common execution of the remainder of the original protocol locally.
Section 2.2 noted that the model defined in this paper uses initial states to
capture processors’ initial inputs and that the model was not extended to allow
inputs in every round only to simplify the presentation. If the model had been so
extended, the agreement-based approach requires t ⫹ 1 simulating rounds for
every round, resulting in a translation with round-complexity higher than the
lower bounds proven here or than the translations exhibited in Sections 6 – 8.
6. Translations from Crash to Send Omission Failures
Neiger and Toueg [1990] showed there can be no uniform 1-round translation
from crash to send-omission failures. They then gave a uniform 2-round canonical translation from crash to send-omission failures that required only n ⬎ t.
Thus, this translation is optimal with respect to both fault-tolerance and roundcomplexity.
Their arguments can also show that there can be no nonuniform 1-round
translation from crash to send-omission failures. Since every uniform translation
is also a nonuniform translation, their translation is also optimal (with respect to
fault-tolerance and round-complexity) for nonuniform translations as well.
7. Translations from Crash to General Omission Failures
Neiger and Toueg [1990] gave a uniform 2-round canonical translation from
crash to general omission failures that requires n ⬎ 2t. They also showed that
any uniform translation from crash to general omission failures requires n ⬎ 2t.
Because they had shown that there can be no 1-round translation from crash to
send-omission failures (see Section 6), there can obviously be no 1-round
translation to the more severe general omission failures. Thus, their translation is

Simplifying Fault-Tolerance

529

optimal for uniform translations with respect to both round-complexity and
fault-tolerance. This section explores nonuniform translations from crash to
general omission failures.
Section 7.1 below presents a hierarchy of nonuniform canonical translations,
only one of which requires n ⬎ 2t. These vary with respect to their faulttolerance and round-complexity. The results of Section 5.2 show that each
translation in the hierarchy is optimal in the sense that it uses the minimum
number of rounds necessary for a given fault-tolerance. Together, these results
show a tight trade-off between the two measures.
The translations are parameterized by n and t; specifically, each translation
requires z rounds to simulate one round, where z ⫽ t/(n ⫺ t) ⫹ t/(n ⫺ t) ⫹ 1.
Section 5.2 showed that there can be no translation from crash to general
omission failures with round-complexity equal to z ⫺ 1 ( z is 1 greater than y as
defined in that section). In the cases where n ⬎ 2t, z ⫽ t/(n ⫺ t) ⫹
t/(n ⫺ t) ⫹ 1 ⫽ 0 ⫹ 1 ⫹ 1 ⫽ 2. This gives a translation with exactly the
round-complexity of that of Neiger and Toueg: 2 rounds. Note that the larger the
ratio t/(n ⫺ t) (i.e., the more failures for a fixed n), the higher the roundcomplexity.
7.1. SPECIFICATION OF THE TRANSLATIONS. A translation parameterized by z
is given in Figure 8. Given a protocol ⌸c with message function ␮ c and
state-transition function ␦ c , the figure shows the code for implementation ⌸g ⫽
᐀(⌸c) that tolerates general omission failures.12
In each phase, each processor maintains in msgs the array of messages for that
phase of which it is aware; initially, it is aware only of its own message, and all
other elements of the array are ⬜. During the z rounds of a phase, processors
exchange these arrays and other information; they use these arrays to decide on
the messages whose receipt they simulate at the end of a phase (see below for
more details). This redundant communication is needed to mask the more severe
general omission failures and to make it appear as if only crash failures occur.
In addition to the array msgs, processors maintain auxiliary variables to track
the failures in the system. These include the set faulty and the set array accuse.
The set faulty is the set of processors that a processor believes to be faulty.
(Lemma 7.3 will show that all processors in the faulty set of a correct processor
are indeed faulty.) A processor includes its set faulty with each message that it
sends. Element accuse[q] of p’s accuse array contains the processors that sent
messages to p accusing q of being faulty (by including q in their faulty sets).
As noted earlier, each processor sends in each round its array msgs and its set
faulty to all processors not in faulty. Upon receiving such messages, a processor
maintains its own set faulty and set array accuse as follows. It adds to faulty any
processor from which it does not receive message in that round. Following this, it
12

Note that ⌸g is not in the standard form (e.g., processors do not send messages to all other
processors in ⌸g). This is done to simplify the exposition and does not affect the applicability of the
results. In addition, certain components of processors’ states (e.g., the accuse sets) are retained from
one round to another. Technically, such components cannot be “remembered” because the definition
of protocols in Section 2.2 does not include the processor’s current state as input to its statetransition function. We could compensate for this by having processors send all their state
components in every round. (Because processors subject to general omission failures correctly send to
and receive from themselves, even faulty processors will maintain their states correctly from round to
round.) Doing so would be straightforward but would complicate the exposition of these translations.

530

R. A. BAZZI AND G. NEIGER

FIG. 8.

Implementation ⌸g ⫽ ᐀(⌸c) as executed by processor p.

ignores any messages it receives directly from processors in faulty. (A processor
may simulate–at the end of a phase–the receipt of messages from processors in
faulty if these messages are relayed to it by other processors.) For each processor
q, it adds to accuse[q] any processor that included q in its faulty set. Finally, q is
added to faulty if sufficiently many processors accuse q of being faulty or if q
accuses too many processors of being faulty. Specifically, if the union of the set p
already believes to be faulty, the set q claims is faulty, and the set accusing q of
being faulty has size greater than t, then p and q cannot both be correct and p
places q in its set faulty. Note that, if q is in p’s faulty set at the end of some
round, then p will be in q’s faulty set by the middle of the next round; this is
because a processor does not send to processors in its faulty set.
Each processor maintains the array msgs so that, at the beginning of each
phase, it is initially all-⬜, except for its own entry. In each round, a processor
updates its msgs array using the corresponding arrays received from processors
not in its faulty set. It does this by combining the arrays, removing ⬜ entries
when possible. In doing so, we say that it processes relayed messages. Note that p
processes messages from q in a round if and only if q is not in p’s set faulty at the

Simplifying Fault-Tolerance

531

end of that round. At the end of round z (the last round) of a phase, a processor
simulates the receipt of messages that are in msgs at that time.
7.2. THE SIMULATION FUNCTIONS. Recall that any canonical translation ᐀
from C(n, t) to G(n, t) must have a canonical state-simulation function ᏿.
Suppose that protocol ⌸c has state set ᏽ. Inspection of Figure 8 shows that the
state of a processor running ⌸g ⫽ ᐀(⌸c) is of the form

具 state, faulty, accuse, msgs典,
so the state set of ⌸g is ᏽ ⫻ 2 ᏼ ⫻ (2 ᏼ ) n ⫻ (ᏹ⬘) n . Define ᏿(具state, faulty,
accuse, msgs典) ⫽ state.
᐀ must also have a canonical history-simulation function Ᏼ. Ᏼ must map any
history Hg of ᐀(⌸) (for some ⌸) to a history Hc that satisfies conditions (c), (d⬘),
and (e) of Section 4.1 (if ᐀ is nonuniform). This section defines a historysimulation function (parameterized by z) that is required to prove that the
parameterized translation given above is correct.
Given history Hg, we construct Hc ⫽ Ᏼ(Hg) so that any processor p 僆
Faulty(Hc) fails by crashing. Specifically, it crashes in round i p of Hc, where i p is
defined as follows: At any point in Hg consider the following directed graph G ⫽
(V, E). The set V of vertices is the set ᏼ of processors. There is a directed edge
( p, q) 僆 E if q has not crashed and has p 僆
兾 faulty. If ( p, q) 僆 E, we write p 3
q. Let 3
* be the transitive and reflexive closure of 3. Now, let i p be the first
phase i such that, at the end of the first round of phase i ⫹ 1, p 3
* q holds for
no q 僆 Correct(Hg).13 If there is no such phase, let i p ⫽ ⬁ and, as we will show,
p 僆 Correct(Hc). Note that, if p 3
* q does not hold at the end of a given round,
then q cannot receive, either directly or indirectly, a message first sent by p in
any subsequent round. This is because processors do not process messages from
those in their faulty sets and crashed processors do not process messages at all.
We define Hc ⫽ 具⌸c, Qc, Sc, Rc典 as follows, considering separately the behavior
of each processor p. Consider first the case where i ⬍ i p :
—Set Qc(i, p) to be p’s value of state at the beginning of phase i.
—For each q 僆 ᏼ, set Sc(i, p, q) to be p’s value of msgs[ p] in round 1 of phase
i of Hg (note that Sc(i, p, q) ⫽ ␮ c (i, p, Qc(i, p)) by Figure 8).
—For each q 僆 ᏼ, set Rc(i, p, q) to be p’s value for msgs[q] at the end of phase
i.
Note that, for all rounds before i p , the behavior of p in Hc corresponds exactly to
the simulation being performed in Hg. Beginning in round i p , its behavior is
specified instead as follows. Set Qc(i p , p) to be p’s value of state at the beginning
of phase i p . For each q 僆 ᏼ, set Sc(i p , p, q) ⫽ Rc(i p , q, p) and Rc(i p , p, q) ⫽
⬜. 14 If i ⬎ i p , then set Qc(i, p) ⫽ Qc(i p , p) and, for each q 僆 ᏼ, Sc(i, p, q) ⫽
Rc(i, p, q) ⫽ ⬜.

13

Note that, since i p is defined to be a phase, it must be at least 1. Thus, if p 3
* q holds for no q 僆
Correct(Hg) at the end of first round of phase 1, i p is 1, not 0.
14
Notice that the only dependence within this definition is that Sc(i p , p, q) is set to Rc(i p , q, p).
Since Rc(i p , q, p) is always defined independently, the definition is well founded.

532

R. A. BAZZI AND G. NEIGER

The following section uses this history-simulation function to show that ᐀
correctly translates from C(n, t) to G(n, t) by showing that ᏿ and Ᏼ satisfy the
conditions of the definition of nonuniform translations given in Section 4.2.
7.3. PROOF OF CORRECTNESS. This section proves various properties about
translation ᐀ and history-simulation function Ᏼ, ending with a proof of correctness of the translation. The following lemma, stated without proof, follows from
a simple inductive proof based on Figure 8:
LEMMA 7.1. If p sets msgs[ p] to m in the first round of a phase, then all
processors have msgs[ p] 僆 {m, ⬜} throughout that phase.
The following lemma shows a relationships between the crashing of a processor in Hg and its crashing in Hc:
LEMMA 7.2.

If p crashes in or before phase i, then i ⱖ ip.

PROOF. If p crashes in or before phase i, then all noncrashed processors have
p 僆 faulty by the end of the first round of phase i ⫹ 1. This implies that, at
that time, there is no q 僆 Correct(Hg) such that p 3
* q. By the definition of i p ,
i ⱖ i p. e
The next lemma shows that no correct processor ever considers another
correct processor to be faulty:
LEMMA 7.3. In the executions of ⌸g , no correct processor ever belongs to the
faulty set of another correct processor.
PROOF. The proof is by induction on the number i of rounds executed. The
base case (i ⫽ 0) is trivial because all processors have empty faulty sets initially.
Now assume that the lemma holds through round i and suppose that some
correct processor p adds another processor q to its faulty set in round i ⫹ 1. This
can happen for one of two reasons:
—p does not receive a message from q. This can happen for one of two reasons:
—q crashed or omitted to send a message because of a failure. In this case, q is
faulty.
—q chose not to send a message to p because it had p 僆 faulty at the end of
round i. By the inductive hypothesis, this means that q is faulty (since p is
assumed to be correct).
Thus, q is faulty in this case.
—p found 兩accuse[q] 艛 faultyq 艛 faulty兩 ⬎ t. Since there are only t faulty
processors and, by induction, all elements of p’s set faulty are faulty, there
must be at least one correct processor r 僆 accuse[q] 艛 faultyq . If r 僆
accuse[q], then q was in r’s faulty set at the end of round i, so q is faulty by
induction. If r 僆 faultyq , then r was in q’s faulty set at the end of round i and,
again by induction, q is faulty.
In all cases, q is faulty, so the lemma holds.

e

The following is a corollary to Lemma 7.3.
COROLLARY 7.4.

If p 僆 Correct(Hg ), then i p ⫽ ⬁.

Simplifying Fault-Tolerance

533

PROOF. By Lemma 7.3, no correct processor q ever has p 僆 faulty. By the
definition of 3, this means that p 3 q is always true for all q 僆 Correct(Hg).
This implies that i p ⫽ ⬁. e
Corollary 7.4 implies that a correct processor’s behavior in Hc always corresponds to the simulation being performed in Hg (see the definition of Ᏼ above).
The behavior of faulty processors need not do so because the translation is
nonuniform.
The following lemma is the core of the correctness proof of the translation. It
shows that, if there are failures sufficient to prevent two processors from
communicating in a given phase, then the faulty sets of all processors will cause a
“partition” to occur in the next phase. At least one of the two processors must be
faulty and will be separated from the correct processors; to them, it will appear
to crash. As we will see, this is sufficient to simulate crash failures.
LEMMA 7.5 (PARTITION LEMMA). Suppose that p and q are two processors such
that p sets msgs[ p] ⫽ ⬜ in the first round of phase i and that q has msgs[ p] ⫽ ⬜ at
the end of the last round of phase i. Then there is a partition of ᏼ into two sets C
and F such that the following hold:
—all correct processors are in C;
—p and q are not both in C;
—every r 僆 C that has not crashed has F 債 faulty by the end of the first round of
phase i ⫹ 1; and
—every s 僆 F that has not crashed has C 債 faulty by the end of the first round of
phase i ⫹ 1.
PROOF. For this proof, round numbers will be measured from the beginning
of phase i; when we talk about a round k we mean the kth round of phase i.
Thus, the first round of phase i ⫹ 1 is called z ⫹ 1.
For 1 ⱕ j ⱕ z, let M j be the set of processors that have not crashed and that
have msgs[ p] ⫽ ⬜ at the end of round j; conventionally define M ⫺1 ⫽  and
M 0 ⫽ { p}. Note that, by the definition of M j , q 僆
兾 M z . Furthermore, for all 0 ⱕ
j ⱕ z, each noncrashed processor r 僆 M j has M j⫺1 債 faulty at the end of round
j; otherwise, r would have processed messages from some processor in M j⫺1 in
round j and would thus have msgs[ p] ⫽ ⬜, putting r in M j . Consequently, each
noncrashed processor in M j⫺1 has M j 債 faulty by the middle of round j ⫹ 1
(recall that processors refuse to send to those in their faulty sets). For all j, 1 ⱕ
j ⱕ z, let L j ⫽ M j ⶿ M j⫺2 . Informally, L j is the set of processors that learn of p’s
message for the first time in round j or round j ⫺ 1 of phase i. Note that, if k is
odd (1 ⱕ k ⱕ z), then M k ⫽ L 1 艛 L 3 艛 . . . 艛 L k . We consider the following
two cases:
—兩L j 兩 ⱖ n ⫺ t for all j, 1 ⱕ j ⱕ z. Recall that z ⫽ t/(n ⫺ t) ⫹ t/(n ⫺ t)
⫹ 1. Consider the following two subcases:
—t is a multiple of n ⫺ t. In this case, z ⫽ 2t/(n ⫺ t) ⫹ 1 is odd, and we have

534

R. A. BAZZI AND G. NEIGER

兩M z 兩

⫽ 兩L 1 兩 ⫹ 兩L 3 兩 ⫹ · · · ⫹ 兩L z⫺2 兩 ⫹ 兩L z 兩
z⫹1
共兩 L j 兩 ⱖ n ⫺ t for all j兲
共n ⫺ t兲
ⱖ
2
t
⫽
⫹ 1 共n ⫺ t兲
共n ⫺ t兲
⫽ n.

冉 冊
冉 冊

This means that all processors, and in particular q, belong to M z ; this is a
contradiction.
—t is not a multiple of n ⫺ t. In this case, z ⫺ 1 ⫽ t/(n ⫺ t) ⫹
t/(n ⫺ t) ⫽ 2t/(n ⫺ t) ⫹ 1 is odd, and we have

兩M z⫺1 兩

⫽ 兩L 1 兩 ⫹ 兩L 3 兩 ⫹ · · · ⫹ 兩L z⫺1 兩
z
共n ⫺ t兲
ⱖ
2
t
⫽
⫹ 1 共n ⫺ t兲
n⫺t
t
⫽
共n ⫺ t兲 ⫹ 共n ⫺ t兲
n⫺t
t ⫺ 共 t mod共n ⫺ t兲兲
⫽
共n ⫺ t兲 ⫹ 共n ⫺ t兲
n⫺t
a
a ⫺ 共a mod b兲
because
⫽
for all b ⬎ 0
b
b
⫽ t ⫺ 共t mod共n ⫺ t兲兲 ⫹ 共n ⫺ t兲
⬎t
共共t mod共n ⫺ t兲兲 ⬍ n ⫺ t兲.

冉冊
冉  冊
 

冉



冊

Because q does not simulate the receipt of p’s message at the end of first
phase, it does not process messages from any processor in M z⫺1 in round
z. Thus, it has all of M z⫺1 , more than t processors, in its faulty set at the
end of phase i (recall that this part of the proof assumes that q does not
crash by the end of phase i). At least one of these is correct, so, by Lemma
7.3, q is faulty. In round z ⫹ 1, each processor r will either receive nothing
from q or will receive faultyq , which contains more than t elements. In
either case, r will add q to its faulty set in that round. Also, if q does not
crash in round z ⫹ 1, it will add r to its faulty set at the same time because
it will find 兩accuse[r] 艛 faultyr 艛 faulty兩 ⱖ 兩faulty兩 ⬎ t. Thus, the desired
partition can then be 具C, F典 ⫽ 具ᏼ ⶿ {q}, {q}典.
—兩L j 兩 ⬍ n ⫺ t for some j, 1 ⱕ j ⱕ z. Recall that, at the end of round j ⫺ 1,
every non-crashed processor in M j⫺1 has M j⫺2 債 faulty. Similarly, each
non-crashed processor in M j has M j⫺1 債 faulty at the end of round j. Thus, at
the end of round j ⫹ 1, each non-crashed processor p a 僆 M j⫺1 will have every
processor p b 僆 M j either in faulty (if it already had it there or if p a receives no
message from p b in round j ⫹ 1) or in accuse[r] for every processor r 僆 M j⫺1

Simplifying Fault-Tolerance

535

(if p b was not in p a ’s faulty set and p a receives a message from p b ). This means
that, at that time, each non-crashed processor in M j⫺1 will find, for each r 僆
M j⫺1 , 兩accuse[r] 艛 faultyr 艛 faulty兩 ⱖ 兩accuse[r] 艛 faulty兩 ⱖ 兩M j 艛 M j⫺2 兩 ⫽
兩ᏼ ⶿ L j 兩. Since 兩L j 兩 ⬍ n ⫺ t, 兩ᏼ ⶿ L j 兩 ⬎ n ⫺ (n ⫺ t) ⫽ t. Thus, all
non-crashed processors in M j⫺1 will have M j⫺1 債 faulty by the end of round
j ⫹ 1. Consider now a processor r 僆 M j⫺1 that does not crash by the end of
round j ⫹ 1. Since each non-crashed processor in M j has r 僆 faulty by the end
of round j, r will have M j 債 faulty by the middle of round j ⫹ 1 (because each
non-crashed processor in M j will decline to send to r in that round). Consider
now some processor s 僆 M j ⶿ M j⫺1 . Because s 僆 M j⫺1 , it will have M j⫺2 債
faulty by the end of round j ⫺ 1. If r does not receive a message from s in
round j ⫹ 1, it adds s to faulty by that time. If it does receive such a message,
r finds 兩accuse[s] 艛 faultys 艛 faulty兩 ⱖ 兩faultys 艛 faulty兩 ⱖ 兩M j⫺2 艛 M j 兩 ⬎ t
(as above) and thus adds s to faulty by that time. This means that, by the end of
round j ⫹ 1, all processors in M j⫺1 that have not crashed have M j 艛
(M j ⶿ M j⫺1 ) ⫽ M j⫺1 債 faulty.
Thus, M j⫺1 and M j⫺1 form a partition of ᏼ such that each non-crashed
processor in one set deems the entire other set to be faulty by the end of round
j ⫹ 1. Since j ⱕ z, this happens by the end of the first round of phase i ⫹ 1.
By Lemma 7.3, the set of correct processors is a subset of either M j⫺1 or M j⫺1 .
Let C be the set containing the correct processors and let F be the other (its
complement). Note that, since p 僆 M 0 債 M j⫺1 and q 僆 M z 債 M j⫺1 , p and q
are in the two different sides of the partition. 具C, F典 is then the desired
partition.
The desired partition exists in either case, completing the proof.

e

We can use the Partition Lemma to show that, in Hg, communication always
proceeds correctly between any two processors that have not yet “crashed.”
LEMMA 7.6. If i ⬍ min{ip, iq} and p sets msgs[ p] to m at the beginning of phase
i, then q has msgs[ p] ⫽ m at the end of that phase.
PROOF. Notice that, by Lemma 7.2, i ⬍ min{i p , i q } implies that neither p nor
q have crashed in or before phase i. Suppose for a contradiction that q does not
have msgs[ p] ⫽ m at the end of phase i. Then, by the Partition Lemma, there is
a partition of ᏼ into C and F such that Correct(Hg) 債 C, p and q are not both in
C, every noncrashed r 僆 C has F 債 faulty by the end of the first round of phase
i ⫹ 1, and every noncrashed s 僆 F has C 債 faulty by the end of the first round
of phase i ⫹ 1. Without loss of generality, suppose that p 僆 F. It is clear that, if
p3
* r at the end of the first round of phase i ⫹ 1, then r 僆 F. Since Correct(Hg)
債 C and since C and F are disjoint, this means that p 3
* r for no r 僆
Correct(Hg). Thus, i p ⱕ i, giving a contradiction. e
We can now show that any processor p behaves correctly in Hc until round i p :
LEMMA 7.7.

If i ⬍ ip in Hc , then p 僆 Correct(Hc , i).

PROOF. Note first that, by Lemma 7.2, p has not crashed through the end of
phase i. The remainder of the proof is by induction on i. The base case is
straightforward as Correct(Hc, 0) ⫽ ᏼ. Assume that, for all k ⬍ i ⬍ i p , p 僆
Correct(Hc, k). The proof must show that p behaves correctly in round i itself.

536

R. A. BAZZI AND G. NEIGER

We first show that p sends correctly in round i of Hc. Let q be any processor in
ᏼ. By the definition of Sc, Sc(i, p, q) ⫽ m, p’s value of msgs[ p] from the first
round of phase i. By the definition of Qc, Qc(i, p) is p’s value of state at the
beginning of phase i. By Figure 8, it is clear that m ⫽ ␮c(i, p, state) (recall that
p has not crashed through phase i), so Sc(i, p, q) ⫽ ␮c(i, p, Qc(i, p)), as desired.
We next see that p receives correctly in round i of Hc. Let q be any processor
in ᏼ. By the definition of Rc, Rc(i, p, q) is p’s value of msgs[q] at the end of
phase i. Consider now three cases:
—i ⬍ i q . By the definition of Sc, Sc(i, q, p) ⫽ m, q’s value of msgs[q] from the
first round of phase i. By Lemma 7.6, this is indeed p’s value of msgs[q] at the
end of phase i, so Rc(i, p, q) ⫽ Sc(i, q, p), as desired.
—i ⫽ i q . In this case, Sc(i, q, p) ⫽ Rc(i, p, q) by definition.
—i ⬎ i q . By the definition of Sc, Sc(i, q, p) ⫽ ⬜. We must verify that Rc(i, p, q)
is also ⬜. Suppose not. Then q’s phase i message was somehow forwarded to
p. By Figure 8, this means that q 3
* p at the end of the first round of phase i.
Since i ⬍ i p , there is some r 僆 Correct(Hg) such that p 3
* r at this time. This
implies that q 3
* r at the end of the first round of phase i, giving i ⱕ i q , a
contradiction. We conclude that Rc(i, p, q) ⫽ ⬜, as desired.
Finally, we see that p makes a correct state transition in round i of Hc. We
must verify that Qc(i ⫹ 1, p) ⫽ ␦c(i, p, Rc(i, p)). Since i ⬍ i p , i ⫹ 1 ⱕ i p . By
the definition of Qc, Qc(i ⫹ 1, p) is p’s value of state at the beginning of phase
i ⫹ 1. By the definition of Rc, Rc(i, p) is p’s value of msgs at the end of phase i.
From Figure 8, state ⫽ ␦c(i, p, msgs) (recall that p has not crashed through
phase i), so Qc(i ⫹ 1, p) ⫽ ␦c(i, p, Rc(i, p)), as desired. e
We can now show the following corollary:
COROLLARY 7.8.

Correct(Hg ) 債 Correct(Hc ).

PROOF. Consider some processor p 僆 Correct(Hg). By Corollary 7.4, i p ⫽ ⬁.
By Lemma 7.7, p 僆 Correct(Hc, i) for all i ⬍ i p . This implies that p is in
Correct(Hc), as desired. e
We can now show that any processor in Faulty(Hc) suffers a crash failure and is
correct before it does so:
LEMMA 7.9. If p 僆 Faulty(Hc ), then p commits a crash failure in round i p of
Hc and is correct before round i p .
PROOF. If p 僆 Faulty(Hc), then Lemma 7.7 implies that i p ⬍ ⬁ and p 僆
Correct(Hc, i) for all i ⬍ i p . Thus, if p 僆 Faulty(Hc, i p ), then i p is the least i such
that p 僆 Faulty(Hc, i). We will see that p crashes in round i p of Hc. Let q be any
processor in ᏼ. By the definition of Sc, Sc(i p , p, q) is Rc(i p , q, p). By the
definition of Rc, Rc(i p , q, p) is either ⬜ or q’s value of msgs[ p] at the end of
phase i p . If the latter is not ⬜, then Lemma 7.1 shows that it must be p’s value of
msgs[ p] from the first round of phase i p . This is ␮c(i p , p, state), where state is p’s
value of the variable at the beginning of phase i p . By the definition of Qc,
Qc(i p , p) is also this value of state. Thus, Sc(i p , p, q) is either ⬜ or ␮c(i p , p,
Qc(i p , p)) and p’s sending behavior is consistent with failure by crashing. By the
definition of Rc, Rc(i p , p, q) ⫽ ⬜ for all q 僆 ᏼ.

Simplifying Fault-Tolerance
TABLE I.

537
SUMMARY

OF

TRANSLATIONS

After round i p , p takes no further actions in Hc. By the definitions of Sc and Rc,
p, q) ⫽ Rc(i, p, q) ⫽ ⬜ for all i ⬎ i p . By the definition of Qc, Qc(i, p) ⫽
Qc(i p , p) for all i ⬎ i p . This concludes the proof that p commits a crash failure in
round i p of Hc. e
Sc(i,

We can now conclude that the translation is correct:
THEOREM 7.10. Translation ᐀ is a canonical nonuniform z-round translation
from C(n, t) to G(n, t), where z ⫽ t/(n ⫺ t) ⫹ t/(n ⫺ t) ⫹ 1:
—The following hold for the state-simulation function ᏿ given above and any
protocol ⌸c:
(a) (@p 僆 ᏼ)(@s imp 僆 ᏽ imp;p )(?s 僆 ᏽ p )(᏿(s imp) ⫽ s); and
(b) (@p 僆 ᏼ)(@s 僆 ᏽ p )(?s imp 僆 ᏽ imp;p )(᏿(s imp) ⫽ s).
—The following hold for the history-simulation function Ᏼ given above for any
history Hg ⫽ 具⌸ g , Qg , Sg , Rg 典 of any ᐀(⌸ c ) running in G(n, t):
(c) Hc ⫽ Ᏼ(Hg ) ⫽ 具⌸ c , Qc , Sc , Rc 典 is a history of ⌸ c running in C(n, t),
(d⬘) (@i 僆 Z)(@p 僆 Correct(Hg ))(᏿( Qg ( z(i ⫺ 1) ⫹ 1, p)) ⫽ Qc (i, p)), and
(e) Correct(Hg ) 債 Correct(Hc ).
PROOF. It is not hard to see that ᏿ satisfies (a) and (b). As specified in Figure
8, each s imp 僆 ᏽimp;p is of the form 具s, ,  n , ⬜ n 典. By definition, ᏿(具s, ,  n ,
⬜ n 典) ⫽ s 僆 ᏽ p , as required for (a). For any s 僆 ᏽ p , 具s, ,  n , ⬜ n 典 僆 ᏽ imp;p ,
satisfying (b).
By definition, Hc is a history of ⌸c. To prove that it is a history running in
C(n, t), it suffices to show that there are at most t faulty processors and that
they are subject only to crash failures. Since there are at most t faulty processors
in Hg, Corollary 7.8 implies that there are most t faulty processors in Hc. Lemma
7.9 shows that these fail only by crashing, satisfying (c). Corollary 7.4 shows that,
if p 僆 Correct(Hg), i p ⫽ ⬁. Thus, the functions ᏿ and Qc are both chosen in such
a way that, if p 僆 Correct(Hg), both ᏿(Qg( z(i ⫺ 1) ⫹ 1, p)) and Qc(i, p) are p’s
value of the variable state at the beginning of round z(i ⫺ 1) ⫹ 1. This shows
that (d⬘) is satisfied. Condition (e) follows directly from Corollary 7.8. e
7.4. DISCUSSION. This section presented a hierarchy of nonuniform translations from crash to general omission failures. The round-complexity of each
translation depends on the particular values of n and t. In general, the larger that
t is relative to n, the more rounds that are needed to perform the translation.
Table I summarizes the hierarchy. The first column gives progressively weaker
conditions on n and t; the second gives a number of rounds that are adequate to

538

R. A. BAZZI AND G. NEIGER

perform the desired translation (fewer rounds may be necessary if a stronger
condition holds). In the weakest case (n ⫽ t ⫹ 2), t ⫹ 1 rounds are required.
(As noted in Section 5, t ⫹ 1 rounds are sufficient regardless of the number of
failures using a translation based on Byzantine Agreement.) For any integer
constant c ⬎ 0 and any n and t such that n ⬎ ((c ⫹ 1)/c)t, there is a translation
with round-complexity at most 2c; if n ⱖ ((c ⫹ 1)/c)t, there is a translation
with round-complexity at most 2c ⫹ 1.
All the translations are efficient in that they generate implementations that do
not require substantially more local computation than the original protocols. In
all cases, if the largest message sent in original protocol has size b, then the
largest message sent in the implementing protocol has size O(bn).
8. Translations from Crash to Arbitrary Failures
Section 5.3 presented lower bounds on the fault-tolerance of translations from
crash to arbitrary failures. In this section, we complement those results by
presenting a set of canonical translations from crash to arbitrary failures. Each
translation in the set has a different round-complexity and fault-tolerance. We
achieve the following results:
—We provide a 2-round canonical translation for systems with n ⬎ max{6t ⫺ 3, 3t}.
—We provide a 3-round canonical translation for systems with n ⬎ max{4t ⫺ 2, 3t}.
—We provide a 4-round canonical translation for systems with n ⬎ 3t.
In all cases, we assume t ⬎ 0; if t ⫽ 0, an identity translation can be used
whereby every protocol implements itself.
8.1. A SET OF CANONICAL TRANSLATIONS. The translations are given in
Figure 9. Given a protocol ⌸c with message function ␮c and state-transition
function ␦c, the figure shows code for implementation ⌸a ⫽ ᐀(⌸c) that tolerates
arbitrary failures.15
Each translation has the same structure. After calculating its message, a
processor sends it in the first round of the phase.16 In subsequent rounds, all
processors echo messages on behalf of the others. In addition to the messages of
a phase, processors exchange control information; this information controls in
part which messages processors echo.
Figure 9 represents a z-round translation, where z is one of 2, 3, or 4. There
are two subroutines that are given in separate figures. One of these, Echoz , is
parameterized by z; the value of z is the round-complexity of the translation.
Each processor begins by initializing its state and two auxiliary variables called
faulty and crashed. These are used to maintain information about the processors
15
Note that certain components of processors’ states are retained from one round to another (e.g.,
processors send their crashed sets only in the first round of each phase). Technically, such
components cannot be “remembered” because the definition of protocols in Section 2.2 does not
include the processor’s current state as input to its state-transition function. We could compensate for
this by having processors send all their state components in every round. (Because an arbitrarily
faulty processor may take on any state, there is no reason— or way—to force such processors to retain
state information from one round to another.) Doing so would be straightforward but would
complicate the exposition of these translations.
16
The presentation of these translations is simplified by requiring the range of ␮c to be ᏹ instead of
ᏹ⬘. A processor cannot choose to send no message—⬜—in a round. It would be straightforward but
tedious to modify the translations to allow otherwise.

Simplifying Fault-Tolerance

FIG. 9.

539

Implementation ⌸a ⫽ ᐀(⌸c) as executed by processor p.

known to be faulty and those considered to have crashed. Each processor begins
a phase by computing the message that it should send in that phase (using the
function ␮c). In the first round of the first phase, a processor p simply sends this
message and receives corresponding messages from the other processors. In the
first round of any subsequent phase, p also sends its faulty and crashed sets and
an array of messages called relay. This array contains, for each processor q, any
message that p may be relaying on behalf of q for the previous phase. (It is a
possible that a faulty processor may send a message that is not in the correct
format. If a correct processor receives such a message, it treats it as if the base
message were ⬜ and, for phases after the first, as if the faulty and crashed sets
were both , and relay an all-⬜ array.) The following describes how p processes
the messages received from others.
First, p updates its faulty and crashed sets based on the faulty sets received
from others. If p received faulty sets containing q from more than t processors,
then it puts q in its own faulty set. If it received faulty sets containing q from at

540

R. A. BAZZI AND G. NEIGER

FIG. 10.

The function Valid.

least n ⫺ t processors, then p puts q in its crashed set. The relay arrays that p
received are used to set an array called believe. If (n ⫺ 1) ⫺ t processors other
than q relay a message from q, then p is willing to believe that the message was
sent by q in the previous phase (even though p may not have simulated receiving
it in that phase).
At the end of the first round of each phase, p determines which of the
messages it received in the first round are valid to be echoed in subsequent
rounds and places these in the array echo. The function Valid is used to make this
determination. (Message-validating functions for systems with arbitrary failures
were developed first by Bracha [1987] and Coan [1988].) Valid is given in Figure
10 and is described in more detail below. It uses the local variables crashed,
faulty, and believe. It returns either the message originally sent (if the message
should be echoed) or ⬜ (if it should not).
Processors echo validated messages through the z ⫺ 1 remaining rounds of a
phase by the procedure Echoz , which is given in Figure 11. Echoz acts as a filter
that constrains the behavior of the faulty processors; it concludes by setting the
rcvd and relay arrays. (The operational details of Echoz are given below.) A
processor completes the phase (at the end of Figure 9) by using the rcvd array
and function ␦c to determine its next state. (As noted earlier, the array relay is
used in the next phase.)
The function Valid operates as follows. Suppose that p is seeking to validate a
message from q. If i ⫽ 1, p validates the received message if and only there is
some initial state that would justify that message being sent. For i ⬎ 1, p checks
first to see if q is in its faulty set and returns ⬜ if it is. Otherwise, p checks that
the set crashedq (which it received with q’s message) is a subset of p’s crashed set.
If not, then the message is not validated. If crashedq 債 crashed, p uses its believe
array to reconstruct the messages that q should have received in phase i ⫺ 1.
(The properties Correctness and Relay, given in Section 8.2 and proven in

Simplifying Fault-Tolerance

FIG. 11.

541

Echoing procedure for the translations.

Section 8.5, imply that, if q has r 僆
兾 crashed, then p has believe[r] ⫽ ⬜; since
crashedq 債 crashed, all the messages that q should have received are in p’s
believe.) Processor p then applies state-transition function ␦c to these messages to
determine the state s in which q should have begun phase i. Finally, it uses ␮c to
check if s would indeed generate the message being validated.
The following is a description of the procedure Echoz . In each of its z ⫺ 1
rounds (the phase’s first round is in Figure 9), processors exchange their echo
arrays (a processor treats receipt of a malformed message as an all-⬜ array).
After receiving the arrays for a round, processor p determines, for each q, the
“strongest” message m being echoed for q. The strength of a message is the
number of processors (other than q) that echo it. A special case applies to the
last round of a 4-round phase; here, m is set to the previous value of
weak_echo[q] (see below) and that message’s strength is evaluated. In either
case, if m’s strength is at least (n ⫺ 1) ⫺ t, p sets echo[q] to m; otherwise, it
sets echo[q] to ⬜ and adds q to faulty. Each processor maintains an auxiliary
array, weak_echo. It sets weak_echo similarly to echo, except that a strength of
n ⫺ 2t is sufficient to place a message in weak_echo.17
In the last round of every phase, rcvd is set to weak_echo. If p finds it is
receiving no message from q (i.e., if rcvd[q] ⫽ ⬜), it adds to q to crashed.
17

Notice that weak_echo is used only to set rcvd in the last round and for the special case of round 4.
Its setting in every round is simply a notational convenience.

542

R. A. BAZZI AND G. NEIGER

Processors then set their relay arrays. This is also based on message strength, with
a strength of n ⫺ 3t ⫹ 1 being sufficient to relay a message.
8.2. TRANSLATION PROPERTIES.
properties:

All three translations have the following four

(1) Correctness. If a correct processor sends a message at the beginning of a
phase, then all correct processors receive that message by the end of the
phase and relay it in the next. That is, if p and q are correct and p sets
message to m at the beginning of phase i, then q has rcvd[ p] ⫽ m at the end
of phase i and sets believe[ p] to m in the first round of phase i ⫹ 1.
Furthermore, q always has p 僆
兾 faulty and p 僆
兾 crashed.
(2) Relay. If a correct processor receives a message from a faulty processor at
the end of a phase, then all correct processors consider that message
believable by the end of the first round of the next phase. That is, if p and q
are correct, r is faulty, and p has rcvd[r] ⫽ m ⫽ ⬜ at the end of phase i,
then q has believe[r] ⫽ m at the end of the first round of phase i ⫹ 1.
(3) Relayed Crashing. If a correct processor considers a processor crashed, then
all correct processors consider that processor faulty at that time and will
consider it crashed soon afterwards. Specifically, if p and q are correct and p
adds r to crashed in a round, then q has r 僆 faulty by the end of that round.
If the round is the first of a phase, then q will have r 僆 crashed by the end of
that phase. If the round is the last of a phase, then q will have r 僆 crashed by
the end of the first round of the next phase.18
(4) Consistency. If a correct processor considers a message from faulty processor
p believable for a given phase, then no correct processor considers a
different message from p believable for that phase. That is, if q and r are
correct, p is faulty, and q sets believe[ p] to m ⫽ ⬜ at the end of the first
round of phase i ⫹ 1, then r sets believe[ p] to either m or ⬜ at that time.
Furthermore, some correct processor validated m for p in phase i, that is,
found Valid(i, p) ⫽ m.
Section 8.3 uses some of these properties to define history-simulation functions
for the translations. Section 8.4 proves that these properties are sufficient to
prove the correctness of the translations. Section 8.5 will prove that these
properties hold for all three translations.
8.3. THE SIMULATION FUNCTIONS. Recall that any canonical translation ᐀
from C(n, t) to A(n, t) must have a canonical state-simulation function ᏿.
Suppose that protocol ⌸c has state set ᏽ. Inspection of Figure 9 shows that the
state of a processor running ⌸a ⫽ ᐀(⌸c) is of the form

具 state, faulty, crashed, echo, weak echo, relay, believe典,
so the state set of ⌸g is ᏽ ⫻ 2 ᏼ ⫻ 2 ᏼ ⫻ (ᏹ⬘) n ⫻ (ᏹ⬘) n ⫻ (ᏹ⬘) n ⫻ (ᏹ⬘) n .
Define

᏿ 共具 state, faulty, crashed, echo, weak echo, relay, believe典兲 ⫽ state.

18

Processors add to crashed only in the first and last rounds of a phase.

Simplifying Fault-Tolerance

543

᐀ must also have a canonical history-simulation function Ᏼ. Ᏼ must map any
history Ha of ᐀(⌸) to a history Hc that satisfies conditions (c), (d⬘), and (e) of
Section 4.1 (if ᐀ is nonuniform). This section defines the history-simulation
functions for the translations given in Section 8.1. Because this is a parameterized function, we use z to refer to the round-complexity of the translation, which
can be 2, 3, or 4.
Given history Ha, we construct Hc ⫽ Ᏼ(Ha) so that any processor p 僆
Faulty(Hc) fails by crashing. Specifically, it crashes in round i p of Hc, where the
value of i p depends on when correct processors “believe” messages from p (based
on their believe arrays). Before giving the definition of i p , we observe the
following fact about the believe arrays:
LEMMA 8.1. If a correct processor sets believe[ p] ⫽ ⬜ in a phase, then all
correct processors will set believe[ p] ⫽ ⬜ in all subsequent phases.
PROOF. Suppose that some correct processor sets believe[ p] ⫽ ⬜ in phase
i ⬎ 1. By Correctness, p is faulty. By Relay, all correct processors must have set
rcvd[ p] ⫽ ⬜ in phase i ⫺ 1. An inspection of Figure 11 shows that they must
also have p in crashed and faulty at that time. By Figure 10, they will refuse to
validate any messages from p starting in phase i. By Consistency, all correct
processors set believe[ p] ⫽ ⬜ in every phase beginning with phase i ⫹ 1. e
Based on this lemma, i p is defined as follows:

ip ⫽

冦

1
if
i ⫺ 1 if
⬁

if

no correct processor ever sets believe关 p兴 ⫽ ⬜
i is the last phase in which some correct processor
sets believe关 p兴 ⫽ ⬜
some correct processor sets believe关 p兴 ⫽ ⬜ in
every phase i ⬎ 1

We will show later that, if i p ⫽ ⬁, then p 僆 Correct(Hc).
Define Hc ⫽ 具⌸c, Qc, Sc, Rc典 as follows. Consider first a processor p 僆
Correct(Ha). Set Qc(i, p) to p’s value of state at the beginning of round
z(i ⫺ 1) ⫹ 1. For each q 僆 ᏼ, set Sc(i, p, q) ⫽ ␮c(i, p, Qc(i, p)) and set
Rc(i, p, q) to be p’s value of rcvd[q] at the end of round zi.
Consider now a processor p 僆 Faulty(Ha). We begin by defining Qc(i, p).
Consider the following cases.
—i ⫽ i p ⫽ 1 because no correct processor ever sets believe[ p] ⫽ ⬜. Set
Qc(1, p) to some state s (s can be any legal initial state).
—i ⱕ i p and there is an m ⫽ ⬜ such that some correct processor set
believe[ p] ⫽ m in phase i ⫹ 1. By Consistency, some correct processor r i, p
validated m in phase i. Set Qc(i, p) to be the state s used by r i, p in validating
that message (see Figure 10).
—i ⬎ i p . Set

Qc(i,

Next, we define

p) ⫽

Sc(i,

Qc(i p ,

p).

p, q). Consider the following cases.

544

R. A. BAZZI AND G. NEIGER

—q 僆 Correct(Ha) or i ⫽ i p . Set Sc(i, p, q) ⫽ Rc(i, q, p). 19
—i ⬍ i p and q 僆 Faulty(Ha). By the definition of i p , there is a correct processor
that set believe[ p] ⫽ m ⫽ ⬜ in phase i ⫹ 1 (by Consistency, the value of m is
unique). Set Sc(i, p, q) ⫽ m.
—i ⬎ i p and q 僆 Faulty(Ha). Set Sc(i, p, q) ⫽ ⬜.
Finally, we define

Rc(i,

p, q). Consider the following cases.

—i ⬍ i p and q 僆 Correct(Ha). Set Rc(i, p, q) ⫽ Sc(i, q, p).
—i ⬍ i p and q 僆 Faulty(Ha). Since 1 ⬍ i ⫹ 1 ⱕ i p , there is an m ⫽ ⬜ such that
some correct processor set believe[ p] ⫽ m in phase i ⫹ 2 and, by Consistency,
some correct processor r i⫹1, p validated a message from p in phase i ⫹ 1; use
the same processor that was used to define Qc(i ⫹ 1, p) above. In this case, set
Rc(i, p, q) to the value v[q] that r i⫹1, p used to validate that message (see
Figure 10).
—i ⱖ i p . Set Rc(i, p, q) ⫽ ⬜ for all q.
The following section uses this history-simulation function to show that ᐀
correctly translates from C(n, t) to A(n, t) by showing that ᏿ and Ᏼ satisfy the
conditions of the definition of nonuniform translations given in Section 4.2.
8.4. PROOF OF CORRECTNESS. The functions ᏿ and Ᏼ must satisfy certain
conditions for ᐀ to be a nonuniform translation from C(n, t) to A(n, t).
Specifically, ᏿ must satisfy the following:
(a) (@p 僆 ᏼ)(@s imp 僆 ᏽimp;p )(?s 僆 ᏽ p )(᏿(s imp) ⫽ s); and
(b) (@p 僆 ᏼ)(@s 僆 ᏽ p )(?s imp 僆 ᏽimp;p )(᏿(s imp) ⫽ s).
It is not hard to see that ᏿ satisfies (a) and (b). As specified in Figure 9, each
s imp 僆 ᏽimp;p is of the form 具s, , , ⬜ n , ⬜ n , ⬜ n , ⬜ n 典. By definition, ᏿(具s, , ,
⬜ n , ⬜ n , ⬜ n , ⬜ n 典) ⫽ s 僆 ᏽ p , as required for (a). For any s 僆 ᏽ p , 具s, , , ⬜ n ,
⬜ n , ⬜ n , ⬜ n 典 僆 ᏽ imp;p , satisfying (b).
We must next prove the following for history Hc ⫽ Ᏼ(Ha) ⫽ 具⌸c, Qc, Sc, Rc典:
(c) Hc is a history of ⌸c running in C(n, t);
(d⬘) (@i 僆 Z)(@p 僆 Correct(Ha))(᏿(Qa( z(i ⫺ 1) ⫹ 1, p)) ⫽ Qc(i, p)); and
(e) Correct(Ha) 債 Correct(Hc).
The proof in this section uses the properties given in Section 8.2. These
properties will be proven in Section 8.5.
For (c), it is clear that Hc is a history of ⌸c by definition. To show that it is a
history in C(n, t), we will make a number of arguments about which processors
may fail and how they may fail. We will then show that (d⬘) follows from the
definition of Hc. The proof of (e) will be done as part of the proof of (c).
Specifically, it is shown by the following lemma:
LEMMA 8.2.

Correct(Ha ) 債 Correct(Hc ).

PROOF. Suppose that p 僆 Correct(Ha). The proof must show that p sends and
receives correctly and makes correct state transitions in every round of Hc.

19

This is the only case in which one processor’s history depends on that of another. For this reason,
Ᏼ is well-defined.

Simplifying Fault-Tolerance

545

We show first that p sends correctly in each round i of Hc; it should be the case
that, for all q, Sc(i, p, q) ⫽ ␮c(i, p, Qc(i, p)). This is precisely the definition of
Sc. For p to receive correctly, it must be that Rc(i, p, q) ⫽ Sc(i, p, q) for all q.
Consider some q 僆 ᏼ and the following two cases.
—q 僆 Faulty(Ha). In this case, Rc(i, p, q) ⫽ Sc(i, q, p) by the definition of Rc
for faulty processors.
—q 僆 Correct(Ha). In this case, Rc(i, p, q) is p’s value of rcvd[q] at the end of
phase i. By Correctness, this is the message m that q sent in the first round of
that phase. Since q 僆 Correct(Ha), an inspection of Figure 9 shows m ⫽
␮c(i, q, state), where state is q’s value of the variable at that time. By the
definition of Qc for correct processors, this is Qc(i, q); by the definition of Sc
for correct processors, m ⫽ ␮c(i, q, Qc(i, q)) ⫽ Sc(i, q, p).
In either case, p receives correctly in round i of Hc. Finally, Qc(i ⫹ 1, p) should
be ␦c(i, p, Rc(i, p)). By definition, Qc(i ⫹ 1, p) is p’s value of state at the end of
phase i. An inspection of Figure 9 shows that this is ␦c(i, p, rcvd); given the
definition of Rc, this is ␦c(i, p, Rc(i, p)), as desired. e
We next show that any faulty processor p behaves correctly in Hc until round
i p:
LEMMA 8.3.

If p 僆 Faulty(Ha ) and i ⬍ i p , then p 僆 Correct(Hc , i).

PROOF. The proof is by induction on i. The base case is straightforward; by
definition, Correct(Hc, 0) ⫽ ᏼ. Assume that, for all k ⬍ i ⬍ i p , p 僆 Correct(Hc,
k). It remains to show that p behaves correctly in round i itself. Note that i ⬎ 0
implies i p ⬎ 1. This means that some correct processor sets believe[ p] ⫽ ⬜ in
phase i p ⫹ 1 ⱖ i ⫹ 2 and, by Lemma 8.1, all correct processors set believe[ p] ⫽
⬜ in all phases through i p ⱖ i ⫹ 1.
We first show that p sends correctly in round i of Hc by showing that Sc(i, p, q)
⫽ ␮c(i, p, Q(i, p)) for all q. We begin by fixing q, setting m ⫽ Sc(i, p, q), and
showing both that m ⫽ ⬜ and that some correct processor validated m in phase
i. Consider two cases.
—q 僆 Correct(Ha). In this case, m ⫽ Rc(i, q, p) by the definition of Sc for faulty
processors. By the definition of Rc for correct processors, m is q’s value of
rcvd[ p] at the end of phase i. If m ⫽ ⬜, p would have q 僆 crashed at that
time, and Relayed Crashing would imply that all correct processors had p 僆
faulty at the end of phase i. This would mean that no correct processor would
validate a message from p in phase i ⫹ 1 and, by Consistency, all correct
processors would set believe[ p] to ⬜ in phase i ⫹ 2. This is a contradiction.
Thus, m ⫽ ⬜ and, by Relay and Consistency, some correct processor validates
m in phase i.
—q 僆 Faulty(Ha). In this case, m ⫽ ⬜ by the definition of Sc for faulty
processors. By Consistency, all correct processors set believe[ p] ⫽ m in phase
i ⫹ 1 and some correct processor validated m in phase i.
In either case, m ⫽ ⬜ and some correct processor validated m for p in phase i.
Consider the validating processor used in the definition of Qc(i, p). By Figure 10,
m ⫽ ␮c(i, p, Qc(i, p)), so p sends correctly.

546

R. A. BAZZI AND G. NEIGER

We next show that p receives correctly in round i of Hc by showing Rc(i, p, q)
⫽ Sc(i, q, p) for all q. If q 僆 Correct(Ha), then Rc(i, p, q) ⫽ Sc(i, q, p) by the
definition of Rc for faulty processors. Otherwise, recall that some correct
processor set believe[ p] ⫽ ⬜ in phase i ⫹ 2; by Consistency, some correct
processor validated a message from p in phase i ⫹ 1. Let r i⫹1, p be the processor
specified in the definition of Rc. By this definition, Rc(i, p, q) is the value of v[q]
used by r i⫹1, p for its validation. This is ⬜ if r i⫹1, p received crashedp from p with
q 僆 crashedp ; otherwise, it is r i⫹1, p ’s value of believe[q] for phase i. Let us
consider each case in turn:
—Rc(i, p, q) ⫽ ⬜ because r i⫹1, p found q 僆 crashedp in round i ⫹ 1. Since
r i⫹1, p validated p’s phase i ⫹ 1 message, it had q in its own crashed at that
time. By Relayed Crashing, each correct processor had q 僆 faulty by the end
of the first round of phase i ⫹ 1. Thus, by Figure 10, no correct processor
validated q’s phase i ⫹ 1 message. By Consistency, all correct processors set
believe[q] ⫽ ⬜ in phase i ⫹ 2. By Lemma 8.1, i q ⱕ i. If i ⫽ i q , then
Sc(i, q, p) ⫽ Rc(i, p, q) ⫽ ⬜ by the definition of Sc for faulty processors. If
i ⬎ i q , then Sc(i, q, p) ⫽ ⬜ ⫽ Rc(i, p, q) similarly. In either case, p receives
correctly.
兾 crashedp in round i ⫹ 1, and Rc(i, p, q) is r i⫹1, p ’s value of
—r i⫹1, p found q 僆
believe[q] in phase i. Since r i⫹1, p validated p’s phase i ⫹ 1 message, it had
believe[q] ⫽ ⬜ at this time. By Lemma 8.1, i q ⱖ i. If i ⫽ i q , then Sc(i, q, p) ⫽
Rc(i, p, q). If i ⬍ i q , then Sc(i, q, p) is the non-⬜ value to which a correct
processor (e.g., r i⫹1, p ) set believe[q] in phase i ⫹ 1; as noted above, this is
Rc(i, p, q). In either case, p receives correctly.
Thus, in either case, p receives correctly in round i.
Finally, we see that p makes a correct state transition in round i of Hc. We
must verify that Qc(i ⫹ 1, p) ⫽ ␦c(i, p, Rc(i, p)). Since i ⫹ 1 ⱕ i p , the
definition of Qc for faulty processors implies that Qc(i ⫹ 1, p) is the state s
computed by the correct processor r i⫹1, p that validated a phase i ⫹ 1 message
for p. The procedure Valid (Figure 10) ensures that s ⫽ ␦c(i, p, v), where v is
the array computed by r i⫹1, p . We next show that, for each q, v[q] ⫽ Rc(i, p, q).
If q 僆 Faulty(Ha), then Rc(i, p, q) ⫽ v[q] by the definition of Rc for faulty
processors. For q 僆 Correct(Ha), the definition sets Rc(i, p, q) ⫽ Sc(i, q, p); we
show v[q] ⫽ Sc(i, q, p). Correctness implies that r i⫹1, p never has q 僆 crashed.
Thus, since r i⫹1, p validated p’s phase i message, it found q 僆
兾 crashedp in the first
round of that phase and set v[q] to be its own value of believe[q]. By
Correctness, this is the message that p sent in phase i. As observed in the proof
of Lemma 8.2, this is Sc(i, q, r i⫹1, p ), which is the same as Sc(i, q, p) by the
definition of Sc for correct processors. Thus, v[q] ⫽ Sc(i, q, p), as desired,
completing the proof. e
We can now show that any processor in Faulty(Hc) suffers a crash failure:
LEMMA 8.4.
Hc .

If p 僆 Faulty(Hc ), then p commits a crash failure in round i p of

PROOF. Suppose p 僆 Faulty(Hc). Lemma 8.2 implies p 僆 Faulty(Ha). Lemma
8.3 implies that i p ⬍ ⬁ and that p 僆 Correct(Hc, i) for all i ⬍ i p . Thus, if p 僆
Faulty(Hc, i p ), then i p is the least i such that p 僆 Faulty(Hc, i). We will see that

Simplifying Fault-Tolerance

547

p crashes in round i p of Hc. Let m ⫽ ␮c(i p , p, Qc(i p , p)); we first show that
Sc(i p , p, q) 僆 {m, ⬜} for all q. The definition of Sc for faulty processors sets
Sc(i p , p, q) ⫽ Rc(i p , q, p) for all q, so it suffices to show Rc(i p , q, p) 僆 {m, ⬜}
for each q. By the definition of i p , there are two cases to consider:
—No correct processor ever has believe[ p] ⫽ ⬜; this means i p ⫽ 1. Consider
the following two subcases:
—q 僆 Correct(Ha). In this case, Rc(1, q, p) is q’s value of rcvd[ p] at the end
of phase 1. Because all correct processors always have believe[ p] ⫽ ⬜,
Relay implies that they also always have rcvd[ p] ⫽ ⬜.
—q 僆 Faulty(Ha). In this case, Rc(1, q, p) is either ⬜ (if i q ⫽ 1) or the
value v[ p] used by r 2, q in validating q’s phase 2 message. This in turn is
either ⬜ or a non-⬜ value that was r 2, q ’s believe[ p] in phase 2. The
hypothesis of this case implies that there is no such non-⬜ value.
—Some correct processor had believe[ p] ⫽ m⬘ ⫽ ⬜ at the end of the first round
of phase i p ⫹ 1. By Consistency, some correct processor validated m⬘ and, by
the definition of Qc for faulty processors, m⬘ is ␮c(i p , p, Qc(i p , p)) ⫽ m.
Consider the following two subcases:
—q 僆 Correct(Ha). In this case, Rc(i p , q, p) is q’s value of rcvd[ p] at the
end of phase i p . Relay implies that this is in {m, ⬜}.
—q 僆 Faulty(Ha). In this case, Rc(i p , q, p) is either ⬜ (if i p ⱖ i q ) or the
value v[ p] used by r i p ⫹1, q in validating q’s phase i p ⫹ 1 message. This in
turn is either ⬜ or a non-⬜ value that was r i p ⫹1, q ’s believe[ p] in phase i p ⫹
1, and this must be m by Consistency.
In all cases, p’s sending behavior in round i p is consistent with failure by
crashing.
We must now show that, after crashing, p takes no further actions. We first
observe that it receives no messages. By the definition of Rc for faulty processors,
Rc(i, p, q) ⫽ ⬜ for all q and all i ⱖ i p . By the definition of Qc, Qc(i, p) ⫽
Qc(i p , p) for all i ⬎ i p . Finally, we show that p sends no messages after round i p .
For q 僆 Faulty(Ha), Sc(i, p, q) ⫽ ⬜ by definition for all i ⬎ i p . For q 僆
Correct(Ha), Sc(i, p, q) ⫽ Rc(i, q, p) for all i. However, if i ⬎ i p , then no correct
processor has believe[ p] ⫽ ⬜ in phase i ⫹ 1. Relay implies that q set
rcvd[ p] ⫽ ⬜ at the end of phase i, so R c(i, q, p) ⫽ ⬜. Thus, S c(i, p, q) ⫽ ⬜,
as desired. e
We can now argue that ᐀ correctly translates from C(n, t) to A(n, t) in z
rounds. To complete the proof of condition (c), we must argue Hc 僆 C(n, t). It
suffices to show that, in Hc, there are at most t faulty processors and that they are
subject only to crash failures. Since there are at most t faulty processors in Ha,
Lemma 8.2 implies that there are most t faulty processors in Hc. Lemma 8.4
shows that these fail by crashing. For condition (d⬘), it should be clear that the
functions ᏿ and Qc were chosen in such a way that, if p 僆 Correct(Ha), both
᏿(Qa( z(i ⫺ 1) ⫹ 1, p)) and Qc(i, p) are p’s value of the variable state at the
beginning of round z(i ⫺ 1) ⫹ 1. Finally, recall that condition (e) follows
immediately from Lemma 8.2.

548

R. A. BAZZI AND G. NEIGER

This section showed that the translations of Section 8.1 are correct so long as
they all satisfy the properties defined in Section 8.2. The following section proves
that they do so.
8.5. PROOF OF TRANSLATION PROPERTIES. This section proves the properties
of Section 8.2 for all three translations. We begin by proving the following
lemmas.
LEMMA 8.5. Suppose that p is faulty and that some correct processor q sets
echo[ p] to m ⫽ ⬜ in some round of Echoz. Then no correct processor r sets echo[ p]
to m⬘ 僆
兾 {m, ⬜} in that round.
PROOF. Since q sets echo[ p] to m, it received echo with echo[ p] ⫽ m from at
least (n ⫺ 1) ⫺ t processors other than p. Thus, there are at most t correct
processors that sent echo with echo[ p] ⫽ m in that round. Since there are at
most t ⫺ 1 faulty processors other than p, r could receive echo with echo[ p] ⫽
m⬘ from at most t ⫹ (t ⫺ 1) ⫽ 2t ⫺ 1 processors other than p. Since n ⬎ 3t
for all translations, 2t ⫺ 1 ⬍ (n ⫺ 1) ⫺ t, r cannot set echo[ p] to m⬘. e
LEMMA 8.6. Suppose that some correct processor q sets weak_echo[ p] to m ⫽ ⬜
in the third or fourth round of a phase (in Echoz). Then no correct processor r sets
weak_echo[ p] to m⬘ 僆
兾 {m, ⬜} in that round.
PROOF. Since q sets weak_echo[ p] to m, it received echo with echo[ p] ⫽ m
from at least n ⫺ 2t processors other than p. Since n ⬎ 3t, at least one of these
is correct and set echo[ p] to m in the previous round in Echoz . By Lemma 8.5,
no other correct processor set echo[ p] to m⬘ in that round. Thus, r can receive
echo with echo[ p] ⫽ m⬘ from at most t processors other than p in the round in
question. Since n ⬎ 3t for all translations, t ⬍ n ⫺ 2t, so r cannot set
weak_echo[ p] to m⬘. e
LEMMA 8.7. Let n ⬎ 6t ⫺ 3 and suppose that p is faulty and that some correct
processor q sets echo[ p] to m ⫽ ⬜ in the second round of a phase (in Echoz). Then
each correct processor r sets weak_echo[ p] to m in that round.
PROOF. Since q sets echo[ p] to m, it received echo with echo[ p] ⫽ m from at
least (n ⫺ 1) ⫺ t processors other than p (at least one of which is correct).
Processor r received at least (n ⫺ 1) ⫺ t ⫺ (t ⫺ 1) ⫽ n ⫺ 2t of these echoes
and will thus set weak_echo[ p] as desired if m is the strongest message received
by r for p. Since n ⬎ 6t ⫺ 3 and t ⬎ 0, n ⫺ 2t ⬎ 4t ⫺ 3 ⱖ 2t ⫺ 1. Thus, r
can have received echo with echo[ p] ⫽ m from at most (n ⫺ 1) ⫺ (n ⫺ 2t) ⫽
2t ⫺ 1 ⬍ n ⫺ 2t processors and m is the strongest message, as desired. e
LEMMA 8.8. Suppose that p is faulty and that some correct processor q sets
echo[ p] to m ⫽ ⬜ in the third round of a phase (in Echoz). Then each correct
processor r sets weak_echo[ p] to m in that round.
PROOF. Since q sets echo[ p] to m, it received echo with echo[ p] ⫽ m from at
least (n ⫺ 1) ⫺ t processors other than p (at least one of which is correct).
Processor r received at least (n ⫺ 1) ⫺ t ⫺ (t ⫺ 1) ⫽ n ⫺ 2t of these echoes
and will thus set weak_echo[ p] as desired if m is the strongest message received
by r for p. Since at least one correct processor set echo[ p] to m in the second
round of the phase, Lemma 8.5 implies that no correct processor set echo[ p] to a

Simplifying Fault-Tolerance

549

different non-⬜ value in that round. Thus, r could receive echo with echo[ p] ⫽
m⬘ ⫽ m from at most (t ⫺ 1) processors other than p (recall that p is faulty).
Since n ⬎ 3t for all translations, t ⫺ 1 ⬍ n ⫺ 2t, so r must find m⬘ weaker than
m. Thus, r will set weak_echo[ p] to m. e
LEMMA 8.9. Suppose that p is faulty and that some correct processor q sets
echo[ p] to m ⫽ ⬜ in the fourth round of a phase (in Echoz). Then each correct
processor r sets weak_echo[ p] to m in that round.
PROOF. Since q sets echo[ p] to m, it received echo with echo[ p] ⫽ m from at
least (n ⫺ 1) ⫺ t processors other than p (at least one of which is correct).
Processor r received at least (n ⫺ 1) ⫺ t ⫺ (t ⫺ 1) ⫽ n ⫺ 2t of these echoes
and will thus set weak_echo[ p] as desired if r set weak_echo[ p] to m in the third
round of the phase. Since some correct processor set echo[ p] to m in that round,
this is implied by Lemma 8.8. Thus, r will set weak_echo[ p] in round 4 as well.
e
These lemmas can now be used to prove that the four properties hold for each
of the translations. The proof is by induction on i, the phase number. It is a
simultaneous proof of all four properties. Because the proof uses induction in
only a small number of places, it is not explicitly broken into base and induction
cases. These are considered separately only when necessary. In all cases, round
numbers are given with respect to the phase under consideration.
(1) Correctness. Suppose that p is correct and sets message to m at the beginning
of some phase. The proof will show that each correct processor q sets
rcvd[ p] to m at the end of the phase and believe[ p] to m in the first round of
next phase. It will also show that no correct processor adds p to either faulty
or crashed in this phase.
The proof begins by showing that no correct processor adds p to either faulty
or crashed in round 1 and that all correct processors validate m in that
round. This is done separately for the base and induction cases:
—Base. In round 1 of phase 1, p sends m to all processors. An inspection of
Figure 9 shows that no correct processor adds any processors to crashed or
faulty in that round. Because p is correct, m ⫽ ␦c(1, p, s), where s is p’s
initial state. All correct processors can use s to validate m, as desired.
—Induction. In round 1 of any phase after phase 1, p sends [m, crashedp ,
faultyp , relayp ] to all (crashedp , faultyp , and relayp are p’s values of crashed,
faulty, and relay at the beginning of that round). Because p is correct, all
processors receive this message. The inductive hypothesis for Correctness
implies that no correct processor had p 僆 faulty at the end of the previous
phase; thus, none will add p to faulty or crashed in this round. The proof
now shows that all correct processors validate m.
Consider now each processor’s execution of Valid. As noted earlier, no
correct processor will have p 僆 faulty. If some processor r is in crashedp ,
then p had r 僆 crashed at the end of the previous phase. The inductive
hypothesis for Relayed Crashing implies that each correct processor has
r 僆 crashed by the time it executes Valid. Thus, each correct processor
verifies crashedp 債 crashed. If some correct processor r is not in crashedp ,
then p had rcvd[r] ⫽ m ⫽ ⬜ at the end of phase i (see Figure 11). If r is

550

R. A. BAZZI AND G. NEIGER

correct, then the inductive hypothesis for Correctness implies that r set
message to m in round 1 of the previous phase and that all correct
processors have believe[r] ⫽ m at the end of round 1 of this phase. If r is
faulty, the inductive hypothesis for Relay implies the same thing. Thus,
after receiving p’s message in round 1 and while executing Valid all correct
processors set v to the array rcvd that p had at the end of the previous
phase (call it i). This means that they all set s to ␦c(i, p, rcvd) (as p did)
and verify that m ⫽ ␮c(i ⫹ 1, p, s) (as p did). Thus, all correct processors
validate m, as desired.
Since all correct processors validate m in round 1, they all send echo with
echo[ p] ⫽ m in round 2 and each correct q thus receives echo with
echo[ p] ⫽ m from at least (n ⫺ 1) ⫺ t processors other than p. Since n ⬎
3t and t ⬎ 0, t ⬍ n ⫺ 2t ⱕ (n ⫺ 1) ⫺ t, so m is the “strongest” echo for
p in round 2. To complete the proof, we consider three cases depending on
the round-complexity of the translation.
—z ⫽ 2. Since m is the strongest echo for p and has strength at least
(n ⫺ 1) ⫺ t, q sets first weak_echo[ p] and then rcvd[ p] to m.
—z ⫽ 3. All correct processors will set echo[ p] ⫽ m at the end of round 2.
In round 3, q will again receive echo with echo[ p] ⫽ m from at least
(n ⫺ 1) ⫺ t processors other than p and will set rcvd[ p] to m as in the
previous item.
—z ⫽ 4. All correct processors will set echo[ p] ⫽ m at the end of round 2.
In round 3, each correct processor will receive echo with echo[ p] ⫽ m
from at least (n ⫺ 1) ⫺ t processors and will thus set echo[ p] (and
weak_echo[ p]) to m again. In round 4, q will receive echo with echo[ p] ⫽
m (its old value weak_echo[ p]) from at least (n ⫺ 1) ⫺ t processors other
than p and will set rcvd[ p] to m.
We next argue that no correct processor adds p to faulty or crashed in the last
round of this phase. No correct processor adds p to faulty in Echoz as m has
strength at least (n ⫺ 1) ⫺ t in each round. No correct processor adds p to
crashed in round z because rcvd[ p] is set to m, not ⬜.
From Figure 11, it is clear that, since each processor sets weak_echo[ p] to m
in round z, it also sets relay[ p] to m at this time (t ⬎ 0 implies (n ⫺ 1) ⫺
t ⱖ n ⫺ 3t ⫹ 1). In the first round of the next phase, q will receive relay
with relay[ p] ⫽ m from at least (n ⫺ 1) ⫺ t processors other than p. Thus,
q will set believe[ p] to m in the first round of the next phase. This completes
the proof for Correctness.
(2) Relay. Suppose that r is faulty and that p is correct and has rcvd[r] ⫽ m ⫽
⬜ at the end of some phase. Let z be the last round of phase 1. We first
prove that all processors set relay[r] to m in round z.
Since r is faulty, there are at most t ⫺ 1 faulty processors other than r. By
Figure 11, p set weak_echo[r] to m and thus received echo with echo[r] ⫽ m
from at least n ⫺ 2t processors other than r in round z. This means that all
correct processors received echo with echo[r] ⫽ m from at least n ⫺ 2t ⫺
(t ⫺ 1) ⫽ n ⫺ 3t ⫹ 1 correct processors other than r in round z. Thus,
these processors will set relay[r] as desired if m is the message they chose in
round z. There are three cases to consider, depending on the roundcomplexity of the translation.

Simplifying Fault-Tolerance

551

—z ⫽ 2. In this case, n ⬎ 6t ⫺ 3 and we must show that m is the strongest
message received for r. Any other message can strength at most
(n ⫺ 1) ⫺ (n ⫺ 3t ⫹ 1) ⫽ 3t ⫺ 2. Since n ⬎ 6t ⫺ 3, n ⫺ 3t ⫹ 1 ⬎
3t ⫺ 2, so m is the strongest message received for r in that round.
—z ⫽ 3. In this case, n ⬎ 4t ⫺ 2 and we must also show that m is the
strongest message received for r. Since p set weak_echo[r] to m in round 3,
some correct processor set echo[r] to m in round 2. By Lemma 8.5, no
correct processor sets echo[r] to m⬘ 僆
兾 {m, ⬜} in that round. Thus, a
correct processor can receive echo with echo[r] 僆
兾 {m, ⬜} only from the
t ⫺ 1 faulty processors (other than r). Since n ⬎ 4t ⫺ 2, t ⫺ 1 ⬍ n ⫺
3t ⫹ 1 and m is the strongest message received for r in that round.
—z ⫽ 4. In this case, n ⬎ 3t and we must show that all correct processors
set weak_echo[r] to m in round 3. Since p received echo with echo[r] ⫽ m
from at least n ⫺ 2t processors other than r in round 4 and n ⫺ 2t ⬎ t, at
least one correct processor set echo[r] to m in round 3. By Lemma 8.8, all
correct processors set weak_echo[r] to m in round 3.
If q is correct, then, when setting the believe array in the first round of the
next phase, it will receive relay with relay[r] ⫽ m from all correct processors,
that is, from at least (n ⫺ 1) ⫺ t processors other than r. Thus, q will set
believe[r] to m in the first round of the next phase, as desired. This completes
the proof for Relay.
(3) Relayed Crashing. Suppose that p is correct and adds r to crashed during
some phase. This can happen either in the first or last round of the phase,
and these cases are considered separately.
—p adds r to crashed in round 1. In this case, p received faultyq with r 僆
faultyq from at least n ⫺ t processors q. All correct processors do so from
at least n ⫺ 2t of these. Since n ⬎ 3t, n ⫺ 2t ⬎ t, so all correct
processors add r to faulty in round 1, as desired. This also means that no
correct processor will validate any message from r in this round. Thus,
throughout the execution of Echoz in this phase, no correct processor will
find a message from r with strength greater than t. Since n ⬎ 3t, t ⬍ n ⫺
2t; thus, no correct processor ever sets echo[r] or weak_echo[r] to
anything other than ⬜ in this phase. Thus, in round z, all correct
processors add r to crashed, as desired.
—p adds r to crashed in round z. By Figure 11, it is clear that p sets
weak_echo[r] to ⬜ in round z. By Lemmas 8.7, 8.8, and 8.9, all correct
processors set echo[r] to ⬜ in that round. By Figure 11, they all add r to
faulty at that time, as desired. When updating the sets crashed and faulty in
the first round of the next phase, each correct processor q will receive sets
faultys with r 僆 faultys from at least n ⫺ t processors s. Thus, q will add r
to crashed at this time, as desired.
(4) Consistency. Suppose that p is faulty and that q is correct and sets believe[ p]
to some m 僆 ᏹ in the first round of some phase (this can happen no earlier
than phase 2). This means that q received relay with relay[ p] ⫽ m from at
least (n ⫺ 1) ⫺ t processors other than p in that round. Since p is faulty,
there are at most t ⫺ 1 other faulty processors. Thus, any other correct
processor r received relay with relay[ p] ⫽ m from at most t ⫹ (t ⫺ 1) ⫽
2t ⫺ 1 ⬍ (n ⫺ 1) ⫺ t processors. Thus, r cannot set believe[ p] to any m⬘ 僆
兾

552

R. A. BAZZI AND G. NEIGER

{m, ⬜} because it cannot receive enough relays for such an m⬘.
It remains to show that some correct processor validated m for p in the first
round of the previous phase. For the remainder of this item, rounds are
numbered with respect to the previous phase. At least ((n ⫺ 1) ⫺ t) ⫺
(t ⫺ 1) ⫽ n ⫺ 2t of the relay messages received by q were from correct
processors. This means that some correct processor s received echo with
echo[ p] ⫽ m from at least n ⫺ 3t ⫹ 1 processors other than p in the round
z. We consider three cases, depending on the round-complexity of the
translation:
—z ⫽ 2. In this case, n ⬎ 6t ⫺ 3. At least (n ⫺ 3t ⫹ 1) ⫺ (t ⫺ 1) ⫽ n ⫺
4t ⫹ 2 ⬎ 0 of the echoes s received for m in round 2 were from correct
processors. All these correct processors validated m.
—z ⫽ 3. In this case, n ⬎ 4t ⫺ 2. Again, at least n ⫺ 4t ⫹ 2 ⬎ 0 of the
echoes received by s in round 3 were from correct processors. This means
that some correct processor received echo with echo[ p] ⫽ m from at least
(n ⫺ 1) ⫺ t processors other than p in round 2. At least ((n ⫺ 1) ⫺ t) ⫺
(t ⫺ 1) ⫽ n ⫺ 2t ⬎ 0 of these were from correct processors. Again, at
least one correct processor validated m for p in round 1.
—z ⫽ 4. In this case, n ⬎ 3t. Recall that s is a correct processor that set
relay[ p] to m in round 4. In this case, s must have set weak_echo[ p] to m
in round 3 and thus received echo with echo[ p] ⫽ m from at least n ⫺ 2t
processors other than p in round 3. At least (n ⫺ 2t) ⫺ (t ⫺ 1) ⫽ n ⫺
3t ⫹ 1 ⬎ 0 of these are correct; let u be one of these correct processors.
It must be that u received echo with echo[ p] ⫽ m from at least (n ⫺ 1) ⫺
t processors other than p in round 2. At least ((n ⫺ 1) ⫺ t) ⫺ (t ⫺ 1) ⫽
n ⫺ 2t ⬎ 0 of these are correct and validated m for p in round 1.
This concludes the proof that all three translations satisfy properties above
and, therefore, they all translate correctly from C(n, t) to A(n, t).
8.6. DISCUSSION. This section presented three nonuniform translations from
crash to arbitrary failures. These translations double, triple, and quadruple the
number of rounds of communication required. All these translations are efficient
in that they generate implementations that do not require substantially more
local computation than the original protocols. In all cases, if the largest message
sent in the original protocol is of size b, then the largest message sent in the
implementing protocol has size O(bn).
These translations are more efficient than those developed by Neiger and
Toueg [1990]. For n ⬎ 4t, they give a 4-round translation;20 for n ⬎ 4t ⫺ 2, this
paper gives a 3-round translation and a 2-round translation if n ⬎ 6t ⫺ 3. For
n ⬎ 3t, Neiger and Toueg give a 6-round translation; this paper gives a 4-round
translation for the same fault-tolerance.
9. Conclusions
The translations given in this paper simplify the task of designing fault-tolerant
protocols. The designer can work with the assumption that the only failures are
those of crashing and then convert the protocol automatically to tolerate more
20

A minor modification of their translation is correct with n ⬎ 4t ⫺ 2.

Simplifying Fault-Tolerance

553

severe failures. Because the translations developed here are nonuniform, they
are applicable to problems with failure-insensitive specifications.
For translations to send-omission failures, the original uniform translation of
Neiger and Toueg [1990] is simultaneously optimal with respect to faulttolerance and round-complexity. For general omission and arbitrary failures,
there are no nonuniform translations that are simultaneously optimal with
respect to both measures.21 For translations to general omission failures, we give
a hierarchy of translations, each of which is proven to have optimal roundcomplexity for a given fault-tolerance (or vice-versa). For arbitrary failures, we
give three translations, each of which is proven to have optimal or near-optimal
fault-tolerance for a given round-complexity.
Using translations such as those developed here does not guarantee the
development of the most efficient algorithms possible. For example, Hadzilacos
[1983] developed an efficient (t ⫹ 1)-round solution to Byzantine Agreement that
tolerates crash failures. We can use the translations given in Sections 7 and 8 to
derive efficient algorithms that uses z(t ⫹ 1) rounds for some z ⬎ 1 that
depends on the type and number of failures to be tolerated. However, there are
solutions to this problem that use fewer rounds. For general omission failures, it
is easy to develop an algorithm that uses only t ⫹ 1 rounds. For arbitrary
failures, Garay and Moses [1998] have developed and efficient (t ⫹ 1)-round
algorithm that requires only n ⬎ 3t.
Section 2.2 briefly discussed an alternative system model in which processors
receive external inputs continuously. It would be straightforward to modify our
lower-bound results and translations for this model. Such modifications would
not affect the fault-tolerance or the round-complexity of the results.
Round-based translations such as those presented here for synchronous systems have been studied for completely asynchronous systems by Coan [1988] and
Bazzi [1994]. Other researchers have studied problems in which there exists
timing uncertainty that is bounded. Attiya et al. [1994] consider Distributed
Consensus in such systems with crash failures. Ponzio [1991] extends this work to
systems with send-omission and arbitrary failures. It seems likely that translations
applicable to such systems would simplify the latter and may lead to improved
algorithms. In fact, the prominence of the term t/(n ⫺ t) in Ponzio’s work (as
well as in our own) suggests that an adaptation of our translations may generalize
his work. In addition, an adaptation of the translations to arbitrary failures may
result in new, more efficient solutions to Distributed Consensus in partially
synchronous systems with arbitrary failures. Preliminary work in this area has
been performed by Bazzi [1994].
It is clear that the notion of processor knowledge [Halpern and Moses 1990] is
closely related to the work presented here, especially that in Section 5.3. For
example, for a processor to refuse to receive a message from another, it must
know that, by the end of the next round, all correct processors refuse messages
from that other processor. We believe that the requirements of these and other
translations can be expressed using a knowledge-based formalism that may make
it possible to unify the proofs presented in Section 5.3 and to provide a
framework within which to prove related results.
21

Neiger and Toueg give a uniform translation from crash to general omission failures that is
simultaneously optimal with respect to fault-tolerance and round-complexity.

554

R. A. BAZZI AND G. NEIGER

ACKNOWLEDGMENTS.

We would like to thank Vassos Hadzilacos and Sam Toueg
for suggestions that led to the improvement of this paper. We also thank the
anonymous referees for many useful comments.

REFERENCES
ATTIYA, H., DWORK, C., LYNCH, N., AND STOCKMEYER, L. 1994. Bounds on the time to reach
agreement in the presence of timing uncertainty. J. ACM 41, 1 (Jan.), 122–152.
BAZZI, R. A. 1994. Automatically Increasing Fault Tolerance in Distributed Systems. Ph.D.
dissertation. Tech. Rep. 94-62. College of Computing, Georgia Institute of Technology.
BAZZI, R. A., AND NEIGER, G. 1992. The complexity and impossibility of achieving fault-tolerant
coordination. In Proceedings of the 11th ACM Symposium on Principles of Distributed Computing
(Vancouver, B.C., Canada, Aug. 10 –12). ACM, New York, pp. 203–214.
BRACHA, G. 1987. Asynchronous Byzantine agreement protocols. Inf. Comput. 75, 2 (Nov.),
130 –143.
COAN, B. A. 1986. A communication-efficient canonical form for fault-tolerant distributed protocols. In Proceedings of the 5th ACM Symposium on Principles of Distributed Computing (Calgary,
Alta., Canada, Aug. 11–13). ACM, New York, pp. 63–72. (A revised version appears in Coan’s
Ph.D. dissertation [Coan 1987].)
COAN, B. A. 1987. Achieving Consensus in Fault-Tolerant Distributed Computer Systems: Protocols, Lower Bounds, and Simulations. Ph.D. dissertation. Massachusetts Institute of Technology,
Cambridge, Mass.
COAN, B. A. 1988. A compiler that increases the fault-tolerance of asynchronous protocols. IEEE
Trans. Comput. 37, 12 (Dec.), 1541–1553.
FISCHER, M. J., AND LYNCH, N. A. 1982. A lower bound for the time to assure interactive
consistency. Inf. Process. Lett. 14, 183–186.
GARAY, J. A., AND MOSES, Y. 1998. Fully polynomial Byzantine agreement for n ⬎ 3t processors
in t ⫹ 1 rounds. SIAM J. Comput. 27, 1 (Feb.), 247–290.
HADZILACOS, V. 1983. Byzantine agreement under restricted types of failures (not telling the truth
is different from telling lies). Tech. Rep. 18-83, Aiken Computation Laboratory, Harvard University. (A revised version appears in Hadzilacos’s Ph.D. dissertation [Hadzilacos 1984].)
HADZILACOS, V. 1984. Issues of fault tolerance in concurrent computations. Ph.D. dissertation.
Harvard Univ.
HALPERN, J. Y., AND MOSES, Y. 1990. Knowledge and common knowledge in a distributed
environment. J. ACM 37, 3 (July), 549 –587.
LAMPORT, L., SHOSTAK, R., AND PEASE, M. 1982. The Byzantine generals problem. ACM Trans.
Program. Lang. Syst. 4, 3 (July), 382– 401.
NEIGER, G., AND TOUEG, S. 1990. Automatically increasing the fault-tolerance of distributed
algorithms. J. Algorithms 11, 3 (Sept.), 374 – 419.
NEIGER, G., AND TUTTLE, M. R. 1993. Common knowledge and consistent simultaneous coordination. Distr. Comput. 6, 3 (Apr.), 181–192.
PERRY, K. J., AND TOUEG, S. 1986. Distributed agreement in the presence of processor and
communication faults. IEEE Trans. Softw. Eng. 12, 3 (Mar.), 477– 482.
PONZIO, S. 1991. Consensus in the presence of timing uncertainty: Omission and Byzantine
failures. In Proceedings of the 10th ACM Symposium on Principles of Distributed Computing
(Montreal, Que., Canada, Aug. 19 –21). ACM, New York, pp. 125–138.
SRIKANTH, T. K., AND TOUEG, S. 1987. Simulating authenticated broadcasts to derive simple
fault-tolerant algorithms. Distr. Comput. 2, 2, 80 –94.
RECEIVED MARCH

1993;

REVISED NOVEMBER

Journal of the ACM, Vol. 48, No. 3, May 2001.

2000;

ACCEPTED DECEMBER

2000

Portable Memory
Rida A. Bazzi
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287-5406 U.S.A .
bazzioasu. edu
Abstract

is not allowed. Recently, and independently of us,
Ramkumar and Strumpen [19] proposed a design to
save the state of an application in a machine independent format even for application that use pointers. Their design still requires that the preprocessor
have some knowledge about the underlying architecture and make some assumptions about how compilers
generate object code.
In this paper, we present a new design that enables the heterogeneous migration of a general class of
applications while introducing a relatively small overhead on performance. The approach we take t o handle
heterogeneity is a middle ground between fully interpreted execution and precompilation. By doing that,
we obtain the advantages of both approaches without suffering from their drawbacks. In our approach,
differences of instruction sets are handled by having
precompiled versions of the application on different
machines. We complement precompilation by providing a high-level representation of main memory that
is portable across different platforms. Our design is
tailored for the C programming language, but can be
adapted for other languages. We chose the C language because it is more difficult t o handle than other
languages especially as far as pointer operations are
concerned.
The rest of the paper is organized as follows.
Section 2 gives an overview of related work in the
literature. Section 3 states the paper assumptions
about pointer operations. Section 4 introduces the
concept of portable memory. Section 6 discusses the
performance overhead incurred by portable memory.

Migrating applications is necesary to provide fault
tolerance in a distributed environment. In this paper, we present a new design that enables the heterogeneous migration of a general class of applications.
Our approach can handle applications that use pointers. Our design is tailored for the C programming language, but can be adapted for other languages. We
chose the C language because it is more difficult t o
handle than other languages, especially its pointer oper at ions.

1 Introduction
Process migration is used to achieve load balancing and provide fault tolerance in a distributed environment [6, lo]. An essential part of process migration is the ability of checkpointing the state of a process and restarting the process from the checkpointed
state. Unfortunately, process migration is not easy
if the environment is heterogeneous [20, 211. This is
due to the difference in the instruction sets and memory representation between different machines. One
approach to overcome these differences is t o use an interpreted language, such as JAVA [2]. Interpreted execution is usually an order of magnitude slower than
the execution of compiled code; Many applications
cannot tolerate such a slow down. Another approach
that was proposed to overcome architectural differences is to precompilation and high-level checkpointing [l,23, 24, 251. In this approach, precompiled versions of the source code on different machines t o overcome the instruction set differences. The application
state is saved at a high level in a machine-independent
format and can be restarted on a different machine.
To facilitate high-level checkpointing, researchers proposed the use of a preprocessor t o introduce the necessary code for checkpointing and recovery. All the
efforts cited above put limitations on the parts of an
application’s state to be saved and restored. For example, high level checkpointing of pointer variables

1071-0485/97$10.000 1997 IEEE

2

Related Work

Checkpointing has been studied extensively in both
uniprocessor and homogeneous distributed systems [7,
10, 11, 12, 13, 14, 15, 16, 17, 22, 24, 261. The emphasis in checkpointing algorithms is on transparency
and efficiency. Checkpointing in uniprocessor systems
is usually done by saving a copy of the memory image

162

of a process on stable storage [lo, 161. When a process fails, the memory image can be used to resume
execution of the process from its last checkpoint. This
type of low-level checkpointing is not portable and it
is difficult t o capture the whole state of the process
using this approach. Work has been done to optimize
the size and time overhead of checkpointing in uniprocessor systems [15, 171. One approach uses differential
checkpointing in which only variables that change are
saved [17]. Another approach uses more sophisticated
compiler techniques to minimize the size of the checkpoint [15].

jacent memory locations for variables that are declared
next to each other is not a correctness requirement for
compilers. The solution we present does not make of
of the two assumptions of [19].

3

Pointer Usage

Any high-level representation of the state of an
application has some limitations. In particular, any
machine-dependent features of a language cannot be
made portable in a generic way. In this section we
explicitly state our assumptions on pointer usage.
We assume that pointer variables are declared explicitly as such. A value is assigned to a pointer p
variable either by:

Theimer and Hayes [25] propose precompilation as
a way to achieve process migration in a heterogeneous system. Their approach takes a much narrower
view of the state of a process than the one we take.
Silva [24] provides a high-level checkpointing technique that only works for programs of a given structure. The DOME project at CMU [l, 231 uses highlevel checkpointing t o achieve portability. DOME is
an environment t o develop parallel programs. The environment has its own variable declarations that are
maintained in a machine-independent format. Highlevel checkpointing primitives are provided in that
project, but those primitives are not sophisticated
enough. For instance, the state of a process cannot be
saved at any point during the execution. Also, only
DOME variables can be saved and other variables declared in the application cannot be saved. There is
no support for saving dynamically allocated memory,
files, or communication descriptions because the types
of applications that DOME support have specific synchronization points. Saving the state of the process is
done at these synchronization points. Another work
related to the proposed research is the Arachne project
at Purdue [9]. In that project, preprocessor tools are
provided to save the state of a thread. Again, there are
restrictions on when a state can be saved. Arachne has
no support for saving dynamic structures and does not
provide support for checkpointing pointer variables.

using the malloc function,
by the use of pointer arithmetic on an array.
by setting p = q, where q is another pointer.
The assumptions we make about pointer usage are
not restrictive. To our knowledge, no related work
on heterogeneous migration allows more flexibility in
pointer usage.

4

Portable Memory

To allow migration of applications in a heterogeneous environment, it is necessary to provide pointers whose values are meaningful across different architecture. To provide pointers that are meaningful in different architectures, we introduce an intermediate Pointer-Array that hides the underlying architecture from the a.pplication. Pointers are represented as integers indexes into the Pointer-Array. The
Pointer-Array is a unidimensional array of pointers to
characters (char *).
Figure 1 shows the basic idea. Each pointer in the
application is replaceld with an index to an entry in
the Pointers array. The entries in the pointer array
are actual pointers to data used by the application.
In Figure 1 boldface arrows are used t o denote actual
pointers to memory locations; thin arrows denote indexes to entries in the Pointer-Array.
To make this approach viable requires that the
source code be automatically transformed. To achieve
that, we propose to use a preprocessor that automatically changes source-level pointer references and dynamic allocations to imake them use the source-level
main memory instead of the machine-level memory.
Each pointer variable is redeclared t o be of POINTER
type, which is a macro definition for long. Figures 2

Recently, and independently of us, Ramkumar and
Strumpen [19] proposed a design t o save the state of an
application in a machine independent format even for
application that use pointers. Their solution requires
that information about the memory representation of
basic types be available. This means that their solution is not truly heterogeneous. Also, their solutions
assumes that variables declared next t o each other in
the source code reside in adjacent locations in memory. While it is true that most compiler will generate
code according to their assumption, we believe that
this an unnecessary restriction because allocating ad-

163

p=5

___

P
q,r = 13
_

_

?

I

-

I

Pointers Array
(a>

(b)

Figure 1: (a) Data structure with Pointers array; (b) Data structure without Pointers array

typedef struct node C
char * employee-name;
struct node * next
} NAME-LIST;
NAME-LIST

*

names;

names = (NAME-LIST

*> malloc(sizeof (NAME-LIST)) ;

names->next = NULL;

Figure 2: Sample program before preprocessing

4.1

and 3 show the transformation required for pointer
variables.

Saving the State of a Program

To save the state of a program, one needs t o save
the values of all variables of all function on the activation stack. One scheme for saving the activation stack
is described in [9]. In this section, we describe how
dynamically allocated data structures can be saved.
By data structure, we mean a set of nodes that point
to each other. To save a dynamically allocated data
structure, the preprocessor needs t o include in each
node of the data structure some information about its
type so that it can be saved at run time. To see that
information about the node types is needed, consider
a data structure that consists of two types of nodes,
A and B , such that nodes of types A and B can point
to each other. Since the data structure is dynamically
allocated, a t any given time, it can contain both types

In Figure 2 a list data type is declared and space
for one node of the list is allocated. In the output
of the preprocessor, Figure 3, two memory allocations
are made. The first allocation returns an index into
the Pointer-Array. This memory allocation is at the
application level and is described below. The second
allocation returns an actual memory pointer which is
assigned to the element whose index is returned by the
first allocation. It is important t o note that because
Pointer-Array is declared as an array of pointers t o
characters, no alignment problems occur when typecasting.

164

#define POINTER long
char

*

Pointer-Array [POINTER-ARRAY-SIZE] ;

typedef struct node 1
POINTER employee-name;
POINTER next
1 NAME-LIST;
POINTER names;
names = malloc-pointer-array();
Pointer-Array [names] = (NAME-LIST *) malloc (sizeof (NAME-LIST)) ;
names->next = NULL;

Figure 3: Sample program after preprocessing
of nodes. To be able t o save the data structure, a way
to identify the types of the nodes that constitute the
data structure is needed. The solution we propose is
to include in each entry of the Pointer-Array a pointer
to a function that knows how to save the data corresponding t o that entry.
To save all dynamically allocated data, the saving
program would save all the nodes pointed to by entries
in the Pointer-Array. Note that if a node contains a
pointer, in other words an index value, that pointer
value is saved as it is because it is a meaningful index value. In addition to the data of node, its address
needs to be saved. For that, it is enough to save the
index of the entry that points to the node. Figure 4
shows how the data structure in Figure 1 can be saved.
Note that Figure 4 is not intended to be an eficient
representation of the structure, rather it is an illustration of the information that need to be saved.

2. Pointer variables values are obtained by simply
reading their values (remember that a pointer
variable is an index into the Pointer-Array).
3. If the array entry is NULL and the pointer variables p points t o some memory location, allocate
memory for the data that p points t o and load
that data.

As an example, consider how the structure of Figure 1 can be restored from the information in Figure 4. Initially, all entries of the array are initialized
to NULL. Then, p is set t o 5 and memory is allocated for the data pointed t o by p and the pointer
to the allocated memory is saved in the 5th entry of
the Pointer-Array. Next, the data pointed to by p is
loaded from secondary storage. Then, g and r are set
t o 13 and memory is allocated for the data at location
13 and the pointer t o the allocated memory is saved
in the 13th entry of the array. Next, the data pointed
to by g and r is loaded into the allocated memory. Finally, memory is allolcated for the data at location 19
and the resulting pointer is saved in location 19 and
the data of location 19 is loaded into the allocated
memory. Note that t,he data loaded contains indexes
and not real memory pointers and therefore it requires
no changes to make it meaningful on a different architecture.

4.2 Restoring the State of a Program
To goal of restoring the state of a program is t o
continue execution from the last checkpoint without
re-executing any of the program code. This can be
achieved by loading one by one the variables of each
function that was on the call stack at the time the
checkpoint was taken. In this section we describe how
dynamically allocated data is recovered.
To recover dynamically allocated data, we need to
reconstruct the Pointer-Array. This is done as follows.

5

Other Considerations
There are some aspects of C programs that are not

1. All the entries in the array are initialized t o

easily ported in a heterogeneous environment. For ex-

ample, union data structure can contain data whose

NULL.

165

p=5 [data p o i n t e d t o by p]
q=13 [data p o i n t e d t o by ql
r = 13
I 9 [data p o i n t e d t o by 191
Figure 4: Sample Representation of Dynamic Data
type is only known a t run time. Our scheme can easily accommodate unions by adding to pointers array
information about the run-time usage of dynamically
allocated memory. Such flexibility comes at the expense of performance.

6

Performance

We study the performance of the proposed scheme
in terms of the space and time overhead it introduces
during normal program execution.
The time overhead introduced by our scheme is
mainly due t o the introduction of a layer of indirection
t o all pointer references. In the worst case, memory access can be doubled for pointer dereference. However,
the worst case degradation is minimized by hardware
cache. Note that this worst case performance affects
only pointer dereferencing and does not affect nonpointer variable. The particular performance degradation of a particular application will depend on the
proportion of pointer references to total memory references. For many applications, such as scientific applications, most memory accesses are array accesses and
not pointer accesses. For such applications, the memory access overhead is minimal. Preliminary experiments on memory access time show that the memory
access can be slowed down by up to 50%.
The space overhead is mainly due to the introduction of the Pointer-Array and the need to save t o replace each pointer variable with an array index and
a pointer. Here we should note that the majority of
pointer variables appear as elements of dynamic structures. The overhead introduced depends on the ratio
of pointer variables to non-pointer variables and is application dependent. In the worst case, the memory

Conclusion

7

We have presented the design of a simple scheme
that makes it possible to migrate a general class of applications in a heterogeneous environment. We have
developed a preliminary version of the preprocessor
and are now working on on implementing a more sophisticated version that that can affects all the needed
source-level transformations.

References
[l] J. N. C. Arabe, A. Beguelin, B. LoweKamp, E.

Seligman, M. Starkey, and P. Stephan. Dome:
Parallel Programming in a Heterogeneous MultiUser Environment. Technical report CMU-CS95-137, School of Computer Science, Carnegie
Mellon University, April 1995.
[2] K. Arnold and J. Gosling The Java Programming
Language. Addison-Wesley, 1996.

[3] 0. Babaoglu, L. Alvisi, S. Amoroso, R. Davoli,
and L. Giachini. Paralex: An environment for
parallel programming distributed systems. In
Proceedings of the Sixth ACM International Conference on Supercomputing, pages 178-187, July
1992.
[4] L. Alvisi and K. Marzulo. Message Logging: Pessimistic, Optimistic, and Causal Checkpointing.
In Proceedings'of the Fiftenth International Conference on Distributed Computing Systems, May
1995.

[5] T. Becker. Application-Transparent Fault tolerance in Distributed Systems. Proceddings of the
2nd International Workshop on Conjigurable Distributed Systems, pages 36-45, March, 1994.

can be doubled if most variables are pointer variables.

In practice, this does not happen. As we mentioned,
for the time overhead, scientific applications would
suffer little from this scheme. Also, in general, we
do not expect the variables of a program to be pointer
variables.

[6] M.J. Litzkow, M.Livny and M.W.Mutka Condor
- A hunter of idle workstations In Proceedings of
the 8th International Conference on Distributed
Computing Systems, San Jose, California, USA,
1988, pp. 104-111.

'We do not discuss in this paper the overhead due to saving
and restoring the state of an application.

[7] K.M. Chandy and L. Lamport. Distributed snapshots: Determining global states of distributed

166

systems. ACM Transactions on Computer Systems, 3(1):63-75, February 1985.

[18] M. Russinovich, Z. Segall, and D. Siewiorek.
Application-Transparent Fault Management in
Fault-Tolerant Mach.
In Proceedings of the

[8] E. Dijkstra, L. Lamport, A. J. Martin, C . S.
Sholten, and E. F . M. Steffens. On-the-fly
garbage collection: an exercise in cooperation.
Communications of the ACM, 21( 11):966-975,
November 1978.

Twenty Third International Symposium on Fault
Tolerant Computing, 1993.
[19] B. Ramkumar and V. Strumpen. Portable Checkpointing for Heteirogeneous Architectures. In Proceedings of the 27th Fault- Tolerant Computing
Symposium, June 1997.

[9] B. Dimitrov
Arachne: A Compiler-Based
Portable Threads Architecture Supporting Migration on Heterogeneous Networked Systems. Master Thesis, Purdue University, May 1996.
[lo] M.J. Litzkow and MSolomon Supporting Checkpointing and Process Migration out-side the
UNIX Kernel In Proceedings of the 1992 Usenix
Winter Conference, San Francisco, California,
USA, 1992.
[ll] E.N.
Elnozahy,
D.B.
Johnson,
and W. Zwaenopoel. The performance of consistent checkpointing. In Proceedings of the 11th
IEE Symposium on Reliable Distributed Systems,
pages 39-47, 1992.

[20] B. Steensgaard and E. Jul Object and Native Code Thread Mobility Among Heterogeneous Computers. Proceedings of the Fijleenth
ACM Symposium on Operating Systems Principles, 1995.
[21] P. Smith and N. IHutchinson. Heterogeneous Process Migration: The Tui System. Technical Report 96-04, Department of Computer Science,
University of British Columbia, Feberuary 1996.
[22] R.E. Strom and S.A. Yemini. Optimistic recovery in distributed systems. ACM Transactions on
Computer Systems, 3(3):204-226, August 1985.
[23] E. Seligman and A. Beguelin High-Level Fault
Tolerance in Distributed Programs. Technical Report CMU-CS-941-223, Carnegie Mellon University, December 1994.

[12] E.N. Elnozahy and W. Zwaenepoel. Manetho:
Transparent rollback-recovery with low overhead,
limited rollback and fast output commit. IEEE
Transactions on Computers, 41(5):526-531, May
1992.

1131 D.B. Johnson and W. Zwaenepoel. Recovery in
distributed systems using optimistic message logging and checkpointing. Journal of Algorithms,
11:462-491, 1990.

[24] L. Silva. Portable Checkpointing and Recovery. In Proceedings of the Fourth IEEE International Symposzum on High Performance Distributed Computing, IEEE Computer Society
Press, August 1995, pp. 188-195.

[14] R. Koo and S. Toueg. Checkpointing and rollback
recovery for distributed systems. IEEE Transactions on Software Engineering, SE-13( 1):23-31,
January 1987.

[25] M.M. Theimer and B. Hayes Heterogeneous Process Migration by Recompilation. In Proceedings IEEE 11th International Conference on Distributed Computing Systems, 1991, pages 18-25.

[15] C. J. Li and W. K. Fuchs. CATCH: Compilerassisted techniques for checkpointing. Proceedings of 20th International Symposium on FaultTolerant Computing, pages 74-81, 1990.

[26] Y. Wang, Y. Huang, K. Vo, P. Chung, and C.
Kintala. Checklpointing and its Applications.
Proceedings of 25th International Symposium on
Fault- Tolerant Computing, pages 22-31, June,
1995.

[16] J.S. Planck, M. Beck, G. Kingsley, and K. Li.
Libckpt: Transparent checkpointing under Unix,
Proceedings Winter 1995 Usenix Technical Conference, pages 213-223, January 1995.

[17] J. S. Planck, J. Xu, and R. H. B. Netzer. Compressed Differences: An Algorithm for Fast Incremental Checkpointing. University of Tennessee,
Technical Report, UT-CS-95-302, 1995.

167

Foreword
It is our pleasure to present this proceedings of the Third Workshop on Hot
Topics in Software Upgrades (HotSWUp III), which took place on April 16,
2011 in Hannover, Germany. The workshop was held as a satellite event of the
International Conference on Data Engineering (ICDE 2011).
The call for papers solicited short papers that address practical and theoretical aspects of software upgrades as applied to applications ranging from
large-scale and distributed to small-scale and embedded. As HotSWUp was colocated with ICDE 2011 we particularly encouraged work considering software
upgrade issues in information systems. Among other approaches, submitted papers might suggest how a successful approach could be applied in a different
context, refute or revisit an old assumption about software upgrades, describe a
new problem, or propose a novel solution to an existing problem.
We received 17 submissions from researchers at locations around the world.
The committee ultimately accepted 9 papers. The submitted papers encompassed a variety of topics, including: formal reasoning about dynamic updates,
facilitating upgrades of embedded and real-time systems, exploring updates in or
facilitated by “the cloud,” and studying database schema and software package
evolution. The committee was impressed with the breadth and quality of the
submissions, and several worthy papers were not selected due to lack of space.
The organizers would like to thank the program committee, which (including
the organizers) consisted of 13 researchers with expertise in databases, systems,
distributed computing, and programming languages. The committee provided
timely and insightful reviews of submitted papers and very useful feedback
throughout the organization process. We would also like to thank Tudor Dumitras, Iulian Neamtiu, and Eli Tilevich, the organizers of HotSWUp II, for
their insight and assistance in putting together this year’s workshop; and the
ICDE organizers for graciously agreeing to host HotSWUp. Finally, we thank
the authors for providing the excellent content of the program.
We hope that you will find this program interesting and thought-provoking
and that HotSWUp III will provide you with impetus to share ideas with other
researchers and practitioners from institutions around the world.

Michael Hicks, Rida A. Bazzi, and Carlo Zaniolo
Workshop co-organizers

132

ICDE Workshops 2011

Workshop Organization
Organizers
Michael Hicks (University of Maryland, College Park, USA)
Rida A. Bazzi (Arizona State University, USA)
Carlo Zaniolo (University of California, Los Angeles, USA)
Program Committee
Rida A. Bazzi (Arizona State University, USA)
Carlo Aldo Curino (Massachusetts Institute of Technology, USA)
Fabien Dagnat (Institut Télécom, Télécom Bretagne, France)
Johann Eder (University of Vienna, Austria)
Michael Hicks (University of Maryland, College Park, USA)
Manuel Oriol (University of York, UK)
George Papastefanatos (Institute for the Management of Information Systems, Greece)
Paolo Papotti (Università Roma Tre, Italy)
Mark Segal (Laboratory for Telecommunications Sciences, USA)
Liuba Shrira (Brandeis University, USA)
Xin Qi (Facebook, USA)
Carlo Zaniolo (University of California, Los Angeles, USA)

133

2013 IEEE 27th International Symposium on Parallel & Distributed Processing

The Bounded Data Reuse Problem in Scientific Workflows
Mohsen Zohrevandi
Rida A. Bazzi
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
699 S Mill Ave, Tempe AZ, USA
Email: {mohsen, bazzi}@asu.edu
can be reused in the new workflow and which data of the
new workflow needs to be computed (not necessarily from
scratch). The problem in this case reduces to determining
the common data (computed as well as input) between the
two workflows. In practice, we cannot in general afford
to save all the intermediate data in one workflow. Instead,
we have limited amount of space that could be used for
that purpose. The workflow data reuse problem becomes
one of determining, given limited storage space for saving
intermediate data, which of the intermediate data to save in
order to maximize the savings in execution time.
The workflow data reuse problem has been studied in a
different form by Yuan et al. [14], which is the closest related
work. There, the goal is to minimize the total workflow execution cost through a combination of storing or regenerating
workflow data. The optimal choice depends on frequency
of access to data as well as the storage and computation
costs. They present a heuristic to decide which data needs
to be stored and which data needs to be regenerated when
needed. The heuristic is tested on only one workflow. Unlike
our work, they place no limit on the available space in their
solution. Also, there is no indication as to how well their
solution compares to an optimal solution.
In this paper, we introduce and study the bounded storage
workflow data reuse problem. We show through a simple
reduction from the 0/1-Knapsack problem [6] that the problem is NP-hard. The problem seems to be considerably
harder than the Knapsack problem because the profit from
one item depends on which other items are already chosen.
In other words, the cost function is non-additive and it
is so in a non-trivial way which rules out using existing
algorithms designed for Knapsack problem. Our attention
turns then to developing efficient heuristics to solve the
problem. In order to evaluate the heuristics, we need to
compare them to optimal solution. We restrict our study to
series-parallel workflow graphs [5], which, according to [3],
constitute more than 80% of scientific workflows (many
of the remaining 20% can be transformed to series-parallel
workflows through the introduction of control points). For
series-parallel graphs, we develop a branch-and-bound algorithm to find the optimal solution using a non-linear integer

Abstract—Large datasets and time-consuming processes have
become the norm in scientific computing applications. The
exploration phase in the development of scientific workflows
involves trial-and-error with workflow components, which can
take a lot of time given the time-consuming nature of the
workflow tasks. These facts suggest the possibility of reducing
the development time by reusing intermediate data whenever
possible. However the storage space is always limited. This
introduces a problem: which intermediate datasets from one
workflow should be kept to be reused in another workflow,
with a limited amount of storage. For the general class of
series parallel graphs, we model this problem using a non-linear
integer programming formulation and show that it is NP-Hard.
We provide a branch and bound optimal algorithm as well as
efficient heuristics. We conducted experiments over a large set
of randomly-generated workflows as well as a smaller set of
synthetic workflows which are based on real-world workflows
used by scientists in different disciplines. Our experiments show
that the best solution produced by the heuristics only differs
from the optimal value by less than 1% on average.
Keywords-Scientific Workflows; Intermediate Data; Data
Reuse; Series-Parallel.

1. I NTRODUCTION
Data- and Time-intensive scientific workflows are used
every day by scientists in various scientific fields with
support for building and running workflows being provided
by academic and industrial projects [1], [9], [10], [13].
The large amount of data processed by scientific workflow
poses challenging scheduling and execution problems [7].
These challenges are exacerbated when a scientist wants to
experiment with a workflow, especially in the exploration
phase. A scientist might try to execute a particular workflow
just to discover that some components do not provide the
expected analysis results. By modifying the workflow and
executing it repeatedly, the scientist can determine the right
workflow for the task. Such repeated execution of a modified
workflow is time consuming and can greatly benefit from using some of the data (including intermediate data) generated
by the previous version of a workflow in order to reduce the
execution time of the new version of the workflow. Such
reuse is only possible if the data is computed in the same
way in both workflows. Ideally, one would save all data
from one workflow execution and determine which data
1530-2075/13 $26.00 © 2013 IEEE
DOI 10.1109/IPDPS.2013.71

1051

The remainder of this paper is organized as follows. We
present related work in section 2 then we introduce the system model in section 3. We formally introduce the boundedspace data reuse problem in section 4 and prove that it is
NP-hard in section 5. In section 6 we study the properties of
space-minimal solutions with the aim of finding constraints
that can be used to improve the search for an optimal
solution. Series-parallel graphs are introduced in section 7
along with a non-linear integer programming formulation of
the problem. We present our Branch and Bound algorithm
for series-parallel graphs in section 8 and our heuristics
in section 9. We conclude with the experimental results
in section 10.

different problem in a simpler storage model compared to
the distributed resources model used in [11].
Guo et al. [8] addressed the problem of reusing intermediate data for Python scripts. Scientists often use scripting languages like Python to perform their data-analysis.
They usually have separate functions for different tasks and
manually save their intermediate results to disk to avoid recomputation in the future. Guo et al. presented an automated
mechanism to memoize intermediate results of long-running
functions to disk, manage dependencies between the source
code and saved data and reuse the memoized results in
subsequent executions whenever it is safe to do so. While
this is a helpful tool that could reduce development time, it
is specific to a programming language rather than an abstract
model of computation and also they are not concerned about
optimizing disk usage nor they consider a bounded storage.
Saha et al. [12] addressed the problem of merging two
DAGs without creating cycles. They considered merging
workflow nodes as a way of helping the scheduler to identify
similar tasks and co-locate them on the same machine to
benefit from caching and paging that could lead to better
performance. We focus on reusing datasets rather than
merging nodes to boost the workflow execution.
Yuan et al. [14] addressed a similar problem for scientific workflows deployed in cloud computing systems.
They considered a workflow and all its intermediate datasets
and based on usage data extracted from system logs, they
proposed a strategy to find a trade-off between computation
cost and storage cost. Our work revolves around finding
common datasets between two workflows and trying to
eliminate some computations from the second workflow by
reusing common datasets. We believe that this is better suited
for exploration phase in the development of a workflow,
since datasets have a short lifetime during this phase that
may not go beyond a single iteration. Also our work does
not depend on a specific environment like Cloud as opposed
to [14] which depends on the cost model, i.e. storage cost
and computation cost, of the cloud service provider.

2. R ELATED W ORK

3. M ODEL

Large-scale data-intensive workflows pose the challenge
of disk-usage management that should be addressed by the
scheduler. Ramakrishnan et al. [11] addressed the problem
of minimizing the amount of space a workflow requires
by removing datasets at run-time when they are no longer
required. They also proposed a storage-aware scheduling
algorithm to ensure that the amount of storage required
by the workflow fits distributed resources. This shows the
importance of storage-awareness in designing practical algorithms for data-intensive workflows. Our work is based
on the same practical viewpoint, however we address a

In a data-flow model of computation, the computation is
modelled by a DAG (Directed Acyclic Graph) in which
nodes represent computational tasks and edges show the
flow of data between tasks. Each task produces zero or
more output files. We use port labels to distinguish between
different output files of a task. The edges of the graph are
labelled with port labels to specify the file that should be
transferred from one task to another.
Tasks are distinguished by labels; two nodes in two
different graphs having the same label represent the same
task. If a task is an executable program, the label can be

programming formulation of the problem that speeds up the
branch-and-bound search.
We developed eighteen core heuristics to solve the problem. These are combined together in one solution that
chooses the best amongst the eighteen for a given problem
instance. The heuristics are computationally efficient so
running all of them does not present added computational
burden. We tested the heuristic solution on a suite of
synthetic workflows [2] and randomly generated workflows.
On average the difference in the objective function value
found by the (combined) heuristic solution and the optimal
solution is less than 1% for series-parallel graphs.
In summary, the contributions of the paper are the following:
1) We introduce the bounded data reuse problem for
scientific workflows.
2) For the important case of series parallel graphs, we
give a non-linear integer programming formulation of
the problem which is amenable to an efficient optimal
branch-and-bound solution.
3) We present a heuristic solution that for a large number
of sample workflows perform extremely well with
less than 1% average difference between the optimal
solution and the heuristic solution for series-parallel
workflow graphs.

1052

thought of as a short representation of the program text (this
can be easily achieved using a hash function).
For any graph G, EG is the set of edges of graph G, VG
is the set nodes of graph G. P red(v) is the set of immediate
predecessors of node v and Succ(v) is the set of immediate
successors of node v. Outgoing and incoming edges of
node v are denoted by Outgoing(v) and Incoming(v)
and the number of outgoing and incoming edges of node
v are denoted by Outdegree(v) and Indegree(v), i.e.
Outdegree(v) = |Outgoing(v)|.
A workflow is a labelled and weighted directed acyclic
graph G = (V, E, `node , `edge , wnode , wdataset ) in which V is
a set of nodes, E is a set of directed edges, `node : V → Σ∗
is a function that assigns labels to nodes (Σ is an arbitrary
alphabet), `edge : E → Σ∗ is a function that assigns
labels to edges, wnode : V → R≥0 is a function that
assigns weights to nodes. The weight of a node wnode (v)
represents the amount of time taken by task v. To describe
wdataset : D(G) → R≥0 , the function that specifies sizes of
datasets, we need the following definitions:
•

Figure 1: There are only 6 datasets in this graph:
d1 = (t1 , a), d2 = (t1 , b), d3 = (t2 , b)
d4 = (t6 , a), d5 = (t6 , b), d6 = (t6 , c)

We distinguish between intermediate datasets and output
datasets: Generally all incoming edges of the sink node are
considered to be output datasets of the workflow and all
other datasets are intermediate, unless specified otherwise.
4. DATA R EUSE P ROBLEM
We need to determine if two datasets in different workflows represent the same data or not. The contents of a
dataset depends on the node that produces that dataset plus
all input datasets of that node. However, those datasets
themselves depend on their parent nodes and other datasets,
etc. but we assumed that the workflow is acyclic, so this
sequence will eventually end at the source node of the
graph. The following definitions and lemma capture this idea
precisely:

P orts(v) is set of port labels of node v:
P orts(v) = {`edge (e) | e ∈ Outgoing(v)}

•

D(G) is the set of datasets of workflow G:
D(G) = {(v, l) | v ∈ VG ∧ l ∈ P orts(v)}

Definition 2. The dependency graph of a dataset d = (v, l)
is a connected subgraph of workflow induced by nodes on
every path from the source node of the workflow to v and
all edges connecting those nodes.

Note that port labels are defined based on existing edge
labels, which means that we are only interested in datasets
that are transferred between nodes (there might be unused
datasets produced by a task). See fig. 1 for an example of
a workflow and its datasets. For a dataset d = (v, l) only
outgoing edges of v can carry the dataset. We define the set
of all edges carrying d:

Lemma 1. Two datasets d1 ∈ D(G1 ) and d2 ∈ D(G2 )
are equal, if they have the same label and there is a labelpreserving isomorphism between their dependency graphs.

Edges(d) = {e | e ∈ Outgoing(v) ∧ `edge (e) = l}

Proof: The proof is by induction on the length of the longest
path from the source node to the node that generates the
dataset. We omit the details for space considerations.

For example for the graph depicted in fig. 1:
Edges(d3 ) = {(t2 , t3 ), (t2 , t4 ), (t2 , t5 )}

We define incoming and outgoing datasets of node v:

Definition 3. Two datasets d1 ∈ D(G1 ) and d2 ∈ D(G2 )
are common to G1 and G2 if they are equal.

in

D (v) = {d | d ∈ D(G) ∧ Edges(d) ⊆ Incoming(v)}
Dout (v) = {d | d ∈ D(G) ∧ Edges(d) ⊆ Outgoing(v)}

Definition 4. Given two workflows G1 and G2 , the common
datasets subgraph GC is a subgraph of G1 that consists of
all datasets common to G1 and G2 .

Definition 1. A two-terminal DAG or TTDAG for short, is
a DAG that has a single source node s and a single sink
node t: Indegree(s) = Outdegree(t) = 0

Figure 2 shows an example of two workflows and their
common datasets subgraph. The workflows have similar
structure but the labels are not the same, the changes are
circled on the second graph. The graph on the right is
the common datasets subgraph which only includes datasets
common to G1 and G2 .

We assume that workflows are TTDAGs. Any DAG with
more than one source(sink) nodes can be easily accommodated by adding a dummy source(sink) node and connecting
all other source(sink) nodes to the dummy node.
1053

Figure 2: GC is the common datasets subgraph of G1 and G2 .
The dependency graph of dataset (g, a) in G1 and G2
are circled, they are not isomorphic preserving labels,
and that’s why (g, a) 6∈ D(GC )

Figure 3: Nodes covered by X = {(t4 , a), (t10 , a)}
Covered(X) = {t4 , t5 , t6 , t7 , t8 , t9 , t10 }

s.t.

Workflow tasks are executed to process input and produce
datasets. If we already have all the datasets that are going
to be produced by a task, then we don’t need to execute
that task and we can save computation resources by not
executing such tasks. We say such a task is covered. In the
same way, if all outputs of a task are fed to other tasks that
are covered themselves, then that task is also covered. In
order to measure the computation saving for a given set of
datasets X, we define Covered(X) to be the set of nodes
that are covered by X.

•

•

d∈X

wdataset (d) ≤ L

X ⊆ D(GC )

(4.2)
(4.3)

This formulation of the data reuse problem does not
require that the optimal solution that maximizes the objective
function f be also space-efficient. Two solutions that maximize f might have different space usage. This can happen if
an optimal solution does not fill the whole space L, in which
case there is slack between the space used by an optimal
solution and the available space. By filling the slack with
any redundant dataset that fits, we obtain another optimal
solution that is not space-efficient. This situation can result
in a large search space for optimal solutions. Our algorithms
for finding optimal solutions avoid the large search space
by explicitly ruling out some solutions that are not spaceefficient.

Definition 5. For X ⊆ D(GC ):
•

X

A node v ∈ Covered(X) if all its outgoing edges are
covered
An edge e is covered if there exists a dataset d ∈ X
such that e ∈ Edges(d)
If v ∈ Covered(X) then all incoming edges of v are
covered

5. NP-H ARDNESS
The 0/1-Knapsack is reducible to Data Reuse problem but
we omit the proof.
It should be noted that the objective function f is not
additive [4, p. 96], for example consider Y = {(t3 , a)} for
the graph of fig. 3, X ∩ Y = ∅ and:

Lemma 2. A node v is an element of Covered(X) if and
only if every path from v to the sink node of the graph goes
through one of the edges in the following set:
[
Edges(X) =
Edges(d)
d∈X

Covered(X) = {t4 , t5 , t6 , t7 , t8 , t9 , t10 }
Covered(Y ) = {t3 }

The proof is omitted. See fig. 3 for an example. These
definitions allow us to precisely define the problem:

Covered(X ∪ Y ) = {t1 , t2 , t3 , t4 , t5 , t6 , t7 , t8 , t9 , t10 }

Definition 6. DATA R EUSE P ROBLEM : Given two workflow
graphs G1 and G2 and space limit L, find a set of datasets
X common to G1 and G2 such that total size of datasets in
X does not exceed L and sum of computation time of tasks
covered by X is maximized. We want to find X ⊆ D(GC )
P
such that t∈Covered(X) wnode (t) which is the total amount
of computation time saved by X, is maximized:
max

f (X) =

X
t ∈ Covered(X)

wnode (t)

If all node weights are positive then:
f (X ∪ Y ) 6= f (X) + f (Y )

Also adding an element to X does not necessarily increase
f (X), for example for Z = X ∪ {(t5 , a)} we have f (Z) =
f (X) since dataset (t5 , a) does not cover any node that is
not already covered by X. Since the objective function is
not additive, we can not directly use existing algorithms for
Knapsack to solve this problem.

(4.1)

1054

chain Nj such that d1 ∈ Nj and d2 ∈ Nj . Since
d1 and d2 belong to the same chain, one of them
covers the other, i.e. Covered(d1 ) ⊂ Covered(d2 )
or Covered(d2 ) ⊂ Covered(d1 ), so we can remove
one of them from X without changing the value of
the objective function. Thus X is not space-minimal
and this is a contradiction.
2) Let’s assume that Covered(A) ⊆ Covered(B). Since
A ∩ B = ∅ we can remove all elements of A from X
without affecting f (X) hence X is not space-minimal
which is a contradiction.

6. P ROPERTIES OF S PACE -M INIMAL S OLUTIONS
Ideally, we would like to find solutions that are spaceminimal. These are solutions that do not have extraneous
datasets. By requiring that a solution be space minimal,
an algorithm can better prune the search space. This is
our main interest in space-minimal solutions. Formally, we
define space-minimal solutions as follows.
Definition 7. A solution X is space-minimal if:
X=∅

or

∀d ∈ X , f (X − {d}) < f (X)

Space-minimality does not conflict with maximizing the
objective function as shown in the following lemma.

Given that in a space minimal solution no two datasets
belong to a chain, we can add a new set of constraints
to our problem formulation with the goal of ruling out
datasets that are in the same chain from the solution. Using
the characteristic function IX of a collection of datasets X
(IX (d) = 0 if d 6∈ X and IX (d) = 1 if d ∈ X), we can
write:

Lemma 3. For any optimal solution Xopt , there is a spaceminimal solution Xm such that f (Xopt ) = f (Xm ).
The proof of the lemma is straightforward and is omitted.
As the lemma states, any optimal solution can be easily
transformed into a space-minimal solution. For technical
reasons, our algorithms do not attempt to search for a spaceminimal solution because, in general, it is not possible to
tell if a partial solution will turn out to be space-minimal.
Nonetheless, for some partial solutions, it is possible to
ascertain that some combinations of datasets cannot be
part of a space-minimal solution, and we do rule out such
combinations. This has the advantage of reducing the search
space and improving the running time of the algorithms.
One situation in which it is possible to tell that two data
sets cannot be part of the same space-minimal solution is
when the datasets are part of a chain.

max
s.t.

f (X) =
X

X

d∈X

Sj =

t ∈ Covered(X)

wnode (t)

(6.1)

wdataset (d) ≤ L

(6.2)

IX (d) ≤ 1

j = 1, 2, ..., k (6.3)

X
d ∈ Nj

X ⊆ D(GC )

(6.4)

Sj is an auxiliary variable that shows the number of datasets
from chain Nj present in the solution.
We can find all chains in a workflow by smoothing the
workflow graph, that is for any pair of edges e1 = (a, b) and
e2 = (b, c) such that e1 and e2 are the only edges incident to
b, replace e1 and e2 with a new edge e = (a, c) and remove
node b. If we keep track of the edges that get deleted in this
process, we can find out which edges of the original graph
are replaced by which edges in the resulting multigraph.
We would label the edges of the resulting multigraph with
N1 , N2 , ..., Nk and associate with each Nj a set of datasets
of the original graph that were replaced by Nj . These sets
are all the chains of the workflow. We would refer to the
result of smoothing the common datasets subgraph GC by
GC which is a multigraph whose edges are chains of GC .
Figure 4 shows the result of smoothing process for the graph
of fig. 3. Here are the chains:

Definition 8. A chain is a sequence of datasets C =
{(v1 , l1 ), (v2 , l2 ), ..., (vk , lk )} such that: Outdegree(vi ) =
Indegree(vi ) = 1 for i = 2, 3, . . . , k and vi ∈ P red(vi+1 )
for i = 1, 2, . . . , k − 1
The last dataset in the chain (vk , lk ) is called the head of
the chain. If {(v1 , l1 ), (v2 , l2 ), ..., (vk , lk )} is a chain, then:
Covered((v1 , l1 )) ⊂ Covered((v2 , l2 ))... ⊂ Covered((vk , lk ))

Lemma 4. The following properties hold for any spaceminimal solution X:
1) X contains at most one dataset from each chain
2) For all disjoint non-empty subsets A, B ⊆ X that
satisfy Covered(A) 6= ∅ and Covered(B) 6= ∅, we
have Covered(A) 6⊆ Covered(B)
Proof:
1) Let’s assume there is a space-minimal solution X such
that d1 ∈ X and d2 ∈ X and there exists some
1055

N1 = {(t1 , b)}
N3 = {(t2 , b), (t3 , a)

N2 = {(t1 , a)}
N4 = {(t2 , b), (t4 , a)}

N5 = {(t2 , b), (t5 , a)}

N6 = {(t6 , b), (t7 , a)}

N7 = {(t6 , c), (t8 , a)}
N9 = {(t10 , a)}

N8 = {(t6 , a), (t9 , a)}

parallel graph depicted in fig. 5:

d1 = (t1 , a)

d2 = (t1 , b)

d3 = (t2 , a)

d4 = (t2 , b)

d5 = (t3 , a)

d6 = (t4 , a)

d7 = (t5 , a)

d8 = (t6 , a)

d9 = (t6 , b)
d13 = (t9 , a)

d10 = (t6 , c)
d14 = (t10 , a)

d11 = (t7 , a)

d12 = (t8 , a)

If d3 ∈ X and d4 ∈ X, then adding d2 to X does not change
the value of f (X). This is an example of a redundancy that
can be avoided by introducing new constraints. Here are
other examples: if X contains d3 and d6 , then d2 would
be redundant. Also having d14 in a candidate solution X,
makes d1 , d8 , d9 , d10 , d11 , d12 , d13 redundant.
To be able to present a general formulation, we introduce
some definitions and lemmas, then we present a general
formulation of the constraints and explain how they translate
to constraints for the graph of fig. 5.

Figure 4: Smoothed multigraph for graph of fig. 3

Definition 10. A node u is called an OR Joint if
Outdegree(u) > 1. A node v is called an AND Joint if
Indegree(v) > 1.
Figure 5: Sample TTSP graph GC (left) and GC (right)

Nodes of GC are joint nodes of GC (see fig. 5 for
example). We denote the set of all AND joints of GC by
AND(GC ) and the set of all OR joints by OR(GC ).

7. I NTEGER P ROGRAMMING F ORMULATION FOR
S ERIES -PARALLEL G RAPHS

Definition 11. For any node v in GC , the region of v denoted
by Rv is the smallest series-parallel subgraph of GC that
includes v and all its outgoing edges. Similarly, for a node
v in GC , the region of v denoted by Rv is the smallest seriesparallel subgraph of GC that includes v and all its outgoing
edges.

In this section we focus on series-parallel graphs [5], we start
by giving a definition for two-terminal series-parallel graphs
then we show an example and continue with the integer
programming formulation for this subclass of DAGs that
includes constraints for generating space-efficient solutions.
Here is a definition for series-parallel graphs that we use:

Note that edges of region Rv are chains and Rv is only
defined for joint nodes. For example Rt2 in fig. 5 is the
subgraph of GC that includes nodes t2 , t11 and two edges
between them, while Rt2 is a subgraph of GC that includes
nodes t2 , t3 , t4 , t5 , t11 and all edges connecting those nodes.

Definition 9. A two-terminal series-parallel graph or TTSP
for short is a TTDAG that can be constructed by a sequence
of series and parallel compositions starting from a set of
basic TTSPs.
•
•

•

Basic TTSP: a two-node single edge TTDAG.
Series Composition: S = Series(G, H) of two TTDAGs G
and H, is a TTDAG created from the disjoint union of graphs
G and H by merging the sink of G with source of H. The
source of G becomes the source of S and the sink of H
becomes the sink of S.
Parallel Composition: P = P arallel(G, H) of two TTDAGs
G and H, is a TTDAG created from the disjoint union of
graphs G and H by merging the sources of G and H to form
the source of P and merging the sinks of G and H to form
the sink of P .

Lemma 5. For a node u such that Succ(u) = {v}, the region
of u is a basic TTSP with source node u and sink node v.
The proof is omitted for lack of space.
Lemma 6. Any path from node u to the sink node of the
graph goes through the sink node of the region of u.
The proof is by strong induction over the number of series
and parallel compositions required to construct the seriesparallel (multi)graph G containing node u. We omit the
details.

For example, the graph depicted in fig. 3 is not a TTSP
graph. Figure 5 shows a sample TTSP graph. We motivate
the formulation by considering all datasets of the series1056

Definition 12. An s-t cut1 is a minimal cut if and only if
the cut-set is minimal in covering s.

We also have AND joint t10 with a single outgoing edge
Outgoing(t10 ) = {N8 }. Chains covered by this node
are: CovChains(t10 ) = {N2 , N5 , N6 , N7 }, so we have 4
constraints for this AND joint:

Definition 13. A chain C = {d1 , d2 , ..., dn } is covered by
node s if Covered(C) ⊆ Covered(Din (s)). The set of all
chains covered by node s is denoted by CovChains(s).

1)
2)
3)
4)

For example in fig. 5:
CovChains(t10 ) = {N2 , N5 , N6 , N7 }

S2 + S8
S5 + S8
S6 + S8
S7 + S8

≤1
≤1
≤1
≤1

To have an integer programming formulation we need to
define binary variables for common datasets and express the
objective function in terms of those binary variables. We use
variables xi = IX (di ) ∈ {0, 1} for i = 1, 2, ..., N where
N = |D(GC )| is the total number of common datasets.
P
We use ai =
t∈Covered({di }) wnode (t) as coefficients of
xi in the objective function, and x = (x1 , x2 , ..., xN ) is
the solution vector. The integer programming formulation is
given in fig. 6.

Equation (6.1) eliminates the basic form of redundancy
in the solution, however other forms of redundancy, related
to joint nodes, can be eliminated as well. Consider a region
R = Rs , assume the sink node of this region is node t, the
cut-set of any minimal s-t cut, covers node s. Such a cut-set
is a set of chains from region R. If we select one dataset
from each one of the chains in the cut-set, the resulting set
of datasets covers s. For any chain Nx that is covered by
node s (see def. 13) we can add the following constraints:
X
Sx +
Sj ≤ |CSiR |
i = 1, 2, ..., MR

Lemma 7. Any optimal solution of the integer programming
problem given in eqs. (7.1) to (7.8) is also an optimal
solution for the problem given in eqs. (6.1) to (6.4) for
series-parallel graphs.

Nj ∈CSiR

Where MR is the number of minimal s-t cuts in region R
and CSiR is the ith minimal cut-set in region R. Considering
the smoothed multigraph GC , any AND joint t with a single
outgoing edge i.e. Outdegree(t) = 1, has an outgoing chain
Ny . If we select a dataset from chain Ny , it covers node t,
so for any chain Nx that is covered by node t we can add
the following constraint:

Proof: First we show that the objective function given in
eqs. (7.1), (7.6) and (7.7) is equivalent to eq. (6.1) for all
solutions that have the two properties stated in lemma 4 and
then show that the constraints given in eqs. (7.2) to (7.8)
satisfy the constraints given in eqs. (6.2) to (6.4) and enforce
those properties, hence any optimal solution to eqs. (7.2)
to (7.8) is an optimal solution to eqs. (6.1) to (6.4) for seriesparallel graphs.
In the following proof X = {d1 , d2 , ..., dm } is a solution
that has the properties stated in lemma 4. Let Ai =
Covered({di }) for i = 1, 2, ..., m and Bj = Covered(Yj )
for j = 1, 2, ..., 2m − m − 1, where Yj is j th subset of X
with two or more elements. Then:

Sx + Sy ≤ 1
As an example, for the graph of fig. 5 we have three regions:
1) Rt2 , this region has one minimal s-t cut:
CS1 = {N3 , N4 }. Chains covered by t2 are:
CovChains(t2 ) = {N1 }. So we have one constraint
for this region: S1 + S3 + S4 ≤ 2
2) Rt6 , this region has one minimal s-t cut:
CS1 = {N5 , N6 , N7 }. Chains covered by t6
are: CovChains(t6 ) = {N2 }. So we have one
constraint for this region: S2 + S5 + S6 + S7 ≤ 3
3) Rt1 , this region has six minimal s-t cuts:
a) CS 1 = {N1 , N2 }
b) CS 2 = {N1 , N5 , N6 , N7 }
c) CS 3 = {N1 , N8 }
d) CS 4 = {N3 , N4 , N2 }
e) CS 5 = {N3 , N4 , N5 , N6 , N7 }
f) CS 6 = {N3 , N4 , N8 }
But no chains are covered by t1 so we have no
constraint for this region.

Covered(X) =

=

m
[

Ai ∪

2m −m−1
[

i=1

j=1

m
[

2m −m−1
[

Ai ∪

i=1

Bj

(7.9)




[

 Bj −

j=1

Covered(S)

(7.10)

S⊂Yj

For Y1 , Y2 , ... we define:
CC(Yj ) = Bj −

[

Covered(S)

S⊂Yj

= Covered(Yj ) −

[

Covered(S)

S⊂Yj

We don’t need to consider all of the 2m − m − 1 subsets
of X since some of these subsets do not cover any node
and some others do not add anything to the second term of
eq. (7.10). We only need to consider those subsets among

1

An s-t cut is a partition of graph nodes into two subsets S and T such
that s ∈ S and t ∈ T . The cut-set of an s-t cut is the set of edges that
have one end in S and the other end in T

1057

max

f (x) =

N
X

ai xi +

i=1

s.t.

N
X




X

MR
X

wR

s∈OR(GC ) , R=Rs

Y

i=1 N ∈CS R
j
i


Sj 

(7.1)

xi wdataset (di ) ≤ L

(7.2)

i=1

Sj =

X

xi ≤ 1

j = 1, 2, ..., k

(7.3)

di ∈Nj

Sx +

X

R

Sj ≤ |CSi |

i = 1, 2, ..., MR

Nx ∈ CovChains(s)

R = Rs , s ∈ OR(GC )

(7.4)

Nx ∈ CovChains(t)

Outgoing(t) = {Ny }

t ∈ AND(GC )

(7.5)

i = 1, 2, ..., N

(7.6)

R = Rs , s ∈ OR(GC )

(7.7)

Nj ∈CS R
i

Sx + Sy ≤ 1
X
ai =

t∈Covered({di })

wR = wnode (s) +

wnode (t)
P

t∈Covered(D in (s))

x = (x1 , x2 , ..., xN ) ⊆ {0, 1}

wnode (t)

N

(7.8)

Figure 6: Non-linear integer programming formulation for series-parallel graphs

Y1 , Y2 , ... that are space-minimal and have CC(Y ) 6= ∅, we
call such subsets core subsets of X and for the rest of this
proof we only consider core subsets of X. Any node u in
Covered(X) is either covered by a single dataset (first term
of eq. (7.10)) or some core subset Y . Let’s assume that v is
the sink node of Ru and Y is the smallest core subset that
covers u. There are two possibilities:
1) Y is all from Ru . In this case u must be an OR joint
and there exists a minimal cut-set CS in Ru such that
|Y | = |CS| and yi ∈ Nri for all i = 1, 2, ..., |Y | and
Y = {y1 , y2 , ...} and CS = {Nr1 , Nr2 , ....}. CS is a
matching minimal cut-set for Y .
In fact, u should be an OR joint because otherwise the
region of u would have a single edge which contradicts
the fact that Y has more than one element which are
all from Ru . If there is no matching minimal cutset in Ru , then either there are elements in Y that
if removed we can find a matching minimal cut-set
which means Y is not space-minimal or we have to
add some elements to Y to be able to find a matching
minimal cut-set in which case it means that Y does
not cover u because there is a path from u to the sink
node of the graph that does not go through the edges
of Y .
2) Y is not all from Ru . In this case Y can not be partly
from Ru and partly from other regions because if that
is the case then neither parts can cover u on their
own (since if one of them covers u then the other part
would be redundant) and so there exists a path from
u to the graph’s sink node that does not go through
edges of Y . So Y should be from outside Ru and it
should cover v because every path from u to the sink
node of the graph passes through v. But v is a different

node that should be covered by Y and by induction we
will reach a point where Y should cover some node s
such that Y is all from the region of s.
So in both cases there is some OR joint w such that Y is
all from the region of w and there is a matching minimal
cut-set for Y in Rw . Since X has the two properties stated
in lemma 4, Y is the only core subset such that u ∈ CC(Y ).
Assuming the core subsets of X are Z1 , Z2 , ..., Zcx , we can
rewrite the objective function of eq. (6.1) in the following
form:
f (X) =
=

X
t∈Covered(X)
m X
X

wnode (t)

wnode (t) +

cx
X

(7.11)
X

wnode (t)

(7.12)

i=1 t∈CC(Zi )

i=1 t∈Ai

Assuming that the matching minimal cut-set for Zi is
P
CSjR , then
t∈CC(Zi ) wnode (t) is equal to wR . So we
can use cut-sets and regions instead of core subsets. Using
xi = IX (di ) for i = 1, 2, ..., N , we have:
f (X) =

N
X
i=1


ai xi +

X
s∈OR(GC ) , R=Rs

MR
X

wR


Y

i=1 N ∈CS R
j
i


Sj 

(7.13)

Q
The term
Nj ∈CSiR Sj is at most 1 because of the
constraint Sj ≤ 1 and the summation over i = 1, 2, ..., MR
is also at most 1 because of the correspondence between
core subsets and minimal cut-sets for every X that has the
properties of lemma 4.
It only remains to show that the constraints given in
eqs. (7.2) to (7.8) satisfy the constraints given in eqs. (6.2)
to (6.4) and enforce the properties stated in lemma 4.
Equations (7.2), (7.3) and (7.8) are the same as eqs. (6.2)
to (6.4) only expressed in terms of binary variables. The first
property in lemma 4 is enforced by eq. (7.3), so we only
1058

need to show that the second property of lemma 4 is enforced
by eqs. (7.4) and (7.5). The proof is by contradiction and
we omit the rest of the proof for lack of space.

best solution, the best solution will be updated. Lines 2026 expand the current subproblem, by picking one of its
free variables and fixing it to 0 and 1 to create two new
subproblems. If the upper bound is a close approximation
of best solution to the subproblem, then we expect to skip
a large portion of the search space.
For a given subproblem, some variables might be fixable
in respect to the constraints. For example, in every chain we
can set one variable to 1 at most, and this is expressed in
the constraints given in eq. (7.3), so if we have a variable
xi = 1 in a subproblem, we can fix all other variables that
are in the same chain to 0. This will further help to limit
the amount of search necessary to find the optimal solution.
To calculate the upper bound z2 as described in previous
section, we need the graph representation of the problem. We
need to find all free variables and variables fixed at 1, and
their respective datasets and find the nodes that are covered
by these datasets.

8. B RANCH AND B OUND A LGORITHM
The non-linear integer programming problem given in
previous section can be solved using the Branch and Bound
technique. The most important task in Branch and Bound is
to calculate a tight upper bound for a given subproblem.
Definition 14. S UBPROBLEM : A subproblem S for problem
P is characterized by a set of fixed variables: f ixedS =
{(i, vi ) : i ∈ {1, 2, ..., N } and vi ∈ {0, 1}}. Subproblem
S is defined as:
S=P ∧

h

xi = vi ∀(i, vi ) ∈ f ixedS

i

Variables that do not appear in the fixed set, are called free
variables:
f reeS = {1, 2, ..., N } − {i : ∃ v, (i, v) ∈ f ixedS }

Algorithm 1 Branch and Bound

A subproblem with empty fixed set is equal to the original
problem (f ixedS = ∅ ⇒ S = P). A subproblem S is
feasible if f ixedS does not violate any constraint of P.

Require: problem P, GC and GC Return: xopt = (x1 , x2 , ..., xN ) ⊆ {0, 1}N
1: Compute a feasible solution xinit to P by certain heuristics
2: xopt ← xinit
3: fopt ← f (xopt )
4: S ← P
. S is the current subproblem
5: Que ← {S}
. Que is a queue of unexplored subproblems
6: while |Que| > 0 do
7:
if S is not feasible then
8:
Fathom()
9:
else
10:
f ixedS ← f ixedS ∪ FixableVariables(S)
11:
f¯ ← UpperBound(S)
. Upper bound of the current subproblem
12:
if f¯ ≤ fopt then
13:
Fathom()
14:
else
15:
x̃ ← FindBetterFeasibleSolution(S)
16:
if f (x̃) > f (xopt ) then
17:
xopt ← x̃
18:
fopt ← f (xopt )
19:
end if
20:
if |f reeS | > 0 then
21:
f ← PickFreeVariable(S)
22:
S0 ← S ∧ [xf = 0]
23:
S1 ← S ∧ [xf = 1]
24:
Que ← Que ∪ {S0 , S1 }
25:
end if
26:
Fathom()
27:
end if
28:
end if
29: end while
30: return xopt

A. Upper bounds
Since at most one variable from each chain can be set to
one, the following is an upper bound on the value of the
objective function f :
z1 =

k
X
j=1

X

max ai +

di ∈Nj

wR

R=Rs , s∈OR(GC )

We can also calculate this bound for a subproblem S by
finding the set of free variables and their corresponding
chains and calculating z1 for those variables and chains and
at the end adding the value for fixed variables. This bound is
not tight because weights of some nodes might be counted
more than once. A better bound can be found using the graph
representation of the problem: since any node’s weight can
be accounted at most once, we can find the set of datasets
corresponding to variables fixed at one plus all free variables
and then find the nodes that are covered by those datasets:
FS = f reeS ∪ {i : (i, 1) ∈ f ixedS }
X
z2 (S) =
wnode (t)
t∈Covered(FS )

B. Algorithm

9. H EURISTICS

The algorithm is given at a high-level in algorithm 1. The
upper bound is calculated in line 11, if the upper bound is
smaller than the value of current best solution, there is no
point in searching that subproblem any further, so it will be
fathomed. The function F IND B ETTER F EASIBLE S OLUTION
acts as a greedy search method for subproblems that have
free variables and finds a good feasible solution for the
given subproblem. If the result is better than the current

We developed multiple heuristics for the bounded data
reuse problem. The heuristics avoid searching the whole
solution space by making local decisions on the value of
various datasets and then combining these local choices.
All the heuristics follow the same pattern but differ in the
choices they make.
In the first phase, the heuristics assign a set of datasets to
each node. These are datasets from the region of the node
1059

To select one set of datasets among a collection of such sets
Di (i = 1, 2, ..., n), the following selection criteria were
used in experiments:
1) M IN W EIGHT: Select the set of datasets that has the
minimum weight among all sets:
X
n
wdataset (d)
x = arg min

that together cover the node. Assigning sets of datasets in
this phase is done by reducing the graph with a succession of
series and parallel combinations until a graph with a single
edge is obtained. Assigning datasets to a node depends on
the combination being done (series or parallel) as well as on
criteria about the relative merits of different sets of datasets.
In the second phase the assignments made in the first
phase are refined. If a node is assigned a set D of datasets
and the node is covered by another node that is assigned a
set D0 of datasets, then a comparison between D and D0
is made (a comparison whose result differs dependent on
the heuristic used) and both nodes are assigned D0 or both
nodes keep their assignments.
In the final phase, the sets of datasets assigned to nodes are
sorted according to an order relation on the sets of datasets (a
relation that differs for different heuristics) and the datasets
are packed into the available space. In what follows we
present the three phases.

i=1

2) B EST R ATIO : Select the set of datasets that has the
maximum profit to weight ratio among all sets:
P
n
t∈Covered(Di ) wnode (t)
P
x = arg max
i=1
d∈Di wdataset (d)
3) M AX P ROFIT: Select the set of datasets that has the
maximum profit among all sets:
X
n
x = arg max
wnode (t)
i=1

A useful property of TTSP graphs is that any TTSP graph
can be reduced to the basic TTSP graph by a sequence of
the following operations:
1) Parallel combination: replace a pair of parallel edges
(edges with identical end-points) with a new edge that
connects their common end-points
2) Series combination (smoothing): for a pair of edges
of form e1 = (a, b) and e2 = (b, c) such that e1 and
e2 are the only edges incident to b, replace e1 and e2
with a new edge e = (a, c) and remove node b.

B. Refining the Assignments
To refine the assignments: For a node v1 that is covered
by another node v2 (v1 ∈ Covered(Din (v2 ))), if the set of
datasets assigned to v2 is superior according to the chosen
criteria, replace datasets of v1 with those of v2 . This process
is repeated until a fixed point is reached. We use the same
criteria that were employed in the previous phase.

In the initial assignment phase, the graph is successively
reduced by the application of these two operations until
one edge graph is obtained (the intermediate graphs are
multigraphs). We assign sets of datasets to edges of the
multigraph in each stage of the reduction process. The
idea is to find a covering (set of datasets that covers a
node) for every node of the graph that adheres to some
selection criteria. During the reduction process, each edge a
is assigned a set Chosen(a) of datasets:

•

•

t∈Covered(Di )

At the end of the reduction process we have a single edge
and a set of datasets assigned to it. The intermediate graphs
obtained during the reduction, as well as the assignments
to their edges, are kept because they are needed to assign
datasets to nodes.
To assign sets of datasets to (non-terminal) nodes of
GC , we distinguish between OR joints and other nodes.
For an OR joint v, find the edge alast originating from
v that appeared last in the reduction process and assign
Chosen(alast ) to v. For other nodes, assign the node’s
single outgoing dataset to the node.

A. Initial Assignment of Datasets

•

d∈Di

C. Sorting and Packing
The final part of the heuristic is to sort the nodes in some
order and fill the storage space in a greedy manner. We
experimented with the following sort orders for nodes:
1) Sort the nodes by the profit to weight ratio of their
assigned datasets
2) Sort the nodes by the profit of their assigned datasets
3) Sort the nodes by the number of nodes covered by
their assigned datasets
We experimented with two packing mechanisms: (a) go
over the sorted list of nodes and fill the storage space
with datasets assigned to the nodes until there is no more
space remaining and (b) instead of considering all nodes
at once to fill the storage space greedily, divide the nodes

For a chain N1 (an edge of multigraph GC ) that contains datasets d1 , d2 , ..., dm , we assign Chosen(N1 ) =
{dx }, choosing x by some criteria which will be
discussed next
For a parallel combination that combines arcs
a1 , a2 , ..., am into a single arc p1 , we assign
Sm
Chosen(p1 ) = i=1 Chosen(ai )
For a series combination that combines arcs
a1 , a2 , ..., am into a single arc s1 , we assign
Chosen(s1 ) = Chosen(ax ), choosing x by the
same criteria
1060

in multiple disjoint groups and assign each group a portion
of the storage space for greedy packing. The node groups
were formed in the following manner: In the last stage
of reduction process we have either a series or parallel
combination of some edges into a single final edge, these
edges naturally divide the set of all nodes into disjoint
groups. We assigned each group a portion of the storage
space proportional to each group’s maximum weight of the
coverings assigned to any of its nodes.
We used two simple search and replace algorithms on
the solution produced by each heuristic to further improve
the results. The first algorithm goes over all datasets in the
solution and tries to replace each dataset with another dataset
not present in the solution, if the objective value is improved,
keeps the change otherwise not. The second algorithm is
similar to the first one with the following difference: it
searches for sets of datasets in the solution among coverings
associated to nodes and tries to replace one covering with
another to improve the objective value.

•

•

•

Create a component (series-parallel graph) of size
base_size
Combine the new component with the graph (initially
empty) using parallel composition with probability 75%
and series composition with probability 25%
Repeat the above operations until the number of edges
falls into a desired range (80-200).

We generated graphs with base_size from 20 to 80
nodes. The components are created by sequence of series
and parallel compositions with basic TTSP with equal probability. The reason for the higher probability for parallel
composition of components is to generate harder instances
of the problem. In fact, we generated random instances of
workflows with different probabilities for series and parallel
combinations, and the test results shown in table II suggest
that random graphs generated with probability of series
composition between 20% to 30% at the component level
are generally harder to solve.
To determine the space limit, we used the following rule
of thumb: Calculate the minimum space M required to
cover all non-terminal nodes of the graph and use 50% of
M as the space limit. This choice of space limit was also
guided by experimental results. Our experiments with other
percentages suggest that the hardest instances are those with
near 50% space limits and that the error rate of the heuristics
do not depend on the space limit. table III shows the average
and maximum error rate for a subset of synthetic workflows
for different space percentages. In this table the top row
shows the percentage of M used as space limit and the
second row shows the average error of the heuristics over
a set of 13 synthetic workflows. The third row shows the
maximum error rate for every space limit. We could not run
this test for all 19 synthetic workflows due to prohibitive
amount of time required to obtain the optimal solution which
is needed to find the error rate.
We summarize the experimental results for synthetic and
random workflows in table I. The first row shows the
statistics for a set of 19 synthetic workflows. It gives average
and maximum error rate of the heuristics compared to the
optimal value generated by the Branch and Bound algorithm.
The last column shows the number of cases where the
heuristics found optimal value. The second row shows this
information for randomly generated graphs. We should note
that our integer programming formulation greatly helps in
pruning the search space. Without the formulation we could
not find optimal solutions for graphs as large as those listed
in table I.

D. Putting the Heuristics Together
Combining 3 dataset selection criteria, 3 sort orders and
2 packing strategies, we experimented with 18 different
heuristics, which for every instance of the problem we
picked the best result among these heuristics. None of the
18 heuristics were redundant and while their error rates
vary, they all contribute to the overall best results. Our
experiments over 19 instances of the problem with synthetic
workflows based on real-world workflows and two large
collections of randomly generated graphs show promising
results. To evaluate the quality of the solution found by the
heuristics, we compare it to the solution found by the optimal
Branch and Bound algorithm discussed earlier.
10. E XPERIMENTAL R ESULTS
We programmed all of our algorithms in the Java language
and ran all tests on a machine with two dual core Intel E8600
3.33 GHz CPUs and 8GB of memory running Ubuntu 12.04.
We conducted our experiments on synthetic workflows [2]
as well as randomly generated workflows. The synthetic
workflows closely resemble real world workflows used by
scientists in various disciplines [2]. The synthetic workflows
are produced by a workflow generator that uses information
gathered from actual executions of scientific workflows and
high-level expert knowledge about the structure of those
workflows. These workflows are meant to be used as a
benchmark for evaluating algorithms dealing with scientific
workflows. We chose a set of 19 workflows that had a seriesparallel structure from the publicly available repository of
synthetic workflows. The random series-parallel workflows
were generated by the following algorithm:

ACKNOWLEDGEMENT
This research was supported in part by NSF grant CSR0849980. We would like to thank Ewa Deelman and Gideon
1061

Type
Synthetic
Random

Instances
19
1610

Datasets per instance
Average
Min Max
74
21
241
106
80
192

Chains per instance
Average
Min Max
41
5
101
51
4
132

Error %
Average
Max
0.111%
1.708%
0.715% 26.514%

Number of instances
with zero error
12 (63%)
1202 (75%)

Table I: Experimental results for series-parallel graphs
Probability
10%
20%
30%
40%
50%
60%
70%
80%
90%
Overall

Instances
996
999
1000
1000
1000
1000
1000
1000
1000
8995

Datasets per instance
Average
Min Max
49
31
98
46
27
98
44
25
79
40
22
88
41
21
55
34
20
59
29
21
59
29
21
56
30
26
32
38
20
98

Chains per instance
Average
Min Max
27
14
57
25
10
53
23
9
43
19
6
43
18
1
27
13
1
26
9
1
26
7
1
21
4
1
12
16
1
57

Error %
Average
Max
0.447%
7.992%
0.731% 20.170%
0.594% 17.964%
0.566% 17.472%
0.665% 28.501%
0.402% 34.491%
0.229% 26.508%
0.055% 11.591%
0.049% 10.606%
0.416% 34.491%

Number of instances
with zero error
720 (72.29%)
693 (69.37%)
776 (77.60%)
806 (80.60%)
804 (80.40%)
889 (88.90%)
948 (94.80%)
984 (98.40%)
991 (99.10%)
7611 (84.61%)

Table II: Experimental results for series-parallel graphs with different probabilities for series combination. The graphs with lower
probability tend to have a more parallel structure compared to those with higher probabilities.
Space Percentage
Average Error
Max Error

10%
0.366
2.201

20%
0.521
2.803

30%
0.214
1.732

40%
0.612
3.835

50%
0.12
0.723

60%
0.079
0.653

70%
0.156
0.823

80%
0.139
0.738

90%
0.223
1.862

Table III: Error rates of the heuristics for 13 synthetic workflows with different percentages for space limit

Juve at USC Information Sciences Institute for providing us
with synthetic workflows that we used as a test set in our
experiments.

[8] P. J. Guo and D. Engler. Towards practical incremental
recomputation for scientists: an implementation for the python
language. In Proceedings of the 2nd conference on Theory
and practice of provenance, TAPP’10.

R EFERENCES

[9] B. Ludäscher, I. Altintas, C. Berkley, D. Higgins, E. Jaeger,
M. Jones, E. A. Lee, J. Tao, and Y. Zhao. Scientific workflow
management and the kepler system. In Concurr. Comput. :
Pract. Exper, 18(10):1039–1065, 2006.

[1] R. Barga, J. Jackson, N. Araujo, D. Guo, N. Gautam, and
Y. Simmhan. The trident scientific workflow workbench. In
Proceedings of the 4th IEEE International Conference on
eScience, ESCIENCE ’08.

[10] T. Oinn, M. Greenwood, M. Addis, M. Nedim Alpdemir,
J. Ferris, K. Glover, C. Goble, A. Goderis, D. Hull, D. Marvin,
P. Li, P. Lord, M. R. Pocock, M. Senger, R. Stevens, A. Wipat,
and C. Wroe. Taverna: lessons in creating a workflow
environment for the life sciences: Research articles. Concurr.
Comput. : Pract. Exper., 18(10):1067–1100, 2006.

[2] S. Bharathi, A. Chervenak, E. Deelman, G. Mehta, Mei-Hui
Su, and K. Vahi. Characterization of scientific workflows.
In Third Workshop on Workflows in Support of Large-Scale
Science, WORKS’08.
[3] S. B. Davidson, Y. Chen, P. Sun, and S. Cohen Boulakia. On
user views in scientific workflow systems. In Proceedings of
the First International Workshop on the role of Semantic Web
in Provenance Management, SWPM’09.

[11] A. Ramakrishnan, G. Singh, H. Zhao, E. Deelman, R. Sakellariou, K. Vahi, K. Blackburn, D. Meyers, and M. Samidi.
Scheduling data-intensive workflows onto storage-constrained
distributed resources. In Seventh IEEE International Symposium on Cluster Computing and the Grid, CCGRID’07.

[4] N. Dunford. Linear operators. Wiley, New York, 1988.
[5] D. Eppstein. Parallel recognition of series-parallel graphs.
Information and Computation, 98(1):41 – 55, 1992.

[12] D. Saha, A. Samanta, and S. R. Sarangi. Theoretical
framework for eliminating redundancy in workflows. IEEE
International Conference on Services Computing, SCC’09.

[6] M. R. Garey and D. S. Johnson. Computers and Intractability:
A Guide to the Theory of NP-Completeness. W. H. Freeman
& Co., New York, NY, USA, 1979.

[13] Amazon Web Services. Amazon simple workflow service,
http://aws.amazon.com/swf/, 2013.

[7] Y. Gil, E. Deelman, M. Ellisman, T. Fahringer, G. Fox,
D. Gannon, C. Goble, M. Livny, L. Moreau, and J. Myers.
Examining the challenges of scientific workflows. IEEE
Computer, 40(12):24–32, 2007.

[14] D. Yuan, Y. Yang, X. Liu, G. Zhang, and J. Chen. A
data dependency based strategy for intermediate data storage
in scientific cloud workflow systems. Concurrency and
Computation: Practice and Experience, 24(9):956–976, 2012.

1062

Distrib. Comput. (2001) 14: 41–48

c Springer-Verlag 2001


Access cost for asynchronous Byzantine quorum systems
Rida A. Bazzi
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287–5406, USA (e-mail: bazzi@asu.edu)
Received: September 1999 / Accepted: September 2000

Summary. Quorum systems have been used to implement
many coordination problems in distributed systems. In this paper, we study the cost of accessing quorums in asynchronous
systems. We formally deﬁne the asynchronous access cost of
quorum systems and argue that the asynchronous access cost
and not the size of a quorum is the right measure of message complexity of protocols using quorums in asynchronous
systems. We show that previous quorum systems proposed in
the literature have a very high asynchronous access cost. We
propose a reformulation of the deﬁnition of Byzantine quorum systems that captures the requirement for non-blocking
access to quorums in asynchronous systems. We present new
Byzantine quorum systems with low asynchronous access cost
whose other performance parameters match those of the best
Byzantine quorum systems proposed in the literature. In particular, we present a construction for the disjoint failure pattern
that outperforms previously proposed systems for that pattern.
Key words: Quorum – Fault tolerance – Byzantine failures –
Distributed systems – Asynchronous – Access cost

1 Introduction
A quorum system is a collection of sets (quorums) that mutually intersect. Quorum systems have been used to implement
mutual exclusion [1,9], replicated data systems [8], commit
protocols [17], and distributed consensus [13]. Work on quorum systems traditionally considered crash failures [1,2,5–9,
16,15]. Malkhi and Reiter [10] proposed the interesting notion of Byzantine quorums–quorum systems that can tolerate
Byzantine failures. They presented protocols to implement a
distributed shared register variable using Byzantine quorums.
Their implementation requires a client accessing a quorum
to wait for responses from every server in a quorum set, but
This material is based upon work supported by the National Science
Foundation under Grant No. CCR-9876052. An earlier version of this
paper appears in the Proceedings of the Thirteenth International Conference Conference on Distributed Computing, Bratislava, Slovakia,
1999.

they did not study the problem of ﬁnding a quorum set whose
elements are available – an available quorum.
In this paper, we study the cost of ﬁnding an available quorum in the presence of Byzantine failures. We consider both the
direct access model in which processes access a quorum in one
round of communication, and the incremental access model
in which processes can access a quorum in multiple rounds of
communication. We formally deﬁne the asynchronous access
cost of quorum systems, and we argue that the asynchronous
access cost and not the size of a quorum is the right measure
of message complexity of protocols using quorums in asynchronous systems. We also show that previous quorum systems proposed in the literature have a very high asynchronous
access cost.
We propose a reformulation of the deﬁnition of Byzantine quorum systems that captures the requirement for nonblocking access to quorums in asynchronous systems. We introduce non-blocking Byzantine quorum systems and show that
they can be achieved at a low cost and we present non-blocking
Byzantine quorum constructions for two failure models. The
constructions we present are the ﬁrst that do not require blocking and have a low cost of access. Also, the construction we
present for the disjoint failure model yields a Byzantine quorum system that has better performance parameters than previously proposed systems. Our constructions rely on a new
access model we call partial access. With partial access, a
processor need not wait for a reply from each process in a quorum set; the quorum system should be designed to ensure that
any two partial accesses have a large enough intersection to
ensure consistency. It turns out that the set of partial accesses
of a non-blocking Byzantine quorum system is a Byzantine
quorum system as deﬁned in [10].
The rest of the paper is organized as follows. Section 2 discusses related work and Sect. 3 summarizes our contributions.
Section 4 presents the deﬁnitions and introduces the notion
of asynchronous access cost. Section 5 gives examples of the
asynchronous access cost for two Byzantine quorum systems.
Section 6 reformulates the deﬁnition of Byzantine quorums
to capture the asynchronous access cost as a design objective. Section 7 presents non-blocking quorum systems with
low asynchronous access cost and whose other performance

42

R.A. Bazzi

parameters match those of the best Byzantine quorum systems
proposed in the literature. Section 8 concludes the paper.

message delivery time or on processors’ speeds and that there
are no failure detectors in the system.

2 Related work

4.2 Failure model

The problem of ﬁnding an available quorum has been addressed by researchers for the case of detectable crash failures [2,12,15]. In [15], the probe complexity of a quorum system is deﬁned. The probe complexity is the minimum number of processors that need to be contacted to establish the
existence or non-existence of an available quorum in the system. In the deﬁnition of probe complexity, processors can be
probed incrementally and the identity of the processor to be
probed next can depend on the responses received from previous probes. In [12], Neilsen proposes a dynamic probe strategy
that improves on the results of [15]. In [2], the author formally
deﬁned the concept of cost of failures, which can be thought
of as the probe complexity per failure.
Both [2] and [15] assume that failures can be detected.
Their incremental access methods are not directly applicable
to asynchronous systems in which failures cannot be detected.
The problem of ﬁnding an available quorum in the presence of Byzantine failures has not been studied by other researchers. Due to the nature of Byzantine failures and system
asynchrony, the deﬁnition of Byzantine quorum systems proposed in [10] requires that an available quorum exists in the
system. Unfortunately, that requirement does not say anything
about the cost of ﬁnding an available quorum. The availability
requirement of Byzantine quorum systems was relaxed in [3]
for systems in which timeouts can be used to detect failures. In
such systems, the author shows that any quorum set Q can be
accessed without a need to access servers that do not belong
to Q.

Server processors can fail.1 The assumptions about failures
affect the way a quorum can be used. In this paper, we consider systems in which processors are subject to Byzantine
failures; i.e., they can deviate arbitrarily from their protocols.
In such systems it is usually assumed that there are bounds
on the number of failures that can occur in the system. Such
bounds have traditionally been expressed with a number t that
is an upper bound on the number of failures that can occur in
the system. This model was later generalized in [10] to allow
more ﬂexibility in describing the failure patterns that the system can exhibit. We adopt the model of [10] in this paper.2
The set f aulty denotes the set of faulty processors in the system. A failure pattern F identiﬁes the possible sets of faulty
processors in the system. We write F = {F1 , F2 , . . . , Fm }.
There exists an element F of F such that at any given instant,
the faulty processors belong to F . The processors do not necessarily know F . A common example of a failure pattern is the
f -threshold pattern in which F = {F ∈ P : |F | = f }. Another interesting failure pattern is the disjoint pattern in which
all elements of F are disjoint [10].

3 Contributions
To our knowledge, this is the ﬁrst work that studies the cost
of accessing Byzantine quorum systems in asynchronous systems. We consider both the direct access model and the incremental access model. We introduce non-blocking Byzantine
quorum systems and provide necessary and sufﬁcient conditions for their existence. Unlike Byzantine quorum systems,
non-blocking Byzantine quorum systems capture the asynchronous access cost. In that respect, they are similar to synchronous Byzantine quorums [3].
We propose optimal non-blocking quorum systems and
show that they are not equivalent to previously proposed
Byzantine quorum systems. For the disjoint failure pattern,
we propose a non-blocking quorum system that yields the best
known Byzantine quorum system for that failure model.
4 Deﬁnitions and system model
4.1 System model
We assume that the system consists of a set P of n server
processors and a number of client processors that are distinct
from the servers. All processors can communicate using reliable message passing. We assume that there are no bounds on

4.3 Quorum and set systems
Deﬁnition 1 A set system S over P is a subset of 2P .
In what follows, we assume that all set systems are over P. A
quorum system is a particular type of set systems.
Deﬁnition 2 A quorum system Q over P is a set of subsets
(called quorums) of P such that any two quorums have a nonempty intersection.
The intersection property of quorums is essential for their use
in coordination problems.
Processors access a quorum to coordinate their actions.
Typically, to access a quorum, a client sends a request to every
server, in a quorum set. Upon receiving a request, a correct
server updates its state and sends a reply. The client waits
until it receives a reply from every server in the quorum. If
the client receives replies from all processors in a quorum,
then the access is considered successful. If one of the servers
failed, then the client attempts to access another quorum that
does not have any faulty processor (the question of ﬁnding a
quorum with no faulty processors has been addressed in [2,
15] for systems with detectable failures). Since processors access a quorum only if all its members are correct, two clients
are always guaranteed to receive a response from a common
correct server that belongs to a non-empty intersection of two
quorums. The correctness of quorum-based protocols rely on
this intersection property. A quorum is said to be available if
all its elements are correct processors [14].
1

We do not consider client failures in this paper.
In a long-lived system, bounds on failure will be exceeded at
some point. We do not address the problem of recovering failed
servers in this paper.
2

Access cost for asynchronous Byzantine quorum systems

43

4.4 Byzantine quorums in asynchronous systems

4.5.2 Access models

If failures are arbitrary, a processor might receive conﬂicting
replies from faulty and correct processors. It follows that a processor must base its coordination decisions on replies that it
knows to be from correct processors. Motivated by this requirement, Malkhi and Reiter gave the following deﬁnition [10]:

In this paper, we concentrate on the direct access model in
which processes access a quorum by sending all requests at
once and then wait for replies. The direct access model is not
the most general model of accessing quorums. For instance,
a process might incrementally access a quorum system by
sending requests to some processes, then send further requests
based on the replies it receives. We also consider incremental
access strategies and show that an incremental strategy would
require more than one round of message exchange which can
be prohibitively high in a fully asynchronous system.3 The
deﬁnition of direct access strategy that we present assumes
that all clients use the same strategy. The deﬁnition can be
modiﬁed to allow different clients to have different strategies.
In what follows, we will talk about accessing a set system
instead of accessing a quorum system. We start by deﬁning
access sets.

Deﬁnition 3 A quorum system tolerates failure pattern F if
1. ∀ Q1 , Q2 ∈ Q ∀ F1 , F2 ∈ F : (Q1 ∩ Q2 ) − F1 ⊆ F2 .
2. ∀F ∈ F ∃ Q ∈ Q : F ∩ Q = ∅
The ﬁrst condition requires that the intersection of two quorums is not contained in the union of two sets in F. This
guarantees that the reply of some correct processor can be
identiﬁed .
The second condition is the availability condition (also
called resiliency requirement in [11]). It requires that some
quorum consists of correct processors. The availability condition is needed in asynchronous systems because there is no
way to differentiate a slow processor from a faulty one. To access a quorum in an asynchronous system, a processor cannot
simply send requests to all processors in a quorum set and wait
for replies. In the worst case, even in a failure-free execution,
a processor might have to send requests to every processor in
the system and then wait for replies from a quorum that consists of correct processors (we give an example below). The
availability condition is needed to ensure that some quorum is
available in the system.
We generalize the deﬁnition of [10] and deﬁne what it
means for a set system to be resilient to a failure pattern F.
Deﬁnition 4 A set system S is resilient to failure pattern F if
∀ F ∈ F ∃ S ∈ S : S ∩ F = ∅.
The work of Malkhi and Reiter [10] and their subsequent
work [11] does not address the problem of ensuring that a
response is received other than by requiring that the system is
resilient to the failure pattern. Addressing this problem is an
important contribution of this paper.
4.5 Cost of access
In this section we introduce the asynchronous access cost of a
quorum system. We ﬁrst discuss the effects of asynchrony on
accessing a quorum system, then we deﬁne our access model
and introduce the cost of access parameter for evaluating quorum systems.
4.5.1 Asynchrony
In this paper, we consider the problem of accessing quorums
in systems that are fully asynchronous and in which there are
no bounds on message delivery delays or the speed of processors. In such systems, one way to guarantee replies from some
quorum is to send requests to every processor in the system
and then wait for replies from some quorum. Obviously, sending requests to every processor is too costly and eliminates
the beneﬁts of using quorum systems. Our aim is to improve
on the worst-case scenario in which every server needs to be
contacted to guarantee a response from some quorum set.

Deﬁnition 5 Let S be an element of set system S that is resilient to F. A set A is an access set of S with respect to F if
S ⊆ A and
∀ F ∈ F ∃ S ∈ S : S ⊆ A − F
Note that a set S might have more than one access set for a
given set system S. Also, an access set A might be the access
set of more than one set S. In fact, we could have deﬁned an
access set independently of the set S and have it depend only
on S. We chose to deﬁne access sets as a function of S to
emphasize the fact that it is the set S that is the target of the
access. While a particular request to an access set of S will
not ensure a reply from all elements of S, such a reply can be
expected under favorable delay conditions (obviously, this is
not guaranteed in asynchronous systems). Also, a request to
access set of S can ensure replies from some elements of S,
which might be desirable, under certain conditions4 .
Deﬁnition 6 A direct access strategy for set system S is a
mapping Ad : S → 2P that assigns for each element S of S
an access set of S.
By sending requests to Ad (S), a client is guaranteed to
receive replies from some quorum set. The quorum set from
which replies are received is not necessarily S, but it can be
S.
We present incremental access strategies informally because our results do not require a formal deﬁnition. In an
incremental access strategy, a process need not contact all
servers of an access set at once. It can contact some servers
and then depending on the replies, it decides what servers to
contact next. By avoiding a commitment to one access set at
the outset, it is conceivable that with an incremental access
strategy a smaller number of servers need to be contacted to
force a reply from a quorum. This will be at the cost of extra
message exchanges.
3

A more detailed study of the cost of access of the incremental
strategy is a subject for future research.
4
A more detailed discussion of the advantages of of making the
deﬁnition dependent on S is beyond the scope of this paper.

44

4.5.3 Cost of access
Deﬁnition 7 Let S be an element of set system S that is resilient to F. The asynchronous access cost of S is the size of
the smallest access set of S with respect to F.

R.A. Bazzi

Deﬁnition 12 Let w be a strategy for a quorum system Q =
{Q1 , . . . , Qm }. For any q ∈ P, the load induced by w on q is
lw (q) = Σq∈Qj wj . The load induced by w on Q is
Lw (Q) = max lw (q)
q∈P

Note that by deﬁnition, a Byzantine quorum that tolerates
failure pattern F is also resilient to F. It follows that the asynchronous access cost is well deﬁned for all quorum sets of a
Byzantine quorum system.

The system load on Q is

Deﬁnition 8 The asynchronous access cost of a set system S
that is resilient to failure pattern F is

The deﬁnition of load implicitly assumes that no extra
servers need to be contacted when a particular quorum is accessed. This is not the case for Byzantine failures in asynchronous systems because extra servers need to be accessed
even in failure-free runs to guarantee a response (assuming
clients do not know that the run is failure-free). From the
discussion about the cost of access, it follows that the load
deﬁnition should take the cost of access into consideration.
The deﬁnition of load can simply be changed by replacing a
quorum with the access set of the quorum, while allowing for
different access sets for the same quorum at different times. If
the only access set of any quorum is the set P of all servers,
it follows that the load is 1, regardless of the quorum size.

cost(S) = min{|A| : ∀ F ∈ F∃ S ∈ S : S ⊆ A − F }
In the direct access model, the asynchronous access cost gives
the minimum number of servers that need to be contacted to
ensure that a response is received from some set in a set system.
As the following theorem shows, in a fully asynchronous
system, a client needs to send requests to cost(S) servers each
time it needs to successfully access a set system.
Theorem 9 Let S be a set system that is resilient to F. In the
direct access model, a client needs to send requests to cost(S)
servers to guarantee a response from each server in some set
in S.
Proof. In the direct access model, a client c sends all requests
at the beginning to some set of servers A. If |A| < cost(S),
then, by deﬁnition of the asynchronous access cost, there is a
faulty set F ∈ F such that A − F contains no element of S.
If processes in F fail, c will not receive a response from every
server in any element of S.


Corollary 10 In the incremental access model, if less than
cost(S) are contacted in the ﬁrst round of communication,
then at least two rounds of message are needed to guarantee
a successful access.
Proof. By Theorem 9, if less than cost(S) servers are contacted in the ﬁrst round of an incremental access strategy, then
no successful access is guaranteed. It follows that at least two
rounds are needed for a successful access.


4.6 Strategies and load
This section presents the formal deﬁnitions of strategy and
load as in [14]. It discusses the implications of the asynchronous access cost on the load of a quorum system.
A protocol using a quorum system chooses a quorum to
access according to some rules. A strategy is a probabilistic
rule to choose a quorum. Formally, a strategy is deﬁned as
follows.
Deﬁnition 11 Let Q = {Q1 , . . . , Qm } be a quorum system.
A strategy w ∈ [0, 1]m for Q is a probability distribution over
Q.
For every processor q ∈ P, a strategy w induces a probability that q is chosen to be accessed. This probability is called
the load on q. The system load is the load of the busiest element
induced by the best possible strategy.

L(Q) = min{Lw (Q)},
w

where the minimum is taken over all strategies.

5 Asynchronous access cost examples
In this section we give examples of the asynchronous access
cost for two Byzantine quorum systems. The ﬁrst system, the
Paths system, has optimal quorum size and load (as traditionally deﬁned) combination and high availability in the presence
of crash failures. We show that it has a large asynchronous access cost. The second system, the threshold system, has small
asynchronous access cost relative to the size of its quorums.
5.1 Paths system
The Paths system [11] is deﬁned for the f -threshold failure
pattern. It is deﬁned as follows. Let n = d2 , be the number
of servers arranged in √
a square grid of the triangular lattice.
A quorum√
consists of 2f + 1 non-intersecting top-bottom
paths and 2f + 1 non-intersecting left-right paths. In [11],
it is shown that any two quorums intersect in 2f + 1 distinct
vertices
and that the Paths system can tolerate no more than
√
n failures.
The Paths systems has small quorum size, small load (not
taking access cost into consideration) and high availability
in the presence of crash failures. Unfortunately, the asynchronous access cost of the Paths system is high as we show
below.
√
Lemma 13 cost(P ath) = Ω((f + f + 1)d).
Proof. Let L and R be the sets consisting of the vertices
of the left and right √
edges of the square grid. If a set A is
of size |A|√< (f + f + 1)d, then A cannot contain more
than (f + f + 1) − 1 disjoint paths that connect L and R
because each left-right path is of size at least d. By Menger’s
theorem [4], it √
follows that there is a set C of vertices of size
less than (f + f + 1) that separates L and R. Removing f

Access cost for asynchronous Byzantine quorum systems

vertices
from C yields a set set A with cut set of size at most
√

√f + 1 − 1. Again, by Menger’s theorem, A cannot have
f + 1 disjoint paths.


In particular, the access cost of the Paths system is very
high if f = Ω(d).
Corollary 14 If f = Ω(d), then cost(P ath) = Ω(n).
5.2 Threshold system
The threshold system is deﬁned for the f -threshold failure
pattern. A quorum of the threshold system consists of any
set of size f +  n+1
2 . Any two quorums are guaranteed to
intersect in at least 2f + 1 elements. For the threshold system,
the cost of the system is not much different from the quorum
size, but the quorum size is large.
Lemma 15 The asynchronous access cost of the threshold
system is O(2f +  n+1
2 ).
Proof. In fact, a set A of size 2f + n+1
2  vertices is guaranteed
to contain a quorum if any f elements are removed from A.
Also, a set A of size less than 2f +  n+1
2  vertices is not
guaranteed to contain a quorum if f elements are removed
from A.


6 Non-blocking quorum systems
The goal of deﬁning non-blocking quorum systems is to emphasize the importance of the asynchronous access cost as a
design parameter, and to provide a more uniform deﬁnition of
Byzantine quorum systems. In the examples above, there is
no clear relationship between the cost of access and the size
of the quorum. Our aim is to reformulate the deﬁnition of the
quorum system so that the cost of access is found as part of
the design of the quorum system and not after the quorum is
already designed. So, instead of designing the quorum sets,
we directly design the access sets.
We deﬁne a non-blocking Byzantine quorum system as
follows.
Deﬁnition 16 A set system Q is a non-blocking masking quorum system that tolerates failure pattern F if and only if:
∀ Q1 , Q2 ∈ Q ∀ F1 , F2 , F3 , F4 ∈ F :
((Q1 − F1 ) ∩ (Q2 − F2 ) − F3 ) ⊆ F4
We deﬁne partial access sets of a non-blocking Byzantine
quorum system as follows.
Deﬁnition 17 The partial access sets of a non-blocking Byzantine quorum system Q that tolerates failure pattern F are the
sets of the form Q − F , where Q ∈ Q and F ∈ F.
Note that the deﬁnition of non-blocking masking quorum
systems is similar to the ﬁrst condition of Deﬁnition 3. In
fact, given a non-blocking quorum system Q, the set of partial access sets of Q form a Byzantine Quorum system as
deﬁned by Malkhi and Reiter in [10]. Nevertheless, ﬁnding
a non-blocking quorum system corresponding to a particular

45

Byzantine quorum system is not always straightforward. Furthermore, not every Byzantine quorum system is equal to the
set of access sets of a non-blocking Byzantine quorum system.
Our deﬁnition non-blocking Byzantine quorum systems
requires that the client be able to determine a correct response
from any partial access set; to have successful partial accesses,
it is enough to guarantee that the quorum system can handle
the worst-case failure scenario. To access a quorum Q, a client
sends requests to all servers in Q, and then waits for a response
from all servers in a partial access set of Q. Such a response is
guaranteed by the deﬁnition of non-blocking Byzantine quorum systems. Once a response from a partial access set is
received, the client can proceed as in [10].
The following theorem gives a sufﬁcient condition for a
collection of sets to be a non-blocking Byzantine quorum system.
Theorem 18 A set Q is a non-blocking quorum system that
tolerates failure pattern F if:
1. ∀ Q1 , Q2 ∈ Q ∀ F1 , F2 ∈ F : (Q1 ∩ Q2 ) − F1 ⊆ F2 ,
and
2. ∀ Q1 ∈ Q ∀ F1 , F2 ∈ F∃ Q2 ∈ Q : Q2 ⊆ (Q1 −
F1 ) ∪ F 2 .
Proof. The proof is by contradiction. Let Q1 and Q2 be two
quorum sets such that ∃ F1 , F2 , F3 , F4 ∈ F : ((Q1 − F1 ) ∩
(Q2 −F2 )−F3 ⊆ F4 . It follows that (Q1 −F1 )∩(Q2 −F2 ) ⊆
F3 ∪ F4 and ((Q1 − F1 ) ∪ F3 ) ∩ ((Q2 − F2 ) ∪ F4 ) ⊆ F3 ∪ F4 .
By Condition 2 of the theorem, it follows that there exists two
quorum sets Q and Q such that Q ∩ Q ⊆ F3 ∪ F4 . This
contradicts Condition 1 of the theorem.


As we saw, to each non-blocking Byzantine quorum system corresponds a Byzantine quorum system as deﬁned in [10].
The importance of the reformulation lies in the fact that it ties
the quorum size to the asynchronous access cost. When designing a non-blocking quorum system, one would have to
design a quorum system with small quorums sets (all other
parameters being equal), which means that the resulting asynchronous access cost is small. This is due to the fact that the
quorums of a non-blocking quorum system are access sets of
the underlying Byzantine quorum system. On the other hand,
when designing Byzantine quorum systems as deﬁned in the
formulation of [10], the asynchronous cost of access is not
directly related to the quorum size even if the quorum systems
have good performance parameters. One might argue that it is
always possible to construct Byzantine quorum systems with
low access cost and without the need for a reformulation of
the deﬁnition. Intuitively, we believe that this is not true unless
the access cost is an explicit design parameter, in which case
one has to use a deﬁnition similar to ours. Also, we believe
that our deﬁnition provides a natural expression of the access
cost as a design parameter. Finally, previous Byzantine quorum constructions in the literature did not take the access cost
into account as we saw in Sect. 5.
6.1 Existence of non-blocking quorum systems
Given a failure pattern, we are interested in deciding whether
there exists a quorum system that tolerates the failure pattern.
The following two propositions give necessary and sufﬁcient
conditions.

46

R.A. Bazzi

Proposition 19 There exists a non-blocking quorum system
that tolerates failure pattern F if and only if Q = {P} tolerates F.
Proof. If Q = {P} tolerates F, then there exists a quorum
system that tolerates F. If there exists a quorum system Q
that tolerates F, then there exists a quorum in Q that cannot
be contained in the union of less than ﬁve elements of F.
It follows that P is not equal to the union of less than ﬁve
elements in F and that {P} tolerates F.


Proposition 20 There exists a non-blocking quorum system
that tolerates failure pattern F if and only if
∀ A, B, C, D ∈ F : P = A ∪ B ∪ C ∪ D.
Proof. Direct application of Proposition 19.




For the case of the f -threshold failure pattern, we get the
following corollary.
Corollary 21 There exists a quorum system that tolerates the
f -threshold failure pattern if and only if n ≥ 4f + 1.
It is interesting to note that the necessary and sufﬁcient
condition for the existence of a non-blocking Byzantine quorum system is also a necessary and sufﬁcient condition for the
existence of a quorum system [10]. This is expected, given the
extremal nature of the system used in the proofs. In fact, the access sets of Q = {P} are the Byzantine quorum system used
in [10] to prove the necessary condition for Byzantine quorum
systems. While the necessary conditions are identical to those
for Byzantine quorum systems, this does not necessarily imply
a correspondence between Byzantine quorum system and nonblocking Byzantine quorum systems. In fact, we will show at
the end of Sect. 7.1 that some Byzantine quorum systems are
not equal to the set of partial access sets of any non-blocking
Byantine quorum system. So, it was conceivable that a Byzantine quorum system can be constructed in a system in which
no non-blocking system could be constructed.

7 Non-blocking quorum systems constructions
Depending on the failure patterns, constructing a non-trivial
non-blocking Byzantine quorum systems can be harder than
constructing a Byzantine quorum system (the set Q = {P} is
a trivial system if a non-blocking quorum system exist). In this
section we present two constructions of non-blocking Byzantine quorum systems, one for the threshold failure pattern and
one for the disjoint failure pattern.

7.1 Threshold failure pattern
Consider a system of n = d2 processors arranged in a d × d
square grid, d > 4 in the presence of the f -threshold failure
pattern, f < (n − 1)/4. Two vertices (x1 , y1 ) and (x2 , y2 )
of the grid are connected if: x1 = x2 ∧ y1 = y2 + 1,
x1 = x2 ∧ y2 = y1 + 1, x1 = x2 + 1 ∧ y1 = y2 ,
x2 = x1 + 1 ∧ y1 = y2 , x1 = x2 + 1 ∧ y1 = y2 + 1, or
x2 = x1 + 1 ∧ y2 = y1 + 1.

Similar to the Paths system [11], we√
deﬁne a non-blocking
quorum system Qf√
n that consists of 2 f + 1 disjoint leftright paths and 2 f + 1 disjoint top-bottom paths. Using
the same arguments as in [11], it is easy to show that any
two quorums are guaranteed to intersect in 4f + 1 elements.
It follows that the quorum system is a non-blocking quorum
system.
The quorum system Qf n has better fault tolerance than the
Paths system. In fact, Qf n can tolerate (n − 1)/4 failures
(Corollary
21), whereas the Paths system can tolerate no more
√
than n failures in the worst case. The reason is that the Paths
system requires each row to have a number of available vertices
for the system to be available. In contrast, a partial access of
Qf n is successful if all but f members of a√quorum respond.
This can be achieved even if less than 2 f + 1 nodes are
available in a √
given row, whereas the Paths system will be
unavailable if 2f + √
1 or less nodes are available in a given
row. Furthermore, if 2 f + 1 ≤ f (i.e. f ≥ 6), then the nonblocking quorum system will tolerate the failure of a whole
row which is not possible for the Paths system. If f ≥ 7,
then Qf n can tolerate the failure of a whole row and a whole
column.
of accessing Qf n is
√More importantly, the cost √
4 f + 1d, compared to Ω((f + f + 1)d) for the Paths
system.


The load of Qf n is ≈ 4 f +1
n . The proof is identical to
that given for the load of the Paths systems given in [11]. The
load of
Qf n is larger than the load of the Paths which is equal

to ≈ 2 2fn+1 . The load given for Qf n holds even when taking
the cost of access into account; the load for the Paths is much
higher than the load of Qf n when taking the cost of access
into account.
In this paper we assume that the system is subject to only
Byzantine failures. For such systems, failures are constrained
by a failure pattern and we do not calculate the failure probability in this model.
The Byzantine quorum system Qf = {Q − F : Q ∈
Qf n and F ∈ F} induced by Qf n is not directly related to
the Paths system. In fact, many quorums of Qf do not contain
a quorum of the Paths system and vice versa. Also, Qf has
better fault tolerance than the Paths system and the quorums of
the Paths system are smaller than those of Qf n . This example
shows the advantage of using the deﬁnition of non-blocking
quorum systems.
The difference between the two quorum systems is illustrated in Fig. 1a. In the ﬁgure, accessing the non-blocking quorum will be successful even if all but one node in a given row
fail. In contrast, the Paths system will be unavailable if less
than three nodes are available in a given row.
Finally, we note that the Paths system is not equal to the set
of partial access sets of any non-blocking Byzantine quorum
system. In fact, let Qnb be a non-blocking quorum system and
let Q be a quorum of the Paths system which is a partial access
set for a quorum in Qnb . It follows that for some Q ∈ Qnb
Q − F = Q, where |F | = t and F is disjoint from Q. The set
Q , is of size at most |Q| + t. By argument similar to that in
the proof of Lemma 13, it is easy to show that for appropriate
n and t we can ﬁnd a set F  , |F  | = t, such that Q − F 
contains no quorum of the Paths system.

Access cost for asynchronous Byzantine quorum systems

47

Many such systems exist. One such system is the triangle lattice system

 [2]. In the triangle lattice system over Si , we can
choose 2|Si | quorums such that each processor belongs to
exactly two quorums. If we choose each quorum with a probability √ 1 , it follows that the load of the triangle lattice
2|Si |

system on Si is at most √ 2 = |S2i | . Deﬁne a quorum on
2|Si |

Fig. 1a. Access set of a non-blocking Byzantine quorum for n = 144
and f = 3 with one quorum in black (the gray squares belong to
access set but not to the quorum). Note that the maximum number of
independent vertical paths in the quorum set is one; b A quorum of the
Paths system. Note that it contains no quorum set of the non-blocking
quorum system

7.2 Disjoint failure pattern
In this section we provide an efﬁcient construction of a nonblocking quorum system Qdn for a failure pattern F whose elements are disjoint. The construction is similar to the construction given in [3]. We present the construction in some detail to
show that our deﬁnition of non-blocking Byzantine quorum
systems yields quorum systems that are not easily designed for
the original deﬁnition. In fact, the Byzantine quorum system
Qd deﬁned by the partial access sets of Qdn outperforms the
ones proposed in [10] for the disjoint failure pattern. Also, we
do not know how to express Qd other than as the set of partial
access sets of Qdn . We do not provide proofs for most of our
claims because they are almost identical to those of [3].
Let F = {F1 , F2 , . . . , Fm } be the set of failure sets ordered in decreasing size. We assume without loss of generality
4
that m > 4. Let α = n − (Σi=1
|Fi |).
Our construction will proceed as follows. First we show
α
that there are ﬁve disjoint sets of size greater than 10
such
that no two of them will have an non-empty intersection with
the same faulty set. Then, on each of the ﬁve sets Si , i =
0, . . . , 4, we construct a traditional quorum system whose load
is O( √1 ). On P, we construct a quorum system whose
|Si |

elements consist of the union of ﬁve quorums, one from each
of the ﬁve sets.
Let m0 = 4 and deﬁne mi , i = 1, . . . , 4 as follows:




α
mi = min j : |Fi ∪
Fk | ≥

10 
mi−1 <k≤j+1

Note that j and k are bound variables in the deﬁnition of mi .
Also, mi , 1 ≤ i ≤ 4, are always guaranteed to exist.
Now, deﬁne the ﬁve sets Si , 0 ≤ i ≤ 4, as follows:
	
• Si = F i ∪
mi <k≤mi+1 Fk , if i < 4, and
	
• S4 = m4 <k Fk

P to be the union of ﬁve quorum sets one from each of the
triangle lattices deﬁned on Si , 0 ≤ i ≤ 4.
Proposition 24 The resulting system Qdn is a non-blocking
quorum system that tolerates F.
Proof. In fact, any two quorums intersect in ﬁve servers, no
two of which belong to the same faulty set. Therefore the
intersection of two quorums does not belong to the union of
four faulty sets.


Let Qd be the Byzantine quorum system deﬁned by the
access sets of Qdn . Qd = {Q − F : Q ∈ Qdn and F ∈ F}.
It follows that Qd tolerates the disjoint failure pattern F.
Proposition 25 The load of the quorum system Qd is O( √1α ).
Lemma 26 The load of Qd is optimal.
Proof. The proof uses the same techniques as those used in [3]
and is omitted.


The load of Qd stated above is the traditional load. Nevertheless, the load of Qdn is of the same order as the load of
Qd and is therefore optimal. Finally, we know of no simpler
way to express Qd or to obtain a Byzantine quorum system
that tolerate F and has a better load.
8 Conclusion
An important contribution of this paper is the recognition that
in asynchronous systems, it is not enough to design a quorum
systems with small quorum sets, but it is more important to
design a quorum set that is amenable to efﬁcient access. We
proposed a new deﬁnition of Byzantine quorum systems that
lend themselves to non-blocking access. We have shown that
designing non-blocking Byzantine quorums with small access
cost is possible in asynchronous systems.
One might think that the only difference between Byzantine quorum systems and non-blocking Byzantine quorum systems is that of how access is achieved, and that it is possible to
come up with a new deﬁnition of access for Byzantine quorum
systems to achieve non-blocking access. While this is possible,
the resulting non-blocking quorum system is not guaranteed to
have a small access cost. To design Byzantine quorum system
with a small access cost, we believe that a formulation similar
to ours is needed.
Acknowledgements. I thank the reviewers for many useful and detailed suggestions.

Proposition 22 Si ∩ Sj = ∅, 0 ≤ i = j ≤ 4.
Proposition 23 Si ≥

α
10

References

for 0 ≤ i ≤ 4.

Now, we describe the non-blocking quorum system Qdn .
On Si , 0 ≤ i ≤ 4, deﬁne a quorum system with load O( √1 ).
|Si |

1. D. Agrawal, A. El–Abbadi: An efﬁcient and fault-tolerant solution for distributed mutual exclusion. ACM Trans Comput Syst
9(1): 1–20 (1991)

48
2. R.A. Bazzi: Planar Quorums. Theor Comput Sci 243: 243–268
(2000)
3. R.A. Bazzi: Synchronous Byzantine quorum systems. Distrib
Comput 13(1): 45–52 (2000)
4. B. Bollobás: Graph Theory, An Introductory Course. Graduate
Texts in Mathematics, Berlin Heidelberg New York: Springer
1979
5. H. Garcia-Molina, D. Barbara: How to assign votes in a distributed system. Journal of the ACM 32(4): 481–860 (1985)
6. D.K. Gifford: Weighted Voting for Replicated Data. Proceeding
of 7th ACM Symposium on Operating Systems Principles, pp
150–162, December 1979
7. A. Kumar: Hierarchical quorum consensus: A new algorithm for
managing replicated data. IEEE Trans Comput 40(9): 996–1004
(1991)
8. A. Kumar, M. Rabinovich, R. Sinha: A performance study of
general grid structures for replicated data. In: Proceedings of
International Conference on Distributed Computing Systems,
pp 178–185, May,√1993
9. M. Maekawa: A n algorithm for mutual exclusion in decentralized systems. ACM Trans Comput Syst 3(2): 145–159 (1985)
10. D. Malkhi, M. Reiter: Byzantine Quorum Systems. Distrib Comput 11(4): 203-213 (1998)
11. D. Malkhi, M. Reiter, A. Wool: The load and availability of
Byzantine quorum systems. In: Proceedings of the Sixteenth
ACM Symposium on Principles of Distrib Comput,August, 1997

R.A. Bazzi
12. M.L. Neilsen: A Dynamic probe strategy for quorum systems.
In: Proceedings of the IEEE 17th International Conference on
Distrib Comput Systems, pp 95–99, 1997
13. M.L. Neilsen: Quorum Structures in Distributed Systems. Ph.D.
Thesis, Department of Computer and Information Sciences,
Kansas State University, 1992
14. M. Naor, A. Wool: The Load, capacity and availability of quorum
systems. SIAM J Comput 27(2): 423–447 (1998)
15. D. Peleg, A. Wool: How to be an Efﬁcient Snoop, or the Probe
Complexity of Quorum Systems. In: Proceedings of the 15th
ACM Symposium on Principles of Distrib Comput, pp 290–299,
1996
16. D. Peleg, A. Wool: The availability of quorum systems. Inform
Comput 123(2): 210–223 (1995)
17. D. Skeen: A quorum–based commit protocol. In: Proceedings of
6th Berkeley Workshop on Distributed Data Management and
Computer Networks, pp 69–80, 1982

Rida A. Bazzi received a B.E. in Computer and Communications
Engineering from the American University of Beirut in 1989, and
an M.Sc. and a Ph.D. in Computer Science from Georgia Institute
of Technology in 1994. He is currently an assistant professor in the
Computer Science and Engineering Department atArizona State University. His major research interests are distributed computing, fault
tolerance, and security.

The Power of Processor Consistency*
(Extended
Mustaque

Rida

Ahamad

Ranjit

A. Bazzit
Institute

memory

mal definition,
garding

ory

model

system.

These

and can be easily
To illustrate

definitions

related

the power

of processor

types

we show that Lamport’s

[20];

that

and Wing

was

pro-

this

mem-

later

called

[15]. Unfortunately,

strongly

consistent

a simple

memories

argument

[21]

efficiency

systems.

problem

Recent

in distributed

shared-

research in high-performance

taining the consistency of shared memory.
Such memories scale well, but they sacrifice some of the consis-

we ex-

tency guarantees of sequential
consistency.
Other research [1,5,7,11] has proposed memories that are “hybrids” of weaker and stronger forms of consistency,
al-

is

algorithm

memoy

condition

by Herlihy

that

a stronger

shared memories [2,3,16,21] has suggested the weakening of memory consistency to reduce the cost of main-

of memories.

Bakery

consistency
later defined

called atomic

to implement

a significant
memory

hibit a non-cooperative
solution to the mutual exclusion
problem that is correct with processor consistency.
As
a contrast,

Neiger

can be used to show that these memories cannot retain
low access latency in large systems. This fact represents

to give two

consistency,

sequential

such as these in large systems;

are non-operational

to other

Gil

Lamport

a correctness

littearizability

distinct definitions
of processors consistency:
one corresponding to Goodman’s
original proposal and the other
corresponding
that given by the implementors
of the
DASH

has

it is costly

claims have been made re-

We use a formal

called

form of memory,

due to the lack of a precise and for-

contradictory

its power.

model

vided such properties.

Shared memories that provide weaker consistency guarantees than the traditional
sequentially consistent or
atomic memories have been claimed to provide the key
to building scalable systems. One influential memory
considency,
has been cited widely in
model, processor
but,

Kohli

of Technology

Abstract

the literature

Prince

John

of Computing

College
Georgia

Abstract)

not correct with processor consistency.

lowing a program to use a strongly
only when it is necessary to do so.

1

posed by Goodman

[14] and is called processor

consis-

tency.

system

of pro-

One influential
Introduction

The abstraction
hardware
to local

of a memory

is attractive
and

remote

shared across distributed

because it allows
information,

uniform

thereby

access

simpli&ing

author

was supported

in part

by a scholarship

from

memory

memory

was pro-

consistent

implements

cessor consistent

memory

that

man’s definition;

this was described
consistency

a type

is dktinct

from

Good-

by Gharachorloo

et

has been cited elsewhere

in the literature
[5,6,9,13].
Unfortunate ely, the original
proposal did not provide a detailed specification
of processor consistency, and only an operational
definition
is
provided by Gharachorloo
et al. As a result, various researchers have developed different interpretations
leadand contradictory
claims.
Jn Section
2,
ing to confusion

* Thk
work
was supported
in part
by the National
Science Foundation
under grants CCR-8619S86,
CCR-8909663j
and
CCR-9106627.
Authors’
address:
College of Computing,
Georgia
Institute
of Technology
Atlanta,
Georgia
30332-0280.
t Tlds

The DASH

al. [11]. Processor

progr amming.
IdeaUy, a distributed
shared memory
should provide all the consistency properties
of a true
shared memory.
To this end, Lamport
[19] defined a

Hariri

weakly

consistent

we discuss

the

Foundation.

In

a possible

this

paper,

of processor

Permission to copy without fee all or part of this material is
granted provided that tha copias are not mada or distributed for
direct commercial advantage, the ACM copyright notice and the
title of the publication
and its date appear, and notice is given
that copying is by permission of the Association for Computing
Machinery.
To copy otherwise,
or to republish, requirea a fee
andlor specific permission.
ACM-SPAA’93-6/93
Nelen,Germany.
Q 1993 ACM 0-89791-599.2/93/0006/0251
. ..$1.50

those

for

allows
to
two

251

consistency

used

defining

tions

by

by Misra

strongly

confusions.

a precise

characterization

developing

types

definitions

[22]

and

consistent

us to explicitly
of processor

other

of such

non-operational

We do so by using a formal

detitions.
to

basis

we present

of shared

relate

memory.

incomparable:

method

Herlihy

memories.

demonstrate

consistency
are

by

how

This
the

to each
We
each

similar

md

Wing
method

two

defini-

other

and

that

the

show
permits

execu-

tions
that,

that not allowed by the other.
contrary
to earlier claims, both

algorithms

that

eration.

provide

mutual

exclusion

consistency

are not correct

This

exclusion

We also show, however,

for mutual

that

fact raises further

utility
of processor
concluding
section.

that

without
with

processor

questions

about

consistency,

mediate

coop-

Scheurich,

and

ory

we discuss in a

and Briggs

accesses,

section

processor
it

intention.

discusses Goodman’s

consistency
consistency

consistency,
sistency
port

and

order

that

operations

executions

by Lam-

in

is some sequential

that

is consistent

with

of all the processors.

of cache consistency

requires

Figure

be identical

[21].

this.

6 and

Finally,

is discussed

to the claim

the

Gharachorloo

et al. give

This

definition

explicitly

ever,

their

definition

&Hers

portant

aspects.

location

followed

by a read

ordered.

observes that strong consistency is costly
and that cache consistency alone is inade-

if the

operations

appear
tial
Since

[to

other

specified

Goodman

cache

However,

it

as it

some

to different

orders

may,

at different

There

by its

is

is not
locations

et al.

must

to

also

be

algorithms
sistency

while

cannot
because

as sequential

consistency,
to appear

as to

argues

be implemented
of its lack

processor

consistency

plies

the

that

Bitar

above

are not

other

would

processor

that
In

difference

give

each

arises

from

these

how

it is

all

opera-

operations

writes

must

invoked).
of other

processor.

conditions

operations

for

were

for

can
order

definition,

processor’s

im-

read

program

is respected

they

Howa write

the

the

executes

by a given

following

the

location,

in which

of
sys-

in two

executes

Goodman’s

order

order

definition

coherence.

words,

that

The
proces-

Gharachorloo
their

processor

or

at all other

with
read

processors,

and

not

processor

con-

We argue

that

coherence,

is allowed to perform
processor, all previous

in different

which

a write

(STORE)

is allowed

respect to any other processor,
(reads and writes)

to perform

all previous

must be performed

with

accesses

at all other

processors.
For

this

other

definition,

if both

one operation

operations

the fist precedes
tion of “perform”

synchronization

with

to any other

accesses must be performed

permit

whether

of coherence.

does include
claims

In

is because

respect

Goodman
meant for processor consistency to include
coherence. Attiya and Friedman [5] and Bitar [6] both
assumed that coherence was not part of processor consistency. This led Attiya and Friedman to claim that a
non-cooperative algorithm for mutual exclusion cannot
be implemented in a system that provides only processor consistency,

execution

Goodman’s

if a processor

program

in the

2. before

disagreement

If
then,

in the DASH

requires
horn

1. before a read (LOAD)

provide

processors.

some

below.

consistency:

as strong

cache

be

is given

5.1

an operational

of a different

sors can be ordered

sequen-

consistency
it

that

(this

second

processor

in the

processor

like

as

First,

be maintained.

“appear”

program.

consistency,

consistency,
writes

individual

in-

not

coherence,

this

as implemented

write.

by the
not

assumed

conszs-

is the same

processor]

defines

than

coherence.

of each

any

order

stronger

of any execution

the

defined
need

tions
tewt if the result

consistency

“bypass”

quate for many applications.
For thh reason, he defines
processor consistency as an intermediate:
is said to be processor

Section

by Goodman,

tem.

A multiprocessor

himself
should

example

include

of
that

be allowed.

cations.
Following
Gharachorloo
et al. [11], we use the
term coherence to refer to the property
of every data
strongly

This
in

not

processor

being

PRAM

is doubtfid

he states

consistency.

above hold only on a per-location
basis: processors need
not agree on the order of two writes to two ditferent lo-

Goodman
to implement

it

Goodman

that

did

to the

Again,

execution

consistency

contrary

Goodthat

Sandberg

by processor

processor

was his

consistency

Lipton

allowed

this

processor

of

meant

that

that

would

a sample

unlikely

“weak”

to proces-

consistency

is the fact

Goodman

attribute

processor

coherence

cludes

not

of mem-

and

that

coherence

on three

as defined

there

seems

type

%trong”

oth-

of Dubois,

argument

without

consistency,
cache
Strong conconsistency.

consistency

of all memory
definition

concentrated

it thus

does

includes

and

this

between

Goodman

Another

to mean that

[7]. However,

of

strong

processor

it guarantees

the individual
man’s

work

models:

is sequential

[19];

definition

[14] and some interpretations

Goodman’s

[5,6,11].

memory

original

was “cache consistency,”

a distinction

which

(or, to use

ordering

weak and strong

requires

to be inter-

consistency

to be intermediordering ). Because Good-

processor

ers have taken ‘(weak ordering”

consistency.

Background

This

consistency

and weak

man’s term for coherence

sor consistency;

2

processor

strong

he defined

ate between

sequential

the nature

which

defined

between

his terms,

some algorithms

are correct
with

Goodman

We further show
definitions
adrnk

is “previous”

to an-

are by the same processor

and

the other in program order. The nowith respect to a processor is defined

as follows, A read operation
to processor p when a write

im-

read

correct,

will

a write

252

not

change

operation

T is performed
with respect
by p to the location
being
the value returned
by T. Similarly,

w is performed

with

respect

top

when

a read of the same location

by p returns

ten by w or a subsequent
of “subsequent”

write

is well-defined

recent

the value writ-

operation

(the notion

because the memory

lead to an ordering

of writes

write

to that

location

that

If H is a history and p is a processor, let HP+W comprise the subsequences of H consisting of all operations

is

quite different than that given by Goodman’s definition.
IiI Section 5, we precisely characterize
the differences

by p and all write
(respectively,

H,+wlz)

between

(respectively,
location z.

HP+W) consisting

3

the two definitions.

Definitions,

This

section

Terminology,
formally

lies our definitions

and

describes

Wmg

[15].

In Sections

define

various

formulations

forms

that

of shared

We use a model

de-

[22] and by Herlihy
memory,

4

under-

including

to

els.

two

(or local history)

An ezecution

histoy

only

(or history)

section

in the preceding

of memory

defines

Lamport’s

of Lipton

and Sandberg

SC:

there is a legal serialization

S of H such that,

and 02 are two operations

in H and 01 ~

combination

with

other

allowable executions.
consistency
(Section
program
program

kinds

considering

Thus,

the values returned

are consistent
cessors cannot

of H.

serializations

be used (sometimes
to constrain

then

will

of certain

Lipton

in

memory

the set of

The serialization
returns

be defined

by the read operations

PRAM:

below by
of H

there

has been

systems

with

non-blocking

erations

may

be partial

a single memory.
if it admits only

histories.

Sandberg

developed

a weaker

they called the pipelined

RAM

form

of

or PRAM.

for each processor p, there is a legal serializa-

tion

SP of HP+W such that,

operations

if S

if 01 and oz are two

in HP+W and 01 =

the operations

02, then 01 ~ 02.
P

is legal if each read operation

This

the value written

each processor may “perceive”
a difFerent serialization.
While the order of two writes by the same processor
must be the same in all these serializations
(even those
for other processors), writes by di&erent processors may

by the most

1Ae defined here, program
order
totally
orders
the operations
of each processor.
The DASH implementation
of processor
consistency
uses a partial
program
order (see Section 5.2 below).
In
addkion,

consistent
and

that

in H

with the sequential ordering in S. Protell, by way of their interactions
with the

if it satisfies the following:

sets of operations.

exactly

02, then

This memory requires that the writes of a single processor be seen in the order in which they were invoked at
all other processors. Specifically,
a history .H is PRAM

we assume that

S is a serialization

sequence containing

a location

at

mem-

The DASH definition
of processor
5.2) uses an alternative
(partial)

of memories

is a history,

is a linear
horn

orders)

operations

of dtierent

order. Unless stated otherwise,
order is as defined above.

Dfierent
If H

will

if 01

that

sequentially

order

to

An operation

as it does not relate

program

[21], and

consistency.

~

ory models,

consis-

The next section uses the same formalism

memory, that they are not accessing
A memory is sequentially
consistent

In the definitions

mod-

sequential

gram order and write 01 I% 02.1 Most memory models
are defined so that a processor’s view of all operations is
consistent with its program order. Note that the order
on H,

section,

consistency

if it satisfies the following:

II is a collection

one for each processor.

processors.

on

cesses (i.e., a single memory).
This is easily defined
within our model. History H is sequentially
consistent

H comprises. If 01 and oz are two operations in Hp and
01 appears first, then we say that 01 precedes 02 in pTo-

is partial

of operations

The idea behind sequential
consistency
is that, although the shared memory accessed by processors may
be distributed
(i.e., may consist of many different modules), the processors’ observations of the memory should
be consistent with one that permits only sequential ac-

opera-

is said to be in H if it is in one of the local histories

different

I@
of H

of proces-

HP, is a sequence of read and write

of local histories,

developed

define two forms of processor

value v in location z; a similarly
denoted read operation, rP(z)w, reports to p that v is stored in location z.

tions.

processors.

the subsequences

Models

a variety

[19], the PRAM

coherence.

interact via a shared memory that consists of a tinite set
of locations. A processor’s interaction
with the memory
is through a series of read and write operations on the
memory. Each such operation acts on some nmned location and haa an associated value. For example, a write
operation
by processor p, denoted WP(Z)V, stores the

sor p, denoted

This

tency

consistency.

history

by other

contains

Memory

we can define

We define a system to be a finite set of processors that

A local execution

Earlier

Given the formalism

and

4 and 5, we use this model

of processor

operations

Notation

the system

and results.

rived from those used by Misra

(based on the

02 in S, we write

is

coherent ).
These conditions

preceding

given by S) .2 If 01 precedes

order

recent
reads,

interest
in which

in the
the

formalization
order

of local

is weaker

than

sequential

consistency

because

of

2It

op-

returned

[12].

253

is assumed

that

each location

by a read of a location

with

has some initial
no preceding

value
write.

that

is

‘:mm
q:mm

dzmmm
‘:amm
Figure

1: A history

that

is not sequentially

‘:mmm
9:

Figure

consistent

5

3: A history

Processor

that

is not coherent

Consistency

We now consider definitions of processor consistency.
As
not ed above, there are two such formulations.
One was

mm

originally

presented

by Goodman

[14] and is referred

here as PCG. The other was described
Figure

2: A history

that

is not PRAM

tors of the DASH

to

by the implemen-

system [11] and is referred

to here as

PCD.

appear in different
thermore,

orders in different

each processor’s

serializations.

serialization

5.1

Fur-

does not contain

consistent;

each processor

Cache consistency,
within
this model,
weaker

than

can order

A memory

the other’s

is PRAM

consistency

because

it is neces-

PCG:

sary only that all processors agree on a common order
of the writes to each location.
A history is coherent (or
cache consistent)

if it satisfies

of processor consistency is, in a
of PRAM
and coherence.
There

perceived in the order in which it invokes them (as in
PRAM).
Furthermore,
the two requirements
must be
mutually
consistent.
Thus, history H is PCG if it satisfies the following:

only

or coheTence, can also be defined
Recall that cache consistency
is

sequential

Consistency

must be a unique view of the writes to each location
(as in coherence) and each processor’s writes must be

writes

if it admits

Processor

Goodman’s
definition
sense, a combination

the read operations
of other processors, as it is not (directly)
aware of these operations.
Figure
1 gives an
example of a PRAM
history
that is not sequentially
after its own read.
PRAM histories.

Goodmnn’s

for each processor,

there is a legal serialization

SP of HP+W such that
1. if 01 and 02 are two operations

the following:

01 ~ 02, then 01 ~
Coherence:

for each location

ization

S= of Hlx

operations
The writes

in Hlz

P

z, there is a legal serial-

such that,

2. for

if 01 and 02 are two

and 01 % 02, then

to x in SZ appeax in the order

operations

upon

alizations.

ations
which

gives a history
gives a history

write

sufficient

to guarantee

linearizable

full

memory

PCG.

on the perceived
orderings
of
lo cat ions.3 The conditions
for
are not comparable.
Figure 2

sequential
[15].

consistency.

Lkearizable

meaning
that, if each location
is Iinearizable,
ory is. If sequential
consistency
were local,
no difference

between

coherence

and

Thk

then
then

sequential

is

01 and 02 to x, then

appear

operations

these oper-

in the same order

In other

words,

write

in all seri-

if we denote

by

of SP consisting

of

to x, then,

for any p and

This is discussed in the full paper.)

q,

A memory

PCG if it admits only PCG histories.
PCG Condhtion 1 is exactly the PRAM condition,
PCG is at least as strong as PRAM.
The history

3

Figure
stronger

3 is PRAM

but not PCG.

than PRAM.

Thus, PCG

Note that PCG Condition

exactly the same as coherence. Nevertheless,
not hard to see that PCG implies coherence,

contrasts

memory

are two

(To disallow certain anomalous
executions,
one additional technical constraint
is added to the definition
of

3In a sense, a coherent
memory
is sequentially
consistent
on
per-location
basis.
Maintaining
this kind of consistency
is not
with

if there

Spl(w, z) = Sq](zo,z).

is coherent

that is coherent but not PRAM. Figure
that is PRAM but not coherent.

z,

SPI (w, z) the subsequence

In a sense, coherence requires that accesses to each
location be sequentially
consistent.
However, coherence
places no requirements
operations
on different
coherence and PRAM

each location

01 :02.
m

all processors agree. Reads of z are also included
in
S= (ordered appropriately)
to ensure that all processors
“perceive”
this order of writing.
A memory
if it admits only coherent histories.

in HP+W and

02; and

is
so
in

is strictly
2 is not
it is also
For any

location z, construct a legal serialization
S= (required
for coherence) as follows. First, include all writes t o z in
the (unique) order ensured by PCG Condition
2 Then,
include each read of z (by ‘any processor) following
the

local,

the entire memthere would be
consistency.

254

Gharachorloo

d@m!l
r:

Figure

As noted

mm
4: A history

that

of an operation

a processor (see Section
operational
definition.

mm

!7:

et al. [11] gave an operational

using the notion

is not processor

consistent

above,

was a combination

The DASH

definition

may be more than

one

read of the same value and the order

of such reads by

different

It is easy to see

processors

is unimportant).

accesses to z. Every

PCG history

is thus coherent.

Intuitively,

this is because the PRAM

The

the causaMy

used to define causal memory

2). Figure

4 gives an example

PRAM and coherent
because the following

that

relation

weaker notion

of program

that

01 “~

tially

Wp(z)o, WP(y)o, Wq(z)l;

Sq =
s. =

qa)o,

qJ?J)c4 ~q(v)o)

~q(~)u

wq(z)l,

r.(z)l,

?,(*)O,

o! “~

The causality

that

satisfy

the PCG

Condition

conditions

cannot

Sequential

consistency

implies

serialization

S7 required

by sequential

5.2

sequential
DASH

(note

The DASH
consistency.
ble with

that

stronger

par-

orders a write
When

causal memory

(~).

with

operation

a “writesbefore

a memory

any

is required

order

forms

is called the weak

and is denoted

by “~b.

that

operation

o’ ~

o’ = w (y)u such that

02, where

y). “That

followed

We write

02 = r(~)zt

o’

Sy is the legal serifllzation

is, 02 must be a read from

01 in program

and

01 ‘=

order.

Note that

a write

a normal

noted by ‘&b. We write 01 ‘&b 02 if and only if 01 = r (z)v,
02 = w(y)u, and there is another operation o’ = W(Z)V’
such that 01 ~ o’ and o’ ‘= 02. Again, there is a twostep chain fro:
01 to 02 but, in this case, the first link
is from a read of an old value (from z) to the write of a
The semi- causality relation, denoted
new value (to Z).

Consistency

it is neither

only

%rites-before”
relation would order o’ before 02; the
weak relation relates only earlier writes to oz.
The
second relation is the weak reads-before order and is de-

con-

&

system also implements
a form of processor
As we will see, this definition
is incompara-

Goodman’s;

order

of these weakened

for location

The single legaJ

consistency.
Processor

order

because the case

used in defining

of program

there is another

tains, as subsequences, the seriahzations
SP required by
PCG. However, the history in Figure 1 is PCG but not
sequentially
consistent.
Two writ e operations,
to different locations
and by different
processors may be perceived in different orders. Thus, PCG is strictly weaker
than

program

operations

01 ‘Ab 02 if and only if 01 = w(z)v,

be satisfied.
consistency,

02 and

The semi-causaMy relation is defined by augmenting
the
weaker program order (“&) with weakened forms of the
“writes-before”
and “reads-before”
orders.

1 will do so also. Thus,
PCG.

by the same

value before a write of a new value (the notions of “old”
and “new” are well-defined
if the memory is coherent).

to z and any histories
both

a

to be coherent, it makes sense to also consider a “readsbefore”
order that orders a read operation
of an old

~p(Y)o

is not PCG because S~ and S, above differ

the two PCG

The

First,

01 ‘W 02) if 01 B

read of the value written.

The fist

of the two writes

earlier writes

This

relation

order that

writes-before

on the order

02.

is a combination

(recall that, if o is r or w, then OPmeans that operation
o is executed by processor p). It is coherent because the
following
legal seritilzations
exist:

The history

x.

as follows.

order is used; this is because

orders a processor’s

before”

Sp =

is defined

in which 01 is a write and 02 is a read of a diiYerent
location is omitted above.

is

but that is not PCG. It is PRAM
legal seriahzations
exist:

Wp(s)o,

semi-causality

there is a legal se-

on each location

weakly precedes 02 (written

and co-

of a history

are coherent,

SZ of operations

to
[2]).

either 01 and 02 are operations on the same location, 01
and 02 are both reads or both writes, 01 is a read and
02 is a write, or there is another operation o’ in HP such

herence orders might not be consistent with each other
(this mutual consistency is specified by the PCG Condition

com-

(this is similar

processor. We use ‘z to represent this weaker program
order. If 01 and 02 are two operations
in HP, then 01

history in Figure 2 is coherent but not PCG. Thus, PCG
is strictly stronger than coherence.
A history that is both PRAM and coherent need not
be PCG.

we refer to as PCD)

reads are allowed to “bypass”

that thk results in a legal serialization
and that this
can be done in a way that preserves the order of local

of processor
and coherence.

“semi-causality”

Since all PCD histories
of the value it reads (there

definition

bines coherence with
relation

at

here a non-

of PRAM

(which

definition,

“performed”

We present

Goodman’s

consistency

rialization
write

2).

being

is the transitive

order

relation

“=,

closure of the union
the weak writes-before

and the weak reads-before

nor weaker.

255

relation

“&b.

of the program
relation

“&b,

We can now formally
if it satisfies
PCD:

define PCD.

History

His

PCD

dzmmilm

the following:

1. 11 is coherent;

that

is, for each location

there

is a legal

that,

if 01 and 02 are two operations

serialization

and 01 K 02, then

s:

of HP+W such that

and 01 S

Figure

OZ, thenol

appear

the coherence
if and

OIdy

in the same order

order S..

if~l

;

that

is not PCD

:

P:mmmm
q:mmmm

01 and 02 to z, then these op-

erations

5: A history

in HP+W

OZ; and
P
z, if there are two write

for each location
operations

mm
——

p, there is a legal serializa-

(a) if 01 and 02 are two operations

(b)

such

in Hlz

01 ~ 02.
a

2. For each processor
tion

S= of IYlz

——

z,

That

as in

is, 01 ~ 02
P

OZ.

Figure

.

6: A history

that

is not PCG

certain anomalous
executions,
one addi(To disallow
tional technical
constraint
is added to the definition
of
5.3

PCD. This is dkcussed in the full paper.) A memory is
PCD if it admits only PCD histories.
We briefly relate this definition
to the one given by
Gharachorloo
et al. (see Section 2). Their first condition

specifies

when

read operations

it corresp ends to the fact that
der “~ respects

the order
second

the partial

processor.

The

condition

operations

can be performed.

fact that

the weak writes-before

specifies

program

whereas PCG

This corresponds
and reads-before

uses a total

some operations

or-

Consistencies

program

ings not

write

defined

operations
that
two examples.

to the
rela-

Figure

tions must be respected.

order;4

that PCD does not.

because it uses semi-causality,

by a given
when

Processor

PCD is neither stronger nor weaker than PCG. PCD
is not stronger because it uses a partial program order,

can be performed;

of read operations

Comparing

which

by program
PCG

5 gives a history

that

orders

introduces

order;

does not.

PCG

PCD is not weaker
PCD

Thk

order-

orders

some

is illustrated

is PCG

but

by

not PCD.

PCG allows this history because the required
serializations exist. PCD does not allow this history.
Suppose it did. Note now that the following
relationships

Sequential
consistency implies PCD in the same way
that it does PCG. However, the history in Figure 1 is
PCD. Thus, PCD is strictly weaker than sequential consistency.
PCD is strictly
to trivially
follow
that the execution

stronger

than coherence.

WP(Z)l

This appears

One should note however, that the semicausality
order
is based on a partial program order, while coherence is

(see above). From
implies coherence.

the operations
this,

PCD is also strictly

stronger

than PRAM,

Note

also that

WP(Z)O “~

dicting the fact that S8 is a legal serialization.
clude that this hktory
is not PCD.

on any given location

it is not hard to see that

ZUT(V)l.

Figure

PCD

6 gives a history

that

is PCD

despite the

following
reason. Suppose
and Sg are the serializations

that exists for processor p according to the definition
of PCD. Using an inductive
argument,
this history can

requires that
writes to z.

be modified to give an S; that satisfies the PRAM condition.
Hence, PCD is at least as strong as PRAM.
Figure 4 gives a history that is PRAM but not PCD.

4 Goodman
does not explicitly
assume that
total.
However,
he gives an example
(illustrated

Thus,

order

is strictly

stronger

but

We connot PCG.

PCD allows this history because the required
izations exist. PCG does not allow this history

fact that it uses a partial program order. Consider some
history H that is PCD. Let SP be the legal serialization

PCD

Wp(z)l.

write operations must all precede that read in S8. This
implies that T8(z )0 returns an incorrect
value, cent ra-

defined based on a total program order. However, the
program
order used in the definition
of PCD is total
when considering

&

Thus, WP(Z)O, WP(Z)l, and w.(v)l
must appear in that
order in all the required serializations,
particularly
S..
For r.(v) 1 to return the correct value in S8, these three

from PCD Condition
1 (and the fact
in Figure 4 is coherent but not PCD).

of an illegal

than PRAM.

256

it did and suppose that SP
required.
PCG Condition
2

SP and S~ agree on the order of the two
Without
loss of generality,
assume that

history

such as “~

serialfor the

that

would

be allowed

were used instead.

program
order
in our Figure

if a partial

program

is
6)

shared

‘RAN

P
•1

/* Initially

n[2] : boolean

PCD

F */

t : integer

Sc

while

1

PCG

5

T do
w(n[i])T

6

w(t)i
repeat
4

whose = T(t)

3

otheT = r(n[i])
until

(whose = i or not

otheT)

section

Critical

2

2oherence

w(n[i])l?
RemaindeT
Figure

7: The structure

of relevant

memory

Figure
both order WP(Z)O first.
tal) program

by (his) processor
includes

did not attribute

consistency.
both

This can only be argued

coherence

and a total

between

in this paper
in the figure

figures

the five diflerent

are given in Figure
correspond

sion [23}, which

does not require

Peterson$s

Exclusion

with

PC

reads and writes.

7. The six

and 1 to compete

shown in

it illustrates

the subtleties

in the structure

of processor
definition
insolvable

6.2

consisor im[5,10].

we show another

algorithm

Mutual

To prove that

for the same problem

that fails with processor consistency.)
A mutual exclusion algorithm
requires

exclu-

is correct

algorithm,

explicitly

It allows two processors
for entry

to a critical

giving

numbered
section.

O

The

i.

Exclusion
the algorithm

with

this algorithm
proforms of processor

PCG

is correct

with

PCG,

as-

sume for a contradiction
that both processors O and 1
enter the critical section.
Then each processor i must
observe one of the following
sequences of operations:

Secondly, it shows that a complex algorithm
with data
races, written
for sequentially
consistent memory, carI
run correctly
with processor consistency.
(Unfortunately,

cooperation,

The next two sections argue that
vides mutual
exclusion with both
consistency.

Memory

tency: a slight change in the memory’s
plementation
can render the problem

consistency).

for mutual

algorithm
is shown for processor i; the other processor
is thus 1 – i (we use i to represent 1 – i). Recall that, if
o is r or w, then oi means that operation
o is executed

This section explores the possibility
of using processor
consistency to provide mutual exclusion without
cooperation.
We choose thk problem for two reasons. One
is that

algorithm

Algorithm

8 shows Peterson’s

memories

by processor
Mutual

to processor

Peterson’s

6.1

of memories.
6

by pi

program

to histories

and place those histories

coherence

We prove that

Figure

The relationships

earlier

as executed

with both forms of processor consistency defined in the
previous section. We then show that Lamport’s
Bakery
algorithm
[18], another such algorithm,
is not correct
with processor consistency.

order.

numbers

algorithm

Because S~ must reflect q’s (to-

the fact
der: WP(Z)O, WP(Z)l, T* (z)O. This contradicts
that S’q is a legal seri~lzation,
and we conclude that the
history is not PCG.
We note here that the history given in Figure 6 was
given by Goodman as an example of hkitory not allowed

discussed

8: Peterson’s

order, WP(Z)O precedes Tq(z)O in Sq. PCG

Condition
1 requires that WP(Z)O and WP(Z)l
both
precede WP(z)O (in that order) in both serializations.
Thus,
S~ must contain the operations on z in the following or-

if PCG

sectzon

models

coopeTatzon if

processors in the critical
and remainder
sections must
interact
with those attempting
to enter or leave the
critical section.
Attiya
and Friedrmm
[5] claimed that
processor consistency
lacked the power to implement
mutual exclusion without
cooperation
(recall that they

1. w~(n[i])T,

w~(t)T,
.... r~(t)i,

T~(n[Z])*;

2. w;(n[i])T’,

w~(t)t,

r~(n[~])r,

where

the

elliptical

. . . . r~(t)*,
sequence

cent

&s

or

OrJY

read

oper-

ations and * may be any value (these sequences show
only the reads and writes of the two entries that violate
mutual

257

exclusion).

Without

loss of generality,

assume

‘

that

zoo(t)l

precedes zol (t)O in both

SO and S1 (as spec-

ified by PCG Condition
2). By PCG
of processor 1’s reads of t must follow
these reads must
sor 1 cannot
Instead,

all return
(2) (with

quence of S1. PCG
precedes

the value O. Thus,

observe sequence

sequence
wo (t)l

Condition

1 requires

W. (n[O])Z’ precedes rl (n[O])F
so only

once (per

entry),

mutual

that

1 for i).

while

/* Initially

F */

s[n] : integer

/* Initially

O */

T do
w (c[i])T

wo(n[O])T

processor

c[n] : boolean

proces-

be a subse-

in S1. Given that

there

shared

me = 1 + max{s[j]

one caa see that

to n[O] and that

writ es to n[O]. Thus,
ing a contradiction.
guarantees

(1) above (with

1 for i) must

in S1; by transitivity,

sor 1 does not write

Condition
1, all
WI (t)O in S1, so

proces-

forj=l

can be no intervening

S1 is not a legal serialization,
givWe conclude that the algorithm

tondo
repeat
test = r (c~])

exclusion.
util

until

infinite

other
Critical

have the following

suffices:

so

=

.-., 7yJ(t)l,
To(n[l])Z’,

SI

=

. . . . ?’I(t)o, T~(?40])2’, ‘rI (t)o, TO(?2[0])2’,----

Consider

now all the writes

this execution;
PCG Condition
write

Tcl(t)l,

to t that

rl)(n[l])T,

are performed

Figure

section

loss of generality,

assume that

6.4

it is

9:

The

Bakery

The Bakery

PCD
given above

algorithm,

program

order

a set of processors

in part

These two executions
PCD

the guarantees
is correct.

of PCG

(However,

with

problem for
9 is for pro-

numbered

either form

1 to n.

It

of processor

by the memory.
This is beis written
by more than one

. . . W1(c[l])Z’, rl (s[0])0, Wl(s[l])l,
Wl(c[l])l’,
‘r~ (c[o])F’, ?’~(s[0])0,

all

other

in this case, and the algorithm

see Section

9, was proposed

--- Wo(c[o])l’, 70(s[1])0, WAJt?[o])l,
WO(CIO])F, ro(c[l])F,
ro(s[l])o,
CS, . . . ; and

(WA) can

provides

with

To see how an incorrect execution is possible, consider
an
the case when n = 2. It is not hard to construct
execution that admits the following local subhistories:

for the two reads in Figure

PCD

algorithm

because it does not take advantage

be partial is when a write operation is immediately
followed by a read from a different location.
Inspection
of
Figure 8 shows that there can be no such pairs of operations in an execution of Peterson’s algorithm.
(This
results from the particular
order that we have chosen
8.) Thus,

pi

processor.

The reason for this is that, when using Peterson’s algorithm, PCD is stronger than PCG. Thk is because PCD
is only more permissive that PCG because the program
order that it considers is partial
and need not match
operations
actually
occur,
Howe~er,
the arder in whic&
PCD’S

shown in Figure

of the coherence provided
cause no shared location

are sufficient to prove that Peterson’s algorithm
is also
correct with the DASH version of processor consistency.

case in which

of Peterson’s

by Lamport
to solve the mutual exclusion
n processors. The code presented in Figure

consistency,

the only

by

Algorithm

does not execute correctly

the same arguments

as executed

perceived in the same order by both processors, only
one of the processors can advance to the critical section.

cessor pi from

It is easy to see that

algorithm

The key to the correctness

it. We conclude that the implementation
is deadlockfree. It is not hard to use similar techniques
to show
that it is lockout-free.

with

Bakery

processor consist ency is coherence. Since both processors write to the shared variable t and these writes are

WI (t)O appears somewhere in this history and there are
(infinitely
many) occurrences of To(t)l that appear after

Exclusion

section

in

there are only finitely
many of these.
2 implies that there is a unique “last”

to t. Without

Mutual

= r(s[j])

othe7- = O or (me, i) < (other,j)

....

a write of the value O performed
by processor 1. This
then contradicts
the fact that So is a legal serialization:

6.3

test

w(s[i])O
Remainder

‘

not

loop infinitely,
trying to enter the critical section. Given
that there are no write operations in the repeat-until
So and S1 must

reads the array s */

if j # i then

repeat

the histories

/*

w(c[i])F

O does

One can also see that this solution is fkee from deadlock. Suppose not. Assume that processors O and 1 both

loop,

I j # i}

w(.s[i])me

after

determines

7 below. )

258

could

because each processor

occur

with

either

PCG

can order the writes

all of its own operations
that

c’s, . . . .

(that

it is safe to enter the critical

or

of the

is, after
section).

it

6.5

General

Although
processor

Comments

searchers are attempting
release consistency with

the Bakery
algorithm
is not correct with
consistency, the problem of implementing
n-

nization

(RCpc)

to develop a similar model for
processor consistent
synchro-

or simple processor

We have recently

learned

[10] that

consistency
the version

[9].
of pro-

processor mutual exclusion can be solved with this memory model.
For example,
Peterson extended the al-

cessor consistency

gorithm

weaker than that described by Gharachorloo
et al. [11]
and defined here as PCD. That is, the DASH system

given

processor

in Figure

problem.

form of processor
Attiya

solves the

is correct

with

n-

eitlier

does actually

consistency.

and Friedman

tency

to not include

tion,

mutual

impossible.

8 to one that

This extension
understood

coherence

exclusion

without

Implementations

processor

[4]. Under

cooperation
of coherent

consis-

histories

PRAM,

or causal memory)

vide mutual

exclusion

cooperation

is necessary

without
for

tion

(as it is for those that

may

be possible

that

memory

level.

7

explicit

[11]

provide

requiriig

Such is the case with

cooperation

PCG

If

then it

exclusion

PCD.

Whale

coherence

semicausalityj

system is

the DASH

and allows only

it does not always

between these two; Condition

of PCD

open question).
importance

implementa-

coherence),

mutual

respect

in the DASH

is not guaranteed

2b

to hold

in

all cases. For this reason, Peterson’s
algorithm
does
not provide mutual
exclusion under this implementation (whether or not mutual exclusion can be achieved
without
cooperation
under this implementation
is an

pro-

cooperation.

a memory’s

to implement

without

cannot

that

in the definition

require processor cooperation
at a lower level. It is not
hard to see that, to provide mutual exclusion, cooperation is necessary at some level. Thus, memories that do
not require cooperation
in their implementation
(e.g.,
slow memory,

implement

does provide

enforce the consistency

is indeed

memory

not

implementation

this assump-

implemented

This revelation

of defining

further

formal

method.

cation

of PCD that matches the DASH

The DASH

with

We are currently
implementation

or not the memory

and PCD.

the

using a careful

working

on a modifi-

implementation.

of processor

is designed for a multiprocessor.

at a Klgher

underscores

shared memories

consistency

It is not clear whether

can scale well to large

distributed

systems, In fact, IMechnan [8] has observed that any
implementation
of PCG or PCD would require access latencies on the order of the longest communication
delay

Conclusions

Processor consistency is an important
but heretofore little understood model of shared memory. By using a sim-

in the system. This suggests that processor consistency
would require access latencies similar to those incurred

ple but formal

by sequential consistency without
providing
as strong
consistency guarantees. These facts suggest that proces-

method

have shown that
processor

for defining

there

consistency

the other.

In addition,

processor

consistency

are indeed
and that

memory

models,

two distinct

neither

we

forms

is stronger

of

sor consistency

than

sequential

we have shown that both forms of
can be used to implement

be a good candidate
a large system.

mutual

exclusion without cooperation.
However, while one mutual exclusion algorithm
is correct with processor consis-

serial memory.

for distributed

Whiie

other researchers

exclusion. Using the DASH system, one can accomplish
this by using read-modify-write
primitives
such as test-

it

weaker

than

shared memory

has never

this technique

in

of using a weak

has been used by

to define shared memories

been

used

tency.
(Other researchers
automaton-based
method

and-set.)
up the difficulties

memory

will not scale well and would not

Our method is hzstoy-based;
it defines a memory
model by comparing its executions to those of an ideal

tency, another is not. (To be fair, processor consistency
was not developed for the purposes of providing mutual

These results point

(or any coherent

consistency)

to

define

[5,15,19,22],

processor

consis-

[12,13] have used a formal
for deiining
shared memo-

such a memory when one classical algorithm
for mutual exclusion is correct and another is not? What is
a good programming
model for using processor consistency? Issues such as these must be addressed if weak

ries.)
We have also used this method
to define another shared memory, called causal memory
[2]. Like
processor consistency, causal memory lies between between PRAM
and sequential
consistency
in a hierarchy of shared memories. However, processor consistency
and causal memory are incomparable;
there are histo-

shared memories

potential.

ries of each that

these issues [9,11,

cally, causal memory

shared memory model
How will programmers

Other

such as processor
develop intuitions

are to realize

researchers

their

are addressing

consistency.
about using

it does guarantee

13]. A simple programming
model, called properiy labelled (PL) programming,
has been developed for release consistency with sequentially
consistent synchronization

operations

(RCSC), a component

are not histories

der of causally
sor consistency
the relative

of the DASH

for future

system; this model allows programs written for sequential consistency to be run with a weaker memory.
Re-

that

all processors

merits

Specifi-

coherence,

but

agree on the or-

related write operations,
does not do. A further
of the two memories

whkh procesexploration
of
are a subject

research.

We have generalized

259

of the other.

does not guarantee

our method

[17] to encompass

more

complex

memory

tency.

Doing

so enabled

models

such as release consis-

us to formally

show that

[11]

the

K.

Gharachorloo,

A.

Gupta,

progr arnrning model developed for RCSC is not adequate
for RCpc. By giving an understanding
of the parame-

event

ordering

sors.

In

ters relevant
also indicate

Symposium

to the definitions
of shared memories, we
what properties
are important
to simplify

progr amming

May
[12]

and implementation.

Acknowledgements
The

authors

Mkhael

would

work with

like

and

to

thank

Mehdi

us. We are especially
Gharachorloo

and discussions
processor

for

W.

grateful

the DASH

Hagit

provided useful comments
sentation.

Hutto,

discussing
to Sarita

for many helpful

regarding

consistency.

Phillip

Sayfi

Proceedings
on

of
[14]

and Roy I+iedman

regarding

clarification

P. B.

of

Fourth

the

S. V. Adve and M. D. Hill. Weak ordering — a new definition. In Proceedings of the Seventeenth Annual kternational
2-14,

[2]

Symposium

Computer

Architecture,

1990.

M.

Symposium

tuTes,

pages

P.

Lecture

on Distributed

Notes

on

Algorithms,

Computer

October

volume

Proceedings

of the Eleventh

Distributed

Computing,

Science,

pages

Cache

Working

Group,

Attiya,

[17]

August

In’lemzational

Conference

274-281,

pages

[18]

1992. Personal communication.

Twenty-Fourth

Computing,

pages
MIMD

Report

University
Dubois,

at

[9]

K.

Gharachorloo,

K.

D.

Hill.
models.

Journal

scalable
manuscript,

July

“Memory

A.

Gupta,

consistency

shared-memory
March

of

G.

Neiger,

of scalable

93/04,

College

L.

and

ec-

sequential

Scalable

con-

Coherent

1989.

Wing.

Linearizability:

A

objects.

ACM

and

Systems,

Languages

Slow memory:

Distributed

Weaken-

~ distributed
Tenth

Int erna-

Computing

version

appears

of Information

Systems,
as Technical

and

Computer

Sci-

of Technology.
and

M.

shared

January

Ahamad.

memories.

of Computing,

Georgia

A

characteri-

Technical

Report

Institute

of Tech-

1993.

Larnport.

A

amrning

L.

[20]

L.

[21]

on

R.

On

formalism.

J. Lipton

shared

Ar-

to

concurrent
of the

ACM,

1974.
make

executes

La.mport.

1986.

Compute.

How

of Dijkstra’s

Communications

a multiprocessor

multiprocess

Computem,

computer

programs.

IEEE

C-28(9):690-691,

Septem-

1979.

1990.
ac-

solution

August

correct

Basic

of the

new

problem.

Lamport.

ber

interprocess

communication;

Distributed

and

memory.

of Computer

[22]

difFerent
and

J. Misra.
hardware

J. L. Hennessy,

of Parallel

J.

Computing,

S. Sandberg.

Technical
Science,

Distributed

[23]

G.

L.

part

I:

1(2):77-85,

PRAM:

Report

Princeton

J.

Hennessy.

event

ordering

Rein

Unpublished

1993.

260

June

for

memory

ACM

and Systems,

Peterson.

problem.
116,

tioms
systems.

Languages

memory

1992.

multiprocessors.”

the

A

180-88,

scalable

Department

University,

September

1988.

for

and

Institute

Transactions

communication.

and

of
ATchit

1991.

concurrent

complete

Georgia

Kohli,

that

1986.

A. Gupta,

August

P.

Science,

Memory

on

[19]

Teeh-

Proceedings

Personal

Programming

1992.

Briggs.
In

S. V. Adve,

15(4):399407,

to

F.

June

1993.

May

November

Symposium

March

Theory

coherence.

Berkeley,
and

434442,

Gharachorloo,

vision

and

1990.

School

17(8):453455,

on

of Computer

multiprocessors.

Intemzational

Friedman,

[10]

Scheurich,

pages

R.

Press,
and

of California

[8]

Systems,

ACM

Department

in

Thirteenth

679-690.
synchronization

C.

chitecture,

Symposium

90/605,

cess bufering

consistency

ACM

Provshared

Proceedings

61, IEEE

M.

on

zation

progr

H. Attiya and R. Friedman.
A correctness condition
for high performance multiprocessors.
k Proceedings

M.

In

Press,

for

89/39,

nology,

1991.

May

A

1992.

Gharaehorloo.

Algorithms

on PmgTamming

1990.

and

June

on

[5]

and

K.

March

J.

hdy

Press,

consistency

Report

condition

Proceedings

A lgoTithms

9–30.

1991.

H.

M.

ACM

and

In

of high-performance

Technical

Conference

Report

579

[4]

[7]

and

Hutto and M. Aharnad.

tional

M. Aharnad, P. W. Hutto, and R. John. Implementing
and progr arnming causal distributed shared memory. In

nical

ACM

abstract).

292–303.

Herlihy

P. W.

ence,

workshop

15-26,

nonblocking

).

Parallel

on PaTallel

J. R. Goodman.

M.

Specifying

on

Merritt,

(extended

ThtTd

tional

P. Bitar.

International
pages

abstract

306–315.

consistency

sequential

May

the

Merritt.

P. B. Gibbons,

Aharnad, J. E. Burns, P, W. Hutto, rmd G. Neiger.
Causal memory.
In S. Toueg, P. G. Spirakis, and
of the Fifth InternaL. Kirousis, editors, Proceedings

of

Seventeenth

ing consistency to efi~ce
conc~ency
of the
shared memories. In Proceedings

Springer-Verlag,

[6]

[16]

pages

M.

ing

12(3):463492,

M.

of

[3]

on

and

multiproces-

Architecture,

.$ymposium

Transactions
[1]

the

(extended

pages

correctness

References

of

and

memories

Interface
[15]

P. Gibbons,

consistency

shared-memory

ComputeT

Gibbons

shared

sist ency.

of pre-

J. Laudon,
Memory

1990.

memories

Adve

comments

implementation

Attiya

[13]

this

Lenoski,

J. Hennessy.
in scalable

Architectures,

Merritt,

and Kourosh

D.

and

Myths

Info?’mation

1981.

access

Transactions
8(1):142-153,
about
processing

the

in

asynchronous

on Programming
January
mutual
LetteTs,

1986.
exclusion
12(3)

:115–

Brief Announcement: Efficient Implementation of a
Byzantine Data Storage System
Rida A. Bazzi

Yin Ding

Computer Science and Engineering
Arizona State University
Tempe, AZ, 85287

Computer Science and Engineering
Arizona State University
Tempe, AZ, 85287

bazzi@asu.edu

yding@asu.edu

Categories and Subject Descriptors

To achieve these improvements, we define and propose implementations for non-skipping timestamps. A non-skipping
timestamp is an abstract data type with two operations:
Get and Get&Inc. Valid implementations of non-skipping
timestamps should have the following properties: (1) NonSkipping. If an operation on a non-skipping timestamp object returns value t0 , then for every timestamp t, 0 < t < t0 ,
there exists a Get&Inc operation that wrote t at an earlier time; (2)Progress. Non-overlapping Get&Inc operations
return different timestamp values, and the value returned
by the later operation is greater than that of the earlier
one; and (3) Partial Linearizability. There exists a total order of the operations that is consistent with process order
and real-time order such that: (1) A Get that appears after
a Get&Inc operation in the global order will not return a
value smaller than that written by the Get&Inc operation;
and (2) A value returned by a Get operation is not smaller
than the value returned by an earlier Get operation.
The implementation assumes that any two quorums intersect in 4f + 1 servers, so the total number of servers is
≥ 4f +1. Each server stores the most up-to-date value of the
timestamp that it is aware of. A key observation in the algorithm is that a client need not choose the largest returned
timestamp received from servers (as is done in traditional
implementations), instead a client can choose the f + 1’st
largest timestamp. This guarantees that the timestamp selected was actually written by a correct client.
A new technique that we use in addition to non-skipping
timestamps is writeback on behalf of a writer. in which the
identity of the original writer is included in the writeback.
It turns out that this is especially important in the presence
of Byzantine failures. It allows us to use arbitrary quorums
to write a value instead of writing all values to all servers
as is done in [1]. It is conceivable that these tools will be
useful in other settings.

C.2.4 [Distributed Systems]: Distributed Applications;
D.4.2 [Storage Management]: Distributed memories

General Terms
Algorithms, Reliability, Security

Keywords
Byzantine Storage, Replication, Fault Tolerance, Timestamps

1.

RESULTS

We study the problem of implementing a replicated data
store in an asynchronous system with an unbounded number
of clients in which servers are subject to Byzantine failures
and we seek to provide atomic access semantics for non selfverifying data.
Implementations of shared registers with atomic semantics on servers subject to Byzantine failures have been proposed by a number of researchers. These solutions typically
use Byzantine quorum systems and provide safe semantics.
To provide stronger semantics, classical implementation of
atomic registers can be used if the number of clients is
bounded. Martin et al. [1] presented the first solution that
provides atomic semantics for non self-verifying data in an
asynchronous system with an unbounded number of readers
and writers and in which servers are subject to Byzantine
failures.
Our solution is a significant improvement over previously
proposed solutions: (1) In contrast to all previous solutions,
the size of the timestamps in our solution is bounded by the
number of operations and cannot be made arbitrarily large
by Byzantine servers as is the case for all previous solutions;
(2) It requires O(max|Q|), |Q| ≤ n, space to be used by
readers compared to O(nf ) space required in [1]; (3) In [1],
each writer writes to all servers in the system which results
in a write load equal to 1. In our solution, both read and
write are to general Byzantine quorum sets which enable us
to use quorums with low load and achieve
a load for write
q

2. ACKNOWLEDGMENTS
This work is supported in part by the National Science
Foundation CAREER award CCR-9876052 and CCR-9988404

operation that is less than or equal to 4fn+1 . If n > 4f +1,
the load is smaller than the load obtained in [1]; and, (4)
Our solution allows readers to short-circuit the forwarding
of late writes by notifying servers of the exact data values
they need to complete their read.

3. REFERENCE
[1] J.-P. Martin, L. Alvisi, and M. Dahlin. Minimal Byzantine
storage. In Distributed Computing, 16th international Conference, DISC 2002, pages 311–325, October 2002.

Copyright is held by the author/owner.
PODC’04, July 25–28, 2004, St. John's, Newfoundland, Canada.
ACM 1-58113-802-4/04/0007.

400

Auto-FBI: A User-friendly Approach for Secure Access to
Sensitive Content on the Web
Mohsen Zohrevandi

∗

Rida A. Bazzi

Arizona State University
699 S. Mill Ave.
Tempe, AZ, USA

{mzohreva, bazzi}@asu.edu
ABSTRACT

all active content for example (which will also diminish
the browsing experience) they do not solve the problems
introduced by these convenience features. The possibility
that a browser process is compromised by malicious content
increases with the usage of the process. The more sites are
visited, the more likely it is that malicious content is running
in the browser.
In this paper we are concerned with the protection
of the content associated with sensitive websites, such
as websites of banks or other ﬁnancial institutions.
Compromising access to such websites can have direct
ﬁnancial implications. We consider both passive compromise
in the form of information leakage and active compromise in
the form of unauthorized access through cross site request
forgery [1] for example, amongst many other active attacks.
Our approach is to automate what an extra careful user
would do. A careful user can manually improve security
through strict compartmentalization of access. At the
extreme, a user can have a computer system dedicated to the
access of one website, for online banking for example, and
nothing else. A less extreme, but still quite cumbersome
approach, would require the user to delete all saved data
(history, cookies, passwords for example) after each access
and start a new fresh browser instance (FBI) for every new
access. We call this approach fresh instance browsing. For
added protection, the most restrictive privacy settings are
also used if they do not interfere with site functionality. A
less extreme approach would have the user use one machine
but multiple browsers; one browser (Firefox for example) can
be used for everyday browsing and one browser (Chrome for
example) for browsing sensitive websites (following FBI).
While such an approach can indeed signiﬁcantly increase
security, it is not practical as most users would not be
willing or even able to go through the hassle it involves.
The common user pays for convenience features through
increased vulnerability. The advantage of using multiple
browsers to protect against a variety of attacks has long
been folk wisdom and has been established by Chen et al. [5]
who propose to implement the isolation and entry-point
restrictions (obtained from using multiple browsers) inside
one browser. They implement their approach by modifying
the Google Chrome browser.
In this paper, we also
argue that using multiple browsers can signiﬁcantly increase
security, but we take a completely diﬀerent approach
in achieving a user-friendly multiple browser experience.
Instead of providing the user with one browser that
implements isolation internally, we automatically launch
an FBI whenever the current browser instance should not

We propose a novel and simple approach for securing
access to sensitive content on the web. The approach
automates the best manual compartmentalization practices
for accessing diﬀerent kinds of content with diﬀerent browser
instances. The automation is transparent to the user and
does not require any modiﬁcation of how non-sensitive
content is accessed. For sensitive content, a Fresh Browser
Instance (FBI) is automatically created to access the
content. Our prototype system Auto-FBI can provide
support for novice users with predeﬁned sensitive content
sites as well as for more experienced users who can deﬁne
conﬂict of interest (COI) classes which allows content from
sites in the same user-deﬁned class to coexist in a browser
instance. Our initial performance evaluation of Auto-FBI
shows that the overhead introduced by the approach is
acceptable (less than 160 ms for sites that already have fast
load time, but for slow sites the overhead can be as high as
750 ms).

1.

INTRODUCTION

Content delivered through web browsers continues to be
a major vector of attack in personal computing systems.
A large variety of attacks [1, 14, 15, 19, 17] exploit what
might be termed convenience features of browser content.
Executing scripts, such as JavaScript [8], in web browsers
enable a richer browsing experience and enabling cookies [11]
allow for the convenience maintaining a state across multiple
pages. Unfortunately, convenience features also enable
richer attack modalities. The tension between providing
richer content and providing security is real and it is not only
due to technical reasons; the more features a system has, the
less likely it is for a novice, or even for an experienced user,
to get the system settings right. Default browser security
settings do not do much to mitigate the situation. Unless
security settings are draconian in nature, such as disabling
∗Contact author

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Request permissions from Permissions@acm.org.
ACSAC ’13 Dec. 9-13, 2013, New Orleans, Louisiana USA
Copyright 2013 ACM 978-1-4503-2015-3/13/12 ...$15.00.
http://dx.doi.org/10.1145/2523649.2523683

349

be used to handle the requested website. The launched
browser need not be the same as its launcher; the launched
browser can be Firefox while the launcher can be Chrome,
for example.
We recognize that not all users are equally savvy in setting
security preferences. For novice users, we have a very simple
default usage model that allows safe browsing for sensitive
sites and does not change the way users access non-sensitive
sites. For more experienced users, we have a more nuanced
usage model that allows users to deﬁne simple yet powerful
security preferences to allow them to access multiple sites
in the same conﬂict of interest (COI) instance. This is
addressed in more details in Section 3.

access to sensitive content through other kinds of attacks
(such as phishing [16]). This protection is due to the strong
compartmentalization of access to sensitive content. On the
downside, our approach does not provide ﬁne-grain control
and, for the novice user case, can be considered restrictive
in its limitation of access to third-party content through
sensitive sites. The security guarantees are discussed in
Section 5.
The main contributions of the paper are the following:
1. We identify and introduce the automatic fresh browser
instance approach (Auto-FBI) as an eﬀective, easy to
use, and general approach for providing secure access
to sensitive content for novice users. Unlike other
approaches, our approach is tolerant of privacy as well
as integrity attacks while being browser-independent.

Auto-FBI.
We have developed a prototype system called Auto-FBI
for 64-bit Linux. It works with Google Chrome and Mozilla
Firefox. The prototype is mostly written in C except a small
part that is written in JavaScript (for Chrome and Firefox
extensions).
Our prototype system Auto-FBI implements our approach
using a number of security techniques. It uses system-call
interception to restrict connections to web servers. This
requires tracing all the browser’s system calls. Auto-FBI
uses a simple browser extension to obtain the URL that
the browser is attempting to access (we give a detailed
justiﬁcation of this approach in Sections 4 and 5). In order
not to interfere with browser functionality, sites for which
access is restricted for a given instance are serviced through
a combination of DNS-query rewriting and replacement of
restricted pages with default pages. In order to keep track
of all active instances and their COI classes, a daemon
outside the browser keeps track of active instances and
launches new instances as needed. To communicate the
URL of the website being accessed to the daemon, an
intermediate process (browser-dependent, written in C)
forwards messages between the browser extension and the
daemon. Finally, to avoid buﬀer overﬂow attacks, URLs
are read using constant-bounds while loops. While these
techniques are individually not new, we believe that we are
using them together in a novel way to achieve our objectives.
Details of Auto-FBI are given in Sections 4 and 6.
We measured the amount of overhead incurred by our
system using a benchmarking software from the Chromium
project to compare the amount of time it takes to load a
webpage with and without our system. As illustrated in the
performance evaluation section later on, the incurred delay
is not perceptible to the user for most sites, so we argue that
this security scheme that we are proposing is practical.

2. We generalize the basic approach to a COI-based
Auto-FBI approach that aﬀords experienced users
more ﬂexibility and allows them to tailor it to their
speciﬁc needs.
3. We provide a prototype implementation, Auto-FBI,
that is the ﬁrst implementation that we are aware
of that provides automatic support for separating
accesses to sensitive sites from those to other sites
while requiring almost no modiﬁcation of how the user
uses the browser.
The rest of the paper is organized as follows. Section 2
introduces the system and attack model. The usage model
from user’s perspective is discussed in Section 3. Section 4
introduces the basic Auto-FBI’s design and Section 5
discusses the system security guarantees. Section 6 presents
Auto-FBI’s implementation details and Section 7 presents
the performance results. Related work is presented in
Section 8. Section 9 concludes the paper.

2.

SYSTEM AND THREAT MODEL

We identify the relevant components of the system. The
system consists of a client side and a server side. Web
clients typically run in web browsers. Web browsers retrieve
web content delivered by web servers. Web servers and web
content have origins associated with them. We consider an
origin to be a principal (whether or not it is authenticated).
Web content is acted upon by the web browser. Content
might be active which requires the browser to execute some
code to process it. We consider a browser that processes
content from a given origin to be a delegate of the origin.
Web content is retrieved using web queries. A query can be
a simple query that speciﬁes the address of the content in the
form of a URL (uniform resource locator) which consists of
a domain name and ﬁle path or an active query that consists
of the above and parameters that are passed to server-side
code – scripts – that use the parameters to process the
query. A web server might require credentials from a web
browser in order to authenticate the principal requesting
queries. Credentials are typically provided when initial
access is requested, typically in the form of a login id and
password. Once credentials are approved, a cookie is stored
by the browser to be used for future authentication. At that
point the browser acts as a delegate of the authenticated
principal.
An instance of a web browser is a software process that
executes the browser code and has associated with it a proﬁle

Contributions.
Unlike most other approaches to browser security whose
aim is to protect against information leakage and are
based on information ﬂow control [2, 6, 7], our approach
provides protection against information leakage (privacy
protection) as well as protection against some active attacks
(integrity threats). Cross site request forgery (CSRF) [1]
and clickjacking (UI redress) [18] attacks are prevented with
our approach as far as access to the sensitive websites is
concerned. We emphasize here that we are not claiming that
our approach prevents all CSRF and clickjacking attacks.
We are claiming that no unauthorized access to sensitive
content can be done using these attacks. We do not rule out

350

which includes information such as preferences, browsing
history, cookies, bookmarks, and saved passwords. A proﬁle
captures an execution state of the browser. When a browser
exits, some information might be deleted from the proﬁle,
non persistent cookies for example, but other information
in the proﬁle is saved. When a browser is restarted after
exiting, the proﬁle provides continuity between the old
browser process and the new process.
Given that a browser instance can be processing content
from diﬀerent sources, and therefore is a delegate for these
sources, it is important that these various roles do not
get mixed together in the browser. For example, a script
running in a browser on behalf of one source should not
be able in general to read data associated with another
source in the browser – such as passwords being entered in
a form. Such restrictions are expressed as security policies
and are enforced by the browser. Unfortunately, given the
great ﬂexibility provided by the browser, scripts have the
ability to obtain information indirectly through attempted
access to forbidden resources and to communicate directly
and indirectly with their origins. This ﬂexibility makes it
hard to enforce policies and even when policies are correctly
enforced, they might still allow undesired behavior. Single
Origin Policy (SOP) is an example of a policy that can be
circumvented rather easily [12, 15]. In general, a security
policy restricts what actions can be done by the various
system processes. Abstractly, one can think of a policy as a
set of allowed system executions. A policy for a web browser
expresses what executions of the browser are allowed and
which executions are not allowed as seen by an external
observer. In other words, a policy on a web browser is a
restriction on the input/output behavior of the browser.
A browser is susceptible to an attack relative to a
security policy if some browser executions do not satisfy
the security policy. We deﬁne attacks not in terms of the
speciﬁc technical modalities to achieve them, but in terms
of what can be achieved. In general, an attack requires
the browser to get and process content from one or more
attack origins. In describing an attack we assume that
there are at least three classes of origin: HIGH, LOW, and
AUTH (authenticated). We assume LOW is disjoint with
both HIGH and AUTH, but HIGH and AUTH need not be
disjoint. The following are attack models we consider in this
paper.

Figure 1: A request is made to www.bankofamerica.com (top
ﬁgure). Instead of displaying the requested page in the current
instance, Auto-FBI displays a message (bottom ﬁgure)

emphasizes the attacks capabilities rather than the minutiae
of the attack mechanisms.
In this paper we assume that a browser cannot be
compromised in a way that enables it to access system
resources it is not supposed to access. For example, we
assume that the browser will not arbitrarily read and write
ﬁles it is not supposed to access.

3.

USE MODEL

In this section, we describe Auto-FBI from a user’s
perspective without consideration to system design or
implementation details. We have two implementations, one
for novice users and one for experienced users.

1. Information Leakage (Leakage): An attacker is able to
launch an information leakage attack if a LOW origin
is able to receive content from a HIGH origin through
the browser.

Novice User.
A security mechanism has a better chance to be eﬀective
if it is well understood and easy to use. This is especially
true for novice users. Our basic usage model is quite simple.
For a novice user there are two kinds of sites. There are
sensitive sites and everything else. Access to a sensitive site
is done through a dedicated FBI. Access to all other sites
is done through separate browser instance(s) as it would be
done without Auto-FBI. When a user attempts to access a
sensitive website by clicking on a link or entering a URL
directly in the address bar, an FBI is created to handle the
access. When the user ﬁnishes accessing the sensitive site,
the user can close the FBI or minimize it. If the user later
attempts to access the sensitive site from another browser
instance, a new instance is activated again, or, if there is an
existing active instance for the site, that instance is used to
process the request. For added security, the FBI is opened

2. Cross Site Request Forgery (CSRF): An attacker is
able to launch CSRF attack if requests to an AUTH
origin to which the browser is already authenticated
depends on content from a LOW origin.
3. Clickjacking (UI Redress): UI redress attack is similar
to to CSRF attack, but requires user participation by
interacting with user interface elements. It is typically
achieved by exploiting features of the user interface
to let a user interact with a target website when they
think that they are interacting with other content.
We realize that the way the attacks are described does not
necessarily follow the way they are typically described in the
literature, but we opted for a more general description that

351

# Common: includes Certificate Authorities, etc.
>
verisign.com
comodo.com
digicert.com
entrust.net
globalsign.com
tt.omtrdc.net
sb-ssl.google.com
# C1: Banking
>!
americanexpress.com
aexp-static.com
aexp.demdex.net
bankofamerica.com
usbank.com

Figure 2: The requested page with sensitive content is accessed
by a fresh instance of a diﬀerent browser (Chrome in the ﬁgure,
top window) while the browser that made the original request
(bottom window) only displays a warning message

# C2: E-Commerce
>C
amazon.com
ebay.com
ebaystatic.com
ebayimg.com

without an address bar, menus or tabs to emphasize that
the access is meant for only one site. This simple usage
model is preferable to a model that requires the user to deﬁne
security policies which is not uncommon in mechanisms that
are claimed to be transparent [6]. Access for a novice user
is illustrated in Figures 1 and 2

Figure 3: Example COI classes written by an experienced user

a type descriptor (that comes immediately after a > in the
sample ﬁle):

Experienced User.
For the experienced user, we generalize our FBI approach
to allow content from more than one domain to coexist
within a browser instance. The idea is to allow content
originating from a conﬂict of interest (COI) class to coexist
within a browser instance. This is essentially the chinese
wall policy [3] applied to content producers, which we view
as principals, and to the browser which we treat as a delegate
of the content producers (it executes code on their behalf).
As an example, an experienced user might deﬁne a COI class
that contains some speciﬁc ﬁnancial institutions and another
class that contains some speciﬁc news sources. In a given
browser instance, content from sources that belong to the
same COI class can coexist as separate tabs or as content
within the same page if a page links to content from multiple
sources in the same COI class.
An experienced user deﬁnes conﬂict of interest classes in a
classes ﬁle. An example ﬁle is shown in Figure 3. The ﬁle
consists of multiple sections, one for each COI class, but the
ﬁrst section speciﬁes common domains, such as certiﬁcate
authorities, that should be accessible by all COI classes.
Each section starts with a line that deﬁnes the browser to be
used followed by one or more lines that list the domains for
the COI class. Any domain that does not appear explicitly
in one of the COI classes or the ﬁrst section is assumed to
belong to an implicitly declared general COI class.
When a browser is launched by the user, it is associated
with the general class. If the user attempts to access a site
from a domain in a class other than the general class, then, if
there is no active browser instance associated with that class,
a new browser instance for that class is created to process
and display the site. If there is an active browser instance for
the class of the requested site, then the request is processed
and displayed in that instance. To allow for using diﬀerent
browsers for diﬀerent classes, e.g. using Google Chrome for
e-commerce and Mozilla Firefox for banking, we specify the
browser type rule that should be followed for each class with

• ’F’: Mozilla Firefox instance should be used
• ’C’: Google Chrome instance should be used
• ’=’: the launched browser instance should be the same
type as the launcher
• ’!’: the launched browser instance type should be
diﬀerent from that of the launcher
• ’_’: places no restriction on the type of the browser
The implicit general class has the implicit type descriptor
’_’, so both Google Chrome and Mozilla Firefox can be used
to browse websites from the general class.
The example ﬁle shown in ﬁgure 3 has two explicit COI
classes (we will refer to them as C1 , and C2 as shown
in the comments). The ﬁrst class C1 has type descriptor
’!’ and the second class has type descriptor ’C’. To
demonstrate the user experience, let’s consider the following
scenario: user opens Google Chrome to start browsing, an
instance of Google Chrome of the general class is created.
The user can navigate to any site that is not listed in
C1 or C2 in that browser instance. If the user tries to
navigate to www.bankofamerica.com which is listed in C1 ,
a fresh instance of Mozilla Firefox will be created to handle
the request. That is because the type descriptor for C1
speciﬁes that an instance with a diﬀerent type from that
of the launcher (Google Chrome in this example) should be
used. If the user tries to access www.facebook.com from the
Mozilla Firefox instance, the request will be handled by the
existing Google Chrome instance of the general class. That
is because the general class has the implicit type descriptor
’_’ which allows the system to use the available instance. If
the user navigates to www.ebay.com from any of the previous
instances, a new instance of Google Chrome will be launched

352

to handle the request regardless of the browser type used to
initiate the request.

4.

SYSTEM DESIGN

The basic functionality that we are interested in is
an enforcement mechanism with the ability to prevent a
browser instance from communicating with sources that
are not in its COI class and an enabler mechanism for
starting FBI’s as needed. We start by discussing the design
alternatives.

4.1

Design Alternatives

There are essentially three ways to provide the proposed
functionality: (1) inside the browser through modiﬁcation of
the browser code, (2) as a browser extension, or (3) outside
the browser. We argue that accessing sensitive sites should
be enforced through an enforcement mechanism that resides
outside the browser and that launches FBI’s. We brieﬂy
discuss each of the design options:

Figure 4: Execution scenario for handling access to content from
a diﬀerent COI class

implementing part of the enabler mechanism inside
the browser. In fact, for technical reasons that we
explain later in this section, this is the approach we
take. Compromising the enabler mechanism would not
allow an attacker to access sensitive content through
the wrong browser instance.

• Enforcement inside the browser: The desired
functionality could be provided by modifying the
browser to add the desired enforcement rules. This
approach is highly browser-dependent and requires
detailed understanding of the browser’s code to be
implemented. Also, it is not easily maintainable
for new versions of a browser and requires diﬀerent
implementations for diﬀerent browsers. Furthermore,
if there are bugs in the implementation, it can
compromise the browser’s security.
We do not
recommend this approach because it does not provide
a clean separation between the browser’s code and the
enforcement mechanism.

4.2

System Components

The system has two main components. One component,
enforcer, intercepts system calls and modiﬁes them as
needed to ensure that sensitive content is only accessed
through the appropriate instance of the browser. Another
component, enabler determines when sensitive content is
requested and, if needed, launches a new fresh instance of
the browser to access the content. These two components
are independent and do not communicate with each other.

• Enforcement in a browser extension: Providing
the desired enforcement as a browser extension is an
improvement over enforcement inside the browser. In a
browser like Chrome, extensions can be a combination
of JavaScript and native code that is controlled
by JavaScript. This is not advisable because the
JavaScript engine is potentially exposed to malicious
scripts from various sources. Any compromise of the
engine through such malicious scripts can compromise
the enforcement mechanism.

4.2.1

Enforcer

Intercepting System Calls.
To intercept system calls, we employed the ptrace API
available in Linux. This API allows one program (tracer)
to be notiﬁed and take control of every system call made by
another program (browser). The kernel puts the browser
to sleep when it makes a system call and notiﬁes the
tracer. The tracer can then modify the contents of process
virtual memory or registers and then let the browser resume
execution. When the system call gets executed and returns
from the kernel, and before the control gets back to the
browser, the kernel notiﬁes the tracer once more. At
this point the tracer can also modify the return value of
the system call. Using this API enables us to control
every interaction between the browser and the operating
system. The ptrace API is also used by many sandboxing
applications to control system access.
Among hundreds of system calls available in Linux, we
are interested in the following calls: connect, recvfrom and
read. The browser issues a connect system call when it
needs to make a network connection, and it uses recvfrom
or read to receive data from a network host.

• Enforcement outside the browser: Enforcement
outside the browser can be achieved by intercepting
the browser’s system calls and only allowing those
calls that are according to policy. This has the
advantage of separating the enforcement mechanism
from the browser’s code. In our implementation, the
enforcement mechanism is around 2K lines of code
and is browser independent. Adding that code as
an extension would require writing diﬀerent versions
for diﬀerent kinds of browsers. Also, enforcement
outside the browser instance obviates the need to make
additional restrictive assumptions about the power
of the attacker. So, instead of adding an executing
browser to the trusted computing base (TCB), we
only consider an FBI to be part of the TCB to access
sensitive content. The possibility that such an instance
is compromised is smaller than that of an executing
instance. We elaborate on this point in Section 5.
Enforcement outside the browser does not preclude

Redirecting DNS Queries.
When a browser makes a DNS query, the tracer checks
whether or not the requested network name is allowed to
be accessed by the current browser instance. If it is not

353

allowed to be accessed, the tracer changes the IP addresses
in the DNS response to a predeﬁned IP address which is the
loopback address in our implementation. This prevents the
browser from connecting to the restricted website. At the
same time, the attempted access is detected by the enabler
mechanism of Auto-FBI and a new instance is launched as
needed.

an existing browser instance by sending a message to the
extension running in that instance or open a fresh instance
in case an appropriate instance is not running.
As we discussed in the previous section, having part of
the enabler mechanism in the browser does not aﬀect the
security of our solution. Even if a browser is compromised
and the extension is also compromised, all that would aﬀect
is the URL communicated by the extension, but that has no
eﬀect on the tracing of system calls and blocking them. That
part is independent and is enforced through a mechanism
that is completely outside the browser by the Enforcer.
So, compromising the browser might aﬀect the progress
requirement of the browser, but does not aﬀect the safety
requirements of the enforcement.

Intercepting IP Communication.
The IP addresses used by the browser are not always
preceded by DNS queries. In fact for security reasons, IP
addresses obtained from DNS queries are typically cached
(IP pinning [13]) by browsers even if the TTL from the DNS
response is set to 0. We distinguish between three kinds of
IP addresses that can be used by a browser and for each
kind we explain how it is handled.

5.

• IP addresses that are cached by the browser and
obtained from a previous DNS query. IP addresses
that are obtained through DNS queries are cached by
Auto-FBI in a whitelist if the original DNS query is
not redirected. A later use of such an IP address is
compared against Auto-FBI’s whitelist of IP addresses.
If the IP address is not on the whitelist, it is blocked.
• Browser hardcoded IP addresses. We do not allow the
use of any hard-coded IP addresses by the browser.
We consider hardcoded addresses to be suspicious and
a security risk.
• Hand-coded IP addresses. IP addresses that are
directly entered by the user have legitimate usages
such as setting up some LAN services (NAT setup for
example). Such usages should be allowed. As a policy
one can allow some LAN IP addresses and associate a
class with each allowed address.
It is interesting to note here that associating a separate class
with LAN addresses can prevent DNS rebinding attacks
which are not completely solved by DNS pinning [13].
Figure 4 illustrates a scenario in which a browser instance
whose COI class is diﬀerent from that of the website it
is trying to connect to. First it makes a DNS query to
resolve the IP address (1), the DNS response is detected by
the system (2) and the requested domain name is checked
against the list of COI classes (3). Since the browser is
running in a diﬀerent class, the system rewrites the IP
address in the DNS response (4) and returns the result to the
browser. The browser would not have access to the actual IP
address for www.bank.com and hence it is not able to connect
to it. The browser could not communicate directly with
www.bank.com using the actual IP address of www.bank.com
because that IP address of www.bank.com is not added to
the whitelist of the current instance.

4.2.2

SECURITY GUARANTEES

Arguing for the security of our approach is based on the
fact that all access to content from one COI class is not
allowed in a browser instance associated with another COI
class. This is enforced by restricting communication to
IP addresses that are on the whitelist for the COI class
of the executing browser instance. The IP addresses on
the whitelist of one class can be one of the following: (1)
IP addresses corresponding to a domain from the common
domain list, or (2) IP addresses of a domain in the COI class.
So, if DNS is not compromised, only content from sources in
a given COI class can run on a browser instance associated
with that class. For sensitive content whose servers are
typically authenticated, the assumption on DNS not being
compromised is not needed and the system provides the
desired isolation that ensures that there is no leakage of
sensitive content between various instances (assuming the
authentication mechanism is secure). This means that
attacks such as CSRF or Clickjacking cannot be used to
access sensitive content. In fact, content (scripts) used to
launch such attacks are assumed to belong to COI classes
that are not part of the sensitive COI classes. This is
especially evident in the novice user scenario. For the
experienced user, a non-judicious choice of classes can pose
a security risk.

6.

IMPLEMENTATION DETAILS

The prototype is composed of two separate modules: (1)
Enforcer and (2) Enabler. These modules are sometimes
referred to by diﬀerent names: we use tracer to denote the
enforcer module and daemon to refer to the main component
of the enabler module. The implementation is entirely
in user space and no modiﬁcation to the Linux kernel is
required.

6.1

Enforcer

Using Linux ptrace API, we can trace every system call
made by a child process to keep track of the application’s
access to system resources including network and ﬁle system.
This technique has been employed by sandboxing systems to
limit the sandboxed program’s access to system resources.
However, this method inherently suﬀers from the Time Of
Check Time Of Use race conditions.
There are two options for a tracer to start tracing the
tracee: (1) TRACEME and (2) ATTACH. The ﬁrst option is
usually used in the following way: the tracer forks a new
child process and in the child process calls ptrace with

Enabler: Launching Fresh Instances

To manage browser instances running in the system and
launching new instances, we implemented a daemon which
keeps track of all browser instances. The information on
which new instances need to be launched comes from the
extension component. The extension reports all URLs
requested by the user to the daemon and also listens for
incoming messages from the daemon in which case it will
open the URL in a new tab. The daemon decide how to
handle the URL sent by the extension: either open it in

354

TRACEME command, which causes it to be traceable by
its parent process (the tracer), then it runs the target
program by calling exec. The second option can be used
to trace an already-running-process by attaching to it. The
enforcer uses the TRACEME option by default (including the
implementation for Firefox), but due to Chrome’s use of the
SETUID access ﬂag in its sandboxing process1 , the enforcer
uses the ATTACH option with Google Chrome. Using ATTACH
leaves an enforcement gap: from the time that tracer
launches Chrome until the tracer attaches to the browser,
the browser is not traced. This should not be a source
of vulnerability if the default home pages are safe. Using
ATTACH means that part of the enforcer that is involved in
setting up the tracing is browser dependent, but the rest
of the enforcement when the ATTACH option is used and the
whole enforcement using the TRACEME option are generic and
not dependent on the browser.
The ptrace API allows one to trace a process and all its
child processes and threads by specifying certain options:
PTRACE_O_TRACEFORK and PTRACE_O_TRACECLONE. So we can
make sure that a process cannot escape the tracer by
creating a child process or a new thread.
For every system call made by the tracee, the tracer is
notiﬁed twice: once before the call is handed over to the
kernel for execution and once after the call returns from
the kernel but before it is handed back to the tracee. So
the tracer has two chances to modify a system call: it can
change the parameters provided by the tracee to the kernel,
or change the values returned by the kernel. To restrict
the browser’s access to websites, we need to monitor the
following system calls:

in the DNS message, we do so by writing to the tracee’s
virtual memory3 .

6.1.2

6.2
6.2.1

Enabler
Browser Extension

All browser instances are equipped with an extension that
is responsible for two tasks: (1) report all user-requested
URLs to the daemon and (2) open a new tab for URLs
requested by the daemon. When the extension reports a
URL to the daemon, the daemon decides if the URL should
be handled by the reporting instance or by another instance.
We have developed separate JavaScript extensions for
Mozilla Firefox and Google Chrome4 . Since the extensions
cannot directly communicate with the daemon, we have
a native application written in C that is responsible for
forwarding messages between the daemon and the extension.
The native application communicates at one end with
the browser extension through the browser API (Native
Messaging API in case of Google Chrome and js-ctypes
in case of Mozilla Firefox) and it communicates with the
daemon on the other end through Unix Domain Sockets.
The native application’s process is a child process of the
browser instance and hence can be considered a part of the
browser extension module. It is interesting to note that
the code size of the extension (and its associated native
application) is small compared to that of the enforcer:
around 3K lines of code for the enforcer and daemon vs. 460
for the chrome extension and 280 for the Firefox extension.

• connect: Connects a socket ﬁle descriptor to a network
address. It is also used for IPC.
• rcvfrom: Receives data through a socket ﬁle descriptor
• read: Can be used to receive data from the network
(through a socket ﬁle descriptor)

6.1.1

IP White-list

To prevent the browser from connecting to a restricted
website by using a hard-coded IP address, we implemented
a white-list of IP addresses that the browser is allowed to
connect to. Initially, it is populated with the DNS service
IP address (usually the local DNS client, 127.0.0.1) and a
list of pre-deﬁned IP addresses (to allow for LAN addresses).
When the browser makes a DNS query, one of the following
happens: either the response is changed to prevent access
in case the URL is not allowed or the IP addresses in the
response are added to the white-list. When the browser
issues a connect system call, we would only allow it to go
through if the IP address is in the white-list.

DNS Queries

To prevent the browser from accessing certain websites,
we keep track of the DNS queries made by the browser to
ﬁnd out if it is trying to connect to a restricted website. We
can prevent the access by rewriting the IP addresses in the
DNS response to point to a predeﬁned address.
In order to accomplish this, we need to process the
following system calls: rcvfrom and read. If the port
number2 matches the port number of the DNS protocol (53),
we read the IO buﬀer from the tracee’s memory and analyze
the DNS message. If we decide to rewrite the IP addresses

6.2.2

Daemon

The daemon is a multi-threaded C program that uses Unix
Domain Sockets to communicate with browser extensions. It
is responsible for deciding how to handle restricted URLs by
either dispatching it to an already-running instance that is
allowed to access the URL or creating a fresh new instance.
For every browser instance running in the system, it has a
separate thread to communicate with the instance. It keeps
a record for each running browser instance along with a
queue of outgoing messages for that instance in a shared
data structure. When it receives a message from a browser
extension, it chooses to either send a message to the browser

1

Using TRACEME with Chrome would either require root privileges
for running the browser which is not desirable or it breaks the
browser’s internal sandboxing mechanism which uses the SETUID
access ﬂag to allow its sandbox process to temporarily run with
root privileges. The kernel does not allow a non-root process to
trace a process with root privileges using the TRACEME option. In
our solution, the enforcer waits until the privilege level is dropped
after the sandbox is setup and then then attaches to the main
browser process using the ATTACH option of ptrace.
2
We have used /proc to ﬁgure out the remote port number of a
socket. Alternatively, we could keep track of all sockets owned by
the tracee and the port number speciﬁed during the connect call,
but this method is error-prone. See [9] for a detailed discussion
on why one should avoid replicating the kernel state.

3
We have used process_vm_readv and process_vm_writev
system calls to read/write data from/to the tracee’s virtual
memory which is much more eﬃcient than using ptrace for this
task.
4
Chrome and Firefox have diﬀerent APIs for writing extensions:
Google Chrome provides chrome.* API and Mozilla Firefox has
Add-on SDK as part of the Jetpack project

355

extension in one of the running browser instances or run
a fresh instance. It compares the COI class of the URL
contained in the incoming message with that of the sender.
If they match, it ignores the message, otherwise it decides
how to handle the message according to the classes ﬁle.

6.2.3

not exactly match user proﬁles employed by browsers.
Diﬀerent browser instances even with diﬀerent types might
still be able to share some information with each other
although they don’t share the same user proﬁle. Local
shared objects (LSO for short) are among the methods that
can be used for such purposes6 . Our implementation does
not address the problem of isolating such objects among
diﬀerent browser instances.

Browser Instances and User Proﬁles

A browser instance, as deﬁned in section 2, is associated
with a proﬁle that captures its execution state. In practice,
most browsers support user proﬁles to enable the multi-user
use case. User proﬁles are very similar in diﬀerent browsers:
a folder stored somewhere in the user’s home directory that
contains information such as the user’s web history, cookies,
cache, and preferences. Fortunately, most browsers also
provide a way of using multiple proﬁles for a user in addition
to the user’s default proﬁle. In case of Mozilla Firefox and
Google Chrome, creating a fresh instance of the browser
is as easy as creating an empty directory and instructing
the browser to use that directory to store the user proﬁle
(though this would be cumbersome to do manually).
In our implementation, the default user proﬁle (the one
managed by the user) is associated with the general COI
class. Other COI classes do not have a persistent proﬁle: a
temporary proﬁle is created whenever the instance is created
and it gets deleted after the instance is closed.
We think this way of managing user proﬁles is both usable
and secure. User spends most of the time using the default
proﬁle since most URLs fall into the general COI category,
hence allowing the user to manage that proﬁle improves
usability. Using temporary proﬁles for other COI classes
ensures tighter security without much eﬀect on usability.

6.3

7.

PERFORMANCE EVALUATION

We measured the amount of overhead incurred by the
tracer module, by running the benchmarking extension
from the Chromium project for a list of 20 most-visited
websites. Table 1 shows the comparison of page load times
for these websites for Chromium browser version 25 executed
with and without the tracer. The benchmarking extension
provides the ability to measure the load times with and
without caching, which are shown in the table. For every
URL the following 4 combinations are tested, each one
averaged over 50 iterations:
1. Chromium running on its own and caching disabled
1
)
(Tnorm
2. Chromium running with the tracer and caching
1
)
disabled (Ttracer
3. Chromium running on its own and caching enabled
2
)
(Tnorm

Implementation Limitations

4. Chromium running with the tracer and caching
2
)
enabled (Ttracer

This section discusses some of the limitations of our
current implementation.
These are limitations of the
implementation and not of the approach.
We have implemented the enforcer on 64-bit Linux system
using the ptrace API. Working with ptrace API is highly
architecture dependent, so porting the program to 32-bit
Linux for example would require taking care of word size
diﬀerences, system call diﬀerences, etc. Porting to other
platforms or other operating systems might not be possible
without major changes. Another limitation of the enforcer
is that it does not support IPv6.
We developed the browser extension only for Chrome
and Firefox. However, all major browsers support some
form of plugin API that allows one to extend the browser
functionality. We believe that the same functionality can be
achieved for other browsers as well.
Another limitation is the restricted format of class
deﬁnition ﬁle. A more powerful format can allow complex
matching expressions for domain names. However, the
format we chose was suﬃcient for our testing purposes.
The daemon process, which is responsible for opening
URLs in fresh browser instances, could perform sanity
checks on URLs to achieve higher security. For example
it could remove the query part 5 of the URL. This was not
implemented in our prototype.
Our abstract deﬁnition of proﬁle given in section 2 does

Then the overhead is calculated by the following formula:
1
1
− Tnorm
ΔT 1 = Ttracer

ΔT 1 % =

ΔT 1
∗ 100
1
Tnorm

And similarly for ΔT 2 . The last three rows of the table show
the average over diﬀerent sets of URLs. Average for fast sites
shows the average for those sites that have Tnorm ≤ 1000
and Average for slow sites shows the average for those sites
with Tnorm > 1000. The last row shows the average over all
20 websites.
Although the relative overhead is large (about 30 %), the
absolute delay is not signiﬁcantly perceptible to the user
and we argue that the overhead is acceptable. In fact for
fast sites, the average overhead is less than 250 ms which in
absolute terms is not signiﬁcantly perceptible. For slow sites,
the overhead is higher, but the relative overhead is lower.
While we think that the overhead of the initial prototype
is acceptable, we believe that further improvements to the
overhead are needed.
We should note that due to variations in data transfer
rates that are unavoidable, the overhead values are negative
in a few cases.

5
An optional part in URLs that comes after a question mark
in the URL and is usually in the form of key value pairs. For
example in http://www.example.com/test.jsp?x=1&y=2, the last
part (?x=1&y=2) is the query part.

6

LSOs are similar to HTTP cookies. Websites which use Adobe
Flash may store such objects on a user’s computer. They can be
shared among browsers that use the Flash Player plugin.

356

Website URL
www.youtube.com
www.yahoo.com
login.live.com
www.msn.com
en.wikipedia.org
blogsofnote.blogspot.com
www.baidu.com
www.microsoft.com
www.qq.com
www.bing.com
www.ask.com
www.adobe.com
www.taobao.com
twitter.com
www.youku.com
www.soso.com
wordpress.com
www.sohu.com
www.hao123.com
www.facebook.com
Avergage for fast sites
Average for slow sites
Overall (20 URLs)

1
Tnorm
800.6
953.3
701.1
368.9
2101.3
522.1
1433.3
1269.5
6430.9
184.1
952
1394.6
2435.7
401.6
6427.5
947.1
601.4
5601.8
4598.4
424.9
623.37
3521.44
1927.5

Caching Disabled
1
Ttracer
ΔT 1
ΔT 1 %
1249
448.4
56.01
890.1
-63.2
-6.63
817.8
116.7
16.65
772
403.1
109.27
2199.5
98.2
4.67
1016.7
494.6
94.73
954.6
-478.7
-33.40
2175
905.5
71.33
8179.3
1748.4
27.19
267.5
83.4
45.30
1184.5
232.5
24.42
2225.9
831.3
59.61
3286.1
850.4
34.91
593
191.4
47.66
5731.2
-696.3
-10.83
983.5
36.4
3.84
799.6
198.2
32.96
7031
1429.2
25.51
4600.2
1.8
0.04
791.7
366.8
86.33
851.4
228.03 36.58
4042.53 521.09
14.8
2287.4
359.9
18.67

2
Tnorm
696.1
869.7
655.9
367.2
823.4
355.8
848.5
1606.3
2717
172.6
891.8
974.8
1802.2
325.4
2009.1
647.4
370
4696.7
2276.5
411.7
600.74
2517.97
1175.9

Caching Enabled
2
Ttracer
ΔT 2
ΔT 2 %
1114.5
418.4
60.11
1133.3
263.6
30.31
498.4
-157.5
-24.01
613.2
246
66.99
1037.6
214.2
26.01
525.9
170.1
47.81
589.3
-259.2
-30.55
1827.9
221.6
13.80
3438.5
721.5
26.56
256.7
84.1
48.73
1034
142.2
15.95
1626.2
651.4
66.82
2625.4
823.2
45.68
447.9
122.5
37.65
2638.5
629.4
31.33
672.5
25.1
3.88
548.9
178.9
48.35
6596.9
1900.2
40.46
2441.5
165
7.25
534.7
123
29.88
759.51 158.77 26.43
3261.45 743.48 29.53
1510.1
334.2
28.42

Table 1: Performance evaluation for the tracer, times are in milliseconds

8.

RELATED WORK

much more changes to be made to the browser code base.
On the other hand, their approach provides two beneﬁts
compared to our method: they can isolate web applications
with ﬁner granularity than origins, namely for sets of URLs,
but in our approach we can not control the browser from
outside at that level of granularity. The other beneﬁt of their
approach is its performance overhead which is negligible
compared to ours.

There is a vast literature on approaches to counter web
attacks. We only summarize some of the most relevant
works that are also representative of other works. We
should also mention that private browsing modes provided
by many browsers e.g. Incognito mode in Google Chrome,
do not solve many of the problems addressed in our work.
The reason for this is that they only separate browsing
history and cookies from regular tabs. Moreover, there is no
automatic separation of sensitive content from non-sensitive
content in private browsing mode.

Multi-Layer MAC.
Hicks et al. [10] address the same problem with a diﬀerent
approach. They propose a multi-layer Mandatory Access
Control (MAC) system to enforce end-to-end security goals.
They implement a labeling system to achieve separation
of data with diﬀerent sensitivity. Their method requires
collaboration from both client and server and also requires a
modiﬁed browser to handle labeling and MAC enforcement.
While their system is quite complicated, it can provide
isolation at various levels of granularity. We argue that our
proposed system is more practical because of its simplicity
and small size while providing a coarse grained isolation of
sensitive data.

App Isolation.
Chen et al. [5] use an approach similar to ours for
browser security. They provide a mechanism inside a
single browser instance to isolate web applications from
each other. Their approach requires the participation of
web application providers to opt in and use their method.
Our approach does not require such participation and is
applicable to every website. Another major diﬀerence is
that our isolation technique works outside the browser and
we do not modify an already-large browser code base. This
makes the software product more maintainable and also
more trustable because of the smaller TCB size. While the
number of lines in our code base is comparable to theirs
(≈ 2K), their implementation depends on the Chromium’s
process architecture which already facilitates their job
by sandboxing the rendering process and using diﬀerent
renderer processes for diﬀerent origins when possible. If one
implements their idea for another browser that does not have
such process architecture (which currently includes almost
all other major browsers including Firefox), it would require

FlowFox.
De Groef et al. [6] proposed an information-ﬂow control
scheme based on secure multi-execution technique [7].
They modiﬁed SpiderMonkey, the JavaScript engine used
in FireFox and other browsers, to enforce information
ﬂow security policies. Their threat model is limited to
information leakage, which is signiﬁcantly weaker than the
threat model we consider. Their approach is also diﬀerent.
Their solution works at the browser abstraction level while

357

our solution works at the operating system abstraction level.
We argue that our approach is more ﬂexible and more
trustable because we do not introduce any code in the
browser code-base. Web browsers have typically very large
code-bases, and adding to this complex code might create
more vulnerabilities. Also, operating system abstractions
are more stable and have much less complexity compared to
browser’s internal abstractions.

[6]

Shadow Execution.

[7]

Capizzi et al. [4] addressed the information leakage
problem in the context of desktop applications. Desktop
applications might connect to network to get the latest
updates for example. Since such applications normally run
with user privileges, they have access to user’s sensitive
information and they might leak this information. The
authors propose a technique called shadow execution in
which they run two instances of the same application, one
with access to user private data but no network access and
another instance with no access to real user data but able
to access the network. As our method, their method works
at the operating system abstraction level, but their method
is not suitable for controlling browsers.

9.

[8]
[9]

[10]

CONCLUSION

We have shown that it is possible to provide eﬀective
protection against active and passive attacks on web
browsers. Unlike other approaches, we manage to have
a browser-independent mechanism to provide the required
execution isolation needed to access diﬀerent kinds of
content.
The overhead of the approach, while being
acceptable, is not ideal and more work is need to further
reduce the overhead.

10.

[11]
[12]
[13]

ACKNOWLEDGMENTS

[14]

We would like to thank the reviewers for many useful
comments. This work has been supported in part through
National Science Foundation grant CSR-0849980. The views
and ﬁndings of this paper do not necessarily reﬂect those of
NSF.
[15]

11.

REFERENCES
[16]

[1] A. Barth, C. Jackson, and J. C. Mitchell. Robust
defenses for cross-site request forgery. In Proceedings
of the 2008 ACM conference on Computer and
Communications Security, CCS’08, pages 75–88, 2008.
[2] N. Bielova, D. Devriese, F. Massacci, and F. Piessens.
Reactive non-interference for a browser model. In
Proceedings of the 2011 international conference on
Network and System Security, NSS’11, pages 97–104,
2011.
[3] D. F. C. Brewer and M. J. Nash. The chinese wall
security policy. In Security and Privacy, 1989.
Proceedings., 1989 IEEE Symposium on, pages
206–214. IEEE, 1989.
[4] R. Capizzi, A. Longo, V. N. Venkatakrishnan, and
A. P. Sistla. Preventing information leaks through
shadow executions. In Annual Computer Security
Applications Conference, 2008, ACSAC’08, pages
322–331, 2008.
[5] E. Y. Chen, J. Bau, C. Reis, A. Barth, and
C. Jackson. App isolation: get the security of multiple

[17]

[18]

[19]

358

browsers with just one. In Proceedings of the 18th
ACM conference on Computer and communications
security, CCS ’11, pages 227–238, 2011.
W. De Groef, D. Devriese, N. Nikiforakis, and
F. Piessens. Flowfox: a web browser with ﬂexible and
precise information ﬂow control. In Proceedings of the
2012 ACM conference on Computer and
Communications Security, CCS’12, pages 748–759,
2012.
D. Devriese and F. Piessens. Noninterference through
secure multi-execution. In Proceedings of the 2010
IEEE Symposium on Security and Privacy, SP’10,
pages 109–124, 2010.
D. Flanagan. JavaScript: the deﬁnitive guide. O’Reilly
Media, 2011.
T. Garﬁnkel. Traps and pitfalls: Practical problems in
system call interposition based security tools. In
Proceedings of the 2003 Network and Distributed
Systems Security Symposium, volume 33 of NDSS’03,
2003.
B. Hicks, S. Rueda, D. King, T. Moyer, J. Schiﬀman,
Y. Sreenivasan, P. McDaniel, and T. Jaeger. An
architecture for enforcing end-to-end access control
over web applications. In Proceedings of the 15th ACM
symposium on Access control models and technologies,
SACMAT’10, pages 163–172, 2010.
Internet Engineering Task Force (IETF). Request for
Comments: 6265.
Internet Engineering Task Force (IETF). Request for
Comments: 6454, 2011.
C. Jackson, A. Barth, A. Bortz, W. Shao, and
D. Boneh. Protecting browsers from dns rebinding
attacks. ACM Transactions on the Web (TWEB),
3(1):2, 2009.
D. Jang, R. Jhala, S. Lerner, and H. Shacham. An
empirical study of privacy-violating information ﬂows
in javascript web applications. In Proceedings of the
2010 ACM conference on Computer and
Communications Security, CCS’10, pages 270–283,
2010.
M. Johns. On javascript malware and related threats.
Journal in Computer Virology, 4(3):161–178, 2008.
E. Kirda and C. Kruegel. Protecting users against
phishing attacks. The Computer Journal,
49(5):554–561, 2006.
N. Provos, P. Mavrommatis, M. A. Rajab, and
F. Monrose. All your iframes point to us. In
Proceedings of the 2008 Security Symposium, SS’08,
pages 1–15, 2008.
G. Rydstedt, E. Bursztein, D. Boneh, and C. Jackson.
Busting frame busting: a study of clickjacking
vulnerabilities at popular sites. IEEE Oakland Web, 2,
2010.
Z. Weinberg, E. Y. Chen, P. R. Jayaraman, and
C. Jackson. I still know what you visited last summer:
Leaking browsing history via user interaction and side
channel attacks. In Proceedings of the 2011 IEEE
Symposium on Security and Privacy, SP’11, pages
147–161, 2011.

A Gap— Theorem

for Consensus

Extended

Gary

L.

Abstract

Rida

Peterson*

Spelman

College

whose

This

T,

cisely

determines

types

to solve

called

important

that

the

High

Gap

copies

for

This

Theorem,
a proof

multiple

pre-

and

the

[Her191]

shared

memory

wait-free

the

that

power

on the
and

combined

assumptions

whether

are permit
hierarchies
There

Jayant

are not

can

not

robustness

read/write
ted,

types

in classifying

made

power

about
shared

i was able

and

to

Robustness

means

separately

cannot

the

using

consensus

(i.e.,

to show

some

as

of types

its

objects

type

that

can

there

registers)

h&,

defined

by Jayanti

not
*Research
under

a grant

supported
to the

in part
Center

by the

for Scientific

W.

F. Kellog

Foundation

Applications

matics at Spehnan
College,
Author’s
address:
Computer
Department,
PO Box 333, Spehnan
College, 350 Spelman
Atlanta,
GA
t Re~emch

30314-0339.
supported

dation

grants

under

address:

College

Atlanta,
GA,
t Supported

by

of Computing,

the
and

Georgia

National

Science

CCR–9301454.
Institute

FourI-

a

scholarship

from

the

Hariri

the

Foundation

A

Permission to copy without fee all or part of this material is
granted provided that the copies are not made or distributed for
direct commercial advantage, the ACM copyright notice and the
title of the publication and its date appear, and notice is given
that copying is by permission of the Association of Computing
Machinery. To copy otherwise, or to republish, requires a fee
and/or specific permission.
PODG 94- 8/94 Los Angeles GA USA
@ 1994 ACM 0-89791 -654-9/94/0008.$3.50

of the

High

can

doesn’t

be used

have
consensus.

but

Gap

property,

objects

types

types

at lower

that

cannot

problems

that

can solve. ) Basic

Theorem

hierarchy

consensus

exist

types

can solve

of any

Therefore,

with

gaps

a

to solve

the

using

is
has

follow

easily:

is robust

for

deter-

deterministic
and

other

2–process
only

shared
issues

types

by

has

con-

cannot

Jayanti

type.

on different
results
therefore

that

type

objects

be

memory.

raised

based
We

its

memory

one of Jayanti’s
about

if

concerns

Different
notions

permitted
state

are used

re-

of type.
types

the

in this

to

following
paper

(see

[BNP94]):

(One
process

344

it

h~

can be obtained

An object

The

objects

of a shared

properties

●

be

Theorem,

If a type

are n-process

by read/write

For example,

●

types.

consensus

be non-deterministic.
also

Gap

consensus

2 if

definition

sults

High

n-process

(There

number

One
by

between

Jayanti’s

simulated

of Technology,

the

to

to consider

types.

sensus

Authors’

useful,

do (n – I)–process

to the

ministic

cannot

test–and–set

more

be simulated

all (n — 2)–process

Corollary.
in part

CCR–9106627

30332-0280.
in
part

Science
Ln SW,

its
If

can

1) process

Corollary.

but

it is common

and

called
then

in general.

corollaries

of Mathe-

as 3–valued

Note

can be accessed

While

n and n — 1. No comparable

do (n–

type.

consensus

of n–process

is a gap

numbers

consensus

that

[LA87].

proper,

consensus.

then

object

to indicate

the

of types.

property,

levels

one hierarchy,

such

result,

n–process

robust.

remained

types
it is more

access

h~

types.

a type

can

Then

implies

memory

3–process

consensus

copies
shared

consensus.
paper

discussing

that

can solve

main

certain

when

a type

read/write

of this

shared

For

multiple

wait–free

test–and–set

a characterization

Depending

variables

consider

Our

problem

properties

careful

that

with

result

of processes

it to be a family

question

main

resolved.

such

augmented

deterministic

4–process

one type,

introduced

solve

for

a 3-valued

solve

solve

was not
value

n–process

The

by 3 processes

for

several

a basic

types.

the

to

defined

[Jay93]

types.

they

to be used

their

concerns

or more

then

measure

formally

It

Jay ant i studied

problem

on

hierarchy

of two

a problem

together.
the

of

complexity

based

of robustness.

if objects

solve

a

Jayanti

of Herlihy’s

notion

on

types

consensus.

variations
the

defined

n.

number

that

T,

can solve
=

We are very

types.

question

of type

is robust

Introduction.

Herlihy

robustness

h~ (T)

Jayanti’s

Technology

let n be the smallest

read/write

deterministic

of

memory,

char-

has several
that

Neigert

of objects

deterministic

consensus.

including

is robust

that

of n–process

wait–free

allows

memory

characterization

ability

corollaries

hierarchy

1.

the

n–process

acterization,

shared

a strong

Bazzit$

Institute

Abstract
presents

A.

Gil
Georgia

paper

Types
—

port

of an n–process
may

allow

is to use which
object

reading,
port

is a restricted

type

is accessed

another

writing.)

is set at compile
finite

state

via

n ports.
Which

time.

automaton

with

transitions

labelled

used

call

in the

the

output

possible

●

for

object

The

above

a fully
one

robust.

The

sets

Definition.

of

total,

number

start

any

shared

to

a given

result

basic
is no

of

more

of input

object

must

parameters),

more

return

Allowing
an object

in hierarchies

properties

that

of types

transition

defined

for

[KM93].

the

are

required

types

That

not

not

are finite.

may

be dif-

some

of the

is,

processes

may

not

be permitted

operations

on

the

object

operations

are

necessarily

permitted

Many

shared

reader,

memory

types

one–writer

consensus

paper

still

variables.

are oblivious

[Her191].

hold

for

oblivious

set of non–oblivious
The
that
Note

that

study

of the

sus

using

bounded

rithms

for

tions

calls

Note

using

istic

the

read/write
a gap

at the

we do not
used

ory.

[BNP94]

Theorem
types

input

values

the

calls.

It

objects

to calls

simula-

Definition.

where

unlim-

Pi,

end

in

simulate

This

also

the existence

of the

hierarchies
shared
assume

simulations

gives

Jayanti’s

between
memory.

having

using

hierarchy

the
for

of

but

Definitions

N,

the

local

Note

that

served.”

In

the
is

mem-

High

Gap

deterministic

ith

process

N“.

We
For

object

denoted
assume
any

numbers,
by

we

string

Pi.

have
X,

is {1,2,...,
Schedules

a fixed
Xi

denotes

n}
are

object
the

with

the

elements
of the

ith

of

given

symbol

in

345

will
The
obtain

easily

Order

l[i]

is Num(S,

and

information

certain

to have

certain

to

include:
been

have

processes
=

the

word

is

started,
could

Sj means

memory
of pj ‘S

to

Pj’s

sj th

and

earlier

calls

Pi will

know

the

input

to its

For

k)

and

all

–

not

between

1.

can

inputs

calls

ordering

that

are

that

are

information

between
for

to

shared

be guaranteed
to calls

to obtain

“ob-

calls

using

from

is possible

shared
output

resp.

j, Sj)),

processes

be sure
that

.n)

i) + 1. For

information

any

i),

l..

k) – 1.

outputs
and

process

.s,

matrix,

processes

completed,

of

inputs

of S, V(S,
1..

“observable”

write

values
and

s and

..n,

but

an

responses.

end

and

to

of calls,

return
order

given

..n],O[n,

algorithms,
read

to

order

is Num(Pre~S,Pos(S,

the
our

can be thought

S of length

i) + 1, k] is Num(S,

memory.

I[j]

(l[l.

Vi

that

ISI.

the

at the

i)).
given

of accesses

given
the

IS= I

a

such

the

and

if the

j)

Sj, k]

a

history,

length.

i), Pref(7Z,

state

state:

operations

matrix

O[i,Num(S,

that

set of process

1/0

0[~,

start

produces

is iVum(S,

Also,
type

read/write

the
l[j]

~, k, sj,

read/write

as whatever

a proof
hm

all j,

bounded–use

implies

read/write
our

called

determin-

for

is a 3–tuple:

from

s =

start

in calls,

of matrices

is 7? ~

values

same

of a series

a schedule

observable

a
of

S.

ql, q2, . . . . q.
where

the

object

denoting

a sequence

Z is an input

consistent

plus

is consistent

For

R

numbers

history

i), Pref(Z,

is

record
used

1*

response

all of the

= (Q,%)

from

to the

P,’s

in [BNP94]

is robust.

2.

type.

the

starting

E
for

sequence

3 states

complete

Therefore
algo-

is Z

i) is (Pref(S,

sequence

object

the

are used

can also be used to simulate
that

of as the

and

Iand

object.

history,

Z)

and

T is (T, ~) where

an output

execution

if

execution

from

all non-trivial

can

to separately

memory

being

memory

~(&),

An

is a pair

that

memory.

general

need

call

qo,

Q,

process

values

of output

sequence

a consen-

all wait–free
differs

~(Qi-I,

the

A type’s

solve

objects

This

assumptions

types

lowest

and

here.

for

state

name

(functional

details.)

S is a schedule,

execution

is

1 is a set

l,

maps

history

to the

is an output

start

implies

basis

algorithm.

shared

result

memory
shared

memory
shared

same

R

that
more

Similarly

An

n ports)

parameters)

of type

invocation

is ISI, and Pref(S.,

are allowed.

useful

shared

to

that

such

read/write

the

are a sub-

are bounded.

is the

ability

implies

of times.

that

and

N
for

a sequence

(S, Z, 7?) where

4)

responses

object

input

of calls

Pref(S,

of a function

As perfection

object.

Definition.

used
of this

above

objects

as given

a wait–free

objects

to object

to prove

its

number

involving

ited

by

consensus

a bounded

they

given

problem
of types

is defined

problem

types

results

since

types

consensus

having

The

sequence

ports.

e.g., one–

Many

to the
denoting

The

about

complexity

complexity

types

of the use of shared

the

all

An

of S of length

(with

of referenced

over

of input

in S k

of occurrences

of states,

xR.

(See [BNP94]

Definition.

R*

T

set

values

n–process

sequence
calls

not

types.

assumptions

all aspects

on

are non-oblivious,

shared

to

since

An

appear

prefix

type

Q is the
(composed

new

not

i, k)
of the

1) is 3 and

R is a set of output

~ is a permutation

to be oblivious

same

n–process

QxNx14Q

to ports.

the

Pos(S,

S = 2133211.

where

values,

6:

are

the

perform

s) denote

invocations

was

to an object.
that

An

let

occurrence

the number

3, 2) is 4, iVum(S,

(n, Q, 1, R, 6),

N*,

kth

if i does

i) denote

PreflS,

Definition.

While

S in

S of the

is undefined

is 2133 for the string

it seems

to

string

in

iVum(S,

Let

types,

call

(It

Pos(S,

memory

specification.

for

Jayanti

number.

i.

E.g.,

any

location

) Let

in S of i.

of times.

for the study

on consensus

the

times.

as the

are reasonable

For

denote
number

and

be specified

a bounded

deterministic

Similarly,

Note

for

can

based

assume

if there

calls

call.

x.

and

s.

transition

by

ferent

type

inputs

are finite.

non-deterministic

to

than

the

values

of the

of functions),

is deterministic

assumptions

allowed

shown

output

of types

have

from

is accessed

reasonable

values

names

object.

complexity
Jayanti

the

the

function
of the

the

The

all

and

state

state

ports,

returned

transition

Any

●

values
input

. The

by

(including

Pi

calls.

to

learn

via

before

Sj —

its next
call to the object
the
1st and earlier
calls and the input
to the
own

object.

For

Si + 1st call

i = j,
and

the

output

of its

o[~,

sith

sj, k] =

call

the

object

will

the

object

occurred

(While
call,

be

in fact
there

Pi will

will

be able
its

the

observable

1/0

information

ory.

The

i

length

an

ces(Z~[l.
der

..n,

s s

I[j]

and

otherwise.
R:

the

That

call

that

for

can

actually

next

using

three

schedules,

if for

obtain
The

read/write

with

can be done

with

is

processes

weaker

qo, a process

state

Pos(S,

S.

from

.s],7?~[l

s’ =

a triple

. ..n.

s],

a visible
of

V(S,

j, s), lh~,
s’ =

s’]

=

is Z~,

Pos(S,

j,s),

the

output
order

is observable

for

all

values

calls
for

or-

history

is observable

to

object
that

A visible

inst antes,
Pi just

the

that

history

each

before

each

to decide

=

visible

and

Ym,

hm = h(qo,Pre~Se,

During
to know

a simulation
the

inputs

a process

choices

may

make

simulation

that

the

and

is a sequence

values.

It

rameters

might

previous

calls

process
that

what

between

strategy

visible

Person

on the

to the

definitions
Alternation

Definition.
point

next,

it

the

the

sequence

input

these

Se =

(S, Z, 7?) and

in their

internal

etc.

choose

This

Peterson

who

invocation

state

pa-

from

However,

Pi

of an object

that

is formally

stated

and

Multiple–

there

us-

graph

x N

+

for

that

Se = (S, Z, l?) is produced

not

1.

(From

the
call.)

be able

sequences

a

object

{1,2,

High

Gap

bound

by y from

346

...,

the

to tell
starts

a

by

CT starting

of execution

=

k and

Pi’s

se-

Se and

visible

apart

two

the

other

S:

histories

The

execution
by

power

se-

pk ) using
of a type

of processes

u,

is

to determine

E

=

{(j,
j

graphs
graph
An

notion

with

G(qo,

a start

strategy

y

C, y) = (V, E)

k)13i

and

QOusing

from

type,

a legal

graph

n},

whether

type

and

between

k

such
in

that

schedules

7}.

are

undirected

for each start

state,

and
sched-

edge

(j, k) in an equivalence

that

at least

at least

one

one pair

process

is

of execution

j or k.

Theorem.

u, and

k

distinguish

of an n–process

and strategy.

deterministic

An

S;

and

equivalence

captures

and

u, and

(S’, Z’, 72-’) produced

that

bound

is an equivalence

ule bound

si.
by

operations.
objects

distinguish
that

i) =

i, j,

Pi cannot

=

j,

an equivalence

=

i

is bounded

bound

a pair

to tell

by a starting

Note

[PR79].

is to call

V

cannot

bounded

a

value

Reif’s

where

other

and

number

Num(S,

numbers

information.

a schedule

qo define

Pi

of steps.

S is bounded

bounded

to the abilities

For

q.,

and

q.,

S:

by Pj

of visible

The

S:, i) are equivalent.

is unable

of their

sj

exists

S1 =

started

related

order

of

from

from

(one

Definition.

an input

(MPA’s)

is 7 : N*
and

as outputs

obtained

state

input

is, Pi

object.

number

a schedule

(i, O) such
H(qo,

processes

one process

(S, Z, 7?)

schedules

where
by

Se, i) and

directly
the

choice

that

q.,

bounded

the

S2, . . . . Sri),

from

-y if there

-y from

k in

some

a process

process
q.,

j

its history

assumption

three
state

-y starting
and

the

cannot

(i, m).

using

q.

On

object

in the schedule

call

S,

=

bet ween

quences

or impossible.
the

object,
to

automata

in a schedule

gives

to

pro-

values,

such

data

process.
from

A strategy

given

execution

calls

by other

by

can

a is (s1, s2, . . . . s~)

~) <
S:

from

H(qo,

needs

object

fair”

on things
object,

is

of input

to expect

be permitted

is dependent
the

the

process

i Num(S,

legal

of times.

where

bound
u = (sl,

Given

That
each

under

“playing

depend

Se, i)

(S, Z, R))

they

bounds

i).

each process

if the

a bounded

sequence

j # k, a start

quences

to the

H(qo,

possible

work

is reasonable

to

cannot

is not
ing

to the

a simulation
are

Pi

s’ (Se =

if b’~ #

object

to

=

numbers

= -y(S’,

an

q.

S:

if

is due

an schedule

bound

from

-y(S, i)

that

that

times

within

Given

strategy

the ordering

these

with

i).

has choices

algorithms

processes

processes

m),

to calls

Since

Our

for
i) =

of an object,

cesses.

input

history
iVtzrn(S,

si.

Definition.

Definition.

where

input

(S, Z, 72) and

number

it

A schedule

~

execution

by
A

many

is unable

are

hz, . . . . h.,)

same

it can do so with

be proven

(i, o) if S is bounded

in time:

calls

how

formalizes

by (i, o)

are observ-

of its

the

algo-

different

go and all process

fact

to determine

and a schedule

actual

one representing

parame-

for two

with

Se =

the

consensus

able

following

object.

(hl,

being

Iul

is J_

are observable,

of what

use in a given

object

a bounded

it will

Definition.

If

Rh [j, s’] is

records

object

no matter

matri-

to Pi at a point

to the

all calls

information.

of visible

Pj

list

will

Sj, i) implies
on

consensus

n–process

not

An
for

Se is consistent

-y is a legal

ff(qo,

hinge

n–process

hand,

of

and

the

strategy

S., i) =
proofs

calling

(1, O).

call

by y from

other

O is the

i)

and

is the same

(S, Z, %3) produced

solve

history

three

A

2-

n)

history

sequences

Our

num-

(S,z,

O)], O).

history

and

=

go,

is

instance

that

values

visible

process

execution

can

than

Zi

is a complete

each

Pi must

all

i, ll(qo,

mem-

easily

=

parameters.

pk

section.

or more

type

sequence

visible

object

If Pi’s

solve

Se, i)

~[j]

is, a strategy

to the

rithm.

sk + 1st

i – 1), Si)

qo.

ters

this. ) For i = j,

processes

a start

history

information
able,

to

is 1 otherwise.

A visible
inputs

two

Pi’s

If s <

and

call

Vi y(PreflS,

if

from

object.

P~’s

its next

the

whose

h(qo,
l..

in

to

Definition.

consistent

Pi

matrix

call

types.

is

for

the

can be obtained

execution

s that

instance

Pi to learn

in

and

Given

and

to

after

before

simple

objects

consensus

F’j ‘S sj th
call

processes

for

memory

next

call.

how

that

Definition.
ber

for

its

that
Skth

occurred

to know

is quite

with

process

call

Skth

before

learn
pk’s

information

object

done

Pi

information

Order

read/write
be

sjth

be shown

a simple

to

after

is no easy way

also

has completed
It

that

able

Pj’s

q.

as well.

Sk means

If,
T,

a legal

for

there
strategy

is

objects
a state
~ starting

of an n–process
q.,

a schedule

from

q. such

that

the

Equivalence

then

objects

otherwise
object
3.

of type
objects

that

We

define

some

full

version

the

consensus

can

basic

can

of the

paper.

its process

and
For

the

of the

number.

be converted

into

consensus,
by

algorithms

constructions

with

one

n–process
be simulated

shared
variables
boolean+
ql, q2
For P1, the reader:
integer
function
Read
ql=q2
return
ql
For P2, the writer,
writing
true:
function
Write
q2=true
For Pi, an overlooker:
integer
function
Overlook
if
ql
# 1 then
return
qi I*
PI done, return
its value
elseif
q2 then
return
true
/*
P2 done, P1 will return
else
return
1 I*
Neither
P1 or P2 done

any

consensus.

types

some

elect

connected

Algorithms

and

problem

is, processes

a, -y) is not

solve
T

and

proofs

the

can

n — l–process

Types

The

return

T

G(qo,

of type

solves

Basic

later.

Graph

needed

are

simplicity

election

problem.

That

processes

an election

a consensus

in

we equate

competing

Clearly

given

and

algorithm

algorithm

and

vice

versa.
3.1

Basic

consensus

Definition.

Let

of type

cesses,

wit h default

start

state

sensus

type.

is,

returns

process

that

That

Definition.

it

it when

Given

two

T.,

type.

the

started

in 1 state

return

to call

type

the

It is easy
n + n’–process

For
ing
for

some

PI

another

false

true
return

the

overlooking

type,

Define
type,
1

initial

is (l,f
then

by

For

An

ing

objects

for

n =

n = 2).
of type

atomic

If either

returned

Theorem.

type

for

(or

will

object

of type

to wait

processes,
called

vari-

other

to it.

ues to a call

the n– 2
PI
have

this

type,

TO:n.

A type

T2 objects

TO:.,

shared

called
given
value.

variables

the same

T2 and

by

To:.

to)

object,

can

read/write

the

can

be implemented

variables

and

the

(and

in the

general

equivalence

TO:n

type

with

two

in

The

that

next

shared

stay

in

memory

347

a

order

read/write
The

in
al-

shared

construction

is

proof

that

if

a type’s

objects

requires

to have

processes

is used

equivalence

that
all

called

simulate

Tn– 1 type

processes
processes

of

graph.

that

The

simulations
choose

algorithm

edge in an equivalence
whose

edge

at least

both

or

are
(or

to
val-

objects

first

lockstep

schedules

cesses

2.

lockstep

result

then

a connected

to ensure

The

and

sequences

us-

The

in

are used
and

the

is discussed

eisame
in the

subsection.

Each

the

be simulated

is used

simulation.

de-

(qi,q2)

has occurred
value

an edge

of

write

problem

of the
using

actions

paper.

is connected,

sub algorithms.

ther

Figure

of the
choice

steps.

The

process

using

edge
structure

and

t, call

type.

version

graph

write

are
con-

to the process

algorithms
The

overlooking

can be simulated

second

the over-

full

partial

two

values

WriteOutput.

order

system

WriteOutput,

The

an ex-

that

instance,

WriteInpu

gorithms

select

non-Boolean

ype

call

The

called

return).
oft

ReadOrder,

3.4 The

or-

and

have

The

and

of the input

to

indicate

schedules

returns

history

to an object.

order:

given

their

WriteInput
processes

not

on the order

a visible

the

if neither

(because

a read

exact

the process

schedule.

information
i.e.,

will

out

ReadOrder,

An

would

but

to rule

actual

called

recent

steps,

whether
not

step,

notify

A special

operations

return

most

the

it

processes

of type

‘2).

is equivalent

ordering

A process

components:

the

processes

P1 or P2’s operation
will

of three
read

that

on whether

PI may

sists

that

pro-

of a series

objects

at most

of operations

The

allows

ordering

since
exact

first).

to the

read-

true

went

algorithm.
which

only

numb~r

(An

of the order
allows

order

using

be obtained

dissimilar

memory.

by

using

of the object’s

the overlookers

reader

process

The

Boolean

an object

has consensus

algorithm

q be a one–

know

n processes

a special

value

alse).

shared

objects

Let

to allow

to

though

is denoted

the

denote

We want
able

to

memory

Definition.

having

its operation

can be simulated
read/write

Let

and

a type

P2 writes

to overlooking

known).

fault

of Loui

process

is a system

consensus.

act view

type.

an approximate

on

cannot

which

can do

three–valued

depending

even

P2 has executed

looking

T~,~)~/

read–once
and

be

value

yet

3.1.

to

or false

is not

object

ordering

need

it saw).

false,

P2 wrote.

der

just

process

of one process

(without

value

or true

is returned

P1 nor

first

for

result

write–once,

processes

its
1

TO:n (which

a variation

we will

the

the

value

or after

return

written

of type

algorithm

variable

to write

return

before

value

started

n–process

simulations

process’s

overlooking
will

cesses to determine

type.

one–reader,

*/

algorithm.

the

algorithm

of operations

the

overlooking

order

outline

order

when

the

3.1 The

process

next

process

sets of pro-

type

that

using

to know

q has initial

read

state.

very

of our

the reader

will

the

objects

consensus

processes

able.

that

overlooking

writer,

We

[LA87].

The

allows

3.3 The

first

to.

tournament

test-and-set

start

con-

let T, ,n,n, denote

set number

to prove

of the

non–empty

calling

belongs

Abu–Amara’s

3.2

the

standard

the

Figure

by n pro-

index

from

disjoint

Processes

accessed

1 be the

cesses ‘PI and ‘PZ (of size n and n’),
splitting

true
*I

types.

objects

calls

*I

apart.

select
execution

one

first

graph

symbols

one process
Our
such

cannot

construction
edge

sequences.

represents

are the

and
Using

end
tell

the

requires

carry

out

objects

a pair

nodes

execution
that

simulations
of

of

of the

type

proof

Tn _ 1,

we cannot
edge,

ensure

but

that

we can

Definition.

come

Given

each node

associated

solves

partial

the

1. Every
2. Process
4. For

all

edge

has started

its

any

shared

7.

If

= {(j,

Pi,

k)},

y set &i of edges.

l&[

=

all

i’,

for

1.

at least

Pi

with

&i

that

is set

chosen

=

(j, k) E tit.
one of P3 or Pk

{(j,

k)},

there

to

(j, k)

Pi

procedure.

>

1, then

&

~

{(i,

j)l(i,

j)

is an edge

in the

graph.}
That
one

is,

edge

tive.

at
and

There

which
the

least

n –

at least

one

process

on

one

process

that

chosen

to its

node.

particular

edge

by

the

other

process

partial

have

is incident

know

will

is at most

edge the others

edge

1 processes

completes

edge

choice

that

but
It

edge

may

will

also
chosen

its algorithm.

on the

that

once

Solvability

hinges

know

be able

to
any

y of t he

graph

being

connected.
Outlined

in

algorithm
using
the

Figure

that
only

solves

type

graph

and

Each

initial

choice

child
all

are

(All

The

point

the

competition

that

node’s

initial

denotes

a call

edge

breadth

has

by P% to the

of

is any

known

doing
going

process

Figure

At

at some

some

node

and

Tn _ 1 ( j , i)

n copies

Tn _ ~

of type

objects.
Theorem.
partial
3.5

The
edge

The
The

lockst

a specified

type

algorithm

Pi of the
of this

jth

lockstep
choice

or

copy

agree

property:

denoted
will

on

solves

one

the

value.

It

shared

uses

in some

all processes
calls

in

/*

only

return

Many

the

return

to lockstep

= winner
choice
[i]
else keep choice
*/
choice
[i]
Figure

a call by
copies

4.

simulations.

has

edge

choice

algorithm.

[j I

memory.

of t ype Tn _ ~ with

ep algorithm

the

3.3 is used
operations

by Tm– 1 ( j , i).

Either

all i and j,

Figure
their

read/write

be needed

lockst

in

execute

uses n objects

The

or for

given

to either
Tn _ 1 and

algorithm

Theorem.

Pj

ep algorithm
order

of

algorithm

algorithm.

processes

objects

edge choice

problem.

lockstep

to force

The

partial

choice

partial

Called
by Pi with initial
choice i.nchoice:
shared
variables
integer
minner[l.
.n]
initially
O
integer
choice
[l.
.nl
integer
function
lockstep(integer
i, inchoice)
choice
[i]
= lnchoice
/*
display
choice
*/
forj=l
tondo
if
j # i then
/*
on n — 1 of n iterations
*/
k = Tn–l ( j , i)
/*
first on jth iteration
*/
choice
[i] = choice
[k]
/*
winner’s
choice
*/
winner
[j]
= k /*
signal who won to Pj */
else
/*
on one of n — 1 iterations
*/
if
winner
[j]
# O then
/*
winner
of ith iteration
known
*/

is initially

itself.

3.2 The

*/

to

processes

is selected.

jth

as its

choice

at the root

towards

terminate

in
the

search

link

and

the

Each

choice

node
that

first

initial

starting

competition
will

any

are fixed

processes.

the

problem

Assume

root’s

nodes

Choice

choice

use its parent

algorithm

active

to direct

in

will

The

Edge

edge
root.

choices

at various

towards

Partial
Select

the

node

initial

processes.)

the

numbered

of an edge.

competitions
down

that

non–root

edge.

trying

is

partial

Tn _ 1 objects.
make

nodes/processes
order.

3.2
the

(

if

is ac-

not

O

(j
is a descendant
of Cent ender)
Contender
# i then
I*
not competing
at your node
*/
/*
compete,
look at winner’s
favorite
*/
j = favorite
CTn_l(Contender,
i)]
if
j <= Contender
then
/*
competition
ended at j */
chosen
= j ‘s initial
edge
return
chosen
/*
indicate
winner
+/
Winner
[Contender]
= j ;
Contender
= j
else
/*
Pi not allowed
to compete
at note i
of i do
for
each
j = child
if
Tn. -l (j ,i)#
i then
exit
loop
if
Winner
[i.]
= O then
return
all edges incident
on i;
else
Contender
= Winner
[1]

on just

it does know

reading

problem

agree

false

active
[i]
= true
Contender
= 1 /*root
of G, first possible
winner
*/
favorite
[i]
= i /*
initially
you’re
trying
to win
*/
while
true
do
if
i is not a descendant
of Contender
then
favorite
[i]
= any j , active
[j]
and

is a

before

graph

integer
favorite
[I. .n]
/*
Pi’s favorite
to win
*/
local
variables
integer
j , contender
set-of
-edges
f unct ion
EdgeChoice
graph
G, integer
i)
/*
let others
know you’re
active
*/

procedure.

process

its

I$il

an algorithm

ifi

a non–empt

(j, k) in any&,

variable

completes

problem

with

is wait–free.

with

&

Assumes
G is a connected
n–node
shared
variables
boolean
active
[1. .n]
initially
/*
who’s active
*/
integer
Winner
[1. .n]
initially
/*
winner
at node i */

the same

graph

1 of n processes,

n – 1 processes

5. For any
For

connected

choice

procedure

Pi with

can choose

enough.

n-node

with

edge

Pi returns

at least

close

an

process’s

3. For

6.

all processes

High

High

Gap

Gap

following

an n–process

the

same

schedule

by Pi and

go such

overlap.

connected

348

3.3 The

lockstep

Theorem:

Theorem

bound
that

u, and
the

then

“then”

(“then

deterministic

type
a legal

Equivalence
objects

algorithm.

of type

case”. )
T,

there

strategy
Graph
T

case.
If,

for

y starting
G(qo,

can

objects

is a state

solve

u, ~)

of
q., a
from

is not

n–process

consensus.
Proof:
of

Given

nodes

tween
the

G(qO, a, y),

into

two

them.

non–empty

Note

processes.

consider

that

Let

the

let

(Pl,

a = (sl,

algorithm

(Pl,

P2)

sets

with

P2)

be
no

is also

. . . . Sn).

in Figure

a partition
edges

be-

a partition

For each

process

deterministic

type

a,

legal

strategies

~ starting

Graph

G(qo,

U, -y) is connected

and

alence

of

type

Pi

process

T can be simulated

integer
*/

m

h [1.
1
R

. Si]

initially

I

initially

/*

null

No.

calls

to

the

object

for

Pi,

“then”

main

features

. The

proof

is by

Ial

1 is trivial.

=

once,

hence

sumes

that

type

using

tion

will

qO

uler

that

algorithms
type

objects

Pi

of them

would

be two
are

That

(and

can

-y, all

ries

above

the

Assume
Pj

uses

and

therefore

must

to

that

but

to

Pj started.

schedule

●

The

after

si steps,

whose

schedules

distinguish

between

j

node

sets P1

tion.

Hence
return

Ton

type objects

which
for

the

in turn

two

and

and

therefore

T2.

For

the

two–process

same

●

equivalence

the

unconnected,
this
the

n–process

TO:2 requires
of type

or more,
version

from
be used

T.

T

first
and

type

can

from

that
objects

n–process

the

the

must
use

all

1,lp,l

each

Note

that

global

will

type

ment

is

it

may

5. High
High

Gap

Gap

Theorem:

Theorem:

“else”
“else”

case.

case.
For

tion

an n–process

the

349

ac-

so that

then

large

we

a new

SimNum(qo,

edges
all

If another

pro-

can

now

and

only

of simulations

the
and

At

simulation

when

doing

,-y)

number,

given

in

the

[w]

only

it must

give

a prothe

assume

maps
edge

sub-

a func-

a start
in

state-

variables

When

We

al-

of ev-

do”

the

All

called

beginning

step.

the

For
w.

of records

the

that

dif-

variables.

sharedmem

number.

we

same

number

variables

subsimulation,

that

simultaneously,

is accessing

w, (j ,k)

simulation

all

chosen

simulation

a simulation

for

have

simulation,

shared

imsl.

that

one

it will

process

in

a “with

process

edge.

than

first

of an array

. maxnum-s

the

more

a simulation

components

the

choose

have

doing

memory

during

use

on.

different

have

we have

to

processes

the

number

variables

use

correspond

The

result.

others

to sim-

that

joins
the

the

shared

[1.

current

the

simulation

beginning

same

processes

shared

are

simulation

up to

strategies

with

back

participating

that

simulation
and

the

from

be

same

cess initiates

❑

n

object

is unique

sequences

process

very

simulations

to

the

processes

to

get

simulation,

edge

ensure

needs

all

simulations

some

the

graph.

In the

edge

of the

sharedmem

oft ype T alone

is for

algorithm

ret urn
the

that

ery function

using

accessing

one strategy

of execution

which

the

(which

allows

consistent

least

might

If no other

gorithms

implement
TO:.

pairs

set.

ferent

memory

consensus.

simulate

processes

using

read/write
alone

algorithm.

~p,

T2 is implemented

Therefore

to solve

algorithm

split

consensus.

only

be an

and

the

Because

to make

multiple

sequences

choice

edge

●

a contradicto stop

edge

process

simulate

and

need
of order

7 that

being

equivalence

one

determine

execution

and

partial

cess does join

in Pt.

must

the

simulations

not

are to our

algorithm

progresses,

simulation

However,
to

a.
sched-

in a specified

out

possible

At

simula-

adversary

of which

when

simulation

of execution

in

from

The

simulation

Hence

as not

asof the

decision).

used

each

of

object

simulation

n – 1 processes

same

case

the

to execute.

a pair

in its

its

k resp.,

graph

are guaranteed

t. Therefore

objects

be two
there

in-

a smaller

the

lockstep

to the object.

base

calls

with

both

the

advance.

core of the

an edge

histo-

all k not

The

ulate

values.

completed

j and

then

can implement

n three

can then

continue

Pi ‘S ~ will

with

But

are not

can solve

and

will

to call the object

would

start

k in the

P2

process,

implemented

there

all processes

will

only

there

visible

k for

in

be discarded
calls

2).

the

one for

tual

calling

different

process

with

them.

of

n – l–

a simulation

the simulation

the

As the

order
is

does

-y being

known

Ial steps.

n

of

to make

are started,

the
if

use

The

be done.

but
forces

case only

strategy

is not

use in

) Eventually,

starting

solves

simulation

The

can

things,

it

or it does

the

IcI.

that

recursively
basically

which

processes

Otherwise,

process

of the

process

Tn._ 1 objects

itself

Either

(in

one

la’ I < Ial,

to do one of two

due

requires

whose

no other

Otherwise,

edge

only

to

1,

-y returns

Pj was the first

sequences
P, cannot

S’s

same

sequences
Pi

(Therefore

any

many
the

Equiv-

objects

that

is no parallelism.

all c’,

type
call

a decision)

case.

memory

proof
on

Only

simulation

order

legal.

before

contain

from
give

execution

same

E Pt.

algorithm

read/write

choose

is, y is not

and

simulation

to communicate

T2

While

the

of the
induction

there
for

advantage.

may
Note

then

by any object

bounds

the

consensus.

. The

~(go,se,i)
= h}
/*
All start
with
nodes in same set? */
if
{jlj
= S1 and S E S} ~ Pt returr,
t
S = any element
in S
1 = -f(S, i) /*
Use strategy
for inputs
*/
Writelnput(i,
rn,l)
R = CallObject
(i, 1) /*
Call the object
*/
WriteOutput(i,
m, R)
m,++
4.1 Algorithm

q.

clude:

while
true
h [ml =ReadOrder(i,
m)
/*
Determine
sequences
that could
correspond
to visible
information
*/
S = {S1%S= = (S, Z, %!) produced
by -y from
and bounded
by u such that

Figure

q., schedule
from

4.1.
The

History
Instance
Obj ectInputType
ObjectOutputType

T, if for all states

the

state,
graph

and

strategy

ulation

that

start

with

The

At

basic

task

of the

object

shown,

this

objects

of type

(2-set

consensus

least

agree

candidates
T._

[Cha93]).

on

two

for

going

which

two

to choose

two

equivalence
cution

from

one process
S:.

values,

order

quences.

That

and

k, it

of j

and

will

1 can
The

quences

with

same

that

making

the

some

cept

for

are

the

to

sequences

first

sequence

a form

a

S and

(denoted

by 1) and simulated

of the

intermixed

with

call.

that

by

operations

sequence
1) and

iff ~k, k’ such
Pos(Sj,

i, k+

1)

will

end

to matching

construct

the

uler

is not

we first

need

but

if it

sequence.

the

steps

all

S1 and

the

events

processes

by 2)

(denoted

and

lined

Sz

in S1

in S2 (denoted

been

linThe

steps

by

end

of a simu-

that

processes

has to be made.

to order

with

are
j.

for

to a lockstep

have

Pj,

jjiiji

algorithms

beginning

calls

then

S =

do simulated

to lockstep

locksteps
required

in an equiva-

operations

if a choice

doesn’t

an edge
an overlap

sequence
the

denotes
only

S: = (S’, 1, 7?) are exe-

processes

denote

(L)

perform

overlap

calls

Brackets

lated

uses

of schedules

two process
Pi and
. . .. . ..
= ZJJZJJZ, SI and S2

The

as

(which

containing

S’ share

simulating

of the

(One

operation,

the

amount

digits

of the

after

that,

ex-

of one
the

[2L

Pj:

[1L2]
E.g.,

se-

end

pro-

S is a linearization

S is a permutation

for

simulate

●

is easy

simulation

During

[2L1]

[Ll

L2

L]

third

a step

construction

exactly

350

its

L]
[Ll

call

simulate

properties

quences

all

and

step,
The

of an over-

of S and

Pi

lockstep,

the

A schedule

[IL2]

third
basic

of its

L1

Pi:

of time,

respective

assume

beginning

immediately

that

in an overlap

ends

can safely

of N*.

of schedules

non–zero

The

order

occurs

S if

then

for

steps

calls

lineariza-

more.

with

below.

L’s).

an

simple

between

algorithm

j, k’+

associated

example.
Given
. .. ... .
~~z]zzj
and S2

will

are elements

denotes
to take

the

Sj

are given

On the

share

is quite

as a possible

an overlap

sequences

all processes

order

graph

lockstep

locksteps,

Si and

processes

there

For

performing

do not
It

variables

If S. = (S, Z, 7?) and

=

each

that

and

an

processes

a simulation.

properties

combined

operation.)

Definition.
lap

In
and

SI

share

the

, j, k’).

sequences

An

that

sequence.

earizations

the

same

basic

i, k) > Pos(Si,

POS(Si,

lence

se-

then

order

two

an overlap

do not

2211.

order
this

an overlap

up.

The

calls

The

sched-

as given

below,

agree

on

the

same

choice.

operations.

cess’s operation
previous

and

is no

If the sched-

to the

state

Lemma.

coop-

to decide.

not

calls

does

(in

S such

for

during

and

process

beNote

processes

S2 share

schedules

This

All
order

need

does

two

non–atomic.

indicate

processes’

and

sequences.

algorithm.

that

steps

sequence

for operations

quence

< Pos(Sj

of the

easy

one of these

Schedules

that

simulated

the

k’th

occur

sequence

use shared

out

Pj’s

must

processes,

1122

Pi and Pj do not share

leaves

there

are

type)

share

which

choose,

simulations,

choice.

Overlap

there

the

is used.

overlap

same

first

Lemma.

j

definitions.

overlap

allows
i.e.

are

do not

Definition.
An

se-

between

actually

chosen

desired

of simulated

more

so that

scheduler

there

the

in both

in that

scheduler

to the lockstep

if the

up

fact,

That
to

in a particular

other

sequence

naive

algorithm

n — 1 processes

that

In

overlook

no matter
first.

need

of the

are at most

algorithm

tion.

We

i, k)

of S.

them

only

the

cution

adversary

steps
to

then

might

sounds

lockstep
calls

hand,

result

the

before

schedules

case of two

to rule

do not

kt + 1

Pos(S,

constrained.

S1 and

if two

between

in the

the processes

return

k and

linearization.

operations

an overlap

that

to have

is

any

it is relatively

in order

between

choose

be shown

in

there

in

linearizations

sequence,

operations,

ends
operation

schedules

of exe-

same

etc.

to

as the

operations

simulation

planned,

have

choose

the

corresponds

that

least
implies

necessarily

exists

sequence

Se and S; (whose

information,
not

the

schedule

several

ule

state,

k resp. ), such

kth

last

is not

overlap

in an

is a pair

distinguish

discussion

the

do their

there

of the

distinguish

two

at

operation
P;’s

S2 are each

example,

our

of connection

see the

does

in

kth

Two

overlap

we want

appearing
< Pos(S,j,k’)

then

if there

It will

are

are adjacent

cannot

are linearizations

is where

can at

j

k’ + 1st operation

S’l and

values

to do that.

that

and

nodes

always

processes

requirement

Definition.

limit

Pi that

be used

above

erate

and

others

the order

sequence

that

Ideally,

is, Pi will

return

n – 1 other
T.-

can

definition

the start
j

Pi

k the

f%,

that

only

two

and

we

means

of execution

Hence

that

the

that

with

Se and

Pj

Pj’s

on just

is, all processes

say

whose

By

at least

most

can be chosen.

graph,

start

at

if Pi’s
ends

fore

to

processes

sequences

on

agree

is,

a linearization)

have

Given

cannot

That

strong

values

an equivalence

process

first.

graph.

schedules

consensus.

processes,

1 is sufficiently

favor

which

i and

j, k’ + 1).

operation

for

as [Her94,Plo89]

agree

with

in S, Pos(S,i,k)

That

as

to the

values

knowing
But

can

as best
calls

appropriate

n–process

they

times

will

< Pos(S,

of the

Tn – 1, n processes

But

i, j, k, k’,

sim-

simulation

is to try

the

is crucial.

is in fact

value.

simulation

return

a unique

the

of 1.

In particular,

first

into

level,

a linearization

and

linearization.

is considered

top
number

to determine

simulated

one

the

a simulation

possible
that

is to be simulated

number.

a call

L2]
[2L1

to the

a step

object

of such
using

orderings

an ordering

these

(L)

L]

(L)

will:

do the

in S1, do the fourth

in S2 and do the fifth

of simulation

(L)

is given
from

properties

lock-

lockstep.
the

and

The
below.

three

is given

sein

algorithm.
to the

one simulation

simulation,
step

in each

each

process

sequence.

will

do

The

●

for
●

number

the

Between

any pair
step

Taking

who

in order
●

tth

Let

last

in fact

m be the
lockstep

Pi

on

its

m’+l
ations

give

step

S1.

between

Pi did

the

same

with

the

m,

tth

call

simulation

integer
t,
/*
Count
of Pi’s calls to object
Obj ectInputType
I ) /*
Input
values to call
static
variables
set
of Strate
y 17 /*
Current
legal strategies
History
h[l.
.
1 /*
Visible
history
*/
local
variables
Schedule
S
set
of Schedule
‘1 /*
Possible
schedules
*/
Obj ectResultType
R /*
Result
from a sire. */
/*
Use the right shared
variables
*/

S2.
the

tth

m’

be the

to the object.

will

do

scheduler
in the

were

same

quence.

(Which

to either

of the

The

of course

locksteps

types

and

the

types

lockstep

from

ulation

lects

values

choice

5.2.

On

history

the

once

the

choice

call,

and

col-

object,

therefore

and

input

ulation

for

strategies.

Proof

for

sume

calls

it

simulating

the

partial

an edge

(and

to simulate.
all

edge

the

for

But

extras

an object
5.3.

level

using

On

its

call

for

step

be from

would

two

schedules

the

order

first

Once

matter

it must
locksteps
chosen

and

are

at

the

It

all

some

does

this

There

the

induction

on

it has two

may

from

be further

subsimulations

the

second

edge.

equal

Another

steps

different

possi-

done

it does

equal

that
But

the

simulation

two

schedules

due

to Pj

351

else

sim-

case:

As-

go,

since

for

some

wing
the

equivalence

is initiated

graph,
that

the

ob-

a 5trat-

a simulation

the

using

this

-Y from

is a proof

possible

ways

But

selected

results
by

the

are correct

possibility
did

not

then

edge.

are

by

for

the

The

returned

induction

first
from

hypothesis,

regardless

is that

the

correspond

the

the

of the

actions

do not

is that
the

will

writing

the

overlap

a process
but

a
for

and

Pj

all

result

to note

of that

call

get
were

has remaining
But

distinguish

be able

may

(which

still

sequences.

cannot

in fact

choose

sequences

subsimulations

simulated
Pi that

the

subsimulation.

and have to choose

the process

scheduler

to

locksteps

same

from

in the

that

the

for

consideration

remaining
to

calls.

incorrect

results

any

calls

fails

choose

in order)

operations

then

the

scheduler.

processes

simulation

are not

subsimulation.

the

Pi.

to fail?

is that

the

run

fails

state

are

*/
for

only

Theorem,

Recall

What

simulation

schedule

simulation

IuI.

*/

one.

equivalence

others).

simulation

The

steps

the

*/

a simulation

it need

start
ff

values

algorithm

algorithm

b’

in

perhaps

of the

at the

bound.

by finishing
value

next

to determine

simulated

are both

If they

their

Gap

some

bounded

edge

possibility

Figure

calls

simulation
from

from

G(qo, a, y) is connected,

The

possible

it looks

toplevel

of its

If there

returning
end.

the

schedule

a call to SimEdge,
from.

what

call

own

in

value

result

chosen

High

the

q. (and

simulating
two

sequence
its

it calls

to return.

the

pk,

each

overlap

completes

which
and

On

for

is given
for

and

a smaller

during

choose.

Pj

etc.

that

with

to choose

etc.

to perform

a process

ble results

edge,
determines

their

Note
but

and locksteps
not

q.,

which

and locksteps.

it

SimEdge,

process,

and

in

ect

algorithm

a given

first

candidates

SimObj

Yj

graph

more

are simulated.

*/

*/

simulation

previously

outline,
starting

egy

there-

Since

is known,

ject

is given

the

on later

the

that

entire
lowest

return

5.1 Top–level

information

dropped.
..
The

R /*

Figure

order

select

others

return

process

be returned

of the

WriteOutput
(i, t ,R) /*
Display
t++
/*
completed
one sire. call

for simI is given

etc.

by some

-{y}

the

y,

produced

*/

~ c r from qO and bounded
by a
such that
H(qo, Se, i) =h}
y E r /*
Remove
invalid
strategies
forall
SE V,
~(S, i)
# 1 then

r=r

the

process

*/

WriteIJIput
(i, t, I ) /* Display
input
values
*/
forall
y E r /*
Simulate
a step in each strategy
R = SimStrategy(qO,
u, G(~o, a,7),
t,l,
~,-y)

a set of possible

possible

sequences)

may

input

strategy

first

edge
in turn

CTsteps,

Simstrategy,

to try

S 13S. = (S, Z, 7?)

forall
if

mem-

partial

The

using

= {

5.1, 5.2,

(which

it creates

to eliminate

execution
edge

at most

a particular

algorithm
one

se-

for simulating

all of them.

algorithm

than

all overlap

Tn _ 1).

information

using

two

with
SharedMem
[wI do
if
t=l
then
/*
determine
possible
strategies
r = {-fl -f is a legal strategy
from
q. for schedules
bounded
by a}
h [t] =ReadOrder(
i, t ) /*
Get order info.
*/
/*
Determine
possible
schedules
*/

correspond

the

call having

call,

simulate

be used

in Figure

qO, for

visible

midlevel

object

than

On the first

The

can

The

of or-

shared

by

SimOb j ect,

state

history

algorithm.

out

in Figures

are used

w, for the tth

and will

visible

oper-

simulation

read/write

needed

powerful

a start

5.1.

strategies

is given
only

algorithm

number

in Figure

not

same

no longer

algorithms

more

toplevel

object

would

the

would

algorithm
that

The

performed

two

V

Note

use no types

that

so

originals.)

5 .3b.

and

were

choose

and

choice

it

locksteps

would

simulation

5.3a,
ory

sequence

the intervening

all processes

arrange

to

*/
*/

hUi

lockstep.

i in S and

recent

one

above

then

m – 1st and mth

of the

to

is exactly
sequence.

in S1 as given

The

in its most

call

there

in either

of i in S1 is at

position

tth

der, then
and

fore

Obj ectResultType
function
SimObj ect (
Stat e q.,
/*
Initial
state of simulation
*/
ScheduleBound
u,
/*
Max. num. steps
*/
integer
w, /*
Simulation
No., init.
1 */

extra

tom.

c If the

the

is 1~1 + 1 (one

calls,

process

occurrence

call by Pi is done
●

of lockstep

by any

does a simulated

does

if the

needed

decision).

simulation
●

of locksteps

final

that

means

between
the
before

the

difference
starting

ObjectResultType
State

q.,

integer

W,

Graph

function

SinStrategy(

Initial

state of simulation
number
*/

/*

G,

/*

/*

Sire.

Connected
Count

Eq.
ofcalls

integer t, /*

Graph
*/

ObjectResultType
function
state
go, /*
Initial
integer
w, /*
Sire.

*/
*/

simulation

algorithm

for

Figure

the

next

equivalence

return

one.

I.e.,

a correct

For
require

that
and

ent results

does

from

different

Since

from

the

the

the first

others

choose

process

simulations

cannow
the

have the one result

from

SimEdge

the

given

and

that
the

from

For

consensus

number

n—

The

High

the

calls

fact

result
means
the

an incorrect

a legal

calls

result

return

some

is chosen.

But

For
that

that

some

strategies

from

all

there

would

known

can

remaining

to

ordering

values.

I.e.,

at

least

strategy

and

would

not

still

is more

h.~ (T’)

This

with

gives

types

T,

of type

T

consensus

than
for

enough

these

n–process

cannot

to an-

types:
types

are less than

combined

T

n then

solve

ob-

n–process

but

they

is in

late

❑

type

T

each

can

in turn

and

T’

be

cannot

do

simulated

cannot

the

High

Gap

strictly

by

do n–process

Theorem

between

In [BNP94]

non-trivial

deterministic

readlwrite

memory

All
in

2–process

exactly

implies

2–process

it is shown
type
(a

low

can
gap

con-

that

ob-

be used

theorem).

one

read/writ

of

deterministic

the

following

e shared

types

can

categories:

lo-

variables,

or 2–process

types.

class

common2

of standard

types

was
that

all can do 2–process
any other

common2

352

which

processes,

variables,

set

of
then

we have:

The

that

objects

are no types

of any

consensus

are

be in I’.

all

type’s

•l

simulate

cal

be elim-

of them

type

question

deterministic

and read/writes.

placed

results

information
one

of any

and

T and T’

Corollary.

re-

values

betwostrategies

Tn_l.

n or objects

two

1 objects,

two

jects

only

calls

are either

n–process

deterministic

number

Theorem

consensus

there

will

different

n–process

robustness
For

T.–

sensus

would

Gap

Since

edge the

different

all

if h~ (T)

consensus.

to use and it is correct

results

up to the

different
not

it

and

data.

1.

oft ype

Hence

to return

Otherwise,

are identical
return

ect

Hence

the same.

Hence,

an

T.

by objects

Jayanti’s

type

That

which

be in between

T has

n–process

differ-

mean

a type

is not,

Pi,

consensus.

different
But

out

others.

thesimstrategy
wrong

inated.

would

for

for

simplification:

Corollary:

T’,

algorithm

graphs

of them

either

jects

simulation.

For SimObj
quire

value

would

find

drop

cannot

Corollary.

in

always

its edge simulations.

and

for

exist

return

one

following

swer

Pi.

simulation

equivalence

can besimulated

will

is returned.

Edge

or

Proof:

simulations

has also started

means

fact

an incorrect

one

edge

in

the

power

SimEdge.

edge

“wrong”

not

all processes

to return

the

process

edge
Hence,

value

Sim%rategy

results
other

the

graph.

5.3a

connected

and
the

*/

integer
mychoice
/*
j,k
choice
*/
ObjectInputType
I, Ij,
Ik)
/*
Input
values to calls */
shared
variables
integer
choice
[l.
.lul+l][l.
.n]
/*
for Locksteps
*/
integer
winner
[l.
.Ial+l][l.
.n]
initially
O
static
variables
integer
m /*
index in schedules
*/
Boolean
decided
initially
false
/*
chosen asim.?
*/
State
q[l.
.2]
/*
states after qo for j and k */
ScheduleBound
c7[I
..2] /*
subsim.
bounds
*/
Schedule
s[l.
.2],
S /*
sequences
for edge */
ObjectResultType
result
[l.
.2]
/*
results
of simulations
*/
ObjectResultType
RII.
.2]
/*
Results
from
lstepof
6 */

/*
Edge simulation
numbers
*/
set
of Edge S /*
set ofedges
to simulate
*/
local
variables
integer
j,k
/*
Process numbers
for an edge */
/*
Use the right
shared
variables
*/
with
SharedMem[w]
do
if
t = 1 then
/*
First
call choose an edge */
I[i]
= I /*
Tell others
input
to call */
~ = EdgeChoi.ce(G,i,chosen,active)
forall
(j,k)E~
do
if
active[j]
then
mychoice[j,k]
= j /*
Initial
choice
*/
else
mychoice[j,k]
= k
/*
Assign
Sire, Nos. */
w[j,k]
= SimNum(qO, w,(j,k),-y)
forall
(j,k)E~
do /*
Sire. each edge lstep
*/
result[j,k]
= SimEdge(go,w[j,k],G,
j,k,a,t,
mychoice[j,k]
,I,I[j],IUil
)
if
chosen
# null
then
/*
Only
1 edge tosim.
*/
E ={chosen}
return
result[j,k]
for
any (j,k)E~
5.2 Strategy–level

*/

Graph
G, /*
Connected
Eq. Graph
*/
integer
j,k,
/*
Processes
on edge */
ScheduleBound
u,
/*
Max. num. steps
integer
t,
/*
Count
of calls
*/

ObjectInputType
I,
/*
Input
values tocall
*/
ScheduleBound
C, /*
Max. num. steps */
Strategy
y)
/*
The strategy
to use */
shared
variables
Edge chosen
/*
From EdgeChoice
*/
Boolean
active[l.
.n]
/*
From EdgeChoice
*/
ObjectInputType
l[l.
.n],
/*
Inputs
to calls */
static
variables
integer
mycholce[l
..n,l.
.n]
/*
current
choice ofwinners
*/
ObjectResultType
result[l.
.n,l.
.n]
/*
Results
ofedge
simulations
*/
integer
w[l.
.n,l.
.n]

Figure

SimEdge(
state of simulation
number
*/

type

introduced
have

consensus

in common2.

can be simulated

in

two

[AWW3]

main

and

each

As a result,

by any

2–process

as a

properties:
can simuall types
type

in

that

/* Use the right shared variables
*/
with
SharedMem[w]
do
if t=l
then /* first call? */
/* If j goes first...
*/
(q[lI,R[ll
)=6(q0, fi(j),
Ij)
*/
/* If k goes first,..
(q[l]
,R[21 )=6(qo,
T(k)
,Ik)
/* What seqs. to simulate
*/
(S [11 , S [21 ,S) = sequences and overlap seq.
in G
for edge (j ,k)
/* smaller schedule bounds for subsimulations
*/
a[ll
= (01,. ... ol, l)...)
0[2]
= (al, . . ..al.
l)...)
if not decided
then /* still simulate both */
while
true
do /* Cycle thru schedules */
if POS(S II] ,i, t) = m then
/* 1 step for i in S[1]
*/
[I]
= R[l]
/*
i=j
*/
if t=l
then result
else
result
[1]=
SimObject(q[l]
,uII]
,w, t,I)
if Pos(S[2],
i,t)
= m then
/* lstepfor
i in S[21 */
i.f t=l
then result
[2] = R[2] /* i=k */
else result
[21=
SimObject(q[2]
,v[2]
,w, t,I)
/* Done all steps? */
if Pos(s, i,t)
< m then exit
loop;
/* Doalockstep
*/
mychoice
=
lockstep(i.,
mychoice,
winner [ml ,choice[ml
);
m++;
i.f result
[l]
= result
[2] then
return
result
[i]
/* no need to choose */
else /* must choose */
for mflnal
= m to Iu\+l do
/* force others to finish */
mychoice
=
lockstep(i,mychoi.ce,lock[mfinal]
);
decided
= true;
return
result[mychoice];
/* made choice */
else
result[mychoice]
= SimObject(q[mychoice]
,
a[mychoice],w,t,I)
return
result[mychoice]
Figure

5.3b Edge simulation

can do 2–process
combined
Corollary.

with

consensus.

the corollary

Let Tbe

algorithm
This

for Pi, code.

latter

point,

when

above, gives:

anyn–process

type

that

canbe

simulated
by some 2–process type.
Either T can be
simulated
by read/write
shared variables or T can be
added to common2.
6.

Conclusions

and

open

problems.

Note that the High Gap Theorem gives a gap between
n–process types with consensus number n and all types
It’s easthat have consensus number
less than n.
ily shown that no such gap exists at even the next
lower level for n–process types.
Hence robustness for
n–process types below n–process consensus is an open
question.

However,

if a “universal”

set of simple

types

were found such that all types could be classified as
being equivalent
to exactly one universal type, then a
technique similar to the one used in this paper might

3.53

be able toprove
While

this more general

a useful characterization

robustness.
for the high end of the

n–process type hierarchy
has now been found, at the
opposite end there is still a need for a comparable
characterization.
Note that for 2–processes there is a significant gap between readjwrite
and non–readiwrite
memory (i.e., 2–process

consensus

ture of the gap between
memory

types),

readiwrite

for more than two processes?

and Shavit

[HS94] have recently

of a very restricted

What

is the na-

and non-readlwrite
Note that

Herlihy

given a characterization

form of wait–free

computations

that

can be solved with read/write
memory. However, their
methods are extremely complex and their results do not
directly apply to our more general definition
of types.
Much simpler characterizations,
such as our High Gap
Theorem, are needed if they are to be used to study the
power of types.
Bibliography
[AWW93] Yehuda Afek, Eytan Weisberger,
and Hanan
Weisman.
“A completeness theorem for a class of synchronization
objects.”
In Proceedings
of the Twelfth
ACM Symposium on Principles
of Distributed
Computing, pages 159–170. ACM Press, August 1993.
[BNP94] Rida
son. “On the
To appear in
on Principles

A. Bazzi, Gil Neiger, and Gary L. Peteruse of registers in wait–free
consensus.”
Proceedings of the 13th ACM Symposium
of Distributed
Computing,
1994.

“Agreement
is harder than
[Cha93] Soma Chaudhuri.
consensus:
Set consensus problems
in totally
asynInformation
and Computation,
chronous
systems.”’
103(1):132-158,
July 1993.
[Her91] Maurice Herlihy.
“Wait-free
synchronization.”
ACM
Transactions
on Programming
Languages
and
Systems, 13(1):124-149,
January 1991.
[HS94J Maurice Herlihy and Nir Shavit.
“A simple constructive computability
theorem for wait–free computation.”
In Proceedings of the 26th ACM Symposium
on
Theory of Computing,
1994.
“On the robustness of Herlihy’s
[Jay93] Prasad Jayanti.
In Proceedings of the Twelfth ACM Symhierarchy.”
posium on Principles
of Distributed
Computing,
pages
145-158. ACM Press, August 1993.
[KM93] Jon Kleinberg
and Sendhil Mullainathan.
“Resource bounds and combinations
of consensus objects.”
In Proceedings of the Twelfth ACM Symposium on Principles of Distributed
Computing,
pages 133–144. ACM
Press, August 1993.
[LA87] Michael C. Loui and Hosame H. Abu-Amara.
‘Memory requirements
for agreement among unreliable
asynchronous processors.”
In Franco P. Preparata,
editor, Advances m Computing
Research, volume 4, pages
163-183. JAI Press, 1987.
~l~~~~~;;~~,~lotkin.
posium
159-175.

“Sticky bits and the universality
In Proceedings of the 8th ACM Symon Principles
of Distributed
Computing,
1989,

[PR79]

John

person

alternation.”

Foundations

Reif

and Gary L. Peterson.
“MultipleIn
Proc.
20zh IEEE ,$’ymp. on
of Computer Science, 1979, 348-363.

2015 IEEE 8th International Conference on Cloud Computing

Instrumentation and Trace Analysis for Ad-hoc
Python Workﬂows in Cloud Environments
Ruben Acuña

Zoé Lacroix

Rida A. Bazzi

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287, USA

School of Electrical, Computer
and Energy Engineering
Arizona State University
Tempe, AZ 85287, USA
Email: zoe.lacroix@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287, USA

Abstract—Knowledge of structure is critical to map legacy
workﬂows to environments suitable to run on the cloud. We
present a method which characterizes a workﬂow structure
with the execution trace produced by instrumented logging
functionality. The method generates the structure of workﬂows
to support their reuse by permitting their transformation into
modern execution environments. The method presented in the
paper is implemented for Python workﬂows and demonstrated
in the context of several legacy scientiﬁc workﬂows.

I.

omit external tools. At present, the workﬂow community
lacks automated tools for extracting structure from ad-hoc
workﬂows. Knowledge of such structure is critical to map
legacy workﬂows to environments suitable to run on the cloud.
The method presented in this paper addresses the problem of
analyzing legacy ad-hoc workﬂows, capturing their internal
orchestration, and producing a new representation for their
execution in the cloud.

I NTRODUCTION

We focus on executable Python workﬂows for which a
valid input is available. This allows the characterization of
the workﬂow by its execution trace, produced with logging
functionality. There has been interest in mining process models
from business workﬂow logs. Constructing process models
was ﬁrst presented, for logs produced by IBM Flowmark, in
[10]. Other types of process models such as Petri nets have
also been explored [11]. More recent work has focused on
trace reconstruction in speciﬁc environments. The issue of
concurrent workﬂow logs, by using temporal dependencies and
reﬁnement is addressed in [12]. In these cases, the execution
platform provides well formatted logs for analysis. Learning
workﬂow traces from users’ usage patterns is an active area
of research. For traces produced by an expert user using
services in a medical domain, [13] presented a model merging
technique to learn repetition and branching. This was explored
more fully in the POIROT project [14] which combines trace
analysis and learning methods to deduce procedural models.
An ontology query method to infer regions of missing dataﬂow
was used for traces produced by expert users which include
unobservable choices or actions in [15]. Analysis of traces for
provenance is also valuable, for instance [16] gives a method
for differencing executions to understand provenance.

Large scale computing was ﬁrst enabled by the advent
of Grid computing. Workﬂow management systems (WFMS)
that target Grid computing environments, such as Pegasus
[1], have been around for some years. However, the Grid
environment has limitations with resource availability and
maintenance overhead [2], [3]. Cloud computing has addressed
many of these practical concerns by providing a distributed
and uniform environment [3]. One aspect of cloud computing
is to execute workﬂows on extremely large datasets such as
needed by bioinformatics. Cloud computing can provide both
tractability and scalability for such work (see [3] for a concrete
case). There is a critical need to adapt current workﬂows to
take advantage of cloud environments. Although WFMS can
enable scientists to target the cloud, legacy workﬂows are
typically excluded. One approach to legacy workﬂow reuse
is encapsulating them as resources which can be deployed by
a larger Cloud or Grid aware workﬂow [4].
Each WFMS is designed with its own paradigm, including
internal workﬂow model and execution engine. While systems
adapted to execution on a local system, such as Galaxy [5],
are suitable to run on the cloud environment others are less
appropriate (e.g., Taverna [6], oriented to service utilization).
This leads to the need for mapping workﬂows designed for
a given WFMS to another, to beneﬁt from a more efﬁcient
environment. For instance, Tavaxy [7] provides interoperability
between Taverna and Galaxy. However many legacy scientiﬁc
workﬂows were not implemented with a WFMS. So-called adhoc workﬂows are implemented directly in a language such
as Python. Scripts are developed either as an orchestrational
program, designed to execute applications and connect their
outputs, or may also contain algorithms which are used to
guide the process. Although methods for tracking provenance
in ad-hoc workﬂow provide mechanisms to detect and record
dataﬂow (e.g., [8], [9]), they focus on direct ﬁle IO and
2159-6190/15 $31.00 © 2015 IEEE
DOI 10.1109/CLOUD.2015.25

This paper is organized as follows. In Section II, we discuss
the factors that lead us to a trace instrumentation approach.
We then deﬁne the terminology and clarify the scope of the
method. In Sections III and IV, we detail the instrumentation
method and how the trace of the instrumented workﬂow
execution is used to produce a more abstract representation of
the workﬂow which captures its overall structure. Section V
illustrates the application of the method to a workﬂow for
protein synthesis and its performance impact. In Section VI,
we conclude with a discussion and future extensions.
114

II.

P RELIMINARY

and/or statically typed. Work such as Shed Skin [21] have
attempted to provide type inference functionality, and in the
process have demonstrated the issues with static analysis in
Python. While static analysis may be the only way to guarantee
the complete characterization of a workﬂow, our initial work
on a static analysis approach, and reviewing real world adhoc workﬂows collected on GitHub, indicated it may not
be tenable. This is partially due to the use of code that
unintentionally invokes many dynamic language features [19].
Performing an analysis of the trace of a workﬂow execution
sidesteps these issues. Other work related to dataﬂow in Python
programs has taken the approach of creating an instrumented
interpreter [22]. This approach may produce a log of events
without the manipulation of the workﬂow code. However, this
requires that a separate interpreter be installed on a system;
which is problematic in the face of interpreter changes. In
contrast, our method captures the level of abstraction that the
workﬂow designer considers: libraries. This takes the form of
a thin layer of code between a workﬂow and the libraries,
thus logging exactly the events the workﬂow designer has
explicitly created. An advantage of instrumentation at this level
is that such a thin layer is more amenable to changes in the
interpreter. If another interpreter such as PyPy [23] is used, or
if the interpreter is running on a different operating system,
the instrumentation will still function.

The ﬁrst step of the method is to produce an instrumentation of the workﬂow. Workﬂow instrumentation is the process
of adding elements (i.e., code instructions) to monitor and
record behavior during execution. Such an instrumentation
produces a description of the workﬂow structure in terms of
calls to tools, algorithms and methods, for a given execution.
A workﬂow instrumentation thus provides a layer between the
workﬂow and the language libraries. The second step is to
analyze a trace produced by the execution of the instrumented
workﬂow on a given input to construct a dataﬂow graph (i.e.,
a data dependency graph). When events such as a system call,
or accessing a ﬁle, occur in the instrumented workﬂow, they
are recorded with the ﬁle system state. The execution of the
instrumented workﬂow produces an event log which can be
analyzed to determine data dependencies. The dataﬂow graph
is created by analyzing ﬁle system changes in the context of
the commands being executed. This graph represents the ﬁle
dependencies for events and can provide an initial workﬂow
structure.
Scientiﬁc data management and bioinformatics provide an
active application domain for workﬂows as discussed in [17],
[18]. In [19], we ﬁrst discussed the various impacts of workﬂow transformation and illustrated them with a case study on
the Structural Prediction for pRotein fOlding UTility System
(SPROUTS) Workﬂow [20]. We argued that a better structural
description for workﬂows in terms of the resources it invokes is
critical to support workﬂow maintenance and transformation,
workﬂow reuse, resource discovery and adaptation. To tackle
the challenging task of workﬂow instrumentation we choose
to develop our method to support Python workﬂows. We will
ﬁrst introduce our terminology and discuss the reasons for our
design choices. We then specify the scope of our method.

C. Scope
At this stage of development we limit the scope of our
method as follows. (a) The dataﬂow is analyzed in terms of the
ﬁlesystem. For many of the workﬂows that we examined, ﬁles
are sufﬁcient to characterize an execution’s dataﬂow. There are
scientiﬁc (e.g., reproducibility) as well as practical (e.g., efﬁciency) reasons for such workﬂows to be preferred. However,
workﬂows using other mechanisms such as web services cannot be completely characterized. (b) The method is designed
to process workﬂows written in Python. According to GitHub
statistics, Python and Perl are used in the majority of hosted
bioinformatics workﬂows or pipelines. However, any language
(e.g., Perl), with a instrumentation engine meeting our requirements, may beneﬁt from our dataﬂow construction methods.
(c) As the method exploits workﬂow traces, a prerequisite is a
functional workﬂow. From a workﬂow reuse perspective, this is
a signiﬁcant drawback as a workﬂow must be maintained until
it is traced. One option is to make performing a trace part of an
archival process to be completed after a workﬂow as served its
primary purpose. Our instrumentation approach is light enough
that a user can easily complete such a task. (d) Traces are
analyzed as semi-structured views of workﬂows. While a trace
gives some insight into the overall orchestration of tasks in a
workﬂow, a trace is not a generalization of how a workﬂow
processes every input. The internal logic which occurred to
generate the speciﬁc interdependencies exists at a higher level
of abstraction. However, a trace always provides a valid view
of a workﬂow. Given multiple executions of a workﬂow, it
is possible to learn a more generalized workﬂow structure
based on the similarities between traces. This is a similar
situation as systems trying to determine how data are produced
on the Web. Although fully structured by the authority that
designed the Web resource, they appear to the other end semistructured as their structure may have desiccated over time
[24]. The secondary issue of determining what latent decisions

A. Terminology
A workﬂow is a program (i.e., Python script) which orchestrates a set of external tools, managing their interdependencies
and dataﬂow, to produce an output with respect to some input.
A job is the execution of a workﬂow on a given input. A
workﬂow may be executed on an input ﬁle and systemically
provides one or more output ﬁles. A tool is an executable
program which takes one or more input ﬁles and may produce
output ﬁles. A tool may be invoked in various ways, based
on a usage proﬁle (i.e., the parameters that it is given). All
tool invocations constitute an instance of a usage proﬁle with
respect to some speciﬁed input and output data. A trace is
a representation of the execution of a workﬂow on a speciﬁc
job, typically an event log. A job relies on information gathered
from two sources: a workﬂow’s library which is a collection
of information built into the workﬂow as a local resource and
the job’s (speciﬁc) input. Rather than being speciﬁc to a job,
libraries are part of workﬂows as standardized inputs. A job’s
input is the collection of ﬁles speciﬁc to its execution.
B. Design Choices
Static analysis techniques may inspect a workﬂow’s source
code to track dependencies among regions of code executing
external programs. However, these dataﬂow techniques are
more suitable for programming languages which are compiled

115

were made or not in a workﬂow’s execution, dependent on
input, is analogous to that of determining the unobservable
choices made by a human operator when implementing a
procedural workﬂow, a problem examined in [15]. (e) Dataﬂow
graphs capture workﬂow explicit tools. Indeed, traces depend
on the manipulation of ﬁles by tools, therefore they cannot
represent the logic internal to a workﬂow which forms an
implicit tool. Although a workﬂow may read or write a ﬁle,
the method cannot determine the manipulation applied and
its dependencies. This can be seen as a problem of program
slicing [25], where the goal is to determine exactly the part
of the workﬂow orchestrational program which corresponds to
an internal tool that manipulates ﬁles.
III.

TABLE I.

I NSTRUMENTED STANDARD LIBRARY COMPONENTS .
Module

Type

Name

Event Type

builtin
codecs
os
subprocess
subprocess
subprocess

function
function
function
function
function
class

open
open
system
call
check call
Popen

internal
internal
external
external
external
external

is scanned to determine which libraries are being used. The
instrumented standard library components shown in Table I
represent all of ﬁle IO mechanisms from the standard library
that were utilized by the suite of workﬂows we reviewed in our
evaluation study. Whenever a module (e.g., from the standard
library) is loaded, that particular module must be instrumented.
The instrumentation module works by preserving access to
the latest loaded function(s) and inserting a new wrapper
function(s) in the runtime. If performed immediately after a
module is loaded, it is guaranteed that any dynamic use of
that name will instead use the instrumented version.

W ORKFLOW I NSTRUMENTATION

The instrumented workﬂow is an equivalent workﬂow
which produces a log at run time, containing information on its
interactions with the ﬁle system. The instrumentation is transparent to the execution in the sense that it does not affect any
part of the dataﬂow and only monitors the relevant calls with
a wrapper which intercepts functions when they are executed
by the workﬂow’s ﬂow control, records their parameters, and
returns control to the standard library. The overall process for
instrumentation and trace generation is: Identify events to be
monitored; Instrument the workﬂow; Execute the instrumented
workﬂow; and Construct the dataﬂow graph.

The ﬁlesystem state is recorded for all external events. The
speciﬁc os.command issued to the system is also recorded.
The other external event corresponds to subprocess.Popen,
which returns an object representing the result of the ongoing execution of a command. Unlike os.command, Popen
includes extra functionality for handing streams. In normal
operation, Popen is designed to be executed non-blocking. To
properly capture the ﬁle system after the execution of the tool,
the instrumented Popen waits for the command’s completion.
Because it is already possible that a Popen invocation completes at any time, this causes no side effects in the execution
of the instrumented workﬂow. The subprocess module also
includes call and check call which are degenerate cases of
Popen. In addition to ﬁle system access, tools executed by
Popen may also receive input and output from the standard
streams: STDIN, STDOUT, and STDERR. It is a somewhat
common pattern that scientiﬁc tools use these streams as their
default means to input or output a single ﬁle. Prior to executing
a command, STDIN is checked for presence of a ﬁle like
object and its hash is recorded, when applicable. Additionally,
STDOUT or STDERR streams are checked for the presence
of a ﬁle like object. If either is a ﬁle, it is ﬂushed, hashed, and
then replaced with an approximate ﬁle object.

The instrumentation aims at recording internal and external
events. Internal events are characterized by the workﬂow’s use
of Python ﬁle IO. External events are system calls, typically
an invocation, which may embody other tasks (e.g., copying
a ﬁle). For each event type, the log must record information
about the workﬂow and the ﬁlesystem, as well as information
inherent to the event type. Existing tools such as strace
or systemtap on Linux can be used to log all interactions
between a process and the OS. Although this provides a more
informative trace, the ﬁle access overhead of the default Python
interpreter means such a trace would need to be ﬁltered. At
present, the ﬁlesystem is recorded directly as a snapshot of
the MD5 [26] hashes for each ﬁle in the workﬂow’s folder.
A speciﬁc path is used to denote the extent of the workﬂow
thus specifying the ﬁlesystem that must be analyzed. The
representation of an entire region of the ﬁlesystem is required
because an event’s interaction with the ﬁlesystem cannot be
determined solely from its parameters. The comparison of
before and after snapshots of a ﬁlesystem for each event
captures some behavior of the system. If a ﬁle exists prior
to an invocation and is not changed, it is possible, but not
certain, that a tool may have read it. If a ﬁle changes, then
data were written in and the ﬁle was possibly read. If a ﬁle
exists only in the after snapshot, it is likely the output of an
invocation. However, a workﬂow may be parallel, thus, this
may not be certain.

Internal events, BUILTIN .open and codecs.open, behave
differently. Rather than representing the single execution of
a command, they produce a ﬁle object which is then later
manipulated by the workﬂow. When such an object is created,
Python loads the ﬁle. The instrumentation delays this operation
and loads the ﬁle instead for hashing, thus a snapshot of its
state prior to opening is saved. In addition to recording the
initial event of opening a ﬁle, the instrumentation also records
when the ﬁle is closed.

Although a workﬂow is a single program, it may be
deﬁned with several scripts across multiple ﬁles, all of which
must be instrumented. They are identiﬁed with snakefood1 ,
which creates a module dependency graph for a set of ﬁles.
Snakefood recursively parses source code ﬁles to determine
which other ﬁles they depend on. Each of the scripts identiﬁed

IV.

C ONSTRUCTION OF THE DATAFLOW G RAPH

Once the workﬂow has been instrumented and an execution
recorded, the method analyses the execution trace and produces the dataﬂow graph as well as a library of application
resources. The nodes of the dataﬂow graph represent events
recorded in the trace and its edges correspond to ﬁle depen-

1 http://furius.ca/snakefood/

116

dencies. The resource library is the list of tools identiﬁed in
the trace and usage proﬁles.

fact that a ﬁle was accessed without information about the
purpose and result of that access. When a ﬁle is read, it is
captured as a command (really part of the workﬂow) with the
ﬁle being opened as its sole dependency. When a ﬁle is closed,
it is captured as a command which produces a single ﬁle.
External events denote the occurrences when the workﬂow
is making a system call. These are typically the invocations
of scientiﬁc or data preparation tools. While any execution
of a tool is an invocation, each different invocation of a tool
may involve different usage proﬁle. For each external event,
we check if the application being executed has already been
seen, existing usage proﬁles are then used to analyze the state
of the workﬂow. An IORecord for the event is constructed
using the usage proﬁles as follows. The local ﬁle system is
examined to determine the ﬁles relevant to the event which
satisfy the ports and indirect patterns found in the usage proﬁle.
If the invocation’s command cannot be parsed, or, if it does
not explain the changes in the ﬁlesystem, a new usage proﬁle
is created. A usage proﬁle deﬁnes not only an abstract system
command used to execute a tool, but expected input and output
dependencies for the tool.

The graph contains two types of nodes: external and
internal. Each node contains an IORecord produced from
the analysis the associated command, and a reference to the
application library. An IORecord is the encapsulation of how
a particular event interacted with the ﬁle system during an
execution. Each IORecord is generated by the application
of a usage pattern to an invocation. Each usage proﬁle has
some speciﬁc ports, which represents speciﬁc ﬁles or folders
that an invocation relies upon. For invocations, ports become
bound to values (ﬁle or folder names). An IORecord tracks
the values for the input and output ports speciﬁed by the
usage proﬁle. An IORecord optionally contains zero or one
input and output streams. A dataﬂow graph is initiated with
three special nodes, which are taken to be external events. The
graph is ﬁrst initiated with a Source node which acts as a
tool which produces the initial input ﬁles. The second node
is the Library node which acts as a tool which produces
any ﬁle existing prior to the workﬂow’s execution which is
not an input. Lastly, a node called Sink acts as a tool whose
input dependency is every ﬁle which exists at the end of a
workﬂow’s execution. This ensures that the graph details any
output ﬁle the workﬂow is intended to generate. The analysis
of the execution trace populates the dataﬂow graph as follows.

C. File Dependencies
We identify three types of dependencies to capture between
ﬁles. A direct ﬁle dependency occurs when an invocation
command names exactly the ﬁle or folder being used. A
ﬁle dependency is indirect when an invocation command
names a substring of a ﬁle or folder being used. The third
category includes the cases when a ﬁle or folder changes
between invocation ﬁlesystem snapshots and some general
data usage pattern is able to capture it. The latter are called
implicit dependencies. We deﬁne a set of heuristics for each
dependency type, organized as a hierarchy, to identify the
dependencies and produce the dependency graph.

A. Event Refactoring
The aim of this step is to streamline events such that each
corresponds to the execution of exactly one tool on a single
input. Indeed the trace of the execution of the instrumented
workﬂow may contain external events (i.e., system commands)
which may not be tool invocations. A system command
typically involves the execution of a program but may display
additional behavior from shell syntax. These events are thus
restructured into a more atomic form that accounts for the
action of these features. The trace is analyzed and transformed
with three steps: Removal of auxiliary (decorator) commands;
Separating commands communicating by pipe operator; and
Deconstructing GNU parallel into multiple commands.
Some commands can be wrapped within an application in
such way that the wrapper does not effect the command (e.g.,
the program time displays the run time of a command). In
those cases the command is irrelevant and removed from the
description of the event. The pipe operator which provides a
simple mechanism to compose programs by passing streams
from one to another if often used in scientiﬁc workﬂows. When
a command contains a pipe, the command is split into two
commands. In ad-hoc workﬂows GNU parallel is often
used to enable a degree of parallelism. When it occurs, the
command is restructured into individual commands by using
the --dry-run capability of parallel to determine the
exact commands that parallel would issue.

Direct dependencies are captured as follows. The ﬁrst step
consists of the identiﬁcation of the portions of the active
command which may correspond to a port; these are direct
access. This is done by tokenizing the command on spaces
and then trying to mount each token as either a ﬁle or folder
in the ﬁlesystem known from the snapshot. Each matching
token is greedily assumed to be a port. A command pattern
is then created by replacing each port substring replaced by
a symbol port name and number. Next, the method checks
for indirect access with two heuristic rules. The ﬁrst heuristic
detects grouped ﬁles, typically seen in programs which utilize
a named database shared between a collection of ﬁles. For
each of the ﬁle ports, the local ﬁle system is examined for
ﬁles which contain its name as a preﬁx. This has the effect
of a expanding a single ﬁle port into a set of ﬁles, which
are speciﬁed. The second indirect heuristic rule enables the
detection of collections of folders when tools perform some
division operation on a single input. This rule is only active
when more than one folder containing ﬁles has been created.
A pattern is guessed from a sample folder, where local names
(e.g., folder or ﬁle names) are abstracted away. The potential
pattern is then checked against the other folders. Whichever
sample folder provides the best coverage of folders (i.e.,
captures the entire folder collection) is selected as the indirect
folder collection rule. Lastly, the method checks for implicit
outputs. This heuristic assumes that only the active event is
being executed by the workﬂow. When GNU parallel is

B. Command Analysis
Once atomic events have been identiﬁed, the next step
is the identiﬁcation of the functional dependencies existing
between them. We ﬁrst analyze each event to identify how
it interacts with the ﬁle system. An internal event denotes
the direct interaction of a workﬂow with the ﬁlesystem. This
kind of event can be understood as a stub representing the

117

being used, this heuristic must be disallowed. Any ﬁle which
appears in the working directory of the active program is
assumed to be an output. Such a ﬁle is assumed to have a
static name which will be the same for any invocation.

were performed in a virtualized ﬁlesystem for reproducibility.
The third-party workﬂows were installed using the instructions
in their respective readme ﬁles.
The method was tested and evaluated on twelve workﬂows: eight synthetic workﬂows designed as a benchmark as
well as four workﬂows retrieved from GitHub. The eighth
workﬂow, a workﬂow in the bioinformatics domain which
simulates Protein Synthesis, will be described in details in
Section V-A. The characteristics of the other workﬂows are
listed in Table II. Each workﬂow is listed with its size
in terms of lines of code (LOC), exclusive of tools, and
the number of dataﬂow graph elements that are found. The
benchmark workﬂows were designed to provide code coverage
of individual structural components seen in the workﬂows
from GitHub. The workﬂows selected from GitHub are in the
bioinformatics domain with input ﬁles available and contain
tools available for the Linux environment. Four workﬂows
were selected: HybSeqPipeline [27], Inmembrane [28], Pycoevol [29], and miR-PREFeR [30]. The method produced a
complete dataﬂow graph for HybSeqPipeline and Inmembrane.
The dataﬂow graph for HybSeqPipeline contain 512 nodes
with edges for 3065 ﬁles, partially displayed in Figure 3.
The method produced a complete dataﬂow graph containing
55 nodes with edges for 3713 ﬁles for Inmembrane. All
nodes are produced by tools with the exception of a single
node produced by an internal event. It is worth noting that
some workﬂow executions contain unused nodes. Workﬂows
may create tool nodes without ﬁle dependencies when they
are checking if a tool is installed. The other two workﬂows
selected from GitHub demonstrate some of the limitations we
discussed in Subsection II-C; they contain ﬁles generated by
untracked events. Pycoevol uses web services in addition to
local tools whereas miR-PREFeR uses tools which involve
external temporary folders.

The three types of heuristics are applied with respect to the
following rules. An indirect heuristic is only applied when its
application will not cause any conﬂict (overlap) with the initial
direct ports discovered or another indirect heuristic being
applied. An implicit heuristic is only applied when no conﬂict
with the direct ports or indirect heuristics would be caused by
its application. The method completes and produces a correct
set of IORecord as needed to construct the dataﬂow graph
as explained in Section IV-D. The analysis of an event always
produces an IORecord. Because the process for generating
IORecord is greedy, its correctness is partially checked by
tracking a virtual representation of the ﬁlesystem. Any ﬁle
being read must be available in the virtual ﬁlesystem. Provided
this is the case, ﬁles being written are then updated in the
virtual ﬁlesystem and are checked against the log. In this way,
the cumulative changes to ﬁle system are simulated.
D. Dependency Graph
Once the set of IORecord has been produced for all
events, the dependency graph (or dataﬂow graph) may be constructed. A node is created for each event (i.e., command) in
addition to the three special nodes (i.e., Library, Source,
and Sink). A command node is speciﬁed by its ﬁle inputs as
well as a possible stream input (i.e., STDIN). The treatment
of streams is different from ﬁles. As part of dependency
construction the aim is to minimize formalities that occur as
common patterns. For example, a ﬁle may be opened in an
internal event although later, that same ﬁle is used as stream
input for an external event. In this case, the internal event is
purely preparatory for the external event and may be replaced
by modifying the external event to directly take the ﬁle as
stream input. Because workﬂows may display parallelism, the
order of the log events cannot always determine how tools are
linked. A ﬁle might not be immediately used; but later might
be matched to the hash produced by the closing of an internal
event or read by the opening of a internal event. Instead, the
algorithm authorizes any tool in the sequence to produce output
for any other tool. There are two possible criteria for matching
a piece of data: hash and ﬁle path. When a ﬁle is used by
an invocation both must match, whereas only the hash must
match for stream usage. For both types of inputs (ﬁle and
stream), a list of candidate source nodes is gathered. A node
is a candidate if it provides the data’s hash as either a ﬁle or
an output stream. Each of these deﬁnes a series of priority
orders for candidate provider nodes to be accepted. In general
external producers of data are preferred over internal sources
and ﬁle sources are preferred over stream sources.
V.

TABLE II.
Workﬂow
test1
test2
test3
test4
test5
test6
test7
test8
HybSeqPipeline
inmembrane
Pycoevol
miR-PREFeR

LOC
20
44
10
16
28
29
12
25
362
2561
4206
3278

C HARACTERISTICS OF TEST WORKFLOWS
Dataﬂow Nodes
6
7
6
8
8
8
6
11
512
55
3112
1160

Dataﬂow Edges
4
3
3
5
5
5
9
16
3065
3713
2092
1194

A. Protein Synthesis Workﬂow
We illustrate our method with a workﬂow in the bioinformatics domain which simulates Protein Synthesis. The input
to the workﬂow is one or more DNA sequences stored in
a ﬁle in FASTA format. The output of the workﬂow is a
folder called aa which contains separate FASTA ﬁles for
each sequence that was found in the input. Each of these
separated inputs undergoes the protein synthesis process of
transcription followed by translation. The source code for
this workﬂow shown in Figure 1 relies on three external tools (split_multifasta.py, dna2rna.py, and
rna2aa.py) to split a FASTA ﬁle, and perform transcription
and translation, respectively.

E VALUATION AND R ESULTS

The trace engine and dataﬂow reconstruction programs
were developed and executed on Python 2.7.6 under Xubuntu
14.04. They use procedural and object-orientated approaches.
Performance evaluations were performed on a virtual machine
with an Intel 2500K (at 4.5Ghz), 4GB RAM, and a 30GB
hard drive. All instrumented workﬂows and trace analyses were
run on the standard CPython 2.7.6 interpreter. Workﬂow traces

118

Fig. 1.

Source code for Protein Synthesis workﬂow.

import sys, os
input_name = sys.argv[1]
cmd = "split_multifasta.py -input " + input_name + " -outfolder dna"
os.system(cmd)
files = [f for f in os.listdir("./dna")]
for filename in files:
cmd = "dna2rna.py -inputfile dna/" + filename +" -outputfile rna/" + filename
os.system(cmd)
files = [f for f in os.listdir("./rna")]
for filename in files:
cmd = "rna2aa.py -inputfile rna/" + filename +" -outputfile aa/" + filename
os.system(cmd)

TABLE III.

S UMMARIZED EVENT LOG .

TABLE IV.

U SAGE PROFILES AND INVOCATIONS DISCOVERED .

Source

ID

Command

File Changes

Application

Proﬁle

Command

main.py

1

split multifasta.py -input seq.fa -outfolder dna

split multifasta.py

main.py
main.py
main.py
main.py
main.py
main.py

2
3
4
5
6
7

dna2rna.py -in dna/1BNI.fa -out rna/1BNI.fa
dna2rna.py -in dna/1ASU.fa -out rna/1ASU.fa
dna2rna.py -in dna/3OE0.fa -out rna/3OE0.fa
rna2aa.py -in rna/1BNI.fa -out aa/1BNI.fa
rna2aa.py -in rna/1ASU.fa -out aa/1ASU.fa
rna2aa.py -in rna/3OE0.fa -out aa/3OE0.fa

+ dna/1BNI.fa
+ dna/1ASU.fa
+ dna/3OE0.fa
+ rna/1BNI.fa
+ rna/1ASU.fa
+ rna/3OE0.fa
+ aa/1BNI.fa
+ aa/1ASU.fa
+ aa/3OE0.fa

1
(instance)
2
(instance)
(instance)
(instance)
3
(instance)
(instance)
(instance)

-input INPUT0 -outfolder FOLDER_OUT0
-input seq col.fa -outfolder dna
-inputﬁle INPUT0 -outputﬁle OUTPUT0
-inputﬁle dna/TEST1.fa -outputﬁle rna/TEST1.fa
-inputﬁle dna/TEST2.fa -outputﬁle rna/TEST2.fa
-inputﬁle dna/TEST3.fa -outputﬁle rna/TEST3.fa
-inputﬁle INPUT0 -outputﬁle OUTPUT0
-inputﬁle rna/TEST1.fa -outputﬁle aa/TEST1.fa
-inputﬁle rna/TEST2.fa -outputﬁle aa/TEST1.fa
-inputﬁle rna/TEST3.fa -outputﬁle aa/TEST1.fa

dna2rna.py

rna2aa.py

Table III reports on external events recorded at execution of the instrumented workﬂow on an input FASTA
(seq_col.fa) containing three sequences. The log shows
that the execution of the workﬂow triggered seven external
events. The ﬁrst was executing split_multifasta.py
on the input ﬁle. During this command, three new ﬁles were
created in the dna subfolder. The tool dna2rna.py was then
executed three times. Each time it read one of the ﬁles in the
dna subfolder and produced a corresponding ﬁle in the rna
subfolder. Last, the tool rna2aa.py tool was executed three
times. Each time it read one of the ﬁles in the rna subfolder
and produced a corresponding ﬁle in the aa subfolder. A
summary of the ﬁle system changed during an event is listed
in the column File Changes (the symbol + indicates that a ﬁle
was added).

Hence, the former provides seq_col.fa while the latter
does not provide any ﬁles. The dataﬂow graph represents the
structure of the workﬂow script, as an abstract view of the
workﬂow. This is the structure that can be used to transform
the workﬂow into a program that can be executed on the cloud.
It is worth observing that a repetitive process is being applied
to all elements created by some process. This implies that such
a workﬂow could be parallelized. This is not immediately clear
from the implementation which is strictly serial.
B. HybSeqPipeline Workﬂow
HybSeqPipeline is a sequence assembly workﬂow. It loads
a data ﬁle consisting of many short sequences, distributes
them into groups (using blastx [33]), and then assembles
each of those groups (using velvet [34]). This workﬂow is
only used to control the execution of tools and the paths of
ﬁles that they read or write. The workﬂow does not modify
the contents of ﬁles directly. Part of the graph produced by
analyzing HybSeqPipeline is shown in Figure 3. The top two
square nodes, library and source, are followed by six data
preparatory nodes and a node which distributes data to many
different processes. From this distributor node, the workﬂow
demonstrates a pattern of two sequential nodes whose output
is then combined, ﬁve at a time, and processed by a third node.
This comprises the middle region of the ﬁgure. For some of
these nodes, a fourth tool is also applied; this can be seen in
the paths to the left. Due to display limitations, we show only

The log was then analyzed (see results in Table IV) to
produce the dataﬂow graph displayed in Figure 2 stored in
GraphML [31] and visualized with Graphviz [32] in hierarchical layout mode. Three applications were discovered with
one usage proﬁle each with a number of instances. Each
usage proﬁle corresponds to one of the system calls, while
the instances correspond to each time a particular call was
executed. The nodes of the dataﬂow graph are labeled with
respect to the program names with an unique event ID. Each
edge, labeled with the corresponding ﬁlename, represents a ﬁle
dependency. The Source node acts as a tool that produced the
initial input, seq_col.fa. The Library node provides ﬁles
which a tool uses but which were not provided by another tool.

119

Fig. 3.

Dataﬂow graph for HybSeqPipeline.

Library (1)

the entire ﬁlesystem. However we intend for an instrumented
workﬂow to be used only to generate logging information, not
as a permanent workﬂow refactoring. Thus these execution
times are acceptable. The dataﬂow construction in general is
quick. For Pycoevol and miR-PREFeR, the dataﬂow construction is slowed due to missing dataﬂow information. This causes
worse case runtime in some internal procedures.

Source (2)
seq_col.fa

split_multifasta.py (3)

3OE0.fa

dna2rna.py (4)

dna2rna.py (5)

3OE0.fa

3OE0.fa

rna2aa.py (7)

1ASU.fa

1BNI.fa

rna2aa.py (8)

3OE0.fa

Workﬂow
test1
test2
test3
test4
test5
test6
test7
test8
HybSeqPipeline
inmembrane
Pycoevol
miR-PREFeR

dna2rna.py (6)

1ASU.fa

1ASU.fa

TABLE V.

1BNI.fa

1BNI.fa

1ASU.fa

1ASU.fa 1BNI.fa 3OE0.fa

rna2aa.py (9)

1BNI.fa

P ERFORMANCE ( IN SECONDS )

Normal
0.029
0.030
0.014
0.024
0.011
0.012
0.026
0.071
51.204
75.578
906.871
25.165

Instrumented
0.045
0.028
0.043
0.046
0.036
0.036
0.045
0.091
54.129
77.919
1376.225
230.332

Dataﬂow Construction
0.001
0.002
0.001
0.002
0.001
0.001
0.002
0.004
15.870
10.611
32.400
77.015

Sink (10)

Fig. 2.

VI.

Visualization of the dataﬂow graph.

C ONCLUSION

In this paper we present a method which instruments an adhoc workﬂow written in Python to produce a log, and use this
log to determine ﬁle dependencies to build a dataﬂow graph
by analyzing ﬁle system changes. The method is implemented,
tested, and evaluated on several real world workﬂows retrieved
from GitHub. The dataﬂow graph provides a structural view
of the workﬂow which can be used as a base for platforms
such as Tavaxy which can support deployment in a cloud
environment. Future work includes addressing some of the
limitations mentioned in the paper to increase the accuracy
and/or generality of the method. Our next goal is to extend
the present trace mechanism to extract semantic information
at run time and propagate it to the dataﬂow graph. This can
be implemented by connecting tools discovered in a trace to
a resource collection. We aim at mapping the dataﬂow graph
to a semantic map where all tools are represented as edges in
a domain ontology [35]. Workﬂow semantics can be extracted
by the analysis of the domain libraries such as BioPython [36]
used. This would support the documentation of the workﬂow

two complete ﬁve node groups. The trace includes a total of
41 such groups. The sink node is not shown.
C. Performance Evaluation
The performance of the initial workﬂow, instrumented
workﬂow and dataﬂow graph construction, averaged after three
runs, are shown in Table V. The impact of instrumentation on
the benchmarks (tests 1-8) are negligible because there are few
invocations and they involve small ﬁles. For HybSeqPipeline
and inmembrane, the execution of the instrumented workﬂow
is 5.7% and 3.1% slower than the initial workﬂow, respectively.
For Pycoevol, the instrumented workﬂow is 51.8% slower that
its non-instrumented version while the instrumented version of
miR-PREFeR runs 815.3% slower than before instrumentation.
The poor performance of instrumented workﬂows is principally
caused by systematic repeated IO access for logs and hashing

120

in terms of its aim expressed conceptually as proposed in [37]
and workﬂow reuse, optimization, etc. [38]. Updates will be
posted at http://bioinformatics.engineering.asu.edu/WISE and
https://github.com/sdml-projects.

[16]

[17]
[18]

ACKNOWLEDGMENT
We thank K. Selcuk Candan and Matthew G. Johnson.
This research was partially supported by the National Science
Foundation2 (grants IIS 0944126 and CNS 0849980).

[19]

[20]

R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[21]

E. Deelman, J. Blythe, Y. Gil, C. Kesselman, G. Mehta, S. Patil, M.-H.
Su, K. Vahi, and M. Livny, “Pegasus: Mapping Scientiﬁc Workﬂows
onto the Grid,” in Grid Computing, ser. Lecture Notes in Computer
Science, vol. 3165. Springer Berlin Heidelberg, 2004, pp. 11–20.
M. Vouk, “Cloud computing - Issues, research and implementations,”
in 30th International Conference on Information Technology Interfaces.
IEEE, 2008, pp. 31–40.
C. Hoffa, G. Mehta, T. Freeman, E. Deelman, K. Keahey, B. Berriman,
and J. Good, “On the Use of Cloud Computing for Scientiﬁc Workﬂows,” in Proc. 4th IEEE Int. Conf. on eScience., 2008, pp. 640–645.
Z. Liyong, Z. Zhuofeng, and L. Houfu, “Leveraging Legacy Workﬂow
Capabilities in a Grid Environment,” in Proc. 6th Int. Conf. on Grid
and Cooperative Computing. IEEE, Aug 2007, pp. 361–365.
J. Goecks, A. Nekrutenko, J. Taylor, and T. G. Team, “Galaxy: a
comprehensive approach for supporting accessible, reproducible, and
transparent computational research in the life sciences,” Genome Biology, vol. 11, no. 8, p. R86, 2010.
D. Hull, K. Wolstencroft, R. Stevens, C. Goble, M. Pocock, P. Li,
and T. Oinn, “Taverna: a tool for building and running workﬂows of
services,” Nucleic Acids Res., vol. 34(Web Server), pp. 729–732, 2006.
M. Abouelhoda, S. Issa, and M. Ghanem, “Tavaxy: Integrating Taverna
and Galaxy workﬂows with cloud computing support,” BMC Bioinformatics, vol. 13, no. 1, p. 77, 2012.
M. R. Huq, P. M. G. Apers, and A. Wombacher, “ProvenanceCurious:
A Tool to Infer Data Provenance from Scripts,” in Proc. of the 16th Int.
Conf. on Extending Database Technology. ACM, 2013, pp. 765–768.
L. Murta, V. Braganholo, F. Chirigati, D. Koop, and J. Freire, “noWorkﬂow: Capturing and Analyzing Provenance of Scripts,” in 5th Int.
Provenance and Annotation Workshop, ser. Lecture Notes in Computer
Science, vol. 8628, 2015, pp. 71,83.
R. Agrawal, D. Gunopulos, and F. Leymann, “Mining Process Models
from Workﬂow Logs,” in Proc. 6th Intl Conf. Extending Database
Technology, ser. Lecture Notes in Computer Science. Springer Berlin
Heidelberg, 1998, vol. 1377, pp. 467–483.
W. van der Aalst, A. Weijter, and L. Maruster, “Workﬂow mining: Discovering process models from event logs,” IEEE Trans. on Knowledge
and Data Engineering, vol. 16, p. 2004, 2003.
J.-G. Lou, Q. Fu, S. Yang, J. Li, and B. Wu, “Mining Program Workﬂow
from Interleaved Traces,” in Proc. 16th ACM Int. Conf. on Knowledge
Discovery and Data Mining, 2010, pp. 613–622.
F. Yaman and T. Oates, “Workﬂow Inference: What to do with one
example and no semantics,” in Workshop on Acquiring Planning Knowledge via Demonstration. AAAI Press, 2007, pp. 46–51.
M. H. Burstein, F. Yaman, R. M. Laddaga, and R. J. Bobrow, “POIROT:
Acquiring Workﬂows by Combining Models Learned from Interpreted
Traces,” in Proc. 5th ACM Int. Conf. on Knowledge Capture, 2009, pp.
129–136.
M. T. Gervasio and J. L. Murdock, “What Were You Thinking?: Filling
in Missing Dataﬂow Through Inference in Learning from Demonstration,” in Proc. 14th ACM Int. Conf. on Intelligent User Interfaces, 2009,
pp. 157–166.

[22]

[23]

[24]

[25]
[26]
[27]
[28]

[29]

[30]

[31]

[32]

[33]

[34]
[35]

[36]

[37]

[38]

2 Any opinion, ﬁnding, and conclusion or recommendation expressed in this
material are those of the authors and do not necessarily reﬂect the views of
the National Science Foundation.

121

Z. Bao, S. Cohen-Boulakia, S. Davidson, A. Eyal, and S. Khanna,
“Differencing Provenance in Scientiﬁc Workﬂows,” in Proc. 25th IEEE
Int. Conf. on Data Engineering, March 2009, pp. 808–819.
Z. Lacroix and T. Critchlow, Eds., Bioinformatics: Managing Scientiﬁc
data. Academic Press, 2003.
Z. Lacroix, B. Ludaescher, and R. Stevens, Bioinformatics - From
Genomes to Therapies (Volume III). Wiley-VCH Publisher, 2007, ch.
Integrating Biological Databases, pp. 1525–1572.
R. Acuña, Z. Lacroix, and J. Chomilier, “Refurbishing Legacy Biological Workﬂows,” in Proc. of the 8th IEEE World Congress on Services,
2012, pp. 41–49.
M. Lonquety, Z. Lacroix, N. Papandreou, and J. Chomilier, “SPROUTS:
a database for the evaluation of protein stability upon point mutation,”
Nucleic Acids Res., vol. 37, pp. D374 – D379, 2009.
M. Dufour, “Shed skin: An optimizing python-to-c++ compiler,” Master’s thesis, Master thesis, Delft University of Technology, 2006.
P. J. Guo and D. Engler, “Using automatic persistent memoization to facilitate data analysis scripting,” in Proc. ACM International Symposium
on Software Testing and Analysis, 2011, pp. 287–297.
C. F. Bolz, A. Cuni, M. Fijalkowski, and A. Rigo, “Tracing the
Meta-level: PyPy’s Tracing JIT Compiler,” in Proc. 4th Workshop
on the Implementation, Compilation, Optimization of Object-Oriented
Languages and Programming Systems. ACM, 2009, pp. 18–25.
S. Abiteboul, P. Buneman, and D. Suciu, Data on the Web: from
relations to semistructured data and XML. San Francisco, CA, USA:
Morgan Kaufmann Publishers Inc., 2000.
J. Silva, “A vocabulary of program slicing-based techniques,” ACM
Comput. Surv., vol. 44, no. 3, pp. 12:1–12:41, Jun. 2012.
R. Rivest, “The md5 message-digest algorithm,” United States, 1992.
M. Johnson, “HybSeqPipeline: First DOI Release,” Oct 2014. [Online].
Available: http://dx.doi.org/10.5281/zenodo.11977
A. Perry and B. Ho, “Inmembrane, a bioinformatic workﬂow for
annotation of bacterial cell-surface proteomes,” Source Code for Biology
and Medicine, vol. 8, no. 1, p. 9, 2013.
F. Madeira and L. Krippahl, “Pycoevol - A Python Workﬂow
to Study Protein-protein Coevolution,” in Proc. BIOINFORMATICS.
SciTePress, 2012, pp. 143–149.
J. Lei and Y. Sun, “miR-PREFeR: an accurate, fast, and easy-to-use
plant miRNA prediction tool using small RNA-Seq data,” Bioinformatics, vol. 30, no. 19, pp. 2837–2839, 2014.
U. Brandes, M. Eiglsperger, I. Herman, M. Himsolt, and M. Marshall,
“GraphML Progress Report Structural Layer Proposal,” in Graph Drawing, ser. Lecture Notes in Computer Science, 2002, vol. 2265, pp. 501–
512.
E. R. Gansner and S. C. North, “An open graph visualization system
and its applications to software engineering,” SoftwarePractice & Experience - Special issue on discrete algorithm engineering, vol. 30, no. 11,
pp. 1203–1233, 2000.
M. Johnson, I. Zaretskaya, Y. Raytselis, Y. Merezhuk, S. McGinnis,
and T. Madden, “NCBI BLAST: a better web interface,” Nucleic Acids
Res., vol. 36(Web Server Issue), no. 1, pp. W5–9, July 2008.
J. R. Miller, S. Koren, and G. Sutton, “Assembly algorithms for nextgeneration sequencing data,” Genomics, vol. 95, no. 6, p. 31527, 2010.
P. Tufféry, Z. Lacroix, and H. Ménager, “Semantic Map of Services for
Structural Bioinformatics,” in Proc. 18th IEEE Int. Conf. on Scientiﬁc
and Statistical database Management. IEEE Computer Society, 2006,
pp. 217–224.
P. J. Cock, T. Antao, J. T. Chang, B. A. Chapman, C. J. Cox, A. Dalke,
I. Friedberg, T. Hamelryck, F. Kauff, B. Wilczynski, and M. J. de Hoon,
“Biopython: freely available Python tools for computational molecular
biology and bioinformatics.” Bioinformatics, vol. 25, no. 11, pp. 1422–
1423, Jun. 2009.
M. Kinsy, Z. Lacroix, C. Legendre, P. Wlodarczyk, and N. Y. Ayadi,
“ProtocolDB: Storing Scientiﬁc Protocols with a Domain Ontology,” in
Web Information Systems Engineering Workshops, ser. Lecture Notes in
Computer Science, vol. 4832, 2007, pp. 17–28.
Z. Lacroix, C. Legendre, and S. Tuzmen, “Reasoning on Scientiﬁc
Workﬂows,” in Proc. IEEE World Conf. on Services, 2009, pp. 306–313.

The

Possibility

Achieving

and

the

Complexity

Fault-Tolerant

of

Coordination*

Rida Bazzit
Gil Neiger
College
Georgia

of Computing

Institute

Atlanta,

of Technology

Georgia

Abstract
The

problem

damental

of fault-tolerant

in

researchers
tion:

distributed
have

general

faulty

considered

ordination,
from

ies the

possibility

acting

in

with

sion

are

in which

bidden

tems

it.

sults

provide

the

local

a thorough
will

thus

approach

to

fault-tolerant

take

when

time,

while

researchers

when

co-

within

can

paper

tion

problems

The

first

the

activity

system

tributed

computing.

required

of coordi-

to

of the processors

is a fundamental
agree

In

such

problems,

on a common

in

These

title

and/or
?oDC

that

lems.

When

these

problems

action

without

fee

the copies

all or part

of this

are not made

the ACM

and its date

copyright

appear,

material

notice

end notice

is

of

has studied
problems,
Con-

of many

range

such

two

general

only

coordination
are

classes.

coordination

agreement

among
problems

in this

class.

coordination

processor

constrain
those

of coordina-

into

of consistent

performs

The
prob-

an action,

it to perform

performed

1/92/0008

/0203

types

systems

one that

by the correct

and the

eyetem,

our

the

is given

both

We

is

pro-

also

omission

are optimum

For

time-complexity

the

and,

solutions.

in the

needed.

We

and

as yn-

consider

sys-

failures.

is to determine

possible

problems

systems.

send-omission,

can be solved

the best

such

synchronous

(stopping),

(send-receive)
goal

to

distributed

passing.

crash

problems

solutions
of

with

message

with

local

paper

are

cases,

to

are solutions

of rounds

eolutions,

of the

each

cases in which

in those

These

number
such

and
For

of com-

we analyze

computation

they

require.

for Computing
requires
a fee

The
tionship

-496-

explores

munication

permission.
0-89791

paper

general

that

for

‘92-81921B.C.
ACM

of the

literature

several

chronous

to per-

or distributed

is by permission
of the Association
To copy otherwise,
or to republish,

spacific

de-

body

Distributed

them

Most

a faulty
with

consider

processors

such

advantage,

publication

that copying
Machinery.

01992

copy

commercial
of the

nonfaulty
actions

that

a survey

require

in the

find

diract

and

of

Fault-

is a large

a broad

class

class is that

within

dis-

Foundation.

to

the

coordination

divides

considered

This

in a dis-

problem

*This work was supported
in part by the National
Science
Foundation
under grants CCR-8909663
and CCR-9106627.
t This author
was supported
in part by a scholarship
from

provided

and

processors.

second

achieve

considers

the correct

t ems

Permission

all)

cessors.

Coordinating

granted

science

[3] provides

is the

problems.

re-

coordination.

tributed

the Hariri

computer
to

co-

not

their

There

Broadcast

(Fischer

This

Introduction

are

that

coordinate

solutions

(but

be faulty.

of othere.

as Reliable

consistent

1

may

requires

successfully

the failures

fault-tolerant

some

a system

is valid

are operating.

can,

in others

to

that

chosen

they

problems).

in determining

attempting

in

action

considers

assumes

coordination

sensus

the

which

specifically

processors

such

that
within

and

fault-tolerant

omis-

These

characterization

aid

co-

solutions

computation.

paper

literature

sys-

it

This

spite

of optimally

optimum

in polynomial
NP-hard

and

and,

context

processors

for-

in which

to ensure

the

tolerant

stud-

general

complexity

cases,

of

of achieving

systems

achieved

In some

require

nation

be

paper

and

given

ordination

co-

are

form

the

actions

asynchronous
and

the

computational

be implemented
they

and

send-omission,

cannot

achieving

This

complexity

past,

consistent

processors

We indicate

the

the

the

and

the

of coordha-

and

inconsistently.

crash,

ordination

types

in which

faulty

is fun-

In

irrelevant,

synchronous

failures.

analyze

two

coordination,

processors

ordination

coordination

computing.

30332?-0280

results

of processor

. ..$1.50

203

in this

between

coordination

knowledge

[8].

based
and

It

on the

different

relaforms

is well-established

that

knowledge

prove

can be used

solutions

computing
Tuttle

[12]

knowledge

to

For

showed

a weak

that

is necessary

for

coordination

fact

optimum

to derive

Neiger

and

Tuttle

form

vious

of

paper

tent),
eral

characterized
types

In

paper,

we

knowledge

work

[13] showed

Thus,

if

some

coordination

protocols.
knowledge

mantic

structure

common

is more

for

the

cases

is closely

edge

which

[2,8],

a group

rather

than

distributed

it is by using
many

of the

The

results

its

that

of simthat

strategy
able

of the
tent

processors

in

to show

distributed

cannot

form
be

is

section
This

easy,

that

and

systems

with

omission

failures

and

with

send-

or general

the

impossibility

potential
correct

confusion

fail,

results
the

between

are

stem

starts

Before

sor may

have

then

r taking
every

in

place

the last

and,

given

time

on the

and

received,

of

actions

time

it

has

sent

A global

and
It

received

with

the

state

consists

messages

with

and

time

analysis

eventual

common

com-

sent

coordina-

times

that

At

of the

it has

the

that

at

which

coordination

when

is a tuple

ar-

is assumed

the

to

have

action.

the

performed.

time

some local

input,

tagged

and

tagged

1 and

that

state

a-

with

messages

a coordination

initial

are

Comput

~ -

performs

a

increments

rounds,

sends

messages

clock,

its

messages
were

input.

a processor’s

send
a clock

each proces-

external

round,

global

in

a sequence

optionally,

time,

can
share

begins,

receives

set P of

a communica-

advances

a processor

pro cessors,
since

by

processor

between

round,

sys-

to study

of a finite

processors

O and

an initial

round
In

All

a computation

proceeds

r.

follow

for

state

a protocol

a round

they

(sl,...

were

per-

, Sn) of local

T is a run

communication

beginning

paired

with
round.

An

is required

of the

of that

an infinite

and 1 is a natural

protocol,

a processor

as a function

at the

stat es, one per

the

a

the messages

Specifies

to send

Intu-

of

at time

of one.1

any

other.

used

[1,8,9,12,15].

consists

that

local

systems

identity

coordination
system

Processors

syn-

from

of a distributed

to others

such

which

(send-receive)

failures.

a model

is similar

to any

processor’s

round.
sequence

ordered

pair

number,

A run

is

of global

(T, 1), where

is called

a point

the
1Note that

a thorough

re-

when

stat es.

the

knowledge

asynchronous
omission

aid

coordination.

we ~~~ using

a round

based

chronous
systems;
that is, we assume that
access to a global clock. This is done purely

ship

dis-

charac-

to take

fault-tolerant

defines
model

network

formed,

processors.

We provide

shown
for

greatly

the approach

tion

actions

consis-

because

systems

general

can

are connected

all

as half

achieving

common

as to

is

a thorough

they

they

tion

are the

as many

These

chronous

itively,

which

provide

n processors;

any

we prove

paper

impossible

achieved.

This
of testing

results

and

putation

Reasoning

of this

This

rived

to

paper.

of eventual

are

require

computation.

of coordination,

tem.

other

knowl-

ascribed

is relatively

may

coordination

necessary

in

solutions

Definitions

they
systems

is pos-

processors

NP-hardness

to achieve

tion

common

processor.

contributions

some

correct

optimum

attempting

that

se-

following.
For

fail-

knowledge.

these

message

eventual

eventual

knowledge

of this

the

A distributed

optimum

showed

been

to

knowledge

local

omission

coordination

the

a majority),

knowledge

is true

because

knowledge

a single

distributed

important

knowl-

of

than

consider,

related

general

consistent

in determining

If test-

to an optimal

is simple

with

in which

searchers

2

is computa-

[16]

we have

we

knowledge

about

of

achieved.

about

Tuttle

However,

prob-

com-

same

is true

complex

correspond

theory.

that,

of sev-

earlier

common
the

be difficult

knowledge;

semantics

is

Reasoning
can

which

tributed

terization

space

of coordination.

same

to the

coordination.

knowledge

knowledge

the

(i.e.,

Because

eventual

common

eventual

common

expensive,

in

sible

consis-

so because

a system,
form

eventual

common

game

of
in

corresponding

for

its

the

coordination

form

ures

systems

by proving

a

a pre-

in terms

on

do

eventual

whenever

most

NP-hard

and

and

explored

We

For

as relevant

fault-tolerant

nonsimultaneous

concentrate

that

is impossible

tionally

ple

and

of achieving

always

related

In

general

solutions

[8,16].

necessary

ing

showed

coordination

(both

their

this

problems.

knowledge.

of knowledge,

this

of the

used

to such

ent

●

of generaJ

and

knowledge

lem

and

common

solutions.

mon

edge

of

solution

considered

problems

optimum

is

we

Moses

form

solutions

common

[13],

coordination

the

consist

distributed

example,

problems

distributed

and im-

in

[15] subsequently

considering

stronger

problems

[1,7,9,10,11].

simultaneous

results

to characterize

various

of the relation-

sentation.

knowledge

asynchronous

and

204

The

full

paper

systems.

defines

a more

model

of asyn-

processors
have
for ease of pregeneral

mode

for

and
r.

represents

The

noted

sent

may

types

which

are

a faulty

which

processor

which

a faulty

send
faulty

behavior

of the

inputs

This

paper

in

round

chronous
liable,

(FIFO
this

which

the

to

A fact

a property
false

is true
are

at

often

convenient
other

determines

under

an

A

the

to

points

finite

(r,/)

(e.g.,

(or

currence

of

of the

faulty

runs).

The

a common

not

perform
To

solve

communication

C =

Earlier

following

problems

of

This

is because

cessors

Solutions

it

is

(thus,

processors
that

Two

X

runs

do not

and the same pattern

ing

runs,

the

run

it

must

protocol

the

it

a fact

is

coordinate

tle

ok~,

identities

to

about

with

have

that

does

not

there

are coordination

C and

a

if every

P is in turn

that

205

P’

dominated

Moses

Byzantine
solution.

X-satisfies
by P.

Tut-

optimum

[13] gave a pre-

coordination

for

have

Nevertheless,

problems.
to

and

other

Agreement

for which

and Bazzi

of these

that

A
G

every

problems

in

X is either
do not

P is X-optimal

Protocol

later
of P2.

problems

solutions

optimal

run

dominates

Eventual

Neiger

Pro-

of correspond-

the

example,

an optimum

do exist.

considered
lems.

we augment

a set of functions

[12] showed

initial

behavior.

c (where

Some

For

runs.

same

an action

in

C and
so.

cise characterization

need

simultaneously.
problem,

does

the

of faulty

for

solutions.

solutions

choose

Pro cewors

that

are compared

have

does

is X-optimum

pro-

run.

in corresponding

performs

than

in

et al. [4].

nonfaulty

P2 if, in any pair

no processor
of PI

systems

of Fischer

in every

if they

co-

coordination

problems

P1 dominates

recently,

consistent

these

require

behavior

ocfrom

concen-

more

in asynchronous

correspond

first

omitted

consider
that

input

optimum

action

to

tocol

protocol
is a

condition

and

is enabled.

actions

a coordination

problem

enabling
input

per-

6’-satisjies
the

coordination

an action

their

(or

[9,12];

to coordination

by comparing

about

on

results

they

to perform

C
with

“nonfaulty”

can be solved

p and

of runs).

Each

performs

y processors

hold

Note

of the impossibility

coordination

am}.

a non-

is enabled.

processor

satisfies

begun

spite

of p.

classes

by

action

coordination

or

protocol

an
the

processors

their

gen-

protocol:

is performed
that

literature

on general

to be

Although

properties

{al,...,

it

about

action

C) if the

nonfault

word

have

true

are

the

[13,15].

about X if fixing

a coordination

with

is a fact

and

a protocol

of the

conditions

ordination

of points,
that

all

consistently
above

researchers

in a system

system.

falsity)

two

general,

associated

which

general

to
non-

action.

the

to have

\

then

that

protocol

each.

Problems

set of actions

has

classes

If a nonfaulty

action,

(?) if the

trated

In

about

be either

facts

p is a fact

truth

set of all
a specified

p is valid

in

refer

defines
In

of the

these

then

or C) if it X-satisfies
problems.

are not
subject

as those

G-sa@ies

processor,

form

which

% interpreted

p will

Fact

to

than

section

are

Formally,

of all runs

2. Agreement.

two

and failures.

as properties

Coordination

This

processors

actions

sent

in

the

(r, 1), denoted

points

a fact
the

We

In one class,

is re-

is convenient

language

interpreted

general,

agree

is enabled.

asyn-

order

statements

a fact

point

all

it

make

respectively.

facts

3

to

in this

if it

faulty

proto-

must

protocols.

If an action

faulty

systems,

the

with

synchrony

systems,

at a given

that

criteria

C (or

performs

an action

processors

their

We call

hold

1. Validity.

delivery

as those

protocol

of points:

(T, J) #p,

objects

in

between

in

a system

about

language

system.

and

messages

as well

identifies

analyze

a logical

sent,

on message

delivered

of a communication

order

are

message-passing

in which

always

set of assumptions

by the
other,

acActions

no processor

of such

respectively.

satisfies

delivered

true.

work

action

correctness

conditions

systems,

always

of asynchronous

communication)

runs

In

they

scope

those
are

is not

This

are

is no bound

we consider

an

processors.

erally

the

that

pTotocoz.

Informally,
runs,

taken

a coordination

an action

in all

classes

in the

consistent,

to

independent

synchronous

there

processors

two

same

faulty

(in

if,

actions

the

perform

be such

choose

relevant;

omit

that

must

the

in

to

result

one action.

is correct

consider

(in

omit

must

than

and

(in

failures

is completely

in which

Within

failures

We assume

messages

in

col

the

processors.

systems,
but

times.

omission
intermittently

considers

processors

Other

possibly

may

in a run

more

r.

when

we call

protocols

the

failures

general

a process

repre-

intermittently

messages).

to the

which
the

and

receive

crash
stops,

may

processor

and/or

are

tell
tion;

We consider

send-omission

processor

to send messages),

in run

simply

of a round),

a faulty

Af(~)

to failures.

These

of

is de-

follow

Let

nonfaulty

be subject

1 rounds
point

correctly

nonfaulty.

of failures.

middle

the first
p at that

processors

thus

set of processors

processors
three

after

of processor

Some

and

the

system

state

by rP(l).

protocol

the

the

local

They

also
prob-

C if it X-satisfies
C and

dominates

4

Definitions

of

a similar

Knowledge

mon
rocessor
and

knowledge

Moses

knows

[8]

was

in

the

p at point

(r’,1)

~

(recall

that

p for

cessor’s

local

runs

global
It

p believes

is in Af,

p is true.

That

to see that

runs

r’

such

Processor
be

used

edge,

to

define

while

to define

weaker

Because
among

knows
All

lent

EMp,

in

the

common

it

knowledge

do

to

know

eventually

all

know

the

0.

(T,l)

~

general,

S~p

lent

the

of the

eventual
denoted

implies

to be the

Ao=ti

W~p,

(but

X

w

is not

of

the

greatest

fixed

now

a

facts

rule

are even-

in a system,

in that

X

S$p

p is fact

then

system.
in a system,

in that

then

system.

two

to be nonempty,

+- p and

about

present

that

W$p

it is

~

p are

runs.

theorems

regarding

coordination

first

second

we

and

proved

theorem

the

eventual

in

shows

knowledge

#

enabling

an

that

re-

com-

earlier

some

is necessary

In

equiva-

1:

action

know

if

that,

pa-

form

of

to achieve

ai,

c,

then,

2:

com-

pToblem.

wheneveT

nonfaulty,

That
oki

If

pTocessoT
is, p must

is

SimilaTly,

whenever

weak
if

pv’ocessor

Let C be a coo?’dination

pe?’foTm

some

action

then,

BpWj oki.

ai,
is

is G-optimum
action

ai.

C, then

for

processoT

these
of

C, then

as soon

Similady,

as KPS~oki

a better

achieving

206

co-

as soon

eventual

C be a coo?’dination

protocol

give

knowledge.

optimum

even-

a protocol
p pe?’fovms

KPS~oki.

Together,

in this pa-

any

an action

becomes

knowledge.

c,

derstanding

here for strong

it

common

as soon

of the equivalence

Let

p peTfoTms

for

and

that

perform

condition

G-satisfies

any

shows

must

a protocol

Theorem

greatest

theorem

problem

Theorem

action

knowledge

OAM(pAX).

given

valid

k

is assumed

common

C-satisfies

comS$p,

Ai>l
(OEN)ip
—

to that

of even-

knowledge:

tual

is also

denoted

N

set

The

as some
mon

p for

eventual

For all cases considered

is equivalent

form

and

an induction

certain

is also valid

between

[13].

The

even-

\

necessarily

point

that

(or

OEMS~p

Each

A TJJ)is valid

to see that

ordination

eventually

is the

~

know

coordination.

others

equivalence

conjunction

VKP(WAX).

common

W~@

eventual

of eventual

common

S~p

is also valid

knowledge

per

As we

knowledge

by set JV,

equivalence

infinite

per, thi~ ~efinition
eventual

p

imply
to a set,

eventually

satisfies

A +)

oAN(p

when

mon

is slightly
stronger
than the original
definition
of
They
defined
e~enturd
common
and Moses
[8].

knowledge
X ~

valid

knowledge

if (?’,1’)

Strong

point

N,

of the

only

common

Weak

all
all

operator

OEN(p

S~@

hard

We

in solving

definition

points.

tied

p by set
point

2 This
Hs,lpern

is

not

coor-

if they

termination

if and

of fact

A X).2

to)

Because

common

common

Even-

in general,

knowledge:

If p ~

is

the

study

that

knowledge

M ~ ~

p >

For

so on ad infinitum.

The

fixed

greatest

in the

know

temporal
Up

relaxes

is eventual

eventual

Eventual

knowledge

of fact
fixed

1.

using

OEN(p

and

problem.
uses

1’ ~

common

equiva-

knowledge.

of processors

achieving

knowledge
We say

tual

●

EN%p,

simultaneous

eventually

[13],

for

coordination

is the

require
a fact

it,

elsewhere

common

appropriate

not

a set
it,

is necessary

mon

in

Informally,

defined

(SNp)

i;

[8,16]

levels

KPp.

&>l

(WNp)

gain

all

knowledge

set

are valid,

can be used t o show

hf

@p,

knowledge

knowledge

is more

that

+ oAMW$yI

lationship

inherent

dination.

some

denoted

knowledge

in

as &GN

conjunction

that

simultaneously.

is, both

that

by the

.4Nip.

reason,

showed

possessed

p,

common

processors

or

common

common

p -

simple

knowledge.

of the

That

eventual

specifi-

EveTyone

common

infinite

are

thk.

cop-

that

that

does not,

tual

coordination

is defined

believe

StTong

simultaneity

tually

~

common

problems

will
knowl-

we

knowledge

denoted

Eventual
this

W$p

hold

is eventual
members

believe)

p ~ ~(T’).

be used

all

.

p,

to Ai>l

p).

BP, will

with

y processors.

weak

~

of group

processors,

in the

to

~ hf

then

ever

common

If a fact

if it

KP operators,

using

will

knowledge

eventual

that

require

simultaneously

tual

“eventually”

weak
note

is weakeT

knowledge

common

for

should

It does not

of knowledge

We

that,

and

notions

nonfault

BPp.

while

the

of

pTocessom

&N

to condition

E KP(p

deals

nonfaulty

equivalent

T’P(l)

of a pro-

nonfaulty.

Tp(z)

belief,

paper

interested
Af

their

notions.

this

the

cally
set

strong

processor

=

if

is true
One

knowledge

knowledge.

Bpp if (T’, ~’) # p for

~

using

common

KPV,

p if p knows

?’~(1) =

knowledge,

>

?$(1)

useful

is, BPp

(T,/)

that

Halpern

part

on its being

pTocessoT

It is easy

that

is always

say that

all

(T, 1)

is often

knowledge

by

P~ocesso?’ p

way.

T’ such
clock

state).

a processor’s

defined

(r, 1), denoted

all

the

first

following

statement

knowledge.

as BpW$oki

if a pTotocol
p must

holds
results

eventual

any

show

that
of
of

the
the

holds for

some

action

common

and

If a

p must

is C-optimum

perform

for

understanding

coordination

problem.

processor

action

ai.

a thorough

un-

knowledge

will

possibility

of

complexity

of

optimum

protocols.

which

Section

consistent

tion

6 considers

nation

5 gives

coordination
the

situations

complexity

Sec-

of optimum

IAI, Il?l

and

in

is impossible.

and

coordi-

failure

protocols.

Impossibility

Results

for

This

pattern

section

chronous
nor

shows

that,
can

systems

with

in asynchronous

omission

of the

systems.

and

consistent

sets

cessors
but

at

this

There

t processors

in different

behavior

such

sets never

is consistent

identity

be two

e with

processors,

cessor

know

whether

it is faulty.

context

of general
since

the

unimportant,
is the

set

is critical:

become

simply

behave

This

to

translations

was
show

[15]

first
the

omission

We

handle

the

asynchronous
the results
omission
ures.

for asynchronous

by

such

and

of

“delayed”

Omission
Thi.

section

general

two

and

considers

omission

B

are

with

a synchronous

failures

and

B partition

nonempty

n

the
and

~

KPS~p,

even-

becomes
k.

runs

and

the

fail-

set

let

A

of pro-

then,

that,
A

is by

result

and

if

rk

B

from

rk

reverse

easily

and

for

all

the

re-

time

k,

assume

that

lj)

~

B in r~+l

the

vpEA

sions

and

change
vice

send

versa.

Since
~

knowledge

holds
‘or

‘ome

to send

be clear

have

that

size at most
fail-

to receive
= r~+l

(j)

for

known

is some

omis-

eventual

becomes
j’

r’

among

omission

Since

for

except

in A fail

r!(j)

there

k.

a fact

to rk+l

general

Sfip.

eventually

processor,

is

result

omissions

(r’, j)

k.

z

of it can-

‘Ps%

both

admits

on
j

are no failures

and

system

p

knowledge

all processors

system:

any

Since

and

is, (rk+l

for

be distinguished

sjp.

is stable

Now

the

simply

is

induction

holds

r cannot

+

= B and

ures,

~ VPCB

fail

that

to send

rk is the
time

k.

j“

any
com-

to every

~ j such

that

the

proof.

can

relate

system

is partitioned

from

4: If
processors

run

holds),
p

~

B

S$-p,

using

that

there

means
~

Note

A and

VPCA

KPS~P,
❑

knowledge

failure-free

207

This
(r~,~”)

common

faulty

if

that

k.

r into

(rk, j’)

above.

such

identi-

all processors

A in round

in

eventual

Theorem

P,

those

of partitioning

given

we

let rk be a run
= A and

Furthermore,

same knowledge

with

to

~ jf

that

Now

J/(rk)

result

argument

completing

say that
=

KpS~yJ.

except

is some

General

A U B

such
into

there

the

messages

We

k
r

of the

from

“parti-

set of processors

disjoint,

about

is a run

in B

to send-

systems

2t.

strong

it also

partition

k. It should

that

Failures

sets A and

A and

Systems

r

the following

in rk for. all

in B in round

cal to r’

is because

be

z

S$p

~(r’)

nonfaulty

failures.

Synchronous

of partitioning

to those

Given

5.1

processors

with

omission
may

1

1. Let r’ be a run identical

~ k +

(r’,j’)

apply

as general

runs,
be lost.

mon

impossi-

synchronous

a system

a

becomes

VPc~

(r ~,1)

p ~ Af(rk+l),

Neiger

the

This

~

every

in A in round

in r, then

that

proof

1, so

t.Because

certain

failures.

systems

a combination

send-omission

of

in systems

of

for

or receives

h. Informally,

be a fact

A

‘Ps%f-

The

A or among

can

Neiger

show

differently,

as well

whether

n < 2t,

situations

Intuitively,

tioned”

and

systems

failures

to

coordination

failures

know

a j

is because

that

to act,

with

it

j

be isolated

by

p

(r, 1)

k + 1. That

cent ext

processors

used

is the result

there

to ~ (the

that,

processor

if a fact

sets

1, the

time

not

are

behavior

impossibility

used

of simultaneous

general

unable

systems

later

not

correct

identical

except

knowledge

Let

vpeA

k z

about

as if it

the
this

may

and
the

and

between

Tuttle

bility

and

faulty,

technique
[14]

processors

information
are

In

however,

the

“paralyzed”

Toueg
and

set can

y processors.

at

a prob-

so each

because

they

is not

processors

~

This

In the

faulty

coordination,

rk

of partitioning

Proof:

set of

of the

important

or not

this

is

there

For

no pro-

actions

of nonfault

of consistent

from

coordination,

pro-

either

the set of faulty

lem,

k,

(rk,j)

communicate.

being

or not

dis-

that

processors
can

If

sult

no trouble,

with

any

that,

be two

cessors.

that

within

in B sends

knowledge

3:

run

same)

time

common

B

such

occurs

the

can

and

r

A is the set of nonfaulty

that

states

Lemma

be-

or from

common

eventual
it

common

distinction

regarding

a set communicant

processors

Since

most

eventual

coordination

processors.

of

that

is the

A and B from

tual

or general

showing

This

uncertainty

within

lemma

failures

send-

by

consisin

syn-

r~ be the

We say that

into

then

neither
omission

strong

those

correct

joint

achieve

in

of the

2t,

with

is done

general

because

<

general

This

to

knowledge
tween

n

systems

failures.

is impossible

if

be achieved

to

note

in r~.

coordination

a run

communication

1 > k, no processor

of r~;

Coordination

tent

1?, let

message

Consistent

Given

(failure-free)

within

round

5

< t,

is complete

is

that
r and

any
to

time

run

is not
time

in

O (and

we can show
a fact

with

one

the

in which

the

the following:

about
valid,

strong

which

the

input

then,

for

1, (r, 1) #

=S~p.

and
every

The

PTOOf :

proof

is by

repeated

Lemma

3, where

the

input

cessors)

is manipulated

in

relate

the

thus

!j~p)

that

S~p

original

run

is not
was

failure-free

runs

of Lemma

the

to

one

in the

application

in

are candidates

run.

for

(and

Theorem

7: In

fact

er general

omission

that

tocol

solves

no action

application

its

coordination
enabling

Theorem

problem

conditions
4 to show

is nontrivial

is valid.
the

It

if none

failures

is easy

to

coordination

is performed

in failure-free

with

general

if a protocol

problem,

Asynchronous
General

This

section

eral

c-solves

then

no

systems

action

general

omission

partitioning

r

identical

to

not

in

1 of rk.
be

there

with

respect

The

B

in

the

is

round

1 >

processor

because
round

in

messages

in

which

of a run

ily

be two

cessors.

there

If

k,

parameters

are

We

consider
system

taken

place.

in this

The

+

be a fact

sets

that

about

runs

partition

VPEA

and

the

KPS~V,

set

then,

let A

results

of pro-

foT

all

full

~,

information

polynomial
generalizes

VpeAKPS%f.

ter

with

the

structed,

any

exception.
simply

as there

are

let T’ be such

to A arrive

when

following

is identical

we cannot

omissions,
st cad,

proof

after

p G ~(rk+l
constructing

that

time
).

j,

The
r k.

to that
When

switch
only

same

run

send

send

messages
Again,

of Lemma

omitted

method

= r~+l
can

to

receive

(j)

u

and

+

I)n

of the

study

that

have

information
which

pro-

processors

available
[12]

to them

showed

that

can be implemented
in

An

processors.

edge

row

Af-

graph

arranged
Each

Thh

as follows.

communication
vertices

systems

considering.

systems

each.

with

synchronous

we are

B

at

for

the

local

end

of the

be used
c1

sors
the

208

of round

communication

in

is a
r

in

execution
each

and

round.

communication

sends
When
graph

that
this
one

between

from

sent
by q

its

a
view

to all proces-

processor
another,

1

vertices

maintain

includes
graph

+

corresponds

r2 — 1. Processors
graph

the

of processors

of rounds

Tuttle

messages

n vertices

To

define

of a problem.

number

information

if p is a

Kpp.

to full

condi-

that,

size

in

primar-

(PI, rl ) and (P2, rz ) means
that
the message
of round
T1 is received
by p at the beginning

Infrom

one

the

of failure

(r

of

Note

number

T, a labeled

with

columns

T’ is conand

omissions.

T~ (j)

3

apply

one must

to asynchronous

round

graph
The

Proof:

types

tests
knowl-

enabling

the

protocols
size

the

these

relationship

common

Bpp

apply

Moses

show

in poly-

whose

protocols

all the

round.

with

be

the

are

can

section

input.

then

below

These

every

this

the

to

and

dis-

for the implementation

determine

this

in the

that

It

for

we

proven

of coordination,
that

that

knowledge.

protocols.
in

input,

com-

eventual

problems

the

need

differ

3 holds

about

of eventual
of knowledge

we show

and

about

5. It

cases,

of the

also hold

given

in

in Section

of testing

some

coordination

results

general

coordination

can be performed

others,

results

the complexity

forms
form

In

distributed

coordination

distributed

tests

In

are facts

fact

> k and an Tk that is the result of
time k such that
T into A and B from

partitioning
(rk,j)

p

(T, 1) ~

is a j

then

it considers

complexity

to coordination

tions

two

Because

these

The

are delivered.

to Lemma

time.

NP-hard.

A in

they

r that

knowledge.

the

of optimum

k if Tk is

the

communicate

6: Let

B

problem,

in which

discussed

about:

necessary

of
of

the

tributed

tocols.

and

a pro-

runs.

as consistent

to another

that

edge,

case:

Lemma

send-

if

2t,

Optimum

Thus,

those

to reason

between

of a

a Tesult

time

every

messages

as gen-

than

considers

are

Because

possibility

r~

any

be partitions

analogue

2t.

for

systems

as well

by relating

nomial

asyn-

definition

from

for
to

that,

to when

following

and
that,

Note

may

the

Now,

B sends

delivered

sent,

A

s

on the

failures.

T except

no processor
round

reconsider

shown

for

as well

as n

depended

into

holds

send-

as long

to

which

or

to that

result

and includes

need

partitioning,

Send-

analogous

This

failures

with

Results

knowledge

then

Failures

a result

section.

systems
we

with

n <

in failure-free

consider

other

begins
mon

Systems

shows

omission

of this,

and

coordination

can be solved.

coordination

runs.

Omission

previous

chronous

a nontrivial

section

is easier

in the

result:

use
This

systems

and n < 2t,

a nontrivial

5.2

an analogue

systems

failures

Complexity
Coordination

of

following:

5: In synchronous

omission

asynchronous

is performed

problems

Theorem

to prove

c1

3.

6
A

be used

in the end the following

to

p

Note

the

now

4, giving

the

which

original

6 can

to Theorem

pro-

runs

contradicts

Lemma

of

set of faulty

partitioned

The

true.

true

(and

receives
it updates

its

communication

the

two

to the

and

message

In the

the

union

We next

of

tual

corresponding

the

that

be calculated

labeled

Eventual

(Section

assumption

can

Common

the

Lemma

truth

10:

begin

knowledge

giving

[2,8].

edge

analogue

Fact

p

is

the

if (r’,

p for

1) ~

(r, 1),

all

runs

all

processors

holds.

Because

sor

W(T)

may

first

belief,

the

combined

and

#

we

fact

distributed

about

about

8:

BpDNp

It

states

all

about

that

systems

that

in all

coordination

It turns

out

that

distributed

to eventual

noting

that

knowledge

knowl-

it

9:

runs,

then

KPp

+

11 and

Lemma

11:

mation

protocol

we can

mon

any

for

weak

will

weak

eventual

prove

the

plication

then

first

case.

is valid,

so

BpDNp,

as

implication

follows

second
Specifically,

systems

it

in which

and

we

from

is valid

for

holds

consistent

can

thus

prove

of a full

infor~

12:

Consider

the execution

in a synchronous
failures

system

OT with

with

general

If p is a fact

c?’ash

omission

about

runs,

then

is valid.

By

sider

common

to

there

p has

at time

is reliable

runs,

at

in

time

any

1. It

system,

im-

(r, 1) ~

any

1 such

available
so (r’,

be

correct

that

rule

suffices

Assume

to

for

strong

show

that

that

com-

DNp

DNp.

(r, 1) #

a

Con-

cases:

b

to the

are

Because

only

crash
all

correctly

in round

sors will

know

or

1 + 2, every

every

other

1 +1,

end

nonfaulty

i% (r>l

+ 2) ~

means

(r, 1 + 2)

desired

result.

of that

that

send
proces-

round.

processor

receive

or know

fail-

processors

all noncrashed

processor,

processor

send-omission

nonfaulty

p by the

round

In

will,

a message
it is faulty.

for
from
That

APep(P @N v KPp), which
EN(DNp A ~), giving the

EM
#

round

proces-

that

Kp~

omission

all nonfaulty
in round
1 + 2, each

messages

from

know

of these

nonfaulty

Bp(DNp

are general

correctly

Since

that

There
Because

(T, 2) # DNp
Kpp; by time 1’,

clear

There
ures.

pro-

each

1 or later.

1) ~

two

time

Because

should

it

A ~).

now

induction

proces-

earliest

from

(T, 1’) ~

knowledge,

that

A p)

desired

11 >

between

to exist.

about

Specif-

that

for

1’ be the

was

p has all the information
processors

time

Let
sent

the

that,

a message

passing

~ is a fact

BpDNp

full-

induction

knowledge,

received

that

the

Assume

show

A p).

sors, 1’ is guaranteed

as desired.

+

a

DNp + ~AN(DNp

is some

Bp(DNp

in N

message

DNp

Tuns,
running

by induction,

the

following:

using

be valid.

suffices

p E N,

this

that

system;

will
It

which

about

common

prove

in the

(T, 1’) ~

~

system

prove

eventual

we will

DN~.

fact
any

in

We

is valid

and

for

and n > 2t.

OEN(DNp

We begin

protocol,

Proof:

cessor

KPp

of the

is impossible

DNp + S$p

is closely

knowledge.

implies

For

is valid

infoTmation

by

&

(r, 1) \

8,

we consider.

OT send-omission

knowledge

common

because

Lemma

W~p

sor

the

the

●

ically,

for

is valid.

related

rule

BP~

converse

cases except

Lemmas

of the

relating

The

PTOOf :

by

only

Lemma

of the first

9.

p

no proces-

say

converse

of

that

knowledge:

p is a ~act

+

•1

failures
Lemma

Then

BpW~p

W$p + p is valid
Bpp.
Since
p is
(T, 1) #

input,

By

the input.

system:

Since

runs,
the

KP9.

Lemma

= ~P(l)

the members

following

proof

about

any

KpDNp.

BpWfiP.

~

facts

The

DN~,

r~(l)

that

p,

of even-

or knowledge

desired.

non faulty

(r, 1) determine

among

the

the

that

knows

in

is given

The
(r, t)

(r, 1) &

knowl-

(r, 1)

be possible

is distributed

Note

to

belief

knowledge.

~’ such
at

explicitly

knowledge
group.

it

processor
denoted

Thus,

nonfaulty

distributed

a group

knowledge

point

p c N(r).

for
the

simply

single

distributed
at

in

is

of

valid

Proof:

for

definition

This
to

processors

edge,

a

Let p be a fact
are

*

Let

and

Knowledge

by

or knowledge

implies

BpDNp and KPS~p

a fact
We

belief

knowledge.

the following

graph.

Knowledge

that

knowledge

6.2),

in polynomial

communication

Distributed

prove

common

of distributed
coordination

standard

of a fact

from

forming
an edge

received.

the

or falsity

6.1

by

adding

case of general

we make

time

graph

graphs

p.

must

failures

processors
1 + 1, (T, 1 +

nonfault

at
Since

least

communicate

1) 1= ENp.

y pro cessor
n -

it knows

be correct,

and n > 2t.

t

receives

t processors

>

that

(r, 1+2)

In

at least
~

one

EN(DNpA

p), as desired.
+-

In

A ~),

•1

either

duction,

209

case, DNp +- OEN(DNp
DNp +- S$p is valid.

A p) so, by in❑

Lemma

12:

formation

6’on8ideI’

protocol

execution

the

in an asynchronous

or with send-

crash failures
ures

and

n

>

DMp

a

S~p

of a full

2t.

If

p

system

or general

is a fact

omission

about

distributed

in-

runs,

with
fail-

low

complexity

then

dination

us to analyze

proof

+- S$p
showing

Let

(r,l)

the

begins

is valid.
first

We

by

will

showing

do this

Complexity

This

section

and

Section

6.3 both

conditions

are

facts

faulty

other

orem

13 showed

that,

for

E~~.

BpDNp.

Theorem

protocol

for

processor

receives

sent

after

Now consider
There

are

a message

time

two

k.

from

Clearly,

every

(r, j)

>

cases

only

crash

failures.

Consider

j + 1, p knows

that

to

forms

any

coordination

At

sent messages

after

it knew

received,
that

n

>

2t.

j

knows

that

each
at

so,

by

DMq

was

at least

be

sent

such

a message

aft er time

t > t such

one is from
+

ODN~

is valid,

+-

S~p

DN~

and

which

Theorem

12 lead

13:

Let

is valid

tem

in

KPS~P

which

p

Proof

Since

clusion.

implies

that

Lemmas

11

systems
This

be a fact

KpSfjp
and

is

about

valid.

as

The

12,

in which

~

W$p

+

KpDMp

DNP
the

the

*

sys-

These
that

ordination

equivalences
the

for

ures

can

are

importaut

or

knowledge

these

be

exactly

expressed

because

in

must
easy

simply

for

examines

whether

support

or not

p.

This

the

can

be

of Optimum

complexity

of consistent

doing

cases,

so for

belief

in,

can

in others

need
It

test

in

out

that,

polynomial

is NP-hard.

cases in which

for

knowl-

turns

be checked

checking

coordi-

to

distributed

conditions.

this

coordi-

general

processors

not

enabling

Optimum

section

doing

We

be-

so is easy.

knowledge

all

the next
with

co-

terms

of

general

say that

210

for knowledge

these

omission
section

efficiently.
tests

considers

about

which

As will
in which

the input

be

be seen in
in systems
the identity

The

coordination

conditions

fail-

cannot

of distributed

to verify.

can be expressed

a fact

in
they

are NP-hard

failures,
is hard

the enabling

that

systems

testing

processors

of this

in

or in which

can be done

in which
input

that,

detected

section,

of faulty

they
for

at all,

Consistent
is Easy

shows

are easily

By

❑

needed

+- Kpp.

time.

as easy

When

detected

is possi-

proof.

belief

BPp

is very

A processor

Coordination

10

valid.

coordination

rela-

Coordination

cases,

while

6.3.1

con-

Lemma

is did

be done

input,

any

second

is also

S$p

coordi-

a processor

This

determines

of, and

of the

time,

der
show

In

is possible,

is valid,

consistent

completes

for

can

that
p.

it is aware

the

is not

some

This
only

we can

general

the input,

that

Complexity

knowledge

noted

the
In

coordination

is gi~en

S~~

and

in polynomial

nation.

is valid

A proof

;

state

gin by considering

consistent

KpDNp

u

to the equivalences

BpDNp

Thus,

BpDNp; thus, Bpp ~
BpDNp + Bpp. Thus,

meaning

the input.

done

nation

theorem:

BpW$yY ~

Then

ble.

local

edge
11 and

this

about

as it believes

of which

in
following

BPP,

Understanding

❑

Lemmas

that

is obvious

Consistent

Since

well.

in the

p per-

processor

it is for p to determine

Kpp +

inputs

6.3

A p) is valid

S~p

about

(r, 1) #

is valid.

~

its

by

~

optimum

pro-

KPp,

Thus,

It
+

as soon

facts

Since

a nonfaulty

EN VPCM

ODNp + OEM(ODMp
OD~p

j.

receives
messages

~

induction,

from

out

earlier,

BpDNp.
act

turns

enThe-

Bp W~p

in any

BpW~p.

hard

con-

the

input.

facts,

that,

of optimum

how

p is a fact

noted

BPDNP

every

such

as

complexity

It

the

coordination,

as soon

their

which

easily.

Because

failures
that

processor

(r, j’)

(r, j’)

This m=s
As

EN~DMp.
A p), as desired.

that

case,

~

j

receives

n –

Thus,

OEM(~DMp
In either

>

nonfaulty

least

that

cessor.
means

j’

tively

for

about

2 showed

by seeing

BpDNp.

be eventually

omission

the

nation

A P).

general

processor

2t,

time

or

it cor-

processors

must

KPODAW.

~

Let

other

>

these

OEN(ODNP
send-

nonfaulty
every

to all nonfaulty

Since

(r, j + 1)

are

and

n

p.

(T, 1) ~

There

●

time

General

restrict

problems

general

an action

address
p c N(r).
rectly

alcoor-

by induc-

abling

●

of optimum

of Optimum

sider

was

knowl-

equivalences

Coordination

ODNp + OEN(ODNpAp).

that

The

that

Let
k ~ 1 be such that
+ ODMO.
(r, k) ~ DN~ and let j > k be such that every non-

that

dkitributed

these

protocols.

6.2
The

~DMp

Because
about,

is valid.

Proof:
tion,

knowledge.

edge is easy to reason

are facts

in a certain
is basic

remainproblems
about

the

form.

We

if it is of the

form

“p’s

initial

conjunctive
&Vi
any

input

is (not)

normal

form

v,”

A formula

(CNF)

our

consideration

those

in the

syst em.3

example,

consider

a synchronous

failures.

Processor

an

send-omission

cessor

q is faulty

cludes

indication

have

sent.

p’s

if and

t such

is,

P =

fact

about

knows
ing

can

each

input.
fact

i,

test
this

as follows.

count

the

to

faulty)

KPDM Vj
could

#ri,j

cessor
or

otherwise,

Any

receive,

sage

that

then

is in

some

method

arrives
In

easily

be

These

this

case,

detected

(in

the

next

observations

and

either

testing
lowing:
(1 <

to

synchronous

14:

6’onsideT

any

failuTes

OT any

send-omission
with

non-FIFO

about

the

polynomial
3 The
under

the

rametrized

sysi!em

communication,

input

that

size.

Then

can

cTash

asynchronous
Let

be exp?’essed

KpDMp

p

its

result

on polynomial

assumption
by

n

that
and

any

local

Proof:

be a fact
(7NF

in

can be checked

in

length
coordination

makes

sense

problem

coordination.

As

for

many

can be implemented
achieved

reduction

The

clique

a clique

is NP-complete

problem

states

giving

and

[5].

The

to

is the

fol-

constant

idea

in a system

k
This

behind

structure

the result

a

clique

in G of size k?

the communication
y processors

by
from

(Vj E)

G =

16:

Consider

omission

the

of the set

forms

a clique.

for synchronous

sys-

state,

and

some

processors,

Given
fact

determining

p

systems

with

a processor
about

the

whether

p,
input

or

not

is NP-hard.
Here

is the

high-level

reduction:

fori=2tokdo
convert

only
is

a synchronous

failures.

G to a system

if Kq DNp
restriction

here

NP-hardness

protocols

is

Turing

following

KpDMp

o?’

system

in

considered
the

optimum

is there

Lemma

results:

with

[15] extended
coordination.

cases

consistent

NP-

or

becomes

following

for

required

Tuttle

problems

Tut-

t ems.

failures

problem

the

and

protocols

time.

is that

The

the

to
6.2,

k ~ IVI),

and faulty

Lemma

only

a graph

of nonfault

knowledge

and

in

when

can be used

of Moses

optimum

coordination),

given

general
the

that,

sys-

coordination

simultaneous

KpDMp.

for

one

that

coordination

problem

section.
lead

note

even

This

coordination

complexity

proof

that

using

is

computation.

Neiger

polynomial-time

send

one

omission

the

be imple-

and n > 2t. In

optimum

to consistent

apply

The

input.

to

showed

in Section

general

failures

NP-hard

computation;

a mes-

the

is similar

who

result

noted

a proto

to

easy

General

can

KpDNp is NP-hard

simultaneous

results

and

be detected

for

again

in polynomial

the case of synchronous

about

(nonsimultaneous

is FIFO,

can

and

the input

Coordination

implementing

result

However,

that

commu-

from

checking
is

systems),
see

hard

known

either

is subsequent

form

protocol

omission

requires

[12],

this

and

that

failure,

solution
about

time.

for

fact

in polynomial
FIFO

failures

that

general

that

general

processors

detect

CNF

Consistent

case, testing

This

is greater

communication

above.

asynchronous
NP-hard;

If

When

tle

If

is correct

distinguished

knowledge

outline

cannot

be

send-omission

omitted).

number

ever

6.3.2

is

con-

facts

all these

omission

transit.

distributed

not

basic

without

to

cannot

(if a message
was

systems

is impossible

is faulty.

to

~~,j

~Kp DA( Vj $i,j.

and

In asynchronous
it

this

in polynomial

to show

negation.

one of these

in

or

system

C be a coordi-

are facts

C-optimum

mented

protocols

For

Vj

are

the

p is a basic

i thus,

graph

(that

If

at least

holds;

see if

of the

be expressed

Then

this

KPDN Vj +i,j trivially

one

can

Let

has a C-optimum

This section considers

to see

~i,j

communication

disjunction.

be faulty

nication,

the

know

t – f, then

Vj

its

conditions

tems with

by inspect-

to see if the

and

of processors

that

the

fact
then

inspect

to

checking

a basic

number

comprise
than

Check

only

is valid,

If not,

KPDM

enabling

cTash

Hard

is a basic

It is not hard

if Ai

whose

CNF;

p is in
~i,j

checked

that

with

asynchronous

communication,

pI’oblem

p knows

KPDN vi +i,j separately.

each

&junction

be

only

in-

or not a processor

graph.

contains

holds.

that
each

can be easily

requires

junction

Recall

non-FIFO

any system

or any

q should

that

where
Whether

if and
for

test

valid;
the

~i,j,

communication

KpDNp

that

~j

the

a basic

the

we

failures.

ConsideT
failuTes

nation

size.

by inspecting

Suppose

15:

that

pro-

state

that

easily

Theorem
with

of

system

local

time.

send-omission

be so

number

p knows

message

graph.

&

the

if p’s

can be checked

communication

of ~ s

only

of a missing

This

that

in

can

processors
As

polynomial

that

in

with

length

to

expressed

polynomial

form

ti~,j (where
i and ~ have finite
range).
While
can expressed in CNF, we
fact about
the input

restrict

of

p is in

if it is of the

return(NO)

pa-

ret urn(YES)

t.

211

execution

/* no clique of size i */

The

following

and

the

that

describes

the

overall

Consider
point,

reduction

the

with

failures.
nonfaulty

protocol
tion

q and

then

In round
none.

occurs

1, q sends
All

from

communicate
in

all

only

there

as far
processors

ones

system,

in the

faulty.

Since

q knows

to have

cannot

be that

any

is no

must

be a clique

q knows

clique

that

cessors

in

that

round

that

that

there

to the

nonfaulty

Then,

(n

<

>

2t),

any

optimum

at time

i – 1 correct

to be correct.

Thus,

p is distributed

was

2,

since

plexity

knowledge

reasoning

systems

with

structed

execution

can

FIFO

tems,

a message

must

arrive

be used

because,

subsequent
any

The

NP-hardness

knowledge

imply

of

Toueg

such

sys-

a problem

in

to an omitted

failure

message

an NP-hardness

for

[6].

tolerates

can be detected.

testing

for consistent

co-

17:

omission

failures

asynchronous
SOTS peTfo?’m
timum

protocol

6’onsideT
that

with

a
is

FIFO

system
either

with

local

for

a consistent

geneTal

synchronous

communication.

NP-hard

computation

or

PTocesin any

coordination

For

paper,

This

lo-

however,

solutions

for

but

not

coordination

the

is the

first

case

computational

solutions

depends

com-

on the

gen-

call

ion

they

Single

omission
is not

this

is ordered

is because

However,

present

then
is FIFO,

sors to perform

NP-hard

Toueg

not

consider

FIFO

and

presented

only

protocols

that

did

local

to
that

when

n > 2t.

If

their

solution

is

that

processors

and

and

a solution

it is not;

the knowledge
so optimally

sys-

Gopal

Agreement

failures

FIFO,

can allow
doing

of

Value

If communication

cation

in asynchronous

algorithms

example,

they

mally,
lier.

NP-hard
coordination

time

general

certain

general

opt imal.

distributed

this

for coordination
to

communicant

ordination:

Theorem

results

con-

In

and

optimum

distinction.

apply

The

One
in syn-

failures

required

in which

of optimum

tems

asynchronous

communication.
is longer

before

in

omission

coordination.

eral/consistent

c1

differences.

are polynomial

literature

results

.[12,15].
. . How-

of whether

or consistent.

consistent

to earlier

coordination,

systems

aneous

do not

coordination

general

regardless

there

nonsimult

Our
Similar

these

general

in the

important

with

impossi-

send-omission

coordination

simultaneous

in

we see that

pro-

processors.

2t.

com-

however,

are related

some

but

local

is FIFO.

case of general

cal computation

for

itself

>

solutions

It

that

are

is the

systems

in

the

there

For

graph.

in

results

simultaneous

chronous

(as it sent to no processors

vertices

of these
for

correct

the

with

results,

passing

is
may

be solved,

systems,

complexity

general

remain

NP-hard

to systems

message

is much
for

processors

may

requires

from

coordination

must

asynchronous

The

unless

coordina-

true

the

problems

extends

ent

cowhen

situation

consistent

a majority

protocol
For

the

as half

the

n

the

to

if

then

that

general

are forbidden

is especially

Here,

Note

for

consist

actions,
This

2t);

result

of these

it

For

im-

analyzed.

1.

protocols

processors

if as many

(n

Table

results

tractable.

failures.

fail

in

was

optimum

faulty

inconsistent

ever,

knowl-

assume

T is faulty

p, it knows

T‘s input,

Now

i – 1).

are

in which

Some

that

~ is distributed

at most

exist,

tion,

shown

T are both

processors

about

they

cor-

non faulty

q and

only

furthermore,

hold

i =

2, the

only

ordination;

failures.

G of size

given

cases,

was
complexity

protocols

are

complex.

In some

coordination

computational

no impossibility

bility

2 if

i (recall

are

1), so q knows

q knows

at time

the

we provide

putation.

from

processors.

corresponding

knows

and pw

G of size

of size

there

in

that
the

knowledge

edge to the nonfaulty
there

p.

at time

be the

that

r

such

coordination

impossible

@ E,

messages

K~ DNp

meaning
T are

to

g and

(v, w)

others,

of fault-tolerant

eds yst ems.

that

results

omission

in G of size i.

is a clique

q knows

If

In

main

more

pV and pW

between

that

might

q and

to

determined

taking

is O.

~ sends

then
1.

as q can tell

responding

input

input

send

round

is no clique

that

Then,

are

communica-

q receives

claim

initial

and

c E,

passes
2,

is a

there

initial

E V)

PV (v
in

The

the problem

in distribut

of optimum

of a full-information

If (v, w)

We

if there

Suppose
n — t.

p be “r’s

to all processors

round

processors.

and

Let

r’s

q.

In

addition,

significant

no communication
1.

In

(only

correctly

round

PV.

Conclusions

considered

possible.

t = i

there

paper

it was

of size

n –

v e V,

Processor

processors

receive

then

each

this

omission

Ieast

and

coordination

\V[ + 2 proces-

at

execution

is mentioned).

and

For

r.

following

At

a clique

n =

be

Discussion

proves

i.

n – i general

must

pro cessor

The

iteration

with

as t =

processors.

processors

then

This

G contains

there

corresponding

is O.”

at

a system

Thus,

p and

is correct.

that

as many

7

of the conversion

fact

conversion

i — 1. Consider

two

of the

it is certain

sors

the details

construction

infor-

communi-

to decide
requires

computation.

ear-

procesGopal

communication
used

polynomial

computation.

op-

prob-

The
timum

lem.

212

complexity
solutions

results
to

of Section

coordination

6 apply
problems

to opwhose

Table 1: The Possibility
General
Synchronous

and Complexity

of Coordination

Consistent

-F’::::’::

Either omission, n > 2t, non-FIFO

Asynchronous

enabling conditions are facts about the input. This
was done by relating knowledge of eventual common knowledge to knowledge about distributed
knowledge.
For asynchronous systems (without
FIFO communication),
these results can be extended to include facts about failures as well. We
are currently exploring ways in which such extensions can be made for other syst ems. We have
already established a different relation between
knowledge of eventual common knowledge and of
distributed knowledge; we plan to extend the results of this paper to coordination problems that
also depend on facts about failures.
Finally, recall that Moses and Tuttle [12] showed
that some coordination problems have no optimum
solution. In an earlier paper [13], we used a new
form of knowledge, called extended common knowledge, to construct optimal solutions to any coordination problem. In the future, we plan to explore
the complexity of this type of knowledge to better
understand the complexity of these optimal solutions and thus complement the results presented
here for optimum solutions.

ronment: Preliminary report.
In Joseph Y.
Halpern, editor, proceedings of the First Conference

on Theoretical

Aspects

of Reasoning

pages 187–206. Morganabout Knowledge,
Kaufmann, March 1986. Also appears as Technical Report RJ4990, IBM Research Laboratory.
[3] Michael J. Fischer. The consensus problem
in unreliable distributed systems (a brief survey). Technical Report 273, Department of
Computer Science, Yale University,
[4] Michael
Michael
tributed
Journal

J. Fischer,

Nancy

June 1983,

A. Lynch,

and

S. Paterson.
Impossibility
of disconsensus with one fault y process.
of the ACM,
32(2):374-382,
April

1985.
[5] Michael R. Garey and David S. Johnson. ComA Guide to the Theputers and Intractability:
ory of NP- Completeness.

W. H. Freeman and

Company, New York, New York, 1979.
Acknowledgements
[6] Ajei Gopal and Sam Toueg. Reliable broadcast in synchronous and asynchronous environments (preliminary version). In J.-C. Bermond
and M. Raynal, editors, Proceedings of the
Third International
Workshop on Distributed
volume 392 of Lecture Notes on
Algorithms,
Computer
Science, pages 110–123, SpringerVerlag, September 1989,

We would like to thank Joe Halpern for pointing
out several problems with an earlier version and
Rimli Sengupta for discussions that helped in the
development of this work.
References
[1]Cynthia Dwork and Yoram Moses. Knowledge and common knowledge in a Byzantine
environment: Crash failures. lnfoTmation and
Computation, 88(2):156-186, October 1990.

A knowledge theoretic
[7] Vassm Hadzilacos.
analysis of atomic commitment protocols (preliminary report). In Proceedings of the Sizth

[2] Ronald Fagin and Moshe Y. Vardi. Knowledge
and implicit knowledge in a distributed envi-

Symposium

on Principles

of Database

Sys-

tems, pages 129–134. ACM Press, March 1987.

213

[8] Joseph Y. Halpern

and Yoram Moses. Knowledge and common knowledge in a distributed
environment. Journal of the ACM, 37(3):549–
587, July 1990.

[9] Joseph Y. Halpern, Yoram Moses, and Orli
Waarts. A characterization of eventual Byzantine agreement. In P?’oceedings of the Ninth
ACM

Symposium

on Principles

of Distributed

Computing, pages 333–346, August 1990.
[10] Murray S. Mazer. A knowledge theoretic account of recovery in distributed systems: The
case of negotiated commitment.
In Moshe Y.
Vardi, editor, proceedings of the Second Conference on Theoretical
Aspects of Reasoning
about Knowledge,
pages 309–324. MorganKaufmann, March 1988.
[11] Ruben
Byzantine

Michel.

Knowledge

in

Distributed

Ph.D. dissertation,
December 1989.

Environments.

Yale University,

[12] Yoram Moses and Mark R. Tuttle.
Programming simultaneous actions using common
3(1):121-169,
1988.
knowledge. Algorithmic,
[13] Gil Neiger and Rida Bazzi. Using knowledge to
optimally achieve coordination in distributed
syst ems. h Yoram Moses, editor, Proceedings of the FouTth
Aspects

Conference

of Reasoning

on TheoTetica~

about Knowledge,

43–59. Morgan-Kaufmann,

pages

March 1992.

[14] Gil Neiger and Sam Toueg. Automatically
increasing the fault-tolerance of distributed algo11(3) :374–419,
rithms. Jow’na/ of Algorithms,
September 1990.
[15] Gil Neiger and Mark R. Tuttle.
Common
knowledge and consistent simultaneous coordination.
In J. van Leeuwen and N. Santoro, editors, %oceed%ngs of the Fowth Inte?’national

workshop

on Distributed

Algorithms,

volume 486 of Lecture Notes on Computer Science, pages 334–352. Springer-Verlag, Sept emComputing.
ber 1990. To appear in Distributed
[16] Mark R. Tuttle, A game-theoretic characterization of eventual common knowledge. Unpublished manuscript, October 1988,

214

Dynamic Software Updates: The
State Mapping Problem ∗
Rida A. Bazzi

Kristis Makris

Peyman Nayeri

Jun Shen

Arizona State University
{bazzi,kristis.makris, peyman.nayeri, jun.shen.1}@asu.edu

...

Abstract
We consider the state mapping problem for dynamic software updates and propose a number of approaches that have
the potential of automating the state mapping in practical
setting.

x = y+z;
w = x/2;
x = w-z;
/* update point */

Categories and Subject Descriptors D.2.7 [Software]:
Distribution, Maintenance, and Enhancement—Enhancement

(a) old version

General Terms Live Software Update, Program Slicing,
Unification, Bug Fixes
Keywords Software Evolution, Dynamic Updates, Backward Compatibility

1.

Introduction

The dynamic software update (DSU) problem consists of
two components. First, there is the need to determine if it is
meaningful (safe) to map a state of the old application to a
state of the new application, and, if it is, determine the required mapping - this is the state mapping problem. Second
the state mapping needs to be effected through a mechanism
that maps an old execution state to a new execution state
- this is the update mechanism problem. A general update
mechanism was proposed in [1]. The mechanism supports
a general state mapping for multithreaded applications including the mapping of active functions state while preserving system resources used by the application(file descriptors,
and socket connections in particular). This paper does not
address the update mechanism problem.
The state mapping problem is undecidable [2]. This implies that, in general, user help is needed to determine safe
update points and to specify the state mapping function.
Nevertheless, this does not mean that it is not possible
to solve the state-mapping problem automatically or semiautomatically without or with little user help for many practical cases of interest. Proposing approaches that bring us
closer to achieving practical state mapping is the goal of this
paper.
∗ This

material is based upon work supported in part by the
National Science Foundation grant #0849980.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and
that copies bear this notice and the full citation on the first page. To
copy otherwise, to republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee.
HotSWUp’09, October 25, 2009, Orlando, Florida, USA.
c 2009 ACM ISBN 978-1-60558-723-3/09/10. . . $10.00
Copyright 

...
if (x > y)
x = y+z;
w = x/2;
x = w-z;

(b) new version

Figure 1. The bug fix problem
The approaches we propose are pragmatic. They vary
between ignoring some differences between the two versions
because they cannot be addressed (bug fixes and corrupted
states) or are benign (changes to log functions), reducing
the amount of state that needs to be mapped (waiting for
light weight functions to exit) or working hard to ensure a
correct mapping for backward compatible features.

2.

Mapping the state

One can identify the following general categories of software
updates: (1) bug fixes; (2) performance improvement; or (3)
adding features (this list is not meant to be comprehensive).
These categories affect how we look at the state mapping
problem (for lack of space, we do not consider performance
improvement).
Some amount of backward compatibility is implicitly
assumed by dynamic software update systems. Otherwise,
dynamic update does not make much sense: in order to
produce a state of the new version from a state of an old
version, whether with user help or automatically, the two
states must be somewhat related. In this paper we are
interested in the problem of automatically mapping all or
a portion of the state from one version to another. This
problem is related to the compatibility assumptions.
2.1

Bug Fixes: the checkpointing approach

A software update that aims at fixing a bug is essentially
incompatible with the version it aims at fixing. One can
have one of two positions vis à vis such an update. On
the one hand, one can consider that it is beyond the scope
of the DSU state mapping problem to deal with such a
situation. The reason is that the state to be mapped might
be corrupted and detecting and fixing a corrupted state is
not part of the DSU problem. A variation on this position is
to assume that the state is not corrupted and hope for the
best! On the other hand, one can consider that mapping a
corrupted state to a non-corrupted state is the quintessential
state mapping problem. The goal is to detect the corruption

...

x = y+z;
w = x/2;
x = w-z;
/* update point */
(a) old version

...
if (newOption)
x = y*z;
else
x = y+z;
w = x/2;
x = w-z;

(b) new version

Figure 2. Code Enhacement
in the state, if any, and replace the state with a noncorrupted state. The code fragment in Figure 1 illustrates
some of the issues in this case.
In the code, x is incorrectly calculated if x ≤ y. At the
update point this information is not available because x has
been redefined. Even though in this program it is possible
to execute the statement in reverse to determine the value
of x at the first assignment, in general this is not possible,
but that is exactly what is needed. This can be achieved
with checkpointing. An extreme, and impractical, approach
is to checkpoint after every statement. A somewhat more
practical approach is to checkpoint at function entries and
log all interactions with the environment and then re-execute
functions from their entry points at the time of an update.
The re-execution would only simulate interaction with the
environment and compare the new interaction with the old
ones for consistency. If the interactions with the environment
are consistent with the old ones, then the new internal state
can be computed. In the example above, the corruption of
the state can be determined if the value of x between the
two versions differ.
2.2

Enhancements: slicing with unification

Enhancements are an important category of updates that
add features to existing applications. The new application
can have new state components that implement new options.
The state mapping in this case should be done in such
a way that preserves the old execution. In other words,
the execution after the update would be equivalent (when
restricting attention to part of the new state) to the old
execution. The problem is to determine how the new state
could be set to that effect. Figure 2 illustrates this situation.
In the figure, the state has a new variable newOption that
controls how the execution proceeds. If newOption is true,
then new behavior is produced, otherwise the old behavior
is maintained. So, at the update point newOption should be
set to false and a general approach is needed to determine
that.
Existing works that compare the behavior of programs [3]
aim at establishing that two programs or pieces of code are
identical. The problem we are facing is related, but with
a twist. The problem is: given two programs, how can you
assign values to free variables so that the programs have
identical behavior. Also, the definition of identical has to
be relaxed to one of compatible. For example an integer
variable is compatible with a long variable as far as update
is concerned (but not the other way around). This recalls
work that uses unification and that was used successfully
for type systems, but also for other problems such as alias
analysis [4, 5].
We propose an approach whereby unification is combined
with techniques to analyze behavioral equivalence (program

slicing and dependency graphs comparison) to achieve a
mapping that results in an execution of the updated program that is compatible with the original execution. Such
an analysis need not always produce positive results (attempting to unify changes due to bug fixes would not work
because there are no free (new) variables in such cases). Exploring this approach is an exciting line of research.
2.3

Smaller state: light-weight functions

In order to avoid mapping active functions on the stack,
DSU systems have traditionally opted to wait for quiscent
points. This also has the advantage of mapping a smaller
state. The update mechanism of [1] can map active functions and in general one cannot wait for a quiescent point.
We adopt a middle ground. An analysis (conservative) of the
program can come up with a list of functions that can be
expected to exit in a timely fashion. We call these functions
light-weight functions. The update system will ensure that
no update points are in light-weight functions. This way,
there is no obligation to handle the mapping of the state of
these functions. A preliminary (and primitive) implementation of program analysis for light-weight functions indicate
that at least 20% and up to 60% of functions can be easily determined to be light-weight. We expect that further
analysis will produce better numbers.
2.4

Ignoring some changes: log functions

Some changes to the code can be considered benign. For example, the format of error messages produced by an application might change. If such messages are consumed outside
the application whether by a human or by other applications that read log files (and that are not available to the
update system), then a reasonable approach is to ignore such
changes in format as harmless. This would require that we
detect changes that are log-related. We have already implemented some heuristics to detect log-related changes. The
heuristics take into consideration the function signature, the
function name and the calling pattern to determine if a function is log-related. Initial results for a number of real-world
applications indicate that 10% of changes can be attributed
to automatically detected log functions.

3.

Conclusion

We believe that it is possible to address the general state
mapping problem by adopting a pragmatic approach that
aims at identifying cases that restrict the problem or by
eliminating some cases from consideration.

References
[1] Kristis Makris and Rida A. Bazzi. Immediate Multi-Threaded
Dynamic Software Updates Using Stack Reconstruction.
U SENIX’09 Annual Technical Conference, 2009.
[2] Deepak Gupta, Pankaj Jalote, and Gautam Barua. A formal
framework for on-line software version change. Software
Engineering, 22(2):120–131, 1996.
[3] Wuu Yang, Susan Horwitz, and Thomas Reps. Detecting
program components with equivalent behaviors. Technical
Report CS-TR-1989-840, 1989
[4] Robin Milner. A Theory of Type Polymorphism in Programming. J. Comput. Syst. Sci. 17(3): 348–375, 1978.
[5] Manuvir Das. Unification-based pointer analysis with
directional assignments. P LDI ’00: Programming language
design and implementation, 35–46, 2000.

Distrib. Comput. (2000) 13: 45–52

c Springer-Verlag 2000


Synchronous Byzantine quorum systems

?

Rida A. Bazzi
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-5406, USA (e-mail: bazzi@asu.edu)
Received: June 1998 / Accepted: August 1999

Summary. Quorum systems have been used to implement
many coordination problems in distributed systems such as
mutual exclusion, data replication, distributed consensus,
and commit protocols. Malkhi and Reiter recently proposed
quorum systems that can tolerate Byzantine failures; they
called these systems Byzantine quorum systems and gave
some examples of such quorum systems. In this paper, we
propose a new definition of Byzantine quorums that is appropriate for synchronous systems. We show how these quorums can be used for data replication and propose a general
construction of synchronous Byzantine quorums using standard quorum systems. We prove tight lower bounds on the
load of synchronous Byzantine quorums for various patterns
of failures and we present synchronous Byzantine quorums
that have optimal loads that match the lower bounds for two
failure patterns.
Key words: Distributed systems – Quorums – Fault tolerance – Byzantine failures – Load

1 Introduction
A quorum system is a collection of sets (quorums) that mutually intersect. Quorum systems have been used to implement mutual exclusion [1, 12], replicated data systems [10],
commit protocols [22], and distributed consensus [16]. For
example, in a typical implementation of mutual exclusion
using a quorum system, processors request access to the
critical section from all members of a quorum. A processor
can enter its critical section only if it receives permission
from all processors in a quorum.1 Since any two quorums
intersect, mutual exclusion is guaranteed. Quorum systems
are usually evaluated according to the following criteria:
1. Quorum Size. The size of a quorum determines the number of messages needed to access it. The size of the
smallest quorum in a quorum system is usually used as
a measure of the number of messages required by protocols using quorum systems.
? An earlier version of this paper appeared in the Proceedings of the
Sixteenth ACM Symposium on Principles of Distributed Computing [3].
1 Additional measures are needed to insure that the implementation is
fair and deadlock free.

2. Load. Informally, the load of a processor measures the
share it has in handling requests to the quorum system.
For a given probabilistic access rule of a quorum system, the load of a quorum system is the probability of
accessing the busiest processor. The load of a quorum
system is the minimum load over all access probability
distributions.
3. Availability and Failure Probability. A quorum system
is said to be available if one of its quorums consists of
correct processors. If failures occur randomly according
to some probability distribution, the failure probability
is the probability that the quorum is not available.
Other criteria are also considered in the literature, but we do
not study them in this paper.
Work on quorum systems traditionally considered crash
failures [1,2,4,5,9,10,12,19–21]. Recently, Malkhi and Reiter [13] proposed the interesting notion of Byzantine quorums - quorum systems that can tolerate Byzantine failures.
They showed that the traditional definition of quorums is
not adequate for tolerating Byzantine failures. For example,
in the mutual exclusion problem, the decision on whether to
enter a critical section depends on the fact that a process that
belongs to the intersection of two quorums should not give
conflicting replies to two different processes. Unfortunately,
consistent replies by one process cannot be guaranteed in
the Byzantine failure model. Therefore, in Byzantine quorum systems, the intersection of two quorums should contain
enough correct processors so that correct processors can unambiguously use the information returned by the elements of
a given quorum. Malkhi and Reiter [13] called these quorum
systems masking quorum systems because they hide the failures from processors using them. They also introduced dissemination Byzantine quorum systems for the case in which
the quorum system is used to replicate self-authenticating
data. Finally, they proposed opaque Byzantine quorum systems that can be used without requiring the processors to
know the failure pattern in the system. They presented a
number of masking, dissemination, and opaque quorum systems for different failure patterns and calculated their load.
The quorum systems they presented make no synchrony assumptions.
2 Contributions
In this paper we study the effect of system synchrony on
how quorum systems are used. This leads us to revisit the

46

traditional definition of availability and load of quorum systems and the way quorums are accessed in asynchronous
systems. We find that the traditional definition of availability is intended to guarantee the correctness of quorum-based
protocols in a model in which failed processors can recover
and, therefore, allows for a probabilistic model of failures.
We argue that the traditional definition of availability is not
needed in a synchronous system subject to failures from
which processors cannot recover. We note that the probabilistic model is not adequate for Byzantine failures and
conclude that weaker definitions of availability and quorum
access are needed in synchronous systems subject to Byzantine failures. We also find that the traditional definition of
load is not appropriate for asynchronous systems with failures.
We propose a definition of Byzantine quorum systems
for synchronous systems (synchronous Byzantine quorums).
The definition we propose is less restrictive than that of [13]
and allows for more failures in the system. We develop necessary and sufficient conditions for the existence of synchronous Byzantine quorums that can tolerate a general failure pattern and we propose a general method for constructing
synchronous Byzantine quorum systems. We develop tight
lower bounds for the load of synchronous Byzantine quorums for various failure patterns and, for each lower bound,
we present a quorum system whose load is optimal and
matches the lower bound.
The rest of the paper is organized as follows. Section 3
presents some basic definitions and introduces the notions
of quorum access and availability. Section 4 argues that, in
synchronous systems, the definition of Byzantine quorum
systems should be relaxed and presents a relaxed definition.
Section 5 presents a general construction of Byzantine quorum systems. Section 6 proves lower bounds on the load
of synchronous Byzantine quorum systems for various failure assumptions. Section 7 presents synchronous Byzantine
quorum systems that have optimal loads.
3 Definitions and system model
3.1 System model
We consider a set P of n server processors and a set C
of client processors that are distinct from the servers. Processors share no memory and communicate by sending and
receiving messages. We assume that message delivery is reliable.
For asynchronous systems, we assume that there are no
bounds on processors’ speeds or message delivery time, and
that failures cannot be detected using timeouts. For synchronous systems, we assume that there are upper and lower
bounds on processors’ speeds and message delivery time,
and that failures can be detected using timeouts.
3.2 Failure model
The types of failures that occur in the system affect the way
a quorum system can be used. In this paper we assume that
server processors can fail and client processors do not fail.
In studying quorum systems, it is sometimes assumed
that processors fail randomly according to some probability distribution and that failures are temporary [18]. In a

R.A. Bazzi

probabilistic model of failures, at any point in time, there
is a positive probability that more than the majority of processors are faulty. A probabilistic failure model is not used
for Byzantine (arbitrary) failures because there is no way to
guarantee system consistency if a majority of the servers is
subject to Byzantine failures. A different model of failures is
usually used for Byzantine failures. In the Byzantine failure
model, it is usually assumed that there is a known bound on
the number of failures that can occur in the system and that
failed processors do not recover.
In this paper we only consider Byzantine failures and we
do not consider systems that are subject to both Byzantine
and probabilistic crash failures as done in [14]. We adopt the
generalized failure model introduced in [13]. The set faulty
denotes the set of faulty processors in the system. A failure
pattern F identifies the possible sets of faulty processors in
the system. We write F = {F1 , F2 , . . . , Fm }. There exists
an element F of F such that at any given time the faulty
processors belong to F . The processors do not necessarily
know F , but the quorum system designer should know F .
A common example of a failure pattern is the f -threshold
pattern in which F = {F ∈ P : |F | = f }. Another
interesting failure pattern is the disjoint pattern in which all
elements of F are disjoint [13]. This pattern is intended
to model the case in which failures are not independent.
For example, many processors might share the same power
source and fail together if the power source fails.
3.3 Coteries and quorums
The standard definition of a quorum is the following.
Definition 1 A quorum system Q over P is a set of subsets
(called quorums) of P such that any two quorums have a
non-empty intersection.
The intersection property of quorums is essential for their
use in coordination problems.
Definition 2 A coterie C over P is a quorum system over
P which is minimal under set inclusion.
In this paper, we only consider quorum systems that are
coteries.
Definition 3 A quorum system Q dominates a quorum sys/ Q 0 and for every Q0 ∈ Q 0 there exists Q ∈ Q
tem Q 0 if Q =
such that Q ⊆ Q0 .
This suggests the definition of a non–dominated quorum systems, or NDQ for short.
Definition 4 A quorum system is non–dominated if there is
no other quorum system that dominates it.
3.4 Strategies and load
This section presents the formal definitions of strategy and
load as introduced in [18].
A protocol using a quorum system chooses a quorum for
access according to some rules. A strategy is a probabilistic
rule to choose a quorum. Formally, a strategy is defined as
follows.

Synchronous Byzantine quorum systems

Definition 5 Let Q = {Q1 , . . . , Qk } be a quorum system. A
strategy w for Q is a probability distribution over Q .
The probability of accessing Qi is w(Qi ). For every processor q ∈ P , a strategy w induces a probability that q is
accessed. This probability is called the load on q. It should
be clear that the definition of a strategy assumes that, to use
a quorum system, a processor needs to contact each element
in some quorum.
The system load is the load on the busiest element induced by the best possible strategy.
Definition 6 Let w be a strategy for a quorum system Q =
{Q1 , . . . , Q
m }. For any q ∈ P , the load induced by w on q
P
is lw (q) = Qj 3q w(Qj ). The load induced by w on Q is
Lw (Q ) = max lw (q)
q∈P

The system load on Q is
L (Q ) = min{Lw (Q )},
w

where the minimum is taken over all strategies.
The load as defined above applies to a system with no
failures. If failures occur, then the load of the system increases.
Naor and Wool [18] point out the fact that the load of
a quorum system is a combinatorial property of the quorum
system, which is not necessarily equal to the load of the
particular access strategy being used. While this is true in
general, the relevance of the concept of load is due to the
fact that, in principle, some access strategy can have a load
that is equal to the load of the quorum system.
3.5 Quorum access and availability
Processors access a quorum to coordinate their actions. The
following is the typical way to access a quorum in systems
subject to crash failures. A processor, the client, sends a
request to every processor, server, in a quorum set. Upon
receiving a request, a correct server updates its state and
sends a reply. The client waits until it receives a reply from
every processor in the quorum. The replies typically reflect
requests made by other clients. If the client receives replies
from all processors in a quorum then the access is considered successful. If one of the servers fails, then the client
attempts to access another quorum that does not have any
faulty processor (the question of finding a quorum with no
faulty processors has been addressed in [2, 19]). Since processors access a quorum only if all its members are correct,
two clients are always guaranteed to receive a response from
a common correct server which belongs to a non-empty intersection of two quorums. The correctness of quorum-based
protocols relies on this intersection property. Traditionally, a
quorum system is said to be available if one of its quorums
consists of correct processors [18].
One might think that, in systems with crash failures, it is
sufficient to have a non-empty intersection between quorums
to have availability. For instance consider the following alternative definition of availability for a quorum system that
satisfies the intersection property.

47

Availability (alternative). A quorum system is available if
one of its quorums intersects every other quorum in a correct
processor.
This definition seems to guarantee that any two replies from
different quorum sets are consistent.2 Unfortunately, this definition is not sufficient if processors can recover from failures - which is usually case if processors are subject to crash
failures. The following example clarifies this point.
Consider the quorum system Q = {{1, 2, 3, 4},{1, 2, 5, 6},
{3, 4, 5, 6}}. This system is such that any two quorums have
two processors in common. Assume that initially processors
1, 3, and 5 are faulty. Trying to get permission to enter
its critical section, a processor accessing quorum {1, 2, 3, 4}
would get permission from {2, 4}, a set that intersects every
other quorum in a correct processor. Assume that after this
request is granted processors 2, 4, and 6 become faulty, and
processors 1, 3, and 5 recover. Now, another processor trying to access its critical section might be granted permission
by the set {1, 3}, a set that intersects every other quorum in
a correct processor. If the alternative definition of availability is adopted and failures are temporary, we will have two
processors in the critical section. It is important to note that
the alternative definition of availability is weaker than the
traditional definition of availability. In fact, if a quorum Q
consists of correct processors, and the quorum system satisfies the intersection property, then Q must intersect every
other quorum in a correct processor.
While the alternative definition of availability cannot be
used if failures are temporary (in both synchronous and asynchronous systems), in synchronous systems, the alternative
definition of availability can be used if failures are not temporary. To access a quorum in a synchronous system in
which faulty processors cannot recover, a processor sends
a request to each element in a quorum set and, for each element in the set, waits to receive a reply or detect the failure
of the element.3 If the set of processors that replied intersects every other quorum, then the replies can be used to
coordinate. This is because processors that have failed will
not recover later and put the system in an inconsistent state.
This is the case if failures are arbitrary.
The results of this paper depend on the assumption that
Byzantine failures are permanent. In the rest of this section,
we will discuss this assumption.
Arbitrary failures are permanent because a server that
suffers an arbitrary failure cannot be recovered by restoring
its last state before failure. Instead, a server that suffers an
arbitrary failure needs to be reset and then brought up to date
with the rest of the system. This is equivalent to removing
the failed server from the system and then reintroducing the
failed server to the system. Adding and removing servers
dynamically is a complicated and expensive operation that
interferes with the use of the quorum system and we do not
consider it in this paper.
The concern about recovering arbitrary failures can be
justified if it is not possible to establish a priori a failure pattern that is valid for the life of system. In such a case, it is
necessary to recover failed servers, but doing that in the con2 We require that the quorum system be an intersecting quorum system
to ensure that the quorum system can be used consistently to start with.
3 Timeouts are not possible in asynchronous systems.

48

text of quorum systems is beyond the scope of this paper. In
some systems, servers subject to Byzantine failures can simply crash. Recovering a crashed server is simply achieved
by restoring its state before the crash. Unfortunately, distinguishing between a server that crashes without corrupting
stored data and a server that crashes and corrupts stored data
is not easy. That distinction can be done manually by the
system administrator, but we do not see a way to achieve
it automatically. That is the reason why we only consider
Byzantine failures in our model and we do not consider a
mixed model of failures.
4 Byzantine quorums
Malkhi and Reiter [13] gave a general definition of quorum
systems that can tolerate Byzantine failures in asynchronous
systems. We will first discuss their definition, then we propose a less restrictive definition that works in synchronous
systems.
4.1 Byzantine quorums in asynchronous systems
If failures are arbitrary, a processor might receive conflicting replies from faulty and correct processors. It follows that
a processor must base its coordination decisions on replies
that it knows to be from correct processors. Motivated by
this requirement, Malkhi and Reiter gave the following definition [13]:
Definition 7 A quorum system tolerates failure pattern F
if
1. ∀Q1 , Q2 ∈ Q ∀F1 , F2 ∈ F : (Q1 ∩ Q2 ) − F1 6⊆ F2 .
2. ∀F ∈ F ∃Q ∈ Q : F ∩ Q = ∅
The first condition requires that the intersection of two quorums is not contained in the union of two sets in F .
This will guarantee that the replies of correct processors
can be identified. To see why this is the case, consider, as
an example, the f -threshold failure pattern. For that pattern, the condition reduces to a familiar-looking condition:
Q1 ∩ Q2 ≥ 2f + 1. So, it is always possible to have f + 1
identical replies in the intersection of two quorums. One of
these replies must be the reply of a correct processor. It
should not be surprising that, even though the failures are
arbitrary, only a simple majority of correct processors (as
opposed to a two-third majority) is required in the intersection. The second condition is based on the traditional definition of availability; it requires that some quorum consists
of correct processors.4 As we mentioned above, this definition of availability is designed for a failure model in which
faulty processor can recover, which is not the case for arbitrary failures. Nevertheless, in asynchronous systems, the
second condition is needed because there is no way to differentiate a slow processor from a faulty one. To access a
quorum in an asynchronous system, a processor cannot simply send requests to all processors in a quorum set and wait
4 Malkhi et al. [14] also suggest that the second condition is also needed
to prevent faulty processors from denying access to a quorum. We will
show that, in synchronous systems, faulty processor cannot deny access to
a quorum.

R.A. Bazzi

for replies. In the worst case, in a failure-free execution,
a processor might have to send requests to every processor in the system and then wait for replies from a quorum
that consists of correct processor; in fact, there is no way
for a client to distinguish a failure-free execution in which
servers are slow from an execution in which some servers
fail to reply. The availability condition is needed to guarantee that some response is received. Sending requests to
every processor limits the efficiency of the use of quorums
in asynchronous systems and negatively affects the load of
any particular access strategy. Sending requests to every processor is not needed in synchronous systems if no failures
occur. In what follows, we will refer to Byzantine quorums
for asynchronous systems as asynchronous Byzantine quorum systems.
4.2 Byzantine quorums in synchronous systems
We should first point to an important distinction between
quorum systems that tolerate Byzantine failures and those
that tolerate crash failures. If failures are benign, then the
availability property can be checked a posteriori: a client
can determine if a quorum system is available or not. This
ability to determine the availability of the quorum system
is not possible if failures are Byzantine. If many failures
occur and the system is not available, then a client cannot
determine that fact because, in general, there is no way to
distinguish a correct processor from a faulty one. It follows
that the availability condition of a Byzantine quorum should
hold a priori. A Byzantine quorum system designed for a
given failure pattern cannot, in general, dynamically adapt if
the failures are different from those specified by the pattern
it is designed to tolerate.
From the discussion above and in the previous section,
we conclude that only the first of the two conditions proposed by [13] is needed for Byzantine quorums in synchronous systems.
Definition 8 For Byzantine failures, a quorum system tolerates failure pattern F if:
∀Q1 , Q2 ∈ Q ∀F1 , F2 ∈ F : (Q1 ∩ Q2 ) − F1 6⊆ F2
This definition, which is identical to the first condition of
Definition 7, requires that the failure pattern be known a
priori. It is enough to guarantee that the quorum system can
handle the worst-case failure scenario. That’s why we do
not have to require an extra condition for availability. Note
that a synchronous Byzantine quorum system is not necessarily an asynchronous Byzantine quorums system, whereas
any asynchronous Byzantine quorum system is also a synchronous Byzantine quorum system.
To see how a quorum satisfying Definition 8 can be used
to coordinate in the presence of Byzantine failures, we consider the implementation of a replicated single-writer singlereader safe register [11]. The implementation can be used to
implement atomic registers using the techniques of [11].
The implementation we present uses standard data replication techniques [6]; a very similar implementation was
used in [13]. To write a value v, a client sends v along with
a timestamp to each server in a quorum Q. Then, it waits to
receive acknowledgments or timeouts for each server in Q.

Synchronous Byzantine quorum systems

49

Value Read(Q: Quorum)

Write(Q: quorum, v: value)

Client:
Send a read request to each server in Q
Wait until each server u in Q replies with a pair (vu , tu )
or is detected to be faulty (using timeout)
A = {(v, t) : ∃F + ⊆ Q[∀F ∈ F [F + 6⊆ F ] ∧ ∀u
∈ F + [vu = v ∧ tu = t]]}
(vmax , tmax ) = max{(v, t) : (v, t) ∈ A}
return vmax

Client:
static t : timestamp

Server:
if read request is received from c
send (v, t) to c

t=t+1
Send (v, t) to all servers in Q
Wait for an “ack” or a timeout for each server in Q
Server:
if (v, t) is received from c then
(vl , tl ) = (v, t)
send “ack” to c
Fig. 2. Write Procedure

Fig. 1. Read procedure

To ensure that the timestamp is unique and always increasing, the single writer always increments the timestamp after
each write (alternatively, the writer can query the elements
of Q for the timestamps they have for their versions of the
register).
To read, a client sends a read request to every element
in some quorum Q, and waits to receive a (v, t) pair or a
timeout for each element of Q. It then forms a set A consisting of those pairs for which enough identical replies have
been received. Finally, the value with the largest timestamp
is returned. In the code, (v, t) ≤ (v 0 , t0 ) if t ≤ t0 .
Figures 1 and 2 contain the code for the Read and Write
procedures.
Theorem 9 The protocol consisting of the read and write
procedures implements a safe single-reader single-writer register.
Proof. We need to show that if a read is not concurrent with
a write, then it returns the last value written. Consider the last
write operation wl and let Qw be the quorum it uses, vl be
the last value written, and tl be the corresponding timestamp.
Every correct processor s in Qw must have (vs , ts ) = (vl , tl )
by the time wl terminates. In fact, wl terminates only after
the writing processor receives acknowledgments or timeouts
for each processor in Qw . It follows that a read following
wl will return (vs , ts ) such that vs = vl and ts = tl for each
correct processor s in Qr ∩ Qw , where Qr is the quorum
used by the read operation. The set of correct processors in
Qr ∩ Qw contains Qr ∩ Qw − F for some F ∈ F . By
the definition of synchronous Byzantine quorum systems, it
follows that the set of correct processors in Qr ∩ Qw is not
a subset of any faulty set. Therefore, vl is an element of
the set A in Figure 1. It remains to show that A contains
no other element with a larger timestamp. We note that no
correct processors in Qr can have a value with a timestamp
larger than or equal to tl because wl is the last write before
the read and correct processors only update their values and
timestamps when a write occurs. So, the only processors
that can have a timestamp larger or equal to tl are faulty
processors in Qr . But such processors are not numerous
enough to include their values and timestamps in A (∀F + ∈
t
u
F [F + 6⊆ F ] does not hold because F ⊆ F ).
Note that the implementation still works even if every
quorum has some faulty processors (system is not available
according to the traditional definition of availability). This

is due to the fact that Definition 8 a priori handles the worst
failure scenario assumed in the system.
Given a failure pattern, we are interested in deciding
whether there exists a quorum system that tolerates the failure pattern. The following two propositions give necessary
and sufficient conditions.
Proposition 10 There exists a quorum system that tolerates
failure pattern F if and only if QP = {P } tolerates F .
Proof. If QP = {P } tolerates F , then there exists a quorum system that tolerates F . If there exists a quorum system
Q that tolerates F , then there exists a quorum in Q that
cannot be contained in the union of any two elements of
F . It follows that P is not equal to the union of any two
elements in F and that {P } tolerates F .
Proposition 11 There exists a quorum system that tolerate
/
failure pattern F if and only if ∀F1 , F2 ∈ F : P =
F1 ∪ F2 .
Proof. Direct application of Proposition 10.

t
u

Corollary 12 There exists a quorum system that tolerates
the f -threshold failure pattern if and only if n ≥ 2f + 1.
This necessary and sufficient condition for the f -threshold
failure pattern is to be contrasted with the requirement that
n ≥ 4f + 1 for the existence of a Byzantine quorum system
for asynchronous systems [13].
5 General construction of Byzantine quorums
In this section we present a simple general method to construct Byzantine quorums and we show that it can be used
to construct Byzantine quorum systems with optimal load.
In [14], a similar construction is independently proposed in
a more general setting.
Let Q be a quorum system on a set P of n processors.
A Byzantine quorum on a set P 0 of n(2f + 1) processors
can be obtained as follows. Partition P 0 into 2f + 1 sets
of n processors each. For each set Pi of the partition, let
βi be a bijection between P and Pi . Define Qi = βi (Q ) =
{βi (Q) : Q ∈ Q }.
Lemma 13 The set Qi is a quorum system on Pi .
Proof. For any two set Pi and Qi of Qi , βi−1 (Pi )∩βi−1 (Qi )=
/∅
t
u
r=
/ ∅. It follows that Pi ∩ Qi = βi (r) =

50

R.A. Bazzi

Definition 14 Let Q 0 = {Q0 : Q0 ∩ Pi ∈ Qi , 1 ≤ i ≤
2f + 1}
Lemma 15 Q 0 is a Byzantine quorum system on P
can tolerate up to f arbitrary failures.

0

that

Proof. We need to show that any two quorums have 2f + 1
elements in common. For any two quorums Q and Q0 of Q 0 ,
we have (Q ∩ Q0 ) ∩ Pi ⊃ (Q ∩ Pi ) ∩ (Q0 ∩ Pi ) = Qi1 ∩ Qi2
for some quorums Qi1 and Qi2 of Qi , i = 1, . . . , 2f + 1.
/ ∅, it follows that (Q ∩ Q0 ) ∩ Pi =
/ ∅
Since Qi1 ∩ Qi2 =
i = 1, . . . , 2f + 1. Since, the sets Pi , i = 1, . . . , 2f + 1,
are disjoint, it follows that Q ∩ Q0 contains 2f + 1 distinct
elements.
t
u
Even though the method is simple, we will see in Section 7.1
that it can be used to construct Byzantine quorum systems
with optimal load.
The following lemma is needed in calculating the load
of Q 0 .
S
Lemma 16 Q 0 = Q∈Q Q 0 Q , where Q 0 Q = {Q0 ∈ Q 0 :
Q = β1 (Q0 ∩ P1 )}.
S
Proof. It is clear that Q∈Q Q0Q ⊆ Q 0 because Q 0 Q ⊆ Q 0
S
by definition. So we need to prove that Q 0 ⊆ Q∈Q Q 0 Q .
This follows from the fact that, for any Q0 ∈ Q 0 , Q0 ∈
t
u
Q 0 β −1 (Q0 ∩P1 ) .
1

The next lemma shows that the load of Q is equal to
the load of Q 0 .
Lemma 17 L (Q 0 ) = L (Q ).
Proof. We need to prove that L (Q 0 ) ≥ L (Q ) and
L (Q 0 ) ≤ L (Q ). We first note that L (Q ) = L (Qi ),
i = 1, . . . , 2f + 1, because βi is a bijection, i = 1, . . . , 2f + 1.
– L (Q 0 ) ≥ L (Q ). Let w0 be an optimum strategy for
0
choosing quorums
P of Q . 0 For0 each element Q of Q ,
define w(Q) = Q0 ∈Q 0 Q w (Q ). w is an access strategy
on Q . In fact
by
P everyPQ ∈ Q , and,
P w(Q) ≥ 0 for
0
0
w
(Q
)
=
Lemma 16, Q∈Q w(Q) = Q∈Q
0
0
Q ∈Q Q
P
0
0
Q0 ∈Q 0 w (Q ) = 1.
For any processor p ∈ P , and optimumPaccess strategy w on Q , we have lwo (p) ≤ lw (p) = Q3p w(Q) =
P
P oP
0
0
=
Q3p
Q0 ∈Q 0 Q w (Q )
{Q0 :β1−1 (Q0 ∩procs1 )3p}
P
P
w0 (Q0 ) = {Q0 :(Q0 ∩procs1 )3β1 (p)} w0 (Q0 ) = Q0 :Q0 3β1 (p)
w0 (Q0 ) = lw (β1 (p)) ≤ L (Q 0 ). Since p was chosen arbitrarily, it follows that L (Q ) ≤ L (Q 0 ).
– L (Q 0 ) ≤ L (Q ). Let w be an optimum strategy for
choosing quorums of Q . For each element Q of Q ,
consider the set ρ(Q) = β1 (Q) ∪ . . . ∪ β2f +1 (Q)). ρ(Q)
is clearly a quorum set of Q 0 . ρ is a one to one mapping from Q to Q 0 . Also ρ is a bijection between Q and
ρ(Q ). Define the strategy w0 such that w0 (Q0 ) = 0 if there
exists no Q such that ρ(Q) = Q0 and w0 (Q0 ) = w(Q) if
Q0 = ρ(Q). It follows, using techniques similar to the
ones above, that L (Q ) = Lw0 (Q 0 ) ≥ L (Q 0 ).
t
u

6 Load lower bounds
Let c(Q ) be the size of the smallest quorum of Q . Naor and
Wool [18] proved the following lower bound on the load of
a quorum system.
)
1
Proposition 18 L (Q ) ≥ max{ c(Q
n , c(Q ) }.

This lower bound obviously applies to Byzantine quorum
systems because they are also quorum systems. Malkhi and
Reiter [13] proved the same lower bounds with more direct
methods. In this section, we prove new lower bounds that are
specific to Byzantine quorums in synchronous systems. The
lower bounds we prove also apply to asynchronous Byzantine quorums [13]. We prove lower bounds for a general
failure pattern, the f -threshold failure pattern, and the disjoint failure pattern.The bounds we prove for the f -threshold
failure pattern and the disjoint failure pattern are tight. In
Section 7, we present quorum constructions whose loads
match the bounds for the f -threshold failure pattern and the
disjoint failure pattern.
6.1 General case
We first present a new form for Definition 8.
Proposition 19 Let Q be a quorum system that tolerates
failure pattern F , then
∀Q1 , Q2 ∈ Q ∀F1 , F2 ∈ F :
(Q1 ∩ (P − (F1 ∪ F2 )) ∩ (Q2 ∩ (P − (F1 ∪ F2 )) =
/∅
Proof. Uses standard manipulations on set operators.

t
u

In other words, if Q tolerates F , then the projection of
Q into P −(F1 ∪F2 ) is a quorum system on P −(F1 ∪F2 ),
where F1 and F2 are elements of F .
Let Q − (F1 ∪ F2 ) = {Q − (F1 ∪ F2 ) : Q ∈ Q } be the
projection of Q on P − (F1 ∪ F2 ).
Proposition 20 Let Q be a quorum system that tolc(Q −(F ∪F ))
erates failure pattern F , then L (Q ) ≥ max{ |Q −(Fii∪Fjj)| ,
1
c(Q −(Fi ∪Fj )) }

for any Fi and Fj in F .

Proof. Follows directly from the application of Proposition 18 to Q − (Fi ∪ Fj ) for any two elements Fi and Fj of
F .
t
u
Proposition 21 Let Q be a quorum system that tolerates
1
.
failure pattern F , then L (Q ) ≥ √
n−maxi,j=1,...,m |Fi ∪Fj |

Proof. Follows directly from Proposition 20.

t
u

6.2 Disjoint failure pattern
Applying Proposition 20 to a disjoint failure pattern Fd , we
obtain:
Proposition 22 Let Q be a quorum system that tolerates
Fd = {F1 , . . . , Fm } and assume that the Fi ’s are sorted in
1
.
a decreasing order of size, then L (Q ) ≥ √
n−(|F1 |+|F2 |)

Proof. Since the sets are disjoint, the cardinality of the
union of any two distinct sets is equal to the sum of the
cardinalities of the two sets.
t
u

Synchronous Byzantine quorum systems

6.3 Threshold failure pattern
In the threshold failure pattern, up to f processors can be
faulty.
Proposition 23 Let Q be a Byzantine quorum system that
)
tolerates the f -threshold pattern, then L(Q ) ≥ max{ c(Q
n ,
2f +1
c(Q ) }.
)
Proof. By Proposition 20, L(Q ) ≥ c(Q
n . So, we only need
+1
to prove that L (Q ) ≥ 2f
c(Q ) . Let w be any strategy for the
quorum system Q . Fix Q1 ∈ Q such that |Q1 | = c(Q ).
Summing the loads induced by w on all the elements of Q1 ,
we obtain:
X X
X
lw (u) =
wi =
u∈Q1

X

51

Proof. For simplicity, we assume that n is divisible by 2f +
1 and that 2fn+1 = k 2 + k + 1 for some k = pi and some prime
q
number p. It follows that k ≈ 2fn+1 . Divide the processors

into 2f + 1 identical sets, each of size s = 2fn+1 . On a set of
size s, we can construct
a projective plane system PP [12]
q
2f +1
whose load √1s =
n . By the results of Section 5, we
can construct a quorum system on P with load identical to
that of PP.
t
u

Corollary 26 The bound of Corollary 24 is tight.
Note that the parallel finite projective plane system is
also an asynchronous Byzantine quorum system. It follows
that the bound of Corollary 24 is also tight for asynchronous
Byzantine quorum systems.

u∈Q1 Qi 3u

X

Qi u∈Q1 ∩Qi

w(Qi ) ≥

X

(2f + 1)wi = 2f + 1.

Qi

It follows that there exists an element in Q1 that suffers a
2f +1
. The calculation above can be understood as
load of c(Q
1)
follows. Each quorum Qi puts a load of wi on each element
it has in common with Q1 . Since it has 2f + 1 elements
in common with Q1 , it follows that it contributes a load of
(2f + 1)wi to the sum of the loads of all elements of Q1 .
The contributions of all quorums to the sum of the loads of
t
u
elements in Q1 is ΣQi f wi = 2f + 1.
Note that for f = 0, Proposition 23 reduces to Proposition 18.
The following lower bound is a corollary of Proposition 23.
q
Corollary 24 L(Q ) ≥ 2fn+1 .
Proof. We have two cases
q to consider. If√ c(Q ) ≥
√
c(Q )
n(2f + 1), L(Q ) ≥ n ≥ 2fn+1 . If c(Q ) ≤ n(2f + 1),
q
+1
2f +1
2f +1
√
≥
=
L(Q ) ≥ 2f
c(Q )
n . So, in all cases,
n(2f +1)
q
t
u
L(Q ) ≥ 2fn+1 .
We should note that in [14], Malkhi et al. independently
prove some lower bounds similar to ours for the f -threshold
failure pattern.
7 Load upper bounds
In this section, we present synchronous quorum constructions that have loads that match the lower bounds proved in
the previous section.
7.1 Threshold failure pattern

7.2 Disjoint failure pattern
In this section, we prove that the lower bound developed for
the disjoint failure pattern is tight by constructing a system
whose load matches the lower bound of Proposition 22. First,
we introduce some sets and then we present the construction
using these sets.
Let F = {F1 , F2 , . . . , Fm } be the set of failure sets
ordered in decreasing size. Let α = n − (|F1 | + |F2 |). By
Proposition 18, the load of a Byzantine quorum that tolerates
F is greater than √1α .
Our construction will proceed as follows. First we show
that there are three disjoint sets of size greater than α6 such
that no two of them will have an non-empty intersection
with the same faulty set. Then, on each of the three sets
Si , i ∈ {0, 1, 2}, we construct a quorum system whose load
is O( √ 1 ). On P , we construct a quorum system whose
|Si |

elements consist of the union of three quorums, one from
each of the three sets - similar to the general construction
of Section 5. Given the sizes of the three sets and the loads
of the three quorum systems on them, the resulting quorum
system will have a load that matches the lower bound. Next,
we define two constants that will be used in defining the
three sets.
Define m1 and m2 as follows:
o
n
S
α
– m1 = min j : |F1 ∪
3≤k≤j+1 Fk | ≥ 3
o
n
S
α
F
|
≥
– m2 = min j : |F2 ∪
k
m1 <k≤j+1
3
Note that j and k are bound variables in the definitions of
m1 and m2 . Also, m1 and m2 are always guaranteed to exist.
Now, define the three sets S0 , S1 and S2 as follows:
S
F
– S0 = F1 ∪
S3≤k≤m1 k
– S1 = F2 ∪
m1 <k≤m2 Fk
S
– S2 = m2 <k Fk = P − (S1 ∪ S2 )
α
3,

In this section, we prove that the lower bound developed for
the threshold failure pattern is tight.

Note that if |F1 | ≥
S1 = F2

Proposition 25 There exists a synchronous quorum system
that tolerates the f -threshold pattern and whose load is
q

Proposition 27 S0 , S1 and S2 are disjoint.

2f +1
n

then S0 = F1 . Also, if |F2 | ≥

α
3,

then

Proof. Follows immediately from the definition of S0 , S1
and S2 and the fact that F consists of disjoint sets.
t
u

52

Proposition 28 Si ≥

R.A. Bazzi
α
6

for i = 0, 1, 2.

References

Proof. There are three cases to consider depending on the
size of F1 and F2 .
– F1 ≥ α3 and F2 ≥ α3 . It follows that S0 = F1 , S1 = F2 ,
and S2 = P − F1 ∪ F2 . So, |S0 | ≥ α6 , |S1 | ≥ α6 , and
|S2 | = α ≥ α6 .
– F1 ≥ α3 and F2 < α3 . We first note that in this case,
S0 = F1 and |S0 | = |F1 |. We know that |S1 ∪ Fm2 +1 | ≥ α3
and |S1 | < α3 . Also, |S1 | ≥ |Fm2 +1 | because the Fi ’s are
sorted in decreasing size. It follows that 2|S1 | ≥ α3 and
|S1 | ≥ α6 . Finally, |S2 | = |P − (S0 ∪ S1 )| ≥ n − (|F1 | +
α
α
α
3) ≥ α− 3 ≥ 6.
α
– F1 < 3 and F2 < α3 . The proof is similar to the proof
of the second case and is omitted.
Note that we do not consider F1 < α3 and F2 ≥
|F1 | ≥ |F2 |. This completes the proof.

α
3

because
t
u

Now, we describe the quorum system. On Si , i ∈
{0, 1, 2}, define a quorum system with load O( √ 1 ). Many
|Si |

such systems exist. One such system is the triangle lattice
system p
[2]. In the triangle lattice system over Si , we can
choose 2|Si | quorums such that each processor belongs
to exactly two quorums. If we choose each quorum with a
probability √ 1 , it follows that the load of the triangle
2|Si |
q
2
=
lattice system on Si is at most √ 2
|Si | . Define a
2|Si |

quorum on P to be the union of three sets: one from the
triangle lattice system on S0 , another from the triangle lattice
system on S1 and the third from the triangle lattice system
on S2 .
Proposition 29 The resulting system tolerates F .
Proof. In fact, any two quorums intersect in three points
that belong to three distinct elements of F and, therefore,
the intersection of two quorums does not belong to the union
of two faulty sets.
t
u
Proposition 30 The load of the quorum system constructed
above is O( √1α ).

p
Proof. On each Si , we can choose 2|Si | quorums such
that each processor belongs
to exactly two quorums. It folp
lows that there are 8|S0 ||S1 ||S2 | quorums on P corresponding to all combinations
p of these quorums. Each processor in Si belongs to 2 4|S(i−1)mod3 ||S(i+1)mod3 | quorums.
It follows that the load on a processor in Si is
√
q
q
q
2 4|S(i−1)mod3 ||S(i+1)mod3 |
2
2
12
√
=
≤
=
|Si |
α/6
α =
q 8|S0 ||S1 ||S2 |
t
u
O( α1 ).
Corollary 31 The bound of Proposition 22 is tight.
Note that the system presented in this section is not an
asynchronous Byzantine quorum system.
Acknowledgments. I thank Dahlia Malkhi for many clarifications and helpful discussions. I thank the reviewers for their careful reading of the paper
and their useful comments.

1. D. Agrawal, A. El–Abbadi: An efficient and fault-tolerant solution for
distributed mutual exclusion. ACM Trans Comput Syst 9(1): 1–20
(1991)
2. R. A. Bazzi: Planar Quorums. In: Proceedings of the 10th International
Workshop on Distributed Algorithms, p 251–268, October 1996
3. R. A. Bazzi: Synchronous Byzantine quorum systems. In: Proceedings
of the 16th ACM Symposium on Principles of Distributed Computing
(PODC), p 259–266, August 1997
4. H. Garcia–Molina, D. Barbara: How to assign votes in a distributed
system. Journal of the ACM, 32(4): 481–860 (1985)
5. D. K. Gifford: Weighted Voting for Replicated Data. Proceeding of
7th ACM Symposium on Operating Systems Principles, p 150–162,
December 1979
6. A. A. Helal, A. A. Heddaya, B. B. Bhargava: Replication techniques
in distributed systems. Kluwer 1996
7. M. P. Herlihy: Replication Methods for Abstract Data Types. Ph.D.
Thesis, Massachusetts Institute of Technology, 1984
8. T. Ibaraki, T. Kameda: A theory of Coteries: Mutual Exclusion in
Distributed Systems. IEEE Trans Parallel Distrib Syst 4(7): 779–749
(1993)
9. A. Kumar. Hierarchical quorum consensus: A new algorithm for managing replicated data. IEEE Trans Comput 40(9): 996–1004 (1991)
10. A. Kumar, M. Rabinovich, R. Sinha: A performance study of general
grid structures for replicated data. In: Proceedings of International
Conference on Distrib Comput Systems, p 178–185, May, 1993
11. L. Lamport: On interprocess Communication. Distrib Comput 1: 76–
101 (1986)
√
12. M. Maekawa: A n algorithm for mutual exclusion in decentralized
systems. ACM Trans Comput Syst 3(2): 145–159 (1985)
13. D. Malkhi, M. Reiter: Byzantine Quorum Systems. Distrib Comput
11(4): 203–213 (1998)
14. D. Malkhi, M. Reiter, A. Wool: The load and availability of Byzantine
quorum systems. In: Proceedings of the Sixteenth ACM Symposium
on Principles of Distributed Computing, August, 1997
15. S. J. Mullender, P. M. B. Vitanyi: Distributed Match Making. Algorithmica 3: 367–391 (1992)
16. M. L. Neilsen: Quorum Structures in Distributed Systems. Ph.D. Thesis, Department of Computer and Information Sciences, Kansas State
University, 1992
17. M. L. Neilsen, M. Mizuno: Decentralized Consensus Protocols. In:
Proceedings of 10th International Phoenix Conference on Computing
and Communications, p 257–262 (1991)
18. M. Naor, A. Wool: The load, capacity and availability of quorum systems. SIAM J Comput 27(2): 423–447, (1998)
19. D. Peleg, A. Wool: How to be an Efficient Snoop, or the Probe Complexity of Quorum Systems. In: Proceedings of the 15th ACM Symposium on Principles of Distrib Comput, p 290–299, 1996
20. D. Peleg, A. Wool: The availability of quorum systems. Inf Comput
123(2): 210–223 (1995)
21. D. Peleg, A. Wool: Crumbling Walls: A class of high availability quorum systems. In: Proceedings of the 14th ACM Symposium on Principles of Distrib Comput, p 120–129, 1995
22. D. Skeen: A quorum–based commit protocol. In: Proceedings of the
6th Berkeley Workshop on Distributed Data Management and Computer Networks, p 69–80, 1982

Rida A. Bazzi received a B.E. in Computer and Communications Engineering from the American University of Beirut in 1989, and an M.Sc.
and a Ph.D. in Computer Science from Georgia Institute of Technology in
1994. He is currently an assistant professor in the Computer Science and
Engineering Department at Arizona State University. His major research
interests are distributed computing, fault tolerance, and security.

A CAPTCHA BASED ON THE HUMAN VISUAL SYSTEMS MASKING CHARACTERISTICS
Rony Ferzli*, Rida Bazzi**, and Lina J. Karam*
*Department of Electrical Engineering, Arizona State University, Tempe AZ 85287-5706
**Department of Computer Science & Engineering, Arizona State University, Tempe AZ 85287-8809
{rony.ferzli, bazzi, karam}@asu.edu
ABSTRACT

Originally, research on CAPTCHAs was motivated by many
incidents on the web. For example, in 1997, Altavista, the
most popular search engine at that time, was receiving
automated submission of a large number of Uniform
Resource Locators (URL) in order to bias the website
ranking. In 2000, Yahoo had a similar problem with
machines joining online chat rooms and posting ads. Since
then, many CAPTCHAs were suggested that can roughly be
categorized into two groups; the first one contained
CAPTCHAs that are legible but easy to break, while the
second one included unbreakable CPATCHAs that are not
always solvable by humans.
Knowing that noise can be masked by texture [3] and
showing in this work that edges can be masked by noise for
humans but can still be detected by computers, a resilientto-attacks human-solvable CAPTCHA is proposed by
exploiting the masking characteristics of the human visual
system.
This paper is organized as follows. Section 2 presents an
overview of available CAPTCHAs. The HVS masking
properties are discussed in Section 3. The proposed
perceptual-based CAPTCHA is presented in Section 4.
Finally, a conclusion is given in Section 5.

In this paper, a CAPTCHA is presented based on the
masking characteristics of the Human Visual System
(HVS). Knowing that noise can be masked by high activity
regions and showing that edges can be masked by noise for
a human observer while still being detected by machines,
the suggested CAPTCHA is composed of English alphabets
that are picked randomly and written with a combination of
texture and edges with added noise such as to deceive the
machine by randomly changing the visibility of characters
for humans. The proposed CAPTCHA is highly legible and
robust to brute-force attacks and sophisticated Object
Character Recognition (OCR) segmentation algorithms.
1. INTRODUCTION
A CAPTCHA, as defined by the CAPTCHA Project [1], is
“a test, any test, that can be automatically generated, which
most humans can pass, but that current computer programs
cannot pass”. CAPTCHA stands for “Completely
Automated Public Turing Test to Tell Computers and
Humans Apart” and is inspired from Turing work. Turing
[2] was the first researcher to investigate machine
intelligence aiming to provide a method to assess whether or
not a machine can think. In the original proposed Turing
Test, a human interrogator was allowed to ask a series of
questions to two players, one of which was a machine and
the other a human. Both players pretended to be the human
and the interrogator had to distinguish between the two
based on their answers. In the CAPTCHA case, the
interrogator is not a human but rather a machine.
A CAPTCHA can have various applications; it can be used
to prevent “bots” from automatic sign up for free email
service (i.e: Yahoo [www.yahoo.com]) or other services
such as Paypal [www.paypal.com]. A CAPTCHA can also
offer a solution to block worms and spam. It may be needed
to block search engine robots from indexing a private
website. It can also be used to prevent password dictionary
attacks by denying the computer to scan through the entire
word dictionary. Many other applications exist including
chat rooms and popular webspace servers such as rapidshare
[www.rapidshare.de].

1­4244­0367­7/06/$20.00 ©2006 IEEE

2. CAPTCHA: AN OVERVIEW
This section presents an overview of published and patented
CAPTCHAs that are based on text, image, or a combination
of the two. Good CAPTCHAS should be generated such
that they satisfy the following desirable properties [1]:
x
The test must be generated automatically (i.e: the
interrogator is a machine).
x
The answer to the test should be quick and easy.
x
The test should accept all human users.
x
The test should reject all machine users.
x
The test should resist attacks even if the algorithm is
known.
Some of the existing popular CAPTCHAs include the
following:
- Altavista CAPTCHA [4]: characters are generated
randomly where the appearance is also randomized. For
example, each character is rendered using selected fonts,
different spacing between the characters, or different

517

ICME 2006

3. HVS MASKING PROPERTIES

stretching. The whole string can also follow a random path.
Finally, a noisy or maze type background can be added as
shown in Fig. 1(a). Though the number of machine attacks
were reduced initially by 95%, algorithms were quickly
developed to break this CAPTCHA due to isolated
characters than can be segmented easily.
- GIMPY [5]: this CAPTCHA is the fruit of collaboration
between Yahoo and Carnegie Mellon University (CMU)
where the term “CAPTCHA” was first used. The developed
algorithm picks English words at random and transforms
them into an image after severe deformation and image
occlusion with some overlapping. The user is challenged to
read some number of words correctly (not necessarily all
displayed words). An example is shown in Fig 1(b). It was
found that users experienced difficulties with this type of
CAPTCHA due to its complexity [6]; so, the algorithm was
quickly replaced by EZ-Gimpy which uses only one English
word, which was better tolerated by users (Fig. 1(c)).
Yahoo used this CAPTCHA until it was broken by Mori et
al. [7] in 2003. Fig 1(d) shows the current Yahoo
CAPTCHA which is still unbreakable but could be
sometimes hard to read as shown in Fig 1(d). No
information is provided concerning the new CAPTCHA.
- PessimalPrint [8]: its lexicon contains only 70 common
English word which are between 5 to 8 characters (Fig.
1(e)). The algorithm uses the Braid [9] degradation model
simulating physical defects caused by copying and scanning
of printed text. An example of degradation includes salt and
pepper noise, condensed fonts and skewed characters. The
algorithm is usually easy to beat since it uses a very small
dictionary and is vulnerable to brute force attacks.
-BaffleText [6]: is a reading-based CAPTCHA that uses
random masking to degrade images of non-English pronounceable character strings. The parameters controlling the
mask include shape, radius, density of black pixels in the
mask; the generated mask can be either added or subtracted.
The generated CAPCHTA could be hard to read as shown
in Fig 1(f) and could be broken using advanced Object
Character Recognition (OCR) segmentation techniques
along with morphological algorithms.
-ScatterType [10]: randomly synthesized images of text
strings rendered in machine-print typefaces. Within the
image, characters are fragmented using horizontal and
vertical cuts, and the fragments are scattered by vertical and
horizontal displacements. In contrast with the other
described CATCHAs, no physics-based image degradations,
occlusions, or extraneous patterns are performed. The
algorithm seems hard to beat but the problem is that human
legibility is only around 53%.
Other CAPTCHAs such as the ones based on images [1,
11] exist but are less popular since they require more
complex answer from user; for example, four images are
provided to the user and should guess the common object
across the images. It is critical that human users not to find
CAPTCHA excessively difficult or irritating.

In this work, we are interested in exploiting the masking
properties of the HVS. Masking is generally defined as any
interference between two or more visual signals or stimuli
that results in an increase or decrease of their visibility [3].
We are mainly interested in noise masking and edge
masking properties. The first characteristic of the HVS is
noise masking where regions of non-regular and highly
changing luminance in an image (i.e: texture), are able to
mask other signals (i.e: noise).
This phenomenon was measured and modeled in numerous
experiments using sinusoidal patterns and noise stimuli. A
set of psychophysical experiments were conducted in [3] on

(a) AltaVista

(b) GIMPY

(c) EZ-GIMPY

(d) New Yahoo CAPTCHA

(e) PessimalPrint

(f) BaffleText

(g) ScatterType
Figure 1. Different described CAPTCHA

518

2.

Images similar to Fig.3 (a) were displayed. The subject
needs to examine the image and identify the edge if
possible. The first exposed image has the highest
variance of 0.4 and is decreased by 0.04 each time a
new image is displayed. Once the subject identifies the
edge, the variance value is recorded.
Six subjects, with normal to corrected-normal vision
participated in the experiment. The collected edge masking
information as well as the HVS-based texture masking
effect will be used for the construction of the proposed
CAPTCHA.
4.
Figure 2. Noise perception when added to regions with
different activities. The Gaussian noise has the same
variance in both regions.

PROPOSED PERCEPTUAL-BASED CAPTCHA

In our proposed CAPTCHA generating scheme, visual
perceptual-based CAPTCHAs are formed by exploiting the
noise and texture masking properties of the HVS. The visual
CAPTCHA is formed by adding noise and texture
throughout the image in different amounts in order to
control the masking so that letters (or patterns) can be made
visible or invisible depending on the amount of masking.
So, in some places, the noise is masked by texture while, in
other places, the noise is itself masking edges. The machine
can see the unseen and can detect both masked and
unmasked edges and noise. This makes it harder for the
machine to estimate the amount of injected noise and to
predict what the human observer can see or not see.
An example of the proposed CAPTCHA is shown in Fig. 4.
Figs. 4(a) and (b) show the initial CAPTHA before adding
the noise and the Canny edge detection output respectively.
Fig. 4(c) shows the perceptual CAPTCHA that will be
provided to users. The characteristics of this CPATCHA can
be summarized as follows:
x
The character ‘I’ is written using smooth edges and can
be easily detected by a machine as shown in Fig. 4(b).
Nevertheless, with the addition of noise throughout the
images in different amounts it is hard for the machine to tell
whether high activity regions are due to noise or texture and
whether the letter is seen or not seen by the user (i.e: ‘I’ ot
‘T’).
x
The character ‘K’ is written using a texture different
from that of the background texture. Again, it is hard for the
machine to tell which character is made out of texture.
x
The characters ‘T’ and ‘E’ have a texture close to the
background one and have a handwritten style.
x
It is clear from Fig. 4(b) that the edge detector will fail
for three out of the 4 characters. Being able to detect the
character ‘I’ is desirable since, when adding noise, and due
to the noise masking properties, the letter will be invisible to
the user but detectable to the machine as discussed in
Section 3; however, the machine will not be able to decide
whether the character is seen since the high variance may be
due to the texture or noise as shown in Fig. 4(c). In addition,
to make it harder, the added noise can be structured (i.e., by
using noise to write a character ‘O’ on top of ‘I’).

the visibility of different types of noise in natural images.
The obtained results reveal that higher level of noise is
detected on a plain background while, on the other hand, a
clear masking effect was observed with high activity
images. In particular, noise thresholds increased
significantly with image activity. Winkler & Susstrunk [3]
showed that observers seem to use only a small part of the
image for making their decision. Fig. 2 shows an example
where an image is split into textured and smooth regions.
The same amount of gaussian noise is injected to the two
regions; the noise is more visible in the region with constant
gray scale level.
A second HVS masking property that we investigated as
part of this work is edge masking. An experiment is
conducted, as part of this work, showing that, when adding
a certain amount of noise, the edges will not be continuous
and will be hidden by noise to the human observer, but can
still be “seen” by the machine.
Fig. 3 shows an example of a noisy edge image where a
Gaussian noise with a variance equal to 0.235, was added to
an image with a diagonal edge. Surprisingly enough,
humans are not able to see the edge due to the added
Gaussian noise. Can the machine vision defeat the human
vision? Performing the Hough transform and displaying the
longest detected line, the machine was able to spot the edge
and its proper orientation. This idea can be investigated in a
reverse Turing test where the interrogator will identify the
machine and not the human. Note that similar results were
obtained for uniform and salt & pepper noise.
As part of this work, subjective testing needs to be
performed to calculate the noise variance required to mask
edges at different orientations and contrast ratios. An initial
subjective testing was conducted and can be summarized as
follows:
1. Subjects were given a set of instructions before starting
such as how to conduct the experiments, and what is
the objective of the experiment.

519

(a) Original image with diagonal edge

(b) Human vision , edge image with
(c) Machine vision, line detection
gaussian noise (variance = 0.235)
using Hough transform
Figure 3. Comparison of human and machine vision when Gaussian noise is added to an edge image.
Test to Tell Computers and Humans Apart,”
www.captcha.net, Dept. of Computer Science, CarnegieMellon Univ., November, 2000.
[2] A. Turing, “Computing Machinery and Intelligence,”
Mind, Vol. 59(236), pp. 433–460, 1950.
[3] S. Winkler, and S. Susstrunk, “Visibility of Noise in
Natural Images”, Proc. IS&T/SPIE Electronic Imaging
2004: Human Vision and Electronic Imaging IX, vol. 529,
p. 121-129, 2004.
[4] M. D. Lillibridge, M. Abadi, K. Bharat, and A. Z.
Broder, “Method for Selectively Restricting Access to
Computer Systems,” U.S. Patent No. 6,195,698, Feb. 2001.
[5] L. V. Ahn, M. Blum, and J. Langford, “Telling Humans
and Computers Apart Automatically,” Commun. ACM,
47(2):pp. 56-60, Feb 2004.
[6] M. Chew and H. S. Baird, “BaffleText: a Human
Interactive Proof,” Proc., 10th SPIE/IS&T Document
Recognition and Retrieval Conf. , January 23–24, 2003.
[7] M. Greg and J. Malik, “Recognizing Objects in
Adversarial Clutter: Breaking a Visual CAPTCHA,” IEEE
Computer Vision and Pattern Recognition, vol. 1, pp. 134331, 2003.
[8] A. L. Coates, H. S. Baird, and R. Fateman, “Pessimal
Print: a Reverse Turing Test,” Proc., IAPR 6th Intl. Conf. on
Document Analysis and Recognition, Seattle, WA,
September 10-13, pp. 1154-1158, 2001.
[9] H. S. Baird, “Document Image Defect Models," in
Structured Document Image Analysis, pp. 546-556,
Springer Verlag: New York, 1992.
[10] H. S. Baird and T. P. Riopka, “ScatterType: a Reading
CAPTCHA
Resistant
to
Segmentation
Attack”, Proc. SPIE, vol. 5676, no. 1, pp 197-201, Jan.
2005.
[11] M. Chew and J. D. Tygar., “Image Recognition
CAPTCHAs”, 7th Info. Security Conference, vol. 3225 of
Lecture Notes in Coomp. Sc., Springer-Verlag, Oct. 2004.
[12] Online CAPTCHA test; www.pwntcha.net/test.html.

(a) Initial CAPTCHA

(b) Edge detection of the initial CAPTCHA

(c) Final CAPTCHA with added noise
Figure 4. Proposed CAPTCHA
Though this is not sufficient by itself in proving the
robustness of the proposed CAPTCHA to future attack
algorithms, we can be assured that it will not be deciphered
by current implemented segmentation or pattern
recognition techniques and by current CAPTCHA detection
techniques [12].
5. CONCLUSION
Taking advantage of the masking properties of the HVS, we
propose a CAPCHTA based on characters that are formed
using a mixture of textures and edges with added noise.
Since the HVS reacts differently to the presence of noise in
texture or edges, it will be hard to the machine to predict the
perceived characters.
6. REFERENCES
[1] M. Blum, L. A. von Ahn, and J. Langford,, The
CAPTCHA Project, “Completely Automatic Public Turing

520

Brief Announcement:Wait-Free Implementation of
Multiple-Writers/Multiple-Readers Atomic Byzantine Data
Storage Systems
Rida A. Bazzi

Yin Ding

Computer Science Dept.
Arizona State University
Tempe, AZ, 85287

Computer Science Dept.
Arizona State University
Tempe, AZ, 85287

bazzi@asu.edu

yding@asu.edu

Categories and Subject Descriptors: C.2.4 [Distributed
Systems]: Distributed Applications D.4.2 [Storage Management]: Distributed memories

the number of concurrent readers. To achieve our results, we
combine non-skipping timestamps with careful signaling between the readers and writers. We briefly describe the protocol for the single writer case. Writer The writer starts
by sending the new value and a timestamp (incremented
with each write). The writer collects from each server, the
set of readers that sent read requests to the servers. Then,
the writer determines the set of readers that are active (more
than f servers received read requests from them). The writer
then sends the set of active readers to the servers so that the
servers can forward any data they have to the active readers. The idea is to have a writer flush the data stored at
the servers to any readers that are currently active. This
helps in storing only the two most recent copies of the data
at the servers. Readers A reader sends read requests to all
servers and wait for n − f replies. Then, it chooses a target
timestamp ts that is equal to the f + 1’st largest timestamp
of values received. If there is a value returned by f + 1
servers and whose timestamp is greater than ts, the reader
reads such a value with the largest timestamp. Otherwise,
the reader reads a value whose timestamp is equal to ts − .
When a reader reads a value, it writeback the value so that
later readers do not read an older value. The reader uses all
values it receives (two most up to date and writeback values)
to determine the target timestamp and to decide what value
to read. Servers When the server receives a new value by
a writer, it updates the two most up to date values if the
timestamp of the received value is larger than that of one
them. When a server receives a writeback value, it keeps
that value separately from the two most up to date values
and updates that separate writeback value if the received
value has a larger timestamp than the stored one. By keeping the writeback values separate from the values written
directly by the writer, we guarantee that the reader is able
to read a value in case there are many writeback that occur
concurrently and at the same ensure atomic semantics.

General Terms: Algorithms, Reliability, Security
Keywords: Atomic, Byzantine, Fault Tolerance, Replication, Timestamps, Wait-Free

1.

RESULTS

We study the problem of implementing a replicated data
store in an asynchronous system in which f out of n servers
can be faulty. We consider a system with an unbounded
number of clients in which servers are subject to Byzantine
failures and the data stored at servers is not self-verifying.
We present the first atomic register construction that is
wait-free (tolerates any number of client crashes) and f resilient (tolerates up to f server failures). Our solution
improves greatly on the solution of Martin et al. [2] and our
previous solution [1]: (1) For a read operation, two messages are sent by the reader to each server, and max 2, 1 + c
messages are sent by each correct server, where c is the number of distinct writers that execute write operations concurrently with the read operation. In contrast, in the previous
implementations [1, 2] the number of messages sent by a
correct server is equal to 1 plus the number of write operations concurrent with a read operation; (2) The solution
is wait-free without requiring servers to communicate with
each other. Previous solutions required Ω(n2 ) messages to
be exchanged by servers per write request in order to tolerate client crashes.
Our solution requires n ≥ 4f + 1 and each server keeps
only the two most recent copies of the data and a copy of
data written back by the readers, which is almost optimal.
Our solution, as well as previous solutions, requires each
server to maintain a list of active readers and therefore, it
technically requires unbounded space per server, but practically that space will be small because only reader identifiers
are maintained and these are small compared to data values
that can be quite large. Also, in a practical setting, if readers connect to the servers before communicating with them,
the space used by the connections is already proportional to

2.

REFERENCES

[1] Rida Bazzi and Yin Ding, “Non-Skipping Timestamps
for Byzantine Data Storage Systems,” Distributed
Computing, 18th international Conference (DISC04),
(October 2004).
[2] J-P. Martin and L. Alvisi and M. Dahlin, “Minimal
Byzantine Storage,” Distributed Computing, 16th
international Conference, DISC 2002, : page 311-325.

Copyright is held by the author/owner.
PODC’05, July 17–20, 2005, Las Vegas, Nevada, USA.
ACM 1-58113-994-2/05/0007.

353

Algorithmica (1997) 17:308-321

Algorithmica
~) 1997Springer-Verlag New York Inc.

The Complexity of Almost-Optimal Simultaneous
Coordination I
R. A. Bazzi 2 and G. N e i g e r ~
Abstract. The problem of fault-tolerant coordination is fundamental in distributed computing. In the past,
researchers have considered achieving simultaneous coordination under various failure assumptions. It has
been shown that doing so optimally in synchronous systems with send/receive omission failures requires
NP-hard local computation. This paper studies almost-optimal simultaneous coordination, which requires
processors to coordinate within a constant additive or multiplicative number of rounds of the coordination time
of an optimal protocol. It shows that achieving such coordination also requires NP-hard computation.
Key Words. Simultaneous coordination, Fault tolerance, Processor knowledge, Common knowledge, NPcompleteness.

1. I n t r o d u c t i o n . Coordinating the activity of the processors in a distributed system is
a fundamental problem in distributed computing. Several aspects of these problems are
captured by the definition of a simultaneous choice problem given by Moses and Tuttle
[17]. A simultaneous choice problem is one in which all of the nonfaulty processors
are required to choose the same action (such as deciding on an output bit) from a set
of actions and to perform that action simultaneously. Instances of simultaneous choice
problems include simultaneous versions of m a n y well-known problems, such as Reliable
Broadcast [ 191, Byzantine Agreement I14], [22], and Distributed Firing Squad [4], I71.
For example, in Byzantine Agreement, each processor starts with an input bit and chooses
an output bit. All correct processors must choose the same output bit and this bit must
be some processor's input bit. In Simultaneous Byzantine Agreement [8], I9l, all correct
processors must choose their output bit in the same round.
This paper considers the complexity of solutions to simultaneous choice problems
in the presence of general omission failures, by which faulty processors may omit to
send or receive messages. It considers synchronous systems in which algorithms operate
in rounds of c o m m u n i c a t i o n (simultaneous choices cannot be made in asynchronous
systems). The results in this paper are based on the relationship between coordination and
different forms of processor knowledge [12]. It is well established that knowledge can be
Used to characterize and improve solutions to various problems in distributed c o m p u t i n g
[31, [ 1 1], [ 13], [ 15], [ 161, [ 18]. For example, Dwork and Moses I9] and Moses and Tuttle

I This work was supported in pan by the National Science Foundation under Grants CCR-9106627 and
CCR-9301454. R. A. Bazzi was supported in part by a scholarship from the Hariri Foundation.
2 Computer Science and Engineering Department. College of Engineering and Applied Sciences. Arizona
State University. Box 875406, Tempe, AZ 85287-5406, USA.
3 Intel Corporation. JF3-359.2111 N.E. 25th Avenue, Hillsboro, OR 97124-5964, USA.
Received February 4, 1994; revised December 5, 1994. and September 26, 1995. Communicated by G. N.
Frederickson.

The Complexityof Almost-OptimalSimultaneousCoordination

309

[ 17] showed that common knowledge is necessary for the solution of simultaneous choice
problems and used this fact to derive optimal solutions to such problems. A solution is
optimal if, in any context, it has processors choose an action at least as early as any other
solution. In addition, Moses and Tuttle proved that optimal solutions tolerant of general
omission failures require processors to perform NP-hard local computation between
rounds of communication. Neiger and Tuttle [21] subsequently showed related results
for stronger types of simultaneous choice and of common knowledge.
It is known that many NP-complete problems can be solved approximately in polynomial time [10]. It seems possible that, in systems with general omission failures,
almost-optimal solutions to simultaneous choices might require only polynomial-time
local computation. A simultaneous choice algorithm is almost-optimal if processors make
their choices within a constant additive or multiplicative number of rounds of the decision
time of an optimal algorithm. The possibility of polynomial-time almost-optimal algorithms in the multiplicative case is suggested by studies of translations between models of
failures [2], [19]. For example, Dwork and Moses showed the existence of optimal algorithms that tolerate crash failures. Neiger and Toueg [ 191 showed how algorithms tolerant
of crash failures could be converted to tolerate general omission failures by doubIing
the number of rounds used; local computation time was increased only by a polynomial amount. If this translation were applied to the optimal (crash-tolerant) algorithm of
Dwork and Moses, the result would be a polynomial-time (general-omission-tolerant) algorithm that might be almost-optimal within a multiplicative constant of 2 (i.e., decision
time would be at worst twice optimal).
This paper shows that that method cannot produce an almost-optimal solution (if P
NP). In particular, it shows that any almost-optimal simultaneous choice algorithm
that tolerates general omission failures must still require processors to perform NP-bard
local computation between rounds of communication in some executions.
The paper is organized as follows. Section 2 defines the model of the system. Section 3 gives a formal definition of simultaneous choice problems and of almost-optimal
solutions. Section 4 introduces the necessary knowledge-theoretic background needed
for the development of the results. Section 5 presents the main results and Section 6
concludes the paper with a brief discussion.

2. Definitions. This section defines a model of a synchronous distributed system. This
model is similar to others used to study knowledge and coordination [9], [12], [13], [17].
In particular, we closely follow our own earlier work [18] and that of Neiger and Tuttle
[211.
A distributed system is a finite set P of n processors and a communication network
that connects them. Computation proceeds in a sequence of rounds, with round e taking
place between time e - 1 and time e. At time 0, each processor starts in some initial
state. 4 In each round, a processor performs local computation (and, optionally, makes
a choice), sends messages to other processors, and receives messages sent to it in that
round (messages sent in a round are received in that round or not at all).

4 A processor'sinitial state is meantto model any input that a processormay receivefromoutside the system.
It is also possibleto model such inputsthat may be receivedafter time 0.

310

R.A. Bazzi and G. Neiger

A processor's local state at any given time comprises its initial state, the current time,
the messages it has sent and received, and the processor's identifier. A global state is
a tuple of local states, one per processor. A run of the system is an infinite sequence
of global states, together with a communication protocol and an operating environment
(see below). An ordered pair (p, ~), where p is a run and ~ is a natural number, is called
a point and represents the state of the system after the first s rounds of p. The local state
of processor p at point (p, e) is denoted by pp(s
Processors follow a communication protocol 79, which is a function of a processor's
local state that specifies the messages a processor is to send in a given round. Recall that
a communication protocol is part of a run. The global states of any run are consistent
with the run's communication protocol (e.g., the run does not reflect the sending of a
message that the protocol would not require).
The behavior of the "system" (a protocol's environment) during an execution is important concept for this paper. A communication protocol may produce different runs
depending on this behavior. Central to the results of this paper is the fact that the performances of different protocols with the same system behavior can be compared. The
behavior of the system in a run is the run's operating environment. The operating environment includes all information (besides the protocol) necessary to reconstruct the
execution represented by ~ n . As suggested above, it specifies the initial states of the
processors and how they fail.
Correct processors properly follow their communication protocol. Other processors
are faulty. Because this paper considers only general omission failures, in which processors fail by omitting to send or receive messages, we can easily describe processors'
faulty behavior. For any round s each processor p has two associated sets of processors
S(p, s and R(p, e) that are, respectively, the sets of processors p failed to send to or to
receive from in round e. A processor is faulty if and only if S(p, e) to R (p, s is nonempty
for some g. C(p) denotes the set of processors correct in run p (we use C when p is
clear from context).
Formally, an operating environment is a triple (I, S, R), such that I is a vector of
initial states ( l [ p ] is the initial state of processor p), and S and R are the functions
that describe how processors fail. Because this paper studies algorithms with a specified
fault-tolerance, we assume that, in every execution, at most t processors are faulty. Thus,
we consider all possible operating environments (e.g., the faulty behavior of processors
is independent of their initial states) in which at most t processors fail.
Two runs of two different communication protocols correspond (or are corresponding
runs) if they have the same operating environment. Different protocols are compared by
comparing their behavior in corresponding runs.
In order to analyze systems, it is convenient to have a logical language to make
statements about the system. A fact in this language is interpreted to be a property of
points: a fact tp will be either true of false at a given point (p, s denoted (p, g) ~ tp
and (p, s ~ ~0, respectively. Fact ~o is valid in a system if it is true at all points in the
system; it is valid if it is valid in all systems. Although facts are interpreted as properties
of points, it is often convenient to refer to facts that are about objects other than points
(e.g., properties of runs). A fact ~o is a fact about X if fixing X determines the truth (or
falsity) of ~o.

The Complexity of Almost-Optimal Simultaneous Coordination

311

3. Simultaneous Choice Problems. This section defines simultaneous choice problems. The definition is essentially that used by Moses and Tuttle [17], by Neiger and
Tuttle [21 ], and by the authors in a previous paper [ 18].
A simultaneous action a is an action with two associated conditions pro(a) and
con(a), both facts about the initial states and the existence of failures, giving conditions
under which a should and should not be chosen. A simultaneous choice problem C is
determined by a set of simultaneous actions and their associated pro and con conditions.
For a protocol to coordinate a choice of actions, there must be a mechanism by which
it can specify when an action is chosen. An action protocol 79(~) is a communication
protocol T~ augmented by an action function ~. For each a ~ C and p ~ 79, ~ , p is a
fact about p's local state (see Section 2 above). P ( ~ ) has p choose action a the first
time ~a.p becomes true. All action protocols are implicitly parametrized by n and t.
For example, action function t~a, p may return different values for the same local state
in systems with different values of t. In any particular system, of course, n and t have
fixed values, and the communication protocol and action function are thus well defined
for all local states.
An action protocol 79(~) is a decision protocol if a processor's choice is always
irrevocable and unique. A decision protocol 79(~) implements choice problem C in a
system if every run p of 79 in the system satisfies the following conditions:
9
9
9
9

9 has each correct processor choose at most one oq the actions,
any action chosen by some correct processor is chosen simultaneously by all of them,
action a is chosen by all correct processors if p satisfies pro(a), and
action a is not chosen by any correct processor if p satisfies con(a).

It is easy to see that this specification allows the correct processors to choose an action
a in run p only if p satisfies the condition

ok(a) = --,con(a)/x A -,pro(a').
a'v~a

The fact ok(a) is called the enabling condition of action a.
This paper studies simultaneous choice problems whose enabling conditions are nontrivial facts about the input and the existence of failures. Such a fact is nontrivial if
neither it nor its negation is true of all runs. Many natural simultaneous choice problems
have enabling conditions that are nontriviai facts about the input and the existence of
failures.
For example, consider Simultaneous Distributed Consensus. In this problem there are
two actions, a0 and an, which correspond to a processor "deciding" 0 and "deciding" 1,
respectively. Processors must decide 0 if all begin with 0 (i.e., their initial states are for
value 0) and similarly for 1. This problem can be defined by choosing

pro(ao)
con(ao)
pro(a i)
con(a1)

= "all processors' initial states are for 0,"
= false (ao is never explicitly forbidden),
= "all processors' initial states are for I," and

= false

312

R.A. Bazzi and G. Neiger

If a processor's initial state is always either for 0 or for 1, then, for this problem, we
obtain the following enabling conditions:

ok(ao) = "some processor's initial state is for 0" and
ok(al) ---- "some processor's initial state is for 1."
The definition of a decision protocol implementing a choice problem given above is
for a specific system. A decision protocol may implement a choice problem for some, but
not all, choices for n and t. In general, however, a protocol that implements a problem
for some n and t will continue to do so if n is increased and t is fixed. For this reason,
we make the following definition. Decision protocol p(qb) implements choice problem
C with function g if 7:'(4)) implements C in every system with n > g(t). (If a solution is
correct regardless of the relation between n and t, then it implements C with function g,
where g(t) = t.)
In a given round g, the time complexity of the local computation of a processor
running 79(d~) in computing 7:" and 9 is measured as a function ofg and the total number
of processors n; Moses and Tuttle provide a justification of this choice of parameters.
A protocol may have processors exchange messages containing their initial states; for
this reason, we assume that these states can be expressed in messages whose size is
polynomial in n. If processors perform only polynomial-time local computation, we say
that T~(~) runs in polynomial time.
Recall that two runs of two protocols correspond if they have the same operating
environment. A protocol 79(qb) implementing simultaneous choice problem C is optimal
for C if, for every protocol p'(qb') that implements C and every pair of corresponding
runs p and p' of 79(qb) and 79'(~'), respectively, if the correct processors choose actions
at time s in p', then they choose actions no later than time s in p (the actions chosen in p
and p' need not be identical). 79(qb) is almost-optimal for C with additive constant k if,
for every protocol 79'(~') implementing C and every pair of corresponding runs p and
p' ofT' and P ' , respectively, if the correct processors choose actions at time ~ in p', then
they do so no later than time s § k in p. T'(qb) is almost-optimal for C with multiplicative
constant k if, for every protocol P'(q~') implementing C and every pair of corresponding
runs p and p' of 7:' and P ' , respectively, if the correct processors choose actions at time
g in p', then they do so no later than time kg in p. When it is clear from context, we omit
mention of the problem C and the words "additive" and "multiplicative"
A decision protocol is afull-information protocol if its communication protocol .T has
processors exchange all the information they have about the run in every round; that is,
in every round every processor sends its state to every other processor. Coan [5] proved
a result that implies that if protocol P ( ~ ) implements simultaneous choice problem C,
then there is a full-information protocol .T(~') that implements C such that, in all pairs
of corresponding runs, .T'(~') has processors choose the same actions as 7:'(0) at the
same times. Moses and Tuttle showed that if P ( ~ ) runs in polynomial time, then so
does ~ ' ( ~ ' ) . This is because, for systems with general omission failures, the messages
of .T(q~') can be encoded in polynomial-size communication graphs. The definition of
these graphs is beyond the scope of this paper.
The sequel uses polynomial-time Turing reductions to show that almost-optimal fullinformation solutions to simultaneous choice problems require NP-hard local computa-

The Complexity of Almost-OptimalSimultaneous Coordination

313

tion. The results discussed above imply that the same is true for solutions that are not
full-information. The sequel thus restricts its attention to full-information protocols.

4. Knowledge and Simultaneous Choices. Processor knowledge was first defined by
Halpern and Moses [ 12] and has been used extensively since then. This section presents
the standard definitions of knowledge that are relevant to this paper.
Processors may "know" certain facts at different points in the runs of a communication
protocol. For the remainder of this section, assume that there is a fixed protocol and
consider only runs of that protocol. Processor p knows fact ~o at point (p, e), denoted
(p, e) ~ Kp~p, if (p', e') ~ ~0 for all points (p', e') such that p'p(e') = pp(e). Since
the current time is part of every processor's local state, it follows that we can define
(p, e) ~ Kp~p if (p', e) ~ ~0 for all runs p' such that p;,(e) = pp(e).
For this paper, it is useful to condition a processor's knowledge on its being correct.
Processor p believes ~o (denoted Bp~p) if p knows that, if it is in C, ~p is true. That is
Bp~o =_ Kp(p ~ C ~ ~p). It is easy to see that (p, e) ~ Bp~p if (p', e) ~ ~p for all runs
p' such that p p ( e ) = pp(e) and p ~ C(p').
Because this paper deals with choices made by the correct processors, the knowledge
possessed by the set C of correct processors is of special interest. Everyone in C knows
~o, denoted I=~o, is defined as A p s e Bp~o. (Moses and Tuttle [17] and Neiger and Tuttle
[21 ] explain why belief, and not knowledge, is appropriate for the problems considered
here.) Fact ~0 is common knowledge, denoted G~, if Ai>l I:=i~0"5
A useful tool for reasoning about knowledge is the similari~ graph [ 17]. The nodes
of this graph are the points of the system, and there is an edge between points (p, e) and
(p', e) if and only if there exists a processor that is correct at both points and has the
same local state at both points. It is not hard to see that if ~p is true at all points adjacent
to (p, e), then (p, e) ~ E~p. The similari~, relation ~ on the similarity graph is defined
to be the transitive closure of the adjacency relation. Two points (p, e) and (p', e) are
similar if (p, e) ~ (p', e). It is not hard to see that a fact is common knowledge at a
point (p, e) if and only if it holds at all points similar to (p, e) [ 17].
Halpern and Moses made an extensive exploration of the logical properties of common
knowledge. One such property is that it obeys an induction rule that is useful for showing
facts to be common knowledge. The induction rule says that if ~0 ==~ E~ is valid in a
system, then so is ~0 ==~ G~. Section 5 below uses this rule to show that certain facts are
common knowledge.
Common knowledge is important for reasoning about simultaneous choice problems because when processors choose an action simultaneously, they must have common knowledge of the enabling condition of the action being chosen [17, Lemma 4]
(Dwork and Moses [9] had previously shown this for Simultaneous Byzantine Agreement; Neiger and Tuttle [21 ] later showed it for consistent simultaneous choice problems
and a stronger form of common knowledge.) Furthermore, processors running an optimal protocol choose a simultaneous action as soon as some enabling condition becomes
5 Halpern and Moses defined common knowledge using a logical fixed-point operator. For the systems we
consider, the two definitions are equivalent and the logical machinerynecessaryto present theirs is beyondthe
scope of this paper.

314

R.A. Bazzi and G. Neiger

common knowledge [17, Theorem 7]. Because of this, an almost-optimal protocol with
additive constant k requires correct processors to take some action at time e when some
enabling condition has been common knowledge since time s - k. Similarly, an almostoptimal protocol with multiplicative constant k requires correct processors to take some
action at time s when some enabling condition has been common knowledge since time

Le/kJ.
5. Complexity Results. This section studies the complexity of almost-optimal solutions to simultaneous choice problems in synchronous systems with general omission
failures. For such systems, Moses and Turtle [17] proved that processors running an optimal protocol may be required to perform NP-hard local computation (in computing the
protocol's action function) between rounds of communication. We show that processors
running an almost-optimal protocol would still be required to perform this NP-hard local
computation. The proof is similar to that of Moses and Tuttle.
The proof of NP-hardness is a Turing reduction from the Vertex Cover problem. This
is the problem of determining whether, given undirected graph G = (V, E) and integer
M, there is a set C _c V of vertices (the vertex cover) of size less than M such that any
edge of G is incident to some vertex in C. Vertex Cover is known to be NP-hard [10].
The high-level intuition behind the reduction is the following. Consider a graph in which
each vertex corresponds to a processor. Let there be an edge between any two processors
between whom a message is omitted during an execution. There must be some vertex
cover of this graph of size at most t: the faulty processors must form a "vertex cover" of
this graph in that some faulty processor must account for each edge (i.e., for each omitted
message). (The actual reduction, given below, is correctly from, and not to, vertex Cover;
the above is only intuition and not a proof sketch.)
Assume that there is a boolean function SmallerCover that takes as input a graph G
and integer c such that, if G has vertex cover of size c, SmallerCover(G, c) is true if and
only if G also has a vertex cover of size less than c. The following then is an algorithm
for Vertex Cover. (This algorithm is due to Moses and Tuttle.) It returns yes if and only
if G has a vertex cover of size less than M; it returns no otherwise.

c--Ivl
while SmallerCover( G, c)
C--C--]

if

then
return(yes)
else
return(no)
c < M

Since the loop iterates at most ]VI steps, it is clear that if SmallerCover can be implemented in polynomial time, then Vertex Cover can besolved in polynomial time.
The complexity results of this section focus on a particular simultaneous choice
problem. This is C = {a} such thatpro(a) =false and con(a) = " p ' s initial state is not
for 0." (Note that ok(a) is " p ' s initial state is for 0.") We choose this simple problem in
order to highlight our proof techniques. Other papers [ l ], [ 17], [ 18] have demonstrated
methods by which results for such simple problems, which use a single fact about one

The Complexity of Almost-OptimalSimultaneousCoordination

315

processor's initial state, can be extended to those that use general facts about initial states
and the existence of failures. These methods are relatively straightforward and, because
they are not a contribution of this paper, they are not presented here.
Section 5.1 shows how instances of Vertex Cover can be related to executions of an
implementation of C. Section 5.2 shows that if there is an almost-optimal protocol for C
within an additive constant whose action function can be computed in polynomial time,
then there is a polynomial-time implementation of SmallerCover, and this would then
give a polynomial-time algorithm for Vertex Cover. This suffices to prove that almostoptimal coordination is NP-hard. Section 5.3 does the same for protocols that are almost
optimal within a multiplicative constant.

5.1. Relating Graphs to Executions. Consider implementations of C that are correct
with function g (i.e., in any system in which n > g(t)). The following shows how to
relate instances for Vertex Cover to executions of such protocols.
Recall that SmallerCover takes, as inputs, a graph G = (V, E) and an integer c such
that G has a vertex cover of size c (if G has no such cover, then SmallerCover may
return either true or false). Let v = I V 1; note that c < v. Smaller Cover must determine
whether or not G has a vertex cover of size less than c. Assume that c > 2; for c < 2,
SmallerCover(G, c) can easily be computed in polynomial time. Notice that c > 2
implies v > 2. Assume also that E -7r 0; if E = 0, the G has an empty vertex cover, a
fact that can be computed in polynomial time.
Consider now a synchronous system of n = max{v + c + 2, g(c + 2)} processors,
t = c + 2 of which may be subject to general omission failures. (Note that n > g(t), as
required.) Identify three processors p, q, and r. A set Pc containing v other processors
corresponds to the vertices of G. The remaining m a x { c - 1, g (c + 2) - (v + 3)} processors
are in a set called A.
Consider an execution of full-information communication protocol ~-.

9 Processor p ' s initial state is for 0. The initial states of the other processors are fixed
but irrelevant.
9 In round 1, q receives p ' s message, but no other processor does so. No processor receives any message from q in round 1. There is no communication between processors
p , and pv in Pc if and only if there is an edge between vertices u and v in G (i.e.,
(u, v) e E). All other messages are delivered in round i.
9 In round 2, no processor receives any message from p, while only r receives q ' s
message. All other messages are delivered in round 2.
9 After round 2, no processor ever receives a message from either p or q while all
messages from all other processors are delivered.
This execution is not a completely described run of the protocol; the operating environment (and thus the identities of the faulty processors) has not been specified. For
example, if pu and Pv are in Pc and (u, v) e E, then there may be a run in which p , is
faulty and one in which p,, is. Fixing the operating environment identifies a unique run.
The sequel refers to such runs as compatible runs because they are compatible with the
history described above. All processors pass through the same sequence of states in all
compatible runs.

316

R.A. Bazzi and G. Neiger

Notice that, in round 1, no message from p is received by r or the processors P c U A.
Thereare I+IPGI+IA[ = l + v + m a x { c - - 1 , g ( c + 2 ) - - ( v + 3 ) } > l + v + ( c - - 1 ) = v + c
such processors. Since v > 2 and t = c + 2 , this number is greater than t; these processors
cannot all be faulty in any compatible run. Thus, p must be faulty in any compatible run.
Similarly, at least v + c + 1 processors do not receive a message from q in round 1, so
q must also be faulty in any compatible run.
Note that ok(a) is not known by any processor in A at time 2. Since c > 2 and
t = c + 2, IAI = 2c > t, so at least one processor in A is correct in every compatible
run. This means that ok(a) cannot be common knowledge at time 2 of any compatible
run.

The sequel uses the following lemma of Moses and Tuttle [17, L e m m a 32]:
LEMMA 1 [17]. Let p and p' be runs differing only in the (faulty) behavior o f some
processor after time j , and suppose that no more than f processors fail in either p or
p'. If s is such that s - j < t + 1 - f , then (p, ~) "~ (p', ~).
The following lemmas applies to all compatible runs:
LEMMA 2. For all m (0 < m < c), ok(a) is common knowledge at time m + 3 of a
compatible run if and only if G has no vertex cover o f size less than c - m.
PROOF. The proof begins with the "only if" direction. Assume that G has a vertex cover
C of size less than c - m . Let p be a compatible run such that fewer than ( c - m ) + 2 = t - m
processors fail by the end of the second round; such a run exists because all missing
messages can be accounted for by assuming that p and q are faulty as are the (fewer than
c - m) processors in PG corresponding to C (since C covers the edges in G, omissions
by these processors can account for all the messages missing among processors in PG
in round 1). Now, consider another run p ' that differs from p only in the behavior of r,
which is silent in p ' after the second round. Note that at most (c - m) + 2 = t - m
processors fail in p ' (one more than in p).
Let j = 2, f = t - m, and ~ = m + 3. L e m m a 1 applies to p and p ' because they
differ only on the faulty behavior of r after time j , at most f processors fail in either
run, a n d s
=t+l-(t-m)=t+l-f.
This means that
(p, m + 3) ~ (p', m + 3). Consider now a run p" identical to p ' except that p ' s initial
state is not for 0; thus, (p", m + 3) ~ ok(a). Let s be a processor correct in p ' (and p");
clearly, p~(m + 3) = p~'(m + 3). Since (p, m + 3) ~ (p', m + 3), the definition of
gives (p, m + 3) ~ (p", m + 3). This implies that (p, m + 3) ~ "-,C(ok(a)), completing
the proof of the "only if" direction.
For the "if" direction, assume that G has no vertex cover of size less than c - m. This
means that there must be at least c - m faulty processors in PG (in all compatible runs)
to account for the messages missing within PG in round 1. Let ~o be " p and q are faulty
and PG contains at least c -- m faulty processors." It follows that Kr (ok(a) A ~o) at the end
of the second round of any compatible run. This is because r receives messages from all
processors other than p in round 2 and its state thus contains the information about all
the messages exchanged in the first round. The missing messages cannot be accounted
for unless ~0 holds.

The Complexity of Almost-OptimalSimultaneous Coordination

317

The remainder of the proof makes use of the idea of a processor learning a fact
through a chain of distinct processors. Let cr = ( p l , p2 . . . . . Pd) be a sequence of d
distinct processors. Processor Pd+I learns 17 through a at time ~ + d of a run if Kp, 1"/
holds at time ~ and communication between pi and pi+l is successful in round e + i for
all i, 1 < i < d. Note that the definition allows Pd+l to be one of the processors in ~r.
Note also that if processor s learns r/through a at time ~ + d, then Ks (s learns 7/through
a at time ~ + d) holds at time e + d (this is true because the communication protocol is
full-information).
Let P r = Pc U {p, q} and let Pc = P - PF = A U {r}. Let ~z be "there exists
a processor that learns ok(a) A ~p at time m + 3 through a chain of m + 1 distinct
processors in Pc:?' The proof next shows that 7t is common knowledge at time m + 3
of any compatible run. Since ~p =:~ ok(a) A ~0 is trivially valid, it will then follow that
ok(a) is common knowledge at such points as well.
Let p be a compatible run. The proof uses the induction rule for common knowledge:
it will first show that (p, m + 3) ~ ~p and then show that ~p =r f i e is valid in the system.
This will imply that ~ =:~ C ~ is valid in the system and (p, m + 3) ~ G ~ .
To show that (p, m -k- 3) ~ ~ , recall first that (p, 2) ~ Kr(ok(a) A ~0). Since there
are at least (c - m) + 2 = t - m faulty processors in PF, there are at most m faulty
processors in Pc:. Since IPc l = 2c + 1, there are 'at least 2c -t- 1 - m that are correct
and, since m < c (by hypothesis), this number is greater than m. Let a be r followed by
any m of these correct processors; it is clear that all correct processors learn ok(a) A q9
through a at time m + 3, so (p, m + 3) ~ ~ .
The proof concludes by showing that, for any point (p', ~) (p' need not be compatible) such that (p', e) ~ ~p holds, (p', g) ~ E~p also holds. Let (p', e) be such a
point. Since (p', e) ~ ~ it follows that (p', e) ~ ~o, which means that there are at least
c - m + 2 = t - m faulty processors in Pp in p'. Since 7z holds at (p', (), some processor
learns ok(a) A ~o through a chain cr o f m + 1 processors in Pc in p'. Since Pc contains at
most t - (t - m) = m faulty processors, one of the processors in cr is correct. Let Pi be
this correct processor, where i is the index of pi in cr and i is maximal among the correct
processors in or. This means that there are at least (m + l ) - i faulty processors in cr and Pc
contains at most m - ((m + I ) - i) = i - 1 faulty processors outside or. Overall, Pc contains IPc I- ]cr I -~ 2c + 1 - (m + 1) > c processors outside or, at least c - (i - 1) = c + 1 - i
of which are correct. Because ~- is full-information, it follows now that every correct
/
processor learns ok(a) A~0 through a chain P'I, P'l', . . . . P,,+l o f m + 1 distinct processors
at time m + 3. The chain is such that pj = pj for 1 < j < i and the remaining m + 1 - i
processors are chosen from the c + 1 - i correct processors in Pc that were not in ct
(recall that m < c). By an observation made earlier, this means that (p', e) ~ ETz.
The preceding paragraph implies that ~p :=~ E~p is valid in the system and, by induction, so is ~ =:~ C ~ . Since (p, m + 3) ~ ap, ( p , m + 3) ~ C~O and ( p , m + 3)
C(ok(a)). This completes the proof.
[]
5.2. Protocols Almost Optimal with Additive Constants.
is the following:

The main result of this section

Suppose that f ( q b ) implements C (with some function g) and is almost
optimal with additive constant k. Then computation of dO is NP-hard.

THEOREM 3.

318

R.A. Bazzi and G, Ne~ger

The proof proceeds as follows. Suppose that r can be computed in polynomial time:
if there are n processors, then any processor q can compute r
at time s in time
polynomial in n and e. The remainder of this section shows how r can be used to
implement SmallerCover in polynomial time.
Lemma 2 suggests a use of action function * to implement SmallerCover. If a processor correct in some compatible run p does not choose action a at time k + 3, then
ok(a) was not common knowledge at time 3 (this is because .T'(*) is almost optimal
with additive constant k). Lemma 2 (with m = 0) implies then that G has a vertex cover
of size smaller than c and that SmallerCover(G, c) should return true. On the other hand,
if a processor does choose a at time k 4- 3, it can be deduced only that ok(a) is common
knowledge at time k + 3; we cannot conclude that ok(a) was common knowledge at
time 3. Thus, Lemma 2 (with m = k) implies only that G does not have a vertex cover
of size less than c - k. The "gap" between c and c - k prevents a direct implementation
of SmallerCover. To implement SmallerCover, we define a new graph.
Let G = (V, E) be any graph and let z >_ 1 be a natural number. Define G z = (V', E')
as follows. Vertex set V' is U~=I Vi, where Vi = {(v,i) [ v ~ V}. Edge set E' is
{((u, i), (v, i) I 1 < i < z and (u, v) c E}. Thus, G: is the union o f z disjoint copies of
G. Note that if G z has a vertex cover of size less than Mz, then G has a vertex cover of
size less than M. If G z does not have a vertex cover of size less than M z - (z - 1) (i.e.,
at most (M - l)z), then G does not have a vertex cover of size less than M.
Consider now our particular G. When asked to compute the function SmallerCover(G, c), construct the system and history described above using G k+l instead of G
and c(k 4- 1) instead o f c . Let s be a processor in the set A; as noted above, s is correct
in some compatible runs and has the same state in all of them. Use q~,,.,. (which can be
computed in polynomial time) to determine whether or not s chooses a by time k 4- 3.
If it does not, then ok(a) is not common knowledge by time 3 (otherwise, an optimal
solution would have s choose a at time 3 and 5t'(~), being almost optimal with additive
constant k, would do so by k + 3). Using a k+t for G and c(k + 1) for c, L e m m a 2 (with
m = 0) implies that G t'+l has a vertex cover of size less than c(k + 1). As noted above,
this implies that G has a vertex cover of size less than c. I f s does choose a by time k 4- 3,
then ok(a) is common knowledge by time k + 3 [91, [17]. Here, Lemma 2 (with m = k)
implies that G ~+ I has no vertex cover of size less than c(k 4- 1) - k. as noted above, this
implies that G has no vertex cover of size less than c. Thus, ~..~ has s choose a by time
k + 3 if and only if G has no vertex cover of size less than c, so SmallerCover(G, c)
should return true if and only if s chooses a by time k + 3. This completes the proof for
the case of protocols almost optimal within additive constants.
The key to the above proof is the fact that the sizes of the vertex covers of G k+l are
completely dependent upon those of G. Notice that, if k = 0, ~(q~) is truly optimal (0
is the additive identity) and G k+l = G. That is, these results generalize those of MOses
and Tuttle.
5.3. Protocols Almost Optimal with Multiplicative Constants.
section is the following:

The main result of this

THEOREM 4. Suppose that f'(dp) implements C (with some function g) and is almost
optimal with multiplicative constant k. Then computation of 9 is NP-hard.

The Complexity of Almost-OptimalSimultaneous Coordination

319

The proof proceeds as follows. Suppose that q~ can be computed in polynomial time.
The remainder of this section shows how 9 can be used to implement S m a l l e r C o v e r in
polynomial time.
The results here are developed almost exactly as in Section 5.2. The same execution is
constructed, this time from G 3k-2 instead of G k+l . Lemma 2 still holds using c ( 3 k - 2)
for c. Consider whether or not processor s ~ A (see above) chooses a by time 3k. If
it does not, then o k ( a ) was not common knowledge at time 3 (this is because .T(~)
is almost optimal with multiplicative constant k). Lemma 2 (with m = 0) implies that
G 3k-2 has a vertex cover of size less than c ( 3 k - 2). This implies that G has a vertex
cover of size less than c. Ifs does choose a by time 3k, then o k ( a ) is common knowledge
by time 3k. Here, Lemma 2 (with m = 3k - 3) implies that G 3k-2 has no vertex cover
of size less than c ( 3 k - 2) - (3k - 3). This implies that G has no vertex cover of size
less than c. Thus, ~,..~ has s choose a by time 3k if and only if G has no vertex cover
of size less than c, so S m a l l e r C o v e r ( G , c) should return true if and only if s chooses
a by time 3k. This complete the proof for the case of protocols almost optimal within
multiplicative constants.
In this case note that, ilk = 1, .Y'(~) is truly optimal (1 is the multiplicative identity)
and G 3k-2 = G. Again, the results generalize those of Moses and Turtle.

6. Conclusions. The results of this paper extend those shown earlier by Moses and
Tuttle [17]. They show that, in synchronous systems with general omission failures,
any almost-optimal solution to a simultaneous choice problem requires processors to
perform NP-hard local computation.
Our results also apply to the consistent simultaneous choice problems of Neiger and
Tuttle [21]; solutions to such problems require that faulty processors, if they act, do so
consistently and simultaneous with the correct ones.
One conclusion that can be drawn from these results is that translations that increase
fault-tolerance cannot be used to obtain almost-optimal algorithms. Consider, for example, systems with crash failures. Dwork and Moses [9] showed that polynomial-time
optimal solutions exist for such systems. For systems with n > 2t, Neiger and Toueg [20]
showed how crash-tolerant algorithms could be converted to tolerate general omission
failures by only doubling the number of rounds used and with only a polynomial increase
in local computation. Since their methods add only a polynomial amount of local computation, the resulting algorithms run in polynomial time. The results of this paper show
that if P :~ N E then the translation of Neiger and Toueg does not preserve optimality
(this could probably be shown independent of the relation between P and NP but that is
beyond the scope of this paper). This translation operates by masking minor failures; the
"awareness" of these failures, however, may allow correct processors to choose early. By
obscuring them, the translation technique deprives processors of information that may
allow them to choose optimally.
Note that the results hold only for additive and multiplicative constants. It is not hard
to show that any simultaneous coordination problem has a solution that is almost optimal
with an additive "constant" of t - 1. In the reductions given in Section 5, the constant
k determines the size of the graph and the cover desired, which in turn determines n
and t in the execution constructed. Thus, k and t cannot be constrained to have a fixed

320

R.A. Bazzi and G. Neiger

relation, such as k ---- t - 1. This paper's results and the existence of solutions optimal
within t - 1 rounds are thus not contradictory.

Acknowledgments.

The authors thank Yoram Moses for suggesting this problem.

References
[ 1] R.A. Bazzi and G. Neiger. The complexity and impossibility of achieving fault-tolerant coordination. In
Proceedings of the Eleventh ACM Symposium on Principles of Distributed Computing, pages 203-214.
ACM Press, New York, August 1992.
[2] R.A. Bazzi and G. Neiger. Simplifying fault-tolerance: Providing the abstraction of crash failures.
Technical Report 93/12, College of computing, Georgia Institute of Technology, February 1993. Earlier versions of parts of this paper appeared as "Optimally Simulating Crash Failures in a Byzantine
Environment" in S. Toeug, P. G. Spirakis, and L. Kirousis, editors, Proceedings of the Fifth International Workshop on Distributed Algorithms, of Lecture Notes on Computer Science, volume 579, pages
108-128. Springer-Verlag, Berlin, October 1991, and as "Simulating Crash Failures with Many Faulty
Processors" in A. Segall and S. Zaks, editors, Proceedings of the Sixth International Workshop on DistributedAlgorithms, Lecture Notes on Computer Science, volume 647, pages 166-184. Springer-Verlag,
Berlin, November 1992.
[3] P. Berman and J. A. Garay. Cloture votes: n/4-resilient, polynomial time distributed consensus in t + 1
rounds. Mathematical Systems Theory., 26( I ):3-20, 1993.
[4] J.E. Burns and N. A. Lynch. The Byzantine firing squad problem. Advances in Computing Research:
Parallel and Distributed Computing, 4:147-161, 1987. Also appears as Technical Report 275, MIT
Laboratory for Computer Science.
J5] B.A. Coan. A communication-efficient canonical form for fault-tolerant distributed protocols. In Proceedings of the Fifth ACM Symposium on Principles of Distributed Computing, pages 63-72, August
1986. A revised version appears in Coan's Ph.D. dissertation [6].
[6] B.A. Coan. Achieving Consensus in Fault-Tolerant Distributed Computer Systems: Protocols, Lower
Bounds and Simulations. Ph.D. dissertation, Massachusetts Institute of Technology, June 1987.
[7] B.A. Coan, D. Dolev, C. Dwork, and L. Stockmeyer. The distributed firing squad problem. SlAM
Journal on Computing, 18(5):990-1012, October 1989.
[8] D. Dolev, R. Reischuk, and H. R. Strong. Early stopping in Byzantine agreement. Journal of the ACM,
37(4): 720-741, October 1990.
[9] C. Dwork and Y. Moses. Knowledge and common knowledge in a Byzantine environment: Crash
failures. Information and Computation, 88(2): 156-186, October 1990.
110] M. R. Garey and D. S. Johnson. Computers and Intractabili~.: A Guide to the Theory. of NPCompleteness. Freeman, New York, 1979.
[I I] V. Hadzilacos. A knowledge theoretic analysis of atomic commitment protocols. In Proceedings of the
Sixth ACM Symposium on Principles of Database Systems, pages 129-134. ACM Press, New York,
March 1987.
[ 12] J.Y. Halpern and Y. Moses. Knowledge and common knowledge in a distributed environment. Journal
of the ACM, 37(3):549-587, July 1990.
113] J.Y. Halpern, Y. Moses, and O. Waarts. A characterization of eventual Byzantine agreement. In Proceedings of the Ninth ACM Symposium on Principles of Distributed Computing, pages 333-346. ACM
Press, New York, August 1990.
[14] L. Lamport, R. Shostak, and M. Pease. The Byzantine generals problem. ACM Transactions on Programming Languages and Systems, 4(3):382--401, July 1982.
[ 15] M.S. Mazer. A knowledge theoretic account of recovery in distributed systems: The case of negotiated
commitment. In M. Y. Vardi, editor, Proceedings of the Second Conference on Theoretical Aspects of
Reasoning about Knowledge, pages 309-324. Morgan-Kaufmann, Los Altos, CA, March 1988.
[16] R. Michel. Knowledge in Distritmted Byzantine Environments. Ph.D. dissertation, Yale University,
December 1989.

The Complexity of Almost-Optimal Simultaneous Coordination
[ 17]
[18]

[ 191
[20]
[21 ]
122]

321

Y. Moses and M. R. Tuttle. Programming simultaneous actions using common knowledge. AIgorithmica,
3(1):121-169, 1988.
G. Neiger and R. Bazzi. Using knowledge to optimally achieve coordination in distributed systems.
In Y. Moses, editor, Proceedings of the kburth Conference on Theoretical Aspects of Reasoning about
Knowledge, pages 43-59. Morgan-Kaufmann, Los Altos, CA, March 1992.
G. Neiger and S. Toueg. Automatically increasing the fault-tolerance of distributed algorithms. Journal
of Algorithms, 11(3):374-419, September 1990.
G. Neiger and S. Toueg. Simulating synchronized clocks and common knowledge in distributed systems.
Journal of the ACM, 40(2):334-367, April 1993.
G. Neiger and M. R. Turtle. Common knowledge and consistent simultaneous coordination. Distributed
Computing, 6(3): 181-192, April 1993.
M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of faults. Journal of the
ACM, 27(2):228-234, April 1980.

2004 International Conference on Image Processing (ICIP)

SELECTIVE FEC FOR ERROR-RESILIENT IMAGE CODING AND TRANSMISSION
USING SIMILARITY CHECK FUNCTIONS
Tuyet-TrungLam', Linti J. Kurum', Rida A. Buzz?', Glen I?Ahouslenian3

'Electrical Engineering Dept., Arizona State University, Tempe, A Z 85287-5706.
Computer Science & Engineering Dept., Arizona State University, Tempe, AZ 85287-5406.
Compression, Comm. & Intelligence Lab, General Dynamics C4 Systems, Scottsdale, A Z 85257.
lumsnow@usu.edu, kuram@usu.edu, huzzi@usu.edu, glen.ahousleman @gdds.com
ABSTRACT
This paper introduces the concept o f similarity check functions that measure the degree of corruption in transmitted
multimedia data packets. Rather than directly discarding
or directly retransmitting erroneous packets, the degree of
corruption contributed by a particular packet is measured
by a similarity check function at the receiver, which does
not require explicit knowledge o f the source data. Accordingly, if the packet is found to be badly corrupted based on
a speciticd similarity criteria, i t can he considered as a lost
packet and recovered using forward error correcting codes
or retransmission. A n example of a similarity check function design and a selective forward error correction (S-FEC)
scheme that allocates channel protection hits only to those
packets that are in most need of correction, is presented to
illustrate the proposed concept.

1. INTRODUCTION
In typical data transmission systems, the original source tile
is compressed, packetized, and transmitted through a noisy
channel. Because bit corruption can cause a catastrophic
loss o f information, the packets are protected with channel
codes, and error detection is performed on each packet at the
receivcr. However, existing error detection schemes do not
provide information about the effects of a corrupted packet
on the quality ofthe reconstructed data. Bit errors on significant bits may propagate throughout the reconstructed data,
while bit errors on less significant bits may not present visible distortions.
I n popular joint source-channel coding schemes [I,21.
the noisy channel is transformed into a packet erasure channel by dropping all packets that cannot be corrected. ReedSolomon (RS) erasure codes are then used to recover missing packets [3, 41. Typically, rate-compatible punctured
convolutional codes (RCPC) are used to correct packets from
random hit errors, while the packets that remain corrupted
are detected using a cyclic redundancy check (CRC) code.
Ifthe CRC fails, the packet is declared to he erroneous
and is discarded. I n [I], i t has k e n shown that combining

0-7803-8554-3/04/$20.0002004 IEEE.

RCPCKRC with a fixed-rate Reed-Solomon erasure code
performs better than using RCPCICRC only. Unfortunately,
a tined rate RS code cannot recover an arbitrary number
o f lost packets. This problem has been addressed in [2],
where each packet is protected against scattered bit errors
by using an RCPC-CRC row code, where the source symbols are distributed across packets, and protected fully using
RS codes o f different strengths according to the importance
o f Lhe source symbol. The amount o f protection is given
by an algorithm that uses the characteristics o f the channel
to optimally allocate the correct amount of RCPCKRC and
RS codes. Additionally, optimal source-channel rate allocation algorithms have been improved for packet loss over
Rayleigh fading channels as shown in p. 41.
In our proposed scheme, rather than directly discarding
erroneous packets, the degree of corruption is measured using a similarity check function that does not require explicit
knowledge ofthe original data at the receiver. Accordingly,
if the packet is found to be badly corrupted based on a specified similarity criteria, i t can be considered as a lost packet
and recovered using forward error correcting codes (FEC).
If the applied similarity check function indicates that the
degree of corruption o f the considered packet is acceptable,
the packet is still usable and no FEC is needed. I n this way,
the total amount of FEC needed to protect the transmitted
datacan he decreased. and more bits can be allocated to the
source coding for enhanced quality.
This paper is orgmizcd as follows. Section 2 describes
the new concept o f the similarity check function. This function can he used to intelligently allocate channel protection
bits to packets that most need to he corrected rather than
protecting all packets. Section 3 illustrates the proposed
selective-FEC scheme using a particular similarity function
design and the wavelet-based TCQ image coder presented
in [ 5 ] . A conclusion is given in Section 4.

2. SIMILARITY CHECK FUNCTION
The proposed concept of a similarity check function i s motivated by the use o f hash functions to provide fault toler-

3217

(a) No noise, PSNR = 20.88 dB

(h) BSC with BER= 0.001, PSNR= 27.34 dB

Fig. 1. 512 x 512 Lena image coded at 0.246 hpp.
ance and security in a distributed storage environment [6].
A hash is computed to uniquely represent a file (or a piece
of a file) using only a few hits. It can be thought of as a
digital fingerprint of a larger document [7]. Because the
hash is much smaller than the original file, it is more efficient to fully protect the hash than the corresponding file.
The protected hash is sent along with the file. When the file
is received, a new hash is computed from that file. If the
received file is identical to the original file, the two hashes
will match. If the hashes do not match, the received file
is discarded. Similarly, our similarity check function is designed such that, when applied to a piece of data (or packet),
it results in an output (i.e., an index) that can he rcpresented with an insignificant number of hits relative to the
original piece of data, without the need to have the original piece of data. However, while a hash can only indicate
whether there is an exact match between the received and
transmitted pieces of data, and thus, cannot provide information about the amount of corruption introduced in a file,
the proposed similarity check function is designed to measure the degree of corruption introduced. For multimedia
applications, a file does not need to be exactly identical to.
the original data to be usable, which motivates the proposed
similarity check function general framework. Note that a
CRC code or a hash function can he considered as a special
similarity check function with the similarity criteria set to
he the identity function.
Let the original data he transmitted (or slored) as N packets (or pieces), pl , . . . ,p ~ each
, of size L. Let p', . . . ,p h
be the corresponding received packets. Let P denote the set
containing all packets (data pieces) of size L. The similarity
check function is defined as a mapping from the set P to the
. . ,I,} as follows:
set I = {II,.

f(p E P ) = I [ , 1 E {l,.. . , T } ,

(1)

where 11in ( I ) is referred to as the similarity check index.

Note that only c = 1og2(r) hits are needed to represent elements in the set I, and c is selected to he very small compared to the packet size. Two packets, PI;and p ; , are considered similar if and only if f ( p r ) = f ( p i ) .
The similarity check function can he designed based on
MSE or perceptual criteria. For example, Fig. 1 shows the
512 x 512 Lena image compressed with thc wavelet-based
TCQ image coder of [SI at 0.25 hits per pixel (hpp) and
transmitted through an ideal channel with no noise (Fig. la),
and through the binary symmetric channel (BSC) with bit
error rate (BER), Pt, = 0.001 (Fig. Ib). For this image, 71
hits have been corrupted, resulting in 65 corrupted packets.
Despite the fact that the PSNR for the image in Fig. Ib is
2 dB lower than the image in Fig. la, both images are very
similar. Thus, it would be wasteful to discard the image in
Fig. l b hccause it is not identical to the image in Fig. la.
The similarity check function can be used to estimate
the amount of protection needed to receive acceptable data.
In the proposed selective-FEC scheme, the similarity check
function is used to evaluate the number of packets to he protected. Then, rather than correcting all corrupted packets,
only packets that result in a high degree of degradation will
he corrected. In this S-FEC method, the received data will
not he exactly the same as the transmitted data, hut annoying noise artifacts can he corrected. Note that if not enough
channel coding is assigned to fully recover all severely corrupted or lost packets, the similarity check function can also
he designed to provide usable information to correct a portion of the lost packets.

3. CODING EXAMPLE USING A SIMILARITY
CHECK FUNCTION AND SELECTIVE FEC
In the following coding example, the wavelet-based TCQ
coder of [SI is used to code 512 x 512 images, which are
then transmitted through the BSC channel with a hit error
rate, Pb. The coder in [SI has been modified to perform a

321 8

BSC

BER

aoni
0.005
0.0 I

band decomposition.

22"
0

protected
packets

S-FEC
PSNR

27.76
23.43
2 1.38

23.1 %
71.9 %
92.2%

0.08 %

29.6

2%
3.1%

28.36
27.21

3. At the receiver, for each received packet, pi. computc
the similarity check function as i n (2), and compare

zze*
0

corrupted
packets

these constitute an insignificant portion of the bit hudget and can bc protccted fully without significant
added overhead.

Fig. 2. Packetization o f wavelet coefficients for a 13 suh-

0

unprotected
PSNR

with the corresponding transmitted similarity check
index, In,: i f { f ( p i ) # f(p:), then p: is discarded}
else {pi is kept and used to reconstruct the image}.
Error correcting codes can then bc used to recover the
discarded packets.

0

0

Fig. 3. Scanning o f coefficients from the LL sub-band
4-level dyadic wavelet decomposition to simplify the packetiration. Each packet consists of 4 quantized wavelet coefticients from the zero-mean LL sub-band, and thcir corrcsponding children in the wavelet decomposition, as shown
in Fig. 2. To limit the propagation of hit errors across packets, coefficients from the lowest sub-band are scanned, as
shown in Fig. 3, and coded using trellis-coded quantization

Onc advantage o f this implementation is that very little overhead is needed to perform packet loss detection as compared
to typical CRC codes that can take 8 to 16 bits. IfN is the
number of packets, L is the packet length, and c is the number o f hits output by the similarity check function per packet
( c = 1 i n the example above), the overhead produced by the
S-FEC scheme i s computed as:
overhead = XLN 2cN bitsl
(3)
where X is the maximum fraction of discarded dissimilar
packets. and assuming that a rate- 1/2 code is uscd to protect
the similarity check indices.
Tdbk I shows the number o f corrupted packets for the
512 x 512 grayscalc Lena image compressed at 0.248 bpp
using thc wavelet-bnsed TCQ coder i n (51. The results in
this tahle were obtained by averaging over 10 simulations.
The percentage of protected packets is given by the maximum percentage of dissimilar packets over all simulations
for a givbn bit error rate. As shown in Table I,for the BSC
with a hit error rate o f 0.01_the proposed S-FEC scheme results in a 6 dB increase in pcak signal-to-noise ratio (PSNR)
by protecting only 3.1% o f the packets, while 92.2% of the
received packets are corrupted.
Fig. 4 provides a comparison o f the proposed S-FEC
scheme with the conventional FEC scheme used in [SI. I n [SI,
the output hit stream is partitioned into blocks of length
L = 200 hits, with c = l G appended parity bits, and 111 = G
extra bits to terminate the convolulionalcodes. For the BSC
with a hit error rate, Pb = 0.001, a rate-8/9 RCPC code
with memory 6 was then used on the obtained block o f
N c m (= 200 16 6 = 222) bits, which results

+

(TCQ).
Since the coefficients from the LL sub-band are morc
important than coefficients from the other suh-bands, a simple similarity check function can he designed as follows.
Let VLL denote a vector consisting o f the 4 quantized LL
coefficients i n a packet, p. VLL can be classified into two
classeswithcentroids,~,, = [-I -I-I- 1 I a n d c l = [ 1I I I].
The similarity check function in this example is defined as

where I/./) is thc L1norm. The S-FEC-based joint sourccchannel coding scheme proceeds as follows:

I . At the transmitter side, for each packet, p i , compute
the similarity check function as in (2). This will result in a I-bit output (similarity check index), Ips t
IO, 1).
2. Send the resulting I -bit similarity check indices, {Ip,},
to the receiver along with the data packets. Note that

3219

+ +

+ +

(a) Not protected, PSNR = 2G.56
source coding: 0.246 bpp (100%)
channel coding: 0 bpp (0%)

(b) FEC from [ X I , PSNR = 28.65
source coding: 0. I815 bpp (75%)
channel coding: 0.0625 bpp (25%)

(c) Proposed S-FEC, PSNR = 29.56
source coding: 0.246 bpp (08%)
channel coding: 0.005 bpp (2%)

Fig. 4. PSNR (in dB) o f 512 x 512 Lena image coded at 0.25 bpp over BSC channel with bit error rate, Pb = 0.001.
i n a total channel coding overhead of 25%. Fig. 4 shows the
received Lena image coded at 0.25 bpp using the waveletbasedTCQcoderoverthe BSC with abiterrorrateof0.001.
We can see that the noisy channel introduces severe impulsive artifacts, as shown in Fig. 4a, resulting in a drop in
PSNRto26.56dB. Figs.4band4cshow thc512x512Lena
image obtained using the FEC scheme of [ X I and our proposed S-FEC scheme, respcctively. The proposed S-FEC
scheme results in a PSNR of 29.56 while the FEC scheme
of [81 results in a PSNR o f 28.65 dB. In addition, thc image produced using the scheme of [8]i s morc blurred as
compared to the one obtained using the proposed S-FEC
scheme. T h i s i s due to the fact that the scheme o f [8] allocates 25% of the total bit rate to the channel coding, leaving
only 75% of the total bit rate to the source coding. In contrast, the proposed 8-FEC scheme allocates only 2% of the
total bit rate to the channel coding, while the remaining 08%
i s used hy the source coder, which results in a sharper image and higher PSNR. For our simulations, the 2% channel
coding overhead was computed using (3) for the proposed
scheme. I n addition, all the corrupted packets were assumed
to be exactly recovered by the channel coding for the result o f Fig. 4b. while only the non-similar corrupted packets
were assumed to be recovered for the result o f Fig. 4c.

4. CONCLUSION
In this p a p , we have presented the concept of a similarity check function that measures the amount of corruption
introduced in an image without explicit knowledge o f the
original data at the receiver. T h i s similarity check function
is used to detect severely corrupted packets in the proposed
S-FEC scheme and can. thus, allow a fast and intelligent allocation between source and channel bits. Future work w i l l
consider the design of similarity check functions that are

optimized based on MSE or perceptual distortion measures,
and the design of practical selective forward error correcting codes that are optimized based on similarity check functions. We w i l l also consider channels with burst errors such
as Rayleigh-fading channels.
5. REFERENCES
[I] P.C. Sherwood and K. Zeger. “Error protection for progressive

image transmission over memoriless and fading channels.” in
IEEEICIP. Oct. 1998.
[?I D. Sachs. A. Raghavan. and K. Ramchandran.

“Wireless
image transmission using multiple-description based concatenated codes:’ IEEE DCC. p. 569. Mar. 2000.

[3] A. Mohr, E. Riskin. and R. Ladner. “Unequal loss protection: graceful degradation o f image quality over packet emsure channels through forward error correction,” IEEE Journal on Selected Areas in Corninunicntions. vol. 18. pp. 819828. June 2000.

141 J. Kim. R. Merscreau. and Y.Altunhasak. “Error-resilient image and video transmission over internet using unequal error
protection,” IEEE Truns. on 1tnug.e Processing. vol. 12. no. 2.
Feb. 2003.

[51 TT
: . Lam. G.P. Abousleman. and L.J. Karam. “Image coding with robust channel-optimizedtrellis-coded quantization.”
IEEE Journal on Selected Arcas in Cotnmunicalions, vol. 18,
pp. 94C-951. June 2000.
[6] D. MacKay. Informarion tlzeopw, Inference and learning algorithms. chapter 12, Cambridge University Press. 2003.
[7] R. Rivest, ‘ T h e MD5 message digest algorithm.” Intemet RFC

1321, Apr. 1992.
181 P. G. Sherwood and K. Zeger. “Progressive image coding for
noisy channels.” IEEE Signal Processing Lerrers, vol. 4. pp.
189-191, July 1997.

3220

Leader Election and Shape Formation with
Self-Organizing Programmable Matter
Joshua J. Daymude1 , Zahra Derakhshandeh1 , Robert Gmyr2 Thim
Strothmann2 , Rida Bazzi1 , Andréa W. Richa1 , and Christian Scheideler2

arXiv:1503.07991v2 [cs.ET] 31 Mar 2016

1

Department of Computer Science and Engineering,
Arizona State University, USA,
{jdaymude,zderakhs,bazzi,aricha}@asu.edu
2
Department of Computer Science,
University of Paderborn, Germany,
{gmyr,thim}@mail.upb.de, scheideler@upb.de

Abstract. We consider programmable matter consisting of simple computational elements, called particles, that can establish and release bonds
and can actively move in a self-organized way, and we investigate the
feasibility of solving fundamental problems relevant for programmable
matter. As a suitable model for such self-organizing particle systems, we
will use a generalization of the geometric amoebot model first proposed
in SPAA 2014. Based on the geometric model, we present efficient localcontrol algorithms for leader election and line formation requiring only
particles with constant size memory, and we also discuss the limitations
of solving these problems within the general amoebot model.

1

Introduction

A central problem for programmable matter is shape formation, and various solutions have already been discovered for that problem using different approaches
like DNA tiles [35], moteins [15], or nubots [41]. We are studying shape formation using the amoebot model that was first proposed in [22]. In order to
determine how decentralized shape formation can be handled, we are particularly interested in the connection between leader election and shape formation.
In the leader election problem we are given a set of particles, and the problem
is to select one of the particles as the leader. Many problems like the consensus
problem (all particles have to agree on some output value) can easily be solved
once the leader election problem can be solved. The same has also been observed
for shape formation, as most shape formation algorithms depend on some seed
element. However, the question is whether shape formation can even be solved in
circumstances where leader election is not possible. The aim of this paper is to
shed some light on the dependency between leader election and shape formation:
On one hand, we present an efficient decentralized algorithm for solving leader
election in a geometric variant of the amoebot model and show how having a
leader leads to an efficient solution for the basic line formation formation problem; on the other hand, we show that both problems cannot be solved efficiently

2

Daymude et al.

under the general version of our model . Before we present our results in more
detail, we first give a formal definition of the model and the problems we study
in this paper.
1.1

Models

We use two models throughout this work. Firstly, we consider a generalization
of the amoebot model [22] which abstracts from any geometry information. We
call this model the general amoebot model. Secondly, we consider a model that
is essentially equivalent to the original amoebot model presented in [22] but is
defined based on the general amoebot model. We refer to this second model as
the geometric amoebot model.
In the general amoebot model, programmable matter consists of a uniform set
of simple computational units called particles that can move and bond to other
particles and use these bonds to exchange information. The particles act asynchronously and they achieve locomotion by expanding and contracting, which
resembles the behavior of amoeba.
As a base of this model, we assume that we have a set of particles that aim
at maintaining a connected structure at all times. This is needed to prevent the
particles from drifting apart in an uncontrolled manner like in fluids and because
in our case particles communicate only via bonds. The shape and positions of
the bonds of the particles mandate that they can only assume discrete positions
in the particle structure. This justifies the use of a possibly infinite, undirected
graph G = (V, E), where V represents all possible positions of a particle (relative
to the other particles in their structure) and E represents all possible transitions
between positions.
Each particle occupies either a single node or a pair of adjacent nodes in G,
i.e., it can be in two different shapes, and every node can be occupied by at most
one particle. Two particles occupying adjacent nodes are connected, and we refer
to such particles as neighbors. Particles are anonymous but the bonds of each
particle have unique labels, which implies that a particle can uniquely identify
each of its outgoing edges. Each particle has a local memory, and any pair of
connected particles has a shared memory that can be read and written by both
particles.
Particles move by expansions and contractions: If a particle occupies one
node (i.e., it is contracted ), it can expand to an unoccupied adjacent node to
occupy two nodes. If a particle occupies two nodes (i.e., it is expanded ), it can
contract to one of these nodes to occupy only a single node. Performing movements via expansions and contractions has various advantages. For example, it
would easily allow a particle to abort a movement if its movement is in conflict with other movements. A particle always knows whether it is contracted
or expanded and this information will be available to neighboring particles. In
a handover, two scenarios are possible: a) a contracted particle p can "push"
a neighboring expanded particle q and expand into the neighboring node previously occupied by q, forcing q to contract, or b) an expanded particle p can

Leader Election and Shape Formation

3

"pull" a neighboring contracted particle q to a cell occupied by it thereby expanding that particle to that cell, which allows p to contract to its other cell.
The ability to use a handover allows the system to stay connected while particles
move (e.g., for particles moving in a worm-like fashion). Note that while expansions and contractions may represent the way particles physically move in space,
they can also be interpreted as a particle "looking ahead" and establishing new
logical connections (by expanding) before it fully moves to a new position and
severs the old connections it had (by contracting).
Summing up over all assumptions above, the state of a particle is uniquely
determined by its shape, the contents of its local memory, the edges it has to
neighboring particles, the contents of their shared memory (which may allow a
particle to obtain further information about the neighboring particles beyond
their shape), and finally the shape of the neighboring particles. The state of
the particle system (or short, system state) is defined as the combination of all
particle states. We say a particle system in a system state in which the particle
occupy a set of nodes A ⊆ V is connected if the graph G|A induced by A
is connected. We assume the standard asynchronous computation model, i.e.,
only one particle can be active at a time. Whenever a particle is active, it can
perform an action (governed by some fixed, finite size program controlling it)
consisting of a finite amount of computation (involving its local memory, the
shared memories with its neighboring particles, and random bits) followed by no
or a single movement. Hence, a computation of a particle system is a potentially
infinite sequence of actions A1 , A2 , . . . based on some initial system state s0 ,
where action Ai transforms system state si−1 into system state si . The (parallel)
time complexity of a computation is usually measured in rounds, where a round
is over once every particle has been given the chance to perform at least one
action.
Let S be the set of all system states in which the particle system is connected.
In general, a computational problem P for the particle system is specified by a
set S 0 ⊆ S of permitted initial system states and a mapping F : S 0 → 2S ,
where F (s) ⊆ S determines the set of permitted final states for any initial state
s ∈ S 0 . A particle system solves problem P = (S 0 , F ) if for any initial system
state s ∈ S 0 , all computations of the particle system eventually reach a system
state in F (s) without losing connectivity, and whenever such a system state is
reached for the first time, the system stays in F (s). If for all computation a
final state is reached in which all particles decided to halt (i.e., they decided not
to perform any further actions, irrespective of future events), then the particle
system is also said to decide problem P . Note that being in a final state does not
necessarily mean that all particles decided to halt. If S 0 = S, so any initial state
is permitted (including arbitrary faulty states, as long as the particle system is
connected), then a particle system solving P is also said to be self-stabilizing. It
is well-known that in general a distributed system solving a problem P cannot
decide it and also be self-stabilizing because if so, it would often be possible to
come up with an initial state s where a member of the system decides to halt
prematurely, disallowing the system to eventually reach a state in F (s).

4

Daymude et al.

Besides the general amoebot model, we will also consider the geometric amoebot model. The geometric amoebot model is a specific variant of the general
amoebot model in which the underlying graph G is defined to be the equilateral
triangular graph Geqt (see Figure 1), and the bonds of the particles are labeled
in a consecutive way in clockwise orientation around a particle so that every
particle has the same sense of clockwise orientation. However, we do not assume
that the labeling is uniform, so the particles do not necessarily share a common
sense of direction in the grid.

Fig. 1. The left part shows an example of a particle structure in the geometric amoebot
model. A contracted particle is depicted as a black dot, and an expanded particle is
depicted as two black dots connected by an edge. The right part shows a particle
structure with 3 boundaries. The outer boundary is shown as a solid line and the two
inner boundaries are shown as dashed lines.

1.2

Problems

In this paper we consider the following two problems. For both problems we
define the set of initial system states as the set of all states such that the particle
system is connected and all memories are empty.
For the leader election problem the set of final system states contains any state
in which the particles form a connected structure and exactly one particle is a
leader (i.e., only this particle is in a leader state while the remaining particles are
in a non-leader state). Our goal will be to come up with a distributed algorithm
that allows a particle system to decide the leader election problem. Note that
the leader election problem is well defined for both the general amoebot model
and the geometric amoebot model.
In a shape formation problem, the set of final states consists of those system
states where the particle structure forms the desired shape. As a specific example
of a shape formation problem, we consider the line formation problem. In the
geometric amoebot model, the shape the particles have to form is a straight line
in the equilateral triangular grid and all particles have to be contracted in a
final system state. Of course, in the general amoebot model a straight line is not
well-defined. Hence, for this model the set of final states for the line formation
problem is defined to consist of all system states in which the particles form a
simple path in G.

Leader Election and Shape Formation

5

Throughout the paper, we assume for the sake of simplicity that in an initial state all particles are contracted. Our algorithms can easily be extended to
dispose of this assumption.
1.3

Our Contributions

We focus on the problem of solving leader election and shape formation for
particles with constant size memory. For shape formation, we just focus on the
already mentioned line formation problem.
For the geometric amoebot model, we show that there is a distributed algorithm that can decide the leader election problem (Section 3), i.e., at the end we
have exactly one leader and the leader knows that it is the only leader left. Moreover, the runtime for our leader election algorithm is worst-case optimal in the
sense that it needs O(Lmax ) rounds with high probability (w.h.p.)3 , where Lmax is
the maximum length of a boundary between the particle structure and an empty
region (inside or outside of it) in Geqt . Based on the leader election algorithm,
we present a distributed algorithm that solves the line formation problem with
worst-case optimal work in Section 4.
On the other hand, for the general amoebot model, we show that neither
leader election nor shape formation can be decided by any distributed algorithm
in Section 2. More concretely, we show that there cannot exist a randomized
algorithm for solving either problem with any bounded probability of success in
the general model.
The algorithms presented for leader election and line formation under the
geometric amoebot model both assume that the system is in a well-initialized
state. It would certainly be desirable to have algorithms that can tolerate any
initial state, but at the end of the paper, in Section 5, we show that there
are certain limitations to solving leader election and line formation in a selfstabilizing fashion.
1.4

Related Work

Many approaches related to programmable matter have recently been proposed.
One can distinguish between active and passive systems. In passive systems the
particles either do not have any intelligence at all (but just move and bond
based on their structural properties or due to chemical interactions with the
environment), or they have limited computational capabilities but cannot control
their movements. Examples of research on passive systems are DNA computing
[1, 9, 15, 21, 39], tile self-assembly systems in general (e.g., see the surveys in [23,
35, 40]), population protocols [4], and slime molds [10, 33]. We will not describe
these models in detail as they are only of little relevance for our approach. On
the other hand in active systems, computational particles can control the way
3

By with high probability, we mean with probability at least 1 − 1/nc , where n is the
number of particles in the system and c > 0 is a constant.

6

Daymude et al.

they act and move in order to solve a specific task. Robotic swarms and modular
robotic systems are some examples of active programmable matter systems.
In the area of swarm robotics it is usually assumed that there is a collection
of autonomous robots that have limited sensing, often including vision, and communication ranges, and that can freely move in a given area. They follow a broad
variety of goals: for example, graph exploration (e.g., [24]), gathering problems
(e.g., [2, 17]), shape formation problems (e.g., [25, 37]), and to understand the
global effects of local behavior in natural swarms like social insects, birds, or fish
(e.g., [8, 12]). Surveys of recent results in swarm robotics can be found in [31,
34]; other samples of representative work can be found in [5, 7, 18–20, 28, 32].
While the analytical techniques developed in the area of swarm robotics and
natural swarms are of some relevance for this work, the individual units in those
systems have more powerful communication and processing capabilities than in
the systems we consider.
The field of modular self-reconfigurable robotic systems focuses on intrarobotic aspects such as the design, fabrication, motion planning, and control
of autonomous kinematic machines with variable morphology (see e.g., [26, 42]).
Metamorphic robots form a subclass of self-reconfigurable robots that share some
of the characteristics of our geometric model [16]. The hardware development in
the field of self-reconfigurable robotics has been complemented by a number of
algorithmic advances (e.g., [11, 37, 38]), but so far mechanisms that automatically scale from a few to hundreds or thousands of individual units are still under
investigation, and no rigorous theoretical foundation is available yet.
The nubot model [13, 14, 41] by Woods et al. aims at providing the theoretical framework that would allow for a more rigorous algorithmic study of
biomolecular-inspired systems, more specifically of self-assembly systems with
active molecular components. Although biomolecular-inspired systems share similarities with our self-organizing particle systems, there are many differences that
do not allow us to translate the algorithms and other results under the nubot
model to our systems — e.g., there is always an arbitrarily large supply of "extra" particles that can be added to the system as needed, and the system allows
for an additional (non-local) notion of rigid-body movement.
Our developed leader election algorithm for the geometric amoebot model
shares some similarities with the algorithm of [?] for cellular automata. However,
since cellular automata work in a synchronous fashion and have access to a global
compass, the approach of [?] is vastly different from our leader election algorithm.

2

Impossibility Results in the General Amoebot Model

In this section, we show that both leader election and line formation are impossible to solve in the general amoebot model. Suppose that there is a distributed
algorithm solving the line formation problem in the general amoebot model
(when starting in a well-initialized state). Since in this case it is possible to decide when G|A0 forms a line, it is also possible to design a protocol that solves
the leader election problem: once the line has been formed, its two endpoints

Leader Election and Shape Formation

7

contend for leadership using tokens with random bits sent back and forth until
one of them wins. On the other hand, one can deduce from [29] that in the
general amoebot model there is no distributed algorithm that can decide when
a leader has been elected (with any reasonable success probability).
More concretely, in [29] the authors show that for a ring of anonymous nodes
there is no algorithm that can correctly decide the leader election problem4
with any probability α > 0, i.e., for any algorithm in which the particles are
guaranteed to halt, the error probability is unbounded. Since in the general
amoebot model G can be any graph, we can set G to be a ring whose size is the
number of particles and the result of [29] is directly applicable.
Hence, there is no a distributed algorithm deciding the line formation problem
(with any reasonable success probability) in the general amoebot model, and
therefore not even an algorithm for solving it since a protocol solving the problem
could easily be transformed into a protocol deciding it.

3

Leader Election in the Geometric Amoebot Model

In this section we show how the leader election problem can be decided in the
geometric amoebot model. Our approach organizes the particle system into a set
of cycles and executes an algorithm on each cycle independently (Section 3.1).
For simplicity and ease of presentation we first state the protocol in a simple
model in which particles have a global view of the cycle they are part of, act
synchronously, and have unbounded local memory (Section 3.2). We prove its
correctness in Section 3.3. In Section 3.5, we present the corresponding localcontrol protocol that works without these assumptions in the geometric amoebot model. However, since the local realization combines many different token
passing schemes and techniques, a formal analysis would be beyond the scope of
this paper. Instead, we show that our approach has expected linear runtime in a
variant of the simple model which is motivated by the token passing schemes of
the local-control protocol, i.e., it takes into account that interaction between two
particles is dependent on the distance between those particles (Section 3.4). We
conclude our analysis of the leader election problem by presenting an extension
of our protocol (in the simpler model) in Section 3.6 with a linear runtime with
high probability.
3.1

Organization into Cycles

Let A ⊆ V be any initial distribution of contracted particles such that Geqt |A
is connected. Consider the graph Geqt |V \A induced by the unoccupied nodes in
Geqt . We call a connected component of Geqt |V \A an empty region. Let N (R)
be the neighborhood of an empty region R in Geqt . Then all nodes in N (R) are
occupied and we call the graph Geqt |N (R) a boundary. Since Geqt |A is a connected
4

Or, in their words, that can solve the leader election problem with distributive
termination.

8

Daymude et al.

finite graph, exactly one empty region has infinite size while the remaining empty
regions have finite size. We define the boundary corresponding to the infinite
empty region to be the unique outer boundary and refer to a boundary that
corresponds to a finite empty region as an inner boundary, see Figure 1.
The particles occupying a boundary can instantly (i.e., without communication) organize themselves into a cycle using only local information: Consider a
boundary corresponding to an empty region R. Let p be a particle occupying a
node v of the boundary. By definition there exists a non-occupied node w ∈ R
that is a adjacent to v in the graph Geqt . The particle p iterates over the neighboring nodes of v in clockwise orientation around v starting at w. Consider the
first occupied node it encounters; the particle occupying that node is the successor of p in the cycle corresponding to that boundary. Analogously, p finds its
predecessor in the cycle by traversing the neighborhood of v in counter-clockwise
orientation.
Note that a single particle can belong to up to three boundaries at once.
Furthermore, a particle cannot locally decide whether two empty regions it sees
(i.e., maximal connected components of non-occupied nodes in the neighborhood
of v) are distinct. We circumvent these problems by letting a particle treat
each empty region in its local view as distinct. For each such empty region, a
particle executes an independent instance of the same algorithm. Hence, we say
a particle acts as a number of (at most three) distinct agents. For each of its
agents a particle determines the predecessor and successor as described above.
This effectively connects the set of all agents into disjoint cycles as depicted in
Figure 2. Observe that from a global perspective the cycle of the outer boundary
is oriented clockwise while a cycle of an inner boundary is oriented counterclockwise. This is a direct consequence of the way the predecessors and successors
of an agent are defined.

Fig. 2. The depicted particle system is the same as in the right part of Figure 1. In this
figure particles are depicted as gray circles. The black dots inside of a particle represent
its agents. As in Figure 1 the outer boundary is solid and the two inner boundaries are
dashed.

Leader Election and Shape Formation

3.2

9

Algorithm

The leader election algorithm operates independently on each cycle. At any given
time, some subset of agents on a cycle will consider themselves candidates, i.e.
potential leaders of the system. Initially, every agent considers itself a candidate.
Between any two candidates on a cycle there is a (possibly empty) sequence of
non-candidate agents. We call such a sequence a segment. For a candidate c
we refer to the segment coming after c in the direction of the cycle as seg(c)
and refer to its length by |seg(c)|. We refer to the candidate coming after c as
the succeeding candidate (succ(c)) and to the candidate coming before c as the
preceding candidate (pred(c)) (see Figure 3). We drop the c in parentheses if it
is clear from the context. We define the distance d(c1 , c2 ) between candidates
c1 and c2 as the number of agents between c1 and c2 when going from c1 to
c2 in direction of the cycle. We say a candidate c1 covers a candidate c2 (or
c2 is covered by c1 ) if |seg(c1 )| > d(c2 , c1 ) (see Figure 3). The leader election

Fig. 3. The figure depicts a part of a cycle that is oriented to the right. Non-candidate
agents are small black dots, candidates are bigger dots. The candidate c covers pred(c)
since |seg(c)| > d(pred(c), c).

progresses in phases. In each phase, each candidate executes Algorithm 1. A
phase consists of three synchronized subphases, i.e., agents can only progress to
the next subphase once all agents have finished the current subphase.

Algorithm 1 Leader Election for a Candidate c
Subphase 1:
pos ← position of succ(c)
if covered by any candidate or |seg(c)| < |seg(pred(c))| then
return not leader
Subphase 2:
if coin flip results in heads then
transfer candidacy to agent at pos
Subphase 3:
if only candidate on boundary then
if outside boundary then
return leader
else
return not leader

10

Daymude et al.

Consider the execution of Algorithm 1 by a candidate c. If the algorithm
returns "not leader" then c revokes its candidacy and becomes part of a segment. If the algorithm returns "leader", c will become the leader of the particle
system. The transferal of candidacy in subphase 2 means that c withdraws its
own candidacy but at the same time promotes the agent at position pos (i.e.,
succ(c) in subphase 1) to be a candidate. Once a candidate becomes a leader it
broadcasts this information such that all particles can halt.
3.3

Correctness

In order to show the correctness of our algorithm, we show that it satisfies the
following conditions, that relate to the entire particle system (not just a single
cycle):
1. Safety: There always exists at least one candidate.
2. Liveness: In each phase if there is more than one candidate, at least one
candidate withdraws leadership with a probability that is bounded below by
a positive constant.
Lemma 1. Algorithm 1 satisfies the safety condition.
Proof. We will show by induction that on the cycle associated with the outer
boundary there will always be at least one candidate. Initially, this holds trivially.
So assume that it holds before a phase. Let c be the candidate with the longest
segment. Then there is no candidate covering c and also |seg(c)| < |seg(pred(c))|
cannot be true. Hence, c will not withdraw candidacy in subphase 1. In subphase
2, the candidacy of c might be transferred but will not vanish. Let c0 be the
agent that received the candidacy if it was transferred and c0 = c otherwise. In
subphase 3, c0 will not withdraw candidacy because it lies on the outer boundary.
Hence, there is still a candidate after the phase.
t
u
Lemma 2. Algorithm 1 satisfies the liveness condition.
Proof. Assume that there are two or more candidates in the system. First we
consider the case that there is a cycle with two or more candidates. If there are
segments of different lengths on that cycle, we have |seg| < |seg(pred)| for at
least one candidate which will therefore withdraw its candidacy in subphase 1.
If all segments are of equal length, we have that in subphase 2 with probability
at least 41 there is a candidate c that transfers candidacy while succ(c) does not.
Hence, the number of candidates is reduced with probability at least 41 . Now
consider the case that all cycles have at most one candidate. Then there is a
cycle corresponding to an inner boundary that has exactly one candidate. That
candidate will withdraw candidacy in subphase 3 and thereby reduce the number
of candidates in the system.
t
u
The following Theorem is a direct consequence of Lemmas 1 and 2.
Theorem 1. Algorithm 1 successfully decides the leader election problem.

Leader Election and Shape Formation

3.4

11

Runtime Analysis

For a cycle of agents let L be the length of the cycle and let li be the longest
segment length before phase i of the execution of Algorithm 1. We define li = L
if there is no candidate on the cycle. It is easy to see that if li ≥ L/2 then in
phase i + 1 either the leader is elected (outer boundary) or all candidates on the
cycle vanish (inner boundary). For the case li < L/2, Lemma 3 provides the key
insight of our analysis.
Lemma 3. For any phase i such that li < L/2 it holds li+1 ≥ li in any case
and li+1 ≥ 2li with probability at least 1/4.
Proof. Consider a candidate c such that |seg(c)| = li . Subphase 1 can only
increase segment lengths and c will not withdraw leadership. So after Subphase
1 we have |seg(c)| ≥ li . Also, we have |seg(pred(c))| ≥ li because c covers any
candidate c0 such that d(c0 , c) < li . Here, we use pred(c) to refer to the cadidate
preceeding c after the execution of Subphase 1. For Subphase 2 we have to
distinguish between two cases based on the outcome of the coin flip of c. If
c does not transfer candidacy, we still have |seg(c)| ≥ li . For the case that c
does transfer candidacy, we have to further distinguish two cases based on the
outcome of the coin flip of pred(c). If pred(c) also transfers its candidacy, c will
receive that candidacy while c itself transfers its candidacy forward by a distance
of li . Therefore, we still have |seg(c)| ≥ li . If pred(c) does not transfer candidacy,
after Subphase 2 we have |seg(pred(c))| ≥ 2li because the segment of pred(c)
now spans both the segment of c after Subphase 1 and the segment of pred(c)
after Subphase 1, which is at least li . The probability that c transfers candidacy
while pred(c) does not is 1/4.
t
u
Let Lmax be the length of the longest cycle in the particle system. Based
on Lemma 3 it is easy to see that under complete synchronization of subphases
and with the agents having a global view of the cycle, our algorithm requires
on expectation O(log(Lmax )) phases to elect a leader. For the following analysis
assume that phases still progress in lockstep for all agents; however, the duration
of a phase is dependent on the longest segment length, i.e., phase i requires
O(li ) rounds. Theorem 2 gives a bound on the number of rounds required by the
algorithm based on this assumption.
Theorem 2. Algorithm 1 requires O(Lmax ) rounds on expectation.
Proof. Let the random variable Xi describe the number of rounds during the
execution of Algorithm 1 such that li ∈ [2i−1 , 2i ). Then, under the assumption
that phase i requires O(li ) rounds, the total runtime of our algorithm is
dlog(Lmax )e

T =

X
i=1

Xi · O(2i ).

12

Daymude et al.

Since E(Xi ) ≤ 4 due to Lemma 3, the expected runtime is
dlog(Lmax )e

E(T ) ≤

X

E(Xi ) · O(2i ) = O(Lmax ).

i=1

t
u
Note that subphase 1 of the algorithm is not important in terms of correctness.
However, it is crucial to achieve a linear runtime in expectation. If agents would
only execute subphases 2 and 3, the runtime would degrade to O(Lmax log Lmax ).
3.5

Asynchronous Local-Control Protocol

In this section we present a realization of Algorithm 1 as an asynchronous localcontrol protocol. The protocol heavily relies on token passing. All tokens used
by the protocol are messages of constant size and at any time an agent has
to hold at most a constant number of tokens. The tokens of each subphase of
Algorithm 1 are independent of each other, so an agent has to handle distinct
tokens for each phase at the same time. If not otherwise specified, we assume
that tokens of a single subphase move through the agents in a pipelined fashion
(i.e., a token does not surpass another token in front of it but waits until the
agent is free to hold it).
Candidate Elimination via Segment Comparison In Subphase 1 of Algorithm 1, a candidate c can become demoted if either the length of its front
segment is strictly less than that of its back segment, or some other candidate
covers c while performing its own comparison. Since c is unable to measure its
segments’ lengths directly in a local-control setting, it will instead use a token
passing scheme to match agents in its front segment with agents in its back segment. Conceptually, this matching proceeds as follows: each agent in the front
segment generates a cover token which travels into the back segment, matching one by one with the agents therein. If the front segment is longer than the
back segment, then some cover token will match with the preceding candidate;
a candidate which matches with a cover token is said to be covered. If c detects
that it was unable to cover any preceding candidates in this process and that
the segments lengths were not equal, it concludes that its front segment is too
short and revokes candidacy.
In detail, consider c as it begins the segment comparison subphase. Since this
subphase requires tokens to be passed in both its front and back segments, we
introduce the notion of tokens being either active or passive to avoid potential
collisions with tokens from other candidates. This can be imagined as two different “lanes” of tokens which can pass by one another unhindered. Candidate c
must first measure the length of its front segment; this is achieved by generating
a passive starting token and forwarding it along its front segment. Each noncandidate that receives this starting token forwards it and generates a passive

Leader Election and Shape Formation

13

cover token which is forwarded back towards c. The starting token is ultimately
forwarded to the succeeding candidate which consumes it and generates a final
cover token.
These cover tokens are forwarded back to c, which then converts them to
active cover tokens when forwarding them to its back segment where they begin
to match with the agents therein. Non-candidates in the back segment consume
the first cover token that is passed to them and continue to forward the others. If
another candidate, say c0 , receives an active cover token, then c0 has been covered
by c and revokes candidacy, henceforth behaving as a non-candidate. However, c0
may have been executing a number of now irrelevant leader election operations in
its front segment; thus, it generates a passive cleaning token which is forwarded
along its front segment, deleting any tokens it encounters with the exception
of those from c, which are active. This cleaning token is consumed by c. Additionally, it is possible that c0 performed work in its back segment, e.g., segment
comparison. Therefore, it is possible that there are upcoming non-candidates in
its back segment which are now holding incorrect information (e.g. the consumption of a cover token of c0 ). As this incorrect information could interfere with
the active cover tokens of c, also an active cleaning token is generated to reset
all non-candidates in the back segment of c0 , which is consumed by the first live
candidate it encounters. Lastly, this active cleaning token is also responsible for
destroying any other cleaning tokens it encounters, as the segment comparison
operations of c are the only ones still relevant to this segment.
Eventually, all active cover tokens will be consumed, completing the matching
originally described. Candidate c must now gather whether or not it has covered
another candidate, which is achieved as follows: instead of matching with the
final cover token, the final agent encountered, say a, consumes it and generates
a final cleaning token which travels in the direction of the cycle towards c. This
final cleaning token records whether or not a was a candidate. At every other
agent it visits, it resets the agent’s matched state and checks if the agent is
a covered candidate. Thus, when c receives this final cleaning token, one of
three cases occurs: (1) if the final cleaning token indicates that c covered some
other candidate, c remains a candidate; otherwise (2) if the final cleaning token
indicates that a was a candidate, c remains a candidate since its segments are
of equal length; otherwise (3) c’s front segment was too short, and it revokes
candidacy.
Coin Flipping and Candidate Transferral In order to realize the second
subphase as a local-control protocol, we need a token passing scheme for the
candidacy transferral, since a candidate is not able to transfer its candidacy in
an instant. Moreover, since candidates do not have a global view of the system,
they cannot know the position of their successor. The local-control protocol
consists of two different token passing schemes and candidates use one of the
two schemes dependent on the result of their coin toss.
A candidate c that flips a coin and receives heads sends a candidacy token along seg(c). However, c itself does not give up its candidacy immediately,

14

Daymude et al.

but (virtually) stays a candidate. This token is forwarded by non-candidates
in seg(c). A candidate c0 that receives a candidacy token sends a confirmation
token back along the segment of its predecessor. This confirmation token is forwarded back to c such that the virtual copy of c can finally give up its candidacy.
Moreover, there can be three different scenarios for a candidate c0 that receives a
candidacy token: (i) if c0 is not in subphase 2 of the protocol, it continues with its
desired behavior, (ii) if it received tails in the coin flip, c0 proceeds to the solitude
verification (with some caveats that will be explained in the next paragraph), or
(iii) if it received heads in the coin flip (i.e. c0 is a virtual candidate copy and
therefore also aims at transferring leadership), c0 will not give up its candidacy
(i.e., it will progress to solitude verification once it receives its own confirmation
token). Consequently, it is possible that even though a candidate receives heads,
it will not give up candidacy, because its predecessor also received heads.
In addition to the above mentioned token passing scheme for candidates
that receive heads, there is also an additional simple scheme for candidates that
receive tails. Before progressing to solitude verification a candidate c that receives
tails sends a token to pred(c) which is simply sent back by pred(c) and therefore
traverses seg(pred(c)) twice.
The first token passing scheme makes sure that candidacy tokens are not
forwarded infinitely often, but are eventually received by some candidate. This
behavior is due to the fact that candidacy is not revoked immediately after a
coin flip, i.e., virtual candidates are able to receive candidacy tokens.
The second scheme guarantees that a candidate c∗ with tails synchronizes
with its preceding candidate, i.e., c∗ waits for the return of its own token and a
preceding candidate that flipped heads has the chance to send its own candidacy
token to c∗ before c∗ progresses to solitude verification. This busy-waiting-like
behavior is needed due to the asynchronicity of the amoebot model which could,
in a worst-case scenario, prevent progress in the leader election process. To be
more specific consider a scenario with only two remaining candidates s, t with
equal segment lengths on a boundary. Leader election will not make any progress
if both candidates continuously get the same result from tossing a coin. In fact,
without the second scheme it is possible to enforce that both candidates always
get the same result. Imagine that candidate s gets heads, while t gets tails in
some round r. Since particles act in an asynchronous fashion we can assume that
t does not perform any action for any amount of time while s does. Without the
second scheme s can progress to solitude verification (which will fail), then start
segment comparison (which will not make any progress) and will flip a coin in
again. We continue this process of going through all the phases until s eventually
gets tails. Note that since t does not perform any action and due to the definition
of an asynchronous round, the round counter does not progress (i.e. the particle
system is still in round r) and we enforced that both candidates get the same coin
toss result. With the second scheme, s cannot immediately progress to solitude
verification, but waits for its token. Thereby, t is able to send its candidacy token
to s before s can flip a coin for the next time.

Leader Election and Shape Formation

15

Solitude Verification The local-control protocol for solitude verification is
based on the following simple observation: A candidate c is the only candidate
left on a cycle if and only if succ(c) = c. To allow the candidates to check this,
let each particle assign a unique identifier from {1, 2, 3} to each of its agents. It
is easy to see that succ(c) = c if and only if
1. c and succ(c) occupy the same node in Geqt , and
2. c and succ(c) have the same identifier.
Note that since a particle can hold multiple agents, Condition 2 is in fact necessary. It can be checked using a trivial token passing scheme. Checking Condition
1 requires a bit more effort: Intuitively, c enforces its own orientation on all
agents in its segment to establish a common coordinate system. For each agent
a in the segment consider the vector pointing from a to succ(a). The sum of
these vectors is (0, 0) if and only if c and succ(c) occupy the same node in Geqt ,
see Figure 4.

Fig. 4. An example of solitude verification: The candidate (shown slightly bigger) enforces a coordinate system (x and y arrows) on all agents. The agents determine the
vectors pointing to the succeeding agent in direction of the cycle (arrows and tuples at
nodes). Since the candidate is the only candidate on the cycle, the vectors add up to
(0, 0).

This algorithmic idea can be implemented as a local-control protocol using
token passing in the following way. We use two different types of tokens: matching
tokens and a unique activation token. The activation token is created by the
candidate c and traverses seg(c) four times. In its first pass, the activation token
moves forward (i.e., in direction of the cycle) from c to succ(c) and establishes
the common coordinate system among the agents. Also, whenever the activation
token is passed forward in the first pass by an agent a, that agent will create a
matching token which stores the vector pointing from a to succ(a) in the common
coordinate system. Once the activation token reaches succ(c), it initiates its
second pass in which it simply moves unhidered backwards (i.e., opposite to the
direction of the cycle) along seg(c) from succ(c) to c. In its third and fourth pass,
the activation token again moves forward from c to succ(c) and back. However,
in these two final passes it is not allowed to surpass any matching token.
The matching tokens move from their initial position forwards to succ(c) and
then backwards towards c. Every agent is only allowed to store one matching

16

Daymude et al.

token moving forward and one matching token moving backward (the direction
of movement is stored as part of a token). Furthermore, a matching token is never
allowed to surpass the activation token or another matching token. Whenever
an agent that holds two matching tokens (one for each direction) is activated,
that agent will try to match these tokens. In this matching, the agent compares
the vectors stored in the tokens coordinate-wise. If one vector has a 1 in a
coordinate while the other vector has a −1 in that coordinate, the agent will
change both these values to 0. Should a token be left with the vector (0, 0)
because of this matching process, it is deleted. Finally, the activated agent passes
on any remaining tokens if possible.
It is not hard to see that Condition 1 holds if and only if all matching tokens
are deleted in this process. Furthermore, c can easily distinguish whether all
matching tokens have been deleted: First, note that c will eventually be able
to delete or forward the matching token it created itself, so assume that c does
not hold that token anymore. If some matching token remains, that token will
reach c before the activation token returns from its fourth pass because the
activation token cannot surpass the matching token. On the other hand, if no
matching token remains, no such token can reach c before the activation token
returns from its fourth pass. Finally, note that the described process only stores
a constant amount of information in a token and every agent holds at most a
constant number of tokens at any time.
Inner Outer Boundary Test The last candidate of a cycle can decide whether
its cycle corresponds to an inner or the outer boundary as follows. A cycle
corresponding to an inner boundary has counter-clockwise rotation while a cycle
corresponding to the outer boundary has clockwise rotation, see Figure 2. The
candidate sends a token along the cycle that sums the angles of the turns the
cycle takes, see Figure 5. When the token returns to the candidate, its value

Fig. 5. The angle between the directions a token enters and exits an agent.

represents the external angle of the polygon corresponding to the cycle while
respecting the rotation of the cycle. So it is −360◦ for an inner boundary and
360◦ for the outer boundary. The token can represent the angle as an integer k

Leader Election and Shape Formation

17

such that the angle is k · 60◦ . Furthermore, to distinguish the two possible final
values of k it is sufficient to store the k modulo 5, so that the token only requires
3 bits of memory.
3.6

Linear Runtime with High Probability

The algorithm presented in Section 3.2 guarantees that a leader will be elected in
an expected linear number of rounds. A small modification of the algorithm can
lead to linear runtime with high probability (w.h.p.), without compromising its
correctness. For this we only need a slight modification of Algorithm 1: Subphase
1 needs to be executed twice, and in Subphase 2 a candidate c generates a
sequence of random bits b(c) = (b(c)1 , b(c)2 , . . .) that are compared with the
random bits of neighboring candidates instead of just flipping a single coin by
themselves. For two candidates c and c0 , b(c) < b(c0 ) if there is an i ≥ 0 so that
b(c)j = b(c0 )j for all j ≤ i and b(c)i+1 < b(c0 )i+1 .

Algorithm 2 Modified Leader Election
Execute Subphase 1 of Algorithm 1 twice
Subphase 2:
generate random bits for b(c) until b(c) 6= b(pred(c)) and b(c) 6= b(succ(c))
if b(c) < b(pred(c)) or b(c) < b(succ(c)) then
return not leader
Execute Subphase 3 of Algorithm 1

In a low-level implementation of Subphase 2, each candidate c continues to
produce random bits for b(c) and sends them in both directions in a pipelined
fashion until it learns that the competition on both sides is over, i.e., b(c) 6=
b(pred(c)) and b(c) 6= b(succ(c)). The competition is realized by pairing off the
bits from consecutive candidates at the agent of the segment between them where
they meet until a bit pair is different (if both bits are equal, they are deleted at
the agent), which is then reported back to the candidates.
As before, let li be the longest segment length before phase i, and let c be a
candidate such that |seg(c)| = li . Subphase 1 can only increase segment lengths
and hence after one execution of Subphase 1, c will not withdraw its leadership
candidacy. So after one application of Subphase 1 we have |seg(c)| ≥ li , and
also, |seg(pred(c))| ≥ li . After another application of subphase 1, various cases
can happen:
– Case 1: c is not a leader candidate any more. Then the segments of c and
pred(c) now belong to one candidate c0 , which means that |seg(c0 )| ≥ 2li .
– Case 2: c is still a leader candidate, but pred(c) is not a candidate any more.
Then the segments of pred(c) and pred(pred(c)) (by which we mean the

18

Daymude et al.

predecessor of pred(c) after the second application of Subphase 1) now belong to pred(pred(c)), and since |seg(pred(pred(c)))| ≥ |seg(pred(c))| before
pred(c) gave up its candidacy, after Subphase 1, |seg(pred(pred(c)))| ≥ 2li .
– Case 3: Both c and pred(c) are still candidates. Note that then, after the
second application of Subphase 1, also |seg(pred(pred(c)))| ≥ li . Since Subphase 2 ensures that of any two consecutive candidates, one of them will give
up its candidacy, either c or pred(c) will give up its candidacy in Subphase
2, which means that there will be a candidate c0 with |seg(c0 )| ≥ 2li .
Hence, in each phase the longest segment length is guaranteed to at least double, which means that the modified leader election algorithm is guaranteed to
terminate after at most log n phases. It is easy to show via Chernoff bounds
that in each phase and for each candidate c, O(log n) bits suffice w.h.p. so that
b(c) 6= b(pred(c)) and b(c) 6= b(succ(c)), which means that phase i takes at most
O(li+1 + log n) rounds w.h.p. Summing up these bounds over all phases results
in a runtime of O(Lmax ) w.h.p.
Note that while we are confident that the asynchronous local-control algorithm presented in the previous section performs close to the given simplified
analytical bound, it might require considerable effort to implement the modifications presented in this section as an asynchronous local-control protocol in such
a way that the given bound holds. The main issue is that our analysis requires
that the executions of Subphase 1 by Algorithm 2 are completely synchronized
among agents, which appears to be quite tricky to realize.

4

Line Formation in the Geometric Amoebot Model

In this section, we consider the line formation problem in the geometric amoebot
model. We assume that initially we have an arbitrary connected structure of
contracted particles with a unique leader. The leader is used as the starting
point for forming the line of particles and specifies the direction along which
this line will grow. As the line grows, every particle touching the line that is
already in a valid line position becomes part of the line. Any other particle
adjacent to the line becomes the root of a tree of particles. Every root aims at
traveling around the line in a clockwise manner until it joins the line. As a root
particle moves, the other particles in its tree follow in a worm-like fashion (i.e.,
via a series of handover operations)5 .
Before we give a detailed description of the algorithm, we provide some preliminaries. We distinguish for the state of a particle between idle, follower, root,
and retired (or halted). Initially, all particles are idle, except for the leader particle, which is always in a retired state. In addition to its state, each particle p
may maintain a constant number of flags in its shared memory. For an expanded
particle, we denote the node the particle last expanded into as the head of the
5

For a simulation video
http://sops.cs.upb.de .

of

the

Line

Formation

Algorithm

please

see

Leader Election and Shape Formation

19

particle and call the other occupied node its tail : In our algorithm, we assume
that every time a particle contracts, it contracts out of its tail.
The spanning forest algorithm (see Algorithm 3) is a basic building block
we use for shape formation problems. This algorithm aims at organizing the
particles as a spanning forest, where the particles that represent the roots of the
trees determine the direction of movement, whom the remaining particles follow.
Each particle p continuously runs Algorithm 3 until it retires. If particle p is a
follower, it stores a flag p.parent in its shared memory corresponding to the edge
adjacent to its parent p0 in the spanning forest (any particle q can then easily
check if p is a child of q). If p is the leader particle, then it sets the flag p.linedir
in the shared memories corresponding to two of its edges in opposite directions
(i.e., an edge i and the edge i + 3 (mod 6) in clockwise order), denoting that it
would like to extend the line through the directions given by these edges.

Algorithm 3 Line Formation Algorithm
SpanningForest (p):
Particle p acts as follows, depending on its current state:
idle:
If p is connected to a retired particle, then p becomes a root particle. Otherwise, if an adjacent particle p0 is a root or a follower, p sets the flag p.parent
on the shared memory corresponding to the edge to p0 and becomes a follower. If none of the above applies, it remains idle.
follower: If p is contracted and connected to a retired particle, then p becomes a
root particle. Otherwise, it considers the following three cases: (i) if p is
contracted and p’s parent p0 (given by the flag p.parent) is expanded, then
p expands in a handover with p0 , adjusting p.parent to still point to p0 after
the handover; (ii) if p is expanded and has a contracted child particle p0 ,
then p executes a handover with p0 ; (iii) if p is expanded, has no children,
and p has no idle neighbor, then p contracts.
root:
Particle p may become retired following CheckRetire (p). Otherwise, it
considers the following three cases: (i) if p is contracted, it tries to expand
in the direction given by LineDir(p); (ii) If p is expanded and has a child
p0 , then p executes a handover contraction with p0 ; (iii) if p is expanded and
has no children, and no idle neighbor, then p contracts.
retired: p performs no further action.
CheckRetire (p):
if p is a contracted root then
if p has an adjacent edge i to p0 with a flag p0 .linedir, where p0 is retired then
Let i0 be the edge opposite to i in clockwise order
p sets the flag p.linedir in the shared memory of edge i0
p becomes retired.
LineDir(p):
Let i be the label of an edge connected to a retired particle.
while edge i points to a retired particle do
i ← label of next edge in clockwise direction
return i

20

Daymude et al.

We have the following theorem, where work is defined as the number of
expansions and contractions executed by all particles in the system:
Theorem 3. Algorithm 3 correctly decides the line formation problem in worstcase optimal O(n2 ) work.
In order to prove Theorem 3, we first prove that the algorithm will eventually
correctly converge to a line in Geqt in Sections 4.1 and 4.2, and then show that
the algorithm terminates within worst-case optimal O(n2 ) work in Section 4.3.
4.1

Spanning Forest Formation

The first three lemmas demonstrate some properties that hold during the execution of the spanning forest algorithm and will be used in Section 4.2 to analyze
the complete algorithm, when we incorporate the check for retirement of particles according to the line formation problem, and the propagation of the line
direction.
The configuration of the system of particles at time t consists, for every
particle p, of the current state of p, including whether the particle is expanded
or contracted, any flags in p’s shared memory, the node(s) p occupies in G (given
by the relative position of p according to the other particles) at time t, as well
as the labeling of the bonds of p. Following the standard asynchronous model,
the system of particles progresses by performing atomic actions, each of which
affects the configuration of one or two particles. We say that followers and roots
particles are active. As specified in Algorithm 3, only followers can set the flag
p.parent.
Lemma 4. For every follower p, the node indicated by the flag p.parent is occupied by a non-idle particle.
Proof. Consider a follower p in any configuration during the execution of Algorithm 3. Note that p can only become a follower from an idle state, and once it
leaves the follower state it will not switch back to that state again. Consider the
first configuration c1 in which p is a follower. In the configuration c0 immediately
before c1 , p must be idle and it becomes a follower because of an active particle
p0 occupying the position indicated by p.parent in c0 . The particle p0 is still
adjacent to the edge flagged by p.parent in c1 . Now assume that p.parent points
to an active particle p0 in a configuration ci , and that p is still a follower in the
next configuration ci+1 that results from executing an action a. If a affects p and
p0 , the action must be a handover in which p updates its flag p.parent such that
p.parent may be moved to the edge that now connects p to p0 in ci+1 . If a affects
p but not p0 , it must be a contraction in which p.parent does not change and
still points to p0 . If a affects p0 but not p, there are multiple possibilities. The
particle p0 might switch from follower to root state, or from root to retired state,
or it might expand, none of which violates the lemma. Furthermore, p0 might
contract. If p.parent points to the head of p0 , p0 is still adjacent to the edge
flagged by p.parent in ci+1 . Otherwise, p is a child adjacent to the tail of p0 in ci

Leader Election and Shape Formation

21

and therefore the contraction must be part of a handover. As p is not involved
in the action, the handover must be between p0 and a third active particle p00 .
After the handover, p00 will occupy the position originally occupied by the tail of
p0 and hence p.parent points to p00 . Finally, if a affects neither p nor p0 , p.parent
will still point to p0 in ci+1 .
t
u
Based on Lemma 4, A(c) contains the same nodes as the nodes occupied in
Geqt by the set of active particles in c. For every expanded particle p in c, A(c)
contains a directed edge from the tail to the head of p, and for every follower p0
in c, A(c) contains a directed edge from the head of p0 to p0 .parent, if p0 .parent
is occupied by an active particle.
Lemma 5. The graph A(c) is a forest, and every connected component of idle
particles is connected to a non-idle particle.
Proof. Since in an initial configuration c0 all particles are idle, except for the
leader particle which is retired, and the particle system is connected, the lemma
holds trivially for c0 . Now assume that the lemma holds up to a cerain configuration ci and consider a connected component of idle particles. If an idle particle
p in the component is activated, it may stay idle or become an active particle.
The former case does not affect the configuration. So consider the latter case.
If p has changed into a follower state, it joins an existing tree, and in case p
has become a root, it forms a new tree in A(ci+1 ). In either case, A(ci+1 ) is a
forest and the connected component of idle particles that p belongs to in ci is
either non-existent or connected to p in ci+1 . If a follower or a root particle p
that is connected to the idle component is activated, it cannot contract unless
by a handover with another active particle, which implies the nodes occupied
by p will remain occupied by the active particles. While such a handover can
change the parent relation among the nodes, it cannot violate the lemma. If an
active particle p is activated and has no child p0 such that p0 .parent is the tail of
p and p is not connected to the idle component, it contracts and its contraction
does not disconnect any particle in its tree in A(ci+1 ). Moreover, an expansion
of an active particle or changing its state to a root, if it is a follower, or to a
retired particle, if it is a root particle, does not violate the lemma too. Finally,
if a retired particle is activated, it does not move. Therefore, the lemma holds
at configuration ci+1 .
t
u
The following lemma shows that the spanning forest always makes progress,
by showing that as long as the roots keep moving, the remaining particles will
eventually follow.
Lemma 6. An expanded particle eventually contracts.
Proof. Consider an expanded particle p in a configuration c. Note that p must be
active. If there is an enabled action that includes the contraction of p, that action
will remain enabled until p eventually contracts when it is selected within the
current or next round. So assume that there is no enabled action that includes the

22

Daymude et al.

contraction of p. According to Lemma 5 and the transition rule from idle to some
active state, at some point in time there will be no idle particles in the system. If
the contraction of p becomes part of an enabled action before this happens, p will
eventually contract. So assume that all particles are non-idle but still p cannot
contract. If p has no children, the isolated contraction of p is an enabled action
which contradicts our assumption. Therefore, p must have children. Furthermore,
at least one child p0 of p must have a p0 .parent flag on the edge connecting to
the tail of p and all children of p must be expanded, as otherwise p could again
contract as part of a handover. If p0 would contract, a handover between p0 and
p would become an enabled action and p would eventually contract. Hence p0
also cannot contract. Applying this argument recursively, we identify to p0 a set
of expanded particles forming a branch of a tree in A(c), until we reach a leaf
q of A(c) (by Lemma 5). Obviously q will contract next time it is activated.
Therefore, we found a sequence of expanded particles that starts with p0 and
ends with a particle that eventually contracts, implying that it in the sequence
to contract and so on. the contraction of p will become part of an enabled action
and therefore p will eventually contract.
t
u
4.2

Line Formation

Now, we can show that the algorithmic primitives as developed in the Section 4
solve the line formation problem.
Theorem 4. Algorithm 3 correctly decides the line formation problem.
Proof. We need to show that the algorithm terminates and that when it does,
the formed shape is a straight line in Geqt . As a result of Lemma 5 and due to the
transition rules from idle to active states, every idle particle p eventually switches
out of idle state. According to the algorithm proposed for the line formation
problem, if p is adjacent to a retired particle, it becomes a root and moves in
clockwise order around the current line structure, until it eventually reaches one
of the valid positions that can extend the line and becomes retired (halted). By
contradiction, assume p never becomes retired. Since the number of particles
is bounded (and therefore the size of the current line structure is bounded), p
cannot move around the line structure indefinitely (since in this case p must
occupy a valid extension position and would have become retired). Thus there
must be an infinite number of configurations ci where p has a particle blocking
its desired path around the current line structure. Let p0 be the last particle
p sees as its clockwise neighbor over the line structure. Since p0 is touching a
retired particle, p0 will become a root particle within at most two rounds, and will
stay connected to the line structure and always attempt to move in a clockwise
manner, p0 is well-defined. Applying the same argument we had for p inductively
to p0 results an infinite sequence of roots adjacent to the line structure that never
touch a valid spot pointed by one of p.linedir flags of an already retired particle
belong to the line. This is a contradiction, since the current line structure (the
current number of retired particles) is bounded. Therefore, every root eventually
changes into a retired state.

Leader Election and Shape Formation

23

According to spanning forest construction, as long as the roots keep moving
the followers eventually follow. From Algorithm 3, every follower in the neighborhood of a retired particle becomes a root. For every root q with at least one
follower child, let c be the first configuration when q becomes retired. If q still
has any child in c then all of its children p become roots. Applying this argument
recursively together with the already proven fact that every root eventually becomes retired we will reach to a configuration such that there exists no root with
a follower child which proves that eventually every follower becomes a root.
Putting it all together, eventually all particles become retired and the algorithm terminates. Note that it also follows from the argument above that the set
of retired particles at the end of the algorithm forms a connected structure (since
the particles start from a connected configuration and never get disconnected
through the process). The connected structure must form a line, since a root
particle may only become retired once it is contracted and occupies a valid spot
extending the current line.
t
u
4.3

Performed Work

Finally, we evaluate the performance of our algorithm in terms of the number
of movements (expansions and contractions) of the particles, i.e., the total work
performed by the algorithm.
Lemma 7. The worst-case work required by any algorithm to solve the line formation problem is Ω(n2 ).
Proof. Assume the initial configuration
of the set of particles forms a connected
√
structure of diameter at most 2 n + 2 (e.g., if it forms a hexagonal or square
shape in Geqt ). Since the line has diameter equal to n − 1, there must
√ exist
1
some particle that will need to traverse a distance of
at
least
n − 3, a
n
−
2
2
√
second particle that will traverse a distance of n − 2 n − 4, etc., irrespective of
the algorithm
used. The number of particle movements incurred will be at least
P 12 n−2√n−1 1
√
2
(
t
u
i=1
2 n − 2 n − i − 2) = Θ(n ).
In the following, we will show a matching upper bound:
Theorem 5. Algorithm 3 terminates in O(n2 ) work.
Proof. To prove the upper bound, we simply show that every particle executes
O(n) movements. The theorem then follows. Consider a particle p. While p is in
an idle or a retired state, it does not move. Let c be the first configuration when
p becomes a follower. Consider the directed path in A(c) from the head of p to
its root p0 . There always is a such a path since every follower belongs to a tree in
A(c) by Lemma 5. Let P = (a0 , a1 , . . . , am ) be that path in A(c) where a0 is the
head of p and am is a child of p0 . According to Algorithm 3, p will follow P by
sequentially expanding into the nodes a0 , a1 , . . . , am . The length of this path is
bounded by 2n and, therefore, the number of movements p executes while being
a follower is O(n). Once p becomes a root, it only performs expansions and

24

Daymude et al.

contractions around the currently constructed line structure, until it reaches one
of the valid positions on the line. Since the total number of retired particles is at
most n, this leads to an additional O(n) movements by p. Therefore, the number
of movements a particle p executes is O(n), which concludes the theorem.
t
u

5

Self-stabilizing Leader Election and Shape Formation

Consider the variant of the geometric amoebot model in which faults can occur
that arbitrarily corrupt the local memory of a particle. Recall that for an algorithm to solve the leader election problem in a self-stabilizing manner, it has to
satisfy the following requirements: First, from any initial system state (in which
the particle structure is connected) the particle system eventually reaches a final
system state while preserving connectivity, i.e., eventually a unique leader will
be established. Second, once a final system state is reached, the system has to
remain in that state as long as no faults occur. Analogous requirements have to
be satisfied for self-stabilizing shape formation.
Our leader election algorithm can be extended to a self-stabilizing leader
election algorithm with O(log∗ n) memory using the results of [6, 30] (i.e., we
use their self-stabilizing reset algorithm on every cycle in order to recover from
failure states). However, it is not possible to design a self-stabilizing algorithm for
the line formation. The reason for this is that even a much simpler problem called
movement problem cannot be solved in a self-stabilizing manner. It is easy to see
that if the movement problem cannot be solved in a self-stabilizing manner, then
also the line formation problem cannot be solved in a self-stabilizing manner.
In the movement problem we are given an initial distribution A of particles
that can be in a contracted as well as expanded state, and the goal is to change
the set of nodes occupied by the particles without causing disconnectivity. For
the ring of expanded particles it holds that for any protocol P there is an initial
state so that P does not solve the movement problem. To show this we consider
two cases: suppose that there is any state s for some particle in the ring that
would cause that particle to contract. In this case set two particles on opposite
sides of the ring to that state, and the ring will break due to their contractions.
Otherwise, P would not move any particle of the ring, so also in this case it
would not solve the movement problem in a self-stabilizing manner.

6

Conclusion

The algorithms presented for the geometric amoebot model can be extended for
the case that G is a different regular grid graph embedded in the two-dimensional
Euclidean plane. As future work, we would like to identify the minimum set of
key geometric properties that G must have in order for the proposed algorithms
to work.

Leader Election and Shape Formation

25

Acknowledgment
We would like to thank John Reif for the helpful discussions that led to the
realization of our algorithm with high probability guarantees.

References
1. L. M. Adleman. Molecular computation of solutions to combinatorial problems.
Science, 266(11):1021–1024, 1994.
2. Chrysovalandis Agathangelou, Chryssis Georgiou, and Marios Mavronicolas. A
distributed algorithm for gathering many fat mobile robots in the plane. In Proceedings of the 2013 ACM symposium on Principles of distributed computing, pages
250–259. ACM, 2013.
3. R. Ananthakrishnan and A. Ehrlicher. The forces behind cell movement. International Journal of Biological Sciences, 3(5):303–317, 2007.
4. D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in
networks of passively mobile finite-state sensors. Distributed Computing, 18(4):235–
253, 2006.
5. D. Arbuckle and A. Requicha. Self-assembly and self-repair of arbitrary shapes
by a swarm of reactive robots: algorithms and simulations. Autonomous Robots,
28(2):197–211, 2010.
6. Baruch Awerbuch and Rafail Ostrovsky. Memory-efficient and self-stabilizing network {RESET} (extended abstract). In Proceedings of the Thirteenth Annual ACM
Symposium on Principles of Distributed Computing, Los Angeles, California, USA,
August 14-17, 1994, pages 254–263, 1994.
7. L. Barriere, P. Flocchini, E. Mesa-Barrameda, and N. Santoro. Uniform scattering
of autonomous mobile robots in a grid. Int. Journal of Foundations of Computer
Science, 22(3):679–697, 2011.
8. A. Bhattacharyya, M. Braverman, B. Chazelle, and H.L. Nguyen. On the convergence of the hegselmann-krause system. CoRR, abs/1211.1909, 2012.
9. D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall. On the computational power
of DNA. Discrete Applied Mathematics, 71:79–94, 1996.
10. V. Bonifaci, K. Mehlhorn, and G. Varma. Physarum can compute shortest paths.
In Proceedings of SODA ’12, pages 233–240, 2012.
11. Z. J. Butler, K. Kotay, D. Rus, and K. Tomita. Generic decentralized control for
lattice-based self-reconfigurable robots. International Journal of Robotics Research,
23(9):919–937, 2004.
12. B. Chazelle. Natural algorithms. In Proc. of ACM-SIAM SODA, pages 422–431,
2009.
13. Ho-Lin Chen, David Doty, Dhiraj Holden, Chris Thachuk, Damien Woods, and
Chun-Tao Yang. Fast algorithmic self-assembly of simple shapes using random
agitation. In DNA Computing and Molecular Programming, pages 20–36. Springer,
2014.
14. Moya Chen, Doris Xin, and Damien Woods. Parallel computation using active selfassembly. In DNA Computing and Molecular Programming, pages 16–30. Springer,
2013.
15. K. C. Cheung, E. D. Demaine, J. R. Bachrach, and S. Griffith. Programmable assembly with universally foldable strings (moteins). IEEE Transactions on Robotics,
27(4):718–729, 2011.

26

Daymude et al.

16. G. Chirikjian. Kinematics of a metamorphic robotic system. In Proceedings of
ICRA ’94, volume 1, pages 449–455, 1994.
17. Mark Cieliebak, Paola Flocchini, Giuseppe Prencipe, and Nicola Santoro. Distributed computing by mobile robots: Gathering. SIAM Journal on Computing,
41(4):829–879, 2012.
18. R. Cohen and D. Peleg. Local spreading algorithms for autonomous robot systems.
Theoretical Computer Science, 399(1-2):71–82, 2008.
19. S. Das, P. Flocchini, N. Santoro, and M. Yamashita. On the computational power
of oblivious robots: forming a series of geometric patterns. In Proceedings of 29th
ACM Symposium on Principles of Distributed Computing (PODC), 2010.
20. X. Defago and S. Souissi. Non-uniform circle formation algorithm for oblivious
mobile robots with convergence toward uniformity. Theoretical Computer Science,
396(1-3):97–112, 2008.
21. E. D. Demaine, M. J. Patitz, R. T. Schweller, and S. M. Summers. Self-assembly of
arbitrary shapes using rnase enzymes: Meeting the kolmogorov bound with small
scale factor (extended abstract). In Proceedings of STACS ’11, pages 201–212,
2011.
22. Zahra Derakhshandeh, Shlomi Dolev, Robert Gmyr, Andréa W. Richa, Christian
Scheideler, and Thim Strothmann. Brief announcement: Amoebot — a new model
for programmable matter. In SPAA, 2014.
23. David Doty. Theory of algorithmic self-assembly. Communications of the ACM,
55(12):78–88, 2012.
24. Paola Flocchini, David Ilcinkas, Andrzej Pelc, and Nicola Santoro. Computing
without communicating: Ring exploration by asynchronous oblivious robots. Algorithmica, 65(3):562–583, 2013.
25. Paola Flocchini, Giuseppe Prencipe, Nicola Santoro, and Peter Widmayer. Arbitrary pattern formation by asynchronous, anonymous, oblivious robots. Theoretical
Computer Science, 407(1):412–447, 2008.
26. T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss. Self organizing robots based
on cell structures - cebot. In Proceedings of IROS ’88, pages 145–150, 1988.
27. Jacob Hendricks, Matthew J Patitz, and Trent A Rogers. Replication of arbitrary hole-free shapes via self-assembly with signal-passing tiles. arXiv preprint
arXiv:1503.01244, 2015.
28. T.-R. Hsiang, E. Arkin, M. Bender, S. Fekete, and J. Mitchell. Algorithms for
rapidly dispersing robot swarms in unknown environments. In Proceedings of the
5th Workshop on Algorithmic Foundations of Robotics (WAFR), pages 77–94, 2002.
29. Alon Itai and Michael Rodeh. Symmetry breaking in distributive networks. In 22nd
Annual Symposium on Foundations of Computer Science, Nashville, Tennessee,
USA, 28-30 October 1981, pages 150–158, 1981.
30. Gene Itkis and Leonid A. Levin. Fast and lean self-stabilizing asynchronous protocols. In 35th Annual Symposium on Foundations of Computer Science, Santa Fe,
New Mexico, USA, 20-22 November 1994, pages 226–239, 1994.
31. S. Kernbach, editor. Handbook of Collective Robotics – Fundamentals and Challanges. Pan Stanford Publishing, 2012.
32. P. Kling and F. Meyer auf der Heide. Convergence of local communication chain
strategies via linear transformations. In Proceedings of the 23rd ACM Symposium
on Parallelism in Algorithms and Architectures, pages 159–166, 2011.
33. K. Li, K. Thomas, C. Torres, L. Rossi, and C.-C. Shen. Slime mold inspired path
formation protocol for wireless sensor networks. In Proceedings of ANTS ’10, pages
299–311, 2010.

Leader Election and Shape Formation

27

34. J. McLurkin. Analysis and Implementation of Distributed Algorithms for MultiRobot Systems. PhD thesis, Massachusetts Institute of Technology, 2008.
35. Matthew J Patitz. An introduction to tile-based self-assembly and a survey of
recent results. Natural Computing, 13(2):195–224, 2014.
36. M. Rubenstein and W. Shen. Automatic scalable size selection for the shape of a
distributed robotic collective. In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent
Robots and Systems (IROS), 2010.
37. Michael Rubenstein, Alejandro Cornejo, and Radhika Nagpal. Programmable selfassembly in a thousand-robot swarm. Science, 345(6198):795–799, 2014.
38. J. E. Walter, J. L. Welch, and N. M. Amato. Distributed reconfiguration of metamorphic robot chains. Distributed Computing, 17(2):171–189, 2004.
39. E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman. Design and self-assembly of
two-dimensional dna crystals. Nature, 394(6693):539–544, 1998.
40. Damien Woods. Intrinsic universality and the computational power of selfassembly. In Turlough Neary and Matthew Cook, editors, Proceedings Machines,
Computations and Universality 2013, MCU 2013, Zürich, Switzerland, September
9-11, 2013., volume 128 of EPTCS, pages 16–22, 2013.
41. Damien Woods, Ho-Lin Chen, Scott Goodfriend, Nadine Dabby, Erik Winfree, and
Peng Yin. Active self-assembly of algorithmic shapes and patterns in polylogarithmic time. In ITCS, pages 353–354, 2013.
42. M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll, H. Lipson, E. Klavins, and G. S.
Chirikjian. Modular self-reconfigurable robot systems. IEEE Robotics Automation
Magazine, 14(1):43–52, 2007.

Brief Announcement: Bounded Wait-Free Implementation
of Optimally Resilient Byzantine Storage without
(Unproven) Cryptographic Assumptions
Amitanand S. Aiyer

Lorenzo Alvisi

Rida A. Bazzi

University of Texas at Austin

University of Texas at Austin

Arizona State University

anand@cs.utexas.edu

lorenzo@cs.utexas.edu

bazzi@asu.edu

Categories and Subject Descriptors

(ii) limits the size of the messages sent by the servers to
the readers: the size of these messages is bound by a constant times the logarithm of the number of write operations
performed in the system—or, equivalently, by a constant
times the size of a timestamp. Unfortunately, this solution
allows messages sent to writers to be as large as the maximum number of potential readers in the system, even during
times when the number of actual readers is small. Recently,
and concurrently with our work, a wait-free atomic solution
that requires not more than 3f + 1 servers was proposed,
but that solution requires unbounded storage, message of
unbounded size, and an unbounded number of messages per
read operation [6].
None of these solutions consider Byzantine readers. Considering Byzantine behavior of readers is relevant because
wait-free atomic solutions require that readers write to servers [5].
All existing work that considers Byzantine readers uses cryptographic techniques based on unproved number-theoretic
assumptions [9, 4].
In this work, we address two fundamental questions left
unanswered by previous works:

C.2.4 [Distributed Systems]: Distributed Applications;
D.4.2 [Storage Management]: Distributed memories; D.4.5
[Reliability]: Fault-tolerance

General Terms
Algorithms, Reliability, Security

Keywords
Atomic, Byzantine, Fault Tolerance, Replication, Wait-Free,
Bounded, Information-Theoretic

1.

INTRODUCTION

Distributed storage systems in which servers are subject
to Byzantine failures have been widely studied. Results vary
in both the assumptions made about the system model, and
the semantics of the storage implementation. The system
parameters include the number of clients (readers and writers), the synchrony assumptions, the level of concurrency,
the fraction of faulty servers, and the behavior of faulty
clients. In the absence of synchrony assumptions, it is known
that atomic [8] read and write semantics are possible, but
stronger semantics are not [7].
We present the ﬁrst Bounded, atomic, wait-free implementation of a Byzantine storage system that is optimally resilent. Further, our implementation can tolerate Byzantine
readers without making unproven cryptographic assumptions as required for PKI.
We consider solutions in an asynchronous system of n
servers that do not communicate with each other (non communicating servers) and in which up to f servers are subject
to Byzantine failures (f -resilient), any number of clients can
fail by crashing (wait-free), and readers can be subject to
Byzantine failures. Systems in which servers do not communicate with each other are interesting because such solutions
can be relatively easily translated into solutions in a shared
object model. Also, solutions that depend on communication between servers tend to have higher message complexity, quadratic in the number of servers [10, 4].
In the non-communicating servers model, the best previous solution that provides wait-free atomic semantics requires 4f + 1 servers [3]. That solution (i) requires clients
and servers to exchange a ﬁnite number of messages and

• Is the additional cost of f replicas over the optimal
for unbounded solutions required to achieve a bounded
wait-free solution?
• Is the use of cryptographic techniques required to tolerate Byzantine readers?
We answer both questions in the negative. We show that tolerating Byzantine readers can be achieved with informationtheoretic guarantees and without the use of unproven numbertheoretic assumptions. We also show that a bounded waitfree implementation of a distributed storage with atomic
semantics is possible for n = 3f + 1 (which is optimal). Our
solution also bounds the size of messages sent to writers—a
signiﬁcant improvement over Bazzi and Ding’s non-optimal
solution [3].

2. MODEL/ASSUMPTIONS
The system consists of a set of n replicas (servers), a set of
m writers and a set of readers. Readers and writers are collectively referred to as clients. Clients have unique identiﬁers
that are totally ordered. When considering boundedness of
the sizes of messages, we assume that a read operation in
the system can be uniquely identiﬁed with a ﬁnite bit string,
otherwise any message sent by a reader can be unbounded
in size. The identiﬁer consists of a reader identiﬁer and a
read operation tag. Similarly write operations are identiﬁed

Copyright is held by the author/owner(s).
PODC’07, August 12–15, 2007, Portland, Oregon, USA.
ACM 978-1-59593-616-5/07/0008.

310

rounds of communication between writers and servers. These
rounds occur in parallel with the ﬁrst two rounds of the write
protocol and no server receives a total of more than two messages across the three rounds. In the ﬁrst round, the writer
estimates the number of concurrent readers; in the second
and third rounds it determines their identities.

by the writer identiﬁer and the timestamp of the value being
written. Since timestamps are non-skipping [2], writes can
also be represented by ﬁnite strings.
Clients execute protocols that specify how read and write
operations are implemented. We assume that clients do not
start a new operation before ﬁnishing a previous operation.
We assume that up to f servers may deviate arbitrarily from
the speciﬁed protocol (Byzantine) and that the remaining
(n−f ) servers are correct. We require that the total number
of servers n be at least 3f + 1.
We assume that messages cannot be spoofed. While such
an assumption can be enforced in practice using cryptographic techniques that rely on unproven assumptions, such
techniques are not needed to enforce our requirement. We
assume FIFO point-to-point asynchronous channels between
clients and servers. Servers do not communicate with other
servers.
We assume that writers can only fail by crashing. If readers are benign, it is suﬃcient to have authenticated channels
between the writer and the servers. For tolerating Byzantine readers, we require an additional assumption that the
channels between the servers and the writers are private.
The probability that a given read operation by a Byzantine
reader improperly writes back a value is 2−k where k is a
security parameter. We choose k to be suﬃciently large so
that the probability of failure for all operations is small. If
k = o + k bits, where o is the number of bits required to
represent one operation, then the system failure probability

is 2−k .
By choosing an appropriate security parameter, our probability of failure can be made negligibly small. Schemes
based on public key cryptography, in the best case, also
suﬀer from this negligible small probability of error. If the
unproven assumptions that they are based upon do not hold,
their probability of error can be signiﬁcantly larger.

3.

Tolerating Byzantine readers.
Our protocols implement lightweight write backs to tolerate Byzantine readers [5]. To complete a read operation, the
reader is only required to write back the timestamp information associated with a value that is being read, instead of the
entire value. Thus the servers only need to verify that the
corresponding write operation has made enough progress, to
accept a write back.
We use write back throttling combined with secret sharing to verify that the write operation has made suﬃcient
progress. The idea is to associate a random secret with each
write and share the secret among the servers in such a way
that it can only be reconstructed if enough servers reveal
their shares. By requiring that a correct server only divulge
its share if the write has made suﬃcient progress, we use a
reader’s ability to reconstruct the secret as a proof that the
write operation has made suﬃcient progress. By using secret sharing, we avoid relying on unproven number theoretic
assumptions and achieve information-theoretic guarantees.

4. REFERENCES
[1] Amitanand S. Aiyer, Lorenzo Alvisi, and Rida A.
Bazzi. Lightweight writeback for byzantine storage
systems. Technical Report TR-07-13, University of
Texas at Austin, Department of Computer Sciences,
March 2007.
[2] Rida A. Bazzi and Yin Ding. Non-skipping
timestamps for byzantine data storage systems. In
DISC ’04, pages 405–419. Springer-Verlag, 2004.
[3] Rida A. Bazzi and Yin Ding. Bounded wait-free
f-resilient atomic byzantine data storage systems for
an unbounded number of clients. In DISC ’06, pages
299–313. Springer-Verlag, 2006.
[4] Christian Cachin and Stefano Tessaro. Optimal
resilience for erasure-coded byzantine distributed
storage. In DSN, pages 115–124, Washington, DC,
USA, 2006. IEEE Computer Society.
[5] R. Fan and N. Lynch. Eﬃcient replication of large
data objects. In DISC ’03, volume 2848 of LNCS,
pages 75–91, October 2003.
[6] Rachid Guerraoui and Marko Vukolic. Reﬁned
Quorum Systems. Technical Report
LPD-REPORT-2007-001, EPFL, 2007.
[7] Maurice Herlihy. Wait-free synchronization. ACM
Transactions on Programming Languages and
Systems, 13(1):124–149, January 1991.
[8] Leslie Lamport. On interprocess communication. part
i: Basic formalism. Distributed Computing,
1(2):77–101, 1986.
[9] Barbara Liskov and Rodrigo Rodrigues. Byzantine
clients rendered harmless. In DISC 2005, pages
311–325. Springer-Verlag, 2005.
[10] Jean-Philippe Martin, Lorenzo Alvisi, and Michael
Dahlin. Minimal byzantine storage. In DISC ’02,
pages 311–325. Springer-Verlag, 2002.

METHODOLOGY

To achieve our results, we reﬁne existing techniques and
introduce some new techniques. The ideas we reﬁne include
concurrent-reader detection and write-back throttling, originally proposed in the atomic wait-free solution of Bazzi and
Ding [3]. In what follows we give a high level overview of the
new techniques we introduce. A more detailed description
with protocols and correctness proofs can be found in [1].

Increasing resiliency.
We increase the resiliency of our solution by introducing a
new way by which a reader selects the timestamp of the value
it will try to read. Instead of choosing the f + 1’st largest
among the received timestamps, in our protocol the reader
chooses the 2f + 1’st smallest. We show that this new way
of selecting the timestamp is necessary to guarantee safety,
i.e. atomic semantics. Indeed, the f + 1 largest timestamp
worked well for n = 4f + 1 simply because, for that value
of n, the f + 1 largest received timestamp coincides with
the 2f + 1 smallest. We guarantee the liveness of our new
selection process by having the reader continuously update
the value of the 2f + 1’st smallest timestamp as it receives
responses from new servers.

Bounding message sizes to writers.
We bound the sizes of messages sent to servers using three

311

