Venting Weight: Analyzing the Discourse of an Online Weight Loss Forum
Lydia Manikonda1 , Heather Pon-Barry2 , Subbarao Kambhampati1 , Eric Hekler3
David W. McDonald4
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University
2
Mount Holyoke College
3
School of Nutrition and Health Promotion, Arizona State University
4
Human Centered Design & Engineering, University of Washington
lmanikon@asu.edu, ponbarry@mtholyoke.edu, {rao, ehekler}@asu.edu, dwmc@uw.edu
1

Abstract
Online social communities are becoming increasingly popular platforms for people to share information, seek emotional support, and maintain accountability for losing weight.
Studying the discourse in these communities can offer insights on how users benefit from using these applications.
This paper presents an analysis of language and discourse
patterns in forum posts by users who lose weight and keep it
off versus users with fluctuating weight dynamics. In contrast
to prior studies, we have access to the weekly self-reported
check-in weights of users along with their forum posts. This
paper also presents a study on how goal-oriented forums are
different from general online forums in terms of language
markers. Our results reveal differences about how the types
of posts made by users vary along with their weight-loss
patterns. These insights are closely related to the power dynamics of social interactions and can enable better design of
weight-loss applications thereby contributing to a healthy society.

1

Introduction

Obesity is a major public health problem; the number of
people suffering from obesity has risen globally in the last
decade (Ogden et al. 2014). The Centers for Disease Control and prevention (CDCP) defined an obese adult (http:
//www.cdc.gov/obesity/adult/defining.html) as a
person with a body mass index (BMI) of 30 or higher. Many
obese people are trying to lose weight as diseases such as
metabolic syndromes, respiratory problems, coronary heart
disease, and psychological challenges are closely associated with obesity (Must et al. 1999; Ngu 2012). Researchers
have been trying to understand how certain factors are affecting the weight loss as large number of over-weight people are trying to lose weight and some others are trying to
avoid gaining weight. Internet services are gaining popularity to support weight loss as they provide users with the
opportunities to seek information by asking questions, answering questions, sharing their experiences and providing
emotional support where people feel more comfortable by
openly expressing their problems and concerns (Ballantine
and Stephenson 2011).
Copyright ¬© 2016, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Social media tools like weblogs, instant messaging platforms, video chat, social networks, online discussion forums, are reengineering the healthcare sector (Hawn 2009).
Especially, social media is a promising tool for studying
public health like tracking flu infections (Lamb, Paul, and
Dredze 2013), studying post-partum depression (De Choudhury, Counts, and Horvitz 2013), dental pain (Heaivilin et al.
2011), etc. Tools like online discussion forums make it easier to find health-related information while at the same time
provides support by maintaining accountability and some
of the popular works like (Black, But, and Russell 2010)
proved that weight loss can be supported through online interactions. Hence, studying the online discussion forums can
help identify the people at risk who need more support and
provide them access to appropriate services and support.
In this paper, we explore the weight loss patterns of users
who participate in online discussions and ground truth in
terms of the weekly check-in weights of users. We perform different analyses on the users‚Äô language in correlation to their weight loss dynamics. From the overall dataset
we identify two preliminary patterns of weight dynamics:
(1) users who lose weight and successfully maintain the
weight loss (i.e., from one week to the next, weight is lost
or weight remains the same) and (2) users whose weight
pattern fluctuates (i.e., from one week to the next, weight
changes are erratic). While there are many possible groupings that we could have utilized, we chose this grouping
because of the known problems with ‚Äúyo-yo‚Äù dieting (diet
that leads to cyclical loss and gain of weight) compared
to a more steady weight-loss (Brownell and Rodin 1994;
Hekler et al. 2014). Our work is novel in terms of automating
the language analysis by handling a bigger dataset and can
help classify the user type based on the language efficiently.
As a follow-up work, linguistic insights are explored which
distinguish goal-oriented forums from general forums.
Our research contributions in this paper are divided into
two main sections where each focuses on a broader perspective as described below:
1. How does the language of users vary within the weight
loss forum based on their patterns of weight loss. Specifically, to understand the patterns of asking questions, using
a specific sentiment, politeness and making excuses.
2. Are there any interesting insights about the linguistic sig-

nals that makes a goal-oriented forum such as a weightloss forum different from other general online forums.
Our analysis resulted in interesting insights as below:
1. users who lose weight in a fluctuating manner are more
active on the discussion forums.
2. users losing weight in a fluctuating manner appear to talk
about themselves as they use higher number of personal
pronouns and adverbs.
3. users of non-increasing weight loss pattern mostly reply
to the posts made by other users and fluctuating users post
more questions.
4. posts from users of fluctuating weight loss pattern contain
more excuses.
5. politeness of posts seems to be uncorrelated with the
weightloss pattern.
6. users on goal-oriented forums contribute to a cohesive
thread of posts compared to users on general online forums which suffer from non-cohesiveness.
We believe that this research can bring forth the different variables related to people who need additional support
in terms of losing weight and thereby can stay healthy in
maintaining their weight. Also, we envision building personalized weight loss applications that can cater the needs of
individuals who need additional support. We hope that this
study will help in bringing more attention from the research
community to study online weight loss communities and understand both the constructive and destructive dimensions of
weight loss so that we can build a healthy society.

2

Related Work

There is a vast amount of literature about online forums ‚Äì statistical and language analysis of the discussion
threads (GoÃÅmez, Kaltenbrunner, and LoÃÅpez 2008), summarizing the discussions (Backstrom et al. 2013), measuring the success and identifying the factors that make users
participate (Kim 2000; Ludford et al. 2004) on these forums, addressing how the roles of users change (Yang et
al. 2010), understanding the lurking behavior and predicting lurkers (Preece, Nonnecke, and Andrews 2004), etc. Different fields like marketing (Bickart and Schindler 2001),
public health (Black, But, and Russell 2010), etc., considered online forums as influential sources of user information. Much of this literature focuses on studying the online
communities and their users from different linguistic and social networking perspectives. Little attention has been given
to analyze forums from the weight loss perspective.
However, most of the existing studies (Ballantine and
Stephenson 2011; Leahey et al. 2012; Das and Faxvaag
2014) on online weight loss discussion forums focused on
why people participate and how the social support can help
them to lose weight. These studies are conducted from the
perspective of medical and psychological domains, where
the data are collected via interviews or a small set of online forum data that are manually analyzed by human experts. Unlike the existing literature, our work considers the
weekly check-in weights of users along with their posts to

Figure 1: Example weight loss patterns from two individual users:
non-increasing (bottom line), and fluctuating (top line). The x-axis
ranges from the 1st through the 80th weekly check-in; the y-axis
shows the weight, measured in lbs.

understand the behavior of users who want to lose weight
and detect the variables that classify users who need additional support and service. Instead of choosing a small subset of a dataset and performing manual coding, our work
is novel in automating the language analysis by handling a
bigger dataset. Identifying and providing better assistance to
users who need help can also have a significant impact on
gaining the trust and confidence of users in these kinds of
services through better decision making.

3

Dataset

We obtained an anonymized text corpus of online discussion
forums from Fit Now, Inc. who developed a popular mobile
and web-based weight loss application. Along with the text
corpus, we also obtain weekly weight check-in data for a
subset of users. The entire corpus consists of eight different
forums that are subdivided into conversation topic threads.
Each thread consists of several posts made by different users.
The forum data in our corpus consists of 884 threads, with a
median length of 20 posts per thread. The posts were made
between January 1, 2010 and July 1, 2012. We identify the
subset of users for whom we have weight check-in data and
who made at least 25 weight check-ins during this time period. This results in a total of 2,270 users.
We partition the users into two groups based on their dynamic weight loss patterns: a non-increasing group and a
fluctuating group.
1. Non-increasing: These are the users who lose weight and
keep it off. For each week j, the user‚Äôs check-in weight
w j is less than or equal to their past week‚Äôs weight w j‚àí1 ,
within a small margin ‚àÜ. That is, w j ‚â§ (1 + ‚àÜ)w j‚àí1 .
2. Fluctuating: These are the users who do not lose weight.
If the difference between two consecutive weekly checkin weights do not follow the non-increasing constraint,
users are grouped into this category.
We empirically set ‚àÜ = 0.04 to divide the users in our

dataset into two groups of similar size. To illustrate the
two patterns of weight change, Figure 1 shows the weekly
weight check-ins of two individual users, one from each
group. This grouping is coarse, but is motivated by studies
(Kraschnewski et al. 2010; Wing and Phelan 2005) acknowledging that approximately 80% of people who set out to lose
weight are successful at long-term weight loss maintenance,
where successful maintenance is defined as losing 10% or
more of the body weight and maintaining that for at least
an year. In the future for further analysis, we aim to separate users less coarsely, e.g., users who maintain their weight
neither gaining nor losing weight, users who lose weight and
maintain it and finally, users who gain weight.
The main distinctive feature of this weight loss application is that users are encouraged to set goals to regularly log
their weight, diet, and exercise. For a subset of users, this
application included a weekly weight ‚Äúcheck-in‚Äù, an average of the user‚Äôs weight check-ins during the week, for the
January 1, 2010 through July 1, 2012 period. This allows us
to juxtapose the weekly weights of the users with their posts
on the discussion forums.

3.1

Characteristics of Online Community

This weight loss application helps users set a personalized
daily calorie budget, track the food they are eating, their exercise and log their weekly weight. It also helps users to stay
motivated by providing an opportunity to connect with other
users who want to lose weight and support each other. Example snippets from forum threads are shown below. The
‚ÄúCan‚Äôt lose weight!‚Äù thread demonstrates users supporting
each other and offering advice. The ‚ÄúSomeday I will‚Äù thread
highlights the complex relationship between text, semantics,
and motivation in the forums.
Example thread: ‚ÄúCan‚Äôt lose weight!‚Äù
User 1: ‚ÄúI gained over 30 lbs in the last year and am
stressed about losing it. I eat 1600 calories a day and
burn more than that in exercise, but I havent lost any
weight. I am so confused.‚Äù
User 2: ‚ÄúYou‚Äôve only been a member for less than 2
months. I suggest you relax. Set your program to 1
pound weight loss a week. Adjust your habits to something you can live with. . . long term.‚Äù
User 3: ‚ÄúYou sound just like me. I think your exercise
is good but maybe you are eating more than you think.
Try diligently logging everything you consume.‚Äù
User 1: ‚ÄúThanks for the suggestions! I am going to get
back to my logging.‚Äù
Example thread: ‚ÄúSomeday I will. . . ‚Äù
User 1: ‚ÄúDo a pull-up :-)‚Äù
User 2: ‚Äú. . . actually enjoy exercising.‚Äù
User 3: ‚ÄúSomeday I will stop participating in these forums, but obviously not today.‚Äù
User 4: ‚ÄúI hope you fail :-)‚Äù

4

Empirical Analysis of Weight Loss Forum

In this section, we present preliminary observations on how
the language and discourse patterns of forum posts vary with

respect to weight loss dynamics. As an initial step, part-ofspeech (POS) tagging is performed on all forum posts using
the Stanford POS Tagger (Toutanova et al. 2003).
Weight Pattern
# Total users
# Forum users
# Forum posts
Posts per user
Words per post

Non-increasing

Fluctuating

1127
29
99
3.5
49.1

1143
68
1279
18.2
77.3

Table 1: Statistics of users and forum posts.

From the weekly check-in data we identified the number of users and the number of posts from each weight-loss
pattern cluster which are shown in Table 1. In our dataset,
out of 1127 users who are expressing non-increasing weight
loss pattern (1143 fluctuating weight loss pattern) only 29 of
them (68 of them respectively) made atleast one post on the
discussion forums. We see that the average number of posts
by fluctuating users is greater than the average number of
posts by non-increasing users. Our data also suggest that the
posts made by non-increasing users are shorter compared to
those made by fluctuating users. Both these suggest the possible loss of social connectedness once users achieve their
goal.

4.1

Lexical Categories

Studies (Pennebaker, Mehl, and Niederhoffer 2003) show
that the language defines an individual and his/her behavior.
We use the measures that characterize the weight loss pattern by using the linguistic classes in posts made by these
users on the forum. Specifically, verbs, conjunctions, adverbs, personal pronouns and prepositions are considered
as shown in Table 2. We collected all the individual posts
made by all the users belonging to each weight loss pattern
and measured the average frequency of a linguistic class per
post. Fluctuating users appear to talk more about themselves
and interact with other individuals one-on-one as they are
using a relatively higher number of personal pronouns. Additionally, we observe that users who lose weight in a fluctuating manner use greater fraction of prepositions and adverbs. Adverbs are primarily used to tell how someone did
something which means these users who lose weight in a
fluctuating manner explain more about themselves, perhaps
in an attempt to seek more information.

4.2

Asking Questions

In order to build and maintain vibrant online communities, it
is very important to understand the complex ways in which
the members interact and how the communities evolve over
time. As a part of that, previous literature (Bambina 2007)
revealed that people on online health communities mainly
engage in two activities: (i) seeking information, and (ii)
getting emotional support. People usually ask questions or
just browse through the community forums to collect information. If we can understand how users post questions and

Weight Pattern
Non-increasing

Fluctuating

Ling. class

Mean

Med.

SD

Mean

Med.

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

3.85
3.44
2.21
4.94
5.44

2.0
3.0
1.0
3.0
4.0

4.11
3.51
2.87
5.42
5.40

6.27
4.53
3.24
8.65
9.67

4.0
3.0
2.0
6.0
6.0

6.81
5.04
3.74
8.59
10.51

Table 2: Results of statistical measures on linguistic class attributes; Med. refers to Median; SD refers to Standard Deviation

how the other members respond to those questions, it will
be very useful in developing personalized profiles of users
so that the system is able to help them get sufficient information even before they post any questions. Below is an example (paraphrased) showing how users ask and respond to
questions.
Example thread: ‚ÄúNew user‚Äù
User 1: ‚ÄúDid anyone upgrade to the premium app?
What do you like about it?‚Äù
User 2: ‚ÄúI upgraded to the premium. I LOVE the functionality to log food in advance. I can track and set
goals that are not related to weight like how much I
sleep, how much water I drink, etc.‚Äù
User 3: ‚ÄúI upgraded my account to premium too. I really liked the added features because it helped me keep
track of my steps and participate in challenges.‚Äù
We are interested in knowing whether these two types of
users are actively seeking information. We deem a forum
post to be a question if it meets one of these two conditions:
1. Wh-question words: If a sentence in the post starts with
a question word: Wh-Determiner (WDT), Wh-pronoun
(WP), Possessive wh-pronoun (WP$), Wh-adverb (WRB).
2. Punctuation: If the post contains a question mark (‚Äò?‚Äô).
We computed the ratio of question-oriented posts made by
each user in the two clusters. After averaging these ratio values across all the users in each cluster separately, we found
that on average, 32.6% of the posts made by non-increasing
users were questions (S tandardError(S E) = 0.061) while
37.7% of the posts made by fluctuating users were questions
(S E = 0.042). This shows that on an average fluctuating
users do post relatively larger number of questions than the
non-increasing users. We conjecture this could be a reflection of the fluctuating users‚Äô aim to seek more information
from the forum.

4.3

Sentiment of Posts

Analyzing the sentiment of user posts in the forums can
provide a suprisingly meaningful sense of how the loss of
weight impacts the sentiment of user‚Äôs post. In this analysis, we report our initial results on extracting the sentiments
of user‚Äôs posts. In order to achieve this, we utilize the Stanford Sentiment Analyzer (Socher et al. 2013). This analyzer

Figure 2: Proportion of sentiments for the two weight-loss patterns. For non-increasing users, percentage of posts with Positive, Neutral and Negative sentiments are: 22%, 46.5% and 31.5%
respectively. For fluctuating users, the percentage of posts with
Positive, Neutral and Negative sentiments are: 20.9%, 37.6% and
41.5% respectively.

classifies a text input into one of five sentiment categories
‚Äì from Very Positive to Very Negative. We merge the five
classes into three: Positive, Neutral and Negative (In future,
we may consider specific (health and nutrition) sentiment
lexicons).
We analyzed the sentiment of posts contributed by the
users from the two clusters. As shown in Figure 2, posts of
users belonging to the non-increasing cluster are more neutral whereas the posts made by users from the fluctuating
cluster are mainly of negative sentiment. This gives an interesting intuition that the users of fluctuating group might
require more emotional support as they use more negative
sentiment in their posts.

4.4

Politeness

Politeness is an important marker which often is a decisive
factor in whether interactions go well or cease (Rogers and
Lee-Wong 2003). Based on this metric, we can understand
if correlation exists between the politeness of posts made
by users and their weight loss pattern. Politeness according to the Webster‚Äôs Dictionary is to show good manners
towards others, as in behavior, speech, etc. We measured
how polite the posts are with respect to the weight loss pattern. We use the politeness classifier (Danescu-NiculescuMizil et al. 2013) that was constructed with a wide range
of domain-independent lexical, sentiment and dependency
features and there by operationalizes the key components of
politeness theory. It was proven that this classifier achieves
near human-level accuracy across domains (shown 83.79%
classification accuracy on in-domain wiki). Below are some
examples obtained after the classification of posts on this forum.
1. Polite text: ‚ÄúGood for you! I started out obese. Now,
Im not even overweight. Its a great feeling. Congrats

to you on your milestone!‚Äù
‚Ä¢ Polite Score: 0.870
‚Ä¢ Impolite Score: 0.130
2. Impolite text: ‚ÄúGrrrr.... I wish I could screen these
posts so that I dont even have to SEE those darn posts
about HCG or 500 calorie diets any more. :twisted:
And why did my search for Grumpy or Rant or
McRant come up empty?????? Grrrrr......‚Äù
‚Ä¢ Polite Score: 0.250
‚Ä¢ Impolite Score: 0.750
The results of the politeness analysis in Table 3 shows
that users on this weight-loss forum are polite overall. We
speculate that users on weight-loss related forums act polite
to get more information and emotional support. Further investigation is needed to conclude if users on goal-oriented
communities talk politely.
Type

Polite

Impolite

Non-increasing
Fluctuating

70.6%
75%

29.4%
25%

Table 3: Statistics of users and politeness percentage posts

4.5

Excuses

Literature (Bambina 2007) suggests that people use online
forums to maintain accountability. This application mainly
serves the user community to set goals and help the members
achieve those goals. It is important to understand if there is a
correlation between the weight loss pattern of the users and
the way they are making excuses as they are accountable for
not losing weight. In general, excuses are put forward when
people experience questions about their conduct or identity
in case of failing at an assigned task, violating a norm, etc.
Existing research (Deppe and Harackiewicz 1996)
demonstrates that people who are provided with the opportunity to make excuses do seem to perform better on a variety
of tasks. In this analysis, we wanted to verify if the hypothesis that users when given an opportunity to make excuses are
better at losing weight. Here is an example that shows how
User 1 posts excuses in a forum thread.
Example thread: ‚ÄúTrouble sticking to a diet‚Äù
User 1: ‚ÄúI am out of town with the family and making
the right food choices is impossible right now.‚Äù
User 2: ‚ÄúI think we all have to find our own motivation
and drive to succeed in weight loss. We just have to let
the motivation be louder than the excuses.‚Äù
To the best of our knowledge, there is no prior work on automatic classification of a post as an excuse or a non-excuse.
In this regard, we initially wanted to find if excuse classification is simply a special case of text-based categorization or
any special classification approaches need to be developed.
We performed experiments with two standard algorithms:
Naive Bayes Classification and Support Vector Machines,
which were shown to be effective in previous text categorization studies. In order to implement these two algorithms

we considered the standard bag-of-words where a document
d can be expressed in terms of the frequency of each of the
n features as d~ = ( f1 (d), ( f2 (d), . . . , ( fn (d)), fi (d) is the number of times feature i occurs in document d.
We also extended the Latent Dirichlet Allocation (Blei,
Ng, and Jordan 2003) (LDA) to build a classifier that also
uses majority class voting approach to provide labels to the
posts. Initially, LDA is used to extract the latent topic distribution over each of the posts present in the training dataset
that are already labeled as excuses and non-excuses. Later,
each post from the testing dataset is represented in this topic
space. For a given post in the testing dataset, the final class
label is the majority class of the k-closest points in the topic
space. The entire process of classifying a post as an excuse
or non-excuse is described in Algorithm 1.
Data: Labelled dataset ‚Äì Excuses (e f ) and Non-excuses
(ne f ); jth post ‚Äì a post with no class label
Result: Labelled Testing data
Œ∏e ne ‚Üê LDAestimation (e f ,ne f );
œÜene
‚Üê LDAin f erence (kth post, Œ∏iene );
k
L ‚Üê ‚àÖ;
for i := 1 to |e f | + |ne f | do
dist ‚Üê KLdivergence(Œ∏iene , œÜene
j );
append(L, dist);
end
label jth post ‚Üê max class(k-nearestpoints( f ullist));
Algorithm 1: Classification Approach
We utilized Weka (Hall et al. 2009) and svmlight (Joachims 1999) libraries to perform classification using Naive Bayes and SVM respectively. Based on the results
shown in Table 4, LDA-based supervised classifier outperforms the other two approaches and so we use it for measuring the correlation between the frequency of excuses posted
by users and their weight loss patterns.
Approach
Naive Bayes
SVM
LDA-based

Cond-1

Cond-2

57.8% (Uni)
50% (Uni)
65% (80-20 split)

63.1% (Uni + Bi)
46.15% (Uni + Bi)
50% (50-50 split)

Table 4: Classification results in terms of accuracy with different
approaches and conditions (Uni ‚Äì Unigrams; Bi ‚Äì Bigrams; 80-20
split ‚Äì 80% training and 20% testing; 50-50 split ‚Äì 50% training
and 50% testing data) and classifiers

We identified that 46% of the users who make at least one
post in the forum give excuses. If we consider the categorywise statistics, 48% of the users who lose weight in a nonincreasing pattern and 54% of the posts made by the users
of fluctuating weight loss pattern made excuses in at least
one post. It is surprising to notice that users exhibit excuse‚Äì
giving behavior on this weight loss community where accountability is one of its characteristics. Early detection of
these kinds of users and providing more assistance to help
them stay motivated can help lose weight. This kind of in-

tervention by these applications can help gain the trust of its
users.
Overall, in this section we have explored how the basic lexical classes, questions, sentiment, politeness and excuses are
correlated with the weight loss patterns of users. As we got a
good level of understanding about these associations, we can
now use these different attributes as a set of features in order
to predict whether a new user can lose weight or not, based
only on the language he/she is using on these forums. Automated classifier can be very beneficial to design effective
weight loss applications that can help users get additional
support. It can also help the users to pay more attention to
their diet and exercise to lose weight effectively.

5

Forums Studied

We used threads from two other popular online forums
that were used in (Biyani et al. 2012) ‚Äì 1) Trip Advisor New York City travel forum that contains travel related discussions for New York City and 2) Ubuntu forum dataset
that contains discussions about the ubuntu operating system.
There are multiple threads of discussions in both these forums and each thread has multiple posts by several users.
The dataset contains total number of 609 threads (6591 total posts) and 621 threads (3603 total posts) for Tripadvisor
and Ubuntu forums respectively. On an average, the thread
length in terms of the number of posts is 10 and 5 for the tripadvisor and ubuntu forums respectively. The average number of users in a thread on tripadvisor forum is 1.98 and
on ubuntu forum is 3.41. As stated in (Biyani et al. 2012),
ubuntu forums have technical discussions which are nonsubjective in nature where as trip advisor, a travel related
discussion forum has discussions which tend to be subjective.

5.2

Trip Advisor Ubuntu
Ling. class

Mean

SD

Mean

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

2.37
1.87
1.42
2.69
4.25

4.9
3.6
3.19
5.7
8.56

2.04
1.86
1.0
2.24
2.78

6.06
3.76
2.17
4.4
6.52

Table 5: Results of statistical significance tests on linguistic class
attributes for Trip Advisor and Ubuntu forums. For the results on
weight loss forum, please refer to Table 2. SD‚ÄìStandard Deviation

Comparison With General Forums

To contextualize this research, we want to understand if the
goal-oriented forums exhibit any specific traits compared to
the general forums. We define goal-oriented forums as the
forums associated with applications that help set goals while
building a social network of users who share similar goals.
Here, we present an analysis of how the type of forum can
affect the language used with a primary focus on understanding the lexical features and cohesiveness of the threads on
these forums.

5.1

Forum Name

Lexical Features

Lexical features like Part-of-Speech (POS) tags are obtained
for both the forums to understand the behavior of users in
terms of using different categories of words. Analysis similar to the earlier section was conducted using the Stanford
POS tagger to find the number of verbs, conjunctions, adverbs, personal pronouns and prepositions appearing in the
posts as shown in Table 5. As we compare these results with
the weight loss forum, we notice that users on these two forums don‚Äôt use as many personal pronouns and adverbs as
users on the weight loss forums. This is understandable as
users on weight loss forum have a primary goal to seek information while maintaining accountability.

5.3

Cohesion with Previous Posts

It is very important for the discussion forums to capture as
much participation as possible to reach their full potential.
When multiple conversations occur simultaneously, it is difficult to decide which utterance belongs to a specific conversation. Users on the online health forums mostly tend to
seek information and if the main topic is drifted to some
other topic, the main purpose of these discussion forums is
lost. Hence, it is important for the system to automatically
track non-cohesiveness in posts. Cohesion is the property of
a well-written document that links together sentences in the
same context. As a first step, we want to find out how similar a user‚Äôs post is with respect to the previous posts in a
thread from the weight loss forum. This can also help identify users in a given thread who elaborate on previous post
versus those who shift the topic.
Below is an example (paraphrased) showing cohesive post
made by the users on the weight loss forum.
Example thread: ‚Äúchanging life for a healthier self‚Äù
showing cohesive post by User 2
User 1: ‚ÄúDid you remove any commitments in your life
to make time to be healthier? If you have, was it a good
choice or did you regret it?‚Äù
User 2: ‚ÄúYes I‚Äôve done it and never regretted it.‚Äù
User 3: ‚ÄúTrying to do everything at once means doing
nothing - Georg Christoph‚Äù
User 2: ‚ÄúI‚Äôm not sure which entrepreneur said this but
focus only on what you need to do.‚Äù
We focus only on content words: verbs and nouns (partof-speech tags VB, VBZ, VBP, VBD, VBN, VBG, NN, NNP,
NNPS) and use WordNet (Miller 1995) to identify synonyms
of the content words. We compute similarity between the
current post and previous posts of other users in the thread in
terms of commonly shared verbs and nouns including synonyms. In our current analysis, we consider this similarity
score to be the measure of cohesion.
We consider all posts that are not thread-initial. To approximate whether a post is cohesive or not, we compare
the nouns and verbs of the current post to the list of nouns
and verbs (plus synonyms) obtained from the previous posts
of the thread. Our analysis (Table 6) on the three forums
‚Äì fit now data (weight loss forum), trip advisor and ubuntu

Data: Posts P1 , . . ., Pk‚àí1 , Pk
Result: CohS core(Pk )
set A ‚Üê ‚àÖ;
for i := 1 to (k ‚àí 1) do
[vbi , nni ] ‚Üê POS tagging (Pi );
set A ‚Üê set A ‚à™ [vbi , nni ];
set A ‚Üê set A ‚à™ synset(vbi ) ‚à™ synset(nni );
end
set B ‚Üê ‚àÖ;
[vbk , nnk ] ‚Üê POS tagging (Pk );
set B ‚Üê set B ‚à™ [vbk , nnk ];
set B ‚Üê set B ‚à™ synset(vbk ) ‚à™ synset(nnk );
A ‚à©set B |
CohS core(Pk ) ‚Üê |set|set
B|
Algorithm 2: Calculating the cohesive score of a post

finds that the threads on weight loss forum are more cohesive compared to the other two forums.
Fit Now data

Trip Advisor

Ubuntu

0.46
2.22

0.42
3.64

0.30
3.87

Cohesiveness
S.E (√ó10‚àí4 )

Table 6: Average value of Cohesiveness (along with Standard Error
(S.E) ) across all the threads in a given forum. Extreme values are:
0 ‚Äì non-cohesive; 1 ‚Äì cohesive

Overall, it is interesting to see that the goal-oriented forums
(like weight loss forums) have more cohesive threads compared to the general forums. Additionally, users on the goaloriented forums tend to post more information about themselves. In the future it will be worth studying if language
cues can help in predicting auto-tagging of threads to a specific type of forum. Studying other language metrics can also
help understand the contributions of different online forums
and their impact on the public.

6

Implications

The different language metrics studied in the two main
sections of this paper have a great potential to differentiate automatically between users who are struggling to lose
weight and the users who lost weight and are keeping it
off. There are for example, other existing technologies that
help users lose weight by ‚Äì providing incentives if they
lose weight (PACT http://www.gym-pact.com/), allowing other fitness applications to synchronize with the current
application to keep track of exercise (MyFitnessPal https:
//www.myfitnesspal.com/ ), posting questions while doing grocery shopping to find out the calorie content (Fooducate http://www.fooducate.com/ ), etc. We envision
tools that utilize the wealth of information present on the discussion forums along with the users activity to automatically
estimate the degree to which a user‚Äôs efforts will yield results. Predictions of success are not the end goals. The value
of these types of predictions are when they are leveraged
to generate alternative behaviors and actions that a user can
take to improve their chances of weight loss success. De-

signing systems that rely on features studied in this paper
could improve weight loss applications and thereby enhance
the quality of life.
People are taking advantage of these kinds of applications
as they can preserve their anonymity and provide genuine
information about their food intake, exercise levels, etc to
safely collect as much information as they can. Even though
the real identity can be hidden, it is important that the tools
being envisioned provide support in a very ethical manner.
On the other hand, deciphering the genuineness of the information provided is an area of research that can be worth
pursuing (Estrin 2014). On the whole, we believe that it is
important to understand the different attributes that affect the
behavior of individuals on the weight loss forums and help
them successfully lose weight. We hope that this work initiates further research on these types of discussion forums to
raise awareness about the different factors faced by individuals who are struggling to lose weight and thereby can help
develop policies that can support them in losing weight.

7

Conclusions and Future Work

In this paper, we analyzed how the online discussion forums
of weight loss applications can act as an important tool to
detect and identify the different metrics that are associated
with weight loss. As a first step, we identified the two types
of weight loss patterns exhibited by the users on this forum
and studied different factors like sentiment, politeness, excuses and questions. We took advantage of existing tools to
study these different factors and correlations between these
factors and the weight loss pattern. Specifically, this analysis reveals interesting insights about two populations of
users who lose weight differently. Users who lose weight
in a fluctuating manner are more active in these forums, give
more excuses, post more questions and the majority of their
posts contain negative sentiment. This shows the information seeking nature and suggests the possible need for more
support to these kinds of users. As a secondary focus, we
studied how the language metrics differ across goal-oriented
forums and general forums. We found that users of goaloriented forums usually contribute to a more cohesive posting threads and users on general forums tend not to reveal
much information about themselves.
Our analyses provides valuable insights on how user behavior within online weight loss forums might correlate with
the weight outcomes. These sorts of analyses, particularly
when replicated, could provide valuable insights for developing new technologies that might facilitate more effective
interactions about weight loss and can help gain trust of
users in these kinds of systems. It could also provide valuable insights for improving theories about behavior change.
Acknowledgments. This research is supported in part by a
Google research award, the ONR grants N00014-13-1-0176,
N00014-13-1-0519 and N00014-15-1-2027, and the ARO
grant W911NF-13- 1-0023.

References
Backstrom, L.; Kleinberg, J.; Lee, L.; and DanescuNiculescu-Mizil, C. 2013. Characterizing and curating con-

versation threads: Expansion, focus, volume, re-entry. In
ACM International Conference on Web Search and Data Mining, WSDM ‚Äô13, 13‚Äì22.
Ballantine, P. W., and Stephenson, R. J. 2011. Help me, I‚Äôm
fat! Social support in online weight loss networks. Journal of
Consumer Behaviour 10(6):332‚Äì337.
Bambina, A. D. 2007. Online Social Support: The Interplay
of Social Networks and Computer-Mediated Communication.
Bickart, B., and Schindler, R. M. 2001. Internet forums as influential sources of consumer information. J. of Int Marketing
15(3):31‚Äì40.
Biyani, P.; Bhatia, S.; Caragea, C.; and Mitra, P. 2012. Thread
specific features are helpful for identifying subjectivity orientation of online forum threads. In Proceedings of the 24th International Conference on Computational Linguistics, 295‚Äì
310.
Black, L. W.; But, J. J.; and Russell, L. D. 2010. The secret is out! supporting weight loss through online interaction.
Cases on online discussion and interaction: Experiences and
outcomes.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research
3:993‚Äì1022.
Brownell, K. D., and Rodin, J. 1994. The dieting maelstrom:
Is it possible and advisable to lose weight? American Psychologist 49(9):781‚Äì791.
Danescu-Niculescu-Mizil, C.; Sudhof, M.; Jurafsky, D.;
Leskovec, J.; and Potts, C. 2013. A computational approach
to politeness with application to social factors. In Proceedings of ACL.
Das, A., and Faxvaag, A. 2014. What influences patient participation in an online forum for weight loss surgery? IJMR
3(1).
De Choudhury, M.; Counts, S.; and Horvitz, E. 2013. Predicting postpartum changes in emotion and behavior via social
media. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ‚Äô13, 3267‚Äì3276.
Deppe, R. K., and Harackiewicz, J. M. 1996. Selfhandicapping and intrinsic motivation: Buffering intrinsic
motivation from the threat of failure. J. of Personality and
Social Psychology 70(4):868‚Äì876.
Estrin, D. 2014. Small data, where n = me. Commun. ACM
57(4):32‚Äì34.
GoÃÅmez, V.; Kaltenbrunner, A.; and LoÃÅpez, V. 2008. Statistical analysis of the social network and discussion threads in
slashdot. In Proceedings of the 17th International Conference
on World Wide Web, WWW ‚Äô08, 645‚Äì654.
Hall, M.; Frank, E.; Holmes, G.; Pfahringer, B.; Reutemann,
P.; and Witten, I. H. 2009. The weka data mining software:
An update. SIGKDD Explorations 11(1).
Hawn, C. 2009. Take two aspirin and tweet me in the morning; how twitter, facebook, and other social media are reshaping health care. Health Affairs 28(2):361‚Äì368.
Heaivilin, N.; Gerbert, B.; Page, J.; and Gibbs, J. 2011. Public health surveillance of dental pain via twitter. Journal of
Dental Research 90(9):1047‚Äì1051.
Hekler, B. E.; Dubey, G.; McDonald, W. D.; Poole, S. E.;
Li, V.; and Eikey, E. 2014. Exploring the relationship between changes in weight and utterances in an online weight
loss forum: A content and correlational analysis study. J Med
Internet Res 16(12).

Joachims, T. 1999. Making large-scale SVM learning practical. In SchoÃàlkopf, B.; Burges, C.; and Smola, A., eds., Advances in Kernel Methods - Support Vector Learning. Cambridge, MA: MIT Press. chapter 11, 169‚Äì184.
Kim, A. J. 2000. Community Building on the Web: Secret
Strategies for Successful Online Communities.
Kraschnewski, J. L.; Boan, J.; Esposito, J.; Sherwood, N. E.;
Lehman, E. B.; Kephart, D. K.; and Sciamanna, C. N. 2010.
Long-term weight loss maintenance in the united states. International J. of Obesity 34(11):1644‚Äì1654.
Lamb, A.; Paul, M. J.; and Dredze, M. 2013. Separating fact
from fear: Tracking flu infections on twitter. In NAACL.
Leahey, T. M.; Kumar, R.; Weinberg, B. M.; and Wing, R. R.
2012. Teammates and social influence affect weight loss
outcomes in a team-based weight loss competition. Obesity
20(7):1413‚Äì1418.
Ludford, P. J.; Cosley, D.; Frankowski, D.; and Terveen, L.
2004. Think different: Increasing online community participation using uniqueness and group dissimilarity. In SIGCHI
Conference on Human Factors in Computing Systems, CHI
‚Äô04, 631‚Äì638.
Miller, G. A. 1995. Wordnet: A lexical database for English.
Communications of the ACM 38(11):39‚Äì41.
Must, A.; Spadano, J.; Coakley, E. H.; Field, A. E.; Colditz,
G.; and H., D. W. 1999. The disease burden associated with
overweight and obesity. JAMA 282(16):1523‚Äì1529.
2012. The obesity epidemic and its impact on hypertension.
Canadian Journal of Cardiology 28(3):326 ‚Äì 333.
Ogden, C. L.; Kit, B. K.; Fakhouri, T. H.; Carroll, M. D.; and
Flegal, K. M. 2014. The epidemiology of obesity among
adults. GI Epidemiology 394‚Äì404.
Pennebaker, J. W.; Mehl, M. R.; and Niederhoffer, K. G.
2003. Psychological aspects of natural language use: Our
words, our selves. Annual Review of Psychology 54(1):547‚Äì
577.
Preece, J.; Nonnecke, B.; and Andrews, D. 2004. The top
five reasons for lurking: improving community experiences
for everyone. Computers in Human Behavior 20(2):201 ‚Äì
223.
Rogers, P. S., and Lee-Wong, S. M. 2003. Reconceptualizing
politeness to accommodate dynamic tensions in subordinateto-superior reporting. 17(4):379‚Äì412.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
EMNLP, 1631‚Äì1642.
Toutanova, K.; Klein, D.; Manning, C. D.; and Singer, Y.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL ‚Äô03, 173‚Äì180.
Wing, R. R., and Phelan, S. 2005. Long-term weight loss
maintenance. The American Journal of Clinical Nutrition
82(suppl):222S‚Äì5S.
Yang, J.; Wei, X.; Ackerman, M. S.; and Adamic, L. A. 2010.
Activity lifespan: An analysis of user survival patterns in online knowledge sharing communities. In ICWSM.

Planning with Resource Conflicts in Human-Robot Cohabitation
1 Department of Computer Science, Arizona State University, AZ Autonomous Systems & Robotics Intelligent Systems Division, NASA Ames Research Center, CA

Tathagata Chakraborti1 , Yu Zhang1 , David E. Smith2 , Subbarao Kambhampati1

2

tchakra2@asu.edu, yzhan442@asu.edu, david.smith@nasa.gov, rao@asu.edu ABSTRACT
In order to be acceptable members of future human-robot ecosystems, it is necessary for autonomous agents to be respectful of the intentions of humans cohabiting a workspace and account for conflicts on shared resources in the environment. In this paper we build an integrated system that demonstrates how maintaining predictive models of its human colleagues can inform the planning process of the robotic agent. We propose an Integer Programming based planner as a general formulation of this flavor of "humanaware" planning and show how the proposed formulation can be used to produce different behaviors of the robotic agent, showcasing compromise, opportunism or negotiation. Finally, we investigate how the proposed approach scales with the different parameters involved, and provide empirical evaluations to illustrate the pros and cons associated with the proposed style of planning. tiation. Specifically, we ask the question, what information can be extracted from the predicted plans, and how this information can be used to guide the behavior of the autonomous agent. There has been previous work [1, 6] on some of the modeling aspects of the problem, in terms of planning with uncertainty in resources and constraints. In this paper we provide an integrated framework (shown in Figure 1) for achieving these behaviors of the autonomous agent, particularly in the context of stigmergic coordination of human-robot cohabitation. To this end, we modularize our architecture so as to handle the uncertainty in the environment separately with the planning process, and show how these individual modules interact with each other by the way of usage profiles of the concerned resources.

1.

INTRODUCTION

In environments where multiple agents are working independently, but utilizing shared resources, it is important for these agents to model the intentions and beliefs of other agents so as to act intelligently and prevent conflicts. In cases where some of these agents are human, as in the case of assistive robots in household environments, these are required (rather than just desired) capabilities of robots in order for them to be considered "socially acceptable" - this has been one of the important objectives of "human-aware" planning, as evident from existing literature in human-aware path planning [13, 10] and human-aware task planning [5, 9, 3, 15]. An interesting aspect of many of these scenarios, is the presence of many of the aspects of multi-agent environments, but absence of typical assumptions often made in explicit teaming scenarios between humans and robots, as pointed out in [4]. Probabilistic plan recognition plays an important role in this regard, because by not committing to a plan, that presumes a particular plan for the other agent, it might be possible to minimize suboptimal (in terms of redundant or conflicting actions performed during the execution phase) behavior of the autonomous agent. Here we look at possible ways to minimize such suboptimal behavior by ways of compromise, opportunism or negoAppears in: Proceedings of the 15th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2016), J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.), May 9≠13, 2016, Singapore. Copyright c 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.

Figure 1: Schematic diagram of our integrated system for belief modeling, goal recognition, information extraction and planning. The robot maintains a belief model of the environment, and uses observations from the environment to extract information about how the world may evolve, which is then used to drive its own planning process.

1069

The general architecture of the system is shown in Figure 1. The autonomous agent, or the robot, is acting (with independent goals) in an environment co-habited with other agents (humans), who are similarly self-interested. The robot has a model of the other agents acting independently in its environment. These models may be partial and hence the robot can only make uncertain predictions on how the world will evolve with time. However, the resources in the environment are limited and are likely to be constrained by the plans of the other agents. The robot thus needs to reason about the future states of the environment in order to make sure that its own plans do not produce conflicts with respect to the plans of the other agents. With the involvement of humans, however, the problem is more skewed against the robot, because humans would expect a higher priority on their plans - robots that produce plans that clash with those of the humans, without any explanation, would be considered incompatible for such an ecosystem. Thus the robot is expected to follow plans that preserve the human plans, rather than follow a globally optimal plan for itself. This aspect makes the current setting distinct from normal human robot teaming scenarios and produces a number of its own interesting challenges. How does the robot model the human's behavior? How does it plan to avoid friction with the human plans? If it is possible to communicate, how does it plan to negotiate and refine plans? These are the questions that we seek to address in this work. Our approach models human beliefs and defines resource profiles as abstract representations of the plans predicted on the basis of these beliefs. The robot updates its beliefs upon receiving new observations, and passes on the resultant profiles onto its planner, which uses an IPformulation to minimize the overlap between these resource profiles and those produced by the human's plans. The contribution of our paper is thus three-fold, we (1) propose resource profiles as a concise mode of representing different types of information from predicted plans; (2) develop an IP-based planner that can utilize this information and provide different modalities of conformant behavior; and (3) provide an integrated framework that supports the proposed mode of planning - the modular approach also provides an elegant way to handle different challenges separately (e.g. uncertainty and/or nested beliefs of humans leaves the planner). The planner, as a consequence of these, has properties not present in existing planners - for example, the work that probably comes closest is [9] that models a specific case of compromise only, while the formulation is also likely to blow up in presense of large hypothesis sets due to absence of concise representation techniques like the profiles. We will discuss the trade-offs and design choices in more detail in the evaluation sections. The rest of the paper is organized as follows. We will start with a brief introduction of the agent models that comprise the belief component, and describe how it facilitates plan recognition. Then, in Sections 2.3 and 2.4, we are going to go into details of how resource profiles may be used to represent information from predicated plans, and describe how our planner converts this information into constraints that can be solved as an integer program during the plan generation process. In Section 3 we will demonstrate how the planner may be used to produce different modes of autonomous behavior. Finally in Section 4 we will provide empirical evaluations of the planner's internal properties.

Figure 2: Use case - Urban Search And Rescue (USAR).

2.

PLANNING WITH CONFLICTS ON SHARED RESOURCES

We will now go into details about each of the modules shown in Figure 1. The setting (adopted from [14]) involves a commander CommX and a robot in a typical USAR (Urban Search and Rescue) task illustrated in Figure 2. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other agents (not shown) in the environment. The shared resources here are the two medkits - some of the plans the commander can execute will lock the use of and/or change the position of these medkits, so that from the set of probable plans of the commander we can extract a probability distribution over the usage (or even the position) of the medkit over time based on the fraction of plans that conform to these facts. These resource availability profiles (i.e. the distribution over the usage or position of the medkit evolving over time) provide a way for the agents to minimize conflicts with the other agents. Before going into details about the planner that achieves this, we will first look at how the agents are modeled and how these profiles are computed in the next section.

2.1

The Belief Modeling Component

The notion of modeling beliefs introduced by the authors in [14] is adopted in this work. Beliefs about state are defined in terms of predicates bel(, ), where  is an agent with belief  = true. Goals are defined by predicates goal(, ), where agent  has a goal . The set of all beliefs that the robot ascribes to  together represents the perspective for the robot of . This is obtained by a belief model Bel of agent , defined as {  | bel(, )  Belself }, where Belself are the first-order beliefs of the robot (e.g., bel(self, at(self, room1))). The set of goals ascribed to  is similarly described by {goal(, )|goal(, )  Belself }. Next, we turn our attention to the domain model D of the agent  that is used in the planning process. We use PDDL [11] style agent models for the rest of the discussion, but most of the analysis easily generalizes to other related modes of representation. Formally, a planning problem  = D ,  consists of the domain model D and the problem instance  . The domain model of  is defined as D = T , V , S , A , where T is a set of object types; V is a set of variables that describe objects that belong to types in T ; S is a set of named first-order logical predicates over the variables V that describe the state; and A is a set of operators available to the agent. The action models a  A are represented as a = N, C, P, E where N denotes

1070

the name of that action; C is the cost of that action; P  S is the list of pre-conditions that must hold for the action a to be applicable in a particular state s  S of the environment; and Ea = ef f + (a), ef f - (a) , ef f ± (a)  S is a tuple that contains the add and delete effects of applying the action to a state. The transition function  (∑) determines the next state after the application of action a in state s as  (a, s) |=  if f  P s.t. f  s;  (a, s) |= (s \ ef f - (a))  ef f + (a) otherwise. The belief model, in conjunction with beliefs about the goals / intentions of another agent, will allow the robot to instantiate a planning problem  = O , I , G , where O is a set of objects of type t  T ; I is the initial state of the world, and G is a set of goals, which are both sets of the predicates from S initialized with objects from O . First, the initial state I is populated by all of the robot's initial beliefs about the agent , i.e. I = { | bel(, )  Belrobot }. Similarly, the goal is set to G = { | goal(, )  Belrobot }. Finally, the set of objects O consists of all the objects that are mentioned in either the initial state, or the goal description: O = {o | o  ( |   (I  G ))}. The solution to the planning problem is an ordered sequence of actions or plan given by  = a1 , a2 , . . . , a| | , ai  A such that  ( , I ) |= G , where the cumulative transition function is given by  (, s) =  ( a2 , a3 , . . . , a|| ,  (a1 , s)). The cost of the plan is given by C ( ) = a Ca and the optimal   plan  is such that C ( )  C ( )  with  ( , I ) |= G . This planning problem instance (though not directly used in the robot's planning process) enables the goal recognition component to solve the compiled problem instances. More on this in the next section.

2.2.2

Goal / Plan Recognition

In the present scenario, we thus have a set  of goals that  may be trying to achieve, and observations of the actions  is currently executing. At this point we refer to the work of Ramirez and Geffner who in [12] provided a technique to compile the problem of plan recognition into a classical planning problem. Given a sequence of observations , we recompute the probability distribution  over G   by using a Bayesian update P (G|)  P (|G), where the likelihood is approximated by the function P (|G) = 1/(1 + e- (G,) ) where (G, ) = Cp (G - ) - Cp (G + ). Here (G, ) gives an estimate of the difference in cost Cp of achieving the goal G without and with the observations, thus increasing P (|G) for goals that explain the given observations. Thus, solving two compiled planning problems, with goals G -  and G + , gives us the required posterior update for the distribution  over possible goals of . The details of the approach is available at [12]. The specific problem we will look at now is how to inform the robot's own planning process from the recognized goal set  . In order to do this, we compute the optimal plans for each goal in the hypothesis goal set  , and associate them with the probabilities of these goals from the distribution thus obtained. Information from these plans is then represented concisely in the form of resource profiles.

Notes on the Recognition Module
For our plan recognition module we use a much faster variation [7] of the above approach that exploits cost and interaction information from plan graphs to estimate the goal probabilities. This saves on the computational effort of having to solve two planning problems per goal. Also, note that while computing the plan to a particular goal G, we use a compiled problem instance with the goal G +  to ensure that the predicted plan conforms to the existing observations. Details on the compilation is available at [12]. Also, the output of the planner does not need to be associated with probabilities - this is just the most general formulation. If we want to deal with just a set of plans that the robot needs to be aware of, we can treat the plan set either with a uniform distribution and/or by requiring exactly zero conflicts in the objective of the planner (this will become clearer in Section 2.4) depending on the preference. Perhaps the biggest computational issue here is the need to compute optimal plans. While we still do it for our domain, as we will note later in Section 2.3, this might not be necessary, and suboptimal plans may be used in larger domains where computation is an issue.

2.2

The Goal Recognition Component

For many real world scenarios, it is unlikely that the goals of the humans are known completely, and that the plan computed by the planner is exactly the plan that they will follow. We are only equipped with a belief of the likely goal(s) of the human - and this may not be a full description of their actual goals. Further, in the case of an incompletely specified goal, there might be a set of likely plans that the human can execute, which brings into consideration the idea of incremental goal recognition over a possible goal set given a stream of observations.

2.2.1

Goal Extension

To begin with, it is worth noting that the robot might have to deal with multiple plans even in the presence of completely specified goals (even if the other agents are fully rational). For example, there may be multiple optimal ways of achieving the same goal, and it is not obvious beforehand which one of these an agent is going to end up following. In the case of incompletely specified goals, the presence of multiple likely plans become more relevant. To accommodate this, we extend the robot's current belief of an agent 's goal, G , to a hypothesis goal set  . The computation of this goal set can be done using the planning graph method [2]. In the worst case,  corresponds to all possible goals in the final level of the converged planning graph. Having further (domain-dependent) knowledge (e.g. in our scenario, information that CommX is only interested in triage-related goals) can prune some of these goals by removing the goal conditions that are not typed on the triage variable.

2.3

Resources and Resource Profiles

As we discussed previously, since the plans of the agents are in parallel execution, the uncertainty introduced by the commander's actions cannot be mapped directly between the commander's final state and the robot's initial state. However, given the commander's possible plans, the robot can extract information about at what points of time the shared resources in the environment are likely to be locked by the commander. This information can be represented by resource usage profiles that capture the expected (over all the recognized plans) variation of probability of usage or availability over time. The robot can, in turn, use this information to make sure that the profile imposed by its own plan has minimal conflicts with those of the commander's.

1071

Figure 3: Different types of resource profiles.

Formally, a profile is defined as a mapping from time step T to a real number between 0 and 1, and is represented by a set of tuples as follows G : N  [0, 1]  {(t, g ) : t  N, g  [0, 1], such that G(t) = g at time step t}. The concept of resource profiles can be handled at two levels of abstraction. Going back to our running example, shared resources that can come under conflict are the two (locatable typed objects) medkits, and the profiles over the medkits can be over both usage and location, as shown in Figure 3. These different types of profiles can be used (possibly in conjunction if needed) for different purposes. For example, just the usage profile shown on top is more helpful in identifying when to use the specific resource, while the resource when bound with the location specific groundings, as shown at the bottom can lead to more complicated higher order reasoning (e.g. the robot can decide to wait for the commander's plans to be over, as he inadvertently brings the medkit closer to it with high probability as a result of his own plans). We will look at this again in Section 3. Let the domain model of the robot be DR = TR , VR , SR , AR with the action models a = N, C, P, E defined in the same way as described in Section 2.1. Also, let   VR be the set of shared resources and for each    we have a set of predicates f   SR that are influenced (as determined by the system designer) by , and let  :   P ( ) be a function that maps the resource variables to the set of predicates  =  f  they influence. Without any external knowledge of the environment, we can set  = V  VR and  = S  SR , though in most cases these sets are much smaller. In the following discussion, we will look at how the knowledge from the hypothesis goal set can be modeled in terms of resource availability graphs for each of the constrained resources   . Consider the set of plans P  containing optimal plans corresponding to each goal in the hypothesis goal set, i.e.   P = G, ai  A i, G   = {G = a1 , a2 , . . . at |  (G , I ) |  } and let l( ) be the likelihood of the plan  modeled on the goal likelihood distribution  G   , p(G)   as l(G ) = c|G | ◊ p(G), where c is a normalization constant. At each time step t, a plan   P  may lock one or more of the resources . Each plan thus provides a profile of usage of a resource with respect to the time step t as  G : N  {0, 1} = {(t, g ) | t  [1, | |] and g = 1 if 

 is locked by  at step t, 0 otherwise} such that G (t) =  g  (t, g )  G . The resultant usage profile of a resource  due to all the plans in P  is obtained by summing over (weighted by the individual likelihoods) all the individual profiles as G  : N  [0, 1] = {(t, g ) | t  {1, maxP | |}   G ( t ) ◊ l (  ) } . and g  |1 P P|     Similarly, we can define profiles over the actual groundings of a variable (shown in the lower part of Figure 3) as f G = {(t, g ) | t  [1, | |] and f  = 1 at step t of plan  , 0 otherwise}, and the resultant usage profile due to all the f plans in P = {(t, g ) | t =  is obtained as before as G f 1 |  | and g  1, 2, . . . , maxP P G (t) ◊ l( )}.   |P  | These profiles are helpful when actions in the robot's domain are conditioned on these variables, and the values of these variables are conditioned on the plans of the other agents in the environment currently under execution. One important aspect of this formulation that should be noted here is that the notion of "resources" is described here in terms of the subset of the common predicates in the domain of the agents (  S  SR ) and can thus be used as a generalized definition to model different types of conflict between the plans between two agents. In as much as these predicates are descriptions (possibly instantiated) of the typed variables in the domain and actually refer to the physical resources in the environment that might be shared by the agents, we will stick to this nomenclature of calling them "resources". We will now look at how an autonomous agent can use these resource profiles to minimize conflicts during plan execution with other agents in its environment.

Notes on Usefulness of Profile Computation
One interesting aspect of computing resource profiles is that it provides a powerful interface between the belief on the environment and the planner. On the one hand, note that the input from the previous stage (goal/plan recognition module) is as generic as possible - a set of plans possibly associated with probabilities. Given any changes in preceding stages, e.g. modeling stochasticity or more complex belief models, still yields a set of plans that the robot needs to be aware of. Thus the plan set and resource profiles provide a surprisingly simple yet powerful way of abstracting away relevant information for the planner to use. The profiles may also be leveraged to address different modalities of conformant behavior, for example with multiple humans and their relative importance, by (1) weighing the contributions from individual profiles by the normalized priority of the human, which would cause the planner to avoid conflicts with these profiles more than with those with lower priorities; or (2) requiring zero conflicts on a subset of profiles which would cause the planner to avoid a subset of conflicts at all costs, while minimizing the rest. A somewhat implicit advantage of using profiles is its ability to form regions of interest given the possible plans. This will become clear later in Section 4.2 when we show that the predicted conflicts provide well-informed guidance to avoiding real conflicts during execution (as evident by the robustness in performance with just 1-3 observations, and zero actual conflicts in low probability areas in the computed profiles). Right now this has the implication that we need not necessarily compute perfect plan costs and goal distributions to get good plans.

1072

2.4

Conflict Minimization

The planning problem of the robot - given by  = DR , R ,  , {G |   }, {Gf | f  (),   } - consists of the domain model DR and the problem instance R = OR , IR , GR similar to that described in section 2.3, and also the constrained resources and all the profiles corresponding to them. This is because the planning process must take into account both goals of achievement as also conflict of resource usages as described by the profiles. Traditional planners provide no direct way to handle such profiles within the planning process. Note here that since the execution of the plans of the agents is occurring in parallel, the uncertainty is evolving at the time of execution, and hence the uncertainty cannot be captured from the goal states of the recognized plans alone, and consequently cannot be simply compiled away to the initial state uncertainty for the robot and solved as a conformant plan. Similarly, the problem does not directly compile into action costs in a metric planning instance because the profiles themselves are varying with time. Thus we need a planner that can handle these resource constraints that are both stochastic and non-stationary due to the uncertainty in the environment. To this end we introduce the following IP-based planner (partly following the technique for IP encoding for state space planning outlined in [16]) as an elegant way to sum over and minimize overlaps in profiles during the plan generation process. The following formulation finds such T-step plans in case of non-durative or instantaneous actions. For action a  AR at step t we have an action variable: 1, if action a is executed in step t xa,t = 0, otherwise; a  AR , t  {1, 2, . . . , T } Also, for every proposition f at step t a binary state variable is introduced as follows: 1, if proposition is true in plan step t yf,t = 0, otherwise; f  SR , t  {0, 1, . . . , T } Note here that the plan computed by the robot introduces a resource consumption profile itself, and thus one optimizing criterion would be to minimize the overlap between the usage profile due to the computed plan with those established by the predicted plans of the other agents in the environment. Let us introduce a new variable to model the resource usage graph imposed by the robot as follows: 1, if f   is locked at plan step t gf,t = 0, otherwise; f  , t  {0, 1, . . . , T } Further, for every resource   , we divide the actions in the domain of the robot into three disjoint sets +  yf,t = 1}, f = {a  AR such that xa,t = 1 = -  yf,t = 0}, and f = {a  AR such that xa,t = 1 = + - o f = AR \ (f  f ), f   . These then specify respectively those actions in the domain that lock, free up, or do not affect the use of a particular resource, and are used to calculate gf,t in the IP. Further, we introduce a variable hf,t to track preconditions required by actions in the generated plan whose success is conditioned on the influence of the plans of the other agents on the world (e.g. position of the medkits are changing, and the action pickup is conditioned on it) as follows: 1, if f  Pa and xa,t+1 = 1 hf,t = 0, otherwise; f  , t  {0, 1, . . . , T - 1}

Then the solution to the IP should ensure that the robot only uses these resources when they are in fact most expected to be available (as obtained by maximizing the overlap between  hf,t and Gf ). These act like demand profiles from the perspective of the robot. We also add a "no-operation" action AR  AR  a so that a = N, C, P, E where N = NOOP, C = 0, P = {} and E = {}. The IP formulation is given by: min k1 aAR t{1,2,...,T } Ca ◊ xa,t  +k2  f () t{1,2,...,T } gf,t ◊ G (t) -k3
 f () t{0,1,...,T -1}

hf,t ◊ Gf (t) (1) (2) (3) (4) (5) (6) (7) (8) (9) xa,t ) ◊ gf,t-1 (10) (11) (12) (13) (14) (15)



yf,0 = 1 f  IR \  yf,0 = 0 f  / IR or f   yf,T = 1 f  GR xa,t  yf,t-1 a s.t. f  Pa , f  / , t  {1, . . . , T } hf,t-1 = xa,t a s.t. f  Pa , f  , t  {1, . . . , T } yf,t  yf,t-1 + aadd(f ) xa,t s.t. add(f ) = {a|f  ef f + (a)}, f, t  {1, . . . , T } yf,t  1 - adel(f ) xa,t s.t. del(f ) = {a|f  ef f - (a)}, f, t  {1, . . . , T }
aAR a+ f

xa,t = 1, t  {1, 2, . . . , T }
t xa,t  1 f  , t  {1, 2, . . . , T } a+ f f

gf,t = hf,t ◊ G

xa,t + (1 -

a+ f

xa,t -

a- f

f  , t  {1, . . . , T } (t)  f  , t  {0, 1, . . . , T - 1}

yf,t  {0, 1} f  SR , t  {0, 1, . . . , T } xa,t  {0, 1} a  AR , t  {1, 2, . . . , T } gf,t  {0, 1} f  SR , t  {0, 1, . . . , T } hf,t  {0, 1} f  SR , t  {0, 1, . . . , T - 1}

where k1 , k2 , k3 are constants determining the relative importance of the optimization criteria and is a constant. Here, the objective function minimizes the sum of the cost of the plan and the overlap between the cumulative resource usage profiles of the predicted plans and that imposed by the current plan of the robot itself while maximizing the validity of the demand profiles. Constraints (1) through (3) model the initial and goal conditions, while the value of the constrained variables are kept uninitialized (and are determined by their profiles). Constraints (4) and (5), depending on the particular predicate, enforces the preconditions, or produces the demand profiles respectively, while (6) and (7) enforces the state equations that maintain the add and delete effects of the actions. Constraint (8) imposes non concurrency on the actions, and (9) ensures that the robot does not repeat the same action indefinitely to increase its utility. Constraint (10) generates the resource profile of the current plan, while (11) maintains that actions are only executed if there is at least a small probability of success. Finally (12) to (15) provide the binary ranges of the variables.

Note on Temporal Expressivity
At this point it is worth acknowledging the implications of having durative actions in our formulation. Note that our approach does not discretize time, but rather uses time

1073

points as steps in the plan - that can be easily augmented with their own durations. So in order to handle durative actions, the only (somewhat minor) change required in the formulation is in the way the conflicts are integrated (instead of summed) over in the objective function. Further, uncertainty in action durations is always a big issue in human interactions; though resource profiles cannot directly handle uncertain durations, it only affects the way the profiles are calculated, and the way in which information is expressed in it remains unchanged (i.e. expectations over action durations add an extra expectation to the already probabilistic profile computation). As noted before in Section 2.3, the ability of profiles to form regions of interest is crucial in handling such scenarios implicitly.

13 14 15 16 17 18 19

NOOP PICK_UP_MEDKIT_ROBOT_MK1_ROOM1 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_ROBOT_HALL2_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 DROP_OFF_ROBOT_MK1_HALL3

3.3

Negotiation

3.

MODULATING BEHAVIOR OF THE ROBOT

The planner is implemented on the IP-solver gurobi and integrates [7] and [8] respectively for goal recognition and plan prediction for the recognized goals. We will now illustrate how the formulation can produce different behaviors of the robot by appropriately configuring the parameters of the planner. For this discussion we will limit ourselves to a singleton hypothesis goal set in order to observe the robot's response more clearly.

In many cases, the robot will have to eventually produce plans that will have potential points of conflict with the expected plans of the commander. This occurs when there is no feasible plan with zero overlap between profiles (specifically gf,t ◊ G (t) = 0) or if the alternative plans for the robot are too costly (as determined by the objective function). If, however, the robot is equipped with the ability to communicate with the human, then it can negotiate a plan that suits both. To this end, we introduce a new variable H  (t) and update the IP as follows: min k1 aAR t{1,2,...,T } Ca ◊ xa,t  +k2  f -1 () t{1,2,...,T } gf,t ◊ H (t) -k3 +k4
 

hf,t ◊ Gf (t) t{0,1,...,T } ||G (t) - H (t)||
f -1 () t{0,1,...,T -1}  



yf,T  hf,t-1  a s.t. f  Pa , f  , t  {1, . . . , T } H  (t)  [0, 1]   , t  {0, 1, . . . , T }
 

(5a) (16)

3.1

Compromise

Let us now look back at the environment we introduced in Figure 1. Consider that the goal of the commander is to perform triage in room1. The robot computes the human's optimal plan (which ends up using medkit1 at time steps 7 through 12) and updates the resource profiles accordingly. If it has its own goal to perform triage in hall3, the plan that it comes up with given a 12 step lookahead is shown below. Notice that the robot opts for the other medkit (medkit2 in room3) even though its plan now incurs a higher cost in terms of execution. The robot thus can adopt a policy of compromise if it is possible for it to preserve the commander's (expected) plan.
01 02 03 04 05 06 07 08 09 10 11 12 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_ROBOT_HALL2_HALL3 MOVE_ROBOT_HALL3_HALL4 MOVE_REVERSE_ROBOT_HALL4_ROOM4 MOVE_REVERSE_ROBOT_ROOM4_ROOM3 PICK_UP_MEDKIT_ROBOT_MK2_ROOM3 MOVE_ROBOT_ROOM3_ROOM4 MOVE_ROBOT_ROOM4_HALL4 MOVE_REVERSE_ROBOT_HALL4_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 DROP_OFF_ROBOT_MK2_HALL3

H (t)  G (t)   , t  {0, 1, . . . , T } (17) Constraint (5a) now complements constraint (5) from the existing formulation, by promising to restore the world state every time a demand is made on a variable. The variable H  (t), maintained by constraints (16) and (17), determine the desired deviation from the given profiles. The objective function has been updated to reflect that overlaps are now measured with the desired profile of usage, and there is a cost associated with the deviation from the real one. The revised plan now produced by the robot is shown below.
01 02 03 04 05 06 07 08 09 10 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_REVERSE_ROBOT_HALL2_ROOM2 PICK_UP_MEDKIT_ROBOT_MK1_ROOM2 MOVE_ROBOT_ROOM2_HALL2 MOVE_ROBOT_HALL2_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 MOVE_REVERSE_ROBOT_HALL3_HALL2 MOVE_REVERSE_ROBOT_HALL2_ROOM2 DROP_OFF_ROBOT_MK1_ROOM2

3.2

Opportunism

Notice that the robot restores the world state that the human is believed to expect, and can now communicate to him "Can you please not use medkit1 from time 7 to 9?" based on how the real and the ideal profiles diverge, i.e. t such that H  (t) < G (t) for each resource .

Notice, however, that the commander is actually bringing the medkit to room1 as predicted by the robot, and this is a favorable change in the world, because robot can use this medkit once the commander is done and achieve its goal at a much lower cost. The robot, indeed, realizes this once we give it a bigger time horizon to plan with, as shown above (on the right). Thus, in this case, the robot shows opportunism based on how it believes the world state will change.
01 NOOP 02 NOOP ...

Notes on Adaptive Behavior Modeling
One might note here that people are often adaptive and it is very much possible that they may be willing to change their goals based on observing the robot or are even unwilling to negotiate if their plans conflict. Hence the policies of compromise and opportunism for the robot are complementary to negotiation in the event the latter fails. Thus, for example, the robot might choose to communicate a negotiation strategy to the human, but fall back on a compromise if that fails. It is a merit of such a simple formulation to be able to handle such interesting adaptive behaviors.

1074

4.

EVALUATION

The power of the proposed approach lies in the modular nature in which it tackles several complicated problems that are separate research areas in their own rights. As we saw throughout the course of the discussion, approaches used in the individual modules may be varied with little to no change in the rest of the architecture. For example the expressivity of the belief modeling or goal recognition component is handled separately as the planner used information from a generic plan set. Again the representation technique introduced in terms of resource profiles provide properties in terms of computational independence with respect to size of the hypothesis set and number of agents (which gets manifested in complexity in number of resources) that general planners do not have. So it becomes a design choice depending on which metric needs to be optimized. For empirical evaluations, we simulated the USAR scenario on 360 different problem instances, randomly generated by varying the specific (as well as the number of probable) goals of the human, and evaluated how the planner behaved with the number of observations it can start with to build its profiles. We fix the domain description, location and goal of the agents, and the position of the resources, and consider randomly generated hypothesis goal sets of size 211. The goals of the commander were assumed to be known to be triage related, but the location of the triage was allocated randomly (one of which was again picked at random as the real goal). Finally for each of these problems, we generate 1-5 observations by simulating the commander's plan over the real goal, and use these observations known a priori the robot's plan generation process. The experiments were conducted on an Intel Xeon(R) CPU E5-1620 v2 3.70GHz◊8 processor with a 62.9GiB memory.

(a) w.r.t. T (|| = 2)

(b) w.r.t. #medkits (T = 10)

Figure 4: Performance of the planner w.r.t. planning horizon T and number of constrained resources (medkits).

4.1

Scaling Up
the agents. On the other hand, the time spent on recognition, and on calculating the profiles, is significantly affected. However, observations on multiple agents are asynchronous, and goal recognition can operate in parallel, so that this is not a huge concern beyond the complexity of a single instance. Similarly the performance is also unaffected by the size of the hypothesis set  , as shown in Figure 5, which shows increase in the number of the possible goals does not complicate the profiles to an extent to affect the complexity.

Our primary contribution is the formulation for planning with resource profiles, while the goal recognition component can be any off-the-shelf algorithm, and as such we compare scalability with respect to the planning component only.

- w.r.t. Length of the Planning Horizon
The performance of the planner with respect to the planning horizon is shown in Figure 4a. This is, as expected, the bottleneck in computation due to exponential growth of the size of the IP. It is however not prohibitively expensive, and the planner is still able to produce plans of length 20 (steps, not durations) for our domain in a matter of seconds.

4.2

Quality of the Plans Produced

- w.r.t. Number of Resources
The performance of the planner with respect to the number of constrained resources (medkits, in the context of the current discussion) is shown in Figure 4b. Evidently, the computational effort is dominated by that due to the planning horizon. This reiterates the usefulness of abstracting the information in predicted plans in the form of resource profiles, thus isolating the complexities of the domain with that of the underlying planning algorithm.

- w.r.t. the Number of Agents and Goals
The planning module (i.e. the IP formulation) is by itself independent of the number of agents being modeled. In fact, this is one of the major advantages of using abstractions like resource profiles in lieu of actual plans of each of

We define U as the average conflicts per plan step when a demand is placed on a resource by the robot, and S as the success probability per plan step that the demand is met. C is the cost of a plan. F is the percentage of times there was an actual conflict during execution (distinct from U which estimates the possible conflict that may occur per plan step). We observe the quality of the plans produced by the planner by varying the ratio of parameters k1 and k3 from the objective function and the length of the planning horizon T . Similar results can be produced by varying k1 /k2 . From Table 1, as k1 /k3 decreases, the planner becomes more conservative (to maximize success probability) and thus plans become costlier. At the same time the expected success rate of actions are also increased (with simultaneous increase in usage conflict), as reflected by a higher failure rate due to actual execution time conflicts.

1075

Figure 5: Performance of the planner w.r.t. size of the goal set. As expected, computational complexity is not affected.

(a) w.r.t. | |

k1 /k3 C U S F

0.05 9.47 0.18 0.85 27.5

0.5 6.37 0.17 0.579 23.0

5.0 6.31 0.17 0.578 21.3

Table 1: Quality of plans produced w.r.t. k1 /k3 . Conservative plans result in lowered utility.

Also note, from Table 2 the impact of the planning horizon T on the types of behaviors we discussed in the previous section. As we increase T , the plan cost falls below the optimal, indicating opportunities for opportunistic behavior on the part of the robot. The expected conflict also falls to almost 0. However the expected success rate of actions also decreases, the ratio k1 /k2 determines how daring the robot is, in choosing between cheap versus possibly invalid plans. Note, however, the actual execution time conflict is extremely low with increasing T , for even sufficiently conservative estimates of S . Thus we see that the robot is successfully able to navigate conflicts and find in many cases plans even cheaper than the original optimal plan, thus highlighting the usefulness of the approach. Finally, we look at the impact of the parameters in the plan recognition module in Figure 6. As expected, with bigger hypothesis sets, the success rate goes down. Interestingly, the plan cost also shows a downward trend which might be because the bigger variety in possible goals give a better idea of which medkits are generally more useful for that instance at what points of time. With more observations, as expected, the success rate goes up and the expected conflict goes down. The cost, however, increases a little as the planner opts for more conservative options.

(b) w.r.t. #obs

Figure 6: Performance of the planner w.r.t. size of goal set and number of observations (k1 /k3 = 0.5, T = 16).

T C U S F

10 9.0 0.46 1.0 53.3

13 5.6 0.04 0.48 11.9

16 4.53 0 0.25 6.6

Optimal 9.0 n/a n/a 53.3

Table 2: Quality of plans produced w.r.t. T . Opportunities for opportunism explored, conflicts minimized.

5.

CONCLUSIONS

In this paper we investigate how plans may be affected by conflicts on shared resources in an environment cohabited by humans and robots, and introduce the concept of resource profiles as a means of representation for concisely modeling the information pertaining to the usage of such resources, contained in predicted behavior of the agents. We propose a general formulation of a planner for such scenarios and show how the planner can be used to model different types of behavior of the robot by appropriately configuring the

objective function and optimization parameters. Finally, we provide an end-to-end framework that integrates belief modeling, goal recognition and an IP-solver that can enforce the desired interaction constraints. One interesting research direction would be to consider nested beliefs on the agents; after all, humans are rarely completely aloof of other agents in its environment. Such interactions should have to consider evolution of beliefs with continued interactions and motivate further exploration of the belief modeling component. The modularity of the proposed approach allows for focused research on each (individually challenging) subtask without significantly affecting the others.

Acknowledgment
This research is supported in part by the ONR grants N0001413-1-0176, N00014-13-1-0519 and N00014-15-1-2027, and the ARO grant W911NF-13-1-0023.

1076

REFERENCES
[1] E. Beaudry, F. Kabanza, and F. Michaud. Planning with concurrency under resources and time uncertainty. In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pages 217≠222, Amsterdam, The Netherlands, The Netherlands, 2010. IOS Press. [2] A. Blum and M. L. Furst. Fast planning through planning graph analysis. In IJCAI, pages 1636≠1642, 1995. [3] F. Cavallo, R. Limosani, A. Manzi, M. Bonaccorsi, R. Esposito, M. Di Rocco, F. Pecora, G. Teti, A. Saffiotti, and P. Dario. Development of a socially believable multi-robot solution from town to home. Cognitive Computation, 6(4):954≠967, 2014. [4] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz, D. Smith, and S. Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015. [5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Proc of the Int Conf on Advanced Robotics (ICAR), 2009. [6] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning: An application to mobile robots. ACM Trans. Intell. Syst. Technol., 1(2):15:1≠15:26, Dec. 2010. [7] Y. E.-Mart¥ in, M. D. R.-Moreno, and D. E. Smith. A fast goal recognition technique based on interaction estimates. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 761≠768, 2015. [8] M. Helmert. The fast downward planning system. CoRR, abs/1109.6051, 2011. [9] U. Koeckemann, F. Pecora, and L. Karlsson. Grandpa hates robots - interaction constraints for planning in inhabited environments. In Proc. AAAI-2010, 2014. [10] M. Kuderer, H. Kretzschmar, C. Sprunk, and W. Burgard. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings of Robotics: Science and Systems, Sydney, Australia, July 2012.

[11] D. Mcdermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and D. Wilkins. Pddl the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control 1998. " [12] M. Ramirez and H. Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In In Proc. AAAI-2010, 2010. [13] E. Sisbot, L. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. Robotics, IEEE Transactions on, 23(5):874≠883, Oct 2007. [14] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and S. Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠2962, Sept 2014. [15] S. Tomic, F. Pecora, and A. Saffiotti. Too cool for school - adding social constraints in human aware planning. In Proc of the International Workshop on Cognitive Robotics (CogRob), 2014. [16] T. Vossen, M. O. Ball, A. Lotem, and D. S. Nau. On the use of integer programming models in ai planning. In T. Dean, editor, IJCAI, pages 304≠309. Morgan Kaufmann, 1999.

1077

Explicable Robot Planning as Minimizing Distance
from Expected Behavior

arXiv:1611.05497v1 [cs.AI] 16 Nov 2016

Anagha Kulkarni1 , Tathagata Chakraborti1 , Yantian Zha1 ,
Satya Gautam Vadlamudi2 , Yu Zhang1 , and Subbarao Kambhampati1
Abstract‚Äî In order for robots to be integrated effectively into
human work-flows, it is not enough to address the question of
autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task
plans without such considerations, they may often demonstrate
what we refer to as inexplicable behavior from the point of view
of humans who may be observing it. This problem arises due to
the human observer‚Äôs partial or inaccurate understanding of the
robot‚Äôs deliberative process and/or the model (i.e. capabilities
of the robot) that informs it. This may have serious implications
on the human-robot work-space, from increased cognitive load
and reduced trust in the robot from the human, to more serious
concerns of safety in human-robot interactions. In this paper, we
propose to address this issue by learning a distance function that
can accurately model the notion of explicability, and develop
an anytime search algorithm that can use this measure in its
search process to come up with progressively explicable plans.
As the first step, robot plans are evaluated by human subjects
based on how explicable they perceive the plan to be, and
a scoring function called explicability distance based on the
different plan distance measures is learned. We then use this
explicability distance as a heuristic to guide our search in order
to generate explicable robot plans, by minimizing the plan
distances between the robot‚Äôs plan and the human‚Äôs expected
plans. We conduct our experiments in a toy autonomous car
domain, and provide empirical evaluations that demonstrate
the usefulness of the approach in making the planning process
of an autonomous agent conform to human expectations.

I. I NTRODUCTION
Recent advancement in the field of robotics has given
us autonomous robots, vehicles, drones, etc. Typically these
autonomous systems have the capability to make their own
plans which help them achieve their goals. These advances
have, naturally, encouraged the possibility of human-robot
teaming where the autonomous robots and humans can
work alongside each other. However, if the plans that are
being generated by the autonomous robots are difficult to
comprehend for the human observer, the unexpected behavior
from the robot can raise several concerns: it may increase
cognitive load, hamper the productivity of the team, and
result in safety concerns and distrust towards the robot [1].
This mismatch between the robot‚Äôs plans and the human
expectations may be explained in terms of difference in the
actual robot model and the human‚Äôs understanding of the
robot model. Thus, even with the knowledge of the robot‚Äôs
1 Anagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Yu Zhang, and
Subbarao Kambhampati are with the Computer Science and Engineering
Department at Arizona State University { akulka16, tchakra2,

yantian.zha, yzhan442, rao } @ asu.edu
2 Satya

Gautam

Vadlamudi

vsatyagautam@gmail.com

is

with

Capillary

Technologies.

Fig. 1: A schematic diagram of the proposed system. Explicability distance is the plan distance measure between the
robot plan and human expected plan. This distance is used
to guide the search to generate explicable plans.

goals, it may still not be possible for the human to make
sense of the robot‚Äôs plan. For example, consider a scenario
with an autonomous car switching lanes on a highway. The
autonomous car, in order to switch the lane, may make
sharp and calculated moves, as opposed to gradually moving
towards the other lane. These moves may well be optimal for
the car, and backed by the car‚Äôs superior sensing and steering
capabilities. Nevertheless, a human passenger sitting inside
may perceive this as dangerous and reckless behavior, in as
much as they might be ascribing the car the sort of driving
abilities they themselves have.
To address this issue of the differences between the robot
model, MR (R), and the human mental model of the robot
capabilities, MH (R), we develop an approach based on the
concept of explicability. An explicable plan is a plan that is
generated with the human‚Äôs expectation of the robot model;
the ability to synthesize explicable plans on the part of the
robot thus involves the ability to take into consideration
both models into its plan generation process. The intuition is
that, the similarity between the robot plan that is generated
from the robot model, and the plan that is generated from
the human understanding of the robot model determines
the explicability of the robot plan. More specifically, the
smaller the explicability distance between these two plans,
the more explicable the robot plan is. Of course, such a

similarity metric is not readily available, which brings us
to the question - How can a robot learn a distance function
between plans that model the notion of explicability, and how
can it use this learned similarity model to inform its own
deliberative process? Keeping this in mind, we address the
following questions in our paper: 1) Given a domain, can we
find an approximation to MH (R), the human mental model
of the robot capabilities? 2) Can the measures of distance
between a robot plan, œÄR , and the plan expected by the
human, œÄH , effectively capture the explicability of the robot
plan? 3) Can we then integrate the explicability estimates
into the robot plan generation process? The outline of our
proposed approach is illustrated in Figure 1.
To address the first question, we start out with a robot
model, MR (R) and generate different robot plans for various initial and goal states. Next, we recruit human subjects
and ask them to evaluate these plans by assigning scores
to them based on how well they understand them. As a
part of the study, the subjects are then asked to answer a
questionnaire based on the robot model, in order to elicit
implicit human preferences. This questionnaire allows the
domain modeler to generate MH (R), based on humans
assumptions regarding the robot model in that domain. In this
paper, we represent both models in PDDL [2], but they can
differ in terms of their action representations, preconditions,
effects, and costs.
To answer the second question, we explore the relationship
between three existing plan distance measures: action set,
causal link set and state sequence distances [3], [4] and the
plan explicability distance. We use the robot plans, assigned
with scores, to determine if the explicability of the plans can
be modeled in terms of the aforementioned plan distance
measures, in terms of a regression function. For this, we
generate the plans expected by the human for the same initial
and goal states using the human understanding of the robot
model. We then compute the plan distances between the robot
plans and human expected plans. We call the function that
maps the plan distances to the explicability scores as the
explicability distance.
To address the third question, we integrate the explicablity
distance in the search process of the Fast-Downward
planner [5]. We perform a cost-bounded anytime search,
that can progressively generate more and more explicable
plans, using the learned explicability distance as a heuristic
guidance. We call this reconciliation search. Note that explicability distance exhibits non-monotonicity, i.e. a new action
that gets added to a plan prefix can either increase or decrease
the explicability distance depending on the context of the
plan. We present an analysis on how this property affects
our search. For evaluation of our system, we demonstrate
the effectiveness of our system in a simulated autonomous
car domain, and use human test subjects to evaluate the
explicability of the generated robot plans.
II. RELATED WORK
The notion of robots working alongside humans for task
achievement has been a popular research direction. It is

challenging, mainly due to the fact that, the robot must
consider the human in the loop while making its own
decisions. One important requirement for achieving this, is
the ability to infer about the human‚Äôs intent and plan. Various
plan recognition algorithms [6], [7] can be applied to perform
plan recognition based on a given set of observations as a
result of the agent interacting with the environment. After
the intent and the plan of the human is identified, researchers
have also discussed how the robot can utilize this information
while avoiding conflicts [8], [9] or providing proactive help
to the human in the loop [10], [11]. There is also work
on performing simultaneous plan recognition and generation
[12]. However, most of the prior work has only focused on
how robots can make plans based on the inferred human
intent.
The motivation for generating explicable task plans was
first provided in our recent paper [13]. While that work
proposes learning explicability as a labeling scheme, in this
work, we consider viewing explicability more directly in
terms of distances between the plans generated by the robot‚Äôs
own model, and the human‚Äôs approximation of the robot‚Äôs
model. While explicability focuses on task plans, a related
notion of ‚Äúlegibility‚Äù has been studied in the context of
motion planning [14] and has been shown to be useful in
generating socially acceptable behaviors for robots [15], [16].
In most human-robot cohabitation work where robots are
proactive agents, it is often assumed that the human model is
provided and complete for inferring about the human intent
and plan. This is often not true. Although we also assume
a human model a priori, our formulation allows us to adjust
this model so as to improve model incompleteness (e.g.,
action preference). There also exists learnable models that
do not assume completeness in the first place [17]. Another
note is that in [13], [14] and this work, since the model is
one level deeper, which is about the robot model from the
humans perspective, learning methods are adopted.
III. BACKGROUND
A. Planning
A classical planning problem can be defined as a tuple
P = hM, I, Gi, where M = hF, Ai is the domain model
(that consists of a finite set F of fluents that define the
state of the world and a set of operators or actions A), and
I ‚äÜ F and G ‚äÜ F are the initial and goal states of the
problem respectively. Each action a ‚àà A is a tuple of the
form hpre(a), ef f (a), c(a)i where c(a) denotes the cost of
an action, pre(a) ‚äÜ F is the set of preconditions for the
action a and ef f (a) ‚äÜ F is the set of the effects. The
solution to the planning problem is a plan or a sequence
of actions œÄ = ha1 , a2 , . . . , an i such that starting from
the initial state, sequentially executing the actions lands the
robot in the goal state, i.e. ŒìM (I, œÄ) |= G where ŒìM (¬∑)
is the transition function defined P
for the domain. The cost
of the plan, denoted as c(œÄ) =
ai ‚ààœÄ c(ai ), is given by
the summation of the cost of all the actions in the plan œÄ.
Henceforth, we denote the robot plan as œÄ R and the human
expected plan as œÄ H .

2) Causal Link Distance: A causal link represents a tuple
of the form hai , pi , ai+1 i, where pi is a predicate variable
that is produced as an effect of action ai and used as a precondition for the next action ai+1 . The causal link distance
measure is represented similarly to the action distance, by
considering the causal link sets Cl(œÄ R ) and Cl(œÄ H ) instead
of action sets described above. It is written as:
Œ¥C (œÄ R , œÄ H ) = 1 ‚àí

Fig. 2: A simple illustration of how a robot‚Äôs optimal plan can
deviate from the human expectation due to model difference.
In this maze, the robot can move in all four direction: up,
down, left, right and also diagonally across the grid cells.
Some of the cell floors have glass floors and some others
have obstacles. The glass floors are harder for the robot to
navigate across, because of the reflective surface, it needs to
use special sensors which results in an expensive action for
the robot. The path in green is the explicable plan whereas
the path in red is the robot plan.

B. Plan Distance Measures
We now look at the three plan distance measures introduced in [3] and later refined in [4]. These plan distances are
action, causal link and state sequence distances. Although
these distance metrics do not satisfy certain mathematical
properties [18], they provide a good domain independent
measure of the difference between any two plans. Since the
goal is to predict the differences in terms of explicability
distance between the robot plans and human expected plans,
the intuition is that they can be approximated using a
combination of plan distance measures that capture different
aspects of plans.
1) Action Distance: We denote the set of unique actions
in a plan œÄ as A(œÄ) = {a | a ‚àà œÄ}. Given the action sets
A(œÄ R ) and A(œÄ H ) of two plans œÄ R and œÄ H respectively, the
action distance, Œ¥a , is computed as the ratio of the actions
that are exclusive to each plan to all the actions in the plans
[4]. It is written as:
Œ¥A (œÄ R , œÄ H ) = 1 ‚àí

|A(œÄ R ) ‚à© A(œÄ H )|
|A(œÄ R ) ‚à™ A(œÄ H )|

(1)

This simply means that two plans are similar (and hence
their distance measure is smaller) if they contain similar
actions. Note that this measure does not take the ordering
of actions into account.

|Cl(œÄ R ) ‚à© Cl(œÄ H )|
|Cl(œÄ R ) ‚à™ Cl(œÄ H )|

(2)

Again, plans are similar, with lower similarity scores, if
they have a large number of overlapping causal links.
3) State Sequence Distance: This distance measure, as
the name suggests, takes the sequences of the states into
consideration. This distance captures the context of an action
in a given plan. The length of the sequences may differ
and therefore there are multiple ways to define this distance
measure [4]. We use the representation shown in Eq. 3. Given
H
H
R
two state sequences (sR
0 , . . . , sn ) and (s0 , . . . , sn0 ) for œÄR
0
and œÄH respectively, where n ‚â• n are the lengths of the
plans, the state sequence distance is written as:
1
Œ¥S (œÄ , œÄ ) =
n
R

H

"

0

n
X

#
H
‚àÜ(sR
k , sk )

+n‚àín

0

(3)

k=1
|sR ‚à©sH |

H
k
k
where ‚àÜ(sR
k , sk ) = 1 ‚àí |sR ‚à™sH | represents the distance
k
k
between two states (where sR
k is overloaded to denote the set
of predicate variables in state sR
k ). The first term measures
the normalized difference between states up to the end of
the shorted plan, while the second term, in the absence of a
state to compare to, assigns maximum difference possible.
Here we illustrate the explicability distance with an example and discuss its relationship with the other distance
measures. Consider the grid structure shown in Figure 2.
Here we have a 5 by 5 grid. The bottom left cell is labeled
as (1, 1). Some of the cells have glass floors while some
others have obstacles. The robot has to find its way across
the obstacles from the start cell to the goal cell. Looking at
the grid structure, the human may expect the robot to take
an optimal path highlighted by the green arrows. Although
unbeknownst to the human, the robot has difficulty traveling
across the glass floor cells because of the reflective surface
and has to use special sensors while navigating across these
floors. Hence, the cost of treading on these glass floors
is higher than the cost of treading across normal cell. In
this case, the robot‚Äôs optimal plan to the goal is the one
highlighted in red, which doesn‚Äôt coincide with human‚Äôs
expectation of the robot plan.
In Figure 3, we provide the actions sets, causal link sets
and state sequences generated for both the robot plan and
human expected plan for our example illustrated in Figure 2.
The corresponding plan distances are shown in Table I. These
three distances capture different aspects of the plans. In this
case, the explicability distance clearly has a high correlation
with these other distance measures. Our goal in this paper

Initial State: at(1, 1)
Goal State: at(5, 4)
Actions:
A(œÄ R )
=
{ move-diagonal(2, 2), move-up(2, 3), move-up(2, 4), move-diagonal(3, 5),
move-diagonal(4, 4), move-right(5, 4) }
A(œÄ H ) = {move-diagonal(2, 2), move-diagonal(3, 3), move-diagonal(4, 4), move-right(5,
4) }
Causal Links:
Cl(œÄ R ) = { <move-diagonal(2, 2), at(2, 2), move-up(2, 3)>, <move-up(2, 3), at(2, 3),
move-up(2, 4)>, <move-up(2, 4), at(2, 4), move-diagonal(3, 5)>, <move-diagonal(3,
5), at(3, 5), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
Cl(œÄ H ) =
{ <move-diagonal(2, 2), at(2, 2), move-diagonal(3, 3)>, <move-diagonal(3,
3), at(3, 3), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
State sequences:
S(œÄ R ) = { {at(2, 2)}, {at(2, 3)}, {at(2, 4)}, {at(3, 5)}, {at(4, 4)} }
S(œÄ H ) = { {at(2, 2)}, {at(3, 3)}, {at(4, 4)} }
Fig. 3: The action sets, causal link sets and state sequence sets for the illustrated example in Figure 2. For the given initial
and goal state, the plans illustrated in red and green in Figure 2 are used to produce respective action, causal link and state
sequence sets.
TABLE I: Action distance, causal link distance and state
sequence distance computed using the sets provided in Figure
3, between the two plans, œÄ R and œÄ H , that are illustrated in
Figure 2.

In order to train our regression model, we use plan traces
whose actions were assigned scores by human subjects. We
can then calculate the explicability score of a plan based on
the average of the individual action scores.

Plan Pair

Œ¥A

Œ¥C

Œ¥S

B. Plan Generation

(œÄ R , œÄ H )

4/7

6/7

4/5

We now present the details of our plan generation phase,
where we use the explicability distance to guide our search to
generate the most explicable robot plan for a given problem.
1) Non-Monotonicity: We will now discuss the nonmonotonic behavior exhibited by explicability function and
how it affects the plan generation process. The explicability
distance function is non-monotonic in nature, meaning, as
the partial plan grows, the explicability distance may both
increase or decrease. This is because, a new action can either
contribute positively or negatively to the total explicability
score of the plan. As pointed out earlier, the explicability
score is computed as an average of the individual action
scores in the context of the plan prefix.
Observation 1: Explicability score of a partial plan P may
increase, stay equal, or even decrease when it is extended
with one or more actions.
Consider the following example, in a car domain, the goal
of the car is to move to the left lane. The car squeezes
leftwards in three consecutive actions and after coming to
the left lane, it turns on its left indicator. Here the turning on
of the left tail light after having moved left is an inexplicable
action. The previous three actions were explicable to the
human drivers and contribute positively to the explicability

is, then, to learn to establish a general relationship between
the established measures of plan distance.
IV. PROPOSED METHODOLOGY
A. Explicability Distance
Since, without the model we do not know which plan
distance is most relevant in capturing explicability, we
present a general formulation in this section. A more
detailed formulation can be found in the following section. Let ‚àÜ be a 3-dimensional vector, such that for a
robot plan, œÄ R , derived from MR (R), and for an explicable plan œÄ H , derived from MH (R), we have ‚àÜ =
hŒ¥A (œÄ R , œÄ H ), Œ¥C (œÄ R , œÄ H ), Œ¥S (œÄ R , œÄ H )iT . We now define
explicability distance of a robot plan, Exp(œÄ R ), as a regression based function of the three plan distances, with b
as the parameter vector:
Exp(œÄ R / œÄ H ) ‚âà f (‚àÜ, b)

(4)

score of the plan but the last action has a negative impact
and decreases the score. Therefore this score and in turn
the explicability distance is not a non-decreasing function.
In essence, depending on the context, the explicability of an
action can either improve the score or worsen it.
Observation 2: A greedy method that expands a node
with the highest explicability score of the corresponding
partial plan at each step does not guarantee to find an
optimal explicable plan (one of the plans with the highest
explicability score) as its first solution.
The above observation is easy to see since, if e1 is
explicability score of the first plan, then a node may exist
in open list (set of unexpanded nodes) whose explicability
score is less than e1 , which when expanded may result in a
solution plan with explicability score higher than e1 .
2) Reconciliation Search: Given the non-monotonic nature of explicability distance function, we have to generate
all the candidate plans in order to find the most explicable
plan. Here, we present a cost-bounded anytime greedy search
algorithm called reconciliation search that generates all the
valid loopless candidate solution plans up to a given cost
bound, and then progressively searches for plans with better
explicability scores. The value of the heuristic h(v) in a
particular state v encountered during search is based entirely
on the explicability distance of the robot plan prefix up to
that state, given by,
h(v) = Exp(œÄ / œÄh )
s.t. ŒìMR (R) (I, œÄ) = v

Algorithm 1 Reconciliation Search
Input: Planning problem P = hMR (R), I, Gi, cost bound
max cost, and explicability distance function Exp
Output: Robot plan with the highest explicability score
œÄ R = arg maxœÄR Exp(œÄ R / œÄH )
1: S ‚Üê ‚àÖ
. Candidate plan solution set
2: open ‚Üê ‚àÖ
. Open list
3: closed ‚Üê ‚àÖ
. Closed list
4: open.insert(I, 0, inf)
5: while open 6= ‚àÖ do
6:
n ‚Üê open.remove()
. Node with highest h(¬∑)
7:
if n |= G then
8:
S.insert(œÄ s.t. ŒìMR (R) (I, œÄ) |= v)
9:
end if
10:
closed.insert(n)
11:
for each v ‚àà successors(n) do
12:
if v ‚àà
/ closed then
13:
if g(n) + cost(n, v) ‚â§ max-cost then
14:
open.insert(v, h(v))
15:
end if
16:
else
17:
if h(n) < h(v) then
18:
closed.remove(v)
19:
open.insert(v, h(v))
20:
end if
21:
end if
22:
end for
23: end while
24: return arg maxœÄ R ‚ààS Exp(œÄ R / œÄH )

and ŒìMH (R) (I, œÄh ) = v
Since we want to find explicable plans which are within
a cost bound, we use the cost of the plan to prune the
nodes in the search graph whenever they exceed the given
maximum cost bound. We implement this search in the
Fast-Downward planner [5]. The approach is described
in detail in Algorithm 1.
At each iteration of the algorithm, the plan prefix of the
robot model is compared with the explicable trace œÄh (these
are the plans generated by the human mental model of the
robot MH (R) up to the current state in the search process)
for the given problem. Using the computed distances, we
predict the explicability score for every candidate robot plan.
The search algorithm then makes a locally optimal choice
of states. After generating the first solution plan we do not
stop the search but instead continue to find all the valid
loopless candidate solution plans within the given cost bound
or until the state space is completely explored. In the end, the
candidate plan with highest explicability score is returned.
V. E XPERIMENTAL A NALYSIS
A. Autonomous Car Simulation Experiment
1) Domain Model: Autonomous cars are a topic of interest from the point of view of explicability problem. In the
recent past, Google‚Äôs self-driving cars [19] have been in the
news for being ‚Äútoo safe‚Äù on the roads. These autonomous
cars governed by strict traffic rules find it hard to blend

in and make judgments that would not make sense in a
predominantly human environment. At four-way stops, these
cars find it difficult to cross the intersection, while the human
drivers keep inching forward. For a robot car, such situations,
where it does not make an explicable decision can pose
problems, and all the human drivers who come into contact
with such cars would have to face the brunt of it.
For these reasons, we focused our studies on a simulated
autonomous car environment, and investigated how the robot
car‚Äôs inexplicable behavior can be avoided by generating
plans with respect to their explicability scores. In our robot
car model (written in PDDL), we try to capture bad driving
etiquette commonly seen on roads, such as, driving below
speed limit in passing lanes, overtaking from the wrong
side, turning and changing lanes without showing signal,
not following the move over law, and so on. The human
mental model of the robot car is defined as per test subjects
assumptions of how the robot car should perform actions.
From the robot model MR (R), we generated 40 plans for
16 different problems. The plans consisted of both explicable
and inexplicable robot car behaviors. These plans were
assessed by 20 test subjects, with each subject evaluating 8
plans. Also, each plan was evaluated by 4 different subjects,
in order to get a general understanding of the assumptions
of different human drivers. Therefore, the overall number of

(a)

(b)

(c)

Fig. 4: Autonomous Car Domain Simulation. Here the red colored car in the images is the robot car and the rest of the cars
are assumed to be human drivers. (a) The cop car is parked on the rightmost lane, and the robot car is following through
the Move Over Law maneuver. (b) The robot car is wrongly trying to overtake from the rightmost lane. (c) The robot car
is waiting at a four-way stop intersection even though it is the turn of the robot car to cross over.

Fig. 5: Here AD, CLD, SD and Score represent action
distance, causal link distance, state sequence distance, and
explicability scores respectively. This is a correlation matrix
for the aforementioned metrics. The red color represents
the negative correlation that exists between the distance
measures and the scores.

training samples was 160. The test subjects were required to
have sufficient real-life driving experience. The assessment
had two parts: one part involved scoring each robot car action
with 1, if explicable, and 0 otherwise (the explicability score
of the overall plan is calculated as the fraction of actions
in the plan that were labeled as explicable); the other part
involved answering a questionnaire aimed at understanding
test subject‚Äôs assumptions regarding the robot car. The information from this questionnaire was used to design the human
mental model MH (R) of the robot car.
The PDDL domain of the robot car, MR (R), consists
of lane and car objects as shown in Figure 4. The red car

is the robot car in the experiments and all other cars seen
in the experiments are assumed to have human drivers.
The car objects are associated with predicates defining the
location of a car on a lane segment, status of left and right
turn lights, status of car being within speed limit, presence
of a parked cop car, and so on. The actions possible
in the domain are with respect to the robot car. These
actions are Accelerate, Decelerate, LeftSqueeze,
RightSqueeze, LeftLightOn, LeftLightOff,
RightLightOn, RightLightOff, SlowDown and
WaitAtStopSign, and so on. In order to change a
lane, three consecutive actions of either LeftSqueeze
or RightSqueeze are required to gradually move to
the other lane. The PDDL domain of the human mental
model, consists of same state predicates, but different action
representations, preconditions, effects and action-costs. Note
that even though representing the human mental model in
PDDL may seem like a strong assumption, we validated the
labels given by the human subjects with the PDDL human
model constructed from the elicited preferences and found
about 72.3% match, which indicates that MH (R) used
in the evaluations is a good approximation of the human
mental model of the robot
2) Defining the Explicability Distance: For the 22 training
problems, explicable plans with MH (R) were generated.
Since some actions are not common to both the domains and
also owing to the difference in the effects and preconditions
of the actions across domains, an explicit mapping was
defined between the actions over the two domains. This
mapping was done in the light of the plan distance operations
performed between plans in the two domains.
The correlation matrix in Figure 5 establishes the negative
correlation of the plan distance measures to the explicability
scores. From the correlation matrix it can be seen that, causal
link distance has significant negative correlation with the
explicability scores. After establishing the negative correlation, we proceed towards training our regression model called
explicability distance.

TABLE II: Parameters of Regression Models
Distance

b

w

Accuracy %
10.14

Œ¥A

0.72

-0.33

Œ¥C

0.73

-0.231

7.06

Œ¥S

0.92

-0.519

27.47

Œ¥A , Œ¥ C , Œ¥ S

0.93

0.207,-0.061,-0.626

28.02

sR
1 = b1 + w1 Œ¥A

(5)

sR
2 = b2 + w2 Œ¥C

(6)

sR
3 = b3 + w3 Œ¥S

(7)

sR
4 = b4 + w4 Œ¥A + w5 Œ¥C + w6 Œ¥S

(8)

At first, individual distances were used to fit the data in the
regression model. This resulted in a poorly learned regression
model. A linear combination of the three distances also
resulted in poor results. For regression model functions 5, 6,
7 and 8, the bias, weight and accuracy values were as shown
in Table II. From this table, we infer that the relationships
are not necessarily linear as we speculated previously. We
improve our model using Random Forest regression. Since
random forests allow selection of random subset of features
while splitting the decision node, the accuracy of our model
improves. All the three distances have statistically significant
contribution in the fitted model. We evaluate the goodness of
the fit of the model, using the coefficient of determination or
R2 . This value determines the measure by which the fitted
model can explain the variations in the target values. This
value lies between 0 to 1. Higher the R2 value, better is
the model fitted to the data. After training process the new
regression model was found to have 0.8721 R2 value. That is
to say, 87% of the variations in the features can be explained
by our model. Our model predicts the explicability distance
between the robot plans and human mental model plans, with
a high accuracy. We call this plan distance regression model
as the explicability distance.
3) Evaluation: For evaluation of our system, we tested it
on 13 different problems. We ran the algorithm with a high
cost bound, in order to cover the most explicable candidate
plans for all the problems. The results of this search process
are as shown in Figure 6, 7 and 8. From these results, we
can see that the reconciliation search is able to incrementally
develop plans with better explicability scores as shown in
Figure 6. In Figure 7, we see that for all the 13 problems
the explicability score of the optimal plans is lesser than the
final plans generated by reconciliation search. From Figure
8, we see that for the first six problems the optimal and
explicable plans have same cost but our modified planner
with reconciliation search produces explicable plan versions
for those problems. The results also clearly show that the
explicable plans can be costlier than plans that are optimal
with respect to the robot‚Äôs own model. This additional cost

Fig. 6: The graph shows that the search process finds plans
with incrementally better explicable scores. Each color line
represents the 13 different problems. The markers on the
lines represent a plan solution for that problem. The y-axis
gives the explicability scores of the plans and the x-axis gives
the solution number. Note that the curves show the nonmonotonic nature of evaluation metric in the search process.
The final output of the algorithm is, of course, the best plan
found in the search process.

Fig. 7: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their explicability scores.

Fig. 8: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their plan costs.

can be seen as the price the robot pays to make its behavior
explicable to the human.
VI. C ONCLUSION
We showed how the plan distance measures play a role in
determining the explicability of a robot plan. We evaluated
our hypothesis in the simulated Autonomous Car PDDL
domain. We generated training samples in robot‚Äôs domain
and assigned them with human scores. We also generated
plans in the human‚Äôs model to find the distances between
plans in two domains. We looked at the relationships between
scores and the distance measures of the plans. We learned
the regression model that could best capture the explicability
of the training samples. In summary, we have proved our
hypothesis that using the human‚Äôs mental model of the robot
model we can assess the explicability of a robot plan as a
function over the plan distance measures between the robot
plan and the plan that the human would expect the robot to
make. We also showed that the explicability distance measure
can be used to bias the robots planning process to generate
plans that are more in concordance with what humans expect.
We are currently in the process of incorporating this theory
into the behavior of a Fetch robot involved in delivery
tasks, to demonstrate how it improves the explicability of
the robot‚Äôs behavior.
R EFERENCES
[1] E. de Visser and R. Parasuraman, ‚ÄúAdaptive aiding of human-robot
teaming effects of imperfect automation on performance, trust, and
workload,‚Äù Journal of Cognitive Engineering and Decision Making,
vol. 5, no. 2, pp. 209‚Äì231, 2011.
[2] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram,
M. Veloso, D. Weld, and D. Wilkins, ‚ÄúPddl-the planning domain
definition language,‚Äù 1998.
[3] B. Srivastava, T. A. Nguyen, A. Gerevini, S. Kambhampati, M. B. Do,
and I. Serina, ‚ÄúDomain independent approaches for finding diverse
plans.‚Äù in IJCAI, 2007, pp. 2016‚Äì2022.

[4] T. A. Nguyen, M. Do, A. E. Gerevini, I. Serina, B. Srivastava,
and S. Kambhampati, ‚ÄúGenerating diverse plans to handle unknown
and partially known user preferences,‚Äù Artificial Intelligence,
vol. 190, no. 0, pp. 1 ‚Äì 31, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0004370212000707
[5] M.
Helmert,
‚ÄúThe
fast
downward
planning
system,‚Äù
CoRR,
vol.
abs/1109.6051,
2011.
[Online].
Available:
http://arxiv.org/abs/1109.6051
[6] H. A. Kautz and J. F. Allen, ‚ÄúGeneralized plan recognition.‚Äù in AAAI,
vol. 86, no. 3237, 1986, p. 5.
[7] M. Ramƒ±rez and H. Geffner, ‚ÄúProbabilistic plan recognition using offthe-shelf classical planners,‚Äù in Proceedings of the Conference of the
Association for the Advancement of Artificial Intelligence (AAAI 2010).
Citeseer, 2010, pp. 1121‚Äì1126.
[8] T. Chakraborti, Y. Zhang, D. E. Smith, and S. Kambhampati, ‚ÄúPlanning
with resource conflicts in human-robot cohabitation,‚Äù in Proceedings
of the 2016 International Conference on Autonomous Agents &
Multiagent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 2016, pp. 1069‚Äì1077.
[9] M. Cirillo, L. Karlsson, and A. Saffiotti, ‚ÄúHuman-aware task planning
for mobile robots,‚Äù in Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1‚Äì7.
[10] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz,
D. Smith, and S. Kambhampati, ‚ÄúPlanning for serendipity,‚Äù in
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2015.
[11] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati, ‚ÄúCoordination in human-robot teams using mental
modeling and plan recognition,‚Äù in Intelligent Robots and Systems
(IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014,
pp. 2957‚Äì2962.
[12] S. J. Levine and B. C. Williams, ‚ÄúConcurrent plan recognition and
execution for human-robot teams.‚Äù in ICAPS, 2014.
[13] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, and S. K.
Hankz Hankui Zhuo, ‚ÄúPlan explainability and predictability for
cobots,‚Äù CoRR, vol. abs/1511.08158, 2015. [Online]. Available:
http://arxiv.org/abs/1511.08158
[14] A. Dragan and S. Srinivasa, ‚ÄúGenerating legible motion,‚Äù in Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.
[15] T. W. Fong, I. Nourbakhsh, and K. Dautenhahn, ‚ÄúA survey of socially
interactive robots,‚Äù Robotics and Autonomous Systems, 2003.
[16] G. Hoffman and C. Breazeal, ‚ÄúCost-based anticipatory action selection
for human‚Äìrobot fluency,‚Äù Robotics, IEEE Transactions on, vol. 23,
no. 5, pp. 952‚Äì961, 2007.
[17] Y. Zhang, S. Sreedharan, and S. Kambhampati, ‚ÄúCapability models
and their applications in planning,‚Äù in Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems.
International Foundation for Autonomous Agents and Multiagent
Systems, 2015, pp. 1151‚Äì1159.
[18] R. P. Goldman and U. Kuter, ‚ÄúMeasuring plan diversity: Pathologies
in existing approaches and a new plan distance metric.‚Äù 2015.
[19] M. Richtel and C. Dougherty, ‚ÄúGoogle‚Äôs driverless cars run into
problem: Cars with drivers,‚Äù The New York Times, vol. 9, p. 1, 2015.

Plan Explicability and Predictability for Robot Task Planning
Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui Zhuo and Subbarao Kambhampati
Abstract--Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave unexpectedly. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in "natural ways", and work that investigated legible motion planning, there lacks a general solution for high level task planning. To address this issue, we introduce the notions of plan explicability and predictability. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with human subjects using physical robots to show the effectiveness of our approach.

I. I NTRODUCTION Intelligent robots and machines are becoming pervasive in human populated environments. Examples include robots for education, entertainment and personal assistance just to name a few. Significant research efforts have been invested to build autonomous agents to make them more helpful. These agents respond to goal specifications instead of basic motor commands, which requires them to autonomously synthesize task plans and execute those plans to achieve the goals. However, if the behaviors of these agents are incomprehensible, it can increase the cognitive load of humans and potentially introduce safety risks to them. As a result, one important requirement for such intelligent agents is to ensure that the synthesized plans are comprehensible to humans. This means that instead of considering only the planning model of the agent, plan synthesis should also consider the interpretation of the agent behavior from the human's perspective. This interpretation is related to our modeling of other agents. More specifically, we tend to have expectations of others' behaviors based on our understanding (modeling) of their capabilities, mental states and etc. If their behaviors do not match with these expectations, we would often be confused. One of the major reasons of this confusion is due to the fact that our understanding of others' models is often partial and inaccurate. This is also true when humans

interact with intelligent agents. For example, to darken a room that is too bright, a robot can either adjust the window blinds, switch off the lights, or break the light bulbs in the room. While breaking the light bulbs may well be the least costly plan to the robot under certain conditions (e.g., when the robot cannot easily move in the environment but we are unaware of it), it is clear that the other two options are far more desirable in the context of robots cohabiting with humans. One of the challenges here is that the human's understanding of the agent model is inherently hidden. Thus, its interpretation from the human's perspective can be arbitrarily different from the agent's own model. While there exists previous work that studied socially acceptable robots [11, 12, 21, 18] that interact with humans in "natural ways", and work that investigated legible motion planning [6], there lacks a general solution for high level task planning. In this paper, we introduce the notions of plan explicability and predictability which are used by autonomous agents (e.g., robots) to synthesize "explicable plans" that can be easily understood by humans. Our problem settings are as follows: an intelligent agent is given a goal by a human (so that the human knows the goal of the agent) working in the same environment and it needs to synthesize a plan to achieve the goal. As suggested in psychological studies [24, 5], we assume that humans naturally interpret a plan as achieving abstract tasks (or subgoals), which are functional interpretations of agent action sequences in the plan. For example, a robot that executes a sequence of manipulation actions may be interpreted as achieving the task of "picking up cup". Based on this assumption, intuitively, the easier it is for humans to associate tasks with actions in a plan, the more explicable the plan is. Similarly, the easier it is to predict the next task given actions in the previous tasks, the more predictable the plan is. In this regard, explicability is concerned with the association between human-interpreted tasks and agent actions, while predictability is concerned with the connections between these abstract tasks. Since the association between tasks and agent actions can be considered as a labeling process, we learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). We then use the learned model to label a new plan to compute its explicability and predictability. These measures are used by agents to proactively choose or directly synthesize plans that are more explicable and predictable without affecting the quality much. Our learning approach does not assume any prior knowledge

arXiv:1511.08158v2 [cs.AI] 12 Apr 2016

Fig. 1. From left to right, the scenarios illustrate the differences between automated task planning, human-aware planning and explicable planning (this work). In human-aware planning, the robot needs to maintain a model of the human (i.e., MH ) which captures the human's capabilities, intents and etc. In explicable planning, the robot considers the differences between its model from the human's perspective (i.e., M R ) and its own model MR .

on the human's interpretation of the agent model. We provide evaluation on a synthetic domain in simulation and with human subjects using physical robots to demonstrate the effectiveness of our approach. II. R ELATED W ORK To build autonomous agents (e.g., robots), one desirable capability is for such agents to respond to goal-oriented commands via automated task planning. A planning capability allows agents to autonomously synthesize plans to achieve a goal given the agent model (MR as shown in the first scenario in Fig. 1) instead of following low level motion commands, thus significantly reducing the human's cognitive load. Furthermore, to work alongside of humans, these agents must be "human-aware" when synthesizing plans. In prior works, this issue is addressed under human-aware planning [22, 4, 2] in which agents take the human's activities and intents into account when constructing their plans. This corresponds to human modeling in human-aware planning as shown in the second scenario in Fig. 1. A prerequisite for human-aware planning is a plan recognition component, which is used to infer the human's goals and plans. This information is then used to avoid interference, and plan for serendipity and teaming with humans. There exists a rich literature on plan recognition [14, 3, 20, 16], and many recent works use these techniques in human-aware planning and human-robot teaming [23, 2, 25]. While our work on plan explicability and predictability falls within the scope of human-in-the-loop planning (which also includes human-aware planning), it differs significantly from the previous work. This is illustrated in Fig. 1. More specifically, in human-aware planning, the challenge is to obtain the human model (MH in Fig. 1) which captures human capabilities [26], intents [23, 2] and etc. The modeling in this work is one level deeper: it is about the interpretation of the agent model from the human's perspective (M R in Fig. 1). In other words, R needs to understand the model of itself in H 's eyes. This information is inherently hidden, difficult to

convey, and can be arbitrarily different (e.g., having different representations) from R's own model (MR in Fig. 1). There exists work on generating legible robot motions [6] which considers a similar issue in motion planning. We are, on the other hand, concerned with task planning. Note that two different task plans may map to exactly the same motions which can be interpreted vastly differently by humans. In such cases, considering only motion becomes insufficient. Nevertheless, there exists similarities between [6] and our work. For example, legibility there is analogous to predictability in ours. In the human-robot interaction (HRI) community, there exists prior works that discuss how to enable natural and fluent human-robot interaction [11, 12, 21, 18] to create more socially acceptable robots [7]. These works, however, apply only to behaviors in specific domains. Compared with model learning via expert teaching, such as inverse reinforcement learning [1] and tutoring systems [17], which is about learning the "right" model from teachers, our work, on the other hand, is concerned with learning model differences. Furthermore, as an extension to our work, when robots cannot find an explicable plan that is also cost efficient, they need to explain the situation. In this regard, our work is also related to excuse [9] and explanation generation [10]. Finally, while our learning approach appears to be similar to information extraction [19], we use the learned model to proactively guide planning instead of passively extracting information. III. E XPLICABILITY AND P REDICTABILITY In our settings, an agent R needs to achieve a goal given by a human in the same environment (so that the human knows about the goal of the robot). The agent has a model of itself (referred to as MR ) which is used to autonomously construct plans to achieve the goal. In this paper, we assume that this model is based on PDDL [8], a general planning domain definition language. As we discussed, for an agent to generate explicable and predictable plans, it must not only consider MR but also M R , which is the interpretation of MR from the human's perspective. A. Problem Formulation Keeping the problem settings in mind, given a domain, the problem is to find a plan for a given goal that satisfies the following: argmin cost(MR ) +  ∑ dist(MR , M ) R
 MR

(1)

where MR is a plan that is constructed using MR (i.e., the agent's plan), M is a plan that is constructed using M R R (i.e., the human's anticipation of the agent's plan), cost returns the cost of a plan, dist returns the distance (i.e., capturing the differences) between two plans, and  is the relative weight. The goal of Eq. (1) is to find a plan that minimizes a weighted sum of the cost of the agent plan and the differences between the two plans. Since the agent model MR is assumed to be given, the challenge lies in the second part in Eq. (1). Note that if we know M R or it can be learned, the only thing left would be to search for a proper dist function.

However, as discussed previously, M R is inherently hidden, difficult to convey, and can be arbitrarily different from MR . Hence, our solution is to use a learning method to directly approximate the returned values. We postulate that humans understand agent plans by associating abstract tasks with actions, which can be considered as a labeling process. Based on this, we assume that dist(MR , M ) can be functionally R decomposed as:

2) Predictability Labeling: Predictability is concerned with the connections between tasks in a plan. An action label for predictability is composed of two parts: a current label and a next label (i.e., L ◊ L). The current label is also the action label for explicability. The next label (similar to the current label) is used to specify the tasks that are anticipated to be achieved next. A next label with multiple task labels is interpreted as having multiple candidate tasks to achieve next; when this label is the empty set, it is interpreted as that the next task is dist(MR , M ) = F  L (MR ) (2) unpredictable, or there are no more tasks to be achieved. R Definition 2 (Plan Predictability): Given a domain, the where F is a domain specific function that takes plan labels as predictability  of a plan  is computed by a mapping,  input, and L is the labeling scheme of the human for agent F : L2   [0, 1] (with 1 being the most predictable). plans based on M R . As a result, Eq. (1) now becomes: L2  denotes the sequence of action labels for predictability. An  i example of F is given below which is used in our evaluation argmin cost(MR ) +  ∑ F  L CRF (MR |{Si |Si = L (MR )})  MR when assuming that the current and next labels are associated (3) with at most one task label: where {Si } is the set of training examples and L CRF is i[0,N ] 1|L(ai )|=1  (1L2 (ai )=L(aj )  1L2 (ai:N )= ) the learned model of L . We can now formally define plan F (L2 ) = N +1 explicability and predictability in our context. Given a plan of (8) agent R as a sequence of actions, we denote it as MR and where a (j > i) is the first action that has a different j simplified below as  for clarity: current label as ai or the last action in the plan if no such 2  = a0 , a1 , a2 , ...aN (4) action is unfound, L (ai ) returns the next label of ai and 1L2 (ai:N )= returns 1 only if the next labels for all actions after where a0 is a null action that denotes plan starting. Given the ai (including ai ) are . Eq. (8) computes the ratio between domain, we assume that a set of task labels T is provided to number of actions that we have correctly predicted the next task and the number of all actions. label agent actions: T = {T1 , T2 , ...TM } (5) B. A Concrete Example Before discussing how to learn the labeling scheme of the human from training examples, we provide a concrete example to connect the previous concepts and show how training examples can be obtained. In this example, there is a rover in a grid environment working with a human. An illustration of this example is presented in Fig. 2. There are resources to be collected which are represented as boxes. There is one storage area that can store one resource which is represented as an open box. The rover can also make observations. The rover actions include {navigate lf rom lto }, {observe l} {load l}, and {unload l}, each representing a set of actions since l (i.e., representing a location) can be instantiated to different locations (i.e., 0 - 8 in Fig. 2). navigate (or nav ) can move the rover from a location to one of its adjacent locations; load can be used to pick up a resource when the rover is not already loaded; unload can be used to unload a resource at a storage area if the area is empty; observe (or obs) can be used to make an observation. Once a location is observed, it remains observed. The goal in this example is for the rover to make the storage area non-empty and observe two locations that contain the eye symbol in Fig. 2. In this domain, we assume that there are three abstract tasks that may be used by the human to interpret the rover's plans: COLLECT (C), STORE (S) and OBSERVE (O). Note that we do not specify any arguments for these tasks (e.g., which resource the rover is collecting) since this information may not be important to the human. This also illustrates that MR and

1) Explicability Labeling: Explicability is concerned with the association between abstract tasks and agent actions; each action in a plan is associated with an action label. The set of action labels for explicability is the power set of the task labels: L = 2T (6) When an action label includes multiple task labels, the action is interpreted as contributing to multiple tasks; when an action label is the empty set, the action is interpreted as inexplicable. When a plan is labeled, we can compute its explicability measure based on its action labels in a domain specific way. More specifically, we define: Definition 1 (Plan explicability): Given a domain, the explicability  of an agent plan  is computed by a mapping, F : L  [0, 1] (with 1 being the most explicable). L above denotes the sequence of action labels for  . An example of F used in our evaluation is given below: F (L ) =
i[1,N ]

1L(ai )=

N

(7)

where N is the plan length, L(ai ) returns the action label of ai , and 1f ormula is an indicator function that returns 1 when the f ormula holds or 0 otherwise. Eq. (7) basically computes the ratio between the number of actions with non-empty action labels and the number of all actions.

fields (CRFs) [15] due to their abilities to model sequential data. An alternative would be HMMs; however, CRFs have been shown to relax assumptions about the input and output sequence distributions and hence are more flexible. The distributions that are captured by CRFs have the following form: p(x, y) = 1 A (xA , yA ) Z (9)

in which Z is a normalization factor that satisfies:
Fig. 2. Example for plan explicability and predictability with action labels (on the right) for a given plan in the rover domain.

Z=
x,y

A (xA , yA )

(10)

M R can be arbitrarily different. In Fig. 2, we present a plan of the rover as connected arrows starting from the its initial location. Human Interpretation as Training Examples: Let us now discuss how humans may interpret this plan (i.e., associating labels with actions) as the actions are observed incrementally: when labeling ai , we only have access to the plan prefix a0 , ..., ai . At the beginning for labeling a0 , the observation is that the rover starts at l5 . Given the environment and knowledge of the rover's goal, we may infer that the first task should be COLLECT (the resource from l4 ). Hence, we may choose to label a0 as ({START}, {C}). The first action of the rover (i.e., nav l5 l4 ) seems to match with our prediction. Furthermore, given that the storage area is closest to the rover's location after completing COLLECT, the next task is likely to be STORE. Hence, we may label a1 as ({C}, {S}) as shown in the figure. The second action (i.e., load l4 ) also matches with our expectation. Hence, we label a2 too as ({C}, {S}). The third action, nav l4 l1 , however, is unexpected since we predicted STORE in the previous steps. Nevertheless, we can still explain it as contributing to OBSERVE (at location l0 ). Hence, we may label this navigation action (a3 ) as ({O}, {S}). For the fourth action, the rover moves back to l4 , which is inexplicable since the rover's behavior seems to be oscillating without particular reasons. Hence, we may choose to label this action as (, ). The labeling for the rest of the plan continues in a similar manner. This thought process reflects how training examples can be obtained from human labelers. IV. L EARNING A PPROACH To compute  and  from Defs. (1) and (2) for a given plan  , the challenge is to provide a label for each action. This requires us to learn the labeling scheme of humans (i.e., L in Eq. (2)) from training examples and then apply the learned model to  (i.e., L CRF in Eq. (3)). To formulate a learning method, we consider the sequence of labels as hidden variables. The plan that is executed by the agent (which also captures the state trajectory), as well as any cognitive cues that may be obtained (e.g., from sensing) during the plan execution constitute the observations. The graphical model that we choose for our learning approach is conditional random

In the equations above, x represents the sequence of observations, y represents the sequence of hidden variables, and (xA , yA ) represents a factor that is related to a subgraph in the CRF model associated with variables xA and yA . In our context, x are the observations made during the execution of a plan; y are the action labels. Each factor is associated with a set of features that can be extracted during the plan execution. Next, we discuss some possible features that can be used for plan explicability and predictability. A. Features for Learning Given an agent plan, the immediate set of features that we have access to is the plan and its associated state trajectory. Note that the human may not be required (nor it is necessary) to fully understand this information. When the dynamics of the agent are known, given the plan, it may also be possible to derive low level motor commands that implement the motions, which can be used to extract motion related features. When the agent is equipped with sensors such as cameras and lasers, we can also extract features from sensor information. For example, from video streams and depth information, we can extract features about the environment, e.g., how crowded the workspace is. Sensor information can also be used to extract dynamic features such as the location of the human. However, note that this information will not be available during the testing phase, and thus these features need to be estimated based on other information (e.g., projected plan of the human based on plan recognition techniques [20, 16]). In this work, we use a linear chain CRF. However, our formulation is easily extensible to more general types of CRFs. Given an agent plan  = a0 , a1 , a2 , ... , each action is associated with a set of features. Hence, each training example is of the following form:
2 2 (F0 , L2 0 ), (F1 , L1 ), (F2 , L2 ), ...

(11)

where L2 i is the action label for predictability (and explicability) for ai . Fi is the set of features for ai . We discuss several feature categories in more detail below:

1) Plan Features: Given the agent model (specified in PDDL), the set of plan features for ai includes the action description and the state variables after executing the sequence of actions a0 , ..., ai from the initial state. This information can be easily extracted given the model. For example, in our rover example in Fig. 2, this set of features for a1 includes navigate, at rover l4 , at resource0 l2 , at resource1 l4 , at storage0 l3 . 2) Action Features: Action features for ai describes the motion (e.g., dynamics) of this action. These features can be used to capture, for example, smoothness of execution within and across actions. Action features sometimes serve as important cognitive cues for humans to understand agent actions. For example, an action that enables a robot to cross a river may be interpreted as swimming, pedaling, or propelling depending on how the robot motion looks like. Action features can be extracted for a plan given the dynamics of the robot. 3) Interaction Features: Interaction features are intended to capture ai 's influence on the human. For example, it can include how far the agent is from human and what the human is performing when ai is being executed. In other words, this set of features captures characteristics of the interactions between the human and agent. Interaction features can be extracted from sensor information or estimated based on the projected human plan. B. Using the Learned Model Given a set of training examples in the form of Eq. (11), we can train the CRF model to learn the labeling scheme in Eq. (3). We discuss two ways to use the learned CRF model. 1) Plan Selection: The most straightforward method is to perform plan selection on a set of candidate plans which can simply be a set of plans that are within a certain cost bound of the optimal plan. Candidate plans can also be generated to be diverse with respect to various plan distances. For each plan, the agent must first extract the features of the actions as we discussed earlier. It then uses the trained model (i.e., L CRF ) to produce the labels for the actions in the plan.  and  can then be computed given the mappings in Defs. (1) and (2). These measures can then be used to choose a plan that is more explicable and predictable. 2) Plan Synthesis: A more efficient way is to incorporate these measures as heuristics into the planning process. Here, we consider the FastForward (FF) planner with enforced hill climbing [13]. To compute the heuristic value given a planning state, we use the relaxed planning graph to construct the remaining planning steps. However, since relaxed planning does not ensure a valid plan, we can only use action descriptions as plan features for actions that are beyond the current planning state when estimating the  and  measures. These estimates are then combined with the relaxed planning heuristic (which only considers plan cost) to guide the search. The algorithm for generating explicable and predictable plans is presented in Alg 1. The capability to synthesize explicable and predictable plans is useful for autonomous agents. For example, in domains

Algorithm 1 Synthesizing Explicable and Predictable Plans Input: agent model MR , trained human labeling scheme L CRF , initial state I and goal state G. Output: EXP 1: Push I into the open set O . 2: while open set is not empty do 3: s = GetNext(O). 4: h = M AX . 5: if G is reached then 6: return s.plan (i.e., the plan that leads to s from I ). 7: end if 8: Compute all possible next states N from s. 9: for n  N do 10: Compute the relaxed plan RELAX for n. 11: Concatenate s.plan (with plan features) with RELAX (with only action descriptions) as  Ø. 12: Compute and add other relevant features.  13: Compute L2  ).  = LCRF (Ø Ø. 14: Compute  and  based on L2  for  15: Compute h = f (, , hcost ) (f is a combination function; hcost is the relaxed planning heuristic). 16: end for 17: Find the state n  N with the minimum h. 18: if h(n ) < h then 19: Clear O. 20: Push n into O. 21: else 22: Push all n  N into O. 23: end if 24: end while

where humans interact closely with robots (e.g., in an assembly warehouse), more preferences should be given to plans that are more explicable and predictable since there would be high risks if the robots act unexpectedly. One note is that the relative weights of explicability and predictability may vary in different domains. For example, in domains where robots do not engage in close interactions with humans, predictability may not matter much. V. E VALUATION We first evaluate our approach systematically on a synthetic dataset based on the rover domain. Then, we evaluate it with human subjects using physical robots to validate that the synthesized plans are more explicable to humans in a blocks world domain. A. Systematic Evaluation with a Synthetic Domain The aim is twofold here: evaluate how well the learning approach can capture an arbitrary labeling scheme; evaluate the effectiveness of plan selection and synthesis with respect to the  and  measures. 1) Dataset Synthesis: To simplify the data synthesis process, we make the following assumptions: all rover actions have the same cost; all rover actions are associated with at

most one task label (i.e., L = T {} in Eq. (6)). To construct a domain in which the optimal plan (in terms of cost) may not be the most explicable (in order to differ MR from M R ), we add "oscillations" to the plans of the rover. These oscillations are incorporated by randomly adding locations for the rover to visit as hidden goals. For these locations, the rover only needs to visit them. As a result, it may demonstrate "unexpected" behaviors given only the public goal, denoted by G, which is known to both the rover and human. We denote the goal that also includes the hidden goals as G . Given a problem with a public goal G, we implement a labeling scheme to automatically provide the "ground truth" of a rover plan, which is constructed by the rover to achieve G . Given a plan of the rover, we label it incrementally by associating each action with a current and next label. These labels are chosen from {{COLLECT}, {STORE}, {OBSERVE}, }. We denote the plan prefix a0 , ...ai for a plan  as i , the state after applying i as si from the initial state, and a plan that is constructed from si to achieve G (i.e., using si as the initial state) as P (si ). For the current label of ai : 1) If |P (si )|  |P (si-1 )|, we label ai as  (i.e., inexplicable). This rule means that humans may label an action as inexplicable if it does not contribute to achieving G. 2) If |P (si )| < |P (si-1 )|, we label ai based on the distances from the current rover location to the targets (i.e., storage areas or observation locations), current state of the rover (i.e., loaded or not), and whether ai moves the rover closer to these targets. For example, if the closest target is a storage area and the rover is loaded, we label ai as {STORE}. When there are ties, we label ai as  (i.e., unclear and hence interpreted as inexplicable). For the next label of ai : 1) This label is determined by the target that is closest to the rover state after the current task is achieved. When there are ties, ai is labeled as  (i.e., unclear and hence interpreted as unpredictable). If the current label is , we also label ai as  (i.e., unpredictable). 2) If the current task is also the last task, we label ai as  since there is no next task. For evaluation, we define F and F as in Eqs. (7) and (8). We randomly generate problems in a 4 ◊ 4 environment. For each problem, we randomly generate 1 - 3 resources as a set RE, 1 - 3 storage areas as a set ST, 1 - 3 observation locations as a set OB. The public goal G of a problem, first, includes making all storage areas non-empty. To ensure a solution, we force |RE | = |ST | if |RE | < |ST |. Furthermore, the rover must make observations at the locations in OB. G for the rover includes G above, as well as a set of hidden goals. Locations of the rover, RE, ST, OB and hidden goals are randomly generated in the environment and do not overlap in the initial state. Although seemingly simple, the state space of this domain is on the order of 1020 . 2) Results: We use only plan features here. First, we evaluate our approach to learning the labeling scheme (i.e.,  L CRF ) as the difference between MR and MR gradually

Fig. 3. Evaluation for predicting  and  measures as the difference between MR and M R increases (i.e., as the maximum number of hidden goals increases).

increases (i.e., as the number of hidden goals increases). Afterwards, we evaluate the effectiveness of plan selection and synthesis with respect to the  and  measures. To verify that our approach can generalize to different problem settings, we fix the level of oscillation when generating training samples while allowing it to vary in testing samples. Using CRFs for Plan Explicability and Predictability: In this evaluation, we randomly generate 1 - 3 hidden goals to include in G in 1000 training samples. After the model is learned, we evaluate it on 100 testing samples in which we vary the maximum number of hidden goals from 1 to 6 with step size 1. The result is presented in Fig. 3. We can see that the prediction performance (i.e., the ratios between  and   computed based on L CRF and L ) is generally between 50% - 150%, We can also see that the oscillation level does not seem to influence the prediction performance much. This shows that our approach is effective whether MR and M R are similar or largely different. Selecting Explicable and Predictable Plans: We evaluate plan selection using  and  measures and compare the selected plans (denoted by EXPD-SELECT) with plans selected by a baseline approach (denoted by RAND-SELECT). Given a set of candidate plans, EXPD-SELECT selects a plan according to the highest predicted explicability or predictability measure while RAND-SELECT randomly selects a plan from the set of candidate plans. To implement this, for a given public goal G, we randomly construct 20 problems with a given level of oscillation as determined by the maximum number of hidden goals. Each such problem corresponds to a different G and a plan is created for it. The set of plans for these 20 problems associated with the same G is the set of candidate plans for G. For each level of oscillation, we randomly generate 50 different Gs and then construct the set of candidate plans for each G. The model here is trained with 1900 samples using the same settings as in our first evaluation and we gradually increase the level of oscillation. We compare the  and  values computed from the ground truth labeling of the chosen plans. The result is provided in Fig. 4. When the oscillation is small, the performances of both approaches are similar. As the oscillation increases, the performances of the two approaches diverge. This is expected since RAND-SELECT randomly chooses plans and hence its performance should decrease as the oscillation increases. On

Fig. 4.

Comparison of EXPD-SELECT and RAND-SELECT Fig. 5.

Comparison of FF and FF-EXPD considering only .

the other hand, EXPD-SELECT is not influenced as much although its performance also tends to decrease. This is partly due to the fact that the model used in this evaluation is trained with samples having a maximum of 3 hidden goals. In Fig. 4 for explicability, almost all results are significantly different at 0.001 level (except at 1); for predictability, results are significantly different at 0.01 level at 3, 5 and 6. The trend to diverge is clearly present. Note that we use linearchain CRFs in our evaluations, which does not directly model correlations among observations across states. These features are common in our rover domain (e.g., navigating back and forth). Hence, we can anticipate performance improvement with more general CRFs. Synthesizing Explicable and Predictable Plans: We evaluate here plan synthesis using Alg. 1. More specifically, we compare FF planner that considers the predicted  and  values in its heuristics with a normal FF planner that only considers the action cost. The FF planner with the new heuristic is called FF-EXPD. In this evaluation, we set the maximum number of hidden locations to visit to be 6. For each trial, we generate 100 problems and apply both FF and FF-EXPD to solve the problems. Given that we are interested in comparing the cases when explicability is low, we only consider problems when the predicted plan explicability for the plan generated by FF is below 0.85. First, we consider the incorporation of  only. The result is presented in Fig. 5. For the explicability measure, we see a significant difference in all trials. Another observation is that the difference in plan predictability is present but not as significant. This evaluation suggests that our heuristic search can produce plans of high explicability. Next, we consider the incorporation of  only. The result is presented in Fig. 6. Similarly, we see a significant difference in all trials for both explicability and predictability. One observation is that improving on plan predictability also improves plan explicability which is expected given Eqs. (7) and (8)). Plan Cost: We consider plan cost here for the evaluation in Fig. 6. The result is presented in Table I. We can see that the plan length for FF-EXPD is longer than the plan produced by FF in general. This is expected since FF only considers plan cost. However, in all settings, FF-EXPD penalizes the plan cost slightly (about 10%) to improve the plan explicability and predictability measures.

Fig. 6.

Comparison of FF and FF-EXPD considering only  . TABLE I P LAN S TEPS C OMPARISON FOR F IG . 6

Trial ID FF (avg. # steps) FF-EXPD (avg. # steps)

1 21.9 23.5

2 24.0 26.3

3 24.1 25.2

4 23.9 24.0

5 22.1 23.4

6 22.4 25.0

B. Evaluation with Physical Robots In this section we evaluate our approach in a blocks world domain with a physical robot. It simulates a smart manufacturing environment where robots are working beside humans. Although the human and robot do not have direct interactions ≠ the robot's goal is independent of the human's, generating explicable plan is still an important issue since it will help humans concentrate more on their own tasks. Here, we evaluate plans generated by the robot using FF-EXPD and a cost-optimal planner (OPT) in various scenarios and compare the plans with human subjects in terms of explicability. 1) Domain Description: In this domain, the robot's goal (which is known to the human) is to build a tower of a certain height using blocks on the table. The towers to be built have different heights in different problems. There are two types of blocks, light ones and heavy ones, which are indistinguishable externally but the robot can identify them based on the markers. Picking up the heavy blocks are more costly than the light blocks for the robot. Hence, the robot may sometimes choose seemingly more costly (i.e., longer) plans to build a tower from the human's perspective. 2) Experimental Setup: We generated a set of 23 problems in this domain in which towers of height 3 are to be built. The plans for these problems were manually generated and labeled as the training set. For 4 out of these 23 problems, the optimal plan is not the most explicable plan. To remove the influence of

grounding, we also generated permutations of each plan using different object names for these 23 problems, which resulted in a total of about 15000 training samples. We then generated a set of 8 testing problems for building towers of various heights (from 3 - 5) to verify that our approach can generalize. Testing problems were generated only for cases where plans are more likely to be inexplicable. For each problem, we generated two plans, one using OPT and the other using FF-EXPD, and recorded the execution of these plans on the robot. We recruited 13 subjects on campus and each human subject was tasked with labeling two plans (generated by OPT and FFEXPD respectively) for each of the 8 testing problems, using the recorded videos and following a process similar to that used in preparing training samples. After labeling each plan, we also asked the subject to provide a score (1 - 10 with 10 being the most explicable) to describe how comprehensible the plan was overall. 3) Results: In this evaluation, we only use one task label "building tower". For all testing problems, the labeling process results in 77.8% explicable actions (i.e., actions with a task label) for OPT and 97.3% explicable actions for FFEXPD. The average explicability measures for FF-EXPD and OPT are 0.98 and 0.78, and the average scores are 9.65 and 6.92, respectively. We analyze the results using a paired Ttest which shows a significant difference between FF-EXPD and OPT in terms of the explicability measures (using Eq. (7)) computed from the human labels and the overall scores (p < 0.001 for both). Furthermore, after normalizing the scores from the human subjects, the Cronbach's  value shows that the explicability measures and the scores are consistent for both FF-EXPD and OPT ( = 0.78, 0.67, respectively). These results verify that: 1) our explicability measure does capture the human's interpretation of the robot plans and 2) our approach can generate plans that are more explicable to humans. In Fig. 7, we present the plans for a testing scenario. The left part of the figure shows the plan generated by OPT and the right part shows the plan generated by FF-EXPD. A video is also attached showing the different behaviors with the two planners in this scenario. VI. C ONCLUSION While we are still far from having intelligent robots and agents working side-by-side of humans as teammates (rather than as tools), it becomes increasingly important to consider issues when such autonomous agents appear in our everyday life. These agents need to create and execute complex plans. In this paper, we introduced plan explicability and predictability for such agents so that they can synthesize plans that are more comprehensible to humans. To achieve this, they must consider not only their own models but also the human's interpretation of their models. To the best of our knowledge, this is the first attempt to model plan explicability and predictability for task planning which differs from previous work on humanaware planning. The proposed measures have a variety of applications (e.g., achieving fluent human-robot interaction and ensuring human safety). To compute these measures, we learn

Fig. 7. Plan execution of two plans generated by OPT (left) and FF-EXPD (right) for one out of the 8 testing scenarios. The top figure shows the setup of this scenario where the goal is to build a tower of height 3. The block that is initially on the left side of the table is a heavy block. The optimal plan involves more actions with the light blocks (i.e., putting the two light blocks on top of the heavy one) while the explicable plan is more costly since it requires moving the heavy one.

the labeling scheme of humans for agent plans from training examples based on CRFs. We then use this learned model to label a new plan to compute its explicability and predictability. The proposed approach is evaluated on a synthetic domain and with human subjects using physical robots to show its effectiveness. A natural extension of our work is to consider human-robot teaming where there exists close interactions. Humans in our current settings are observers. Finally, while we focus on the explicability and predictability measures for robot task planning, they also have many other interesting applications. For example, many defense applications use planning to create unpredictable and inexplicable plans, which can help deter or confuse enemies and are also useful for testing defenses against novel or unexpected attacks. These applications can be implemented using our approach by minimizing the  and  measures instead of maximizing them.

R EFERENCES [1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML '04, 2004. [2] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith, and Subbarao Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. [3] Eugene Charniak and Robert P. Goldman. A bayesian model of plan recognition. Artificial Intelligence, 64(1): 53 ≠ 79, 1993. ISSN 0004-3702. [4] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1≠7, June 2009. [5] Gergely Csibra and Gy® orgy Gergely. Obsessed with goals?: Functions and mechanisms of teleological interpretation of actions in humans. Acta psychologica, 124 (1):60≠78, 2007. [6] Anca Dragan and Siddhartha Srinivasa. Generating legible motion. In Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013. [7] Terrence W Fong, Illah Nourbakhsh, and Kerstin Dautenhahn. A survey of socially interactive robots. Robotics and Autonomous Systems, 2003. [8] Maria Fox and Derek Long. Pddl2.1: An extension to pddl for expressing temporal planning domains. J. Artif. Int. Res., 20(1), December 2003. [9] Moritz Gbelbecker, Thomas Keller, Patrick Eyerich, Michael Brenner, and Bernhard Nebel. Coming up with good excuses: What to do when no plan can be found. In International Conference on Automated Planning and Scheduling, 2010. [10] Marc Hanheide, Moritz Gbelbecker, Graham S. Horn, Andrzej Pronobis, Kristoffer Sj, Alper Aydemir, Patric Jensfelt, Charles Gretton, Richard Dearden, Miroslav Janicek, Hendrik Zender, Geert-Jan Kruijff, Nick Hawes, and Jeremy L. Wyatt. Robot task planning and explanation in open and uncertain worlds. Artificial Intelligence, 2015. ISSN 0004-3702. [11] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human≠robot fluency. Robotics, IEEE Transactions on, 23(5):952≠961, 2007. [12] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team. In Proceedings of the ACM/IEEE international conference on Human-robot interaction, pages 1≠8. ACM, 2007. [13] J® org Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. J. Artif. Int. Res., 14(1):253≠302, May 2001. ISSN 10769757. [14] Henry A. Kautz and James F. Allen. Generalized Plan [15]

[16]

[17] [18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

Recognition. In National Conference on Artificial Intelligence, pages 32≠37, 1986. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01, pages 282≠289, 2001. ISBN 1-55860-778-1. Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot teams. In Twenty-Fourth International Conference on Automated Planning and Scheduling, 2014. Tom Murray. Authoring Intelligent Tutoring Systems: An analysis of the state of the art. Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao Kambhampati. Automated planning for peerto-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. Fuchun Peng and Andrew McCallum. Information extraction from research papers using conditional random fields. Information Processing & Management, 42(4): 963 ≠ 979, 2006. ISSN 0306-4573. Miquel Ram¥ irez and Hector Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010. Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal. Improved human-robot team performance using chaski, a human-inspired plan execution system. In Proceedings of the 6th international conference on Human-robot interaction, pages 29≠36. ACM, 2011. E.A Sisbot, L.F. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. Robotics, IEEE Transactions on, 23(5):874≠883, Oct 2007. Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias Scheutz, and Subbarao Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠ 2962, Sept 2014. Robin R. Vallacher and Daniel M. Wegner. What do people think they're doing? action identification and human behavior. Psychological Review, 94(1):3≠15, 1987. Yu Zhang, Vignesh Narayanan, Tathagata Chakraborty, and Subbarao Kambhampati. A human factors analysis of proactive assistance in human-robot teaming. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. Yu Zhang, Sarath Sreedharan, and Subbarao Kambhampati. Capability models and their applications in planning. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '15, 2015.

TweetSense: Context Recovery for Orphan Tweets by Exploiting Social Signals in Twitter
Manikandan Vijayakumar, Tejas Mallapura Umamaheshwar Subbarao Kambhampati
Arizona State University,Tempe, AZ 85281

Kartik Talamadupula
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598

krtalamad@us.ibm.com

{manikandan.v,tejas.m.u,rao}@asu.edu ABSTRACT
As the popularity of Twitter, and the volume of tweets increased dramatically, hashtags have naturally evolved to become a de facto context providing/categorizing mechanism on Twitter. Despite their wide-spread adoption, fueled in part by hashtag recommendation systems, lay users continue to generate tweets without hashtags. When such "orphan" tweets show up in a (browsing) user's time-line, it is hard to make sense of their context. In this paper, we present a system called TweetSense which aims to rectify such orphan tweeets by recovering their context in terms of their missing hashtags. TweetSense enables this context recovery by using both the content and social network features of the orphan tweet. We characterize the context recovery problem, present the details of TweetSense and present a systematic evaluation of its effectiveness over a 7 million tweet corpus. people that the originator follows (friends). Originators are most likely to use hashtags which are temporally close, and are also more likely to reuse hashtags from other users whose tweets they have favorited, retweeted, and @mentioned (a tweet that conatins "@username"). To reflect this generative model, in TweetSense, a statistical model is built to capture a set of social signals, temporal signals related to the tweet and the originator of the tweet. These features measure the tie strength between users, temporal locality and trendiness of the hashtags within the users' social graph. TweetSense learns a model to predict whether a hashtag is applicable to a tweet or not. Given a test tweet lacking a hashtag (context), the model is used to predict hashtags from a set of hashtags collected from the timeline of the creator of the test tweet.

2.

RELATED WORK

1.

INTRODUCTION

Twitter has grown beyond the role of a platform that is used merely for sharing status updates, as it was initially envisioned. Recent work including that of Java et. al. [5] has identified daily chatter, conversations, information sharing, and news reporting as some of the motivations for users that actively participate in the Twitter network. On average, a user's feed gets a few hundred new tweets every ten minutes It is hard to make sense out of such a feed unassisted, especially when many tweets appear without a hashtag. Hashtags are one of the major features of tweets (and Twitter); they are either a single word or an unspaced phrase prefixed with a pound sign #. The context of a tweet can then be described as a set of one or more hashtags. Twitter provides hashtags, partly in an attempt to organize the stream of tweets. However, using hashtags as a method to find the topic of a tweet does not always work, mainly because users do not always tag their tweets with hashtags. As an illustration, in the data that we crawled for our experimentation (all from the year 2014), 76.30% of tweets are orphan tweets. In this paper, we present the TweetSense system that helps in recovering the context of a tweet by tagging the tweet with a suitable hashtag. TweetSense captures the most relevant data from a given user's social graph in order to recover hashtag(s) for a given tweet. The underlying hypothesis is that when the creator of a tweet, called the originator, uses a hashtag (to define the context for a tweet), they are likely to reuse one or more hashtags that they see on their own timeline. This includes both tweets posted by the originator, as well as tweets created by the

A problem that is related to the context recovery problem is that of recommending a hashtag for a tweet that the originator is about to post. There has been some previous work on the hashtag recommendation problem. Eva et al. [7] present a recommender system that aims at creating a more homogeneous set of hashtags by considering similarity of tweet text. This candidate recommendation list is later refined using recently used hashtags, popularity of hashtags with in the recommendation list, and popularity of a hashtag within the underlying data set. Jieying She et al. [6] propose a TOpic MOdel-based HAshtag recommendation (TOMOHA) solution. The model learns whether the topic of a tweet is related to a topic which is local to the user or to a global background topic of the corpus. The trained model is used to recommend the most probable hashtags for a tweet. Wei Fang et al. [3] propose a Personalized Hashtag Recommendation system which suggests both content-relevant and user-relevant hashtags when users are composing tweets. The hashtag-relevant features are also used to create hybrid versions of the two systems. In the hashtag recovery problem, the time taken to predict a hashtag is not as critical as compared to a recommender system. The accuracy of prediction is more important in the problem of context recovery as we are aiding in finding the topic of the tweet rather than suggesting possible topics for the tweet being composed. The temporal information corresponding to the orphan tweet and its creator becomes very important.The problem of recovering a hashtag for tweets on a user's timeline has so far not been addressed.

3.

OVERVIEW OF TWEETSENSE

To set up the model for the problem of context recovery, given a tweet Qx created by a user Oy , we track down the most promising hashtags for it. The candidate set of tweets CTx is derived based on the generative model of our system by tracing down tweets Tx on the user Oy 's timeline. The candidate set of tweets CTx contains only the tweets that are created before the tweet Qx was created. Given a query tweet Qx , without a context created by an originator Oy appearing on the time-line of a browsing user on Twitter, a set of candidate tweets (containing hashtags) - CTxi , CHxj extracted from the social circle of the user Oy , and U - the creator of CTxi , CHxj , we want to compute P (CHxj |Qx , CTxi , Oy , U ) - which is the probability that hashtag CHxj of tweet CTxi from the candidate set CTx is actually the context of Qx . We estimate the probability discriminatively using a Logistic Regression model. The features are derived from tweets Qx and CTxi , users Oy and U . The tweet-content related features include similarity between tweet text, hashtag popularity and temporal information of the tweet. The user related features include mutual friends, mutual followers, and social signals like @mentions, favorites and common hashtags between the user who created tweet Oy , and the user U who is a part of Oy 's social network and created the tweet CTxi . The scoring methods for each feature is described in the following section.

3.1

Tweet-Content Related Features

Similarity Score: is based on the cosine similarity between the text content of the tweet Qx and the tweets contained in the set of candidate tweets CTx . We assume that the tweets in CTx that share the text content with Qx is more likely to share the hashtag with Qx . cosxi =
Q.CTxi Q CTxi

We only consider the tweets in English and ignore query tweets in other languages, special characters, emoticons, URLs, and HTTP links. We also remove stop words. Recency Score: Hashtags that are temporally close to the query tweet get a higher ranking. We determine the time window for the tweet, hashtag pair, CTxi , CHxj , using the "created at" timestamp, CR(CTxi ), associated with the tweet CTxi . We adapt the exponential decay function to compute the recency score of a hashtag. We use the exprest , where t = 60 ◊ 103 , to compute the sion e- recency score. By varying the sensitivity of the time window from 1 minute to 170 hours, we found that the results are more promising when the time window is set to 17 hours. This corresponds to a value of t equal to 60 ◊ 103 . Social Trend Score: corresponds to the popularity of hashtags within the candidate hashtag set, CHx . As the candidate hashtag set CHx is derived from the timeline of the user U who posted the tweet Qx , it is intuitive that a hashtag with high frequency is popular in the user's social network. The social trend score is computed based on the "One person, One vote" approach. It is used to get the count of frequently used hashtags in CHx . CR(Qx )-CR(CTxi )

compute a user's attention score by a weighted average sum on the conversations between two users. Let AT (TOy ) be the set of all tweets of user Oy and AT (TU ) be the set of all tweets of user U , let AT (TOy ,U ) be the set of all tweets which has @mentions and replies of Oy with U and let AT (TU,Oy ) be the set of all tweets which has @mentions and replies of U with Oy , where U is a user who belongs to the list of friends of Oy . We compute the weighted average of @men|AT (TO ,U )| tion and replies between the users as: ai,j = AT (T y ) | Oy | |AT (TOy ,U )| aj,i = |AT (TU )| Final Score = () ai,j + (1 - ) aj,i where  = 0.5 We have set  = 0.5. Favorite Score: When a user favorites a tweet posted by his friend, the user is consciously letting his friend know that he shares interest with the friend on that specific topic. Higher the number of times a user favorites a tweet of another user, higher is the favorite score. Let F V (TOy ,U ) be the set of all tweets which has favorites of Oy with U and let F V (TU,Oy ) be the set of all tweets which has favorites of U with Oy , where U is a user who belongs to the set of friends of Oy . Favorite score can be computed by us|F V (TO ,U )| |F V (TO ,U )| ing the expression: ai,j = F V (T y ) aj,i = |F V (Ty U )| | Oy | Final Score = () ai,j + (1 - ) aj,i where  = 0.5 We have set  = 0.5. Mutual Friends Score: Mutual friends score is computed to rank the friends based on their number of common friends that they share in their social network. If FOy contains set of users that are friends with user Oy and FU contains set of users that are friends with user U . We use the same Jaccard's coefficient [4] on the two set as the measure of the "mutual friends" feature. Mutual Followers Score: Mutual followers score is computed to rank friends based on the number of followers they share in their network. If F WOy contains set of users following user Oy and F WU contains set of users following user U . We use the same Jaccard's coefficient [4] on the two set as the measure of the "mutual followers" feature. Common Hashtags Score: Common hashtags score is computed between any two users based on the hashtags that are shared between them. If two users Oy and U use the same set of hashtags for a particular time window, then both the users are talking about the same topic. To compute this, we first collect the unique set of hashtags used by each user, and then use Jaccard's coefficient [4] on the hashtag sets HOy and HU . Reciprocal Score: The user might follow his friend but also follow a topic of his interest such as a news channel or a celebrity. To give more importance to a user's friends over others, the reciprocal rank assigns fixed values to classify the user's followers as a "friend", or as "not a friend". The users who follow each other will receive a fixed score of 1.0, and 0.5 other wise.

3.3

Statistical Model

3.2

User Related Features

Attention Score: If a particular user was @mentioned recently, it is more likely that they share topics of interest. This also means that they might use similar hashtags. We

The problem is modeled as shown in Figure 1. We build a Logistic Regression model based on the feature matrix extracted based on the tweets corresponding to the set of training users. Training dataset: The training data set is constructed by considering many training tweets Q. The corresponding set of candidate tweet and hashtag pairs CTx , CHx is

Characteristics Total number of users Total number of originator users Total Tweets Crawled Tweets with Hashtags Tweets without Hashtags Tweets with exactly one Hashtag Tweets with more than one Hashtag Tweets with Favorites Tweets with @mentions

Value 8,949 63 7,212,855 1,883,086 6,062,167 1,322,237 560,849 716,738 4,658,659

Percentage N/A N/A 100% 23.70% 76.30% 16.64% 7.06% 9.02% 58.63%

Table 1: Characteristics of the dataset used for the experiments user's timeline, the method can only return up to 3,200 of a user's most recent tweets from his timeline. The favorite tweets that can be crawled are limited to 200 most recent tweets per user. We randomly picked users by navigating through the trending hashtags during a fixed time interval. For each of the selected users, we crawled the most recent 1500 tweets, and further crawled recent 1500 tweets for each friend (followee) of the selected user. Since, the number of tweets crawled to build a user's social graph is directly proportional to the number of friends, we randomly constrained the user selection process to choose users with at most 300 friends. We crawled 7,212,855 tweets for 8,949 users. Further details about the characteristics of the dataset can be found in Table2.

Figure 1: Training the Model from Tweets With Hashtags to Predict the Hashtags for Tweets Without Hashtag identified. Here, the candidate set of tweets are the tweets from the timeline of the user Oy who posted the tweet Qx containing the hashtag CHx . For each candidate tweet, and candidate hashtag pair CTxi , CHxj created by user U in the candidate tweet set, the feature scores are computed with respect to the Qx , and user Oy . The training dataset is a feature matrix containing the feature vectors of all CTxi , CHxj pair corresponding to all training tweets Q. The class label for a feature vector is 1 if the hashtag CHxj in the candidate set of tweets is equal to the hashtag in Qx , the tweet at consideration, and 0 otherwise. Handling unbalanced training set: The training dataset has a class distribution of 95% negative samples and 5% positive samples. Learning the model from an unbalanced dataset will cause very low precision. We use the Synthetic Minority Oversampling Technique (SMOTE) [2] to re-sample the unbalanced dataset to a balanced dataset with 50% positive samples and 50% negative samples. Classifier learning: We apply the Logistic regression to learn a statistical model from the training dataset to predict the probabilities of the top K most promising hashtags for a given test tweet. Logistic regression assumes that all data points share the same parameter vector with the test tweet. Using the Classifier: For each test tweet, its candidate set of tweet-hashtag pairs are tracked down and feature vectors are computed. When the test dataset is passed to the learned model, it predicts the maximum likelihood probability for each of the candidate hashtags CHxj in tweet hashtag pairs CTxi , CHxj corresponding to the test tweet. The candidate hashtags with predicated class label as 1 are then ranked using the probabilities.

5.

EMPIRICAL EVALUATION

We present an internal and external evaluation of TweetSense. The testing dataset comprised of tweets that had exactly one hashtag associated with it. The hashtag was removed for the purpose of testing, and this served as the ground truth for the test tweet.

5.1

External Evaluation

The closest related work for the problem of context recovery is the problem of recommending hashtags. Therefore, we choose the system proposed by Eva et al. [7] as our baseline. Their system aims at creating a more homogeneous set of hashtags by considering the similarity of tweet text to create a candidate recommendation list. This candidate recommendation list is later refined using recently used hashtags, and popularity of hashtags with in the candidate recommendation list. External Evaluation Of TweetSense Based On Precision at N :

Figure 2: External evaluation againt state-of-the-art system for Precison @ N Our system was able to recommend correct hashtags for precision at 20 for 59% of the tweets, which in general is above 50%. Also compared to the best possible ranking method of the baseline model which could recommend cor-

4.

EXPERIMENTAL SETUP

Dataset: We use Sprintze [1] to crawl Twitter data through the Twitter Streaming API. In order to crawl a

rect hashtags for 35% of the tweets.On an average, TweetSense dominates the baselines for different values of N . A user tweets about his interests and also about what he is exposed to on his timeline. A user would rarely use a hashtag, which he has never seen. There are many indicators that indicate how a user adapts hashtags and most of these are related to user's social network. TweetSense picks the most suitable set of hashtags as candidate hashtag set by looking at the user's timeline rather than at global Twitter ecosystem. We have identified different features that can further help in determining the most important indicator of all the indicator by assuming the user's environment at the time of the creation of a tweet. These indicators change with time, and we also model this by considering different set of candidate hashtags for the same user for different tweets.

All Features Similarity Score Recency Score Social Trend Score Attention Score Favorite Score Mutual Friends Score Mutual Followers Score Common Hashtag Score Reciprocal Score

Exp1 0.0942 0.0022 0.0017 0 0.2837 13538.65 0.0923 0 0.7144

Exp2 0.1123 0.0024 0.0017 0 0.24 N/A 3.115 0 0.7717

Exp3 0.1134 0.0026 0.0016 0 0.2112 N/A N/A 0 N/A

Exp4 N/A N/A N/A N/A N/A 0.2081 N/A N/A N/A

Table 2: Estimation of Odds Ratio by Feature Selection Friends" feature. As we could see, the score is low in this case while it is higher when the model was built with all other features. This indicates that we require all the other features along with the "Mutual Friends" feature to make better predictions. All these experiments emphasize the fact that social features rather than the tweet-content related features are the most important features in recovering context of an orphan tweet. Results on Accuracy of Ranking based on Rank Position: The accuracy on hashtags recommended by the system is shown by determining the ranking positions of the top 10 recommended hashtags for the test dataset.Results for each ranking position as follows: Rank1-27.75%,Rank221.53%,Rank3-13.56%,Rank4-12.28%, Rank5-6.70%,Rank65.10%, Rank7-4.31%,Rank8-2.07%, Rank9-3.51%, Rank103.19%.The consistent performance by the system for the top four positions of the top K ranking positions imply that the system is more accurate.

5.2

Internal Evaluation

The correctness of the system is evaluated using Precision at N . We compare the importance of different features in the model by using odds ratio. Results of Internal Evaluation Of Precision at N by Varying the Training Dataset: We compare the precision at N at 5,10,15, and 20 of the proposed system. Our approach gets better precision as the size of N is increased. For a total sample size of 1599 random tweets with hashtags whose hashtags are deliberately removed for evaluation. At the value of N = 5, 720/1599 sample tweets are recommended with the correct hashtags. Similarly, 849/1599 at N = 10, 901/1599 at N = 15 and 944/1599 at N = 20 are predicted correctly.

6.

CONCLUSION

Figure 3: Precision at N = 5, 10, 15, and 20 on Varying the Size of the Training Dataset. Results for Estimation of Odds Ratio by Feature Selection: We measure the association between an exposure and an outcome using odds ratio. In the Table 2, Exp1 column indicates that "Mutual Friends" feature is contributing the most to the odds of the outcome when compared to the other features. This reinforces the hypothesis that the social signals are more important than the tweet-content related features while predicting hashtags. In order to validate whether the prediction capability of the model is based solely on a single feature, we created a model by ignoring the "Mutual Friends" feature during the training phase. In this case - Exp2, as shown in the Table 2, we can see that the "Mutual Followers" feature becomes very important. There could be a correlation between the two features because of feature redundancy. In Exp3, we remove most of the social features - "Mutual Friends", "Mutual Followers", and "Reciprocal" features to build a model. We can observe that the odds ratio of the features being considered do not improve significantly. In Exp4, we ignore all the features, but the "Mutual Friends" feature to build a model to further verify the importance of the "Mutual

In this paper, we defined and motivated the context recovery problem from orphan tweets. We then described TweetSense a discriminative learning approach for recovering the context of the orphan tweets in terms of their missing hashtags. TweetSense uses a variety of features drawn from the timeline, content and social network. Our experiments on a large tweet corpus demonstrate the effectiveness of TweetSense. Acknowledgments: We gratefully acknowledge the significant help from Sushovan De in this research. This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13- 1-0176, N00014-13-10519 and N00014-15-1-2027, and a Google faculty research award.

References
[1] Twitter"s streaming api ,https://blog.gnip.com/tag/spritzer/. [2] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. arXiv preprint arXiv:1106.1813, 2011. [3] W. Feng and J. Wang. We can learn your hashtags: Connecting tweets to explicit topics. In Data Engineering (ICDE), 2014 IEEE 30th International Conference on, pages 856≠867, March 2014. ¥ [4] P. Jaccard. Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci¥ et¥ e Vaudoise des Sciences Naturelles, 37:547≠579, 1901. [5] A. Java, X. Song, T. Finin, and B. Tseng. Why we twitter: Understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop on Web Mining and Social Network Analysis, WebKDD/SNA-KDD '07, pages 56≠65, New York, NY, USA, 2007. ACM. [6] J. She and L. Chen. Tomoha: Topic model-based hashtag recommendation on twitter. In Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion, WWW Companion '14, pages 371≠ 372, Republic and Canton of Geneva, Switzerland, 2014. International World Wide Web Conferences Steering Committee. [7] E. Zangerle, W. Gassler, and G. Specht. On the impact of text similarity functions on hashtag recommendations in microblogging environments. Eva2013, 3(4):889≠898, 2013.

TweetSense: Context Recovery for Orphan Tweets by
Exploiting Social Signals in Twitter
Manikandan Vijayakumar,
Tejas Mallapura Umamaheshwar
Subbarao Kambhampati

Kartik Talamadupula
IBM T.J. Watson Research Center,
Yorktown Heights, NY 10598

krtalamad@us.ibm.com

Arizona State University,Tempe, AZ 85281

{manikandan.v,tejas.m.u,rao}@asu.edu
ABSTRACT
As the popularity of Twitter, and the volume of tweets increased dramatically, hashtags have naturally evolved to become a de facto context providing/categorizing mechanism
on Twitter. Despite their wide-spread adoption, fueled in
part by hashtag recommendation systems, lay users continue
to generate tweets without hashtags. When such ‚Äúorphan‚Äù
tweets show up in a (browsing) user‚Äôs time-line, it is hard
to make sense of their context. In this paper, we present a
system called TweetSense which aims to rectify such orphan
tweeets by recovering their context in terms of their missing hashtags. TweetSense enables this context recovery by
using both the content and social network features of the orphan tweet. We characterize the context recovery problem,
present the details of TweetSense and present a systematic
evaluation of its effectiveness over a 7 million tweet corpus.

1.

INTRODUCTION

Twitter has grown beyond the role of a platform that is
used merely for sharing status updates, as it was initially envisioned. Recent work including that of Java et. al. [5] has
identified daily chatter, conversations, information sharing,
and news reporting as some of the motivations for users that
actively participate in the Twitter network. On average, a
user‚Äôs feed gets a few hundred new tweets every ten minutes It is hard to make sense out of such a feed unassisted,
especially when many tweets appear without a hashtag.
Hashtags are one of the major features of tweets (and
Twitter); they are either a single word or an unspaced phrase
prefixed with a pound sign #. The context of a tweet can
then be described as a set of one or more hashtags. Twitter provides hashtags, partly in an attempt to organize the
stream of tweets. However, using hashtags as a method to
find the topic of a tweet does not always work, mainly because users do not always tag their tweets with hashtags.
As an illustration, in the data that we crawled for our experimentation (all from the year 2014), 76.30% of tweets are
orphan tweets. In this paper, we present the TweetSense
system that helps in recovering the context of a tweet by
tagging the tweet with a suitable hashtag. TweetSense captures the most relevant data from a given user‚Äôs social graph
in order to recover hashtag(s) for a given tweet. The underlying hypothesis is that when the creator of a tweet, called
the originator, uses a hashtag (to define the context for a
tweet), they are likely to reuse one or more hashtags that
they see on their own timeline. This includes both tweets
posted by the originator, as well as tweets created by the

people that the originator follows (friends). Originators are
most likely to use hashtags which are temporally close, and
are also more likely to reuse hashtags from other users whose
tweets they have favorited, retweeted, and @mentioned (a
tweet that conatins ‚Äú@username‚Äù).
To reflect this generative model, in TweetSense, a statistical model is built to capture a set of social signals,
temporal signals related to the tweet and the originator
of the tweet. These features measure the tie strength between users, temporal locality and trendiness of the hashtags
within the users‚Äô social graph. TweetSense learns a model
to predict whether a hashtag is applicable to a tweet or not.
Given a test tweet lacking a hashtag (context), the model
is used to predict hashtags from a set of hashtags collected
from the timeline of the creator of the test tweet.

2.

RELATED WORK

A problem that is related to the context recovery problem is that of recommending a hashtag for a tweet that the
originator is about to post. There has been some previous
work on the hashtag recommendation problem. Eva et al.
[7] present a recommender system that aims at creating a
more homogeneous set of hashtags by considering similarity
of tweet text. This candidate recommendation list is later
refined using recently used hashtags, popularity of hashtags
with in the recommendation list, and popularity of a hashtag
within the underlying data set. Jieying She et al. [6] propose a TOpic MOdel-based HAshtag recommendation (TOMOHA) solution. The model learns whether the topic of a
tweet is related to a topic which is local to the user or to a
global background topic of the corpus. The trained model is
used to recommend the most probable hashtags for a tweet.
Wei Fang et al. [3] propose a Personalized Hashtag Recommendation system which suggests both content-relevant
and user-relevant hashtags when users are composing tweets.
The hashtag-relevant features are also used to create hybrid
versions of the two systems.
In the hashtag recovery problem, the time taken to predict
a hashtag is not as critical as compared to a recommender
system. The accuracy of prediction is more important in the
problem of context recovery as we are aiding in finding the
topic of the tweet rather than suggesting possible topics for
the tweet being composed. The temporal information corresponding to the orphan tweet and its creator becomes very
important.The problem of recovering a hashtag for tweets
on a user‚Äôs timeline has so far not been addressed.

3.

OVERVIEW OF TWEETSENSE

To set up the model for the problem of context recovery,
given a tweet Qx created by a user Oy , we track down the
most promising hashtags for it. The candidate set of tweets
CTx is derived based on the generative model of our system
by tracing down tweets Tx on the user Oy ‚Äôs timeline. The
candidate set of tweets CTx contains only the tweets that
are created before the tweet Qx was created.
Given a query tweet Qx , without a context created by
an originator Oy appearing on the time-line of a browsing
user on Twitter, a set of candidate tweets (containing hashtags) - hCTxi , CHxj i extracted from the social circle of the
user Oy , and U - the creator of hCTxi , CHxj i, we want to
compute P (CHxj |Qx , CTxi , Oy , U ) - which is the probability that hashtag CHxj of tweet CTxi from the candidate set
CTx is actually the context of Qx . We estimate the probability discriminatively using a Logistic Regression model.
The features are derived from tweets Qx and CTxi , users
Oy and U .
The tweet-content related features include similarity between tweet text, hashtag popularity and temporal information of the tweet. The user related features include mutual
friends, mutual followers, and social signals like @mentions,
favorites and common hashtags between the user who created tweet Oy , and the user U who is a part of Oy ‚Äôs social
network and created the tweet CTxi . The scoring methods
for each feature is described in the following section.

3.1

Tweet-Content Related Features

Similarity Score: is based on the cosine similarity between the text content of the tweet Qx and the tweets contained in the set of candidate tweets CTx . We assume
that the tweets in CTx that share the text content with
Qx is more likely to share the hashtag with Qx . cosŒòxi =
~ CT
~xi
Q.
~ kkCT
~xi k
kQ
We only consider the tweets in English and ignore query
tweets in other languages, special characters, emoticons, URLs,
and HTTP links. We also remove stop words.
Recency Score: Hashtags that are temporally close to
the query tweet get a higher ranking. We determine the
time window for the tweet, hashtag pair, hCTxi , CHxj i, using the ‚Äúcreated at‚Äù timestamp, CR(CTxi ), associated with
the tweet CTxi . We adapt the exponential decay function to
compute the recency score of a hashtag. We use the expresCR(Qx )‚àíCR(CTxi )

t
, where t = 60 √ó 103 , to compute the
sion e‚àí
recency score. By varying the sensitivity of the time window
from 1 minute to 170 hours, we found that the results are
more promising when the time window is set to 17 hours.
This corresponds to a value of t equal to 60 √ó 103 .
Social Trend Score: corresponds to the popularity of
hashtags within the candidate hashtag set, CHx . As the
candidate hashtag set CHx is derived from the timeline of
the user U who posted the tweet Qx , it is intuitive that a
hashtag with high frequency is popular in the user‚Äôs social
network. The social trend score is computed based on the
‚ÄùOne person, One vote‚Äù approach. It is used to get the count
of frequently used hashtags in CHx .

3.2

User Related Features

Attention Score: If a particular user was @mentioned
recently, it is more likely that they share topics of interest.
This also means that they might use similar hashtags. We

compute a user‚Äôs attention score by a weighted average sum
on the conversations between two users. Let AT (TOy ) be
the set of all tweets of user Oy and AT (TU ) be the set of all
tweets of user U , let AT (TOy ,U ) be the set of all tweets which
has @mentions and replies of Oy with U and let AT (TU,Oy )
be the set of all tweets which has @mentions and replies of
U with Oy , where U is a user who belongs to the list of
friends of Oy . We compute the weighted average of @men|AT (TO ,U )|
tion and replies between the users as: ai,j = AT (T y )
|
Oy |
|AT (TOy ,U )|
aj,i = |AT (TU )|
Final Score = (Œ±) ai,j + (1 ‚àí Œ±) aj,i where Œ± = 0.5 We
have set Œ± = 0.5.
Favorite Score: When a user favorites a tweet posted
by his friend, the user is consciously letting his friend know
that he shares interest with the friend on that specific topic.
Higher the number of times a user favorites a tweet of another user, higher is the favorite score. Let F V (TOy ,U ) be
the set of all tweets which has favorites of Oy with U and
let F V (TU,Oy ) be the set of all tweets which has favorites
of U with Oy , where U is a user who belongs to the set
of friends of Oy . Favorite score can be computed by us|F V (TO ,U )|
|F V (TO ,U )|
ing the expression: ai,j = F V (T y ) aj,i = |F V (TyU )|
|
Oy |
Final Score = (Œ±) ai,j + (1 ‚àí Œ±) aj,i where Œ± = 0.5 We
have set Œ± = 0.5.
Mutual Friends Score: Mutual friends score is computed to rank the friends based on their number of common
friends that they share in their social network. If FOy contains set of users that are friends with user Oy and FU contains set of users that are friends with user U . We use the
same Jaccard‚Äôs coefficient [4] on the two set as the measure
of the ‚Äúmutual friends‚Äù feature.
Mutual Followers Score: Mutual followers score is computed to rank friends based on the number of followers they
share in their network. If F WOy contains set of users following user Oy and F WU contains set of users following user
U . We use the same Jaccard‚Äôs coefficient [4] on the two set
as the measure of the ‚Äúmutual followers‚Äù feature.
Common Hashtags Score: Common hashtags score is
computed between any two users based on the hashtags that
are shared between them. If two users Oy and U use the
same set of hashtags for a particular time window, then both
the users are talking about the same topic. To compute this,
we first collect the unique set of hashtags used by each user,
and then use Jaccard‚Äôs coefficient [4] on the hashtag sets
HOy and HU .
Reciprocal Score: The user might follow his friend but
also follow a topic of his interest such as a news channel or
a celebrity. To give more importance to a user‚Äôs friends over
others, the reciprocal rank assigns fixed values to classify
the user‚Äôs followers as a ‚Äúfriend‚Äù, or as ‚Äúnot a friend‚Äù. The
users who follow each other will receive a fixed score of 1.0,
and 0.5 other wise.

3.3

Statistical Model

The problem is modeled as shown in Figure 1. We build
a Logistic Regression model based on the feature matrix
extracted based on the tweets corresponding to the set of
training users.
Training dataset: The training data set is constructed
by considering many training tweets Q. The corresponding set of candidate tweet and hashtag pairs hCTx , CHx i is

Characteristics
Total number of users
Total number of originator users
Total Tweets Crawled
Tweets with Hashtags
Tweets without Hashtags
Tweets with exactly one Hashtag
Tweets with more than one Hashtag
Tweets with Favorites
Tweets with @mentions

Value
8,949
63
7,212,855
1,883,086
6,062,167
1,322,237
560,849
716,738
4,658,659

Percentage
N/A
N/A
100%
23.70%
76.30%
16.64%
7.06%
9.02%
58.63%

Table 1: Characteristics of the dataset used for the experiments

Figure 1: Training the Model from Tweets With Hashtags
to Predict the Hashtags for Tweets Without Hashtag
identified. Here, the candidate set of tweets are the tweets
from the timeline of the user Oy who posted the tweet Qx
containing the hashtag CHx . For each candidate tweet,
and candidate hashtag pair hCTxi , CHxj i created by user U
in the candidate tweet set, the feature scores are computed
with respect to the Qx , and user Oy .
The training dataset is a feature matrix containing the
feature vectors of all hCTxi , CHxj i pair corresponding to
all training tweets Q. The class label for a feature vector
is 1 if the hashtag CHxj in the candidate set of tweets is
equal to the hashtag in Qx , the tweet at consideration, and
0 otherwise.
Handling unbalanced training set: The training dataset
has a class distribution of 95% negative samples and 5%
positive samples. Learning the model from an unbalanced
dataset will cause very low precision. We use the Synthetic Minority Oversampling Technique (SMOTE) [2] to
re-sample the unbalanced dataset to a balanced dataset with
50% positive samples and 50% negative samples.
Classifier learning: We apply the Logistic regression to
learn a statistical model from the training dataset to predict
the probabilities of the top K most promising hashtags for
a given test tweet. Logistic regression assumes that all data
points share the same parameter vector with the test tweet.
Using the Classifier: For each test tweet, its candidate
set of tweet-hashtag pairs are tracked down and feature vectors are computed. When the test dataset is passed to the
learned model, it predicts the maximum likelihood probability for each of the candidate hashtags CHxj in tweet
hashtag pairs hCTxi , CHxj i corresponding to the test tweet.
The candidate hashtags with predicated class label as 1 are
then ranked using the probabilities.

4.

EXPERIMENTAL SETUP

Dataset: We use Sprintze [1] to crawl Twitter data
through the Twitter Streaming API. In order to crawl a

user‚Äôs timeline, the method can only return up to 3,200 of
a user‚Äôs most recent tweets from his timeline. The favorite
tweets that can be crawled are limited to 200 most recent
tweets per user.
We randomly picked users by navigating through the trending hashtags during a fixed time interval. For each of the
selected users, we crawled the most recent 1500 tweets, and
further crawled recent 1500 tweets for each friend (followee)
of the selected user. Since, the number of tweets crawled
to build a user‚Äôs social graph is directly proportional to the
number of friends, we randomly constrained the user selection process to choose users with at most 300 friends.
We crawled 7,212,855 tweets for 8,949 users. Further details about the characteristics of the dataset can be found
in Table2.

5.

EMPIRICAL EVALUATION

We present an internal and external evaluation of TweetSense. The testing dataset comprised of tweets that had
exactly one hashtag associated with it. The hashtag was
removed for the purpose of testing, and this served as the
ground truth for the test tweet.

5.1

External Evaluation

The closest related work for the problem of context recovery is the problem of recommending hashtags. Therefore,
we choose the system proposed by Eva et al. [7] as our
baseline. Their system aims at creating a more homogeneous set of hashtags by considering the similarity of tweet
text to create a candidate recommendation list. This candidate recommendation list is later refined using recently used
hashtags, and popularity of hashtags with in the candidate
recommendation list.
External Evaluation Of TweetSense Based On Precision at N :

Figure 2: External evaluation againt state-of-the-art system
for Precison @ N
Our system was able to recommend correct hashtags for
precision at 20 for 59% of the tweets, which in general is
above 50%. Also compared to the best possible ranking
method of the baseline model which could recommend cor-

rect hashtags for 35% of the tweets.On an average, TweetSense dominates the baselines for different values of N .
A user tweets about his interests and also about what
he is exposed to on his timeline. A user would rarely use a
hashtag, which he has never seen. There are many indicators
that indicate how a user adapts hashtags and most of these
are related to user‚Äôs social network. TweetSense picks the
most suitable set of hashtags as candidate hashtag set by
looking at the user‚Äôs timeline rather than at global Twitter
ecosystem. We have identified different features that can
further help in determining the most important indicator of
all the indicator by assuming the user‚Äôs environment at the
time of the creation of a tweet. These indicators change with
time, and we also model this by considering different set of
candidate hashtags for the same user for different tweets.

5.2

Internal Evaluation

The correctness of the system is evaluated using Precision
at N . We compare the importance of different features in
the model by using odds ratio.
Results of Internal Evaluation Of Precision at N
by Varying the Training Dataset: We compare the precision at N at 5,10,15, and 20 of the proposed system. Our
approach gets better precision as the size of N is increased.
For a total sample size of 1599 random tweets with hashtags
whose hashtags are deliberately removed for evaluation. At
the value of N = 5, 720/1599 sample tweets are recommended with the correct hashtags. Similarly, 849/1599 at
N = 10, 901/1599 at N = 15 and 944/1599 at N = 20 are
predicted correctly.

All Features
Similarity Score
Recency Score
Social Trend Score
Attention Score
Favorite Score
Mutual
Friends Score
Mutual
Followers Score
Common
Hashtag Score
Reciprocal Score

Results for Estimation of Odds Ratio by Feature
Selection: We measure the association between an exposure and an outcome using odds ratio. In the Table 2, Exp1
column indicates that ‚ÄúMutual Friends‚Äù feature is contributing the most to the odds of the outcome when compared to
the other features. This reinforces the hypothesis that the
social signals are more important than the tweet-content related features while predicting hashtags.
In order to validate whether the prediction capability of
the model is based solely on a single feature, we created a
model by ignoring the ‚ÄúMutual Friends‚Äù feature during the
training phase. In this case - Exp2, as shown in the Table
2, we can see that the ‚ÄúMutual Followers‚Äù feature becomes
very important. There could be a correlation between the
two features because of feature redundancy. In Exp3, we
remove most of the social features - ‚ÄúMutual Friends‚Äù, ‚ÄúMutual Followers‚Äù, and ‚ÄúReciprocal‚Äù features to build a model.
We can observe that the odds ratio of the features being
considered do not improve significantly. In Exp4, we ignore
all the features, but the ‚ÄúMutual Friends‚Äù feature to build
a model to further verify the importance of the ‚ÄúMutual

Exp2
0.1123
0.0024
0.0017
0
0.24

Exp3
0.1134
0.0026
0.0016
0
0.2112

Exp4
N/A
N/A
N/A
N/A
N/A

13538.65

N/A

N/A

0.2081

0.0923

3.115

N/A

N/A

0
0.7144

0
0.7717

0
N/A

N/A
N/A

Table 2: Estimation of Odds Ratio by Feature Selection
Friends‚Äù feature. As we could see, the score is low in this
case while it is higher when the model was built with all
other features. This indicates that we require all the other
features along with the ‚ÄúMutual Friends‚Äù feature to make
better predictions.
All these experiments emphasize the fact that social features rather than the tweet-content related features are the
most important features in recovering context of an orphan
tweet.
Results on Accuracy of Ranking based on Rank
Position: The accuracy on hashtags recommended by the
system is shown by determining the ranking positions of the
top 10 recommended hashtags for the test dataset.Results
for each ranking position as follows: Rank1-27.75%,Rank221.53%,Rank3-13.56%,Rank4-12.28%, Rank5-6.70%,Rank65.10%, Rank7-4.31%,Rank8-2.07%, Rank9-3.51%, Rank103.19%.The consistent performance by the system for the top
four positions of the top K ranking positions imply that the
system is more accurate.

6.

Figure 3: Precision at N = 5, 10, 15, and 20 on Varying the
Size of the Training Dataset.

Exp1
0.0942
0.0022
0.0017
0
0.2837

CONCLUSION

In this paper, we defined and motivated the context recovery problem from orphan tweets. We then described TweetSense a discriminative learning approach for recovering the
context of the orphan tweets in terms of their missing hashtags. TweetSense uses a variety of features drawn from the
timeline, content and social network. Our experiments on a
large tweet corpus demonstrate the effectiveness of TweetSense.
Acknowledgments: We gratefully acknowledge the significant help from Sushovan De in this research. This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13- 1-0176, N00014-13-10519 and N00014-15-1-2027, and a Google faculty research
award.

References
[1] Twitter‚Äùs streaming api ,https://blog.gnip.com/tag/spritzer/.
[2] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. arXiv preprint arXiv:1106.1813, 2011.
[3] W. Feng and J. Wang. We can learn your hashtags: Connecting tweets to explicit topics. In Data Engineering (ICDE), 2014 IEEE 30th International Conference
on, pages 856‚Äì867, March 2014.
[4] P. Jaccard. EÃÅtude comparative de la distribution florale dans une portion
des Alpes et des Jura. Bulletin del la SocieÃÅteÃÅ Vaudoise des Sciences Naturelles,
37:547‚Äì579, 1901.
[5] A. Java, X. Song, T. Finin, and B. Tseng.
Why we twitter: Understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop on Web Mining and Social Network Analysis,
WebKDD/SNA-KDD ‚Äô07, pages 56‚Äì65, New York, NY, USA, 2007. ACM.
[6] J. She and L. Chen. Tomoha: Topic model-based hashtag recommendation
on twitter. In Proceedings of the Companion Publication of the 23rd International
Conference on World Wide Web Companion, WWW Companion ‚Äô14, pages 371‚Äì
372, Republic and Canton of Geneva, Switzerland, 2014. International World
Wide Web Conferences Steering Committee.
[7] E. Zangerle, W. Gassler, and G. Specht. On the impact of text similarity functions on hashtag recommendations in microblogging environments.
Eva2013, 3(4):889‚Äì898, 2013.

Discovering Underlying Plans Based on Distributed Representations of Actions
Xin Tiana , Hankz Hankui Zhuoa & Subbarao Kambhampati b
Sun Yat-Sen University & a Arizona State University tianxin1860@gmail.com, zhuohank@mail.sysu.edu.cn, rao@asu.edu
a

arXiv:1511.05662v1 [cs.AI] 18 Nov 2015

Abstract
Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally "matching" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.

Introduction
As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning chellenge (c.f. (Cohen et al. 2015; Dong et al. 2004; Manikonda et al. 2014)). An important aspect of such a support (Kambhampati and Talamadupula 2015) is recognizing what plans the human in the loop is making, and provide appropriate suggestions about their next actions. Although there is a lot of work on plan recognition, much of it has traditionally depended on the availability of a complete domain model (Ram¥ irez and Geffner 2009a; Zhuo, Yang, and Kambhampati 2012). As has been argued elsewhere (Kambhampati and Talamadupula 2015), such models are hard to get in human-in-the-loop planning scenarios. Here, the decision support systems have to make themselves useful without insisting on complete action models of the domain. The situation here is akin to that faced by search engines and other tools for computer supported cooperate work, and is thus a significant departure for the "planning as pure inference" mindset of the automated planning community. As such, the
Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

problem calls for plan recognition with "shallow" models of the domain (c.f. (Kambhampati 2007)), that can be easily learned automatically. There has been very little work on learning such shallow models to support human-in-the-loop planning. Some examples include the work on Woogle system (Dong et al. 2004) that aimed to provide support to humans in web-service composition. That work however relied on very primitive understanding of the actions (web services in their case) that consisted merely of learning the input/output types of individual services. In this paper, we focus on learning more informative models that that can help recognize the plans under construction by the humans, and provide active support by suggesting relevant actions. To drive this process, we need to learn shallow models of the domain. We propose to adapt the recent successes of word-vector models (Mikolov et al. 2013) in language to our problem. Specifically, we assume that we have access to a corpus of previous plans that the human user has made. Viewing these plans as made up of action words, we learn word vector models for these actions. These models provide us a way to induce the distribution over the identity of each unobserved action. Given the distributions over individual unobserved actions, we use an expectationmaximization approach to infer the joint distribution over all unobserved actions. This distribution then forms the basis for action suggestions. We will present the details of our approach, and will also empirically demonstrate that it does capture a surprising amount of structure in the observed plan sequences, leading to effective plan recognition. We further compare its performance to traditional plan recognition techniques, including one that uses the same plan traces to learn the STRIPS-style action models, and use the learned model to support plan recognition.

Problem Definition
A plan library, denoted by L, is composed of a set of plans {p}, where p is a sequence of actions, i.e., p = a1 , a2 , . . . , an where ai , 1  i  n, is an action name (without any parameter) represented by a string. For example, a string unstack-A-B is an action meaning that a robot unstacks block A from block B. We denote the set of all posØ which is assumed to be known beforesible actions by A

hand. For ease of presentation, we assume that there is an empty action, ÿ, indicating an unknown or not observed acØ  {ÿ}. An observation of an unknown tion, i.e., A = A plan p ~ is denoted by O = o1 , o2 , . . . , oM , where oi  A, Ø or an empty action 1  i  M , is either an action in A ÿ indicating the corresponding action is missing or not observed. Note that p ~ is not necessarily in the plan library L, which makes the plan recognition problem more challenging, since matching the observation to the plan library will not work any more. We assume that the human is making a plan of at most length M . We also assume that at any given point, the planner is able to observe M - k of these actions. The k unobserved actions might either be in the suffiix (i.e., yet to be formed part) of the plan, or in the middle (due to observational gaps). Our aim is to suggest, for each of the k unobserved actions, m possible choices≠from which the user can select the action. (Note that we would like to keep m small, ideally close to 1, so as not to overwhelm the user) Accordingly, we will evaluate the effectiveness of the decision support in terms of whether or not the user's best/intended action is within the suggested m actions. Specifically, our recognition problem can be represented by a triple = (L, O, A). The solution to is to discover the unknown plan p ~ that best explains O given L and A. An example of our plan recognition problem in the blocks1 domain is shown below. Example: A plan library L in the blocks domain is assumed to have four plans as shown below: plan 1: pick-up-B stack-B-A pick-up-D stack-D-C plan 2: unstack-B-A put-down-B unstack-D-C put-downD plan 3: pick-up-B stack-B-A pick-up-C stack-C-B pickup-D stack-D-C plan 4: unstack-D-C put-down-D unstack-C-B put-downC unstack-B-A put-down-B An observation O of action sequence is shown below: observation: pick-up-B ÿ unstack-D-C put-down-D ÿ stack-C-B ÿ ÿ Given the above input, our DUP algorithm outputs plans as follows: pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C

Learning Vector Representations of Actions
Since actions are denoted by a name strings, actions can be viewed as words, and a plan can be viewed as a sentence. Furthermore, the plan library L can be seen as a corpus, and the set of all possible actions A is the vocabulary. We thus can learn the vector representations for actions using the Skip-gram model with hierarchical softmax, which has been shown an efficient method for learning high-quality vector representations of words from unstructured corpora (Mikolov et al. 2013). The objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document. Given a corpus C , composed of a sequence of training words w1 , w2 , . . . , wT , where T = |C|, the Skip-gram model maximizes the average log probability 1 T
T

log p(wt+j |wt )
t=1 -cj c,j =0

(1)

where c is the size of the training window or context. The basic probability p(wt+j |wt ) is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the K words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes (Mikolov et al. 2013). For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector representations for words. Instead, each inner node has an output vector vn(w,j ) , and the probability of a word being the output word is defined by
L(wt+j )-1

p(wt+j |wt ) =
i=1

 (I(n(wt+j , i + 1) = (2)

child(n(wt+j , i))) ∑ vn(wt+j ,i) ∑ vwt ) , where  (x) = 1/(1 + exp(-x)).

Our DUP Algorithm
Our DUP approach to the recognition problem functions by two phases. We first learn vector representations of actions using the plan library L. We then iteratively sample actions for unobserved actions oi by maximizing the probability of the unknown plan p ~ via the EM framework. We present DUP in detail in the following subsections.
1

L(w) is the length from the root to the word w in the binary tree, e.g., L(w) = 4 if there are four nodes from the root to w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) = root and n(w, L(w)) = w. child(n) is a fixed child (e.g., left child) of node n. vn is the vector representation of the inner node n. vwt is the input vector representation of word wt . The identity function I(x) is 1 if x is true; otherwise it is -1. We can thus build vector representations of actions by maximizing Equation (1) with corpora or plan libraries L as input. We will exploit the vector representations to discover the unknown plan p ~ in the next subsection.

Maximizing Probability of Unknown Plan p ~
With the vector representations learnt in the last subsection, a straightforward way to discover the unknown plan p ~ is to Ø such that p explore all possible actions in A ~ has the highest

http://www.cs.toronto.edu/aips2000/

probability, which can be defined similar to Equation (1), i.e.,
M

F (~ p) =
k=1 -cj c,j =0

log p(wk+j |wk )

(3)

where wk denotes the k th action of p ~ and M is the length of p ~. As we can see, this approach is exponentially hard with Ø and number of unobserved actions. respect to the size of A We thus design an approximate approach in the ExpectationMaximization framework to estimate an unknown plan p ~ that best explains the observation O. To do this, we introduce new parameters to capture "weights" of values for each unobserved action. Specifically speaking, assuming there are X unobserved actions in O, i.e., the number of ÿs in O is X , we denote these unobserved actions by a Ø1 , ..., a Øx , ..., a ØX , where the indices indicate the order they appear in O. Note that each a Øx can be any action Ø. We associate each possible value of a in A Øx with a weight, Øa Ø Ø denoted by  Øx ,x .  is a |A| ◊ X matrix, satisfying Ø o,x = 1   Ø o,x  0, 
Ø oA

An overview of our DUP algorithm is shown in Algorithm 1. In Step 2 of Algorithm 1, we initialize o,k = 1/M for Ø, if k is an index of unobserved actions in O; and all o  A Ø  o = o. otherwise, o,k = 1 and o ,k = 0 for all o  A In Step 4, we view ∑,k as a probability distribution, and Ø based on ∑,k if k is an unobserved sample an action from A action index in O. In Step 5, we only update ∑,k where k is an unobserved action index. In Step 6, we linearly project all elements of the updated  to between 0 and 1, such that we can do sampling directly based on  in Step 4. In Step 8, we simply select a Øx based on a Øx = arg max o,x ,
Ø oA

for all unobserved action index x. Algorithm 1 Framework of our DUP algorithm Input: plan library L, observed actions O Output: plan p ~ 1: learn vector representation of actions Ø, when k is an 2: initialize o,k with 1/M for all o  A unobserved action index 3: while the maximal number of repetitions is not reached do 4: sample unobserved actions in O based on  5: update  based on Equation (6) 6: project  to [0,1] 7: end while 8: select actions for unobserved actions with the largest weights in  9: return p ~

Ø to a for each x. For the ease of specification, we extend  Ø M , denoted by , such that bigger matrix with a size of |A|◊ Ø o,x if y is the index of the xth unobserved action in o,y =  Ø; otherwise, o,y = 1 and o ,y = 0 for all O, for all o  A Ø o  A  o = o. Our intuition is to estimate the unknown plan p ~ by selecting actions with the highest weights. We thus introduce the weights to Equation (2), as shown below,
L(wk+j )-1

p(wk+j |wk ) =
i=1

 (I(n(wk+j , i + 1) = (4)

child(n(wk+j , i))) ∑ avn(wk+j ,i) ∑ bvwk ) ,

Experiments
In this section, we evaluate our DUP algorithms in three planning domains from International Planning Competition, i.e., blocks1 , depots2 , and driverlog2 . To generate training and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems with a planning solver, such as FF3 , to produce 5000 plans. We then randomly divided the plans into ten folds, with 500 plans in each fold. We ran our DUP algorithm ten times to calculate an average of accuracies, each time with one fold for testing and the rest for training. In the testing data, we randomly removed actions from each testing plan (i.e., O) with a specific percentage  of the plan length. Features of datasets are shown in Table 1, where the second column is the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last column is the size of vocabulary used in all plans. We define the accuracy of our DUP algorithm as follows. For each unobserved action a Øx DUP suggests a set of possible actions Sx which have the highest value of a Øx ,x Ø. If Sx covers the truth action atruth , i.e., for all a Øx  A
2 http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume20/long03ahtml/JAIRIPC.html 3 https://fai.cs.uni-saarland.de/hoffmann/ff.html

where a = wk+j ,k+j and b = wk ,k . We can see that the impact of wk+j and wk is penalized by weights a and b if they are unobserved actions, and stays unchanged, otherwise (since both a and b equal to 1 if they are observed actions). We redefine the objective function as shown below,
M

F (~ p, ) =
k=1 -cj c,j =0

log p(wk+j |wk ),

(5)

where p(wk+j |wk ) is defined by Equation (4). The only parameters needed to be updated are , which can be easily done by gradient descent, as shown below, o,x = o,x +  F ,  o,x (6)

if x is the index of unobserved action in O; otherwise, o,x stays unchanged, i.e., o,x = 1. Note that  is a learning constant. With Equation (6), we can design an EM algorithm by repeatedly sampling an unknown plan according to  and updating  based on Equation (6) until reaching convergence (e.g., a constant number of repetitions is reached).

(a) blocks
0.8   0.7   0.6   0.8   0.7   0.6  

(b) depots
0.8   0.7   0.6  

(c) driverlog
DUP   MatchPlan   ARMS+PRP  

accuracy

accuracy

accuracy
DUP   MatchPlan   ARMS+PRP  
0.05   0.1   0.15   0.2   0.25  

0.5   0.4   0.3   0.2   0.1   0   0.05   0.1   0.15   0.2   0.25  

0.5   0.4   0.3   0.2   0.1   0  

0.5   0.4   0.3   0.2   0.1   0   0.05   0.1   0.15   0.2   0.25  

DUP   MatchPlan   ARMS+PRP  

percentage of unobserved actions

percentage of unobserved actions

percentage of unobserved actions

Figure 1: Accuracy with respect to different percentage of unobserved actions Table 1: Features of datasets domain blocks depots driverlog #plan 5000 5000 5000 #word 292250 209711 179621 #vocabulary 1250 2273 1441 extra information, i.e., initial state and goal of each plan in the plan library, to ARMS+PRP. In addition, PRP requires as input a set of candidate goals G for each plan to be recognized in the testing set, which was also generated and fed to PRP when testing. In summary, the hybrid plan recognition approach ARMS+PRP has more input information, i.e., initial states and goals in plan library and candidate goals G for each testing example, than our DUP approach.

atruth  Sx , we increase the number of correct suggestions g by 1. We thus define the accuracy acc as shown below: acc = 1 T
T

Accuracy w.r.t. Percentage of Unobserved Actions
We first evaluate our DUP algorithm with respect to different percentage of unobserved actions  in O. We set the window of training context c in Equation (1) to be three and the size of recommendations to be ten. We compare our DUP algorithm to both MatchPlan and ARMS+PRP. To make fair comparison (to MatchPlan), we set the matching window MatchPlan to be three as well when searching plans from plan libraries L. In other words, to estimate an unobserved action a Øx in O, MatchPlan matches previous three actions and subsequent three actions of a Øx to plans in L, and recommends ten actions with maximal number of matched actions, considering unobserved actions (ÿ in the context of a Øx ) and actions in L as a successful matching. For ARMS+PRP, we generated 20 candidate goals for each testing example including the ground-truth goal which corresponds to the ground-truth plan to be recognized. The results are shown in Figure 1. From Figure 1, we can see that in all three domains, the accuracy of our DUP algorithm is generally higher than MatchPlan and ARMS+PRP, which verifies that our DUP algorithm can indeed capture relations among actions better than previous matching approaches. The rationale is that we explore global plan information from the plan library to learn a "shallow" model (distributed representations of actions) and use this model with global information to best

i=1

# correct-suggestions i , Ki

where T is the size of testing set, # correct-suggestions i is the number of correct suggestions for the ith testing plan, Ki is the number of unobserved actions in the ith testing plan. We can see that the accuracy acc may be influenced by Sx . We will test different size of Sx in the experiment. State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries to best explain the observed actions (Geib and Steedman 2007), which we denote by MatchPlan. We develop a MatchPlan system based on the idea of (Geib and Steedman 2007) and compare our DUP algorithm to MatchPlan with respect to different percentage of unobserved actions  and different size of suggestion set Sx . Another baseline is action-models based plan recognition approach (Ramirez and Geffner 2009b) (denoted by PRP, short for Plan Recognition as Planning). Since we do not have action models as input in our DUP algorithm, we exploited the action model learning system ARMS (Yang, Wu, and Jiang 2007) to learn action models from the plan library and feed the action models to the PRP approach. We call this hybrid plan recognition approach ARMS+PRP. To learn action models, ARMS requires state information of plans as input. We thus added

(a) blocks
0.8   0.7   0.6  

(b) depots
0.8   0.7   0.6  
0.8   0.7   0.6  

(c) driverlog

accuracy

accuracy

accuracy

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

size of recommendations
DUP  

size  of  recommenda9ons  
MatchPlan   ARMS+PRP  

size of recommendations

Figure 2: Accuracy with respect to different size of recommendations explain the observed actions. In contrast, MatchPlan just utilizes local plan information when matching the observed actions to the plan library which results in lower accuracies. Although ARMS+PRP tries to leverage global plan information from the plan library to learn action models and uses the models to recognize observed actions, it enforces itself to extract "exact" models represented by planning models which are often with noise. When feeding those noisy models to PRP, since PRP that uses planning techniques to recognize plans is very sensitive to noise of planning models, the recognition accuracy is lower than DUP, even though ARMS+PRP has more input information (i.e., initial states and candidate goals) than our DUP algorithm. Looking at the changes of accuracies with respect to the percentage of unobserved actions, we can see that our DUP algorithm performs fairly well even when the percentage of unobserved action reaches 25%. In contrast, ARMS+PRP is sensitive to the percentage of unobserved actions, i.e., the accuracy goes down when more actions are unobserved. This is because the noise of planning models induces more uncertain information, which harms the recognition accuracy, when the percentage of unobserved actions becomes larger. Comparing accuracies of different domains, we can see that our DUP algorithm functions better in the blocks domain than the other two domains. This is because the ratio of #word over #vocabulary in the blocks domain is much larger than the other two domains, as shown in Table 1. We would conjecture that increasing the ratio could improve the accuracy of DUP. ARMS+PRP, the number of candidate goals for each testing example is set to 20. ARMS+PRP aims to recognize plans that are optimal with respect to the cost of actions. We relax ARMS+PRP to output |Sx | optimal plans, some of which might be suboptimal. We varied the number of actions recommended by DUP (or MatchPlan) from 1 to 10. The results are shown in Figure 2. From Figure 2, we find that accuracies of the three approaches generally become larger when the size of the recommended action set increases in all three domains. This is consistent with our intuition, since the larger the recommended action set is, the higher the possibility for the truth action to be in the recommended action set. We can also see that the accuracy of our DUP algorithm are generally larger than both MatchPlan and ARMS+PRP in all three domains, which verifies that our DUP algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the matching approach MatchPlan and the planning model learning approach ARMS+PRP. The reason is similar to the one given for Figure 1 in the previous section. That is, the "shadow" model learnt by our DUP algorithm is better for recognizing plans than both the "exact" planning model learnt by ARMS for recognizing plans with planning techniques and the local matching approach MatchPlan. On the other hand, we can also see the accuracy of ARMS+PRP is generally higher than MatchPlan. This verifies that the additional information of initial states and candidate goals exploited by ARMS+PRP can indeed help improve the accuracy. Furthermore, the advantage of DUP becomes even larger when the size of recommended action set increases, which suggests our vector representation based learning approach can better capture action relations when the size of recommended action set is larger. The possibility of actions correctly recognized by DUP becomes much larger than the other two approaches when the size of recommendations increases.

Accuracy w.r.t. Size of Recommendation Set
We next evaluate the performance of our DUP algorithm with respect to the size of recommendation set Sx . Likewise, we set the context window c used in Equation (1) to be three, which was also set when matching the observed actions O to plan libraries L in the MatchPlan approach. For

Related work
Kautz and Allen proposed an approach to recognizing plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an "action grammar" (Kautz and Allen 1986). All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events. Lesh and Etzioni further presented methods in scaling up activity recognition to scale up his work computationally (Lesh and Etzioni 1995). They automatically constructed plan-library from domain primitives, which was different from (Kautz and Allen 1986) where the plan library was explicitly represented. In these approaches, the problem of combinatorial explosion of plan execution models impedes its application to real-world domains. Kabanza and Filion (Kabanza et al. 2013) proposed an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted model counting. These approaches are, however, difficult to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. Instead of using a library of plans, Ramirez and Geffner (Ramirez and Geffner 2009b) proposed an approach to solving the plan recognition problem using slightly modified planning algorithms, assuming the action models were given as input. Except previous work (Kautz and Allen 1986; Bui 2003; Geib and Goldman 2009; Ramirez and Geffner 2009b) on the plan recognition problem presented in the introduction section, Note that action models can be created by experts or learnt by previous systems, such as ARMS (Yang, Wu, and Jiang 2007) and LAMP (Zhuo et al. 2010). Saria and Mahadevan presented a hierarchical multi-agent markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems (Saria and Mahadevan 2004). Singla and Mooney proposed an approach to abductive reasoning using a first-order probabilistic logic to recognize plans (Singla and Mooney 2011). Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories (Amir and Gal 2011). Ramirez and Geffner exploited offthe-shelf classical planners to recognize probabilistic plans (Ramirez and Geffner 2010). Early work on human-in-the-loop planning scenarios in automated planning went under the name of "mixedinitiative planning" (e.g. (Ferguson, Allen, and Miller 1996)). An important limitation of that work was that the humans in the loop were helping the automated planner (with a complete action model) navigate its search space of plans more efficiently. In contrast, we are interested in planning technology that helping humans develop plans, even in the absence of complete formal models of the planning domain. While some work in web-service composition (c.f. (Dong et al. 2004)) did focus on this type of planning support, they were hobbled by being limited to simple input/output

type comparison. In contrast, we believe that DUP learns and uses a model that captures more of the structure of the planning domain (while still not insisting on complete action models). While DUP focuses on learning models from plan corpora, some recent work looked at using crowdsourcing to acquire domain models. For example, Lasecki et al. (Lasecki et al. 2013) introduce Legion:AR, which combines the benefits of automatic and human activity labeling for robust and deployable activity recognition. The system exploits an active learning approach (Zhao, Sukthankar, and Sukthankar 2011) in which automatic activity recognition is augmented with on-demand activity labels from the crowd when an observed activity cannot be confidently classified. By engaging a group of people, Legion:AR is able to label activities as they occur more reliably than a single person can, especially in complex domains with multiple actors performing activities quickly. Lasecki et al. (Lasecki et al. 2014) built a crowdsourcing based system called ARchitect, using the crowd to capture the dependency structure of the actions that make up activities. Such crowd-sourcing methods can complement the plan-corpus based approach proposed in DUP.

Conclusion and Discussion
In this paper we present a novel plan recognition approach DUP based on vector representation of actions. We first learn the vector representations of actions from plan libraries using the Skip-gram model which has been demonstrated to be effective. We then discover unobserved actions with the vector representations by repeatedly sampling actions and optimizing the probability of potential plans to be recognized. We also empirically exhibit the effectiveness of our approach. While we focused on a one-shot recognition task in this paper, in practice, human-in-the-loop planning will consist of multiple iterations, with DUP recognizing the plan and suggesting action addition alternatives; the human making a selection and revising the plan. The aim is to provide a form of flexible plan completion tool, akin to auto-completers for search engine queries. To do this efficiently, we need to make the DUP recognition algorithm "incremental." The word-vector based domain model we developed in this paper provides interesting contrasts to the standard precondition and effect based action models used in automated planning community. One of our future aims is to provide a more systematic comparison of the tradeoffs offered by these models. Although we have focused on the "plan recognition" aspects of this model until now, and assumed that "planning support" will be limited to suggesting potential actions to the humans. In future, we will also consider "critiquing" the plans being generated by the humans (e.g. detecting that an action introduced by the human is not consistent with the model learned by DUP), and "explaining/justifying" the suggestions generated by humans. Here, we cannot expect causal explanations of the sorts that can be generated with the help of complete action models (e.g. (Petrie 1992)), and will have to develop justifications analogous to those used in recommendation systems.

References
[Amir and Gal 2011] Amir, O., and Gal, Y. K. 2011. Plan recognition in virtual laboratories. In Proceedings of IJCAI, 2392≠2397. [Bui 2003] Bui, H. H. 2003. A general model for online probabilistic plan recognition. In Proceedings of IJCAI, 1309≠1318. [Cohen et al. 2015] Cohen, P. R.; Kaiser, E. C.; Buchanan, M. C.; Lind, S.; Corrigan, M. J.; and Wesson, R. M. 2015. Sketch-thru-plan: a multimodal interface for command and control. Commun. ACM 58(4):56≠65. [Dong et al. 2004] Dong, X.; Halevy, A. Y.; Madhavan, J.; Nemes, E.; and Zhang, J. 2004. Simlarity search for web services. In (e)Proceedings of the Thirtieth International Conference on Very Large Data Bases, Toronto, Canada, August 31 - September 3 2004, 372≠383. [Ferguson, Allen, and Miller 1996] Ferguson, G.; Allen, J.; and Miller, B. 1996. Trains-95: Towards a mixed-initiative planning assistant. In Proceedings of the Third Conference on Artificial Intelligence Planning Systems (AIPS-96), 70≠ 77. Edinburgh, Scotland. [Geib and Goldman 2009] Geib, C. W., and Goldman, R. P. 2009. A probabilistic plan recognition algorithm based on plan tree grammars. Artificial Intelligence 173(11):1101≠ 1132. [Geib and Steedman 2007] Geib, C. W., and Steedman, M. 2007. On natural language processing and plan recognition. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, 1612≠1617. [Kabanza et al. 2013] Kabanza, F.; Filion, J.; Benaskeur, A. R.; and Irandoust, H. 2013. Controlling the hypothesis space in probabilistic plan recognition. In IJCAI. [Kambhampati and Talamadupula 2015] Kambhampati, S., and Talamadupula, K. 2015. Human-in-the-loop planning and decision support. rakaposhi.eas.asu.edu/hilp-tutorial. [Kambhampati 2007] Kambhampati, S. 2007. Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, 1601≠1605. [Kautz and Allen 1986] Kautz, H. A., and Allen, J. F. 1986. Generalized plan recognition. In Proceedings of AAAI, 32≠ 37. [Lasecki et al. 2013] Lasecki, W. S.; Song, Y. C.; Kautz, H. A.; and Bigham, J. P. 2013. Real-time crowd labeling for deployable activity recognition. In CSCW, 1203≠1212. [Lasecki et al. 2014] Lasecki, W. S.; Weingard, L.; Ferguson, G.; and Bigham, J. P. 2014. Finding dependencies between actions using the crowd. In Proceedings of CHI, 3095≠3098. [Lesh and Etzioni 1995] Lesh, N., and Etzioni, O. 1995. A sound and fast goal recognizer. In IJCAI, 1704≠1710. [Manikonda et al. 2014] Manikonda, L.; Chakraborti, T.; De, S.; Talamadupula, K.; and Kambhampati, S. 2014. AI-MIX:

using automated planning to steer human workers towards better crowdsourced plans. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu¥ ebec City, Qu¥ ebec, Canada., 3004≠3009. [Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111≠3119. [Petrie 1992] Petrie, C. J. 1992. Constrained decision revision. In Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16, 1992., 393≠ 400. [Ram¥ irez and Geffner 2009a] Ram¥ irez, M., and Geffner, H. 2009a. Plan recognition as planning. In IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17, 2009, 1778≠1783. [Ramirez and Geffner 2009b] Ramirez, M., and Geffner, H. 2009b. Plan recognition as planning. In Proceedings of IJCAI, 1778≠1783. [Ramirez and Geffner 2010] Ramirez, M., and Geffner, H. 2010. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of AAAI, 1121≠1126. [Saria and Mahadevan 2004] Saria, S., and Mahadevan, S. 2004. Probabilistic plan recognitionin multiagent systems. In Proceedings of AAAI. [Singla and Mooney 2011] Singla, P., and Mooney, R. 2011. Abductive markov logic for plan recognition. In Proceedings of AAAI, 1069≠1075. [Yang, Wu, and Jiang 2007] Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence Journal 171:107≠143. [Zhao, Sukthankar, and Sukthankar 2011] Zhao, L.; Sukthankar, G.; and Sukthankar, R. 2011. Robust active learning using crowdsourced annotations for activity recognition. In AAAI workshop. [Zhuo et al. 2010] Zhuo, H. H.; Yang, Q.; Hu, D. H.; and Li, L. 2010. Learning complex action models with quantifiers and implications. Artificial Intelligence 174(18):1540≠1569. [Zhuo, Yang, and Kambhampati 2012] Zhuo, H. H.; Yang, Q.; and Kambhampati, S. 2012. Action-model based multiagent plan recognition. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., 377≠385.

Proactive Decision Support using Automated Planning
Satya Gautam Vadlamudi, Tathagata Chakraborti, Yu Zhang, Subbarao Kambhampati {gautam,tchakra2,Yu.Zhang.442,rao}@asu.edu, Arizona State University, Tempe, AZ Proactive decision support (PDS) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support, since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain. Human-in-the-loop planning (HILP) is a necessary requirement today in many complex decision making or planning environments. In this paper we consider the case of HILP where the human(s) responsible for making the decisions in complex scenarios are supported by automated planning systems. Thus the planners in this scenario are the humans themselves, and we investigate the role of an automated planner in their deliberative process. This is, in effect, a role reversal of the traditional notion of the humanplanner interaction in mixed initiative planning; and we refer to the proposed system as a reverse mixed initiative planner. These systems are capable of providing plans or course-ofactions (COAs) when a model of the world where the plans are to be executed is given to them, along with the knowledge of the initial state and the list of goals to be achieved/tasks to be accomplished. Examples where such technologies can be helpful include disaster response strategies from the navy, or responses to fire or emergency from local law enforcement. Providing a complete model of the world where the plans are to be executed is, however, known to be very difficult (Kambhampati, 2007). This implies that the system generated plan cannot be completely relied upon. Not only executing such plans may no longer accomplish the goals/tasks provided, but also their execution may result in undesired consequences. This calls for active participation from the human in the loop rather than simply adopt a system generated plan. Furthermore, in many cases, the human in the loop may be held responsible for the plan under execution and its results. Therefore, it is also necessary in such cases that the human keeps control of the plan/COA being given for execution. This motivates us to build a proactive decision support system, which is context-sensitive and focuses on aiding and alerting the human in the loop with his/her decisions rather than generate a static COA that may not work in the dynamic worlds that the plan has to execute in. In this paper, we propose a proactive decision support (PDS) system, named RADAR, using automated planning technology, which is augmentable, context sensitive, controllable and adaptive to the human's decisions. It supports the human in the loop through suggestions and alerts, which can be considered by the human as he/she sees fit. In the following we explain the meaning of the above terms: ∑ Augmentable: The model of the world such as the rules that specify what are the preconditions for a particular action/decision and what would be the effects of that action/decision could be added/modified. The state of the world such as the values of various variables and availabilities of various resources could be updated. The list of goals/tasks can also be updated. Context-sensitive: Whenever the model or the state of the world is augmented either by the human or by some other source internal/external to the PDS system, the system takes the new context into account and responds with updated suggestions and alerts. Controllable: The decision making process of the PDS system is completely controllable by the human in the loop, who retains with the decision making power wherein he/she can either choose to follow the system generated suggestions or make a different decision as they see fit. Adaptive: Since the decision making power lies with the human in the loop, the PDS system has to adapt itself to the decisions being made by the human and provide new suggestions and alerts that are relevant based on the decisions of the humans. Note that, adaptive nature may also be viewed as part of context-sensitivity in the sense that the context changes whenever decisions are made. In this paper, we keep the distinction to differentiate the changes in the world and tasks, and the changes related to the actions prescribed/decisions of the human in the loop.

∑

∑

∑

As mentioned before, our proactive decision support system uses automated planning technology widely studied in the field of Artificial Intelligence. In particular, we adopted the Planning Domain Description Language (PDDL) to describe the model of the world, details of the current state (context) and the goals, to the system. Then, we used the existing landmark generation method (Hoffmann et al., 2004) to generate landmarks, which are then analyzed to come up with relevant suggestions and alerts. Landmarks are those set of states (of the world) that has to be visited by any successful plan that achieves goals from the current state.

We implemented our proposed system using the Fast Downward planner (Helmert, 2006) and tested it on a firefighting domain, where search-and-rescue missions are to be carried out (goals). We also conducted some preliminary human factor studies to evaluate the utility of the above proposal, which gave positive feedback. RELATED WORK The proposed proactive decision support system supports naturalistic decision making (Zsambok and Klein, 2014; Klein, 2008), which is acknowledged as a necessary element in PDS systems (Morrison et al., 2013). Systems which do not support naturalistic decision making have been found to have detrimental impact on work flow causing frustration to decision makers (Feigh et al., 2007). In (Parasuraman, 2000), a study of human performance consequences of different levels and types of automation is provided, where aspects such as, mental workload and situation awareness are considered as evaluative criteria. A model for types and levels of automation that provides an objective basis for deciding which system functions should be automated and to what extent is given in (Parasuraman et al., 2000). (Parasuraman and Manzey, 2010) shows that human use of automation may result in automation bias leading to omission and commission errors, which underlines the importance of reliability of the automation (Parasuraman and Riley, 1997). Various elements of human-automation interaction such as, adaptive nature and context sensitivity are presented in (Sheridan and Parasuraman, 2005). (Warm et al., 2008) show that vigilance requires hard mental work and is stressful via converging evidence from behavioral, neural and subjective measures. Our system could be considered as a part of such vigilance support thereby reducing the stress for human in the loop. High-level information fusion that characterizes complex situations and that support planning of effective responses is considered the greatest need in crisis-response situations (Laskey et al., 2016). Automated planning based proactive support systems were shown to be preferred by humans in studies involving human-robot teaming (Zhang et al., 2015) and the cognitive load of the subjects involved was observed to have been reduced (Narayanan et al., 2015). PROPOSED PROACTIVE DECISION SUPPORT SYSTEM: RADAR Now, we present the proposed proactive decision support (PDS) system ≠ RADAR, based on automated planning technology. First, we present the different elements of the PDS system and then briefly present the details of the methodology behind the generation of suggestions and alerts. The various elements of the planning PDS system are: ∑ Tasks/goals: The tasks or goals to be accomplished clearly form an important and necessary element. ∑ State/context, resources: The current state or context is needed for the PDS system to produce relevant alerts. The availability of resources also forms part of the context which we separately indicated to display as an important constituent of the Context.

Model, actions/decisions: The model consists of set of rules which are applicable in the world where the plan is being executed. Actions, for example, are part of a model, which give information about when a particular action is applicable (what are the preconditions to be satisfied in order for it to be applicable), and what would be the effects of taking that action (how it would impact various elements of the world). Actions are also closely tied to decisions that need to be made since each decision typically corresponds to certain action being taken. ∑ Current plan/course of action (COA): The information about current plan of the human in the loop, if any, can help the PDS system produce better suggestions and alerts by reducing the uncertainty. However, this could just consist of actions already taken, in which case the proposed PDS system can come up with relevant alerts pertaining to the future. More details on this are given next. Now, we briefly present details on how the proposed planning based system with above elements produces relevant suggestions and alerts. In order to explain this, first we need to define what are called Landmarks, which are central to the suggestions and alerts system. ∑ Definition: Landmarks. (Hoffmann et al., 2004) A state/partial state is a landmark (for the current state, tasks/goals, and model) if all plans/course-of-actions that can accomplish the tasks from the current state must go through that state/partial state during their execution. Note that, all goal states are trivial landmarks since they have to be accomplished by all successful plans. Consider there exists only one state, A, which can take one to the goal state(s), meaning, all plans have to visit A in order to accomplish the goals, making it a landmark (derived; nontrivial). Further, if there exist two states A and B through which the goal state(s) must be reached, then either A or B must be visited before accomplishing the tasks/goals. In such a scenario, A or B can be called as a landmark, in particular, a disjunctive landmark. Continuing this process, one can derive recursively the set of all landmarks starting from the goal state(s) leading back to the current state. Generating Suggestions and Alerts (PDS) Now, we present the details on how generating and analyzing the landmarks can help in producing suggestions and alerts. Note that, since the landmarks are the states that must be visited in order to accomplish the goals, if there is no possible way of reaching a certain landmark generated above, then the system can generate an alert conveying that the goal cannot be accomplished. This could be because there is no action available, which would help in visiting the landmark under consideration, or the preconditions of actions that can help reaching the landmark are not satisfied. In some cases, the preconditions, which are not currently satisfied for an action to be applicable, may be because of resource constraints. In such cases, the system instead generates a suggestion mentioning that those resources are needed in order to accomplish the task. For example, in order

Figure 1 ≠ RADAR interface showing data support and decision support for the human commanders making plans. for an action such as put-off-fire to be applicable, a precondition on the availability of the resource: fire-engine need to be satisfied. Failing which an alert may be generated. More details are described through a case study next. Further, provision to update the model, state, and resources is provided to make the system augmentable. In order to support context-sensitivity and adaptive nature, we re-execute the alerts generation method whenever there is a change in the context/tasks or new action is executed or plan is changed, so that the suggestions/alerts become relevant to the situation. Case Study: Fire-fighting Domain We use a fire-fighting scenario to illustrate the ideas expressed so far, as shown in Figure 1. The scenario plays out in a particular location (we use Tempe in the example) and involves the local fire chief, police, medical and transport authorities, who try to build a plan in response to the fire in the given platform (which is augmented with decision support capabilities from an automated planner). The left pane gives event updates that the commanders can incorporate into the Tasks panel at the top, which shows what high level goals or tasks needs to be addressed. The panel on the right (currently empty) will display the plan being constructed. Each of the human commanders have access to the resources that they can use to control the fire outbreak (as can be seen from the table at the bottom of Figure 1). For example, the police can deploy police cars and policemen, and the fire chief can deploy fire engines, ladders, rescuers, etc. The plans that can be produced by the commanders, of course, depend on the availability of these resources, and certain actions can only be executed when the required number of resources are available or the preconditions are satisfied. For example, in order to dispatch police cars from a particular police station, the police chief needs to make sure that the respective police station has enough police cars and it has been notified of the demand previously. Given this knowledge, the automated planner integrated into the system keeps an eye on the planning process of the human commanders. The three panels in the middle of Figure 1 provide these functionalities. The one on the left provides a way to add or request for resources in case of insufficient resources, while the one on the right provides suggested actions that the commanders may use to complete their plans. The panel in the middle is the most important part of the automated component where it produces alerts or suggestions to problems in the current plan or with respect to problems that may appear in future given the current state and availability of resources. In the following we will go through two use cases to illustrate how the system responds to situations as per the guidelines we discussed in the introduction ≠ Scenario I: (see Figure 2) 1) The scenario starts with a small fire in a building. Once selected from suggested tasks, this populates the task chosen to be addressed. 2) The planning model does landmark analysis on the current state and immediately populates the alerts panel with an alert saying either big fire engines or small fire engines are needed to put out the fire (a disjunctive landmark). 3) The commander tries to dispatch big engines now, but is stopped by the system which detects that there are not enough resources (big engines). 4) The commander addresses the shortage by requesting additional engines from the left. 5) Now the commander can proceed with and finish the plan until the fire has been extinguished.

Note that the above examples are illustrative and only intended for understanding the underlying concepts of the proposed system. The same system can handle scenarios where hundreds of actions and variables are involved, where it becomes nearly impossible for a human to account for all possible drawbacks. Furthermore, any other domain (say, disaster response) can be readily handled by the implemented system, by just changing the PDDL domain file used by it without any renewed effort.

Figure 2 ≠ Use case illustrating automated decision support using disjunctive landmarks. Scenario II: (see Figure 3) 1) The scenario now starts with a big fire. This calls for big engines as alerted by the system. Note that it also alerts for insufficient rescuers, which did not happen in the previous case, as the model used conveys that rescuers are needed only in case of a big fire. 2) The commander once again requests for additional resources (big engines as well as rescuers) and was able to generate a feasible plan as shown. In this way the system is able to assist the human commander in his planning process. The system is augmentable in the way it supports the commander's goal preferences and world state information. It is context-sensitive in how it provides relevant alerts based on the stage of the plan, and adaptive with respect to the choice taken by the human in the loop to address these alerts. Finally, the entire process is controllable because the human commander has authority over the choices at all times. Figure 3 ≠ Use case illustrating how automated decision support adapts in response to a necessary landmark. EVALUATIONS WITH HUMANS ON UTILITY In order to assess the utility of the proposed proactive decision support (PDS) system, RADAR, we have conducted a preliminary survey of its usefulness on 7 subjects on the questions given below. The system would be enhanced and refined continuously as we incorporate more and more features in our PDS system. We also plan to take the learning from the user feedback back into the design of the PDS system during the process. We discuss the responses obtained for each of the questions, next to it. 1) Do you think the suggestions & alerts are relevant to the task/goal? Yes/No

7 out of 7 subjects have answered it Yes. This suggests that the domain shown (fire-fighting) has been modeled well so that relevant landmarks could be generated. 2) Do you think the suggestions & alerts are context-sensitive (current state & current plan)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the context sensitivity of the landmarks make a good fit for use in PDS systems. 3) Do you think that the suggestions & alerts are dynamic (change with changing context/plan)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the regeneration of suggestions & alerts whenever context changes is notable to the users. 4) Do you think the suggestions & alerts increased your situational awareness (e.g. resources available, status of execution)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the suggestions and alerts continuously improve the situational awareness of the human in the loop, particularly in terms of the critical points relevant to the plan/COA. 5) Do you think that the suggestions & alerts interfere with the ability to interact with the system? Yes/No 1 out of 7 subjects have answered it Yes. This suggests that the alerts are displayed without interfering with the user interaction experience in most cases, and may be improved. 6) Do you think the suggestions & alerts helped in debugging the plan or would you rather execute and replan in case of failure? Former/Latter 6 out of 7 subjects have answered that the suggestions and alerts helped. This suggests that the users prefer to be alerted in advance rather than re-plan in most of the cases. 7) Do you think the suggestions & alerts should be an error and stop the plan from being dispatched, or should it be a warning and let you proceed with execution? Former/Latter 4 out of 7 subjects have answered that the alerts can be shown as errors and stop the execution whereas others preferred them as warnings that allow execution to move forward. In this case, there is a split amongst the users as to whether the user should be able to proceed with warnings or be stopped completely. Here there is an opportunity to amend the system so as to learn the cases where one could proceed despite the error/warning, and incorporate it as a soft constraint in the future leading to only a warning. 8) Do you feel that you can be in control of the decision making/planning process when using the proposed PDS system? Yes/No 6 out of 7 subjects have answered it Yes. This suggests that most of the users feel in control of the decision making process rather than being forced by the system. 9) On a scale of 1-10 how much would you rate your satisfaction with the proactive decision support capabilities of the system? 1-10 This received an average score of 7.14. This suggests that the users had a good first experience with the proposed PDS system and would like to see new and improved features. 10) On a scale of 1-10 how much would you recommend using the proposed proactive decision support platform? 1-10 This received an average score of 7.28. This suggests that the users are open to recommending the PDS system to others.

Overall, the PDS system was well received and perceived to be promising. Users have appreciated the current features and suggested minor modifications. As part of the future work, we consider addressing several important aspects such as: Where do we get the models? Can we automatically learn them from observing the users and the contexts? How do we deal with incomplete models? Does the human in the loop deviate from cost-optimal plans? How to understand the preferences of the human in the loop and how to address them? And so on. CONCLUSION We presented a Proactive decision support (PDS) framework called RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts and suggestions on potential drawbacks in the plan and resource constraints. This was achieved by generating and analyzing the landmarks that must be accomplished by any successful plan before achieving the goals. The proposed approach is aligned with the concept of naturalistic decision making. We demonstrated the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain and human factors studies. ACKNOWLEDGMENTS This research is supported in part by the ONR grant N0001415-1-2027. We thank Vivek Dondeti for his help with the implementation of parts of the RADAR system. REFERENCES
Feigh, K. M., Pritchett, A. R., Denq, T. W., & Jacko, J. A. (2007). Contextual Control Modes During an Airline Rescheduling Task. Journal of Cognitive Engineering and Decision Making, 1(2), 169-185. Helmert, M. (2006). The Fast Downward Planning System. J. Artif. Intell. Res.(JAIR), 26, 191-246. Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. J. Artif. Intell. Res.(JAIR), 22, 215-278. Kambhampati, S. (2007, July). Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models. InProceedings of the National Conference on Artificial Intelligence (Vol. 22, No. 2, p. 1601). Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999. Klein, G. (2008). Naturalistic decision making. Human Factors: The Journal of the Human Factors and Ergonomics Society, 50(3), 456-460. Laskey, K. B., Marques, H. C., & da Costa, P. C. (2016). High-Level Fusion for Crisis Response Planning. In Fusion Methodologies in Crisis Management (pp. 257-285). Springer International Publishing. Morrison J. G., Feigh K. M., Smallman H. S., Burns C. M., Moore K. E. (2013). The Quest For Anticipatory Decision Support Systems (Panel). Human Factors and Ergonomics Society Annual Meeting. Narayanan, V., Zhang, Y., Mendoza, N., & Kambhampati, S. (2015, March). Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote Human-Robot Interaction. In HRI (Extended Abstracts) (pp. 161-162). Parasuraman, R. (2000). Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7), 931-951. Parasuraman, R., & Manzey, D. H. (2010). Complacency and bias in human use of automation: An attentional integration. Human Factors: The Journal of the Human Factors and Ergonomics Society, 52(3), 381-410. Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. Human Factors: The Journal of the Human Factors and Ergonomics Society, 39(2), 230-253. Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 30(3), 286-297. Sheridan, T. B., & Parasuraman, R. (2005). Human-automation interaction.Reviews of human factors and ergonomics, 1(1), 89-129.

Warm, J. S., Parasuraman, R., & Matthews, G. (2008). Vigilance requires hard mental work and is stressful. Human Factors: The Journal of the Human Factors and Ergonomics Society, 50(3), 433-441. Zhang, Y., Narayanan, V., Chakraborti, T., & Kambhampati, S. (2015, September). A human factors analysis of proactive support in human-robot teaming. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on (pp. 3586-3593). IEEE. Zsambok, C. E., & Klein, G. (2014). Naturalistic decision making. Psychology Press.

Planning with Resource Conflicts
in Human-Robot Cohabitation
Tathagata Chakraborti1 , Yu Zhang1 , David E. Smith2 , Subbarao Kambhampati1

2

1
Department of Computer Science, Arizona State University, AZ
Autonomous Systems & Robotics Intelligent Systems Division, NASA Ames Research Center, CA

tchakra2@asu.edu, yzhan442@asu.edu, david.smith@nasa.gov, rao@asu.edu
ABSTRACT

tiation. Specifically, we ask the question, what information
can be extracted from the predicted plans, and how this
information can be used to guide the behavior of the autonomous agent. There has been previous work [1, 6] on
some of the modeling aspects of the problem, in terms of
planning with uncertainty in resources and constraints. In
this paper we provide an integrated framework (shown in
Figure 1) for achieving these behaviors of the autonomous
agent, particularly in the context of stigmergic coordination
of human-robot cohabitation. To this end, we modularize
our architecture so as to handle the uncertainty in the environment separately with the planning process, and show
how these individual modules interact with each other by
the way of usage profiles of the concerned resources.

In order to be acceptable members of future human-robot
ecosystems, it is necessary for autonomous agents to be respectful of the intentions of humans cohabiting a workspace
and account for conflicts on shared resources in the environment. In this paper we build an integrated system
that demonstrates how maintaining predictive models of its
human colleagues can inform the planning process of the
robotic agent. We propose an Integer Programming based
planner as a general formulation of this flavor of ‚Äúhumanaware‚Äù planning and show how the proposed formulation
can be used to produce different behaviors of the robotic
agent, showcasing compromise, opportunism or negotiation.
Finally, we investigate how the proposed approach scales
with the different parameters involved, and provide empirical evaluations to illustrate the pros and cons associated
with the proposed style of planning.

1.

INTRODUCTION

In environments where multiple agents are working independently, but utilizing shared resources, it is important for
these agents to model the intentions and beliefs of other
agents so as to act intelligently and prevent conflicts. In
cases where some of these agents are human, as in the case
of assistive robots in household environments, these are required (rather than just desired) capabilities of robots in
order for them to be considered ‚Äúsocially acceptable‚Äù - this
has been one of the important objectives of ‚Äúhuman-aware‚Äù
planning, as evident from existing literature in human-aware
path planning [13, 10] and human-aware task planning [5,
9, 3, 15]. An interesting aspect of many of these scenarios,
is the presence of many of the aspects of multi-agent environments, but absence of typical assumptions often made in
explicit teaming scenarios between humans and robots, as
pointed out in [4]. Probabilistic plan recognition plays an
important role in this regard, because by not committing to
a plan, that presumes a particular plan for the other agent,
it might be possible to minimize suboptimal (in terms of
redundant or conflicting actions performed during the execution phase) behavior of the autonomous agent.
Here we look at possible ways to minimize such suboptimal behavior by ways of compromise, opportunism or negoFigure 1: Schematic diagram of our integrated system for belief modeling, goal recognition, information extraction and
planning. The robot maintains a belief model of the environment, and uses observations from the environment to
extract information about how the world may evolve, which
is then used to drive its own planning process.

Appears in: Proceedings of the 15th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2016),
J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.),
May 9‚Äì13, 2016, Singapore.
c 2016, International Foundation for Autonomous Agents and
Copyright 
Multiagent Systems (www.ifaamas.org). All rights reserved.

1069

The general architecture of the system is shown in Figure
1. The autonomous agent, or the robot, is acting (with independent goals) in an environment co-habited with other
agents (humans), who are similarly self-interested. The robot
has a model of the other agents acting independently in its
environment. These models may be partial and hence the
robot can only make uncertain predictions on how the world
will evolve with time. However, the resources in the environment are limited and are likely to be constrained by the
plans of the other agents. The robot thus needs to reason
about the future states of the environment in order to make
sure that its own plans do not produce conflicts with respect
to the plans of the other agents.
With the involvement of humans, however, the problem
is more skewed against the robot, because humans would
expect a higher priority on their plans - robots that produce plans that clash with those of the humans, without
any explanation, would be considered incompatible for such
an ecosystem. Thus the robot is expected to follow plans
that preserve the human plans, rather than follow a globally optimal plan for itself. This aspect makes the current
setting distinct from normal human robot teaming scenarios and produces a number of its own interesting challenges.
How does the robot model the human‚Äôs behavior? How does
it plan to avoid friction with the human plans? If it is possible to communicate, how does it plan to negotiate and refine
plans? These are the questions that we seek to address
in this work. Our approach models human beliefs and defines resource profiles as abstract representations of the plans
predicted on the basis of these beliefs. The robot updates
its beliefs upon receiving new observations, and passes on
the resultant profiles onto its planner, which uses an IPformulation to minimize the overlap between these resource
profiles and those produced by the human‚Äôs plans.
The contribution of our paper is thus three-fold, we (1)
propose resource profiles as a concise mode of representing different types of information from predicted plans; (2)
develop an IP-based planner that can utilize this information and provide different modalities of conformant behavior; and (3) provide an integrated framework that supports
the proposed mode of planning - the modular approach also
provides an elegant way to handle different challenges separately (e.g. uncertainty and/or nested beliefs of humans
leaves the planner). The planner, as a consequence of these,
has properties not present in existing planners - for example, the work that probably comes closest is [9] that models
a specific case of compromise only, while the formulation is
also likely to blow up in presense of large hypothesis sets
due to absence of concise representation techniques like the
profiles. We will discuss the trade-offs and design choices in
more detail in the evaluation sections.
The rest of the paper is organized as follows. We will start
with a brief introduction of the agent models that comprise
the belief component, and describe how it facilitates plan
recognition. Then, in Sections 2.3 and 2.4, we are going
to go into details of how resource profiles may be used to
represent information from predicated plans, and describe
how our planner converts this information into constraints
that can be solved as an integer program during the plan
generation process. In Section 3 we will demonstrate how
the planner may be used to produce different modes of autonomous behavior. Finally in Section 4 we will provide
empirical evaluations of the planner‚Äôs internal properties.

Figure 2: Use case - Urban Search And Rescue (USAR).

2.

PLANNING WITH CONFLICTS
ON SHARED RESOURCES

We will now go into details about each of the modules
shown in Figure 1. The setting (adopted from [14]) involves
a commander CommX and a robot in a typical USAR (Urban
Search and Rescue) task illustrated in Figure 2. The commander can perform triage in certain locations, for which
he needs the medkit. The robot can also fetch medkits if
requested by other agents (not shown) in the environment.
The shared resources here are the two medkits - some of
the plans the commander can execute will lock the use of
and/or change the position of these medkits, so that from
the set of probable plans of the commander we can extract
a probability distribution over the usage (or even the position) of the medkit over time based on the fraction of plans
that conform to these facts. These resource availability profiles (i.e. the distribution over the usage or position of the
medkit evolving over time) provide a way for the agents to
minimize conflicts with the other agents. Before going into
details about the planner that achieves this, we will first
look at how the agents are modeled and how these profiles
are computed in the next section.

2.1

The Belief Modeling Component

The notion of modeling beliefs introduced by the authors
in [14] is adopted in this work. Beliefs about state are defined
in terms of predicates bel(Œ±, œÜ), where Œ± is an agent with belief œÜ = true. Goals are defined by predicates goal(Œ±, œÜ),
where agent Œ± has a goal œÜ. The set of all beliefs that
the robot ascribes to Œ± together represents the perspective for the robot of Œ±. This is obtained by a belief model
BelŒ± of agent Œ±, defined as { œÜ | bel(Œ±, œÜ) ‚àà Belself },
where Belself are the first-order beliefs of the robot (e.g.,
bel(self, at(self, room1))). The set of goals ascribed to Œ± is
similarly described by {goal(Œ±, œÜ)|goal(Œ±, œÜ) ‚àà Belself }.
Next, we turn our attention to the domain model DŒ± of
the agent Œ± that is used in the planning process. We use
PDDL [11] style agent models for the rest of the discussion, but most of the analysis easily generalizes to other
related modes of representation. Formally, a planning problem Œ† = hDŒ± , œÄŒ± i consists of the domain model DŒ± and the
problem instance œÄŒ± . The domain model of Œ± is defined as
DŒ± = hTŒ± , VŒ± , SŒ± , AŒ± i, where TŒ± is a set of object types;
VŒ± is a set of variables that describe objects that belong to
types in TŒ± ; SŒ± is a set of named first-order logical predicates over the variables VŒ± that describe the state; and AŒ± is
a set of operators available to the agent. The action models
a ‚àà AŒ± are represented as a = hN, C, P, Ei where N denotes

1070

2.2.2

the name of that action; C is the cost of that action; P ‚äÜ SŒ±
is the list of pre-conditions that must hold for the action
a to be applicable in a particular state s ‚äÜ SŒ± of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ SŒ±
is a tuple that contains the add and delete effects of applying the action to a state. The transition function Œ¥(¬∑)
determines the next state after the application of action a
in state s as Œ¥(a, s) |= ‚ä• if ‚àÉf ‚àà P s.t. f 6‚àà s; Œ¥(a, s) |=
(s \ ef f ‚àí (a)) ‚à™ ef f + (a) otherwise.
The belief model, in conjunction with beliefs about the
goals / intentions of another agent, will allow the robot to
instantiate a planning problem œÄŒ± = hOŒ± , IŒ± , GŒ± i, where
OŒ± is a set of objects of type t ‚àà TŒ± ; IŒ± is the initial state of
the world, and GŒ± is a set of goals, which are both sets of the
predicates from SŒ± initialized with objects from OŒ± . First,
the initial state IŒ± is populated by all of the robot‚Äôs initial beliefs about the agent Œ±, i.e. IŒ± = {œÜ | bel(Œ±, œÜ) ‚àà Belrobot }.
Similarly, the goal is set to GŒ± = {œÜ | goal(Œ±, œÜ) ‚àà Belrobot }.
Finally, the set of objects OŒ± consists of all the objects that
are mentioned in either the initial state, or the goal description: OŒ± = {o | o ‚àà (œÜ | œÜ ‚àà (IŒ± ‚à™ GŒ± ))}. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄŒ± = ha1 , a2 , . . . , a|œÄŒ± | i, ai ‚àà AŒ± such that
Œ¥(œÄŒ± , IŒ± ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . . ,P
a|œÄ| i, Œ¥(a1 , s)). The cost of
the plan is given by C(œÄŒ± ) =
a‚ààœÄŒ± Ca and the optimal
‚àó
‚àó
plan œÄŒ±
is such that C(œÄŒ±
) ‚â§ C(œÄŒ± ) ‚àÄœÄŒ± with Œ¥(œÄŒ± , IŒ± ) |=
GŒ± . This planning problem instance (though not directly
used in the robot‚Äôs planning process) enables the goal recognition component to solve the compiled problem instances.
More on this in the next section.

2.2

In the present scenario, we thus have a set Œ®Œ± of goals that
Œ± may be trying to achieve, and observations of the actions
Œ± is currently executing. At this point we refer to the work
of Ramirez and Geffner who in [12] provided a technique to
compile the problem of plan recognition into a classical planning problem. Given a sequence of observations Œ∏, we recompute the probability distribution Œò over G ‚àà Œ®Œ± by using a
Bayesian update P (G|Œ∏) ‚àù P (Œ∏|G), where the likelihood is
approximated by the function P (Œ∏|G) = 1/(1 + e‚àíŒ≤‚àÜ(G,Œ∏) )
where ‚àÜ(G, Œ∏) = Cp (G ‚àí Œ∏) ‚àí Cp (G + Œ∏). Here ‚àÜ(G, Œ∏)
gives an estimate of the difference in cost Cp of achieving the
goal G without and with the observations, thus increasing
P (Œ∏|G) for goals that explain the given observations. Thus,
solving two compiled planning problems, with goals G ‚àí Œ∏
and G + Œ∏, gives us the required posterior update for the
distribution Œò over possible goals of Œ±. The details of the
approach is available at [12].
The specific problem we will look at now is how to inform
the robot‚Äôs own planning process from the recognized goal
set Œ®Œ± . In order to do this, we compute the optimal plans
for each goal in the hypothesis goal set Œ®Œ± , and associate
them with the probabilities of these goals from the distribution thus obtained. Information from these plans is then
represented concisely in the form of resource profiles.

Notes on the Recognition Module
For our plan recognition module we use a much faster variation [7] of the above approach that exploits cost and interaction information from plan graphs to estimate the goal
probabilities. This saves on the computational effort of having to solve two planning problems per goal. Also, note that
while computing the plan to a particular goal G, we use a
compiled problem instance with the goal G + Œ∏ to ensure
that the predicted plan conforms to the existing observations. Details on the compilation is available at [12].
Also, the output of the planner does not need to be associated with probabilities - this is just the most general
formulation. If we want to deal with just a set of plans that
the robot needs to be aware of, we can treat the plan set
either with a uniform distribution and/or by requiring exactly zero conflicts in the objective of the planner (this will
become clearer in Section 2.4) depending on the preference.
Perhaps the biggest computational issue here is the need
to compute optimal plans. While we still do it for our domain, as we will note later in Section 2.3, this might not
be necessary, and suboptimal plans may be used in larger
domains where computation is an issue.

The Goal Recognition Component

For many real world scenarios, it is unlikely that the goals
of the humans are known completely, and that the plan computed by the planner is exactly the plan that they will follow.
We are only equipped with a belief of the likely goal(s) of
the human - and this may not be a full description of their
actual goals. Further, in the case of an incompletely specified goal, there might be a set of likely plans that the human
can execute, which brings into consideration the idea of incremental goal recognition over a possible goal set given a
stream of observations.

2.2.1

Goal / Plan Recognition

Goal Extension

To begin with, it is worth noting that the robot might
have to deal with multiple plans even in the presence of
completely specified goals (even if the other agents are fully
rational). For example, there may be multiple optimal ways
of achieving the same goal, and it is not obvious beforehand
which one of these an agent is going to end up following. In
the case of incompletely specified goals, the presence of multiple likely plans become more relevant. To accommodate
this, we extend the robot‚Äôs current belief of an agent Œ±‚Äôs
goal, GŒ± , to a hypothesis goal set Œ®Œ± . The computation of
this goal set can be done using the planning graph method
[2]. In the worst case, Œ®Œ± corresponds to all possible goals in
the final level of the converged planning graph. Having further (domain-dependent) knowledge (e.g. in our scenario,
information that CommX is only interested in triage-related
goals) can prune some of these goals by removing the goal
conditions that are not typed on the triage variable.

2.3

Resources and Resource Profiles

As we discussed previously, since the plans of the agents
are in parallel execution, the uncertainty introduced by the
commander‚Äôs actions cannot be mapped directly between
the commander‚Äôs final state and the robot‚Äôs initial state.
However, given the commander‚Äôs possible plans, the robot
can extract information about at what points of time the
shared resources in the environment are likely to be locked
by the commander. This information can be represented
by resource usage profiles that capture the expected (over
all the recognized plans) variation of probability of usage
or availability over time. The robot can, in turn, use this
information to make sure that the profile imposed by its own
plan has minimal conflicts with those of the commander‚Äôs.

1071

is locked by œÄ at step t, 0 otherwise} such that GœÄŒª (t) =
g ‚àÄ (t, g) ‚àà GœÄŒª . The resultant usage profile of a resource
Œª due to all the plans in Œ®P
Œ± is obtained by summing over
(weighted by the individual likelihoods) all the individual
profiles as G Œª : N ‚Üí [0, 1] = {(t, g) | t ‚àà {1, maxœÄ‚ààŒ®PŒ± |œÄ|}
P
and g ‚àù |Œ®1P | œÄ‚ààŒ®P GœÄŒª (t) √ó l(œÄ)}.
Œ±
Œ±
Similarly, we can define profiles over the actual groundings of a variable (shown in the lower part of Figure 3) as
Œª
GœÄf = {(t, g) | t ‚àà [1, |œÄ|] and f Œª = 1 at step t of plan œÄ,
0 otherwise}, and the resultant usage profile due to all the
fŒª
plans in Œ®P
= {(t, g) | t =
Œ± is obtained as before as G
P
Œª
1, 2, . . . , maxœÄ‚ààŒ®PŒ± |œÄ| and g ‚àù |Œ®1P | œÄ‚ààŒ®P GœÄf (t) √ó l(œÄ)}.
Œ±
Œ±
These profiles are helpful when actions in the robot‚Äôs domain
are conditioned on these variables, and the values of these
variables are conditioned on the plans of the other agents in
the environment currently under execution.
One important aspect of this formulation that should be
noted here is that the notion of ‚Äúresources‚Äù is described here
in terms of the subset of the common predicates in the domain of the agents (Œæ ‚äÜ SŒ± ‚à© SR ) and can thus be used
as a generalized definition to model different types of conflict between the plans between two agents. In as much as
these predicates are descriptions (possibly instantiated) of
the typed variables in the domain and actually refer to the
physical resources in the environment that might be shared
by the agents, we will stick to this nomenclature of calling
them ‚Äúresources‚Äù. We will now look at how an autonomous
agent can use these resource profiles to minimize conflicts
during plan execution with other agents in its environment.

Figure 3: Different types of resource profiles.

Formally, a profile is defined as a mapping from time step
T to a real number between 0 and 1, and is represented by
a set of tuples as follows G : N ‚Üí [0, 1] ‚â° {(t, g) : t ‚àà N, g ‚àà
[0, 1], such that G(t) = g at time step t}.
The concept of resource profiles can be handled at two
levels of abstraction. Going back to our running example,
shared resources that can come under conflict are the two
(locatable typed objects) medkits, and the profiles over the
medkits can be over both usage and location, as shown in
Figure 3. These different types of profiles can be used (possibly in conjunction if needed) for different purposes. For
example, just the usage profile shown on top is more helpful
in identifying when to use the specific resource, while the
resource when bound with the location specific groundings,
as shown at the bottom can lead to more complicated higher
order reasoning (e.g. the robot can decide to wait for the
commander‚Äôs plans to be over, as he inadvertently brings
the medkit closer to it with high probability as a result of
his own plans). We will look at this again in Section 3.
Let the domain model of the robot be DR = hTR , VR , SR ,
AR i with the action models a = hN, C, P, Ei defined in the
same way as described in Section 2.1. Also, let Œõ ‚äÜ VR
be the set of shared resources and for each Œª ‚àà Œõ we have
a set of predicates f Œª ‚äÜ SR that are influenced (as determined by the system designer) by Œª, and let Œì : Œõ ‚Üí P(Œæ)
be a function that maps the resource variables to the set
of predicates Œæ = ‚à™Œª f Œª they influence. Without any external knowledge of the environment, we can set Œõ = VŒ± ‚à© VR
and Œæ = SŒ± ‚à© SR , though in most cases these sets are much
smaller. In the following discussion, we will look at how
the knowledge from the hypothesis goal set can be modeled
in terms of resource availability graphs for each of the constrained resources Œª ‚àà Œõ.
Consider the set of plans Œ®P
Œ± containing optimal plans
corresponding to each goal in the hypothesis goal set, i.e.
‚àó
‚àó
Œ®P
Œ± = {œÄG = ha1 , a2 , . . . at i | Œ¥(œÄG , IŒ± ) |= G, ai ‚àà AŒ± ‚àÄi, G ‚àà
Œ®Œ± } and let l(œÄ) be the likelihood of the plan œÄ modeled on
the goal likelihood distribution ‚àÄ G ‚àà Œ®Œ± , p(G) ‚àº Œò as
l(œÄG ) = c|œÄG | √ó p(G), where c is a normalization constant.
At each time step t, a plan œÄ ‚àà Œ®P
Œ± may lock one or
more of the resources Œª. Each plan thus provides a profile
of usage of a resource with respect to the time step t as
GœÄŒª : N ‚Üí {0, 1} = {(t, g) | t ‚àà [1, |œÄ|] and g = 1 if Œª

Notes on Usefulness of Profile Computation
One interesting aspect of computing resource profiles is that
it provides a powerful interface between the belief on the environment and the planner. On the one hand, note that the
input from the previous stage (goal/plan recognition module) is as generic as possible - a set of plans possibly associated with probabilities. Given any changes in preceding
stages, e.g. modeling stochasticity or more complex belief
models, still yields a set of plans that the robot needs to be
aware of. Thus the plan set and resource profiles provide
a surprisingly simple yet powerful way of abstracting away
relevant information for the planner to use.
The profiles may also be leveraged to address different
modalities of conformant behavior, for example with multiple humans and their relative importance, by (1) weighing
the contributions from individual profiles by the normalized
priority of the human, which would cause the planner to
avoid conflicts with these profiles more than with those with
lower priorities; or (2) requiring zero conflicts on a subset of
profiles which would cause the planner to avoid a subset of
conflicts at all costs, while minimizing the rest.
A somewhat implicit advantage of using profiles is its ability to form regions of interest given the possible plans. This
will become clear later in Section 4.2 when we show that the
predicted conflicts provide well-informed guidance to avoiding real conflicts during execution (as evident by the robustness in performance with just 1-3 observations, and zero actual conflicts in low probability areas in the computed profiles). Right now this has the implication that we need not
necessarily compute perfect plan costs and goal distributions
to get good plans.

1072

2.4

Conflict Minimization

Then the solution to the IP should ensure that the robot only
uses these resources when they are in fact most expected to
be available (as obtained by maximizing the overlap between
Œª
hf,t and Gf ). These act like demand profiles from the perspective of the robot. We also add a ‚Äúno-operation‚Äù action
AR ‚Üê AR ‚à™ aœÜ so that aœÜ = hN, C, P, Ei where N = NOOP,
C = 0, P = {} and E = {}.

The planning problem of the robot - given by Œ† = hDR , œÄR ,
Œª
Œõ, {GŒª | ‚àÄŒª ‚àà Œõ}, {Gf | ‚àÄf ‚àà Œì(Œª), ‚àÄŒª ‚àà Œõ}i - consists of
the domain model DR and the problem instance œÄR = hOR ,
IR , GR i similar to that described in section 2.3, and also
the constrained resources and all the profiles corresponding
to them. This is because the planning process must take
into account both goals of achievement as also conflict of resource usages as described by the profiles. Traditional planners provide no direct way to handle such profiles within the
planning process. Note here that since the execution of the
plans of the agents is occurring in parallel, the uncertainty is
evolving at the time of execution, and hence the uncertainty
cannot be captured from the goal states of the recognized
plans alone, and consequently cannot be simply compiled
away to the initial state uncertainty for the robot and solved
as a conformant plan. Similarly, the problem does not directly compile into action costs in a metric planning instance
because the profiles themselves are varying with time. Thus
we need a planner that can handle these resource constraints
that are both stochastic and non-stationary due to the uncertainty in the environment. To this end we introduce the
following IP-based planner (partly following the technique
for IP encoding for state space planning outlined in [16])
as an elegant way to sum over and minimize overlaps in
profiles during the plan generation process. The following
formulation finds such T-step plans in case of non-durative
or instantaneous actions.
For action
( a ‚àà AR at step t we have an action variable:
1, if action a is executed in step t
xa,t =
0, otherwise; ‚àÄa ‚àà AR , t ‚àà {1, 2, . . . , T }
Also, for every proposition f at step t a binary state variable
is introduced
( as follows:
1, if proposition is true in plan step t
yf,t =
0, otherwise; ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }
Note here that the plan computed by the robot introduces a
resource consumption profile itself, and thus one optimizing
criterion would be to minimize the overlap between the usage
profile due to the computed plan with those established by
the predicted plans of the other agents in the environment.
Let us introduce a new variable to model the resource usage
graph imposed
( by the robot as follows:
1, if f ‚àà Œæ is locked at plan step t
gf,t =
0, otherwise; ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T }
Further, for every resource Œª ‚àà Œõ, we divide the actions in
the domain of the robot into three disjoint sets -

The IP formulation is given by:
P
P
min k1 a‚ààAR
t‚àà{1,2,...,T } Ca √ó xa,t
P
P
P
Œª
+k2 Œª‚ààŒõ
f ‚ààŒì(Œª)
t‚àà{1,2,...,T } gf,t √ó G (t)
P
P
P
fŒª
‚àík3 Œª‚ààŒõ
(t)
f ‚ààŒì(Œª)
t‚àà{0,1,...,T ‚àí1} hf,t √ó G
yf,0 = 1 ‚àÄf ‚àà IR \ Œæ

(1)

yf,0 = 0 ‚àÄf ‚àà
/ IR or f ‚àà Œæ

(2)

yf,T = 1 ‚àÄf ‚àà GR

(3)

xa,t ‚â§ yf,t‚àí1 ‚àÄa s.t. f ‚àà Pa , ‚àÄf ‚àà
/ Œæ, t ‚àà {1, . . . , T }

(4)

hf,t‚àí1 = xa,t ‚àÄa s.t. f ‚àà Pa , ‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
(5)
P
yf,t ‚â§ yf,t‚àí1 + a‚ààadd(f ) xa,t
s.t. add(f ) = {a|f ‚àà ef f + (a)}, ‚àÄf, t ‚àà {1, . . . , T }
(6)
P
yf,t ‚â§ 1 ‚àí a‚ààdel(f ) xa,t
s.t. del(f ) = {a|f ‚àà ef f ‚àí (a)}, ‚àÄf, t ‚àà {1, . . . , T }
(7)
P
(8)
a‚ààAR xa,t = 1, t ‚àà {1, 2, . . . , T }
P
P
x
‚â§
1
‚àÄf
‚àà
Œæ,
t
‚àà
{1,
2,
.
.
.
,
T
}
(9)
t a,t
a‚àà‚Ñ¶+
fP
P
P
gf,t = a‚àà‚Ñ¶+ xa,t + (1 ‚àí a‚àà‚Ñ¶+ xa,t ‚àí a‚àà‚Ñ¶‚àí xa,t ) √ó gf,t‚àí1
f

f

f

‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
hf,t √ó G

fŒª

(t) ‚â•  ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T ‚àí 1}

(10)
(11)

yf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }

(12)

xa,t ‚àà {0, 1} ‚àÄa ‚àà AR , t ‚àà {1, 2, . . . , T }

(13)

gf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }

(14)

hf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T ‚àí 1}

(15)

where k1 , k2 , k3 are constants determining the relative importance of the optimization criteria and  is a constant.
Here, the objective function minimizes the sum of the cost
of the plan and the overlap between the cumulative resource
usage profiles of the predicted plans and that imposed by the
current plan of the robot itself while maximizing the validity
of the demand profiles. Constraints (1) through (3) model
the initial and goal conditions, while the value of the constrained variables are kept uninitialized (and are determined
by their profiles). Constraints (4) and (5), depending on the
particular predicate, enforces the preconditions, or produces
the demand profiles respectively, while (6) and (7) enforces
the state equations that maintain the add and delete effects
of the actions. Constraint (8) imposes non concurrency on
the actions, and (9) ensures that the robot does not repeat
the same action indefinitely to increase its utility. Constraint
(10) generates the resource profile of the current plan, while
(11) maintains that actions are only executed if there is at
least a small probability  of success. Finally (12) to (15)
provide the binary ranges of the variables.

‚Ñ¶+
f = {a ‚àà AR such that xa,t = 1 =‚áí yf,t = 1},
‚Ñ¶‚àí
f = {a ‚àà AR such that xa,t = 1 =‚áí yf,t = 0}, and
‚àí
‚Ñ¶of = AR \ (‚Ñ¶+
f ‚à™ ‚Ñ¶f ), ‚àÄf ‚àà Œæ.
These then specify respectively those actions in the domain
that lock, free up, or do not affect the use of a particular
resource, and are used to calculate gf,t in the IP. Further, we
introduce a variable hf,t to track preconditions required by
actions in the generated plan whose success is conditioned
on the influence of the plans of the other agents on the world
(e.g. position of the medkits are changing, and the action
pickup is (
conditioned on it) as follows:
1, if f ‚àà Pa and xa,t+1 = 1
hf,t =
0, otherwise; ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T ‚àí 1}

Note on Temporal Expressivity
At this point it is worth acknowledging the implications
of having durative actions in our formulation. Note that
our approach does not discretize time, but rather uses time

1073

points as steps in the plan - that can be easily augmented
with their own durations. So in order to handle durative
actions, the only (somewhat minor) change required in the
formulation is in the way the conflicts are integrated (instead
of summed) over in the objective function. Further, uncertainty in action durations is always a big issue in human
interactions; though resource profiles cannot directly handle
uncertain durations, it only affects the way the profiles are
calculated, and the way in which information is expressed
in it remains unchanged (i.e. expectations over action durations add an extra expectation to the already probabilistic
profile computation). As noted before in Section 2.3, the
ability of profiles to form regions of interest is crucial in
handling such scenarios implicitly.

3.

13
14
15
16
17
18
19

3.3

The planner is implemented on the IP-solver gurobi and
integrates [7] and [8] respectively for goal recognition and
plan prediction for the recognized goals. We will now illustrate how the formulation can produce different behaviors
of the robot by appropriately configuring the parameters of
the planner. For this discussion we will limit ourselves to a
singleton hypothesis goal set in order to observe the robot‚Äôs
response more clearly.

Compromise

(5a)
(16)

Œª

H (t) ‚â§ G (t) ‚àÄŒª ‚àà Œõ, t ‚àà {0, 1, . . . , T }
(17)
Constraint (5a) now complements constraint (5) from the
existing formulation, by promising to restore the world state
every time a demand is made on a variable. The variable
H Œª (t), maintained by constraints (16) and (17), determine
the desired deviation from the given profiles. The objective
function has been updated to reflect that overlaps are now
measured with the desired profile of usage, and there is a
cost associated with the deviation from the real one. The
revised plan now produced by the robot is shown below.
01
02
03
04
05
06
07
08
09
10

MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_ROBOT_HALL2_HALL3
MOVE_ROBOT_HALL3_HALL4
MOVE_REVERSE_ROBOT_HALL4_ROOM4
MOVE_REVERSE_ROBOT_ROOM4_ROOM3
PICK_UP_MEDKIT_ROBOT_MK2_ROOM3
MOVE_ROBOT_ROOM3_ROOM4
MOVE_ROBOT_ROOM4_HALL4
MOVE_REVERSE_ROBOT_HALL4_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
DROP_OFF_ROBOT_MK2_HALL3

3.2

yf,T ‚â• hf,t‚àí1 ‚àÄ a s.t. f ‚àà Pa , ‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
H Œª (t) ‚àà [0, 1] ‚àÄŒª ‚àà Œõ, t ‚àà {0, 1, . . . , T }
Œª

Let us now look back at the environment we introduced
in Figure 1. Consider that the goal of the commander is to
perform triage in room1. The robot computes the human‚Äôs
optimal plan (which ends up using medkit1 at time steps 7
through 12) and updates the resource profiles accordingly. If
it has its own goal to perform triage in hall3, the plan that
it comes up with given a 12 step lookahead is shown below.
Notice that the robot opts for the other medkit (medkit2
in room3) even though its plan now incurs a higher cost in
terms of execution. The robot thus can adopt a policy of
compromise if it is possible for it to preserve the commander‚Äôs (expected) plan.
01
02
03
04
05
06
07
08
09
10
11
12

Negotiation

In many cases, the robot will have to eventually produce
plans that will have potential points of conflict with the expected plans of the commander. This occurs when there is
no feasible
plan with zero overlap between profiles (specifiP
cally
gf,t √ó GŒª (t) = 0) or if the alternative plans for the
robot are too costly (as determined by the objective function). If, however, the robot is equipped with the ability to
communicate with the human, then it can negotiate a plan
that suits both. To this end, we introduce a new variable
H Œª (t) and update the IP as follows:
P
P
min k1 a‚ààAR
t‚àà{1,2,...,T } Ca √ó xa,t
P
P
P
Œª
+k2 Œª‚ààŒª
f ‚ààŒì‚àí1 (Œª)
t‚àà{1,2,...,T } gf,t √ó H (t)
P
P
P
fŒª
(t)
‚àík3 Œª‚ààŒõ
‚àí1
t‚àà{0,1,...,T ‚àí1} hf,t √ó G
P
Pf ‚ààŒì (Œª)
Œª
Œª
+k4 Œª‚ààŒõ
t‚àà{0,1,...,T } ||G (t) ‚àí H (t)||

MODULATING BEHAVIOR
OF THE ROBOT

3.1

NOOP
PICK_UP_MEDKIT_ROBOT_MK1_ROOM1
MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_ROBOT_HALL2_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
DROP_OFF_ROBOT_MK1_HALL3

MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_REVERSE_ROBOT_HALL2_ROOM2
PICK_UP_MEDKIT_ROBOT_MK1_ROOM2
MOVE_ROBOT_ROOM2_HALL2
MOVE_ROBOT_HALL2_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
MOVE_REVERSE_ROBOT_HALL3_HALL2
MOVE_REVERSE_ROBOT_HALL2_ROOM2
DROP_OFF_ROBOT_MK1_ROOM2

Notice that the robot restores the world state that the human is believed to expect, and can now communicate to him
‚ÄúCan you please not use medkit1 from time 7 to 9?‚Äù based
on how the real and the ideal profiles diverge, i.e. t such
that H Œª (t) < GŒª (t) for each resource Œª.

Opportunism

Notes on Adaptive Behavior Modeling

Notice, however, that the commander is actually bringing
the medkit to room1 as predicted by the robot, and this is
a favorable change in the world, because robot can use this
medkit once the commander is done and achieve its goal at
a much lower cost. The robot, indeed, realizes this once we
give it a bigger time horizon to plan with, as shown above (on
the right). Thus, in this case, the robot shows opportunism
based on how it believes the world state will change.

One might note here that people are often adaptive and it is
very much possible that they may be willing to change their
goals based on observing the robot or are even unwilling to
negotiate if their plans conflict. Hence the policies of compromise and opportunism for the robot are complementary
to negotiation in the event the latter fails. Thus, for example, the robot might choose to communicate a negotiation
strategy to the human, but fall back on a compromise if that
fails. It is a merit of such a simple formulation to be able to
handle such interesting adaptive behaviors.

01 NOOP
02 NOOP
...

1074

4.

EVALUATION

The power of the proposed approach lies in the modular
nature in which it tackles several complicated problems that
are separate research areas in their own rights. As we saw
throughout the course of the discussion, approaches used
in the individual modules may be varied with little to no
change in the rest of the architecture. For example the expressivity of the belief modeling or goal recognition component is handled separately as the planner used information
from a generic plan set. Again the representation technique
introduced in terms of resource profiles provide properties in
terms of computational independence with respect to size of
the hypothesis set and number of agents (which gets manifested in complexity in number of resources) that general
planners do not have. So it becomes a design choice depending on which metric needs to be optimized.
For empirical evaluations, we simulated the USAR scenario on 360 different problem instances, randomly generated by varying the specific (as well as the number of probable) goals of the human, and evaluated how the planner
behaved with the number of observations it can start with
to build its profiles. We fix the domain description, location
and goal of the agents, and the position of the resources, and
consider randomly generated hypothesis goal sets of size 211. The goals of the commander were assumed to be known
to be triage related, but the location of the triage was allocated randomly (one of which was again picked at random
as the real goal). Finally for each of these problems, we
generate 1-5 observations by simulating the commander‚Äôs
plan over the real goal, and use these observations known
a priori the robot‚Äôs plan generation process. The experiments were conducted on an Intel Xeon(R) CPU E5-1620
v2 3.70GHz√ó8 processor with a 62.9GiB memory.

4.1

(a) w.r.t. T (|Œõ| = 2)

(b) w.r.t. #medkits (T = 10)

Figure 4: Performance of the planner w.r.t. planning horizon T and number of constrained resources (medkits).

Scaling Up

Our primary contribution is the formulation for planning
with resource profiles, while the goal recognition component
can be any off-the-shelf algorithm, and as such we compare
scalability with respect to the planning component only.

the agents. On the other hand, the time spent on recognition, and on calculating the profiles, is significantly affected.
However, observations on multiple agents are asynchronous,
and goal recognition can operate in parallel, so that this is
not a huge concern beyond the complexity of a single instance. Similarly the performance is also unaffected by the
size of the hypothesis set Œ®Œ± , as shown in Figure 5, which
shows increase in the number of the possible goals does not
complicate the profiles to an extent to affect the complexity.

- w.r.t. Length of the Planning Horizon
The performance of the planner with respect to the planning
horizon is shown in Figure 4a. This is, as expected, the
bottleneck in computation due to exponential growth of the
size of the IP. It is however not prohibitively expensive, and
the planner is still able to produce plans of length 20 (steps,
not durations) for our domain in a matter of seconds.

4.2

Quality of the Plans Produced

We define U as the average conflicts per plan step when
a demand is placed on a resource by the robot, and S as
the success probability per plan step that the demand is
met. C is the cost of a plan. F is the percentage of times
there was an actual conflict during execution (distinct from
U which estimates the possible conflict that may occur per
plan step). We observe the quality of the plans produced
by the planner by varying the ratio of parameters k1 and k3
from the objective function and the length of the planning
horizon T . Similar results can be produced by varying k1 /k2 .
From Table 1, as k1 /k3 decreases, the planner becomes
more conservative (to maximize success probability) and
thus plans become costlier. At the same time the expected
success rate of actions are also increased (with simultaneous
increase in usage conflict), as reflected by a higher failure
rate due to actual execution time conflicts.

- w.r.t. Number of Resources
The performance of the planner with respect to the number of constrained resources (medkits, in the context of the
current discussion) is shown in Figure 4b. Evidently, the
computational effort is dominated by that due to the planning horizon. This reiterates the usefulness of abstracting
the information in predicted plans in the form of resource
profiles, thus isolating the complexities of the domain with
that of the underlying planning algorithm.

- w.r.t. the Number of Agents and Goals
The planning module (i.e. the IP formulation) is by itself
independent of the number of agents being modeled. In
fact, this is one of the major advantages of using abstractions like resource profiles in lieu of actual plans of each of

1075

Figure 5: Performance of the planner w.r.t. size of the goal
set. As expected, computational complexity is not affected.

k1 /k3
C
U
S
F

0.05
9.47
0.18
0.85
27.5

0.5
6.37
0.17
0.579
23.0

(a) w.r.t. |Œ®Œ± |

5.0
6.31
0.17
0.578
21.3

Table 1: Quality of plans produced w.r.t. k1 /k3 . Conservative plans result in lowered utility.

Also note, from Table 2 the impact of the planning horizon T on the types of behaviors we discussed in the previous section. As we increase T , the plan cost falls below the
optimal, indicating opportunities for opportunistic behavior
on the part of the robot. The expected conflict also falls
to almost 0. However the expected success rate of actions
also decreases, the ratio k1 /k2 determines how daring the
robot is, in choosing between cheap versus possibly invalid
plans. Note, however, the actual execution time conflict is
extremely low with increasing T , for even sufficiently conservative estimates of S.
Thus we see that the robot is successfully able to navigate conflicts and find in many cases plans even cheaper
than the original optimal plan, thus highlighting the usefulness of the approach. Finally, we look at the impact of the
parameters in the plan recognition module in Figure 6. As
expected, with bigger hypothesis sets, the success rate goes
down. Interestingly, the plan cost also shows a downward
trend which might be because the bigger variety in possible
goals give a better idea of which medkits are generally more
useful for that instance at what points of time. With more
observations, as expected, the success rate goes up and the
expected conflict goes down. The cost, however, increases a
little as the planner opts for more conservative options.

5.

(b) w.r.t. #obs

Figure 6: Performance of the planner w.r.t. size of goal set
and number of observations (k1 /k3 = 0.5, T = 16).

T
C
U
S
F

10
9.0
0.46
1.0
53.3

13
5.6
0.04
0.48
11.9

16
4.53
‚âà0
0.25
6.6

Optimal
9.0
n/a
n/a
53.3

Table 2: Quality of plans produced w.r.t. T . Opportunities
for opportunism explored, conflicts minimized.

objective function and optimization parameters. Finally, we
provide an end-to-end framework that integrates belief modeling, goal recognition and an IP-solver that can enforce the
desired interaction constraints. One interesting research direction would be to consider nested beliefs on the agents; after all, humans are rarely completely aloof of other agents in
its environment. Such interactions should have to consider
evolution of beliefs with continued interactions and motivate further exploration of the belief modeling component.
The modularity of the proposed approach allows for focused
research on each (individually challenging) subtask without
significantly affecting the others.

CONCLUSIONS

In this paper we investigate how plans may be affected by
conflicts on shared resources in an environment cohabited by
humans and robots, and introduce the concept of resource
profiles as a means of representation for concisely modeling
the information pertaining to the usage of such resources,
contained in predicted behavior of the agents. We propose
a general formulation of a planner for such scenarios and
show how the planner can be used to model different types
of behavior of the robot by appropriately configuring the

Acknowledgment
This research is supported in part by the ONR grants N0001413-1-0176, N00014-13-1-0519 and N00014-15-1-2027, and the
ARO grant W911NF-13-1-0023.

1076

REFERENCES

[11] D. Mcdermott, M. Ghallab, A. Howe, C. Knoblock,
A. Ram, M. Veloso, D. Weld, and D. Wilkins. Pddl the planning domain definition language. Technical
Report TR-98-003, Yale Center for Computational
Vision and Control 1998.
‚Äù
[12] M. Ramirez and H. Geffner. Probabilistic plan
recognition using off-the-shelf classical planners. In In
Proc. AAAI-2010, 2010.
[13] E. Sisbot, L. Marin-Urias, R. Alami, and T. Simeon. A
human aware mobile robot motion planner. Robotics,
IEEE Transactions on, 23(5):874‚Äì883, Oct 2007.
[14] K. Talamadupula, G. Briggs, T. Chakraborti,
M. Scheutz, and S. Kambhampati. Coordination in
human-robot teams using mental modeling and plan
recognition. In Intelligent Robots and Systems (IROS
2014), 2014 IEEE/RSJ International Conference on,
pages 2957‚Äì2962, Sept 2014.
[15] S. Tomic, F. Pecora, and A. Saffiotti. Too cool for
school - adding social constraints in human aware
planning. In Proc of the International Workshop on
Cognitive Robotics (CogRob), 2014.
[16] T. Vossen, M. O. Ball, A. Lotem, and D. S. Nau. On
the use of integer programming models in ai planning.
In T. Dean, editor, IJCAI, pages 304‚Äì309. Morgan
Kaufmann, 1999.

[1] E. Beaudry, F. Kabanza, and F. Michaud. Planning
with concurrency under resources and time
uncertainty. In Proceedings of the 2010 Conference on
ECAI 2010: 19th European Conference on Artificial
Intelligence, pages 217‚Äì222, Amsterdam, The
Netherlands, The Netherlands, 2010. IOS Press.
[2] A. Blum and M. L. Furst. Fast planning through
planning graph analysis. In IJCAI, pages 1636‚Äì1642,
1995.
[3] F. Cavallo, R. Limosani, A. Manzi, M. Bonaccorsi,
R. Esposito, M. Di Rocco, F. Pecora, G. Teti,
A. Saffiotti, and P. Dario. Development of a socially
believable multi-robot solution from town to home.
Cognitive Computation, 6(4):954‚Äì967, 2014.
[4] T. Chakraborti, G. Briggs, K. Talamadupula,
Y. Zhang, M. Scheutz, D. Smith, and
S. Kambhampati. Planning for serendipity. In
IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2015.
[5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning for mobile robots. In Proc of the Int
Conf on Advanced Robotics (ICAR), 2009.
[6] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning: An application to mobile robots. ACM
Trans. Intell. Syst. Technol., 1(2):15:1‚Äì15:26, Dec.
2010.
[7] Y. E.-Martƒ±ÃÅn, M. D. R.-Moreno, and D. E. Smith. A
fast goal recognition technique based on interaction
estimates. In Proceedings of the Twenty-Fourth
International Joint Conference on Artificial
Intelligence, IJCAI 2015, Buenos Aires, Argentina,
July 25-31, 2015, pages 761‚Äì768, 2015.
[8] M. Helmert. The fast downward planning system.
CoRR, abs/1109.6051, 2011.
[9] U. Koeckemann, F. Pecora, and L. Karlsson. Grandpa
hates robots - interaction constraints for planning in
inhabited environments. In Proc. AAAI-2010, 2014.
[10] M. Kuderer, H. Kretzschmar, C. Sprunk, and
W. Burgard. Feature-based prediction of trajectories
for socially compliant navigation. In Proceedings of
Robotics: Science and Systems, Sydney, Australia,
July 2012.

1077

A Combinatorial Search Perspective on Diverse Solution Generation
Satya Gautam Vadlamudi and Subbarao Kambhampati
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{gautam , rao}@asu.edu

Abstract
Finding diverse solutions has become important in many
combinatorial search domains, including Automated Planning, Path Planning and Constraint Programming. Much of
the work in these directions has however focussed on coming up with appropriate diversity metrics and compiling
those metrics in to the solvers/planners. Most approaches use
linear-time greedy algorithms for exploring the state space
of solution combinations for generating a diverse set of solutions, limiting not only their completeness but also their
effectiveness within a time bound. In this paper, we take a
combinatorial search perspective on generating diverse solutions. We present a generic bi-level optimization framework
for finding cost-sensitive diverse solutions. We propose complete methods under this framework, which guarantee finding
a set of cost sensitive diverse solutions satisficing the given
criteria whenever there exists such a set. We identify various aspects that affect the performance of these exhaustive
algorithms and propose techniques to improve them. Experimental results show the efficacy of the proposed framework
compared to an existing greedy approach.

In many real-world domains involving combinatorial search
such as automated planning, path planning and constraint
programming, generating diverse solutions is of much importance. In the case of automated planning, real-world scenario often involves working with unknown or partially
known user preferences (Kambhampati 2007), as the user
preferences are many times difficult to be articulated and
specified completely. Such situations lead to multiple, often, large number of plans that satisfy a given problem instance. In order to facilitate serving the user with a closest
plan possible as per her (hidden) preferences, presenting a
diverse set of plans to the user is explored (Roberts, Howe,
and Ray 2014; Nguyen et al. 2012) so that the user can make
a well-informed decision. In the constraint programming domain, diverse (resp. similar) solutions are explored in order
to handle unknown user preferences as well as to generate
robust solutions (Hebrard et al. 2005).
Several methods have been proposed in the literature for
finding a diverse set of plans. In the context of constraint
programming, (Hebrard et al. 2005) presents a complete
method which creates K copies of the Constraint Satisfacc 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

tion Problem (CSP),each copy with a different set of variable names, adds K
2 additional constraints for handling the
minimum distance requirement between all pairs, and uses
off-the-shelf solvers to generate solutions. As one would expect, they report that this approach generates prohibitively
large CSPs and therefore propose a greedy method. The
greedy approach which has since been widely adopted (Petit and Trapp 2015; Bloem 2015; Roberts, Howe, and Ray
2014; Nguyen et al. 2012) works as follows:
Obtain a candidate solution satisfying any given cost criteria, add this to the solution K-set, provide feedback to the
method finding candidate solutions about the current composition of the K-set so that it tries to find the next candidate
solution distant to the current K-set. Upon obtaining the next
candidate solution, the greedy method adds it to the K-set if
it indeed satisfies the distance criteria and provides feedback
to find the next solution, otherwise the candidate solution is
simply discarded. This process is continued until a set of K
diverse solutions is found. (Roberts, Howe, and Ray 2014;
Eiter et al. 2013) and (Nguyen et al. 2012) consider the
first solution generated to be the starting solution (permanent member) for constructing the K-set through the above
greedy approach. (Bloem 2015) considers an optimal solution to be the starting solution of the greedy method. (Petit
and Trapp 2015) attempt to address the issue of fixed starting solution by running the greedy approach multiple times
with different optimal solutions as the starting solution on
each occasion.
A pertinent issue with the above approaches is that the
first solution (or an optimal solution) is always considered
to be part of the solution set, which may often result in not
finding a K-set even when there exists one, even with a very
good feedback strategy to search for distant solutions after
finding the initial solution. Note that an optimal solution (or
the first found solution) need not be part of a diverse set of
the required size at all. Figure 1 shows an example of an
instance where the optimal solution is not part of the most
diverse solution set of size 2 (assuming that the distance between two plans is inversely proportional to the number of
edges/actions they have in common; p1 and p2 have edge a
in common and p2 and p3 have edge b in common).
In this paper, we address this problem in depth by proposing complete algorithms which guarantee to find a set of K
diverse solutions whenever there exists one. In order to ac-

Start
p1

1

3
G1

a

1
p2

1
2

1 b

p3

G2
Figure 1: A state-space graph where the optimal cost path
p2, does not belong to the most diverse solution set of size 2
{p1,p3}. G1 and G2 indicate goal nodes.
complish completeness, we first need methods for exploring
all possible cost sensitive solutions in the given domain. For
this, we present extensions of the m-A* algorithm (Dechter,
Flerova, and Marinescu 2012) and the Depth-First Branch
and Bound algorithm (Lawler and Wood 1966) for finding
all cost-bounded plans. One could adapt other types of methods such as anytime heuristic search algorithms as well for
this purpose. Second, we present a simple strategy for exhaustively exploring the set of all plan combinations. This
would guarantee completeness for finding a K-set whenever there exists one, thereby addressing the issues in existing methods. However, we note that the simple exhaustive
method ends up having to explore a large number of combinations as one finds more and more candidate plans, severely
impacting the performance of the overall algorithm. In order
to address this problem, we propose a method which considers only a few most promising plan combinations whenever
a new candidate solution is found, and postpones the exploration of remaining combinations for the end to guarantee
completeness. This new method is advantageous over the
widely followed greedy approach on two fronts: it explores
a larger (compared to only one combination of the greedy
approach) but limited number of combinations upon finding
a candidate solution thereby increasing its likelihood of finding a diverse K-set quickly, and it keeps track of the combinations that are left postponed to revisit at the end thereby
guaranteeing completeness.
Further, our method for exploring the plan-combinations
space can be used in conjunction with any of the existing
methods to improve their performance, as it is complementary in nature, replacing the weaker section of those methods
where the greedy approach is present.

Refanidis 2013), air traffic control advisories (Bloem and
Bambos 2014), and robotics (Voss, Moll, and Kavraki 2015).
An important measure in determining diversity is the distance between plans. In this paper, we assume that the distance measure is given as input by the user. Several distance measures have been proposed in the literature that are
quantitative or qualitative (Scala 2014; Coman and MunÃÉozAvila 2011; Goldman and Kuter 2015). Solution diversity
is explored in both deterministic and non-deterministic domains using the distance metrics (Coman 2012). Distance
measures for finding semantically distinct plans are explored
in (Bryce 2014) based on landmarks. In the context of constraint programming, distance constraints in terms of ideal
and non-ideal solutions are studied in (Hebrard, O‚ÄôSullivan,
and Walsh 2007).
SAT-based heuristic methods for generating diverse solutions were proposed in (Nadel 2011). Methods through
compilation to CSP, and using heuristic local search have
been proposed in (Srivastava et al. 2007), which use GPCSP planner (Do and Kambhampati 2001) and LPG planner (Gerevini, Saetti, and Serina 2003). Comparison of firstprinciple techniques and case-based planning techniques to
find diverse plans is shown in (Coman and MunÃÉoz-Avila
2012a). These algorithms too use the greedy approach presented before for exploring the space of plan combinations,
leading to the same issues pointed in the Introduction.

Problem Setup
In this paper, we consider the problem of finding a set of solutions that are not only diverse but are also cost sensitive.
In particular, we consider the problem of finding a set of K
cost sensitive diverse (loopless) solutions. Cost sensitivity
of the solutions is controlled by the input c (maximum cost
of each of the solutions) and diversity of the solution sets is
controlled by the input d (minimum distance between each
pair of solutions; or an appropriate set based diversity metric). Both the cost metric and the distance metric are also assumed to be inputs from the user, hence, the studies on good
quality cost metrics and distance metrics are orthogonal to
our work. Further, we choose the planning domain to showcase our framework and methods, which could be adapted to
other domains. Hence, the problem at-hand can be formally
stated as: Given a planning problem with the set of loopless
solution plans S, a cost metric for the plans C : S ‚Üí R and
a distance metric for the pairs of plans Œ¥ : S √ó S ‚Üí R (a set
based diversity metric may also be used here), the problem
is defined as:
cCOSTdDISTANTkSET: Find P with P ‚äÜ S,
(1)
|P| =k, min Œ¥(p, q) ‚â• d and C(p) ‚â§ c ‚àÄp ‚àà P
p, q ‚àà P

Related Work
Several applications have been related to using diverse solutions in recent years, such as, for course of action generation in cyber security (Boddy et al. 2005), personalized security agents (Roberts et al. 2012), diverse finite
state machines for non-player characters in games (Coman
and MunÃÉoz-Avila 2013; 2012b), formal verification (Nadel
2011), mining group patterns (Vadlamudi, Chakrabarti, and
Sarkar 2012), scheduling personal activities (Alexiadis and

The problem is computationally hard given that the problem
of finding cost-bounded plans is PSPACE-complete (Bylander 1991) and the problem of finding a diverse set of plans
is NP-complete (Bloem 2015) with input size (number of
plans) that can potentially be exponential in terms of the
number of state variables. Finding a set of diverse solutions
is shown to be FPN P [log n] -complete in the context of constraint programming (Hebrard et al. 2005), where n is the
size of the input.

Explore All Solution Combinations
(Level 2)
Solution Stream

Feedback

Explore All Satisficing Solutions
(Level 1)

Figure 2: A framework for finding a diverse set of solutions
that supports completeness.

Proposed Methods & Properties
Now, we present the proposed framework which supports
completeness, and specific methods that obey the framework
requirements. As mentioned above, the problem of finding a
diverse set of plans is a bi-level optimization problem which
involves exploring the set of all candidate plans (Level 1)
and considering the set of all combinations of these plans
(Level 2). Therefore, in order to guarantee completeness,
we propose to have a framework with a complete method
which guarantees finding all the candidate plans and outputs as a stream, and another complete method that takes
the stream of plans being generated by the previous method
as input and explores all combinations of plan sets as per
the diversity criteria until a diverse set of size K is found.
Such a framework ensures that all possible cases are considered and hence guarantees completeness. Figure 2 shows the
framework with the control flow. In this paper, we emphasize
mainly on how to explore all the solutions and all the solution combinations efficiently so as to guarantee completeness while not impeding the search progress due to their individual exhaustive nature. The feedback component shown
in the figure is particular to the domain elements and their
distance measures which we do not explore in this work
leaving it as an option for the user to plug-in to the proposed
framework and methods.

Algorithms for Finding All Cost Sensitive Solutions
For the Level 1, the problem is to find the set of all candidate plans that can potentially be part of a diverse solution
set. In our case, the set of all candidate plans correspond to
the set of all valid loopless plans whose cost is ‚â§ max cost.
We present two complete methods which guarantee generating the set of all candidate plans, one based on DepthFirst Branch and Bound (DFBB) (Lawler and Wood 1966;
Russell and Norvig 1995) and another based on a recent algorithm for finding M best solutions in graphical models,
called m-A* (Dechter, Flerova, and Marinescu 2012). One
may also use other complete methods and extend them to
generate all candidate solutions whose cost is ‚â§ max cost.
First, we present the DFBB based algorithm for finding
all candidate plans, called DFA. It works similar to regular
DFBB search on graph spaces except for the following two
differences: (i) it does not stop after finding a single solution within the max cost bound, and (ii) it does not conduct
full duplicate detection (any state reached through a differ-

ent path from the start state leads to a new node unless there
is a loop, at which point it simply backtracks to find other
solutions). It is easy to prove that:
Lemma 1 DFA generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.
Now, we describe the second algorithm for finding the set
of all candidate plans, called A*A. This is based on the mA* algorithm (Dechter, Flerova, and Marinescu 2012) which
guarantees finding m best solutions/plans by expanding the
minimum set of nodes. First, we describe the basic idea behind m-A* and then we extend it to find the set of all candidate plans for our problem. The basic idea behind m-A* is to
proceed in a manner similar to A* and whenever a duplicate
state is found, the most promising m nodes corresponding to
that state are to be considered for expansion and the rest be
discarded. For our problem, where we want to find the set
of all cost-bounded plans, we will have to keep all the nodes
corresponding to same state for expansion without the mlimit. This eliminates the requirement of full-scale duplicate
detection all-together, instead suggests treatment of all children being generated as new. However, since we are only
interested in loopless plans, we will discard all nodes which
cause loops in the partial plan at any stage, through cycle
checking. We call this adapted strategy- A*A (A* based approach for finding All cost-bounded solutions). Once again,
it can be proven that:
Lemma 2 A*A generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.

Complete Algorithms for Finding a Diverse
Solution Set
Now, we present the algorithms for Level 2 of our framework, where the stream of all candidate plans, the distance
measure, the size of the diverse plan set needed and the minimum distance between any two plans of the solution set is
given as input, to produce a set of satisficing diverse plans.
Algorithm 1 presents a simple strategy called ACER
which explores all combinations of plans in each run for the
in-coming new plan in conjunction with all of the existing
valid plan sets. It is easy to prove that:
Lemma 3 ACER finds a diverse plan set of size K from the
set/stream of all candidate plans whenever there exists one.
However, G can grow rapidly and become an exponential sized set in terms of K with base being the number of
candidate plans (which itself can be of exponential size in
terms of the planning problem input) before finding a diverse plan set. This severely limits its scalability when there
are large number of candidate plans (even for moderate values of K), which is often the case in practice. Next, we will
present a method which does not explore all plan set combinations in one shot instead only a select most promising sets
at each stage, while keeping track of unexplored combinations that may be explored at the end (after processing the
entire stream of candidate plans once) for completeness.

Algorithm 1 Explore All Possible Combinations of Solutions in Each Run (ACER)

Algorithm 2 Explore Most-promising Combinations of Solutions in Each Run (MCER)

1: INPUT :: A candidate plan p (from the stream of all candidate plans), a distance measure dist(), the minimum distance
needed between any two plans of a set min dist, and the size
of the diverse plan set K.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets.
4: for each plan set P ‚àà G do
5:
if dist(p, l) > min dist ‚àÄl ‚àà P then
6:
P 0 ‚Üê P + {p};
7:
if |P 0 | = K then
8:
return P 0 ;
9:
end if
10:
G ‚Üê G‚à™P 0 ;
11:
end if
12: end for
13: G ‚Üê G‚à™{p};
14: return largest P ‚àà G;

1: INPUT :: A candidate plan p (from the stream of all candidate
plans), its sequence number in the stream i, a distance measure
dist(), the minimum distance needed between any two plans
of a set min dist, the size of the diverse plan set K, and the
number of seed plan sets to be explored n.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets with satellite data.
4: Open ‚Üê œÜ; Children ‚Üê œÜ;
5: ExpandMostPromising(G, n, Children);
6: if a diverse set P of size K is found then
7:
return P ;
8: end if
9: while Children 6= œÜ do
10:
Swap Children and Open;
11:
ExpandMostPromising(Open, n, Children);
12:
if a diverse set P of size K is found then
13:
return P ;
14:
end if
15:
Move all plan sets in Open to G;
16: end while
17: P ‚Üê {p}; Pexp ‚Üê i;
18: G ‚Üê G‚à™P ;
19: return largest P ‚àà G;

The second technique, which is an adaptation of the
Anytime Pack Search method (Vadlamudi, Aine, and
Chakrabarti 2015; 2013), focuses on exploring a limited set
of seed nodes in each iteration in a beam search like manner.
It processes the stream of candidate plans much faster than
the previous approach by focusing only on a select number
of most promising plan sets to begin with. The combinations
which this technique ignores while processing the candidate
plan stream are kept track of separately for processing at the
end, which helps in guaranteeing the completeness. Algorithm 2 presents the proposed method MCER for faster processing of the stream of candidate plans. It takes as input,
a plan from the stream of candidate plans and its sequence
number for reasons that will become clear shortly, and the
inputs for determining a diverse set similar to the previous
approach, and the number of seed plan sets to be explored
upon finding a new candidate plan.The plan-combinations
space can be visualized as a set enumeration tree (Rymon
1992), where new branches come at all levels (> 0) dynamically as the stream of candidate plans is processed.
MCER maintains a global set of valid plan sets which
have been produced until now, G, that could be further expanded with new candidate plans. It expands n most promising nodes from this set, which are populated into Children.
If a diverse set of size K is found, it terminates returning the
set. Otherwise, n most promising plan sets from Children
are expanded, and then their n most promising children and
so on until there are no further children to be expanded. It
should also be mentioned at this point, as to what we mean
by ‚Äòmost promising‚Äô, which would be based on f -value of a
plan set (the largest being most promising), and f -value is
in-turn computed as g + h where g is the size of the plan
set and h is the heuristic estimate denoting potential number of plans that can be added to this plan set. In this paper, we have explored using three heuristics: (i) 0, (ii) dispersion of the current set (arithmetic mean of all pair-wise
distances (Myers and Lee 1999)) divided by min dist, and

(iii) quadratic mean of all distances divided by min dist.
The idea behind these heuristics is that the more dispersion
the sets have the more accommodative they could be of new
candidate plans. However, in our experiments, we did not
observe gains of using the dispersion based heuristics in our
experiments compared to the trivial heuristic possibly due
to the limitation of the said heuristics in accounting for the
actions that are not part of the plans found yet, at any given
moment during the runtime. More distance metric based and
domain based estimates can be explored here in future.
Algorithm 3 presents the pseudo-code of ExpandMostPromising routine. It expands the n most promising nodes
from the given list (either G or Open) and puts them in
Children. One significant difference to note here is that,
since all the candidate plans are not available apriori, one
must add the expanded nodes back to G for future consideration with newer candidate plans. While doing so, in order
to avoid repetition, we keep track of the last child generation attempt through the sequence number of candidate plan
considered.
Finally, after the entire stream of candidate solutions has
been processed one by one using MCER, if a diverse set of
size K is not found, we continue to call MCER repeatedly
(this time, without adding back the explored plan sets into G)
until it finds a K-set or terminates exhausting the exploration
of all possible combinations.
Below, we present some of the properties of the proposed
method MCER:
Lemma 4 MCER does not generate the same combination
of plans more than once.
Proof outline:

This is ensured by keeping track of the

Algorithm 3 ExpandMostPromising
1: INPUT :: A set of valid plan sets S to expand, Children,
a distance measure dist(), the minimum distance needed between any two plans of a set min dist, the size of the diverse
plan set K, and the number of plan sets to be explored n.
2: OUTPUT :: Populates Children with new valid plan sets,
returns a diverse plan set of size K if found.
3: T emp ‚Üê œÜ (empty set);
4: for n times do
5:
P ‚Üê most promising plan set from S;
6:
for each candidate plan in the stream from sequence number
i = Pexp + 1 to the latest do
7:
if dist(p, l) > min dist ‚àÄl ‚àà P then
0
8:
P 0 ‚Üê P + {p}; Pexp
‚Üê i;
9:
if |P 0 | = K then
10:
return P 0 ;
11:
end if
12:
Children ‚Üê Children‚à™P 0 ;
13:
end if
14:
Pexp ‚Üê i;
15:
end for
16:
T emp ‚Üê T emp‚à™P ;
17: end for
18: G ‚Üê G‚à™T emp;

sequence number of candidate plan from the last child
generation attempt while expanding a plan set P via Pexp ,
which increases by 1 at each step during expansion (see
Line 14 in Algorithm 3) and the child generation attempts
start from Pexp + 1 every time (see Line 6 in Algorithm 3),
thereby avoiding repetition.
2
Lemma 5 MCER expands at-most n √ó (K ‚àí 1) + 1 number
of plan sets in each execution.
Proof outline: Note that, after expansion of n most promising nodes from G, their n most promising children, and then
their n most promising children and so on are expanded,
until a child of size K is found. Further, size of the children
at each step increases by 1 since a new candidate plan gets
added to the plan set. Therefore, even if we assume that the
initial set of seed nodes are all of size 1, MCER executes
at-most K steps at which point a diverse set of size K will
be found if possible through that set. And at each step,
at-most n number of children are expanded, with only 1 at
level K. Hence, together, at-most n √ó (K ‚àí 1) + 1 number
of plan sets are expanded in each execution of MCER. 2
Lemma 6 MCER guarantees finding a diverse set of plans
of size K if there exists one.
Proof outline: Note that, while we execute MCER several
times with incoming plans from the stream of candidate
plans, each time without exhausting all possible combinations, we keep track of the last expansion attempt for
each node (plan set), and store them in G. Hence all plan
sets which may not have been exhaustively explored with
the candidate plans are present in G when the entire set
of candidate plans has been generated. These plan sets are
then exhaustively explored without re-inserting back in to G

thereby guaranteeing completeness and termination.

2

Now, given a planning problem, a cost metric, a distance
measure, max cost, min dist, and K, for finding a set of
K cost sensitive diverse plans, one could use any one of the
following four combinations: 1) DFA with ACER, wherein
the DFA is executed and whenever a valid plan with cost
< max cost is found, ACER is invoked to find a diverse
set, and then the execution of DFA is continued if a diverse
set with the given requirements is not found, and the process is repeated until termination. We call this combination
DFAA. 2) DFA with MCER, similar to the above strategy of
invoking MCER whenever DFA find a valid cost sensitive
plan, followed by repeated calls to MCER at the end to explore all the remaining plan combinations until termination.
This is denoted by DFAM. 3) A*A with ACER (denoted
by A*AA), and 4) A*A with MCER (denoted by A*AM).
Next section presents the comparison of performances of the
above combinations of methods.

Experimental Results
In this section, we present the experimental results comparing the performances of various proposed algorithms among
themselves as well as with a greedy approach proposed in
the literature. We have implemented all our methods on
top of the Fast Downward planning environment (Helmert
2006), and hence could run problem instances from any of
the supported planning domains. Accordingly, we have conducted experiments on several domains, including, blocks,
rovers, pathways-noneg, airport, driverlog, tpp, zenotravel.
We present the representative results in this paper. All the experiments have been performed on a machine with Intel(R)
Xeon(R) CPU E5-1620 v2 at 3.70GHz and 64GB RAM. The
following distance measure for measuring diversity has been
adopted from (Nguyen et al. 2012):
dist(p1 , p2 ) = 1 ‚àí

A(p1 )‚à©A(p2 )
A(p1 )‚à™A(p2 )

(2)

where A(p) denotes the set of all actions in plan p.
Table 1 shows the comparison of DFAA and A*AA methods on problems (denoted by P.no.) from Blocks domain.
We have used the LM cut heuristic which is admissible, to
guide the search. The algorithms are given a maximum time
of 60sec for solving each problem. Given a set of inputs,
the output shows whether a diverse set of plans of size K is
found (otherwise the size of the largest diverse set found in
parenthesis), the time taken, and the number of plans generated during the process. * denotes that the algorithm stopped
due to the time limit. We see that the DFA based method
generates the cost sensitive plans faster than the A*A based
method in this case, resulting in the processing of more number of plans in a given time.
The difference in the number of plans generated to find
a diverse set of same size highlights the importance of the
order in which the candidate plans are generated. Depending on the order of the plans generated, the number of plans
required to be processed by an exhaustive algorithm to produce a diverse set of specific size varies. As mentioned before, this could be influenced by devising an appropriate distance metric and domain dependent feedback mechanism.

Table 1: Comparison of DFAA and A*AA complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

K

max cost

4
8

20

8

30

8

30

DFAA
min dist
0.6
0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (4)
No* (3)

A*AA

Time (Sec.)

Plans gentd.

0.00
0.00
32.90
2.14
60.00
60.00

22
43
231
323
2567
5140

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
11.06
3.42
60.00
60.00

26
43
130
323
923
4115

Table 2: Comparison of DFAM and A*AM complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAM

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (2)

A*AM

Time (Sec.)

Plans gentd.

0.00
0.00
0.58
1.74
60.00
60.00

26
43
323
323
11680
11908

K-set
found?
Yes
No (4)
Yes
No (5)
No* (7)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
2.62
60.00
60.00

26
43
172
323
12226
12311

Table 3: Comparison of DFA based and A*A based greedy algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAG

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
No (3)
No (3)
No (6)
No (4)
No (6)
No (2)

A*AG

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
0.02
16.70
24.24

43
43
323
323
104712
104712

Table 2 presents the comparison of DFAM and A*AM
methods (with plan-combinations seed set size equal to 30
in each execution). Note that, both algorithms guarantee to
find a diverse set of required size if there exists one, given
they are given enough time to terminate. Our objective with
the MCER based methods is to quickly process the incoming
candidate plans so as to find a diverse set quicker, postponing the exhaustive exploration to the end. Accordingly, we
see that both the methods perform much better than they did
compared to Table 1 by processing larger number of plans.
They are able to find larger sized sets of diverse plans in the
given time than before, although, as one can observe it may
also happen that the select exploration could occasionally
(see DFAM vs DFAA in last rows of both the tables) delay
finding large sets compared to ACER based approaches.
Next, we present the results obtained by integrating the
greedy approach discussed in the Introduction with DFA and
A*A, in Table 3. We call these methods DFAG and A*AG
respectively. As one can observe, while the greedy methods
process the incoming candidate plans very fast, they terminate without finding a diverse set of given size even when
there exists one. Furthermore, even for finding the diverse
sets that they produced, they involve generating far more
number of plans compared to the complete algorithms. This
can be a crucial element when finding multiple plans is difficult for a domain. Also, note that, A*A runs out of memory (4GB per instance) in this case which makes the case

K-set
found?
No (3)
No (3)
No (7)
No (3)
No* (6)
No* (2)

Time (Sec.)

Plans gentd.

0.00
0.00
0.06
0.04
Mem-limit
Mem-limit

43
43
323
323
101908
101908

for memory bounded methods while attempting to generate multiple solutions. Although, one can improve the performance of the greedy approach through feedback mechanisms, considering only one seed plan set for exploration is
likely to continue to affect the performance. Thus, it would
be beneficial to have multiple seed plan sets to be explored
at each stage for better performance.
Now, we present the results obtained on two other domains, namely, Rovers and Zeno-Travel. We show the results with DFA as the base method (for generating all costbounded solutions) in these cases since A*A based methods
were quickly reaching the memory limit on these instances.
Table 4 shows the comparison of DFAA, DFAM and DFAG
methods on a problem from the Rovers domain with 14 objects. Here, a diverse set of size 8 with cost bound 20 is to
be found within 60 seconds. Three sets of results comparing the above three algorithms are presented with different
diversity criterion in each case. Note that, amongst the three
methods, DFAA spends the most amount of effort on exploring plan combinations (exhaustive) whenever a new plan is
found, therefore is only able to generate and process a small
number of plans. Since DFAG spends least amount of effort on exploring plan combinations (greedy) upon finding
a new plan, it is able to generate and scan through a large
number of plans. Whereas DFAM distributes its effort intelligently across plan generation and plan combination exploration, by adjusting the number of seeds n as per domain and

Table 4: Comparison of DFAA, DFAM and DFAG methods. Domain: Rovers. Problem: roverprob4213 (14 objects), K: 8,
max cost : 20, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
Yes
No* (6)
No* (4)

DFAM

Time (Sec.)

Plans gentd.

0.88
60.00
60.00

64
372
1048

K-set
found?
Yes
Yes
No* (6)

DFAG

Time (Sec.)

Plans gentd.

0.00
9.84
60.00

65
306061
2047871

K-set
found?
Yes
Yes
No* (4)

Time (Sec.)

Plans gentd.

0.00
1.22
60.00

64
304553
15988265

Table 5: Comparison of DFAA, DFAM and DFAG methods. Domain: Zeno-Travel. Problem: ZTRAVEL-2-5 (17 objects), K: 8,
max cost : 15, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
No* (6)
No* (4)
No* (3)

DFAM

Time (Sec.)

Plans gentd.

60.00
60.00
60.00

294
579
1171

K-set
found?
Yes
Yes
Yes

problem size (in this case, n = 30). Note that, MCER with
n = 1 would result in an exploration similar to that of the
greedy method, with the exception of going further and guaranteeing completeness. And, MCER with n = ‚àû would result in an exploration similar to that of ACER. Results show
that DFAA performs poorly on large instances due to its exhaustive exploration. Between DFAM and DFAG, on easier
problems (with low diversity/distance requirement; first 2 instances), greedy and MCER based methods fare similarly,
with the greedy method slightly outperforming the MCER
based method (note that, one can change the n value to 1
here, to make MCER deliver results similar to that of the
greedy method, however, this is not beneficial in general).
On the other hand, when the diversity required is higher
(third instance), MCER based method outperforms greedy
approach by finding a larger diverse set using only 12.81%
of the plans generated by that of the greedy method, showcasing the advantage of exploring plan combinations more
thoroughly.
Table 5 presents the comparison of DFAA, DFAM and
DFAG methods on a problem from the Zeno-Travel domain
with 17 objects. Once again, we observe similar results as
that of the previous two domains. ACER based method fares
poorly due to its exhaustive nature which limits its reach
in scanning through the full space plans in the given time.
And, between DFAM and DFAG, while DFAG may find
the K-set in shorter time in some cases, DFAM continues
to leverage the advantage of exploring plan combinations
thoroughly and is able to find required K-sets using lesser
number of plans. This is a crucial element in working with
domains where producing individual plans itself is very difficult, which is especially the case when large problem sizes
are involved.
Before we conclude, we present a note on solving large
sized problems. In such cases, while completeness may not
be a practical expectation, one should be able to gain performance over using the greedy approach by carefully integrating the proposed MCER approach with the state-of-the-art
solvers/planners, feedback mechanisms, and using efficient
heuristics for exploring the space of plan combinations. Fur-

DFAG

Time (Sec.)

Plans gentd.

0.18
0.62
21.46

1447
7884
505186

K-set
found?
Yes
Yes
Yes

Time (Sec.)

Plans gentd.

0.20
0.44
10.30

1447
8545
619579

thermore, the proposed methods can be easily extended to
solve related problems such as, finding a K-set with maximum diversity, finding largest K-set with a given diversity,
finding high quality (in terms of the cost of the plans) K-sets
with given diversity, and a combination thereof involving the
generation of multi-objective pareto fronts.

Conclusion
In this paper, we take a combinatorial search perspective of
the widely studied diverse solution generation problem. We
observe that many of the approaches proposed in various
domains such as automated planning and constraint satisfaction use a linear-time greedy method for exploring plan
set combinations, that makes them fail while searching for
a diverse set of required size even when there exists one.
We propose a bi-level optimization framework and methods
to find cost-sensitive diverse solutions which guarantee to
find a diverse set of required size whenever there exists one.
We identify the critical elements that affect the performance
in such scenarios and propose efficient methods to handle
them. We showcased the efficacy of the proposed methods
by implementing our methods as part of the Fast Downward
planning system and comparing with the existing greedy approach across various domains. The proposed methods have
found larger sets of diverse solutions compared to the greedy
approach on almost all problem instances, within the same
time bound, proving their utility.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
Alexiadis, A., and Refanidis, I. 2013. Generating alternative plans
for scheduling personal activities. In Proceedings of the Seventh
Scheduling and Planning Applications workshop, 35‚Äì40.
Bloem, M., and Bambos, N. 2014. Air traffic control area configuration advisories from near-optimal distinct paths. Journal of
Aerospace Information Systems 11(11):764‚Äì784.

Bloem, M. J. 2015. Optimization and Analytics for Air Traffic
Management. Ph.D. Dissertation, Stanford University.
Boddy, M. S.; Gohde, J.; Haigh, T.; and Harp, S. A. 2005. Course
of action generation for cyber security using classical planning.
In Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005), June 5-10 2005,
Monterey, California, USA, 12‚Äì21.
Bryce, D. 2014. Landmark-based plan distance measures for diverse planning. In Proceedings of the Twenty-Fourth International
Conference on Automated Planning and Scheduling, ICAPS 2014,
Portsmouth, New Hampshire, USA, June 21-26, 2014.
Bylander, T. 1991. Complexity results for planning. In Proceedings
of the 12th International Joint Conference on Artificial Intelligence
- Volume 1, IJCAI‚Äô91, 274‚Äì279. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc.
Coman, A., and MunÃÉoz-Avila, H. 2011. Generating diverse plans
using quantitative and qualitative plan distance metrics. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11,
2011.
Coman, A., and MunÃÉoz-Avila, H. 2012a. Diverse plan generation
by plan adaptation and by first-principles planning: A comparative
study. In Case-Based Reasoning Research and Development - 20th
International Conference, ICCBR 2012, Lyon, France, September
3-6, 2012. Proceedings, 32‚Äì46.
Coman, A., and MunÃÉoz-Avila, H. 2012b. Plan-based character
diversity. In Proceedings of the Eighth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-12,
Stanford, California, October 8-12, 2012.
Coman, A., and MunÃÉoz-Avila, H. 2013. Automated generation of
diverse npc-controlling fsms using nondeterministic planning techniques. In Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-13,
Boston, Massachusetts, USA, October 14-18, 2013.
Coman, A. 2012. Solution diversity in planning. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Dechter, R.; Flerova, N.; and Marinescu, R. 2012. Search algorithms for m best solutions for graphical models. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Do, M. B., and Kambhampati, S. 2001. Planning as constraint
satisfaction: Solving the planning graph by compiling it into CSP.
Artif. Intell. 132(2):151‚Äì182.
Eiter, T.; Erdem, E.; Erdogan, H.; and Fink, M. 2013. Finding
similar/diverse solutions in answer set programming. Theory and
Practice of Logic Programming 13(03):303‚Äì359.
Gerevini, A.; Saetti, A.; and Serina, I. 2003. Planning through
stochastic local search and temporal action graphs in lpg. J. Artif.
Int. Res. 20(1):239‚Äì290.
Goldman, R. P., and Kuter, U. 2015. Measuring plan diversity:
Pathologies in existing approaches and A new plan distance metric. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., 3275‚Äì
3282.
Hebrard, E.; Hnich, B.; O‚ÄôSullivan, B.; and Walsh, T. 2005. Finding diverse and similar solutions in constraint programming. In
Proceedings of the 20th National Conference on Artificial Intelligence - Volume 1, AAAI‚Äô05, 372‚Äì377. AAAI Press.
Hebrard, E.; O‚ÄôSullivan, B.; and Walsh, T. 2007. Distance constraints in constraint satisfaction. In Proceedings of the 20th In-

ternational Joint Conference on Artifical Intelligence, IJCAI‚Äô07,
106‚Äì111. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc.
Helmert, M. 2006. The fast downward planning system. J. Artif.
Intell. Res. (JAIR) 26:191‚Äì246.
Kambhampati, S. 2007. Model-lite planning for the web age
masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, July 22-26, 2007, Vancouver,
British Columbia, Canada, 1601‚Äì1605.
Lawler, E. L., and Wood, D. E. 1966. Branch-and-bound methods:
A survey. Operations Research 14(4):699‚Äì719.
Myers, K. L., and Lee, T. J. 1999. Generating qualitatively different
plans through metatheoretic biases. In AAAI, 570‚Äì576. American
Association for Artificial Intelligence.
Nadel, A. 2011. Generating diverse solutions in sat. In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing, SAT‚Äô11, 287‚Äì301. Berlin, Heidelberg:
Springer-Verlag.
Nguyen, T. A.; Do, M. B.; Gerevini, A.; Serina, I.; Srivastava, B.;
and Kambhampati, S. 2012. Generating diverse plans to handle
unknown and partially known user preferences. Artif. Intell. 190:1‚Äì
31.
Petit, T., and Trapp, A. C. 2015. Finding diverse solutions of
high quality to constraint optimization problems. In IJCAI. International Joint Conference on Artificial Intelligence.
Roberts, M.; Howe, A.; Ray, I.; and Urbanska, M. 2012. Using
planning for a personalized security agent. In AAAI Workshops.
Roberts, M.; Howe, A. E.; and Ray, I. 2014. Evaluating diversity
in classical planning. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Russell, S., and Norvig, P. 1995. Artificial intelligence: a modern
approach. Prentice Hall.
Rymon, R. 1992. Search through systematic set enumeration. Technical Reports (CIS) 297.
Scala, E. 2014. Plan repair for resource constrained tasks via numeric macro actions. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Srivastava, B.; Nguyen, T. A.; Gerevini, A.; Kambhampati, S.; Do,
M. B.; and Serina, I. 2007. Domain independent approaches for
finding diverse plans. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad,
India, January 6-12, 2007, 2016‚Äì2022.
Vadlamudi, S. G.; Aine, S.; and Chakrabarti, P. P. 2013. Anytime
pack heuristic search. In Pattern Recognition and Machine Intelligence - 5th International Conference, PReMI 2013, Kolkata, India,
December 10-14, 2013. Proceedings, 628‚Äì634.
Vadlamudi, S.; Aine, S.; and Chakrabarti, P. 2015. Anytime pack
search. Natural Computing 1‚Äì20.
Vadlamudi, S. G.; Chakrabarti, P. P.; and Sarkar, S. 2012. Anytime
algorithms for mining groups with maximum coverage. In Tenth
Australasian Data Mining Conference, AusDM 2012, Sydney, Australia, December 5-7, 2012, 209‚Äì220.
Voss, C.; Moll, M.; and Kavraki, L. E. 2015. A heuristic approach
to finding diverse short paths. In IEEE International Conference
on Robotics and Automation, ICRA 2015, Seattle, WA, USA, 26-30
May, 2015, 4173‚Äì4179.

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence Multiagent Interaction without Prior Coordination: Technical Report WS-16-11

A Game Theoretic Approach to Ad-hoc Coalitions in Human-Robot Societies
Tathagata Chakraborti, Venkata Vamsikrishna Meduri, Vivek Dondeti, Subbarao Kambhampati
Department of Computer Science Arizona State University Tempe, AZ 85281, USA {tchakra2,vmeduri,vdondeti,rao}@asu.edu Abstract
As robots evolve into fully autonomous agents, settings involving human-robot teams will evolve into humanrobot societies, where multiple independent agents and teams, both humans and robots, coexist and work in harmony. Given such a scenario, the question we ask is - How can two or more such agents dynamically form coalitions or teams for mutual benefit with minimal prior coordination? In this work, we provide a game theoretic solution to address this problem. We will first look at a situation with full information, provide approximations to compute the extensive form game more efficiently, and then extend the formulation to account for scenarios when the human is not totally confident of its potential partner's intentions. Finally we will look at possible extensions of the game, that can capture different aspects of decision making with respect to ad-hoc coalition formation in human-robot societies.

Robots are increasingly becoming capable of performing daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made the dream of human-robot cohabitation a not so distant reality. In this work we envisage such an environment where humans and robots participate autonomously (possibly with required interactions) with their own set of tasks to achieve. It has been argued (Chakraborti et al. 2016) that interactions in such situations are inherently different from those studied in traditional human-robot teams. One typical aspect of such interactions is the lack of prior coordination or shared information, due to the absence of an explicit team. This brings us to the problem we intend to address in this paper - given a set of tasks to achieve, how can an agent proceed to select which one to achieve? In a shared environment such as the one we described, this problem cannot be simply solved by picking the goal with the highest individual utility, because the utility, and sometimes even the success of the plan (and hence the corresponding goal) of an agent are contingent on the intentions of the other agents around it. However, such interactions are not adversarial - it is just that the environment is shared among self-interested agents. Thus, an agent may choose to form an ad-hoc team with another agent in order to increase its utility, and such coalition formation should preferably be feasible with minimum prior

coordination. For example, a human with a goal to deliver two items to two different locations may team up with a delivery robot that can accomplish half of his task. Further, if the robot was itself going to be headed in one of those directions, then it is in the interest of both these agents to form this coalition. However, if the robot's plan becomes too expensive as a result, it might decide that there is not enough incentive to form this coalition. Moreover, as we highlighted before, possible interactions between agents are not just restricted to cooperative scenarios only - the plans of one agent can make the other agent's plans fail, and it may happen that it is not feasible at all for all agents to achieve their respective goals. Thus there are many possible modes of interaction between such agents, some cooperative and some destructive, that needs to be accounted for before the agents can decide on their best course of action - both in terms of which goal to choose and how to achieve it. In this paper we model this problem of optimal goal selection as a two player game with perfect information, and propose to cut down on the prior coordination of forming such ad-hoc coalitions by looking for Nash equilibriums or socially optimal solutions (because neither agent participating in such a coalition would have incentive to deviate). We subsequently extend it to a Bayesian game to account for situations when agents are not sure of each other's intent. We will also look at properties, approximations, and interesting caveats of these games, and motivate several extensions that can capture a wide variety of ad-hoc interactions.

1

Related Work

There is a huge variety of work that looks at team formation from different angles. The scope of our discussion has close ties with concepts of required cooperation and capabilities of teams to solve general planning problems, introduced in (Zhang and Kambhampati 2014), and work on team formation mechanisms and properties of teams (Shoham and Tennenholtz 1992; Tambe 1997). However, in this particular work, we are more interested in the mechanism of choosing goals that can lend to possible cooperative interactions, as opposed to the mechanism of team design based on the goals themselves. Thus the work of Zhang and Kambhampati can provide interesting heuristics towards cutting down on the computation of the extensive form game we will propose, while existing work on different modes of team formation

546

contribute to the motivation of the Bayesian formulation of the game discussed in later sections. From the game theoretic point of view, coalition formation has been a subject of intense study (Ray and Vohra 2014) and the human-robot interaction community can derive significant insights from it. Of particular interest are Overlapping Coalition Formation or OCF Games (Zick, Chalkiadakis, and Elkind 2012; Zick and Elkind 2014), which look at a cooperative game where the players are endowed with resources, with provisions for the players to display different modes of coalitions based on how they utilize the resources. OCF games use arbitration functions that decide the payoffs for the deviating players based on how it is affecting the non-deviating players and it helps in forming stable coalitions. This becomes increasingly relevant in shared environments such as the one we discuss here. Finally, an interesting problem that can often occur is such situations (especially with the way we have formulated the game in the human's favor) is the problem of free-riding where agents take advantage of coalitions and try to minimize their effort (Ackerman and Br^ anzei 2014), which is certainly an important aspect of designing such games.

2.2

Representation of Human-Robot Coalitions

We will represent coalitions of such agents by means of a super-agent transformation (Chakraborti et al. 2015a) on a set of agents that combines the capabilities of one or more agents to perform complex tasks that a single agent might not be capable of doing. Note that this does not preclude joint actions among agents, because some actions that need that need more than one agent (as required in the preconditions) will only be doable in the composite domain. Definition 1.1 A super-agent is a tuple  = , D where    is a set of agents in the environment E , and D is the transformation from the individual domain models to a composite domain model given by D = FO ,  A . Definition 1.2 The planning problem of a super-agent  is given by  = FO , D , I , G where the composite initial and goal states are given by I =  I and G =  G respectively. The solution to the planning problem is a composite plan  = µ1 , µ2 , . . . , µ| | where µi = {a1 , . . . , a|| }, µ() = a  A µ   such that  (I ,  ) |= G , where the modified transition function  (µ, s) = (s \ aµ eff- (a))  aµ eff+ (a). The cost of a composite plan is C ( ) = µ aµ Ca and    is optimal if C ( )  C ( ) with  (I ,  ) |= G . The composite plan can be viewed as a union of plans contributed by each agent    whose component can be written as  () = a1 , a2 , . . . , an , ai = µi ()  µi   .

2
2.1

Preliminaries

Environment and Agent Models

Definition 1.0 The environment is defined as a tuple E = F, O, , G ,  , where F is a set of first order predicates that describes the environment, and O is the set of objects,   O is the set of agents (which may be humans or robots), G = {g | g  FO } 1 is the set of goals that these agents are tasked with, and   O is the set of resources. Each goal has a reward R(g )  R+ associated with it. We use PDDL (Mcdermott et al. 1998) style agent models for the rest of the discussion, but most of the analysis easily generalizes to other modes of representation. The domain model D of an agent    is defined as D = FO , A , where A is a set of operators available to the agent. The action models a  A are represented as a = Ca , Pa , Ea where Ca is the cost of the action, Pa  FO is the list of pre-conditions that must hold for the action a to be applicable in a particular state S  FO of the environment; and Ea = ef f + (a), ef f - (a) , ef f ± (a)  FO is a tuple that contains the add and delete effects of applying the action to a state. The transition function  (∑) determines the next state after the application of action a in state S as  (a, S ) |=  if Pa  S ; |= (S \ ef f - (a))  ef f + (a) otherwise. A planning problem for the agent  is given by the tuple  = FO , D , I , G , where I , G  FO are the initial and goal states respectively. The solution to the planning problem is an ordered sequence of actions or plan given by  = a1 , a2 , . . . , a| | , ai  A such that  ( , I ) |= G , where the cumulative transition function is given by  (, s) =  ( a2 , a3 , . . . , a|| ,  (a1 , s)). The cost of the plan is given by C ( ) = a Ca and the optimal plan    is such that C ( )  C ( )  with  ( , I ) |= G .
1

2.3

The Use Case

Throughout the rest of the discussion we will use the setting from Talamadupula et al. which involves a human commander CommX and a robot in a typical Urban Search and Rescue (USAR) scenario, as illustrated in Figure 1. The environment consists of interconnected rooms and hallways, which the agents can navigate and search. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other agents (not shown) in the environment. A sample domain is available at http://bit.ly/1Fko7MAhttp://bit.ly/1Fko7MA. The shared resources here are the two medkits - i.e. some of the plans the agents can execute will lock the use of and/or change the position of these medkits, so as to make the other agent's plans, contingent on that particular resource, invalid.

SO is any S  F instantiated / grounded with objects from O.

Figure 1: Use case - Urban Search And Rescue (USAR).

547

3

Ad-hoc Human-Robot Coalitions

In this section we will look at how two agents (the human and the robot) in our scenario, can coordinate dynamically by forming impromptu teams in order to achieve either individually rational or socially optimal behaviors.

3.1

Motivation

 = , D is the super-agent representing the coalition formed by  = {H, R} with I = IH  IR and j G = Gi H  GR . Here, the first term in the expression for utility denotes the utility of the goal itself as defined in the environment in Section 2.1, while the second term captures the resultant best case utility of plans due to agent interactions. More on this below. Human-centric robots. At this point we make an assumption about the role of the robots in our human-robot society - we assume that the robots exist only in the capacity of autonomous assistance, i.e. in coalitions that may be formed with humans and robots, the robot's role is to improve the quality of life of the humans (by possibly, in our case, reducing the costs of plans) and not vice versa. Thus, in the expression of utility, the human uses a min  imizing term - with no interactions C (H ) = C ( (H )),   otherwise C (H ) > C ( (H )). Similarly, in case of   the robot, with no interactions C (R )  C ( (R)) and   C (H ) <=> C ( (H )) otherwise, since the interactions may or may not be always cooperative for the robot. Note that this formulation also takes care of the cases when the robot goal becomes unachievable due to negative interactions with the human (this is why we have the maximizing term; the difference is triggered due to negative interactions with the human plan in absence of coalitions). Also note that the goal utility is using a combined goal due to the particular action profile, this captures cases when goals have interactions, i.e. a conjunction of goals may have higher (or lower) utility than the sum of its components. This can be easily ensured while generating plans for a given coalition, by either discounting the costs of actions of the robot with respect to those of the humans by a suitable factor, or more preferably, by just penalizing the total cost of the human component in the composite plan more. The assumption of course does not change the formulation in any way, it is just more aligned with the notion of the social robots being envisioned currently. Of course, in this sense the utilities of both the humans and robots will now become identical, with a minimizing cost term. Now that we have defined the game, the question is how do we choose actions for each agent? Remember that we want to find solutions that will preclude the need to coordinate. We can take two approaches here - we can make agents individually rational (in which case both the human and the robot looks for a Nash equilibrium, so neither has a reason to defect; or we can make the agents look for a socially optimal solution (so that sum of utilities is maximized).

Consider the scenario shown in Figure 1. Suppose one of CommX's goal is to perform triage in room1, while one of the Robot's goals is to deliver a medkit to room1. Clearly, if both the agents choose to do their optimal plans and plan to use medkit1 in room2, the Robot's plan fails (assuming the CommX gets there first). The robot then has two choices - (1) it can choose to achieve some other goal, i.e. maximize it's own rewards, (2) it can choose to deliver the other medkit2 from room3, i.e. maximize social good. Indeed there are many possible ways that these agents can interact. For example, the utility of choosing any goal may be defined by the optimal cost of achieving that goal individually, or as a team. This in turn affects the choice whether to form such teams or not. In the discussion that follows, we model this goal selection (and team formation) problem as a strategic game with perfect information.

3.2

Formulation of the Game

We refer to our static two-player strategic game Goal Allocation with Perfect Information as GAPI = , {A }, {U  } . The game attempts to determine, given complete information about the domain model and goals of the other agent, which goal to achieve and whether forming a coalition is beneficial. The game is defined as follows - Players - The game has two players  = {H, R} the human H and the robot R respectively. - Actions - The actions of the agents in the strategic game are the goals that they can select to achieve. Thus, for each agent    we define a set of goals G  = |G  | 2  {G1  , G , . . . , G }  G , and the action set A of the agent  is the mapping that assigns one of these goals as its planning goal, i.e. A : G  - G. Note that this is distinct from the action models defined in PDDL for each of the individual agents (which helps the agent figure out how this goal G is achieved, and the resultant utility). - Utilities - Finally, as discussed previously, the utility of an action depends on (apart from the utility of the goal itself) the way the agent chooses to achieve it, and is contingent also on the plans of the other agent (due to, for example, resource conflicts), and is given by --
R i j   UH (AH i , Aj ) = R(GH  GR ) - min{C (H ), C ( (H ))} R UR (AH i , Aj ) i    = R(GH  Gj R ) - C ( (R)) if C (H ) > C ( (H )) j i   = R(GH  GR ) - max{C (R ), C ( (R))}, otherwise.

3.3

Solving for Nash Equilibriums

 where, H is the optimal plan or solution of the planning  problem defined by H = FO , DH , IH , Gi H , R is the j  optimal solution of R = FO , DR , IR , GR , and  is the optimal solution of  = FO , D , I , G , where

As usual, the Nash equilibriums in GAPI are R given by action profiles AH such that i , Aj H R H R R UH (Ai , Aj )  UH (Ak : k=i , Aj ) and UR (AH i , Aj )  H R UH (Ai , Ak : k=j ). It is easy to prove that there is no guaranteed Nash equilibrium in GAPI. We will instead motivate a slightly different game GAPI-Bounded where the robot only agrees to deviate from its optimal plan up to a certain degree, i.e. there is a bound on the amount of assistance the robot chooses to provide.

548

Definition 1.3. The differential help  (g, Gi R ) provided i R by the robot R with goal GR  G , when the human H picks goal g  G H , measures the decrease in utility of the robot upon forming a coalition with the human, and is    given by  (g, Gi R ) = |C ( (R)) - C (R )|, where R is i  the optimal solution of R = FO , DR , IR , GR , and  is the optimal solution of  = FO , D , IH  IR , g  Gi R , where  =  = {H, R}, D . Thus in GAPI-Bounded the utility function is modified from the one in GAPI as follows R i  UH (AH i , Ai ) = R(GH ) - C (H ) j R  UR (AH i , Aj ) = R(GR ) - C (R ) : k=j j j  if Gk  G H s.t.  (Gi H , GR ) > {R(GR ) - C (R )} - R k     {R(GR ) - C (R )}, where R , R and H are the optimal plans or solutions to the planning problems j k k i R = FO , DR , IR , GR , R = FO , DR , IR , GR and respectively; and otherwise H = FO , DH , IH , Gi H R i  UH (AH i , Ai ) = R(GH ) - C ( (H )) j R  UR (AH i , Aj ) = R(GR ) - C ( (R))  where  is the optimal solution of  = FO , D , IH  IR , g  Gj R , where  =  = {H, R}, D . This basically means that if the penalty that the robot incurs by choosing to assist the human is so great that it could rather do something else instead (i.e. choose another goal), then it switches back to using its individual optimal plan, i.e. no coalition is formed. If the individual optimal plans are always feasible (otherwise these do not participate in the Nash equilibriums below), this leads to the following result. R Claim. AH must be a Nash equilibrium of i , Aj  GAPI-Bounded when j  = arg maxGj G R R(Gj R) - j C (R ) and i = arg maxi UH (Gi H , GR ).
 R

Further, it may be noted here that there may be many such Nash equilibriums in GAPI-Bounded and these are also the only ones, i.e. all Nash equilibriums in GAPI-bounded must satisfy the conditions in the above claim.

3.4

Solving for Social Good

Similarly, the socially optimal goal selection strategies are   R given by the action profiles AH i , Aj  where {i , j } = R H R arg maxi,j UH (AH i , Aj ) + UR (Ai , Aj ). The socially optimal action profiles may not necessarily correspond to any Nash equilibriums of either GAPI or GAPI-Bounded. Individual Irrationality and -Equilibrium. Given the way the game is defined, it is easy to see that the socially good outcome may not be individually rational for either the human or the robot, since the robot always has the incentive to defect to choosing G R and the human will then choose the corresponding highest utility goal for himself. This leaves room for designing autonomy that can settle for action proH R files A^ , A^ referred to as -equilibriums, for the purpose i j
R H R of social good, i.e. |UH (AH , A^ )|  and i , Aj  ) - UH (A^ i j R H R |UR (AH , A^ )|  . Note that this deviai , Aj  ) - UR (A^ i j tion is distinct from the concept of bounded differential assistance we introduced in Section 3.3.

Price of Anarchy. The price of deviating from individual rationality is referred to as the Price of Anarchy and is measured by POS =
H R H R UH (A^ ,A^ )+UR (A^ ,A^ ) i j i i

UH (AH ,AR )+UR (AH ,AR ) i j i j

.

3.5

Caveats

Proof Sketch. Let us define the utility function of the robot R for achieving a goal g  G R by itself as  (g ) =   R(g ) - C (R ), where R is the optimal solution to the planning problem R = F, O, DR , IR , g . Further, given the  goal set G R of the robot, we set Gj R = arg maxg G R  (g ),  i.e. Gj R corresponds to the highest utility goal that the robot can achieve by itself. Now consider any two goals j j j R i H Gj R , GR  G , GR = GR . We argue that GH  G , j H R H R UR (Ai , Aj  )  UR (Ai , Aj ). This is because  (GR )  H R  (Gj R ) and by problem definition i, k |UR (Ai , Aj  ) -
R UR (AH - Thus, in general, the k , Aj  )|  goal ordering induced by the function  is preserved by the utility function UR , and consequently AR j  is a dominant strategy of the robot. It follows that AH i such that  i = arg maxi UH (Gi , G ) is the corresponding best reH R R sponse for the human. Hence AH , A must be a Nash   i j equilibrium. Hence proved.
  (Gj R)

No or Multiple Nash Equilibriums. One of the obvious problems with this approach is that it does not guarantee a unique Nash equilibrium, if it exists at all. This has serious implications on the problem we set out to solve in the first place - which goals do the agents choose to plan for, and how? Note, however, that this is not really a feature of the formulation itself but of the domain or the environment, i.e. the action models of the agents and the utilities in the goals will determine whether there is a single best coalition that may be formed given a particular situation. Thus, there seems to be no principled way of solving this problem in a detached manner, without any form of communication between the agents. But our approach still provides a way to deliberate over the possible options, and communicate to resolve ambiguities only with respect to the Nash equilibriums, rather than the whole set of goals, or even just those in each agent's dominant strategy, which can still provide significant reduction in the communication overhead. Infeasibility of the Extensive Form Game. Note here that the utilities of the actions are calculated from the cost of plans to achieve the corresponding goals, which involves solving two planning problems per action. This means that, in order to get the extensive form of GAPI, we need to solve O(|G H |◊|G R |) planning problems in total (note that solving for  gives utilities for both agents H and R), which may be infeasible for large domains. So we need a way to speed up our computation (either by computing an approximation

 (Gj R ).

549

and/or finding ways to calculate multiple utility values at once), while simultaneously preserving guarantees from our original game in our approximate version. Fortunately, we have good news. Note that all we require are costs of the plans, not the plans themselves. So a promising approach towards cutting down on the computational complexity is by using heuristic values for the initial state of a particular planning problem as a proxy towards the true plan cost. Note that the better the heuristic is, the better our approximation is. So the immediate question is - What guarantees can we provide on the values of the utilities when we use heuristic approximation? Are the Nash equilibriums in the original game still preserved? This brings us to the notion of "well-behaved heuristics" as follows Definition 1.4 A well-behaved heuristic h : S ◊ S  R+ , S  FO is such that h(I, G1 )  h(I, G2 ) whenever     C (1 )  C (2 ), where 1 and 2 are the optimal solutions to the planning problems 1 = FO , D, I, G1 and 2 = FO , D, I, G2 respectively. We define GAPI as a game identical to GAPI but with a modified utility function as follows R i i i UH (AH i , Aj ) = R(GH ) - min{h(GH , IH ), h(GH , IH  IR )} R UR (AH i , Aj ) i i i = R(GH ) - h(Gj R , IH  IR ) if h(GH , IH ) > h(GH , IH  IR ) j j ) - max { h ( G , I ) , h ( G , I  I ) } , otherwise. = R(Gi R H R R R H

i-1 1 1 ^ ^ ^ max{h(I, Gi  ), h(I, G )}; h(I, G ) = h(I, G ). Then h is well-behaved. Hence proved. These properties of GAPI-Bounded, GAPI and GAPI enables computation of approximations, and partial profiles, to the extensive form of GAPI, while maintaining the nature of interactions, thus making the formulation more tractable.

4
4.1

Bayesian Modeling of Teaming Intent
Motivation

In the previous sections we considered both individual and team plans, and as teams we considered optimal plans for a coalition. In reality there are many ways that a particular coalition can achieve a particular goal, and correspondingly there are different modes of interaction between the teammates. We discuss four such possibilities briefly here ∑ Individual Optimality - In this type of planning, each agent computes the individual optimal plan to achieve their goals. Note that this plan may not be actually valid in the environment during execution time, due to factors such as resource conflicts due to plans of the other agents. ∑ Joint Optimality - Here we compute the joint optimal for a coalition; and this optimal plan is computed in favor of the human as discussed previously in Section 3.2. ∑ Planning with Resource Conflicts - In (Chakraborti et al. 2015b) we explored a technique for the robot to produce plans so as to ensure the success of the human plans only, and explored different modes of such behavior of the robot in terms of compromise, opportunism and negotiation. Thus utilities for the human plans computed this way is, at times, same as the joint optimal, but in general is greater than or equal to the individual optimal and less than or equal to the joint optimal. ∑ Planning for Serendipity - In (Chakraborti et al. 2015a) we looked at a special case of multi-agent coordination, where the robot computes opportunities for assisting the human in the event the human is not planning to exploit the robot's help. Here, as in the previous case, utilities for the human plans computed this way is again greater than or equal to the individual optimal and less than or equal to the joint optimal plans. Going back to our use case in Figure 1, suppose the robot has a goal to deliver a medkit to room1, and CommX has a goal to conduct triage in room1, for which he also requires a medkit (and his optimal plan involves picking up medkit1 in room2). For individual optimal plans both the robot and the human will go for medkit2 (thus, in this situation, individual optimal plans are actually not feasible). For the joint optimal, the coalition can team up to both use the same medkit thus achieving mutual benefit. In case the robot is only planning to avoid conflicts, it can settle for using medkit3 which is further away, or the robot can also intervene serendipitously by handing over medkit2 in the hallway thus achieving higher utility through cooperation without directly coordinating. For our problem, this has the implication that we can no longer be sure of the plan (and consequently the utility) even

Note that in order to get a heuristic estimate of an agent's contribution to the composite plan, we compute the heuristic with respect to achieving the individual agent goal using the composite domain of the super-agent, which of course gives a lower bound on the real cost of the composite plan used to achieve that agent's goal only. Claim. NEs in GAPI are preserved in GAPI. Proof Sketch. This is easy to see because orderings among costs are preserved by a well-behaved heuristic, and hence ordering among utilities, which is known to keep the Nash equilibriums unchanged. Note that the reverse does not hold, i.e. GAPI may have extra Nash equilibriums due to the equality in the definition of well-behaved heuristics. Definition 1.5 We define a goal-ordering on the goal set G  of agent  as a function f : [1, |G  |]  [1, |G  |] such that G  G  . . .  G . This means that the goals of an agent are such that they are all different subgoals of a single conjunctive goal. We will refer to the game with agents with such ordered goal sets as GAPI (identical to GAPI otherwise). Claim. NEs in GAPI are preserved in GAPI.
 Proof Sketch. Since G  is goal-ordered, C (f (1) )     C (f (2) )  . . .  C (f (|G  |) ), where, as usual, i is the optimal solution to the planning problem i = FO , D, I, Gi  . Let us consider a non-trivial admissible ^ such that h ^ (I, Gi ) = heuristic h and define a heuristic h  f (1) f (2) f (|G |)


550

when a particular goal has been chosen. Rather what we have is a possible set of utilities for each goal. However we can do better than to just take the maximum (or minimum as the case may be) of these utilities as we did previously, because we now know how such behaviors are being generated and so we can leverage additional information from an agent's beliefs about the other agent to come up with optimal response strategies. This readily lends the problem to a formulation in terms of Bayesian strategic games, which we will discuss in the next section.

powerful approximations, and the ability to deal with issues such as synchronization and coalitions evolving across individual goal allocations. For GAPI-Bayesian, this also includes evolving beliefs as we will see below.

5.2

Impact of Intent Recognition

4.2

Formulation of the Game

We define our two-person static Bayesian game GAPI-Bayesian = , B , AH , {AR,B }, U H , {U R,B } with belief B over the type of robot as follows - Players - We still have two players - the human H and the robot R, as in the previous games. - Actions - The actions of the players are similarly identical to GAPI, i.e. the action set of agent   {H, R} is the mapping A : G  - G. - Beliefs - The human has a set of beliefs on the robot B = {B1 , B2 , . . . , B|B| } characterized by the distribution B  P , i.e. the robot can be of any of the types in B with probability P (B ). The type of the robot is essentially the algorithm it uses to compute the optimal plan given the initial state and the selected goal, and thus affects the cost of achieving the goal, and hence the utility function. - Utilities - The utilities are defined as R i  UH (AH i , Aj , B ) = R(GH ) - C ( (H )|B ) j H R  UR (Ai , Aj , B ) = R(GR ) - C ( (R)|B ) where symbols have their usual meaning. As before, the Nash equilibriums in GAPI-Bayesian are given by action profiles R AH such that the human has no reason i , Aj H R to defect, i.e.  B B UH (Ai , Aj , B )P (B ) H R U ( A , A , B ) P ( B ) while the robot also has H j k : k=i B B R no incentive to change, i.e. B B UR (AH i , Aj , B )P (B )  H R UH (Ai , Ak : k=j , B )P (B ), given the distribution P over the beliefs B of robot type. Similarly, the socially optiR mal solution is given by the action profiles AH i , Aj    H R where {i , j } = arg maxi,j B B [UH (Ai , Aj , B ) + R UR (AH i , Aj , B )]P (B ).

Evolving Utilities. Often, and certainly in the examples provided in Section 4.1, the behavior of the robot depends on understanding the intent(s) of its human counterpart. Thus the utilities will keep evolving based on the actions of the human after the goal has been selected. This is even more relevant in scenarios where communication is severely limited, when the agents in a coalition are not aware of the exact goals that the other agents have selected. Evolving Beliefs. Intent recognition has a direct effect on the belief over the robot type itself. For example, as the human observes the actions of the robot, it can infer which behavior the robot is going to exhibit. Thus intent recognition over the robot's actions will result in evolving belief of the human, as opposed to intent recognition over the human's activities which informed the planning process and hence the utilities of the robot.

5.3

Implications of Implicit Preferences

Finally, as agents interact with each other over time, in different capacities as teammates and colleagues, their expectations over which agent is likely to form which form of coalition will also evolve. This will give the prior belief over the robot type that the human starts with, and will get updated as further interactions occur.

6

Conclusions

In conclusion, we introduced a two-player static game that can be used to form optimal coalitions on the go among two autonomous members of a human-robot society, with minimum prior coordination. We also looked at several properties of such games that may be used to make the problem tractable while still maintaining key properties of the game. Finally, we explored an extension of the game to a general Bayesian formulation when the human is not sure of the intent of the robot, and motivated the implications and expressiveness of this model. We believe the work will stimulate discussion on ad-hoc interaction among agents in the context of human-robot cohabitation settings and provide insight towards generating efficient synergy.

5

Discussions and Future Work

The concept of Bayesian games lends GAPI to several interesting possibilities, and promising directions for future work, with respect to how interactions evolve with time.

Acknowledgments
This research is supported in part by the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023. I would also like to give special thanks to Prof. Guoliang Xue (with the Department of Computer Science at Arizona State University) for his valuable support and inputs.

5.1

Unrolling the Entire Game

Notice that we formulated the game such that each of the agents  has a set of goals G  to achieve. Thus GAPI immediately lends itself to a finite horizon dynamic game unrolled max |G  | times, so that the agents can figure out their most effective long-term strategy and coalitions. Finding optimal policies in such cases will involve devising more

References
Ackerman, M., and Br^ anzei, S. 2014. The authorship dilemma: Alphabetical or contribution? In Proceedings of

551

the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '14, 1487≠1488. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems. Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and Kambhampati, S. 2015a. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Chakraborti, T.; Zhang, Y.; Smith, D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot co-habitation. In ICAPS Workshop on Planning and Robotics. Chakraborti, T.; Talamadupula, K.; Zhang, Y.; and Kambhampati, S. 2016. Interaction in human-robot societies. In AAAI Workshop on Symbiotic Cognitive Systems. Mcdermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR98-003, Yale Center for Computational Vision and Control,. Ray, D., and Vohra, R. 2014. Handbook of Game Theory. Handbooks in economics. Elsevier Science. Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI'92, 276≠281. Talamadupula, K.; Briggs, G.; Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014. Coordination in human-robot teams using mental modeling and plan recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2957≠2962. Tambe, M. 1997. Towards flexible teamwork. J. Artif. Int. Res. 7(1):83≠124. Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in multi-agent planning. In ICAPS Workshop on Distributed Multi-Agent Planning (DMAP). Zick, Y., and Elkind, E. 2014. Arbitration and stability in cooperative games. SIGecom Exch. 12(2):36≠41. Zick, Y.; Chalkiadakis, G.; and Elkind, E. 2012. Overlapping coalition formation games: Charting the tractability frontier. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS '12, 787≠794. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems.

552

Tweeting the Mind and Instagramming the Heart:
Exploring Differentiated Content Sharing on Social Media
Lydia Manikonda

Venkata Vamsikrishna Meduri

Subbarao Kambhampati

arXiv:1603.02718v1 [cs.SI] 8 Mar 2016

Department of Computer Science, Arizona State University, Tempe AZ 85281
{lmanikon, vmeduri, rao}@asu.edu

Abstract
Understanding the usage of multiple OSNs (Online Social
Networks) has been of significant research interest as it helps
in identifying the unique and distinguishing trait in each social media platform that contributes to its continued existence. The comparison between the OSNs is insightful when
it is done based on the representative majority of the users
holding active accounts on all the platforms. In this research,
we collected a set of user profiles holding accounts on both
Twitter and Instagram, these platforms being of prominence
among a majority of users. An extensive textual and visual
analysis on the media content posted by these users revealed
that both these platforms are indeed perceived differently at a
fundamental level with Instagram engaging more of the users‚Äô
heart and Twitter capturing more of their mind. These differences got reflected in almost every microscopic analysis done
upon the linguistic, topical and visual aspects.

1

Introduction

Online Social Networks (OSNs) are gaining attention for
being rich sources of information about individuals including many aspects of their daily life through the way they
connect, communicate and share information. Over the past
few years, given their ubiquity and accessibility, social media platforms like Twitter and Instagram have emerged as
very popular microblogging services for web users to communicate with each other through text and photos. In 2015,
when Instagram broke the record of having more than 400
million active monthly users, Twitter was projected as its
main rival. In fact according to a recent article by Pew Research,1 28% of American adults use Instagram and 23%
use Twitter. More interestingly, many users have active accounts on both these sites (or platforms) (Lim et al. 2015;
Chen et al. 2014). While research has recognized immense
practical value in understanding the user behavioral characteristics on these platforms separately, there is no existing research that has examined how the content posted by
individuals differs across these two platforms. Instagram is
a photo-sharing application whereas Twitter emerged as a
text-based application which currently lets users post both
text and multimedia data. Of particular interest is the question of why and how individuals use these two sites when
both of them are similar in their current functionalities.
1
http://www.pewinternet.org/2015/08/19/the-demographics-ofsocial-media-users/

In this research, we aim to answer the aforementioned
questions by analyzing content from the same set of individuals across these two popular platforms and quantifying
their posting patterns (we focus on ordinary users who are
common users but not celebrities or popular users or organizations). By leveraging NLP and Computer Vision techniques, we present some of the first qualitative insights about
the types of trending topics, and social engagement of the
user posts across these two platforms. Analysis on the visual
and linguistic cues indicates the dominance of personal and
social aspects on Instagram and news, opinions and workrelated aspects on Twitter. Despite considering the same set
of users on both platforms, we see remarkably different categories of visual content ‚Äì predominantly eight categories on
Instagram and four categories of images on Twitter. These
results suggest that Instagram is largely a sphere of positive
personal and social information where as Twitter is primarily
a news sharing media with higher negative emotions shared
by users.
Background: Twitter has been explored extensively with
respect to the content (Honey and Herring 2009), language (Hong, Convertino, and Chi 2011), etc and it is established that it is primarily a news medium (Kwak et
al. 2010). Research on Instagram has focused mostly on
understanding the user behavior through analyzing color
palettes (Hochman and Schwartz 2012), categories (Hu,
Manikonda, and Kambhampati 2014), filters (Bakhshi et al.
2015), etc. On the other hand, it has been of significant
interest to the researchers to investigate the behavior of a
user (Benevenuto et al. 2009), connect users (Zafarani and
Liu 2013), study how users reveal their personal information (Chen et al. 2012), etc all across multiple OSNs. We
extend the current state of the art by examining the nature of
a given user‚Äôs behavior manifested across Twitter and Instagram. Close to our work is the work of Bang et al. (Lim et al.
2015) where six OSNs were studied to analyze the temporal
and topical signature (only w.r.t user‚Äôs profession) of user‚Äôs
sharing behavior but they did not focus on studying the comparative linguistic aspects and visual cues across the platforms. Here we employ both textual and visual techniques
to conduct a deeper analysis of content on both Twitter and
Instagram.
Dataset: In order to investigate and characterize a given
user‚Äôs behavior across multiple sites, we use a personal web
hosting service called About.me (http://about.me/) that enables individuals to create an online identity by letting them

Twitter
stories, international, food, web, naÃÉo,
angelo, jaÃÅ
time, people, love, work, world, social,
life

ID
0

happy, love, home, birthday, weekend,
beautiful, park
maÃÅs, dƒ±ÃÅa, vƒ±ÃÅa, gracias, mi, si, las

2

#football, #sports, #news, #art, facebook, google, iphone

4

1

3

Instagram
#food, delicious, coffee, sunset, beautiful, happy, #wedding
#streetart, #brightongraffiti, #belize,
#sussex,
#hipstamatic,
#urbanart,
#lawton
#fashion, #hair, #makeup, #health,
#workout, #vegan, #fit
#instagood,
#photooftheday,
#menswear, #style, #travel, #beach,
#summer
birthday, beautiful, love, christmas,
friends, fun, home

Table 1: Words corresponding to the 5 latent topics from Twitter
and Instagram

provide a brief biography, connections to other individuals
and their personal websites. Using its API, we performed
the data collection of 10,000 users and pruned the individuals who do not have profiles on both Instagram and Twitter.
The final crawl includes 1,035,840 posts from Twitter (using
the Twitter API https://dev.twitter.com/overview/
api) and 327,507 posts from Instagram (using the Instagram
API https://www.instagram.com/developer/) for the
same set of users. Each post in this dataset is public and
the data include user profiles along with their followers and
friends list, tweets (insta posts), meta data for tweets that include favorites (likes), retweets (Instagram has no explicit
reshares; so we use comments in lieu of the attention the
post receives), geo-location tagged, date posted, media content attached and hashtags.

2
2.1

Text Analysis

Latent Topic Analysis

In order to explain the types of content posted by a user
across Twitter and Instagram, we first mine the latent topics from the corpus of Twitter (aggregated posts on Twitter of all users) and corpus of Instagram (aggregated posts
on Instagram of all users where we use captions associated
with posts for this analysis). We use TwitterLDA (https://
github.com/minghui/Twitter-LDA) developed for topic
modeling of short text corpora to mine the latent topics. With
the user accounts obtained from About.me, the topic inference is meaningful as it is pertinent to the bi-platform posts
from users who use both the social media venues.
The topic vocabulary listed for both the platforms in Table 1 indicates the unique topics for each site as well as
the overlapping topics. For instance topics 0 and 4 on Instagram are similar to the topics 1 and 2 on Twitter. However,
a significant difference is that Instagram is predominantly
used to post about art, food, fitness, fashion, travel, friends
and family but Twitter hosts a significantly higher percentage of posts on sports, news and business as compared to
other topics. Another notable difference is that the vocabulary from non-English language posts like French and Spanish is higher on Twitter as compared to the captions on Instagram mostly using English as the language medium. The
topic distributions obtained from the two corpora are listed
in Figure 1 which show that friends and food are the most
frequently posted topics on Instagram as against sports and
news followed by work and social life being popular on
Twitter.
To further validate the observations made about the distinctive topical content across the two platforms, we compared the topic distributions for each individual on the two
platforms by estimating the KL-Divergence (entropy) for

Figure 1: Topic distributions of all the user posts on Twitter and
Instagram

Figure 2: Sorted entropies between the topic distributions of the
user posts on Twitter and Instagram

each user. However for this entropy computation to be possible, a unified topic model needed to be built on the combined corpus of tweets and captions of Instagram posts. The
unified topics are listed in the description of Figure 3. The
resultant entropy plot in Figure 2 follows a power law distribution showing that most users post on Twitter and Instagram equally differently barring a few (where the estimated
p-value < 10‚àí15 for each user).

2.2

Social Engagement

Since our findings revealed that the bi-platform topics are
significantly different and so we wanted to investigate how
these posts made by the same user engage other individuals on the two sites. We define the social engagement as the
attention received by a user‚Äôs post on the social media platform and can be quantified in various ways ranging from the
sum of likes and comments on Instagram and the sum of favorites and reshares on Twitter. For each topic in the unified
topic model for both Twitter and Instagram, the logarithmic
frequency of posts is plotted against the magnitude of social
engagement that is binned to discrete ranges in Figure 3.
An interesting observation is that the socially engaging
topics in the combined model are same as the overlapping
topics from the topic models built in isolation on the Twitter
and Instagram posts (Figure 1 in Section 2.1). The dominating topic on Twitter is about sports, news and business but
the overlapping influential topic is about social and personal
life comprising friends and family. Surprisingly, we found
that the overlapping topics (Topics 2 and 3) fetched predominant social engagement on both Twitter and Instagram.
A notable difference between the platforms with respect
to social engagement is that the magnitude of attention received for Instagram posts is significantly higher than the
level of attention received on Twitter as we can notice from
the ranges plotted on the x-axes in Figure 3. This observation
is consistent regardless of the activity of the user. Even when
a user is more active (Figure 4) on Twitter than Instagram,
the observation of higher social engagement on Instagram
on an absolute scale holds. A possible explanation to this is

Platform
Twitter

Instagram

0.60
0.19

0.49
0.19

0.15
0.14
0.05
0.17

0.30
0.21
0.1
0.21

0.81
0.6
0.08
0.07
0.16

0.5
0.93
0.06
0.04
0.2

Emotionality
Negemo
Posemo
Social Relationships
home
family
friend
humans

(a) Twitter

Individual Differences
work
bio
swear
death
gender

(b) Instagram
Figure 3: Social Engagement Vs Post Frequency where the topics are ‚Äì Topic 0:{people, life, world, social, app, game, business}, Topic 1:{stories, artists, #lastfm, level, #football, #sports,
news}, Topic 2:{birthday, beautiful, work, weekend, park, dinner,
christmas}, Topic 3:{ yang, run, #fitness, #runkeeper, #art, sale,
#menswear}, Topic 4:{#instagood, #photooftheday, #love, maÃÅs,
#fashion, #travel, #food}

Figure 4: Distributions of Followers/Followings vs Media

that the users on Twitter use it as a news source to read informative tweets but not necessarily all of the content that is
read will be ‚Äúliked‚Äù.
On average, there are 30% more number of hashtags for a
Twitter post compared to an Instagram post (Pearson correlation coefficient = 0.34 between distributions with p-value
< 10‚àí15 ). This may also indicate that on Instagram since the
main content is image, textual caption may not receive as
much attention from the user.

2.3

Linguistic Nature

To characterize and compare the type of language used on
both platforms, we use the psycholinguistic lexicon LIWC
((http://liwc.wpengine.com/)) on the text associated
with a Twitter post and an Instagram post. We obtain measures of attributes related to user behavior ‚Äì emotionality
(how people are reacting to different events), social relationships (friends, family, other humans) and individual differences (attributes like bio, gender, age, etc).
It is clear from Table 2 that posts on Twitter are more negatively emotional and contain more work-related and swear
words where as positive social patterns are more evident
on Instagram. By relating these results to the topic analysis results in the previous section, we identify that on Instagram users share less negative content and more lighthearted happy personal updates. To further support these
claims from the textual data, n-gram analysis indicates that

Table 2: Linguistic attributes across Twitter vs Instagram. Each
value indicates the fraction of a post belonging to the corresponding
attribute

users on Instagram focus on things that give them pleasure such as, fashion or travel (top bi/Trigrams like last
night, Good morning, right now, fashion design streetwear),
whereas on Twitter they mainly share check-in feeds from
their apps or news (top bi/Trigrams like Stories via, Just
posted, @YouTube video, Just posted photo).

3

Visual Analysis

Extensive studies have been conducted on the textual and
visual content on these two platforms in isolation but to the
best of our knowledge, a comparative content analysis has
never been conducted. Considering this as a main objective,
this section develops a better understanding on the types of
photos individuals post on Twitter in comparison with their
Instagram posts. To achieve this we employ Computer Vision techniques mainly in terms of ‚Äì visual categories (kinds
of photos) and visual features (color palettes).

3.1

Visual Categories

We first sampled two sets of 5K images from both platforms
separately and using the OpenCV library (http://opencv.
org/) on these two datasets, we extracted Speeded Up Robust Features (SURF) for each image. We used the vector
quantization approach on these features that eventually converted each image into a codebook format. Using the codebook, we clustered images using k-means algorithm (best
value of k is found by SSE (Sum of Squared Error)). We
employed the same approach on both datasets separately. To
our surprise the clusters we obtained on Instagram were very
refined compared to the coarse nature of clusters on Twitter
dataset. After computing these clusters, two researchers separately identified the overall themes of these two datasets
and agreed upon the final visual categories of the photos
from these platforms.
Visual categories on Instagram agree with our previous
work (Hu, Manikonda, and Kambhampati 2014) which detected eight different categories of images. We tried to categorize the Twitter images into the same format as Instagram
images and there are four prominent cluster categories on
Twitter. Figure 7 shows that the percentage of photos in the
activity category outnumbered any other category followed
by captioned photos. To better understand the kinds of activities and captions shown in these two sections, we sampled
around 200 images and asked the two researchers to label

(a)
(b)
(c)
(d)
Figure 5: Subcategories of activity: a) TV shows, b) Running, c)
Conferences, d) Live shows
Figure 7: Photo categories on Twitter vs Instagram
(a)
(b)
(c)
Figure 6: Subcategories of captioned photos: a) Snapshots, b)
Memes, c) Quotes

them manually into different sub-categories. Figure 5 indicates the most popular sub-categories in the activity category
‚Äì news, events (football games, concerts, conferences) and
races and Figure 6 indicates that majority of the captioned
photos are snapshots, memes, and quotes or opinions. These
categories suggest that the topics of photos on Twitter are
mainly related to news, opinions or other general user interests where as on Instagram they mainly share their joyful
and happy moments of their personal lives.

3.2

Visual Features

Existing literature (Bakhshi et al. 2015) shows that the images with a single dominant color gain more popularity
on Instagram. To verify this we compared the visual luminance of all images on these two platforms. We extracted
the grayscale histograms (range from 0 to 259) by utilizing
the OpenCV library as they capture the information about
the brightness, saturation and contrast distribution. The images with darker pixels were binned into the low intensity
value bins close to 0 and images with brighter pixels were
binned into the high intensity values close to 259. Later we
clustered the images in our dataset by employing k-means
algorithm with the grayscale histograms as features for each
image on both these platforms following which 4 types of
clusters were detected based on their color distributions. We
measured the image distribution across each of these 4 categories for both the platforms. We noticed that the images
on Instagram containing darker and brighter pixels are negligible when compared to Twitter as shown in Figure 8. This
suggest that Twitter posts may be less socially engaging than
Instagram owing to a huge presence of captioned photos on
Twitter.

4

Conclusions

In this paper, we presented a detailed comparison of the textual and visual analysis of the content posted by the same set
of users on both Twitter and Instagram. Some of the insights
obtained from linguistic analysis reveal the fundamental differences in the thinking style and emotionality of the users
on these two platforms and how the posts receive varying degrees of attention as per the underlying topics. Interestingly,
user posts on Instagram seem to receive significantly more
attention than Twitter. The visual analyses with respect to
categories and color palettes indicate that the pictures posted
on Instagram contains more selfies and photos with friends
where as Twitter contains more about user opinions in the
form of captioned photos ‚Äì memes, quotes, etc. We observed
that the differences are deeply rooted in the very intention

Figure 8: Example images corresponding to the four major color
categories obtained by extracting color histograms of images associated with the Twitter and Instagram posts.

with which users post on these platforms with Twitter being
a venue for serious posts about news, opinions and business
life where as Instagram acting as the host for light-hearted
personal moments and posts on leisure activities.

References
[Bakhshi et al. 2015] Bakhshi, S.; Shamma, D.; Kennedy, L.; and
Gilbert, E. 2015. Why we filter our photos and how it impacts
engagement. In Proc. ICWSM.
[Benevenuto et al. 2009] Benevenuto, F.; Rodrigues, T.; Cha, M.;
and Almeida, V. 2009. Characterizing user behavior in online
social networks. In Proc. IMC.
[Chen et al. 2012] Chen, T.; Kaafar, M. A.; Friedman, A.; and
Boreli, R. 2012. Is more always merrier?: A deep dive into online social footprints. In Proc. WOSN.
[Chen et al. 2014] Chen, Y.; Zhuang, C.; Cao, Q.; and Hui, P. 2014.
Understanding cross-site linking in online social networks. In Proc.
SNAKDD.
[Hochman and Schwartz 2012] Hochman, N., and Schwartz, R.
2012. Visualizing instagram: Tracing cultural visual rhythms. In
Proc. ICWSM.
[Honey and Herring 2009] Honey, C., and Herring, S. 2009. Beyond microblogging: Conversation and collaboration via twitter. In
Proc. HICSS, 1‚Äì10.
[Hong, Convertino, and Chi 2011] Hong, L.; Convertino, G.; and
Chi, E. 2011. Language matters in twitter: A large scale study.
In Proc. ICWSM.
[Hu, Manikonda, and Kambhampati 2014] Hu, Y.; Manikonda, L.;
and Kambhampati, S. 2014. What we instagram: A first analysis
of instagram photo content and user types. In Proc. ICWSM.
[Kwak et al. 2010] Kwak, H.; Lee, C.; Park, H.; and Moon, S. 2010.
What is twitter, a social network or a news media? In Proc. WWW,
591‚Äì600.
[Lim et al. 2015] Lim, B. H.; Lu, D.; Chen, T.; and Kan, M.-Y.
2015. #mytweet via instagram: Exploring user behaviour across
multiple social networks. In Proc. ASONAM.
[Zafarani and Liu 2013] Zafarani, R., and Liu, H. 2013. Connecting
users across social media sites: A behavioral-modeling approach.
In Proc. KDD.

A Formal Framework for Studying Interaction in Human-Robot Societies
Tathagata Chakraborti1

Kartik Talamadupula2

Yu Zhang1

Subbarao Kambhampati1

Department of Computer Science1

Cognitive Learning Department2

Arizona State University
Tempe, AZ 85281, USA

IBM Thomas J. Watson Research Center
Yorktown Heights, NY 10598, USA

{tchakra2, yzhan442, rao}@asu.edu

krtalamad@us.ibm.com

Abstract
As robots evolve into an integral part of the human
ecosystem, humans and robots will be involved in a
multitude of collaborative tasks that require complex
coordination and cooperation. Indeed there has been
extensive work in the robotics, planning as well as
the human-robot interaction communities to understand
and facilitate such seamless teaming. However, it has
been argued that their increased participation as independent autonomous agents in hitherto human-habited
environments has introduced many new challenges to
the view of traditional human-robot teaming. When
robots are deployed with independent and often selfsufficient tasks in a shared workspace, teams are often
not formed explicitly and multiple teams cohabiting an
environment interact more like colleagues rather than
teammates. In this paper, we formalize these differences
and analyze metrics to characterize autonomous behavior in such human-robot cohabitation settings.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made
the dream of human-robot cohabitation a not so distant reality. We are now witnessing the development of autonomous
agents that are especially designed to operate in predominantly human-inhabited environments often with completely
independent tasks and goals. Examples of such agents include robotic security guards like Knightscope, virtual presence platforms like Double and iRobot Ava, and even autonomous assistance in hospitals such as Aethon TUG. Of
particular fame are the CoBots (Rosenthal, Biswas, and
Veloso 2010) that can ask for help from unknown humans,
and thus interact with agents not directly involved in its plan.
Indeed there has been a lot of work recently in the context
of ‚Äúhuman-aware‚Äù planning, both from a point of view of
path planning (Sisbot et al. 2007; Kuderer et al. 2012) and
task planning (Koeckemann, Pecora, and Karlsson 2014;
Cirillo, Karlsson, and Saffiotti 2010), with the intention of
making the robot‚Äôs plans socially acceptable, e.g. resolving conflicts with the plans of fellow humans. Even though
all of these scenarios involve significantly different levels
of autonomy from the robotic agent, the underlying theme
of autonomy in such settings involves the robot achieving
some sense of independence of purpose in so much as its

existence is not just defined by the goals of the humans
around it but is rather contingent on tasks it is supposed to
be achieving on its own. Thus the robots in a way become
colleagues rather than teammates. This becomes even more
prominent when we consider interactions between multiple independent teams in a human-robot cohabited environment. We thus postulate that the notions of coordination
and cooperation between the humans and their robotic colleagues is inherently different from those investigated in existing literature on interaction in human-robot teams, and
should rather reflect the kind of interaction we have come
to expect from human colleagues themselves. Indeed recent
work (Chakraborti et al. 2015a; Chakraborti et al. 2015b;
Talamadupula et al. 2014) hints at these distinctions, but has
neither made any attempt at formalizing these ideas, nor provided methods to quantify behavior is such settings. To this
end, we propose a formal framework for studying inter-team
and intra-team interactions in human-robot societies, show
how existing metrics are grounded in this framework and
propose newer metrics that are useful for evaluating performance of autonomous agents in such environments.

1

Human Robot Cohabitation

At some abstracted level, agents in any environment can be
seen as part of a team achieving a high level goal. Consider, for example, your university or organization. At a micro level, it consists of many individual labs or groups that
work independently on their specific tasks. But when taken
as a whole, the entire institute is a team trying to achieve
some higher order tasks like increasing its relative standing
among its peers or competitors. So in the discussion that follows, we talk about environments, and teams or colleagues
acting within it, in the context of the goals they achieve.

1.1

Goal-oriented Environments

Definition 1.0 A goal-oriented environment is defined as
a tuple E = hF, O, Œ¶, G, Œõi, where F is a set of first order
predicates that describes the environment, and O is a set
of objects in the environment, Œ¶ ‚äÜ O is the set of agents,
G = {g | g ‚äÜ FO } is the set of goals that these agents
are tasked with, and Œõ ‚äÜ O is the set of resources that are
required by the agents to achieve their goals. Each goal has

a reward R(g) ‚àà R+ associated with it.1
These agents and goals are, of course, related to each
other by their tasks, and these relationships determine the
nature of their interactions in the environment, i.e. in the
form of teams or colleagues. Before we formalize such relations, however, we would look at the way the agent models
are defined. We use PDDL (Mcdermott et al. 1998) models
for the rest of the discussion, as described below, but most of
the discussion easily generalizes to other modes of representation. The domain model DœÜ of an agent œÜ ‚àà Œ¶ is defined
as DœÜ = hFO , AœÜ i, where AœÜ is a set of operators available
to the agent. The action models a ‚àà AœÜ are represented as
a = hCa , Pa , Ea i where Ca is the cost of the action, Pa ‚äÜ
FO is the list of pre-conditions that must hold for the action
a to be applicable in a particular state S ‚äÜ FO of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO
is a tuple that contains the add and delete effects of applying
the action to a state. The transition function Œ¥(¬∑) determines
the next state after the application of action a in state S as
Œ¥(a, S) = (S\ef f ‚àí (a))‚à™ef f + (a) if Pa ‚äÜ S; ‚ä• otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†Œ± = hF, O, DœÜ , IœÜ , GœÜ i, where IœÜ ‚äÜ FO is the initial state
of the world and GœÜ ‚äÜ FO is the goal state. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that
Œ¥(œÄœÜ , IœÜ ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . .P
, a|œÄ| i, Œ¥(a1 , s)). The cost
of the plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca .
We will now introduce the concept of a super-agent
transformation on a set of agents that combines the capabilities of one or more agents to perform complex tasks
that a single agent might not be able to do. This will help
us later to formalize the nature of interactions among agents.
Definition 1.1a A super-agent is a tuple Œò = hŒ∏, DŒ∏ i
where Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏
is the transformation from the individual domainSmodels to
a composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Note that this does not preclude joint actions among
agents, because some actions that need that need more than
one agent (as required in the preconditions) will only be
doable in the composite domain.
Definition 1.1b The planning problem of a super-agent Œò
is similarly given by Œ†Œò = hF, O, DŒ∏ , IŒ∏ , GŒ∏ i where
S the
composite initial and goal states are given by IŒ∏ = œÜ‚ààŒ∏ IœÜ
S
and GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i
where ¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |=SGŒ∏ , where the modified
transition function
S
Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). We denote
the set of all such plans as œÄŒò .
P
P
The cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca
and œÄŒ∏‚àó is optimal if Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ =‚áí C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ ).
The composite plan can thus be viewed as a union of plans
contributed by each agent œÜ ‚àà Œ∏ so that œÜ‚Äôs component can
be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà
1

SO is S ‚äÜ F instantiated or grounded with objects from O.

œÄŒò . Now we will define the relations among the components
of the environment E in terms of these agent models.
Definition 1.2 At any given state S ‚äÜ FO of the
environment E, a goal-agent correspondence is defined as the relation œÑ : G ‚Üí P(Œ¶); G, Œ¶ ‚àà E, that
induces a set of super-agents œÑ (g) = {Œò | Œ†Œò =
hF, O, DŒ∏ , S, gi has a solution, i.e. ‚àÉœÄ s.t. Œ¥(œÄ, S) |= g}.
In other words, œÑ (g) gives a list of sets of agents in the environment that are capable of performing a specific task g.
We will see in the next section how the notions of teammates
and colleagues are derived from it.

1.2

Teams and Colleagues

Definition 2.0 A team Tg w.r.t. a goal g ‚àà G is defined as
any super-agent Œò = hŒ∏, DŒ∏ i ‚àà œÑ (g) iff 6 ‚àÉœÜ ‚àà Œ∏ such that
Œò0 = hŒ∏ \ œÜ, DŒ∏\œÜ i and œÄŒò = œÄŒò0 .
This means that any super-agent belonging to a particular
goal-agent correspondence defines a team w.r.t that specific
goal when every agent that forms the super-agent plays
some part in the plans that achieves the task described
by g, i.e. the super-agent cannot use the same plans to
achieve g if an agent is removed from its composition.
This, then, leads to the concept of strong, weak, or optimal teams, depending on if the composition of the
super-agent is necessary, sufficient or optimal respectively
(note that an optimal team may or may not be a strong team).
Definition 2.0a A team Tgs = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is strong iff 6 ‚àÉœÜ ‚àà Œ∏ such that hŒ∏ \ œÜ, DŒ∏\œÜ i ‚àà œÑ (g).
A team Tgw is weak otherwise.
Definition 2.0b A team Tgo = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is optimal iff ‚àÄŒò0 ‚àà œÑ (g), C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏‚àó0 ).
This has close ties with concepts of required cooperation and capabilities of teams to solve general planning
problems, introduced in (Zhang and Kambhampati
2014), and work on team formation mechanisms and
properties of teams (Shoham and Tennenholtz 1992;
Tambe 1997). In this paper, we are more concerned about
the consequences of such team formations on teaming metrics. So, with these different types of teams we have seen
thus far, the question we ask is: What is the relation among
the rest of the agents in the environment? How do these
different teams interact among and between themselves?
Definition 2.1 The set of teams in E are defined by the
relation Œ∫ : G ‚Üí R(œÑ ); G ‚àà E, where Œ∫(g) ‚àà œÑ (g) denotes
the team assigned to the goal g, ‚àÄg ‚àà G.
This, then, gives rise to the idea of collegiality among
agents, due to both inter-team and intra-team interactions.
Note that how useful or necessary such interactions are
will depend on whether the colleagues can contribute to
each other‚Äôs goals, or to what extent they influence their
respective plans, which leads us to the following two
definitions of colleagues based on the concept of teams.
Definition 2.2a Let Œ∫(g) = hŒ∏1 , DŒ∏1 i, Œ∫(g 0 ) = hŒ∏2 , DŒ∏2 i
be two teams in E. An agent œÜ1 ‚àà Œ∏1 is a type-1 colleague
to an agent œÜ2 ‚àà Œ∏2 when Œ∫0 (g) = hŒ∏1 ‚à™ œÜ1 , DŒ∏1 ‚à™œÜ1 i is a
weak team w.r.t. the goal g.

Definition 2.2b Agents œÜ1 , œÜ2 ‚àà Œ¶ are type-2 colleagues
when ‚àÄŒ∫(g) = hŒ∏, DŒ∏ i s.t. {œÜ1 , œÜ2 } ‚à© Œ∏ 6= ‚àÖ, {œÜ1 , œÜ2 } 6‚àà
Œ∏ ‚àß Œ∫0 (g) = hŒ∏ ‚à™ {œÜ1 , œÜ2 }, DŒ∏‚à™{œÜ1 ,œÜ2 } i is a weak team.
Thus type-1 colleagues can potentially contribute to the
plans of their colleagues, while type-2 colleagues cannot.
Plans of type-2 colleagues can, however, influence each
other (for example due to conflicts on usage of shared resources), while type1-colleagues are capable of becoming
teammates dynamically during plan execution.
Humans in the loop. Instead of a general set of agents, we
define the set of agents Œ∏ in a super-agent as composition of
humans and robots Œ∏ = h(Œ∏)‚à™r(Œ∏) so that the domain model
of the super-agent is alsoS
a composition
of the human and
S
robot capabilities DŒ∏ = œÜ‚ààh(Œ∏) œÜ‚ààr(Œ∏) AœÜ = h(DŒ∏ ) ‚à™
r(DŒ∏ ). We denote the communication actions of the superagent as the subset c(DŒ∏ ) ‚äÜ DŒ∏ .

2
2.1

Metrics for Human Robot Interaction
Metrics for Human Robot Teams

We will now ground popular (Olsen Jr. and Goodrich 2003;
Steinfeld et al. 2006; Hoffman and Breazeal 2007;
Hoffman 2013) metrics for human-robot teams in our
current formulation.
Task Effectiveness These are the metrics that measure
the effectiveness of a team in completing its tasks.
‚Ä¢ Cost-based
Metrics - This simply measures the cost
P
‚àó
g‚ààŒ∫‚àí1 (Œò) C(œÄŒò ) of all the (optimal) plans a specific
team executes (for all the goals it has been assigned to).
‚Ä¢ Net Benefit Based Metrics - This is based on both
plan costs as well as the value of goals and is given by
P
‚àó
g‚ààŒ∫‚àí1 (Œò) R(g) ‚àí C(œÄŒò ).
‚Ä¢ Coverage Metrics - Coverage metrics for a particular
team determine the diversity of its capabilities in terms of
the number of goals it can achieve |Œ∫‚àí1 (Œò)|.
Team Effectiveness These measure the effectiveness of
(particularly human-robot) teaming in terms of communication overhead and smoothness of coordination.
‚Ä¢ Neglect Tolerance - This measures how long the
robots in a team Œò is able to perform well without human intervention. WeT can measure this as
‚àó
N T = max{|i ‚àí j| s.t. h(DŒ∏ ) œÜ‚ààŒ∏ œÄŒò
(œÜ)[i : j] = ‚àÖ}.
‚Ä¢ P
Interaction Time - This is given by IT
=
‚àó
|{i | c(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|, and measures the
time spent by a team Œò in communication.
‚Ä¢ Robot Attention Demand - Measures how much attention the robot is demanding and is given by IT IT
+N T .
‚Ä¢ Secondary Task Time - This measures the ‚Äúdistraction‚Äù to a team, and can be expressed as
time not spent on achieving a given goal g, i.e.
‚àó
‚àó
ST T = |{i | œÄŒò
[i] ¬∑¬∑= ‚àÖ ‚àß Œ¥ 0 (s, œÄŒò
) |= g}|.
‚Ä¢ Free Time - F T = 1 ‚àí RAD is a measure of the fraction
of time the humans are not interacting with the robot.
‚Ä¢ Human Attention Demand - HAD = F T ‚àí h(ST T )
‚àó
‚àó
where h(ST T ) = |{i | œÄŒò
[i]‚à©h(DŒ∏ ) ¬∑¬∑= ‚àÖ‚àßŒ¥ 0 (s, œÄŒò
) |=
‚àó
g}|/|œÄŒò | is the time humans spend on the secondary task.

‚Ä¢ Fan Out - This is a measure of the communication load
on the humans, and consequently the number of robots
that should participate in a human-robot team, and is
proportional to F O ‚àù |h(Œ∏)|/RAD.
‚Ä¢ Interaction Time - Measures how quickly and effectively
T)
interaction takes places as IT = N T (1‚àíST
.
ST T
‚Ä¢ Robot Idle Time - Captures inconsistency or irregularity
in coordination from the point of view of the robotic
agent, and can be measured as the amount of time the
‚àó
robots are idle, i.e. RIT = |{i | r(DŒ∏ ) ‚à© œÄŒò
[i] = ‚àÖ|.
‚Ä¢ Concurrent Activity - We can talk of concurrency within
a team as the time that humans and robots are working
‚àó
concurrently CA1 = |{i | r(DŒ∏ ) ‚à© h(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|
and also across teams as the maximum time teams are
operating concurrently CA2 = max{|{i | œÄŒò [i] 6=
‚àÖ ‚àß œÄŒò0 [i] 6= ‚àÖ}| ‚àÄŒò, Œò0 ‚àà R(Œ∫)}.
In measuring performance of agents in cohabitation, both
as teammates and colleagues, we would still like to reduce
interactions times and attentions demand, while simultaneously increasing neglect tolerance and concurrency. However, as we will see in Section 3, these metrics do not effectively capture all the implications of the interactions desired
in human-robot cohabitation. So the purpose of the rest of
our paper is to establish metrics that can measure the effective behavior of human-robot colleagues, and to see to
what extent they can capture desired behaviors of robotic
colleagues suggested in existing literature.

2.2

Metrics for Human Robot Colleagues

We will now propose new metrics that are useful for
measuring collegial interactions, see how they differ from
teaming metrics discussed so far, and then relate them to
existing work on human-robot cohabitation.
Task Effectiveness The measures for task effectiveness
must take into account that agents are not necessarily
involved in their assigned team task only.
‚Ä¢ Altruism - This is a measure of how useful it is for
a robotic agent r to showcase altruistic behavior in
assisting their human colleagues, and is given by the ratio
of the gain in utility by adding a robotic colleague to a
team Œò to the decrease in utilityP
of plans of the teams r is
‚àó
‚àó
‚àó
involved in |œÄŒò
‚àíœÄhŒ∏‚à™r,D
|/
Œò=hŒ∏,DŒ∏ i s.t. r‚ààŒ∏ ‚àÜ|œÄŒò |.
Œ∏‚à™r i
For such a dynamic coalition to be useful, r must be a
type-1 colleague to the agents Œ∏ ‚àà Œò.
‚Ä¢ Lateral Coverage - This measures how deviating from
optimal team compositions can achieve global good in
terms
of number of goals achieved by a team, LT =
P
‚àí1
(Tg )| ‚àí |Œ∫‚àí1 (Tgo )|]/|Œ∫‚àí1 (Tgo )|}
Tg =Œ∫(g),‚àÄg‚ààG {[|Œ∫
across all the teams that have been formed in E.
‚Ä¢ Social Good - Many times, while planning with humans
in the loop, cost optimal plans are not necessarily the
optimal plans in the social context. This is useful to
measure particularly when agents are interacting outside
teams, and the compromise in team utility is compensated
by the gain in
This can be
Pmutual utility of colleagues.
‚àó
expressed as g‚ààG {C(œÄŒ∫(g) ) ‚àí C(œÄŒ∫(g)
)}.

Interaction Effectiveness The team effectiveness measures need to be augmented with measures corresponding
to interactions among non-team members. While all these
metrics are relevant for robotic colleagues as well, they
become particularly important in human-robot interactions,
where information is often not readily sharable due to
higher cognitive mismatch, so as to reduce cognitive
demand/overload.
‚Ä¢ Interaction Time - In addition to Interaction Time for
human-robot teams, and measures derived from it, we
propose two separate components of interaction time for
general human-robot cohabitation scenarios.
- External Interaction Time - This is the time spent by
agents interacting with type-1 colleagues (EIT1 ).
- Extraneous Interaction Time - This is the time spent
by agents interacting with type-2 colleagues (EIT2 ).
‚Ä¢ Compliance - This refers to how much actions of an agent
disambiguate its intentions. Though relevant for both, this
becomes even more important in absence of teams, when
information pertaining to goals or plans are not necessarily sharable. Thus the intention should be to maximize
the probability P (GŒ∏ = g | s = Œ¥(œÄŒ∏ [1 : i], IŒ∏ )), Œ∫(g) =
hŒ∏, DŒ∏ i, ‚àÄg ‚àà G given any stage i of plan execution and
P (¬∑) is a generic goal recognition algorithm. This can be
relevant both in terms of disambiguating goals (Keren,
Gal, and Karpas 2014) or explaining plans given a goal
(Zhang, Zhuo, and Kambhampati 2015).
‚Ä¢ External Failure - This is the number of times optimal
plans fail when resources are contested among colleagues.
‚Ä¢ Stability - Of course with continuous interactions, team
formations change, so this gives a measure of stability of
the system as a whole. If teams Œ∫(g) = hŒ∏1 , DŒ∏1 i and
Œ∫(g) = hŒ∏2 , DŒ∏2 i achieves a P
goal g ‚àà G at two different
instances, then stability S = g‚ààG |Œ∏1 ‚à© Œ∏2 |/|Œ∏1 ||Œ∏2 |.

3

Discussion and Related Work

We will now investigate the usefulness of the proposed metrics in quantifying behavioral traits proposed in existing literature as desirable among cohabiting human and robots.
Human-Aware Planning. In (Koeckemann, Pecora, and
Karlsson 2014; Cirillo, Karlsson, and Saffiotti 2010) the authors talk of adapting robot plans to suit social norms (e.g.
not to vacuum a room while a human is asleep). Clearly,
this involves the robots departing from their preferred plans
to conform to human preferences. In such cases, involving
assistive robots, measures of Altruism and Social Good become particularly relevant, while it is also crucial to reduce
unwanted interactions (EIT1 + EIT2 ).
Planning with Resource Conflicts. In (Chakraborti et al.
2015b) the authors outline an approach for robots sharing
resources with humans to compute plans that minimize conflicts in resource usage. Thus, this line of work is aimed at
reducing External Failures, while simultaneously increasing
Social Good. Measures of Stability and Compliance become
relevant, to capture evolving beliefs and their consequences
on plans. Extraneous Interaction Time is also an important

measure, since additional communication is always a proxy
to minimizing coordination problems between colleagues.
Planning for Serendipity. In (Chakraborti et al. 2015a)
the authors propose a formulation for the robot to produce
positive exogenous events during the execution of the human‚Äôs plans, i.e. interventions which will be useful to the human regardless of whether he was expecting assistance from
the robot. This work particularly looks at planning for Altruism. Increasing Compliance in agent behavior can provide
better performance in this regard. Further, External Interaction is crucial in such cases for forming such impromptu
coalitions among colleagues.
Relation to Metrics in Human Factor Studies It is useful
to see an example of how the general formulation of metrics
we discussed so far are actually grounded in human factors
studies (Zhang et al. 2015) of scenarios that display some aspects of collegial interaction. The environment studied was
a disaster response scenario, involving an autonomous robot
that may or may not chose to proactively help the human.
The authors used External Interaction Time or EIT1 to measure the effectiveness of proactive support (how often the
proactive support resulted in further deliberation over goals),
while Lateral Coverage (in terms of number of people rescued) showed the effectiveness of proactive support. Further,
qualitative analysis on acceptance and usefulness of agents
that display proactive support are closely related to measures
such as Social Good and Altruism.
Work on Ad-hoc Coalition Formations Given the framework we have discussed thus far, the question is then, apart
from measuring performance, how we can use it to facilitate collegial interactions among agents. Especially relevant
in such scenarios are work on ad-hoc coalition formation
among agents sharing an environment but not necessarily
goals (Stone et al. 2010). In (Chakraborti et al. 2016) we
show how this framework may be used to cut down on prior
coordination while forming coalitions.

4

Conclusion and Future Work

In conclusion, we discussed interaction in human-robot societies involving multiple teams of humans and robots in the
capacity of teammates or as colleagues, provided a formal
framework for talking about various modes of cooperation,
and reviewed existing metrics and proposed new ones that
can capture these different modalities of teaming or collegial behavior. Finally we discussed how such metrics can be
useful in evaluating existing works in human-robot cohabitation. One line of future inquiry would be to see how such
quantitative metrics are complemented by qualitative feedback from human factor studies, to establish what the desired trade-offs are, in order to ensure well-informed design
of symbiotic systems involving humans and robots.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
[Aethon TUG ] Aethon TUG. Intralogistics automation platform for hospitals. http://www.aethon.com/.
[Chakraborti et al. 2015a] Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and
Kambhampati, S. 2015a. Planning for serendipity. In International Conference on Intelligent Robots and Systems.
[Chakraborti et al. 2015b] Chakraborti, T.; Zhang, Y.; Smith,
D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot cohabitation. In ICAPS Workshop on Planning and Robotics.
[Chakraborti et al. 2016] Chakraborti, T.; Dondeti, V.;
Meduri, V. V.; and Kambhampati, S. 2016. A game theoretic approach to ad-hoc coalition formation in human-robot
societies. In AAAI Workshop on Multi-Agent Interaction
without Prior Coordination.
[Cirillo, Karlsson, and Saffiotti 2010] Cirillo, M.; Karlsson,
L.; and Saffiotti, A. 2010. Human-aware task planning:
An application to mobile robots. ACM Trans. Intell. Syst.
Technol. 1(2):15:1‚Äì15:26.
[Double ] Double. The ultimate tool for telecommuting.
http://www.doublerobotics.com/.
[Hoffman and Breazeal 2007] Hoffman, G., and Breazeal, C.
2007. Effects of anticipatory action on human-robot teamwork: Efficiency, fluency, and perception of team. In
Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on, 1‚Äì8.
[Hoffman 2013] Hoffman, G. 2013. Evaluating fluency in
human-robot collaboration. In Robotics: Science and Systems (RSS) Workshop on Human-Robot Collaboration.
[iRobot Ava ] iRobot Ava.
Video collaboration robot.
http://www.irobot.com/For-Business.aspx.
[Keren, Gal, and Karpas 2014] Keren, S.; Gal, A.; and
Karpas, E. 2014. Goal recognition design. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS 2014, Portsmouth,
New Hampshire, USA, June 21-26, 2014.
[Knightscope ] Knightscope. Autonomous data machines.
http://knightscope.com/about.html.
[Koeckemann, Pecora, and Karlsson 2014] Koeckemann,
U.; Pecora, F.; and Karlsson, L. 2014. Grandpa hates
robots - interaction constraints for planning in inhabited
environments. In Proc. AAAI-2010.
[Kuderer et al. 2012] Kuderer, M.; Kretzschmar, H.; Sprunk,
C.; and Burgard, W. 2012. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings
of Robotics: Science and Systems.
[Mcdermott et al. 1998] Mcdermott, D.; Ghallab, M.; Howe,
A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and
Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control,.
[Olsen Jr. and Goodrich 2003] Olsen Jr., D., and Goodrich,
M. A. 2003. Metrics for evaluating human-robot interactions. In Performance Metrics for Intelligent Systems.

[Rosenthal, Biswas, and Veloso 2010] Rosenthal,
S.;
Biswas, J.; and Veloso, M. 2010. An effective personal
mobile robot agent through symbiotic human-robot interaction. In Proceedings of the 9th International Conference
on Autonomous Agents and Multiagent Systems: Volume 1
- Volume 1, AAMAS ‚Äô10, 915‚Äì922. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.
[Shoham and Tennenholtz 1992] Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for
artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì
281. AAAI Press.
[Sisbot et al. 2007] Sisbot, E.; Marin-Urias, L.; Alami, R.;
and Simeon, T. 2007. A human aware mobile robot motion
planner. Robotics, IEEE Transactions on 23(5):874‚Äì883.
[Steinfeld et al. 2006] Steinfeld, A.; Fong, T.; Kaber, D.;
Lewis, M.; Scholtz, J.; Schultz, A.; and Goodrich, M. 2006.
Common metrics for human-robot interaction. In Proceedings of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, 33‚Äì40.
[Stone et al. 2010] Stone, P.; Kaminka, G. A.; Kraus, S.; and
Rosenschein, J. S. 2010. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. In Proceedings of
the Twenty-Fourth Conference on Artificial Intelligence.
[Talamadupula et al. 2014] Talamadupula, K.; Briggs, G.;
Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014.
Coordination in human-robot teams using mental modeling
and plan recognition. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2957‚Äì2962.
[Tambe 1997] Tambe, M. 1997. Towards flexible teamwork.
J. Artif. Int. Res. 7(1):83‚Äì124.
[Zhang and Kambhampati 2014] Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in
multi-agent planning. In ICAPS Workshop on Distributed
Multi-Agent Planning (DMAP).
[Zhang et al. 2015] Zhang, Y.; Narayanan, V.; Chakraborti,
T.; and Kambhampati, S. 2015. A human factors analysis
of proactive support in human-robot teaming. In IEEE/RSJ
International Conference on Intelligent Robots and Systems.
[Zhang, Zhuo, and Kambhampati 2015] Zhang, Y.; Zhuo,
H. H.; and Kambhampati, S. 2015. Plan explainability and
predictability for cobots. CoRR abs/1511.08158.

A Human Factors Analysis of Proactive Support in Human-robot Teaming
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborti and Subbarao Kambhampati
Abstract-- It has long been assumed that for effective humanrobot teaming, it is desirable for assistive robots to infer the goals and intents of the humans, and take proactive actions to help them achieve their goals. However, there has not been any systematic evaluation of the accuracy of this claim. On the face of it, there are several ways a proactive robot assistant can in fact reduce the effectiveness of teaming. For example, it can increase the cognitive load of the human teammate by performing actions that are unanticipated by the human. In such cases, even though the teaming performance could be improved, it is unclear whether humans are willing to adapt to robot actions or are able to adapt in a timely manner. Furthermore, misinterpretations and delays in goal and intent recognition due to partial observations and limited communication can also reduce the performance. In this paper, our aim is to perform an analysis of human factors on the effectiveness of such proactive support in human-robot teaming. We perform our evaluation in a simulated Urban Search and Rescue (USAR) task, in which the efficacy of teaming is not only dependent on individual performance but also on teammates' interactions with each other. In this task, the human teammate is remotely controlling a robot while working with an intelligent robot teammate `Mary'. Our main result shows that the subjects generally preferred Mary with the ability to provide proactive support (compared to Mary without this ability). Our results also show that human cognitive load was increased with a proactive assistant (albeit not significantly) even though the subjects appeared to interact with it less.

Fig. 1. Illustration of our USAR task in which the human teammate remotely controls a robot while working with an intelligent robot `Mary'. We intend to compare Mary with and without a proactive support ability.

I. I NTRODUCTION The efficacy of teaming [8] is not only dependent on individual performance, but also on teammates' interactions with each other. It has long been assumed that for effective human-robot teaming, it is desirable for assistive robots to infer the goals and intents of the humans, and take proactive actions to help them achieve their goals. For example, the ability of goal and intent recognition is considered to be required for an assistive robot to be socially acceptable [22], [5], [16], [2], [24]. This claim is also assumed in other human-robot teaming tasks, such as collaborative manufacturing [25] and urban search and rescue (USAR) [23]. However, there has not been any systematic evaluation of the accuracy of this claim.1
*This work was supported in part by the ARO grant W911NF-13-1- 0023, and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N0001415-1-2027. The authors would like to thank Nathaniel Mendoza for help with the simulator as well as the anonymous participants in the study. The authors are with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA

{yzhan442,vnaray15,tchakra2,rao}@asu.edu

1 The authors in [13] considered anticipatory action in interaction scenarios involving repetitive actions and the task settings are for human-robot teaming with proximal interactions.

There are several ways a proactive robot assistant can in fact reduce the effectiveness of teaming. For example, it can increase the cognitive load of the human teammate by performing actions that are unanticipated by the human. In such cases, even though the teaming performance could be improved, it is unclear whether humans are willing to adapt to robot actions or are able to adapt in a timely manner. Furthermore, misinterpretations and delays in goal and intent recognition due to partial observations and limited communication can also reduce performance. For example, consider a case in which you want to make an omelet and need eggs to be fetched from the fridge. Even if an assistive robot has started to fetch the eggs (after recognizing your intent), you may decide that the robot is too slow and fetch the eggs by yourself although you could improve the performance by letting the robot fetch the eggs while you preheat the pan. On the other hand, adapting to the robot's actions in such scenarios to improve teaming performance can increase the human cognitive load, which leads to unsatisfactory teaming experience. These conflicting factors make us investigate the utility of Proactive Support (PS) in human-robot teaming. In this paper, we start this investigation in a simulated USAR task with a general way to implement the proactive support ability on a robot in similar scenarios. Previous work [13] that investigates the effects of this ability is restricted to human-robot teaming with more proximal interactions. Meanwhile, to maintain the generality of this task, we only introduced a few necessary simplifications. In our task, the human teammate is remotely controlling a robot while working with an intelligent robot `Mary' (as shown in Fig. 1). The human-robot team is deployed during the early phase

of an emergency response where they need to search, rescue and provide preliminary treatment to casualties. This USAR scenario considers many of the complexities (e.g., partial observations) that often occur in real-world USAR tasks and we intend to learn whether these complexities influence the overall evaluation of the intelligent robot (i.e., Mary) with a proactive support (PS) ability, compared to Mary without this ability. We also aim to investigate the various trade-offs, e.g., mental workload and situation awareness through this human factors study. II. R ELATED W ORK There are many early works on goal and intent recognition (e.g., [15], [14], [4]). More recently, a technique to compile the problem of plan recognition into a classical planning problem is provided [20]. There is also a rich literature in the area of plan adaptation, which handles how robots plan under human-introduced constraints (e.g., social rules [24]). Using simple temporal networks (STNs), there has been development in efficient dispatchers that perform fast, online, least-commitment scheduling adaptation [6]. There are also a number of adaptation techniques that focus on integrated planning and execution [7], [21], [1]. There are existing systems that combine both goal and plan recognition and plan adaptation to achieve a proactive support ability on robots. In [13], [12], the authors propose a cost based anticipatory adaptive action selection mechanism for a robotic teammate to make decisions based on the confidence of the action's validity and relative risk. However, only repetitive tasks are considered and the task settings are for human-robot teaming with more proximal interactions compared to that in USAR scenarios. In [5], a humanaware planning paradigm is introduced where the robot only passively interacts with the human by avoiding conflicts with the recognized human plan. In USAR scenarios, it is also desirable for the robot to proactively provide support to the human. A recent paper proposes a planning for serendipity paradigm in which the authors investigate planning for stigmergic collaboration without explicit commitments [3]. In [17], the authors propose a unified approach to concurrent plan recognition and execution for human-robot teams, in which they represent alternative plans for both the human and robot, thus allowing recognition and adaptation to be performed concurrently and holistically. However, the limitation is that the plan choices must be specified a priori instead of dynamically constructed based on the current goal and intent of the human. This renders the approach impractical for realworld scenarios since even moderate number of choices (i.e., branching factors) can make the approach infeasible. Part of our goal is to provide a general way to achieve a proactive support ability in scenarios that are similar to our USAR task, in which the task is composed of subtasks with priorities that are dependent on the current situation. Note that a framework to achieve general proactive support can be arbitrarily complex depending on the task and level of support that is needed. In our work, similar to [23], we use the plan recognition technique in [20] and then feed its

outputs to a planner which determines the priorities of the subtasks and computes a plan accordingly. The main goal of this work is to start the investigation of humans factors for proactive support in various human-robot teaming scenarios. Regarding the benefits of automation in human-robot teaming, it is well known that automation can have both positive and negative effects on human performance. Empirical proofs have been provided in four main areas: mental workload, situation awareness, complacency and skill degradation [19]. We also aim to study the influence of proactive support on these factors in our USAR task. III. BACKGROUND A. USAR Task Settings Overview In our simulated USAR task, the human and intelligent robot (i.e., Mary) share the same set of candidate goals (i.e., subtasks), and the overall team goal is to achieve them all (which will be distributed among the human and Mary). These goals are not independent of each other. In particular, the priorities of goals are dependent on which goals are achieved in the current situation. Given these task settings, we aim to investigate the influence of a proactive support (PS) ability on a robot. We compare two cases: Mary (i.e., the intelligent robot) has a PS ability and Mary does not have this ability. During the task execution, in both cases, Mary chooses her own goal to maximize the teaming performance accordingly to the human's current goal. When Mary does not have a proactive support ability, she can only know the human's current goal when the human explicitly communicates it to her. When Mary has this ability, if the human does not inform Mary of his/her current goal,2 Mary can infer it based on her observations. To summarize, Mary in both cases can adapt to human goals while Mary with a PS ability can adapt in a more "proactive" fashion (hence proactive support). Finally, in both cases, Mary has an automated planner (see a brief description below) that can create a plan to achieve her current goal and she can autonomously execute the plan. B. Automated Planner In our settings, a task or subtask is compiled into a problem instance for an automated planner to solve. The planner creates a plan by connecting an initial state to a goal state using agent actions. A planning problem can be specified using a planning domain definition language (PDDL) [11]. Depending on the task, there are many extensions of PDDL (e.g., [9], [10]) that can incorporate various modeling requirements. We use the extension of PDDL described in [9] to model the USAR domain. Using an automated planner allows an agent to reason directly about the goal. Human factors study on the incorporation of automated planners for human-robot teaming has appeared previously in [18].
2 In both cases, when the human (optionally) informs Mary of his/her current goal, it is used directly by Mary assuming that this information is accurate.

Fig. 3.

Example puzzle problem used in our USAR task.

(a)

(b)

Fig. 2. (a) Simulated Environment for our USAR task. (b) The environment (from robot X 's cameras) that the human subject actually sees.

C. Goal and Intent Recognition To recognize the human intents and goals, assuming that humans are rational, we use the technique in [20]. In our task, Mary maintains a belief of the human's current goal (denoted by GX ) as a hypothesis goal set YX , in which YX corresponds to all remaining candidate goals. Given a sequence of observations q that are obtained periodically from sensors (on Mary or fixed in the environment), the probability distribution Q over G 2 YX is recomputed using a Bayesian update P(G|q ) µ P(q |G), where the prior is approximated by the function P(q |G) = 1/(1 + e b D(G,q ) ) in which D(G, q ) = C p (G q ) C p (G + q ). C p (G + q ) and C p (G q ) represent the cost of the optimal plan to achieve G with and without the observation of q , respectively. Having known the probability distribution Q, the goal that has the highest probability is assumed to be the current goal of the human. This goal is correspondingly taken out of the consideration of Mary and Mary then adapts her current goal if necessary (from her remaining goals) to optimize the teaming performance. Mary then makes a plan using an automated planner described previously to achieve her current goal. IV. S TUDY D ESIGN A. Hypotheses We aim to investigate the following hypotheses: ∑ H 1) Mary with a proactive support (PS) ability enables more effective teaming (e.g., less communication and more efficiency) in our task settings. ∑ H 2) Mary with a PS ability increases human mental workload (e.g., due to unanticipated actions from Mary). In our study, we also make efforts to maintain the task settings as general as possible. For a discussion on the generalization of the results, refer to the conclusion section. B. Environment Fig. 2(a) shows the simulated environment (created in Webots) in our USAR task, which represents the floor plan of an office building where a disaster occurs (e.g., a fire). Fig. 2(a) is the visual feedback from the remotely controlled robot

(i.e., robot X in Fig. 1) that the human subject actually sees. The environment is organized as segments, and each segment is identified by a unique label (e.g., R01). Furthermore, the segments are grouped into four regions: medical kit storage region (represented by segments starting with `S'), casualty search region (starting with `R'), medical room region where treatment (or triage) is performed (starting with `M'), and the hallway region (starting with `H'). Each region can be accessed via a door that connects to a hallway segment and R regions are further divided into rooms that are also connected by doors. The doors are initially closed and can be pushed open by the robots. The doors remain open after being pushed open. Both the remotely-controlled robot (denoted by `X ') and Mary work inside this environment. There are two networked CCTV cameras that Mary can obtain observations from and the field of views of these cameras are also shown in Fig. 2(a). C. Task Settings The overall team goal is to find and treat all the casualties in the environment, which includes searching for casualties in the R regions, carrying casualties to medical rooms, fetching medical kits and performing triages. In Fig. 2(a), the two colored boxes (i.e., red and blue) in R regions represent casualties and the white boxes in S regions represent medical kits. We impose two constraints on the agents: 1) either robot X or Mary can carry only one medical kit or one casualty at one time. 2) The triage can only be performed by robot X for which the human subject needs to solve a few puzzle problems (see Fig. 3 for an example) in 2 minutes. Out of the two casualties, we assume that one is critically injured (i.e., the red box in R02) who should be treated immediately after being found. The other one is lightly injured (i.e., the blue box in R05). It is also assumed that a medical room can only accommodate one casualty and each medical kit can only be used towards one casualty. D. Interface Design In this USAR task, the human subject needs to manually control robot X while interacting with Mary. To create a more realistic USAR environment, the human subject only has access to the visual feeds from robot X . In other words, the human subject can only observe the part of the environment from robot X 's "eyes" (i.e., two cameras, one mounted above the other). The interaction interface between the human subject and robot X is shown in Fig. 4. More specifically, robot X displays a list of applicable actions that it can perform given

Fig. 4.

Interaction interface between the human subject and robot X .

Fig. 5.

Interaction interface between the human subject and Mary.

the current state. The human subject interacts with robot X to choose an action from the list of applicable actions. When the chosen action is completed by X , the interaction interface displays the next set of actions. This process is repeated until the task is finished (i.e., all the casualties are found and treated). Following are the list of all possible action types that the human can choose. Compare the list with that shown in Fig 4. This interface also allows the human subject to optionally inform Mary about his/her current goal so that Mary can remove it from consideration and adapt her goal accordingly when necessary. ∑ move X H01 H02 - Move robot X from hallway segment H 01 to hallway segment H 02. ∑ pushdoor X R01 R02 - Push the door between room R01 and room R02. ∑ grab medkit X S01 - Grab the medical kit from storage room S01. ∑ carry casualty X R01 - Carry the casualty at room R05. ∑ drop medkit X M01 - Drop the medical kit in medical room M 01. ∑ lay down casualty X M01 - Lay down the casualty in medical room M 01. ∑ perform triage X M01 - Perform medical triage in medical room M 01. ∑ Press `i' - Inform Mary about the human subject's current or intended goal. (A list of all remaining candidate goals will be displayed to be chosen.) Note that these actions are modeled to respect the constraints that we discussed in Sec. IV-C. For example, lay down casualty X M01 is only available when there is no other casualties in medical room M01; perform triage X M01 is only available when there is a casualty and a medical kit in M01. The interaction interface between the human subject and Mary is shown in Fig. 5. This interface is first used by Mary to update the human subject about her current goal. When the human subject wants to take over the goal that Mary is

Fig. 6.

Experimental setup in the USAR task

acting to achieve, this interface is also used to display the choices (to be selected by the human subject) for Mary to terminate her current (uncompleted) goal. E. Study Setup and Flow The study was set up in our lab space, similar to that shown in Fig. 6. Before the beginning of the task, the human subject is given the floor plan without the annotations of the casualties (i.e., colored boxes). Furthermore, the human subject is informed that there are two casualties (that cannot move) and they are located inside the casualty search regions. However, no information about their exact locations is provided (i.e., which rooms the casualties are in). The human subject is also informed that the casualty that is represented by a red box is seriously injured, and should be treated as soon as possible. Note that Mary has no more information than the human subject. The remotely controlled robot X and Mary start in the same segment H 01, which is specified by the green arrows. Subjects were assigned alternately to team up with either Mary with a PS ability or without. Each subject is only

allowed to take part in one experimental trial to avoid performance fluctuation due to experience. All subjects completed the consent form before participating in the study. Prior to each run, the subject was asked to read the instruction materials that contain the background knowledge and the above information. The subject was then exposed to the simulator and the interface and was asked to experiment with them to gain some familiarity. The subject was asked to collaborate with Mary to find and treat the two casualties. After the trial, the subject was asked to complete a questionnaire (in Likert scale). F. Example Scenario Next, we walk through an example scenario in our USAR task. Consider a scenario in which the human subject found the critically injured casualty and the current goal (GX ) of the human subject becomes `bring the critically injured casualty to the top medical room in Fig. 2(a): goal(X ,`bring the critically injured casualty to the top medical room') = { (at critically injured casualty M01)} However, assume that the human subject failed to inform Mary of his/her current goal. Also, assume the following states for the medical kits: {(at med kit 1 S01), (at med kit 2 S04)}, and that Mary at that time is still searching the casualties in the other casualty search region. When robot X enters the field of view of the CCTV cameras the action and state of X are detected by the cameras and are fed to Mary as observations. In this example, some of robot X 's actions, such as {(move X H02 H03), (move X H04 H08)} will be observed by Mary, which triggers the goal and intent recognition process. After computing the probability distribution Q for all goals in the candidate goal set for the human, the goal that has the higher probability (and falls above a pre-specified threshold) is assumed to be the current goal of the human (GX ), which in this case is `bring the critically injured casualty to the top medical room'. Mary now knows that the critically injured casualty has been found and can remove this goal from her own candidate goal set. Furthermore, given this information, Mary recomputes the priorities of the remaining goals in the current situation and adapts her goal accordingly. In particular, although the searching task is still undergoing, Mary realizes that in this case helping the human subject by bringing a medical kit to M 01 would achieve a better utility for the team. Note that should the casualty found by the human subject be lightly injured instead, Mary would decide to continue her search; also, should the casualty found by the human subject be lightly injured but the critically injured casualty has already been treated, Mary would choose to help the human fetch the medical kit. Note also that in the case that Mary does not have a PS ability, the above update can only occur in a timely manner if the human subject chooses to inform Mary about his/her current goal. In our running example, the goal

that Mary chooses is: goal(GM ,`bring med kit 1 to the top medical room') = {(at med kit 1 M01)} Having chosen her current goal GM , Mary then uses an automated planner to generate a plan (PM ) that achieves the goal. Meanwhile, Mary will update the human subject with her current goal. Assuming that Mary is at segment H 01 at the time, the following plan would be generated: PM = h(pushdoor Mary H01 S03), (move Mary H01 S03), (move Mary S03 S04), (grab medkit Mary S04), (move Mary S04 S03), (move Mary S03 H01), (move Mary H01 H02), (move Mary H02 H03), (move Mary H03 H04), (move Mary H04 H08), (pushdoor Mary H08 M02), (move Mary H08 M02), (move Mary M02 M01), (drop medkit Mary M01)i Note that various other scenarios can arise in this task, which may not always favor Mary with a PS ability. For example, the human subject may decide to deliver the medical kits to the medical rooms even before finding any casualties. or the human subject may walk robot X to the medical room empty-handed. These can confuse the goal and intent recognition process on Mary and lead to reduced teaming performance. Although not all of these scenarios occurred during our experimental study, they demonstrate the conflicting factors for proactive support in human-robot teaming tasks. It is also clear that these tradeoffs are dependent on the task and robot settings, which require more investigations in future work. V. R ESULTS The study was performed over 4 weeks and involved 16 volunteers (9 males, 7 females), Volunteers have ages with M = 24 and SD = 1.15. Subjects were recruited from students on campus. Due to the requirement of understanding English instructions, subjects must indicate that they are confident with English communication skills before taking part in the study. We also asked about the subject's familiarity with computers (M = 6.56, SD = 0.63), robots (M = 4.19, SD = 0.91), puzzle problems (M = 3.19, SD = 0.83) and computer gaming (M = 4.69, SD = 1.49), in seven-point scales after the study (with 1 being least familiar and 7 being most familiar). The subjects reported familiarity with computers, but not so much with robots, puzzle problems or computer gaming.

Fig. 7. Results for objective performance and measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

Fig. 8. Results for task performance and measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

A. Measurement A post-study questionnaire is used to evaluate three of four areas that are often used to assess automated systems: mental workload, situation awareness, and complacency [19]. Furthermore, we also use the questionnaire to evaluate several psychological distances between individuals and the environment (including robots), which include immediacy, effectiveness, likability and trust. Immediacy describes how realistic the subject felt about the task and Mary. Effectiveness describes the subject's feeling about how effective the subject considered Mary as a teammate. Likability describes how likable the subject felt about Mary. Trust describes whether the subject felt that Mary was trustworthy. We also collect the subjects' opinions on whether they considered that Mary should be improved (i.e., improvability). One way fixed-effects ANOVA tests were performed to analyze the objective performance and measures, as well as the subjective questions. The fixed factor in the tests is the type of Mary, the intelligent robot, which is either Mary with a PS ability or without (denoted by No-PS). B. Objective Performance We first investigate the objective performance and measures. The overall performance (presented in in Fig. 7) is evaluated based on the total time taken for the team to find and treat the critically injured casualty, and the total time taken for the team to finish the entire task (i.e., find and treat both casualties). It is interesting to observe that while there is a significant difference between PS and No-PS for the time taken to complete the entire USAR task (F (1, 14) = 8.34, p < 0.01), we do not find any significant difference for treating the critically injured casualty. This may be due to the fact that humans are proficient at prioritizing goals. However, this may negatively impact the teaming performance since the subject may more often choose to neglect the help of Mary when he/she does not feel comfortable with entrusting Mary with important goals. This conjecture is also consistent with the results in Fig. 8, which is discussed next. We provide a more detailed analysis of task performance in Fig. 8. We compare the average number of times the subject stopped Mary from executing her current goal and the average number of times the subject had goal conflicts with Mary. The results show that these numbers are generally

smaller for the PS case but we did not find any significant difference. However, we did find a significant difference for the average number of times the subject informed his/her goal to Mary (F (1, 14) = 18.27, p < 0.001). This shows that the subject felt less necessity to inform Mary in the PS case. There is also a significant difference in the number of goal updates the subject received from Mary (F (1, 14) = 7.58, p < 0.05), This confirms that Mary changed her goal less frequently in the PS case. We also compare the accuracy of the puzzle problems for the triage operations. To discourage subjects from guessing the answers to the puzzle questions, they were told that each incorrect answer would give them negative scores. Our analysis, interestingly, shows a significant difference on this performance measure (F (1, 14) = 4.64, p < 0.01), which suggests that the human mental workload may have been reduced in the PS case, which is not consistent with the second hypothesis (i.e., H 2). Furthermore, as we show in the evaluation of subjective measures, this interpretation contradicts with the results there. C. Subjective Performance In this section, we investigate the subjective performance based on the questionnaire (23 questions in total). For these 23 questions, we categorize them into 8 different (partially overlapping) groups. This includes 3 groups for evaluating automation: mental workload (3 items, Cronbach's a = 0.713), situation awareness (1 item), and complacency (2 items, Cronbach's a = 0.769). Furthermore, we also evaluate several psychological distances between the human subject and environment (including Mary), which include immediacy (1 item), effectiveness (7 items, Cronbach's a = 0.724), likability (1 item), and trust (3 items, Cronbach's a = 0.871). We also include improvability (1 item). The answers to the questions are in seven-point scales. The results are presented accumulatively in Fig. 9. 1) Mental Workload: For mental workload, we include questions that inquire about the ease of working with Mary, and questions to rate the subject's mental workload to interact with Mary during the task. Although our analysis does not find any significant difference ( p = 0.404), the subjects still reported some difference in their mental workloads. This is an interesting result that confirms our hypothesis (i.e., H 2):

Fig. 9.

Results for subjective measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

although the PS ability enables more effect human-robot teaming, it also tends to increase the human mental workload at the same time. It is also worth noting that even though the subjects in the PS case reported increased mental workload, they also tended to perform well on the puzzle problems. This may be due to the fact that subjects felt less necessity to communicate with Mary and thus can concentrate more on these problems. 2) Situation Awareness: For situation awareness, we include questions that inquire about whether the subject felt that he/she had enough information to determine what the next goal should be. Our analysis does not show a significant difference (F (1, 14) = 2.78, p = 0.35), although the subjects reported slightly more situation awareness in the No-PS case, which is consistent with the side effects of automation in general. Although the number of updates for the No-PS case was significantly more than that for the PS case, the fact that situation awareness of the subject was not reduced much in the PS case is encouraging. We attribute this to the fact that the subject still needed to occasionally interact with Mary when they had goals conflicts, and the subject could gain situation awareness through such interactions. 3) Complacency: For complacency, we include questions about the comfort and ease of the teaming, as well as how well the subject felt about their performance in the task. Our analysis shows a significant difference (F (1, 14) = 11.29, p < 0.001). This is consistent with the objective performance and measures, which shows that the human subject generally felt more satisfied and confident working with Mary in the PS case. This is important for human-robot teaming. 4) Immediacy, Effectiveness, Likability & Trust: For immediacy, we include questions about how much the subject considered the simulated task as a realistic USAR task, and Mary as a teammate. Our analysis shows a significant difference (F (1, 14) = 11.63, p < 0.001), which is consistent with our prior results. For effectiveness, we include questions about the perceived effectiveness of the team, the balance of workload between the team members, and whether or not the subject felt that Mary performed expectedly. Our analysis shows a significant difference (F (1, 14) = 6.57, p < 0.05). This result suggests

that the proactive support ability indeed increases teaming effectiveness. For likability, we include questions about whether the subject felt that Mary was a good teammate. Our analysis shows a significant difference (F (1, 14) = 23.26, p < 0.001), which suggests that the subjects preferred Mary with a PS ability for teaming. For trust, we include questions about the evaluation of the Mary's trustworthiness with the assignments (or tasks) she took and with her updates during the task. Our analysis did not show any significant difference with F (1, 14) = 3.78, p = 0.072, although subjects in the PS case reported slightly higher trust. 5) Improvability: For improvability, we include questions about how much the subject felt that Mary could be improved, and how the subject evaluated his/her interaction with Mary. Our analysis shows a significant difference for improvability with F (1, 14) = 17.80, p < 0.001, which, again, suggests that the subjects preferred Mary with a PS ability. D. Summary In summary, our results are mostly consistent with our hypotheses. Our main result shows that the subjects generally preferred Mary with a PS ability. With the PS ability, the human cognitive load was indeed increased (albeit not significantly), even though the subjects appeared to interact less with Mary. More specifically, while the result on mental workload confirms our hypothesis, it also seems to be conflicting with the objective performance on the puzzle problems. This is likely due to the fact that the subject felt less necessity to interact with Mary in the PS case. Furthermore, given that situation awareness was not reduced significantly in the team with Mary having a PS ability, and that the subjects had positive feelings towards her, it seems to suggest that intelligent robots with a PS ability is welcomed in general. This is, of course, largely dependent on the fact that the subject's cognitive load is not increased significantly, which may change when the human needs to adapt to the robot's action more frequently in more complex tasks, and more communication may be needed. More investigations

are needed to be conducted in such scenarios where the task and robot settings largely differ. VI. C ONCLUSIONS In this paper, we aim to start the investigation of humans factors for proactive support in human-robot teaming. We start in a simulated USAR task with a general way to implement the proactive support (PS) ability on a robot in similar scenarios in which the task is composed of subtasks with priorities that are dependent on the current situation. Meanwhile, to maintain the generality of this task, we only introduced a few necessary simplifications. However, given the richness of USAR scenarios, more in depth studies are required to generalize the conclusions to scenarios where the task and robot settings largely differ. In such cases, our plan recognition and plan adaptation approaches may also need to be extended to implement proactive support. Note that a framework to achieve general proactive support can be arbitrarily complex depending on the task and level of support that is needed (e.g., whether the support is active [13] or passive [5] and whether it is commitment sensitive or not [3]). In our task, the human teammate is remotely controlling a robot while working with an intelligent robot Mary to search for and treat casualties. Our results show that, in general, the human teammates prefer to work with a robot that has a PS ability. However, our results also show that teaming with PS robots also increases the human's cognitive load, albeit not significantly. This is understandable since working with a proactive teammate may require more interactions and/or mental modeling on the human side in order to achieve better teaming performance. Furthermore, we also show that situation awareness when working with robots with a PS ability is not significantly reduced compared to working with robots without it. This seems to suggest that intelligent robots with a PS ability is welcomed in general. R EFERENCES
[1] Samir Alili, Matthieu Warnier, Muhammad Ali, and Rachid Alami. Planning and plan-execution for human-robot cooperative task achievement. Proc. of the 19th ICAPS, pages 1≠6, 2009. [2] Filippo Cavallo, Raffaele Limosani, Alessandro Manzi, Manuele Bonaccorsi, Raffaele Esposito, Maurizio Di Rocco, Federico Pecora, Giancarlo Teti, Alessandro Saffiotti, and Paolo Dario. Development of a socially believable multi-robot solution from town to home. Cognitive Computation, 6(4):954≠967, 2014. [3] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith, and Subbarao Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. [4] Eugene Charniak and Robert P. Goldman. A bayesian model of plan recognition. Artificial Intelligence, 64(1):53 ≠ 79, 1993. [5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1≠7, June 2009. [6] Rina Dechter, Itay Meiri, and Judea Pearl. Temporal constraint networks. Artificial intelligence, 49(1):61≠95, 1991. [7] Alberto Finzi, F¥ elix Ingrand, and Nicola Muscettola. Model-based executive control through reactive planning for autonomous rovers. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, volume 1, pages 879≠ 884. IEEE, 2004.

[8] Terrence Fong, Illah Nourbakhsh andClayton Kunz, Lorenzo Fluckiger, John Schreiner, Robert Ambrose, Robert Burridge, Reid Simmons, Laura Hiatt, Alan Schultz, J. Gregory Trafton, Magda Bugajska, and Jean Scholtz. The peer-to-peer human-robot interaction project. Space 2005. [9] Maria Fox and Derek Long. Pddl2. 1: An extension to pddl for expressing temporal planning domains. J. Artif. Intell. Res.(JAIR), 20:61≠124, 2003. [10] Alfonso Gerevini and Derek Long. Plan constraints and preferences in pddl3. The Language of the Fifth International Planning Competition. Tech. Rep. Technical Report, Department of Electronics for Automation, University of Brescia, Italy, 75, 2005. [11] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett, Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy, David E Smith, et al. Pddl-the planning domain definition language. 1998. [12] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human≠robot fluency. Robotics, IEEE Transactions on, 23(5):952≠961, 2007. [13] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team. In Proceedings of the ACM/IEEE international conference on Humanrobot interaction, pages 1≠8. ACM, 2007. [14] Henry A. Kautz. Reasoning about plans. chapter A Formal Theory of Plan Recognition and Its Implementation, pages 69≠124. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1991. [15] Henry A. Kautz and James F. Allen. Generalized Plan Recognition. In National Conference on Artificial Intelligence, pages 32≠37, 1986. [16] Uwe K® ockemann, Federico Pecora, and Lars Karlsson. Grandpa hates robots - interaction constraints for planning in inhabited environments. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu¥ ebec City, Qu¥ ebec, Canada., pages 2293≠2299, 2014. [17] Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot teams. In Twenty-Fourth International Conference on Automated Planning and Scheduling, 2014. [18] Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao Kambhampati. Automated planning for peer-to-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. [19] Raja Parasuraman. Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7):931≠951, 2000. [20] Miquel Ram¥ irez and Hector Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010. [21] Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal. Improved human-robot team performance using chaski, a humaninspired plan execution system. In Proceedings of the 6th international conference on Human-robot interaction, pages 29≠36. ACM, 2011. [22] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. IEEE Transactions on Robotics, 2007. [23] Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias Scheutz, and Subbarao Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠2962, Sept 2014. [24] Stevan Tomic, Federico Pecora, and Alessandro Saffiotti. Too cool for school - adding social constraints in human aware planning. In Proceedings of the International Workshop on Cognitive Robotics (CogRob), 2014. [25] Vaibhav V Unhelkar, Ho Chit Siu, and Julie A Shah. Comparative performance of human and mobile robotic assistants in collaborative fetch-and-deliver tasks. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pages 82≠89. ACM, 2014.

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
Multiagent Interaction without Prior Coordination: Technical Report WS-16-11

A Game Theoretic Approach to Ad-hoc
Coalitions in Human-Robot Societies
Tathagata Chakraborti, Venkata Vamsikrishna Meduri,
Vivek Dondeti, Subbarao Kambhampati
Department of Computer Science
Arizona State University
Tempe, AZ 85281, USA
{tchakra2,vmeduri,vdondeti,rao}@asu.edu
Abstract

coordination. For example, a human with a goal to deliver
two items to two different locations may team up with a delivery robot that can accomplish half of his task. Further, if
the robot was itself going to be headed in one of those directions, then it is in the interest of both these agents to form
this coalition. However, if the robot‚Äôs plan becomes too expensive as a result, it might decide that there is not enough
incentive to form this coalition. Moreover, as we highlighted
before, possible interactions between agents are not just restricted to cooperative scenarios only - the plans of one agent
can make the other agent‚Äôs plans fail, and it may happen that
it is not feasible at all for all agents to achieve their respective goals. Thus there are many possible modes of interaction between such agents, some cooperative and some destructive, that needs to be accounted for before the agents
can decide on their best course of action - both in terms of
which goal to choose and how to achieve it.
In this paper we model this problem of optimal goal selection as a two player game with perfect information, and
propose to cut down on the prior coordination of forming
such ad-hoc coalitions by looking for Nash equilibriums or
socially optimal solutions (because neither agent participating in such a coalition would have incentive to deviate). We
subsequently extend it to a Bayesian game to account for situations when agents are not sure of each other‚Äôs intent. We
will also look at properties, approximations, and interesting
caveats of these games, and motivate several extensions that
can capture a wide variety of ad-hoc interactions.

As robots evolve into fully autonomous agents, settings
involving human-robot teams will evolve into humanrobot societies, where multiple independent agents and
teams, both humans and robots, coexist and work in
harmony. Given such a scenario, the question we ask
is - How can two or more such agents dynamically
form coalitions or teams for mutual benefit with minimal prior coordination? In this work, we provide a
game theoretic solution to address this problem. We will
first look at a situation with full information, provide approximations to compute the extensive form game more
efficiently, and then extend the formulation to account
for scenarios when the human is not totally confident of
its potential partner‚Äôs intentions. Finally we will look at
possible extensions of the game, that can capture different aspects of decision making with respect to ad-hoc
coalition formation in human-robot societies.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until
now traditionally limited to humans only. This has made the
dream of human-robot cohabitation a not so distant reality.
In this work we envisage such an environment where humans and robots participate autonomously (possibly with required interactions) with their own set of tasks to achieve. It
has been argued (Chakraborti et al. 2016) that interactions
in such situations are inherently different from those studied in traditional human-robot teams. One typical aspect of
such interactions is the lack of prior coordination or shared
information, due to the absence of an explicit team.
This brings us to the problem we intend to address in this
paper - given a set of tasks to achieve, how can an agent proceed to select which one to achieve? In a shared environment
such as the one we described, this problem cannot be simply solved by picking the goal with the highest individual
utility, because the utility, and sometimes even the success
of the plan (and hence the corresponding goal) of an agent
are contingent on the intentions of the other agents around
it. However, such interactions are not adversarial - it is just
that the environment is shared among self-interested agents.
Thus, an agent may choose to form an ad-hoc team with another agent in order to increase its utility, and such coalition
formation should preferably be feasible with minimum prior

1

Related Work

There is a huge variety of work that looks at team formation from different angles. The scope of our discussion has
close ties with concepts of required cooperation and capabilities of teams to solve general planning problems, introduced
in (Zhang and Kambhampati 2014), and work on team formation mechanisms and properties of teams (Shoham and
Tennenholtz 1992; Tambe 1997). However, in this particular
work, we are more interested in the mechanism of choosing
goals that can lend to possible cooperative interactions, as
opposed to the mechanism of team design based on the goals
themselves. Thus the work of Zhang and Kambhampati can
provide interesting heuristics towards cutting down on the
computation of the extensive form game we will propose,
while existing work on different modes of team formation

546

2.2

contribute to the motivation of the Bayesian formulation of
the game discussed in later sections.
From the game theoretic point of view, coalition formation has been a subject of intense study (Ray and Vohra
2014) and the human-robot interaction community can derive significant insights from it. Of particular interest are
Overlapping Coalition Formation or OCF Games (Zick,
Chalkiadakis, and Elkind 2012; Zick and Elkind 2014),
which look at a cooperative game where the players are endowed with resources, with provisions for the players to display different modes of coalitions based on how they utilize
the resources. OCF games use arbitration functions that decide the payoffs for the deviating players based on how it
is affecting the non-deviating players and it helps in forming stable coalitions. This becomes increasingly relevant in
shared environments such as the one we discuss here. Finally, an interesting problem that can often occur is such
situations (especially with the way we have formulated the
game in the human‚Äôs favor) is the problem of free-riding
where agents take advantage of coalitions and try to minimize their effort (Ackerman and BraÃÇnzei 2014), which is
certainly an important aspect of designing such games.

2
2.1

We will represent coalitions of such agents by means of a
super-agent transformation (Chakraborti et al. 2015a) on a
set of agents that combines the capabilities of one or more
agents to perform complex tasks that a single agent might
not be capable of doing. Note that this does not preclude
joint actions among agents, because some actions that need
that need more than one agent (as required in the preconditions) will only be doable in the composite domain.
Definition 1.1 A super-agent is a tuple Œò = hŒ∏, DŒ∏ i where
Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏ is
the transformation from the individual domainSmodels to a
composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Definition 1.2 The planning problem of a super-agent Œò
is given by Œ†Œò = hFO , DŒ∏ , IŒ∏ , GŒ∏ i where the
S composite
initial and goal states are given by IŒ∏ =
œÜ‚ààŒ∏ IœÜ and
S
GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning
problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i where
¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ ,Swhere the modified
S transition function Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). The
P
P
cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca and
œÄŒ∏‚àó is optimal if C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ )‚àÄœÄŒ∏ with Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ .
The composite plan can be viewed as a union of plans contributed by each agent œÜ ‚àà Œ∏ whose component can be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà œÄŒ∏ .

Preliminaries

Environment and Agent Models

Definition 1.0 The environment is defined as a tuple
E = hF, O, Œ¶, G, Œõi, where F is a set of first order predicates that describes the environment, and O is the set of
objects, Œ¶ ‚äÜ O is the set of agents (which may be humans
or robots), G = {g | g ‚äÜ FO } 1 is the set of goals that these
agents are tasked with, and Œõ ‚äÜ O is the set of resources.
Each goal has a reward R(g) ‚àà R+ associated with it.
We use PDDL (Mcdermott et al. 1998) style agent models
for the rest of the discussion, but most of the analysis easily generalizes to other modes of representation. The domain
model DœÜ of an agent œÜ ‚àà Œ¶ is defined as DœÜ = hFO , AœÜ i,
where AœÜ is a set of operators available to the agent. The
action models a ‚àà AœÜ are represented as a = hCa , Pa , Ea i
where Ca is the cost of the action, Pa ‚äÜ FO is the list of
pre-conditions that must hold for the action a to be applicable in a particular state S ‚äÜ FO of the environment; and
Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO is a tuple that
contains the add and delete effects of applying the action
to a state. The transition function Œ¥(¬∑) determines the next
state after the application of action a in state S as Œ¥(a, S) |=
‚ä• if Pa 6‚äÜ S; |= (S \ ef f ‚àí (a)) ‚à™ ef f + (a) otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†œÜ = hFO , DœÜ , IœÜ , GœÜ i, where IœÜ , GœÜ ‚äÜ FO are the initial and goal states respectively. The solution to the planning
problem is an ordered sequence of actions or plan given by
œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that Œ¥(œÄœÜ , IœÜ ) |=
GœÜ , where the cumulative transition function is given by
Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . . , aP
|œÄ| i, Œ¥(a1 , s)). The cost of the
plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca and the optimal plan
œÄœÜ‚àó is such that C(œÄœÜ‚àó ) ‚â§ C(œÄœÜ ) ‚àÄœÄœÜ with Œ¥(œÄœÜ , IœÜ ) |= GœÜ .
1

Representation of Human-Robot Coalitions

2.3

The Use Case

Throughout the rest of the discussion we will use the setting
from Talamadupula et al. which involves a human commander CommX and a robot in a typical Urban Search and Rescue
(USAR) scenario, as illustrated in Figure 1. The environment consists of interconnected rooms and hallways, which
the agents can navigate and search. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other
agents (not shown) in the environment. A sample domain
is available at http://bit.ly/1Fko7MAhttp://bit.ly/1Fko7MA.
The shared resources here are the two medkits - i.e. some of
the plans the agents can execute will lock the use of and/or
change the position of these medkits, so as to make the other
agent‚Äôs plans, contingent on that particular resource, invalid.

Figure 1: Use case - Urban Search And Rescue (USAR).

SO is any S ‚äÜ F instantiated / grounded with objects from O.

547

3

Ad-hoc Human-Robot Coalitions

Œò = hŒ∏, DŒ∏ i is the super-agent representing the coalition formed by Œ∏ = {H, R} with IŒ∏ = IH ‚à™ IR and
GŒ∏ = GiH ‚à™ GjR . Here, the first term in the expression for
utility denotes the utility of the goal itself as defined in the
environment in Section 2.1, while the second term captures the resultant best case utility of plans due to agent
interactions. More on this below.

In this section we will look at how two agents (the human
and the robot) in our scenario, can coordinate dynamically
by forming impromptu teams in order to achieve either individually rational or socially optimal behaviors.

3.1

Motivation

Consider the scenario shown in Figure 1. Suppose one of
CommX‚Äôs goal is to perform triage in room1, while one of
the Robot‚Äôs goals is to deliver a medkit to room1. Clearly,
if both the agents choose to do their optimal plans and plan
to use medkit1 in room2, the Robot‚Äôs plan fails (assuming the CommX gets there first). The robot then has two
choices - (1) it can choose to achieve some other goal, i.e.
maximize it‚Äôs own rewards, (2) it can choose to deliver the
other medkit2 from room3, i.e. maximize social good.
Indeed there are many possible ways that these agents can
interact. For example, the utility of choosing any goal may
be defined by the optimal cost of achieving that goal individually, or as a team. This in turn affects the choice whether to
form such teams or not. In the discussion that follows, we
model this goal selection (and team formation) problem as a
strategic game with perfect information.

3.2

Human-centric robots. At this point we make an assumption about the role of the robots in our human-robot society
- we assume that the robots exist only in the capacity of autonomous assistance, i.e. in coalitions that may be formed
with humans and robots, the robot‚Äôs role is to improve the
quality of life of the humans (by possibly, in our case, reducing the costs of plans) and not vice versa.
Thus, in the expression of utility, the human uses a min‚àó
‚àó
imizing term - with no interactions C(œÄH
) = C(œÄŒò
(H)),
‚àó
‚àó
otherwise C(œÄH ) > C(œÄŒò (H)). Similarly, in case of
‚àó
‚àó
the robot, with no interactions C(œÄR
) ‚â• C(œÄŒò
(R)) and
‚àó
‚àó
C(œÄH ) <=> C(œÄŒò (H)) otherwise, since the interactions
may or may not be always cooperative for the robot. Note
that this formulation also takes care of the cases when the
robot goal becomes unachievable due to negative interactions with the human (this is why we have the maximizing
term; the difference is triggered due to negative interactions
with the human plan in absence of coalitions). Also note that
the goal utility is using a combined goal due to the particular
action profile, this captures cases when goals have interactions, i.e. a conjunction of goals may have higher (or lower)
utility than the sum of its components.
This can be easily ensured while generating plans for a
given coalition, by either discounting the costs of actions
of the robot with respect to those of the humans by a suitable factor, or more preferably, by just penalizing the total
cost of the human component in the composite plan more.
The assumption of course does not change the formulation
in any way, it is just more aligned with the notion of the
social robots being envisioned currently. Of course, in this
sense the utilities of both the humans and robots will now
become identical, with a minimizing cost term.
Now that we have defined the game, the question is how
do we choose actions for each agent? Remember that we
want to find solutions that will preclude the need to coordinate. We can take two approaches here - we can make agents
individually rational (in which case both the human and the
robot looks for a Nash equilibrium, so neither has a reason to
defect; or we can make the agents look for a socially optimal
solution (so that sum of utilities is maximized).

Formulation of the Game

We refer to our static two-player strategic game Goal Allocation with Perfect Information as GAPI = hŒ¶, {AœÜ }, {U œÜ }i.
The game attempts to determine, given complete information about the domain model and goals of the other agent,
which goal to achieve and whether forming a coalition is
beneficial. The game is defined as follows - Players - The game has two players Œ¶ = {H, R} the
human H and the robot R respectively.
- Actions - The actions of the agents in the strategic game
are the goals that they can select to achieve. Thus, for
each agent œÜ ‚àà Œ¶ we define a set of goals G œÜ =
|G œÜ |
{G1œÜ , G2œÜ , . . . , GœÜ } ‚äÜ G, and the action set AœÜ of the
agent œÜ is the mapping that assigns one of these goals as
its planning goal, i.e. AœÜ : G œÜ 7‚àí‚Üí G. Note that this is
distinct from the action models defined in PDDL for each
of the individual agents (which helps the agent figure out
how this goal G is achieved, and the resultant utility).
- Utilities - Finally, as discussed previously, the utility of an
action depends on (apart from the utility of the goal itself)
the way the agent chooses to achieve it, and is contingent
also on the plans of the other agent (due to, for example,
resource conflicts), and is given by ‚Äî

3.3

Solving for Nash Equilibriums

As usual, the Nash equilibriums in GAPI are
R
given by action profiles hAH
such that
i , Aj i
H
R
H
R
R
UH (Ai , Aj ) ‚â• UH (Ak : ‚àÄk6=i , Aj ) and UR (AH
i , Aj ) ‚â•
H
R
UH (Ai , Ak : ‚àÄk6=j ). It is easy to prove that there is no
guaranteed Nash equilibrium in GAPI. We will instead
motivate a slightly different game GAPI-Bounded where
the robot only agrees to deviate from its optimal plan up
to a certain degree, i.e. there is a bound on the amount of
assistance the robot chooses to provide.

R
i
j
‚àó
‚àó
UH (AH
i , Aj ) = R(GH ‚à™ GR ) ‚àí min{C(œÄH ), C(œÄŒò (H))}
R
UR (AH
i , Aj )
i
‚àó
‚àó
‚àó
= R(GH ‚à™ GjR ) ‚àí C(œÄŒò
(R)) if C(œÄH
) > C(œÄŒò
(H))
j
i
‚àó
‚àó
= R(GH ‚à™ GR ) ‚àí max{C(œÄR ), C(œÄŒò (R))}, otherwise.

‚àó
where, œÄH
is the optimal plan or solution of the planning
‚àó
problem defined by Œ†H = hFO , DH , IH , GiH i, œÄR
is the
j
‚àó
optimal solution of Œ†R = hFO , DR , IR , GR i, and œÄŒò
is
the optimal solution of Œ†Œò = hFO , DŒ∏ , IŒ∏ , GŒ∏ i, where

548

Definition 1.3. The differential help Œ¥(g, GiR ) provided
by the robot R with goal GiR ‚àà G R , when the human H
picks goal g ‚àà G H , measures the decrease in utility of
the robot upon forming a coalition with the human, and is
‚àó
‚àó
‚àó
given by Œ¥(g, GiR ) = |C(œÄŒò
(R)) ‚àí C(œÄR
)|, where œÄR
is
i
‚àó
the optimal solution of Œ†R = hFO , DR , IR , GR i, and œÄŒò
is
the optimal solution of Œ†Œò = hFO , DŒ∏ , IH ‚à™ IR , g ‚à™ GiR i,
where Œò = hŒ∏ = {H, R}, DŒ∏ i.

Further, it may be noted here that there may be many such
Nash equilibriums in GAPI-Bounded and these are also
the only ones, i.e. all Nash equilibriums in GAPI-bounded
must satisfy the conditions in the above claim.

3.4

Similarly, the socially optimal goal selection strategies are
‚àó ‚àó
R
given by the action profiles hAH
i‚àó , Aj ‚àó i where {i , j } =
R
H
R
arg maxi,j UH (AH
i , Aj ) + UR (Ai , Aj ). The socially optimal action profiles may not necessarily correspond to any
Nash equilibriums of either GAPI or GAPI-Bounded.

Thus in GAPI-Bounded the utility function is modified
from the one in GAPI as follows R
i
‚àó
UH (AH
i , Ai ) = R(GH ) ‚àí C(œÄH )

Individual Irrationality and ‚àíEquilibrium. Given the
way the game is defined, it is easy to see that the socially
good outcome may not be individually rational for either the
human or the robot, since the robot always has the incentive
to defect to choosing G‚àóR and the human will then choose the
corresponding highest utility goal for himself. This leaves
room for designing autonomy that can settle for action profiles hAH
, AR
i referred to as -equilibriums, for the purpose
iÃÇ
jÃÇ

j
R
‚àó
UR (AH
i , Aj ) = R(GR ) ‚àí C(œÄR )
‚àó
if ‚àÉGkR : k6=j ‚àà G H s.t. Œ¥(GiH , GjR ) > {R(GjR ) ‚àí C(œÄR
)} ‚àí
k
‚àó‚àó
‚àó
‚àó‚àó
‚àó
{R(GR ) ‚àí C(œÄR )}, where œÄR , œÄR and œÄH are the
optimal plans or solutions to the planning problems
Œ†iR = hFO , DR , IR , GjR i, Œ†kR = hFO , DR , IR , GkR i and
Œ†H = hFO , DH , IH , GiH i respectively; and otherwise -

R
H
R
of social good, i.e. |UH (AH
i‚àó , Aj ‚àó ) ‚àí UH (AiÃÇ , AjÃÇ )| ‚â§  and

R
i
‚àó
UH (AH
i , Ai ) = R(GH ) ‚àí C(œÄŒò (H))

R
H
R
|UR (AH
i‚àó , Aj ‚àó ) ‚àí UR (AiÃÇ , AjÃÇ )| ‚â§ . Note that this deviation is distinct from the concept of bounded differential assistance we introduced in Section 3.3.

j
R
‚àó
UR (AH
i , Aj ) = R(GR ) ‚àí C(œÄŒò (R))
‚àó
where œÄŒò
is the optimal solution of Œ†Œò = hFO , DŒ∏ , IH ‚à™
IR , g ‚à™ GjR i, where Œò = hŒ∏ = {H, R}, DŒ∏ i.
This basically means that if the penalty that the robot incurs by choosing to assist the human is so great that it could
rather do something else instead (i.e. choose another goal),
then it switches back to using its individual optimal plan, i.e.
no coalition is formed. If the individual optimal plans are always feasible (otherwise these do not participate in the Nash
equilibriums below), this leads to the following result.

Price of Anarchy. The price of deviating from individual
rationality is referred to as the Price of Anarchy and is measured by POS =

3.5

R

C(œÄR ) and i = arg maxi UH (GiH , GjR ).
Proof Sketch. Let us define the utility function of the
robot R for achieving a goal g ‚àà G R by itself as œÑ (g) =
‚àó
‚àó
R(g) ‚àí C(œÄR
), where œÄR
is the optimal solution to the planning problem Œ†R = hF, O, DR , IR , gi. Further, given the
‚àó
goal set G R of the robot, we set GjR = arg maxg‚ààG R œÑ (g),
‚àó
i.e. GjR corresponds to the highest utility goal that the
robot can achieve by itself. Now consider any two goals
‚àó
‚àó
GjR , GjR ‚àà G R , GjR 6= GjR . We argue that ‚àÄGiH ‚àà G H ,
j‚àó
R
H
R
UR (AH
i , Aj ‚àó ) ‚â• UR (Ai , Aj ). This is because œÑ (GR ) ‚â•
R
œÑ (GjR ) and by problem definition ‚àÄi, k |UR (AH
i , Aj ‚àó ) ‚àí
‚àó
œÑ (GjR )

UH (AH
,AR
)+UR (AH
,AR
)
iÃÇ
jÃÇ
iÃÇ
iÃÇ
UH (AH
,AR
)+UR (AH
,AR
)
i‚àó
j‚àó
i‚àó
j‚àó

.

Caveats

No or Multiple Nash Equilibriums. One of the obvious
problems with this approach is that it does not guarantee a
unique Nash equilibrium, if it exists at all. This has serious implications on the problem we set out to solve in the
first place - which goals do the agents choose to plan for,
and how? Note, however, that this is not really a feature of
the formulation itself but of the domain or the environment,
i.e. the action models of the agents and the utilities in the
goals will determine whether there is a single best coalition
that may be formed given a particular situation. Thus, there
seems to be no principled way of solving this problem in a
detached manner, without any form of communication between the agents. But our approach still provides a way to
deliberate over the possible options, and communicate to resolve ambiguities only with respect to the Nash equilibriums, rather than the whole set of goals, or even just those
in each agent‚Äôs dominant strategy, which can still provide
significant reduction in the communication overhead.

R
Claim. hAH
i‚àó , Aj ‚àó i must be a Nash equilibrium of
GAPI-Bounded when j ‚àó = arg maxGj ‚ààG R R(GjR ) ‚àí
‚àó

Solving for Social Good

Infeasibility of the Extensive Form Game. Note here
that the utilities of the actions are calculated from the cost
of plans to achieve the corresponding goals, which involves
solving two planning problems per action. This means that,
in order to get the extensive form of GAPI, we need to solve
O(|G H |√ó|G R |) planning problems in total (note that solving
for œÄŒò gives utilities for both agents H and R), which may
be infeasible for large domains. So we need a way to speed
up our computation (either by computing an approximation

œÑ (GjR ).

R
UR (AH
‚àí
Thus, in general, the
k , Aj ‚àó )| ‚â§
goal ordering induced by the function œÑ is preserved by
the utility function UR , and consequently AR
j ‚àó is a dominant strategy of the robot. It follows that AH
i‚àó such that
i‚àó = arg maxi UH (GiH , G‚àóR ) is the corresponding best reR
sponse for the human. Hence hAH
i‚àó , Aj ‚àó i must be a Nash
equilibrium. Hence proved.


549

1
1
max{h(I, GiœÜ ), hÃÇ(I, Gi‚àí1
œÜ )}; hÃÇ(I, GœÜ ) = h(I, GœÜ ). Then hÃÇ
is well-behaved. Hence proved.

These properties of GAPI-Bounded, GAPI and GAPI
enables computation of approximations, and partial profiles,
to the extensive form of GAPI, while maintaining the nature
of interactions, thus making the formulation more tractable.

and/or finding ways to calculate multiple utility values at
once), while simultaneously preserving guarantees from our
original game in our approximate version.
Fortunately, we have good news. Note that all we require
are costs of the plans, not the plans themselves. So a promising approach towards cutting down on the computational
complexity is by using heuristic values for the initial state
of a particular planning problem as a proxy towards the true
plan cost. Note that the better the heuristic is, the better our
approximation is. So the immediate question is - What guarantees can we provide on the values of the utilities when we
use heuristic approximation? Are the Nash equilibriums in
the original game still preserved? This brings us to the notion of ‚Äúwell-behaved heuristics‚Äù as follows -

4
4.1

‚Ä¢ Individual Optimality - In this type of planning, each
agent computes the individual optimal plan to achieve
their goals. Note that this plan may not be actually valid
in the environment during execution time, due to factors
such as resource conflicts due to plans of the other agents.

We define GAPI as a game identical to GAPI but with a
modified utility function as follows -

‚Ä¢ Joint Optimality - Here we compute the joint optimal for
a coalition; and this optimal plan is computed in favor of
the human as discussed previously in Section 3.2.

R
i
i
i
UH (AH
i , Aj ) = R(GH ) ‚àí min{h(GH , IH ), h(GH , IH ‚à™ IR )}
R
UR (AH
i , Aj )
i
= R(GH ) ‚àí h(GjR , IH ‚à™ IR ) if h(GiH , IH ) > h(GiH , IH ‚à™ IR )
= R(GiH ) ‚àí max{h(GjR , IR ), h(GjR , IH ‚à™ IR )}, otherwise.

‚Ä¢ Planning with Resource Conflicts - In (Chakraborti et
al. 2015b) we explored a technique for the robot to produce plans so as to ensure the success of the human plans
only, and explored different modes of such behavior of
the robot in terms of compromise, opportunism and negotiation. Thus utilities for the human plans computed this
way is, at times, same as the joint optimal, but in general
is greater than or equal to the individual optimal and less
than or equal to the joint optimal.

Note that in order to get a heuristic estimate of an agent‚Äôs
contribution to the composite plan, we compute the heuristic
with respect to achieving the individual agent goal using the
composite domain of the super-agent, which of course gives
a lower bound on the real cost of the composite plan used to
achieve that agent‚Äôs goal only.
Claim. NEs in GAPI are preserved in GAPI.

‚Ä¢ Planning for Serendipity - In (Chakraborti et al. 2015a)
we looked at a special case of multi-agent coordination,
where the robot computes opportunities for assisting the
human in the event the human is not planning to exploit
the robot‚Äôs help. Here, as in the previous case, utilities for
the human plans computed this way is again greater than
or equal to the individual optimal and less than or equal
to the joint optimal plans.

Proof Sketch. This is easy to see because orderings
among costs are preserved by a well-behaved heuristic, and
hence ordering among utilities, which is known to keep the
Nash equilibriums unchanged. Note that the reverse does not
hold, i.e. GAPI may have extra Nash equilibriums due to the
equality in the definition of well-behaved heuristics.

Definition 1.5 We define a goal-ordering on the goal set
G œÜ of agent œÜ as a function f : [1, |G œÜ |] 7‚Üí [1, |G œÜ |] such
f (2)

Motivation

In the previous sections we considered both individual and
team plans, and as teams we considered optimal plans for
a coalition. In reality there are many ways that a particular
coalition can achieve a particular goal, and correspondingly
there are different modes of interaction between the teammates. We discuss four such possibilities briefly here -

Definition 1.4 A well-behaved heuristic h : S √ó S 7‚Üí
R+ , S ‚äÜ FO is such that h(I, G1 ) ‚â§ h(I, G2 ) whenever
C(œÄ1‚àó ) ‚â§ C(œÄ2‚àó ), where œÄ1‚àó and œÄ2‚àó are the optimal solutions to the planning problems Œ†1 = hFO , D, I, G1 i and
Œ†2 = hFO , D, I, G2 i respectively.

f (1)

Bayesian Modeling of Teaming Intent

Going back to our use case in Figure 1, suppose the robot
has a goal to deliver a medkit to room1, and CommX has
a goal to conduct triage in room1, for which he also requires a medkit (and his optimal plan involves picking up
medkit1 in room2). For individual optimal plans both the
robot and the human will go for medkit2 (thus, in this situation, individual optimal plans are actually not feasible).
For the joint optimal, the coalition can team up to both use
the same medkit thus achieving mutual benefit. In case the
robot is only planning to avoid conflicts, it can settle for using medkit3 which is further away, or the robot can also
intervene serendipitously by handing over medkit2 in the
hallway thus achieving higher utility through cooperation
without directly coordinating.
For our problem, this has the implication that we can no
longer be sure of the plan (and consequently the utility) even

œÜ

f (|G |)

that GœÜ ‚äÜ GœÜ ‚äÜ . . . ‚äÜ GœÜ
. This means that the
goals of an agent are such that they are all different subgoals
of a single conjunctive goal.
We will refer to the game with agents with such ordered
goal sets as GAPI (identical to GAPI otherwise).
Claim. NEs in GAPI are preserved in GAPI.
Proof Sketch. Since G œÜ is goal-ordered, C(œÄf‚àó(1) ) ‚â§
C(œÄf‚àó(2) ) ‚â§ . . . ‚â§ C(œÄf‚àó(|G œÜ |) ), where, as usual, œÄi‚àó
is the optimal solution to the planning problem Œ†i =
hFO , D, I, GiœÜ i. Let us consider a non-trivial admissible
heuristic h and define a heuristic hÃÇ such that hÃÇ(I, GiœÜ ) =

550

powerful approximations, and the ability to deal with issues
such as synchronization and coalitions evolving across individual goal allocations. For GAPI-Bayesian, this also
includes evolving beliefs as we will see below.

when a particular goal has been chosen. Rather what we
have is a possible set of utilities for each goal. However we
can do better than to just take the maximum (or minimum
as the case may be) of these utilities as we did previously,
because we now know how such behaviors are being generated and so we can leverage additional information from an
agent‚Äôs beliefs about the other agent to come up with optimal response strategies. This readily lends the problem to a
formulation in terms of Bayesian strategic games, which we
will discuss in the next section.

4.2

5.2

Evolving Utilities. Often, and certainly in the examples
provided in Section 4.1, the behavior of the robot depends on
understanding the intent(s) of its human counterpart. Thus
the utilities will keep evolving based on the actions of the
human after the goal has been selected. This is even more
relevant in scenarios where communication is severely limited, when the agents in a coalition are not aware of the exact
goals that the other agents have selected.

Formulation of the Game

We define our two-person static Bayesian game
GAPI-Bayesian = hŒ¶, B, AH , {AR,B }, U H , {U R,B }i
with belief B over the type of robot as follows -

Evolving Beliefs. Intent recognition has a direct effect on
the belief over the robot type itself. For example, as the human observes the actions of the robot, it can infer which behavior the robot is going to exhibit. Thus intent recognition
over the robot‚Äôs actions will result in evolving belief of the
human, as opposed to intent recognition over the human‚Äôs
activities which informed the planning process and hence
the utilities of the robot.

- Players - We still have two players - the human H and the
robot R, as in the previous games.
- Actions - The actions of the players are similarly identical
to GAPI, i.e. the action set of agent œÜ ‚àà {H, R} is the
mapping AœÜ : G œÜ 7‚àí‚Üí G.
- Beliefs - The human has a set of beliefs on the robot
B = {B1 , B2 , . . . , B|B| } characterized by the distribution
B ‚àº P , i.e. the robot can be of any of the types in B with
probability P (B). The type of the robot is essentially the
algorithm it uses to compute the optimal plan given the
initial state and the selected goal, and thus affects the cost
of achieving the goal, and hence the utility function.
- Utilities - The utilities are defined as
R
i
‚àó
UH (AH
i , Aj , B) = R(GH ) ‚àí C(œÄŒò (H)|B)
j
H
R
‚àó
UR (Ai , Aj , B) = R(GR ) ‚àí C(œÄŒò (R)|B)
where symbols have their usual meaning.

5.3

6

Conclusions

In conclusion, we introduced a two-player static game that
can be used to form optimal coalitions on the go among two
autonomous members of a human-robot society, with minimum prior coordination. We also looked at several properties of such games that may be used to make the problem
tractable while still maintaining key properties of the game.
Finally, we explored an extension of the game to a general
Bayesian formulation when the human is not sure of the intent of the robot, and motivated the implications and expressiveness of this model. We believe the work will stimulate
discussion on ad-hoc interaction among agents in the context
of human-robot cohabitation settings and provide insight towards generating efficient synergy.

Discussions and Future Work

Acknowledgments

The concept of Bayesian games lends GAPI to several interesting possibilities, and promising directions for future
work, with respect to how interactions evolve with time.

5.1

Implications of Implicit Preferences

Finally, as agents interact with each other over time, in different capacities as teammates and colleagues, their expectations over which agent is likely to form which form of coalition will also evolve. This will give the prior belief over the
robot type that the human starts with, and will get updated
as further interactions occur.

As
before,
the
Nash
equilibriums
in
GAPI-Bayesian are given by action profiles
R
hAH
such that the human has no reason
i , Aj i
P
H
R
to defect, i.e.
‚â•
B‚ààB UH (Ai , Aj , B)P (B)
P
H
R
U
(A
,
A
,
B)P
(B)
while
the
robot
also
has
H
j
k : ‚àÄk6=i
B‚ààB
P
R
no incentive to change, i.e. B‚ààB UR (AH
i , Aj , B)P (B) ‚â•
H
R
UH (Ai , Ak : ‚àÄk6=j , B)P (B), given the distribution P over
the beliefs B of robot type. Similarly, the socially optiR
mal solution is given by the action profiles hAH
i‚àó , Aj ‚àó i
P
‚àó ‚àó
H
R
where {i , j } = arg maxi,j B‚ààB [UH (Ai , Aj , B) +
R
UR (AH
i , Aj , B)]P (B).

5

Impact of Intent Recognition

This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023. I would also
like to give special thanks to Prof. Guoliang Xue (with the
Department of Computer Science at Arizona State University) for his valuable support and inputs.

Unrolling the Entire Game

Notice that we formulated the game such that each of the
agents œÜ has a set of goals G œÜ to achieve. Thus GAPI immediately lends itself to a finite horizon dynamic game unrolled maxœÜ‚ààŒ¶ |G œÜ | times, so that the agents can figure out
their most effective long-term strategy and coalitions. Finding optimal policies in such cases will involve devising more

References
Ackerman, M., and BraÃÇnzei, S. 2014. The authorship
dilemma: Alphabetical or contribution? In Proceedings of

551

the 2014 International Conference on Autonomous Agents
and Multi-agent Systems, AAMAS ‚Äô14, 1487‚Äì1488. Richland, SC: International Foundation for Autonomous Agents
and Multiagent Systems.
Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.;
Scheutz, M.; Smith, D.; and Kambhampati, S. 2015a. Planning for serendipity. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).
Chakraborti, T.; Zhang, Y.; Smith, D.; and Kambhampati, S.
2015b. Planning with stochastic resource profiles: An application to human-robot co-habitation. In ICAPS Workshop
on Planning and Robotics.
Chakraborti, T.; Talamadupula, K.; Zhang, Y.; and Kambhampati, S. 2016. Interaction in human-robot societies. In
AAAI Workshop on Symbiotic Cognitive Systems.
Mcdermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram,
A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. Pddl - the
planning domain definition language. Technical Report TR98-003, Yale Center for Computational Vision and Control,.
Ray, D., and Vohra, R. 2014. Handbook of Game Theory.
Handbooks in economics. Elsevier Science.
Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of
useful social laws for artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì281.
Talamadupula, K.; Briggs, G.; Chakraborti, T.; Scheutz, M.;
and Kambhampati, S. 2014. Coordination in human-robot
teams using mental modeling and plan recognition. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), 2957‚Äì2962.
Tambe, M. 1997. Towards flexible teamwork. J. Artif. Int.
Res. 7(1):83‚Äì124.
Zhang, Y., and Kambhampati, S. 2014. A formal analysis
of required cooperation in multi-agent planning. In ICAPS
Workshop on Distributed Multi-Agent Planning (DMAP).
Zick, Y., and Elkind, E. 2014. Arbitration and stability in
cooperative games. SIGecom Exch. 12(2):36‚Äì41.
Zick, Y.; Chalkiadakis, G.; and Elkind, E. 2012. Overlapping coalition formation games: Charting the tractability frontier. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS ‚Äô12, 787‚Äì794. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.

552

A Formal Framework for Studying Interaction in Human-Robot Societies
Tathagata Chakraborti1

Kartik Talamadupula2

Yu Zhang1

Subbarao Kambhampati1

Department of Computer Science1

Cognitive Learning Department2

Arizona State University
Tempe, AZ 85281, USA

IBM Thomas J. Watson Research Center
Yorktown Heights, NY 10598, USA

{tchakra2, yzhan442, rao}@asu.edu

krtalamad@us.ibm.com

Abstract
As robots evolve into an integral part of the human
ecosystem, humans and robots will be involved in a
multitude of collaborative tasks that require complex
coordination and cooperation. Indeed there has been
extensive work in the robotics, planning as well as
the human-robot interaction communities to understand
and facilitate such seamless teaming. However, it has
been argued that their increased participation as independent autonomous agents in hitherto human-habited
environments has introduced many new challenges to
the view of traditional human-robot teaming. When
robots are deployed with independent and often selfsufficient tasks in a shared workspace, teams are often
not formed explicitly and multiple teams cohabiting an
environment interact more like colleagues rather than
teammates. In this paper, we formalize these differences
and analyze metrics to characterize autonomous behavior in such human-robot cohabitation settings.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made
the dream of human-robot cohabitation a not so distant reality. We are now witnessing the development of autonomous
agents that are especially designed to operate in predominantly human-inhabited environments often with completely
independent tasks and goals. Examples of such agents include robotic security guards like Knightscope, virtual presence platforms like Double and iRobot Ava, and even autonomous assistance in hospitals such as Aethon TUG. Of
particular fame are the CoBots (Rosenthal, Biswas, and
Veloso 2010) that can ask for help from unknown humans,
and thus interact with agents not directly involved in its plan.
Indeed there has been a lot of work recently in the context
of ‚Äúhuman-aware‚Äù planning, both from a point of view of
path planning (Sisbot et al. 2007; Kuderer et al. 2012) and
task planning (Koeckemann, Pecora, and Karlsson 2014;
Cirillo, Karlsson, and Saffiotti 2010), with the intention of
making the robot‚Äôs plans socially acceptable, e.g. resolving conflicts with the plans of fellow humans. Even though
all of these scenarios involve significantly different levels
of autonomy from the robotic agent, the underlying theme
of autonomy in such settings involves the robot achieving
some sense of independence of purpose in so much as its

existence is not just defined by the goals of the humans
around it but is rather contingent on tasks it is supposed to
be achieving on its own. Thus the robots in a way become
colleagues rather than teammates. This becomes even more
prominent when we consider interactions between multiple independent teams in a human-robot cohabited environment. We thus postulate that the notions of coordination
and cooperation between the humans and their robotic colleagues is inherently different from those investigated in existing literature on interaction in human-robot teams, and
should rather reflect the kind of interaction we have come
to expect from human colleagues themselves. Indeed recent
work (Chakraborti et al. 2015a; Chakraborti et al. 2015b;
Talamadupula et al. 2014) hints at these distinctions, but has
neither made any attempt at formalizing these ideas, nor provided methods to quantify behavior is such settings. To this
end, we propose a formal framework for studying inter-team
and intra-team interactions in human-robot societies, show
how existing metrics are grounded in this framework and
propose newer metrics that are useful for evaluating performance of autonomous agents in such environments.

1

Human Robot Cohabitation

At some abstracted level, agents in any environment can be
seen as part of a team achieving a high level goal. Consider, for example, your university or organization. At a micro level, it consists of many individual labs or groups that
work independently on their specific tasks. But when taken
as a whole, the entire institute is a team trying to achieve
some higher order tasks like increasing its relative standing
among its peers or competitors. So in the discussion that follows, we talk about environments, and teams or colleagues
acting within it, in the context of the goals they achieve.

1.1

Goal-oriented Environments

Definition 1.0 A goal-oriented environment is defined as
a tuple E = hF, O, Œ¶, G, Œõi, where F is a set of first order
predicates that describes the environment, and O is a set
of objects in the environment, Œ¶ ‚äÜ O is the set of agents,
G = {g | g ‚äÜ FO } is the set of goals that these agents
are tasked with, and Œõ ‚äÜ O is the set of resources that are
required by the agents to achieve their goals. Each goal has

a reward R(g) ‚àà R+ associated with it.1
These agents and goals are, of course, related to each
other by their tasks, and these relationships determine the
nature of their interactions in the environment, i.e. in the
form of teams or colleagues. Before we formalize such relations, however, we would look at the way the agent models
are defined. We use PDDL (Mcdermott et al. 1998) models
for the rest of the discussion, as described below, but most of
the discussion easily generalizes to other modes of representation. The domain model DœÜ of an agent œÜ ‚àà Œ¶ is defined
as DœÜ = hFO , AœÜ i, where AœÜ is a set of operators available
to the agent. The action models a ‚àà AœÜ are represented as
a = hCa , Pa , Ea i where Ca is the cost of the action, Pa ‚äÜ
FO is the list of pre-conditions that must hold for the action
a to be applicable in a particular state S ‚äÜ FO of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO
is a tuple that contains the add and delete effects of applying
the action to a state. The transition function Œ¥(¬∑) determines
the next state after the application of action a in state S as
Œ¥(a, S) = (S\ef f ‚àí (a))‚à™ef f + (a) if Pa ‚äÜ S; ‚ä• otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†Œ± = hF, O, DœÜ , IœÜ , GœÜ i, where IœÜ ‚äÜ FO is the initial state
of the world and GœÜ ‚äÜ FO is the goal state. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that
Œ¥(œÄœÜ , IœÜ ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . .P
, a|œÄ| i, Œ¥(a1 , s)). The cost
of the plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca .
We will now introduce the concept of a super-agent
transformation on a set of agents that combines the capabilities of one or more agents to perform complex tasks
that a single agent might not be able to do. This will help
us later to formalize the nature of interactions among agents.
Definition 1.1a A super-agent is a tuple Œò = hŒ∏, DŒ∏ i
where Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏
is the transformation from the individual domainSmodels to
a composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Note that this does not preclude joint actions among
agents, because some actions that need that need more than
one agent (as required in the preconditions) will only be
doable in the composite domain.
Definition 1.1b The planning problem of a super-agent Œò
is similarly given by Œ†Œò = hF, O, DŒ∏ , IŒ∏ , GŒ∏ i where
S the
composite initial and goal states are given by IŒ∏ = œÜ‚ààŒ∏ IœÜ
S
and GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i
where ¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |=SGŒ∏ , where the modified
transition function
S
Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). We denote
the set of all such plans as œÄŒò .
P
P
The cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca
and œÄŒ∏‚àó is optimal if Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ =‚áí C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ ).
The composite plan can thus be viewed as a union of plans
contributed by each agent œÜ ‚àà Œ∏ so that œÜ‚Äôs component can
be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà
1

SO is S ‚äÜ F instantiated or grounded with objects from O.

œÄŒò . Now we will define the relations among the components
of the environment E in terms of these agent models.
Definition 1.2 At any given state S ‚äÜ FO of the
environment E, a goal-agent correspondence is defined as the relation œÑ : G ‚Üí P(Œ¶); G, Œ¶ ‚àà E, that
induces a set of super-agents œÑ (g) = {Œò | Œ†Œò =
hF, O, DŒ∏ , S, gi has a solution, i.e. ‚àÉœÄ s.t. Œ¥(œÄ, S) |= g}.
In other words, œÑ (g) gives a list of sets of agents in the environment that are capable of performing a specific task g.
We will see in the next section how the notions of teammates
and colleagues are derived from it.

1.2

Teams and Colleagues

Definition 2.0 A team Tg w.r.t. a goal g ‚àà G is defined as
any super-agent Œò = hŒ∏, DŒ∏ i ‚àà œÑ (g) iff 6 ‚àÉœÜ ‚àà Œ∏ such that
Œò0 = hŒ∏ \ œÜ, DŒ∏\œÜ i and œÄŒò = œÄŒò0 .
This means that any super-agent belonging to a particular
goal-agent correspondence defines a team w.r.t that specific
goal when every agent that forms the super-agent plays
some part in the plans that achieves the task described
by g, i.e. the super-agent cannot use the same plans to
achieve g if an agent is removed from its composition.
This, then, leads to the concept of strong, weak, or optimal teams, depending on if the composition of the
super-agent is necessary, sufficient or optimal respectively
(note that an optimal team may or may not be a strong team).
Definition 2.0a A team Tgs = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is strong iff 6 ‚àÉœÜ ‚àà Œ∏ such that hŒ∏ \ œÜ, DŒ∏\œÜ i ‚àà œÑ (g).
A team Tgw is weak otherwise.
Definition 2.0b A team Tgo = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is optimal iff ‚àÄŒò0 ‚àà œÑ (g), C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏‚àó0 ).
This has close ties with concepts of required cooperation and capabilities of teams to solve general planning
problems, introduced in (Zhang and Kambhampati
2014), and work on team formation mechanisms and
properties of teams (Shoham and Tennenholtz 1992;
Tambe 1997). In this paper, we are more concerned about
the consequences of such team formations on teaming metrics. So, with these different types of teams we have seen
thus far, the question we ask is: What is the relation among
the rest of the agents in the environment? How do these
different teams interact among and between themselves?
Definition 2.1 The set of teams in E are defined by the
relation Œ∫ : G ‚Üí R(œÑ ); G ‚àà E, where Œ∫(g) ‚àà œÑ (g) denotes
the team assigned to the goal g, ‚àÄg ‚àà G.
This, then, gives rise to the idea of collegiality among
agents, due to both inter-team and intra-team interactions.
Note that how useful or necessary such interactions are
will depend on whether the colleagues can contribute to
each other‚Äôs goals, or to what extent they influence their
respective plans, which leads us to the following two
definitions of colleagues based on the concept of teams.
Definition 2.2a Let Œ∫(g) = hŒ∏1 , DŒ∏1 i, Œ∫(g 0 ) = hŒ∏2 , DŒ∏2 i
be two teams in E. An agent œÜ1 ‚àà Œ∏1 is a type-1 colleague
to an agent œÜ2 ‚àà Œ∏2 when Œ∫0 (g) = hŒ∏1 ‚à™ œÜ1 , DŒ∏1 ‚à™œÜ1 i is a
weak team w.r.t. the goal g.

Definition 2.2b Agents œÜ1 , œÜ2 ‚àà Œ¶ are type-2 colleagues
when ‚àÄŒ∫(g) = hŒ∏, DŒ∏ i s.t. {œÜ1 , œÜ2 } ‚à© Œ∏ 6= ‚àÖ, {œÜ1 , œÜ2 } 6‚àà
Œ∏ ‚àß Œ∫0 (g) = hŒ∏ ‚à™ {œÜ1 , œÜ2 }, DŒ∏‚à™{œÜ1 ,œÜ2 } i is a weak team.
Thus type-1 colleagues can potentially contribute to the
plans of their colleagues, while type-2 colleagues cannot.
Plans of type-2 colleagues can, however, influence each
other (for example due to conflicts on usage of shared resources), while type1-colleagues are capable of becoming
teammates dynamically during plan execution.
Humans in the loop. Instead of a general set of agents, we
define the set of agents Œ∏ in a super-agent as composition of
humans and robots Œ∏ = h(Œ∏)‚à™r(Œ∏) so that the domain model
of the super-agent is alsoS
a composition
of the human and
S
robot capabilities DŒ∏ = œÜ‚ààh(Œ∏) œÜ‚ààr(Œ∏) AœÜ = h(DŒ∏ ) ‚à™
r(DŒ∏ ). We denote the communication actions of the superagent as the subset c(DŒ∏ ) ‚äÜ DŒ∏ .

2
2.1

Metrics for Human Robot Interaction
Metrics for Human Robot Teams

We will now ground popular (Olsen Jr. and Goodrich 2003;
Steinfeld et al. 2006; Hoffman and Breazeal 2007;
Hoffman 2013) metrics for human-robot teams in our
current formulation.
Task Effectiveness These are the metrics that measure
the effectiveness of a team in completing its tasks.
‚Ä¢ Cost-based
Metrics - This simply measures the cost
P
‚àó
g‚ààŒ∫‚àí1 (Œò) C(œÄŒò ) of all the (optimal) plans a specific
team executes (for all the goals it has been assigned to).
‚Ä¢ Net Benefit Based Metrics - This is based on both
plan costs as well as the value of goals and is given by
P
‚àó
g‚ààŒ∫‚àí1 (Œò) R(g) ‚àí C(œÄŒò ).
‚Ä¢ Coverage Metrics - Coverage metrics for a particular
team determine the diversity of its capabilities in terms of
the number of goals it can achieve |Œ∫‚àí1 (Œò)|.
Team Effectiveness These measure the effectiveness of
(particularly human-robot) teaming in terms of communication overhead and smoothness of coordination.
‚Ä¢ Neglect Tolerance - This measures how long the
robots in a team Œò is able to perform well without human intervention. WeT can measure this as
‚àó
N T = max{|i ‚àí j| s.t. h(DŒ∏ ) œÜ‚ààŒ∏ œÄŒò
(œÜ)[i : j] = ‚àÖ}.
‚Ä¢ P
Interaction Time - This is given by IT
=
‚àó
|{i | c(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|, and measures the
time spent by a team Œò in communication.
‚Ä¢ Robot Attention Demand - Measures how much attention the robot is demanding and is given by IT IT
+N T .
‚Ä¢ Secondary Task Time - This measures the ‚Äúdistraction‚Äù to a team, and can be expressed as
time not spent on achieving a given goal g, i.e.
‚àó
‚àó
ST T = |{i | œÄŒò
[i] ¬∑¬∑= ‚àÖ ‚àß Œ¥ 0 (s, œÄŒò
) |= g}|.
‚Ä¢ Free Time - F T = 1 ‚àí RAD is a measure of the fraction
of time the humans are not interacting with the robot.
‚Ä¢ Human Attention Demand - HAD = F T ‚àí h(ST T )
‚àó
‚àó
where h(ST T ) = |{i | œÄŒò
[i]‚à©h(DŒ∏ ) ¬∑¬∑= ‚àÖ‚àßŒ¥ 0 (s, œÄŒò
) |=
‚àó
g}|/|œÄŒò | is the time humans spend on the secondary task.

‚Ä¢ Fan Out - This is a measure of the communication load
on the humans, and consequently the number of robots
that should participate in a human-robot team, and is
proportional to F O ‚àù |h(Œ∏)|/RAD.
‚Ä¢ Interaction Time - Measures how quickly and effectively
T)
interaction takes places as IT = N T (1‚àíST
.
ST T
‚Ä¢ Robot Idle Time - Captures inconsistency or irregularity
in coordination from the point of view of the robotic
agent, and can be measured as the amount of time the
‚àó
robots are idle, i.e. RIT = |{i | r(DŒ∏ ) ‚à© œÄŒò
[i] = ‚àÖ|.
‚Ä¢ Concurrent Activity - We can talk of concurrency within
a team as the time that humans and robots are working
‚àó
concurrently CA1 = |{i | r(DŒ∏ ) ‚à© h(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|
and also across teams as the maximum time teams are
operating concurrently CA2 = max{|{i | œÄŒò [i] 6=
‚àÖ ‚àß œÄŒò0 [i] 6= ‚àÖ}| ‚àÄŒò, Œò0 ‚àà R(Œ∫)}.
In measuring performance of agents in cohabitation, both
as teammates and colleagues, we would still like to reduce
interactions times and attentions demand, while simultaneously increasing neglect tolerance and concurrency. However, as we will see in Section 3, these metrics do not effectively capture all the implications of the interactions desired
in human-robot cohabitation. So the purpose of the rest of
our paper is to establish metrics that can measure the effective behavior of human-robot colleagues, and to see to
what extent they can capture desired behaviors of robotic
colleagues suggested in existing literature.

2.2

Metrics for Human Robot Colleagues

We will now propose new metrics that are useful for
measuring collegial interactions, see how they differ from
teaming metrics discussed so far, and then relate them to
existing work on human-robot cohabitation.
Task Effectiveness The measures for task effectiveness
must take into account that agents are not necessarily
involved in their assigned team task only.
‚Ä¢ Altruism - This is a measure of how useful it is for
a robotic agent r to showcase altruistic behavior in
assisting their human colleagues, and is given by the ratio
of the gain in utility by adding a robotic colleague to a
team Œò to the decrease in utilityP
of plans of the teams r is
‚àó
‚àó
‚àó
involved in |œÄŒò
‚àíœÄhŒ∏‚à™r,D
|/
Œò=hŒ∏,DŒ∏ i s.t. r‚ààŒ∏ ‚àÜ|œÄŒò |.
Œ∏‚à™r i
For such a dynamic coalition to be useful, r must be a
type-1 colleague to the agents Œ∏ ‚àà Œò.
‚Ä¢ Lateral Coverage - This measures how deviating from
optimal team compositions can achieve global good in
terms
of number of goals achieved by a team, LT =
P
‚àí1
(Tg )| ‚àí |Œ∫‚àí1 (Tgo )|]/|Œ∫‚àí1 (Tgo )|}
Tg =Œ∫(g),‚àÄg‚ààG {[|Œ∫
across all the teams that have been formed in E.
‚Ä¢ Social Good - Many times, while planning with humans
in the loop, cost optimal plans are not necessarily the
optimal plans in the social context. This is useful to
measure particularly when agents are interacting outside
teams, and the compromise in team utility is compensated
by the gain in
This can be
Pmutual utility of colleagues.
‚àó
expressed as g‚ààG {C(œÄŒ∫(g) ) ‚àí C(œÄŒ∫(g)
)}.

Interaction Effectiveness The team effectiveness measures need to be augmented with measures corresponding
to interactions among non-team members. While all these
metrics are relevant for robotic colleagues as well, they
become particularly important in human-robot interactions,
where information is often not readily sharable due to
higher cognitive mismatch, so as to reduce cognitive
demand/overload.
‚Ä¢ Interaction Time - In addition to Interaction Time for
human-robot teams, and measures derived from it, we
propose two separate components of interaction time for
general human-robot cohabitation scenarios.
- External Interaction Time - This is the time spent by
agents interacting with type-1 colleagues (EIT1 ).
- Extraneous Interaction Time - This is the time spent
by agents interacting with type-2 colleagues (EIT2 ).
‚Ä¢ Compliance - This refers to how much actions of an agent
disambiguate its intentions. Though relevant for both, this
becomes even more important in absence of teams, when
information pertaining to goals or plans are not necessarily sharable. Thus the intention should be to maximize
the probability P (GŒ∏ = g | s = Œ¥(œÄŒ∏ [1 : i], IŒ∏ )), Œ∫(g) =
hŒ∏, DŒ∏ i, ‚àÄg ‚àà G given any stage i of plan execution and
P (¬∑) is a generic goal recognition algorithm. This can be
relevant both in terms of disambiguating goals (Keren,
Gal, and Karpas 2014) or explaining plans given a goal
(Zhang, Zhuo, and Kambhampati 2015).
‚Ä¢ External Failure - This is the number of times optimal
plans fail when resources are contested among colleagues.
‚Ä¢ Stability - Of course with continuous interactions, team
formations change, so this gives a measure of stability of
the system as a whole. If teams Œ∫(g) = hŒ∏1 , DŒ∏1 i and
Œ∫(g) = hŒ∏2 , DŒ∏2 i achieves a P
goal g ‚àà G at two different
instances, then stability S = g‚ààG |Œ∏1 ‚à© Œ∏2 |/|Œ∏1 ||Œ∏2 |.

3

Discussion and Related Work

We will now investigate the usefulness of the proposed metrics in quantifying behavioral traits proposed in existing literature as desirable among cohabiting human and robots.
Human-Aware Planning. In (Koeckemann, Pecora, and
Karlsson 2014; Cirillo, Karlsson, and Saffiotti 2010) the authors talk of adapting robot plans to suit social norms (e.g.
not to vacuum a room while a human is asleep). Clearly,
this involves the robots departing from their preferred plans
to conform to human preferences. In such cases, involving
assistive robots, measures of Altruism and Social Good become particularly relevant, while it is also crucial to reduce
unwanted interactions (EIT1 + EIT2 ).
Planning with Resource Conflicts. In (Chakraborti et al.
2015b) the authors outline an approach for robots sharing
resources with humans to compute plans that minimize conflicts in resource usage. Thus, this line of work is aimed at
reducing External Failures, while simultaneously increasing
Social Good. Measures of Stability and Compliance become
relevant, to capture evolving beliefs and their consequences
on plans. Extraneous Interaction Time is also an important

measure, since additional communication is always a proxy
to minimizing coordination problems between colleagues.
Planning for Serendipity. In (Chakraborti et al. 2015a)
the authors propose a formulation for the robot to produce
positive exogenous events during the execution of the human‚Äôs plans, i.e. interventions which will be useful to the human regardless of whether he was expecting assistance from
the robot. This work particularly looks at planning for Altruism. Increasing Compliance in agent behavior can provide
better performance in this regard. Further, External Interaction is crucial in such cases for forming such impromptu
coalitions among colleagues.
Relation to Metrics in Human Factor Studies It is useful
to see an example of how the general formulation of metrics
we discussed so far are actually grounded in human factors
studies (Zhang et al. 2015) of scenarios that display some aspects of collegial interaction. The environment studied was
a disaster response scenario, involving an autonomous robot
that may or may not chose to proactively help the human.
The authors used External Interaction Time or EIT1 to measure the effectiveness of proactive support (how often the
proactive support resulted in further deliberation over goals),
while Lateral Coverage (in terms of number of people rescued) showed the effectiveness of proactive support. Further,
qualitative analysis on acceptance and usefulness of agents
that display proactive support are closely related to measures
such as Social Good and Altruism.
Work on Ad-hoc Coalition Formations Given the framework we have discussed thus far, the question is then, apart
from measuring performance, how we can use it to facilitate collegial interactions among agents. Especially relevant
in such scenarios are work on ad-hoc coalition formation
among agents sharing an environment but not necessarily
goals (Stone et al. 2010). In (Chakraborti et al. 2016) we
show how this framework may be used to cut down on prior
coordination while forming coalitions.

4

Conclusion and Future Work

In conclusion, we discussed interaction in human-robot societies involving multiple teams of humans and robots in the
capacity of teammates or as colleagues, provided a formal
framework for talking about various modes of cooperation,
and reviewed existing metrics and proposed new ones that
can capture these different modalities of teaming or collegial behavior. Finally we discussed how such metrics can be
useful in evaluating existing works in human-robot cohabitation. One line of future inquiry would be to see how such
quantitative metrics are complemented by qualitative feedback from human factor studies, to establish what the desired trade-offs are, in order to ensure well-informed design
of symbiotic systems involving humans and robots.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
[Aethon TUG ] Aethon TUG. Intralogistics automation platform for hospitals. http://www.aethon.com/.
[Chakraborti et al. 2015a] Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and
Kambhampati, S. 2015a. Planning for serendipity. In International Conference on Intelligent Robots and Systems.
[Chakraborti et al. 2015b] Chakraborti, T.; Zhang, Y.; Smith,
D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot cohabitation. In ICAPS Workshop on Planning and Robotics.
[Chakraborti et al. 2016] Chakraborti, T.; Dondeti, V.;
Meduri, V. V.; and Kambhampati, S. 2016. A game theoretic approach to ad-hoc coalition formation in human-robot
societies. In AAAI Workshop on Multi-Agent Interaction
without Prior Coordination.
[Cirillo, Karlsson, and Saffiotti 2010] Cirillo, M.; Karlsson,
L.; and Saffiotti, A. 2010. Human-aware task planning:
An application to mobile robots. ACM Trans. Intell. Syst.
Technol. 1(2):15:1‚Äì15:26.
[Double ] Double. The ultimate tool for telecommuting.
http://www.doublerobotics.com/.
[Hoffman and Breazeal 2007] Hoffman, G., and Breazeal, C.
2007. Effects of anticipatory action on human-robot teamwork: Efficiency, fluency, and perception of team. In
Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on, 1‚Äì8.
[Hoffman 2013] Hoffman, G. 2013. Evaluating fluency in
human-robot collaboration. In Robotics: Science and Systems (RSS) Workshop on Human-Robot Collaboration.
[iRobot Ava ] iRobot Ava.
Video collaboration robot.
http://www.irobot.com/For-Business.aspx.
[Keren, Gal, and Karpas 2014] Keren, S.; Gal, A.; and
Karpas, E. 2014. Goal recognition design. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS 2014, Portsmouth,
New Hampshire, USA, June 21-26, 2014.
[Knightscope ] Knightscope. Autonomous data machines.
http://knightscope.com/about.html.
[Koeckemann, Pecora, and Karlsson 2014] Koeckemann,
U.; Pecora, F.; and Karlsson, L. 2014. Grandpa hates
robots - interaction constraints for planning in inhabited
environments. In Proc. AAAI-2010.
[Kuderer et al. 2012] Kuderer, M.; Kretzschmar, H.; Sprunk,
C.; and Burgard, W. 2012. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings
of Robotics: Science and Systems.
[Mcdermott et al. 1998] Mcdermott, D.; Ghallab, M.; Howe,
A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and
Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control,.
[Olsen Jr. and Goodrich 2003] Olsen Jr., D., and Goodrich,
M. A. 2003. Metrics for evaluating human-robot interactions. In Performance Metrics for Intelligent Systems.

[Rosenthal, Biswas, and Veloso 2010] Rosenthal,
S.;
Biswas, J.; and Veloso, M. 2010. An effective personal
mobile robot agent through symbiotic human-robot interaction. In Proceedings of the 9th International Conference
on Autonomous Agents and Multiagent Systems: Volume 1
- Volume 1, AAMAS ‚Äô10, 915‚Äì922. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.
[Shoham and Tennenholtz 1992] Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for
artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì
281. AAAI Press.
[Sisbot et al. 2007] Sisbot, E.; Marin-Urias, L.; Alami, R.;
and Simeon, T. 2007. A human aware mobile robot motion
planner. Robotics, IEEE Transactions on 23(5):874‚Äì883.
[Steinfeld et al. 2006] Steinfeld, A.; Fong, T.; Kaber, D.;
Lewis, M.; Scholtz, J.; Schultz, A.; and Goodrich, M. 2006.
Common metrics for human-robot interaction. In Proceedings of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, 33‚Äì40.
[Stone et al. 2010] Stone, P.; Kaminka, G. A.; Kraus, S.; and
Rosenschein, J. S. 2010. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. In Proceedings of
the Twenty-Fourth Conference on Artificial Intelligence.
[Talamadupula et al. 2014] Talamadupula, K.; Briggs, G.;
Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014.
Coordination in human-robot teams using mental modeling
and plan recognition. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2957‚Äì2962.
[Tambe 1997] Tambe, M. 1997. Towards flexible teamwork.
J. Artif. Int. Res. 7(1):83‚Äì124.
[Zhang and Kambhampati 2014] Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in
multi-agent planning. In ICAPS Workshop on Distributed
Multi-Agent Planning (DMAP).
[Zhang et al. 2015] Zhang, Y.; Narayanan, V.; Chakraborti,
T.; and Kambhampati, S. 2015. A human factors analysis
of proactive support in human-robot teaming. In IEEE/RSJ
International Conference on Intelligent Robots and Systems.
[Zhang, Zhuo, and Kambhampati 2015] Zhang, Y.; Zhuo,
H. H.; and Kambhampati, S. 2015. Plan explainability and
predictability for cobots. CoRR abs/1511.08158.

Capability Models and Their Applications in Planning
Yu Zhang
Dept. of Computer Science Arizona State University Tempe, AZ

Sarath Sreedharan
Dept. of Computer Science Arizona State University Tempe, AZ

Subbarao Kambhampati
Dept. of Computer Science Arizona State University Tempe, AZ

yzhan442@asu.edu

ssreedh3@asu.edu

rao@asu.edu

ABSTRACT
One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often not provided, and hence must be learned from plan execution traces. As a result, these models of other agents are inherently partial and incomplete. Most existing agent models are based on action modeling and do not naturally allow for incompleteness. In this paper, we introduce a new and inherently incomplete modeling approach based on the representation of capabilities, which has several unique advantages. First, we show that the structures of capability models can be learned or easily specified, and both model structure and parameter learning are robust to high degrees of incompleteness in plan traces (e.g., with only start and end states partially observed). Furthermore, parameter learning can be performed efficiently online via Bayesian learning. While high degrees of incompleteness in plan traces presents learning challenges for traditional (complete) models, capability models can still learn to extract useful information. As a result, capability models are useful in applications in which traditional models are difficult to obtain, or models must be learned from incomplete plan traces, e.g., robots learning human models from observations and interactions. Furthermore, we discuss using capability models for single agent planning, and then extend it to multi-agent planning (with each agent modeled separately by a capability model), in which the capability models of agents are used by a centralized planner. The limitation, however, is that the synthesized "plans" (called c-plans) are incomplete, i.e., there may or may not be a complete plan for a c-plan. This is, however, unavoidable for planning using partial and incomplete models (e.g., considering planning using action models learned from partial and noisy plan traces).

1.

INTRODUCTION

Categories and Subject Descriptors
I.2.11 [Multiagent systems]; I.2.6 [Knowledge acquisition]; I.2.8 [Plan execution, formation, and generation]

Keywords
Capability models; Agent theories and models; Teamwork in humanagent mixed networks

Appears in: Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015), Bordini, Elkind, Weiss, Yolum (eds.), May 4≠8, 2015, Istanbul, Turkey. Copyright c 2015, International Foundation for Autonomous Agents
and Multiagent Systems (www.ifaamas.org). All rights reserved.

One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of others. These models can be used by a centralized planner (e.g., on a robot) or via a distributed planning process to perform task planning and allocation, or by the agents themselves to reduce communication and collaboration efforts. In many applications, these models of other agents are not provided and hence must be learned. As a result, these models are going to be inherently partial and incomplete. Thus far, most traditional agent models are based on action modeling (e.g., [18, 7]). These models are not designed with partial information in mind and hence are complete in nature. In this paper, we introduce a new and inherently incomplete modeling approach based on the representation of capabilities. We represent a capability as the ability to achieve a partial state given another partial state. A capability can be fulfilled (or realized) by any action sequence (or plan) that can implement a transition between the two partial states. Each such action sequence is called an operation in this paper. A capability model can encode all possible capabilities for an agent in a given domain, as well as capture the probabilities of the existence of an operation to fulfill these capabilities (to implement the associated transitions). These probabilities determine which capabilities are more likely to be used in planning. Compared to traditional agent models which are complete in nature, capability models have their unique benefits and limitations. In this aspect, capability models should not be considered as a competitor to complete models. Instead, they are useful when complete models are difficult to obtain or only partial and incomplete information can be retrieved to learn the models. This is often true when humans must be modeled. The representation of a capability model is a generalization of a two time slice dynamic Bayesian network (2-TBN). While a 2TBN is often used to represent a single action (e.g., [19]), a capability model can encode all possible capabilities for an agent in a given domain. In a capability model, each node in the first time slice (also called a fact node) represents a variable that is used in the specification of the initial world state. For each fact node, there is also a corresponding node in the second time slice, which represents the fact node in the eventual state (i.e., after applying a capability). These corresponding nodes are called eventual nodes or e-nodes. The state specified by the fact nodes is referred to as the initial state, and the state specified by the e-nodes is referred to as the eventual state. The edges between the nodes within the initial and eventual states, respectively, encode the correlations between the variables at the same time instance (i.e., variables in synchrony). The edges from the fact nodes to e-nodes encode causal relationships. Both types of edges can be learned (e.g., using learning techniques in [24, 25] for causal relationships). In the case that no prior infor-

Figure 1: Capability model (as a generalized 2-TBN) for a human delivery agent (denoted as AG) in our motivating example with 6 fact node and e-node pairs. This model can encode all possible capabilities of the agent in this domain. Each fact node corresponds to a variable in the specification of the initial state. e-nodes are labeled with a dot on top of the variable. Edges from the fact nodes to the corresponding e-nodes are simplified for a cleaner representation. Any partial initial state coupled with any partial eventual state may imply a capability.

situation can naturally occur in human-robot teaming scenarios, in which human models need to be learned and the robot models are given. Similarly, the synthesized multi-agent c-plans are incomplete (unless all plan steps use robot actions). c-plans are useful when we have to plan using partial and incomplete models (e.g., considering planning using action models learned from partial and noisy plan traces), since they inform the user how likely complete plans can be realized by following their "guidelines". The rest of the paper is organized as follows. First, we provide a motivating example in Section 2. In Section 3, we discuss capability models in detail. We discuss how to use capability models in planning in Sections 4 and 5. The relationships between capability models and other existing approaches to modeling the dynamics of agents are discussed as related work in Section 6. We conclude afterwards.

2.

MOTIVATING EXAMPLE

mation is provided, the safest approach is to not assume any independence. Consequently, we can specify a total ordering between the nodes within the initial and final states, and connect a node at a given level to all nodes at lower levels, as well as every fact node to every e-node. After model construction, parameter learning of a capability model can be performed efficiently online via Bayesian learning. One of the unique advantages of capability models is that learning for both model structure and parameters is robust to high degrees of incompleteness in plan execution traces. This incompleteness occurs commonly. For example, the execution observer may not be constantly monitoring the executing agent, and the executing agent may not always report the full traces [26]. Capability models can be learned even when only the initial and final states are partially observed in plan traces for training. While high degrees of incompleteness in plan traces presents learning challenges for traditional models, capability models can still learn to extract useful information out of them, e.g., probabilities of the existence of an operation to achieve certain eventual states given certain initial states. Furthermore, compared to traditional models for planning, capability models only suggest transitions between partial states (as capabilities), along with the probabilities that specify how likely such transitions can be implemented. Hence, a "plan" synthesized with capability models is incomplete, and we refer to such a plan as a c-plan (i.e., involving the application of capabilities). More specifically, we show that "planning" with capability models occurs in the belief space, and is performed via Bayesian inference. At any planning step, the current planning state is a belief state. Applying a capability to this belief state requires the planner to update this state using Bayesian inference on the capability model. After the discussion of single agent planning, we then extend to multi-agent planning.1 In such cases, we can consider planning as assigning subtasks to agents without understanding precisely how these subtasks are to be handled. Moreover, multi-agent planning can be performed when part of the agents are modeled by capability models, and the other agents are modeled by complete models. This
1 In this paper, we consider sequential multi-agent plans; synchronous or joint actions of agents are not considered.

We start with a motivating example for capability models. In this example, we have a set of human delivery agents, and the tasks involve delivering packages to their destinations. However, there are a few complications here. An agent may be able to carry a package if this agent is strong enough. While an agent can use a trolley to load a package, this agent may not remember to bring a trolley initially. An agent can visit an equipment rental office to rent a trolley, if this agent remembers to bring money for the rental. A trolley increases the chance of an agent being able to deliver a package (i.e., how likely the package is deliverable by this agent). Figure 1 presents the capability model for a human delivery agent, which is a generalized 2-TBN that can encode all possible capabilities for the agent in this domain. Similar to 2-TBN, fact nodes (e-nodes) in a capability model are in synchrony in the initial (eventual) state. Note that any partial initial state coupled with any partial eventual state may imply a capability. In this example, the correlations between the variables are clear. For example, whether an agent can carry a package is dependent on whether the agent is strong; whether an agent can deliver a package may be influenced by whether the agent has a trolley. These correlations are represented as edges between the variables within the initial and eventual states, respectively and identically. Assuming that no prior information is provided for the causal relationships, we connect every fact node with every e-node (denoted as a single edge in Figure 1). After obtaining the model structure, to perform parameter learning for the capability model of an agent, this agent can be given delivery tasks spanning a period of time for training purpose. However, the only observations that a manager has access to may be the number of packages that have been successfully delivered by this agent during this period. While this presents significant challenges for learning the parameters of traditional complete models, the useful information for the manager is already encoded: the probability that this agent can deliver a package. Capability models can learn this information from the incomplete traces. Now, suppose that the manager also has a set of robots (with action models) to use that can fetch items for the delivery agents. Since it is observed that the presence of a trolley increases the probability of successful delivery based on the capability models, the manager can make a multi-agent c-plan, which ensures that the robots deliver a trolley at least to some of the delivery agents. This also illustrates that it is beneficial to combine capability models with other complete models (i.e., action models) when they are available (e.g., for robots in this example).

3.

CAPABILITY MODEL

following probability distribution:
T

In our settings, the environment includes a set of agents  working inside it. For each agent, for simplicity, we assume that the state of the world and this agent is specified by a set of boolean variables X (  ). This implies that an agent can only interact with other agents through variables that pertain to the world state. More specifically, for all Xi  X , Xi has domain D(Xi ) = {true, false}. To specify partial state, we augment the domains of variables to include an unknown or unspecified value as in [1]. We write D+ (Xi ) = D(Xi )  {u}. Hence, the (partial) state space is denoted as S = D+ (X1 ) ◊ D+ (X2 ) ◊ ... ◊ D+ (XN ), in which N = |  X |.

 ) = P (X , X
0

  , t) dt P (X , X

(1)

in which T represents the maximum length of any operation (i.e.,  number of actions) for .2 X represents the initial state and X  represents the eventual state. Furthermore, P (X , t| X ) is the   in exact time t given probability of any operation resulting in X X . Hence, the probability that is associated with a capability sI  sE (i.e., P (sI  sE )) encodes:   = sE | X = sI ) = P (X
t

  = sE , t | X = sI ) dt (2) P (X

3.1

Capability

First, we formally define capability for an agent   . The state space of agent  is denoted as S . D EFINITION 1 (C APABILITY ). Given an agent , a capability specified as a mapping S ◊ S  [0, 1], is an assertion about the probability of the existence of complete plans (for ) that connect an initial state (i.e., the first component on the left hand side of the mapping) to an eventual state (i.e., the second component). A capability is also denoted as sI  sE (i.e., the initial state  the eventual state) when we do not need to reference the associated probability value. The probability value is denoted as P (sI  sE ), which we will show later is computed based on the capability model via Bayesian inference. There are a few notes about Definition 1. First, both sI and sE can be partial states: the variables that have the value u are assumed to be unknown (in the initial state) and unspecified (in the eventual state). In this paper, we refer to a complete plan that fulfills (or realizes) a capability as an operation, which is going to be decided and implemented by the executing agent during execution. Although capabilities seem to be similar to high-level actions in HTN, the semantics is different from angelic uncertainty in HTN [12]. While the existence of a concretization is ensured in HTN planning via reduction schemas, it is only known with some level of certainty that a concretization (i.e., operation) exists for a capability in a capability model, and the capability model does not provide specifics for such a concretization. More discussions on this are provided in Section 6 when we discuss the relationships between capability models and existing approaches to modeling agent dynamics. Capability models also address the qualification and ramification problems, which are assumed away in STRIPS planning (and planning with many complete models). More specifically, an operation for a capability sI  sE may be dependent on variables with unknown values in sI , and updating variables with unspecified values in sE . This is a unique characterization of capability and critical for learning capability models with incomplete observations. For example, a capability may specify that given coffee beans, an agent can make a cup of coffee. Thus, we have a capability {Has coffee beans = true}  {Has coffee = true}. An operation for this capability to make a coffee may be dependent on the availability of water initially, which is not specified in the capability. Similarly, this operation may negate the fact that the kitchen is clean, which is not specified in the capability either. Note that a capability may be fulfilled by operations with different specifications of the initial and eventual states, as long as these specifications all satisfy the specification of this capability. A capability model of an agent is capable of encoding all capabilities of this agent given a domain; it is designed to encode the

We construct the capability model of an agent as a Bayesian network. As an inherently incomplete model, it not only allows the initial and eventual states to be partially specified for any capability (and hence the fulfilling operations) that it encodes, but also allows the correlations between the variables within the initial and eventual states, as well as the causal relationships between them to be partially specified (e.g., when learning from plan traces). However, there are certain implications in this (e.g., the modeling can lose some information), which we will discuss in Section 3.4. Along the line of the qualification and ramification problems, a capability model also allows certain variables to be excluded completely from the network (i.e., related variables that are not captured in X ) due to incomplete knowledge. For example, whether an agent can drive a car (with a manual transmission) to a goal location is dependent on whether the agent can drive a manual car, even through the agent has a driver license. In this case, the ability to drive a manual car may have been ignored when creating the model.

3.2

Model Construction

We construct the capability model of each agent as an augmented Bayesian network [14]. Any partial initial state coupled with any partial eventual state may imply a capability; the probability that a capability actually exists (i.e., it can be fulfilled by an operation) is computed via a Bayesian inference in the network. We use augmented Bayesian network since it allows prior beliefs of conditional relative frequencies to be specified before observations are made, as well as enables us to adjust how fast these beliefs should change. D EFINITION 2. An augmented Bayesian network (ABN) (G, F, ) is a Bayesian network with the following specifications: ∑ A DAG G = (V, E ), where V is a set of random variables, V = {V1 , V2 , ..., Vn }. ∑ F is a set of auxiliary parent variables for V . ∑ Vi  V , an auxiliary parent variable Fi  F of Vi , and a density function i associated with Fi . Each Fi is a root and it is only connected to Vi . ∑ Vi  V , for all values pai of the parents P Ai  V of Vi , and for all values fi of Fi , a probability distribution P (Vi |pai , fi ). A capability model of an agent  is then defined as follows: D EFINITION 3 (C APABILITY M ODEL ). A capability model of an agent , as a binomial ABN (G , F, ), has the following specifications:
2

We assume in this paper that time is discretized.

 . ∑ V = X  X ∑ Vi  V , the domain of Vi is D(Vi ) = {true, false}. ∑ Vi  V , Fi = {Fi1 , Fi2 , ...}, and each Fij is a root and has a density function ij (fij ) (0  fij  1). (For each value paij of the parents P Ai , there is an associated variable Fij .) ∑ Vi  V , P (Vi = true|paij , fi1 , ...fij , ...) = fij . in which j in paij indexes into the values of P Ai . j in fij indexes into the variables in Fi . Note that defining partial states (i.e., allowing variables to assume the value u in a state) is used to more conveniently specify the distribution in Equation 1. Variables in the capability model do not need to expand their domains to include u. For edge construction, we can learn the correlations and causal relationships from plan traces. Note that the correlations between variables must not form loops; otherwise, they need to be broken randomly. When no training information is provided, we can specify a total ordering between the nodes within the initial and final states, and connect a node at a given level to all nodes at lower levels, as well as every fact node to every e-node. Denote the set of edges as E . We then have constructed the capability model G = (V , E ) for . Figure 1 provides a simple example of a capability model.

it is the result of a plan (i.e., operation) execution. Henceforth, when we refer to plan traces, we always intend to mean partial plan traces, unless otherwise specified. When more than two states are observed in a plan trace, it can be considered as a set of traces, with each pair of contiguous states as a separate trace. When the states are partially observed in a plan trace, it can be considered as a set of compatible traces with complete state observations.3 For simplicity, we assume in the following that the plan execution traces used in learning have complete state observations. We denote this set of traces as D. To learn the parameters of a capability model, a common way is to model Fij using a beta distribution (i.e., as its density function ). Denote the parameters for the beta distribution of Fij as aij and bij . Then, we have: aij (3) P (Xi = true|paij ) = aij + bij Suppose that the initial values or the current values for aij and bij are given. The remaining task is to update aij and bij from the given traces. Given the training set D, we can now follow Bayesian inference to update the parameters of Fij as follows: Initially or currently, (fij ) = beta(fij ; aij , bij ) After observing new training examples D, we have: (fij |D) = beta(fij ; aij + sij , bij + tij ) (5) (4)

3.3

Parameter Learning

In this section, we describe how the model parameters can be learned. The parameter learning of a capability model is performed online through Bayesian learning. The initial model parameters can be computed from existing plan traces by learning the density functions (i.e., fij ) in Definition 3. These parameters can then be updated online as more traces are collected (i.e., as the agent interacting with the environment). Plan execution traces can be collected each time that a plan (or a sequence of actions) is executed, whether succeeds or fails. D EFINITION 4 (C OMPLETE P LAN T RACE ). A complete plan trace is a continuous sequence of state observations over time, de  noted as T = s 1 , s2 , ..., sL , in which L is the length of the plan  and si denotes a complete state (i.e., s i  D (X1 ) ◊ D (X2 ) ◊ ... ◊ D(XN )). However, in real-world situations, plan traces may be incomplete. The incompleteness can come from two aspects. First, the observed state may be partial. Second, the observations may not be continuous. Hence, we are going to have partial plan traces. D EFINITION 5 (PARTIAL P LAN T RACE ). A partial plan trace is a discontinuous sequence of partial state observations over time, denoted as T = si , si+k1 , si+k2 , ... , in which i denotes the time step in the complete plan and si denotes a partial state (i.e., si  D+ (X1 ) ◊ D+ (X2 ) ◊ ... ◊ D+ (XN )). Note that the only assumption that is made in Definition 5 is that at least two different partial states must be observed during the plan execution. This means that even the start and end states of a plan execution do not necessarily have to be observed or partially observed, which is especially useful in real-world situations where a plan trace may only be a few samplings of (partial) observations during a plan execution. Note also that since the observations are in discontinuous time steps, the transition between contiguous state observations is not necessarily the result of a single action. Instead,

in which sij is the number of times for which Xi is true while P Ai assuming the value of paij , and tij is the number in which it equals Xi is false while P Ai assuming the value of paij .

3.4

Implications

In this section, we discuss several implications of capability models with a simple example. We first investigate how information can be lost during learning when the correlations or causal relationships among the variables are only partially captured. This can occur, for example, when model structure is learned. In this example, we have two blocks A, B , and a table. Both blocks can be placed on the table or on each other. The capability model for an agent is specified in Figure 2. Initially, assuming that we do not have any knowledge of the domain, the density functions can be specified as beta distributions with a = b. In Figure 2, we use a = b = 1 for all distributions. Suppose that we observe the following plan trace, which can be the result of executing a single action that places A on B : s1 : OnT able(A)  OnT able(B )  ¨On(A, B )  ¨On(B, A) s2 : ¨OnT able(A)  OnT able(B )  On(A, B )  ¨On(B, A) Based on the learning rules in Equation (5), we can update the beta distributions accordingly. For example, the beta distribution  3 (i.e., On(A, B ) in the eventual state) is updated to beta(X1 = for X  4 = false, ...; 2, 1). This means that if both A true, X4 = true, X and B are on the table, it becomes more likely that a capability exists for making On(A, B ) = true. This is understandable since an action that places A on B would achieve On(A, B ) = true in such cases. For actions with uncertainties (i.e., when using a robotic arm to perform the placing action), this beta distribution would converge to the success rate as more experiences are obtained.  2 is Meanwhile, we also have that the beta distribution for X  4 = false, X 3 = updated to beta(X1 = true, X3 = false, X There is no need to expand such traces into sets of traces with complete state observations for learning, since it can be equivalently considered using arithmetic operations.
3

that satisfies sE P (s) =

s in the resulting belief state as follows:   = s|X = s ) P (X P (s  s) =    = sE |X = s ) P (s  sE ) P (X (6)

For any state s that does not satisfy sE s, we have P (s) = 0. Denote S in b(S ) as S = {s|sE s and s is a complete state}. Clearly, we have: P (s) = 1
sS

(7)

Figure 2: Capability model for an agent in a simple domain with four variables. OnT able(A) means that object A is on the table. On(A, B ) means that object A is on B . The correlations that correspond to the light blue arrows distinguish between two scenarios: one with proper model structure and one without (i.e., when certain correlations are missing). For clarity, we only specify the density functions of the variables in the initial state. We also show the two sets of density functions for the augmented variables for X2 for the two different scenarios, respectively. true, ...; 1, 2). This means that if A is on B in the eventual state, it becomes less likely that B can also be on A in the eventual state. This is intuitive since we know that achieving On(A, B ) = true and On(B, A) = true at the same time is impossible. In this way, capability models can reinforce the correlations between the variables as experiences are observed. The implication is that the edges (capturing the correlations) between these variables must be present; otherwise, information may be lost as described above. If the correlations are not fully captured by the model, for example, when the light blue arrows in Figure 2 are not present, the beta dis 2 would not be updated as above, since X  3 would no tribution of X  2 . A similar implication also applies longer be a parent node of X to causal relationships. When environment changes, previous knowledge that is learned by the model may no longer be applicable. However, as the parameters grow, the learned model can be reluctant to adapt to the new environment. This issue can be alleviated by performing normalizations occasionally (i.e., dividing all a and b by a constant in the beta distributions), or by weighting previous experiences with a discounting factor.

Since there can be an exponential number of complete states in a belief state, depending on how many variables are assigned to u, we can use a sampling approach (e.g., Monte Carlo sampling) to keep a set of complete states to represent b(S ). We denote the belief state after sampling as ^ b (S ). When applying a capability sI  sE to a given belief state ^ b(S ), for each complete state in S , we can perform sampling based on Equation (6), which returns a set of complete states with weights after applying the capability. We can then perform resampling on the computed sets of states for all states in S to compute the new belief state ^ b(S ). In this way, we can connect different capabilities of an agent to create c-plans, which are plans in which capabilities are involved. Next, we formally define a planning problem for a single agent with a capability model. D EFINITION 6 (S INGLE AGENT P LANNING P ROBLEM ). A single agent planning problem with capability models is a tuple , b(I ), G,  , in which b(I ) is the initial belief state, G is the set of goal variables, and  is a real value in (0, 1]. The capability model of the agent is G = (V , E ). When we write  instead of  in the problem, it indicates a variance of the problem that needs to maximize . D EFINITION 7 (S INGLE AGENT C-P LAN ). A single agent cplan for a problem , b(I ), G,  is a sequence of application of capabilities, such that the sum of the weights of the complete states in the belief state which include the goal variables (i.e., G s), is no less than  (or maximized for  ) after the application. The synthesized single agent "plan" (i.e., a c-plan) is incomplete: it does not specify which operation fulfills each capability. In fact, it only informs the user how likely there exists such an operation. A single agent c-plan can be considered to provide likely landmarks for the agent to follow. For example, in Figure 2, suppose that the initial state is sI : On(A, B )  ¨On(B, A) and the goal is sE : ¨On(A, B )  On(B, A) A c-plan created with a capability model may include an intermediate state in the form of sI  sin  sE ,4 in which sin can include, e.g., OnT able(A) = true and OnT able(B ) = true, such that the probability of the existence of a sequence of operations to fulfill the c-plan may be increased. Another example is our motivating example (Figure 1) in which having has_trolley (AG) = true as an intermediate state helps delivery when has_trolley (AG) is false at the beginning. Although a single agent c-plan may seem to be less useful than a complete plan synthesized with action models, it is unavoidable
4 Note that P (sI  sE ) is not equivalent to P (sI  sin ) ∑ P (sin  sE ).

4.

USING CAPABILITY MODELS IN PLANNING

Capability models described in the previous section allow us to capture the capabilities of agents. In this section, we discuss the application of capability models in single agent planning (i.e., with an agent  modeled by a capability model). We extend the discussion to multi-agent planing in the next section. Generally, planning with capability models occurs in the belief space, as with POMDPs [10].

4.1

Single Agent Planning

First, note that applying a capability sI  sE of agent  to a complete state s results in a belief state b(S ) as long as sI s (otherwise, this capability cannot be applied), in which sI s denotes that all the variables that are not assigned to u in sI have the same values as those in s . After applying the capability, assuming successfully, we can compute the probability weight of a state s

when we have to plan with incomplete knowledge (e.g., incomplete action models). This is arguably more common in multi-agent systems, in which the planner needs to reason about likely plans for agents even when it does not have complete information about these agents. A specific application in such cases is when we need to perform task allocation, in which it is useful for the system to specify subtasks or state requirements for agents that are likely to achieve them without understanding how the agents achieve them.

4.2

Planning Heuristic

Given a single agent planning problem , b(I ), G,  , besides achieving the goal variables, planning should also aim to reduce the cost of the c-plan (i.e., probability of success for the sequence of application of capabilities in the c-plan). ASSUMPTION: To create a heuristic, we make the following assumption ≠ capabilities do not have variables with false values in sI . With this assumption, we have the following monotonicity properties hold: P (sI  sE )  P (sI  sE )(T (sI )  T (sI )  F (sI )  F (sI )) (8) in which T , F are used as operators on a set of variables to denote the set of variables with true and false values, respectively. Equation (8) implies it is always easier to achieve the desired state with more true-value variables and less false-value variables in the initial state; and P (sI  sE )  P (sI  sE )(T (sE )  T (sE )F (sE )  F (sE )) (9) which implies that it is always more difficult to achieve the specified values for more variables in the eventual state. HEURISTIC: We use A to perform the planning. At any time the current planning state is a belief state ^ b(S ) (i.e., b(S ) after sampling); there is also a sequence of application of capabilities (i.e., a c-plan prefix), denoted as  , to reach this belief state from the initial state. We compute the heuristic value for ^ b(S ) (i.e., f (^ b(S )) = g (^ b(S )) + h(^ b(S ))) as follows. First, we compute g (^ b(S )) as the sum of the negative logarithms of the associated probabilities of capabilities in  . To compute h(^ b(S )), we need to first compute h(s) for each complete state s  S . To compute an admissible h(s) value, we denote the set of goal variables that are currently false in s as Gs . Then, we compute h(s) as follows: h(s) = argmax - log P (s¨v  s{v = true})
v Gs ,s¨v

Figure 3: Illustration of solving a single agent planning problem with a capability model. The topology of the capability model used is shown in the top part of the figure.

Lemma 1 implies that the A search using the heuristic in Equation (11) would continue to improve  given sufficient time. Hence, the heuristic should be used as an anytime heuristic and stop when a desired value of  is obtained or the increment is below a threshold. Also, approximation solutions should be considered in future work to scale to large networks.

4.3

Evaluation

(10)

in which s¨v denotes a complete state with only v as false, and s{v = true} denotes the state of s after making v true. Finally, we compute h(^ b(S )) as: h(^ b(S )) =
sS

P (s) ∑ h(s)

(11)

L EMMA 1. The heuristic given in Eq. (11) is admissible for finding a c-plan that maximizes  (i.e., with  ), given that ^ b (S ) accurately represents b(S ). P ROOF. We need to prove that h(^ b(S )) is not an over-estimate of the cost for S to reach the goal state G while maximizing . First, note that we can always increase  by trying to move a nongoal state (in the current planning state) to a goal state (i.e., G s). Furthermore, for each s  S , given the monotonicity properties, we know that at least h(s) cost must be incurred to satisfy G. Hence, the conclusion holds.

We provide a preliminary evaluation of single agent planning with a capability model. We build this evaluation based on the blocksworld domain. First, we use 20 problem instances with 3 blocks5 and generate a complete plan for each instance. Then, we randomly remove 1 - 5 actions from each complete plan to simulate partial plan traces. The capability model for this domain contains 12 variables and we ignore the holding and handempty predicates to simulate partial state observations. We manually construct the correlations between the variables in the initial and eventual states. For example, On(A, B ) is connected with On(B, A) since they are clearly conflicting. For causal relationships, we connect every node in the initial state to every node in the eventual state. After learning the parameters of this capability model based on the partial plan traces, we apply the capability model to solve a problem as shown in Figure 3. The topology of the capability model constructed is shown in the top part of the figure, which is also the model used in Figure 4. Since we have connected every fact node with every e-node in this case, the model appears to be quite complex. Initially, we have On(B 3, B 2), On(B 2, B 1), and OnT able(B 1) (a complete state), and the goal is to achieve On(B 2, B 3). We can see in Figure 3 that I in b(I ) contains a single complete state. The c-plan involves the application of two different capabilities. For illustration purpose, we only show two possible states (in the belief state) after applying the first capability, For one of the two states, we then show two possible states after applying the second capability. Note that each arrow represents a sequence of actions in the original action model. Also, the possible states must be compatible with the specifications of the eventual states in the capabilities. We see some interesting capabilities being
5 It does not have to be three blocks but we assume only three blocks to simplify the implementation.

learned. For example, the first capability is to pull out a block that is between two other blocks, and place it somewhere else.

5.

MULTI-AGENT PLANNING

In this section, we extend our discussion from single agent planning to multi-agent planning. In particular, we discuss how capability models can be used along with other complete models (i.e., action models) by a centralized planner to synthesize c-plans. The settings are similar to those in our motivating example. In particular, we refer to multi-agent planning with mixed models as MAPMM. This formulation is useful in applications in which both human and robotic agents are involved. While robotic agents are programmed and hence have complete models, the models of human agents must be learned. Hence, we use capability models to model human agents and assume that the models for the human agents are already learned (e.g., by the robots) for planning. For robotic agents, we assume STRIPS action model R, O , in which R is a set of predicates with typed variables, O is a set of STRIPS operators. Each operator o  O is associated with a set of preconditions P re(o)  R, add effects Add(o)  R and delete effects Del(o)  R. D EFINITION 8. Given a set of robots R = {r}, a set of human agents  = {}, and a set of typed objects O, a multiagent planning problem with mixed models is given by a tuple  = , R, b(I ), G,  , where: ∑ Each r  R is associated with a set of actions A(r) that are instantiated from O and O, which r  R can perform; each action may not always succeed when executed and hence is associated with a cost. ∑ Each    is associated with a capability model G =   . X  X , in which X V , E , in which V = X  X represents the state variables of the world and agent  and X represents the joint set of state variables of all agents. Note that the grounded predicates (which are variables) for robots that are instantiated from R and O also belong to X . Human and robotic agents interact through the shared variables. L EMMA 2. The MAP-MM problem is at least PSPACE-complete. P ROOF. We only need to prove the result for one of the extreme cases: when there are only robotic agents. The problem then essentially becomes a multi-agent planning problem, which is more general than the classical planning problem, which is known to be PSPACE-complete.

Figure 4: Illustration of solving a MAP-MM problem with a robotic and a human agent, in which the robotic agent has an action model and the human agent has a capability model that is assumed to be learned by the robotic agent. If we have chosen an action a, for any s  S that satisfies T (P re(a))  T (s), the complete state s after applying a becomes s , such that T (s ) = (T (s)  Add(a)) \ Del(a), and F (s ) = (F (s)  Del(a)) \ Add(a). The probability weight of s in the new belief state after applying a does not change. For states in S that do not satisfy T (P re(a))  T (s), we assume that the application of a does not change anything. In this way, we can construct the new belief state after applying this action. If we have chosen a capability in the form of sI  sE from a human agent , for any s  S that satisfies T (sI )  T (s), we can use follow discussion in Section 4.1 to compute the new belief state. Similarly, for states in S that do not satisfy T (sI )  T (s), we assume that the application of this capability does not change anything. With the new belief state, we can continue the state expansion process to expand the c-plan further.

5.2

Planning Heuristic for MAP-MM

5.1

State Expansion for MAP-MM

First, we make the same assumption as we made in single agent planning, such that the monotonicity properties still hold. Given the current planning state ^ b(S ) (i.e., sampled from b(S )), for each s  S , a centralized planner can expand it in the next step using the following two options in MAP-MM: ∑ Choose a robot r and an action a  A(r) such that at least one of the complete states s in S satisfies T (P re(a))  T (s). ∑ Choose a capability on human agent  with the specification sI  sE , such that at least one of the states s in S satisfies T (sI )  T (s).

In this section, we discuss a planning heuristic that informs us which state should be chosen to expand at any time. We can adapt the heuristic in Equation (11) to address MAP-MM. Given the current planning state ^ b(S ), we need to compute h(^ b(S )). For each s  S , there are three cases: 1) If only capabilities are used afterwards, h(s) can be computed as in Equation (11), except that all capability models must be considered. 2) If only actions are going to be used, h(s) can be computed based on the relaxed plan heuristic (i.e., ignoring all deleting effects), while considering all robot actions. 3) If both capabilities and actions can be used, h(s) can be computed as the minimum cost of an action that achieves any variable in Gs . The final h(s) b(S )) is chosen as the smallest value among the three cases and h(^ can subsequently be computed. C OROLLARY 1. The heuristic above is admissible for finding a c-plan for MAP-MM that maxmizes , given that ^ b(S ) accurately represents b(S ).

5.3

Evaluation

In this section, we describe a simple application of MAP-MM involving a human and a robot, in which the capability model of the human is assumed to be learned by the robot. The robot then makes a multi-agent c-plan for both the human and itself. The setup of this evaluation is identical to that in the evaluation of single agent planning, as we discussed in the previous section.

In this example, we associate the robot actions with a constant cost. After learning the parameters of the human capability model based on the generated partial plan traces, we apply the capability model to solve a problem as shown in Figure 4. Initially, we have On(B 1, B 3), OnT able(B 3), and OnT able(B 2) (a complete state), and the goal is to achieve On(B 3, B 2). The multi-agent c-plan involves the application of two robot actions and one capability of the human. We show three possible complete states (in the belief state) which satisfy the goal variable after applying the capability.

7.

CONCLUSIONS AND FUTURE WORK

6.

RELATED WORK

Most existing approaches for representing the dynamics of agents assume that the models are completely specified. This holds whether the underlying models are based on STRIPS actions (e.g. PDDL [7]) or stochastic action models (such as RDDL [19]). This assumption of complete knowledge is also the default in the existing multi-agent planning systems [3]. Capability models, in contrast, start with the default of incomplete models. They are thus related to the work on planning with incompletely specified actions (c.f. [15, 16, 11]). An important difference is that while this line of work models only incompleteness in the precondition/effect descriptions of the individual actions, capabilities are incomplete in that they completely abstract over actual plans that realize them. In this sense, a capability has some similarities to non-primitive tasks in HTN planning [6, 27]. For example, an abstract HTN plan with a single non-primitive task only posits that there exists some concrete realization of the nonprimitive task which will achieve the goal supported by the nonprimitive task. However, in practice, all HTN planners use "complete models" in that they provide all the reduction schemas to take the non-primitive task to its concretization. So, the "uncertainty" here is "angelic" [12] ≠ the planner can resolve it by the choice of reduction schemas. In contrast, capability models do not have to (and cannot) provide any specifics about the exact plan with which a capability will be realized. Capability models also have connections to macro operators [2], as well as options, their MDP counterparts [21, 20, 9], and the BDI models [17]. Capability models are useful when plans must be made with partial knowledge. With complete models, this means that not all actions or macro-actions or options or capabilities in BDI to achieve the goal are provided. None of HTN, SMDP or BDI models can handle the question of what it means to plan when faced with such model incompleteness. Capability models in contrast propose approximate plans (referred to as c-plans) as a useful solution concept in informing the user of how likely there is an actual complete plan. On the other hand, due to the inherent incompleteness of capability models, they are lossy in the following sense. It is possible to compile a complete model to a capability model (e.g., converting actions in RDDL [19] to a capability model), but new capabilities may also be introduced along with the actions. As a result, the synthesized plans would still be incomplete unless the use of these new capabilities are forcibly restricted. The learning of capability models has connections to learning probabilistic relational models using Bayesian networks [8]. The notion of eventual state captured in capability models is similar to that captured by the F operator (i.e., eventually) in LTL and CTL [5, 23]. Although there are other works that discuss about capability models, e.g., [4], they are still based on action modeling.

In this paper, we have introduced a new representation to model agents based on capabilities. The associated model, called a capability model, is an inherently incomplete model that has several unique advantages compared to traditional complete models (i.e., action models). The underlying structure of a capability model is a generalized 2-TBN, which can encode all the capabilities of an agent. The associated probabilities computed (i.e., via Bayesian inference on the capability model) based on the specifications of capabilities (i.e., a partial initial state coupled with a partial eventual state) determine how likely the capabilities can be fulfilled by an operation (i.e., a complete plan). This information can be used to synthesize incomplete "plans" (referred to as c-plans). One of the unique advantages of capability models is that learning for both model structure and parameters is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). Furthermore, we show that parameter learning for capability models can be performed efficiently online via Bayesian learning. Additionally, we provide the details of using capability models in planning. Compared to traditional models for planning, in a planning state, capability models only suggest transitions between partial states (i.e., specified by the capabilities), along with the probabilities that specify how likely such transitions can be fulfilled by an operation. The limitation is that the synthesized c-plans are incomplete. However, we realize that this is unavoidable for planning with incomplete knowledge (i.e., incomplete models). In such cases, the synthesized c-plans can inform the user how likely complete plans exist when following the "guidelines" of the c-plans. In general, a c-plan with a higher probability of success should imply that a complete plan is more likely to exist. We discuss using capability models for single agent planning first, and then extend it to multi-agent planning (with each agent modeled separately by a capability model), in which the capability models of agents are used by a centralized planner. We also discuss how capability models can be mixed with complete models. In future work, we plan to further investigate the relationships between capability models and traditional models. We also plan to explore applications of capability models in our ongoing work on human-robot teaming [22, 13]. For example, we plan to investigate how to enable robots to learn capability models of humans and plan to coordinate with the consideration of these models.

Acknowledgments
This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027.

REFERENCES
[1] C. Backstrom and B. Nebel. Complexity results for sas+ planning. Computational Intelligence, 11:625≠655, 1996. [2] A. Botea, M. Enzenberger, M. M¸ller, and J. Schaeffer. Macro-ff: Improving ai planning with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24:581≠621, 2005. [3] R. I. Brafman and C. Domshlak. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In ICAPS, pages 28≠35. AAAI Press, 2008.

[4] J. Buehler and M. Pagnucco. A framework for task planning in heterogeneous multi robot systems based on robot capabilities. In AAAI Conference on Artificial Intelligence, 2014. [5] E. Clarke and E. Emerson. Design and synthesis of synchronization skeletons using branching time temporal logic. In D. Kozen, editor, Logics of Programs, volume 131 of Lecture Notes in Computer Science, pages 52≠71. Springer Berlin Heidelberg, 1982. [6] K. Erol, J. Hendler, and D. S. Nau. Htn planning: Complexity and expressivity. In In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 1123≠1128. AAAI Press, 1994. [7] M. Fox and D. Long. PDDL2.1: An extension to pddl for expressing temporal planning domains. Journal of Artificial Intelligence Research, 20:2003, 2003. [8] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models. In In IJCAI, pages 1300≠1309. Springer-Verlag, 1999. [9] P. J. Gmytrasiewicz and P. Doshi. Interactive pomdps: Properties and preliminary results. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS '04, pages 1374≠1375, Washington, DC, USA, 2004. IEEE Computer Society. [10] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. J. Artif. Int. Res., 4(1):237≠285, May 1996. [11] S. Kambhampati. Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models, 2007. [12] B. Marthi, S. J. Russell, and J. Wolfe. Angelic semantics for high-level actions. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS), 2007. [13] V. Narayanan, Y. Zhang, N. Mendoza, and S. Kambhampati. Automated planning for peer-to-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. [14] R. E. Neapolitan. Learning Bayesian networks. Prentice Hall, 2004. [15] T. Nguyen and S. Kambhampati. A heuristic approach to planning with incomplete strips action models. In International Conference on Automated Planning and Scheduling, 2014.

[16] T. A. Nguyen, S. Kambhampati, and M. Do. Synthesizing robust plans under incomplete domain models. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2472≠2480, 2013. [17] L. Padgham and P. Lambrix. Formalisations of capabilities for bdi-agents. Autonomous Agents and Multi-Agent Systems, 10(3):249≠271, May 2005. [18] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. [19] S. Sanner. Relational dynamic influence diagram language (rddl): Language description, 2011. [20] S. Seuken and S. Zilberstein. Memory-bounded dynamic programming for dec-pomdps. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07, pages 2009≠2015, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. [21] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181≠211, Aug. 1999. [22] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and S. Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2957≠2962, Sept 2014. [23] M. Vardi. An automata-theoretic approach to linear temporal logic. In F. Moller and G. Birtwistle, editors, Logics for Concurrency, volume 1043 of Lecture Notes in Computer Science, pages 238≠266. Springer Berlin Heidelberg, 1996. [24] B. Y. White and J. R. Frederiksen. Causal model progressions as a foundation for intelligent learning environments. Artificial Intelligence, 42(1):99 ≠ 157, 1990. [25] C. Yuan and B. Malone. Learning optimal bayesian networks: A shortest path perspective. J. Artif. Int. Res., 48(1):23≠65, Oct. 2013. [26] H. H. Zhuo and S. Kambhampati. Action-model acquisition from noisy plan traces. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI'13, pages 2444≠2450. AAAI Press, 2013. [27] H. H. Zhuo, H. MuÒoz Avila, and Q. Yang. Learning hierarchical task network domains from partially observed plan traces. Artificial Intelligence, 212:134≠157, July 2014.

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Tweeting the Mind and Instagramming the Heart:
Exploring Differentiated Content Sharing on Social Media
Lydia Manikonda

Venkata Vamsikrishna Meduri

Subbarao Kambhampati

arXiv:1603.02718v1 [cs.SI] 8 Mar 2016

Department of Computer Science, Arizona State University, Tempe AZ 85281
{lmanikon, vmeduri, rao}@asu.edu

Abstract
Understanding the usage of multiple OSNs (Online Social
Networks) has been of significant research interest as it helps
in identifying the unique and distinguishing trait in each social media platform that contributes to its continued existence. The comparison between the OSNs is insightful when
it is done based on the representative majority of the users
holding active accounts on all the platforms. In this research,
we collected a set of user profiles holding accounts on both
Twitter and Instagram, these platforms being of prominence
among a majority of users. An extensive textual and visual
analysis on the media content posted by these users revealed
that both these platforms are indeed perceived differently at a
fundamental level with Instagram engaging more of the users‚Äô
heart and Twitter capturing more of their mind. These differences got reflected in almost every microscopic analysis done
upon the linguistic, topical and visual aspects.

1

Introduction

Online Social Networks (OSNs) are gaining attention for
being rich sources of information about individuals including many aspects of their daily life through the way they
connect, communicate and share information. Over the past
few years, given their ubiquity and accessibility, social media platforms like Twitter and Instagram have emerged as
very popular microblogging services for web users to communicate with each other through text and photos. In 2015,
when Instagram broke the record of having more than 400
million active monthly users, Twitter was projected as its
main rival. In fact according to a recent article by Pew Research,1 28% of American adults use Instagram and 23%
use Twitter. More interestingly, many users have active accounts on both these sites (or platforms) (Lim et al. 2015;
Chen et al. 2014). While research has recognized immense
practical value in understanding the user behavioral characteristics on these platforms separately, there is no existing research that has examined how the content posted by
individuals differs across these two platforms. Instagram is
a photo-sharing application whereas Twitter emerged as a
text-based application which currently lets users post both
text and multimedia data. Of particular interest is the question of why and how individuals use these two sites when
both of them are similar in their current functionalities.
1
http://www.pewinternet.org/2015/08/19/the-demographics-ofsocial-media-users/

In this research, we aim to answer the aforementioned
questions by analyzing content from the same set of individuals across these two popular platforms and quantifying
their posting patterns (we focus on ordinary users who are
common users but not celebrities or popular users or organizations). By leveraging NLP and Computer Vision techniques, we present some of the first qualitative insights about
the types of trending topics, and social engagement of the
user posts across these two platforms. Analysis on the visual
and linguistic cues indicates the dominance of personal and
social aspects on Instagram and news, opinions and workrelated aspects on Twitter. Despite considering the same set
of users on both platforms, we see remarkably different categories of visual content ‚Äì predominantly eight categories on
Instagram and four categories of images on Twitter. These
results suggest that Instagram is largely a sphere of positive
personal and social information where as Twitter is primarily
a news sharing media with higher negative emotions shared
by users.
Background: Twitter has been explored extensively with
respect to the content (Honey and Herring 2009), language (Hong, Convertino, and Chi 2011), etc and it is established that it is primarily a news medium (Kwak et
al. 2010). Research on Instagram has focused mostly on
understanding the user behavior through analyzing color
palettes (Hochman and Schwartz 2012), categories (Hu,
Manikonda, and Kambhampati 2014), filters (Bakhshi et al.
2015), etc. On the other hand, it has been of significant
interest to the researchers to investigate the behavior of a
user (Benevenuto et al. 2009), connect users (Zafarani and
Liu 2013), study how users reveal their personal information (Chen et al. 2012), etc all across multiple OSNs. We
extend the current state of the art by examining the nature of
a given user‚Äôs behavior manifested across Twitter and Instagram. Close to our work is the work of Bang et al. (Lim et al.
2015) where six OSNs were studied to analyze the temporal
and topical signature (only w.r.t user‚Äôs profession) of user‚Äôs
sharing behavior but they did not focus on studying the comparative linguistic aspects and visual cues across the platforms. Here we employ both textual and visual techniques
to conduct a deeper analysis of content on both Twitter and
Instagram.
Dataset: In order to investigate and characterize a given
user‚Äôs behavior across multiple sites, we use a personal web
hosting service called About.me (http://about.me/) that enables individuals to create an online identity by letting them

Twitter
stories, international, food, web, naÃÉo,
angelo, jaÃÅ
time, people, love, work, world, social,
life

ID
0

happy, love, home, birthday, weekend,
beautiful, park
maÃÅs, dƒ±ÃÅa, vƒ±ÃÅa, gracias, mi, si, las

2

#football, #sports, #news, #art, facebook, google, iphone

4

1

3

Instagram
#food, delicious, coffee, sunset, beautiful, happy, #wedding
#streetart, #brightongraffiti, #belize,
#sussex,
#hipstamatic,
#urbanart,
#lawton
#fashion, #hair, #makeup, #health,
#workout, #vegan, #fit
#instagood,
#photooftheday,
#menswear, #style, #travel, #beach,
#summer
birthday, beautiful, love, christmas,
friends, fun, home

Table 1: Words corresponding to the 5 latent topics from Twitter
and Instagram

provide a brief biography, connections to other individuals
and their personal websites. Using its API, we performed
the data collection of 10,000 users and pruned the individuals who do not have profiles on both Instagram and Twitter.
The final crawl includes 1,035,840 posts from Twitter (using
the Twitter API https://dev.twitter.com/overview/
api) and 327,507 posts from Instagram (using the Instagram
API https://www.instagram.com/developer/) for the
same set of users. Each post in this dataset is public and
the data include user profiles along with their followers and
friends list, tweets (insta posts), meta data for tweets that include favorites (likes), retweets (Instagram has no explicit
reshares; so we use comments in lieu of the attention the
post receives), geo-location tagged, date posted, media content attached and hashtags.

2
2.1

Text Analysis

Latent Topic Analysis

In order to explain the types of content posted by a user
across Twitter and Instagram, we first mine the latent topics from the corpus of Twitter (aggregated posts on Twitter of all users) and corpus of Instagram (aggregated posts
on Instagram of all users where we use captions associated
with posts for this analysis). We use TwitterLDA (https://
github.com/minghui/Twitter-LDA) developed for topic
modeling of short text corpora to mine the latent topics. With
the user accounts obtained from About.me, the topic inference is meaningful as it is pertinent to the bi-platform posts
from users who use both the social media venues.
The topic vocabulary listed for both the platforms in Table 1 indicates the unique topics for each site as well as
the overlapping topics. For instance topics 0 and 4 on Instagram are similar to the topics 1 and 2 on Twitter. However,
a significant difference is that Instagram is predominantly
used to post about art, food, fitness, fashion, travel, friends
and family but Twitter hosts a significantly higher percentage of posts on sports, news and business as compared to
other topics. Another notable difference is that the vocabulary from non-English language posts like French and Spanish is higher on Twitter as compared to the captions on Instagram mostly using English as the language medium. The
topic distributions obtained from the two corpora are listed
in Figure 1 which show that friends and food are the most
frequently posted topics on Instagram as against sports and
news followed by work and social life being popular on
Twitter.
To further validate the observations made about the distinctive topical content across the two platforms, we compared the topic distributions for each individual on the two
platforms by estimating the KL-Divergence (entropy) for

Figure 1: Topic distributions of all the user posts on Twitter and
Instagram

Figure 2: Sorted entropies between the topic distributions of the
user posts on Twitter and Instagram

each user. However for this entropy computation to be possible, a unified topic model needed to be built on the combined corpus of tweets and captions of Instagram posts. The
unified topics are listed in the description of Figure 3. The
resultant entropy plot in Figure 2 follows a power law distribution showing that most users post on Twitter and Instagram equally differently barring a few (where the estimated
p-value < 10‚àí15 for each user).

2.2

Social Engagement

Since our findings revealed that the bi-platform topics are
significantly different and so we wanted to investigate how
these posts made by the same user engage other individuals on the two sites. We define the social engagement as the
attention received by a user‚Äôs post on the social media platform and can be quantified in various ways ranging from the
sum of likes and comments on Instagram and the sum of favorites and reshares on Twitter. For each topic in the unified
topic model for both Twitter and Instagram, the logarithmic
frequency of posts is plotted against the magnitude of social
engagement that is binned to discrete ranges in Figure 3.
An interesting observation is that the socially engaging
topics in the combined model are same as the overlapping
topics from the topic models built in isolation on the Twitter
and Instagram posts (Figure 1 in Section 2.1). The dominating topic on Twitter is about sports, news and business but
the overlapping influential topic is about social and personal
life comprising friends and family. Surprisingly, we found
that the overlapping topics (Topics 2 and 3) fetched predominant social engagement on both Twitter and Instagram.
A notable difference between the platforms with respect
to social engagement is that the magnitude of attention received for Instagram posts is significantly higher than the
level of attention received on Twitter as we can notice from
the ranges plotted on the x-axes in Figure 3. This observation
is consistent regardless of the activity of the user. Even when
a user is more active (Figure 4) on Twitter than Instagram,
the observation of higher social engagement on Instagram
on an absolute scale holds. A possible explanation to this is

Platform
Twitter

Instagram

0.60
0.19

0.49
0.19

0.15
0.14
0.05
0.17

0.30
0.21
0.1
0.21

0.81
0.6
0.08
0.07
0.16

0.5
0.93
0.06
0.04
0.2

Emotionality
Negemo
Posemo
Social Relationships
home
family
friend
humans

(a) Twitter

Individual Differences
work
bio
swear
death
gender

(b) Instagram
Figure 3: Social Engagement Vs Post Frequency where the topics are ‚Äì Topic 0:{people, life, world, social, app, game, business}, Topic 1:{stories, artists, #lastfm, level, #football, #sports,
news}, Topic 2:{birthday, beautiful, work, weekend, park, dinner,
christmas}, Topic 3:{ yang, run, #fitness, #runkeeper, #art, sale,
#menswear}, Topic 4:{#instagood, #photooftheday, #love, maÃÅs,
#fashion, #travel, #food}

Figure 4: Distributions of Followers/Followings vs Media

that the users on Twitter use it as a news source to read informative tweets but not necessarily all of the content that is
read will be ‚Äúliked‚Äù.
On average, there are 30% more number of hashtags for a
Twitter post compared to an Instagram post (Pearson correlation coefficient = 0.34 between distributions with p-value
< 10‚àí15 ). This may also indicate that on Instagram since the
main content is image, textual caption may not receive as
much attention from the user.

2.3

Linguistic Nature

To characterize and compare the type of language used on
both platforms, we use the psycholinguistic lexicon LIWC
((http://liwc.wpengine.com/)) on the text associated
with a Twitter post and an Instagram post. We obtain measures of attributes related to user behavior ‚Äì emotionality
(how people are reacting to different events), social relationships (friends, family, other humans) and individual differences (attributes like bio, gender, age, etc).
It is clear from Table 2 that posts on Twitter are more negatively emotional and contain more work-related and swear
words where as positive social patterns are more evident
on Instagram. By relating these results to the topic analysis results in the previous section, we identify that on Instagram users share less negative content and more lighthearted happy personal updates. To further support these
claims from the textual data, n-gram analysis indicates that

Table 2: Linguistic attributes across Twitter vs Instagram. Each
value indicates the fraction of a post belonging to the corresponding
attribute

users on Instagram focus on things that give them pleasure such as, fashion or travel (top bi/Trigrams like last
night, Good morning, right now, fashion design streetwear),
whereas on Twitter they mainly share check-in feeds from
their apps or news (top bi/Trigrams like Stories via, Just
posted, @YouTube video, Just posted photo).

3

Visual Analysis

Extensive studies have been conducted on the textual and
visual content on these two platforms in isolation but to the
best of our knowledge, a comparative content analysis has
never been conducted. Considering this as a main objective,
this section develops a better understanding on the types of
photos individuals post on Twitter in comparison with their
Instagram posts. To achieve this we employ Computer Vision techniques mainly in terms of ‚Äì visual categories (kinds
of photos) and visual features (color palettes).

3.1

Visual Categories

We first sampled two sets of 5K images from both platforms
separately and using the OpenCV library (http://opencv.
org/) on these two datasets, we extracted Speeded Up Robust Features (SURF) for each image. We used the vector
quantization approach on these features that eventually converted each image into a codebook format. Using the codebook, we clustered images using k-means algorithm (best
value of k is found by SSE (Sum of Squared Error)). We
employed the same approach on both datasets separately. To
our surprise the clusters we obtained on Instagram were very
refined compared to the coarse nature of clusters on Twitter
dataset. After computing these clusters, two researchers separately identified the overall themes of these two datasets
and agreed upon the final visual categories of the photos
from these platforms.
Visual categories on Instagram agree with our previous
work (Hu, Manikonda, and Kambhampati 2014) which detected eight different categories of images. We tried to categorize the Twitter images into the same format as Instagram
images and there are four prominent cluster categories on
Twitter. Figure 7 shows that the percentage of photos in the
activity category outnumbered any other category followed
by captioned photos. To better understand the kinds of activities and captions shown in these two sections, we sampled
around 200 images and asked the two researchers to label

(a)
(b)
(c)
(d)
Figure 5: Subcategories of activity: a) TV shows, b) Running, c)
Conferences, d) Live shows
Figure 7: Photo categories on Twitter vs Instagram
(a)
(b)
(c)
Figure 6: Subcategories of captioned photos: a) Snapshots, b)
Memes, c) Quotes

them manually into different sub-categories. Figure 5 indicates the most popular sub-categories in the activity category
‚Äì news, events (football games, concerts, conferences) and
races and Figure 6 indicates that majority of the captioned
photos are snapshots, memes, and quotes or opinions. These
categories suggest that the topics of photos on Twitter are
mainly related to news, opinions or other general user interests where as on Instagram they mainly share their joyful
and happy moments of their personal lives.

3.2

Visual Features

Existing literature (Bakhshi et al. 2015) shows that the images with a single dominant color gain more popularity
on Instagram. To verify this we compared the visual luminance of all images on these two platforms. We extracted
the grayscale histograms (range from 0 to 259) by utilizing
the OpenCV library as they capture the information about
the brightness, saturation and contrast distribution. The images with darker pixels were binned into the low intensity
value bins close to 0 and images with brighter pixels were
binned into the high intensity values close to 259. Later we
clustered the images in our dataset by employing k-means
algorithm with the grayscale histograms as features for each
image on both these platforms following which 4 types of
clusters were detected based on their color distributions. We
measured the image distribution across each of these 4 categories for both the platforms. We noticed that the images
on Instagram containing darker and brighter pixels are negligible when compared to Twitter as shown in Figure 8. This
suggest that Twitter posts may be less socially engaging than
Instagram owing to a huge presence of captioned photos on
Twitter.

4

Conclusions

In this paper, we presented a detailed comparison of the textual and visual analysis of the content posted by the same set
of users on both Twitter and Instagram. Some of the insights
obtained from linguistic analysis reveal the fundamental differences in the thinking style and emotionality of the users
on these two platforms and how the posts receive varying degrees of attention as per the underlying topics. Interestingly,
user posts on Instagram seem to receive significantly more
attention than Twitter. The visual analyses with respect to
categories and color palettes indicate that the pictures posted
on Instagram contains more selfies and photos with friends
where as Twitter contains more about user opinions in the
form of captioned photos ‚Äì memes, quotes, etc. We observed
that the differences are deeply rooted in the very intention

Figure 8: Example images corresponding to the four major color
categories obtained by extracting color histograms of images associated with the Twitter and Instagram posts.

with which users post on these platforms with Twitter being
a venue for serious posts about news, opinions and business
life where as Instagram acting as the host for light-hearted
personal moments and posts on leisure activities.

References
[Bakhshi et al. 2015] Bakhshi, S.; Shamma, D.; Kennedy, L.; and
Gilbert, E. 2015. Why we filter our photos and how it impacts
engagement. In Proc. ICWSM.
[Benevenuto et al. 2009] Benevenuto, F.; Rodrigues, T.; Cha, M.;
and Almeida, V. 2009. Characterizing user behavior in online
social networks. In Proc. IMC.
[Chen et al. 2012] Chen, T.; Kaafar, M. A.; Friedman, A.; and
Boreli, R. 2012. Is more always merrier?: A deep dive into online social footprints. In Proc. WOSN.
[Chen et al. 2014] Chen, Y.; Zhuang, C.; Cao, Q.; and Hui, P. 2014.
Understanding cross-site linking in online social networks. In Proc.
SNAKDD.
[Hochman and Schwartz 2012] Hochman, N., and Schwartz, R.
2012. Visualizing instagram: Tracing cultural visual rhythms. In
Proc. ICWSM.
[Honey and Herring 2009] Honey, C., and Herring, S. 2009. Beyond microblogging: Conversation and collaboration via twitter. In
Proc. HICSS, 1‚Äì10.
[Hong, Convertino, and Chi 2011] Hong, L.; Convertino, G.; and
Chi, E. 2011. Language matters in twitter: A large scale study.
In Proc. ICWSM.
[Hu, Manikonda, and Kambhampati 2014] Hu, Y.; Manikonda, L.;
and Kambhampati, S. 2014. What we instagram: A first analysis
of instagram photo content and user types. In Proc. ICWSM.
[Kwak et al. 2010] Kwak, H.; Lee, C.; Park, H.; and Moon, S. 2010.
What is twitter, a social network or a news media? In Proc. WWW,
591‚Äì600.
[Lim et al. 2015] Lim, B. H.; Lu, D.; Chen, T.; and Kan, M.-Y.
2015. #mytweet via instagram: Exploring user behaviour across
multiple social networks. In Proc. ASONAM.
[Zafarani and Liu 2013] Zafarani, R., and Liu, H. 2013. Connecting
users across social media sites: A behavioral-modeling approach.
In Proc. KDD.

Venting Weight: Analyzing the Discourse of an Online Weight Loss Forum
Lydia Manikonda1 , Heather Pon-Barry2 , Subbarao Kambhampati1 , Eric Hekler3
David W. McDonald4
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University
2
Mount Holyoke College
3
School of Nutrition and Health Promotion, Arizona State University
4
Human Centered Design & Engineering, University of Washington
lmanikon@asu.edu, ponbarry@mtholyoke.edu, {rao, ehekler}@asu.edu, dwmc@uw.edu
1

Abstract
Online social communities are becoming increasingly popular platforms for people to share information, seek emotional support, and maintain accountability for losing weight.
Studying the discourse in these communities can offer insights on how users benefit from using these applications.
This paper presents an analysis of language and discourse
patterns in forum posts by users who lose weight and keep it
off versus users with fluctuating weight dynamics. In contrast
to prior studies, we have access to the weekly self-reported
check-in weights of users along with their forum posts. This
paper also presents a study on how goal-oriented forums are
different from general online forums in terms of language
markers. Our results reveal differences about how the types
of posts made by users vary along with their weight-loss
patterns. These insights are closely related to the power dynamics of social interactions and can enable better design of
weight-loss applications thereby contributing to a healthy society.

1

Introduction

Obesity is a major public health problem; the number of
people suffering from obesity has risen globally in the last
decade (Ogden et al. 2014). The Centers for Disease Control and prevention (CDCP) defined an obese adult (http:
//www.cdc.gov/obesity/adult/defining.html) as a
person with a body mass index (BMI) of 30 or higher. Many
obese people are trying to lose weight as diseases such as
metabolic syndromes, respiratory problems, coronary heart
disease, and psychological challenges are closely associated with obesity (Must et al. 1999; Ngu 2012). Researchers
have been trying to understand how certain factors are affecting the weight loss as large number of over-weight people are trying to lose weight and some others are trying to
avoid gaining weight. Internet services are gaining popularity to support weight loss as they provide users with the
opportunities to seek information by asking questions, answering questions, sharing their experiences and providing
emotional support where people feel more comfortable by
openly expressing their problems and concerns (Ballantine
and Stephenson 2011).
Copyright ¬© 2016, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Social media tools like weblogs, instant messaging platforms, video chat, social networks, online discussion forums, are reengineering the healthcare sector (Hawn 2009).
Especially, social media is a promising tool for studying
public health like tracking flu infections (Lamb, Paul, and
Dredze 2013), studying post-partum depression (De Choudhury, Counts, and Horvitz 2013), dental pain (Heaivilin et al.
2011), etc. Tools like online discussion forums make it easier to find health-related information while at the same time
provides support by maintaining accountability and some
of the popular works like (Black, But, and Russell 2010)
proved that weight loss can be supported through online interactions. Hence, studying the online discussion forums can
help identify the people at risk who need more support and
provide them access to appropriate services and support.
In this paper, we explore the weight loss patterns of users
who participate in online discussions and ground truth in
terms of the weekly check-in weights of users. We perform different analyses on the users‚Äô language in correlation to their weight loss dynamics. From the overall dataset
we identify two preliminary patterns of weight dynamics:
(1) users who lose weight and successfully maintain the
weight loss (i.e., from one week to the next, weight is lost
or weight remains the same) and (2) users whose weight
pattern fluctuates (i.e., from one week to the next, weight
changes are erratic). While there are many possible groupings that we could have utilized, we chose this grouping
because of the known problems with ‚Äúyo-yo‚Äù dieting (diet
that leads to cyclical loss and gain of weight) compared
to a more steady weight-loss (Brownell and Rodin 1994;
Hekler et al. 2014). Our work is novel in terms of automating
the language analysis by handling a bigger dataset and can
help classify the user type based on the language efficiently.
As a follow-up work, linguistic insights are explored which
distinguish goal-oriented forums from general forums.
Our research contributions in this paper are divided into
two main sections where each focuses on a broader perspective as described below:
1. How does the language of users vary within the weight
loss forum based on their patterns of weight loss. Specifically, to understand the patterns of asking questions, using
a specific sentiment, politeness and making excuses.
2. Are there any interesting insights about the linguistic sig-

nals that makes a goal-oriented forum such as a weightloss forum different from other general online forums.
Our analysis resulted in interesting insights as below:
1. users who lose weight in a fluctuating manner are more
active on the discussion forums.
2. users losing weight in a fluctuating manner appear to talk
about themselves as they use higher number of personal
pronouns and adverbs.
3. users of non-increasing weight loss pattern mostly reply
to the posts made by other users and fluctuating users post
more questions.
4. posts from users of fluctuating weight loss pattern contain
more excuses.
5. politeness of posts seems to be uncorrelated with the
weightloss pattern.
6. users on goal-oriented forums contribute to a cohesive
thread of posts compared to users on general online forums which suffer from non-cohesiveness.
We believe that this research can bring forth the different variables related to people who need additional support
in terms of losing weight and thereby can stay healthy in
maintaining their weight. Also, we envision building personalized weight loss applications that can cater the needs of
individuals who need additional support. We hope that this
study will help in bringing more attention from the research
community to study online weight loss communities and understand both the constructive and destructive dimensions of
weight loss so that we can build a healthy society.

2

Related Work

There is a vast amount of literature about online forums ‚Äì statistical and language analysis of the discussion
threads (GoÃÅmez, Kaltenbrunner, and LoÃÅpez 2008), summarizing the discussions (Backstrom et al. 2013), measuring the success and identifying the factors that make users
participate (Kim 2000; Ludford et al. 2004) on these forums, addressing how the roles of users change (Yang et
al. 2010), understanding the lurking behavior and predicting lurkers (Preece, Nonnecke, and Andrews 2004), etc. Different fields like marketing (Bickart and Schindler 2001),
public health (Black, But, and Russell 2010), etc., considered online forums as influential sources of user information. Much of this literature focuses on studying the online
communities and their users from different linguistic and social networking perspectives. Little attention has been given
to analyze forums from the weight loss perspective.
However, most of the existing studies (Ballantine and
Stephenson 2011; Leahey et al. 2012; Das and Faxvaag
2014) on online weight loss discussion forums focused on
why people participate and how the social support can help
them to lose weight. These studies are conducted from the
perspective of medical and psychological domains, where
the data are collected via interviews or a small set of online forum data that are manually analyzed by human experts. Unlike the existing literature, our work considers the
weekly check-in weights of users along with their posts to

Figure 1: Example weight loss patterns from two individual users:
non-increasing (bottom line), and fluctuating (top line). The x-axis
ranges from the 1st through the 80th weekly check-in; the y-axis
shows the weight, measured in lbs.

understand the behavior of users who want to lose weight
and detect the variables that classify users who need additional support and service. Instead of choosing a small subset of a dataset and performing manual coding, our work
is novel in automating the language analysis by handling a
bigger dataset. Identifying and providing better assistance to
users who need help can also have a significant impact on
gaining the trust and confidence of users in these kinds of
services through better decision making.

3

Dataset

We obtained an anonymized text corpus of online discussion
forums from Fit Now, Inc. who developed a popular mobile
and web-based weight loss application. Along with the text
corpus, we also obtain weekly weight check-in data for a
subset of users. The entire corpus consists of eight different
forums that are subdivided into conversation topic threads.
Each thread consists of several posts made by different users.
The forum data in our corpus consists of 884 threads, with a
median length of 20 posts per thread. The posts were made
between January 1, 2010 and July 1, 2012. We identify the
subset of users for whom we have weight check-in data and
who made at least 25 weight check-ins during this time period. This results in a total of 2,270 users.
We partition the users into two groups based on their dynamic weight loss patterns: a non-increasing group and a
fluctuating group.
1. Non-increasing: These are the users who lose weight and
keep it off. For each week j, the user‚Äôs check-in weight
w j is less than or equal to their past week‚Äôs weight w j‚àí1 ,
within a small margin ‚àÜ. That is, w j ‚â§ (1 + ‚àÜ)w j‚àí1 .
2. Fluctuating: These are the users who do not lose weight.
If the difference between two consecutive weekly checkin weights do not follow the non-increasing constraint,
users are grouped into this category.
We empirically set ‚àÜ = 0.04 to divide the users in our

dataset into two groups of similar size. To illustrate the
two patterns of weight change, Figure 1 shows the weekly
weight check-ins of two individual users, one from each
group. This grouping is coarse, but is motivated by studies
(Kraschnewski et al. 2010; Wing and Phelan 2005) acknowledging that approximately 80% of people who set out to lose
weight are successful at long-term weight loss maintenance,
where successful maintenance is defined as losing 10% or
more of the body weight and maintaining that for at least
an year. In the future for further analysis, we aim to separate users less coarsely, e.g., users who maintain their weight
neither gaining nor losing weight, users who lose weight and
maintain it and finally, users who gain weight.
The main distinctive feature of this weight loss application is that users are encouraged to set goals to regularly log
their weight, diet, and exercise. For a subset of users, this
application included a weekly weight ‚Äúcheck-in‚Äù, an average of the user‚Äôs weight check-ins during the week, for the
January 1, 2010 through July 1, 2012 period. This allows us
to juxtapose the weekly weights of the users with their posts
on the discussion forums.

3.1

Characteristics of Online Community

This weight loss application helps users set a personalized
daily calorie budget, track the food they are eating, their exercise and log their weekly weight. It also helps users to stay
motivated by providing an opportunity to connect with other
users who want to lose weight and support each other. Example snippets from forum threads are shown below. The
‚ÄúCan‚Äôt lose weight!‚Äù thread demonstrates users supporting
each other and offering advice. The ‚ÄúSomeday I will‚Äù thread
highlights the complex relationship between text, semantics,
and motivation in the forums.
Example thread: ‚ÄúCan‚Äôt lose weight!‚Äù
User 1: ‚ÄúI gained over 30 lbs in the last year and am
stressed about losing it. I eat 1600 calories a day and
burn more than that in exercise, but I havent lost any
weight. I am so confused.‚Äù
User 2: ‚ÄúYou‚Äôve only been a member for less than 2
months. I suggest you relax. Set your program to 1
pound weight loss a week. Adjust your habits to something you can live with. . . long term.‚Äù
User 3: ‚ÄúYou sound just like me. I think your exercise
is good but maybe you are eating more than you think.
Try diligently logging everything you consume.‚Äù
User 1: ‚ÄúThanks for the suggestions! I am going to get
back to my logging.‚Äù
Example thread: ‚ÄúSomeday I will. . . ‚Äù
User 1: ‚ÄúDo a pull-up :-)‚Äù
User 2: ‚Äú. . . actually enjoy exercising.‚Äù
User 3: ‚ÄúSomeday I will stop participating in these forums, but obviously not today.‚Äù
User 4: ‚ÄúI hope you fail :-)‚Äù

4

Empirical Analysis of Weight Loss Forum

In this section, we present preliminary observations on how
the language and discourse patterns of forum posts vary with

respect to weight loss dynamics. As an initial step, part-ofspeech (POS) tagging is performed on all forum posts using
the Stanford POS Tagger (Toutanova et al. 2003).
Weight Pattern
# Total users
# Forum users
# Forum posts
Posts per user
Words per post

Non-increasing

Fluctuating

1127
29
99
3.5
49.1

1143
68
1279
18.2
77.3

Table 1: Statistics of users and forum posts.

From the weekly check-in data we identified the number of users and the number of posts from each weight-loss
pattern cluster which are shown in Table 1. In our dataset,
out of 1127 users who are expressing non-increasing weight
loss pattern (1143 fluctuating weight loss pattern) only 29 of
them (68 of them respectively) made atleast one post on the
discussion forums. We see that the average number of posts
by fluctuating users is greater than the average number of
posts by non-increasing users. Our data also suggest that the
posts made by non-increasing users are shorter compared to
those made by fluctuating users. Both these suggest the possible loss of social connectedness once users achieve their
goal.

4.1

Lexical Categories

Studies (Pennebaker, Mehl, and Niederhoffer 2003) show
that the language defines an individual and his/her behavior.
We use the measures that characterize the weight loss pattern by using the linguistic classes in posts made by these
users on the forum. Specifically, verbs, conjunctions, adverbs, personal pronouns and prepositions are considered
as shown in Table 2. We collected all the individual posts
made by all the users belonging to each weight loss pattern
and measured the average frequency of a linguistic class per
post. Fluctuating users appear to talk more about themselves
and interact with other individuals one-on-one as they are
using a relatively higher number of personal pronouns. Additionally, we observe that users who lose weight in a fluctuating manner use greater fraction of prepositions and adverbs. Adverbs are primarily used to tell how someone did
something which means these users who lose weight in a
fluctuating manner explain more about themselves, perhaps
in an attempt to seek more information.

4.2

Asking Questions

In order to build and maintain vibrant online communities, it
is very important to understand the complex ways in which
the members interact and how the communities evolve over
time. As a part of that, previous literature (Bambina 2007)
revealed that people on online health communities mainly
engage in two activities: (i) seeking information, and (ii)
getting emotional support. People usually ask questions or
just browse through the community forums to collect information. If we can understand how users post questions and

Weight Pattern
Non-increasing

Fluctuating

Ling. class

Mean

Med.

SD

Mean

Med.

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

3.85
3.44
2.21
4.94
5.44

2.0
3.0
1.0
3.0
4.0

4.11
3.51
2.87
5.42
5.40

6.27
4.53
3.24
8.65
9.67

4.0
3.0
2.0
6.0
6.0

6.81
5.04
3.74
8.59
10.51

Table 2: Results of statistical measures on linguistic class attributes; Med. refers to Median; SD refers to Standard Deviation

how the other members respond to those questions, it will
be very useful in developing personalized profiles of users
so that the system is able to help them get sufficient information even before they post any questions. Below is an example (paraphrased) showing how users ask and respond to
questions.
Example thread: ‚ÄúNew user‚Äù
User 1: ‚ÄúDid anyone upgrade to the premium app?
What do you like about it?‚Äù
User 2: ‚ÄúI upgraded to the premium. I LOVE the functionality to log food in advance. I can track and set
goals that are not related to weight like how much I
sleep, how much water I drink, etc.‚Äù
User 3: ‚ÄúI upgraded my account to premium too. I really liked the added features because it helped me keep
track of my steps and participate in challenges.‚Äù
We are interested in knowing whether these two types of
users are actively seeking information. We deem a forum
post to be a question if it meets one of these two conditions:
1. Wh-question words: If a sentence in the post starts with
a question word: Wh-Determiner (WDT), Wh-pronoun
(WP), Possessive wh-pronoun (WP$), Wh-adverb (WRB).
2. Punctuation: If the post contains a question mark (‚Äò?‚Äô).
We computed the ratio of question-oriented posts made by
each user in the two clusters. After averaging these ratio values across all the users in each cluster separately, we found
that on average, 32.6% of the posts made by non-increasing
users were questions (S tandardError(S E) = 0.061) while
37.7% of the posts made by fluctuating users were questions
(S E = 0.042). This shows that on an average fluctuating
users do post relatively larger number of questions than the
non-increasing users. We conjecture this could be a reflection of the fluctuating users‚Äô aim to seek more information
from the forum.

4.3

Sentiment of Posts

Analyzing the sentiment of user posts in the forums can
provide a suprisingly meaningful sense of how the loss of
weight impacts the sentiment of user‚Äôs post. In this analysis, we report our initial results on extracting the sentiments
of user‚Äôs posts. In order to achieve this, we utilize the Stanford Sentiment Analyzer (Socher et al. 2013). This analyzer

Figure 2: Proportion of sentiments for the two weight-loss patterns. For non-increasing users, percentage of posts with Positive, Neutral and Negative sentiments are: 22%, 46.5% and 31.5%
respectively. For fluctuating users, the percentage of posts with
Positive, Neutral and Negative sentiments are: 20.9%, 37.6% and
41.5% respectively.

classifies a text input into one of five sentiment categories
‚Äì from Very Positive to Very Negative. We merge the five
classes into three: Positive, Neutral and Negative (In future,
we may consider specific (health and nutrition) sentiment
lexicons).
We analyzed the sentiment of posts contributed by the
users from the two clusters. As shown in Figure 2, posts of
users belonging to the non-increasing cluster are more neutral whereas the posts made by users from the fluctuating
cluster are mainly of negative sentiment. This gives an interesting intuition that the users of fluctuating group might
require more emotional support as they use more negative
sentiment in their posts.

4.4

Politeness

Politeness is an important marker which often is a decisive
factor in whether interactions go well or cease (Rogers and
Lee-Wong 2003). Based on this metric, we can understand
if correlation exists between the politeness of posts made
by users and their weight loss pattern. Politeness according to the Webster‚Äôs Dictionary is to show good manners
towards others, as in behavior, speech, etc. We measured
how polite the posts are with respect to the weight loss pattern. We use the politeness classifier (Danescu-NiculescuMizil et al. 2013) that was constructed with a wide range
of domain-independent lexical, sentiment and dependency
features and there by operationalizes the key components of
politeness theory. It was proven that this classifier achieves
near human-level accuracy across domains (shown 83.79%
classification accuracy on in-domain wiki). Below are some
examples obtained after the classification of posts on this forum.
1. Polite text: ‚ÄúGood for you! I started out obese. Now,
Im not even overweight. Its a great feeling. Congrats

to you on your milestone!‚Äù
‚Ä¢ Polite Score: 0.870
‚Ä¢ Impolite Score: 0.130
2. Impolite text: ‚ÄúGrrrr.... I wish I could screen these
posts so that I dont even have to SEE those darn posts
about HCG or 500 calorie diets any more. :twisted:
And why did my search for Grumpy or Rant or
McRant come up empty?????? Grrrrr......‚Äù
‚Ä¢ Polite Score: 0.250
‚Ä¢ Impolite Score: 0.750
The results of the politeness analysis in Table 3 shows
that users on this weight-loss forum are polite overall. We
speculate that users on weight-loss related forums act polite
to get more information and emotional support. Further investigation is needed to conclude if users on goal-oriented
communities talk politely.
Type

Polite

Impolite

Non-increasing
Fluctuating

70.6%
75%

29.4%
25%

Table 3: Statistics of users and politeness percentage posts

4.5

Excuses

Literature (Bambina 2007) suggests that people use online
forums to maintain accountability. This application mainly
serves the user community to set goals and help the members
achieve those goals. It is important to understand if there is a
correlation between the weight loss pattern of the users and
the way they are making excuses as they are accountable for
not losing weight. In general, excuses are put forward when
people experience questions about their conduct or identity
in case of failing at an assigned task, violating a norm, etc.
Existing research (Deppe and Harackiewicz 1996)
demonstrates that people who are provided with the opportunity to make excuses do seem to perform better on a variety
of tasks. In this analysis, we wanted to verify if the hypothesis that users when given an opportunity to make excuses are
better at losing weight. Here is an example that shows how
User 1 posts excuses in a forum thread.
Example thread: ‚ÄúTrouble sticking to a diet‚Äù
User 1: ‚ÄúI am out of town with the family and making
the right food choices is impossible right now.‚Äù
User 2: ‚ÄúI think we all have to find our own motivation
and drive to succeed in weight loss. We just have to let
the motivation be louder than the excuses.‚Äù
To the best of our knowledge, there is no prior work on automatic classification of a post as an excuse or a non-excuse.
In this regard, we initially wanted to find if excuse classification is simply a special case of text-based categorization or
any special classification approaches need to be developed.
We performed experiments with two standard algorithms:
Naive Bayes Classification and Support Vector Machines,
which were shown to be effective in previous text categorization studies. In order to implement these two algorithms

we considered the standard bag-of-words where a document
d can be expressed in terms of the frequency of each of the
n features as d~ = ( f1 (d), ( f2 (d), . . . , ( fn (d)), fi (d) is the number of times feature i occurs in document d.
We also extended the Latent Dirichlet Allocation (Blei,
Ng, and Jordan 2003) (LDA) to build a classifier that also
uses majority class voting approach to provide labels to the
posts. Initially, LDA is used to extract the latent topic distribution over each of the posts present in the training dataset
that are already labeled as excuses and non-excuses. Later,
each post from the testing dataset is represented in this topic
space. For a given post in the testing dataset, the final class
label is the majority class of the k-closest points in the topic
space. The entire process of classifying a post as an excuse
or non-excuse is described in Algorithm 1.
Data: Labelled dataset ‚Äì Excuses (e f ) and Non-excuses
(ne f ); jth post ‚Äì a post with no class label
Result: Labelled Testing data
Œ∏e ne ‚Üê LDAestimation (e f ,ne f );
œÜene
‚Üê LDAin f erence (kth post, Œ∏iene );
k
L ‚Üê ‚àÖ;
for i := 1 to |e f | + |ne f | do
dist ‚Üê KLdivergence(Œ∏iene , œÜene
j );
append(L, dist);
end
label jth post ‚Üê max class(k-nearestpoints( f ullist));
Algorithm 1: Classification Approach
We utilized Weka (Hall et al. 2009) and svmlight (Joachims 1999) libraries to perform classification using Naive Bayes and SVM respectively. Based on the results
shown in Table 4, LDA-based supervised classifier outperforms the other two approaches and so we use it for measuring the correlation between the frequency of excuses posted
by users and their weight loss patterns.
Approach
Naive Bayes
SVM
LDA-based

Cond-1

Cond-2

57.8% (Uni)
50% (Uni)
65% (80-20 split)

63.1% (Uni + Bi)
46.15% (Uni + Bi)
50% (50-50 split)

Table 4: Classification results in terms of accuracy with different
approaches and conditions (Uni ‚Äì Unigrams; Bi ‚Äì Bigrams; 80-20
split ‚Äì 80% training and 20% testing; 50-50 split ‚Äì 50% training
and 50% testing data) and classifiers

We identified that 46% of the users who make at least one
post in the forum give excuses. If we consider the categorywise statistics, 48% of the users who lose weight in a nonincreasing pattern and 54% of the posts made by the users
of fluctuating weight loss pattern made excuses in at least
one post. It is surprising to notice that users exhibit excuse‚Äì
giving behavior on this weight loss community where accountability is one of its characteristics. Early detection of
these kinds of users and providing more assistance to help
them stay motivated can help lose weight. This kind of in-

tervention by these applications can help gain the trust of its
users.
Overall, in this section we have explored how the basic lexical classes, questions, sentiment, politeness and excuses are
correlated with the weight loss patterns of users. As we got a
good level of understanding about these associations, we can
now use these different attributes as a set of features in order
to predict whether a new user can lose weight or not, based
only on the language he/she is using on these forums. Automated classifier can be very beneficial to design effective
weight loss applications that can help users get additional
support. It can also help the users to pay more attention to
their diet and exercise to lose weight effectively.

5

Forums Studied

We used threads from two other popular online forums
that were used in (Biyani et al. 2012) ‚Äì 1) Trip Advisor New York City travel forum that contains travel related discussions for New York City and 2) Ubuntu forum dataset
that contains discussions about the ubuntu operating system.
There are multiple threads of discussions in both these forums and each thread has multiple posts by several users.
The dataset contains total number of 609 threads (6591 total posts) and 621 threads (3603 total posts) for Tripadvisor
and Ubuntu forums respectively. On an average, the thread
length in terms of the number of posts is 10 and 5 for the tripadvisor and ubuntu forums respectively. The average number of users in a thread on tripadvisor forum is 1.98 and
on ubuntu forum is 3.41. As stated in (Biyani et al. 2012),
ubuntu forums have technical discussions which are nonsubjective in nature where as trip advisor, a travel related
discussion forum has discussions which tend to be subjective.

5.2

Trip Advisor Ubuntu
Ling. class

Mean

SD

Mean

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

2.37
1.87
1.42
2.69
4.25

4.9
3.6
3.19
5.7
8.56

2.04
1.86
1.0
2.24
2.78

6.06
3.76
2.17
4.4
6.52

Table 5: Results of statistical significance tests on linguistic class
attributes for Trip Advisor and Ubuntu forums. For the results on
weight loss forum, please refer to Table 2. SD‚ÄìStandard Deviation

Comparison With General Forums

To contextualize this research, we want to understand if the
goal-oriented forums exhibit any specific traits compared to
the general forums. We define goal-oriented forums as the
forums associated with applications that help set goals while
building a social network of users who share similar goals.
Here, we present an analysis of how the type of forum can
affect the language used with a primary focus on understanding the lexical features and cohesiveness of the threads on
these forums.

5.1

Forum Name

Lexical Features

Lexical features like Part-of-Speech (POS) tags are obtained
for both the forums to understand the behavior of users in
terms of using different categories of words. Analysis similar to the earlier section was conducted using the Stanford
POS tagger to find the number of verbs, conjunctions, adverbs, personal pronouns and prepositions appearing in the
posts as shown in Table 5. As we compare these results with
the weight loss forum, we notice that users on these two forums don‚Äôt use as many personal pronouns and adverbs as
users on the weight loss forums. This is understandable as
users on weight loss forum have a primary goal to seek information while maintaining accountability.

5.3

Cohesion with Previous Posts

It is very important for the discussion forums to capture as
much participation as possible to reach their full potential.
When multiple conversations occur simultaneously, it is difficult to decide which utterance belongs to a specific conversation. Users on the online health forums mostly tend to
seek information and if the main topic is drifted to some
other topic, the main purpose of these discussion forums is
lost. Hence, it is important for the system to automatically
track non-cohesiveness in posts. Cohesion is the property of
a well-written document that links together sentences in the
same context. As a first step, we want to find out how similar a user‚Äôs post is with respect to the previous posts in a
thread from the weight loss forum. This can also help identify users in a given thread who elaborate on previous post
versus those who shift the topic.
Below is an example (paraphrased) showing cohesive post
made by the users on the weight loss forum.
Example thread: ‚Äúchanging life for a healthier self‚Äù
showing cohesive post by User 2
User 1: ‚ÄúDid you remove any commitments in your life
to make time to be healthier? If you have, was it a good
choice or did you regret it?‚Äù
User 2: ‚ÄúYes I‚Äôve done it and never regretted it.‚Äù
User 3: ‚ÄúTrying to do everything at once means doing
nothing - Georg Christoph‚Äù
User 2: ‚ÄúI‚Äôm not sure which entrepreneur said this but
focus only on what you need to do.‚Äù
We focus only on content words: verbs and nouns (partof-speech tags VB, VBZ, VBP, VBD, VBN, VBG, NN, NNP,
NNPS) and use WordNet (Miller 1995) to identify synonyms
of the content words. We compute similarity between the
current post and previous posts of other users in the thread in
terms of commonly shared verbs and nouns including synonyms. In our current analysis, we consider this similarity
score to be the measure of cohesion.
We consider all posts that are not thread-initial. To approximate whether a post is cohesive or not, we compare
the nouns and verbs of the current post to the list of nouns
and verbs (plus synonyms) obtained from the previous posts
of the thread. Our analysis (Table 6) on the three forums
‚Äì fit now data (weight loss forum), trip advisor and ubuntu

Data: Posts P1 , . . ., Pk‚àí1 , Pk
Result: CohS core(Pk )
set A ‚Üê ‚àÖ;
for i := 1 to (k ‚àí 1) do
[vbi , nni ] ‚Üê POS tagging (Pi );
set A ‚Üê set A ‚à™ [vbi , nni ];
set A ‚Üê set A ‚à™ synset(vbi ) ‚à™ synset(nni );
end
set B ‚Üê ‚àÖ;
[vbk , nnk ] ‚Üê POS tagging (Pk );
set B ‚Üê set B ‚à™ [vbk , nnk ];
set B ‚Üê set B ‚à™ synset(vbk ) ‚à™ synset(nnk );
A ‚à©set B |
CohS core(Pk ) ‚Üê |set|set
B|
Algorithm 2: Calculating the cohesive score of a post

finds that the threads on weight loss forum are more cohesive compared to the other two forums.
Fit Now data

Trip Advisor

Ubuntu

0.46
2.22

0.42
3.64

0.30
3.87

Cohesiveness
S.E (√ó10‚àí4 )

Table 6: Average value of Cohesiveness (along with Standard Error
(S.E) ) across all the threads in a given forum. Extreme values are:
0 ‚Äì non-cohesive; 1 ‚Äì cohesive

Overall, it is interesting to see that the goal-oriented forums
(like weight loss forums) have more cohesive threads compared to the general forums. Additionally, users on the goaloriented forums tend to post more information about themselves. In the future it will be worth studying if language
cues can help in predicting auto-tagging of threads to a specific type of forum. Studying other language metrics can also
help understand the contributions of different online forums
and their impact on the public.

6

Implications

The different language metrics studied in the two main
sections of this paper have a great potential to differentiate automatically between users who are struggling to lose
weight and the users who lost weight and are keeping it
off. There are for example, other existing technologies that
help users lose weight by ‚Äì providing incentives if they
lose weight (PACT http://www.gym-pact.com/), allowing other fitness applications to synchronize with the current
application to keep track of exercise (MyFitnessPal https:
//www.myfitnesspal.com/ ), posting questions while doing grocery shopping to find out the calorie content (Fooducate http://www.fooducate.com/ ), etc. We envision
tools that utilize the wealth of information present on the discussion forums along with the users activity to automatically
estimate the degree to which a user‚Äôs efforts will yield results. Predictions of success are not the end goals. The value
of these types of predictions are when they are leveraged
to generate alternative behaviors and actions that a user can
take to improve their chances of weight loss success. De-

signing systems that rely on features studied in this paper
could improve weight loss applications and thereby enhance
the quality of life.
People are taking advantage of these kinds of applications
as they can preserve their anonymity and provide genuine
information about their food intake, exercise levels, etc to
safely collect as much information as they can. Even though
the real identity can be hidden, it is important that the tools
being envisioned provide support in a very ethical manner.
On the other hand, deciphering the genuineness of the information provided is an area of research that can be worth
pursuing (Estrin 2014). On the whole, we believe that it is
important to understand the different attributes that affect the
behavior of individuals on the weight loss forums and help
them successfully lose weight. We hope that this work initiates further research on these types of discussion forums to
raise awareness about the different factors faced by individuals who are struggling to lose weight and thereby can help
develop policies that can support them in losing weight.

7

Conclusions and Future Work

In this paper, we analyzed how the online discussion forums
of weight loss applications can act as an important tool to
detect and identify the different metrics that are associated
with weight loss. As a first step, we identified the two types
of weight loss patterns exhibited by the users on this forum
and studied different factors like sentiment, politeness, excuses and questions. We took advantage of existing tools to
study these different factors and correlations between these
factors and the weight loss pattern. Specifically, this analysis reveals interesting insights about two populations of
users who lose weight differently. Users who lose weight
in a fluctuating manner are more active in these forums, give
more excuses, post more questions and the majority of their
posts contain negative sentiment. This shows the information seeking nature and suggests the possible need for more
support to these kinds of users. As a secondary focus, we
studied how the language metrics differ across goal-oriented
forums and general forums. We found that users of goaloriented forums usually contribute to a more cohesive posting threads and users on general forums tend not to reveal
much information about themselves.
Our analyses provides valuable insights on how user behavior within online weight loss forums might correlate with
the weight outcomes. These sorts of analyses, particularly
when replicated, could provide valuable insights for developing new technologies that might facilitate more effective
interactions about weight loss and can help gain trust of
users in these kinds of systems. It could also provide valuable insights for improving theories about behavior change.
Acknowledgments. This research is supported in part by a
Google research award, the ONR grants N00014-13-1-0176,
N00014-13-1-0519 and N00014-15-1-2027, and the ARO
grant W911NF-13- 1-0023.

References
Backstrom, L.; Kleinberg, J.; Lee, L.; and DanescuNiculescu-Mizil, C. 2013. Characterizing and curating con-

versation threads: Expansion, focus, volume, re-entry. In
ACM International Conference on Web Search and Data Mining, WSDM ‚Äô13, 13‚Äì22.
Ballantine, P. W., and Stephenson, R. J. 2011. Help me, I‚Äôm
fat! Social support in online weight loss networks. Journal of
Consumer Behaviour 10(6):332‚Äì337.
Bambina, A. D. 2007. Online Social Support: The Interplay
of Social Networks and Computer-Mediated Communication.
Bickart, B., and Schindler, R. M. 2001. Internet forums as influential sources of consumer information. J. of Int Marketing
15(3):31‚Äì40.
Biyani, P.; Bhatia, S.; Caragea, C.; and Mitra, P. 2012. Thread
specific features are helpful for identifying subjectivity orientation of online forum threads. In Proceedings of the 24th International Conference on Computational Linguistics, 295‚Äì
310.
Black, L. W.; But, J. J.; and Russell, L. D. 2010. The secret is out! supporting weight loss through online interaction.
Cases on online discussion and interaction: Experiences and
outcomes.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research
3:993‚Äì1022.
Brownell, K. D., and Rodin, J. 1994. The dieting maelstrom:
Is it possible and advisable to lose weight? American Psychologist 49(9):781‚Äì791.
Danescu-Niculescu-Mizil, C.; Sudhof, M.; Jurafsky, D.;
Leskovec, J.; and Potts, C. 2013. A computational approach
to politeness with application to social factors. In Proceedings of ACL.
Das, A., and Faxvaag, A. 2014. What influences patient participation in an online forum for weight loss surgery? IJMR
3(1).
De Choudhury, M.; Counts, S.; and Horvitz, E. 2013. Predicting postpartum changes in emotion and behavior via social
media. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ‚Äô13, 3267‚Äì3276.
Deppe, R. K., and Harackiewicz, J. M. 1996. Selfhandicapping and intrinsic motivation: Buffering intrinsic
motivation from the threat of failure. J. of Personality and
Social Psychology 70(4):868‚Äì876.
Estrin, D. 2014. Small data, where n = me. Commun. ACM
57(4):32‚Äì34.
GoÃÅmez, V.; Kaltenbrunner, A.; and LoÃÅpez, V. 2008. Statistical analysis of the social network and discussion threads in
slashdot. In Proceedings of the 17th International Conference
on World Wide Web, WWW ‚Äô08, 645‚Äì654.
Hall, M.; Frank, E.; Holmes, G.; Pfahringer, B.; Reutemann,
P.; and Witten, I. H. 2009. The weka data mining software:
An update. SIGKDD Explorations 11(1).
Hawn, C. 2009. Take two aspirin and tweet me in the morning; how twitter, facebook, and other social media are reshaping health care. Health Affairs 28(2):361‚Äì368.
Heaivilin, N.; Gerbert, B.; Page, J.; and Gibbs, J. 2011. Public health surveillance of dental pain via twitter. Journal of
Dental Research 90(9):1047‚Äì1051.
Hekler, B. E.; Dubey, G.; McDonald, W. D.; Poole, S. E.;
Li, V.; and Eikey, E. 2014. Exploring the relationship between changes in weight and utterances in an online weight
loss forum: A content and correlational analysis study. J Med
Internet Res 16(12).

Joachims, T. 1999. Making large-scale SVM learning practical. In SchoÃàlkopf, B.; Burges, C.; and Smola, A., eds., Advances in Kernel Methods - Support Vector Learning. Cambridge, MA: MIT Press. chapter 11, 169‚Äì184.
Kim, A. J. 2000. Community Building on the Web: Secret
Strategies for Successful Online Communities.
Kraschnewski, J. L.; Boan, J.; Esposito, J.; Sherwood, N. E.;
Lehman, E. B.; Kephart, D. K.; and Sciamanna, C. N. 2010.
Long-term weight loss maintenance in the united states. International J. of Obesity 34(11):1644‚Äì1654.
Lamb, A.; Paul, M. J.; and Dredze, M. 2013. Separating fact
from fear: Tracking flu infections on twitter. In NAACL.
Leahey, T. M.; Kumar, R.; Weinberg, B. M.; and Wing, R. R.
2012. Teammates and social influence affect weight loss
outcomes in a team-based weight loss competition. Obesity
20(7):1413‚Äì1418.
Ludford, P. J.; Cosley, D.; Frankowski, D.; and Terveen, L.
2004. Think different: Increasing online community participation using uniqueness and group dissimilarity. In SIGCHI
Conference on Human Factors in Computing Systems, CHI
‚Äô04, 631‚Äì638.
Miller, G. A. 1995. Wordnet: A lexical database for English.
Communications of the ACM 38(11):39‚Äì41.
Must, A.; Spadano, J.; Coakley, E. H.; Field, A. E.; Colditz,
G.; and H., D. W. 1999. The disease burden associated with
overweight and obesity. JAMA 282(16):1523‚Äì1529.
2012. The obesity epidemic and its impact on hypertension.
Canadian Journal of Cardiology 28(3):326 ‚Äì 333.
Ogden, C. L.; Kit, B. K.; Fakhouri, T. H.; Carroll, M. D.; and
Flegal, K. M. 2014. The epidemiology of obesity among
adults. GI Epidemiology 394‚Äì404.
Pennebaker, J. W.; Mehl, M. R.; and Niederhoffer, K. G.
2003. Psychological aspects of natural language use: Our
words, our selves. Annual Review of Psychology 54(1):547‚Äì
577.
Preece, J.; Nonnecke, B.; and Andrews, D. 2004. The top
five reasons for lurking: improving community experiences
for everyone. Computers in Human Behavior 20(2):201 ‚Äì
223.
Rogers, P. S., and Lee-Wong, S. M. 2003. Reconceptualizing
politeness to accommodate dynamic tensions in subordinateto-superior reporting. 17(4):379‚Äì412.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
EMNLP, 1631‚Äì1642.
Toutanova, K.; Klein, D.; Manning, C. D.; and Singer, Y.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL ‚Äô03, 173‚Äì180.
Wing, R. R., and Phelan, S. 2005. Long-term weight loss
maintenance. The American Journal of Clinical Nutrition
82(suppl):222S‚Äì5S.
Yang, J.; Wei, X.; Ackerman, M. S.; and Adamic, L. A. 2010.
Activity lifespan: An analysis of user survival patterns in online knowledge sharing communities. In ICWSM.

A Human Factors Analysis of Proactive Support in
Human-robot Teaming
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborti and Subbarao Kambhampati
Abstract‚Äî It has long been assumed that for effective humanrobot teaming, it is desirable for assistive robots to infer the
goals and intents of the humans, and take proactive actions
to help them achieve their goals. However, there has not
been any systematic evaluation of the accuracy of this claim.
On the face of it, there are several ways a proactive robot
assistant can in fact reduce the effectiveness of teaming. For
example, it can increase the cognitive load of the human
teammate by performing actions that are unanticipated by the
human. In such cases, even though the teaming performance
could be improved, it is unclear whether humans are willing
to adapt to robot actions or are able to adapt in a timely
manner. Furthermore, misinterpretations and delays in goal
and intent recognition due to partial observations and limited
communication can also reduce the performance. In this paper,
our aim is to perform an analysis of human factors on the
effectiveness of such proactive support in human-robot teaming.
We perform our evaluation in a simulated Urban Search
and Rescue (USAR) task, in which the efficacy of teaming
is not only dependent on individual performance but also on
teammates‚Äô interactions with each other. In this task, the human
teammate is remotely controlling a robot while working with an
intelligent robot teammate ‚ÄòMary‚Äô. Our main result shows that
the subjects generally preferred Mary with the ability to provide
proactive support (compared to Mary without this ability). Our
results also show that human cognitive load was increased with
a proactive assistant (albeit not significantly) even though the
subjects appeared to interact with it less.

I. I NTRODUCTION
The efficacy of teaming [8] is not only dependent on
individual performance, but also on teammates‚Äô interactions
with each other. It has long been assumed that for effective
human-robot teaming, it is desirable for assistive robots to
infer the goals and intents of the humans, and take proactive
actions to help them achieve their goals. For example, the
ability of goal and intent recognition is considered to be
required for an assistive robot to be socially acceptable
[22], [5], [16], [2], [24]. This claim is also assumed in
other human-robot teaming tasks, such as collaborative manufacturing [25] and urban search and rescue (USAR) [23].
However, there has not been any systematic evaluation of the
accuracy of this claim.1
*This work was supported in part by the ARO grant W911NF-13-1- 0023,
and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N0001415-1-2027.
The authors would like to thank Nathaniel Mendoza for help with the
simulator as well as the anonymous participants in the study.
The authors are with the Department of Computer Science and
Engineering, Arizona State University, Tempe, AZ 85281, USA

{yzhan442,vnaray15,tchakra2,rao}@asu.edu

1 The authors in [13] considered anticipatory action in interaction scenarios involving repetitive actions and the task settings are for human-robot
teaming with proximal interactions.

Fig. 1. Illustration of our USAR task in which the human teammate
remotely controls a robot while working with an intelligent robot ‚ÄòMary‚Äô.
We intend to compare Mary with and without a proactive support ability.

There are several ways a proactive robot assistant can
in fact reduce the effectiveness of teaming. For example,
it can increase the cognitive load of the human teammate by
performing actions that are unanticipated by the human. In
such cases, even though the teaming performance could be
improved, it is unclear whether humans are willing to adapt
to robot actions or are able to adapt in a timely manner.
Furthermore, misinterpretations and delays in goal and intent
recognition due to partial observations and limited communication can also reduce performance. For example, consider
a case in which you want to make an omelet and need eggs
to be fetched from the fridge. Even if an assistive robot has
started to fetch the eggs (after recognizing your intent), you
may decide that the robot is too slow and fetch the eggs
by yourself although you could improve the performance by
letting the robot fetch the eggs while you preheat the pan.
On the other hand, adapting to the robot‚Äôs actions in such
scenarios to improve teaming performance can increase the
human cognitive load, which leads to unsatisfactory teaming
experience. These conflicting factors make us investigate the
utility of Proactive Support (PS) in human-robot teaming.
In this paper, we start this investigation in a simulated
USAR task with a general way to implement the proactive
support ability on a robot in similar scenarios. Previous work
[13] that investigates the effects of this ability is restricted
to human-robot teaming with more proximal interactions.
Meanwhile, to maintain the generality of this task, we only
introduced a few necessary simplifications. In our task,
the human teammate is remotely controlling a robot while
working with an intelligent robot ‚ÄòMary‚Äô (as shown in Fig.
1). The human-robot team is deployed during the early phase

of an emergency response where they need to search, rescue
and provide preliminary treatment to casualties.
This USAR scenario considers many of the complexities
(e.g., partial observations) that often occur in real-world
USAR tasks and we intend to learn whether these complexities influence the overall evaluation of the intelligent robot
(i.e., Mary) with a proactive support (PS) ability, compared
to Mary without this ability. We also aim to investigate
the various trade-offs, e.g., mental workload and situation
awareness through this human factors study.
II. R ELATED W ORK
There are many early works on goal and intent recognition
(e.g., [15], [14], [4]). More recently, a technique to compile
the problem of plan recognition into a classical planning
problem is provided [20]. There is also a rich literature
in the area of plan adaptation, which handles how robots
plan under human-introduced constraints (e.g., social rules
[24]). Using simple temporal networks (STNs), there has
been development in efficient dispatchers that perform fast,
online, least-commitment scheduling adaptation [6]. There
are also a number of adaptation techniques that focus on
integrated planning and execution [7], [21], [1].
There are existing systems that combine both goal and
plan recognition and plan adaptation to achieve a proactive
support ability on robots. In [13], [12], the authors propose a
cost based anticipatory adaptive action selection mechanism
for a robotic teammate to make decisions based on the
confidence of the action‚Äôs validity and relative risk. However,
only repetitive tasks are considered and the task settings are
for human-robot teaming with more proximal interactions
compared to that in USAR scenarios. In [5], a humanaware planning paradigm is introduced where the robot only
passively interacts with the human by avoiding conflicts with
the recognized human plan. In USAR scenarios, it is also
desirable for the robot to proactively provide support to the
human. A recent paper proposes a planning for serendipity
paradigm in which the authors investigate planning for
stigmergic collaboration without explicit commitments [3].
In [17], the authors propose a unified approach to concurrent
plan recognition and execution for human-robot teams, in
which they represent alternative plans for both the human
and robot, thus allowing recognition and adaptation to be performed concurrently and holistically. However, the limitation
is that the plan choices must be specified a priori instead of
dynamically constructed based on the current goal and intent
of the human. This renders the approach impractical for realworld scenarios since even moderate number of choices (i.e.,
branching factors) can make the approach infeasible.
Part of our goal is to provide a general way to achieve
a proactive support ability in scenarios that are similar to
our USAR task, in which the task is composed of subtasks
with priorities that are dependent on the current situation.
Note that a framework to achieve general proactive support
can be arbitrarily complex depending on the task and level
of support that is needed. In our work, similar to [23], we
use the plan recognition technique in [20] and then feed its

outputs to a planner which determines the priorities of the
subtasks and computes a plan accordingly. The main goal of
this work is to start the investigation of humans factors for
proactive support in various human-robot teaming scenarios.
Regarding the benefits of automation in human-robot
teaming, it is well known that automation can have both
positive and negative effects on human performance. Empirical proofs have been provided in four main areas: mental
workload, situation awareness, complacency and skill degradation [19]. We also aim to study the influence of proactive
support on these factors in our USAR task.
III. BACKGROUND
A. USAR Task Settings Overview
In our simulated USAR task, the human and intelligent
robot (i.e., Mary) share the same set of candidate goals
(i.e., subtasks), and the overall team goal is to achieve
them all (which will be distributed among the human and
Mary). These goals are not independent of each other. In
particular, the priorities of goals are dependent on which
goals are achieved in the current situation. Given these task
settings, we aim to investigate the influence of a proactive
support (PS) ability on a robot. We compare two cases:
Mary (i.e., the intelligent robot) has a PS ability and Mary
does not have this ability. During the task execution, in
both cases, Mary chooses her own goal to maximize the
teaming performance accordingly to the human‚Äôs current
goal. When Mary does not have a proactive support ability,
she can only know the human‚Äôs current goal when the human
explicitly communicates it to her. When Mary has this ability,
if the human does not inform Mary of his/her current goal,2
Mary can infer it based on her observations. To summarize,
Mary in both cases can adapt to human goals while Mary
with a PS ability can adapt in a more ‚Äúproactive‚Äù fashion
(hence proactive support). Finally, in both cases, Mary has
an automated planner (see a brief description below) that
can create a plan to achieve her current goal and she can
autonomously execute the plan.
B. Automated Planner
In our settings, a task or subtask is compiled into a
problem instance for an automated planner to solve. The
planner creates a plan by connecting an initial state to a goal
state using agent actions. A planning problem can be specified using a planning domain definition language (PDDL)
[11]. Depending on the task, there are many extensions of
PDDL (e.g., [9], [10]) that can incorporate various modeling
requirements. We use the extension of PDDL described in
[9] to model the USAR domain. Using an automated planner
allows an agent to reason directly about the goal. Human
factors study on the incorporation of automated planners for
human-robot teaming has appeared previously in [18].
2 In both cases, when the human (optionally) informs Mary of his/her
current goal, it is used directly by Mary assuming that this information is
accurate.

Fig. 3.

(a)

(b)

Fig. 2. (a) Simulated Environment for our USAR task. (b) The environment
(from robot X‚Äôs cameras) that the human subject actually sees.

C. Goal and Intent Recognition
To recognize the human intents and goals, assuming that
humans are rational, we use the technique in [20]. In our
task, Mary maintains a belief of the human‚Äôs current goal
(denoted by GX ) as a hypothesis goal set YX , in which
YX corresponds to all remaining candidate goals. Given a
sequence of observations q that are obtained periodically
from sensors (on Mary or fixed in the environment), the
probability distribution Q over G 2 YX is recomputed using
a Bayesian update P(G|q ) ¬µ P(q |G), where the prior is
approximated by the function P(q |G) = 1/(1 + e b D(G,q ) )
in which D(G, q ) = C p (G q ) C p (G+q ). C p (G+q ) and
C p (G q ) represent the cost of the optimal plan to achieve G
with and without the observation of q , respectively. Having
known the probability distribution Q, the goal that has the
highest probability is assumed to be the current goal of
the human. This goal is correspondingly taken out of the
consideration of Mary and Mary then adapts her current
goal if necessary (from her remaining goals) to optimize
the teaming performance. Mary then makes a plan using
an automated planner described previously to achieve her
current goal.
IV. S TUDY D ESIGN
A. Hypotheses
We aim to investigate the following hypotheses:
‚Ä¢ H1) Mary with a proactive support (PS) ability enables
more effective teaming (e.g., less communication and
more efficiency) in our task settings.
‚Ä¢ H2) Mary with a PS ability increases human mental
workload (e.g., due to unanticipated actions from Mary).
In our study, we also make efforts to maintain the task
settings as general as possible. For a discussion on the
generalization of the results, refer to the conclusion section.
B. Environment
Fig. 2(a) shows the simulated environment (created in
Webots) in our USAR task, which represents the floor plan
of an office building where a disaster occurs (e.g., a fire). Fig.
2(a) is the visual feedback from the remotely controlled robot

Example puzzle problem used in our USAR task.

(i.e., robot X in Fig. 1) that the human subject actually sees.
The environment is organized as segments, and each segment
is identified by a unique label (e.g., R01). Furthermore, the
segments are grouped into four regions: medical kit storage
region (represented by segments starting with ‚ÄòS‚Äô), casualty
search region (starting with ‚ÄòR‚Äô), medical room region where
treatment (or triage) is performed (starting with ‚ÄòM‚Äô), and
the hallway region (starting with ‚ÄòH‚Äô). Each region can be
accessed via a door that connects to a hallway segment and R
regions are further divided into rooms that are also connected
by doors. The doors are initially closed and can be pushed
open by the robots. The doors remain open after being
pushed open. Both the remotely-controlled robot (denoted by
‚ÄòX‚Äô) and Mary work inside this environment. There are two
networked CCTV cameras that Mary can obtain observations
from and the field of views of these cameras are also shown
in Fig. 2(a).
C. Task Settings
The overall team goal is to find and treat all the casualties
in the environment, which includes searching for casualties in
the R regions, carrying casualties to medical rooms, fetching
medical kits and performing triages. In Fig. 2(a), the two
colored boxes (i.e., red and blue) in R regions represent
casualties and the white boxes in S regions represent medical
kits.
We impose two constraints on the agents: 1) either robot
X or Mary can carry only one medical kit or one casualty
at one time. 2) The triage can only be performed by robot
X for which the human subject needs to solve a few puzzle
problems (see Fig. 3 for an example) in 2 minutes. Out of
the two casualties, we assume that one is critically injured
(i.e., the red box in R02) who should be treated immediately
after being found. The other one is lightly injured (i.e., the
blue box in R05). It is also assumed that a medical room
can only accommodate one casualty and each medical kit
can only be used towards one casualty.
D. Interface Design
In this USAR task, the human subject needs to manually
control robot X while interacting with Mary. To create a more
realistic USAR environment, the human subject only has
access to the visual feeds from robot X. In other words, the
human subject can only observe the part of the environment
from robot X‚Äôs ‚Äúeyes‚Äù (i.e., two cameras, one mounted above
the other).
The interaction interface between the human subject and
robot X is shown in Fig. 4. More specifically, robot X
displays a list of applicable actions that it can perform given

Fig. 4.

Interaction interface between the human subject and robot X.

the current state. The human subject interacts with robot X
to choose an action from the list of applicable actions. When
the chosen action is completed by X, the interaction interface
displays the next set of actions. This process is repeated
until the task is finished (i.e., all the casualties are found
and treated). Following are the list of all possible action
types that the human can choose. Compare the list with that
shown in Fig 4. This interface also allows the human subject
to optionally inform Mary about his/her current goal so that
Mary can remove it from consideration and adapt her goal
accordingly when necessary.
‚Ä¢ move X H01 H02 - Move robot X from hallway
segment H01 to hallway segment H02.
‚Ä¢ pushdoor X R01 R02 - Push the door between
room R01 and room R02.
‚Ä¢ grab medkit X S01 - Grab the medical kit from
storage room S01.
‚Ä¢ carry casualty X R01 - Carry the casualty at
room R05.
‚Ä¢ drop medkit X M01 - Drop the medical kit in medical room M01.
‚Ä¢ lay down casualty X M01 - Lay down the casualty in medical room M01.
‚Ä¢ perform triage X M01 - Perform medical triage
in medical room M01.
‚Ä¢ Press ‚Äòi‚Äô - Inform Mary about the human subject‚Äôs
current or intended goal. (A list of all remaining candidate goals will be displayed to be chosen.)
Note that these actions are modeled to respect the
constraints that we discussed in Sec. IV-C. For example, lay down casualty X M01 is only available
when there is no other casualties in medical room M01;
perform triage X M01 is only available when there is
a casualty and a medical kit in M01.
The interaction interface between the human subject and
Mary is shown in Fig. 5. This interface is first used by Mary
to update the human subject about her current goal. When
the human subject wants to take over the goal that Mary is

Fig. 5.

Interaction interface between the human subject and Mary.

Fig. 6.

Experimental setup in the USAR task

acting to achieve, this interface is also used to display the
choices (to be selected by the human subject) for Mary to
terminate her current (uncompleted) goal.
E. Study Setup and Flow
The study was set up in our lab space, similar to that
shown in Fig. 6. Before the beginning of the task, the human
subject is given the floor plan without the annotations of
the casualties (i.e., colored boxes). Furthermore, the human
subject is informed that there are two casualties (that cannot
move) and they are located inside the casualty search regions.
However, no information about their exact locations is provided (i.e., which rooms the casualties are in). The human
subject is also informed that the casualty that is represented
by a red box is seriously injured, and should be treated as
soon as possible. Note that Mary has no more information
than the human subject. The remotely controlled robot X and
Mary start in the same segment H01, which is specified by
the green arrows.
Subjects were assigned alternately to team up with either
Mary with a PS ability or without. Each subject is only

allowed to take part in one experimental trial to avoid performance fluctuation due to experience. All subjects completed
the consent form before participating in the study. Prior to
each run, the subject was asked to read the instruction materials that contain the background knowledge and the above
information. The subject was then exposed to the simulator
and the interface and was asked to experiment with them to
gain some familiarity. The subject was asked to collaborate
with Mary to find and treat the two casualties. After the trial,
the subject was asked to complete a questionnaire (in Likert
scale).

that Mary chooses is:

F. Example Scenario

PM = h(pushdoor Mary H01 S03),
(move Mary H01 S03),
(move Mary S03 S04),
(grab medkit Mary S04),
(move Mary S04 S03),
(move Mary S03 H01),
(move Mary H01 H02),
(move Mary H02 H03),
(move Mary H03 H04),
(move Mary H04 H08),
(pushdoor Mary H08 M02),
(move Mary H08 M02),
(move Mary M02 M01),
(drop medkit Mary M01)i

Next, we walk through an example scenario in our USAR
task. Consider a scenario in which the human subject found
the critically injured casualty and the current goal (GX )
of the human subject becomes ‚Äòbring the critically injured
casualty to the top medical room in Fig. 2(a):
goal(X,‚Äòbring the critically injured
casualty to the top medical room‚Äô) =
{ (at critically injured casualty M01)}
However, assume that the human subject failed to inform
Mary of his/her current goal. Also, assume the following
states for the medical kits: {(at med kit 1 S01),
(at med kit 2 S04)}, and that Mary at that time is
still searching the casualties in the other casualty search
region. When robot X enters the field of view of the
CCTV cameras the action and state of X are detected by
the cameras and are fed to Mary as observations. In this
example, some of robot X‚Äôs actions, such as {(move X
H02 H03), (move X H04 H08)} will be observed by
Mary, which triggers the goal and intent recognition process.
After computing the probability distribution Q for all goals
in the candidate goal set for the human, the goal that has the
higher probability (and falls above a pre-specified threshold)
is assumed to be the current goal of the human (GX ), which
in this case is ‚Äòbring the critically injured
casualty to the top medical room‚Äô. Mary now
knows that the critically injured casualty has been found and
can remove this goal from her own candidate goal set.
Furthermore, given this information, Mary recomputes
the priorities of the remaining goals in the current situation
and adapts her goal accordingly. In particular, although the
searching task is still undergoing, Mary realizes that in this
case helping the human subject by bringing a medical kit to
M01 would achieve a better utility for the team. Note that
should the casualty found by the human subject be lightly
injured instead, Mary would decide to continue her search;
also, should the casualty found by the human subject be
lightly injured but the critically injured casualty has already
been treated, Mary would choose to help the human fetch
the medical kit. Note also that in the case that Mary does
not have a PS ability, the above update can only occur in a
timely manner if the human subject chooses to inform Mary
about his/her current goal. In our running example, the goal

goal(GM ,‚Äòbring med kit 1 to the top
medical room‚Äô) =
{(at med kit 1 M01)}
Having chosen her current goal GM , Mary then uses an
automated planner to generate a plan (PM ) that achieves the
goal. Meanwhile, Mary will update the human subject with
her current goal. Assuming that Mary is at segment H01 at
the time, the following plan would be generated:

Note that various other scenarios can arise in this task,
which may not always favor Mary with a PS ability. For
example, the human subject may decide to deliver the
medical kits to the medical rooms even before finding any
casualties. or the human subject may walk robot X to the
medical room empty-handed. These can confuse the goal
and intent recognition process on Mary and lead to reduced
teaming performance. Although not all of these scenarios
occurred during our experimental study, they demonstrate
the conflicting factors for proactive support in human-robot
teaming tasks. It is also clear that these tradeoffs are dependent on the task and robot settings, which require more
investigations in future work.
V. R ESULTS
The study was performed over 4 weeks and involved
16 volunteers (9 males, 7 females), Volunteers have ages
with M = 24 and SD = 1.15. Subjects were recruited from
students on campus. Due to the requirement of understanding
English instructions, subjects must indicate that they are
confident with English communication skills before taking
part in the study. We also asked about the subject‚Äôs familiarity
with computers (M = 6.56, SD = 0.63), robots (M = 4.19,
SD = 0.91), puzzle problems (M = 3.19, SD = 0.83) and
computer gaming (M = 4.69, SD = 1.49), in seven-point
scales after the study (with 1 being least familiar and 7
being most familiar). The subjects reported familiarity with
computers, but not so much with robots, puzzle problems or
computer gaming.

Fig. 7. Results for objective performance and measures. ‚á§ denotes p < 0.05,
‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

Fig. 8. Results for task performance and measures. ‚á§ denotes p < 0.05,
‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

A. Measurement
A post-study questionnaire is used to evaluate three of
four areas that are often used to assess automated systems:
mental workload, situation awareness, and complacency [19].
Furthermore, we also use the questionnaire to evaluate
several psychological distances between individuals and the
environment (including robots), which include immediacy,
effectiveness, likability and trust. Immediacy describes how
realistic the subject felt about the task and Mary. Effectiveness describes the subject‚Äôs feeling about how effective the
subject considered Mary as a teammate. Likability describes
how likable the subject felt about Mary. Trust describes
whether the subject felt that Mary was trustworthy. We also
collect the subjects‚Äô opinions on whether they considered that
Mary should be improved (i.e., improvability).
One way fixed-effects ANOVA tests were performed to
analyze the objective performance and measures, as well as
the subjective questions. The fixed factor in the tests is the
type of Mary, the intelligent robot, which is either Mary with
a PS ability or without (denoted by No-PS).

smaller for the PS case but we did not find any significant
difference. However, we did find a significant difference for
the average number of times the subject informed his/her
goal to Mary (F(1, 14) = 18.27, p < 0.001). This shows that
the subject felt less necessity to inform Mary in the PS
case. There is also a significant difference in the number
of goal updates the subject received from Mary (F(1, 14) =
7.58, p < 0.05), This confirms that Mary changed her goal
less frequently in the PS case.
We also compare the accuracy of the puzzle problems for
the triage operations. To discourage subjects from guessing
the answers to the puzzle questions, they were told that
each incorrect answer would give them negative scores.
Our analysis, interestingly, shows a significant difference
on this performance measure (F(1, 14) = 4.64, p < 0.01),
which suggests that the human mental workload may have
been reduced in the PS case, which is not consistent with
the second hypothesis (i.e., H2). Furthermore, as we show
in the evaluation of subjective measures, this interpretation
contradicts with the results there.

B. Objective Performance

C. Subjective Performance

We first investigate the objective performance and measures. The overall performance (presented in in Fig. 7) is
evaluated based on the total time taken for the team to find
and treat the critically injured casualty, and the total time
taken for the team to finish the entire task (i.e., find and
treat both casualties). It is interesting to observe that while
there is a significant difference between PS and No-PS for
the time taken to complete the entire USAR task (F(1, 14) =
8.34, p < 0.01), we do not find any significant difference for
treating the critically injured casualty. This may be due to the
fact that humans are proficient at prioritizing goals. However,
this may negatively impact the teaming performance since
the subject may more often choose to neglect the help of
Mary when he/she does not feel comfortable with entrusting
Mary with important goals. This conjecture is also consistent
with the results in Fig. 8, which is discussed next.
We provide a more detailed analysis of task performance
in Fig. 8. We compare the average number of times the
subject stopped Mary from executing her current goal and
the average number of times the subject had goal conflicts
with Mary. The results show that these numbers are generally

In this section, we investigate the subjective performance
based on the questionnaire (23 questions in total). For these
23 questions, we categorize them into 8 different (partially
overlapping) groups. This includes 3 groups for evaluating
automation: mental workload (3 items, Cronbach‚Äôs a =
0.713), situation awareness (1 item), and complacency (2
items, Cronbach‚Äôs a = 0.769). Furthermore, we also evaluate
several psychological distances between the human subject
and environment (including Mary), which include immediacy
(1 item), effectiveness (7 items, Cronbach‚Äôs a = 0.724),
likability (1 item), and trust (3 items, Cronbach‚Äôs a = 0.871).
We also include improvability (1 item). The answers to the
questions are in seven-point scales. The results are presented
accumulatively in Fig. 9.
1) Mental Workload: For mental workload, we include
questions that inquire about the ease of working with Mary,
and questions to rate the subject‚Äôs mental workload to interact
with Mary during the task. Although our analysis does not
find any significant difference (p = 0.404), the subjects still
reported some difference in their mental workloads. This is
an interesting result that confirms our hypothesis (i.e., H2):

Fig. 9.

Results for subjective measures. ‚á§ denotes p < 0.05, ‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

although the PS ability enables more effect human-robot
teaming, it also tends to increase the human mental workload
at the same time. It is also worth noting that even though the
subjects in the PS case reported increased mental workload,
they also tended to perform well on the puzzle problems.
This may be due to the fact that subjects felt less necessity
to communicate with Mary and thus can concentrate more
on these problems.
2) Situation Awareness: For situation awareness, we include questions that inquire about whether the subject felt
that he/she had enough information to determine what the
next goal should be. Our analysis does not show a significant
difference (F(1, 14) = 2.78, p = 0.35), although the subjects
reported slightly more situation awareness in the No-PS case,
which is consistent with the side effects of automation in
general. Although the number of updates for the No-PS case
was significantly more than that for the PS case, the fact that
situation awareness of the subject was not reduced much in
the PS case is encouraging. We attribute this to the fact that
the subject still needed to occasionally interact with Mary
when they had goals conflicts, and the subject could gain
situation awareness through such interactions.
3) Complacency: For complacency, we include questions
about the comfort and ease of the teaming, as well as how
well the subject felt about their performance in the task. Our
analysis shows a significant difference (F(1, 14) = 11.29, p <
0.001). This is consistent with the objective performance and
measures, which shows that the human subject generally felt
more satisfied and confident working with Mary in the PS
case. This is important for human-robot teaming.
4) Immediacy, Effectiveness, Likability & Trust: For immediacy, we include questions about how much the subject
considered the simulated task as a realistic USAR task,
and Mary as a teammate. Our analysis shows a significant
difference (F(1, 14) = 11.63, p < 0.001), which is consistent
with our prior results.
For effectiveness, we include questions about the perceived
effectiveness of the team, the balance of workload between
the team members, and whether or not the subject felt that
Mary performed expectedly. Our analysis shows a significant
difference (F(1, 14) = 6.57, p < 0.05). This result suggests

that the proactive support ability indeed increases teaming
effectiveness.
For likability, we include questions about whether the
subject felt that Mary was a good teammate. Our analysis
shows a significant difference (F(1, 14) = 23.26, p < 0.001),
which suggests that the subjects preferred Mary with a PS
ability for teaming.
For trust, we include questions about the evaluation of the
Mary‚Äôs trustworthiness with the assignments (or tasks) she
took and with her updates during the task. Our analysis did
not show any significant difference with F(1, 14) = 3.78, p =
0.072, although subjects in the PS case reported slightly
higher trust.
5) Improvability: For improvability, we include questions
about how much the subject felt that Mary could be improved, and how the subject evaluated his/her interaction
with Mary. Our analysis shows a significant difference for
improvability with F(1, 14) = 17.80, p < 0.001, which, again,
suggests that the subjects preferred Mary with a PS ability.
D. Summary
In summary, our results are mostly consistent with our
hypotheses. Our main result shows that the subjects generally
preferred Mary with a PS ability. With the PS ability,
the human cognitive load was indeed increased (albeit not
significantly), even though the subjects appeared to interact less with Mary. More specifically, while the result on
mental workload confirms our hypothesis, it also seems to
be conflicting with the objective performance on the puzzle
problems. This is likely due to the fact that the subject
felt less necessity to interact with Mary in the PS case.
Furthermore, given that situation awareness was not reduced
significantly in the team with Mary having a PS ability, and
that the subjects had positive feelings towards her, it seems to
suggest that intelligent robots with a PS ability is welcomed
in general. This is, of course, largely dependent on the fact
that the subject‚Äôs cognitive load is not increased significantly,
which may change when the human needs to adapt to the
robot‚Äôs action more frequently in more complex tasks, and
more communication may be needed. More investigations

are needed to be conducted in such scenarios where the task
and robot settings largely differ.
VI. C ONCLUSIONS
In this paper, we aim to start the investigation of humans
factors for proactive support in human-robot teaming. We
start in a simulated USAR task with a general way to
implement the proactive support (PS) ability on a robot in
similar scenarios in which the task is composed of subtasks
with priorities that are dependent on the current situation.
Meanwhile, to maintain the generality of this task, we only
introduced a few necessary simplifications. However, given
the richness of USAR scenarios, more in depth studies are
required to generalize the conclusions to scenarios where
the task and robot settings largely differ. In such cases, our
plan recognition and plan adaptation approaches may also
need to be extended to implement proactive support. Note
that a framework to achieve general proactive support can
be arbitrarily complex depending on the task and level of
support that is needed (e.g., whether the support is active
[13] or passive [5] and whether it is commitment sensitive
or not [3]).
In our task, the human teammate is remotely controlling a
robot while working with an intelligent robot Mary to search
for and treat casualties. Our results show that, in general, the
human teammates prefer to work with a robot that has a PS
ability. However, our results also show that teaming with
PS robots also increases the human‚Äôs cognitive load, albeit
not significantly. This is understandable since working with
a proactive teammate may require more interactions and/or
mental modeling on the human side in order to achieve
better teaming performance. Furthermore, we also show that
situation awareness when working with robots with a PS
ability is not significantly reduced compared to working with
robots without it. This seems to suggest that intelligent robots
with a PS ability is welcomed in general.
R EFERENCES
[1] Samir Alili, Matthieu Warnier, Muhammad Ali, and Rachid Alami.
Planning and plan-execution for human-robot cooperative task
achievement. Proc. of the 19th ICAPS, pages 1‚Äì6, 2009.
[2] Filippo Cavallo, Raffaele Limosani, Alessandro Manzi, Manuele
Bonaccorsi, Raffaele Esposito, Maurizio Di Rocco, Federico Pecora,
Giancarlo Teti, Alessandro Saffiotti, and Paolo Dario. Development
of a socially believable multi-robot solution from town to home.
Cognitive Computation, 6(4):954‚Äì967, 2014.
[3] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula,
Yu Zhang, Matthias Scheutz, David Smith, and Subbarao
Kambhampati. Planning for serendipity. In IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2015.
[4] Eugene Charniak and Robert P. Goldman. A bayesian model of plan
recognition. Artificial Intelligence, 64(1):53 ‚Äì 79, 1993.
[5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning
for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1‚Äì7, June 2009.
[6] Rina Dechter, Itay Meiri, and Judea Pearl. Temporal constraint
networks. Artificial intelligence, 49(1):61‚Äì95, 1991.
[7] Alberto Finzi, FeÃÅlix Ingrand, and Nicola Muscettola. Model-based
executive control through reactive planning for autonomous rovers.
In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings.
2004 IEEE/RSJ International Conference on, volume 1, pages 879‚Äì
884. IEEE, 2004.

[8] Terrence Fong, Illah Nourbakhsh andClayton Kunz, Lorenzo Fluckiger, John Schreiner, Robert Ambrose, Robert Burridge, Reid Simmons, Laura Hiatt, Alan Schultz, J. Gregory Trafton, Magda Bugajska,
and Jean Scholtz. The peer-to-peer human-robot interaction project.
Space 2005.
[9] Maria Fox and Derek Long. Pddl2. 1: An extension to pddl for
expressing temporal planning domains. J. Artif. Intell. Res.(JAIR),
20:61‚Äì124, 2003.
[10] Alfonso Gerevini and Derek Long. Plan constraints and preferences
in pddl3. The Language of the Fifth International Planning Competition. Tech. Rep. Technical Report, Department of Electronics for
Automation, University of Brescia, Italy, 75, 2005.
[11] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett,
Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott
Penberthy, David E Smith, et al. Pddl-the planning domain definition
language. 1998.
[12] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action
selection for human‚Äìrobot fluency. Robotics, IEEE Transactions on,
23(5):952‚Äì961, 2007.
[13] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on
human-robot teamwork efficiency, fluency, and perception of team. In
Proceedings of the ACM/IEEE international conference on Humanrobot interaction, pages 1‚Äì8. ACM, 2007.
[14] Henry A. Kautz. Reasoning about plans. chapter A Formal Theory
of Plan Recognition and Its Implementation, pages 69‚Äì124. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1991.
[15] Henry A. Kautz and James F. Allen. Generalized Plan Recognition.
In National Conference on Artificial Intelligence, pages 32‚Äì37, 1986.
[16] Uwe KoÃàckemann, Federico Pecora, and Lars Karlsson. Grandpa hates
robots - interaction constraints for planning in inhabited environments.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial
Intelligence, July 27 -31, 2014, QueÃÅbec City, QueÃÅbec, Canada., pages
2293‚Äì2299, 2014.
[17] Steven James Levine and Brian Charles Williams. Concurrent plan
recognition and execution for human-robot teams. In Twenty-Fourth
International Conference on Automated Planning and Scheduling,
2014.
[18] Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao
Kambhampati. Automated planning for peer-to-peer teaming and
its evaluation in remote human-robot interaction. In ACM/IEEE
International Conference on Human Robot Interaction (HRI), 2015.
[19] Raja Parasuraman. Designing automation for human use: empirical
studies and quantitative models. Ergonomics, 43(7):931‚Äì951, 2000.
[20] Miquel Ramƒ±ÃÅrez and Hector Geffner. Probabilistic plan recognition
using off-the-shelf classical planners. In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta,
Georgia, USA, July 11-15, 2010, 2010.
[21] Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal.
Improved human-robot team performance using chaski, a humaninspired plan execution system. In Proceedings of the 6th international
conference on Human-robot interaction, pages 29‚Äì36. ACM, 2011.
[22] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon. A human
aware mobile robot motion planner. IEEE Transactions on Robotics,
2007.
[23] Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias
Scheutz, and Subbarao Kambhampati. Coordination in human-robot
teams using mental modeling and plan recognition. In Intelligent
Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957‚Äì2962, Sept 2014.
[24] Stevan Tomic, Federico Pecora, and Alessandro Saffiotti. Too cool
for school - adding social constraints in human aware planning. In
Proceedings of the International Workshop on Cognitive Robotics
(CogRob), 2014.
[25] Vaibhav V Unhelkar, Ho Chit Siu, and Julie A Shah. Comparative
performance of human and mobile robotic assistants in collaborative
fetch-and-deliver tasks. In Proceedings of the 2014 ACM/IEEE
international conference on Human-robot interaction, pages 82‚Äì89.
ACM, 2014.

Plan Explicability and Predictability for
Robot Task Planning

arXiv:1511.08158v2 [cs.AI] 12 Apr 2016

Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti,
Hankz Hankui Zhuo and Subbarao Kambhampati
Abstract‚ÄîIntelligent robots and machines are becoming pervasive in human populated environments. A desirable capability
of these agents is to respond to goal-oriented commands by
autonomously constructing task plans. However, such autonomy
can add significant cognitive load and potentially introduce safety
risks to humans when agents behave unexpectedly. Hence, for
such agents to be helpful, one important requirement is for them
to synthesize plans that can be easily understood by humans.
While there exists previous work that studied socially acceptable
robots that interact with humans in ‚Äúnatural ways‚Äù, and work
that investigated legible motion planning, there lacks a general
solution for high level task planning. To address this issue,
we introduce the notions of plan explicability and predictability.
To compute these measures, first, we postulate that humans
understand agent plans by associating abstract tasks with agent
actions, which can be considered as a labeling process. We learn
the labeling scheme of humans for agent plans from training
examples using conditional random fields (CRFs). Then, we use
the learned model to label a new plan to compute its explicability
and predictability. These measures can be used by agents to
proactively choose or directly synthesize plans that are more
explicable and predictable to humans. We provide evaluations
on a synthetic domain and with human subjects using physical
robots to show the effectiveness of our approach.

I. I NTRODUCTION
Intelligent robots and machines are becoming pervasive in
human populated environments. Examples include robots for
education, entertainment and personal assistance just to name
a few. Significant research efforts have been invested to build
autonomous agents to make them more helpful. These agents
respond to goal specifications instead of basic motor commands, which requires them to autonomously synthesize task
plans and execute those plans to achieve the goals. However,
if the behaviors of these agents are incomprehensible, it can
increase the cognitive load of humans and potentially introduce
safety risks to them.
As a result, one important requirement for such intelligent
agents is to ensure that the synthesized plans are comprehensible to humans. This means that instead of considering
only the planning model of the agent, plan synthesis should
also consider the interpretation of the agent behavior from
the human‚Äôs perspective. This interpretation is related to our
modeling of other agents. More specifically, we tend to have
expectations of others‚Äô behaviors based on our understanding
(modeling) of their capabilities, mental states and etc. If their
behaviors do not match with these expectations, we would
often be confused. One of the major reasons of this confusion
is due to the fact that our understanding of others‚Äô models
is often partial and inaccurate. This is also true when humans

interact with intelligent agents. For example, to darken a room
that is too bright, a robot can either adjust the window blinds,
switch off the lights, or break the light bulbs in the room.
While breaking the light bulbs may well be the least costly
plan to the robot under certain conditions (e.g., when the robot
cannot easily move in the environment but we are unaware
of it), it is clear that the other two options are far more
desirable in the context of robots cohabiting with humans.
One of the challenges here is that the human‚Äôs understanding
of the agent model is inherently hidden. Thus, its interpretation
from the human‚Äôs perspective can be arbitrarily different from
the agent‚Äôs own model. While there exists previous work that
studied socially acceptable robots [11, 12, 21, 18] that interact
with humans in ‚Äúnatural ways‚Äù, and work that investigated
legible motion planning [6], there lacks a general solution for
high level task planning.
In this paper, we introduce the notions of plan explicability
and predictability which are used by autonomous agents (e.g.,
robots) to synthesize ‚Äúexplicable plans‚Äù that can be easily
understood by humans. Our problem settings are as follows:
an intelligent agent is given a goal by a human (so that the
human knows the goal of the agent) working in the same
environment and it needs to synthesize a plan to achieve the
goal. As suggested in psychological studies [24, 5], we assume
that humans naturally interpret a plan as achieving abstract
tasks (or subgoals), which are functional interpretations of
agent action sequences in the plan. For example, a robot
that executes a sequence of manipulation actions may be
interpreted as achieving the task of ‚Äúpicking up cup‚Äù. Based
on this assumption, intuitively, the easier it is for humans to
associate tasks with actions in a plan, the more explicable
the plan is. Similarly, the easier it is to predict the next
task given actions in the previous tasks, the more predictable
the plan is. In this regard, explicability is concerned with
the association between human-interpreted tasks and agent
actions, while predictability is concerned with the connections
between these abstract tasks.
Since the association between tasks and agent actions can
be considered as a labeling process, we learn the labeling
scheme of humans for agent plans from training examples
using conditional random fields (CRFs). We then use the
learned model to label a new plan to compute its explicability
and predictability. These measures are used by agents to
proactively choose or directly synthesize plans that are more
explicable and predictable without affecting the quality much.
Our learning approach does not assume any prior knowledge

Fig. 1.
From left to right, the scenarios illustrate the differences between
automated task planning, human-aware planning and explicable planning (this
work). In human-aware planning, the robot needs to maintain a model of the
human (i.e., MH ) which captures the human‚Äôs capabilities, intents and etc.
In explicable planning, the robot considers the differences between its model
from the human‚Äôs perspective (i.e., M‚àóR ) and its own model MR .

on the human‚Äôs interpretation of the agent model. We provide
evaluation on a synthetic domain in simulation and with human
subjects using physical robots to demonstrate the effectiveness
of our approach.
II. R ELATED W ORK
To build autonomous agents (e.g., robots), one desirable
capability is for such agents to respond to goal-oriented
commands via automated task planning. A planning capability
allows agents to autonomously synthesize plans to achieve a
goal given the agent model (MR as shown in the first scenario
in Fig. 1) instead of following low level motion commands,
thus significantly reducing the human‚Äôs cognitive load. Furthermore, to work alongside of humans, these agents must
be ‚Äúhuman-aware‚Äù when synthesizing plans. In prior works,
this issue is addressed under human-aware planning [22, 4, 2]
in which agents take the human‚Äôs activities and intents into
account when constructing their plans. This corresponds to
human modeling in human-aware planning as shown in the
second scenario in Fig. 1. A prerequisite for human-aware
planning is a plan recognition component, which is used
to infer the human‚Äôs goals and plans. This information is
then used to avoid interference, and plan for serendipity and
teaming with humans. There exists a rich literature on plan
recognition [14, 3, 20, 16], and many recent works use these
techniques in human-aware planning and human-robot teaming
[23, 2, 25].
While our work on plan explicability and predictability
falls within the scope of human-in-the-loop planning (which
also includes human-aware planning), it differs significantly
from the previous work. This is illustrated in Fig. 1. More
specifically, in human-aware planning, the challenge is to
obtain the human model (MH in Fig. 1) which captures human
capabilities [26], intents [23, 2] and etc. The modeling in this
work is one level deeper: it is about the interpretation of the
agent model from the human‚Äôs perspective (M‚àóR in Fig. 1).
In other words, R needs to understand the model of itself in
H‚Äôs eyes. This information is inherently hidden, difficult to

convey, and can be arbitrarily different (e.g., having different
representations) from R‚Äôs own model (MR in Fig. 1).
There exists work on generating legible robot motions [6]
which considers a similar issue in motion planning. We are,
on the other hand, concerned with task planning. Note that
two different task plans may map to exactly the same motions
which can be interpreted vastly differently by humans. In such
cases, considering only motion becomes insufficient. Nevertheless, there exists similarities between [6] and our work. For
example, legibility there is analogous to predictability in ours.
In the human-robot interaction (HRI) community, there
exists prior works that discuss how to enable natural and
fluent human-robot interaction [11, 12, 21, 18] to create more
socially acceptable robots [7]. These works, however, apply
only to behaviors in specific domains. Compared with model
learning via expert teaching, such as inverse reinforcement
learning [1] and tutoring systems [17], which is about learning
the ‚Äúright‚Äù model from teachers, our work, on the other hand,
is concerned with learning model differences. Furthermore,
as an extension to our work, when robots cannot find an
explicable plan that is also cost efficient, they need to explain
the situation. In this regard, our work is also related to excuse
[9] and explanation generation [10]. Finally, while our learning
approach appears to be similar to information extraction [19],
we use the learned model to proactively guide planning instead
of passively extracting information.
III. E XPLICABILITY AND P REDICTABILITY
In our settings, an agent R needs to achieve a goal given
by a human in the same environment (so that the human
knows about the goal of the robot). The agent has a model
of itself (referred to as MR ) which is used to autonomously
construct plans to achieve the goal. In this paper, we assume
that this model is based on PDDL [8], a general planning
domain definition language. As we discussed, for an agent
to generate explicable and predictable plans, it must not only
consider MR but also M‚àóR , which is the interpretation of MR
from the human‚Äôs perspective.
A. Problem Formulation
Keeping the problem settings in mind, given a domain, the
problem is to find a plan for a given goal that satisfies the
following:
argmin cost(œÄMR ) + Œ± ¬∑ dist(œÄMR , œÄM‚àóR )

(1)

œÄ MR

where œÄMR is a plan that is constructed using MR (i.e., the
agent‚Äôs plan), œÄM‚àóR is a plan that is constructed using M‚àóR
(i.e., the human‚Äôs anticipation of the agent‚Äôs plan), cost returns
the cost of a plan, dist returns the distance (i.e., capturing the
differences) between two plans, and Œ± is the relative weight.
The goal of Eq. (1) is to find a plan that minimizes a weighted
sum of the cost of the agent plan and the differences between
the two plans. Since the agent model MR is assumed to be
given, the challenge lies in the second part in Eq. (1).
Note that if we know M‚àóR or it can be learned, the only
thing left would be to search for a proper dist function.

However, as discussed previously, M‚àóR is inherently hidden,
difficult to convey, and can be arbitrarily different from MR .
Hence, our solution is to use a learning method to directly
approximate the returned values. We postulate that humans
understand agent plans by associating abstract tasks with
actions, which can be considered as a labeling process. Based
on this, we assume that dist(œÄMR , œÄM‚àóR ) can be functionally
decomposed as:

2) Predictability Labeling: Predictability is concerned with
the connections between tasks in a plan. An action label for
predictability is composed of two parts: a current label and a
next label (i.e., L√óL). The current label is also the action label
for explicability. The next label (similar to the current label)
is used to specify the tasks that are anticipated to be achieved
next. A next label with multiple task labels is interpreted as
having multiple candidate tasks to achieve next; when this
label is the empty set, it is interpreted as that the next task is
dist(œÄMR , œÄM‚àóR ) = F ‚ó¶ L‚àó (œÄMR )
(2) unpredictable, or there are no more tasks to be achieved.
Definition 2 (Plan Predictability): Given a domain, the
where F is a domain specific function that takes plan labels as
predictability Œ≤œÄ of a plan œÄ is computed by a mapping,
‚àó
input, and L is the labeling scheme of the human for agent
FŒ≤ : L2œÄ ‚Üí [0, 1] (with 1 being the most predictable).
plans based on M‚àóR . As a result, Eq. (1) now becomes:
L2œÄ denotes the sequence of action labels for predictability. An
‚àó
‚àó i
argmin cost(œÄMR ) + Œ± ¬∑ F ‚ó¶ LCRF (œÄMR |{Si |Si = L (œÄMR )}) example of FŒ≤ is given below which is used in our evaluation
œÄ MR
when assuming that the current and next labels are associated
(3) with at most one task label:
P
where {Si } is the set of training examples and L‚àóCRF is
1
‚àß (1L2 (ai )=L(aj ) ‚à® 1L2 (ai:N )=‚àÖ )
the learned model of L‚àó . We can now formally define plan FŒ≤ (L2œÄ ) = i‚àà[0,N ] |L(ai )|=1
N +1
explicability and predictability in our context. Given a plan of
(8)
agent R as a sequence of actions, we denote it as œÄMR and where a (j > i) is the first action that has a different
j
simplified below as œÄ for clarity:
current label as ai or the last action in the plan if no such
2
œÄ = ha0 , a1 , a2 , ...aN i
(4) action is unfound, L (ai ) returns the next label of ai and
1L2 (ai:N )=‚àÖ returns 1 only if the next labels for all actions after
where a0 is a null action that denotes plan starting. Given the ai (including ai ) are ‚àÖ. Eq. (8) computes the ratio between
domain, we assume that a set of task labels T is provided to number of actions that we have correctly predicted the next
task and the number of all actions.
label agent actions:
T = {T1 , T2 , ...TM }

(5)

1) Explicability Labeling: Explicability is concerned with
the association between abstract tasks and agent actions; each
action in a plan is associated with an action label. The set
of action labels for explicability is the power set of the task
labels:
L = 2T
(6)
When an action label includes multiple task labels, the action
is interpreted as contributing to multiple tasks; when an action
label is the empty set, the action is interpreted as inexplicable.
When a plan is labeled, we can compute its explicability
measure based on its action labels in a domain specific way.
More specifically, we define:
Definition 1 (Plan explicability): Given a domain, the explicability Œ∏œÄ of an agent plan œÄ is computed by a mapping,
FŒ∏ : LœÄ ‚Üí [0, 1] (with 1 being the most explicable).
LœÄ above denotes the sequence of action labels for œÄ. An
example of FŒ∏ used in our evaluation is given below:
P
i‚àà[1,N ] 1L(ai )6=‚àÖ
(7)
FŒ∏ (LœÄ ) =
N
where N is the plan length, L(ai ) returns the action label of
ai , and 1f ormula is an indicator function that returns 1 when
the f ormula holds or 0 otherwise. Eq. (7) basically computes
the ratio between the number of actions with non-empty action
labels and the number of all actions.

B. A Concrete Example
Before discussing how to learn the labeling scheme of the
human from training examples, we provide a concrete example
to connect the previous concepts and show how training
examples can be obtained. In this example, there is a rover
in a grid environment working with a human. An illustration
of this example is presented in Fig. 2. There are resources
to be collected which are represented as boxes. There is one
storage area that can store one resource which is represented
as an open box. The rover can also make observations. The
rover actions include {navigate lf rom lto }, {observe l} {load
l}, and {unload l}, each representing a set of actions since
l (i.e., representing a location) can be instantiated to different
locations (i.e., 0 ‚àí 8 in Fig. 2). navigate (or nav) can move
the rover from a location to one of its adjacent locations; load
can be used to pick up a resource when the rover is not already
loaded; unload can be used to unload a resource at a storage
area if the area is empty; observe (or obs) can be used to
make an observation. Once a location is observed, it remains
observed. The goal in this example is for the rover to make the
storage area non-empty and observe two locations that contain
the eye symbol in Fig. 2.
In this domain, we assume that there are three abstract tasks
that may be used by the human to interpret the rover‚Äôs plans:
COLLECT (C), STORE (S) and OBSERVE (O). Note that
we do not specify any arguments for these tasks (e.g., which
resource the rover is collecting) since this information may not
be important to the human. This also illustrates that MR and

fields (CRFs) [15] due to their abilities to model sequential
data. An alternative would be HMMs; however, CRFs have
been shown to relax assumptions about the input and output
sequence distributions and hence are more flexible.
The distributions that are captured by CRFs have the following form:
p(x, y) =

Fig. 2. Example for plan explicability and predictability with action labels
(on the right) for a given plan in the rover domain.

1
Œ†A Œ¶(xA , yA )
Z

in which Z is a normalization factor that satisfies:
X
Z=
Œ†A Œ¶(xA , yA )

(9)

(10)

x,y

M‚àóR can be arbitrarily different. In Fig. 2, we present a plan
of the rover as connected arrows starting from the its initial
location.
Human Interpretation as Training Examples: Let us now
discuss how humans may interpret this plan (i.e., associating
labels with actions) as the actions are observed incrementally:
when labeling ai , we only have access to the plan prefix
ha0 , ..., ai i. At the beginning for labeling a0 , the observation
is that the rover starts at l5 . Given the environment and
knowledge of the rover‚Äôs goal, we may infer that the first task
should be COLLECT (the resource from l4 ). Hence, we may
choose to label a0 as ({START}, {C}). The first action of
the rover (i.e., nav l5 l4 ) seems to match with our prediction.
Furthermore, given that the storage area is closest to the rover‚Äôs
location after completing COLLECT, the next task is likely to
be STORE. Hence, we may label a1 as ({C}, {S}) as shown
in the figure. The second action (i.e., load l4 ) also matches
with our expectation. Hence, we label a2 too as ({C}, {S}).
The third action, nav l4 l1 , however, is unexpected since we
predicted STORE in the previous steps. Nevertheless, we can
still explain it as contributing to OBSERVE (at location l0 ).
Hence, we may label this navigation action (a3 ) as ({O}, {S}).
For the fourth action, the rover moves back to l4 , which is
inexplicable since the rover‚Äôs behavior seems to be oscillating
without particular reasons. Hence, we may choose to label this
action as (‚àÖ, ‚àÖ). The labeling for the rest of the plan continues
in a similar manner. This thought process reflects how training
examples can be obtained from human labelers.
IV. L EARNING A PPROACH
To compute Œ∏œÄ and Œ≤œÄ from Defs. (1) and (2) for a given
plan œÄ, the challenge is to provide a label for each action.
This requires us to learn the labeling scheme of humans (i.e.,
L‚àó in Eq. (2)) from training examples and then apply the
learned model to œÄ (i.e., L‚àóCRF in Eq. (3)). To formulate a
learning method, we consider the sequence of labels as hidden
variables. The plan that is executed by the agent (which also
captures the state trajectory), as well as any cognitive cues
that may be obtained (e.g., from sensing) during the plan
execution constitute the observations. The graphical model that
we choose for our learning approach is conditional random

In the equations above, x represents the sequence of observations, y represents the sequence of hidden variables, and
Œ¶(xA , yA ) represents a factor that is related to a subgraph in
the CRF model associated with variables xA and yA . In our
context, x are the observations made during the execution of a
plan; y are the action labels. Each factor is associated with a
set of features that can be extracted during the plan execution.
Next, we discuss some possible features that can be used for
plan explicability and predictability.
A. Features for Learning
Given an agent plan, the immediate set of features that we
have access to is the plan and its associated state trajectory.
Note that the human may not be required (nor it is necessary)
to fully understand this information. When the dynamics of the
agent are known, given the plan, it may also be possible to
derive low level motor commands that implement the motions,
which can be used to extract motion related features.
When the agent is equipped with sensors such as cameras
and lasers, we can also extract features from sensor information. For example, from video streams and depth information,
we can extract features about the environment, e.g., how
crowded the workspace is.
Sensor information can also be used to extract dynamic
features such as the location of the human. However, note
that this information will not be available during the testing
phase, and thus these features need to be estimated based on
other information (e.g., projected plan of the human based on
plan recognition techniques [20, 16]).
In this work, we use a linear chain CRF. However, our
formulation is easily extensible to more general types of
CRFs. Given an agent plan œÄ = ha0 , a1 , a2 , ...i, each action is
associated with a set of features. Hence, each training example
is of the following form:
h(F0 , L20 ), (F1 , L21 ), (F2 , L22 ), ...i

(11)

where L2i is the action label for predictability (and explicability) for ai . Fi is the set of features for ai . We discuss several
feature categories in more detail below:

1) Plan Features: Given the agent model (specified in
PDDL), the set of plan features for ai includes the action
description and the state variables after executing the sequence
of actions ha0 , ..., ai i from the initial state. This information
can be easily extracted given the model. For example, in our
rover example in Fig. 2, this set of features for a1 includes
navigate, at rover l4 , at resource0 l2 , at resource1 l4 , at
storage0 l3 .
2) Action Features: Action features for ai describes the
motion (e.g., dynamics) of this action. These features can
be used to capture, for example, smoothness of execution
within and across actions. Action features sometimes serve
as important cognitive cues for humans to understand agent
actions. For example, an action that enables a robot to cross a
river may be interpreted as swimming, pedaling, or propelling
depending on how the robot motion looks like. Action features
can be extracted for a plan given the dynamics of the robot.
3) Interaction Features: Interaction features are intended
to capture ai ‚Äôs influence on the human. For example, it can
include how far the agent is from human and what the human
is performing when ai is being executed. In other words,
this set of features captures characteristics of the interactions
between the human and agent. Interaction features can be
extracted from sensor information or estimated based on the
projected human plan.
B. Using the Learned Model
Given a set of training examples in the form of Eq. (11),
we can train the CRF model to learn the labeling scheme in
Eq. (3). We discuss two ways to use the learned CRF model.
1) Plan Selection: The most straightforward method is to
perform plan selection on a set of candidate plans which can
simply be a set of plans that are within a certain cost bound
of the optimal plan. Candidate plans can also be generated
to be diverse with respect to various plan distances. For each
plan, the agent must first extract the features of the actions
as we discussed earlier. It then uses the trained model (i.e.,
L‚àóCRF ) to produce the labels for the actions in the plan. Œ∏ and
Œ≤ can then be computed given the mappings in Defs. (1) and
(2). These measures can then be used to choose a plan that is
more explicable and predictable.
2) Plan Synthesis: A more efficient way is to incorporate
these measures as heuristics into the planning process. Here,
we consider the FastForward (FF) planner with enforced hill
climbing [13]. To compute the heuristic value given a planning
state, we use the relaxed planning graph to construct the remaining planning steps. However, since relaxed planning does
not ensure a valid plan, we can only use action descriptions as
plan features for actions that are beyond the current planning
state when estimating the Œ∏ and Œ≤ measures. These estimates
are then combined with the relaxed planning heuristic (which
only considers plan cost) to guide the search. The algorithm
for generating explicable and predictable plans is presented in
Alg 1.
The capability to synthesize explicable and predictable plans
is useful for autonomous agents. For example, in domains

Algorithm 1 Synthesizing Explicable and Predictable Plans
Input: agent model MR , trained human labeling scheme
L‚àóCRF , initial state I and goal state G.
Output: œÄEXP
1: Push I into the open set O.
2: while open set is not empty do
3:
s = GetNext(O).
4:
h‚àó = M AX.
5:
if G is reached then
6:
return s.plan (i.e., the plan that leads to s from I).
7:
end if
8:
Compute all possible next states N from s.
9:
for n ‚àà N do
10:
Compute the relaxed plan œÄRELAX for n.
11:
Concatenate s.plan (with plan features) with
œÄRELAX (with only action descriptions) as œÄÃÑ.
12:
Compute and add other relevant features.
13:
Compute L2œÄ = L‚àóCRF (œÄÃÑ).
14:
Compute Œ∏ and Œ≤ based on L2œÄ for œÄÃÑ.
15:
Compute h = f (Œ∏, Œ≤, hcost ) (f is a combination
function; hcost is the relaxed planning heuristic).
16:
end for
17:
Find the state n‚àó ‚àà N with the minimum h.
18:
if h(n‚àó ) < h‚àó then
19:
Clear O.
20:
Push n‚àó into O.
21:
else
22:
Push all n ‚àà N into O.
23:
end if
24: end while

where humans interact closely with robots (e.g., in an assembly warehouse), more preferences should be given to plans
that are more explicable and predictable since there would be
high risks if the robots act unexpectedly. One note is that the
relative weights of explicability and predictability may vary in
different domains. For example, in domains where robots do
not engage in close interactions with humans, predictability
may not matter much.
V. E VALUATION
We first evaluate our approach systematically on a synthetic
dataset based on the rover domain. Then, we evaluate it with
human subjects using physical robots to validate that the
synthesized plans are more explicable to humans in a blocks
world domain.
A. Systematic Evaluation with a Synthetic Domain
The aim is twofold here: evaluate how well the learning
approach can capture an arbitrary labeling scheme; evaluate
the effectiveness of plan selection and synthesis with respect
to the Œ∏ and Œ≤ measures.
1) Dataset Synthesis: To simplify the data synthesis process, we make the following assumptions: all rover actions
have the same cost; all rover actions are associated with at

most one task label (i.e., L = T ‚à™{‚àÖ} in Eq. (6)). To construct
a domain in which the optimal plan (in terms of cost) may not
be the most explicable (in order to differ MR from M‚àóR ), we
add ‚Äúoscillations‚Äù to the plans of the rover. These oscillations
are incorporated by randomly adding locations for the rover to
visit as hidden goals. For these locations, the rover only needs
to visit them. As a result, it may demonstrate ‚Äúunexpected‚Äù
behaviors given only the public goal, denoted by G, which
is known to both the rover and human. We denote the goal
that also includes the hidden goals as G0 . Given a problem
with a public goal G, we implement a labeling scheme to
automatically provide the ‚Äúground truth‚Äù of a rover plan, which
is constructed by the rover to achieve G0 .
Given a plan of the rover, we label it incrementally by associating each action with a current and next label. These labels
are chosen from {{COLLECT}, {STORE}, {OBSERVE}, ‚àÖ}.
We denote the plan prefix ha0 , ...ai i for a plan œÄ as œÄi , the
state after applying œÄi as si from the initial state, and a plan
that is constructed from si to achieve G (i.e., using si as the
initial state) as P (si ). For the current label of ai :
1) If |P (si )| ‚â• |P (si‚àí1 )|, we label ai as ‚àÖ (i.e., inexplicable). This rule means that humans may label an action
as inexplicable if it does not contribute to achieving G.
2) If |P (si )| < |P (si‚àí1 )|, we label ai based on the
distances from the current rover location to the targets
(i.e., storage areas or observation locations), current state
of the rover (i.e., loaded or not), and whether ai moves
the rover closer to these targets. For example, if the
closest target is a storage area and the rover is loaded, we
label ai as {STORE}. When there are ties, we label ai
as ‚àÖ (i.e., unclear and hence interpreted as inexplicable).
For the next label of ai :
1) This label is determined by the target that is closest to
the rover state after the current task is achieved. When
there are ties, ai is labeled as ‚àÖ (i.e., unclear and hence
interpreted as unpredictable). If the current label is ‚àÖ,
we also label ai as ‚àÖ (i.e., unpredictable).
2) If the current task is also the last task, we label ai as ‚àÖ
since there is no next task.
For evaluation, we define FŒ∏ and FŒ≤ as in Eqs. (7) and (8).
We randomly generate problems in a 4 √ó 4 environment. For
each problem, we randomly generate 1 ‚àí 3 resources as a set
RE, 1‚àí3 storage areas as a set ST, 1‚àí3 observation locations
as a set OB. The public goal G of a problem, first, includes
making all storage areas non-empty. To ensure a solution, we
force |RE| = |ST | if |RE| < |ST |. Furthermore, the rover
must make observations at the locations in OB. G0 for the
rover includes G above, as well as a set of hidden goals.
Locations of the rover, RE, ST, OB and hidden goals are
randomly generated in the environment and do not overlap
in the initial state. Although seemingly simple, the state space
of this domain is on the order of 1020 .
2) Results: We use only plan features here. First, we
evaluate our approach to learning the labeling scheme (i.e.,
L‚àóCRF ) as the difference between MR and M‚àóR gradually

Fig. 3. Evaluation for predicting Œ∏ and Œ≤ measures as the difference between
MR and M‚àóR increases (i.e., as the maximum number of hidden goals
increases).

increases (i.e., as the number of hidden goals increases).
Afterwards, we evaluate the effectiveness of plan selection and
synthesis with respect to the Œ∏ and Œ≤ measures. To verify that
our approach can generalize to different problem settings, we
fix the level of oscillation when generating training samples
while allowing it to vary in testing samples.
Using CRFs for Plan Explicability and Predictability: In
this evaluation, we randomly generate 1 ‚àí 3 hidden goals to
include in G0 in 1000 training samples. After the model is
learned, we evaluate it on 100 testing samples in which we
vary the maximum number of hidden goals from 1 to 6 with
step size 1. The result is presented in Fig. 3. We can see
that the prediction performance (i.e., the ratios between Œ∏ and
Œ≤ computed based on L‚àóCRF and L‚àó ) is generally between
50% ‚àí 150%, We can also see that the oscillation level does
not seem to influence the prediction performance much. This
shows that our approach is effective whether MR and M‚àóR are
similar or largely different.
Selecting Explicable and Predictable Plans: We evaluate
plan selection using Œ∏ and Œ≤ measures and compare the selected plans (denoted by EXPD-SELECT) with plans selected
by a baseline approach (denoted by RAND-SELECT). Given
a set of candidate plans, EXPD-SELECT selects a plan according to the highest predicted explicability or predictability
measure while RAND-SELECT randomly selects a plan from
the set of candidate plans. To implement this, for a given
public goal G, we randomly construct 20 problems with a
given level of oscillation as determined by the maximum
number of hidden goals. Each such problem corresponds to
a different G0 and a plan is created for it. The set of plans
for these 20 problems associated with the same G is the set
of candidate plans for G. For each level of oscillation, we
randomly generate 50 different Gs and then construct the set
of candidate plans for each G. The model here is trained with
1900 samples using the same settings as in our first evaluation
and we gradually increase the level of oscillation.
We compare the Œ∏ and Œ≤ values computed from the ground
truth labeling of the chosen plans. The result is provided in
Fig. 4. When the oscillation is small, the performances of
both approaches are similar. As the oscillation increases, the
performances of the two approaches diverge. This is expected
since RAND-SELECT randomly chooses plans and hence its
performance should decrease as the oscillation increases. On

Fig. 4.

Comparison of EXPD-SELECT and RAND-SELECT

the other hand, EXPD-SELECT is not influenced as much
although its performance also tends to decrease. This is partly
due to the fact that the model used in this evaluation is trained
with samples having a maximum of 3 hidden goals.
In Fig. 4 for explicability, almost all results are significantly
different at 0.001 level (except at 1); for predictability, results
are significantly different at 0.01 level at 3, 5 and 6. The
trend to diverge is clearly present. Note that we use linearchain CRFs in our evaluations, which does not directly model
correlations among observations across states. These features
are common in our rover domain (e.g., navigating back and
forth). Hence, we can anticipate performance improvement
with more general CRFs.
Synthesizing Explicable and Predictable Plans: We evaluate
here plan synthesis using Alg. 1. More specifically, we compare FF planner that considers the predicted Œ∏ and Œ≤ values in
its heuristics with a normal FF planner that only considers the
action cost. The FF planner with the new heuristic is called
FF-EXPD. In this evaluation, we set the maximum number of
hidden locations to visit to be 6. For each trial, we generate
100 problems and apply both FF and FF-EXPD to solve the
problems. Given that we are interested in comparing the cases
when explicability is low, we only consider problems when
the predicted plan explicability for the plan generated by FF
is below 0.85.
First, we consider the incorporation of Œ∏ only. The result
is presented in Fig. 5. For the explicability measure, we see
a significant difference in all trials. Another observation is
that the difference in plan predictability is present but not as
significant. This evaluation suggests that our heuristic search
can produce plans of high explicability.
Next, we consider the incorporation of Œ≤ only. The result is
presented in Fig. 6. Similarly, we see a significant difference
in all trials for both explicability and predictability. One observation is that improving on plan predictability also improves
plan explicability which is expected given Eqs. (7) and (8)).
Plan Cost: We consider plan cost here for the evaluation in
Fig. 6. The result is presented in Table I. We can see that the
plan length for FF-EXPD is longer than the plan produced by
FF in general. This is expected since FF only considers plan
cost. However, in all settings, FF-EXPD penalizes the plan
cost slightly (about 10%) to improve the plan explicability
and predictability measures.

Fig. 5.

Comparison of FF and FF-EXPD considering only Œ∏.

Fig. 6.

Comparison of FF and FF-EXPD considering only Œ≤.
TABLE I
P LAN S TEPS C OMPARISON FOR F IG . 6

Trial ID
FF (avg. # steps)
FF-EXPD (avg. # steps)

1
21.9
23.5

2
24.0
26.3

3
24.1
25.2

4
23.9
24.0

5
22.1
23.4

6
22.4
25.0

B. Evaluation with Physical Robots
In this section we evaluate our approach in a blocks
world domain with a physical robot. It simulates a smart
manufacturing environment where robots are working beside
humans. Although the human and robot do not have direct
interactions ‚Äì the robot‚Äôs goal is independent of the human‚Äôs,
generating explicable plan is still an important issue since it
will help humans concentrate more on their own tasks. Here,
we evaluate plans generated by the robot using FF-EXPD and
a cost-optimal planner (OPT) in various scenarios and compare
the plans with human subjects in terms of explicability.
1) Domain Description: In this domain, the robot‚Äôs goal
(which is known to the human) is to build a tower of a
certain height using blocks on the table. The towers to be
built have different heights in different problems. There are
two types of blocks, light ones and heavy ones, which are
indistinguishable externally but the robot can identify them
based on the markers. Picking up the heavy blocks are more
costly than the light blocks for the robot. Hence, the robot may
sometimes choose seemingly more costly (i.e., longer) plans
to build a tower from the human‚Äôs perspective.
2) Experimental Setup: We generated a set of 23 problems
in this domain in which towers of height 3 are to be built. The
plans for these problems were manually generated and labeled
as the training set. For 4 out of these 23 problems, the optimal
plan is not the most explicable plan. To remove the influence of

grounding, we also generated permutations of each plan using
different object names for these 23 problems, which resulted
in a total of about 15000 training samples. We then generated a
set of 8 testing problems for building towers of various heights
(from 3‚àí5) to verify that our approach can generalize. Testing
problems were generated only for cases where plans are more
likely to be inexplicable. For each problem, we generated
two plans, one using OPT and the other using FF-EXPD,
and recorded the execution of these plans on the robot. We
recruited 13 subjects on campus and each human subject was
tasked with labeling two plans (generated by OPT and FFEXPD respectively) for each of the 8 testing problems, using
the recorded videos and following a process similar to that
used in preparing training samples. After labeling each plan,
we also asked the subject to provide a score (1 ‚àí 10 with 10
being the most explicable) to describe how comprehensible
the plan was overall.
3) Results: In this evaluation, we only use one task label
‚Äúbuilding tower‚Äù. For all testing problems, the labeling
process results in 77.8% explicable actions (i.e., actions with
a task label) for OPT and 97.3% explicable actions for FFEXPD. The average explicability measures for FF-EXPD and
OPT are 0.98 and 0.78, and the average scores are 9.65 and
6.92, respectively. We analyze the results using a paired Ttest which shows a significant difference between FF-EXPD
and OPT in terms of the explicability measures (using Eq.
(7)) computed from the human labels and the overall scores
(p < 0.001 for both). Furthermore, after normalizing the
scores from the human subjects, the Cronbach‚Äôs Œ± value shows
that the explicability measures and the scores are consistent
for both FF-EXPD and OPT (Œ± = 0.78, 0.67, respectively).
These results verify that: 1) our explicability measure does
capture the human‚Äôs interpretation of the robot plans and 2)
our approach can generate plans that are more explicable to
humans. In Fig. 7, we present the plans for a testing scenario.
The left part of the figure shows the plan generated by OPT
and the right part shows the plan generated by FF-EXPD. A
video is also attached showing the different behaviors with the
two planners in this scenario.
VI. C ONCLUSION
While we are still far from having intelligent robots and
agents working side-by-side of humans as teammates (rather
than as tools), it becomes increasingly important to consider
issues when such autonomous agents appear in our everyday
life. These agents need to create and execute complex plans. In
this paper, we introduced plan explicability and predictability
for such agents so that they can synthesize plans that are more
comprehensible to humans. To achieve this, they must consider
not only their own models but also the human‚Äôs interpretation
of their models. To the best of our knowledge, this is the
first attempt to model plan explicability and predictability for
task planning which differs from previous work on humanaware planning. The proposed measures have a variety of applications (e.g., achieving fluent human-robot interaction and
ensuring human safety). To compute these measures, we learn

Fig. 7. Plan execution of two plans generated by OPT (left) and FF-EXPD
(right) for one out of the 8 testing scenarios. The top figure shows the setup
of this scenario where the goal is to build a tower of height 3. The block that
is initially on the left side of the table is a heavy block. The optimal plan
involves more actions with the light blocks (i.e., putting the two light blocks
on top of the heavy one) while the explicable plan is more costly since it
requires moving the heavy one.

the labeling scheme of humans for agent plans from training
examples based on CRFs. We then use this learned model to
label a new plan to compute its explicability and predictability.
The proposed approach is evaluated on a synthetic domain
and with human subjects using physical robots to show its
effectiveness. A natural extension of our work is to consider
human-robot teaming where there exists close interactions.
Humans in our current settings are observers.
Finally, while we focus on the explicability and predictability measures for robot task planning, they also have many other
interesting applications. For example, many defense applications use planning to create unpredictable and inexplicable
plans, which can help deter or confuse enemies and are also
useful for testing defenses against novel or unexpected attacks.
These applications can be implemented using our approach by
minimizing the Œ∏ and Œ≤ measures instead of maximizing them.

R EFERENCES
[1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
of the Twenty-first International Conference on Machine
Learning, ICML ‚Äô04, 2004.
[2] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith,
and Subbarao Kambhampati. Planning for serendipity. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2015.
[3] Eugene Charniak and Robert P. Goldman. A bayesian
model of plan recognition. Artificial Intelligence, 64(1):
53 ‚Äì 79, 1993. ISSN 0004-3702.
[4] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning for mobile robots. In Advanced Robotics,
2009. ICAR 2009. International Conference on, pages
1‚Äì7, June 2009.
[5] Gergely Csibra and GyoÃàrgy Gergely. Obsessed with
goals?: Functions and mechanisms of teleological interpretation of actions in humans. Acta psychologica, 124
(1):60‚Äì78, 2007.
[6] Anca Dragan and Siddhartha Srinivasa. Generating
legible motion. In Proceedings of Robotics: Science and
Systems, Berlin, Germany, June 2013.
[7] Terrence W Fong, Illah Nourbakhsh, and Kerstin Dautenhahn. A survey of socially interactive robots. Robotics
and Autonomous Systems, 2003.
[8] Maria Fox and Derek Long. Pddl2.1: An extension to
pddl for expressing temporal planning domains. J. Artif.
Int. Res., 20(1), December 2003.
[9] Moritz Gbelbecker, Thomas Keller, Patrick Eyerich,
Michael Brenner, and Bernhard Nebel. Coming up with
good excuses: What to do when no plan can be found.
In International Conference on Automated Planning and
Scheduling, 2010.
[10] Marc Hanheide, Moritz Gbelbecker, Graham S. Horn,
Andrzej Pronobis, Kristoffer Sj, Alper Aydemir, Patric
Jensfelt, Charles Gretton, Richard Dearden, Miroslav
Janicek, Hendrik Zender, Geert-Jan Kruijff, Nick Hawes,
and Jeremy L. Wyatt. Robot task planning and explanation in open and uncertain worlds. Artificial Intelligence,
2015. ISSN 0004-3702.
[11] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human‚Äìrobot fluency. Robotics,
IEEE Transactions on, 23(5):952‚Äì961, 2007.
[12] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency,
fluency, and perception of team. In Proceedings of
the ACM/IEEE international conference on Human-robot
interaction, pages 1‚Äì8. ACM, 2007.
[13] JoÃàrg Hoffmann and Bernhard Nebel. The ff planning
system: Fast plan generation through heuristic search. J.
Artif. Int. Res., 14(1):253‚Äì302, May 2001. ISSN 10769757.
[14] Henry A. Kautz and James F. Allen. Generalized Plan

[15]

[16]

[17]
[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

Recognition. In National Conference on Artificial Intelligence, pages 32‚Äì37, 1986.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ‚Äô01, pages 282‚Äì289, 2001.
ISBN 1-55860-778-1.
Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot
teams. In Twenty-Fourth International Conference on
Automated Planning and Scheduling, 2014.
Tom Murray. Authoring Intelligent Tutoring Systems:
An analysis of the state of the art.
Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and
Subbarao Kambhampati. Automated planning for peerto-peer teaming and its evaluation in remote human-robot
interaction. In ACM/IEEE International Conference on
Human Robot Interaction (HRI), 2015.
Fuchun Peng and Andrew McCallum. Information extraction from research papers using conditional random
fields. Information Processing & Management, 42(4):
963 ‚Äì 979, 2006. ISSN 0306-4573.
Miquel Ramƒ±ÃÅrez and Hector Geffner. Probabilistic plan
recognition using off-the-shelf classical planners. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA,
July 11-15, 2010, 2010.
Julie Shah, James Wiken, Brian Williams, and Cynthia
Breazeal. Improved human-robot team performance
using chaski, a human-inspired plan execution system.
In Proceedings of the 6th international conference on
Human-robot interaction, pages 29‚Äì36. ACM, 2011.
E.A Sisbot, L.F. Marin-Urias, R. Alami, and T. Simeon.
A human aware mobile robot motion planner. Robotics,
IEEE Transactions on, 23(5):874‚Äì883, Oct 2007.
Kartik Talamadupula, Gordon Briggs, Tathagata
Chakraborti, Matthias Scheutz, and Subbarao
Kambhampati.
Coordination in human-robot teams
using mental modeling and plan recognition.
In
Intelligent Robots and Systems (IROS 2014), 2014
IEEE/RSJ International Conference on, pages 2957‚Äì
2962, Sept 2014.
Robin R. Vallacher and Daniel M. Wegner. What do people think they‚Äôre doing? action identification and human
behavior. Psychological Review, 94(1):3‚Äì15, 1987.
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborty,
and Subbarao Kambhampati. A human factors analysis
of proactive assistance in human-robot teaming. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2015.
Yu Zhang, Sarath Sreedharan, and Subbarao Kambhampati. Capability models and their applications in
planning. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
AAMAS ‚Äô15, 2015.

Human Computation and Crowdsourcing: Works in Progress Abstracts:
An Adjunct to the Proceedings of the Third AAAI Conference on Human Computation and Crowdsourcing

Acquiring Planning Knowledge via Crowdsourcing
Jie Gaoa and Hankz Hankui Zhuob and Subbarao Kambhampatic and Lei Lib
a
Zhuhai college,
Jilin Univ., Zhuhai, China
jiegao26@163.com

b

School of Data & Computer Sci.,
Sun Yat-sen Univ., China
{lnslilei,zhuohank}@mail.sysu.edu.cn

Introduction

c

Dept. of Computer Sci. & Eng.,
Arizona State Univ., US
rao@asu.edu

initial states and action models based on the answers to HITs
given by the crowd. We then feed the reÔ¨Åned initial states and
action models to planners to solve the problem.

Plan synthesis often requires complete domain models and
initial states as input. In many real world applications, it is
difÔ¨Åcult to build domain models and provide complete initial state beforehand. In this paper we propose to turn to the
crowd for help before planning. We assume there are annotators available to provide information needed for building
domain models and initial states. However, there might be a
substantial amount of discrepancy within the inputs from the
crowd. It is thus challenging to address the planning problem
with possibly noisy information provided by the crowd. We
address the problem by two phases. We Ô¨Årst build a set of
Human Intelligence Tasks (HITs), and collect values from
the crowd. We then estimate the actual values of variables
and feed the values to a planner to solve the problem.
In contrast to previous efforts (Zhang et al. 2012;
Manikonda et al. 2014) that ask the crowd to do planning,
we exploit knowledge about initial states and/or action models from the crowd and feed the knowledge to planners to
do planning. We call our approach PAN-CROWD, stands for
Planning by Acquiring kNowledge from the CROWD.

Building HITs for Action Models: For this part, we build
on our work with the CAMA system (Zhuo 2015). We enumerate all possible preconditions and effects for each action.
SpeciÔ¨Åcally, we generate actions‚Äô preconditions and effects
as follows (Zhuo and Yang 2014). If the parameters of predicate p, denoted by Para(p), are included by the parameters
of action a, denoted by Para(a), i.e., Para(p) ‚äÜ Para(a), p is
likely a precondition, or an add effect, or a delete effect of a.
We therefore generate three new proposition variables ‚Äúp ‚àà
Pre(a)‚Äù, ‚Äúp ‚àà Add(a)‚Äù and ‚Äúp ‚àà Del(a)‚Äù, the set of which
is denoted by Hpre = {p ‚àà Pre(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A},
Hadd = {p ‚àà Add(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, Hdel = {p ‚àà
Del(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, respectively. We put all the
proposition variables together and estimate the label of each
variable by querying the crowd or annotators.
For each proposition in H, we build a Human Intelligent
Task (HIT) in the form of a short survey. For example, for
the proposition ‚Äú(ontable ?x) ‚àà Pre(pickup)‚Äù, we generate
a survey as shown below: Is the fact that ‚Äúx is on table‚Äù
a precondition of picking up the block x? There are three
possible responses, i.e., Yes, No, Cannot tell, out of which
an annotator has to choose exactly one. Each annotator is
allowed to annotate a given survey once. Note that the set of
proposition variables H will not be large, since all predicates
and actions are in the ‚Äúvariable‚Äù form rather than instantiated
form, and we only consider predicates whose parameters
are included by actions. For example, for the blocks domain,
there are only 78 proposition variables in H.

The Formulation of Our Planning Problem
We formulate our planning problem as a quadruple P =
sÀú0 , g, O, AÃÑ, where sÀú0 is an incomplete initial state which
is composed of a set of open propositions. A proposition is
called open if there exist variables in the parameter list of
the proposition, e.g., ‚Äúon(A, ?x)‚Äù (a symbol preceded by ‚Äú?‚Äù
denotes a variable that can be instantiated by an object) is an
open proposition since ?x is a variable in the parameter list
of proposition on. An open initial state can be incomplete,
i.e., some propositions are missing. The set of variables in
sÀú0 is denoted by V. O is a set of objects that can be selected
and assigned to variables in V. We assume O can be easily
collected based on historical applications. AÃÑ is a set of incomplete STRIPS action models. aÃÑ ‚àà AÃÑ is called ‚Äúincomplete‚Äù
if there are some predicates missing in the preconditions or
effects of aÃÑ. A solution to the problem is a plan and a set of
‚ÄúreÔ¨Åned‚Äù action models.

Building HITs for Initial States: To generate surveys that
are as simple as possible, we assume there are sets of objects
known to our approach, each of which corresponds to a type.
For example, {A, B, C} is a set of objects with type ‚ÄúBlock‚Äù
in the blocks domain, i.e., there are three blocks known to our
approach. We can thus generate a set of possible propositions
S with the sets of objects and predicates of a domain. For
each proposition, we formulate the Human Intelligent Task
(HIT) as a short survey, the set of which is denoted by H.
To reduce the number of HITs, we start from the goal g, we
search for the action whose add effects match with g, and
update g to be an internal state by deleting add effects and
adding preconditions of the action into g. We then check

The PAN-CROWD approach
To acquire planning knowledge from the crowd, we Ô¨Årst build
HITs for action models and initial states, and then reÔ¨Åne
c 2015, Association for the Advancement of ArtiÔ¨Åcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

6

base on a ratio Œ±, resulting in open problems P . Propositions
were then changed to short surveys in natural language. We
performed extensive simulations using 20 simulated annotators to complete the human intelligent tasks (HITs). We
deÔ¨Åne the accuracy of our approach by comparing to groundtruth solutions (generated by simulators). In other words, we
solve all 150 problems using our approach, and compare the
150 solutions to ground-truth solutions, viewing the ratio of
identical solutions over 150 as the accuracy.
We ran our PAN-CROWD algorithm on testing problems by varying from 0.1 to 0.5 the ratio Œ±, setting the number of objects to be
10. The result is shown in Figure 1.
From Figure 1, we can see that the
accuracy generally becomes lower
Figure 1: The accuwhen the ratio increases in all three
racy w.r.t. ratio Œ±.
testing domains. This is consistent
with our intuition, since the larger the ratio is, the more
uncertain the information that is introduced to the original
planning problem when using crowdsourcing. However, from
the curves we can see the accuracies are no less than 70%
when the ratio is smaller than 0.3.

whether open initial state matches with internal state. Secondly, from the matched internal state, we select those with
unknown variables, and choose them with the same variables
to build a set of propositions with unknown variables, which
is Ô¨Ånally transformed into a set of HITs.
Estimating True Labels Assume there are R annotators
and N tasks with binary labels {1, 0}. The true labels of
tasks are denoted by Z = {zi ‚àà {1, 0}, i = 1, 2, . . . , N }
. Let Nj is the set of tasks labeled by annotator j, and Ri
is the set of annotators labeling task i. The task assignment
scheme can be represented by a bipartite graph where an
edge (i, j) denotes that the task i is labeled by the worker j.
The labeling results form a matrix Y ‚àà {1, 0}N √óR . The goal
is to Ô¨Ånd an optimal estimator ZÃÇ of the true labels Z given
the
observation Y, minimizing the average bit-wise error rate
1
i‚àà{1,2,...,N } prob[zÃÇi = zi ].
N
We model the accuracy of annotators separately on the
positive and negative examples (Raykar et al. 2010). If the
true label is one, the true positive rate T P j for the jth annotator is deÔ¨Åned as the probability that the annotator labels
it as one, i.e., T P j = p(yij = 1|zi = 1). On the other hand
if the true label is zero, the true negative rate T N j is deÔ¨Åned as the probability that annotator labels it as zero, i.e.,
T N j = p(yij = 0|zi = 0). Suppose we have the training
data set D = {xi , yi1 , . . . , yiR }N
i=1 with N instances from
R annotators, where xi ‚àà X is an instance (typically a ddimensional feature vector), yij is the label (1 or 0) given
by the jth annotator. Considering the family of linear discriminating functions, the probability for the positive class is
modeled as a logistic sigmoid, i.e., p(y = 1|x, w) = œÉ(wT x),
where x, œâ ‚àà Rd , and œÉ(z) = 1+e1‚àíz .
The task is to estimate the parameter w as well as the
true positive P = T P 1 , . . . , T P R  and the true negative
N = T N 1 , . . . , T N R . Let Œ∏ = {w, P, N }, the probability of training data D can be deÔ¨Åned by p(D|Œ∏) =
N
1
R
i=1 p(yi , . . . , yi |xi , Œ∏). The EM algorithm can be exploited to estimate the parameter Œ∏ by maximizing the loglikelihood of p(D|Œ∏) (Raykar et al. 2010). Let Œºi = p(zi =
1|yi1 , . . . , yiR , xi , Œ∏). We simply set the threshold as 0.5, i.e.,
if Œºi > 0.5, the value of zÃÇi is assigned with 1, otherwise 0.










	
	













Discussion and Conclusion
We propose to acquire knowledge about initial states and
action models from the crowd. Since the number of HITS
sent to the crowd related to initial states could be large, in the
future, we could consider how to reduce the number of HITS,
e.g. by exploiting backward chaining planning techniques ‚Äì
which only might need to know small parts of the initial state,
rather than the whole state. In addition, we could also think
of ways of engaging the crowd in more active ways, rather
than answering yes/no to HITS. For example, we can give
them candidate plans and ask them to critique the plan and/or
modify them to make them correct. We then learn knowledge
from the modiÔ¨Åcation process and use it for computing plans.

Acknowledgements
Zhuo‚Äôs research is supported by NSFC (No. 61309011) and
Fundamental Research Funds for the Central Universities
(No. 14lgzd06). Kambhampati‚Äôs research is supported in part
by a Google Research Award and the ONR grants N0001413-1-0176 and N00014-15-1-2027.

References
Manikonda, L.; Chakraborti, T.; De, S.; Talamadupula, K.; and
Kambhampati, S. 2014. AI-MIX: using automated planning to
steer human workers towards better crowdsourced plans. In IAAI,
3004‚Äì3009.
Raykar, V. C.; Yu, S.; Zhao, L. H.; Valadez, G. H.; Florin, C.; Bogoni,
L.; and Moy, L. 2010. Learning from crowds. JMLR 11:1297‚Äì1322.
Zhang, H.; Law, E.; Miller, R.; Gajos, K.; Parkes, D. C.; and Horvitz,
E. 2012. Human computation tasks with global constraints. In CHI,
217‚Äì226.
Zhuo, H. H., and Yang, Q. 2014. Action-model acquisition for
planning via transfer learning. Artif. Intell. 212:80‚Äì103.
Zhuo, H. H. 2015. Crowdsourced action-model acquisition for
planning. In AAAI, 3004‚Äì3009.

Experiment
Since the action model acquisition part has been evaluated
in the context of CAMA already (Zhuo 2015), here we focus
on evaluating the initial state acquisition part. We evaluated
our approach in three planning domains, i.e., blocks1 , depots2
and driverlog4 . We Ô¨Årst generated 150 planning problems
with complete initial states, denoted by PÃÑ . After that we
randomly removed propositions from the initial states in PÃÑ
2


	



ReÔ¨Åning Initial States and Action Models: We reÔ¨Åne the
initial state and action models based on estimated true labels.
Once the initial state and action models reÔ¨Åned, we solve
the corresponding revised planning problem using an off-theshelf planner.

1





http://www.cs.toronto.edu/aips2000/
http://planning.cis.strath.ac.uk/competition/

7

Compliant Conditions for Polynomial Time Approximation of Operator Counts
Tathagata Chakraborti Sarath Sreedharan Sailik Sengupta T. K. Satish Kumar Subbarao Kambhampati Dept. of Computer Science, Arizona State University Dept. of Computer Science, University of Southern California


arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

 

{tchakra2, ssreedh3, sailiks, rao}@asu.edu  tkskwork@gmail.com

Abstract In this paper, we develop a computationally simpler version of the operator count heuristic for a particular class of domains. The contribution of this abstract is threefold, we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer approximation for operator counts in polynomial time; and (3) discuss the relationship of the proposed formulation to existing heuristics and investigate properties of domains where such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model. The domain is described by a set of variables f  F which can assume values from a (finite) domain D (f )  N. A state is given by the particular assignment of values to these variables: S = {f = v | v  D (f ) f  F }. The value of variable f in state S is referred to as S(f ). The action model A consists of operators a = Ca , Ea where Ca is the cost of the action, and Ea = { f, vo , vn | f  F ; vo , vn  {-1}  D (f )} is the set of effects. The transition function  (∑) determines the next state after the application of action a to state S as (a, S) =  if  f, vo , vn  Ea s.t. vo = -1  vo = S(f ); = {f = vn  f, vo , vn  Ea ; else f = S(f )} otherwise.

Plans and Operator Counts. A planning problem is a tuple  = F , A, I, G , where I, G are the initial and (partial) goal states respectively. The solution to the planning problem is a plan  = a1 , a2 , . . . ,  (i) = ai  A such that  (, I) |= G, where the cumulative transition function is given by  (, S) =  ( a2 , a3 , . . . ,  (a1 , S)). The cost of the plan is given by C ( ) = a Ca and an optimal plan   is such that C (  )  C ( )  . The operator count for an action a given a plan  is given by (a,  ) = |{i | a =  (i)}| and the total operator count of the plan ( ) = | |. 1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables. We define compliant variables as those that whenever they occur as a precondition of an action, they must also be an effect, and vice versa. Thus, f  F is compliant iff a  A, f, vo, vn  Ea = vo = -1  vn = -1; f is referred to as rogue otherwise. Let   F be the set of all compliant variables, and the set of compliant variables whose values are specified in the goal be   , henceforth referred to as goal compliant conditions. The State Transformation Equation. Let || = m and |A| = n. Consider an m ◊ n matrix M whose ij th element Mij  Z is the numerical change in fi   produced by action aj  A, i.e. Mij = vn - vo ; fi , vo , vn  Eaj . Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f   from the current state to the final state, i.e. di = vg - vc ; vg = fi  G, vc = fi  S; and let x be a vector of size n, whose ith element is xi  N. Then the following equality holds: Mx = D (1)

The integer solution x to this system of linear equations with the least |x | gives a lower bound on the operator counts required to solve the planning problem, i.e. |x |  |  |. We can compute a real-valued approximation in closed-form, by min ||Qx||2 2 s.t. Mx = D using the Lagrangian multiplier method for this optimization problem as follows 1 L(x) = ||Qx||2 + T (D - Mx) 2 = x = Q-2 MT (MQ-2 MT )-1 D (4) (5) (2) (3)

Here Q is a n ◊ n matrix of action costs whose ij th entry Qij = Cai if i = j ; 0 otherwise (for unit cost domains) Q is an identity matrix and x = MT (MMT )-1 D The most costly operation here is the calculation of the pseudo inverse, which can be done in  O(n2.3 ) time. Further, M is problem independent, and hence the factor Z = Q-2 MT (MQ-2 MT )-1 can be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a heuristic for state-space search. Note that this formulation can also determine infeasibility of goal reachability immediately (in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining the closed-form polynomial bound heuristic, also makes the heuristic inadmissible. Sparse coding. Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian method described above does not address this aspect giving rise to bad heuristic values for certain section of problems. To describe this problem geometrically, we consider a planning domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm calculated above results in small fractional values, and hence a less informed heuristic. As can 2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search procedure P RE - COMPUTE() Compute M, Q Convert M to row echelon form  T is the transformation matrix, r is the rank Y  M[1 : r, :], Z  Q-2 Y T (YQ-2 Y T )-1 procedure h(S) = OP-COUNT(S, G) Compute D = G - S Compute T d = T ◊ D and  = Td [1 : r ] if td i = 0 i  r + 1 then No solution! else return Q ◊ Z ◊   be seen in the figure 1, the actual operator counts for the given example (with M = 15 4 and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead of |  | = 3. x2 Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2 Thus, we propose a different approximation method to obtain integer values for individual operator counts, remaining within the polynomial time bound. We notice that in most cases n  m and also n  |x | due to the combinatorial explosion during grounding of domains. Thus, we propose an operator count heuristic that exploits this knowledge about the sparsity of x . Ideally, we would like to solve the following problem, min |x|l0 Mx = D x 0

s.t.

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems. Firstly, the optimal operator counts (x ), although sparse, might not be the sparsest solution. Secondly, minimizing the l0 norm is NP-hard [5]. Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted l1 -norm ( -l1 -LP) [4] to enforce positive integer solutions. 3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic (hl1 = 1.60 and h-l1 = 3.4) for the aforementioned problem. This method tries to compress (minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing the increase along the tightest dimension till convergence is reached or maximum number of iterations are achieved, resulting in a more informed heuristic (h-l1 = 3.4). x2 Mx = D x2 Mx = D

l1 norm x1  -l1 norm x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2 For  -l1 -LP, we empirically observe that rounding up the individual operator counts produce a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions. Evaluations. The table shows the evaluation of the proposed heuristics across a total of 83 problems from five well-known unit cost planning domains. Each entry in the table represents the percentage difference in the initial state heuristic value and the optimal plan length averaged across the problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (`-' indicates that the heuristics could not be computed due to absence of any goal complaint variables). Rows 3-6 show the performance in domains where the %-compliance was increased (this was done by identifying instances in the action model where variables assume a don't care condition, i.e. a value of -1, and replacing it with appropriate values as entailed by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with more completely specified goals (which results in higher percentage compliance). As expected, our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and  -l1 LP highlights the usefulness of compressed sensing techniques in obtaining better integer approximations to the MILP.

4

Domains GED Blocks-3ops Blocks-4ops Visitall GED Blocks-3ops Blocks-4ops Visitall Blocks-3ops Blocks-4ops 8-puzzle

%-compliance 34.29% 31.25% 19.64% 25.49% 31.25% 19.64% 21.75% 48.13% 42.86% 88.89%

l1 -MILP 55.48% 47.80% 67.71% 37.61% 47.80% 67.71% 28.41% 28.68% 56.25% 33.33%

l1 -LP 55.48% 47.80% 67.71% 34.02% 47.80% 67.71% 28.41% 28.68% 56.25% 40.00%

 - l1 -LP 75.76% 23.60% 35.42% 53.36% 23.60% 35.42% 44.37% 44.38% 12.50% 46.67%

OP-COUNT 55.48% 52.60% 67.71% 48.32% 52.60% 67.71% 100.00% 32.32% 64.58% 40.00%

Discussion and Related Work
Relation to Existing Heuristics. The proposed heuristic has close associations with both heuristics on state change equations and operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very succinctly and are thus extremely useful where such properties are relevant. Another interesting connection to existing work is with respect to graph-plan based heuristics [2], except here we are relaxing preconditions instead of delete effects. Compliance. Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate representations such as transition normal form (TNF) [7] should be investigated in this context. Landmarks. Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version of this formulation here, but it can be easily extended to incorporate more informative features like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net change from D: di  di - ka ◊ (xn - xo ) if di , xo , xn  Ea and a  A is an action landmark with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This demonstrates the flexibility of our approach. Resource Constrained Interaction. The approach is especially relevant in the context of multi-agent interactions constrained by usage   ( ) of a shared resource  by a plan   of an agent . For example, in an adversarial setting, if an agent 2 wanted to stop 1 from executing its plan, all it needs to do is to ensure that  s.t.  1 ( ) +  2 ( ) > | |. Similarly, in a cooperative setting, if agent 2 wanted to ensure that 1 's plan succeeds, it would need to make sure that   1 ( ) +  2 ( )  | |.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an agent's intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40≠42, 2010. [2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281≠300, 1997. [3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning: Landmarks and merges. In ICAPS, 2014. [4] Emmanuel J Cand` es, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier analysis and applications, 2008. [5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization. Mathematical programming, 129(2):285≠299, 2011. [6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995. [7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In ICAPS, pages 188≠192, 2015. [8] Florian Pommerening, Gabriele R® oger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014. [9] Julie Porteous, Laura Sebastia, and J® org Hoffmann. On the extraction, ordering, and usage of landmarks in planning. In ECP, pages 37≠48, 2001. [10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651≠665. Springer, 2007.

6

Explicable Robot Planning as Minimizing Distance
from Expected Behavior

arXiv:1611.05497v1 [cs.AI] 16 Nov 2016

Anagha Kulkarni1 , Tathagata Chakraborti1 , Yantian Zha1 ,
Satya Gautam Vadlamudi2 , Yu Zhang1 , and Subbarao Kambhampati1
Abstract‚Äî In order for robots to be integrated effectively into
human work-flows, it is not enough to address the question of
autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task
plans without such considerations, they may often demonstrate
what we refer to as inexplicable behavior from the point of view
of humans who may be observing it. This problem arises due to
the human observer‚Äôs partial or inaccurate understanding of the
robot‚Äôs deliberative process and/or the model (i.e. capabilities
of the robot) that informs it. This may have serious implications
on the human-robot work-space, from increased cognitive load
and reduced trust in the robot from the human, to more serious
concerns of safety in human-robot interactions. In this paper, we
propose to address this issue by learning a distance function that
can accurately model the notion of explicability, and develop
an anytime search algorithm that can use this measure in its
search process to come up with progressively explicable plans.
As the first step, robot plans are evaluated by human subjects
based on how explicable they perceive the plan to be, and
a scoring function called explicability distance based on the
different plan distance measures is learned. We then use this
explicability distance as a heuristic to guide our search in order
to generate explicable robot plans, by minimizing the plan
distances between the robot‚Äôs plan and the human‚Äôs expected
plans. We conduct our experiments in a toy autonomous car
domain, and provide empirical evaluations that demonstrate
the usefulness of the approach in making the planning process
of an autonomous agent conform to human expectations.

I. I NTRODUCTION
Recent advancement in the field of robotics has given
us autonomous robots, vehicles, drones, etc. Typically these
autonomous systems have the capability to make their own
plans which help them achieve their goals. These advances
have, naturally, encouraged the possibility of human-robot
teaming where the autonomous robots and humans can
work alongside each other. However, if the plans that are
being generated by the autonomous robots are difficult to
comprehend for the human observer, the unexpected behavior
from the robot can raise several concerns: it may increase
cognitive load, hamper the productivity of the team, and
result in safety concerns and distrust towards the robot [1].
This mismatch between the robot‚Äôs plans and the human
expectations may be explained in terms of difference in the
actual robot model and the human‚Äôs understanding of the
robot model. Thus, even with the knowledge of the robot‚Äôs
1 Anagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Yu Zhang, and
Subbarao Kambhampati are with the Computer Science and Engineering
Department at Arizona State University { akulka16, tchakra2,

yantian.zha, yzhan442, rao } @ asu.edu
2 Satya

Gautam

Vadlamudi

vsatyagautam@gmail.com

is

with

Capillary

Technologies.

Fig. 1: A schematic diagram of the proposed system. Explicability distance is the plan distance measure between the
robot plan and human expected plan. This distance is used
to guide the search to generate explicable plans.

goals, it may still not be possible for the human to make
sense of the robot‚Äôs plan. For example, consider a scenario
with an autonomous car switching lanes on a highway. The
autonomous car, in order to switch the lane, may make
sharp and calculated moves, as opposed to gradually moving
towards the other lane. These moves may well be optimal for
the car, and backed by the car‚Äôs superior sensing and steering
capabilities. Nevertheless, a human passenger sitting inside
may perceive this as dangerous and reckless behavior, in as
much as they might be ascribing the car the sort of driving
abilities they themselves have.
To address this issue of the differences between the robot
model, MR (R), and the human mental model of the robot
capabilities, MH (R), we develop an approach based on the
concept of explicability. An explicable plan is a plan that is
generated with the human‚Äôs expectation of the robot model;
the ability to synthesize explicable plans on the part of the
robot thus involves the ability to take into consideration
both models into its plan generation process. The intuition is
that, the similarity between the robot plan that is generated
from the robot model, and the plan that is generated from
the human understanding of the robot model determines
the explicability of the robot plan. More specifically, the
smaller the explicability distance between these two plans,
the more explicable the robot plan is. Of course, such a

similarity metric is not readily available, which brings us
to the question - How can a robot learn a distance function
between plans that model the notion of explicability, and how
can it use this learned similarity model to inform its own
deliberative process? Keeping this in mind, we address the
following questions in our paper: 1) Given a domain, can we
find an approximation to MH (R), the human mental model
of the robot capabilities? 2) Can the measures of distance
between a robot plan, œÄR , and the plan expected by the
human, œÄH , effectively capture the explicability of the robot
plan? 3) Can we then integrate the explicability estimates
into the robot plan generation process? The outline of our
proposed approach is illustrated in Figure 1.
To address the first question, we start out with a robot
model, MR (R) and generate different robot plans for various initial and goal states. Next, we recruit human subjects
and ask them to evaluate these plans by assigning scores
to them based on how well they understand them. As a
part of the study, the subjects are then asked to answer a
questionnaire based on the robot model, in order to elicit
implicit human preferences. This questionnaire allows the
domain modeler to generate MH (R), based on humans
assumptions regarding the robot model in that domain. In this
paper, we represent both models in PDDL [2], but they can
differ in terms of their action representations, preconditions,
effects, and costs.
To answer the second question, we explore the relationship
between three existing plan distance measures: action set,
causal link set and state sequence distances [3], [4] and the
plan explicability distance. We use the robot plans, assigned
with scores, to determine if the explicability of the plans can
be modeled in terms of the aforementioned plan distance
measures, in terms of a regression function. For this, we
generate the plans expected by the human for the same initial
and goal states using the human understanding of the robot
model. We then compute the plan distances between the robot
plans and human expected plans. We call the function that
maps the plan distances to the explicability scores as the
explicability distance.
To address the third question, we integrate the explicablity
distance in the search process of the Fast-Downward
planner [5]. We perform a cost-bounded anytime search,
that can progressively generate more and more explicable
plans, using the learned explicability distance as a heuristic
guidance. We call this reconciliation search. Note that explicability distance exhibits non-monotonicity, i.e. a new action
that gets added to a plan prefix can either increase or decrease
the explicability distance depending on the context of the
plan. We present an analysis on how this property affects
our search. For evaluation of our system, we demonstrate
the effectiveness of our system in a simulated autonomous
car domain, and use human test subjects to evaluate the
explicability of the generated robot plans.
II. RELATED WORK
The notion of robots working alongside humans for task
achievement has been a popular research direction. It is

challenging, mainly due to the fact that, the robot must
consider the human in the loop while making its own
decisions. One important requirement for achieving this, is
the ability to infer about the human‚Äôs intent and plan. Various
plan recognition algorithms [6], [7] can be applied to perform
plan recognition based on a given set of observations as a
result of the agent interacting with the environment. After
the intent and the plan of the human is identified, researchers
have also discussed how the robot can utilize this information
while avoiding conflicts [8], [9] or providing proactive help
to the human in the loop [10], [11]. There is also work
on performing simultaneous plan recognition and generation
[12]. However, most of the prior work has only focused on
how robots can make plans based on the inferred human
intent.
The motivation for generating explicable task plans was
first provided in our recent paper [13]. While that work
proposes learning explicability as a labeling scheme, in this
work, we consider viewing explicability more directly in
terms of distances between the plans generated by the robot‚Äôs
own model, and the human‚Äôs approximation of the robot‚Äôs
model. While explicability focuses on task plans, a related
notion of ‚Äúlegibility‚Äù has been studied in the context of
motion planning [14] and has been shown to be useful in
generating socially acceptable behaviors for robots [15], [16].
In most human-robot cohabitation work where robots are
proactive agents, it is often assumed that the human model is
provided and complete for inferring about the human intent
and plan. This is often not true. Although we also assume
a human model a priori, our formulation allows us to adjust
this model so as to improve model incompleteness (e.g.,
action preference). There also exists learnable models that
do not assume completeness in the first place [17]. Another
note is that in [13], [14] and this work, since the model is
one level deeper, which is about the robot model from the
humans perspective, learning methods are adopted.
III. BACKGROUND
A. Planning
A classical planning problem can be defined as a tuple
P = hM, I, Gi, where M = hF, Ai is the domain model
(that consists of a finite set F of fluents that define the
state of the world and a set of operators or actions A), and
I ‚äÜ F and G ‚äÜ F are the initial and goal states of the
problem respectively. Each action a ‚àà A is a tuple of the
form hpre(a), ef f (a), c(a)i where c(a) denotes the cost of
an action, pre(a) ‚äÜ F is the set of preconditions for the
action a and ef f (a) ‚äÜ F is the set of the effects. The
solution to the planning problem is a plan or a sequence
of actions œÄ = ha1 , a2 , . . . , an i such that starting from
the initial state, sequentially executing the actions lands the
robot in the goal state, i.e. ŒìM (I, œÄ) |= G where ŒìM (¬∑)
is the transition function defined P
for the domain. The cost
of the plan, denoted as c(œÄ) =
ai ‚ààœÄ c(ai ), is given by
the summation of the cost of all the actions in the plan œÄ.
Henceforth, we denote the robot plan as œÄ R and the human
expected plan as œÄ H .

2) Causal Link Distance: A causal link represents a tuple
of the form hai , pi , ai+1 i, where pi is a predicate variable
that is produced as an effect of action ai and used as a precondition for the next action ai+1 . The causal link distance
measure is represented similarly to the action distance, by
considering the causal link sets Cl(œÄ R ) and Cl(œÄ H ) instead
of action sets described above. It is written as:
Œ¥C (œÄ R , œÄ H ) = 1 ‚àí

Fig. 2: A simple illustration of how a robot‚Äôs optimal plan can
deviate from the human expectation due to model difference.
In this maze, the robot can move in all four direction: up,
down, left, right and also diagonally across the grid cells.
Some of the cell floors have glass floors and some others
have obstacles. The glass floors are harder for the robot to
navigate across, because of the reflective surface, it needs to
use special sensors which results in an expensive action for
the robot. The path in green is the explicable plan whereas
the path in red is the robot plan.

B. Plan Distance Measures
We now look at the three plan distance measures introduced in [3] and later refined in [4]. These plan distances are
action, causal link and state sequence distances. Although
these distance metrics do not satisfy certain mathematical
properties [18], they provide a good domain independent
measure of the difference between any two plans. Since the
goal is to predict the differences in terms of explicability
distance between the robot plans and human expected plans,
the intuition is that they can be approximated using a
combination of plan distance measures that capture different
aspects of plans.
1) Action Distance: We denote the set of unique actions
in a plan œÄ as A(œÄ) = {a | a ‚àà œÄ}. Given the action sets
A(œÄ R ) and A(œÄ H ) of two plans œÄ R and œÄ H respectively, the
action distance, Œ¥a , is computed as the ratio of the actions
that are exclusive to each plan to all the actions in the plans
[4]. It is written as:
Œ¥A (œÄ R , œÄ H ) = 1 ‚àí

|A(œÄ R ) ‚à© A(œÄ H )|
|A(œÄ R ) ‚à™ A(œÄ H )|

(1)

This simply means that two plans are similar (and hence
their distance measure is smaller) if they contain similar
actions. Note that this measure does not take the ordering
of actions into account.

|Cl(œÄ R ) ‚à© Cl(œÄ H )|
|Cl(œÄ R ) ‚à™ Cl(œÄ H )|

(2)

Again, plans are similar, with lower similarity scores, if
they have a large number of overlapping causal links.
3) State Sequence Distance: This distance measure, as
the name suggests, takes the sequences of the states into
consideration. This distance captures the context of an action
in a given plan. The length of the sequences may differ
and therefore there are multiple ways to define this distance
measure [4]. We use the representation shown in Eq. 3. Given
H
H
R
two state sequences (sR
0 , . . . , sn ) and (s0 , . . . , sn0 ) for œÄR
0
and œÄH respectively, where n ‚â• n are the lengths of the
plans, the state sequence distance is written as:
1
Œ¥S (œÄ , œÄ ) =
n
R

H

"

0

n
X

#
H
‚àÜ(sR
k , sk )

+n‚àín

0

(3)

k=1
|sR ‚à©sH |

H
k
k
where ‚àÜ(sR
k , sk ) = 1 ‚àí |sR ‚à™sH | represents the distance
k
k
between two states (where sR
k is overloaded to denote the set
of predicate variables in state sR
k ). The first term measures
the normalized difference between states up to the end of
the shorted plan, while the second term, in the absence of a
state to compare to, assigns maximum difference possible.
Here we illustrate the explicability distance with an example and discuss its relationship with the other distance
measures. Consider the grid structure shown in Figure 2.
Here we have a 5 by 5 grid. The bottom left cell is labeled
as (1, 1). Some of the cells have glass floors while some
others have obstacles. The robot has to find its way across
the obstacles from the start cell to the goal cell. Looking at
the grid structure, the human may expect the robot to take
an optimal path highlighted by the green arrows. Although
unbeknownst to the human, the robot has difficulty traveling
across the glass floor cells because of the reflective surface
and has to use special sensors while navigating across these
floors. Hence, the cost of treading on these glass floors
is higher than the cost of treading across normal cell. In
this case, the robot‚Äôs optimal plan to the goal is the one
highlighted in red, which doesn‚Äôt coincide with human‚Äôs
expectation of the robot plan.
In Figure 3, we provide the actions sets, causal link sets
and state sequences generated for both the robot plan and
human expected plan for our example illustrated in Figure 2.
The corresponding plan distances are shown in Table I. These
three distances capture different aspects of the plans. In this
case, the explicability distance clearly has a high correlation
with these other distance measures. Our goal in this paper

Initial State: at(1, 1)
Goal State: at(5, 4)
Actions:
A(œÄ R )
=
{ move-diagonal(2, 2), move-up(2, 3), move-up(2, 4), move-diagonal(3, 5),
move-diagonal(4, 4), move-right(5, 4) }
A(œÄ H ) = {move-diagonal(2, 2), move-diagonal(3, 3), move-diagonal(4, 4), move-right(5,
4) }
Causal Links:
Cl(œÄ R ) = { <move-diagonal(2, 2), at(2, 2), move-up(2, 3)>, <move-up(2, 3), at(2, 3),
move-up(2, 4)>, <move-up(2, 4), at(2, 4), move-diagonal(3, 5)>, <move-diagonal(3,
5), at(3, 5), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
Cl(œÄ H ) =
{ <move-diagonal(2, 2), at(2, 2), move-diagonal(3, 3)>, <move-diagonal(3,
3), at(3, 3), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
State sequences:
S(œÄ R ) = { {at(2, 2)}, {at(2, 3)}, {at(2, 4)}, {at(3, 5)}, {at(4, 4)} }
S(œÄ H ) = { {at(2, 2)}, {at(3, 3)}, {at(4, 4)} }
Fig. 3: The action sets, causal link sets and state sequence sets for the illustrated example in Figure 2. For the given initial
and goal state, the plans illustrated in red and green in Figure 2 are used to produce respective action, causal link and state
sequence sets.
TABLE I: Action distance, causal link distance and state
sequence distance computed using the sets provided in Figure
3, between the two plans, œÄ R and œÄ H , that are illustrated in
Figure 2.

In order to train our regression model, we use plan traces
whose actions were assigned scores by human subjects. We
can then calculate the explicability score of a plan based on
the average of the individual action scores.

Plan Pair

Œ¥A

Œ¥C

Œ¥S

B. Plan Generation

(œÄ R , œÄ H )

4/7

6/7

4/5

We now present the details of our plan generation phase,
where we use the explicability distance to guide our search to
generate the most explicable robot plan for a given problem.
1) Non-Monotonicity: We will now discuss the nonmonotonic behavior exhibited by explicability function and
how it affects the plan generation process. The explicability
distance function is non-monotonic in nature, meaning, as
the partial plan grows, the explicability distance may both
increase or decrease. This is because, a new action can either
contribute positively or negatively to the total explicability
score of the plan. As pointed out earlier, the explicability
score is computed as an average of the individual action
scores in the context of the plan prefix.
Observation 1: Explicability score of a partial plan P may
increase, stay equal, or even decrease when it is extended
with one or more actions.
Consider the following example, in a car domain, the goal
of the car is to move to the left lane. The car squeezes
leftwards in three consecutive actions and after coming to
the left lane, it turns on its left indicator. Here the turning on
of the left tail light after having moved left is an inexplicable
action. The previous three actions were explicable to the
human drivers and contribute positively to the explicability

is, then, to learn to establish a general relationship between
the established measures of plan distance.
IV. PROPOSED METHODOLOGY
A. Explicability Distance
Since, without the model we do not know which plan
distance is most relevant in capturing explicability, we
present a general formulation in this section. A more
detailed formulation can be found in the following section. Let ‚àÜ be a 3-dimensional vector, such that for a
robot plan, œÄ R , derived from MR (R), and for an explicable plan œÄ H , derived from MH (R), we have ‚àÜ =
hŒ¥A (œÄ R , œÄ H ), Œ¥C (œÄ R , œÄ H ), Œ¥S (œÄ R , œÄ H )iT . We now define
explicability distance of a robot plan, Exp(œÄ R ), as a regression based function of the three plan distances, with b
as the parameter vector:
Exp(œÄ R / œÄ H ) ‚âà f (‚àÜ, b)

(4)

score of the plan but the last action has a negative impact
and decreases the score. Therefore this score and in turn
the explicability distance is not a non-decreasing function.
In essence, depending on the context, the explicability of an
action can either improve the score or worsen it.
Observation 2: A greedy method that expands a node
with the highest explicability score of the corresponding
partial plan at each step does not guarantee to find an
optimal explicable plan (one of the plans with the highest
explicability score) as its first solution.
The above observation is easy to see since, if e1 is
explicability score of the first plan, then a node may exist
in open list (set of unexpanded nodes) whose explicability
score is less than e1 , which when expanded may result in a
solution plan with explicability score higher than e1 .
2) Reconciliation Search: Given the non-monotonic nature of explicability distance function, we have to generate
all the candidate plans in order to find the most explicable
plan. Here, we present a cost-bounded anytime greedy search
algorithm called reconciliation search that generates all the
valid loopless candidate solution plans up to a given cost
bound, and then progressively searches for plans with better
explicability scores. The value of the heuristic h(v) in a
particular state v encountered during search is based entirely
on the explicability distance of the robot plan prefix up to
that state, given by,
h(v) = Exp(œÄ / œÄh )
s.t. ŒìMR (R) (I, œÄ) = v

Algorithm 1 Reconciliation Search
Input: Planning problem P = hMR (R), I, Gi, cost bound
max cost, and explicability distance function Exp
Output: Robot plan with the highest explicability score
œÄ R = arg maxœÄR Exp(œÄ R / œÄH )
1: S ‚Üê ‚àÖ
. Candidate plan solution set
2: open ‚Üê ‚àÖ
. Open list
3: closed ‚Üê ‚àÖ
. Closed list
4: open.insert(I, 0, inf)
5: while open 6= ‚àÖ do
6:
n ‚Üê open.remove()
. Node with highest h(¬∑)
7:
if n |= G then
8:
S.insert(œÄ s.t. ŒìMR (R) (I, œÄ) |= v)
9:
end if
10:
closed.insert(n)
11:
for each v ‚àà successors(n) do
12:
if v ‚àà
/ closed then
13:
if g(n) + cost(n, v) ‚â§ max-cost then
14:
open.insert(v, h(v))
15:
end if
16:
else
17:
if h(n) < h(v) then
18:
closed.remove(v)
19:
open.insert(v, h(v))
20:
end if
21:
end if
22:
end for
23: end while
24: return arg maxœÄ R ‚ààS Exp(œÄ R / œÄH )

and ŒìMH (R) (I, œÄh ) = v
Since we want to find explicable plans which are within
a cost bound, we use the cost of the plan to prune the
nodes in the search graph whenever they exceed the given
maximum cost bound. We implement this search in the
Fast-Downward planner [5]. The approach is described
in detail in Algorithm 1.
At each iteration of the algorithm, the plan prefix of the
robot model is compared with the explicable trace œÄh (these
are the plans generated by the human mental model of the
robot MH (R) up to the current state in the search process)
for the given problem. Using the computed distances, we
predict the explicability score for every candidate robot plan.
The search algorithm then makes a locally optimal choice
of states. After generating the first solution plan we do not
stop the search but instead continue to find all the valid
loopless candidate solution plans within the given cost bound
or until the state space is completely explored. In the end, the
candidate plan with highest explicability score is returned.
V. E XPERIMENTAL A NALYSIS
A. Autonomous Car Simulation Experiment
1) Domain Model: Autonomous cars are a topic of interest from the point of view of explicability problem. In the
recent past, Google‚Äôs self-driving cars [19] have been in the
news for being ‚Äútoo safe‚Äù on the roads. These autonomous
cars governed by strict traffic rules find it hard to blend

in and make judgments that would not make sense in a
predominantly human environment. At four-way stops, these
cars find it difficult to cross the intersection, while the human
drivers keep inching forward. For a robot car, such situations,
where it does not make an explicable decision can pose
problems, and all the human drivers who come into contact
with such cars would have to face the brunt of it.
For these reasons, we focused our studies on a simulated
autonomous car environment, and investigated how the robot
car‚Äôs inexplicable behavior can be avoided by generating
plans with respect to their explicability scores. In our robot
car model (written in PDDL), we try to capture bad driving
etiquette commonly seen on roads, such as, driving below
speed limit in passing lanes, overtaking from the wrong
side, turning and changing lanes without showing signal,
not following the move over law, and so on. The human
mental model of the robot car is defined as per test subjects
assumptions of how the robot car should perform actions.
From the robot model MR (R), we generated 40 plans for
16 different problems. The plans consisted of both explicable
and inexplicable robot car behaviors. These plans were
assessed by 20 test subjects, with each subject evaluating 8
plans. Also, each plan was evaluated by 4 different subjects,
in order to get a general understanding of the assumptions
of different human drivers. Therefore, the overall number of

(a)

(b)

(c)

Fig. 4: Autonomous Car Domain Simulation. Here the red colored car in the images is the robot car and the rest of the cars
are assumed to be human drivers. (a) The cop car is parked on the rightmost lane, and the robot car is following through
the Move Over Law maneuver. (b) The robot car is wrongly trying to overtake from the rightmost lane. (c) The robot car
is waiting at a four-way stop intersection even though it is the turn of the robot car to cross over.

Fig. 5: Here AD, CLD, SD and Score represent action
distance, causal link distance, state sequence distance, and
explicability scores respectively. This is a correlation matrix
for the aforementioned metrics. The red color represents
the negative correlation that exists between the distance
measures and the scores.

training samples was 160. The test subjects were required to
have sufficient real-life driving experience. The assessment
had two parts: one part involved scoring each robot car action
with 1, if explicable, and 0 otherwise (the explicability score
of the overall plan is calculated as the fraction of actions
in the plan that were labeled as explicable); the other part
involved answering a questionnaire aimed at understanding
test subject‚Äôs assumptions regarding the robot car. The information from this questionnaire was used to design the human
mental model MH (R) of the robot car.
The PDDL domain of the robot car, MR (R), consists
of lane and car objects as shown in Figure 4. The red car

is the robot car in the experiments and all other cars seen
in the experiments are assumed to have human drivers.
The car objects are associated with predicates defining the
location of a car on a lane segment, status of left and right
turn lights, status of car being within speed limit, presence
of a parked cop car, and so on. The actions possible
in the domain are with respect to the robot car. These
actions are Accelerate, Decelerate, LeftSqueeze,
RightSqueeze, LeftLightOn, LeftLightOff,
RightLightOn, RightLightOff, SlowDown and
WaitAtStopSign, and so on. In order to change a
lane, three consecutive actions of either LeftSqueeze
or RightSqueeze are required to gradually move to
the other lane. The PDDL domain of the human mental
model, consists of same state predicates, but different action
representations, preconditions, effects and action-costs. Note
that even though representing the human mental model in
PDDL may seem like a strong assumption, we validated the
labels given by the human subjects with the PDDL human
model constructed from the elicited preferences and found
about 72.3% match, which indicates that MH (R) used
in the evaluations is a good approximation of the human
mental model of the robot
2) Defining the Explicability Distance: For the 22 training
problems, explicable plans with MH (R) were generated.
Since some actions are not common to both the domains and
also owing to the difference in the effects and preconditions
of the actions across domains, an explicit mapping was
defined between the actions over the two domains. This
mapping was done in the light of the plan distance operations
performed between plans in the two domains.
The correlation matrix in Figure 5 establishes the negative
correlation of the plan distance measures to the explicability
scores. From the correlation matrix it can be seen that, causal
link distance has significant negative correlation with the
explicability scores. After establishing the negative correlation, we proceed towards training our regression model called
explicability distance.

TABLE II: Parameters of Regression Models
Distance

b

w

Accuracy %
10.14

Œ¥A

0.72

-0.33

Œ¥C

0.73

-0.231

7.06

Œ¥S

0.92

-0.519

27.47

Œ¥A , Œ¥ C , Œ¥ S

0.93

0.207,-0.061,-0.626

28.02

sR
1 = b1 + w1 Œ¥A

(5)

sR
2 = b2 + w2 Œ¥C

(6)

sR
3 = b3 + w3 Œ¥S

(7)

sR
4 = b4 + w4 Œ¥A + w5 Œ¥C + w6 Œ¥S

(8)

At first, individual distances were used to fit the data in the
regression model. This resulted in a poorly learned regression
model. A linear combination of the three distances also
resulted in poor results. For regression model functions 5, 6,
7 and 8, the bias, weight and accuracy values were as shown
in Table II. From this table, we infer that the relationships
are not necessarily linear as we speculated previously. We
improve our model using Random Forest regression. Since
random forests allow selection of random subset of features
while splitting the decision node, the accuracy of our model
improves. All the three distances have statistically significant
contribution in the fitted model. We evaluate the goodness of
the fit of the model, using the coefficient of determination or
R2 . This value determines the measure by which the fitted
model can explain the variations in the target values. This
value lies between 0 to 1. Higher the R2 value, better is
the model fitted to the data. After training process the new
regression model was found to have 0.8721 R2 value. That is
to say, 87% of the variations in the features can be explained
by our model. Our model predicts the explicability distance
between the robot plans and human mental model plans, with
a high accuracy. We call this plan distance regression model
as the explicability distance.
3) Evaluation: For evaluation of our system, we tested it
on 13 different problems. We ran the algorithm with a high
cost bound, in order to cover the most explicable candidate
plans for all the problems. The results of this search process
are as shown in Figure 6, 7 and 8. From these results, we
can see that the reconciliation search is able to incrementally
develop plans with better explicability scores as shown in
Figure 6. In Figure 7, we see that for all the 13 problems
the explicability score of the optimal plans is lesser than the
final plans generated by reconciliation search. From Figure
8, we see that for the first six problems the optimal and
explicable plans have same cost but our modified planner
with reconciliation search produces explicable plan versions
for those problems. The results also clearly show that the
explicable plans can be costlier than plans that are optimal
with respect to the robot‚Äôs own model. This additional cost

Fig. 6: The graph shows that the search process finds plans
with incrementally better explicable scores. Each color line
represents the 13 different problems. The markers on the
lines represent a plan solution for that problem. The y-axis
gives the explicability scores of the plans and the x-axis gives
the solution number. Note that the curves show the nonmonotonic nature of evaluation metric in the search process.
The final output of the algorithm is, of course, the best plan
found in the search process.

Fig. 7: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their explicability scores.

Fig. 8: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their plan costs.

can be seen as the price the robot pays to make its behavior
explicable to the human.
VI. C ONCLUSION
We showed how the plan distance measures play a role in
determining the explicability of a robot plan. We evaluated
our hypothesis in the simulated Autonomous Car PDDL
domain. We generated training samples in robot‚Äôs domain
and assigned them with human scores. We also generated
plans in the human‚Äôs model to find the distances between
plans in two domains. We looked at the relationships between
scores and the distance measures of the plans. We learned
the regression model that could best capture the explicability
of the training samples. In summary, we have proved our
hypothesis that using the human‚Äôs mental model of the robot
model we can assess the explicability of a robot plan as a
function over the plan distance measures between the robot
plan and the plan that the human would expect the robot to
make. We also showed that the explicability distance measure
can be used to bias the robots planning process to generate
plans that are more in concordance with what humans expect.
We are currently in the process of incorporating this theory
into the behavior of a Fetch robot involved in delivery
tasks, to demonstrate how it improves the explicability of
the robot‚Äôs behavior.
R EFERENCES
[1] E. de Visser and R. Parasuraman, ‚ÄúAdaptive aiding of human-robot
teaming effects of imperfect automation on performance, trust, and
workload,‚Äù Journal of Cognitive Engineering and Decision Making,
vol. 5, no. 2, pp. 209‚Äì231, 2011.
[2] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram,
M. Veloso, D. Weld, and D. Wilkins, ‚ÄúPddl-the planning domain
definition language,‚Äù 1998.
[3] B. Srivastava, T. A. Nguyen, A. Gerevini, S. Kambhampati, M. B. Do,
and I. Serina, ‚ÄúDomain independent approaches for finding diverse
plans.‚Äù in IJCAI, 2007, pp. 2016‚Äì2022.

[4] T. A. Nguyen, M. Do, A. E. Gerevini, I. Serina, B. Srivastava,
and S. Kambhampati, ‚ÄúGenerating diverse plans to handle unknown
and partially known user preferences,‚Äù Artificial Intelligence,
vol. 190, no. 0, pp. 1 ‚Äì 31, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0004370212000707
[5] M.
Helmert,
‚ÄúThe
fast
downward
planning
system,‚Äù
CoRR,
vol.
abs/1109.6051,
2011.
[Online].
Available:
http://arxiv.org/abs/1109.6051
[6] H. A. Kautz and J. F. Allen, ‚ÄúGeneralized plan recognition.‚Äù in AAAI,
vol. 86, no. 3237, 1986, p. 5.
[7] M. Ramƒ±rez and H. Geffner, ‚ÄúProbabilistic plan recognition using offthe-shelf classical planners,‚Äù in Proceedings of the Conference of the
Association for the Advancement of Artificial Intelligence (AAAI 2010).
Citeseer, 2010, pp. 1121‚Äì1126.
[8] T. Chakraborti, Y. Zhang, D. E. Smith, and S. Kambhampati, ‚ÄúPlanning
with resource conflicts in human-robot cohabitation,‚Äù in Proceedings
of the 2016 International Conference on Autonomous Agents &
Multiagent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 2016, pp. 1069‚Äì1077.
[9] M. Cirillo, L. Karlsson, and A. Saffiotti, ‚ÄúHuman-aware task planning
for mobile robots,‚Äù in Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1‚Äì7.
[10] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz,
D. Smith, and S. Kambhampati, ‚ÄúPlanning for serendipity,‚Äù in
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2015.
[11] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati, ‚ÄúCoordination in human-robot teams using mental
modeling and plan recognition,‚Äù in Intelligent Robots and Systems
(IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014,
pp. 2957‚Äì2962.
[12] S. J. Levine and B. C. Williams, ‚ÄúConcurrent plan recognition and
execution for human-robot teams.‚Äù in ICAPS, 2014.
[13] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, and S. K.
Hankz Hankui Zhuo, ‚ÄúPlan explainability and predictability for
cobots,‚Äù CoRR, vol. abs/1511.08158, 2015. [Online]. Available:
http://arxiv.org/abs/1511.08158
[14] A. Dragan and S. Srinivasa, ‚ÄúGenerating legible motion,‚Äù in Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.
[15] T. W. Fong, I. Nourbakhsh, and K. Dautenhahn, ‚ÄúA survey of socially
interactive robots,‚Äù Robotics and Autonomous Systems, 2003.
[16] G. Hoffman and C. Breazeal, ‚ÄúCost-based anticipatory action selection
for human‚Äìrobot fluency,‚Äù Robotics, IEEE Transactions on, vol. 23,
no. 5, pp. 952‚Äì961, 2007.
[17] Y. Zhang, S. Sreedharan, and S. Kambhampati, ‚ÄúCapability models
and their applications in planning,‚Äù in Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems.
International Foundation for Autonomous Agents and Multiagent
Systems, 2015, pp. 1151‚Äì1159.
[18] R. P. Goldman and U. Kuter, ‚ÄúMeasuring plan diversity: Pathologies
in existing approaches and a new plan distance metric.‚Äù 2015.
[19] M. Richtel and C. Dougherty, ‚ÄúGoogle‚Äôs driverless cars run into
problem: Cars with drivers,‚Äù The New York Times, vol. 9, p. 1, 2015.

A Formal Analysis of Required Cooperation in Multi-agent Planning
Yu Zhang and Subbarao Kambhampati

arXiv:1404.5643v1 [cs.AI] 22 Apr 2014

School of Computing and Informatics
Arizona State University
Tempe, Arizona 85281 USA
{yzhan442,rao}@asu.edu

Abstract
Research on multi-agent planning has been popular in
recent years. While previous research has been motivated by the understanding that, through cooperation,
multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required
to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a
formal discussion from the planning aspect. We first
show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing
the problems that require cooperation (referred to as RC
problems) into two classes ‚Äì problems with heterogeneous and homogeneous agents, we aim to identify all
the conditions that can cause RC in these two classes.
We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper
bound on the minimum number of agents required for
RC problems with homogeneous agents. This study not
only provides new insights into multi-agent planning,
but also has many applications. For example, in humanrobot teaming, when a robot cannot achieve a task, it
may be due to RC. In such cases, the human teammate
should be informed and, consequently, coordinate with
other available robots for a solution.

Introduction
A multi-agent planning (MAP) problem differs from a single
agent planning (SAP) problem in that more than one agent
is used in planning. While a (non-temporal) MAP problem
can be compiled into a SAP problem by considering agents
as resources, the search space grows exponentially with the
number of such resources. Given that a SAP problem with
a single such resource is in general PSPACE-complete (Bylander 1991), running a single planner to solve MAP is inefficient. Hence, previous research has generally agreed that
agents should be considered as separate entities for planning,
and thus has been mainly concentrated on how to explore
the interactions between the agents (i.e., loosely-coupled vs.
tightly-coupled) to reduce the search space, and how to perform the search more efficiently in a distributed fashion.
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

However, there has been little discussion on whether multiple agents are required for a planning problem in the first
place. If a single agent is sufficient, solving the problem with
multiple agents becomes an efficiency matter, e.g., shortening the makespan of the plan. Problems of this nature can
be solved in two separate steps: planning with a single agent
and optimizing with multiple agents. In such a way, the difficulty of finding a solution may potentially be reduced.
In this paper, we aim to answer the following questions: 1)
Given a problem with a set of agents, what are the conditions
that make cooperation between multiple agents required to
solve the problem; 2) How to determine the minimum number of agents required for the problem. We show that providing the exact answers is intractable. Instead, we attempt to
provide approximate answers. To facilitate our analysis, we
first divide MAP problems into two classes ‚Äì MAP problems with heterogeneous agents, and MAP problems with
homogeneous agents. Consequently, the MAP problems that
require cooperation (referred to as RC problems) are also divided into two classes ‚Äì type-1 RC (RC with heterogeneous
agents) and type-2 RC (RC with homogeneous agents) problems. Figure 1 shows these divisions.
For the two classes of RC problems, we aim to identify
all the conditions that can cause RC. Figure 2 presents these
conditions and their relationships to the two classes of RC
problems. We establish that at least one of these conditions
must be present in order to have RC. Furthermore, we show
that most of the problems in common planning domains belong to type-1 RC, which is identified by three conditions
in the problem formulation that define the heterogeneity of
agents; most of the problems in type-1 RC can be solved by a
super agent. For type-2 RC, we show that RC is only caused
when the state space is not traversable or when there are
causal loops in the causal graph. We provide upper bounds
for the answer of the second question for type-2 RC problems, based on different relaxations of the conditions that
cause RC, which are associated with, for example, how certain causal loops can be broken in the causal graph.
The answers to these questions not only enrich our fundamental understanding of MAP, but also have many applications. For example, in a human robot teaming scenario, a human may be remotely working with multiple robots. When
a robot is assigned a task that it cannot achieve, it is useful
to determine whether the failure is due to the fact that the

Figure 1: Division of MAP problems into MAP with heterogeneous and homogeneous agents. Consequently, RC problems are also divided into two classes: type-1 RC involves
RC problems with heterogeneous agents and type-2 RC involves RC problems with homogeneous agents.

Figure 2: Causes of required cooperation in RC problems.
task is simply unachievable or the task requires more than
one robot. In the latter case, it is useful then to determine
how many extra robots must be sent to help. The answers
can also be applied to multi-robot systems, and are useful in
general to any multi-agent systems in which the team compositions can dynamically change (e.g., when the team must
be divided to solve different problems).
The rest of the paper is organized as follows. After a review of the related literature, we start the discussion of required cooperation for MAP, in which we answer the above
questions in an orderly fashion. We conclude afterward.

Related Work
One of the earlier works on MAP is the PGP framework
by (Durfee and Lesser 1991; Decker and Lesser 1992). Recently, the MAP problem has started to receive an increasing amount of attention. Most of these recent research works
consider agents separately for planning, and have been concentrated on how to explore the structure of agent interactions to reduce the search space, as well as solving the problem in a distributed fashion. (Nissim, Brafman, and Domshlak 2010) provide a search method by compiling MAP into a
constraint satisfaction problem (CSP), and then using a distributed CSP framework to solve it. The MAP formulation is
based on an extension of the STRIPS language called MASTRIPS (Brafman and Domshlak 2008). In MA-STRIPS,
actions are categorized into public and private actions. Public actions can influence other agents while private actions
cannot. In this way, it is shown by (Brafman and Domsh-

lak 2008) that the search complexity of MAP is exponential in the tree-width of the agent interaction graph. Due to
the poor performance of DisCSP based approaches, (Nissim and Brafman 2012) apply the A‚àó search algorithm in a
distributed manner, which represents one of the state-of-art
MAP solvers. (Torreno, Onaindia, and Sapena 2012) propose a POP-based distributed planning framework for MAP,
which uses a cooperative refinement planning technique that
can handle planning with any level of coupling between the
agents. Each agent at any step proposes a refinement step
to improve the current group plan. Their approach does not
assume complete information. A similar paradigm is taken
by (Kvarnstrom 2011). An iterative best-response planning
and plan improvement technique using standard SAP algorithms is provided by (Jonsson and Rovatsos 2011), which
considers the previous singe agent plans as constraints to be
satisfied while the following agents perform planning.
Given a problem, all of these MAP approaches solve it
using the given set of agents, without first asking whether
multiple agents are really required, let alone what is the minimum number of agents required. Answers to these questions not only separate MAP from SAP in a fundamental
way, but also have real world applications when the team
compositions can dynamically change. In this paper, we
analyze these questions using the SAS+ formalism (Backstrom and Nebel 1996) with causal graph (Knoblock 1994;
Helmert 2006), which is often discussed in the context
of factored planning (Bacchus and Yang 1993; Amir and
Engelhardt 2003; Brafman 2006; Brafman and Domshlak
2013). The causal graph captures the interaction between
different variables; intuitively, it can also capture the interactions between agents since agents affect each other through
these variables. In fact, (Brafman and Domshlak 2013) mention the causal graph‚Äôs relation to the agent interaction graph
when each variable is associated with a single agent.

Multi-agent Planning (MAP)
In this paper, we start the analysis of RC in the simplest
scenarios ‚Äì with instantaneous actions and sequential execution. The possibility of RC can only increase when we
extend the model to the temporal domain, in which concurrent or synchronous actions must be considered. We develop
our analysis of required cooperation for MAP based on the
SAS+ formalism (Backstrom and Nebel 1996).

Background
Definition 1. A SAS+ problem is given by a tuple P =
hV, A, I, Gi, where:
‚Ä¢ V = {v1 , ..., vn } is a set of state variables. Each variable
vi ‚àà V is associated with its domain D(vi ), which is used
to define an extended domain D(vi )+ = D(vi )‚à™u, where
u denotes the undefined value. The state space is defined
as SV+ = D(v1 )+ √ó ... √ó D(vn )+ ; s[vi ] denotes the value
of the variable vi in a state s ‚àà SV+ .
‚Ä¢ A = {a1 , ..., am } is a finite set of actions. Each action aj is a tuple hpre(aj ), post(aj ), prv(aj )i, where
pre(aj ), post(aj ), prv(aj ) ‚äÜ SV+ are the preconditions,
postconditions and prevail conditions of aj , respectively.

We also use pre(aj )[vi ], post(aj )[vi ], prv(aj )[vi ] to denote the corresponding values of vi .
‚Ä¢ I and G denote the initial and goal state, respectively.

Lemma 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining whether it satisfies RC is PSPACEcomplete.

A plan in SAS+ is often defined to be a total-order plan:
Definition 2. A plan œÄ in SAS+ is a sequence of actions
œÄ = ha1 , ..., al i.

Proof. First, it is not difficult to show that the RC decision problem belongs to PSPACE, since we only need to
verify that P = hV, œÜ, I, Gi is unsolvable for all œÜ ‚àà Œ¶,
given that the initial problem is known to be solvable. Then,
we complete the proof by reducing from the PLANSAT
problem, which is PSPACE-complete in general (Bylander
1991). Given a PLANSAT problem (with a single agent), the
idea is that we can introduce a second agent with only one
action. This action directly achieves the goal but requires
an action (with all preconditions satisfied in the initial state)
of the initial agent to provide a precondition that is not initially satisfied. We know that this constructed MAP problem
is solvable. If the algorithm for the RC decision problem
returns that cooperation is required for this MAP problem,
we know that the original PLANSAT problem is unsolvable;
otherwise, it is solvable.

Given two states s1 , s2 ‚àà SV+ , (s1 ‚äï s2 ) denotes that s1 is
updated by s2 , and is subject to the following for all vi ‚àà V :

s2 [vi ] if s2 [vi ] 6= u,
(1)
(s1 ‚äï s2 )[vi ] =
s1 [vi ] otherwise.
Given a variable with two values x, y in which one of
them is u, x t y is defined to be the other value. t can be
extended to two states s1 and s2 , such that s1 t s2 [vi] =
s1 [vi ] t s2 [vi ] for all vi ‚àà V . s1 v s2 if and only if
‚àÄvi ‚àà V, s1 [vi ] = u or s1 [vi ] = s2 [vi ]. The state resulting from executing a plan œÄ can then be defined recursively
using a re operator as follows:
(
re(s, hœÄ; oi) =

re(s, hœÄi) ‚äï post(o)
if pre(o) t prv(o) v re(s, hœÄi),
s otherwise.

(2)

in which re(s, hi) = s, o is an action, and ; is the concatenation operator.

Extension to MAP
To extend the previous formalism to MAP without losing
generality, we minimally modify the definitions.
Definition 3. A SAS+ MAP problem is given by a tuple Œ† =
hV, Œ¶, I, Gi, where:
‚Ä¢ Œ¶ = {œÜg } is the set of agents; each agent œÜg is associated
with a set of actions A(œÜg ).
Definition 4. A plan œÄM AP in MAP is a sequence of agentaction pairs œÄM AP = h(a1 , œÜ(a1 )), ..., (aL , œÜ(aL ))i, in
which œÜ(a) returns the agent for the action a and L is the
length of the plan.
We do not need to consider concurrency or synchronization given that actions are assumed to be instantaneous.

Required Cooperation for MAP
Next, we formally define the notion of required cooperation
and other useful terms that are used in the following analyses. We assume throughout the paper that more than one
agent is considered (i.e., |Œ¶| > 1).

Required Cooperation
Definition 5 (k-agent Solvable). Given a MAP problem
P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), the problem is k-agent solvable
if ‚àÉŒ¶k ‚äÜ Œ¶ (|Œ¶k | = k), such that hV, Œ¶k , I, Gi is solvable.
Definition 6 (Required Cooperation (RC)). Given a solvable MAP problem P = hV, Œ¶, I, Gi, there is required cooperation if it is not 1-agent solvable.
In other words, given a solvable MAP problem that satisfies RC, any plan must involve more than one agent.

Definition 7 (Minimally k-agent Solvable). Given a solvable MAP problem P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), it is minimally k-agent solvable if it is k-agent solvable, and not
(k‚àí1)-agent solvable.
Corollary 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining the minimally solvable k (k ‚â§ |Œ¶|)
is PSPACE-complete.
Although directly querying for RC is intractable, we aim
to identify all the conditions (which can be quickly checked)
that can cause RC. We first define a few terms that are used
in the following discussions.
We note that the reference of agent is explicit in the action (i.e., ground operator) parameters. Although actions are
unique for each agent, two different agents may be capable
of executing actions that are instantiated from the same operator, with all other parameters being identical. To identify
such cases, we introduce the notion of action signature.
Definition 8 (Action Signature (AS)). An action signature is
an action with the reference of the executing agent replaced
by a global EX-AG symbol.
For example, an action signature in the IPC logistics domain is drive(EX-AG, pgh-po, pgh-airport). EX-AG is
a global symbol to denote the executing agent, which is not
used to distinguish between action signatures. We denote the
set of action signatures for œÜ ‚àà Œ¶ as AS(œÜ), which specifies
the capabilities of œÜ. Furthermore, we define the notion of
agent variable.
Definition 9 (Agent Variable (Agent Fluent)). A variable
(fluent) is an agent variable (fluent) if it is associated with
the reference of an agent.
Agent variables are used to specify agent state. For example, location(truck-pgh) is an agent variable since it is
associated with an agent truck-pgh. We use VœÜ ‚äÜ V to denote the set of agent variables that are associated with œÜ (i.e.,
variables that are present in the initial state or actions of œÜ).
Following this notation, we can rewrite a MAP problem
as P = hVo ‚à™VŒ¶ , Œ¶, Io ‚à™IŒ¶ , Go ‚à™GŒ¶ i, in which VŒ¶ = {VœÜ },

IŒ¶ = {IœÜ }, GŒ¶ = {GœÜ }, IœÜ = I ‚à©VœÜ and GœÜ = G‚à©VœÜ . Vo
denotes the set of non-agent variables; Io and Go are the set
of non-agent variables in I and G, respectively. In this paper, we assume that agents can only interact with each other
through non-agent variables (i.e., Vo ). In other words, agent
variables contain one and only one reference of agent. As a
result, we have VœÜ ‚à© VœÜ0 ‚â° ‚àÖ (œÜ 6= œÜ0 ). It seems to be possible to compile away exceptions by breaking agent variables
(with more than one reference of agent) into multiple variables and introducing non-agent variables to correlate them.
Definition 10 (Variable (Fluent) Signature (VS)). Given an
agent variable (fluent), its variable (fluent) signature is the
variable (fluent) with the reference of agent replaced by
EX-AG.
For example, location(truck-pgh) is an agent variable
for truck-pgh and its variable signature is location(EXAG). We denote the set of VSs for VœÜ as V S(œÜ), and use
V S as an operator so that V S(v) returns the VS of a variable
v; this operator returns any non-agent variable unchanged.

Classes of RC
In the following discussion, we assume that the specification of goal (i.e., G) in the MAP problems does not involve
agent variables (i.e., G ‚à© VœÜ = ‚àÖ or GœÜ = ‚àÖ), since we
are mostly interested in how to reach the desired world state
(i.e., specified in terms of Vo ). As aforementioned, we divide
RC problems into two classes as shown in Figure 1. Type1 RC involves problems with heterogeneous agents; type2 RC involves problems with homogeneous agents. Next,
we formally define each class and discuss the causes of RC.
Throughout this paper, when we denote a condition as X, the
negated condition is denoted as N-X.

Type-1 RC (RC with Heterogeneous Agents)
Given a MAP problem P = hV, Œ¶, I, Gi, the heterogeneity
of agents can be characterized by the following conditions:
‚Ä¢ Domain Heterogeneity (DH): ‚àÉv ‚àà VœÜ and D(v) \
D(V 0 ) 6= ‚àÖ, in which V 0 = {v 0 |v 0 ‚àà VœÜ0 (œÜ0 6= œÜ) and
V S(v) = V S(v 0 )}.
‚Ä¢ Variable Heterogeneity (VH): V S(œÜ) \ V S(Œ¶ \ œÜ) 6= ‚àÖ.
‚Ä¢ Capability Heterogeneity (CH): AS(œÜ) \ AS(Œ¶ \ œÜ) 6= ‚àÖ.
Definition 11 (Type-1 RC). An RC problem belongs to type1 RC if at least one of DH, VH and CH is satisfied for an
agent.
The condition that requires at least one of DH, VH and
CH to be satisfied is denoted as DVC in Figure 1. It is worth
noting that when considering certain objects (e.g., truck and
plane in the logistics domain) as agents rather than as resources, most of the RC problems in the IPC domains belong
to type-1 RC.

Causes of RC in Type-1
The most obvious condition for RC in type-1 RC problems
is due to the heterogeneity of agents. In the logistics
domain, for example, if any truck agent can only stay in
one city, the domains of the location variable for different

truck agents are different (DH). When there are packages that must be transferred between different locations
within cities, at least one truck agent for each city is
required (hence RC). In the rover domain, a rover that is
not equipped with a camera sensor would not be associated with the agent variable equipped f or imaging.
When we need both equipped f or imaging and
equipped f or rock analysis, and no rovers are equipped
with the sensors for both (VH), we have RC. Note that VH
does not specify any requirement on the variable value (i.e.,
the state); however, when the domain of a variable contains
only a single value, e.g., equipped f or imaging, we
assume in this paper that this variable is always defined in
a positive manner, e.g., expressing cans instead of cannots.
In the logistics domain, given that the truck agent cannot
fly (CH), when a package must be delivered from a city to
a non-airport location of another city, at least a truck and a
plane are required. Note that DH, VH and CH are closely
correlated.
However, note that 1) the presence of DVC in a solvable
MAP problem does not always cause RC, as shown in Figure
1; 2) the presence of DVC in a type-1 RC problem is not
always the cause of RC, as shown in Figure 2.
As an example for 1), when there is only one package to
be delivered from one location to another within the same
city, there is no need for a plane agent, even though we can
create a non-RC MAP problem with a plane and a truck
agent that satisfies CH (thus DVC).
As an example for 2), for navigating in a grid world, the
traversability of the world for all mobile agents can be restricted based on edge connections, i.e., connected(a, b),
in which a and b are vertices in the grid. Suppose that we
have two packages to be delivered to locations b and c,
respectively, which are both initially at a. There are two
truck agents at a that can be used for delivery. However,
the paths from a to both b and c are one-way only (i.e.,
connected(a, b) = true and connected(b, a) = f alse).
Even if one of the truck agents uses gas and the other one
uses diesel, thus satisfying DVC, it is clear that RC in this
problem is not caused by the heterogeneity of agents.
Type-1 RC problems in which RC is caused by only DVC
can be solved by a super agent (defined below), which is
an agent that combines all the domain values, variable signatures and capabilities (i.e., action signatures) of the other
agents. We refer to the subset of type-1 RC problems that
can be solved by a super agent as super-agent solvable, as
shown in Figure 2.
Definition 12 (Super Agent). A super agent is an agent œÜ‚àó
that satisfies:
‚Ä¢ ‚àÄv ‚àà VŒ¶ , ‚àÉv ‚àó ‚àà VœÜ‚àó , D(v ‚àó ) = D(V ), in which V =
{v|v ‚àà VŒ¶ and V S(v ‚àó ) = V S(v)}.
‚Ä¢ V S(œÜ‚àó ) = V S(Œ¶).
‚Ä¢ AS(œÜ‚àó ) = AS(Œ¶).
It is not difficult to see that most problems in the IPC domains are also super-agent solvable. For example, when we
have a truck-plane agent in the logistics domain that can both
fly (between airports of different cities) and drive (between
locations in the same cities), or when we have a rover that is

equipped with all sensors and can traverse all waypoints in
the rover domain.
From Figure 2, one may have already noticed that the conditions that cause RC in type-2 problems may also cause RC
in type-1 problems (i.e., indicated by the mixed cause region in Figure 2). For example, the aforementioned example
for navigating in a grid world demonstrates that the initial
states (specified in terms of the values for variables) of different agents may cause RC in type-1 problems. Note that
the initial states of different agents cannot be combined as
for domain values, variable signatures and capabilities in a
super agent construction; however, the special cases when
the domains of variables contain only a single value (when
we discussed VH in Causes of RC in Type-1) can also be
considered as cases when RC is caused by the initial state.

Figure 3: Example of a causal graph (ICGS). Variables in
goal G are shown as bold-circle nodes and agent VSs are
shown as double-circle nodes.

Type-2 RC (RC with Homogeneous Agents)
Type-2 RC involves homogeneous agents:
Definition 13 (Type-2 RC). An RC problem belongs to type2 RC if it satisfies N-DVC (for all agents).
Definition 13 states that an RC problem belongs to type-2
RC when all the agents are homogeneous.

Type-2 RC Caused by Traversability
One condition that causes RC in type-2 RC problems is the
traversability of the state space of variables, which is related to the initial states of the agents and the world, as we
previously discussed. Since the traversability is associated
with the evolution of variable values, we use causal graphs
to perform the analysis.
Definition 14 (Causal Graph). Given a MAP problem P =
hV, Œ¶, I, Gi, the causal graph G is a graph with directed and
undirected edges over the nodes V . For two nodes v and v 0
(v 6= v 0 ), a directed edge v ‚Üí v 0 is introduced if there exists
an action that updates v 0 while having a prevail condition
associated with v. An undirected edge v ‚àí v 0 is introduced if
there exists an action that updates both.
A typical example of a causal graph for an individual
agent is presented in Figure 3. For type-2 RC study, since
the agents are homogeneous, the causal graphs for all agents
are the same. Hence, we can use agent VSs to replace agent
variables; we refer to this modified causal graph for a single
agent in a type-2 RC problem as an individual causal graph
signature (ICGS). Next, we define the notions of closures
and traversable state space.
Definition 15 (Inner and Outer Closures (IC and OC)). An
inner closure (IC) in an ICGS is any set of variables for
which no other variables are connected to them with undirected edges; an outer closure (OC) of an IC is the set of
nodes that have directed edges going into nodes in the IC.
In Figure 3, {v2 , v3 } and {v4 } are examples of ICs. The
OC of {v2 , v3 } is {v1 } and the OC of {v4 } is {v3 }.
Definition 16 (Traversable State Space (TSS)). An IC has a
traversable state space if and only if: given any two states of
this IC, denoted by s and s0 , there exists a plan that connects
them, assuming that the state of the OC of this IC can be
changed freely within its state space.

In other words, an IC has a TSS if the traversal of its state
space is only dependent on the variables in its OC; this also
means that when the OC of an IC is empty, the state of the IC
can change freely. Note that static variables in the OC of an
IC can assume values that do not influence the traversability.
For example, the variables that are used to specify the connectivity of vertices in a grid, e.g., connected(a, b), can be
assigned to be true or f alse; although the variables that are
assigned to be true cannot change their values to be f alse,
they do not influence the traversability of the grid world. In
such cases, the associated ICs are still considered to have a
TSS. An ICGS in which all ICs have TSSs is referred to as
being traversable.

Type-2 RC Caused by Causal Loops
However, even a solvable MAP problem that satisfies NDVC for all agents while having a traversable ICGS can still
satisfy RC. An example is presented below.
The goal of this problem is to steal a diamond from a
room, in which the diamond is secured, and place it in another room. The diamond is protected by a stealth detection
system. If the diamond is taken, the system locks the door
of the room in which the diamond is kept, so that the insiders cannot exit. There is a switch to override the detection
system but it is located outside of the room. This problem is
modeled as above, in which the value is immediately specified after each variable. It is not difficult to see that the above
problem cannot be solved with a single agent.
Initial State:
location(agent1) room1
location(agent2) room1
location(diamond1) room1
doorLocked(room1) f alse
location(switch1) room2
Goal State:
location(diamond1) room2

Operators:
W alkT hrough(agent, door, f romRoom, toRoom):
prv: doorLocked(door) f alse
pre: location(agent) f romRoom
post: location(agent) toRoom
Steal(agent, diamond, room, door):
prv: location(agent) room
pre: doorLocked(door) u
pre: location(diamond) room
post: doorLocked(door) true
post: location(diamond) agent
Switch(agent, switch, room, door):
prv: location(switch) room
prv: location(agent) room
pre: doorLocked(door) u
post: doorLocked(door) f alse
P lace(agent, diamond, room):
prv: location(agent) room
pre: location(diamond) agent
post: location(diamond) room
Again, we construct the ICGS for this type-2 RC example, as shown in Figure 4. One key observation is that a
single agent cannot address this problem due to the fact
that W alkT hrough with the diamond to room2 requires
doorLocked(door1) = f alse, which is violated by the
Steal action to obtain the diamond in the first place. This
is clearly related to the loops in Figure 4. In particular, we
define the notion of causal loops.
Definition 17 (Causal Loop (CL)). A causal loop in the
ICGS is a directed loop that contains at least one directed
edge.
Note that undirected edges can be considered as edges in
either direction but at least one directed edge must be present
in a causal loop.

Gap between MAP and Single Agent Planning
We now establish in the following theorem that when none
of the previously discussed conditions (for both type-1 and
type-2 RC) hold in a MAP problem, this problem can be
solved by a single agent.
Theorem 1. Given a solvable MAP problem that satisfies
N-DVC for all agents, and for which the ICGS is traversable
and contains no causal loops, any single agent can also
achieve the goal.
Proof. Given no causal loops, the directed edges in the
ICGS divides the variables into levels, in which: 1) variables at each level do not appear in other levels; 2) higher
level variables are connected to lower level variables with
only directed edges going from higher levels to lower levels; 3) variables within each level are either not connected
or connected with undirected edges. For example, the variables in Figure 3 are divided into the following levels (from
high to low): {v1 }, {v2 , v3 }, {v4 }, {v5 , v7 }, {v6 , v8 }. Note
that this division is not unique.

Figure 4: ICGS for the diamond example that illustrates the
second condition that causes RC in type-2 RC problems. Actions (without parameters) are labeled along with their corresponding edges. The variables in G are shown as bold-box
nodes and agent VSs are shown as dashed-box nodes.
Next, we prove the result by induction based on the level.
Suppose that the ICGS has k levels and we have the following holds: given any trajectory of states for all variables,
there exists a plan whose execution traces of states include
this trajectory in the correct order.
When the ICGS has k + 1 levels: given any state s for all
variables from level 1 to k + 1, we know from the assumption that the ICGS is traversable that there exists a plan that
can update the variables at the k + 1 level from their current
states to the corresponding states in s. This plan (denoted
by œÄ), meanwhile, requires the freedom to change the states
of variables from level 1 to k. Given the induction assumption, we know that we can update these variables to their
required states in the correct order to satisfy œÄ; furthermore,
these updates (at level k and above) also do not influence
the variables at the k + 1 level (hence do not influence œÄ).
Once the states of the variables at the k + 1 level are updated to match those in s, we can then update variables at
level 1 to k to match their states in s accordingly. Using this
process, we can incrementally build a plan whose execution
traces of states contain any given trajectory of states for all
the variables in the correct order.
Furthermore, the induction holds when there is only one
level given that ICGS is traversable. Hence, the induction
conclusion holds. The main conclusion directly follows.

Towards an Upper Bound for Type-2 RC
In this section, we investigate type-2 RC problem to obtain
upper bounds on the k (Definition 7), based on different relaxations of the two conditions that cause RC in type-2 RC
problems. We first relax the assumption regarding causal
loops (CLs) and show that the relaxation process is associated with how certain CLs can be broken.
We notice that there are two kinds of CLs in ICGS. The
first kind contains agent VSs while the second kind does not.
Although we cannot break CLs for the second kind, it is possible to break CLs for the first kind. The motivation is that
certain edges in these CLs can be removed when there is

Figure 5: Illustration of the process for breaking causal loops
in the diamond example, in which the CLs are broken by
removing the edge marked with a triangle in Figure 4. Two
agent VSs are introduced to replace the original agent VS.
no need to update the associated agent VSs. In our diamond
example, when there are two agents in room1 and room2,
respectively, and they can stay where they are during the execution of the plan, there is no need to W alkT hrough and
hence the associated edges can be removed to break the CLs.
Figure 5 shows this process. Based on this observation, we
introduce the following lemma.
Lemma 2. Given a solvable MAP problem that satisfies NDVC for all agents and for which the ICGS is traversable, if
no CLs contain agent VSs and all the edges going in and out
of agent VSs are directed, the minimum number of agents required is upper bounded by √óv‚ààCR(Œ¶) |D(v)|, when assuming that the agents can choose their initial states, in which
CR(Œ¶) is constructed as follows:
1. add the set of agent VSs that are in the CLs into CR(Œ¶);
2. add in an agent VS into CR(Œ¶) if there exists a directed
edge that goes into it from any variable in CR(Œ¶);
3. iterate 2 until no agent VSs can be added.
Proof. Based on the previous discussions, we can remove
edges that are connected to agent VSs to break loops. For
each variable in CR(Œ¶), denoted by v, we introduce a set of
variables N = {v1 , v2 , ..., v|D(v)| } to replace v. Any edges
connecting to v from other variables are duplicated on all
variables in N , except for the edges that go into v. Each
variable vi ‚àà N has a domain with a single value; this value
for each variable in N is different and chosen from D(v).
Note that these new variables do not affect the traversability
of the ICGS.
From Theorem 1, we know that a virtual agent œÜ+ that
can simultaneously assume all the states that are the different
permutations of states for CR(Œ¶) can achieve the goal. We
can simulate œÜ+ using √óv‚ààCR(Œ¶) |D(v)| agents as follows.
We choose the agent initial states according to the permutations of states for CR(Œ¶), while choosing the same states
for all the other agent VSs according to œÜ+ . Given a plan for
œÜ+ , we start from the first action. Given that all permutations
of states for CR(Œ¶) are assumed by an agent, we can find an
agent, denoted by œÜ, that can execute this action: 1) If this

action updates an agent VS in CR(Œ¶), we do not need to
execute this action based on the following reasoning. Given
that all edges going in and out of agent VSs are directed, we
know that this action does not update Vo . (Otherwise, there
must be an undirected edge connecting a variable in Vo to
this agent VS. Similarly, we also know that this action does
not update more than one agent VS.). As a result, it does not
influence the execution of the next action. 2) If this action
updates an agent VS that is not in CR(Œ¶), we know that this
action cannot have variables in CR(Œ¶) as preconditions or
prevail conditions, since otherwise this agent VS would be
included in CR(Œ¶) given its construction process. Hence,
all the agents can execute the action to update this agent VS,
given that all the agent VSs outside of CR(Œ¶) are always
kept synchronized in the entire process (in order to simulate
œÜ+ ). 3) Otherwise, this action must be updating only Vo and
we can execute the action on œÜ.
Following the above process for all the actions in œÜ+ ‚Äôs
plan to achieve the goal. Hence, the conclusion holds.
Next, we investigate the relaxation of the traversability of
the ICGS.
Lemma 3. Given a solvable MAP problem that satisfies NDVC for all agents, if all the edges going in and out of agent
VSs are directed, the minimum number of agents required is
upper bounded by √óv‚ààV S(Œ¶) |D(v)|, when assuming that the
agents can choose their initial states.
Proof. Given a valid plan œÄM AP for the problem, we can
solve the problem using √óv‚ààV S(Œ¶) |D(v)| agents as follows:
first, we choose the agent initial states according to the permutations of state for V S(Œ¶).
The process is similar to that in Lemma 2. We start from
the first action. Given that all permutations of V S(Œ¶) are assumed by an agent, we can find an agent, denoted by œÜ, that
can execute this action: if this action updates some agent
VSs in V S(Œ¶), we do not need to execute this action; otherwise, the action must be updating only Vo and we can execute the action on œÜ.
Following the above process for all the actions in œÄM AP
to achieve the goal. Hence, the conclusion holds.
Note that the bounds in Lemma 2 and 3 are upper bounds
and the minimum number of agents actually required may
be smaller. Nevertheless, for the simple scenario in our diamond example, the assumptions of both lemmas are satisfied and the bounds returned are 2 for both, which happens
to be exactly the k in Definition 7. In future work, we plan
to investigate other relaxations and establish the tightness of
these bounds.

Conclusion
In this paper, we introduce the notion of required cooperation (RC), which answers two questions: 1) whether more
than one agent is required for a solvable MAP problem, and
2) what is the minimum number of agents required for the
problem. We show that the exact answers to these questions
are difficult to provide. To facilitate our analysis, we first
divide RC problems into two class ‚Äì type-1 RC involves

heterogeneous agents and type-2 RC involves homogeneous
agents. For the first question, we show that most of the problems in the common planning domains belong to type-1 RC;
the set of type-1 RC problems in which RC is only caused
by DVC can be solved with a super agent. For type-2 RC
problems, we show that RC is caused when the state space
is not traversable or when there are causal loops in the causal
graph; we provide upper bounds for the answer of the second question, based on different relaxations of the conditions that cause RC in type-2 RC problems. These relaxations are associated with, for example, how certain causal
loops can be broken in the causal graph.

Acknowledgement
This research is supported in part by the ARO grant
W911NF-13-1-0023, and the ONR grants N00014-13-10176 and N00014-13-1-0519.

References
[Amir and Engelhardt 2003] Amir, E., and Engelhardt, B.
2003. Factored planning. In Proceedings of the 18th International Joint Conferences on Artificial Intelligence, 929‚Äì935.
[Bacchus and Yang 1993] Bacchus, F., and Yang, Q. 1993.
Downward refinement and the efficiency of hierarchical
problem solving. Artificial Intelligence 71:43‚Äì100.
[Backstrom and Nebel 1996] Backstrom, C., and Nebel, B.
1996. Complexity results for sas+ planning. Computational
Intelligence 11:625‚Äì655.
[Brafman and Domshlak 2008] Brafman, R. I., and Domshlak, C. 2008. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In Proceedings of the 18th International Conference on Automated Planning and Scheduling, 28‚Äì35. AAAI Press.
[Brafman and Domshlak 2013] Brafman, R. I., and Domshlak, C. 2013. On the complexity of planning for agent teams
and its implications for single agent planning. Artificial Intelligence 198(0):52 ‚Äì 71.
[Brafman 2006] Brafman, R. I. 2006. Factored planning:
How, when, and when not. In Proceedings of the 21st National Conference on Artificial Intelligence, 809‚Äì814.
[Bylander 1991] Bylander, T. 1991. Complexity results for
planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence, volume 1, 274‚Äì279.
[Decker and Lesser 1992] Decker, K. S., and Lesser, V. R.
1992. Generalizing the partial global planning algorithm.
International Journal of Cooperative Information Systems
1:319‚Äì346.
[Durfee and Lesser 1991] Durfee, E., and Lesser, V. R. 1991.
Partial global planning: A coordination framework for distributed hypothesis formation. IEEE Transactions on Systems, Man, and Cybernetics 21:1167‚Äì1183.
[Helmert 2006] Helmert, M. 2006. The fast downward planning system. Journal of Artificial Intelligence Research
26:191‚Äì246.
[Jonsson and Rovatsos 2011] Jonsson, A., and Rovatsos, M.
2011. Scaling Up Multiagent Planning: A Best-Response

Approach. In Proceedings of the 21th International Conference on Automated Planning and Scheduling, 114‚Äì121.
AAAI Press.
[Knoblock 1994] Knoblock, C. 1994. Automatically generating abstractions for planning. Artificial Intelligence
68:243‚Äì302.
[Kvarnstrom 2011] Kvarnstrom, J. 2011. Planning for
loosely coupled agents using partial order forward-chaining.
In Proceedings of the 21th International Conference on Automated Planning and Scheduling.
[Nissim and Brafman 2012] Nissim, R., and Brafman, R. I.
2012. Multi-agent a* for parallel and distributed systems.
In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, volume 3, 1265‚Äì
1266.
[Nissim, Brafman, and Domshlak 2010] Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010. A general, fully distributed multi-agent planning algorithm. In Proceedings of
the 11th International Conference on Autonomous Agents
and Multiagent Systems, 1323‚Äì1330.
[Torreno, Onaindia, and Sapena 2012] Torreno, A.; Onaindia, E.; and Sapena, O. 2012. An approach to multi-agent
planning with incomplete information. In European Conference on Artificial Intelligence, volume 242, 762‚Äì767.

A Formal Analysis of Required Cooperation in Multi-agent Planning
Yu Zhang and Subbarao Kambhampati

arXiv:1404.5643v1 [cs.AI] 22 Apr 2014

School of Computing and Informatics
Arizona State University
Tempe, Arizona 85281 USA
{yzhan442,rao}@asu.edu

Abstract
Research on multi-agent planning has been popular in
recent years. While previous research has been motivated by the understanding that, through cooperation,
multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required
to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a
formal discussion from the planning aspect. We first
show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing
the problems that require cooperation (referred to as RC
problems) into two classes ‚Äì problems with heterogeneous and homogeneous agents, we aim to identify all
the conditions that can cause RC in these two classes.
We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper
bound on the minimum number of agents required for
RC problems with homogeneous agents. This study not
only provides new insights into multi-agent planning,
but also has many applications. For example, in humanrobot teaming, when a robot cannot achieve a task, it
may be due to RC. In such cases, the human teammate
should be informed and, consequently, coordinate with
other available robots for a solution.

Introduction
A multi-agent planning (MAP) problem differs from a single
agent planning (SAP) problem in that more than one agent
is used in planning. While a (non-temporal) MAP problem
can be compiled into a SAP problem by considering agents
as resources, the search space grows exponentially with the
number of such resources. Given that a SAP problem with
a single such resource is in general PSPACE-complete (Bylander 1991), running a single planner to solve MAP is inefficient. Hence, previous research has generally agreed that
agents should be considered as separate entities for planning,
and thus has been mainly concentrated on how to explore
the interactions between the agents (i.e., loosely-coupled vs.
tightly-coupled) to reduce the search space, and how to perform the search more efficiently in a distributed fashion.
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

However, there has been little discussion on whether multiple agents are required for a planning problem in the first
place. If a single agent is sufficient, solving the problem with
multiple agents becomes an efficiency matter, e.g., shortening the makespan of the plan. Problems of this nature can
be solved in two separate steps: planning with a single agent
and optimizing with multiple agents. In such a way, the difficulty of finding a solution may potentially be reduced.
In this paper, we aim to answer the following questions: 1)
Given a problem with a set of agents, what are the conditions
that make cooperation between multiple agents required to
solve the problem; 2) How to determine the minimum number of agents required for the problem. We show that providing the exact answers is intractable. Instead, we attempt to
provide approximate answers. To facilitate our analysis, we
first divide MAP problems into two classes ‚Äì MAP problems with heterogeneous agents, and MAP problems with
homogeneous agents. Consequently, the MAP problems that
require cooperation (referred to as RC problems) are also divided into two classes ‚Äì type-1 RC (RC with heterogeneous
agents) and type-2 RC (RC with homogeneous agents) problems. Figure 1 shows these divisions.
For the two classes of RC problems, we aim to identify
all the conditions that can cause RC. Figure 2 presents these
conditions and their relationships to the two classes of RC
problems. We establish that at least one of these conditions
must be present in order to have RC. Furthermore, we show
that most of the problems in common planning domains belong to type-1 RC, which is identified by three conditions
in the problem formulation that define the heterogeneity of
agents; most of the problems in type-1 RC can be solved by a
super agent. For type-2 RC, we show that RC is only caused
when the state space is not traversable or when there are
causal loops in the causal graph. We provide upper bounds
for the answer of the second question for type-2 RC problems, based on different relaxations of the conditions that
cause RC, which are associated with, for example, how certain causal loops can be broken in the causal graph.
The answers to these questions not only enrich our fundamental understanding of MAP, but also have many applications. For example, in a human robot teaming scenario, a human may be remotely working with multiple robots. When
a robot is assigned a task that it cannot achieve, it is useful
to determine whether the failure is due to the fact that the

Figure 1: Division of MAP problems into MAP with heterogeneous and homogeneous agents. Consequently, RC problems are also divided into two classes: type-1 RC involves
RC problems with heterogeneous agents and type-2 RC involves RC problems with homogeneous agents.

Figure 2: Causes of required cooperation in RC problems.
task is simply unachievable or the task requires more than
one robot. In the latter case, it is useful then to determine
how many extra robots must be sent to help. The answers
can also be applied to multi-robot systems, and are useful in
general to any multi-agent systems in which the team compositions can dynamically change (e.g., when the team must
be divided to solve different problems).
The rest of the paper is organized as follows. After a review of the related literature, we start the discussion of required cooperation for MAP, in which we answer the above
questions in an orderly fashion. We conclude afterward.

Related Work
One of the earlier works on MAP is the PGP framework
by (Durfee and Lesser 1991; Decker and Lesser 1992). Recently, the MAP problem has started to receive an increasing amount of attention. Most of these recent research works
consider agents separately for planning, and have been concentrated on how to explore the structure of agent interactions to reduce the search space, as well as solving the problem in a distributed fashion. (Nissim, Brafman, and Domshlak 2010) provide a search method by compiling MAP into a
constraint satisfaction problem (CSP), and then using a distributed CSP framework to solve it. The MAP formulation is
based on an extension of the STRIPS language called MASTRIPS (Brafman and Domshlak 2008). In MA-STRIPS,
actions are categorized into public and private actions. Public actions can influence other agents while private actions
cannot. In this way, it is shown by (Brafman and Domsh-

lak 2008) that the search complexity of MAP is exponential in the tree-width of the agent interaction graph. Due to
the poor performance of DisCSP based approaches, (Nissim and Brafman 2012) apply the A‚àó search algorithm in a
distributed manner, which represents one of the state-of-art
MAP solvers. (Torreno, Onaindia, and Sapena 2012) propose a POP-based distributed planning framework for MAP,
which uses a cooperative refinement planning technique that
can handle planning with any level of coupling between the
agents. Each agent at any step proposes a refinement step
to improve the current group plan. Their approach does not
assume complete information. A similar paradigm is taken
by (Kvarnstrom 2011). An iterative best-response planning
and plan improvement technique using standard SAP algorithms is provided by (Jonsson and Rovatsos 2011), which
considers the previous singe agent plans as constraints to be
satisfied while the following agents perform planning.
Given a problem, all of these MAP approaches solve it
using the given set of agents, without first asking whether
multiple agents are really required, let alone what is the minimum number of agents required. Answers to these questions not only separate MAP from SAP in a fundamental
way, but also have real world applications when the team
compositions can dynamically change. In this paper, we
analyze these questions using the SAS+ formalism (Backstrom and Nebel 1996) with causal graph (Knoblock 1994;
Helmert 2006), which is often discussed in the context
of factored planning (Bacchus and Yang 1993; Amir and
Engelhardt 2003; Brafman 2006; Brafman and Domshlak
2013). The causal graph captures the interaction between
different variables; intuitively, it can also capture the interactions between agents since agents affect each other through
these variables. In fact, (Brafman and Domshlak 2013) mention the causal graph‚Äôs relation to the agent interaction graph
when each variable is associated with a single agent.

Multi-agent Planning (MAP)
In this paper, we start the analysis of RC in the simplest
scenarios ‚Äì with instantaneous actions and sequential execution. The possibility of RC can only increase when we
extend the model to the temporal domain, in which concurrent or synchronous actions must be considered. We develop
our analysis of required cooperation for MAP based on the
SAS+ formalism (Backstrom and Nebel 1996).

Background
Definition 1. A SAS+ problem is given by a tuple P =
hV, A, I, Gi, where:
‚Ä¢ V = {v1 , ..., vn } is a set of state variables. Each variable
vi ‚àà V is associated with its domain D(vi ), which is used
to define an extended domain D(vi )+ = D(vi )‚à™u, where
u denotes the undefined value. The state space is defined
as SV+ = D(v1 )+ √ó ... √ó D(vn )+ ; s[vi ] denotes the value
of the variable vi in a state s ‚àà SV+ .
‚Ä¢ A = {a1 , ..., am } is a finite set of actions. Each action aj is a tuple hpre(aj ), post(aj ), prv(aj )i, where
pre(aj ), post(aj ), prv(aj ) ‚äÜ SV+ are the preconditions,
postconditions and prevail conditions of aj , respectively.

We also use pre(aj )[vi ], post(aj )[vi ], prv(aj )[vi ] to denote the corresponding values of vi .
‚Ä¢ I and G denote the initial and goal state, respectively.

Lemma 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining whether it satisfies RC is PSPACEcomplete.

A plan in SAS+ is often defined to be a total-order plan:
Definition 2. A plan œÄ in SAS+ is a sequence of actions
œÄ = ha1 , ..., al i.

Proof. First, it is not difficult to show that the RC decision problem belongs to PSPACE, since we only need to
verify that P = hV, œÜ, I, Gi is unsolvable for all œÜ ‚àà Œ¶,
given that the initial problem is known to be solvable. Then,
we complete the proof by reducing from the PLANSAT
problem, which is PSPACE-complete in general (Bylander
1991). Given a PLANSAT problem (with a single agent), the
idea is that we can introduce a second agent with only one
action. This action directly achieves the goal but requires
an action (with all preconditions satisfied in the initial state)
of the initial agent to provide a precondition that is not initially satisfied. We know that this constructed MAP problem
is solvable. If the algorithm for the RC decision problem
returns that cooperation is required for this MAP problem,
we know that the original PLANSAT problem is unsolvable;
otherwise, it is solvable.

Given two states s1 , s2 ‚àà SV+ , (s1 ‚äï s2 ) denotes that s1 is
updated by s2 , and is subject to the following for all vi ‚àà V :

s2 [vi ] if s2 [vi ] 6= u,
(1)
(s1 ‚äï s2 )[vi ] =
s1 [vi ] otherwise.
Given a variable with two values x, y in which one of
them is u, x t y is defined to be the other value. t can be
extended to two states s1 and s2 , such that s1 t s2 [vi] =
s1 [vi ] t s2 [vi ] for all vi ‚àà V . s1 v s2 if and only if
‚àÄvi ‚àà V, s1 [vi ] = u or s1 [vi ] = s2 [vi ]. The state resulting from executing a plan œÄ can then be defined recursively
using a re operator as follows:
(
re(s, hœÄ; oi) =

re(s, hœÄi) ‚äï post(o)
if pre(o) t prv(o) v re(s, hœÄi),
s otherwise.

(2)

in which re(s, hi) = s, o is an action, and ; is the concatenation operator.

Extension to MAP
To extend the previous formalism to MAP without losing
generality, we minimally modify the definitions.
Definition 3. A SAS+ MAP problem is given by a tuple Œ† =
hV, Œ¶, I, Gi, where:
‚Ä¢ Œ¶ = {œÜg } is the set of agents; each agent œÜg is associated
with a set of actions A(œÜg ).
Definition 4. A plan œÄM AP in MAP is a sequence of agentaction pairs œÄM AP = h(a1 , œÜ(a1 )), ..., (aL , œÜ(aL ))i, in
which œÜ(a) returns the agent for the action a and L is the
length of the plan.
We do not need to consider concurrency or synchronization given that actions are assumed to be instantaneous.

Required Cooperation for MAP
Next, we formally define the notion of required cooperation
and other useful terms that are used in the following analyses. We assume throughout the paper that more than one
agent is considered (i.e., |Œ¶| > 1).

Required Cooperation
Definition 5 (k-agent Solvable). Given a MAP problem
P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), the problem is k-agent solvable
if ‚àÉŒ¶k ‚äÜ Œ¶ (|Œ¶k | = k), such that hV, Œ¶k , I, Gi is solvable.
Definition 6 (Required Cooperation (RC)). Given a solvable MAP problem P = hV, Œ¶, I, Gi, there is required cooperation if it is not 1-agent solvable.
In other words, given a solvable MAP problem that satisfies RC, any plan must involve more than one agent.

Definition 7 (Minimally k-agent Solvable). Given a solvable MAP problem P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), it is minimally k-agent solvable if it is k-agent solvable, and not
(k‚àí1)-agent solvable.
Corollary 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining the minimally solvable k (k ‚â§ |Œ¶|)
is PSPACE-complete.
Although directly querying for RC is intractable, we aim
to identify all the conditions (which can be quickly checked)
that can cause RC. We first define a few terms that are used
in the following discussions.
We note that the reference of agent is explicit in the action (i.e., ground operator) parameters. Although actions are
unique for each agent, two different agents may be capable
of executing actions that are instantiated from the same operator, with all other parameters being identical. To identify
such cases, we introduce the notion of action signature.
Definition 8 (Action Signature (AS)). An action signature is
an action with the reference of the executing agent replaced
by a global EX-AG symbol.
For example, an action signature in the IPC logistics domain is drive(EX-AG, pgh-po, pgh-airport). EX-AG is
a global symbol to denote the executing agent, which is not
used to distinguish between action signatures. We denote the
set of action signatures for œÜ ‚àà Œ¶ as AS(œÜ), which specifies
the capabilities of œÜ. Furthermore, we define the notion of
agent variable.
Definition 9 (Agent Variable (Agent Fluent)). A variable
(fluent) is an agent variable (fluent) if it is associated with
the reference of an agent.
Agent variables are used to specify agent state. For example, location(truck-pgh) is an agent variable since it is
associated with an agent truck-pgh. We use VœÜ ‚äÜ V to denote the set of agent variables that are associated with œÜ (i.e.,
variables that are present in the initial state or actions of œÜ).
Following this notation, we can rewrite a MAP problem
as P = hVo ‚à™VŒ¶ , Œ¶, Io ‚à™IŒ¶ , Go ‚à™GŒ¶ i, in which VŒ¶ = {VœÜ },

IŒ¶ = {IœÜ }, GŒ¶ = {GœÜ }, IœÜ = I ‚à©VœÜ and GœÜ = G‚à©VœÜ . Vo
denotes the set of non-agent variables; Io and Go are the set
of non-agent variables in I and G, respectively. In this paper, we assume that agents can only interact with each other
through non-agent variables (i.e., Vo ). In other words, agent
variables contain one and only one reference of agent. As a
result, we have VœÜ ‚à© VœÜ0 ‚â° ‚àÖ (œÜ 6= œÜ0 ). It seems to be possible to compile away exceptions by breaking agent variables
(with more than one reference of agent) into multiple variables and introducing non-agent variables to correlate them.
Definition 10 (Variable (Fluent) Signature (VS)). Given an
agent variable (fluent), its variable (fluent) signature is the
variable (fluent) with the reference of agent replaced by
EX-AG.
For example, location(truck-pgh) is an agent variable
for truck-pgh and its variable signature is location(EXAG). We denote the set of VSs for VœÜ as V S(œÜ), and use
V S as an operator so that V S(v) returns the VS of a variable
v; this operator returns any non-agent variable unchanged.

Classes of RC
In the following discussion, we assume that the specification of goal (i.e., G) in the MAP problems does not involve
agent variables (i.e., G ‚à© VœÜ = ‚àÖ or GœÜ = ‚àÖ), since we
are mostly interested in how to reach the desired world state
(i.e., specified in terms of Vo ). As aforementioned, we divide
RC problems into two classes as shown in Figure 1. Type1 RC involves problems with heterogeneous agents; type2 RC involves problems with homogeneous agents. Next,
we formally define each class and discuss the causes of RC.
Throughout this paper, when we denote a condition as X, the
negated condition is denoted as N-X.

Type-1 RC (RC with Heterogeneous Agents)
Given a MAP problem P = hV, Œ¶, I, Gi, the heterogeneity
of agents can be characterized by the following conditions:
‚Ä¢ Domain Heterogeneity (DH): ‚àÉv ‚àà VœÜ and D(v) \
D(V 0 ) 6= ‚àÖ, in which V 0 = {v 0 |v 0 ‚àà VœÜ0 (œÜ0 6= œÜ) and
V S(v) = V S(v 0 )}.
‚Ä¢ Variable Heterogeneity (VH): V S(œÜ) \ V S(Œ¶ \ œÜ) 6= ‚àÖ.
‚Ä¢ Capability Heterogeneity (CH): AS(œÜ) \ AS(Œ¶ \ œÜ) 6= ‚àÖ.
Definition 11 (Type-1 RC). An RC problem belongs to type1 RC if at least one of DH, VH and CH is satisfied for an
agent.
The condition that requires at least one of DH, VH and
CH to be satisfied is denoted as DVC in Figure 1. It is worth
noting that when considering certain objects (e.g., truck and
plane in the logistics domain) as agents rather than as resources, most of the RC problems in the IPC domains belong
to type-1 RC.

Causes of RC in Type-1
The most obvious condition for RC in type-1 RC problems
is due to the heterogeneity of agents. In the logistics
domain, for example, if any truck agent can only stay in
one city, the domains of the location variable for different

truck agents are different (DH). When there are packages that must be transferred between different locations
within cities, at least one truck agent for each city is
required (hence RC). In the rover domain, a rover that is
not equipped with a camera sensor would not be associated with the agent variable equipped f or imaging.
When we need both equipped f or imaging and
equipped f or rock analysis, and no rovers are equipped
with the sensors for both (VH), we have RC. Note that VH
does not specify any requirement on the variable value (i.e.,
the state); however, when the domain of a variable contains
only a single value, e.g., equipped f or imaging, we
assume in this paper that this variable is always defined in
a positive manner, e.g., expressing cans instead of cannots.
In the logistics domain, given that the truck agent cannot
fly (CH), when a package must be delivered from a city to
a non-airport location of another city, at least a truck and a
plane are required. Note that DH, VH and CH are closely
correlated.
However, note that 1) the presence of DVC in a solvable
MAP problem does not always cause RC, as shown in Figure
1; 2) the presence of DVC in a type-1 RC problem is not
always the cause of RC, as shown in Figure 2.
As an example for 1), when there is only one package to
be delivered from one location to another within the same
city, there is no need for a plane agent, even though we can
create a non-RC MAP problem with a plane and a truck
agent that satisfies CH (thus DVC).
As an example for 2), for navigating in a grid world, the
traversability of the world for all mobile agents can be restricted based on edge connections, i.e., connected(a, b),
in which a and b are vertices in the grid. Suppose that we
have two packages to be delivered to locations b and c,
respectively, which are both initially at a. There are two
truck agents at a that can be used for delivery. However,
the paths from a to both b and c are one-way only (i.e.,
connected(a, b) = true and connected(b, a) = f alse).
Even if one of the truck agents uses gas and the other one
uses diesel, thus satisfying DVC, it is clear that RC in this
problem is not caused by the heterogeneity of agents.
Type-1 RC problems in which RC is caused by only DVC
can be solved by a super agent (defined below), which is
an agent that combines all the domain values, variable signatures and capabilities (i.e., action signatures) of the other
agents. We refer to the subset of type-1 RC problems that
can be solved by a super agent as super-agent solvable, as
shown in Figure 2.
Definition 12 (Super Agent). A super agent is an agent œÜ‚àó
that satisfies:
‚Ä¢ ‚àÄv ‚àà VŒ¶ , ‚àÉv ‚àó ‚àà VœÜ‚àó , D(v ‚àó ) = D(V ), in which V =
{v|v ‚àà VŒ¶ and V S(v ‚àó ) = V S(v)}.
‚Ä¢ V S(œÜ‚àó ) = V S(Œ¶).
‚Ä¢ AS(œÜ‚àó ) = AS(Œ¶).
It is not difficult to see that most problems in the IPC domains are also super-agent solvable. For example, when we
have a truck-plane agent in the logistics domain that can both
fly (between airports of different cities) and drive (between
locations in the same cities), or when we have a rover that is

equipped with all sensors and can traverse all waypoints in
the rover domain.
From Figure 2, one may have already noticed that the conditions that cause RC in type-2 problems may also cause RC
in type-1 problems (i.e., indicated by the mixed cause region in Figure 2). For example, the aforementioned example
for navigating in a grid world demonstrates that the initial
states (specified in terms of the values for variables) of different agents may cause RC in type-1 problems. Note that
the initial states of different agents cannot be combined as
for domain values, variable signatures and capabilities in a
super agent construction; however, the special cases when
the domains of variables contain only a single value (when
we discussed VH in Causes of RC in Type-1) can also be
considered as cases when RC is caused by the initial state.

Figure 3: Example of a causal graph (ICGS). Variables in
goal G are shown as bold-circle nodes and agent VSs are
shown as double-circle nodes.

Type-2 RC (RC with Homogeneous Agents)
Type-2 RC involves homogeneous agents:
Definition 13 (Type-2 RC). An RC problem belongs to type2 RC if it satisfies N-DVC (for all agents).
Definition 13 states that an RC problem belongs to type-2
RC when all the agents are homogeneous.

Type-2 RC Caused by Traversability
One condition that causes RC in type-2 RC problems is the
traversability of the state space of variables, which is related to the initial states of the agents and the world, as we
previously discussed. Since the traversability is associated
with the evolution of variable values, we use causal graphs
to perform the analysis.
Definition 14 (Causal Graph). Given a MAP problem P =
hV, Œ¶, I, Gi, the causal graph G is a graph with directed and
undirected edges over the nodes V . For two nodes v and v 0
(v 6= v 0 ), a directed edge v ‚Üí v 0 is introduced if there exists
an action that updates v 0 while having a prevail condition
associated with v. An undirected edge v ‚àí v 0 is introduced if
there exists an action that updates both.
A typical example of a causal graph for an individual
agent is presented in Figure 3. For type-2 RC study, since
the agents are homogeneous, the causal graphs for all agents
are the same. Hence, we can use agent VSs to replace agent
variables; we refer to this modified causal graph for a single
agent in a type-2 RC problem as an individual causal graph
signature (ICGS). Next, we define the notions of closures
and traversable state space.
Definition 15 (Inner and Outer Closures (IC and OC)). An
inner closure (IC) in an ICGS is any set of variables for
which no other variables are connected to them with undirected edges; an outer closure (OC) of an IC is the set of
nodes that have directed edges going into nodes in the IC.
In Figure 3, {v2 , v3 } and {v4 } are examples of ICs. The
OC of {v2 , v3 } is {v1 } and the OC of {v4 } is {v3 }.
Definition 16 (Traversable State Space (TSS)). An IC has a
traversable state space if and only if: given any two states of
this IC, denoted by s and s0 , there exists a plan that connects
them, assuming that the state of the OC of this IC can be
changed freely within its state space.

In other words, an IC has a TSS if the traversal of its state
space is only dependent on the variables in its OC; this also
means that when the OC of an IC is empty, the state of the IC
can change freely. Note that static variables in the OC of an
IC can assume values that do not influence the traversability.
For example, the variables that are used to specify the connectivity of vertices in a grid, e.g., connected(a, b), can be
assigned to be true or f alse; although the variables that are
assigned to be true cannot change their values to be f alse,
they do not influence the traversability of the grid world. In
such cases, the associated ICs are still considered to have a
TSS. An ICGS in which all ICs have TSSs is referred to as
being traversable.

Type-2 RC Caused by Causal Loops
However, even a solvable MAP problem that satisfies NDVC for all agents while having a traversable ICGS can still
satisfy RC. An example is presented below.
The goal of this problem is to steal a diamond from a
room, in which the diamond is secured, and place it in another room. The diamond is protected by a stealth detection
system. If the diamond is taken, the system locks the door
of the room in which the diamond is kept, so that the insiders cannot exit. There is a switch to override the detection
system but it is located outside of the room. This problem is
modeled as above, in which the value is immediately specified after each variable. It is not difficult to see that the above
problem cannot be solved with a single agent.
Initial State:
location(agent1) room1
location(agent2) room1
location(diamond1) room1
doorLocked(room1) f alse
location(switch1) room2
Goal State:
location(diamond1) room2

Operators:
W alkT hrough(agent, door, f romRoom, toRoom):
prv: doorLocked(door) f alse
pre: location(agent) f romRoom
post: location(agent) toRoom
Steal(agent, diamond, room, door):
prv: location(agent) room
pre: doorLocked(door) u
pre: location(diamond) room
post: doorLocked(door) true
post: location(diamond) agent
Switch(agent, switch, room, door):
prv: location(switch) room
prv: location(agent) room
pre: doorLocked(door) u
post: doorLocked(door) f alse
P lace(agent, diamond, room):
prv: location(agent) room
pre: location(diamond) agent
post: location(diamond) room
Again, we construct the ICGS for this type-2 RC example, as shown in Figure 4. One key observation is that a
single agent cannot address this problem due to the fact
that W alkT hrough with the diamond to room2 requires
doorLocked(door1) = f alse, which is violated by the
Steal action to obtain the diamond in the first place. This
is clearly related to the loops in Figure 4. In particular, we
define the notion of causal loops.
Definition 17 (Causal Loop (CL)). A causal loop in the
ICGS is a directed loop that contains at least one directed
edge.
Note that undirected edges can be considered as edges in
either direction but at least one directed edge must be present
in a causal loop.

Gap between MAP and Single Agent Planning
We now establish in the following theorem that when none
of the previously discussed conditions (for both type-1 and
type-2 RC) hold in a MAP problem, this problem can be
solved by a single agent.
Theorem 1. Given a solvable MAP problem that satisfies
N-DVC for all agents, and for which the ICGS is traversable
and contains no causal loops, any single agent can also
achieve the goal.
Proof. Given no causal loops, the directed edges in the
ICGS divides the variables into levels, in which: 1) variables at each level do not appear in other levels; 2) higher
level variables are connected to lower level variables with
only directed edges going from higher levels to lower levels; 3) variables within each level are either not connected
or connected with undirected edges. For example, the variables in Figure 3 are divided into the following levels (from
high to low): {v1 }, {v2 , v3 }, {v4 }, {v5 , v7 }, {v6 , v8 }. Note
that this division is not unique.

Figure 4: ICGS for the diamond example that illustrates the
second condition that causes RC in type-2 RC problems. Actions (without parameters) are labeled along with their corresponding edges. The variables in G are shown as bold-box
nodes and agent VSs are shown as dashed-box nodes.
Next, we prove the result by induction based on the level.
Suppose that the ICGS has k levels and we have the following holds: given any trajectory of states for all variables,
there exists a plan whose execution traces of states include
this trajectory in the correct order.
When the ICGS has k + 1 levels: given any state s for all
variables from level 1 to k + 1, we know from the assumption that the ICGS is traversable that there exists a plan that
can update the variables at the k + 1 level from their current
states to the corresponding states in s. This plan (denoted
by œÄ), meanwhile, requires the freedom to change the states
of variables from level 1 to k. Given the induction assumption, we know that we can update these variables to their
required states in the correct order to satisfy œÄ; furthermore,
these updates (at level k and above) also do not influence
the variables at the k + 1 level (hence do not influence œÄ).
Once the states of the variables at the k + 1 level are updated to match those in s, we can then update variables at
level 1 to k to match their states in s accordingly. Using this
process, we can incrementally build a plan whose execution
traces of states contain any given trajectory of states for all
the variables in the correct order.
Furthermore, the induction holds when there is only one
level given that ICGS is traversable. Hence, the induction
conclusion holds. The main conclusion directly follows.

Towards an Upper Bound for Type-2 RC
In this section, we investigate type-2 RC problem to obtain
upper bounds on the k (Definition 7), based on different relaxations of the two conditions that cause RC in type-2 RC
problems. We first relax the assumption regarding causal
loops (CLs) and show that the relaxation process is associated with how certain CLs can be broken.
We notice that there are two kinds of CLs in ICGS. The
first kind contains agent VSs while the second kind does not.
Although we cannot break CLs for the second kind, it is possible to break CLs for the first kind. The motivation is that
certain edges in these CLs can be removed when there is

Figure 5: Illustration of the process for breaking causal loops
in the diamond example, in which the CLs are broken by
removing the edge marked with a triangle in Figure 4. Two
agent VSs are introduced to replace the original agent VS.
no need to update the associated agent VSs. In our diamond
example, when there are two agents in room1 and room2,
respectively, and they can stay where they are during the execution of the plan, there is no need to W alkT hrough and
hence the associated edges can be removed to break the CLs.
Figure 5 shows this process. Based on this observation, we
introduce the following lemma.
Lemma 2. Given a solvable MAP problem that satisfies NDVC for all agents and for which the ICGS is traversable, if
no CLs contain agent VSs and all the edges going in and out
of agent VSs are directed, the minimum number of agents required is upper bounded by √óv‚ààCR(Œ¶) |D(v)|, when assuming that the agents can choose their initial states, in which
CR(Œ¶) is constructed as follows:
1. add the set of agent VSs that are in the CLs into CR(Œ¶);
2. add in an agent VS into CR(Œ¶) if there exists a directed
edge that goes into it from any variable in CR(Œ¶);
3. iterate 2 until no agent VSs can be added.
Proof. Based on the previous discussions, we can remove
edges that are connected to agent VSs to break loops. For
each variable in CR(Œ¶), denoted by v, we introduce a set of
variables N = {v1 , v2 , ..., v|D(v)| } to replace v. Any edges
connecting to v from other variables are duplicated on all
variables in N , except for the edges that go into v. Each
variable vi ‚àà N has a domain with a single value; this value
for each variable in N is different and chosen from D(v).
Note that these new variables do not affect the traversability
of the ICGS.
From Theorem 1, we know that a virtual agent œÜ+ that
can simultaneously assume all the states that are the different
permutations of states for CR(Œ¶) can achieve the goal. We
can simulate œÜ+ using √óv‚ààCR(Œ¶) |D(v)| agents as follows.
We choose the agent initial states according to the permutations of states for CR(Œ¶), while choosing the same states
for all the other agent VSs according to œÜ+ . Given a plan for
œÜ+ , we start from the first action. Given that all permutations
of states for CR(Œ¶) are assumed by an agent, we can find an
agent, denoted by œÜ, that can execute this action: 1) If this

action updates an agent VS in CR(Œ¶), we do not need to
execute this action based on the following reasoning. Given
that all edges going in and out of agent VSs are directed, we
know that this action does not update Vo . (Otherwise, there
must be an undirected edge connecting a variable in Vo to
this agent VS. Similarly, we also know that this action does
not update more than one agent VS.). As a result, it does not
influence the execution of the next action. 2) If this action
updates an agent VS that is not in CR(Œ¶), we know that this
action cannot have variables in CR(Œ¶) as preconditions or
prevail conditions, since otherwise this agent VS would be
included in CR(Œ¶) given its construction process. Hence,
all the agents can execute the action to update this agent VS,
given that all the agent VSs outside of CR(Œ¶) are always
kept synchronized in the entire process (in order to simulate
œÜ+ ). 3) Otherwise, this action must be updating only Vo and
we can execute the action on œÜ.
Following the above process for all the actions in œÜ+ ‚Äôs
plan to achieve the goal. Hence, the conclusion holds.
Next, we investigate the relaxation of the traversability of
the ICGS.
Lemma 3. Given a solvable MAP problem that satisfies NDVC for all agents, if all the edges going in and out of agent
VSs are directed, the minimum number of agents required is
upper bounded by √óv‚ààV S(Œ¶) |D(v)|, when assuming that the
agents can choose their initial states.
Proof. Given a valid plan œÄM AP for the problem, we can
solve the problem using √óv‚ààV S(Œ¶) |D(v)| agents as follows:
first, we choose the agent initial states according to the permutations of state for V S(Œ¶).
The process is similar to that in Lemma 2. We start from
the first action. Given that all permutations of V S(Œ¶) are assumed by an agent, we can find an agent, denoted by œÜ, that
can execute this action: if this action updates some agent
VSs in V S(Œ¶), we do not need to execute this action; otherwise, the action must be updating only Vo and we can execute the action on œÜ.
Following the above process for all the actions in œÄM AP
to achieve the goal. Hence, the conclusion holds.
Note that the bounds in Lemma 2 and 3 are upper bounds
and the minimum number of agents actually required may
be smaller. Nevertheless, for the simple scenario in our diamond example, the assumptions of both lemmas are satisfied and the bounds returned are 2 for both, which happens
to be exactly the k in Definition 7. In future work, we plan
to investigate other relaxations and establish the tightness of
these bounds.

Conclusion
In this paper, we introduce the notion of required cooperation (RC), which answers two questions: 1) whether more
than one agent is required for a solvable MAP problem, and
2) what is the minimum number of agents required for the
problem. We show that the exact answers to these questions
are difficult to provide. To facilitate our analysis, we first
divide RC problems into two class ‚Äì type-1 RC involves

heterogeneous agents and type-2 RC involves homogeneous
agents. For the first question, we show that most of the problems in the common planning domains belong to type-1 RC;
the set of type-1 RC problems in which RC is only caused
by DVC can be solved with a super agent. For type-2 RC
problems, we show that RC is caused when the state space
is not traversable or when there are causal loops in the causal
graph; we provide upper bounds for the answer of the second question, based on different relaxations of the conditions that cause RC in type-2 RC problems. These relaxations are associated with, for example, how certain causal
loops can be broken in the causal graph.

Acknowledgement
This research is supported in part by the ARO grant
W911NF-13-1-0023, and the ONR grants N00014-13-10176 and N00014-13-1-0519.

References
[Amir and Engelhardt 2003] Amir, E., and Engelhardt, B.
2003. Factored planning. In Proceedings of the 18th International Joint Conferences on Artificial Intelligence, 929‚Äì935.
[Bacchus and Yang 1993] Bacchus, F., and Yang, Q. 1993.
Downward refinement and the efficiency of hierarchical
problem solving. Artificial Intelligence 71:43‚Äì100.
[Backstrom and Nebel 1996] Backstrom, C., and Nebel, B.
1996. Complexity results for sas+ planning. Computational
Intelligence 11:625‚Äì655.
[Brafman and Domshlak 2008] Brafman, R. I., and Domshlak, C. 2008. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In Proceedings of the 18th International Conference on Automated Planning and Scheduling, 28‚Äì35. AAAI Press.
[Brafman and Domshlak 2013] Brafman, R. I., and Domshlak, C. 2013. On the complexity of planning for agent teams
and its implications for single agent planning. Artificial Intelligence 198(0):52 ‚Äì 71.
[Brafman 2006] Brafman, R. I. 2006. Factored planning:
How, when, and when not. In Proceedings of the 21st National Conference on Artificial Intelligence, 809‚Äì814.
[Bylander 1991] Bylander, T. 1991. Complexity results for
planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence, volume 1, 274‚Äì279.
[Decker and Lesser 1992] Decker, K. S., and Lesser, V. R.
1992. Generalizing the partial global planning algorithm.
International Journal of Cooperative Information Systems
1:319‚Äì346.
[Durfee and Lesser 1991] Durfee, E., and Lesser, V. R. 1991.
Partial global planning: A coordination framework for distributed hypothesis formation. IEEE Transactions on Systems, Man, and Cybernetics 21:1167‚Äì1183.
[Helmert 2006] Helmert, M. 2006. The fast downward planning system. Journal of Artificial Intelligence Research
26:191‚Äì246.
[Jonsson and Rovatsos 2011] Jonsson, A., and Rovatsos, M.
2011. Scaling Up Multiagent Planning: A Best-Response

Approach. In Proceedings of the 21th International Conference on Automated Planning and Scheduling, 114‚Äì121.
AAAI Press.
[Knoblock 1994] Knoblock, C. 1994. Automatically generating abstractions for planning. Artificial Intelligence
68:243‚Äì302.
[Kvarnstrom 2011] Kvarnstrom, J. 2011. Planning for
loosely coupled agents using partial order forward-chaining.
In Proceedings of the 21th International Conference on Automated Planning and Scheduling.
[Nissim and Brafman 2012] Nissim, R., and Brafman, R. I.
2012. Multi-agent a* for parallel and distributed systems.
In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, volume 3, 1265‚Äì
1266.
[Nissim, Brafman, and Domshlak 2010] Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010. A general, fully distributed multi-agent planning algorithm. In Proceedings of
the 11th International Conference on Autonomous Agents
and Multiagent Systems, 1323‚Äì1330.
[Torreno, Onaindia, and Sapena 2012] Torreno, A.; Onaindia, E.; and Sapena, O. 2012. An approach to multi-agent
planning with incomplete information. In European Conference on Artificial Intelligence, volume 242, 762‚Äì767.

J Intell Inf Syst
DOI 10.1007/s10844-015-0366-3

Click efficiency: a unified optimal ranking for online Ads
and documents
Raju Balakrishnan1 ¬∑ Subbarao Kambhampati2

Received: 21 December 2013 / Revised: 23 March 2015 / Accepted: 12 May 2015
¬© Springer Science+Business Media New York 2015

Abstract Ranking of search results and ads has traditionally been studied separately. The
probability ranking principle is commonly used to rank the search results while the ranking
based on expected profits is commonly used for paid placement of ads. These rankings try
to maximize the expected utilities based on the user click models. Recent empirical analysis on search engine logs suggests unified click models for both ranked ads and search
results (documents). These new models consider parameters of (i) probability of the user
abandoning browsing results (ii) perceived relevance of result snippets. However, current
document and ad ranking methods do not consider these parameters. In this paper we propose a generalized ranking function‚Äînamely Click Efficiency (CE)‚Äîfor documents and
ads based on empirically proven user click models. The ranking considers parameters (i)
and (ii) above, optimal and has the same time complexity as sorting. Furthermore, the CE
ranking exploits the commonality of click models, hence is applicable for both documents
and ads. We examine the reduced forms of CE ranking based upon different underlying
assumptions, enumerating a hierarchy of ranking functions. Interestingly, some of the rankings in the hierarchy are currently used ad and document ranking functions; while others
suggest new rankings. Thus, this hierarchy illustrates the relationships between different
rankings, and clarifies the underlying assumptions. While optimality of ranking is sufficient for document ranking, applying CE ranking to ad auctions requires an appropriate
pricing mechanism. We incorporate a second price based mechanism with the proposed
ranking. Our analysis proves several desirable properties including revenue dominance over

 Raju Balakrishnan

raju.balakrishnan@gmail.com
Subbarao Kambhampati
rao@asu.edu
1

Groupon., Park Blvd, Palo Alto, CA 94306, USA

2

Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

J Intell Inf Syst

Vickrey Clarke Groves (VCG) for the same bid vector and existence of a Nash equilibrium
in pure strategies. The equilibrium is socially optimal, and revenue equivalent to the truthful
VCG equilibrium. As a result of its generality, the auction mechanism and the equilibrium
reduces to the current mechanisms including Generalized Second Price Auction (GSP) and
corresponding equilibria. Furthermore, we relax the independence assumption in CE ranking and analyze the diversity ranking problem. We show that optimal diversity ranking is
NP-Hard in general, and a constant time approximation algorithm is not likely. Finally our
simulations to quantify the amount of increase in different utility functions conform to the
results, and suggest potentially significant increase in utilities.
Keywords Ad ranking ¬∑ Document ranking ¬∑ Diversity ¬∑ Auctions ¬∑ Click models

1 Introduction
Search engines rank results to maximize the relevance of the top documents. On the other
hand, targeted ads are ranked primarily to maximize the profit from clicks. In general,
users browse through ranked lists of search results or ads from top to bottom, either clicking or skipping the results, or abandoning browsing the list altogether due to impatience
or satiation. The goal of the ranking is to maximize the expected relevances (or profits) of
clicked results based on the click model of the users. The sort by relevance ranking suggested by Probability Ranking Principle (PRP) has been commonly used for search results
for decades (Robertson 1977; Gordon and Lenk 1991). In contrast, sorting by the expected
profits calculated as the product of bid amount and Click Through Rate (CTR) is popular
for ranking ads (Richardson et al. 2007).
Recent click models suggests that the user click behaviors for both search results and targeted ads is the same (Guo et al. 2009; Zhu et al. 2010). Considering this commonality, the
only difference between the two ranking problems is the utility of entities ranked: for documents utility is the relevance and for the ads it is the cost-per-click (CPC). This suggests the
possibility of a unified ranking function for search results and ads. The current segregation
of document and ad ranking as separate areas does not consider this commonality. A unified approach often helps to widen the scope of the related research to these two areas, and
enables applications of existing ranking function in one area on isomorphic problems in the
other area as we will show below.
In addition to the unified approach, the recent click models consider the following
parameters:
1.

2.

Browsing Abandonment: The user may abandon browsing ranked list at any point.
The likelihood of abandonment may depend on the entities the user has already
seen (Zhu et al. 2010).
Perceived Relevance: Perceived relevance is the user‚Äôs relevance assessment viewing
only the search snippet or ad impression. The decision to click or not depends on the
perceived relevance, not on the actual relevance of the results (Yue et al. 2010; Clarke
et al. 2007).

Though these parameters are part of the click models (Guo et al. 2009; Zhu et al. 2010) how
to exploit these parameters to improve ranking is currently unknown. The current document
ranking is based on the simplifying assumption that the perceived relevance is the same as
the actual relevance of the document, and ignores browsing abandonment. The ad placement
partially considers perceived relevance, but ignores abandonment probabilities.

J Intell Inf Syst

In this paper, we propose a unified optimal ranking function‚Äînamely Click Efficiency
(CE)‚Äîbased on a generalized click model of the user. CE is defined as the ratio of the
standalone utility generated by an entity to the sum of the abandonment probability and the
click probability of that entity, where the abandonment probability is the probability for the
user to leave browsing the list after seeing the entity. We show that sorting entities in the
descending order of CE guarantees optimum ranking utility. We do not make assumptions
on the utilities of the entities, which may be assessed relevance for documents or cost per
click (CPC) charged based on the auction for ads. On plugging in the appropriate utilities‚Äî
relevance for documents and CPC for the ads‚Äîthe ranking specializes to document and ad
ranking.
As a consequence of the generality, the proposed ranking will reduce to specific ranking
problems on assumptions about user behavior. We enumerate a hierarchy of ranking functions corresponding to different assumptions on the click model. Most interestingly, some of
these special cases correspond to the currently used document and ad ranking functions‚Äî
including PRP and sort by expected profit described above. Further, some of the reduced
ranking functions suggest new rankings for special cases of the click model‚Äîlike a click
model in which the user never abandons the search, or the perceived relevance is approximated as the actual relevance. This hierarchy elucidates interconnection between different
ranking functions and the assumptions behind the rankings. We believe that this will help in
choosing the appropriate ranking function for a particular user click behavior.
Ranking in ad placement used in conjunction with a pricing strategy to form the complete
auction mechanism. Hence to apply the CE ranking on ad placement, a pricing mechanism has to be associated. We incorporate a second-price based pricing mechanism with
the proposed ranking. Our analysis establishes many interesting properties of the proposed
mechanism. Particularly, we state and prove the existence of a Nash Equilibrium in pure
strategies. At this equilibrium, the profits of the search engine and the total revenue of the
advertisers is simultaneously optimized. Like ranking, the proposed auction this is a generalized mechanism, and reduces to the existing GSP and Overture mechanisms under the
same assumptions as that of the ranking. Further, the stated Nash Equilibrium is a general
case of the equilibriums of these existing mechanisms. Comparing the mechanism properties with that of VCG (Vickrey 1961; Clarke 1971; Groves 1973), we show that for the same
bid vector, search engine revenue for the CE mechanism is greater or equal to that of VCG.
Furthermore, the revenue for the proposed equilibrium is equal to the revenue of the truthful
dominant strategy equilibrium of VCG.
Our analysis so far has been based on the assumption of parameter independence between
the ranked entities. We relax this assumption and analyze the implications based on a specific well known problem‚Äîdiversity ranking (Carterette 2010; Agrawal et al. 2009; Rafiei
et al. 2010). Diversity ranking tries to maximize the collective utility of top-k ranked entities. For a ranked list, an entity will reduce residual utility of a similar entity in the list blow
it. Though optimizing all the current ranking functions incorporating diversity is known to
be NP-Hard (Carterette 2010), an understanding of why this is an inherently hard problem
is lacking. We show that optimizing set utilities is NP-Hard even for the basic form of diversity ranking. Furthermore we extend our proof showing that a constant ratio approximation
algorithm is unlikely. As a benefit of the generality of ranking, these results are applicable
both for ads and documents.
Although we prove the optimality of the proposed ranking, the amount by which the
profit may improve is not clear. Considering the very restricted access to online experiments on ads, we performed simulations to this end. We compare the profit improvement
by the CE and reduced forms to existing rankings. These experiments suggest potentially

J Intell Inf Syst

significant increase in profits. We believe that these experiments will motivate further online
evaluations.
In summary, the contributions of the unified ranking, including both ad and document
domains are:
1.
2.
3.
4.
5.

Unified optimal ranking.
Optimal ranking considering abandonment probabilities for documents and ads.
Optimal Ranking considering perceived relevance of documents and ads.
A unified hierarchy of ranking functions and enumerating optimal rankings for different
click models.
Analysis of general diversity ranking problem and hardness proofs.

Our contributions to ad placement are:
1.
2.
3.

Design and analysis of a generalized ad auction mechanism incorporating pricing with
CE ranking.
Proof of the existence of a socially optimal Nash Equilibrium with optimal advertisers
revenue as well as optimal search engine profit.
Proof of search engine revenue dominance over VCG for equivalent bid vectors, and
equilibrium revenue equivalence to the truthful VCG equilibrium.

1.1 Background
In search and search advertising, both search results and ads are ranked to maximize utility.
At a high level, search results are ranked to maximize the information content (or relevance)
of the top documents to the users; whereas ads are ranked to maximize both the relevance
as well as the profit to the search engines. Users generally browse through ranked search
results starting from the top, either clicking or skipping the results. This browsing pattern
of users is called the click model. Search and ad rankings try to maximize the utility to the
users based on a click model.
In addition to the standalone relevance of the results, another important aspect of ranking
is the diversity of the results. Although information contained in a document may be highly
relevant, if the information is similar to that in the documents above in the ranking, the document will be of little utility. To account for this factor, the mutual influence of documents
or ads ranked needs to be considered to maximize total utility by a set of documents rather
than individual documents. To account for this factor, diversity-sensitive ranking maximizes
residual relevance of ads or documents in the context of other items in the ranked list.
In search ad ranking (paid placements), ads are selected based on the user query. Generally, the click model for ads is similar to that of the search results. In the most common
pay-per-click ad campaigns, advertisers pay a certain amount to the search engines whenever a user clicks on their ads. This amount is determined by a pricing mechanism. The
advertisers place a bid on the queries. The ads are ranked based on the bid amounts and
relevance of the ad to the query. For example, in commonly used Generalized Second
Price (GSP) auction Edelman et al. (2007) ads are ranked by the product of their click
rates (ratio of the number of clicks to impressions) and bid amounts. The amount the
advertisers pay to the search engine need not be equal to the bid amount, but rather determined by the pricing mechanism. For example, in GSP auction, this amount is determined
based the the bid amount and the click rates of the given ad and the ad placed below the
given ad. Thus ranking and pricing together determines the auction mechanism of the ad
placement.

J Intell Inf Syst

The rest of this paper is organized as the follows. The next section reviews related work.
Section 3 explains the click model used for our analysis. Subsequently we introduce our
optimal ranking function, and discuss the intuitions and implications. In Section 5 reductions of our ranking function to several document and ad ranking functions under limiting
assumptions are enumerated. Furthermore we discuss several useful special cases of our
ranking and assumptions under which they are optimal. In Section 6, we incorporate a pricing strategy to design a complete auction mechanism for ads. Several useful properties are
established, including the existence of a Nash equilibrium and revenue dominance over
VCG. Section 7 explores the ranking considering mutual influences and proves our hardness results. We present the experiments and results in Section 8. Finally we discuss our
conclusions and discuss potential future research directions.

2 Related work
The impact of click models on ranking has been analyzed in ad-placement. In our previous
paper Balakrishnan and Kambhampati (2008) we proposed an optimal ad ranking considering mutual influences. The ranking uses the same user model, but the paper considers only
ad ranking, and does not include generalizations and auctions. Later Aggarwal et al. (2008)
as well as Kempe and Mahdian (2008) analyzed placement of ads using a similar Markovian click model. The click model used is less detailed than our model since abandonment
is not modeled separately from click probability. These two papers optimize the sum of the
revenues of the advertisers. We optimize search engine profits in this paper. Nevertheless,
the ranking formulation has common components with these two papers, as workshop version of this paper Balakrishnan and Kambhampati (2008) as these three papers formulated
ranking based on the similar browsing models independently at almost the same time frame.
But, unlike this paper, any of the other two papers do not have a pricing, auctions, or a
generalized taxonomy.
Edelman et al. (2007) analyze a version of GSP auction in their classic paper. They
assume that the click probability at a position is a constant. We relax this assumption, and
account for the influence of ads above on the click probabilities at a position. This difference
gives rise to additional complexities and interesting differences in our mechanism. We show
that GSP proposed by Edelman et al. is a special case of our proposed mechanism.
Giotis and Karlin (2008) extend Markovian model ranking by applying GSP pricing and
analyzing the equilibrium. The GSP pricing and ranking lacks the optimality and generality
properties we prove in this paper. Deng and Yu (2009) extend Markovian models by suggesting a ranking and pricing schema for the search engines and prove the existence of a
Nash Equilibrium. The ranking is a simpler bid based ranking (not based on CPC as in our
case); and mechanism as well as equilibrium do not show optimality properties. Our paper
is different from both the above works by using a more detailed model, by having optimality properties, detailed comparisons with other baseline mechanisms, and in the ability to
generalize to a family of rankings.
Kuminov and Tennenholtz (2009) proposed a Pay Per Action (PPA) model similar to
the click models and compared the equilibrium of GSP mechanism on the model with the
VCG. Ad auctions considering influence of other ads on conversion rates are analyzed by
Ghosh and Sayedi (2010). Both these papers address different problems than considered in
this paper.
Our proposed model is a general case of the positional auctions model by Varian (2007).
Positional auctions assume static click probabilities for each position independent of other

J Intell Inf Syst

ads. We assume more realistic dynamic click probabilities depending on the ads above.
Since we consider these externalities, our model, auction, and analysis are more complex.
(e.g. monotonically increasing values and prices with positions).
The existing document ranking based on PRP (Robertson 1977) claims that a retrieval
order sorted on relevance leads to the largest number of relevant documents in a result set
than any other policy. Gordon and Lenk (1991, 1992) identified the required assumptions for
the optimality of the ranking according to PRP. Our discussion on PRP may be considered
as an independent formulation of assumptions under which PRP is optimal for web ranking.
There are number of user behavior studies in click models validating our assumed user
model and ranking function. There are a number of position based and cascade models
studied (Dupret and Piwowarski 2008; Craswell et al. 2008; Guo et al. 2009; Chapelle and
Zhang 2009; Zhu et al. 2010; Xu et al. 2010; Hu et al. 2011). In particular, General Click
Model (GCM) by Zhu et al. (2010) is interesting, since many other click models are special
cases of GCM. Zhu et al. (2010) list assumptions under which the GCM would reduce to
other click models. We will discuss the relations of our model to GCM below. Optimizing
utilities of two dimensional placement of search results has been studied by Chierichetti
et al. (2011). Many of the recent click models are more general than the click model used
in our paper, but please note that the contribution of our paper is not the click model, but a
unified optimal ranking and auction mechanism based on the click model.
Along with the current click models, there has been research on evaluating perceived
relevance of the search snippets (Yue et al. 2010) and ad impressions (Clarke et al. 2007).
Research in this direction neatly complements our new ranking function by estimating the
parameters required. Chapelle and Zhang (2009) demonstrated that separately modeling perceived and actual relevances improves relevance assessment of documents using click logs.
Diversity ranking has received considerable attention recently (Agrawal et al. 2009;
Rafiei et al. 2010). The objective functions used to measure diversity by prior works are
known to be NP-Hard (Carterette 2010). We provide a stronger proof showing that even
the basic diversity ranking problem is NP-Hard irrespective of any specific objective function, and further show that a constant ratio approximation is unlikely. To the best of our
knowledge, this paper is the first unified optimal ranking and auction mechanism based on
a generalized click model.

3 Click model
As we mentioned above, we approach the ranking as an optimization based on the user‚Äôs
click model on the ads. The expected utilities are maximized based on the click model.
For the optimization, we assume a basic user click model in which the web user browses
the entity list in the ranked order, as shown in Fig. 1. The symbols used in this paper are
explained in Table 1. At every result entity, the user may:
1.

2.

3.

Click the result with perceived relevance C(e). We define the perceived relevance as the
probability of clicking the entity ei having seen ei i.e. C(ei ) = P (click(ei )|view(ei )).
Note that the Click Through Rate (CTR) defined in ad placement is the same as the
perceived relevance defined here (Richardson et al. 2007).
Abandon browsing the result list with abandonment probability Œ≥ (ei ). Œ≥ (ei ) is defined
as the probability of abandoning the search at ei having seen ei . i.e. Œ≥ (ei ) =
P (abandonment (ei )|view(ei )).
Go to the next entity with probability [1 ‚àí (C(ei ) + Œ≥ (ei ))]

J Intell Inf Syst
Table 1 Definition of the symbols
e

A ranked entity.

C(e)

Perceived relevance.

Œ≥ (e)

Abandonment probability.

U (e)

Utility.

Pc (e)

The click probability of the entity at position i in the ranking.

d

A ranked document.

R(d)

Relevance of the document.

a

A ranked ad.

SE

An abbreviation indicating Search Engine.

$(a)

Cost-Per-Click (CPC) of the ad.

v(a)

Private value of the ad for the advertiser.

b(a)

Bid for the ad.

w(a)

Ratio of the click probability to the sum of abandonment and click probability.

Œº(a)

Sum of abandonment and click probability (i.e. C(a) + Œ≥ (a)).

CE(a)

Proposed Click-Efficiency ranking score of the ad.

pi

Payment by the advertiser (CPC) to the search in a given mechanism.

Ur (e)

Residual utility in the context of other entities in the ranked list.

Œ±

Simulation constant to balance between the click and the abandonment probabilities.

The click model can be schematically represented as the flow graph shown in Fig. 1.
Labels on the edges refer to the probability of the user traversing them. Each vertex in the
figure corresponds to a view epoch (see below), and the flow balance holds at each vertex.
Starting from the top entity, the probability of the user clicking the first ad is C(e1 ) and
probability of him abandoning browsing is Œ≥ (e1 ). The user goes beyond the first entity with
probability 1 ‚àí (C(e1 ) + Œ≥ (e1 )) and so on for the subsequent results.
In this model, we assume that the parameters‚ÄîC(ei ), Œ≥ (ei ) and U (ei )‚Äîare functions
of the entity at the current position i.e. these parameters are independent of other entities
the user has already seen. We recognize that this assumption is not fully accurate, since
the user‚Äôs decision to click the current item or to leave the search may depend not just
on the current item but rather on all the entities he has seen before in the list. We stick
to the assumption for the optimal ranking analysis below, since considering mutual influence of ads may lead to combinatorial optimization problems with intractable solutions. We

Fig. 1 Flow graph for an user browsing the first two entities. The labels are the view probabilities and ei
denotes the entity at the i th position

J Intell Inf Syst

will show that even the simplest dependence between the parameters will indeed lead to
intractable optimal ranking in Section 7.
Although the proposed model is intuitive enough, we would like to mention that our
model is also confirmed by the recent empirical click models. For example, the General
Click Model (GCM) by Zhu et al. (2010) is based on the same basic user behavior. The GCM
is empirically validated for both search results and ads (Zhu et al. 2010). Furthermore, other
click models are shown to be special cases of GCM. Please refer to Zhu et al. (2010) for
a detailed discussion. These previous works avoid the need for separate model validation,
as well as confirm the feasibility of the parameter estimation. Further, Yilmaz et al. (2010)
proposes an expected browsing utility metric based on a similar user model.

4 Optimal ranking
Based on the click model, we formally define the ranking problem and derive optimal
ranking in this section. The problem may be stated as,
Choose the optimal ranking Eopt = e1 , e2 , .., eN  of N entities to maximize the
expected utility
N

E(U ) =
U (ei )Pc (ei )
(1)
i=1

where N is the total number of entities to be ranked.
The utility function U (ei ) denotes the stand-alone utility of the entity ei to the search
engine (or one who performs the ranking). This may vary depending on the specific ranking problem. For example, for ranking search results, the utility will be the relevance of
document ei ; whereas for ranking ads to maximize the revenue of the search engine, the
U (ei ) will be pay-per-click of ad ei . We define the specific utility function for entities as
we discuss the specific ranking problems below.
For the browsing model in Fig. 1, the click probability for the entity at the i th position is,
Pc (ei ) = C(ei )

i‚àí1




1 ‚àí C(ej ) + Œ≥ (ej )

(2)

j =1

Substituting click probability Pc from (2) in (1) we get,
E(U ) =

N

i=1

U (ei )C(ei )

i‚àí1



1 ‚àí (C(ej ) + Œ≥ (ej ))



(3)

j =1

The optimal ranking maximizing this expected utility can be shown to be a sorting
problem with a simple ranking function:
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
U (ei )C(ei )
(4)
CE(ei ) =
C(ei ) + Œ≥ (ei )
Proof Sketch The proof shows that any inversion in this order will reduce the expected
profit. CE function is deduced from expected profits of two placements‚Äîthe CE ranked

J Intell Inf Syst

placement and placement in which the order of two adjacent ads are inverted. We show
that the expected profit from the inverted placement can be no greater than the CE ranked
placement. Please refer to Appendix A-1 for the complete proof.
As mentioned in the introduction, the ranking function CE is the utility generated per
unit view probability consumed by the entity. With respect to browsing model in Fig. 1, the
top entities in the ranked list have greater view probabilities, and placing ads with greater
utility per consumed view probability at higher positions intuitively increases total utility.
The proof of Theorem 1 assumes that the user clicks only one entity in the list. Since this
may not always be true, we extend the optimality to multiple clicks in Theorem 2.
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Sketch We proved that ordering according to CE provides maximum expected utility
for single click above. Multiple clicks are the same as the user restarting her browsing from
the entity immediately below the last clicked entity. A simple induction on number of clicks
based on this idea, using a single click as base case is sufficient to prove that the proposed
placement provides maximum expected utility for multiple clicks. See Appendix A-2 for
the complete proof.
Note that the ordering above does not maximize the utility for selecting a subset of items.
The seemingly intuitive method of ranking the set of items by CE and selecting top-k may
not be optimal (Aggarwal et al. 2008). For optimal selection, the proposed ranking can be
extended by a dynamic programming based selection (Aggarwal et al. 2008). In this paper,
we discuss only the ranking problem.

5 Ranking taxonomy
The click model in Fig. 1 is common to many types of rankings including document searches
and search ads. The only difference between these rankings sharing a common click model
is the utility to be maximized. Consequently, the CE ranking can be made applicable to
different ranking problems by plugging in different utilities. For example, if we plug in relevance as utility (U (e) in (4)), the ranking function is applicable for the documents, whereas
if we plug in cost per click of ads, the ranking function is applicable to ads. Furthermore,
we may assume specific constraints on one or more of the three parameters of CE ranking (e.g. ‚àÄi Œ≥ (ei ) = 0). Through these assumptions, CE ranking will suggest a number of
reduced ranking functions with specific applications. These substitutions and reductions can
be enumerated as a taxonomy of ranking functions.
We show the taxonomy in Fig. 2. The three top branches of the taxonomy (U (e) = R(d),
U (e) = $(a), and U (e) = v(a) branches) are for document ranking, ad ranking maximizing
search engine profit, and ad ranking maximizing advertisers revenue respectively. These
branches correspond to the substitution of utilities by document relevance, CPC, and private
value of the advertisers. The sub-trees below these branches are the further reduced cases of
these three main categories. The solid lines in Fig. 2 denote already known functions, while
the dotted lines are the new ranking functions suggested by CE ranking. Sections 5.1, 5.2,
and 5.3 below discuss the further reductions of document ranking, search engine optimal
ad ranking, and social optimal ad ranking respectively.

J Intell Inf Syst

Fig. 2 Taxonomy reduced CE ranking functions. The assumptions and corresponding reduced ranking functions are illustrated. The dotted lines denote predicted ranking functions incorporating new click model
parameters

5.1 Optimal document ranking
For document ranking the utility of ranking is the probability of relevance of the document.
Hence by substituting the document relevance‚Äîdenoted by R(d)‚Äîin (4) we get
CE(d) =

C(d)R(d)
C(d) + Œ≥ (d)

(5)

This function suggests the general optimal relevance ranking for the documents. We discuss
some intuitively valid assumptions on user model for the document ranking and the corresponding ranking functions below. The three assumptions discussed below correspond to
the three branches under Optimal Document Ranking subtree in Fig. 2.

Sort by relevance (PRP) We elucidate two sets of assumptions under which the CE(d)
in (5) will reduce to PRP.
First assume that the user has infinite patience, and never abandons results (i.e. Œ≥ (d) ‚âà
0). Substituting this assumption in (5),
R(d)C(d)
= R(d)
(6)
C(d)
which is exactly the ranking suggested by PRP.
In other words, the PRP is still optimal for scenarios in which the user has infinite
patience and never abandons checking the results (i.e. the user leaves browsing the results
only by clicking a result).
The second set of slightly weaker assumptions under which the CE(d) will reduce to
PRP is
CE(d) ‚âà

1.

C(d) ‚âà R(d).

J Intell Inf Syst

2.

Abandonment probability Œ≥ (d) is negatively proportional to the document relevance
i.e. Œ≥ (d) ‚âà k ‚àí R(d), where k is a constant between one and zero. This assumption
corresponds to the intuition that the higher the perceived relevance of the current result,
the less likely is the user abandoning the search.

Now CE(d) reduces to,
R(d)2
(7)
k
Since this function is strictly increasing with zero and positive values of R(d), ordering just
by R(d) results in the same ranking as suggested by the function. This implies that PRP is
optimal under these assumptions also.
It may be noted that abandonment probability decreasing with perceived relevance is a
more intuitively valid assumption than the infinite patience assumption above.
CE(d) ‚âà

Ranking considering perceived relevance Recent click log studies effectively assess
perceived relevance of document search snippets (Yue et al. 2010; Clarke et al. 2007). But,
how to use the perceived relevance for improved document ranking is still an open question.
The proposed perceived relevance ranking addresses this question.
If we assume that Œ≥ (d) ‚âà 0 in (5), the optimal perceived relevance ranking is the same
as that suggested by PRP as we have seen in (6).
On the other hand, if we assume that the abandonment probability is negatively proportional to the perceived relevance (Œ≥ (d) = k ‚àí C(d)) as above, the optimal ranking
considering perceived relevance is
C(d)R(d)
‚àù C(d)R(d)
(8)
k
i.e. sorting in the order of the product of document relevance and perceived relevance is
optimal under these assumptions. The assumption of abandonment probabilities being negatively proportional to relevance is more realistic than the infinite patience assumption as
we discussed above. This discussion shows that by estimating the nature of abandonment
probability, one would be able to decide on the optimal perceived relevance ranking.
CE(d) ‚âà

Ranking considering abandonment We now examine the ranking considering abandonment probability Œ≥ (d), with the assumption that the perceived relevance is approximately
equal to the actual relevance. In this case CE(d) becomes,
CE(d) ‚âà

R(d)2
R(d) + Œ≥ (d)

(9)

Clearly this is not a strictly increasing function with R(d). Hence the ranking considering
abandonment is different from PRP ranking, even if we assume that the perceived relevance
is equal to the actual relevance. assumption that ‚àÄd Œ≥ (d) = 0, the abandonment ranking
becomes the same as PRP.

5.2 Optimal Ad ranking for search engines
For the paid placement of ads, the utilities of ads to the search engine are Cost-Per-Click
(CPC) of the ads. Hence, by substituting the CPC of the ad‚Äîdenoted by $(a)‚Äî in (4) we
get
C(a)$(a)
(10)
CE(a) =
C(a) + Œ≥ (a)

J Intell Inf Syst

Thus this function suggests the general optimal ranking for the ads. Please recall
that the perceived relevance C(a) is the same as the CTR used for ad placement
(Richardson et al. 2007).
In the following subsections we demonstrate how the general ranking presented reduces
to the currently used ad placement strategies under various assumptions. We will show that
they all correspond to specific assumptions about the abandonment probability Œ≥ (a). These
two functions below corresponds to the two branches under the SE (Search Engine) Optimal
Ad Placement subtree in Fig. 2.

Ranking by bid amount The sort by bid amount ranking was used by Overture Services
(and was later used by Yahoo! for a while after acquisition of Overture). Assuming that the
user never abandons browsing (i.e. ‚àÄa Œ≥ (a) = 0), then (10) reduces to
CE(a) = $(a)

(11)

This means that the ads are ranked purely in terms of their payment. In fact overture ranking
is by bid amount, which is different from payment in a second price auction. But both will
result in the same ranking as higher bids implies higher payments also.
When Œ≥ (a) = 0, we essentially have a user with infinite patience who will keep browsing
downwards until he finds a relevant ad. Hence ranking by bid amount maximizes profit.
More generally, for small abandonment probabilities, ranking by bid amount is near optimal.
Note that this ranking is isomorphic to PRP ranking discussed above for document ranking,
since both ranks are based only on utilities.

Ranking by expected profit Google and Microsoft supposedly place the ads in the order
of expected profit based on product of CTR (C(a) in CE) and bid amount ($(a)) (Richardson et al. 2006). The mechanism is called Generalized Second Price (GSP) auction, and
the most popular one as well. If we approximate abandonment probability as negatively
proportional to the CTR of the ad (i.e. ‚àÄa Œ≥ (a) = k ‚àí C(a)) , the (10) reduces to,
$(a)R(a)
‚àù $(a)R(a)
(12)
k
This shows that ranking ads by their standalone expected profit is near optimal as long as
the abandonment probability is negatively proportional to the relevance. To be accurate,
the Google mechanism‚ÄîGSP‚Äîuses the bid amount of the advertisers (instead of CPC in
(12)) for ranking. Although CPC and bids are different for GSP, we will show that both will
result in the same ranking in Section 6. Note that this ranking is isomorphic to the perceived
relevance ranking of documents discussed above.
CE(a) ‚âà

5.3 Social optimal Ad ranking
An important property of any auction mechanism is social utility, i.e. total utilities of all
the players. In our case this is equal to the sum of the utilities of all the advertisers and
the search engine. To analyze advertiser‚Äôs profit, a private value model is commonly used.
Each advertiser has a private value for the click, which is equal to the expected benefit
(direct and indirect revenue) from the click. Advertisers pay a fraction of this benefit to the
search engine as CPC. The utility for the advertisers is the difference between the private
value and payment to the search engine. The utility for the search engine is the payment
from the advertisers. Hence the social utility is equal to the sum of private values of all the
clicks for the advertisers (which is the sum of utilities of the search engine and advertisers).

J Intell Inf Syst

Consequently, to prove the social optimality all we need to prove is that the total private
values of clicks for the advertisers is optimal.
The social-optimal branch in Fig. 2 corresponds to the ranking to maximize total revenue.
Private value of advertisers ai is denoted as‚Äîv(ai ). By substituting the utility by private
values in (4) we get,
CE(d) =

C(a)v(a)
C(a) + Œ≥ (a)

(13)

If the ads are ranked in this order, the ranking will guarantee maximum revenue. Note that
the optimal revenue does not imply optimal net profits for the advertisers, since part of this
revenue is paid to the search engine as CPC. But optimal revenue implies a maximum total
profit (utility)‚Äîsum of profits of search engine and advertisers.
In Figure 2 the two left branches of the Social Optimal subtree (labeled Œ≥ (a) = 0 and
Œ≥ (a) = k ‚àí C(a)) correspond respectively to the assumption of no abandonment, and
abandonment probabilities being negatively proportional to the click probability. These two
cases are isomorphic to the Overture and Google ranking discussed in Section 5.2 above.
The social optimal ranking is not directly implementable as search engines do not know
the private value of the advertisers. But this ranking is useful in analysis of auctions mechanisms. Furthermore, the search engine may try to effectuate this order through auction
mechanism equilibriums as we demonstrate in Section 6.

6 Applying CE ranking for Ad placement
We have shown that CE ranking maximizes the profits for search engines for given CPCs.
The CPCs are determined by the pricing mechanism used by the search engine. Hence
the overall profit of ranking can be analyzed only in association with a pricing mechanism. The existing ad pricing mechanisms like GSP do not preserve any of their appealing
properties for CE ranking as they do not consider the additional parameter abandonment
probability. For example, the GSP pricing Edelman et al. (2007) is no longer the minimum
amount need to be paid by the advertiser to maintain his position in the CE ranking. To
this end, we design a full auction mechanism by proposing a new second price based pricing to be used with the CE ranking. Subsequently, we analyze the properties of the auction
mechanism.
Let us start by describing the dynamics of ad auctions briefly, the search engine decides
the ranking and pricing (CPC) of the ads based on the bid amounts of the advertisers. Generally the pricing is not equal to the bid amount of advertisers, but derived based on the
bids (Easley and Kleinberg 2010; Edelman et al. 2007; Aggarwal et al. 2006). In response to
these ranking and pricing strategies, the advertisers (more commonly, the software agents of
the advertisers) may change their bids to maximize their profits. They may change bids hundreds of times a day. Eventually, the bids may stabilize at a fixed point where no advertiser
can increase his profit by unilaterally changing his bid, depending on the initial bids and
behavior of the advertisers. This set of bids corresponds to a Nash Equilibrium of the auction
mechanism. Hence the expected profits of a search engine will be the profits corresponding
to the Nash Equilibrium, if the auction attains a Nash Equilibrium.
The next section discusses properties of any mechanism based on the user model‚Äî
independent of the ranking and pricing strategies. In Section 6.2, we introduce a pricing
mechanism and analyze the properties including the equilibrium.

J Intell Inf Syst

6.1 User model based properties
We discuss general properties of all auction mechanisms using the browsing model (Fig. 1).
These properties are implications of the user behavior and applicable to any pricing and
ranking.
Lemma 1 (Individual Rationality) In any equilibrium the payment by the advertisers is less
than or equal to their private values.1
If this is not true, this advertiser may opt out from the auction by bidding zero and
increase the profit, violating the assumption of equilibrium.
Lemma 2 (Pricing Monotonicity) In any equilibrium, the price paid by an advertiser
increases monotonically as he moves up in the ranking unilaterally.
From the browsing model, click probability of the advertisers is non-decreasing as he
moves up in the position. Unless the price increases monotonically, the advertiser may
increase his profit by moving up, thereby violating assumption of an equilibrium.
Lemma 3 (Revenue Maximum) The sum of the payoffs of the advertisers and the search
engine is less than or equal to
E(V ) =

N


v(ai )C(ai )

1 ‚àí (C(aj ) + Œ≥ (aj ))



(14)

j =1

i=1

when the advertisers are ordered by

i‚àí1



C(a)v(a)
C(a)+Œ≥ (a) .

Note that this quantity is the maximal advertiser revenue corresponding to the social
optimal placement in (13), and is a direct consequence. The advertiser pay a fraction of his
revenue to the search engine. Payoff for the advertisers is the difference between the total
revenue and the payment to the search engine. The total payoff of the search engine is the
sum of these payments by all the advertisers. Since the suggested order above in Lemma 3
maximizes total revenue of the advertisers, the sum of the payoffs for the search engine and
the advertisers will not exceed this value.
A corollary of the social optimality combined with the individual rationality result
expressed in Lemma 1 is that,
Lemma 4 (Profit Maximum) The quantity E(V ) in Lemma 3 is an upper bound for the
search engine profit in any equilibrium.

6.2 Pricing and equilibrium
An interesting property of the proposed mechanism is the existence of an equilibrium in
which the search engine optimal ranking coincides with the social optimality. As we proved
above, CE ranking is search engine optimal as it maximizes the revenue for the given CPCs.
On the other hand, social optimal ordering maximizes the total profits for all the players

1 This

property is called individual rationality

J Intell Inf Syst

(search engine and advertisers) for given CPCs. Social optimality is desirable for search
engines, as the increased profits will improve the advertiser‚Äôs preference of one search
engine over others. Since search engines do not know the private value of the advertisers, social optimal ranking is not directly achievable (note that the search engines do the
ranking). A possibility is to design a mechanism having an equilibrium coinciding with the
social optimality, as we propose below. This may cause the bid vector to stabilize in a social
optima.
For defining the pricing strategy for the auction mechanism, we define the pricing order
as the decreasing order of w(a)b(a), where b(a) is the bid value and w(a) is,
w(a) =

C(a)
C(a) + Œ≥ (a)

(15)

In this pricing order, we denote the i th advertiser‚Äôs w(ai ) as wi , C(ai ) as ci , b(ai ) as bi , and
the abandonment probability Œ≥ (ai ) as Œ≥i for convenience. Let Œºi = ci + Œ≥i . For each click,
advertiser ai is charged price pi (CPC) equal to the minimum bid required to maintain its
position in the pricing order,
pi =

wi+1 bi+1
bi+1 ci+1 Œºi
=
wi
Œºi+1 ci

Substituting pi in (10) for the ranking order, CE of the i th advertiser is,
pi ci
CEi =
Œºi

(16)

(17)

This proposed mechanism preserves the pricing order in the ranking as well, i.e.
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj

(18)

The proof for theorem 3 is given in Appendix A-3. This order preservation property
implies that the final ranking is the same as that based on bid amounts. In other words, ads
can be ranked based on the bid amounts instead of CPCs. After the ranking, the CPCs can
be decided based on this ranking order. A corollary of this order preservation is that the
CPC is equal to the minimum amount the advertisers have to pay to maintain their position
in the ranking order.
Furthermore we show below that any advertiser‚Äôs CPC is less than or equal to his bid.
Lemma 5 (Individual Rationality) The payment pi of any advertiser is less or equal to his
bid amount.
Proof
pi =

bi+1 ci+1 Œºi
bi+1 ci+1 Œºi
CEi+1
=
bi =
bi ‚â§ bi (since CEi ‚â• CEi+1 )
Œºi+1 ci
Œºi+1 ci bi
CEi

This means advertisers will never have to pay more than their bid, similar to GSP. This
property makes it easy for the advertiser to decide his bid, as he may bid up to his click
valuation. He will never have to pay more than his revenue, irrespective of bids of other
advertisers.

J Intell Inf Syst

Interestingly, this mechanism is a general case of existing mechanisms, similar to CE
ranking above. The mechanism reduces to GSP (Google mechanism) and Overture mechanisms on the same assumptions on which CE ranking reduces to respective rankings
(described in Section 5.2).
Lemma 6 The mechanism reduces to Overture ranking with a second price auction on the
assumption ‚àÄi Œ≥i = 0
Proof This assumption implies
wi = 1
‚áí pi = bi+1 (second price auction)
‚áí CEi = bi+1 ‚â° bi (i.e. ranking by bi+1 is equivalent to ranking by bi )

Lemma 7 The mechanism reduces to GSP on the assumption ‚àÄi Œ≥i = k ‚àí ci
Proof This assumption implies
wi = c i
bi+1 ci+1
(i.e. ranking reduces to GSP ranking)
ci
bi c i
bi+1 ci+1
‚â°
(by Theorem 3)
‚áí CEi =
k
k
‚àù bi ci
‚áí pi =

This lemma in conjunction with Theorem 3 implies that GSP ranking by ci bi (i.e. by
bids) is the same as the ranking by ci pi (by CPCs).
Now we will look at the equilibrium properties of the mechanism. We start by noting that
truth telling is not a dominant strategy. This trivially follows, since GSP is a special case
of the proposed mechanism, and it is generally known that truth telling is not a dominant
strategy for GSP. Hence we focus on Nash Equilibrium conditions in our analysis.
Theorem 4 (Nash Equilibrium) Without loss of generality, assume that advertisers are
ordered in decreasing order of cŒºi vi i where vi is the private value of the i th advertiser. The
advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
(19)
bi =
ci
Œºi+1
This equilibrium is socially optimal as well as optimal for search engines for the given
CPC‚Äôs.
Proof Sketch The inductive proof shows that for these bid values, no advertisers can
increase his profit by moving up or down in the ranking. The full proof is given in
Appendix A-4. Since the ranking is the same as the social optima order in (13), social
optimality is a direct implication.

J Intell Inf Syst

We do not rule out the existence of multiple equilibriums. The stated equilibrium is
particularly interesting, due to the social optimality and search engine optimality. Furthermore, although the equilibrium depends on the private values of the advertisers unknown
to the search engine, please keep in mind that search engines do not implement equilibriums directly. Instead, search engines decide the pricing and ranking, and the advertisers
may reach an equilibrium by repeatedly revising auction prices. The pricing and ranking are
practical, since they depend solely on the quantities known to the search engine.
The following Lemmas show that equilibriums of other placement mechanisms are special cases of the proposed CE equilibrium. The stated equilibrium reduces to equilibriums
in the Overture mechanism and GSP under the same assumptions (discussed above) under
which the CE ranking reduces to Overture and GSP rankings.
Lemma 8 The bid values
bi = vi ci + (1 ‚àí ci )bi+1

(20)

are in a pure strategy Nash Equilibrium in the Overture mechanism. This corresponds to
the substitution of the assumption ‚àÄi Œ≥i = 0 (i.e. Œºi = ci ) in Theorem 4.
The proof follows from Theorem 4 as both pricing and ranking are shown to be a special
case of our proposed mechanism.
Similarly for GSP,
Lemma 9 The bid values
bi = vi k + (1 ‚àí k)bi+1 ci+1

(21)

is a pure strategy Nash Equilibrium in the GSP mechanism.
This equilibrium corresponds to the substitution of the assumption ‚àÄi Œ≥i = k ‚àí ci (1 ‚â•
k ‚â• 0) in Theorem 4. Since this is a special case, this result follows from Theorem 4.

6.3 Comparison with VCG mechanism
We compare the revenue and equilibrium of CE mechanism with those of VCG (Vickrey
1961; Clarke 1971; Groves 1973). VCG auctions combine an optimal allocation (ranking)
with VCG pricing. VCG payment of a bidder is equal to the reduction of revenues of other
bidders due to the presence of the bidder. A well known property is that VCG pricing with
any socially optimal allocation has truth telling as the the dominant strategy equilibrium.
In the context of online ads, a ranking optimal with respect to the bid amounts is socially
optimal ranking for VCG. This optimal ranking is bŒºi ci i ; as directly implied by the (1) on
substituting bi for utilities. Hence this ranking combined with VCG pricing has truth telling
as the dominant strategy equilibrium. Since bi = vi at the dominant strategy equilibrium,
ranking is socially optimal for advertiser‚Äôs true value as suggested in (13).
The CE ranking function is different from VCG since CE ranking by payments optimizes
search engine profits. On the other hand, VCG ranking optimizes the advertiser‚Äôs profit.
But Theorem 3 shows that for the pricing used in CE, ordering of CE is the same as that
of VCG. This order preserving property facilitates the comparison of CE with VCG. The
theorem below shows revenue dominance of CE over VCG for the same bid values of the
advertisers.

J Intell Inf Syst

Theorem 5 (Search Engine Revenue Dominance) For the same bid values for all the advertisers, the search engine revenue by CE mechanism is greater than or equal to its revenue
by VCG.
Proof Sketch The proof is an induction based on the fact that the ranking by CE and VCG
are the same, as mentioned above. Full proof is given in Appendix A-5.
This theorem shows that the CE mechanism is likely to provide higher revenue to the
search engine even during transient times before the bids settle on equilibriums.
Based on Theorem 5, we prove revenue equivalence of the proposed CE equilibrium
with dominant strategy equilibrium of VCG.
Theorem 6 (Equilibrium Revenue Equivalence) At the equilibrium in Theorem 4, the
revenue of the search engine is equal to the revenue of the truthful dominant strategy
equilibrium of VCG.
Proof Sketch The proof is an inductive extension of Theorem 5. Please see Appendix A-6
for complete proof.
Note that the CE equilibrium has lower bid values than VCG at the equilibrium, but
provides the same profit to the search engine.

7 CE ranking considering mutual influences: diversity ranking
An assumption in CE ranking is that the entities are mutually independent as we pointed out
in Section 3. In other words, the three parameters‚ÄîU (e), C(e) and Œ≥ (e)‚Äîof an entity do
not depend on other entities in the ranked list. In this section we relax this assumption and
analyze the implications. Since the nature of the mutual influence may vary for different
problems, we base our analysis on a specific well known problem‚Äîranking considering
diversity (Carterette 2010; Agrawal et al. 2009; Rafiei et al. 2010).
Diversity ranking accounts for the fact that the utility of an entity is reduced by the
presence of a similar entity above in the ranked list. This is a typical example of the mutual
influence between the entities. All the existing objective functions for the diversity ranking
are known to be NP-Hard (Carterette 2010). We analyze a basic form of diversity ranking
to explain why this is a fundamentally hard problem.
We modify the objective function in (1) slightly to distinguish between the standalone
utilities and the residual utilities‚Äîutility of an entity in the context of other entities in the
list‚Äîas,
E(U ) =

N


Ur (ei )Pc (ei )

(22)

i=1

where Ur (ei ) denotes the residual utility.
We examine a simple case of diversity ranking problem by considering a set of entities‚Äî
all having the same utilities, perceived relevances and abandonment probabilities. Some of
these entities are repeating. If an entity in the ranked list is the same as the entity in the
list above, the residual utility of that entity becomes zero. In this case, it is intuitive that
the optimal ranking is to place the maximum number of pair-wise dissimilar entities in the

J Intell Inf Syst

top slots. The theorem below shows that even in this simple case the optimal ranking is
NP-Hard.
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Sketch The proof is by reduction from the independent set problem. See
Appendix A-7 for the complete proof.
Moreover, the proof by reduction from independent set problem has more implications
than NP-Hardness as shown in the following corollary,
Corollary 1 The constant approximation algorithm for ranking considering diversity is
hard.
Proof The proof of NP-Hardness in the theorem above shows that the independent set problem is a special case of diversity ranking. This implies that a constant ratio approximation
algorithm for the optimal diversity ranking would be a constant ratio approximation algorithm for the independent set problem. Since a constant ratio approximation algorithm for
the independent set is known to be hard (cf. Garey and Johnson 1976 and HaÃästad 1996), the
corollary follows. To define hard, in his landmark paper HaÃästad proved that independent set
problem cannot be solved within n1‚àí for  > 0 unless all problems in N P are solvable in
probabilistic polynomial time, which is widely believed to be not possible.2
This section shows that the optimal ranking considering mutual influences of parameters
is hard. We leave formulating approximation algorithms (not necessarily constant ratio) for
future research.
Beyond proving the intractability of mutual influence ranking, we believe that the
intractability of the simple scenario here explains why all optimal diversity rankings and
constant ratio approximations are likely to be intractable. Furthermore, the proof based
on the reduction from the well explored independent set problem may help in adapting
approximation algorithms from graph theory.

8 Experiments
We compare the profit improvement by CE and reduced forms to existing rankings.
Although the optimality of the proposed ranking is proven above, experiments help to quantify the increase in utilities. Considering the very restricted access to real users and ad click
logs, we limit our evaluations to simulations as it is common in computational advertisement
research. We believe that these experimental results will motivate future online evaluations
in industry settings.
In our first experiment in Fig. 3a, we compare the CE ranking with rank by bid
amount (11) strategy by Overture and rank by bid √ó perceived relevance (12) by Google.
We assign the perceived relevance values as a uniform random number between 0 and Œ±
(0 ‚â§ Œ± ‚â§ 1) and abandonment probabilities as random between 0 and 1 ‚àí Œ±. This assures
‚àÄi (C(ai ) + Œ≥ (ai )) ‚â§ 1 condition required in the click model. The bid amounts for ads are

2 This

belief is almost as strong as the belief P  = N P

J Intell Inf Syst

Fig. 3 a Comparison of Overture, Google and CE rankings. Perceived relevances are uniformly random
in [0, Œ±] and abandonment probabilities are uniformly random in [0, 1 ‚àí Œ±]. CE provides optimal expected
profits for all values of Œ±. b Comparison of CE, PRP and abandonment ranking (9). Abandonment ranking
dominates PRP

J Intell Inf Syst

Fig. 4 Optimality of reduced forms under assumptions (a) setting Œ≥ (d) = k ‚àí R(d). Perceived relevance
ranking is optimal for all values of Œ±. (b) setting C(d) = R(d). In this case, abandonment ranking is optimal

assigned uniform randomly between 0 and 1. We use uniform random for values as it is the
maximum entropy distribution and hence makes least assumptions about the bid amounts.
The number of relevant ads (corresponding to the number of bids on a query) is set to fifty.

J Intell Inf Syst

Simulated users are made to click on ads. The number of ads clicked is set to a random
number generated in a zipf distribution with exponent 1.5. A power law is most intuitive for
the distribution of the number of clicks.
Simulated users browse down the list. Users click an entity with probability equal to the
perceived relevance and abandon the search results with a probability equal to the abandonment probability. The set of entities to be placed is created at random for each run. For the
same set of entities, three runs‚Äîone with each ranking strategy‚Äîare performed. Simulation
is repeated 2 √ó 105 times for each value of Œ±.
An alternate interpretation of Fig. 3a is as the comparison of ranking by CE, PRP and
perceived relevance ranking (8). As we discussed, PRP and perceived relevance rankings
are isomorphic to ad rankings by bid and bid √ó perceived relevance respectively, with utility
being relevance instead of bid amounts. The simulation results are the same.
In Fig. 3b we compare CE, PRP and abandonment ranking (9) under the same settings
used for Fig. 3a. CE provides the maximum utility as expected, and abandonment ranking
occupies the second place. Abandonment ranking provides sub-optimal utility‚Äîsince the
condition ‚àÄd R(d) = C(d) is not satisfied‚Äîbut dominates over PRP. Further, as abandonment probability becomes zero (i.e. Œ± = 1) abandonment rankings becomes same as PRP
and optimal as we predicted in Section 5.1.
Figure 4a compares the perceived relevance ranking (8), CE, and PRP under the condition for optimality for perceived relevance ranking (i.e. ‚àÄd Œ≥ (d) = k ‚àí R(d)). For this,
we set Œ≥ (d) = Œ± ‚àí C(d) keeping all other settings same as the previous experiments.
Figure 4a shows that the perceived relevance ranking provides optimal utility, exactly overlapping with CE curve as expected. Furthermore, note that utilities by PRP are very low
under this condition. The utilities by PRP in fact goes down after Œ± = 0.2. The increase
in abandonment probability, as well as increased sub-optimality of PRP for higher abandonment (since PRP does not consider abandonment) probabilities may be causing this
reduction.
In our next experiment shown in Fig. 4b, we compare abandonment ranking (9) with PRP
and CE under the condition ‚àÄd C(d) = R(d) (i.e. optimality condition for abandonment
ranking). All other settings are the same as those for the experiments in Fig. 3a and b.
Here we observe that the abandonment ranking is optimal and exactly overlaps with CE as
expected. PRP is sub-optimal but closer to optimal than random C(d) used for experiments
in Fig. 3b. The reason may be that C(d) = R(d) is one of the two conditions required for
PRP to be optimal for both sets of assumptions as we discussed in Subsection 5.1. When
abandonment probability becomes zero PRP relevance reaches optimum as we have already
seen.
All these simulation experiments confirm the predictions by the theoretical analysis
above. Although the simulation is no substitute for experiments on real data, we expect that
the observed significant improvements in expected utilities would motivate future online
experiments to quantify profit.

9 Conclusion and future work
We approach the document and ad ranking as a utility maximization based on the user
click model, and derive an optimal ranking‚Äînamely CE ranking. CE ranking is simple and
intuitive; and optimal considering perceived relevance and abandonment probability of user
behavior.

J Intell Inf Syst

On specific assumptions on parameters, the CE ranking function spawns a taxonomy of
rankings in multiple domains. The taxonomy shows that the existing document and ad ranking strategies are special cases of the proposed ranking function under specific assumptions.
The taxonomy is helpful in selecting optimal ranking for a specific user behavior.
To apply CE ranking to ad auctions, we incorporate a second-price based pricing mechanism. The resulting CE mechanism has a Nash Equilibrium which simultaneously optimizes
both the search engine and advertiser revenues. The CE mechanism is revenue dominant
over VCG for the same bid vectors, and has an equilibrium which is revenue equivalent with
the truthful equilibrium of VCG.
We relax the assumption of independence between entities in CE ranking and apply it
to diversity ranking. The ensuing analysis reveals that diversity ranking is an inherently
hard problem; since even the basic formulations are NP-Hard with unlikely constant ratio
approximation algorithms. Furthermore our simulation experiments confirm the results, and
suggest potentially significant increase in profits over the existing rankings.
As future research, assessing profits by CE ranking in an online experiment on a
large scale search engine will quantify improvement in ranking. Estimation and prediction of abandonment probability using click logs and statistical models are interesting
problems. The suggested ranking is optimal for other web ranking scenarios with similar
click models‚Äîlike product and friend recommendations‚Äîand may be extended to these
problems. Furthermore, effective approximation schemes for diversity ranking based on
similarity with the independent set problem may be investigated.
Acknowledgments This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR
grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027, two Google faculty research awards
(2010 & 2013), and a Yahoo key scientific challenges program award (2009).

Appendix
A-1 Proof of theorem 1
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
CE(ei ) =

U (ei )C(ei )
C(ei ) + Œ≥ (ei )

Proof Consider results ei and ei+1 in positions i and i + 1 respectively. Let Œºi = Œ≥ (ei ) +
C(ei ) for notational convenience. The total expected utility from ei and ei+1 when ei is
placed above ei+1 is
i‚àí1




(1 ‚àí Œºj ) U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 )

j =1

If the order of ei and ei+1 are inverted by placing ei above ei+1 , the expected utility from
these entities will be,
i‚àí1




(1 ‚àí Œºj ) U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei ))

j =1

J Intell Inf Syst

Since utilities from all other results in the list will remain the same, the expected utility of
placing ei above ei+1 is greater than inverse placement iff
U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 ) ‚â• U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei )

U (ei+1 )C(ei+1 )
U (ei )C(ei )
‚â•
Œºi
Œºi+1
U (e)C(e)
This means if entities are ranked in the descending order of C(e)+Œ≥
(e) any inversions will
reduce the profit. Since any arbitrary order can be effected by a number of inversions on the
U (e)C(e)
ranking by CE, this implies that ranking by C(e)+Œ≥
(e) is optimal.

A-2 Proof of theorem 2
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Induction on number of clicks.
Base Case: Single click, proved in Theorem 1.
Inductive Hypothesis: The proposed ordering is optimal for n clicks.
Let there be total of n ranked entities and ec be the nth clicked entity. The user will browse
down starting next to ec . Since there is only one click remaining, optimal ordering of entities is in the descending order of CE by the base case. Since the relevance and abandonment
probabilities ec+1 to en remain unchanged by the independence assumption above, the
optimal sequence will be the sub-sequence of ec+1 to en in the ranking.

A-3 Proof of theorem 3
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj
Proof Without loss of generality, we assume that ai refers to ad in the position i in the
descending order of wi bi .
pi ci
Œºi
bi+1 ci+1 Œºi ci
=
Œºi+1 ci Œºi
bi+1 ci+1
=
Œºi+1
= wi+1 bi+1

CEi =

‚â• wi+2 bi+2 = CEi+1

J Intell Inf Syst

A-4 Proof of theorem 4
Theorem 4 (Nash Equilibrium) : Without the loss of generality assume that the advertisers
are ordered in the decreasing order of cŒºi vi i where vi is the private value of the i th advertiser.
The advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
bi =
ci
Œºi+1
This equilibrium is socially optimal for advertisers as well as optimal for search engines for
the given CPC‚Äôs.
Proof Let there are n advertisers. Without loss of generality, let us assume that advertisers
are indexed in the descending order of vŒºi ci i . We prove equilibrium in two steps.
Step 1: Prove that
wi bi ‚â• wi+1 bi+1

w i bi =

(1)

bi c i
Œºi

Expanding bi by (19),
bi+1 ci+1
Œºi+1
= vi ci + (1 ‚àí Œºi )wi+1 bi+1
vi ci
=
Œºi + (1 ‚àí Œºi )wi+1 bi+1
Œºi

wi bi = vi ci + (1 ‚àí Œºi )

Notice that wi bi is a convex linear combination of wi+1 bi+1 and vŒºi ci i . This means that
the value of wi bi is in between (or equal to) the values of wi+1 bi+1 and vŒºi ci i . Hence to
prove that wi bi ‚â• wi+1 bi+1 all we need to prove is that vŒºi ci i ‚â• wi+1 bi+1 . This inductive
proof is given below.
Induction hypothesis: Assume that
‚àÄi‚â•j

vi ci
‚â• wi+1 bi+1
Œºi

Base case: Prove for i = N i.e. for the bottommost ad.
vN‚àí1 cN‚àí1
‚â• wN b N
ŒºN‚àí1
Assuming ‚àÄi>N bi = 0
wN bN = vN cN ‚â§

vN cN
vN‚àí1 cN‚àí1
vi ci
(as ŒºN ‚â§ 1) ‚â§
(by the assumed order i.e. by
)
ŒºN
ŒºN‚àí1
Œºi

Induction: Expanding wj bj by (19),
wj bj =

vj cj
Œºj + (1 ‚àí Œºj )wj +1 bj +1
Œºj

J Intell Inf Syst

wj bj is the convex linear combination, i.e
vj cj
Œºj

vj cj
Œºj

‚â• wj bj ‚â• wj +1 bj +1 , as we know that

‚â• wj +1 bj +1 by induction hypothesis. Consequently,
wj bj ‚â§

vj ‚àí1 cj ‚àí1
vj cj
‚â§
(by the assumed order)
Œºj
Œºj ‚àí1

This completes the induction.

Since advertisers are ordered by wi bi for pricing, the above proof says that the pricing
order is the same as the assumed order in this proof (i.e. ordering by vŒºi ci i ). Consequently,
pi =

bi+1 ci+1 Œºi
Œºi+1 ci

As corollary of Theorem 3 we know that CEi ‚â• CEi+1 .
In the second step we prove the equilibrium using results in Step 1.
Step 2: No advertiser can increase his profit by changing his bids unilaterally
Proof (of lack of incentive to undercut to advertisers below) In the first step let us prove that
ad ai can not increase his profit by decreasing his bid to move to a position j ‚â• i below.
Inductive hypothesis: Assume true for i ‚â§ j ‚â§ m.
Base Case: Trivially true for j = i.
Induction: Prove that the expected profit of ai at m + 1 is less or equal to the expected
profit of ai at i.
Let œÅk denotes the amount paid by ai when he is at the position k. By inductive hypothesis, the expected profit at m is less or equal to the expected profit at i. So we just need to
prove that the expected profit at m + 1 is less or equal to the expected profit at m. i.e.
m
m+1
(vi ‚àí œÅm+1 ) 
(vi ‚àí œÅm ) 
(1 ‚àí Œºl ) ‚â•
(1 ‚àí Œºl )
(1 ‚àí Œºi )
(1 ‚àí Œºi )
l=1

l=1

Canceling the common terms,
vi ‚àí œÅm ‚â• (vi ‚àí œÅm+1 )(1 ‚àí Œºm+1 )

(2)

œÅm ‚Äîthe price charged to ai at position m‚Äîis based on the Equations 16 and 19. Since the
ai is moving downward, ai will occupy position m by shifting ad am upwards. Hence the ad
just below ai is am+1 . Consequently, the price charged to ai when it is at the mth position is,

	
bm+1 cm+1 Œºi
Œºi
bm+2 cm+2
=
vm+1 cm+1 + (1 ‚àí Œºm+1 )
œÅm =
Œºm+1 ci
ci
Œºm+2
Substituting for œÅm and œÅm+1 in (2),

	
Œºi
bm+2 cm+2
vm+1 cm+1 + (1 ‚àí Œºm+1 )
vi ‚àí
ci
Œºm+2

	


Œºi
bm+3 cm+3
vm+2 cm+2 + (1 ‚àí Œºm+2 )
(1‚àíŒºm+1 )
‚â• vi ‚àí
ci
Œºm+3

J Intell Inf Syst

Simplifying, and multiplying both sides by ‚àí1

	
bm+2 cm+2
Œºi
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
	

bm+3 cm+3
√ó vm+2 cm+2 + (1 ‚àí Œºm+2 )
Œºm+3
Substituting by bm+2 from (19) on RHS.

	
bm+2 cm+2
Œºi
bm+2 cm+2
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
Œºm+2
Canceling out the common terms on both sides,
Œºi
vm+1 cm+1 ‚â§ vi Œºm+1
ci

vi ci
vm+1 cm+1
‚â§
Œºm+1
Œºi
Which is true by the assumed order as m ‚â• i
Inductive proof for m ‚â§ i is somewhat similar and enumerated below.
Inductive hypothesis: Assume true for j ‚â§ m.
Base Case: Trivially true for j = i.
Proof (of lack of incentive to overbid ad one above) The case in which ai increases his bid
to move one position up i.e. to i ‚àí 1 is a special case and need to be proved separately. In
this case, by moving a single slot up, the index of the ad below ai will change from i + 1
to i ‚àí 1 (a difference of two). For all other movements of ai to a position one above or one
below, the index of the advertisers below will change only by one. Since the amount paid
by ai depends on the ad below ai , this case warrants a slightly different proof,
(vi ‚àí œÅi )

i‚àí1


(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

l=1

i‚àí2


(1 ‚àí Œºl )

l=1


(vi ‚àí œÅi )(1 ‚àí Œºi‚àí1 ) ‚â• vi ‚àí œÅi‚àí1
Expanding œÅi is straight forward.To expand œÅi‚àí1 , note that when ai has moved upwards to
i ‚àí 1, the ad just
 below ai is ai‚àí1 . Since ai‚àí1 has not changed its bids, the œÅi‚àí1 can be
Œºi
expanded as ci vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 ) bŒºi ci i . Substituting for œÅi and œÅi‚àí1 ,


vi ‚àí



	
	
Œºi
Œºi
bi+2 ci+2
bi ci
vi+1 ci+1 + ‚â• vi ‚àí
vi‚àí1 ci‚àí1 + (1‚àíŒºi+1 )
(1 ‚àí Œºi‚àí1 )(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi+2
Œºi

Simplifying and multiplying by ‚àí1
vi Œºi‚àí1 +



	
	
Œºi
Œºi
bi ci
bi+2 ci+2
vi+1 ci+1 + ‚â§
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi+1 )
(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi
Œºi+2

J Intell Inf Syst

Substituting bi+1 from (19)


	
Œºi bi+1 ci+1
Œºi
bi c i
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi‚àí1 ) ‚â§
ci Œºi+1
ci
Œºi

Œºi
bi+1 ci+1
Œºi vi‚àí1 ci‚àí1
Œºi
bi c i
‚â§
+ (1 ‚àí Œºi‚àí1 )
vi Œºi‚àí1 + (1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
ci
Œºi
vi Œºi‚àí1 +

We now prove that both the terms in RHS are greater or equal to the corresponding terms in
LHS separately.
Œºi vi‚àí1 ci‚àí1
vi Œºi‚àí1 ‚â§
ci

vi‚àí1 ci‚àí1
vi ci
‚â§
Œºi
Œºi‚àí1
Which is true by our assumed order.
Similarly,
bi+1 ci+1
Œºi
bi c i
Œºi
(1 ‚àí Œºi‚àí1 )
‚â§
(1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
Œºi

bi ci
bi+1 ci+1
‚â§
Œºi+1
Œºi
Which is true by (1) above. This completes the proof for this case.
Induction: Prove that the expected profit at m ‚àí 1 is less or equal to the expected profit
at m. The proof is similar to the induction for the case m > i.
Proof Base case is trivially true.
(vi ‚àí œÅm )

m‚àí1


m‚àí2


l=1

l=1

(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

(1 ‚àí Œºl )

Canceling common terms,
(vi ‚àí œÅm )(1 ‚àí Œºm‚àí1 ) ‚â• vi ‚àí œÅm‚àí1
In this case, note that ai is moving upwards. This means that ai will occupy position m by
pushing the ad originally at m one position downwards. Hence the original ad at m is the
one just below ai now. i.e.

	
bm cm Œºi
Œºi
bm+1 cm+1
œÅm =
vm cm + (1 ‚àí Œºm )
=
Œºm ci
ci
Œºm+1
Substituting for œÅm and œÅm‚àí1


vi ‚àí



	
	
Œºi
Œºi
bm+1 cm+1
bm cm
vm cm + ‚â• vi ‚àí
vm‚àí1 cm‚àí1 + (1‚àíŒºm )
(1‚àíŒºm‚àí1 )(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm+1
Œºm

Simplifying and multiplying by ‚àí1
vi Œºm‚àí1 +



	
	
Œºi
Œºi
bm cm
bm+1 cm+1
vm cm + ‚â§
vm‚àí1 cm‚àí1 + (1‚àíŒºm‚àí1 )
(1 ‚àí Œºm )
(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm
Œºm+1

J Intell Inf Syst

Substituting by bm from (19)

	
Œºi bm cm
Œºi
bm c m
vm‚àí1 cm‚àí1 + (1 ‚àí Œºm‚àí1 )
(1 ‚àí Œºm‚àí1 ) ‚â§
vi Œºm‚àí1 +
ci Œºm
ci
Œºm
Canceling common terms,
Œºi
vm‚àí1 cm‚àí1
ci

vi Œºm‚àí1 ‚â§


vm‚àí1 cm‚àí1
vi ci
‚â§
Œºi
Œºm‚àí1
Which is true by the assumed order as m < i.

A-5 Proof of theorem 5
Theorem 5 (Search Engine Revenue Dominance) : For the same bid values for all the
advertisers, the revenue of search engine by CE mechanism is greater or equal to the
revenue by VCG.
Proof VCG payment of the ad at position i (i.e. ai ) is equal to the reduction in utility of
the ads below due to the presence of ai . For each user viewing the list of ads (i.e. for unit
view probability), the total expected loss of ads below ai due to ai is,

piVu =
=

=

j
‚àí1
j
‚àí1
n
n


1
bj c j
(1 ‚àí Œºk ) ‚àí
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi

Œºi
1 ‚àí Œºi

j =i+1

k=1

n


j
‚àí1

bj c j

j =i+1

j =i+1

k=1

(1 ‚àí Œºk )

k=1

j
‚àí1
n
i

Œºi 
(1 ‚àí Œºk )
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi
j =i+1

k=1

= Œºi

i‚àí1


(1 ‚àí Œºk )

n


bj c j

j =i+1

k=1

k=i+1

j
‚àí1

(1 ‚àí Œºk )

k=i+1

This is the expected lose per user browsing the ad list. Pay per click should be equal to the
lose per click. To calculate the pay per click, we divide by the click probability of ai . i.e.

piV =
=

Œºi

i‚àí1

j ‚àí1
j =i+1 bj cj
k=i+1 (1 ‚àí Œºk )
i‚àí1
ci k=1 (1 ‚àí Œºk )
j
‚àí1

k=1 (1 ‚àí Œºk )

n
Œºi 
bj c j
ci
j =i+1

n

(1 ‚àí Œºk )

k=i+1

J Intell Inf Syst

Converting to recursive form,
bi+1 Œºi
Œºi ci+1 V
ci+1 + (1 ‚àí Œºi+1 )
p
ci
ci Œºi+1 i+1
bi+1 Œºi ci+1
Œºi ci+1 V
=
Œºi+1 + (1 ‚àí Œºi+1 )
p
ci Œºi+1
ci Œºi+1 i+1

piV =

For the CE mechanism payment from (16) is,
piCE =

bi+1 ci+1 Œºi
Œºi+1 ci

Note that piV is convex combination of PiCE and
two values. To prove that

piCE

‚â•

piV

Œºi ci+1 V
ci Œºi+1 pi+1 ,

and hence is between these

all we need to prove is that PiCE ‚â•

Œºi ci+1 V
ci Œºi+1 pi+1

‚áî

bi ‚â• piV . This directly follows from individual rationality property of VCG. Alternatively, a
V = 0 (bottommost ad) will prove the same. Note that
simple recursion with base case as pN
we consider only the ranking (not selection), and hence the VCG pricing of the bottommost
ad in the ranking is zero.

A-6 Proof of theorem 6
Theorem 6 (Equilibrium Revenue Equivalence) : At the equilibrium in Theorem 4, the revenue of search engine is equal to the revenue of the truthful dominant strategy equilibrium
of VCG.
Proof Rearranging (3) and substituting true values for bid amounts,

	
Œºi
(1 ‚àí Œºi+1 )ci+1 V
V
vi+1 ci+1 +
pi+1
pi =
ci
Œºi+1
For the CE mechanism, substituting equilibrium bids from (19) in payment (16),

	
bi+1 ci+1 Œºi
Œºi
bi+2 ci+2
=
vi+1 ci+1 + (1 ‚àí Œºi+1 )
piCE =
Œºi+1 ci
ci
Œºi+2
Rewriting bi+2 in terms of pi+1 ,
piCE =


	
Œºi
(1 ‚àí Œºi+1 )ci+1 CE
vi+1 ci+1 +
pi+1
ci
Œºi+1

= piV

V
CE
(iff pi+1
= pi+1
)

Ad at the bottommost position pays same amount zero, a simple recursion will prove that
the payment for all positions for both VCG and the proposed equilibrium is the same.

A-7 Proof of theorem 7
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Independent set problem can be formulated as a ranking problem considering similarities. Consider an unweighed graph G of n vertices {e1 , e2 , ..en } represented as an

J Intell Inf Syst

adjacency matrix. This conversion is clearly polynomial time. Now, consider the values in
the adjacency matrix as the similarity values between the entities to be ranked. Let the entities have the same utilities, perceive relevances and abandonment probabilities. In this set of
n entities from {e1 , e2 , .., en }, clearly the optimal ranking will have k pairwise independent
entities as the top k entities for a maximum possible value of k. But the set of k independent
entities corresponds to the maximum independent set in graph G.

References
Aggarwal, G., Feldman, J., Muthukrishnan, S., & PaÃÅl, M. (2008). Sponsored search auctions with markovian
users. Internet and Network Economics, 621‚Äì628.
Aggarwal, G., Goel, A., & Motwani, R. (2006). Truthful auctions for pricing search keywords. In Proceedings
of the 7th ACM conference on Electronic commerce (pp. 1‚Äì7), ACM.
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying search results. In Proceedings of
the Second ACM International Conference on Web Search and Data Mining (pp. 5‚Äì14). ACM.
Balakrishnan, R., & Kambhampati, S. (2008). Optimal ad ranking for profit maximization. In Proceedings
of the 11th International Workshop on the Web and Databases.
Carterette, B. (2010). An analysis of NP-completeness in novelty and diversity ranking. Advances in
Information Retrieval Theory, 200‚Äì211.
Chapelle, O., & Zhang, Y. (2009). A dynamic bayesian network click model for web search ranking. In
Proceedings of World Wide Web (pp. 1‚Äì10). ACM.
Chierichetti, F., Kumar, R., & Raghavan, P. (2011). Optimizing two-dimensional search results presentation.
In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 257‚Äì
266). ACM.
Clarke, C.L.A., Agichtein, E., Dumais, S., & White, R.W. (2007). The influence of caption features on
clickthrough patterns in web search. In Proceedings of SIGIR (pp. 135‚Äì142). ACM.
Clarke, E.H. (1971). Multipart pricing of public goods. Public Choice, 11(1), 17‚Äì33.
Craswell, N., Zoeter, O., Tayler, M., & Ramsey, B. (2008). An experimental comparison of click position
bias models. In Proceedings of WSDM (pp. 87‚Äì94).
Deng, X., & Yu, J. (2009). A new ranking scheme of the GSP mechanism with markovian users. Internet and
Network Economics, 583‚Äì590.
Dupret, G.E., & Piwowarski, B. (2008). A user browsing model to predict search engine click data from past
observations. In Proceedings of SIGIR, (pp. 331‚Äì338). ACM.
Easley, D., & Kleinberg, J. (2010). Networks, crowds, and markets: Reasoning about a highly connected
world: Cambridge Univ Press.
Edelman, B., Ostrovsky, M., & Schwarz, M. (2007). Internet advertising and the generalized second price
auction: Selling billions of dollars worth of keywords. The American Economic Review, 97(1).
Garey, M.R., & Johnson, D.S. (1976). The complexity of near-optimal graph coloring. Journal of the ACM
(JACM), 23(1), 43‚Äì49.
Ghosh, A., & Sayedi, A. (2010). Expressive auctions for externalities in online advertising. In Proceedings
of the 19th international conference on World wide web (pp. 371‚Äì380). ACM.
Giotis, I., & Karlin, A. (2008). On the equilibria and efficiency of the GSP mechanism in keyword auctions
with externalities. Internet and Network Economics, 629‚Äì638.
Gordon, M.G., & Lenk, P. (1991). A utility theory examination of probability ranking principle in information
retrieval. Journal of American Society of Information Science, 41, 703‚Äì714.
Gordon, M.G., & Lenk, P. (1992). When is probability ranking principle suboptimal. Journal of American
Society of Information Science, 42.
Groves, T. (1973). Incentives in teams. Econometrica: Journal of the Econometric Society, 617‚Äì631.
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.M., & Faloutsos, C. (2009). Click chain model
in web search. In Proceedings of World Wide Web (pp. 11‚Äì20). New York: ACM.
HaÃästad, J. (1996). Clique is hard to approximate within n. In Foundations of Computer Science, 1996. 37th
Annual Symposium on Proceedings (pp. 627‚Äì636).
Hu, B., Zhang, Y., Chen, W., Wang, G., & Yang, Q. (2011). Characterizing search intent diversity into click
models. In Proceedings of the 20th international conference on World wide web (pp. 17‚Äì26). ACM.
Kempe, D., & Mahdian, M. (2008). A cascade model for externalities in sponsored search. Internet and
Network Economics, 585‚Äì596.

J Intell Inf Syst
Kuminov, D., & Tennenholtz, M. (2009). User modeling in position auctions: re-considering the gsp and vcg
mechanisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 1 (pp. 273‚Äì280).
Rafiei, D., Bharat, K., & Shukla, A. (2010). Diversifying Web Search Results. In Proceedings of World Wide
Web.
Richardson, M., Dominowska, E., & Ragno, R. (2007). Predicting clicks: Estimating the click-through rate
for new ads. In Proceedings of World Wide Web.
Richardson, M., Prakash, A., & Brill, E. (2006). Beyond pagerank: Machine learning for static ranking. In
World Wide Web Proceedings (pp. 707‚Äì714). ACM.
Robertson, S.E. (1977). The probability ranking principle in ir. Journal of Documentation, 33, 294‚Äì304.
Varian, H.R. (2007). Position auctions. International Journal of Industrial Organization, 25(6), 1163‚Äì1178.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1), 8‚Äì37.
Xu, W., Manavoglu, E., & Cantu-Paz, E. (2010). Temporal click model for sponsored search. In Proceedings
of the 33rd international ACM SIGIR conference on Research and development in information retrieval
(pp. 106‚Äì113). ACM.
Yilmaz, E., Shokouhi, M., Craswell, N., & Robertson, S. (2010). Expected browsing utility for web search
evaluation. In Proceedings of the 19th ACM international conference on Information and knowledge
management (pp. 1561‚Äì1564). ACM.
Yue, Y., Patel, R., & Roehrig, H. (2010). Beyond position bias: Examining result attractiveness as a source
of presentation bias in clickthrough data. In Proceedings of World Wide Web.
Zhu, Z.A., Chen, W., Minka, T., Zhu, C., & Chen, Z. (2010). A novel click model and its applications to
online advertising. In In Proceedings of Web search and data mining (pp. 321‚Äì330). ACM.

A Combinatorial Search Perspective on Diverse Solution Generation
Satya Gautam Vadlamudi and Subbarao Kambhampati
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{gautam , rao}@asu.edu

Abstract
Finding diverse solutions has become important in many
combinatorial search domains, including Automated Planning, Path Planning and Constraint Programming. Much of
the work in these directions has however focussed on coming up with appropriate diversity metrics and compiling
those metrics in to the solvers/planners. Most approaches use
linear-time greedy algorithms for exploring the state space
of solution combinations for generating a diverse set of solutions, limiting not only their completeness but also their
effectiveness within a time bound. In this paper, we take a
combinatorial search perspective on generating diverse solutions. We present a generic bi-level optimization framework
for finding cost-sensitive diverse solutions. We propose complete methods under this framework, which guarantee finding
a set of cost sensitive diverse solutions satisficing the given
criteria whenever there exists such a set. We identify various aspects that affect the performance of these exhaustive
algorithms and propose techniques to improve them. Experimental results show the efficacy of the proposed framework
compared to an existing greedy approach.

In many real-world domains involving combinatorial search
such as automated planning, path planning and constraint
programming, generating diverse solutions is of much importance. In the case of automated planning, real-world scenario often involves working with unknown or partially
known user preferences (Kambhampati 2007), as the user
preferences are many times difficult to be articulated and
specified completely. Such situations lead to multiple, often, large number of plans that satisfy a given problem instance. In order to facilitate serving the user with a closest
plan possible as per her (hidden) preferences, presenting a
diverse set of plans to the user is explored (Roberts, Howe,
and Ray 2014; Nguyen et al. 2012) so that the user can make
a well-informed decision. In the constraint programming domain, diverse (resp. similar) solutions are explored in order
to handle unknown user preferences as well as to generate
robust solutions (Hebrard et al. 2005).
Several methods have been proposed in the literature for
finding a diverse set of plans. In the context of constraint
programming, (Hebrard et al. 2005) presents a complete
method which creates K copies of the Constraint Satisfacc 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

tion Problem (CSP),each copy with a different set of variable names, adds K
2 additional constraints for handling the
minimum distance requirement between all pairs, and uses
off-the-shelf solvers to generate solutions. As one would expect, they report that this approach generates prohibitively
large CSPs and therefore propose a greedy method. The
greedy approach which has since been widely adopted (Petit and Trapp 2015; Bloem 2015; Roberts, Howe, and Ray
2014; Nguyen et al. 2012) works as follows:
Obtain a candidate solution satisfying any given cost criteria, add this to the solution K-set, provide feedback to the
method finding candidate solutions about the current composition of the K-set so that it tries to find the next candidate
solution distant to the current K-set. Upon obtaining the next
candidate solution, the greedy method adds it to the K-set if
it indeed satisfies the distance criteria and provides feedback
to find the next solution, otherwise the candidate solution is
simply discarded. This process is continued until a set of K
diverse solutions is found. (Roberts, Howe, and Ray 2014;
Eiter et al. 2013) and (Nguyen et al. 2012) consider the
first solution generated to be the starting solution (permanent member) for constructing the K-set through the above
greedy approach. (Bloem 2015) considers an optimal solution to be the starting solution of the greedy method. (Petit
and Trapp 2015) attempt to address the issue of fixed starting solution by running the greedy approach multiple times
with different optimal solutions as the starting solution on
each occasion.
A pertinent issue with the above approaches is that the
first solution (or an optimal solution) is always considered
to be part of the solution set, which may often result in not
finding a K-set even when there exists one, even with a very
good feedback strategy to search for distant solutions after
finding the initial solution. Note that an optimal solution (or
the first found solution) need not be part of a diverse set of
the required size at all. Figure 1 shows an example of an
instance where the optimal solution is not part of the most
diverse solution set of size 2 (assuming that the distance between two plans is inversely proportional to the number of
edges/actions they have in common; p1 and p2 have edge a
in common and p2 and p3 have edge b in common).
In this paper, we address this problem in depth by proposing complete algorithms which guarantee to find a set of K
diverse solutions whenever there exists one. In order to ac-

Start
p1

1

3
G1

a

1
p2

1
2

1 b

p3

G2
Figure 1: A state-space graph where the optimal cost path
p2, does not belong to the most diverse solution set of size 2
{p1,p3}. G1 and G2 indicate goal nodes.
complish completeness, we first need methods for exploring
all possible cost sensitive solutions in the given domain. For
this, we present extensions of the m-A* algorithm (Dechter,
Flerova, and Marinescu 2012) and the Depth-First Branch
and Bound algorithm (Lawler and Wood 1966) for finding
all cost-bounded plans. One could adapt other types of methods such as anytime heuristic search algorithms as well for
this purpose. Second, we present a simple strategy for exhaustively exploring the set of all plan combinations. This
would guarantee completeness for finding a K-set whenever there exists one, thereby addressing the issues in existing methods. However, we note that the simple exhaustive
method ends up having to explore a large number of combinations as one finds more and more candidate plans, severely
impacting the performance of the overall algorithm. In order
to address this problem, we propose a method which considers only a few most promising plan combinations whenever
a new candidate solution is found, and postpones the exploration of remaining combinations for the end to guarantee
completeness. This new method is advantageous over the
widely followed greedy approach on two fronts: it explores
a larger (compared to only one combination of the greedy
approach) but limited number of combinations upon finding
a candidate solution thereby increasing its likelihood of finding a diverse K-set quickly, and it keeps track of the combinations that are left postponed to revisit at the end thereby
guaranteeing completeness.
Further, our method for exploring the plan-combinations
space can be used in conjunction with any of the existing
methods to improve their performance, as it is complementary in nature, replacing the weaker section of those methods
where the greedy approach is present.

Refanidis 2013), air traffic control advisories (Bloem and
Bambos 2014), and robotics (Voss, Moll, and Kavraki 2015).
An important measure in determining diversity is the distance between plans. In this paper, we assume that the distance measure is given as input by the user. Several distance measures have been proposed in the literature that are
quantitative or qualitative (Scala 2014; Coman and MunÃÉozAvila 2011; Goldman and Kuter 2015). Solution diversity
is explored in both deterministic and non-deterministic domains using the distance metrics (Coman 2012). Distance
measures for finding semantically distinct plans are explored
in (Bryce 2014) based on landmarks. In the context of constraint programming, distance constraints in terms of ideal
and non-ideal solutions are studied in (Hebrard, O‚ÄôSullivan,
and Walsh 2007).
SAT-based heuristic methods for generating diverse solutions were proposed in (Nadel 2011). Methods through
compilation to CSP, and using heuristic local search have
been proposed in (Srivastava et al. 2007), which use GPCSP planner (Do and Kambhampati 2001) and LPG planner (Gerevini, Saetti, and Serina 2003). Comparison of firstprinciple techniques and case-based planning techniques to
find diverse plans is shown in (Coman and MunÃÉoz-Avila
2012a). These algorithms too use the greedy approach presented before for exploring the space of plan combinations,
leading to the same issues pointed in the Introduction.

Problem Setup
In this paper, we consider the problem of finding a set of solutions that are not only diverse but are also cost sensitive.
In particular, we consider the problem of finding a set of K
cost sensitive diverse (loopless) solutions. Cost sensitivity
of the solutions is controlled by the input c (maximum cost
of each of the solutions) and diversity of the solution sets is
controlled by the input d (minimum distance between each
pair of solutions; or an appropriate set based diversity metric). Both the cost metric and the distance metric are also assumed to be inputs from the user, hence, the studies on good
quality cost metrics and distance metrics are orthogonal to
our work. Further, we choose the planning domain to showcase our framework and methods, which could be adapted to
other domains. Hence, the problem at-hand can be formally
stated as: Given a planning problem with the set of loopless
solution plans S, a cost metric for the plans C : S ‚Üí R and
a distance metric for the pairs of plans Œ¥ : S √ó S ‚Üí R (a set
based diversity metric may also be used here), the problem
is defined as:
cCOSTdDISTANTkSET: Find P with P ‚äÜ S,
(1)
|P| =k, min Œ¥(p, q) ‚â• d and C(p) ‚â§ c ‚àÄp ‚àà P
p, q ‚àà P

Related Work
Several applications have been related to using diverse solutions in recent years, such as, for course of action generation in cyber security (Boddy et al. 2005), personalized security agents (Roberts et al. 2012), diverse finite
state machines for non-player characters in games (Coman
and MunÃÉoz-Avila 2013; 2012b), formal verification (Nadel
2011), mining group patterns (Vadlamudi, Chakrabarti, and
Sarkar 2012), scheduling personal activities (Alexiadis and

The problem is computationally hard given that the problem
of finding cost-bounded plans is PSPACE-complete (Bylander 1991) and the problem of finding a diverse set of plans
is NP-complete (Bloem 2015) with input size (number of
plans) that can potentially be exponential in terms of the
number of state variables. Finding a set of diverse solutions
is shown to be FPN P [log n] -complete in the context of constraint programming (Hebrard et al. 2005), where n is the
size of the input.

Explore All Solution Combinations
(Level 2)
Solution Stream

Feedback

Explore All Satisficing Solutions
(Level 1)

Figure 2: A framework for finding a diverse set of solutions
that supports completeness.

Proposed Methods & Properties
Now, we present the proposed framework which supports
completeness, and specific methods that obey the framework
requirements. As mentioned above, the problem of finding a
diverse set of plans is a bi-level optimization problem which
involves exploring the set of all candidate plans (Level 1)
and considering the set of all combinations of these plans
(Level 2). Therefore, in order to guarantee completeness,
we propose to have a framework with a complete method
which guarantees finding all the candidate plans and outputs as a stream, and another complete method that takes
the stream of plans being generated by the previous method
as input and explores all combinations of plan sets as per
the diversity criteria until a diverse set of size K is found.
Such a framework ensures that all possible cases are considered and hence guarantees completeness. Figure 2 shows the
framework with the control flow. In this paper, we emphasize
mainly on how to explore all the solutions and all the solution combinations efficiently so as to guarantee completeness while not impeding the search progress due to their individual exhaustive nature. The feedback component shown
in the figure is particular to the domain elements and their
distance measures which we do not explore in this work
leaving it as an option for the user to plug-in to the proposed
framework and methods.

Algorithms for Finding All Cost Sensitive Solutions
For the Level 1, the problem is to find the set of all candidate plans that can potentially be part of a diverse solution
set. In our case, the set of all candidate plans correspond to
the set of all valid loopless plans whose cost is ‚â§ max cost.
We present two complete methods which guarantee generating the set of all candidate plans, one based on DepthFirst Branch and Bound (DFBB) (Lawler and Wood 1966;
Russell and Norvig 1995) and another based on a recent algorithm for finding M best solutions in graphical models,
called m-A* (Dechter, Flerova, and Marinescu 2012). One
may also use other complete methods and extend them to
generate all candidate solutions whose cost is ‚â§ max cost.
First, we present the DFBB based algorithm for finding
all candidate plans, called DFA. It works similar to regular
DFBB search on graph spaces except for the following two
differences: (i) it does not stop after finding a single solution within the max cost bound, and (ii) it does not conduct
full duplicate detection (any state reached through a differ-

ent path from the start state leads to a new node unless there
is a loop, at which point it simply backtracks to find other
solutions). It is easy to prove that:
Lemma 1 DFA generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.
Now, we describe the second algorithm for finding the set
of all candidate plans, called A*A. This is based on the mA* algorithm (Dechter, Flerova, and Marinescu 2012) which
guarantees finding m best solutions/plans by expanding the
minimum set of nodes. First, we describe the basic idea behind m-A* and then we extend it to find the set of all candidate plans for our problem. The basic idea behind m-A* is to
proceed in a manner similar to A* and whenever a duplicate
state is found, the most promising m nodes corresponding to
that state are to be considered for expansion and the rest be
discarded. For our problem, where we want to find the set
of all cost-bounded plans, we will have to keep all the nodes
corresponding to same state for expansion without the mlimit. This eliminates the requirement of full-scale duplicate
detection all-together, instead suggests treatment of all children being generated as new. However, since we are only
interested in loopless plans, we will discard all nodes which
cause loops in the partial plan at any stage, through cycle
checking. We call this adapted strategy- A*A (A* based approach for finding All cost-bounded solutions). Once again,
it can be proven that:
Lemma 2 A*A generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.

Complete Algorithms for Finding a Diverse
Solution Set
Now, we present the algorithms for Level 2 of our framework, where the stream of all candidate plans, the distance
measure, the size of the diverse plan set needed and the minimum distance between any two plans of the solution set is
given as input, to produce a set of satisficing diverse plans.
Algorithm 1 presents a simple strategy called ACER
which explores all combinations of plans in each run for the
in-coming new plan in conjunction with all of the existing
valid plan sets. It is easy to prove that:
Lemma 3 ACER finds a diverse plan set of size K from the
set/stream of all candidate plans whenever there exists one.
However, G can grow rapidly and become an exponential sized set in terms of K with base being the number of
candidate plans (which itself can be of exponential size in
terms of the planning problem input) before finding a diverse plan set. This severely limits its scalability when there
are large number of candidate plans (even for moderate values of K), which is often the case in practice. Next, we will
present a method which does not explore all plan set combinations in one shot instead only a select most promising sets
at each stage, while keeping track of unexplored combinations that may be explored at the end (after processing the
entire stream of candidate plans once) for completeness.

Algorithm 1 Explore All Possible Combinations of Solutions in Each Run (ACER)

Algorithm 2 Explore Most-promising Combinations of Solutions in Each Run (MCER)

1: INPUT :: A candidate plan p (from the stream of all candidate plans), a distance measure dist(), the minimum distance
needed between any two plans of a set min dist, and the size
of the diverse plan set K.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets.
4: for each plan set P ‚àà G do
5:
if dist(p, l) > min dist ‚àÄl ‚àà P then
6:
P 0 ‚Üê P + {p};
7:
if |P 0 | = K then
8:
return P 0 ;
9:
end if
10:
G ‚Üê G‚à™P 0 ;
11:
end if
12: end for
13: G ‚Üê G‚à™{p};
14: return largest P ‚àà G;

1: INPUT :: A candidate plan p (from the stream of all candidate
plans), its sequence number in the stream i, a distance measure
dist(), the minimum distance needed between any two plans
of a set min dist, the size of the diverse plan set K, and the
number of seed plan sets to be explored n.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets with satellite data.
4: Open ‚Üê œÜ; Children ‚Üê œÜ;
5: ExpandMostPromising(G, n, Children);
6: if a diverse set P of size K is found then
7:
return P ;
8: end if
9: while Children 6= œÜ do
10:
Swap Children and Open;
11:
ExpandMostPromising(Open, n, Children);
12:
if a diverse set P of size K is found then
13:
return P ;
14:
end if
15:
Move all plan sets in Open to G;
16: end while
17: P ‚Üê {p}; Pexp ‚Üê i;
18: G ‚Üê G‚à™P ;
19: return largest P ‚àà G;

The second technique, which is an adaptation of the
Anytime Pack Search method (Vadlamudi, Aine, and
Chakrabarti 2015; 2013), focuses on exploring a limited set
of seed nodes in each iteration in a beam search like manner.
It processes the stream of candidate plans much faster than
the previous approach by focusing only on a select number
of most promising plan sets to begin with. The combinations
which this technique ignores while processing the candidate
plan stream are kept track of separately for processing at the
end, which helps in guaranteeing the completeness. Algorithm 2 presents the proposed method MCER for faster processing of the stream of candidate plans. It takes as input,
a plan from the stream of candidate plans and its sequence
number for reasons that will become clear shortly, and the
inputs for determining a diverse set similar to the previous
approach, and the number of seed plan sets to be explored
upon finding a new candidate plan.The plan-combinations
space can be visualized as a set enumeration tree (Rymon
1992), where new branches come at all levels (> 0) dynamically as the stream of candidate plans is processed.
MCER maintains a global set of valid plan sets which
have been produced until now, G, that could be further expanded with new candidate plans. It expands n most promising nodes from this set, which are populated into Children.
If a diverse set of size K is found, it terminates returning the
set. Otherwise, n most promising plan sets from Children
are expanded, and then their n most promising children and
so on until there are no further children to be expanded. It
should also be mentioned at this point, as to what we mean
by ‚Äòmost promising‚Äô, which would be based on f -value of a
plan set (the largest being most promising), and f -value is
in-turn computed as g + h where g is the size of the plan
set and h is the heuristic estimate denoting potential number of plans that can be added to this plan set. In this paper, we have explored using three heuristics: (i) 0, (ii) dispersion of the current set (arithmetic mean of all pair-wise
distances (Myers and Lee 1999)) divided by min dist, and

(iii) quadratic mean of all distances divided by min dist.
The idea behind these heuristics is that the more dispersion
the sets have the more accommodative they could be of new
candidate plans. However, in our experiments, we did not
observe gains of using the dispersion based heuristics in our
experiments compared to the trivial heuristic possibly due
to the limitation of the said heuristics in accounting for the
actions that are not part of the plans found yet, at any given
moment during the runtime. More distance metric based and
domain based estimates can be explored here in future.
Algorithm 3 presents the pseudo-code of ExpandMostPromising routine. It expands the n most promising nodes
from the given list (either G or Open) and puts them in
Children. One significant difference to note here is that,
since all the candidate plans are not available apriori, one
must add the expanded nodes back to G for future consideration with newer candidate plans. While doing so, in order
to avoid repetition, we keep track of the last child generation attempt through the sequence number of candidate plan
considered.
Finally, after the entire stream of candidate solutions has
been processed one by one using MCER, if a diverse set of
size K is not found, we continue to call MCER repeatedly
(this time, without adding back the explored plan sets into G)
until it finds a K-set or terminates exhausting the exploration
of all possible combinations.
Below, we present some of the properties of the proposed
method MCER:
Lemma 4 MCER does not generate the same combination
of plans more than once.
Proof outline:

This is ensured by keeping track of the

Algorithm 3 ExpandMostPromising
1: INPUT :: A set of valid plan sets S to expand, Children,
a distance measure dist(), the minimum distance needed between any two plans of a set min dist, the size of the diverse
plan set K, and the number of plan sets to be explored n.
2: OUTPUT :: Populates Children with new valid plan sets,
returns a diverse plan set of size K if found.
3: T emp ‚Üê œÜ (empty set);
4: for n times do
5:
P ‚Üê most promising plan set from S;
6:
for each candidate plan in the stream from sequence number
i = Pexp + 1 to the latest do
7:
if dist(p, l) > min dist ‚àÄl ‚àà P then
0
8:
P 0 ‚Üê P + {p}; Pexp
‚Üê i;
9:
if |P 0 | = K then
10:
return P 0 ;
11:
end if
12:
Children ‚Üê Children‚à™P 0 ;
13:
end if
14:
Pexp ‚Üê i;
15:
end for
16:
T emp ‚Üê T emp‚à™P ;
17: end for
18: G ‚Üê G‚à™T emp;

sequence number of candidate plan from the last child
generation attempt while expanding a plan set P via Pexp ,
which increases by 1 at each step during expansion (see
Line 14 in Algorithm 3) and the child generation attempts
start from Pexp + 1 every time (see Line 6 in Algorithm 3),
thereby avoiding repetition.
2
Lemma 5 MCER expands at-most n √ó (K ‚àí 1) + 1 number
of plan sets in each execution.
Proof outline: Note that, after expansion of n most promising nodes from G, their n most promising children, and then
their n most promising children and so on are expanded,
until a child of size K is found. Further, size of the children
at each step increases by 1 since a new candidate plan gets
added to the plan set. Therefore, even if we assume that the
initial set of seed nodes are all of size 1, MCER executes
at-most K steps at which point a diverse set of size K will
be found if possible through that set. And at each step,
at-most n number of children are expanded, with only 1 at
level K. Hence, together, at-most n √ó (K ‚àí 1) + 1 number
of plan sets are expanded in each execution of MCER. 2
Lemma 6 MCER guarantees finding a diverse set of plans
of size K if there exists one.
Proof outline: Note that, while we execute MCER several
times with incoming plans from the stream of candidate
plans, each time without exhausting all possible combinations, we keep track of the last expansion attempt for
each node (plan set), and store them in G. Hence all plan
sets which may not have been exhaustively explored with
the candidate plans are present in G when the entire set
of candidate plans has been generated. These plan sets are
then exhaustively explored without re-inserting back in to G

thereby guaranteeing completeness and termination.

2

Now, given a planning problem, a cost metric, a distance
measure, max cost, min dist, and K, for finding a set of
K cost sensitive diverse plans, one could use any one of the
following four combinations: 1) DFA with ACER, wherein
the DFA is executed and whenever a valid plan with cost
< max cost is found, ACER is invoked to find a diverse
set, and then the execution of DFA is continued if a diverse
set with the given requirements is not found, and the process is repeated until termination. We call this combination
DFAA. 2) DFA with MCER, similar to the above strategy of
invoking MCER whenever DFA find a valid cost sensitive
plan, followed by repeated calls to MCER at the end to explore all the remaining plan combinations until termination.
This is denoted by DFAM. 3) A*A with ACER (denoted
by A*AA), and 4) A*A with MCER (denoted by A*AM).
Next section presents the comparison of performances of the
above combinations of methods.

Experimental Results
In this section, we present the experimental results comparing the performances of various proposed algorithms among
themselves as well as with a greedy approach proposed in
the literature. We have implemented all our methods on
top of the Fast Downward planning environment (Helmert
2006), and hence could run problem instances from any of
the supported planning domains. Accordingly, we have conducted experiments on several domains, including, blocks,
rovers, pathways-noneg, airport, driverlog, tpp, zenotravel.
We present the representative results in this paper. All the experiments have been performed on a machine with Intel(R)
Xeon(R) CPU E5-1620 v2 at 3.70GHz and 64GB RAM. The
following distance measure for measuring diversity has been
adopted from (Nguyen et al. 2012):
dist(p1 , p2 ) = 1 ‚àí

A(p1 )‚à©A(p2 )
A(p1 )‚à™A(p2 )

(2)

where A(p) denotes the set of all actions in plan p.
Table 1 shows the comparison of DFAA and A*AA methods on problems (denoted by P.no.) from Blocks domain.
We have used the LM cut heuristic which is admissible, to
guide the search. The algorithms are given a maximum time
of 60sec for solving each problem. Given a set of inputs,
the output shows whether a diverse set of plans of size K is
found (otherwise the size of the largest diverse set found in
parenthesis), the time taken, and the number of plans generated during the process. * denotes that the algorithm stopped
due to the time limit. We see that the DFA based method
generates the cost sensitive plans faster than the A*A based
method in this case, resulting in the processing of more number of plans in a given time.
The difference in the number of plans generated to find
a diverse set of same size highlights the importance of the
order in which the candidate plans are generated. Depending on the order of the plans generated, the number of plans
required to be processed by an exhaustive algorithm to produce a diverse set of specific size varies. As mentioned before, this could be influenced by devising an appropriate distance metric and domain dependent feedback mechanism.

Table 1: Comparison of DFAA and A*AA complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

K

max cost

4
8

20

8

30

8

30

DFAA
min dist
0.6
0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (4)
No* (3)

A*AA

Time (Sec.)

Plans gentd.

0.00
0.00
32.90
2.14
60.00
60.00

22
43
231
323
2567
5140

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
11.06
3.42
60.00
60.00

26
43
130
323
923
4115

Table 2: Comparison of DFAM and A*AM complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAM

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (2)

A*AM

Time (Sec.)

Plans gentd.

0.00
0.00
0.58
1.74
60.00
60.00

26
43
323
323
11680
11908

K-set
found?
Yes
No (4)
Yes
No (5)
No* (7)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
2.62
60.00
60.00

26
43
172
323
12226
12311

Table 3: Comparison of DFA based and A*A based greedy algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAG

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
No (3)
No (3)
No (6)
No (4)
No (6)
No (2)

A*AG

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
0.02
16.70
24.24

43
43
323
323
104712
104712

Table 2 presents the comparison of DFAM and A*AM
methods (with plan-combinations seed set size equal to 30
in each execution). Note that, both algorithms guarantee to
find a diverse set of required size if there exists one, given
they are given enough time to terminate. Our objective with
the MCER based methods is to quickly process the incoming
candidate plans so as to find a diverse set quicker, postponing the exhaustive exploration to the end. Accordingly, we
see that both the methods perform much better than they did
compared to Table 1 by processing larger number of plans.
They are able to find larger sized sets of diverse plans in the
given time than before, although, as one can observe it may
also happen that the select exploration could occasionally
(see DFAM vs DFAA in last rows of both the tables) delay
finding large sets compared to ACER based approaches.
Next, we present the results obtained by integrating the
greedy approach discussed in the Introduction with DFA and
A*A, in Table 3. We call these methods DFAG and A*AG
respectively. As one can observe, while the greedy methods
process the incoming candidate plans very fast, they terminate without finding a diverse set of given size even when
there exists one. Furthermore, even for finding the diverse
sets that they produced, they involve generating far more
number of plans compared to the complete algorithms. This
can be a crucial element when finding multiple plans is difficult for a domain. Also, note that, A*A runs out of memory (4GB per instance) in this case which makes the case

K-set
found?
No (3)
No (3)
No (7)
No (3)
No* (6)
No* (2)

Time (Sec.)

Plans gentd.

0.00
0.00
0.06
0.04
Mem-limit
Mem-limit

43
43
323
323
101908
101908

for memory bounded methods while attempting to generate multiple solutions. Although, one can improve the performance of the greedy approach through feedback mechanisms, considering only one seed plan set for exploration is
likely to continue to affect the performance. Thus, it would
be beneficial to have multiple seed plan sets to be explored
at each stage for better performance.
Now, we present the results obtained on two other domains, namely, Rovers and Zeno-Travel. We show the results with DFA as the base method (for generating all costbounded solutions) in these cases since A*A based methods
were quickly reaching the memory limit on these instances.
Table 4 shows the comparison of DFAA, DFAM and DFAG
methods on a problem from the Rovers domain with 14 objects. Here, a diverse set of size 8 with cost bound 20 is to
be found within 60 seconds. Three sets of results comparing the above three algorithms are presented with different
diversity criterion in each case. Note that, amongst the three
methods, DFAA spends the most amount of effort on exploring plan combinations (exhaustive) whenever a new plan is
found, therefore is only able to generate and process a small
number of plans. Since DFAG spends least amount of effort on exploring plan combinations (greedy) upon finding
a new plan, it is able to generate and scan through a large
number of plans. Whereas DFAM distributes its effort intelligently across plan generation and plan combination exploration, by adjusting the number of seeds n as per domain and

Table 4: Comparison of DFAA, DFAM and DFAG methods. Domain: Rovers. Problem: roverprob4213 (14 objects), K: 8,
max cost : 20, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
Yes
No* (6)
No* (4)

DFAM

Time (Sec.)

Plans gentd.

0.88
60.00
60.00

64
372
1048

K-set
found?
Yes
Yes
No* (6)

DFAG

Time (Sec.)

Plans gentd.

0.00
9.84
60.00

65
306061
2047871

K-set
found?
Yes
Yes
No* (4)

Time (Sec.)

Plans gentd.

0.00
1.22
60.00

64
304553
15988265

Table 5: Comparison of DFAA, DFAM and DFAG methods. Domain: Zeno-Travel. Problem: ZTRAVEL-2-5 (17 objects), K: 8,
max cost : 15, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
No* (6)
No* (4)
No* (3)

DFAM

Time (Sec.)

Plans gentd.

60.00
60.00
60.00

294
579
1171

K-set
found?
Yes
Yes
Yes

problem size (in this case, n = 30). Note that, MCER with
n = 1 would result in an exploration similar to that of the
greedy method, with the exception of going further and guaranteeing completeness. And, MCER with n = ‚àû would result in an exploration similar to that of ACER. Results show
that DFAA performs poorly on large instances due to its exhaustive exploration. Between DFAM and DFAG, on easier
problems (with low diversity/distance requirement; first 2 instances), greedy and MCER based methods fare similarly,
with the greedy method slightly outperforming the MCER
based method (note that, one can change the n value to 1
here, to make MCER deliver results similar to that of the
greedy method, however, this is not beneficial in general).
On the other hand, when the diversity required is higher
(third instance), MCER based method outperforms greedy
approach by finding a larger diverse set using only 12.81%
of the plans generated by that of the greedy method, showcasing the advantage of exploring plan combinations more
thoroughly.
Table 5 presents the comparison of DFAA, DFAM and
DFAG methods on a problem from the Zeno-Travel domain
with 17 objects. Once again, we observe similar results as
that of the previous two domains. ACER based method fares
poorly due to its exhaustive nature which limits its reach
in scanning through the full space plans in the given time.
And, between DFAM and DFAG, while DFAG may find
the K-set in shorter time in some cases, DFAM continues
to leverage the advantage of exploring plan combinations
thoroughly and is able to find required K-sets using lesser
number of plans. This is a crucial element in working with
domains where producing individual plans itself is very difficult, which is especially the case when large problem sizes
are involved.
Before we conclude, we present a note on solving large
sized problems. In such cases, while completeness may not
be a practical expectation, one should be able to gain performance over using the greedy approach by carefully integrating the proposed MCER approach with the state-of-the-art
solvers/planners, feedback mechanisms, and using efficient
heuristics for exploring the space of plan combinations. Fur-

DFAG

Time (Sec.)

Plans gentd.

0.18
0.62
21.46

1447
7884
505186

K-set
found?
Yes
Yes
Yes

Time (Sec.)

Plans gentd.

0.20
0.44
10.30

1447
8545
619579

thermore, the proposed methods can be easily extended to
solve related problems such as, finding a K-set with maximum diversity, finding largest K-set with a given diversity,
finding high quality (in terms of the cost of the plans) K-sets
with given diversity, and a combination thereof involving the
generation of multi-objective pareto fronts.

Conclusion
In this paper, we take a combinatorial search perspective of
the widely studied diverse solution generation problem. We
observe that many of the approaches proposed in various
domains such as automated planning and constraint satisfaction use a linear-time greedy method for exploring plan
set combinations, that makes them fail while searching for
a diverse set of required size even when there exists one.
We propose a bi-level optimization framework and methods
to find cost-sensitive diverse solutions which guarantee to
find a diverse set of required size whenever there exists one.
We identify the critical elements that affect the performance
in such scenarios and propose efficient methods to handle
them. We showcased the efficacy of the proposed methods
by implementing our methods as part of the Fast Downward
planning system and comparing with the existing greedy approach across various domains. The proposed methods have
found larger sets of diverse solutions compared to the greedy
approach on almost all problem instances, within the same
time bound, proving their utility.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
Alexiadis, A., and Refanidis, I. 2013. Generating alternative plans
for scheduling personal activities. In Proceedings of the Seventh
Scheduling and Planning Applications workshop, 35‚Äì40.
Bloem, M., and Bambos, N. 2014. Air traffic control area configuration advisories from near-optimal distinct paths. Journal of
Aerospace Information Systems 11(11):764‚Äì784.

Bloem, M. J. 2015. Optimization and Analytics for Air Traffic
Management. Ph.D. Dissertation, Stanford University.
Boddy, M. S.; Gohde, J.; Haigh, T.; and Harp, S. A. 2005. Course
of action generation for cyber security using classical planning.
In Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005), June 5-10 2005,
Monterey, California, USA, 12‚Äì21.
Bryce, D. 2014. Landmark-based plan distance measures for diverse planning. In Proceedings of the Twenty-Fourth International
Conference on Automated Planning and Scheduling, ICAPS 2014,
Portsmouth, New Hampshire, USA, June 21-26, 2014.
Bylander, T. 1991. Complexity results for planning. In Proceedings
of the 12th International Joint Conference on Artificial Intelligence
- Volume 1, IJCAI‚Äô91, 274‚Äì279. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc.
Coman, A., and MunÃÉoz-Avila, H. 2011. Generating diverse plans
using quantitative and qualitative plan distance metrics. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11,
2011.
Coman, A., and MunÃÉoz-Avila, H. 2012a. Diverse plan generation
by plan adaptation and by first-principles planning: A comparative
study. In Case-Based Reasoning Research and Development - 20th
International Conference, ICCBR 2012, Lyon, France, September
3-6, 2012. Proceedings, 32‚Äì46.
Coman, A., and MunÃÉoz-Avila, H. 2012b. Plan-based character
diversity. In Proceedings of the Eighth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-12,
Stanford, California, October 8-12, 2012.
Coman, A., and MunÃÉoz-Avila, H. 2013. Automated generation of
diverse npc-controlling fsms using nondeterministic planning techniques. In Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-13,
Boston, Massachusetts, USA, October 14-18, 2013.
Coman, A. 2012. Solution diversity in planning. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Dechter, R.; Flerova, N.; and Marinescu, R. 2012. Search algorithms for m best solutions for graphical models. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Do, M. B., and Kambhampati, S. 2001. Planning as constraint
satisfaction: Solving the planning graph by compiling it into CSP.
Artif. Intell. 132(2):151‚Äì182.
Eiter, T.; Erdem, E.; Erdogan, H.; and Fink, M. 2013. Finding
similar/diverse solutions in answer set programming. Theory and
Practice of Logic Programming 13(03):303‚Äì359.
Gerevini, A.; Saetti, A.; and Serina, I. 2003. Planning through
stochastic local search and temporal action graphs in lpg. J. Artif.
Int. Res. 20(1):239‚Äì290.
Goldman, R. P., and Kuter, U. 2015. Measuring plan diversity:
Pathologies in existing approaches and A new plan distance metric. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., 3275‚Äì
3282.
Hebrard, E.; Hnich, B.; O‚ÄôSullivan, B.; and Walsh, T. 2005. Finding diverse and similar solutions in constraint programming. In
Proceedings of the 20th National Conference on Artificial Intelligence - Volume 1, AAAI‚Äô05, 372‚Äì377. AAAI Press.
Hebrard, E.; O‚ÄôSullivan, B.; and Walsh, T. 2007. Distance constraints in constraint satisfaction. In Proceedings of the 20th In-

ternational Joint Conference on Artifical Intelligence, IJCAI‚Äô07,
106‚Äì111. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc.
Helmert, M. 2006. The fast downward planning system. J. Artif.
Intell. Res. (JAIR) 26:191‚Äì246.
Kambhampati, S. 2007. Model-lite planning for the web age
masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, July 22-26, 2007, Vancouver,
British Columbia, Canada, 1601‚Äì1605.
Lawler, E. L., and Wood, D. E. 1966. Branch-and-bound methods:
A survey. Operations Research 14(4):699‚Äì719.
Myers, K. L., and Lee, T. J. 1999. Generating qualitatively different
plans through metatheoretic biases. In AAAI, 570‚Äì576. American
Association for Artificial Intelligence.
Nadel, A. 2011. Generating diverse solutions in sat. In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing, SAT‚Äô11, 287‚Äì301. Berlin, Heidelberg:
Springer-Verlag.
Nguyen, T. A.; Do, M. B.; Gerevini, A.; Serina, I.; Srivastava, B.;
and Kambhampati, S. 2012. Generating diverse plans to handle
unknown and partially known user preferences. Artif. Intell. 190:1‚Äì
31.
Petit, T., and Trapp, A. C. 2015. Finding diverse solutions of
high quality to constraint optimization problems. In IJCAI. International Joint Conference on Artificial Intelligence.
Roberts, M.; Howe, A.; Ray, I.; and Urbanska, M. 2012. Using
planning for a personalized security agent. In AAAI Workshops.
Roberts, M.; Howe, A. E.; and Ray, I. 2014. Evaluating diversity
in classical planning. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Russell, S., and Norvig, P. 1995. Artificial intelligence: a modern
approach. Prentice Hall.
Rymon, R. 1992. Search through systematic set enumeration. Technical Reports (CIS) 297.
Scala, E. 2014. Plan repair for resource constrained tasks via numeric macro actions. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Srivastava, B.; Nguyen, T. A.; Gerevini, A.; Kambhampati, S.; Do,
M. B.; and Serina, I. 2007. Domain independent approaches for
finding diverse plans. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad,
India, January 6-12, 2007, 2016‚Äì2022.
Vadlamudi, S. G.; Aine, S.; and Chakrabarti, P. P. 2013. Anytime
pack heuristic search. In Pattern Recognition and Machine Intelligence - 5th International Conference, PReMI 2013, Kolkata, India,
December 10-14, 2013. Proceedings, 628‚Äì634.
Vadlamudi, S.; Aine, S.; and Chakrabarti, P. 2015. Anytime pack
search. Natural Computing 1‚Äì20.
Vadlamudi, S. G.; Chakrabarti, P. P.; and Sarkar, S. 2012. Anytime
algorithms for mining groups with maximum coverage. In Tenth
Australasian Data Mining Conference, AusDM 2012, Sydney, Australia, December 5-7, 2012, 209‚Äì220.
Voss, C.; Moll, M.; and Kavraki, L. E. 2015. A heuristic approach
to finding diverse short paths. In IEEE International Conference
on Robotics and Automation, ICRA 2015, Seattle, WA, USA, 26-30
May, 2015, 4173‚Äì4179.

Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote Human-Robot Interaction
Vignesh Narayanan
Dept. of Computer Science Arizona State University Tempe, AZ

Yu Zhang
Dept. of Computer Science Arizona State University Tempe, AZ

Nathaniel Mendoza
Dept. of Computer Science Arizona State University Tempe, AZ

vnaray15@asu.edu

yzhan442@asu.edu Subbarao Kambhampati
Dept. of Computer Science Arizona State University Tempe, AZ

namendoz@asu.edu

rao@asu.edu ABSTRACT
Human factor studies on remote human-robot interaction are often restricted to various forms of supervision, in which the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this study, we investigate the incorporation of a general planning capability into the robot to facilitate peer-to-peer human-robot teaming, in which the human and robot are viewed as teammates that are physically separated. One intriguing question is to what extent humans may feel uncomfortable at such robot autonomy and lose situation awareness, which can potentially reduce teaming performance. Our results suggest that peer-to-peer teaming is preferred by humans and leads to better performance. Furthermore, our results show that peer-to-peer teaming reduces cognitive loads from objective measures (even though subjects did not report this in their subjective evaluations), and it does not reduce situation awareness for short-term tasks. (or sub-goals) for the robot to handle. In peer-to-peer (P2P) teaming, the human and robot share the same global goal and collaborate to achieve it. While increasing robot autonomy is generally viewed as desirable, humans may feel uncomfortable at the loss of control entailed by such autonomy in P2P teaming, which can potentially reduce situation awareness for humans and affect teaming performance. In this study,1 we mainly concentrate on remote interaction and perform our investigation for emergency response in an urban search and rescue (USAR) task. Here, the task cannot be fully specified a priori due to incomplete information about the models, goals or settings in such scenarios. Furthermore, information can be continuously changing throughout the task and may not always be synchronized between the human and robot (e.g., goal updates and human preference models) due to communication or interpretation delays. The goal of this USAR task is to explore areas of the disaster scene to provide real-time information, which is then used to aid the management team to create rescue plans (e.g., identifying locations of casualties). The aim of our study is to compare P2P and supervised teaming in terms of objective measures as well as subjective measures such as situation awareness and mental workload. Related Work: In most previous works on human-robot and human-machine interactions (e.g., [1]) for teaming, the human always plays a supervisor role. While there are works that incorporate general planning capabilities into robots to achieve P2P teaming (e.g., [3]), as yet, there exists no empirical investigation of its influence on the teaming performance. In this study, we implement P2P teaming in which the human and robot are viewed as teammates, and the robot exhibits autonomy through automated planning capabilities. In terms of automation in human-robot interaction, it is well known that it can have both positive and negative effects on human performance [2].

Categories and Subject Descriptors
H.1.2 [Human factors]; I.2.8 [Plan execution, formation, and generation]; J.7 [Command and control]

Keywords
Robot design principles; Autonomous robot capabilities; User study/Evaluation; Teamwork & group dynamics

1.

INTRODUCTION

In supervised human-robot teaming, the human creates the plan to achieve the global goal, and then either directly provides motion commands or breaks the plan into sub-plans

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). HRI'15 Extended Abstracts, March 2≠5, 2015, Portland, OR, USA. ACM 978-1-4503-3318-4/15/03. http://dx.doi.org/10.1145/2701973.2702042.

2.

STUDY DESIGN

Fig. 1 presents the simulated environment and humanrobot interface used in our USAR task, which represents the floor plan of an office building before a disaster occurs (e.g.,
1

A longer version at rakaposhi.eas.asu.edu/hri15-long.html.

Figure 1: Environment (left) and interface (right) used in the USAR task with a simulated Nao robot.

Figure 3: Results for subjective measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001. within 20 minutes and the secondary task performance (left part of Fig. 2). In fact, performance for the primary task in supervised teaming was no better than the robot with a planning capability executing the task alone (i.e., P2P-NI). Results for subjective measures as assessed by the survey questionnaire are presented in Fig. 3. Our analysis on mental workload did not show any significant difference due to the fact that most humans preferred to rely on themselves at the beginning even in P2P teaming, which might be a result of the lack of trust in the robot for handling the task initially. This effect can be seen from the right part of Fig. 2, which shows how many robot recommended actions were followed by the human subjects in P2P teaming. Even though the workload was seen to be almost the same based on subjective measures (Fig. 3), the objective measures suggest that the cognitive load was indeed reduced (based on the time spent on the secondary task (left part of Fig. 2)). Our analysis on situational awareness did not show any significant difference either. This might be partially due to the fact that the recommended action of the robot from the planning capability during execution provided situation awareness to the human subject, since the same action would likely be chosen by the subject in the same situation. This is interesting since it suggests that humans in P2P teaming can maintain situation awareness, at least for short-term tasks such as our USAR task. Our analysis on likability, improvability, and complacency showed that the subjects generally preferred and felt more satisfied working with the robot in P2P teaming. Our conclusions from this preliminary study are that humans prefer working with robots with a planning capability for P2P teaming, and the planning capability helps reduce cognitive load and maintain situation awareness for short-term tasks. Acknowledgments: This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR grants N00014-13-1-0176 and N00014-13-1-0519.

Figure 2: Results for objective measures.

a fire). The study was performed over 4 weeks and involved 19 volunteers. Each subject took part in one experimental trial of either P2P or supervised teaming. Both the human subject and robot had access to the floor plan before the disaster. The robot in P2P teaming could use the planning capability (based on SapaReplan [3]) to independently create its own plan based on the floor plan and current status of the task. In both teaming scenarios, the robot displayed a list of applicable actions that it could perform given the current state. The interaction interfaces were the same except that the robot in P2P teaming also recommended the next action (from along the applicable ones) in its plan. The global goal was to report the number of casualties in as many rooms as possible in 20 minutes. The incomplete task information was assumed to be a result of blocked doors. We provided the information regarding which doors might be blocked to the human subject, but only after the task had run for 1 minute to simulate dynamic information. This information, however, remained unknown to the robot. The human subject could interact with the robot at specific times to reduce the influence of this information asymmetry, or the robot had to learn this information by pushing the door and failing (which could reduce the teaming performance). In a real USAR task, the human would also have other information to process and analyze. To simulate this, the human subject was also assigned to a secondary task. This secondary task involved solving three-dimensional spatial visualization puzzles. The performance of the team was evaluated on both the primary and secondary tasks. Each trial ended when the given time elapsed. Finally, the human subject completed a questionnaire (in Likert scale) that included questions for evaluating situation awareness, mental workload and other subjective human-robot interaction aspects.

4.

REFERENCES

3.

RESULTS AND CONCLUSIONS

The results for objective measures are presented in Fig. 2. Overall, subjects in P2P teaming outperformed subjects in supervised teaming in terms of the number of rooms visited

[1] J. Casper and R. Murphy. Human-robot interactions during the robot-assisted urban search and rescue response at the world trade center. IEEE Trans. on SMC Part B, 33(3):367≠385, June 2003. [2] R. Parasuraman. Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7):931≠951, 2000. PMID: 10929828. [3] K. Talamadupula, J. Benton, S. Kambhampati, P. Schermerhorn, and M. Scheutz. Planning for human-robot teaming in open worlds. ACM Trans. Intell. Syst. Technol., 1(2):14:1≠14:24, Dec. 2010.

Proactive Decision Support using Automated Planning
Satya Gautam Vadlamudi, Tathagata Chakraborti, Yu Zhang, Subbarao Kambhampati
{gautam,tchakra2,Yu.Zhang.442,rao}@asu.edu, Arizona State University, Tempe, AZ
Proactive decision support (PDS) helps in improving the decision making experience of human decision
makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of
making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the
research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her
goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource
constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that
must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach
also supports naturalistic decision making which is being acknowledged as a necessary element in
proactive decision support, since it only aids the human decision maker through suggestions and alerts
rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework
through search-and-rescue examples in a fire-fighting domain.
Human-in-the-loop planning (HILP) is a necessary
requirement today in many complex decision making or
planning environments. In this paper we consider the case of
HILP where the human(s) responsible for making the
decisions in complex scenarios are supported by automated
planning systems. Thus the planners in this scenario are the
humans themselves, and we investigate the role of an
automated planner in their deliberative process. This is, in
effect, a role reversal of the traditional notion of the humanplanner interaction in mixed initiative planning; and we refer
to the proposed system as a reverse mixed initiative planner.
These systems are capable of providing plans or course-ofactions (COAs) when a model of the world where the plans
are to be executed is given to them, along with the knowledge
of the initial state and the list of goals to be achieved/tasks to
be accomplished. Examples where such technologies can be
helpful include disaster response strategies from the navy, or
responses to fire or emergency from local law enforcement.
Providing a complete model of the world where the plans
are to be executed is, however, known to be very difficult
(Kambhampati, 2007). This implies that the system generated
plan cannot be completely relied upon. Not only executing
such plans may no longer accomplish the goals/tasks provided,
but also their execution may result in undesired consequences.
This calls for active participation from the human in the loop
rather than simply adopt a system generated plan.
Furthermore, in many cases, the human in the loop may be
held responsible for the plan under execution and its results.
Therefore, it is also necessary in such cases that the human
keeps control of the plan/COA being given for execution. This
motivates us to build a proactive decision support system,
which is context-sensitive and focuses on aiding and alerting
the human in the loop with his/her decisions rather than
generate a static COA that may not work in the dynamic
worlds that the plan has to execute in.
In this paper, we propose a proactive decision support
(PDS) system, named RADAR, using automated planning
technology, which is augmentable, context sensitive,
controllable and adaptive to the human‚Äôs decisions. It supports
the human in the loop through suggestions and alerts, which
can be considered by the human as he/she sees fit.
In the following we explain the meaning of the above terms:

‚Ä¢

‚Ä¢

‚Ä¢

‚Ä¢

Augmentable: The model of the world such as the
rules that specify what are the preconditions for a
particular action/decision and what would be the
effects
of
that
action/decision
could
be
added/modified. The state of the world such as the
values of various variables and availabilities of
various resources could be updated. The list of
goals/tasks can also be updated.
Context-sensitive: Whenever the model or the state
of the world is augmented either by the human or by
some other source internal/external to the PDS
system, the system takes the new context into account
and responds with updated suggestions and alerts.
Controllable: The decision making process of the
PDS system is completely controllable by the human
in the loop, who retains with the decision making
power wherein he/she can either choose to follow the
system generated suggestions or make a different
decision as they see fit.
Adaptive: Since the decision making power lies with
the human in the loop, the PDS system has to adapt
itself to the decisions being made by the human and
provide new suggestions and alerts that are relevant
based on the decisions of the humans. Note that,
adaptive nature may also be viewed as part of
context-sensitivity in the sense that the context
changes whenever decisions are made. In this paper,
we keep the distinction to differentiate the changes in
the world and tasks, and the changes related to the
actions prescribed/decisions of the human in the loop.

As mentioned before, our proactive decision support system
uses automated planning technology widely studied in the
field of Artificial Intelligence. In particular, we adopted the
Planning Domain Description Language (PDDL) to describe
the model of the world, details of the current state (context)
and the goals, to the system. Then, we used the existing
landmark generation method (Hoffmann et al., 2004) to
generate landmarks, which are then analyzed to come up with
relevant suggestions and alerts. Landmarks are those set of
states (of the world) that has to be visited by any successful
plan that achieves goals from the current state.

We implemented our proposed system using the Fast
Downward planner (Helmert, 2006) and tested it on a firefighting domain, where search-and-rescue missions are to be
carried out (goals). We also conducted some preliminary
human factor studies to evaluate the utility of the above
proposal, which gave positive feedback.
RELATED WORK
The proposed proactive decision support system supports
naturalistic decision making (Zsambok and Klein, 2014;
Klein, 2008), which is acknowledged as a necessary element
in PDS systems (Morrison et al., 2013). Systems which do not
support naturalistic decision making have been found to have
detrimental impact on work flow causing frustration to
decision makers (Feigh et al., 2007).
In (Parasuraman, 2000), a study of human performance
consequences of different levels and types of automation is
provided, where aspects such as, mental workload and
situation awareness are considered as evaluative criteria. A
model for types and levels of automation that provides an
objective basis for deciding which system functions should be
automated and to what extent is given in (Parasuraman et al.,
2000). (Parasuraman and Manzey, 2010) shows that human
use of automation may result in automation bias leading to
omission and commission errors, which underlines the
importance of reliability of the automation (Parasuraman and
Riley, 1997). Various elements of human-automation
interaction such as, adaptive nature and context sensitivity are
presented in (Sheridan and Parasuraman, 2005). (Warm et al.,
2008) show that vigilance requires hard mental work and is
stressful via converging evidence from behavioral, neural and
subjective measures. Our system could be considered as a part
of such vigilance support thereby reducing the stress for
human in the loop.
High-level information fusion that characterizes complex
situations and that support planning of effective responses is
considered the greatest need in crisis-response situations
(Laskey et al., 2016). Automated planning based proactive
support systems were shown to be preferred by humans in
studies involving human-robot teaming (Zhang et al., 2015)
and the cognitive load of the subjects involved was observed
to have been reduced (Narayanan et al., 2015).
PROPOSED PROACTIVE DECISION SUPPORT
SYSTEM: RADAR
Now, we present the proposed proactive decision support
(PDS) system ‚Äì RADAR, based on automated planning
technology. First, we present the different elements of the PDS
system and then briefly present the details of the methodology
behind the generation of suggestions and alerts. The various
elements of the planning PDS system are:
‚Ä¢ Tasks/goals: The tasks or goals to be accomplished
clearly form an important and necessary element.
‚Ä¢ State/context, resources: The current state or context
is needed for the PDS system to produce relevant
alerts. The availability of resources also forms part of
the context which we separately indicated to display
as an important constituent of the Context.

Model, actions/decisions: The model consists of set
of rules which are applicable in the world where the
plan is being executed. Actions, for example, are part
of a model, which give information about when a
particular action is applicable (what are the preconditions to be satisfied in order for it to be
applicable), and what would be the effects of taking
that action (how it would impact various elements of
the world). Actions are also closely tied to decisions
that need to be made since each decision typically
corresponds to certain action being taken.
‚Ä¢ Current plan/course of action (COA): The
information about current plan of the human in the
loop, if any, can help the PDS system produce better
suggestions and alerts by reducing the uncertainty.
However, this could just consist of actions already
taken, in which case the proposed PDS system can
come up with relevant alerts pertaining to the future.
More details on this are given next.
Now, we briefly present details on how the proposed
planning based system with above elements produces relevant
suggestions and alerts. In order to explain this, first we need to
define what are called Landmarks, which are central to the
suggestions and alerts system.
‚Ä¢

Definition: Landmarks. (Hoffmann et al., 2004) A
state/partial state is a landmark (for the current state,
tasks/goals, and model) if all plans/course-of-actions that can
accomplish the tasks from the current state must go through
that state/partial state during their execution.
Note that, all goal states are trivial landmarks since they have
to be accomplished by all successful plans. Consider there
exists only one state, A, which can take one to the goal
state(s), meaning, all plans have to visit A in order to
accomplish the goals, making it a landmark (derived; nontrivial). Further, if there exist two states A and B through
which the goal state(s) must be reached, then either A or B
must be visited before accomplishing the tasks/goals. In such a
scenario, A or B can be called as a landmark, in particular, a
disjunctive landmark. Continuing this process, one can
derive recursively the set of all landmarks starting from the
goal state(s) leading back to the current state.
Generating Suggestions and Alerts (PDS)
Now, we present the details on how generating and analyzing
the landmarks can help in producing suggestions and alerts.
Note that, since the landmarks are the states that must be
visited in order to accomplish the goals, if there is no possible
way of reaching a certain landmark generated above, then the
system can generate an alert conveying that the goal cannot be
accomplished. This could be because there is no action
available, which would help in visiting the landmark under
consideration, or the preconditions of actions that can help
reaching the landmark are not satisfied.
In some cases, the preconditions, which are not currently
satisfied for an action to be applicable, may be because of
resource constraints. In such cases, the system instead
generates a suggestion mentioning that those resources are
needed in order to accomplish the task. For example, in order

Figure 1 ‚Äì RADAR interface showing data support and decision support for the human commanders making plans.
for an action such as put-off-fire to be applicable, a
precondition on the availability of the resource: fire-engine
need to be satisfied. Failing which an alert may be generated.
More details are described through a case study next.
Further, provision to update the model, state, and resources
is provided to make the system augmentable. In order to
support context-sensitivity and adaptive nature, we re-execute
the alerts generation method whenever there is a change in the
context/tasks or new action is executed or plan is changed, so
that the suggestions/alerts become relevant to the situation.
Case Study: Fire-fighting Domain
We use a fire-fighting scenario to illustrate the ideas expressed
so far, as shown in Figure 1. The scenario plays out in a
particular location (we use Tempe in the example) and
involves the local fire chief, police, medical and transport
authorities, who try to build a plan in response to the fire in
the given platform (which is augmented with decision support
capabilities from an automated planner). The left pane gives
event updates that the commanders can incorporate into the
Tasks panel at the top, which shows what high level goals or
tasks needs to be addressed. The panel on the right (currently
empty) will display the plan being constructed. Each of the
human commanders have access to the resources that they can
use to control the fire outbreak (as can be seen from the table
at the bottom of Figure 1). For example, the police can deploy
police cars and policemen, and the fire chief can deploy fire
engines, ladders, rescuers, etc.
The plans that can be produced by the commanders, of
course, depend on the availability of these resources, and
certain actions can only be executed when the required
number of resources are available or the preconditions are
satisfied. For example, in order to dispatch police cars from a
particular police station, the police chief needs to make sure

that the respective police station has enough police cars and it
has been notified of the demand previously.
Given this knowledge, the automated planner integrated
into the system keeps an eye on the planning process of the
human commanders. The three panels in the middle of Figure
1 provide these functionalities. The one on the left provides a
way to add or request for resources in case of insufficient
resources, while the one on the right provides suggested
actions that the commanders may use to complete their plans.
The panel in the middle is the most important part of the
automated component where it produces alerts or suggestions
to problems in the current plan or with respect to problems
that may appear in future given the current state and
availability of resources.
In the following we will go through two use cases to
illustrate how the system responds to situations as per the
guidelines we discussed in the introduction ‚Äì
Scenario I: (see Figure 2)
1) The scenario starts with a small fire in a building.
Once selected from suggested tasks, this populates
the task chosen to be addressed.
2) The planning model does landmark analysis on the
current state and immediately populates the alerts
panel with an alert saying either big fire engines or
small fire engines are needed to put out the fire (a
disjunctive landmark).
3) The commander tries to dispatch big engines now,
but is stopped by the system which detects that there
are not enough resources (big engines).
4) The commander addresses the shortage by requesting
additional engines from the left.
5) Now the commander can proceed with and finish the
plan until the fire has been extinguished.

Note that the above examples are illustrative and only
intended for understanding the underlying concepts of the
proposed system. The same system can handle scenarios
where hundreds of actions and variables are involved, where it
becomes nearly impossible for a human to account for all
possible drawbacks. Furthermore, any other domain (say,
disaster response) can be readily handled by the implemented
system, by just changing the PDDL domain file used by it
without any renewed effort.

Figure 2 ‚Äì Use case illustrating automated decision support
using disjunctive landmarks.
Scenario II: (see Figure 3)
1) The scenario now starts with a big fire. This calls for
big engines as alerted by the system. Note that it also
alerts for insufficient rescuers, which did not happen
in the previous case, as the model used conveys that
rescuers are needed only in case of a big fire.
2) The commander once again requests for additional
resources (big engines as well as rescuers) and was
able to generate a feasible plan as shown.
In this way the system is able to assist the human commander
in his planning process. The system is augmentable in the way
it supports the commander‚Äôs goal preferences and world state
information. It is context-sensitive in how it provides relevant
alerts based on the stage of the plan, and adaptive with respect
to the choice taken by the human in the loop to address these
alerts. Finally, the entire process is controllable because the
human commander has authority over the choices at all times.

Figure 3 ‚Äì Use case illustrating how automated decision
support adapts in response to a necessary landmark.
EVALUATIONS WITH HUMANS ON UTILITY
In order to assess the utility of the proposed proactive decision
support (PDS) system, RADAR, we have conducted a
preliminary survey of its usefulness on 7 subjects on the
questions given below. The system would be enhanced and
refined continuously as we incorporate more and more
features in our PDS system. We also plan to take the learning
from the user feedback back into the design of the PDS system
during the process. We discuss the responses obtained for each
of the questions, next to it.
1) Do you think the suggestions & alerts are relevant to the
task/goal? Yes/No

7 out of 7 subjects have answered it Yes. This suggests that
the domain shown (fire-fighting) has been modeled well so
that relevant landmarks could be generated.
2) Do you think the suggestions & alerts are context-sensitive
(current state & current plan)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the context sensitivity of the landmarks make a good fit for
use in PDS systems.
3) Do you think that the suggestions & alerts are dynamic
(change with changing context/plan)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the regeneration of suggestions & alerts whenever context
changes is notable to the users.
4) Do you think the suggestions & alerts increased your
situational awareness (e.g. resources available, status of
execution)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the suggestions and alerts continuously improve the situational
awareness of the human in the loop, particularly in terms of
the critical points relevant to the plan/COA.
5) Do you think that the suggestions & alerts interfere with the
ability to interact with the system? Yes/No
1 out of 7 subjects have answered it Yes. This suggests that
the alerts are displayed without interfering with the user
interaction experience in most cases, and may be improved.
6) Do you think the suggestions & alerts helped in debugging
the plan or would you rather execute and replan in case of
failure? Former/Latter
6 out of 7 subjects have answered that the suggestions and
alerts helped. This suggests that the users prefer to be alerted
in advance rather than re-plan in most of the cases.
7) Do you think the suggestions & alerts should be an error
and stop the plan from being dispatched, or should it be a
warning and let you proceed with execution? Former/Latter
4 out of 7 subjects have answered that the alerts can be shown
as errors and stop the execution whereas others preferred them
as warnings that allow execution to move forward. In this
case, there is a split amongst the users as to whether the user
should be able to proceed with warnings or be stopped
completely. Here there is an opportunity to amend the system
so as to learn the cases where one could proceed despite the
error/warning, and incorporate it as a soft constraint in the
future leading to only a warning.
8) Do you feel that you can be in control of the decision
making/planning process when using the proposed PDS
system? Yes/No
6 out of 7 subjects have answered it Yes. This suggests that
most of the users feel in control of the decision making
process rather than being forced by the system.
9) On a scale of 1-10 how much would you rate your
satisfaction with the proactive decision support capabilities of
the system? 1-10
This received an average score of 7.14. This suggests that the
users had a good first experience with the proposed PDS
system and would like to see new and improved features.
10) On a scale of 1-10 how much would you recommend using
the proposed proactive decision support platform? 1-10
This received an average score of 7.28. This suggests that the
users are open to recommending the PDS system to others.

Overall, the PDS system was well received and perceived to
be promising. Users have appreciated the current features and
suggested minor modifications. As part of the future work, we
consider addressing several important aspects such as: Where
do we get the models? Can we automatically learn them from
observing the users and the contexts? How do we deal with
incomplete models? Does the human in the loop deviate from
cost-optimal plans? How to understand the preferences of the
human in the loop and how to address them? And so on.
CONCLUSION
We presented a Proactive decision support (PDS) framework
called RADAR, based on the research in Automated Planning
in AI, that aids the human decision maker with her plan to
achieve her goals by providing alerts and suggestions on
potential drawbacks in the plan and resource constraints. This
was achieved by generating and analyzing the landmarks that
must be accomplished by any successful plan before achieving
the goals. The proposed approach is aligned with the concept
of naturalistic decision making. We demonstrated the utility of
the proposed framework through search-and-rescue examples
in a fire-fighting domain and human factors studies.
ACKNOWLEDGMENTS
This research is supported in part by the ONR grant N0001415-1-2027. We thank Vivek Dondeti for his help with the
implementation of parts of the RADAR system.
REFERENCES
Feigh, K. M., Pritchett, A. R., Denq, T. W., & Jacko, J. A. (2007). Contextual
Control Modes During an Airline Rescheduling Task. Journal of Cognitive
Engineering and Decision Making, 1(2), 169-185.
Helmert, M. (2006). The Fast Downward Planning System. J. Artif. Intell.
Res.(JAIR), 26, 191-246.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in
planning. J. Artif. Intell. Res.(JAIR), 22, 215-278.
Kambhampati, S. (2007, July). Model-lite planning for the web age masses:
The challenges of planning with incomplete and evolving domain models.
InProceedings of the National Conference on Artificial Intelligence (Vol.
22, No. 2, p. 1601). Menlo Park, CA; Cambridge, MA; London; AAAI
Press; MIT Press; 1999.
Klein, G. (2008). Naturalistic decision making. Human Factors: The Journal
of the Human Factors and Ergonomics Society, 50(3), 456-460.
Laskey, K. B., Marques, H. C., & da Costa, P. C. (2016). High-Level Fusion
for Crisis Response Planning. In Fusion Methodologies in Crisis
Management (pp. 257-285). Springer International Publishing.
Morrison J. G., Feigh K. M., Smallman H. S., Burns C. M., Moore K. E.
(2013). The Quest For Anticipatory Decision Support Systems (Panel).
Human Factors and Ergonomics Society Annual Meeting.
Narayanan, V., Zhang, Y., Mendoza, N., & Kambhampati, S. (2015, March).
Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote
Human-Robot Interaction. In HRI (Extended Abstracts) (pp. 161-162).
Parasuraman, R. (2000). Designing automation for human use: empirical
studies and quantitative models. Ergonomics, 43(7), 931-951.
Parasuraman, R., & Manzey, D. H. (2010). Complacency and bias in human
use of automation: An attentional integration. Human Factors: The Journal
of the Human Factors and Ergonomics Society, 52(3), 381-410.
Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse,
disuse, abuse. Human Factors: The Journal of the Human Factors and
Ergonomics Society, 39(2), 230-253.
Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types
and levels of human interaction with automation. Systems, Man and
Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 30(3),
286-297.
Sheridan, T. B., & Parasuraman, R. (2005). Human-automation
interaction.Reviews of human factors and ergonomics, 1(1), 89-129.

Warm, J. S., Parasuraman, R., & Matthews, G. (2008). Vigilance requires
hard mental work and is stressful. Human Factors: The Journal of the
Human Factors and Ergonomics Society, 50(3), 433-441.
Zhang, Y., Narayanan, V., Chakraborti, T., & Kambhampati, S. (2015,
September). A human factors analysis of proactive support in human-robot
teaming. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
International Conference on (pp. 3586-3593). IEEE.
Zsambok, C. E., & Klein, G. (2014). Naturalistic decision making.
Psychology Press.

1

BayesWipe: A Scalable Probabilistic Framework
for Cleaning BigData

arXiv:1506.08908v1 [cs.DB] 30 Jun 2015

Sushovan De, Yuheng Hu, Meduri Venkata Vamsikrishna, Yi Chen, and Subbarao Kambhampati
Abstract‚ÄîRecent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record
matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in
tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like CFDs
(which have to be provided by domain experts, or learned from a clean sample of the database). In this paper, we provide a method for
correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned
from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently
perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable.
We evaluate our methods over both synthetic and real data.
Index Terms‚Äîdatabases; web databases; data cleaning; query rewriting; uncertainty

F

1

I NTRODUCTION

A

LTHOUGH data cleaning has been a long standing
problem, it has become critically important again because of the increased interest in big data and web data.
Most of the focus of the work on big data has been on
the volume, velocity, or variety of the data; however, an
important part of making big data useful is to ensure the
veracity of the data. Enterprise data is known to have a
typical error rate of 1‚Äì5% [1] (error rates of up to 30% have
been observed). This has led to renewed interest in cleaning
of big data sources, where manual data cleansing tasks are
seen as prohibitively expensive and time-consuming [2],
or the data has been generated by users and cannot be
implicitly trusted [3]. Among the various types of big data,
the need to efficiently handle large scaled structured data
that is rife with inconsistency and incompleteness is also
more significant than ever. Indeed, multiple studies, such as
[4] emphasize the importance of effective, efficient methods
for handling ‚Äúdirty big data‚Äù.

TABLE 1: A snapshot of car data extracted from cars.com
using information extraction techniques
TID
t1
t2
t3
t4
t5
t6

Model
Civic
Focus
Civik
Civic
Accord

Make
Honda
Ford
Honda
Ford
Honda
Honda

Orig
JPN
USA
JPN
USA
JPN
JPN

Size
Mid-size
Compact
Mid-size
Compact
Mid-size
Full-size

Engine
V4
V4
V4
V4
V4
V6

Condition
NEW
USED
USED
USED
NEW
NEW

Most of the current techniques are based on deterministic rules, which have a number of problems: Suppose that
the user is interested in finding ‚ÄòCivic‚Äô cars from Table 1.
‚Ä¢

This work was done when all the authors were with the Department of
Computer Science & Engineering at Arizona State University, Tempe,
AZ 85287. Sushovan De is now with Google Inc. Yuheng Hu is now with
IBM Research, Almaden. Yi Chen is now with the School of Management
and the College of Computing at New Jersey Institute of Technology.

Traditional data retrieval systems would return tuples t1
and t4 for the query, because they are the only ones that
are a match for the query term. Thus, they completely miss
the fact that t4 is in fact a dirty tuple ‚Äî A Ford Focus car
mislabeled as a Civic. Additionally, tuple t3 and t5 would
not be returned as a result tuples since they have a typos
or missing values, although they represent desirable results.
The objective of this work is to provide the true result set
(t1 , t3 , t5 ) to the user.
Although this problem has received significant attention
over the years in the traditional database literature, the stateof-the-art approaches fall far short of an effective solution
for big data and web data. Traditional methods include
outlier detection [5], noise removal [6], entity resolution [6],
[7], and imputation [8]. Although these methods are efficient
in their own scenarios, their dependence on clean master
data is a significant drawback.
Specifically, state of the art approaches (e.g., [9], [10],
[11]) attempt to clean data by exploiting patterns in the data,
which they express in the form of conditional functional
dependencies (or CFDs). In the motivating example, the fact
that Honda cars have ‚ÄòJPN‚Äô as the origin of the manufacturer
would be an example of such a pattern. However, these
approaches depend on the availability of a clean data corpus
or an external reference table to learn data quality rules or
patterns before fixing the errors in the dirty data. Systems
such as ConQuer [12] depend upon a set of clean constraints
provided by the user. Such clean corpora or constraints
may be easy to establish in a tightly controlled enterprise
environment but are infeasible for web data and big data.
One may attempt to learn data quality rules directly from
the noisy data. Unfortunately however, our experimental
evaluation shows that even small amounts of noise severely
impairs the ability to learn useful constraints from the data.
To avoid dependence on clean master data, we propose
a novel system called BayesWipe [13] that assumes that
a statistical process underlies the generation of clean data
(which we call the data source model) as well as the cor-

2

ruption of data (which we call the data error model). The
noisy data itself is used to learn the parameters of these
the generative and error models, eliminating dependence
on clean master data. Then, by treating the clean value as
a latent random variable, BayesWipe leverages these two
learned models and automatically infers its value through a
Bayesian estimation.
We designed BayesWipe so that it can be used in two different modes: a traditional offline cleaning mode, and a novel
online query processing mode. The offline cleaning mode of
BayesWipe follows the classical data cleaning model, where
the entire database is accessible and can be cleaned in situ.
This mode is particularly useful when one has complete
control over the data, and a one-time cleaning of the data is
needed. Data warehousing scenarios such as data crawled
from the web, or aggregated from various noisy sources
can be effectively cleaned in this mode. The cleaned data
can be stored either in a deterministic database, or in a
probabilistic database. If a probabilistic database is chosen
as the output mode, BayesWipe stores not only the clean
version of the tuple it believes to be most likely correct
one, but the entire distribution over possible clean tuples.
The choice of a probabilistic output mode for the cleaned
tuples is most useful for those scenarios where recall is very
important for further data processing on the cleaned tuples.
One of the features of the offline mode of BayesWipe
is that a probabilistic database (PDB) can be generated as
a result of the data cleaning. In the first instance, notice
that BayesWipe was built for deterministic databases. It
can operate on a deterministic database and produce a
probabilistic cleaned database as an output. Probabilistic
databases are complex and unintuitive, because each single
input tuple is mapped into a distribution over resulting
clean alternatives. We show how the top-k results can be
retrieved from a PDB while displaying the clean data that is
comprehensible to the user.
The online query processing mode of BayesWipe is motivated by web data scenarios where it is impractical to create
a local copy of the data and clean it offline, either due to
large size, high frequency of change, or access restrictions. In
such cases, the best way to obtain clean answers is to clean
the resultset as we retrieve it, which also provides us the
opportunity of improving the efficiency of the system, since
we can now ignore entire portions of the database which are
likely to be unclean or irrelevant to the top-k . BayesWipe
uses a query rewriting system that enables it to efficiently
retrieve only those tuples that are important to the top-k
result set. This rewriting approach is inspired by, and is a
significant extension of our earlier work on QPIAD system
for handling data incompleteness [14]. In big data scenarios,
clean master data is rarely available, and write access is
either unavailable, or undesirable due to the efficiency and
indexing concerns. The online mode is particularly suited to
get clean results in such results.
We implement BayesWipe in a Map-Reduce architecture,
so that we can run it very quickly for massive datasets. The
architecture for parallelizing BayesWipe is explained more
fully in Sec 7. In short, there is a two-stage map-reduce
architecture, where in the first stage, the dirty tuples are
routed to a set of reducer nodes which hold the relevant
candidate clean tuples for them. In the second stage, the

resulting candidate clean tuples along with their scores are
collated, and the best replacement tuple is selected from
them.
To summarize our contributions, we:
‚Ä¢
‚Ä¢

‚Ä¢
‚Ä¢
‚Ä¢

Propose that data cleaning should be done using a
principled, probabilistic approach.
Develop a novel algorithm following those principles, which uses a Bayes network as the generative
model and maximum entropy as the error model of
the data.
Develop novel query rewriting techniques so that
this algorithm can also be used in a big data scenario.
Develop a parallelized version of this algorithm using map-reduce framework.
Empirically evaluate the performance of our algorithm using both controlled and real datasets.

The rest of the paper is organized as follows. We begin by discussing the related work and then describe the
architecture of BayesWipe in the next section, where we
also present the overall algorithm. Section 4 describes the
learning phase of BayesWipe, where we find the generative
and error models. Section 5 describes the offline cleaning
mode, and the next section details the query rewriting
and online data processing. We describe the parallelized
version of BayesWipe in Section 7 and the results of the
our empirical evaluation in Section 8, and then conclude
by summarizing our contributions. Further details about
BayesWipe can be found in the thesis [15].

2

R ELATED W ORK

Much of the work in data cleaning focused on deterministic
dependency relations such as FD, CFD, and INDs. Bohannon et al. proposed using Conditional Functional Dependencies (CFD) to clean data [16], [17]. Indeed, CFDs are
very effective in cleaning data. However, the precision and
recall of cleaning data with CFDs completely depends on
the quality of the set of dependencies used for the cleaning.
As our experiments show, learning CFDs from dirty data
produces very unsatisfactory results. In order for CFDbased methods to perform well, they need to be learned
from a clean sample of the database [10] which must be
large enough to be representative of all the patterns in the
data. Finding such a large corpus of clean master data is a
non-trivial problem, and is infeasible in all but the most
controlled of environments (like a corporation with high
quality data).
A recent variant on the deterministic dependency based
cleaning by J.Wang et al. [18] proposes using fixing rules
which contain negative(possible errors) and positive(clean
replacements) patterns for an attribute. However, there can
be several ways in which a tuple can go wrong and the
detection of the positive pattern requires clean master data.
BayesWipe on the other hand uses an error model to detect
errors automatically and clean them in the absence of clean
master data. Recent work by J.Wang et al. [19] plugs in one
of the rule based cleaning techniques to clean a sample of
the entire data and use it as a guideline to clean the entire
data. It is important to note that this method only caters
to aggregate numerical queries whereas the online mode

3

of BayesWipe supports all types of SQL queries (not just
aggregates) and returns clean result tuples.
Although it is possible to ameliorate some of the difficulties of CFD/AFD methods by considering approximate versions of them, the work in the uncertainty in AI
community demonstrated the semantic pitfalls of handling
uncertainty in this way. In particular, approximate versions
of CFDs/AFDs considered in works such as [20], [21] are
similar to the certainty factors approaches for handling
uncertainty that were popular in the heyday of expert
systems, but whose semantic inconsistencies are by now
well-established (see, for example, Section 14.7.1 of [34]).
Because of this, in this paper we focus on a more systematic
probabilistic approach.
Even if a curated set of integrity constraints are provided, existing methods do not use a probabilistically principled method of choosing a candidate correction. They resort
to either heuristic based methods, finding an approximate
algorithm for the least-cost repair of the database [9], [22],
[23]; using a human-guided repair [24], or sampling from
a space of possible repairs [25]. There has been work that
attempts to guarantee a correct repair of the database [26],
but they can only provide guarantees for corrections of those
tuples that are supported by data from a perfectly clean
master database. Recently, [27] have shown how the relative
trust one places on the constraints and the data itself plays
into the choice of cleaning tuples. A Bayesian source model
of data was used by [28], but was limited in scope to figuring
out the evolution over time of the data value.
Recent work has also focused on the metrics to use to
evaluate data cleaning techniques [29]. In this work, we
focus on evaluating our method against the ground truth
(when the ground truth is known), and user studies (when
the ground truth is not known).
While BayesWipe uses crowdsourcing to evaluate the
accuracy of the proposed clean tuple alternatives for the
experiments on real world datasets, there are other systems that try to use the crowd for cleaning the data itself.
X.Chu et al. [30] clean the database tuples by discovering
patterns that overlap with Knowledge Base(KB)s like Yago
and validating the top-k candidates using the crowd. J.Wang
et al. [31] perform entity resolution (which is to identify
several values corresponding to the same entity value) using
crowdsourcing. They reduce the complexity of the number
of HIT(Human Intelligence Task)s generated by clustering
them into several bins so that a set of pairs can be resolved
at a time as against evaluating one pair at a time. Y.Zheng
et al. [32] pick a set of k questions to be included in the HITs
for the human workers out of a total set of n questions using
estimates on the expected increase in the answer quality by
assigning those questions to the crowd. Crowdsourcing to
perform data cleaning may be infeasible in the context of Big
Data cleaning targeted by BayesWipe . However, suggestions from the crowd can be used to provide cleaner master
data from which BayesWipe learns the Bayes network.
The query rewriting part of this work is inspired
by the QPIAD system [14], but significantly improves
upon it. QPIAD performed query rewriting over incomplete databases using approximate functional dependencies
(AFD), and only cleaned data with null values, not wrong
values.

Offline Cleaning

Model Learning
Data source
model
Error
Model

Cleaning to a
Deterministic DB
or
Cleaning to a
Probabilistic DB

Clean
Data

OR
Candidate
Set Index

Query Processing
Query
Rewriting

Database
Sampler

Result
Ranking

Data Source

Fig. 1: The architecture of BayesWipe. Our framework learns
both data source model and error model from the raw data
during the model learning phase. It can perform offline
cleaning or query processing to provide clean data.

3

BAYES W IPE OVERVIEW

BayesWipe views the data cleaning problem as a statistical
inference problem over the structured text data. Let D =
{T1 , ..., Tn } be the input structured data which contains a
number of corruptions. Ti ‚àà D is a tuple with m attributes
{A1 , ..., Am } which may have one or more corruptions in
its attribute values. Given a candidate replacement set C
for a possibly corrupted tuple T in D, we can clean the
database by replacing T with the candidate clean tuple
T ‚àó ‚àà C that has the maximum Pr(T ‚àó |T ). Using Bayes rule
(and dropping the common denominator), we can rewrite
this to
‚àó
Tbest
= arg max[Pr(T |T ‚àó )Pr(T ‚àó )]

(1)

‚àó
By replacing T with Tbest
, we get a deterministic database.
If we wish to create a probabilistic database (PDB), we don‚Äôt
take an arg max over the Pr(T ‚àó |T ), instead we store the
entire distribution over the T ‚àó in the resulting PDB.
For online query processing we take the user query Q‚àó ,
and find the relevance score of a tuple T as
X
Score(T ) =
Pr(T ‚àó ) Pr(T |T ‚àó ) R(T ‚àó |Q‚àó )
(2)
| {z } | {z } | {z }
‚àó
T ‚ààC

source model error model

relevance

In this work, we used a binary relevance model, where R
is 1 if T ‚àó is relevant to the user‚Äôs query, and 0 otherwise.
Note that R is the relevance of the query Q‚àó to the candidate
clean tuple T ‚àó and not the observed tuple T . This allows the
query rewriting phase of BayesWipe which aims to retrieve
tuples with the highest Score(.) to achieve the non-lossy
effect of using a PDB without explicitly rectifying the entire
database.
Architecture:
Figure 1 shows the system architecture for BayesWipe.
During the model learning phase (Section 4), we first obtain
a sample database by sending some queries to the database.
On this sample data, we learn the generative model of the
data as a Bayes network (Section 4.1). In parallel, we define
and learn an error model which incorporates common kinds

4

of errors (Section 4.2). We also create an index to quickly
propose candidate T ‚àó s.
We can then choose to do either offline cleaning (Section 5) or online query processing (Section 6), as per the
scenario. In the offline cleaning mode, we iterate over all
the tuples in the database and clean them one by one. We
can choose whether to store the resulting cleaned tuple in a
deterministic database (where we store only the T ‚àó with the
maximum posterior probability) or probabilistic database
(where we store the entire distribution over the T ‚àó ). In the
online query processing mode, we obtain a query from the
user, and do query rewriting in order to find a set of queries
that are likely to retrieve a set of highly relevant tuples.
We execute these queries and re-rank the results, and then
display them.
Algorithm 1: The algorithm for offline data cleaning
Input: D, the dirty dataset.
BN ‚Üê Learn Bayes Network (D)
foreach Tuple T ‚àà D do
C ‚Üê Find Candidate Replacements (T )
foreach Candidate T ‚àó ‚àà C do
P (T ‚àó ) ‚Üê Find Joint Probability (T ‚àó , BN )
P (T |T ‚àó ) ‚Üê Error Model (T, T ‚àó )
end
T ‚Üê arg max
P (T ‚àó )P (T |T ‚àó )
‚àó
T ‚ààC

end

Algorithm 2: Algorithm for online query processing.
Input: D, the dirty dataset
Input: Q, the user‚Äôs query
S ‚Üê Sample the source dataset D
BN ‚Üê Learn Bayes Network (S )
ES ‚Üê Learn Error Statistics (S )
R ‚Üê Query and score results (Q, D, BN )
ESQ ‚Üê Get expanded queries (Q)
foreach Expanded query E ‚àà ESQ do
R ‚Üê R ‚à™ Query and score results (E, D, BN )
RQ ‚Üê RQ ‚à™ Get all relaxed queries (E )
end
Sort(RQ) by expected relevance, using ES
while top-k confidence not attained do
B ‚Üê Pick and remove top RQ
R ‚Üê R ‚à™ Query and score results (B, D, BN )
end
Sort(R) by score
return R
In Algorithms 1 and 2, we present the overall algorithm
for BayesWipe. In the offline mode, we show how we iterate
over all the tuples in the dirty database, D and replace them
with cleaned tuples. In the query processing mode, the first
three operations are performed offline, and the remaining
operations show how the tuples are efficiently retrieved
from the database, ranked and displayed to the user.

4

M ODEL L EARNING

This section details the process by which we estimate the
components of Equation 2: the data source model Pr(T ‚àó )

Make

Condition

occupation

Model

Year

gender

workingclass

Door

Drivetrain

country

race
education

marital status

Engine

Car Type

(a) Auto dataset

filing-status

(b) Census dataset

Fig. 2: The learned Bayes networks
and the error model Pr(T |T ‚àó )
4.1

Data Source Model

The data that we work with can have dependencies among
various attributes (e.g., a car‚Äôs engine depends on its make).
Therefore, we represent the data source model as a Bayes
network, since it naturally captures relationships between
the attributes via structure learning and infers probability
distributions over values of the input tuples.
Constructing a Bayes network over D requires two steps:
first, the induction of the graph structure of the network,
which encodes the conditional independences between the
m attributes of D‚Äôs schema; and second, the estimation
of the parameters of the resulting network. The resulting
model allows us to compute probability distributions over
an arbitrary input tuple T .
Whenever the underlying patterns in the source database
changes, we have to learn the structure and parameters
of the Bayes network again. In our scenario, we observed
that the structure of a Bayes network of a given dataset
remains constant with small perturbations, but the parameters (CPTs) change more frequently. As a result, we spend a
larger amount of time learning the structure of the network
with a slower, but more accurate tool, Banjo [33]. Figures
2a and 2b show automatically learned structures for two
data domains. The learned structure seems to be intuitively
correct, since the nodes that are connected (for example,
‚Äòcountry‚Äô and ‚Äòrace‚Äô in Figure 2b) are expected to be highly
correlated1 .
Then, given a learned graphical structure G of D, we
can estimate the conditional probability tables (CPTs) that
parameterize each node in G using a faster package called
Infer.NET [35]. This process of inferring the parameters is
run offline, but more frequently than the structure learning.
Once the Bayesian network is constructed, we can infer
the joint distributions for arbitrary tuple T , which can
be decomposed to the multiplication of several marginal
distributions of the sets of random variables, conditioned
on their parent nodes depending on G .
4.2

Error Model

Having described the data source model, we now turn to
the estimation of the error model Pr(T |T ‚àó ) from noisy data.
There are many types of errors that can occur in data. We
focus on the most common types of errors that occur in data
1. Note that the direction of the arrow in a Bayes network does not
necessarily determine causality, see Chapter 14 from Russel and Norvig
[34].

5

that is manually entered by naƒ±Ãàve users: typos, deletions,
and substitution of one word with another. We also make
an additional assumption that error in one attribute does
not affect the errors in other attributes. This is a reasonable assumption to make, since we are allowing the data
itself to have dependencies between attributes, while only
constraining the error process to be independent across
attributes. With these assumptions, we are able to come up
with a simple and efficient error model, where we combine
the three types of errors using a maximum entropy model.
Given a set of clean candidate tuples C where T ‚àó ‚àà C ,
our error model Pr(T |T ‚àó ) essentially measures how clean T
is, or in other words, how similar T is to T ‚àó .
Edit distance similarity: This similarity measure is used to
detect spelling errors. Edit distance between two strings TAi
and TA‚àó i is defined as the minimum cost of edit operations
applied to dirty tuple TAi transform it to clean TA‚àó i . Edit
operations include character-level copy, insert, delete and
substitute. The cost for each operation can be modified as
required; in this paper we use the Levenshtein distance,
which uses a uniform cost function. This gives us a distance,
which we then convert to a probability using [36]:

4.3

Finding the Candidate Set

The set of candidate tuples, C(T ) for a given tuple T are
the possible replacement tuples that the system considers
as possible corrections to T . The larger the set C is, the
longer it will take for the system to perform the cleaning. If
C contains many unclean tuples, then the system will waste
time scoring tuples that are not clean to begin with.
An efficient approach to finding a reasonably clean
C(T ) is to consider the set of all the tuples in the sample
database that differ from T in not more than j attributes.
In order to find C(T ) that satisfies this, conceptually, we
have to iterate over every tuple t in the sample database
D, comparing it to the tuple T and checking how many
attributes it differs in. This operation can take O(n) time,
where n is the number of tuples in the sample database.
Even with j = 3, the naƒ±Ãàve approach of constructing C from
the sample database directly is too time consuming, since
it requires one to go through the sample database in its
entirety once for every result tuple encountered. To make
this process faster, we create indices over (j + 1) attributes
because searching through indices reduces the number of
comparisons required to compute C(T ). If any candidate
fed (TAi , TA‚àó i ) = exp{‚àícosted (TAi , TA‚àó i )}
(3) tuple T ‚àó differs from T in less than or equal to j attributes,
Distributional Similarity Feature: This similarity measure then it will be present in at least one of the indices, since
is used to detect both substitution and omission errors. we created j + 1 of them (pigeon hole principle). These
Looking at each attribute in isolation is not enough to fix j + 1 indices are created over those attributes that have the
these errors. We propose a context-based similarity measure highest cardinalities, such as Make and Model (as opposed
called Distributional similarity (fds ), which is based on the to attributes like Condition and Doors which can take only
probability of replacing one value with another under a a few values). This ensures that the set of tuples returned
similar context [37]. Formally, for each string TAi and TA‚àó i , from the index would be small in number.
For every possibly dirty tuple T in the database, we go
we have:
over
each such index and find all the tuples that match the
‚àó
X
Pr(c|TAi )Pr(c|TAi )Pr(TAi )
fds (TAi , TA‚àó i ) =
(4)corresponding attribute. The union of all these tuples is then
Pr(c)
‚àó )
examined and the candidate set C is constructed by keeping
c‚ààC(TAi ,TA
i
only those tuples from this union set that do not differ from
where C(TAi , TA‚àó i ) is the context of a tuple attribute value, T in more than j attributes. Thus we can be sure that by
which is a set of attribute values that co-occur with using this method, we have obtained the entire set C 2 .
both TAi and TA‚àó i . Pr(c|TA‚àó i ) = (#(c, TA‚àó i ) + ¬µ)/#(TA‚àó i )
is the probability that a context value c appears given
the clean attribute TA‚àó i in the sample database. Similarly, 5 O FFLINE CLEANING
P (TAi ) = #(TAi )/#tuples is the probability that a dirty
attribute value appears in the sample database. We calculate 5.1 Cleaning to a Deterministic Database
Pr(c|TAi ) and Pr(TAi ) in the same way. To avoid zero In order to clean the data in situ, we first use the techniques
estimates for attribute values that do not appear in the of the previous section to learn the data source model, the
database sample, we use Laplace smoothing factor ¬µ.
error model and create the index. Then, we iterate over all
Unified error model: In practice, we do not know before- the tuples in the database and use Equation 1 to find the
hand which kind of error has occurred for a particular T ‚àó with the best score. We then replace the tuple with that
attribute; we need a unified error model which can accom- T ‚àó , thus creating a deterministic database using the offline
modate all three types of errors (and be flexible enough to mode of BayesWipe.
accommodate more errors when necessary). For this purComputing Pr(T ‚àó )Pr(T |T ‚àó ) is very fast. Even though
pose, we use the well-known maximum entropy framework we do a Bayesian inference for Pr(T ‚àó ), the tuple has all the
[38] to leverage both the similarity measures, (Edit distance values specified, so the inference ends up being a simple
fed and distributional similarity fds ). For each attribute of multiplication over the CPTs of the Bayes network, and is
the input tuple T and T ‚àó , we have the unified error model very cheap. Pr(T |T ‚àó ) involves simple edit distance and disPr(T |T ‚àó ) given by:
tributional similarity calculations all of which involve sim( m
)
ple arithmetic operations and lookups devoid of Bayesian
m
X
X
1
exp Œ±
fed (TAi , TA‚àó i ) + Œ≤
fds (TAi , TA‚àó i )
(5) inference.
Z
i=1
i=1
where Œ± and Œ≤ are the weight of each feature, m is the
numberPof attributes
P in the tuple. The normalization factor
is Z = T ‚àó exp { i Œªi fi (T ‚àó , T )}.

2. There is a small possibility that the true tuple T ‚àó is not in the
sample database at all. This probability can be reduced by choosing
a larger sample set. In future work, we will expand the strategy of
generating C to include all possible k-repairs of a tuple.

6

Recall from Section 4.2 that there are parameters in the
error model called Œ± and Œ≤ , which need to be set. Interestingly, in addition to controlling the relative weight given to
the various features in the error model, these parameters
can be used to control overcorrection by the system.
Overcorrection: Any data cleaning system is vulnerable to
overcorrection, where a legitimate tuple is modified by the
system to an unclean value. Overcorrection can have many
causes. In a traditional, deterministic system, overcorrection
can be caused by erroneous rules learned from infrequent
data. For example, certain makes of cars are all owned by the
same conglomerate (GM owns Chevrolet). In a misguided
attempt to simplify their inventory, a car salesman might list
all the cars under the name of the conglomerate. This may
provide enough support to learn the wrong rule (Malibu ‚Üí
GM).
Typically, once an erroneous rule has been learned, there
is no way to correct it or ignore it without a lot of oversight
from domain experts. However, BayesWipe provides a way
to regulate the amount of overcorrection in the system with
the help of a ‚Äòdegree of change‚Äô parameter. Without loss of
generality, we can rewrite Equation 5 to the following:
  X
m
1
Pr(T |T ) = exp Œ≥ Œ¥
fed (TAi , TA‚àó i )
Z
i=1
‚àó

+ (1 ‚àí Œ¥)

m
X


fds (TAi , TA‚àó i )

i=1

Since we are only interested in their relative weights, the
parameters Œ± and Œ≤ have been replaced by Œ¥ and (1‚àíŒ¥) with
the help of a normalization constant, Œ≥ . This parameter, Œ≥ ,
can be used to modify the degree of variation in Pr(T |T ‚àó ).
High values of Œ≥ imply that small differences in T and
T ‚àó cause a larger difference in the value of Pr(T |T ‚àó ),
causing the system to give higher scores to the original tuple
(compared to a modified tuple).
Example: Consider the following fragment from the
database. The first tuple is a very frequent tuple in the
database, the second one is an erroneous tuple, and the third
tuple is an infrequent, correct tuple. The ‚Äòtrue‚Äô correction
of the second tuple is the third tuple. The Pr(T ‚àó ) values
shown reflect the values that the data source model might
predict for them, roughly based on the frequency with
which they occur in the source data.
Id

Make

Model Type

1
2
3

Honda Civic
Honda Z4
BMW Z4

Engine

Sedan V4
Sedan V6
Sedan V6

Condition P (T ‚àó )
New
New
New

0.400
0.001
0.005

A proper data cleaning system will correct tuple 2 to
tuple 3, and not modify any of the others. However, if
incorrect rules (for example, Z4 ‚Üí Honda) were learned,
there could be overcorrection, where tuple 3 is modified to
tuple 2.
On the other hand, BayesWipe handles this situation
based on the value of Œ≥ . Looking at tuple 3 (which is a clean
tuple), suppose the candidate replacement tuples for it are
also tuples 1, 2 and 3. In that case, the situation may look
like the following:

Cd.
1
2
3

P (T ‚àó )
0.400
0.001
0.005

low Œ≥
P (T |T ‚àó )
score
0.02 0.0080
0.30 0.0003
1.00 0.0050

high Œ≥
P (T |T ‚àó )
score
0.002 0.00080
0.030 0.00003
1.000 0.00500

As we can see, if we choose a low value of Œ≥ , the
candidate with the highest score is tuple 1, which means
an overcorrection will occur. However, with higher Œ≥ , the
candidate with the highest score is tuple 3 itself, which
means the tuple will not be modified, and overcorrection
will not occur. On the other hand, if we set Œ≥ too high, then
even legitimately dirty tuples like tuple 2 won‚Äôt get changed,
thus the number of actual corrections will also be lower.
To make full use of this capability of regulating overcorrection, we need to be able to set the value of Œ≥ appropriately. In the absence of a training dataset (for which the
ground truth is known), we can only estimate the best Œ≥
approximately. We do this by finding a value of Œ≥ for which
the percentage of tuples modified by the system is equal to
the expected percentage of noise in the dataset.
5.2

Cleaning to a Probabilistic Database

We note that many data cleaning approaches ‚Äî including
the one we described in the previous sections ‚Äî come up
with multiple alternatives for the clean version for any given
tuple, and evaluate their confidence in each of the alternatives. For example, if a tuple is observed as ‚ÄòHonda, Corolla‚Äô,
two correct alternatives for that tuple might be ‚ÄòHonda,
Civic‚Äô and ‚ÄòToyota, Corolla‚Äô. In such cases, where the choice
of the clean tuple is not an obvious one, picking the mostlikely option may lead to the wrong answer. Additionally,
if we intend to do further processing on the results, such as
perform aggregate queries, join with other tables, or transfer
the data to someone else for processing, then storing the
most likely outcome is lossy.
A better approach (also suggested by others [4]) is to
store all of the alternative clean tuples along with their
confidence values. Doing this, however, means that the
resulting database will be a probabilistic database (PDB),
even when the source database is deterministic.
It is not clear upfront whether PDB-based cleaning will
have advantages over cleaning to a deterministic database.
On the positive side, using a PDB helps reduce loss of
information arising from discarding all alternatives to tuples
that did not have the maximum confidence. On the negative
side, PDB-based cleaning increases the query processing
cost (as querying PDBs are harder than querying deterministic databases [39]).
Another challenge is one of presentation: users usually
assume that they are dealing with a deterministic source
of data, and presenting all alternatives to them can be
overwhelming to them. In this section, and in the associated
experiments, we investigate the potential advantages to using the BayesWipe system and storing the resulting cleaned
data in a probabilistic database. For our experiments, we
used Mystiq [40], a prototype probabilistic database system
from University of Washington, as the substrate. In order to
create a probabilistic database from the corrections of the input data, we follow the offline cleaning procedure described
previously in Section 4. Instead of storing the most likely T ‚àó ,
we store all the T ‚àó s along with their P (T ‚àó |T ) values. When

7

evaluating the performance of the probabilistic database, we
used simple select queries on the resulting database. Since
representing the results of a probabilistic database to the
user is a complex task, in this paper we focus on showing
the XOR representation of the tuple alternatives to the user.
The rationale for our decision is that in a used car scenario,
the user will be provided with a URL link to the car through
the clickable tuple id and the several alternative clean values
for the dirty attributes are shown within the single tuple
returned to the user. As a result, the form of our output is a
tuple-disjoint independent database [41]. This can be better
explained with an example:
TABLE 2: Cleaned probabilistic database
TID
t1

Model
Civic
Civic

Make
Honda
Honda

Orig.
JPN
JPN

Civic
Civik

Honda
Honda

JPN
JPN

Size
Mid-size
Compact

Eng.
V4
V6

Cond.
NEW
NEW

P
0.6
0.4

Mid-size
Mid-size

V4
V4

USED
USED

0.9
0.1

...
t3

Example: Suppose we clean our running example of
Table 1. We will obtain a tuple-disjoint independent3 probabilistic database [41]; a fragment of which is shown in
Table 2. Each original input tuple (t1 , t3 ), has been cleaned,
and their alternatives are stored along with the computed
confidence values for the alternatives (0.6 and 0.4 for t1 ,
in this example). Suppose the user issues a query Model =
Civic. Both options of tuple t1 of the probabilistic database
satisfy the constraints of the query. Since there are two valid
alternatives to tuple t1 in the result with probabilities 0.6
and 0.4, in order to get a single tuple representation, the
matching attributes in the alternatives are shown deterministically whereas the unclean attributes like Size, Engine and
Condition with several possible clean values are shown as
options. Only the first option in tuple t3 matches the query.
Thus the XOR result will contain only a single alternative
for t3 with probability 0.9. It is important to note that in the
case of t1 , the Mid-size car can be associated with an Eng.
value of V4 and a probability of 0.6 respectively. The XOR
representation doesn‚Äôt necessarily allow for combining Midsize with either an Eng. value of V6 or a probability value
of 0.4.
The experimental results compare the tuple ids when
computing the recall of the method because tuple id provides the URL to the car‚Äôs web page which can be used
to determine a match. The output probabilistic relation is
shown in Table 3.
TABLE 3: Result probabilistic database
TID

Model

Make

Orig.

t1

Civic

Honda

JPN

t3

Civic

Honda

JPN

Size
Midsize/Compact
Mid-size

Eng.

Cond.

P

V4/V6

NEW

0.6/0.4

V4

USED

0.9

3. A tuple-disjoint independent probabilistic database is one where
every tuple, identified by its primary key, is independent of all other
tuples. Each tuple is, however, allowed to have multiple alternatives
with associated probabilities. In a tuple-independent database, each
tuple has a single probability, which is the probability of that tuple
existing.

The interesting fact here is that the result of any query
will always be a tuple-independent database. This is because we projected out every attribute except for the tupleID, and the tuple-IDs are independent of each other.
When showing the results of our experiments, we evaluate the precision and recall of the system. Since precision
and recall are deterministic concepts, we have to convert
the probabilistic database into a deterministic database (that
will be shown to the user) prior to computing these values.
We can do this conversion in two ways: (1) by picking only
those tuples whose probability is higher than some threshold. We call this method the threshold based determinization.
(2) by picking the top-k tuples and discarding the probability values (top-k determinization). The experiment section
(Section 8.2) shows results with both determinizations.

6

Q UERY REWRITING FOR O NLINE Q UERY P RO -

CESSING

In this section we extend the techniques of the previous
section so that it can be used in an online query processing
method where the result tuples are cleaned at query time.
Certain tuples that do not satisfy the query constraints, but
are relevant to the user, need to be retrieved, ranked and
shown to the user. The process also needs to be efficient,
since the time that the users are willing to wait before
results are shown to them is very small. We show our query
rewriting mechanisms aimed at addressing both.
We begin by executing the user‚Äôs query (Q‚àó ) on the
database. We store the retrieved results, but do not show
them to the user immediately. We then find rewritten queries
that are most likely to retrieve clean tuples. We do that in
a two-stage process: we first expand the query to increase
the precision, and then relax the query by deleting some
constraints (to increase the recall).
6.1

Increasing the precision of rewritten queries

We can improve precision by adding relevant constraints to the query Q‚àó given by the user. For example, when a user issues the query Model = Civic,
we can expand the query to add relevant constraints Make = Honda, Country = Japan, Size = Mid-Size.
These additions capture the essence of the query ‚Äî because
they limit the results to the specific kind of car the user
is probably looking for. These expanded structured queries
generated from the user‚Äôs query are called ESQs.
Each user query Q‚àó is a select query with one or more
attribute-value pairs as constraints. In order to create an
ESQ, we will have to add highly correlated constraints to
Q‚àó .
Searching for correlated constraints to add requires
Bayesian inference, which is an expensive operation. Therefore, when searching for constraints to add to Q‚àó , we restrict
the search to the union of all the attributes in the Markov
blanket [42]. The Markov blanket of an attribute comprises
its children, its parents, and its children‚Äôs other parents. It
is the set of attributes whose value being given, the node
becomes independent of all other nodes in the network.
Thus, it makes sense to consider these nodes when finding
correlated attributes. This correlation is computed using
the Bayes Network that was learned offline on a sample
database (recall the architecture of BayesWipe in Figure 1.)

8

Given a Q‚àó , we attempt to generate multiple ESQs that
maximizes both the relevance of the results and the coverage
of the queries of the solution space.
Note that if there are m attributes, each of which can
take n values, then the total number of possible ESQs is
nm . Searching for the ESQ that globally maximizes the
objectives in this space is infeasible; we therefore approximately search for it by performing a heuristic-informed
search. Our objective is to create an ESQ with m attributevalue pairs as constraints.We begin with the constraints
specified by the user query Q‚àó . We set these as evidence in
the Bayes network, and then query the Markov blanket of
these attributes for the attribute-value pairs with the highest
posterior probability given this evidence. We take the top-k
attribute-value pairs and append them to Q‚àó to produce k
search nodes, each search node being a query fragment. If Q
has p constraints in it, then the heuristic value of Q is given
by Pr(Q)m/p . This represents the expected joint probability
of Q when expanded to m attributes, assuming that all the
constraints will have the same average posterior probability.
We expand them further, until we find k queries of size m
with the highest probabilities.
Make=Honda

0.4

Model=
Accord

0.1

Make=
Honda

0.3

0.9

Model=
Civic
Fuel=
Gas

Miles=
10k

‚Ä¶

(0.1)6
Make=Honda, Model = Accord

Engine=
V4

‚Ä¶

Doors=
4

‚Ä¶

(0.1 √ó 0.3)3

Model=
Civic

‚Ä¶

(0.1 √ó 0.9)3

(0.1 √ó 0.4)3
Make=Honda, Model = Civic

Make=Honda, Fuel = Gas

Fig. 3: Query Expansion Example. The tree shows the candidate constraints that can be added to a query, and the
rectangles show the expanded queries with the computed
probability values.
Example: In Figure 3, we show an example of the query
expansion. The node on the left represents the query given
by the user ‚ÄúMake=Honda‚Äù. First, we look at the Markov
Blanket of the attribute Make, and determine that Model
and Condition are the nodes in the Markov blanket. we
then set ‚ÄúMake=Honda‚Äù as evidence in the Bayes network
and then run an inference over the values of the attribute
Model. The two values of the Model attribute with the
highest posterior probability are Accord and Civic. The
most probable values of the Condition attribute are ‚Äúnew‚Äù
and ‚Äúold‚Äù. Using each of these values, new queries are
constructed and added to the queue. Thus, the queue now
consists of the 4 queries: ‚ÄúMake=Honda, Model=Civic‚Äù,
‚ÄúMake=Honda, Model=Accord‚Äù and ‚ÄúMake=Honda, Condition=old‚Äù. A fragment of these queries are shown in
the middle column of Figure 3. We dequeue the highest
probability item from the queue and repeat the process
of setting the evidence, finding the Markov Blanket, and
running the inference. We stop when we get the required
number of ESQs with a sufficient number of constraints.
6.2 Increasing the recall
Adding constraints to the query causes the precision of the
results to increase, but reduces the recall drastically. Therefore, in this stage, we choose to delete some constraints from

the ESQs, thus generating relaxed queries (RQ). Notice that
tuples that have corruptions in the attribute constrained by
the user can only be retrieved by relaxed queries that do
not specify a value for those attributes. Instead, we have to
depend on rewritten queries that contain correlated values
in other attributes to retrieve these tuples. Using relaxed
queries can be seen as a trade-off between the recall of the
resultset and the time taken, since there are an exponential
number of relaxed queries for any given ESQ. As a result,
an important question is the choice of RQs to execute. We
take the approach of generating every possible RQ, and
then ranking them according to their expected relevance.
This operation is performed entirely on the learned error
statistics, and is thus very fast.
We score each relaxed query by the expected relevance of
its result set.
!
P
‚àó
Tq Score(Tq |Q )
Rank(q) = E
|Tq |
where Tq are the tuples returned by a query q , and Q‚àó is
the user‚Äôs query. Executing an RQ with a higher rank will
have a more beneficial result on the result set because it will
bring in better quality result tuples. Estimating this quantity
is difficult because we do not have complete information
about the tuples that will be returned for any query q . The
best we can do, therefore, is to approximate this quantity.
Let the relaxed query be Q, and the expanded query
that it was relaxed from be ESQ. We wish to estimate
E[P (T |T ‚àó )] where T are the tuples returned by Q. Using the
attribute-error
independence assumption, we can rewrite
Qm
that as i=0 Pr(T.Ai |T ‚àó .Ai ), where T.Ai is the value of the
i-th attribute in T. Since ESQ was obtained by expanding
Q‚àó using the Bayes network, it has values that can be
considered clean for this evaluation. Now, we divide the m
attributes of the database into 3 classes: (1) The attribute
is specified both in ESQ and in Q. In this case, we set
Pr(T.Ai |T ‚àó .Ai ) to 1, since T.Ai = T ‚àó .Ai . (2) The attribute
is specified in ESQ but not in Q. In this case, we know
what T ‚àó .Ai is, but not T.Ai . However, we can generate an
average statistic of how often T ‚àó .Ai is erroneous by looking
at our sample database. Therefore, in the offline learning
stage, we precompute tables of error statistics for every T ‚àó
that appears in our sample database, and use that value.
(3) The attribute is not specified in either ESQ or Q. In
this case, we know neither the attribute value in T nor in
T ‚àó . We, therefore, use the average error rate of the entire
attribute as the value for Pr(T.Ai |T ‚àó .Ai ). This statistic is
also precomputed during the learning phase. This product
gives the expected rank of the tuples returned by Q.
Example: In Figure 4, we show an example for finding
the probability values of a relaxed query. Assume that the
user‚Äôs query Q‚àó is ‚ÄúCivic‚Äù, and the ESQ is shown in the
second row. For an RQ that removes the attribute values
‚ÄúCivic‚Äù and ‚ÄúMid-Size‚Äù from the ESQ, the probabilities are
calculated as follows: For the attributes ‚ÄúMake, Country‚Äù
and ‚ÄúEngine‚Äù, the values are present in both the ESQ as
well as the RQ, and therefore, the P (T |T ‚àó ) for them is 1.
For the attribute ‚ÄúModel‚Äù and ‚ÄúType‚Äù, the values are present
in ESQ but not in RQ, hence the value for them can be
computed from the learned error statistics. For example, for
‚ÄúCivic‚Äù, the average value of P (T |Civic) as learned from

9

Model
Q*:

Civic

ESQ:

Civic

RQ:

E[P(T|T*)]:

0.8

Make

Country

Honda

JPN

Honda

JPN

1

1

Type

Mid-size

Engine

Cond.

V4
V4

0.5

1

0.5

=0.2

Fig. 4: Query Relaxation Example.
the sample database (0.8) is used. Finally, for the attribute
‚ÄúCondition‚Äù, which is present neither in ESQ nor in RQ,
we use the average error statistic for that attribute (i.e. the
average of P (Ta |Ta‚àó ) for a = ‚ÄúCondition‚Äù which is 0.5).
The final value of E[P (T |T ‚àó )] is found from the product
of all these attributes as 0.2. This process is very fast because it only involves lookups and multiplication - bayesian
inference is not needed.
6.3

Terminating the process

We begin by looking at all the RQs in descending order
of their rank. If the current k -th tuple in our resultset has
a relevance of Œª, and the estimated rank of the Q we are
about to execute is R(Tq |Q), then we stop evaluating any
more queries if the probability Pr(R(Tq |Q) > Œª) is less
than some user defined threshold P . This ensures that we
have the true top-k resultset with a probability P .

7

M AP -R EDUCE FRAMEWORK

BayesWipe is most useful for big-data related scenarios.
BayesWipe has two modes: online and offline. The online
mode of BayesWipe already works for big data scenarios by
optimising the rewritten queries it issues. Now, we show
that the offline mode can also be optimized for a big-data
scenario by implementing it as a Map-Reduce application.
So far, BayesWipe-Offline has been implemented as a
two-phase, single threaded program. In the first phase,
the program learns the Bayes network (both structure and
parameters), learns the error statistics, and creates the candidate index. Recall from section 4.3 that we create an
index on the attributes of the sample database to speed
up the creation of the candidate set of clean tuples; which
we refer to as the candidate index. The candidate index is
constructed on a set of j + 1 attributes when the restriction
on a candidate clean tuple is to differ from the dirty tuple in
not more than j attributes. The attributes in the dirty tuple
are compared to the attributes of the tuples in the sample
database using the candidate index to generate the set of
candidate clean tuples. Note that this candidate index can be
constructed on any arbitrary set of j + 1 attributes present
in the sample database. In the second phase, the program
goes through every tuple in the input database, picks a set
of candidate tuples, and then evaluates the P (T ‚àó |T )P (T ‚àó )
for every candidate tuple, and replaces T with the T ‚àó that
maximises that value. Since the learning is typically done
on a sample of the data, it is more important to focus
on the second phase for the parallelizing efforts. Later, we
will see how the learning of the error statistics can also be
parallelized.

7.1

Simple Approach

The simplest approach to parallelizing BayesWipe is to run
the first phase (the learning phase) on a single machine.
Then, a copy of the bayes network (structure and CPTs),
the error statistics, and the candidate index can be sent to
a number of other machines. Each of those machines also
receives a fraction of the input data from the dirty database.
With the help of the generative model and the input data, it
can clean the tuples, and then create the output.
If we express this in Map-Reduce terminology, we will
have a pre-processing step where we create the generative
and error models. The Map-Reduce architecture will have
only mappers, and no reducers. The result of the mapping
will be the tuple hT, T ‚àó i.
The problem with this approach is that in a truly big
data scenario, the candidate index can become very large.
Indeed, as the number of tuples increases, the size of the
domain of each attribute also increases (see Figure 8a for
1 shard). Further, the number of different combinations,
and the number of erroneous values for each attribute also
increase (Figure 8b). All of this results in a rather large candidate index. Transmitting and using the entire index on each
mapper node is wasteful of both network, memory, (and
if swapped out, disk resources). Note that to create a rich
and useful data correction system, we have to accommodate
a large candidate clean-tuple set, C(T ), for every T . C(T )
roughly tracks the sample database size. If we are unable to
shard C(T ), then sharding the input data is pointless. In the
following section we endeavor to show a strategy where not
just the input, but also the index on the candidate set C(T )
can be sharded across machines.
7.2

Improved Approach

In order to split both the input tuples and the candidate
index, we use a two-stage approach. In the first stage, we
run a map-reduce that splits the problem into multiple
shards, each shard having a small fraction of the candidate
index. The second stage is a simple map-reduce that picks
the best output from stage 1 for each input tuple.
Stage 1: Given an input tuple T and a set of candidate
tuples, the T ‚àó s, suppose the candidate index is created on k
attributes, A1 ...Ak . We can say that for every tuple T , and
one of its candidate tuples T ‚àó , they will have at least one
matching attribute ai from this set. We can use this common
element ai to predict which shards the candidate T ‚àó s might
be available in. We therefore, send the tuple T to each shard
that matches the hash of the value ai .
In the map-reduce architecture, it is possible to define
a ‚Äòpartition‚Äô function. Given a mapped key-value pair, this
function determines which reducer nodes will process the
data. We can use an exact equivalence on each value that
the matching attribute can take, ai as the partition function.
However, notice that the number of possible values that
A1 ...Ak can take is rather large. If we naƒ±Ãàvely use ai as
the partition function, we will have to create those many
reducer nodes. Therefore, more generally, we hash this value
into a fixed number of reducer nodes, using a deterministic
hash function. This will then find all candidate tuples that
are eligible for this tuple, compute the similarity, and output
it.

10

Example: Suppose we have tuple T1 that has values
(a1 , a2 , a3 , a4 , a5 ). Suppose our candidate index is created
on attributes A1 , A2 , A4 . This means that any candidates T ‚àó
that are eligible for this tuple have to match one of the values
a1 , a2 or a4 . Then the mapper will create the pairs (a1 , T ),
(a2 , T ) and (a4 , T ), and send to the reducers. The partition
function is the hash of the key - so in this case, the first one
will be sent to the reducer number hash(A1 = a1), the second will be sent to the reducer numbered hash(A2 = a2),
and so on.
In the reducer, the similarity computation and computation of the prior probabilities from the BayesWipe algorithm
are run. Since each reducer only has a fraction of the candidate index (the part that matches A1 = a1, for instance), it
can hold it in memory and computation is quite fast. Each
‚àó
reducer produces a pair (T1 , (T1n
, score)). Since there are
several candidate clean tuples, n is used to identify a specific
tuple among those alternatives.
Stage 2: This stage is a simple max calculation. The
mapper does nothing, it simply passes on the key-value
‚àó
pair (T1 , (T1n
, score)) that was generated in the previous
Map-Reduce job. Notice that the key of this pair is the
original, dirty tuple T1 . The Map-Reduce architecture thus
automatically groups together all the possible clean versions
of T1 along with their scores. The reducer picks the best
T* based on the score (using a simple max function), and
outputs it to the database.

Index
Fragment

Reducer
(a, T)
Mapper

Index
Fragment

Reducer
(T, (T*, score))
Index
Fragment

Mapper

Input Data
Fragments

Reducer
Index
Fragment

Reducer

Fig. 5: Stage-1 Map-Reduce Framework for BayesWipe.
7.3

Results of This Strategy

In Figure 8a and Figure 8b we can see how this map
reduce strategy helps in reducing the memory footprint
of the reducer. First, we plot the size of the index that
needs to be held in each node as the number of tuples in
the input increases. The topmost curve shows the size of
index in bytes if there was no sharding - as expected, it
increases sharply. The other curves show how the size of
the index in the one of the nodes varies for the same dataset
sizes. From the graph, it can be seen that as the number of
tuples increases, the size of the index grows at a lower rate
when the number of shards is increased. This shows that
increasing the number of reduce nodes is a credible strategy
for distributing the burden of the index.
In the second figure (Figure 8b), we see how the size
of the index varies with the percentage of noise in the
dataset. As expected, when the noise increases, the number

of possible candidate tuples increase (since there are more
variations of each attribute value in the pool). Without
sharding, we see that the size of the dataset increases. While
the increase in the size of the index is not as sharp as the
increase due to the size of the dataset, it is still significant.
Once again, we observe that as the number of shards is
increased, the size of the index in the shard reduces to a
much more manageable value.
Note that a slight downside to this architecture is that a
shuffle/reduce of (T, (Tn‚àó , score)) is needed in the second
stage, and this intermediate data structure can be quite
large. While this leads to some network and temporary
storage overhead, the primary objective of sharding the expensive computation has been achieved by this architecture.

8

E MPIRICAL E VALUATION

We quantitatively study the performance of BayesWipe in
both its modes ‚Äî offline, and online, and compare it against
state-of-the-art CFD approaches. We present experiments
on evaluating the approach in terms of the effectiveness of
data cleaning, efficiency and precision of query rewriting.
A demo for the offline cleaning mode of BayesWipe can be
downloaded from http://bayeswipe.sushovan.de/.
8.1

Datasets

To perform the experiments, we obtained the real data from
the web. The first dataset is Used car sales dataset Dcar
crawled from Google Base. Such ‚Äúdirty‚Äù dataset is referred
0
‚Äù. The second dataset we used was adapted
to as ‚ÄúDcar
from the Census Income dataset Dcensus from the UCI machine learning repository [43]. From the fourteen available
attributes, we picked the attributes that were categorical
in nature, resulting in the following 8 attributes: workingclass, education, marital status, occupation, race, gender,
filing status. country. The same setup was used for both
datasets ‚Äì including parameters and error features.
These datasets were observed to be mostly clean. We
then introduced4 three types of noise to the attributes. To
add noise to an attribute, we randomly changed it either to
a new value which is close in terms of string edit distance
(distance between 1 and 4, simulating spelling errors) or to
a new value which was from the same attribute (simulating
replacement errors) or just delete it (simulating deletion errors). As we have mentioned before, one of the assumptions
of this paper is that the error model is a combination of these
three kinds of errors, and that the errors are independent
of each other. By synthetically generating these errors, we
were able to test our system against a dataset that satisfies
the assumption.
The next dataset tests our system against a real-world
scenario where we do not control the error process, and thus
validates that this assumption was not unrealistic.
To test our system against real-world noise where we
do not have any control over amount, type or behavior
of the noise generation process, we crawled car inventory
data from the website ‚Äòcars.com‚Äô. We manually verified that
the data obtained did, in fact, have a reasonable number of
inaccuracies, making it a suitable candidate for testing our
system.
4. We note that the introduction of synthetic errors into clean data for
experimental evaluation purposes is common practice in data cleaning
research [16], [23].

11

8.2

Experiments

Offline Cleaning Evaluation: The first set of evaluations
shows the effectiveness of the offline cleaning mode. In
Figure 6a, we compare BayesWipe against CFDs [44].
The dotted line that shows the number of CFDs learned
from the noisy data quickly falls to zero, which is not surprising: CFDs learning was designed with a clean training
dataset in mind. Further, the only constraints learned by
this algorithm are the ones that have not been violated in
the dataset ‚Äî unless a tuple violates some CFD, it cannot
be cleaned. As a result, the CFD method cleans exactly
zero tuples independent of the noise percentage. On the
other hand, BayesWipe is able to clean between 20% to
40% of the incorrect values. It is interesting to note that
the percentage of tuples cleaned increases initially and then
slowly decreases, because for very low values of noise, there
aren‚Äôt enough errors for the system to learn a reliable error
model from; and at larger values of noise, the data source
model learned from the noisy data is of poorer quality.
While Figure 6a showed only percentages, in Figure 6b
we report the actual number of tuples cleaned in the dataset
along with the percentage cleaned. This curve shows that
the raw number of tuples cleaned always increases with
higher input noise percentages.
Setting Œ≥ : As explained in Section 5.1, the weight given
to the edit distance (Œ¥ ) compared to the weight given to
the distributional similarity (1 ‚àí Œ¥ ); and the overcorrection
parameter (Œ≥ ) are parameters that can be tuned, and should
be set based on which kind of error is more likely to occur. In
our experiments, we performed a grid search to determine
the best values of Œ¥ and Œ≥ to use. In Figure 6c, we show a
portion of the grid search where Œ¥ = 2/5, and varying Œ≥ .
The ‚Äúvalues corrected‚Äù data points in the graph correspond to the number of erroneous attribute values that
the algorithm successfully corrected (when checked against
the ground truth). The ‚Äúfalse positives‚Äù are the number of
legitimate values that the algorithm changes to an erroneous
value. When cleaning the data, our algorithm chooses a
candidate tuple based on both the prior of the candidate as
well as the likelihood of the correction given the evidence.
Low values of Œ≥ give a higher weight to the prior than
the likelihood, allowing tuples to be changed more easily
to candidates with high prior. The ‚Äúoverall gain‚Äù in the
number of clean values is calculated as the difference of
clean values between the output and input of the algorithm.
If we set the parameter values too low, we will correct
most wrong tuples in the input dataset, but we will also
‚Äòovercorrect‚Äô a larger number of tuples. If the parameters are
set too high, then the system will not correct many errors
‚Äî but the number of ‚Äòovercorrections‚Äô will also be lower.
Based on these experiments, we picked a parameter value
of Œ¥ = 0.638, Œ≥ = 5.8 and kept it constant throughout.
Using probabilistic databases: We empirically evaluate the
PDB-mode of BayesWipe in Figure 7. In the first figure,
we show the performance of the PDB mode of BayesWipe against the deterministic mode for specific queries.
As we can see from the first, third and seventh queries,
the BayesWipe-PDB improves the recall without any loss
of precision. However, in most cases (and on average),
BayesWipe-PDB provides a better recall at the cost of some
precision.

The second figure shows the performance of BayesWipePDB as the probability threshold for inclusion of a tuple in
the resultset is varied. As expected, with low values of the
threshold, the system allows most tuples into the resultset,
thus showing high recall and low precision. As the threshold
increased, the precision increases, but the recall falls.
In Figure 7c, we compare the precision of the PDB mode
using top-k determinization against the deterministic mode
of BayesWipe. As expected, both the modes show high precision for low values of k , indicating that the initial results
are clean and relevant to the user. For higher values of k ,
the PDB precision falls off, indicating that PDB methods
are more useful for scenarios where high recall is important
without sacrificing too much precision.
Online Query Processing: While in the offline mode, we
had the luxury of changing the tuples in the database itself,
in online query processing, we use query rewriting to obtain
a resultset that is similar to the offline results, without
modification to the database. We consider an SQL select
query system as our baseline. We evaluate the precision and
recall of our method against the ground truth and compare
it with the baseline, using randomly generated queries.
We issued randomly generated queries to both BayesWipe and the baseline system. Figure 8c shows the average
precision over 10 queries at various recall values. It shows
that our system outperforms the SQL select query system
in top-k precision, especially since our system considers the
relevance of the results when ranking them. On the other
hand, the SQL search approach is oblivious to ranking and
returns all tuples that satisfy the user query. Thus it may
return irrelevant tuples early on, leading to less precision.
Figure 8d shows the improvement in the absolute numbers of tuples returned by the BayesWipe system. The graph
shows the number of true positive tuples returned (tuples
that match the query results from the ground truth) minus
the number of false positives (tuples that are returned but do
not appear in the ground truth result set). We also plot the
number of true positive results from the ground truth, which
is the theoretical maximum that any algorithm can achieve.
The graph shows that the BayesWipe system outperforms
the SQL query system at nearly every level of noise. Further,
the graph also illustrates that ‚Äî compared to an SQL query
baseline ‚Äî BayesWipe closes the gap to the maximum
possible number of tuples to a large extent. In addition to
showing the performance of BayesWipe against the SQL
query baseline, we also show the performance of BayesWipe
without the query relaxation part (called BW-exp5 ). We can
see that the full BayesWipe system outperforms the BW-exp
system significantly, showing that query relaxation plays an
important role in bringing relevant tuples to the resultset,
especially for higher values of noise.
This shows that our proposed query ranking strategy indeed captures the expected relevance of the to-be-retrieved
tuples, and the query rewriting module is able to generate
the highly ranked queries.
Efficiency: In Figure 10 we show the data cleaning time
taken by the system in its various modes. The first two
graphs show the offline mode, and the second two show the
5. BW-exp stands for BayesWipe-expanded, since the only query
rewriting operation done is query expansion.

CFD

#CFDs

40%

4

30%
20%

2

10%
0%

40%

30%

2000

20%

1000

10%

0

5 10 15 20 25 30 35 40
Noise Percent

4

5

200

Values Corrected

160

False Positives

120

Cleanliness Gain

80
40
0
2

0%
3

(a) % performance of BayesWipe compared to CFD, for the used-car dataset.

50%

3000

0

0

Net Tuples Cleaned
Percent Cleaned

4000

6

Number of tuples

BayesWipe

50%

Num CFDs learned

% Tuples Cleaned

12

2.5

3

3.5

4

4.5

5

5.5

6

Distributional Similarity Weight

10 15 20 25 30 35

Percentage of noise

(b) % net corrupt values cleaned, car
database

(c) Net corrections vs Œ≥ . (The x-axis values show the un-normalized distributional similarity weight, which is simply
Œ≥ √ó 3/5.)

1
0.75
0.5
0.25
0
1
0.75
0.5
0.25
0

1

1

0.8

0.8

0.6

PDB Precision

0.4

PDB Recall

0.2

model =
outlander
sports

cartype =
sedan

make = bmw & model = jetta
condition =
used

BayesWipe-PDB Precision

model =
cooper s

model = h3
mini

Average

BayesWipe-DET Precision

(a) Precision and recall of the PDB method shown against the deterministic method for specific queries.

0.6
Deterministic Precision
PDB Precision

0.4

0.2

0

make = acura

Precision

PRECISION

RECALL

Fig. 6: Offline cleaning mode of BayesWipe

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Threshold

0
0

25

50
top-K

75

100

(b) Precision and recall of
(c) top-k precision of PDB
the PDB method using a
vs deterministic method.
threshold.

Fig. 7: Results of probabilistic method.
online mode. As can be seen from the graphs, BayesWipe
performs reasonably well both in datasets of large size and
datasets with large noise.
Evaluation on real data with naturally occurring errors: In
this section we used a dataset of 1.2 million tuples crawled
from the cars.com website6 to check the performance of
the system with real-world data, where the corruptions
were not synthetically introduced. Since this data is large,
and the noise is completely naturally occurring, we do not
have ground truth for this data. To evaluate this system,
we conducted an experiment on Amazon Mechanical Turk.
First, we ran the offline mode of BayesWipe on the entire database. We then picked only those tuples that were
changed during the cleaning, and then created an interface
in mechanical turk where only those tuples were shown to
the user in random order. Due to resource constraints, the
experiment was run with the first 200 tuples that the system
found to be unclean. We inserted 3 known answers into
the questionnaire, and removed any responses that failed
to annotate at least 2 out of the 3 answers correctly.
An example is shown in Figure 11. The turker is presented with two cars, and she does not know which of
the cars was originally present in the dirty dataset, and
which one was produced by BayesWipe. The turker will use
her own domain knowledge, or perform a web search and
discover that a Mazda CX-9 touring is only available in a 3.7l
engine, not a 3.5l. Then the turker will be able to declare the
second tuple as the correct option with high confidence.
6. http://www.cars.com

The results of this experiment are shown in Table 4. As
we can see, the users consistently picked the tuples cleaned
by BayesWipe more favorably compared to the original
dirty tuples, proving that it is indeed effective in real-world
datasets. Notice that it is not trivial to obtain a 56% rate of
success in these experiments. Finding a tuple which convinces the turkers that it is better than the original requires
searching through a huge space of possible corrections. An
algorithm that picks a possible correction randomly from
this space is likely to get a near 0% accuracy.
The first row of Table 4 shows the fraction of tuples for
which the turkers picked the version cleaned by BayesWipe
and indicated that they were either ‚Äòvery confident‚Äô or
‚Äòconfident‚Äô. The second row shows the fraction of tuples for
all turker confidence values, and therefore is a less reliable
indicator of success.
In order to show the efficacy of BayesWipe we also
performed an experiment in which the same tuples (the
ones that BayesWipe had changed) were modified by a random perturbation. The random perturbation was done by
the same error process as described before (typo, deletion,
substitution with equal probability). Then these tuples (the
original tuple from the database and the perturbed tuple)
were presented as two choices to the turkers. The preference
by the turkers for the randomly perturbed tuple over the
original dirty tuple is shown in the third column, ‚ÄòRandom‚Äô.
It is obvious from this that the turkers overwhelmingly do
not favor the random perturbed tuples. This demonstrates
two things. First, it shows the fact that BayesWipe was
performing useful cleaning of the tuples. In fact, BayesWipe
shows a tenfold improvement over the random perturbation

13
1600000

none

2

3

4

5

300000
none

2

3

4

5

250000

1200000

Bytes in one shard

Bytes in one shard

1400000

1000000

800000
600000
400000

200000
150000
100000

200000

50000

0
1

20

40

80
100
number of tuples

120

0

140

0

5

10

15
20
25
Noise percentage

30

35

40

(a) vs the Number of Tuples (in
(b) vs the Noise in the Dataset, for
Thousands) in the Dataset, for VarVarious Number of Shards.
ious Number of Shards.

Fig. 8: Map-Reduce index sizes
1000

Precision

1.00

980
960

.95
SQL Select Query

.90

940
920

BayesWipe Online
Query Processing

.85
.01

.03

.05

.07 .09
Recall

.11

900
1

.13

2

BW

3

4

BW-exp

5

10 15 20 25 30 35
Noise %
SQL
Ground truth

(d) Net improvement in data quality
(TP-FP)

(c) Average precision vs recall

150

car-offline

800
600

census

400

car

200

100

10k 15k 20k 25k
Number of tuples

30k

car-online

census-online

50

0

10
20
30
Percentage of noise

census-online
50
0

5k

40

car-online

100

0

0

5k

150
Time taken (s)

census-offline

Time Taken (s)

1000
Time taken (s)

Time Taken (s)

Fig. 9: Online cleaning mode of BayesWipe
600
500
400
300
200
100
0

10k 15k 20k 25k
Number of tuples

30k

0

10
20
30
Percentage of noise

40

(a) Time taken vs number of (b) Time taken vs noise per- (c) Time taken vs number of (d) Time taken vs noise pertuples in offline mode
centage in offline mode
tuples in online mode
centage in online mode

Fig. 10: Performance evaluations
Confidence
High confidence only
All
confidence
values

BayesWipe

Original

Random

56.3%

43.6%

5.5%

53.3%

46.7%

12.4%

Increase over Random
50.8% points
...
(10x better)

make

model

cx-9
Car: mazda
touring

40.9% points
(4x better)

cx-9
Car: mazda
touring

TABLE 4: Results of the Mechanical Turk Experiment, showing the percentage of tuples for which the users picked the
results obtained by BayesWipe as against the original tuple.
Also shows performance against a random modification.

cartype fueltype

engine

transmission

drivetrain doors wheelbase

suv

3.5l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113‚Äù

suv

3.7l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113"

ÔÇ° First is correct
ÔÇ° Second is correct
How confident are you about your selection?
ÔÇ° Very confident ÔÇ° Confident ÔÇ° Slightly confident ÔÇ° Slightly Unsure ÔÇ° Totally Unsure

Fig. 11: A fragment of the questionnaire provided to the
Mechanical Turk workers.
model, as judged by human turkers. This shows that in
the large space of possible modifications of a wrong tuple,
BayesWipe picks the correct one most of the time. Second, it
provides additional support for the fact that the turkers are
picking the tuple carefully, and are not randomly submitting
their responses.
In this experiment, we also found the average fraction
of known answers that the turkers gave wrong answers to.
This value was 8%. This leads to the conclusion that the
difference between the turker‚Äôs preference of BayesWipe
over both the original tuples (which is 12%) and the random
perturbation (which is 50%) are both significant.

9

C ONCLUSION

In this paper we presented a novel system, BayesWipe
that works using end-to-end probabilistic semantics, and
without access to clean master data. We showed how to
effectively learn the data source model as a Bayes network,
and how to model the error as a mixture of error features.
We showed the operation of this system in two modalities:
(1) offline data cleaning, an in situ rectification of data and
(2) online query processing mode, a highly efficient way to
obtain clean query results over inconsistent data. There is
an option to generate a standard, deterministic database as
the output, as well as a probabilistic database, where all

14

the alternatives are preserved for further processing. We
empirically showed that BayesWipe outperformed existing
baseline techniques in quality of results, and was highly
efficient. We also showed the performance of the BayesWipe
system at various stages of the query rewriting operation.
We demonstrated how BayesWipe can be run on the mapreduce architecture so that it can scale to huge data sizes.
User experiments showed that the system is useful in cleaning real-world noisy data.

R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

W. Fan and F. Geerts, ‚ÄúFoundations of data quality management,‚Äù
Synthesis Lectures on Data Management, 2012.
P. Gray, ‚ÄúBefore Big Data, clean data,‚Äù 2013. [Online]. Available:
http://www.techrepublic.com/blog/big-data-analytics/
before-big-data-clean-data/
H. Leslie, ‚ÄúHealth data quality ‚Äì a two-edged sword,‚Äù 2010.
[Online]. Available: http://omowizard.wordpress.com/2010/02/
21/health-data-quality-a-two-edged-sword/
Computing Research Association, ‚ÄúChallenges and opportunities
with big data,‚Äù http://cra.org/ccc/docs/init/bigdatawhitepaper.
pdf, 2012.
E. Knorr, R. Ng, and V. Tucakov, ‚ÄúDistance-based outliers: algorithms and applications,‚Äù VLDB, 2000.
H. Xiong, G. Pandey, M. Steinbach, and V. Kumar, ‚ÄúEnhancing
data analysis with noise removal,‚Äù TKDE, 2006.
P. Singla and P. Domingos, ‚ÄúEntity resolution with markov logic,‚Äù
in ICDM, 2006.
I. Fellegi and D. Holt, ‚ÄúA systematic approach to automatic edit
and imputation,‚Äù J. American Statistical association, 1976.
P. Bohannon, W. Fan, M. Flaster, and R. Rastogi, ‚ÄúA cost-based
model and effective heuristic for repairing constraints by value
modification,‚Äù in SIGMOD, 2005.
W. Fan, F. Geerts, L. Lakshmanan, and M. Xiong, ‚ÄúDiscovering
conditional functional dependencies,‚Äù ICDE, 2009.
L. E. Bertossi, S. Kolahi, and L. V. S. Lakshmanan, ‚ÄúData cleaning
and query answering with matching dependencies and matching
functions,‚Äù ICDT, 2011.
A. Fuxman, E. Fazli, and R. J. Miller, ‚ÄúConquer: Efficient management of inconsistent databases,‚Äù SIGMOD, 2005.
S. De, Y. Hu, Y. Chen, and S. Kambhampati, ‚ÄúBayeswipe: A multimodal system for data cleaning and consistent query answering
on structured bigdata,‚Äù IEEE Big Data, 2014.
G. Wolf, A. Kalavagattu, H. Khatri, R. Balakrishnan, B. Chokshi,
J. Fan, Y. Chen, and S. Kambhampati, ‚ÄúQuery processing over
incomplete autonomous databases: query rewriting using learned
data dependencies,‚Äù VLDB, 2009.
S. De, ‚ÄúUnsupervised bayesian data cleaning techniques for structured data,‚Äù Ph.D. dissertation, ASU, 2014.
P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis,
‚ÄúConditional functional dependencies for data cleaning,‚Äù ICDE,
2007.
W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, ‚ÄúConditional functional dependencies for capturing data inconsistencies,‚Äù
TODS, 2008.
J. Wang and N. Tang, ‚ÄúTowards dependable data repairing with
fixing rules,‚ÄùSIGMOD, 2014.
J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg, T. Kraska, and
T. Milo, ‚ÄúA sample-and-clean framework for fast and accurate
query processing on dirty data,‚Äù SIGMOD, 2014.
L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu, ‚ÄúOn
generating near-optimal tableaux for conditional functional dependencies,‚Äù PVLDB, 2008.
G. Cormode, L. Golab, K. Flip, A. McGregor, D. Srivastava, and
X. Zhang, ‚ÄúEstimating the confidence of conditional functional
dependencies,‚Äù in SIGMOD, 2009.
M. Arenas, L. Bertossi, and J. Chomicki, ‚ÄúConsistent query answers in inconsistent databases,‚Äù PODS, 1999.
G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma, ‚ÄúImproving data
quality: Consistency and accuracy,‚Äù VLDB, 2007.
M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F.
Ilyas, ‚ÄúGuided data repair,‚Äù VLDB, 2011.
G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúSampling from
repairs of conditional functional dependency violations,‚Äù VLDB,
2013.

[26] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, ‚ÄúTowards certain fixes
with editing rules and master data,‚Äù VLDB, 2010.
[27] G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúOn the relative
trust between inconsistent data and inaccurate constraints,‚Äù ICDE,
2013.
[28] X. L. Dong, L. Berti-Equille, and D. Srivastava, ‚ÄúTruth discovery
and copying detection in a dynamic world,‚Äù VLDB,2009.
[29] T. Dasu and J. M. Loh, ‚ÄúStatistical distortion: Consequences of data
cleaning,‚Äù VLDB, 2012.
[30] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and
Y. Ye, ‚ÄúKatara: A data cleaning system powered by knowledge
bases and crowdsourcing,‚Äù SIGMOD, 2015.
[31] J. Wang, T. Kraska, M. J. Franklin, and J. Feng, ‚ÄúCrowder: Crowdsourcing entity resolution,‚Äù VLDB, 2012.
[32] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng, ‚ÄúQasca: A qualityaware task assignment system for crowdsourcing applications,‚Äù
SIGMOD, 2015.
[33] A. Hartemink., ‚ÄúBanjo: Bayesian network inference with java
objects.‚Äù http://www.cs.duke.edu/‚àºamink/software/banjo.
[34] S. Russell and P. Norvig, Artificial intelligence: a modern approach.
Prentice Hall, 2010.
[35] T. Minka, W. J.M., J. Guiver, and D. Knowles, ‚ÄúInfer.NET 2.4‚Äù,
Microsoft Research Cambridge, 2010. http://research.microsoft.
com/infernet.
[36] E. Ristad and P. Yianilos, ‚ÄúLearning string-edit distance,‚Äù Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 1998.
[37] M. Li, Y. Zhang, M. Zhu, and M. Zhou, ‚ÄúExploring distributional
similarity based models for query spelling correction,‚Äù ICCL, 2006.
[38] A. Berger, V. Pietra, and S. Pietra, ‚ÄúA maximum entropy approach
to natural language processing,‚Äù Computational linguistics, 1996.
[39] N. Dalvi and D. Suciu, ‚ÄúEfficient query evaluation on probabilistic
databases,‚Äù VLDB, 2004.
[40] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, and D. Suciu,
‚ÄúMystiq: a system for finding more answers by using probabilities,‚Äù SIGMOD, 2005.
[41] D. Suciu and N. Dalvi, ‚ÄúFoundations of probabilistic answers to
queries,‚Äù SIGMOD, 2005.
[42] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann Publishers, 1988.
[43] A. Asuncion and D. Newman, ‚ÄúUCI machine learning repository,‚Äù
2007.
[44] F. Chiang and R. Miller, ‚ÄúDiscovering data quality rules,‚Äù VLDB,
2008.
Sushovan De is currently employed at Google. He graduated with a
Ph.D. in Computer Science from Arizona State University. His research
interests comprise Information Retrieval, Data Cleaning, and Probabilistic Databases.
Yuheng Hu is a Research Staff Member in the USER (User Systems
and Experience Research) group at IBM Research - Almaden. He
obtained his Ph.D in Computer Science at Arizona State University.
His research interests are in the areas of Machine Learning, Social
Computing and Human-Computer Interaction.
Meduri Venkata Vamsikrishna is a Ph.D student in Computer Science
at Arizona State University. He received a Master‚Äôs degree in Computer
Science from the National Unviersity of Singapore. His research interests include Data Mining from Social Media and Data Cleaning.
Yi Chen is an associate professor and the Henry J. Leir Chair in
Healthcare in the School of Management with a joint appointment in the
College of Computing Sciences at New Jersey Institute of Technology
(NJIT). She received her Ph.D. degree in Computer Science from the
University of Pennsylvania. Her current research focuses on Information
Discovery on Big Data, Social Computing and Information Integration.
Subbarao Kambhampati is a professor in Computer Science at Arizona
State University. He directs the Yochan research group which is associated with the Artifical Intelligence Lab at Arizona State University. He
is the ‚ÄùPresident-elect‚Äù of AAAI, the Association for the Advancement of
Artificial Intelligence. He secured a Ph.D. degree in Computer Science
from the University of Maryland, College Park. His research interests are
Automated Planning in Artificial Intelligence and Data and Information
Integration on the Web

1

BayesWipe: A Scalable Probabilistic Framework
for Cleaning BigData

arXiv:1506.08908v1 [cs.DB] 30 Jun 2015

Sushovan De, Yuheng Hu, Meduri Venkata Vamsikrishna, Yi Chen, and Subbarao Kambhampati
Abstract‚ÄîRecent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record
matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in
tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like CFDs
(which have to be provided by domain experts, or learned from a clean sample of the database). In this paper, we provide a method for
correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned
from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently
perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable.
We evaluate our methods over both synthetic and real data.
Index Terms‚Äîdatabases; web databases; data cleaning; query rewriting; uncertainty

F

1

I NTRODUCTION

A

LTHOUGH data cleaning has been a long standing
problem, it has become critically important again because of the increased interest in big data and web data.
Most of the focus of the work on big data has been on
the volume, velocity, or variety of the data; however, an
important part of making big data useful is to ensure the
veracity of the data. Enterprise data is known to have a
typical error rate of 1‚Äì5% [1] (error rates of up to 30% have
been observed). This has led to renewed interest in cleaning
of big data sources, where manual data cleansing tasks are
seen as prohibitively expensive and time-consuming [2],
or the data has been generated by users and cannot be
implicitly trusted [3]. Among the various types of big data,
the need to efficiently handle large scaled structured data
that is rife with inconsistency and incompleteness is also
more significant than ever. Indeed, multiple studies, such as
[4] emphasize the importance of effective, efficient methods
for handling ‚Äúdirty big data‚Äù.

TABLE 1: A snapshot of car data extracted from cars.com
using information extraction techniques
TID
t1
t2
t3
t4
t5
t6

Model
Civic
Focus
Civik
Civic
Accord

Make
Honda
Ford
Honda
Ford
Honda
Honda

Orig
JPN
USA
JPN
USA
JPN
JPN

Size
Mid-size
Compact
Mid-size
Compact
Mid-size
Full-size

Engine
V4
V4
V4
V4
V4
V6

Condition
NEW
USED
USED
USED
NEW
NEW

Most of the current techniques are based on deterministic rules, which have a number of problems: Suppose that
the user is interested in finding ‚ÄòCivic‚Äô cars from Table 1.
‚Ä¢

This work was done when all the authors were with the Department of
Computer Science & Engineering at Arizona State University, Tempe,
AZ 85287. Sushovan De is now with Google Inc. Yuheng Hu is now with
IBM Research, Almaden. Yi Chen is now with the School of Management
and the College of Computing at New Jersey Institute of Technology.

Traditional data retrieval systems would return tuples t1
and t4 for the query, because they are the only ones that
are a match for the query term. Thus, they completely miss
the fact that t4 is in fact a dirty tuple ‚Äî A Ford Focus car
mislabeled as a Civic. Additionally, tuple t3 and t5 would
not be returned as a result tuples since they have a typos
or missing values, although they represent desirable results.
The objective of this work is to provide the true result set
(t1 , t3 , t5 ) to the user.
Although this problem has received significant attention
over the years in the traditional database literature, the stateof-the-art approaches fall far short of an effective solution
for big data and web data. Traditional methods include
outlier detection [5], noise removal [6], entity resolution [6],
[7], and imputation [8]. Although these methods are efficient
in their own scenarios, their dependence on clean master
data is a significant drawback.
Specifically, state of the art approaches (e.g., [9], [10],
[11]) attempt to clean data by exploiting patterns in the data,
which they express in the form of conditional functional
dependencies (or CFDs). In the motivating example, the fact
that Honda cars have ‚ÄòJPN‚Äô as the origin of the manufacturer
would be an example of such a pattern. However, these
approaches depend on the availability of a clean data corpus
or an external reference table to learn data quality rules or
patterns before fixing the errors in the dirty data. Systems
such as ConQuer [12] depend upon a set of clean constraints
provided by the user. Such clean corpora or constraints
may be easy to establish in a tightly controlled enterprise
environment but are infeasible for web data and big data.
One may attempt to learn data quality rules directly from
the noisy data. Unfortunately however, our experimental
evaluation shows that even small amounts of noise severely
impairs the ability to learn useful constraints from the data.
To avoid dependence on clean master data, we propose
a novel system called BayesWipe [13] that assumes that
a statistical process underlies the generation of clean data
(which we call the data source model) as well as the cor-

2

ruption of data (which we call the data error model). The
noisy data itself is used to learn the parameters of these
the generative and error models, eliminating dependence
on clean master data. Then, by treating the clean value as
a latent random variable, BayesWipe leverages these two
learned models and automatically infers its value through a
Bayesian estimation.
We designed BayesWipe so that it can be used in two different modes: a traditional offline cleaning mode, and a novel
online query processing mode. The offline cleaning mode of
BayesWipe follows the classical data cleaning model, where
the entire database is accessible and can be cleaned in situ.
This mode is particularly useful when one has complete
control over the data, and a one-time cleaning of the data is
needed. Data warehousing scenarios such as data crawled
from the web, or aggregated from various noisy sources
can be effectively cleaned in this mode. The cleaned data
can be stored either in a deterministic database, or in a
probabilistic database. If a probabilistic database is chosen
as the output mode, BayesWipe stores not only the clean
version of the tuple it believes to be most likely correct
one, but the entire distribution over possible clean tuples.
The choice of a probabilistic output mode for the cleaned
tuples is most useful for those scenarios where recall is very
important for further data processing on the cleaned tuples.
One of the features of the offline mode of BayesWipe
is that a probabilistic database (PDB) can be generated as
a result of the data cleaning. In the first instance, notice
that BayesWipe was built for deterministic databases. It
can operate on a deterministic database and produce a
probabilistic cleaned database as an output. Probabilistic
databases are complex and unintuitive, because each single
input tuple is mapped into a distribution over resulting
clean alternatives. We show how the top-k results can be
retrieved from a PDB while displaying the clean data that is
comprehensible to the user.
The online query processing mode of BayesWipe is motivated by web data scenarios where it is impractical to create
a local copy of the data and clean it offline, either due to
large size, high frequency of change, or access restrictions. In
such cases, the best way to obtain clean answers is to clean
the resultset as we retrieve it, which also provides us the
opportunity of improving the efficiency of the system, since
we can now ignore entire portions of the database which are
likely to be unclean or irrelevant to the top-k . BayesWipe
uses a query rewriting system that enables it to efficiently
retrieve only those tuples that are important to the top-k
result set. This rewriting approach is inspired by, and is a
significant extension of our earlier work on QPIAD system
for handling data incompleteness [14]. In big data scenarios,
clean master data is rarely available, and write access is
either unavailable, or undesirable due to the efficiency and
indexing concerns. The online mode is particularly suited to
get clean results in such results.
We implement BayesWipe in a Map-Reduce architecture,
so that we can run it very quickly for massive datasets. The
architecture for parallelizing BayesWipe is explained more
fully in Sec 7. In short, there is a two-stage map-reduce
architecture, where in the first stage, the dirty tuples are
routed to a set of reducer nodes which hold the relevant
candidate clean tuples for them. In the second stage, the

resulting candidate clean tuples along with their scores are
collated, and the best replacement tuple is selected from
them.
To summarize our contributions, we:
‚Ä¢
‚Ä¢

‚Ä¢
‚Ä¢
‚Ä¢

Propose that data cleaning should be done using a
principled, probabilistic approach.
Develop a novel algorithm following those principles, which uses a Bayes network as the generative
model and maximum entropy as the error model of
the data.
Develop novel query rewriting techniques so that
this algorithm can also be used in a big data scenario.
Develop a parallelized version of this algorithm using map-reduce framework.
Empirically evaluate the performance of our algorithm using both controlled and real datasets.

The rest of the paper is organized as follows. We begin by discussing the related work and then describe the
architecture of BayesWipe in the next section, where we
also present the overall algorithm. Section 4 describes the
learning phase of BayesWipe, where we find the generative
and error models. Section 5 describes the offline cleaning
mode, and the next section details the query rewriting
and online data processing. We describe the parallelized
version of BayesWipe in Section 7 and the results of the
our empirical evaluation in Section 8, and then conclude
by summarizing our contributions. Further details about
BayesWipe can be found in the thesis [15].

2

R ELATED W ORK

Much of the work in data cleaning focused on deterministic
dependency relations such as FD, CFD, and INDs. Bohannon et al. proposed using Conditional Functional Dependencies (CFD) to clean data [16], [17]. Indeed, CFDs are
very effective in cleaning data. However, the precision and
recall of cleaning data with CFDs completely depends on
the quality of the set of dependencies used for the cleaning.
As our experiments show, learning CFDs from dirty data
produces very unsatisfactory results. In order for CFDbased methods to perform well, they need to be learned
from a clean sample of the database [10] which must be
large enough to be representative of all the patterns in the
data. Finding such a large corpus of clean master data is a
non-trivial problem, and is infeasible in all but the most
controlled of environments (like a corporation with high
quality data).
A recent variant on the deterministic dependency based
cleaning by J.Wang et al. [18] proposes using fixing rules
which contain negative(possible errors) and positive(clean
replacements) patterns for an attribute. However, there can
be several ways in which a tuple can go wrong and the
detection of the positive pattern requires clean master data.
BayesWipe on the other hand uses an error model to detect
errors automatically and clean them in the absence of clean
master data. Recent work by J.Wang et al. [19] plugs in one
of the rule based cleaning techniques to clean a sample of
the entire data and use it as a guideline to clean the entire
data. It is important to note that this method only caters
to aggregate numerical queries whereas the online mode

3

of BayesWipe supports all types of SQL queries (not just
aggregates) and returns clean result tuples.
Although it is possible to ameliorate some of the difficulties of CFD/AFD methods by considering approximate versions of them, the work in the uncertainty in AI
community demonstrated the semantic pitfalls of handling
uncertainty in this way. In particular, approximate versions
of CFDs/AFDs considered in works such as [20], [21] are
similar to the certainty factors approaches for handling
uncertainty that were popular in the heyday of expert
systems, but whose semantic inconsistencies are by now
well-established (see, for example, Section 14.7.1 of [34]).
Because of this, in this paper we focus on a more systematic
probabilistic approach.
Even if a curated set of integrity constraints are provided, existing methods do not use a probabilistically principled method of choosing a candidate correction. They resort
to either heuristic based methods, finding an approximate
algorithm for the least-cost repair of the database [9], [22],
[23]; using a human-guided repair [24], or sampling from
a space of possible repairs [25]. There has been work that
attempts to guarantee a correct repair of the database [26],
but they can only provide guarantees for corrections of those
tuples that are supported by data from a perfectly clean
master database. Recently, [27] have shown how the relative
trust one places on the constraints and the data itself plays
into the choice of cleaning tuples. A Bayesian source model
of data was used by [28], but was limited in scope to figuring
out the evolution over time of the data value.
Recent work has also focused on the metrics to use to
evaluate data cleaning techniques [29]. In this work, we
focus on evaluating our method against the ground truth
(when the ground truth is known), and user studies (when
the ground truth is not known).
While BayesWipe uses crowdsourcing to evaluate the
accuracy of the proposed clean tuple alternatives for the
experiments on real world datasets, there are other systems that try to use the crowd for cleaning the data itself.
X.Chu et al. [30] clean the database tuples by discovering
patterns that overlap with Knowledge Base(KB)s like Yago
and validating the top-k candidates using the crowd. J.Wang
et al. [31] perform entity resolution (which is to identify
several values corresponding to the same entity value) using
crowdsourcing. They reduce the complexity of the number
of HIT(Human Intelligence Task)s generated by clustering
them into several bins so that a set of pairs can be resolved
at a time as against evaluating one pair at a time. Y.Zheng
et al. [32] pick a set of k questions to be included in the HITs
for the human workers out of a total set of n questions using
estimates on the expected increase in the answer quality by
assigning those questions to the crowd. Crowdsourcing to
perform data cleaning may be infeasible in the context of Big
Data cleaning targeted by BayesWipe . However, suggestions from the crowd can be used to provide cleaner master
data from which BayesWipe learns the Bayes network.
The query rewriting part of this work is inspired
by the QPIAD system [14], but significantly improves
upon it. QPIAD performed query rewriting over incomplete databases using approximate functional dependencies
(AFD), and only cleaned data with null values, not wrong
values.

Offline Cleaning

Model Learning
Data source
model
Error
Model

Cleaning to a
Deterministic DB
or
Cleaning to a
Probabilistic DB

Clean
Data

OR
Candidate
Set Index

Query Processing
Query
Rewriting

Database
Sampler

Result
Ranking

Data Source

Fig. 1: The architecture of BayesWipe. Our framework learns
both data source model and error model from the raw data
during the model learning phase. It can perform offline
cleaning or query processing to provide clean data.

3

BAYES W IPE OVERVIEW

BayesWipe views the data cleaning problem as a statistical
inference problem over the structured text data. Let D =
{T1 , ..., Tn } be the input structured data which contains a
number of corruptions. Ti ‚àà D is a tuple with m attributes
{A1 , ..., Am } which may have one or more corruptions in
its attribute values. Given a candidate replacement set C
for a possibly corrupted tuple T in D, we can clean the
database by replacing T with the candidate clean tuple
T ‚àó ‚àà C that has the maximum Pr(T ‚àó |T ). Using Bayes rule
(and dropping the common denominator), we can rewrite
this to
‚àó
Tbest
= arg max[Pr(T |T ‚àó )Pr(T ‚àó )]

(1)

‚àó
By replacing T with Tbest
, we get a deterministic database.
If we wish to create a probabilistic database (PDB), we don‚Äôt
take an arg max over the Pr(T ‚àó |T ), instead we store the
entire distribution over the T ‚àó in the resulting PDB.
For online query processing we take the user query Q‚àó ,
and find the relevance score of a tuple T as
X
Score(T ) =
Pr(T ‚àó ) Pr(T |T ‚àó ) R(T ‚àó |Q‚àó )
(2)
| {z } | {z } | {z }
‚àó
T ‚ààC

source model error model

relevance

In this work, we used a binary relevance model, where R
is 1 if T ‚àó is relevant to the user‚Äôs query, and 0 otherwise.
Note that R is the relevance of the query Q‚àó to the candidate
clean tuple T ‚àó and not the observed tuple T . This allows the
query rewriting phase of BayesWipe which aims to retrieve
tuples with the highest Score(.) to achieve the non-lossy
effect of using a PDB without explicitly rectifying the entire
database.
Architecture:
Figure 1 shows the system architecture for BayesWipe.
During the model learning phase (Section 4), we first obtain
a sample database by sending some queries to the database.
On this sample data, we learn the generative model of the
data as a Bayes network (Section 4.1). In parallel, we define
and learn an error model which incorporates common kinds

4

of errors (Section 4.2). We also create an index to quickly
propose candidate T ‚àó s.
We can then choose to do either offline cleaning (Section 5) or online query processing (Section 6), as per the
scenario. In the offline cleaning mode, we iterate over all
the tuples in the database and clean them one by one. We
can choose whether to store the resulting cleaned tuple in a
deterministic database (where we store only the T ‚àó with the
maximum posterior probability) or probabilistic database
(where we store the entire distribution over the T ‚àó ). In the
online query processing mode, we obtain a query from the
user, and do query rewriting in order to find a set of queries
that are likely to retrieve a set of highly relevant tuples.
We execute these queries and re-rank the results, and then
display them.
Algorithm 1: The algorithm for offline data cleaning
Input: D, the dirty dataset.
BN ‚Üê Learn Bayes Network (D)
foreach Tuple T ‚àà D do
C ‚Üê Find Candidate Replacements (T )
foreach Candidate T ‚àó ‚àà C do
P (T ‚àó ) ‚Üê Find Joint Probability (T ‚àó , BN )
P (T |T ‚àó ) ‚Üê Error Model (T, T ‚àó )
end
T ‚Üê arg max
P (T ‚àó )P (T |T ‚àó )
‚àó
T ‚ààC

end

Algorithm 2: Algorithm for online query processing.
Input: D, the dirty dataset
Input: Q, the user‚Äôs query
S ‚Üê Sample the source dataset D
BN ‚Üê Learn Bayes Network (S )
ES ‚Üê Learn Error Statistics (S )
R ‚Üê Query and score results (Q, D, BN )
ESQ ‚Üê Get expanded queries (Q)
foreach Expanded query E ‚àà ESQ do
R ‚Üê R ‚à™ Query and score results (E, D, BN )
RQ ‚Üê RQ ‚à™ Get all relaxed queries (E )
end
Sort(RQ) by expected relevance, using ES
while top-k confidence not attained do
B ‚Üê Pick and remove top RQ
R ‚Üê R ‚à™ Query and score results (B, D, BN )
end
Sort(R) by score
return R
In Algorithms 1 and 2, we present the overall algorithm
for BayesWipe. In the offline mode, we show how we iterate
over all the tuples in the dirty database, D and replace them
with cleaned tuples. In the query processing mode, the first
three operations are performed offline, and the remaining
operations show how the tuples are efficiently retrieved
from the database, ranked and displayed to the user.

4

M ODEL L EARNING

This section details the process by which we estimate the
components of Equation 2: the data source model Pr(T ‚àó )

Make

Condition

occupation

Model

Year

gender

workingclass

Door

Drivetrain

country

race
education

marital status

Engine

Car Type

(a) Auto dataset

filing-status

(b) Census dataset

Fig. 2: The learned Bayes networks
and the error model Pr(T |T ‚àó )
4.1

Data Source Model

The data that we work with can have dependencies among
various attributes (e.g., a car‚Äôs engine depends on its make).
Therefore, we represent the data source model as a Bayes
network, since it naturally captures relationships between
the attributes via structure learning and infers probability
distributions over values of the input tuples.
Constructing a Bayes network over D requires two steps:
first, the induction of the graph structure of the network,
which encodes the conditional independences between the
m attributes of D‚Äôs schema; and second, the estimation
of the parameters of the resulting network. The resulting
model allows us to compute probability distributions over
an arbitrary input tuple T .
Whenever the underlying patterns in the source database
changes, we have to learn the structure and parameters
of the Bayes network again. In our scenario, we observed
that the structure of a Bayes network of a given dataset
remains constant with small perturbations, but the parameters (CPTs) change more frequently. As a result, we spend a
larger amount of time learning the structure of the network
with a slower, but more accurate tool, Banjo [33]. Figures
2a and 2b show automatically learned structures for two
data domains. The learned structure seems to be intuitively
correct, since the nodes that are connected (for example,
‚Äòcountry‚Äô and ‚Äòrace‚Äô in Figure 2b) are expected to be highly
correlated1 .
Then, given a learned graphical structure G of D, we
can estimate the conditional probability tables (CPTs) that
parameterize each node in G using a faster package called
Infer.NET [35]. This process of inferring the parameters is
run offline, but more frequently than the structure learning.
Once the Bayesian network is constructed, we can infer
the joint distributions for arbitrary tuple T , which can
be decomposed to the multiplication of several marginal
distributions of the sets of random variables, conditioned
on their parent nodes depending on G .
4.2

Error Model

Having described the data source model, we now turn to
the estimation of the error model Pr(T |T ‚àó ) from noisy data.
There are many types of errors that can occur in data. We
focus on the most common types of errors that occur in data
1. Note that the direction of the arrow in a Bayes network does not
necessarily determine causality, see Chapter 14 from Russel and Norvig
[34].

5

that is manually entered by naƒ±Ãàve users: typos, deletions,
and substitution of one word with another. We also make
an additional assumption that error in one attribute does
not affect the errors in other attributes. This is a reasonable assumption to make, since we are allowing the data
itself to have dependencies between attributes, while only
constraining the error process to be independent across
attributes. With these assumptions, we are able to come up
with a simple and efficient error model, where we combine
the three types of errors using a maximum entropy model.
Given a set of clean candidate tuples C where T ‚àó ‚àà C ,
our error model Pr(T |T ‚àó ) essentially measures how clean T
is, or in other words, how similar T is to T ‚àó .
Edit distance similarity: This similarity measure is used to
detect spelling errors. Edit distance between two strings TAi
and TA‚àó i is defined as the minimum cost of edit operations
applied to dirty tuple TAi transform it to clean TA‚àó i . Edit
operations include character-level copy, insert, delete and
substitute. The cost for each operation can be modified as
required; in this paper we use the Levenshtein distance,
which uses a uniform cost function. This gives us a distance,
which we then convert to a probability using [36]:

4.3

Finding the Candidate Set

The set of candidate tuples, C(T ) for a given tuple T are
the possible replacement tuples that the system considers
as possible corrections to T . The larger the set C is, the
longer it will take for the system to perform the cleaning. If
C contains many unclean tuples, then the system will waste
time scoring tuples that are not clean to begin with.
An efficient approach to finding a reasonably clean
C(T ) is to consider the set of all the tuples in the sample
database that differ from T in not more than j attributes.
In order to find C(T ) that satisfies this, conceptually, we
have to iterate over every tuple t in the sample database
D, comparing it to the tuple T and checking how many
attributes it differs in. This operation can take O(n) time,
where n is the number of tuples in the sample database.
Even with j = 3, the naƒ±Ãàve approach of constructing C from
the sample database directly is too time consuming, since
it requires one to go through the sample database in its
entirety once for every result tuple encountered. To make
this process faster, we create indices over (j + 1) attributes
because searching through indices reduces the number of
comparisons required to compute C(T ). If any candidate
fed (TAi , TA‚àó i ) = exp{‚àícosted (TAi , TA‚àó i )}
(3) tuple T ‚àó differs from T in less than or equal to j attributes,
Distributional Similarity Feature: This similarity measure then it will be present in at least one of the indices, since
is used to detect both substitution and omission errors. we created j + 1 of them (pigeon hole principle). These
Looking at each attribute in isolation is not enough to fix j + 1 indices are created over those attributes that have the
these errors. We propose a context-based similarity measure highest cardinalities, such as Make and Model (as opposed
called Distributional similarity (fds ), which is based on the to attributes like Condition and Doors which can take only
probability of replacing one value with another under a a few values). This ensures that the set of tuples returned
similar context [37]. Formally, for each string TAi and TA‚àó i , from the index would be small in number.
For every possibly dirty tuple T in the database, we go
we have:
over
each such index and find all the tuples that match the
‚àó
X
Pr(c|TAi )Pr(c|TAi )Pr(TAi )
fds (TAi , TA‚àó i ) =
(4)corresponding attribute. The union of all these tuples is then
Pr(c)
‚àó )
examined and the candidate set C is constructed by keeping
c‚ààC(TAi ,TA
i
only those tuples from this union set that do not differ from
where C(TAi , TA‚àó i ) is the context of a tuple attribute value, T in more than j attributes. Thus we can be sure that by
which is a set of attribute values that co-occur with using this method, we have obtained the entire set C 2 .
both TAi and TA‚àó i . Pr(c|TA‚àó i ) = (#(c, TA‚àó i ) + ¬µ)/#(TA‚àó i )
is the probability that a context value c appears given
the clean attribute TA‚àó i in the sample database. Similarly, 5 O FFLINE CLEANING
P (TAi ) = #(TAi )/#tuples is the probability that a dirty
attribute value appears in the sample database. We calculate 5.1 Cleaning to a Deterministic Database
Pr(c|TAi ) and Pr(TAi ) in the same way. To avoid zero In order to clean the data in situ, we first use the techniques
estimates for attribute values that do not appear in the of the previous section to learn the data source model, the
database sample, we use Laplace smoothing factor ¬µ.
error model and create the index. Then, we iterate over all
Unified error model: In practice, we do not know before- the tuples in the database and use Equation 1 to find the
hand which kind of error has occurred for a particular T ‚àó with the best score. We then replace the tuple with that
attribute; we need a unified error model which can accom- T ‚àó , thus creating a deterministic database using the offline
modate all three types of errors (and be flexible enough to mode of BayesWipe.
accommodate more errors when necessary). For this purComputing Pr(T ‚àó )Pr(T |T ‚àó ) is very fast. Even though
pose, we use the well-known maximum entropy framework we do a Bayesian inference for Pr(T ‚àó ), the tuple has all the
[38] to leverage both the similarity measures, (Edit distance values specified, so the inference ends up being a simple
fed and distributional similarity fds ). For each attribute of multiplication over the CPTs of the Bayes network, and is
the input tuple T and T ‚àó , we have the unified error model very cheap. Pr(T |T ‚àó ) involves simple edit distance and disPr(T |T ‚àó ) given by:
tributional similarity calculations all of which involve sim( m
)
ple arithmetic operations and lookups devoid of Bayesian
m
X
X
1
exp Œ±
fed (TAi , TA‚àó i ) + Œ≤
fds (TAi , TA‚àó i )
(5) inference.
Z
i=1
i=1
where Œ± and Œ≤ are the weight of each feature, m is the
numberPof attributes
P in the tuple. The normalization factor
is Z = T ‚àó exp { i Œªi fi (T ‚àó , T )}.

2. There is a small possibility that the true tuple T ‚àó is not in the
sample database at all. This probability can be reduced by choosing
a larger sample set. In future work, we will expand the strategy of
generating C to include all possible k-repairs of a tuple.

6

Recall from Section 4.2 that there are parameters in the
error model called Œ± and Œ≤ , which need to be set. Interestingly, in addition to controlling the relative weight given to
the various features in the error model, these parameters
can be used to control overcorrection by the system.
Overcorrection: Any data cleaning system is vulnerable to
overcorrection, where a legitimate tuple is modified by the
system to an unclean value. Overcorrection can have many
causes. In a traditional, deterministic system, overcorrection
can be caused by erroneous rules learned from infrequent
data. For example, certain makes of cars are all owned by the
same conglomerate (GM owns Chevrolet). In a misguided
attempt to simplify their inventory, a car salesman might list
all the cars under the name of the conglomerate. This may
provide enough support to learn the wrong rule (Malibu ‚Üí
GM).
Typically, once an erroneous rule has been learned, there
is no way to correct it or ignore it without a lot of oversight
from domain experts. However, BayesWipe provides a way
to regulate the amount of overcorrection in the system with
the help of a ‚Äòdegree of change‚Äô parameter. Without loss of
generality, we can rewrite Equation 5 to the following:
  X
m
1
Pr(T |T ) = exp Œ≥ Œ¥
fed (TAi , TA‚àó i )
Z
i=1
‚àó

+ (1 ‚àí Œ¥)

m
X


fds (TAi , TA‚àó i )

i=1

Since we are only interested in their relative weights, the
parameters Œ± and Œ≤ have been replaced by Œ¥ and (1‚àíŒ¥) with
the help of a normalization constant, Œ≥ . This parameter, Œ≥ ,
can be used to modify the degree of variation in Pr(T |T ‚àó ).
High values of Œ≥ imply that small differences in T and
T ‚àó cause a larger difference in the value of Pr(T |T ‚àó ),
causing the system to give higher scores to the original tuple
(compared to a modified tuple).
Example: Consider the following fragment from the
database. The first tuple is a very frequent tuple in the
database, the second one is an erroneous tuple, and the third
tuple is an infrequent, correct tuple. The ‚Äòtrue‚Äô correction
of the second tuple is the third tuple. The Pr(T ‚àó ) values
shown reflect the values that the data source model might
predict for them, roughly based on the frequency with
which they occur in the source data.
Id

Make

Model Type

1
2
3

Honda Civic
Honda Z4
BMW Z4

Engine

Sedan V4
Sedan V6
Sedan V6

Condition P (T ‚àó )
New
New
New

0.400
0.001
0.005

A proper data cleaning system will correct tuple 2 to
tuple 3, and not modify any of the others. However, if
incorrect rules (for example, Z4 ‚Üí Honda) were learned,
there could be overcorrection, where tuple 3 is modified to
tuple 2.
On the other hand, BayesWipe handles this situation
based on the value of Œ≥ . Looking at tuple 3 (which is a clean
tuple), suppose the candidate replacement tuples for it are
also tuples 1, 2 and 3. In that case, the situation may look
like the following:

Cd.
1
2
3

P (T ‚àó )
0.400
0.001
0.005

low Œ≥
P (T |T ‚àó )
score
0.02 0.0080
0.30 0.0003
1.00 0.0050

high Œ≥
P (T |T ‚àó )
score
0.002 0.00080
0.030 0.00003
1.000 0.00500

As we can see, if we choose a low value of Œ≥ , the
candidate with the highest score is tuple 1, which means
an overcorrection will occur. However, with higher Œ≥ , the
candidate with the highest score is tuple 3 itself, which
means the tuple will not be modified, and overcorrection
will not occur. On the other hand, if we set Œ≥ too high, then
even legitimately dirty tuples like tuple 2 won‚Äôt get changed,
thus the number of actual corrections will also be lower.
To make full use of this capability of regulating overcorrection, we need to be able to set the value of Œ≥ appropriately. In the absence of a training dataset (for which the
ground truth is known), we can only estimate the best Œ≥
approximately. We do this by finding a value of Œ≥ for which
the percentage of tuples modified by the system is equal to
the expected percentage of noise in the dataset.
5.2

Cleaning to a Probabilistic Database

We note that many data cleaning approaches ‚Äî including
the one we described in the previous sections ‚Äî come up
with multiple alternatives for the clean version for any given
tuple, and evaluate their confidence in each of the alternatives. For example, if a tuple is observed as ‚ÄòHonda, Corolla‚Äô,
two correct alternatives for that tuple might be ‚ÄòHonda,
Civic‚Äô and ‚ÄòToyota, Corolla‚Äô. In such cases, where the choice
of the clean tuple is not an obvious one, picking the mostlikely option may lead to the wrong answer. Additionally,
if we intend to do further processing on the results, such as
perform aggregate queries, join with other tables, or transfer
the data to someone else for processing, then storing the
most likely outcome is lossy.
A better approach (also suggested by others [4]) is to
store all of the alternative clean tuples along with their
confidence values. Doing this, however, means that the
resulting database will be a probabilistic database (PDB),
even when the source database is deterministic.
It is not clear upfront whether PDB-based cleaning will
have advantages over cleaning to a deterministic database.
On the positive side, using a PDB helps reduce loss of
information arising from discarding all alternatives to tuples
that did not have the maximum confidence. On the negative
side, PDB-based cleaning increases the query processing
cost (as querying PDBs are harder than querying deterministic databases [39]).
Another challenge is one of presentation: users usually
assume that they are dealing with a deterministic source
of data, and presenting all alternatives to them can be
overwhelming to them. In this section, and in the associated
experiments, we investigate the potential advantages to using the BayesWipe system and storing the resulting cleaned
data in a probabilistic database. For our experiments, we
used Mystiq [40], a prototype probabilistic database system
from University of Washington, as the substrate. In order to
create a probabilistic database from the corrections of the input data, we follow the offline cleaning procedure described
previously in Section 4. Instead of storing the most likely T ‚àó ,
we store all the T ‚àó s along with their P (T ‚àó |T ) values. When

7

evaluating the performance of the probabilistic database, we
used simple select queries on the resulting database. Since
representing the results of a probabilistic database to the
user is a complex task, in this paper we focus on showing
the XOR representation of the tuple alternatives to the user.
The rationale for our decision is that in a used car scenario,
the user will be provided with a URL link to the car through
the clickable tuple id and the several alternative clean values
for the dirty attributes are shown within the single tuple
returned to the user. As a result, the form of our output is a
tuple-disjoint independent database [41]. This can be better
explained with an example:
TABLE 2: Cleaned probabilistic database
TID
t1

Model
Civic
Civic

Make
Honda
Honda

Orig.
JPN
JPN

Civic
Civik

Honda
Honda

JPN
JPN

Size
Mid-size
Compact

Eng.
V4
V6

Cond.
NEW
NEW

P
0.6
0.4

Mid-size
Mid-size

V4
V4

USED
USED

0.9
0.1

...
t3

Example: Suppose we clean our running example of
Table 1. We will obtain a tuple-disjoint independent3 probabilistic database [41]; a fragment of which is shown in
Table 2. Each original input tuple (t1 , t3 ), has been cleaned,
and their alternatives are stored along with the computed
confidence values for the alternatives (0.6 and 0.4 for t1 ,
in this example). Suppose the user issues a query Model =
Civic. Both options of tuple t1 of the probabilistic database
satisfy the constraints of the query. Since there are two valid
alternatives to tuple t1 in the result with probabilities 0.6
and 0.4, in order to get a single tuple representation, the
matching attributes in the alternatives are shown deterministically whereas the unclean attributes like Size, Engine and
Condition with several possible clean values are shown as
options. Only the first option in tuple t3 matches the query.
Thus the XOR result will contain only a single alternative
for t3 with probability 0.9. It is important to note that in the
case of t1 , the Mid-size car can be associated with an Eng.
value of V4 and a probability of 0.6 respectively. The XOR
representation doesn‚Äôt necessarily allow for combining Midsize with either an Eng. value of V6 or a probability value
of 0.4.
The experimental results compare the tuple ids when
computing the recall of the method because tuple id provides the URL to the car‚Äôs web page which can be used
to determine a match. The output probabilistic relation is
shown in Table 3.
TABLE 3: Result probabilistic database
TID

Model

Make

Orig.

t1

Civic

Honda

JPN

t3

Civic

Honda

JPN

Size
Midsize/Compact
Mid-size

Eng.

Cond.

P

V4/V6

NEW

0.6/0.4

V4

USED

0.9

3. A tuple-disjoint independent probabilistic database is one where
every tuple, identified by its primary key, is independent of all other
tuples. Each tuple is, however, allowed to have multiple alternatives
with associated probabilities. In a tuple-independent database, each
tuple has a single probability, which is the probability of that tuple
existing.

The interesting fact here is that the result of any query
will always be a tuple-independent database. This is because we projected out every attribute except for the tupleID, and the tuple-IDs are independent of each other.
When showing the results of our experiments, we evaluate the precision and recall of the system. Since precision
and recall are deterministic concepts, we have to convert
the probabilistic database into a deterministic database (that
will be shown to the user) prior to computing these values.
We can do this conversion in two ways: (1) by picking only
those tuples whose probability is higher than some threshold. We call this method the threshold based determinization.
(2) by picking the top-k tuples and discarding the probability values (top-k determinization). The experiment section
(Section 8.2) shows results with both determinizations.

6

Q UERY REWRITING FOR O NLINE Q UERY P RO -

CESSING

In this section we extend the techniques of the previous
section so that it can be used in an online query processing
method where the result tuples are cleaned at query time.
Certain tuples that do not satisfy the query constraints, but
are relevant to the user, need to be retrieved, ranked and
shown to the user. The process also needs to be efficient,
since the time that the users are willing to wait before
results are shown to them is very small. We show our query
rewriting mechanisms aimed at addressing both.
We begin by executing the user‚Äôs query (Q‚àó ) on the
database. We store the retrieved results, but do not show
them to the user immediately. We then find rewritten queries
that are most likely to retrieve clean tuples. We do that in
a two-stage process: we first expand the query to increase
the precision, and then relax the query by deleting some
constraints (to increase the recall).
6.1

Increasing the precision of rewritten queries

We can improve precision by adding relevant constraints to the query Q‚àó given by the user. For example, when a user issues the query Model = Civic,
we can expand the query to add relevant constraints Make = Honda, Country = Japan, Size = Mid-Size.
These additions capture the essence of the query ‚Äî because
they limit the results to the specific kind of car the user
is probably looking for. These expanded structured queries
generated from the user‚Äôs query are called ESQs.
Each user query Q‚àó is a select query with one or more
attribute-value pairs as constraints. In order to create an
ESQ, we will have to add highly correlated constraints to
Q‚àó .
Searching for correlated constraints to add requires
Bayesian inference, which is an expensive operation. Therefore, when searching for constraints to add to Q‚àó , we restrict
the search to the union of all the attributes in the Markov
blanket [42]. The Markov blanket of an attribute comprises
its children, its parents, and its children‚Äôs other parents. It
is the set of attributes whose value being given, the node
becomes independent of all other nodes in the network.
Thus, it makes sense to consider these nodes when finding
correlated attributes. This correlation is computed using
the Bayes Network that was learned offline on a sample
database (recall the architecture of BayesWipe in Figure 1.)

8

Given a Q‚àó , we attempt to generate multiple ESQs that
maximizes both the relevance of the results and the coverage
of the queries of the solution space.
Note that if there are m attributes, each of which can
take n values, then the total number of possible ESQs is
nm . Searching for the ESQ that globally maximizes the
objectives in this space is infeasible; we therefore approximately search for it by performing a heuristic-informed
search. Our objective is to create an ESQ with m attributevalue pairs as constraints.We begin with the constraints
specified by the user query Q‚àó . We set these as evidence in
the Bayes network, and then query the Markov blanket of
these attributes for the attribute-value pairs with the highest
posterior probability given this evidence. We take the top-k
attribute-value pairs and append them to Q‚àó to produce k
search nodes, each search node being a query fragment. If Q
has p constraints in it, then the heuristic value of Q is given
by Pr(Q)m/p . This represents the expected joint probability
of Q when expanded to m attributes, assuming that all the
constraints will have the same average posterior probability.
We expand them further, until we find k queries of size m
with the highest probabilities.
Make=Honda

0.4

Model=
Accord

0.1

Make=
Honda

0.3

0.9

Model=
Civic
Fuel=
Gas

Miles=
10k

‚Ä¶

(0.1)6
Make=Honda, Model = Accord

Engine=
V4

‚Ä¶

Doors=
4

‚Ä¶

(0.1 √ó 0.3)3

Model=
Civic

‚Ä¶

(0.1 √ó 0.9)3

(0.1 √ó 0.4)3
Make=Honda, Model = Civic

Make=Honda, Fuel = Gas

Fig. 3: Query Expansion Example. The tree shows the candidate constraints that can be added to a query, and the
rectangles show the expanded queries with the computed
probability values.
Example: In Figure 3, we show an example of the query
expansion. The node on the left represents the query given
by the user ‚ÄúMake=Honda‚Äù. First, we look at the Markov
Blanket of the attribute Make, and determine that Model
and Condition are the nodes in the Markov blanket. we
then set ‚ÄúMake=Honda‚Äù as evidence in the Bayes network
and then run an inference over the values of the attribute
Model. The two values of the Model attribute with the
highest posterior probability are Accord and Civic. The
most probable values of the Condition attribute are ‚Äúnew‚Äù
and ‚Äúold‚Äù. Using each of these values, new queries are
constructed and added to the queue. Thus, the queue now
consists of the 4 queries: ‚ÄúMake=Honda, Model=Civic‚Äù,
‚ÄúMake=Honda, Model=Accord‚Äù and ‚ÄúMake=Honda, Condition=old‚Äù. A fragment of these queries are shown in
the middle column of Figure 3. We dequeue the highest
probability item from the queue and repeat the process
of setting the evidence, finding the Markov Blanket, and
running the inference. We stop when we get the required
number of ESQs with a sufficient number of constraints.
6.2 Increasing the recall
Adding constraints to the query causes the precision of the
results to increase, but reduces the recall drastically. Therefore, in this stage, we choose to delete some constraints from

the ESQs, thus generating relaxed queries (RQ). Notice that
tuples that have corruptions in the attribute constrained by
the user can only be retrieved by relaxed queries that do
not specify a value for those attributes. Instead, we have to
depend on rewritten queries that contain correlated values
in other attributes to retrieve these tuples. Using relaxed
queries can be seen as a trade-off between the recall of the
resultset and the time taken, since there are an exponential
number of relaxed queries for any given ESQ. As a result,
an important question is the choice of RQs to execute. We
take the approach of generating every possible RQ, and
then ranking them according to their expected relevance.
This operation is performed entirely on the learned error
statistics, and is thus very fast.
We score each relaxed query by the expected relevance of
its result set.
!
P
‚àó
Tq Score(Tq |Q )
Rank(q) = E
|Tq |
where Tq are the tuples returned by a query q , and Q‚àó is
the user‚Äôs query. Executing an RQ with a higher rank will
have a more beneficial result on the result set because it will
bring in better quality result tuples. Estimating this quantity
is difficult because we do not have complete information
about the tuples that will be returned for any query q . The
best we can do, therefore, is to approximate this quantity.
Let the relaxed query be Q, and the expanded query
that it was relaxed from be ESQ. We wish to estimate
E[P (T |T ‚àó )] where T are the tuples returned by Q. Using the
attribute-error
independence assumption, we can rewrite
Qm
that as i=0 Pr(T.Ai |T ‚àó .Ai ), where T.Ai is the value of the
i-th attribute in T. Since ESQ was obtained by expanding
Q‚àó using the Bayes network, it has values that can be
considered clean for this evaluation. Now, we divide the m
attributes of the database into 3 classes: (1) The attribute
is specified both in ESQ and in Q. In this case, we set
Pr(T.Ai |T ‚àó .Ai ) to 1, since T.Ai = T ‚àó .Ai . (2) The attribute
is specified in ESQ but not in Q. In this case, we know
what T ‚àó .Ai is, but not T.Ai . However, we can generate an
average statistic of how often T ‚àó .Ai is erroneous by looking
at our sample database. Therefore, in the offline learning
stage, we precompute tables of error statistics for every T ‚àó
that appears in our sample database, and use that value.
(3) The attribute is not specified in either ESQ or Q. In
this case, we know neither the attribute value in T nor in
T ‚àó . We, therefore, use the average error rate of the entire
attribute as the value for Pr(T.Ai |T ‚àó .Ai ). This statistic is
also precomputed during the learning phase. This product
gives the expected rank of the tuples returned by Q.
Example: In Figure 4, we show an example for finding
the probability values of a relaxed query. Assume that the
user‚Äôs query Q‚àó is ‚ÄúCivic‚Äù, and the ESQ is shown in the
second row. For an RQ that removes the attribute values
‚ÄúCivic‚Äù and ‚ÄúMid-Size‚Äù from the ESQ, the probabilities are
calculated as follows: For the attributes ‚ÄúMake, Country‚Äù
and ‚ÄúEngine‚Äù, the values are present in both the ESQ as
well as the RQ, and therefore, the P (T |T ‚àó ) for them is 1.
For the attribute ‚ÄúModel‚Äù and ‚ÄúType‚Äù, the values are present
in ESQ but not in RQ, hence the value for them can be
computed from the learned error statistics. For example, for
‚ÄúCivic‚Äù, the average value of P (T |Civic) as learned from

9

Model
Q*:

Civic

ESQ:

Civic

RQ:

E[P(T|T*)]:

0.8

Make

Country

Honda

JPN

Honda

JPN

1

1

Type

Mid-size

Engine

Cond.

V4
V4

0.5

1

0.5

=0.2

Fig. 4: Query Relaxation Example.
the sample database (0.8) is used. Finally, for the attribute
‚ÄúCondition‚Äù, which is present neither in ESQ nor in RQ,
we use the average error statistic for that attribute (i.e. the
average of P (Ta |Ta‚àó ) for a = ‚ÄúCondition‚Äù which is 0.5).
The final value of E[P (T |T ‚àó )] is found from the product
of all these attributes as 0.2. This process is very fast because it only involves lookups and multiplication - bayesian
inference is not needed.
6.3

Terminating the process

We begin by looking at all the RQs in descending order
of their rank. If the current k -th tuple in our resultset has
a relevance of Œª, and the estimated rank of the Q we are
about to execute is R(Tq |Q), then we stop evaluating any
more queries if the probability Pr(R(Tq |Q) > Œª) is less
than some user defined threshold P . This ensures that we
have the true top-k resultset with a probability P .

7

M AP -R EDUCE FRAMEWORK

BayesWipe is most useful for big-data related scenarios.
BayesWipe has two modes: online and offline. The online
mode of BayesWipe already works for big data scenarios by
optimising the rewritten queries it issues. Now, we show
that the offline mode can also be optimized for a big-data
scenario by implementing it as a Map-Reduce application.
So far, BayesWipe-Offline has been implemented as a
two-phase, single threaded program. In the first phase,
the program learns the Bayes network (both structure and
parameters), learns the error statistics, and creates the candidate index. Recall from section 4.3 that we create an
index on the attributes of the sample database to speed
up the creation of the candidate set of clean tuples; which
we refer to as the candidate index. The candidate index is
constructed on a set of j + 1 attributes when the restriction
on a candidate clean tuple is to differ from the dirty tuple in
not more than j attributes. The attributes in the dirty tuple
are compared to the attributes of the tuples in the sample
database using the candidate index to generate the set of
candidate clean tuples. Note that this candidate index can be
constructed on any arbitrary set of j + 1 attributes present
in the sample database. In the second phase, the program
goes through every tuple in the input database, picks a set
of candidate tuples, and then evaluates the P (T ‚àó |T )P (T ‚àó )
for every candidate tuple, and replaces T with the T ‚àó that
maximises that value. Since the learning is typically done
on a sample of the data, it is more important to focus
on the second phase for the parallelizing efforts. Later, we
will see how the learning of the error statistics can also be
parallelized.

7.1

Simple Approach

The simplest approach to parallelizing BayesWipe is to run
the first phase (the learning phase) on a single machine.
Then, a copy of the bayes network (structure and CPTs),
the error statistics, and the candidate index can be sent to
a number of other machines. Each of those machines also
receives a fraction of the input data from the dirty database.
With the help of the generative model and the input data, it
can clean the tuples, and then create the output.
If we express this in Map-Reduce terminology, we will
have a pre-processing step where we create the generative
and error models. The Map-Reduce architecture will have
only mappers, and no reducers. The result of the mapping
will be the tuple hT, T ‚àó i.
The problem with this approach is that in a truly big
data scenario, the candidate index can become very large.
Indeed, as the number of tuples increases, the size of the
domain of each attribute also increases (see Figure 8a for
1 shard). Further, the number of different combinations,
and the number of erroneous values for each attribute also
increase (Figure 8b). All of this results in a rather large candidate index. Transmitting and using the entire index on each
mapper node is wasteful of both network, memory, (and
if swapped out, disk resources). Note that to create a rich
and useful data correction system, we have to accommodate
a large candidate clean-tuple set, C(T ), for every T . C(T )
roughly tracks the sample database size. If we are unable to
shard C(T ), then sharding the input data is pointless. In the
following section we endeavor to show a strategy where not
just the input, but also the index on the candidate set C(T )
can be sharded across machines.
7.2

Improved Approach

In order to split both the input tuples and the candidate
index, we use a two-stage approach. In the first stage, we
run a map-reduce that splits the problem into multiple
shards, each shard having a small fraction of the candidate
index. The second stage is a simple map-reduce that picks
the best output from stage 1 for each input tuple.
Stage 1: Given an input tuple T and a set of candidate
tuples, the T ‚àó s, suppose the candidate index is created on k
attributes, A1 ...Ak . We can say that for every tuple T , and
one of its candidate tuples T ‚àó , they will have at least one
matching attribute ai from this set. We can use this common
element ai to predict which shards the candidate T ‚àó s might
be available in. We therefore, send the tuple T to each shard
that matches the hash of the value ai .
In the map-reduce architecture, it is possible to define
a ‚Äòpartition‚Äô function. Given a mapped key-value pair, this
function determines which reducer nodes will process the
data. We can use an exact equivalence on each value that
the matching attribute can take, ai as the partition function.
However, notice that the number of possible values that
A1 ...Ak can take is rather large. If we naƒ±Ãàvely use ai as
the partition function, we will have to create those many
reducer nodes. Therefore, more generally, we hash this value
into a fixed number of reducer nodes, using a deterministic
hash function. This will then find all candidate tuples that
are eligible for this tuple, compute the similarity, and output
it.

10

Example: Suppose we have tuple T1 that has values
(a1 , a2 , a3 , a4 , a5 ). Suppose our candidate index is created
on attributes A1 , A2 , A4 . This means that any candidates T ‚àó
that are eligible for this tuple have to match one of the values
a1 , a2 or a4 . Then the mapper will create the pairs (a1 , T ),
(a2 , T ) and (a4 , T ), and send to the reducers. The partition
function is the hash of the key - so in this case, the first one
will be sent to the reducer number hash(A1 = a1), the second will be sent to the reducer numbered hash(A2 = a2),
and so on.
In the reducer, the similarity computation and computation of the prior probabilities from the BayesWipe algorithm
are run. Since each reducer only has a fraction of the candidate index (the part that matches A1 = a1, for instance), it
can hold it in memory and computation is quite fast. Each
‚àó
reducer produces a pair (T1 , (T1n
, score)). Since there are
several candidate clean tuples, n is used to identify a specific
tuple among those alternatives.
Stage 2: This stage is a simple max calculation. The
mapper does nothing, it simply passes on the key-value
‚àó
pair (T1 , (T1n
, score)) that was generated in the previous
Map-Reduce job. Notice that the key of this pair is the
original, dirty tuple T1 . The Map-Reduce architecture thus
automatically groups together all the possible clean versions
of T1 along with their scores. The reducer picks the best
T* based on the score (using a simple max function), and
outputs it to the database.

Index
Fragment

Reducer
(a, T)
Mapper

Index
Fragment

Reducer
(T, (T*, score))
Index
Fragment

Mapper

Input Data
Fragments

Reducer
Index
Fragment

Reducer

Fig. 5: Stage-1 Map-Reduce Framework for BayesWipe.
7.3

Results of This Strategy

In Figure 8a and Figure 8b we can see how this map
reduce strategy helps in reducing the memory footprint
of the reducer. First, we plot the size of the index that
needs to be held in each node as the number of tuples in
the input increases. The topmost curve shows the size of
index in bytes if there was no sharding - as expected, it
increases sharply. The other curves show how the size of
the index in the one of the nodes varies for the same dataset
sizes. From the graph, it can be seen that as the number of
tuples increases, the size of the index grows at a lower rate
when the number of shards is increased. This shows that
increasing the number of reduce nodes is a credible strategy
for distributing the burden of the index.
In the second figure (Figure 8b), we see how the size
of the index varies with the percentage of noise in the
dataset. As expected, when the noise increases, the number

of possible candidate tuples increase (since there are more
variations of each attribute value in the pool). Without
sharding, we see that the size of the dataset increases. While
the increase in the size of the index is not as sharp as the
increase due to the size of the dataset, it is still significant.
Once again, we observe that as the number of shards is
increased, the size of the index in the shard reduces to a
much more manageable value.
Note that a slight downside to this architecture is that a
shuffle/reduce of (T, (Tn‚àó , score)) is needed in the second
stage, and this intermediate data structure can be quite
large. While this leads to some network and temporary
storage overhead, the primary objective of sharding the expensive computation has been achieved by this architecture.

8

E MPIRICAL E VALUATION

We quantitatively study the performance of BayesWipe in
both its modes ‚Äî offline, and online, and compare it against
state-of-the-art CFD approaches. We present experiments
on evaluating the approach in terms of the effectiveness of
data cleaning, efficiency and precision of query rewriting.
A demo for the offline cleaning mode of BayesWipe can be
downloaded from http://bayeswipe.sushovan.de/.
8.1

Datasets

To perform the experiments, we obtained the real data from
the web. The first dataset is Used car sales dataset Dcar
crawled from Google Base. Such ‚Äúdirty‚Äù dataset is referred
0
‚Äù. The second dataset we used was adapted
to as ‚ÄúDcar
from the Census Income dataset Dcensus from the UCI machine learning repository [43]. From the fourteen available
attributes, we picked the attributes that were categorical
in nature, resulting in the following 8 attributes: workingclass, education, marital status, occupation, race, gender,
filing status. country. The same setup was used for both
datasets ‚Äì including parameters and error features.
These datasets were observed to be mostly clean. We
then introduced4 three types of noise to the attributes. To
add noise to an attribute, we randomly changed it either to
a new value which is close in terms of string edit distance
(distance between 1 and 4, simulating spelling errors) or to
a new value which was from the same attribute (simulating
replacement errors) or just delete it (simulating deletion errors). As we have mentioned before, one of the assumptions
of this paper is that the error model is a combination of these
three kinds of errors, and that the errors are independent
of each other. By synthetically generating these errors, we
were able to test our system against a dataset that satisfies
the assumption.
The next dataset tests our system against a real-world
scenario where we do not control the error process, and thus
validates that this assumption was not unrealistic.
To test our system against real-world noise where we
do not have any control over amount, type or behavior
of the noise generation process, we crawled car inventory
data from the website ‚Äòcars.com‚Äô. We manually verified that
the data obtained did, in fact, have a reasonable number of
inaccuracies, making it a suitable candidate for testing our
system.
4. We note that the introduction of synthetic errors into clean data for
experimental evaluation purposes is common practice in data cleaning
research [16], [23].

11

8.2

Experiments

Offline Cleaning Evaluation: The first set of evaluations
shows the effectiveness of the offline cleaning mode. In
Figure 6a, we compare BayesWipe against CFDs [44].
The dotted line that shows the number of CFDs learned
from the noisy data quickly falls to zero, which is not surprising: CFDs learning was designed with a clean training
dataset in mind. Further, the only constraints learned by
this algorithm are the ones that have not been violated in
the dataset ‚Äî unless a tuple violates some CFD, it cannot
be cleaned. As a result, the CFD method cleans exactly
zero tuples independent of the noise percentage. On the
other hand, BayesWipe is able to clean between 20% to
40% of the incorrect values. It is interesting to note that
the percentage of tuples cleaned increases initially and then
slowly decreases, because for very low values of noise, there
aren‚Äôt enough errors for the system to learn a reliable error
model from; and at larger values of noise, the data source
model learned from the noisy data is of poorer quality.
While Figure 6a showed only percentages, in Figure 6b
we report the actual number of tuples cleaned in the dataset
along with the percentage cleaned. This curve shows that
the raw number of tuples cleaned always increases with
higher input noise percentages.
Setting Œ≥ : As explained in Section 5.1, the weight given
to the edit distance (Œ¥ ) compared to the weight given to
the distributional similarity (1 ‚àí Œ¥ ); and the overcorrection
parameter (Œ≥ ) are parameters that can be tuned, and should
be set based on which kind of error is more likely to occur. In
our experiments, we performed a grid search to determine
the best values of Œ¥ and Œ≥ to use. In Figure 6c, we show a
portion of the grid search where Œ¥ = 2/5, and varying Œ≥ .
The ‚Äúvalues corrected‚Äù data points in the graph correspond to the number of erroneous attribute values that
the algorithm successfully corrected (when checked against
the ground truth). The ‚Äúfalse positives‚Äù are the number of
legitimate values that the algorithm changes to an erroneous
value. When cleaning the data, our algorithm chooses a
candidate tuple based on both the prior of the candidate as
well as the likelihood of the correction given the evidence.
Low values of Œ≥ give a higher weight to the prior than
the likelihood, allowing tuples to be changed more easily
to candidates with high prior. The ‚Äúoverall gain‚Äù in the
number of clean values is calculated as the difference of
clean values between the output and input of the algorithm.
If we set the parameter values too low, we will correct
most wrong tuples in the input dataset, but we will also
‚Äòovercorrect‚Äô a larger number of tuples. If the parameters are
set too high, then the system will not correct many errors
‚Äî but the number of ‚Äòovercorrections‚Äô will also be lower.
Based on these experiments, we picked a parameter value
of Œ¥ = 0.638, Œ≥ = 5.8 and kept it constant throughout.
Using probabilistic databases: We empirically evaluate the
PDB-mode of BayesWipe in Figure 7. In the first figure,
we show the performance of the PDB mode of BayesWipe against the deterministic mode for specific queries.
As we can see from the first, third and seventh queries,
the BayesWipe-PDB improves the recall without any loss
of precision. However, in most cases (and on average),
BayesWipe-PDB provides a better recall at the cost of some
precision.

The second figure shows the performance of BayesWipePDB as the probability threshold for inclusion of a tuple in
the resultset is varied. As expected, with low values of the
threshold, the system allows most tuples into the resultset,
thus showing high recall and low precision. As the threshold
increased, the precision increases, but the recall falls.
In Figure 7c, we compare the precision of the PDB mode
using top-k determinization against the deterministic mode
of BayesWipe. As expected, both the modes show high precision for low values of k , indicating that the initial results
are clean and relevant to the user. For higher values of k ,
the PDB precision falls off, indicating that PDB methods
are more useful for scenarios where high recall is important
without sacrificing too much precision.
Online Query Processing: While in the offline mode, we
had the luxury of changing the tuples in the database itself,
in online query processing, we use query rewriting to obtain
a resultset that is similar to the offline results, without
modification to the database. We consider an SQL select
query system as our baseline. We evaluate the precision and
recall of our method against the ground truth and compare
it with the baseline, using randomly generated queries.
We issued randomly generated queries to both BayesWipe and the baseline system. Figure 8c shows the average
precision over 10 queries at various recall values. It shows
that our system outperforms the SQL select query system
in top-k precision, especially since our system considers the
relevance of the results when ranking them. On the other
hand, the SQL search approach is oblivious to ranking and
returns all tuples that satisfy the user query. Thus it may
return irrelevant tuples early on, leading to less precision.
Figure 8d shows the improvement in the absolute numbers of tuples returned by the BayesWipe system. The graph
shows the number of true positive tuples returned (tuples
that match the query results from the ground truth) minus
the number of false positives (tuples that are returned but do
not appear in the ground truth result set). We also plot the
number of true positive results from the ground truth, which
is the theoretical maximum that any algorithm can achieve.
The graph shows that the BayesWipe system outperforms
the SQL query system at nearly every level of noise. Further,
the graph also illustrates that ‚Äî compared to an SQL query
baseline ‚Äî BayesWipe closes the gap to the maximum
possible number of tuples to a large extent. In addition to
showing the performance of BayesWipe against the SQL
query baseline, we also show the performance of BayesWipe
without the query relaxation part (called BW-exp5 ). We can
see that the full BayesWipe system outperforms the BW-exp
system significantly, showing that query relaxation plays an
important role in bringing relevant tuples to the resultset,
especially for higher values of noise.
This shows that our proposed query ranking strategy indeed captures the expected relevance of the to-be-retrieved
tuples, and the query rewriting module is able to generate
the highly ranked queries.
Efficiency: In Figure 10 we show the data cleaning time
taken by the system in its various modes. The first two
graphs show the offline mode, and the second two show the
5. BW-exp stands for BayesWipe-expanded, since the only query
rewriting operation done is query expansion.

CFD

#CFDs

40%

4

30%
20%

2

10%
0%

40%

30%

2000

20%

1000

10%

0

5 10 15 20 25 30 35 40
Noise Percent

4

5

200

Values Corrected

160

False Positives

120

Cleanliness Gain

80
40
0
2

0%
3

(a) % performance of BayesWipe compared to CFD, for the used-car dataset.

50%

3000

0

0

Net Tuples Cleaned
Percent Cleaned

4000

6

Number of tuples

BayesWipe

50%

Num CFDs learned

% Tuples Cleaned

12

2.5

3

3.5

4

4.5

5

5.5

6

Distributional Similarity Weight

10 15 20 25 30 35

Percentage of noise

(b) % net corrupt values cleaned, car
database

(c) Net corrections vs Œ≥ . (The x-axis values show the un-normalized distributional similarity weight, which is simply
Œ≥ √ó 3/5.)

1
0.75
0.5
0.25
0
1
0.75
0.5
0.25
0

1

1

0.8

0.8

0.6

PDB Precision

0.4

PDB Recall

0.2

model =
outlander
sports

cartype =
sedan

make = bmw & model = jetta
condition =
used

BayesWipe-PDB Precision

model =
cooper s

model = h3
mini

Average

BayesWipe-DET Precision

(a) Precision and recall of the PDB method shown against the deterministic method for specific queries.

0.6
Deterministic Precision
PDB Precision

0.4

0.2

0

make = acura

Precision

PRECISION

RECALL

Fig. 6: Offline cleaning mode of BayesWipe

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Threshold

0
0

25

50
top-K

75

100

(b) Precision and recall of
(c) top-k precision of PDB
the PDB method using a
vs deterministic method.
threshold.

Fig. 7: Results of probabilistic method.
online mode. As can be seen from the graphs, BayesWipe
performs reasonably well both in datasets of large size and
datasets with large noise.
Evaluation on real data with naturally occurring errors: In
this section we used a dataset of 1.2 million tuples crawled
from the cars.com website6 to check the performance of
the system with real-world data, where the corruptions
were not synthetically introduced. Since this data is large,
and the noise is completely naturally occurring, we do not
have ground truth for this data. To evaluate this system,
we conducted an experiment on Amazon Mechanical Turk.
First, we ran the offline mode of BayesWipe on the entire database. We then picked only those tuples that were
changed during the cleaning, and then created an interface
in mechanical turk where only those tuples were shown to
the user in random order. Due to resource constraints, the
experiment was run with the first 200 tuples that the system
found to be unclean. We inserted 3 known answers into
the questionnaire, and removed any responses that failed
to annotate at least 2 out of the 3 answers correctly.
An example is shown in Figure 11. The turker is presented with two cars, and she does not know which of
the cars was originally present in the dirty dataset, and
which one was produced by BayesWipe. The turker will use
her own domain knowledge, or perform a web search and
discover that a Mazda CX-9 touring is only available in a 3.7l
engine, not a 3.5l. Then the turker will be able to declare the
second tuple as the correct option with high confidence.
6. http://www.cars.com

The results of this experiment are shown in Table 4. As
we can see, the users consistently picked the tuples cleaned
by BayesWipe more favorably compared to the original
dirty tuples, proving that it is indeed effective in real-world
datasets. Notice that it is not trivial to obtain a 56% rate of
success in these experiments. Finding a tuple which convinces the turkers that it is better than the original requires
searching through a huge space of possible corrections. An
algorithm that picks a possible correction randomly from
this space is likely to get a near 0% accuracy.
The first row of Table 4 shows the fraction of tuples for
which the turkers picked the version cleaned by BayesWipe
and indicated that they were either ‚Äòvery confident‚Äô or
‚Äòconfident‚Äô. The second row shows the fraction of tuples for
all turker confidence values, and therefore is a less reliable
indicator of success.
In order to show the efficacy of BayesWipe we also
performed an experiment in which the same tuples (the
ones that BayesWipe had changed) were modified by a random perturbation. The random perturbation was done by
the same error process as described before (typo, deletion,
substitution with equal probability). Then these tuples (the
original tuple from the database and the perturbed tuple)
were presented as two choices to the turkers. The preference
by the turkers for the randomly perturbed tuple over the
original dirty tuple is shown in the third column, ‚ÄòRandom‚Äô.
It is obvious from this that the turkers overwhelmingly do
not favor the random perturbed tuples. This demonstrates
two things. First, it shows the fact that BayesWipe was
performing useful cleaning of the tuples. In fact, BayesWipe
shows a tenfold improvement over the random perturbation

13
1600000

none

2

3

4

5

300000
none

2

3

4

5

250000

1200000

Bytes in one shard

Bytes in one shard

1400000

1000000

800000
600000
400000

200000
150000
100000

200000

50000

0
1

20

40

80
100
number of tuples

120

0

140

0

5

10

15
20
25
Noise percentage

30

35

40

(a) vs the Number of Tuples (in
(b) vs the Noise in the Dataset, for
Thousands) in the Dataset, for VarVarious Number of Shards.
ious Number of Shards.

Fig. 8: Map-Reduce index sizes
1000

Precision

1.00

980
960

.95
SQL Select Query

.90

940
920

BayesWipe Online
Query Processing

.85
.01

.03

.05

.07 .09
Recall

.11

900
1

.13

2

BW

3

4

BW-exp

5

10 15 20 25 30 35
Noise %
SQL
Ground truth

(d) Net improvement in data quality
(TP-FP)

(c) Average precision vs recall

150

car-offline

800
600

census

400

car

200

100

10k 15k 20k 25k
Number of tuples

30k

car-online

census-online

50

0

10
20
30
Percentage of noise

census-online
50
0

5k

40

car-online

100

0

0

5k

150
Time taken (s)

census-offline

Time Taken (s)

1000
Time taken (s)

Time Taken (s)

Fig. 9: Online cleaning mode of BayesWipe
600
500
400
300
200
100
0

10k 15k 20k 25k
Number of tuples

30k

0

10
20
30
Percentage of noise

40

(a) Time taken vs number of (b) Time taken vs noise per- (c) Time taken vs number of (d) Time taken vs noise pertuples in offline mode
centage in offline mode
tuples in online mode
centage in online mode

Fig. 10: Performance evaluations
Confidence
High confidence only
All
confidence
values

BayesWipe

Original

Random

56.3%

43.6%

5.5%

53.3%

46.7%

12.4%

Increase over Random
50.8% points
...
(10x better)

make

model

cx-9
Car: mazda
touring

40.9% points
(4x better)

cx-9
Car: mazda
touring

TABLE 4: Results of the Mechanical Turk Experiment, showing the percentage of tuples for which the users picked the
results obtained by BayesWipe as against the original tuple.
Also shows performance against a random modification.

cartype fueltype

engine

transmission

drivetrain doors wheelbase

suv

3.5l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113‚Äù

suv

3.7l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113"

ÔÇ° First is correct
ÔÇ° Second is correct
How confident are you about your selection?
ÔÇ° Very confident ÔÇ° Confident ÔÇ° Slightly confident ÔÇ° Slightly Unsure ÔÇ° Totally Unsure

Fig. 11: A fragment of the questionnaire provided to the
Mechanical Turk workers.
model, as judged by human turkers. This shows that in
the large space of possible modifications of a wrong tuple,
BayesWipe picks the correct one most of the time. Second, it
provides additional support for the fact that the turkers are
picking the tuple carefully, and are not randomly submitting
their responses.
In this experiment, we also found the average fraction
of known answers that the turkers gave wrong answers to.
This value was 8%. This leads to the conclusion that the
difference between the turker‚Äôs preference of BayesWipe
over both the original tuples (which is 12%) and the random
perturbation (which is 50%) are both significant.

9

C ONCLUSION

In this paper we presented a novel system, BayesWipe
that works using end-to-end probabilistic semantics, and
without access to clean master data. We showed how to
effectively learn the data source model as a Bayes network,
and how to model the error as a mixture of error features.
We showed the operation of this system in two modalities:
(1) offline data cleaning, an in situ rectification of data and
(2) online query processing mode, a highly efficient way to
obtain clean query results over inconsistent data. There is
an option to generate a standard, deterministic database as
the output, as well as a probabilistic database, where all

14

the alternatives are preserved for further processing. We
empirically showed that BayesWipe outperformed existing
baseline techniques in quality of results, and was highly
efficient. We also showed the performance of the BayesWipe
system at various stages of the query rewriting operation.
We demonstrated how BayesWipe can be run on the mapreduce architecture so that it can scale to huge data sizes.
User experiments showed that the system is useful in cleaning real-world noisy data.

R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

W. Fan and F. Geerts, ‚ÄúFoundations of data quality management,‚Äù
Synthesis Lectures on Data Management, 2012.
P. Gray, ‚ÄúBefore Big Data, clean data,‚Äù 2013. [Online]. Available:
http://www.techrepublic.com/blog/big-data-analytics/
before-big-data-clean-data/
H. Leslie, ‚ÄúHealth data quality ‚Äì a two-edged sword,‚Äù 2010.
[Online]. Available: http://omowizard.wordpress.com/2010/02/
21/health-data-quality-a-two-edged-sword/
Computing Research Association, ‚ÄúChallenges and opportunities
with big data,‚Äù http://cra.org/ccc/docs/init/bigdatawhitepaper.
pdf, 2012.
E. Knorr, R. Ng, and V. Tucakov, ‚ÄúDistance-based outliers: algorithms and applications,‚Äù VLDB, 2000.
H. Xiong, G. Pandey, M. Steinbach, and V. Kumar, ‚ÄúEnhancing
data analysis with noise removal,‚Äù TKDE, 2006.
P. Singla and P. Domingos, ‚ÄúEntity resolution with markov logic,‚Äù
in ICDM, 2006.
I. Fellegi and D. Holt, ‚ÄúA systematic approach to automatic edit
and imputation,‚Äù J. American Statistical association, 1976.
P. Bohannon, W. Fan, M. Flaster, and R. Rastogi, ‚ÄúA cost-based
model and effective heuristic for repairing constraints by value
modification,‚Äù in SIGMOD, 2005.
W. Fan, F. Geerts, L. Lakshmanan, and M. Xiong, ‚ÄúDiscovering
conditional functional dependencies,‚Äù ICDE, 2009.
L. E. Bertossi, S. Kolahi, and L. V. S. Lakshmanan, ‚ÄúData cleaning
and query answering with matching dependencies and matching
functions,‚Äù ICDT, 2011.
A. Fuxman, E. Fazli, and R. J. Miller, ‚ÄúConquer: Efficient management of inconsistent databases,‚Äù SIGMOD, 2005.
S. De, Y. Hu, Y. Chen, and S. Kambhampati, ‚ÄúBayeswipe: A multimodal system for data cleaning and consistent query answering
on structured bigdata,‚Äù IEEE Big Data, 2014.
G. Wolf, A. Kalavagattu, H. Khatri, R. Balakrishnan, B. Chokshi,
J. Fan, Y. Chen, and S. Kambhampati, ‚ÄúQuery processing over
incomplete autonomous databases: query rewriting using learned
data dependencies,‚Äù VLDB, 2009.
S. De, ‚ÄúUnsupervised bayesian data cleaning techniques for structured data,‚Äù Ph.D. dissertation, ASU, 2014.
P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis,
‚ÄúConditional functional dependencies for data cleaning,‚Äù ICDE,
2007.
W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, ‚ÄúConditional functional dependencies for capturing data inconsistencies,‚Äù
TODS, 2008.
J. Wang and N. Tang, ‚ÄúTowards dependable data repairing with
fixing rules,‚ÄùSIGMOD, 2014.
J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg, T. Kraska, and
T. Milo, ‚ÄúA sample-and-clean framework for fast and accurate
query processing on dirty data,‚Äù SIGMOD, 2014.
L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu, ‚ÄúOn
generating near-optimal tableaux for conditional functional dependencies,‚Äù PVLDB, 2008.
G. Cormode, L. Golab, K. Flip, A. McGregor, D. Srivastava, and
X. Zhang, ‚ÄúEstimating the confidence of conditional functional
dependencies,‚Äù in SIGMOD, 2009.
M. Arenas, L. Bertossi, and J. Chomicki, ‚ÄúConsistent query answers in inconsistent databases,‚Äù PODS, 1999.
G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma, ‚ÄúImproving data
quality: Consistency and accuracy,‚Äù VLDB, 2007.
M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F.
Ilyas, ‚ÄúGuided data repair,‚Äù VLDB, 2011.
G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúSampling from
repairs of conditional functional dependency violations,‚Äù VLDB,
2013.

[26] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, ‚ÄúTowards certain fixes
with editing rules and master data,‚Äù VLDB, 2010.
[27] G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúOn the relative
trust between inconsistent data and inaccurate constraints,‚Äù ICDE,
2013.
[28] X. L. Dong, L. Berti-Equille, and D. Srivastava, ‚ÄúTruth discovery
and copying detection in a dynamic world,‚Äù VLDB,2009.
[29] T. Dasu and J. M. Loh, ‚ÄúStatistical distortion: Consequences of data
cleaning,‚Äù VLDB, 2012.
[30] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and
Y. Ye, ‚ÄúKatara: A data cleaning system powered by knowledge
bases and crowdsourcing,‚Äù SIGMOD, 2015.
[31] J. Wang, T. Kraska, M. J. Franklin, and J. Feng, ‚ÄúCrowder: Crowdsourcing entity resolution,‚Äù VLDB, 2012.
[32] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng, ‚ÄúQasca: A qualityaware task assignment system for crowdsourcing applications,‚Äù
SIGMOD, 2015.
[33] A. Hartemink., ‚ÄúBanjo: Bayesian network inference with java
objects.‚Äù http://www.cs.duke.edu/‚àºamink/software/banjo.
[34] S. Russell and P. Norvig, Artificial intelligence: a modern approach.
Prentice Hall, 2010.
[35] T. Minka, W. J.M., J. Guiver, and D. Knowles, ‚ÄúInfer.NET 2.4‚Äù,
Microsoft Research Cambridge, 2010. http://research.microsoft.
com/infernet.
[36] E. Ristad and P. Yianilos, ‚ÄúLearning string-edit distance,‚Äù Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 1998.
[37] M. Li, Y. Zhang, M. Zhu, and M. Zhou, ‚ÄúExploring distributional
similarity based models for query spelling correction,‚Äù ICCL, 2006.
[38] A. Berger, V. Pietra, and S. Pietra, ‚ÄúA maximum entropy approach
to natural language processing,‚Äù Computational linguistics, 1996.
[39] N. Dalvi and D. Suciu, ‚ÄúEfficient query evaluation on probabilistic
databases,‚Äù VLDB, 2004.
[40] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, and D. Suciu,
‚ÄúMystiq: a system for finding more answers by using probabilities,‚Äù SIGMOD, 2005.
[41] D. Suciu and N. Dalvi, ‚ÄúFoundations of probabilistic answers to
queries,‚Äù SIGMOD, 2005.
[42] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann Publishers, 1988.
[43] A. Asuncion and D. Newman, ‚ÄúUCI machine learning repository,‚Äù
2007.
[44] F. Chiang and R. Miller, ‚ÄúDiscovering data quality rules,‚Äù VLDB,
2008.
Sushovan De is currently employed at Google. He graduated with a
Ph.D. in Computer Science from Arizona State University. His research
interests comprise Information Retrieval, Data Cleaning, and Probabilistic Databases.
Yuheng Hu is a Research Staff Member in the USER (User Systems
and Experience Research) group at IBM Research - Almaden. He
obtained his Ph.D in Computer Science at Arizona State University.
His research interests are in the areas of Machine Learning, Social
Computing and Human-Computer Interaction.
Meduri Venkata Vamsikrishna is a Ph.D student in Computer Science
at Arizona State University. He received a Master‚Äôs degree in Computer
Science from the National Unviersity of Singapore. His research interests include Data Mining from Social Media and Data Cleaning.
Yi Chen is an associate professor and the Henry J. Leir Chair in
Healthcare in the School of Management with a joint appointment in the
College of Computing Sciences at New Jersey Institute of Technology
(NJIT). She received her Ph.D. degree in Computer Science from the
University of Pennsylvania. Her current research focuses on Information
Discovery on Big Data, Social Computing and Information Integration.
Subbarao Kambhampati is a professor in Computer Science at Arizona
State University. He directs the Yochan research group which is associated with the Artifical Intelligence Lab at Arizona State University. He
is the ‚ÄùPresident-elect‚Äù of AAAI, the Association for the Advancement of
Artificial Intelligence. He secured a Ph.D. degree in Computer Science
from the University of Maryland, College Park. His research interests are
Automated Planning in Artificial Intelligence and Data and Information
Integration on the Web

Discovering Underlying Plans Based on Distributed Representations of Actions
Xin Tiana , Hankz Hankui Zhuoa & Subbarao Kambhampati b
Sun Yat-Sen University & a Arizona State University
tianxin1860@gmail.com, zhuohank@mail.sysu.edu.cn, rao@asu.edu

arXiv:1511.05662v1 [cs.AI] 18 Nov 2015

a

Abstract
Plan recognition aims to discover target plans (i.e., sequences
of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either
discover plans by maximally ‚Äúmatching‚Äù observed actions to
plan libraries, assuming target plans are from plan libraries,
or infer plans by executing domain models to best explain
the observed actions, assuming complete domain models are
available. In real world applications, however, target plans
are often not from plan libraries and complete domain models are often not available, since building complete sets of
plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and
learn vector representations of actions using the corpora; we
then discover target plans based on the vector representations.
Our approach is capable of discovering underlying plans that
are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness
of our approach by comparing its performance to traditional
plan recognition approaches in three planning domains.

Introduction
As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning chellenge (c.f.
(Cohen et al. 2015; Dong et al. 2004; Manikonda et al.
2014)). An important aspect of such a support (Kambhampati and Talamadupula 2015) is recognizing what plans the
human in the loop is making, and provide appropriate suggestions about their next actions. Although there is a lot
of work on plan recognition, much of it has traditionally
depended on the availability of a complete domain model
(Ramƒ±ÃÅrez and Geffner 2009a; Zhuo, Yang, and Kambhampati 2012). As has been argued elsewhere (Kambhampati
and Talamadupula 2015), such models are hard to get in
human-in-the-loop planning scenarios. Here, the decision
support systems have to make themselves useful without insisting on complete action models of the domain. The situation here is akin to that faced by search engines and other
tools for computer supported cooperate work, and is thus
a significant departure for the ‚Äúplanning as pure inference‚Äù
mindset of the automated planning community. As such, the
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

problem calls for plan recognition with ‚Äúshallow‚Äù models of
the domain (c.f. (Kambhampati 2007)), that can be easily
learned automatically.
There has been very little work on learning such shallow
models to support human-in-the-loop planning. Some examples include the work on Woogle system (Dong et al. 2004)
that aimed to provide support to humans in web-service
composition. That work however relied on very primitive
understanding of the actions (web services in their case) that
consisted merely of learning the input/output types of individual services.
In this paper, we focus on learning more informative models that that can help recognize the plans under construction
by the humans, and provide active support by suggesting relevant actions. To drive this process, we need to learn shallow models of the domain. We propose to adapt the recent
successes of word-vector models (Mikolov et al. 2013) in
language to our problem. Specifically, we assume that we
have access to a corpus of previous plans that the human
user has made. Viewing these plans as made up of action
words, we learn word vector models for these actions. These
models provide us a way to induce the distribution over the
identity of each unobserved action. Given the distributions
over individual unobserved actions, we use an expectationmaximization approach to infer the joint distribution over all
unobserved actions. This distribution then forms the basis
for action suggestions.
We will present the details of our approach, and will also
empirically demonstrate that it does capture a surprising
amount of structure in the observed plan sequences, leading
to effective plan recognition. We further compare its performance to traditional plan recognition techniques, including
one that uses the same plan traces to learn the STRIPS-style
action models, and use the learned model to support plan
recognition.

Problem Definition
A plan library, denoted by L, is composed of a set of
plans {p}, where p is a sequence of actions, i.e., p =
ha1 , a2 , . . . , an i where ai , 1 ‚â§ i ‚â§ n, is an action name
(without any parameter) represented by a string. For example, a string unstack-A-B is an action meaning that a robot
unstacks block A from block B. We denote the set of all possible actions by AÃÑ which is assumed to be known before-

hand. For ease of presentation, we assume that there is an
empty action, √ò, indicating an unknown or not observed action, i.e., A = AÃÑ ‚à™ {√ò}. An observation of an unknown
plan pÃÉ is denoted by O = ho1 , o2 , . . . , oM i, where oi ‚àà A,
1 ‚â§ i ‚â§ M , is either an action in AÃÑ or an empty action
√ò indicating the corresponding action is missing or not observed. Note that pÃÉ is not necessarily in the plan library L,
which makes the plan recognition problem more challenging, since matching the observation to the plan library will
not work any more.
We assume that the human is making a plan of at most
length M . We also assume that at any given point, the planner is able to observe M ‚àí k of these actions. The k unobserved actions might either be in the suffiix (i.e., yet to be
formed part) of the plan, or in the middle (due to observational gaps). Our aim is to suggest, for each of the k unobserved actions, m possible choices‚Äìfrom which the user can
select the action. (Note that we would like to keep m small,
ideally close to 1, so as not to overwhelm the user) Accordingly, we will evaluate the effectiveness of the decision support in terms of whether or not the user‚Äôs best/intended action is within the suggested m actions.
Specifically, our recognition problem can be represented
by a triple < = (L, O, A). The solution to < is to discover
the unknown plan pÃÉ that best explains O given L and A.
An example of our plan recognition problem in the blocks1
domain is shown below.
Example: A plan library L in the blocks domain is assumed to have four plans as shown below:
plan 1: pick-up-B stack-B-A pick-up-D stack-D-C
plan 2: unstack-B-A put-down-B unstack-D-C put-downD
plan 3: pick-up-B stack-B-A pick-up-C stack-C-B pickup-D stack-D-C
plan 4: unstack-D-C put-down-D unstack-C-B put-downC unstack-B-A put-down-B
An observation O of action sequence is shown below:
observation: pick-up-B √ò unstack-D-C put-down-D √ò
stack-C-B √ò √ò
Given the above input, our DUP algorithm outputs plans as
follows:
pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C
stack-C-B pick-up-D stack-D-C

Our DUP Algorithm
Our DUP approach to the recognition problem < functions
by two phases. We first learn vector representations of actions using the plan library L. We then iteratively sample
actions for unobserved actions oi by maximizing the probability of the unknown plan pÃÉ via the EM framework. We
present DUP in detail in the following subsections.
1

http://www.cs.toronto.edu/aips2000/

Learning Vector Representations of Actions
Since actions are denoted by a name strings, actions can be
viewed as words, and a plan can be viewed as a sentence.
Furthermore, the plan library L can be seen as a corpus,
and the set of all possible actions A is the vocabulary. We
thus can learn the vector representations for actions using
the Skip-gram model with hierarchical softmax, which has
been shown an efficient method for learning high-quality
vector representations of words from unstructured corpora
(Mikolov et al. 2013).
The objective of the Skip-gram model is to learn vector
representations for predicting the surrounding words in a
sentence or document. Given a corpus C, composed of a sequence of training words hw1 , w2 , . . . , wT i, where T = |C|,
the Skip-gram model maximizes the average log probability
T
1X
T t=1

X

log p(wt+j |wt )

(1)

‚àíc‚â§j‚â§c,j6=0

where c is the size of the training window or context.
The basic probability p(wt+j |wt ) is defined by the hierarchical softmax, which uses a binary tree representation
of the output layer with the K words as its leaves and for
each node, explicitly represents the relative probabilities of
its child nodes (Mikolov et al. 2013). For each leaf node,
there is an unique path from the root to the node, and this
path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector
representations for words. Instead, each inner node has an
0
output vector vn(w,j)
, and the probability of a word being
the output word is defined by
p(wt+j |wt ) =

L(wt+j )‚àí1 n

Y

œÉ(I(n(wt+j , i + 1) =

i=1

o
child(n(wt+j , i))) ¬∑ vn(wt+j ,i) ¬∑ vwt ) ,

(2)

where
œÉ(x) = 1/(1 + exp(‚àíx)).
L(w) is the length from the root to the word w in the binary
tree, e.g., L(w) = 4 if there are four nodes from the root to
w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) =
root and n(w, L(w)) = w. child(n) is a fixed child (e.g.,
left child) of node n. vn is the vector representation of the
inner node n. vwt is the input vector representation of word
wt . The identity function I(x) is 1 if x is true; otherwise it is
-1.
We can thus build vector representations of actions by
maximizing Equation (1) with corpora or plan libraries L as
input. We will exploit the vector representations to discover
the unknown plan pÃÉ in the next subsection.

Maximizing Probability of Unknown Plan pÃÉ
With the vector representations learnt in the last subsection,
a straightforward way to discover the unknown plan pÃÉ is to
explore all possible actions in AÃÑ such that pÃÉ has the highest

probability, which can be defined similar to Equation (1),
i.e.,
M
X
X
F(pÃÉ) =
log p(wk+j |wk )
(3)
k=1 ‚àíc‚â§j‚â§c,j6=0

where wk denotes the kth action of pÃÉ and M is the length of
pÃÉ. As we can see, this approach is exponentially hard with
respect to the size of AÃÑ and number of unobserved actions.
We thus design an approximate approach in the ExpectationMaximization framework to estimate an unknown plan pÃÉ
that best explains the observation O.
To do this, we introduce new parameters to capture
‚Äúweights‚Äù of values for each unobserved action. Specifically
speaking, assuming there are X unobserved actions in O,
i.e., the number of √òs in O is X, we denote these unobserved
actions by aÃÑ1 , ..., aÃÑx , ..., aÃÑX , where the indices indicate the
order they appear in O. Note that each aÃÑx can be any action
in AÃÑ. We associate each possible value of aÃÑx with a weight,
denoted by ŒìÃÑaÃÑx ,x . ŒìÃÑ is a |AÃÑ| √ó X matrix, satisfying
X
ŒìÃÑo,x = 1 ‚àß ŒìÃÑo,x ‚â• 0,
o‚ààAÃÑ

for each x. For the ease of specification, we extend ŒìÃÑ to a
bigger matrix with a size of |AÃÑ|√óM , denoted by Œì, such that
Œìo,y = ŒìÃÑo,x if y is the index of the xth unobserved action in
O, for all o ‚àà AÃÑ; otherwise, Œìo,y = 1 and Œìo0 ,y = 0 for all
o0 ‚àà AÃÑ ‚àß o0 6= o. Our intuition is to estimate the unknown
plan pÃÉ by selecting actions with the highest weights. We thus
introduce the weights to Equation (2), as shown below,
p(wk+j |wk ) =

L(wk+j )‚àí1 n

Y

An overview of our DUP algorithm is shown in Algorithm
1. In Step 2 of Algorithm 1, we initialize Œìo,k = 1/M for
all o ‚àà AÃÑ, if k is an index of unobserved actions in O; and
otherwise, Œìo,k = 1 and Œìo0 ,k = 0 for all o0 ‚àà AÃÑ ‚àß o0 6= o.
In Step 4, we view Œì¬∑,k as a probability distribution, and
sample an action from AÃÑ based on Œì¬∑,k if k is an unobserved
action index in O. In Step 5, we only update Œì¬∑,k where k
is an unobserved action index. In Step 6, we linearly project
all elements of the updated Œì to between 0 and 1, such that
we can do sampling directly based on Œì in Step 4. In Step 8,
we simply select aÃÑx based on
aÃÑx = arg max Œìo,x ,
o‚ààAÃÑ

for all unobserved action index x.
Algorithm 1 Framework of our DUP algorithm
Input: plan library L, observed actions O
Output: plan pÃÉ
1: learn vector representation of actions
2: initialize Œìo,k with 1/M for all o ‚àà AÃÑ, when k is an
unobserved action index
3: while the maximal number of repetitions is not reached
do
4:
sample unobserved actions in O based on Œì
5:
update Œì based on Equation (6)
6:
project Œì to [0,1]
7: end while
8: select actions for unobserved actions with the largest
weights in Œì
9: return pÃÉ

œÉ(I(n(wk+j , i + 1) =

i=1

o
child(n(wk+j , i))) ¬∑ avn(wk+j ,i) ¬∑ bvwk ) ,

(4)

where a = Œìwk+j ,k+j and b = Œìwk ,k . We can see that the
impact of wk+j and wk is penalized by weights a and b if
they are unobserved actions, and stays unchanged, otherwise
(since both a and b equal to 1 if they are observed actions).
We redefine the objective function as shown below,
F(pÃÉ, Œì) =

M
X

X

log p(wk+j |wk ),

(5)

k=1 ‚àíc‚â§j‚â§c,j6=0

where p(wk+j |wk ) is defined by Equation (4). The only parameters needed to be updated are Œì, which can be easily
done by gradient descent, as shown below,
Œìo,x = Œìo,x + Œ¥

‚àÇF
,
‚àÇŒìo,x

(6)

if x is the index of unobserved action in O; otherwise, Œìo,x
stays unchanged, i.e., Œìo,x = 1. Note that Œ¥ is a learning
constant.
With Equation (6), we can design an EM algorithm by repeatedly sampling an unknown plan according to Œì and updating Œì based on Equation (6) until reaching convergence
(e.g., a constant number of repetitions is reached).

Experiments
In this section, we evaluate our DUP algorithms in three
planning domains from International Planning Competition,
i.e., blocks1 , depots2 , and driverlog2 . To generate training
and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems
with a planning solver, such as FF3 , to produce 5000 plans.
We then randomly divided the plans into ten folds, with 500
plans in each fold. We ran our DUP algorithm ten times to
calculate an average of accuracies, each time with one fold
for testing and the rest for training. In the testing data, we
randomly removed actions from each testing plan (i.e., O)
with a specific percentage Œæ of the plan length. Features of
datasets are shown in Table 1, where the second column is
the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last
column is the size of vocabulary used in all plans.
We define the accuracy of our DUP algorithm as follows. For each unobserved action aÃÑx DUP suggests a set of
possible actions Sx which have the highest value of ŒìaÃÑx ,x
for all aÃÑx ‚àà AÃÑ. If Sx covers the truth action atruth , i.e.,
2
http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume20/long03ahtml/JAIRIPC.html
3
https://fai.cs.uni-saarland.de/hoffmann/ff.html

(b) depots

(c) driverlog
0.8	 ¬†

0.7	 ¬†

0.7	 ¬†

0.7	 ¬†

0.6	 ¬†

0.6	 ¬†

0.6	 ¬†

0.5	 ¬†

0.5	 ¬†

0.5	 ¬†

0.4	 ¬†
0.3	 ¬†

DUP	 ¬†

0.2	 ¬†

MatchPlan	 ¬†

0.1	 ¬†

ARMS+PRP	 ¬†

0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

accuracy

0.8	 ¬†

accuracy

accuracy

(a) blocks
0.8	 ¬†

0.4	 ¬†
0.3	 ¬†
0.2	 ¬†

DUP	 ¬†
MatchPlan	 ¬†
ARMS+PRP	 ¬†

0.1	 ¬†
0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

DUP	 ¬†
MatchPlan	 ¬†
ARMS+PRP	 ¬†

0.4	 ¬†
0.3	 ¬†
0.2	 ¬†
0.1	 ¬†
0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

Figure 1: Accuracy with respect to different percentage of unobserved actions
Table 1: Features of datasets
domain
blocks
depots
driverlog

#plan
5000
5000
5000

#word
292250
209711
179621

#vocabulary
1250
2273
1441

atruth ‚àà Sx , we increase the number of correct suggestions
g by 1. We thus define the accuracy acc as shown below:
acc =

1
T

T
X
i=1

#hcorrect-suggestionsii
,
Ki

where T is the size of testing set, #hcorrect-suggestionsii
is the number of correct suggestions for the ith testing plan,
Ki is the number of unobserved actions in the ith testing
plan. We can see that the accuracy acc may be influenced by
Sx . We will test different size of Sx in the experiment.
State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries
to best explain the observed actions (Geib and Steedman
2007), which we denote by MatchPlan. We develop a
MatchPlan system based on the idea of (Geib and Steedman 2007) and compare our DUP algorithm to MatchPlan
with respect to different percentage of unobserved actions
Œæ and different size of suggestion set Sx . Another baseline
is action-models based plan recognition approach (Ramirez
and Geffner 2009b) (denoted by PRP, short for Plan Recognition as Planning). Since we do not have action models as
input in our DUP algorithm, we exploited the action model
learning system ARMS (Yang, Wu, and Jiang 2007) to learn
action models from the plan library and feed the action models to the PRP approach. We call this hybrid plan recognition approach ARMS+PRP. To learn action models, ARMS
requires state information of plans as input. We thus added

extra information, i.e., initial state and goal of each plan in
the plan library, to ARMS+PRP. In addition, PRP requires as
input a set of candidate goals G for each plan to be recognized in the testing set, which was also generated and fed to
PRP when testing. In summary, the hybrid plan recognition
approach ARMS+PRP has more input information, i.e., initial states and goals in plan library and candidate goals G for
each testing example, than our DUP approach.

Accuracy w.r.t. Percentage of Unobserved Actions
We first evaluate our DUP algorithm with respect to different
percentage of unobserved actions Œæ in O. We set the window of training context c in Equation (1) to be three and
the size of recommendations to be ten. We compare our
DUP algorithm to both MatchPlan and ARMS+PRP. To
make fair comparison (to MatchPlan), we set the matching window MatchPlan to be three as well when searching plans from plan libraries L. In other words, to estimate
an unobserved action aÃÑx in O, MatchPlan matches previous three actions and subsequent three actions of aÃÑx to
plans in L, and recommends ten actions with maximal number of matched actions, considering unobserved actions (√ò
in the context of aÃÑx ) and actions in L as a successful matching. For ARMS+PRP, we generated 20 candidate goals for
each testing example including the ground-truth goal which
corresponds to the ground-truth plan to be recognized. The
results are shown in Figure 1.
From Figure 1, we can see that in all three domains,
the accuracy of our DUP algorithm is generally higher than
MatchPlan and ARMS+PRP, which verifies that our DUP
algorithm can indeed capture relations among actions better than previous matching approaches. The rationale is that
we explore global plan information from the plan library to
learn a ‚Äúshallow‚Äù model (distributed representations of actions) and use this model with global information to best

(a) blocks

0.6	 ¬†

accuracy

0.5	 ¬†
0.4	 ¬†

0.8	 ¬†

0.8	 ¬†

0.7	 ¬†

0.7	 ¬†

0.6	 ¬†

0.6	 ¬†

accuracy

0.7	 ¬†

accuracy

(c) driverlog

(b) depots

0.8	 ¬†

0.5	 ¬†
0.4	 ¬†

0.5	 ¬†
0.4	 ¬†

0.3	 ¬†

0.3	 ¬†

0.3	 ¬†

0.2	 ¬†

0.2	 ¬†

0.2	 ¬†

0.1	 ¬†

0.1	 ¬†

0.1	 ¬†

0	 ¬†
1	 ¬†

2	 ¬†

3	 ¬†

4	 ¬†

5	 ¬†

6	 ¬†

7	 ¬†

8	 ¬†

9	 ¬† 10	 ¬†

0	 ¬†

0	 ¬†

1	 ¬† 2	 ¬† 3	 ¬†

size of recommendations

4	 ¬† 5	 ¬† 6	 ¬†

7	 ¬† 8	 ¬† 9	 ¬† 10	 ¬†

size	 ¬†of	 ¬†recommenda9ons	 ¬†
DUP	 ¬†

MatchPlan	 ¬†

1	 ¬†

2	 ¬†

3	 ¬†

4	 ¬†

5	 ¬†

6	 ¬†

7	 ¬†

8	 ¬†

9	 ¬† 10	 ¬†

size of recommendations

ARMS+PRP	 ¬†

Figure 2: Accuracy with respect to different size of recommendations
explain the observed actions. In contrast, MatchPlan just
utilizes local plan information when matching the observed
actions to the plan library which results in lower accuracies.
Although ARMS+PRP tries to leverage global plan information from the plan library to learn action models and uses
the models to recognize observed actions, it enforces itself
to extract ‚Äúexact‚Äù models represented by planning models
which are often with noise. When feeding those noisy models to PRP, since PRP that uses planning techniques to recognize plans is very sensitive to noise of planning models,
the recognition accuracy is lower than DUP, even though
ARMS+PRP has more input information (i.e., initial states
and candidate goals) than our DUP algorithm.
Looking at the changes of accuracies with respect to the
percentage of unobserved actions, we can see that our DUP
algorithm performs fairly well even when the percentage of
unobserved action reaches 25%. In contrast, ARMS+PRP is
sensitive to the percentage of unobserved actions, i.e., the accuracy goes down when more actions are unobserved. This
is because the noise of planning models induces more uncertain information, which harms the recognition accuracy,
when the percentage of unobserved actions becomes larger.
Comparing accuracies of different domains, we can see that
our DUP algorithm functions better in the blocks domain
than the other two domains. This is because the ratio of
#word over #vocabulary in the blocks domain is much larger
than the other two domains, as shown in Table 1. We would
conjecture that increasing the ratio could improve the accuracy of DUP.

Accuracy w.r.t. Size of Recommendation Set
We next evaluate the performance of our DUP algorithm
with respect to the size of recommendation set Sx . Likewise, we set the context window c used in Equation (1) to be
three, which was also set when matching the observed actions O to plan libraries L in the MatchPlan approach. For

ARMS+PRP, the number of candidate goals for each testing
example is set to 20. ARMS+PRP aims to recognize plans
that are optimal with respect to the cost of actions. We relax ARMS+PRP to output |Sx | optimal plans, some of which
might be suboptimal. We varied the number of actions recommended by DUP (or MatchPlan) from 1 to 10. The results
are shown in Figure 2.
From Figure 2, we find that accuracies of the three approaches generally become larger when the size of the recommended action set increases in all three domains. This
is consistent with our intuition, since the larger the recommended action set is, the higher the possibility for the
truth action to be in the recommended action set. We can
also see that the accuracy of our DUP algorithm are generally larger than both MatchPlan and ARMS+PRP in
all three domains, which verifies that our DUP algorithm
can indeed better capture relations among actions and thus
recognize unobserved actions better than the matching approach MatchPlan and the planning model learning approach ARMS+PRP. The reason is similar to the one given
for Figure 1 in the previous section. That is, the ‚Äúshadow‚Äù
model learnt by our DUP algorithm is better for recognizing
plans than both the ‚Äúexact‚Äù planning model learnt by ARMS
for recognizing plans with planning techniques and the local
matching approach MatchPlan. On the other hand, we can
also see the accuracy of ARMS+PRP is generally higher than
MatchPlan. This verifies that the additional information
of initial states and candidate goals exploited by ARMS+PRP
can indeed help improve the accuracy. Furthermore, the advantage of DUP becomes even larger when the size of recommended action set increases, which suggests our vector
representation based learning approach can better capture
action relations when the size of recommended action set
is larger. The possibility of actions correctly recognized by
DUP becomes much larger than the other two approaches
when the size of recommendations increases.

Related work
Kautz and Allen proposed an approach to recognizing plans
based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free
rule in an ‚Äúaction grammar‚Äù (Kautz and Allen 1986). All actions, plans are uniformly referred to as goals, and a recognizer‚Äôs knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic,
which defines abstraction, decomposition and functional relationships between types of events. Lesh and Etzioni further
presented methods in scaling up activity recognition to scale
up his work computationally (Lesh and Etzioni 1995). They
automatically constructed plan-library from domain primitives, which was different from (Kautz and Allen 1986)
where the plan library was explicitly represented. In these
approaches, the problem of combinatorial explosion of plan
execution models impedes its application to real-world domains. Kabanza and Filion (Kabanza et al. 2013) proposed
an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted
model counting. These approaches are, however, difficult to
represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to
explain the actions observed.
Instead of using a library of plans, Ramirez and Geffner
(Ramirez and Geffner 2009b) proposed an approach to solving the plan recognition problem using slightly modified
planning algorithms, assuming the action models were given
as input. Except previous work (Kautz and Allen 1986;
Bui 2003; Geib and Goldman 2009; Ramirez and Geffner
2009b) on the plan recognition problem presented in the
introduction section, Note that action models can be created by experts or learnt by previous systems, such as ARMS
(Yang, Wu, and Jiang 2007) and LAMP (Zhuo et al. 2010).
Saria and Mahadevan presented a hierarchical multi-agent
markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems
(Saria and Mahadevan 2004). Singla and Mooney proposed
an approach to abductive reasoning using a first-order probabilistic logic to recognize plans (Singla and Mooney 2011).
Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories
(Amir and Gal 2011). Ramirez and Geffner exploited offthe-shelf classical planners to recognize probabilistic plans
(Ramirez and Geffner 2010).
Early work on human-in-the-loop planning scenarios
in automated planning went under the name of ‚Äúmixedinitiative planning‚Äù (e.g. (Ferguson, Allen, and Miller
1996)). An important limitation of that work was that the humans in the loop were helping the automated planner (with
a complete action model) navigate its search space of plans
more efficiently. In contrast, we are interested in planning
technology that helping humans develop plans, even in the
absence of complete formal models of the planning domain.
While some work in web-service composition (c.f. (Dong
et al. 2004)) did focus on this type of planning support,
they were hobbled by being limited to simple input/output

type comparison. In contrast, we believe that DUP learns
and uses a model that captures more of the structure of the
planning domain (while still not insisting on complete action
models).
While DUP focuses on learning models from plan corpora, some recent work looked at using crowdsourcing to
acquire domain models. For example, Lasecki et al. (Lasecki
et al. 2013) introduce Legion:AR, which combines the benefits of automatic and human activity labeling for robust and
deployable activity recognition. The system exploits an active learning approach (Zhao, Sukthankar, and Sukthankar
2011) in which automatic activity recognition is augmented
with on-demand activity labels from the crowd when an observed activity cannot be confidently classified. By engaging a group of people, Legion:AR is able to label activities
as they occur more reliably than a single person can, especially in complex domains with multiple actors performing
activities quickly. Lasecki et al. (Lasecki et al. 2014) built
a crowdsourcing based system called ARchitect, using the
crowd to capture the dependency structure of the actions that
make up activities. Such crowd-sourcing methods can complement the plan-corpus based approach proposed in DUP.

Conclusion and Discussion
In this paper we present a novel plan recognition approach
DUP based on vector representation of actions. We first learn
the vector representations of actions from plan libraries using the Skip-gram model which has been demonstrated to
be effective. We then discover unobserved actions with the
vector representations by repeatedly sampling actions and
optimizing the probability of potential plans to be recognized. We also empirically exhibit the effectiveness of our
approach.
While we focused on a one-shot recognition task in this
paper, in practice, human-in-the-loop planning will consist
of multiple iterations, with DUP recognizing the plan and
suggesting action addition alternatives; the human making a
selection and revising the plan. The aim is to provide a form
of flexible plan completion tool, akin to auto-completers for
search engine queries. To do this efficiently, we need to make
the DUP recognition algorithm ‚Äúincremental.‚Äù
The word-vector based domain model we developed in
this paper provides interesting contrasts to the standard precondition and effect based action models used in automated
planning community. One of our future aims is to provide a more systematic comparison of the tradeoffs offered
by these models. Although we have focused on the ‚Äúplan
recognition‚Äù aspects of this model until now, and assumed
that ‚Äúplanning support‚Äù will be limited to suggesting potential actions to the humans. In future, we will also consider ‚Äúcritiquing‚Äù the plans being generated by the humans
(e.g. detecting that an action introduced by the human is not
consistent with the model learned by DUP), and ‚Äúexplaining/justifying‚Äù the suggestions generated by humans. Here,
we cannot expect causal explanations of the sorts that can
be generated with the help of complete action models (e.g.
(Petrie 1992)), and will have to develop justifications analogous to those used in recommendation systems.

References
[Amir and Gal 2011] Amir, O., and Gal, Y. K. 2011. Plan
recognition in virtual laboratories. In Proceedings of IJCAI,
2392‚Äì2397.
[Bui 2003] Bui, H. H. 2003. A general model for online
probabilistic plan recognition. In Proceedings of IJCAI,
1309‚Äì1318.
[Cohen et al. 2015] Cohen, P. R.; Kaiser, E. C.; Buchanan,
M. C.; Lind, S.; Corrigan, M. J.; and Wesson, R. M. 2015.
Sketch-thru-plan: a multimodal interface for command and
control. Commun. ACM 58(4):56‚Äì65.
[Dong et al. 2004] Dong, X.; Halevy, A. Y.; Madhavan, J.;
Nemes, E.; and Zhang, J. 2004. Simlarity search for web services. In (e)Proceedings of the Thirtieth International Conference on Very Large Data Bases, Toronto, Canada, August
31 - September 3 2004, 372‚Äì383.
[Ferguson, Allen, and Miller 1996] Ferguson, G.; Allen, J.;
and Miller, B. 1996. Trains-95: Towards a mixed-initiative
planning assistant. In Proceedings of the Third Conference
on Artificial Intelligence Planning Systems (AIPS-96), 70‚Äì
77. Edinburgh, Scotland.
[Geib and Goldman 2009] Geib, C. W., and Goldman, R. P.
2009. A probabilistic plan recognition algorithm based on
plan tree grammars. Artificial Intelligence 173(11):1101‚Äì
1132.
[Geib and Steedman 2007] Geib, C. W., and Steedman, M.
2007. On natural language processing and plan recognition. In IJCAI 2007, Proceedings of the 20th International
Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, 1612‚Äì1617.
[Kabanza et al. 2013] Kabanza, F.; Filion, J.; Benaskeur,
A. R.; and Irandoust, H. 2013. Controlling the hypothesis
space in probabilistic plan recognition. In IJCAI.
[Kambhampati and Talamadupula 2015] Kambhampati, S.,
and Talamadupula, K. 2015. Human-in-the-loop planning
and decision support. rakaposhi.eas.asu.edu/hilp-tutorial.
[Kambhampati 2007] Kambhampati, S. 2007. Model-lite
planning for the web age masses: The challenges of planning
with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia,
Canada, 1601‚Äì1605.
[Kautz and Allen 1986] Kautz, H. A., and Allen, J. F. 1986.
Generalized plan recognition. In Proceedings of AAAI, 32‚Äì
37.
[Lasecki et al. 2013] Lasecki, W. S.; Song, Y. C.; Kautz,
H. A.; and Bigham, J. P. 2013. Real-time crowd labeling
for deployable activity recognition. In CSCW, 1203‚Äì1212.
[Lasecki et al. 2014] Lasecki, W. S.; Weingard, L.; Ferguson,
G.; and Bigham, J. P. 2014. Finding dependencies between
actions using the crowd. In Proceedings of CHI, 3095‚Äì3098.
[Lesh and Etzioni 1995] Lesh, N., and Etzioni, O. 1995. A
sound and fast goal recognizer. In IJCAI, 1704‚Äì1710.
[Manikonda et al. 2014] Manikonda, L.; Chakraborti, T.; De,
S.; Talamadupula, K.; and Kambhampati, S. 2014. AI-MIX:

using automated planning to steer human workers towards
better crowdsourced plans. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27
-31, 2014, QueÃÅbec City, QueÃÅbec, Canada., 3004‚Äì3009.
[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.;
Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In
NIPS, 3111‚Äì3119.
[Petrie 1992] Petrie, C. J. 1992. Constrained decision revision. In Proceedings of the 10th National Conference on
Artificial Intelligence. San Jose, CA, July 12-16, 1992., 393‚Äì
400.
[Ramƒ±ÃÅrez and Geffner 2009a] Ramƒ±ÃÅrez, M., and Geffner, H.
2009a. Plan recognition as planning. In IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17,
2009, 1778‚Äì1783.
[Ramirez and Geffner 2009b] Ramirez, M., and Geffner, H.
2009b. Plan recognition as planning. In Proceedings of IJCAI, 1778‚Äì1783.
[Ramirez and Geffner 2010] Ramirez, M., and Geffner, H.
2010. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of AAAI, 1121‚Äì1126.
[Saria and Mahadevan 2004] Saria, S., and Mahadevan, S.
2004. Probabilistic plan recognitionin multiagent systems.
In Proceedings of AAAI.
[Singla and Mooney 2011] Singla, P., and Mooney, R. 2011.
Abductive markov logic for plan recognition. In Proceedings of AAAI, 1069‚Äì1075.
[Yang, Wu, and Jiang 2007] Yang, Q.; Wu, K.; and Jiang, Y.
2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence Journal
171:107‚Äì143.
[Zhao, Sukthankar, and Sukthankar 2011] Zhao, L.; Sukthankar, G.; and Sukthankar, R. 2011. Robust active
learning using crowdsourced annotations for activity
recognition. In AAAI workshop.
[Zhuo et al. 2010] Zhuo, H. H.; Yang, Q.; Hu, D. H.; and Li,
L. 2010. Learning complex action models with quantifiers
and implications. Artificial Intelligence 174(18):1540‚Äì1569.
[Zhuo, Yang, and Kambhampati 2012] Zhuo, H. H.; Yang,
Q.; and Kambhampati, S. 2012. Action-model based multiagent plan recognition. In Advances in Neural Information
Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of
a meeting held December 3-6, 2012, Lake Tahoe, Nevada,
United States., 377‚Äì385.

J Intell Inf Syst
DOI 10.1007/s10844-015-0366-3

Click efficiency: a unified optimal ranking for online Ads
and documents
Raju Balakrishnan1 ¬∑ Subbarao Kambhampati2

Received: 21 December 2013 / Revised: 23 March 2015 / Accepted: 12 May 2015
¬© Springer Science+Business Media New York 2015

Abstract Ranking of search results and ads has traditionally been studied separately. The
probability ranking principle is commonly used to rank the search results while the ranking
based on expected profits is commonly used for paid placement of ads. These rankings try
to maximize the expected utilities based on the user click models. Recent empirical analysis on search engine logs suggests unified click models for both ranked ads and search
results (documents). These new models consider parameters of (i) probability of the user
abandoning browsing results (ii) perceived relevance of result snippets. However, current
document and ad ranking methods do not consider these parameters. In this paper we propose a generalized ranking function‚Äînamely Click Efficiency (CE)‚Äîfor documents and
ads based on empirically proven user click models. The ranking considers parameters (i)
and (ii) above, optimal and has the same time complexity as sorting. Furthermore, the CE
ranking exploits the commonality of click models, hence is applicable for both documents
and ads. We examine the reduced forms of CE ranking based upon different underlying
assumptions, enumerating a hierarchy of ranking functions. Interestingly, some of the rankings in the hierarchy are currently used ad and document ranking functions; while others
suggest new rankings. Thus, this hierarchy illustrates the relationships between different
rankings, and clarifies the underlying assumptions. While optimality of ranking is sufficient for document ranking, applying CE ranking to ad auctions requires an appropriate
pricing mechanism. We incorporate a second price based mechanism with the proposed
ranking. Our analysis proves several desirable properties including revenue dominance over

 Raju Balakrishnan

raju.balakrishnan@gmail.com
Subbarao Kambhampati
rao@asu.edu
1

Groupon., Park Blvd, Palo Alto, CA 94306, USA

2

Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

J Intell Inf Syst

Vickrey Clarke Groves (VCG) for the same bid vector and existence of a Nash equilibrium
in pure strategies. The equilibrium is socially optimal, and revenue equivalent to the truthful
VCG equilibrium. As a result of its generality, the auction mechanism and the equilibrium
reduces to the current mechanisms including Generalized Second Price Auction (GSP) and
corresponding equilibria. Furthermore, we relax the independence assumption in CE ranking and analyze the diversity ranking problem. We show that optimal diversity ranking is
NP-Hard in general, and a constant time approximation algorithm is not likely. Finally our
simulations to quantify the amount of increase in different utility functions conform to the
results, and suggest potentially significant increase in utilities.
Keywords Ad ranking ¬∑ Document ranking ¬∑ Diversity ¬∑ Auctions ¬∑ Click models

1 Introduction
Search engines rank results to maximize the relevance of the top documents. On the other
hand, targeted ads are ranked primarily to maximize the profit from clicks. In general,
users browse through ranked lists of search results or ads from top to bottom, either clicking or skipping the results, or abandoning browsing the list altogether due to impatience
or satiation. The goal of the ranking is to maximize the expected relevances (or profits) of
clicked results based on the click model of the users. The sort by relevance ranking suggested by Probability Ranking Principle (PRP) has been commonly used for search results
for decades (Robertson 1977; Gordon and Lenk 1991). In contrast, sorting by the expected
profits calculated as the product of bid amount and Click Through Rate (CTR) is popular
for ranking ads (Richardson et al. 2007).
Recent click models suggests that the user click behaviors for both search results and targeted ads is the same (Guo et al. 2009; Zhu et al. 2010). Considering this commonality, the
only difference between the two ranking problems is the utility of entities ranked: for documents utility is the relevance and for the ads it is the cost-per-click (CPC). This suggests the
possibility of a unified ranking function for search results and ads. The current segregation
of document and ad ranking as separate areas does not consider this commonality. A unified approach often helps to widen the scope of the related research to these two areas, and
enables applications of existing ranking function in one area on isomorphic problems in the
other area as we will show below.
In addition to the unified approach, the recent click models consider the following
parameters:
1.

2.

Browsing Abandonment: The user may abandon browsing ranked list at any point.
The likelihood of abandonment may depend on the entities the user has already
seen (Zhu et al. 2010).
Perceived Relevance: Perceived relevance is the user‚Äôs relevance assessment viewing
only the search snippet or ad impression. The decision to click or not depends on the
perceived relevance, not on the actual relevance of the results (Yue et al. 2010; Clarke
et al. 2007).

Though these parameters are part of the click models (Guo et al. 2009; Zhu et al. 2010) how
to exploit these parameters to improve ranking is currently unknown. The current document
ranking is based on the simplifying assumption that the perceived relevance is the same as
the actual relevance of the document, and ignores browsing abandonment. The ad placement
partially considers perceived relevance, but ignores abandonment probabilities.

J Intell Inf Syst

In this paper, we propose a unified optimal ranking function‚Äînamely Click Efficiency
(CE)‚Äîbased on a generalized click model of the user. CE is defined as the ratio of the
standalone utility generated by an entity to the sum of the abandonment probability and the
click probability of that entity, where the abandonment probability is the probability for the
user to leave browsing the list after seeing the entity. We show that sorting entities in the
descending order of CE guarantees optimum ranking utility. We do not make assumptions
on the utilities of the entities, which may be assessed relevance for documents or cost per
click (CPC) charged based on the auction for ads. On plugging in the appropriate utilities‚Äî
relevance for documents and CPC for the ads‚Äîthe ranking specializes to document and ad
ranking.
As a consequence of the generality, the proposed ranking will reduce to specific ranking
problems on assumptions about user behavior. We enumerate a hierarchy of ranking functions corresponding to different assumptions on the click model. Most interestingly, some of
these special cases correspond to the currently used document and ad ranking functions‚Äî
including PRP and sort by expected profit described above. Further, some of the reduced
ranking functions suggest new rankings for special cases of the click model‚Äîlike a click
model in which the user never abandons the search, or the perceived relevance is approximated as the actual relevance. This hierarchy elucidates interconnection between different
ranking functions and the assumptions behind the rankings. We believe that this will help in
choosing the appropriate ranking function for a particular user click behavior.
Ranking in ad placement used in conjunction with a pricing strategy to form the complete
auction mechanism. Hence to apply the CE ranking on ad placement, a pricing mechanism has to be associated. We incorporate a second-price based pricing mechanism with
the proposed ranking. Our analysis establishes many interesting properties of the proposed
mechanism. Particularly, we state and prove the existence of a Nash Equilibrium in pure
strategies. At this equilibrium, the profits of the search engine and the total revenue of the
advertisers is simultaneously optimized. Like ranking, the proposed auction this is a generalized mechanism, and reduces to the existing GSP and Overture mechanisms under the
same assumptions as that of the ranking. Further, the stated Nash Equilibrium is a general
case of the equilibriums of these existing mechanisms. Comparing the mechanism properties with that of VCG (Vickrey 1961; Clarke 1971; Groves 1973), we show that for the same
bid vector, search engine revenue for the CE mechanism is greater or equal to that of VCG.
Furthermore, the revenue for the proposed equilibrium is equal to the revenue of the truthful
dominant strategy equilibrium of VCG.
Our analysis so far has been based on the assumption of parameter independence between
the ranked entities. We relax this assumption and analyze the implications based on a specific well known problem‚Äîdiversity ranking (Carterette 2010; Agrawal et al. 2009; Rafiei
et al. 2010). Diversity ranking tries to maximize the collective utility of top-k ranked entities. For a ranked list, an entity will reduce residual utility of a similar entity in the list blow
it. Though optimizing all the current ranking functions incorporating diversity is known to
be NP-Hard (Carterette 2010), an understanding of why this is an inherently hard problem
is lacking. We show that optimizing set utilities is NP-Hard even for the basic form of diversity ranking. Furthermore we extend our proof showing that a constant ratio approximation
algorithm is unlikely. As a benefit of the generality of ranking, these results are applicable
both for ads and documents.
Although we prove the optimality of the proposed ranking, the amount by which the
profit may improve is not clear. Considering the very restricted access to online experiments on ads, we performed simulations to this end. We compare the profit improvement
by the CE and reduced forms to existing rankings. These experiments suggest potentially

J Intell Inf Syst

significant increase in profits. We believe that these experiments will motivate further online
evaluations.
In summary, the contributions of the unified ranking, including both ad and document
domains are:
1.
2.
3.
4.
5.

Unified optimal ranking.
Optimal ranking considering abandonment probabilities for documents and ads.
Optimal Ranking considering perceived relevance of documents and ads.
A unified hierarchy of ranking functions and enumerating optimal rankings for different
click models.
Analysis of general diversity ranking problem and hardness proofs.

Our contributions to ad placement are:
1.
2.
3.

Design and analysis of a generalized ad auction mechanism incorporating pricing with
CE ranking.
Proof of the existence of a socially optimal Nash Equilibrium with optimal advertisers
revenue as well as optimal search engine profit.
Proof of search engine revenue dominance over VCG for equivalent bid vectors, and
equilibrium revenue equivalence to the truthful VCG equilibrium.

1.1 Background
In search and search advertising, both search results and ads are ranked to maximize utility.
At a high level, search results are ranked to maximize the information content (or relevance)
of the top documents to the users; whereas ads are ranked to maximize both the relevance
as well as the profit to the search engines. Users generally browse through ranked search
results starting from the top, either clicking or skipping the results. This browsing pattern
of users is called the click model. Search and ad rankings try to maximize the utility to the
users based on a click model.
In addition to the standalone relevance of the results, another important aspect of ranking
is the diversity of the results. Although information contained in a document may be highly
relevant, if the information is similar to that in the documents above in the ranking, the document will be of little utility. To account for this factor, the mutual influence of documents
or ads ranked needs to be considered to maximize total utility by a set of documents rather
than individual documents. To account for this factor, diversity-sensitive ranking maximizes
residual relevance of ads or documents in the context of other items in the ranked list.
In search ad ranking (paid placements), ads are selected based on the user query. Generally, the click model for ads is similar to that of the search results. In the most common
pay-per-click ad campaigns, advertisers pay a certain amount to the search engines whenever a user clicks on their ads. This amount is determined by a pricing mechanism. The
advertisers place a bid on the queries. The ads are ranked based on the bid amounts and
relevance of the ad to the query. For example, in commonly used Generalized Second
Price (GSP) auction Edelman et al. (2007) ads are ranked by the product of their click
rates (ratio of the number of clicks to impressions) and bid amounts. The amount the
advertisers pay to the search engine need not be equal to the bid amount, but rather determined by the pricing mechanism. For example, in GSP auction, this amount is determined
based the the bid amount and the click rates of the given ad and the ad placed below the
given ad. Thus ranking and pricing together determines the auction mechanism of the ad
placement.

J Intell Inf Syst

The rest of this paper is organized as the follows. The next section reviews related work.
Section 3 explains the click model used for our analysis. Subsequently we introduce our
optimal ranking function, and discuss the intuitions and implications. In Section 5 reductions of our ranking function to several document and ad ranking functions under limiting
assumptions are enumerated. Furthermore we discuss several useful special cases of our
ranking and assumptions under which they are optimal. In Section 6, we incorporate a pricing strategy to design a complete auction mechanism for ads. Several useful properties are
established, including the existence of a Nash equilibrium and revenue dominance over
VCG. Section 7 explores the ranking considering mutual influences and proves our hardness results. We present the experiments and results in Section 8. Finally we discuss our
conclusions and discuss potential future research directions.

2 Related work
The impact of click models on ranking has been analyzed in ad-placement. In our previous
paper Balakrishnan and Kambhampati (2008) we proposed an optimal ad ranking considering mutual influences. The ranking uses the same user model, but the paper considers only
ad ranking, and does not include generalizations and auctions. Later Aggarwal et al. (2008)
as well as Kempe and Mahdian (2008) analyzed placement of ads using a similar Markovian click model. The click model used is less detailed than our model since abandonment
is not modeled separately from click probability. These two papers optimize the sum of the
revenues of the advertisers. We optimize search engine profits in this paper. Nevertheless,
the ranking formulation has common components with these two papers, as workshop version of this paper Balakrishnan and Kambhampati (2008) as these three papers formulated
ranking based on the similar browsing models independently at almost the same time frame.
But, unlike this paper, any of the other two papers do not have a pricing, auctions, or a
generalized taxonomy.
Edelman et al. (2007) analyze a version of GSP auction in their classic paper. They
assume that the click probability at a position is a constant. We relax this assumption, and
account for the influence of ads above on the click probabilities at a position. This difference
gives rise to additional complexities and interesting differences in our mechanism. We show
that GSP proposed by Edelman et al. is a special case of our proposed mechanism.
Giotis and Karlin (2008) extend Markovian model ranking by applying GSP pricing and
analyzing the equilibrium. The GSP pricing and ranking lacks the optimality and generality
properties we prove in this paper. Deng and Yu (2009) extend Markovian models by suggesting a ranking and pricing schema for the search engines and prove the existence of a
Nash Equilibrium. The ranking is a simpler bid based ranking (not based on CPC as in our
case); and mechanism as well as equilibrium do not show optimality properties. Our paper
is different from both the above works by using a more detailed model, by having optimality properties, detailed comparisons with other baseline mechanisms, and in the ability to
generalize to a family of rankings.
Kuminov and Tennenholtz (2009) proposed a Pay Per Action (PPA) model similar to
the click models and compared the equilibrium of GSP mechanism on the model with the
VCG. Ad auctions considering influence of other ads on conversion rates are analyzed by
Ghosh and Sayedi (2010). Both these papers address different problems than considered in
this paper.
Our proposed model is a general case of the positional auctions model by Varian (2007).
Positional auctions assume static click probabilities for each position independent of other

J Intell Inf Syst

ads. We assume more realistic dynamic click probabilities depending on the ads above.
Since we consider these externalities, our model, auction, and analysis are more complex.
(e.g. monotonically increasing values and prices with positions).
The existing document ranking based on PRP (Robertson 1977) claims that a retrieval
order sorted on relevance leads to the largest number of relevant documents in a result set
than any other policy. Gordon and Lenk (1991, 1992) identified the required assumptions for
the optimality of the ranking according to PRP. Our discussion on PRP may be considered
as an independent formulation of assumptions under which PRP is optimal for web ranking.
There are number of user behavior studies in click models validating our assumed user
model and ranking function. There are a number of position based and cascade models
studied (Dupret and Piwowarski 2008; Craswell et al. 2008; Guo et al. 2009; Chapelle and
Zhang 2009; Zhu et al. 2010; Xu et al. 2010; Hu et al. 2011). In particular, General Click
Model (GCM) by Zhu et al. (2010) is interesting, since many other click models are special
cases of GCM. Zhu et al. (2010) list assumptions under which the GCM would reduce to
other click models. We will discuss the relations of our model to GCM below. Optimizing
utilities of two dimensional placement of search results has been studied by Chierichetti
et al. (2011). Many of the recent click models are more general than the click model used
in our paper, but please note that the contribution of our paper is not the click model, but a
unified optimal ranking and auction mechanism based on the click model.
Along with the current click models, there has been research on evaluating perceived
relevance of the search snippets (Yue et al. 2010) and ad impressions (Clarke et al. 2007).
Research in this direction neatly complements our new ranking function by estimating the
parameters required. Chapelle and Zhang (2009) demonstrated that separately modeling perceived and actual relevances improves relevance assessment of documents using click logs.
Diversity ranking has received considerable attention recently (Agrawal et al. 2009;
Rafiei et al. 2010). The objective functions used to measure diversity by prior works are
known to be NP-Hard (Carterette 2010). We provide a stronger proof showing that even
the basic diversity ranking problem is NP-Hard irrespective of any specific objective function, and further show that a constant ratio approximation is unlikely. To the best of our
knowledge, this paper is the first unified optimal ranking and auction mechanism based on
a generalized click model.

3 Click model
As we mentioned above, we approach the ranking as an optimization based on the user‚Äôs
click model on the ads. The expected utilities are maximized based on the click model.
For the optimization, we assume a basic user click model in which the web user browses
the entity list in the ranked order, as shown in Fig. 1. The symbols used in this paper are
explained in Table 1. At every result entity, the user may:
1.

2.

3.

Click the result with perceived relevance C(e). We define the perceived relevance as the
probability of clicking the entity ei having seen ei i.e. C(ei ) = P (click(ei )|view(ei )).
Note that the Click Through Rate (CTR) defined in ad placement is the same as the
perceived relevance defined here (Richardson et al. 2007).
Abandon browsing the result list with abandonment probability Œ≥ (ei ). Œ≥ (ei ) is defined
as the probability of abandoning the search at ei having seen ei . i.e. Œ≥ (ei ) =
P (abandonment (ei )|view(ei )).
Go to the next entity with probability [1 ‚àí (C(ei ) + Œ≥ (ei ))]

J Intell Inf Syst
Table 1 Definition of the symbols
e

A ranked entity.

C(e)

Perceived relevance.

Œ≥ (e)

Abandonment probability.

U (e)

Utility.

Pc (e)

The click probability of the entity at position i in the ranking.

d

A ranked document.

R(d)

Relevance of the document.

a

A ranked ad.

SE

An abbreviation indicating Search Engine.

$(a)

Cost-Per-Click (CPC) of the ad.

v(a)

Private value of the ad for the advertiser.

b(a)

Bid for the ad.

w(a)

Ratio of the click probability to the sum of abandonment and click probability.

Œº(a)

Sum of abandonment and click probability (i.e. C(a) + Œ≥ (a)).

CE(a)

Proposed Click-Efficiency ranking score of the ad.

pi

Payment by the advertiser (CPC) to the search in a given mechanism.

Ur (e)

Residual utility in the context of other entities in the ranked list.

Œ±

Simulation constant to balance between the click and the abandonment probabilities.

The click model can be schematically represented as the flow graph shown in Fig. 1.
Labels on the edges refer to the probability of the user traversing them. Each vertex in the
figure corresponds to a view epoch (see below), and the flow balance holds at each vertex.
Starting from the top entity, the probability of the user clicking the first ad is C(e1 ) and
probability of him abandoning browsing is Œ≥ (e1 ). The user goes beyond the first entity with
probability 1 ‚àí (C(e1 ) + Œ≥ (e1 )) and so on for the subsequent results.
In this model, we assume that the parameters‚ÄîC(ei ), Œ≥ (ei ) and U (ei )‚Äîare functions
of the entity at the current position i.e. these parameters are independent of other entities
the user has already seen. We recognize that this assumption is not fully accurate, since
the user‚Äôs decision to click the current item or to leave the search may depend not just
on the current item but rather on all the entities he has seen before in the list. We stick
to the assumption for the optimal ranking analysis below, since considering mutual influence of ads may lead to combinatorial optimization problems with intractable solutions. We

Fig. 1 Flow graph for an user browsing the first two entities. The labels are the view probabilities and ei
denotes the entity at the i th position

J Intell Inf Syst

will show that even the simplest dependence between the parameters will indeed lead to
intractable optimal ranking in Section 7.
Although the proposed model is intuitive enough, we would like to mention that our
model is also confirmed by the recent empirical click models. For example, the General
Click Model (GCM) by Zhu et al. (2010) is based on the same basic user behavior. The GCM
is empirically validated for both search results and ads (Zhu et al. 2010). Furthermore, other
click models are shown to be special cases of GCM. Please refer to Zhu et al. (2010) for
a detailed discussion. These previous works avoid the need for separate model validation,
as well as confirm the feasibility of the parameter estimation. Further, Yilmaz et al. (2010)
proposes an expected browsing utility metric based on a similar user model.

4 Optimal ranking
Based on the click model, we formally define the ranking problem and derive optimal
ranking in this section. The problem may be stated as,
Choose the optimal ranking Eopt = e1 , e2 , .., eN  of N entities to maximize the
expected utility
N

E(U ) =
U (ei )Pc (ei )
(1)
i=1

where N is the total number of entities to be ranked.
The utility function U (ei ) denotes the stand-alone utility of the entity ei to the search
engine (or one who performs the ranking). This may vary depending on the specific ranking problem. For example, for ranking search results, the utility will be the relevance of
document ei ; whereas for ranking ads to maximize the revenue of the search engine, the
U (ei ) will be pay-per-click of ad ei . We define the specific utility function for entities as
we discuss the specific ranking problems below.
For the browsing model in Fig. 1, the click probability for the entity at the i th position is,
Pc (ei ) = C(ei )

i‚àí1




1 ‚àí C(ej ) + Œ≥ (ej )

(2)

j =1

Substituting click probability Pc from (2) in (1) we get,
E(U ) =

N

i=1

U (ei )C(ei )

i‚àí1



1 ‚àí (C(ej ) + Œ≥ (ej ))



(3)

j =1

The optimal ranking maximizing this expected utility can be shown to be a sorting
problem with a simple ranking function:
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
U (ei )C(ei )
(4)
CE(ei ) =
C(ei ) + Œ≥ (ei )
Proof Sketch The proof shows that any inversion in this order will reduce the expected
profit. CE function is deduced from expected profits of two placements‚Äîthe CE ranked

J Intell Inf Syst

placement and placement in which the order of two adjacent ads are inverted. We show
that the expected profit from the inverted placement can be no greater than the CE ranked
placement. Please refer to Appendix A-1 for the complete proof.
As mentioned in the introduction, the ranking function CE is the utility generated per
unit view probability consumed by the entity. With respect to browsing model in Fig. 1, the
top entities in the ranked list have greater view probabilities, and placing ads with greater
utility per consumed view probability at higher positions intuitively increases total utility.
The proof of Theorem 1 assumes that the user clicks only one entity in the list. Since this
may not always be true, we extend the optimality to multiple clicks in Theorem 2.
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Sketch We proved that ordering according to CE provides maximum expected utility
for single click above. Multiple clicks are the same as the user restarting her browsing from
the entity immediately below the last clicked entity. A simple induction on number of clicks
based on this idea, using a single click as base case is sufficient to prove that the proposed
placement provides maximum expected utility for multiple clicks. See Appendix A-2 for
the complete proof.
Note that the ordering above does not maximize the utility for selecting a subset of items.
The seemingly intuitive method of ranking the set of items by CE and selecting top-k may
not be optimal (Aggarwal et al. 2008). For optimal selection, the proposed ranking can be
extended by a dynamic programming based selection (Aggarwal et al. 2008). In this paper,
we discuss only the ranking problem.

5 Ranking taxonomy
The click model in Fig. 1 is common to many types of rankings including document searches
and search ads. The only difference between these rankings sharing a common click model
is the utility to be maximized. Consequently, the CE ranking can be made applicable to
different ranking problems by plugging in different utilities. For example, if we plug in relevance as utility (U (e) in (4)), the ranking function is applicable for the documents, whereas
if we plug in cost per click of ads, the ranking function is applicable to ads. Furthermore,
we may assume specific constraints on one or more of the three parameters of CE ranking (e.g. ‚àÄi Œ≥ (ei ) = 0). Through these assumptions, CE ranking will suggest a number of
reduced ranking functions with specific applications. These substitutions and reductions can
be enumerated as a taxonomy of ranking functions.
We show the taxonomy in Fig. 2. The three top branches of the taxonomy (U (e) = R(d),
U (e) = $(a), and U (e) = v(a) branches) are for document ranking, ad ranking maximizing
search engine profit, and ad ranking maximizing advertisers revenue respectively. These
branches correspond to the substitution of utilities by document relevance, CPC, and private
value of the advertisers. The sub-trees below these branches are the further reduced cases of
these three main categories. The solid lines in Fig. 2 denote already known functions, while
the dotted lines are the new ranking functions suggested by CE ranking. Sections 5.1, 5.2,
and 5.3 below discuss the further reductions of document ranking, search engine optimal
ad ranking, and social optimal ad ranking respectively.

J Intell Inf Syst

Fig. 2 Taxonomy reduced CE ranking functions. The assumptions and corresponding reduced ranking functions are illustrated. The dotted lines denote predicted ranking functions incorporating new click model
parameters

5.1 Optimal document ranking
For document ranking the utility of ranking is the probability of relevance of the document.
Hence by substituting the document relevance‚Äîdenoted by R(d)‚Äîin (4) we get
CE(d) =

C(d)R(d)
C(d) + Œ≥ (d)

(5)

This function suggests the general optimal relevance ranking for the documents. We discuss
some intuitively valid assumptions on user model for the document ranking and the corresponding ranking functions below. The three assumptions discussed below correspond to
the three branches under Optimal Document Ranking subtree in Fig. 2.

Sort by relevance (PRP) We elucidate two sets of assumptions under which the CE(d)
in (5) will reduce to PRP.
First assume that the user has infinite patience, and never abandons results (i.e. Œ≥ (d) ‚âà
0). Substituting this assumption in (5),
R(d)C(d)
= R(d)
(6)
C(d)
which is exactly the ranking suggested by PRP.
In other words, the PRP is still optimal for scenarios in which the user has infinite
patience and never abandons checking the results (i.e. the user leaves browsing the results
only by clicking a result).
The second set of slightly weaker assumptions under which the CE(d) will reduce to
PRP is
CE(d) ‚âà

1.

C(d) ‚âà R(d).

J Intell Inf Syst

2.

Abandonment probability Œ≥ (d) is negatively proportional to the document relevance
i.e. Œ≥ (d) ‚âà k ‚àí R(d), where k is a constant between one and zero. This assumption
corresponds to the intuition that the higher the perceived relevance of the current result,
the less likely is the user abandoning the search.

Now CE(d) reduces to,
R(d)2
(7)
k
Since this function is strictly increasing with zero and positive values of R(d), ordering just
by R(d) results in the same ranking as suggested by the function. This implies that PRP is
optimal under these assumptions also.
It may be noted that abandonment probability decreasing with perceived relevance is a
more intuitively valid assumption than the infinite patience assumption above.
CE(d) ‚âà

Ranking considering perceived relevance Recent click log studies effectively assess
perceived relevance of document search snippets (Yue et al. 2010; Clarke et al. 2007). But,
how to use the perceived relevance for improved document ranking is still an open question.
The proposed perceived relevance ranking addresses this question.
If we assume that Œ≥ (d) ‚âà 0 in (5), the optimal perceived relevance ranking is the same
as that suggested by PRP as we have seen in (6).
On the other hand, if we assume that the abandonment probability is negatively proportional to the perceived relevance (Œ≥ (d) = k ‚àí C(d)) as above, the optimal ranking
considering perceived relevance is
C(d)R(d)
‚àù C(d)R(d)
(8)
k
i.e. sorting in the order of the product of document relevance and perceived relevance is
optimal under these assumptions. The assumption of abandonment probabilities being negatively proportional to relevance is more realistic than the infinite patience assumption as
we discussed above. This discussion shows that by estimating the nature of abandonment
probability, one would be able to decide on the optimal perceived relevance ranking.
CE(d) ‚âà

Ranking considering abandonment We now examine the ranking considering abandonment probability Œ≥ (d), with the assumption that the perceived relevance is approximately
equal to the actual relevance. In this case CE(d) becomes,
CE(d) ‚âà

R(d)2
R(d) + Œ≥ (d)

(9)

Clearly this is not a strictly increasing function with R(d). Hence the ranking considering
abandonment is different from PRP ranking, even if we assume that the perceived relevance
is equal to the actual relevance. assumption that ‚àÄd Œ≥ (d) = 0, the abandonment ranking
becomes the same as PRP.

5.2 Optimal Ad ranking for search engines
For the paid placement of ads, the utilities of ads to the search engine are Cost-Per-Click
(CPC) of the ads. Hence, by substituting the CPC of the ad‚Äîdenoted by $(a)‚Äî in (4) we
get
C(a)$(a)
(10)
CE(a) =
C(a) + Œ≥ (a)

J Intell Inf Syst

Thus this function suggests the general optimal ranking for the ads. Please recall
that the perceived relevance C(a) is the same as the CTR used for ad placement
(Richardson et al. 2007).
In the following subsections we demonstrate how the general ranking presented reduces
to the currently used ad placement strategies under various assumptions. We will show that
they all correspond to specific assumptions about the abandonment probability Œ≥ (a). These
two functions below corresponds to the two branches under the SE (Search Engine) Optimal
Ad Placement subtree in Fig. 2.

Ranking by bid amount The sort by bid amount ranking was used by Overture Services
(and was later used by Yahoo! for a while after acquisition of Overture). Assuming that the
user never abandons browsing (i.e. ‚àÄa Œ≥ (a) = 0), then (10) reduces to
CE(a) = $(a)

(11)

This means that the ads are ranked purely in terms of their payment. In fact overture ranking
is by bid amount, which is different from payment in a second price auction. But both will
result in the same ranking as higher bids implies higher payments also.
When Œ≥ (a) = 0, we essentially have a user with infinite patience who will keep browsing
downwards until he finds a relevant ad. Hence ranking by bid amount maximizes profit.
More generally, for small abandonment probabilities, ranking by bid amount is near optimal.
Note that this ranking is isomorphic to PRP ranking discussed above for document ranking,
since both ranks are based only on utilities.

Ranking by expected profit Google and Microsoft supposedly place the ads in the order
of expected profit based on product of CTR (C(a) in CE) and bid amount ($(a)) (Richardson et al. 2006). The mechanism is called Generalized Second Price (GSP) auction, and
the most popular one as well. If we approximate abandonment probability as negatively
proportional to the CTR of the ad (i.e. ‚àÄa Œ≥ (a) = k ‚àí C(a)) , the (10) reduces to,
$(a)R(a)
‚àù $(a)R(a)
(12)
k
This shows that ranking ads by their standalone expected profit is near optimal as long as
the abandonment probability is negatively proportional to the relevance. To be accurate,
the Google mechanism‚ÄîGSP‚Äîuses the bid amount of the advertisers (instead of CPC in
(12)) for ranking. Although CPC and bids are different for GSP, we will show that both will
result in the same ranking in Section 6. Note that this ranking is isomorphic to the perceived
relevance ranking of documents discussed above.
CE(a) ‚âà

5.3 Social optimal Ad ranking
An important property of any auction mechanism is social utility, i.e. total utilities of all
the players. In our case this is equal to the sum of the utilities of all the advertisers and
the search engine. To analyze advertiser‚Äôs profit, a private value model is commonly used.
Each advertiser has a private value for the click, which is equal to the expected benefit
(direct and indirect revenue) from the click. Advertisers pay a fraction of this benefit to the
search engine as CPC. The utility for the advertisers is the difference between the private
value and payment to the search engine. The utility for the search engine is the payment
from the advertisers. Hence the social utility is equal to the sum of private values of all the
clicks for the advertisers (which is the sum of utilities of the search engine and advertisers).

J Intell Inf Syst

Consequently, to prove the social optimality all we need to prove is that the total private
values of clicks for the advertisers is optimal.
The social-optimal branch in Fig. 2 corresponds to the ranking to maximize total revenue.
Private value of advertisers ai is denoted as‚Äîv(ai ). By substituting the utility by private
values in (4) we get,
CE(d) =

C(a)v(a)
C(a) + Œ≥ (a)

(13)

If the ads are ranked in this order, the ranking will guarantee maximum revenue. Note that
the optimal revenue does not imply optimal net profits for the advertisers, since part of this
revenue is paid to the search engine as CPC. But optimal revenue implies a maximum total
profit (utility)‚Äîsum of profits of search engine and advertisers.
In Figure 2 the two left branches of the Social Optimal subtree (labeled Œ≥ (a) = 0 and
Œ≥ (a) = k ‚àí C(a)) correspond respectively to the assumption of no abandonment, and
abandonment probabilities being negatively proportional to the click probability. These two
cases are isomorphic to the Overture and Google ranking discussed in Section 5.2 above.
The social optimal ranking is not directly implementable as search engines do not know
the private value of the advertisers. But this ranking is useful in analysis of auctions mechanisms. Furthermore, the search engine may try to effectuate this order through auction
mechanism equilibriums as we demonstrate in Section 6.

6 Applying CE ranking for Ad placement
We have shown that CE ranking maximizes the profits for search engines for given CPCs.
The CPCs are determined by the pricing mechanism used by the search engine. Hence
the overall profit of ranking can be analyzed only in association with a pricing mechanism. The existing ad pricing mechanisms like GSP do not preserve any of their appealing
properties for CE ranking as they do not consider the additional parameter abandonment
probability. For example, the GSP pricing Edelman et al. (2007) is no longer the minimum
amount need to be paid by the advertiser to maintain his position in the CE ranking. To
this end, we design a full auction mechanism by proposing a new second price based pricing to be used with the CE ranking. Subsequently, we analyze the properties of the auction
mechanism.
Let us start by describing the dynamics of ad auctions briefly, the search engine decides
the ranking and pricing (CPC) of the ads based on the bid amounts of the advertisers. Generally the pricing is not equal to the bid amount of advertisers, but derived based on the
bids (Easley and Kleinberg 2010; Edelman et al. 2007; Aggarwal et al. 2006). In response to
these ranking and pricing strategies, the advertisers (more commonly, the software agents of
the advertisers) may change their bids to maximize their profits. They may change bids hundreds of times a day. Eventually, the bids may stabilize at a fixed point where no advertiser
can increase his profit by unilaterally changing his bid, depending on the initial bids and
behavior of the advertisers. This set of bids corresponds to a Nash Equilibrium of the auction
mechanism. Hence the expected profits of a search engine will be the profits corresponding
to the Nash Equilibrium, if the auction attains a Nash Equilibrium.
The next section discusses properties of any mechanism based on the user model‚Äî
independent of the ranking and pricing strategies. In Section 6.2, we introduce a pricing
mechanism and analyze the properties including the equilibrium.

J Intell Inf Syst

6.1 User model based properties
We discuss general properties of all auction mechanisms using the browsing model (Fig. 1).
These properties are implications of the user behavior and applicable to any pricing and
ranking.
Lemma 1 (Individual Rationality) In any equilibrium the payment by the advertisers is less
than or equal to their private values.1
If this is not true, this advertiser may opt out from the auction by bidding zero and
increase the profit, violating the assumption of equilibrium.
Lemma 2 (Pricing Monotonicity) In any equilibrium, the price paid by an advertiser
increases monotonically as he moves up in the ranking unilaterally.
From the browsing model, click probability of the advertisers is non-decreasing as he
moves up in the position. Unless the price increases monotonically, the advertiser may
increase his profit by moving up, thereby violating assumption of an equilibrium.
Lemma 3 (Revenue Maximum) The sum of the payoffs of the advertisers and the search
engine is less than or equal to
E(V ) =

N


v(ai )C(ai )

1 ‚àí (C(aj ) + Œ≥ (aj ))



(14)

j =1

i=1

when the advertisers are ordered by

i‚àí1



C(a)v(a)
C(a)+Œ≥ (a) .

Note that this quantity is the maximal advertiser revenue corresponding to the social
optimal placement in (13), and is a direct consequence. The advertiser pay a fraction of his
revenue to the search engine. Payoff for the advertisers is the difference between the total
revenue and the payment to the search engine. The total payoff of the search engine is the
sum of these payments by all the advertisers. Since the suggested order above in Lemma 3
maximizes total revenue of the advertisers, the sum of the payoffs for the search engine and
the advertisers will not exceed this value.
A corollary of the social optimality combined with the individual rationality result
expressed in Lemma 1 is that,
Lemma 4 (Profit Maximum) The quantity E(V ) in Lemma 3 is an upper bound for the
search engine profit in any equilibrium.

6.2 Pricing and equilibrium
An interesting property of the proposed mechanism is the existence of an equilibrium in
which the search engine optimal ranking coincides with the social optimality. As we proved
above, CE ranking is search engine optimal as it maximizes the revenue for the given CPCs.
On the other hand, social optimal ordering maximizes the total profits for all the players

1 This

property is called individual rationality

J Intell Inf Syst

(search engine and advertisers) for given CPCs. Social optimality is desirable for search
engines, as the increased profits will improve the advertiser‚Äôs preference of one search
engine over others. Since search engines do not know the private value of the advertisers, social optimal ranking is not directly achievable (note that the search engines do the
ranking). A possibility is to design a mechanism having an equilibrium coinciding with the
social optimality, as we propose below. This may cause the bid vector to stabilize in a social
optima.
For defining the pricing strategy for the auction mechanism, we define the pricing order
as the decreasing order of w(a)b(a), where b(a) is the bid value and w(a) is,
w(a) =

C(a)
C(a) + Œ≥ (a)

(15)

In this pricing order, we denote the i th advertiser‚Äôs w(ai ) as wi , C(ai ) as ci , b(ai ) as bi , and
the abandonment probability Œ≥ (ai ) as Œ≥i for convenience. Let Œºi = ci + Œ≥i . For each click,
advertiser ai is charged price pi (CPC) equal to the minimum bid required to maintain its
position in the pricing order,
pi =

wi+1 bi+1
bi+1 ci+1 Œºi
=
wi
Œºi+1 ci

Substituting pi in (10) for the ranking order, CE of the i th advertiser is,
pi ci
CEi =
Œºi

(16)

(17)

This proposed mechanism preserves the pricing order in the ranking as well, i.e.
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj

(18)

The proof for theorem 3 is given in Appendix A-3. This order preservation property
implies that the final ranking is the same as that based on bid amounts. In other words, ads
can be ranked based on the bid amounts instead of CPCs. After the ranking, the CPCs can
be decided based on this ranking order. A corollary of this order preservation is that the
CPC is equal to the minimum amount the advertisers have to pay to maintain their position
in the ranking order.
Furthermore we show below that any advertiser‚Äôs CPC is less than or equal to his bid.
Lemma 5 (Individual Rationality) The payment pi of any advertiser is less or equal to his
bid amount.
Proof
pi =

bi+1 ci+1 Œºi
bi+1 ci+1 Œºi
CEi+1
=
bi =
bi ‚â§ bi (since CEi ‚â• CEi+1 )
Œºi+1 ci
Œºi+1 ci bi
CEi

This means advertisers will never have to pay more than their bid, similar to GSP. This
property makes it easy for the advertiser to decide his bid, as he may bid up to his click
valuation. He will never have to pay more than his revenue, irrespective of bids of other
advertisers.

J Intell Inf Syst

Interestingly, this mechanism is a general case of existing mechanisms, similar to CE
ranking above. The mechanism reduces to GSP (Google mechanism) and Overture mechanisms on the same assumptions on which CE ranking reduces to respective rankings
(described in Section 5.2).
Lemma 6 The mechanism reduces to Overture ranking with a second price auction on the
assumption ‚àÄi Œ≥i = 0
Proof This assumption implies
wi = 1
‚áí pi = bi+1 (second price auction)
‚áí CEi = bi+1 ‚â° bi (i.e. ranking by bi+1 is equivalent to ranking by bi )

Lemma 7 The mechanism reduces to GSP on the assumption ‚àÄi Œ≥i = k ‚àí ci
Proof This assumption implies
wi = c i
bi+1 ci+1
(i.e. ranking reduces to GSP ranking)
ci
bi c i
bi+1 ci+1
‚â°
(by Theorem 3)
‚áí CEi =
k
k
‚àù bi ci
‚áí pi =

This lemma in conjunction with Theorem 3 implies that GSP ranking by ci bi (i.e. by
bids) is the same as the ranking by ci pi (by CPCs).
Now we will look at the equilibrium properties of the mechanism. We start by noting that
truth telling is not a dominant strategy. This trivially follows, since GSP is a special case
of the proposed mechanism, and it is generally known that truth telling is not a dominant
strategy for GSP. Hence we focus on Nash Equilibrium conditions in our analysis.
Theorem 4 (Nash Equilibrium) Without loss of generality, assume that advertisers are
ordered in decreasing order of cŒºi vi i where vi is the private value of the i th advertiser. The
advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
(19)
bi =
ci
Œºi+1
This equilibrium is socially optimal as well as optimal for search engines for the given
CPC‚Äôs.
Proof Sketch The inductive proof shows that for these bid values, no advertisers can
increase his profit by moving up or down in the ranking. The full proof is given in
Appendix A-4. Since the ranking is the same as the social optima order in (13), social
optimality is a direct implication.

J Intell Inf Syst

We do not rule out the existence of multiple equilibriums. The stated equilibrium is
particularly interesting, due to the social optimality and search engine optimality. Furthermore, although the equilibrium depends on the private values of the advertisers unknown
to the search engine, please keep in mind that search engines do not implement equilibriums directly. Instead, search engines decide the pricing and ranking, and the advertisers
may reach an equilibrium by repeatedly revising auction prices. The pricing and ranking are
practical, since they depend solely on the quantities known to the search engine.
The following Lemmas show that equilibriums of other placement mechanisms are special cases of the proposed CE equilibrium. The stated equilibrium reduces to equilibriums
in the Overture mechanism and GSP under the same assumptions (discussed above) under
which the CE ranking reduces to Overture and GSP rankings.
Lemma 8 The bid values
bi = vi ci + (1 ‚àí ci )bi+1

(20)

are in a pure strategy Nash Equilibrium in the Overture mechanism. This corresponds to
the substitution of the assumption ‚àÄi Œ≥i = 0 (i.e. Œºi = ci ) in Theorem 4.
The proof follows from Theorem 4 as both pricing and ranking are shown to be a special
case of our proposed mechanism.
Similarly for GSP,
Lemma 9 The bid values
bi = vi k + (1 ‚àí k)bi+1 ci+1

(21)

is a pure strategy Nash Equilibrium in the GSP mechanism.
This equilibrium corresponds to the substitution of the assumption ‚àÄi Œ≥i = k ‚àí ci (1 ‚â•
k ‚â• 0) in Theorem 4. Since this is a special case, this result follows from Theorem 4.

6.3 Comparison with VCG mechanism
We compare the revenue and equilibrium of CE mechanism with those of VCG (Vickrey
1961; Clarke 1971; Groves 1973). VCG auctions combine an optimal allocation (ranking)
with VCG pricing. VCG payment of a bidder is equal to the reduction of revenues of other
bidders due to the presence of the bidder. A well known property is that VCG pricing with
any socially optimal allocation has truth telling as the the dominant strategy equilibrium.
In the context of online ads, a ranking optimal with respect to the bid amounts is socially
optimal ranking for VCG. This optimal ranking is bŒºi ci i ; as directly implied by the (1) on
substituting bi for utilities. Hence this ranking combined with VCG pricing has truth telling
as the dominant strategy equilibrium. Since bi = vi at the dominant strategy equilibrium,
ranking is socially optimal for advertiser‚Äôs true value as suggested in (13).
The CE ranking function is different from VCG since CE ranking by payments optimizes
search engine profits. On the other hand, VCG ranking optimizes the advertiser‚Äôs profit.
But Theorem 3 shows that for the pricing used in CE, ordering of CE is the same as that
of VCG. This order preserving property facilitates the comparison of CE with VCG. The
theorem below shows revenue dominance of CE over VCG for the same bid values of the
advertisers.

J Intell Inf Syst

Theorem 5 (Search Engine Revenue Dominance) For the same bid values for all the advertisers, the search engine revenue by CE mechanism is greater than or equal to its revenue
by VCG.
Proof Sketch The proof is an induction based on the fact that the ranking by CE and VCG
are the same, as mentioned above. Full proof is given in Appendix A-5.
This theorem shows that the CE mechanism is likely to provide higher revenue to the
search engine even during transient times before the bids settle on equilibriums.
Based on Theorem 5, we prove revenue equivalence of the proposed CE equilibrium
with dominant strategy equilibrium of VCG.
Theorem 6 (Equilibrium Revenue Equivalence) At the equilibrium in Theorem 4, the
revenue of the search engine is equal to the revenue of the truthful dominant strategy
equilibrium of VCG.
Proof Sketch The proof is an inductive extension of Theorem 5. Please see Appendix A-6
for complete proof.
Note that the CE equilibrium has lower bid values than VCG at the equilibrium, but
provides the same profit to the search engine.

7 CE ranking considering mutual influences: diversity ranking
An assumption in CE ranking is that the entities are mutually independent as we pointed out
in Section 3. In other words, the three parameters‚ÄîU (e), C(e) and Œ≥ (e)‚Äîof an entity do
not depend on other entities in the ranked list. In this section we relax this assumption and
analyze the implications. Since the nature of the mutual influence may vary for different
problems, we base our analysis on a specific well known problem‚Äîranking considering
diversity (Carterette 2010; Agrawal et al. 2009; Rafiei et al. 2010).
Diversity ranking accounts for the fact that the utility of an entity is reduced by the
presence of a similar entity above in the ranked list. This is a typical example of the mutual
influence between the entities. All the existing objective functions for the diversity ranking
are known to be NP-Hard (Carterette 2010). We analyze a basic form of diversity ranking
to explain why this is a fundamentally hard problem.
We modify the objective function in (1) slightly to distinguish between the standalone
utilities and the residual utilities‚Äîutility of an entity in the context of other entities in the
list‚Äîas,
E(U ) =

N


Ur (ei )Pc (ei )

(22)

i=1

where Ur (ei ) denotes the residual utility.
We examine a simple case of diversity ranking problem by considering a set of entities‚Äî
all having the same utilities, perceived relevances and abandonment probabilities. Some of
these entities are repeating. If an entity in the ranked list is the same as the entity in the
list above, the residual utility of that entity becomes zero. In this case, it is intuitive that
the optimal ranking is to place the maximum number of pair-wise dissimilar entities in the

J Intell Inf Syst

top slots. The theorem below shows that even in this simple case the optimal ranking is
NP-Hard.
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Sketch The proof is by reduction from the independent set problem. See
Appendix A-7 for the complete proof.
Moreover, the proof by reduction from independent set problem has more implications
than NP-Hardness as shown in the following corollary,
Corollary 1 The constant approximation algorithm for ranking considering diversity is
hard.
Proof The proof of NP-Hardness in the theorem above shows that the independent set problem is a special case of diversity ranking. This implies that a constant ratio approximation
algorithm for the optimal diversity ranking would be a constant ratio approximation algorithm for the independent set problem. Since a constant ratio approximation algorithm for
the independent set is known to be hard (cf. Garey and Johnson 1976 and HaÃästad 1996), the
corollary follows. To define hard, in his landmark paper HaÃästad proved that independent set
problem cannot be solved within n1‚àí for  > 0 unless all problems in N P are solvable in
probabilistic polynomial time, which is widely believed to be not possible.2
This section shows that the optimal ranking considering mutual influences of parameters
is hard. We leave formulating approximation algorithms (not necessarily constant ratio) for
future research.
Beyond proving the intractability of mutual influence ranking, we believe that the
intractability of the simple scenario here explains why all optimal diversity rankings and
constant ratio approximations are likely to be intractable. Furthermore, the proof based
on the reduction from the well explored independent set problem may help in adapting
approximation algorithms from graph theory.

8 Experiments
We compare the profit improvement by CE and reduced forms to existing rankings.
Although the optimality of the proposed ranking is proven above, experiments help to quantify the increase in utilities. Considering the very restricted access to real users and ad click
logs, we limit our evaluations to simulations as it is common in computational advertisement
research. We believe that these experimental results will motivate future online evaluations
in industry settings.
In our first experiment in Fig. 3a, we compare the CE ranking with rank by bid
amount (11) strategy by Overture and rank by bid √ó perceived relevance (12) by Google.
We assign the perceived relevance values as a uniform random number between 0 and Œ±
(0 ‚â§ Œ± ‚â§ 1) and abandonment probabilities as random between 0 and 1 ‚àí Œ±. This assures
‚àÄi (C(ai ) + Œ≥ (ai )) ‚â§ 1 condition required in the click model. The bid amounts for ads are

2 This

belief is almost as strong as the belief P  = N P

J Intell Inf Syst

Fig. 3 a Comparison of Overture, Google and CE rankings. Perceived relevances are uniformly random
in [0, Œ±] and abandonment probabilities are uniformly random in [0, 1 ‚àí Œ±]. CE provides optimal expected
profits for all values of Œ±. b Comparison of CE, PRP and abandonment ranking (9). Abandonment ranking
dominates PRP

J Intell Inf Syst

Fig. 4 Optimality of reduced forms under assumptions (a) setting Œ≥ (d) = k ‚àí R(d). Perceived relevance
ranking is optimal for all values of Œ±. (b) setting C(d) = R(d). In this case, abandonment ranking is optimal

assigned uniform randomly between 0 and 1. We use uniform random for values as it is the
maximum entropy distribution and hence makes least assumptions about the bid amounts.
The number of relevant ads (corresponding to the number of bids on a query) is set to fifty.

J Intell Inf Syst

Simulated users are made to click on ads. The number of ads clicked is set to a random
number generated in a zipf distribution with exponent 1.5. A power law is most intuitive for
the distribution of the number of clicks.
Simulated users browse down the list. Users click an entity with probability equal to the
perceived relevance and abandon the search results with a probability equal to the abandonment probability. The set of entities to be placed is created at random for each run. For the
same set of entities, three runs‚Äîone with each ranking strategy‚Äîare performed. Simulation
is repeated 2 √ó 105 times for each value of Œ±.
An alternate interpretation of Fig. 3a is as the comparison of ranking by CE, PRP and
perceived relevance ranking (8). As we discussed, PRP and perceived relevance rankings
are isomorphic to ad rankings by bid and bid √ó perceived relevance respectively, with utility
being relevance instead of bid amounts. The simulation results are the same.
In Fig. 3b we compare CE, PRP and abandonment ranking (9) under the same settings
used for Fig. 3a. CE provides the maximum utility as expected, and abandonment ranking
occupies the second place. Abandonment ranking provides sub-optimal utility‚Äîsince the
condition ‚àÄd R(d) = C(d) is not satisfied‚Äîbut dominates over PRP. Further, as abandonment probability becomes zero (i.e. Œ± = 1) abandonment rankings becomes same as PRP
and optimal as we predicted in Section 5.1.
Figure 4a compares the perceived relevance ranking (8), CE, and PRP under the condition for optimality for perceived relevance ranking (i.e. ‚àÄd Œ≥ (d) = k ‚àí R(d)). For this,
we set Œ≥ (d) = Œ± ‚àí C(d) keeping all other settings same as the previous experiments.
Figure 4a shows that the perceived relevance ranking provides optimal utility, exactly overlapping with CE curve as expected. Furthermore, note that utilities by PRP are very low
under this condition. The utilities by PRP in fact goes down after Œ± = 0.2. The increase
in abandonment probability, as well as increased sub-optimality of PRP for higher abandonment (since PRP does not consider abandonment) probabilities may be causing this
reduction.
In our next experiment shown in Fig. 4b, we compare abandonment ranking (9) with PRP
and CE under the condition ‚àÄd C(d) = R(d) (i.e. optimality condition for abandonment
ranking). All other settings are the same as those for the experiments in Fig. 3a and b.
Here we observe that the abandonment ranking is optimal and exactly overlaps with CE as
expected. PRP is sub-optimal but closer to optimal than random C(d) used for experiments
in Fig. 3b. The reason may be that C(d) = R(d) is one of the two conditions required for
PRP to be optimal for both sets of assumptions as we discussed in Subsection 5.1. When
abandonment probability becomes zero PRP relevance reaches optimum as we have already
seen.
All these simulation experiments confirm the predictions by the theoretical analysis
above. Although the simulation is no substitute for experiments on real data, we expect that
the observed significant improvements in expected utilities would motivate future online
experiments to quantify profit.

9 Conclusion and future work
We approach the document and ad ranking as a utility maximization based on the user
click model, and derive an optimal ranking‚Äînamely CE ranking. CE ranking is simple and
intuitive; and optimal considering perceived relevance and abandonment probability of user
behavior.

J Intell Inf Syst

On specific assumptions on parameters, the CE ranking function spawns a taxonomy of
rankings in multiple domains. The taxonomy shows that the existing document and ad ranking strategies are special cases of the proposed ranking function under specific assumptions.
The taxonomy is helpful in selecting optimal ranking for a specific user behavior.
To apply CE ranking to ad auctions, we incorporate a second-price based pricing mechanism. The resulting CE mechanism has a Nash Equilibrium which simultaneously optimizes
both the search engine and advertiser revenues. The CE mechanism is revenue dominant
over VCG for the same bid vectors, and has an equilibrium which is revenue equivalent with
the truthful equilibrium of VCG.
We relax the assumption of independence between entities in CE ranking and apply it
to diversity ranking. The ensuing analysis reveals that diversity ranking is an inherently
hard problem; since even the basic formulations are NP-Hard with unlikely constant ratio
approximation algorithms. Furthermore our simulation experiments confirm the results, and
suggest potentially significant increase in profits over the existing rankings.
As future research, assessing profits by CE ranking in an online experiment on a
large scale search engine will quantify improvement in ranking. Estimation and prediction of abandonment probability using click logs and statistical models are interesting
problems. The suggested ranking is optimal for other web ranking scenarios with similar
click models‚Äîlike product and friend recommendations‚Äîand may be extended to these
problems. Furthermore, effective approximation schemes for diversity ranking based on
similarity with the independent set problem may be investigated.
Acknowledgments This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR
grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027, two Google faculty research awards
(2010 & 2013), and a Yahoo key scientific challenges program award (2009).

Appendix
A-1 Proof of theorem 1
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
CE(ei ) =

U (ei )C(ei )
C(ei ) + Œ≥ (ei )

Proof Consider results ei and ei+1 in positions i and i + 1 respectively. Let Œºi = Œ≥ (ei ) +
C(ei ) for notational convenience. The total expected utility from ei and ei+1 when ei is
placed above ei+1 is
i‚àí1




(1 ‚àí Œºj ) U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 )

j =1

If the order of ei and ei+1 are inverted by placing ei above ei+1 , the expected utility from
these entities will be,
i‚àí1




(1 ‚àí Œºj ) U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei ))

j =1

J Intell Inf Syst

Since utilities from all other results in the list will remain the same, the expected utility of
placing ei above ei+1 is greater than inverse placement iff
U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 ) ‚â• U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei )

U (ei+1 )C(ei+1 )
U (ei )C(ei )
‚â•
Œºi
Œºi+1
U (e)C(e)
This means if entities are ranked in the descending order of C(e)+Œ≥
(e) any inversions will
reduce the profit. Since any arbitrary order can be effected by a number of inversions on the
U (e)C(e)
ranking by CE, this implies that ranking by C(e)+Œ≥
(e) is optimal.

A-2 Proof of theorem 2
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Induction on number of clicks.
Base Case: Single click, proved in Theorem 1.
Inductive Hypothesis: The proposed ordering is optimal for n clicks.
Let there be total of n ranked entities and ec be the nth clicked entity. The user will browse
down starting next to ec . Since there is only one click remaining, optimal ordering of entities is in the descending order of CE by the base case. Since the relevance and abandonment
probabilities ec+1 to en remain unchanged by the independence assumption above, the
optimal sequence will be the sub-sequence of ec+1 to en in the ranking.

A-3 Proof of theorem 3
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj
Proof Without loss of generality, we assume that ai refers to ad in the position i in the
descending order of wi bi .
pi ci
Œºi
bi+1 ci+1 Œºi ci
=
Œºi+1 ci Œºi
bi+1 ci+1
=
Œºi+1
= wi+1 bi+1

CEi =

‚â• wi+2 bi+2 = CEi+1

J Intell Inf Syst

A-4 Proof of theorem 4
Theorem 4 (Nash Equilibrium) : Without the loss of generality assume that the advertisers
are ordered in the decreasing order of cŒºi vi i where vi is the private value of the i th advertiser.
The advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
bi =
ci
Œºi+1
This equilibrium is socially optimal for advertisers as well as optimal for search engines for
the given CPC‚Äôs.
Proof Let there are n advertisers. Without loss of generality, let us assume that advertisers
are indexed in the descending order of vŒºi ci i . We prove equilibrium in two steps.
Step 1: Prove that
wi bi ‚â• wi+1 bi+1

w i bi =

(1)

bi c i
Œºi

Expanding bi by (19),
bi+1 ci+1
Œºi+1
= vi ci + (1 ‚àí Œºi )wi+1 bi+1
vi ci
=
Œºi + (1 ‚àí Œºi )wi+1 bi+1
Œºi

wi bi = vi ci + (1 ‚àí Œºi )

Notice that wi bi is a convex linear combination of wi+1 bi+1 and vŒºi ci i . This means that
the value of wi bi is in between (or equal to) the values of wi+1 bi+1 and vŒºi ci i . Hence to
prove that wi bi ‚â• wi+1 bi+1 all we need to prove is that vŒºi ci i ‚â• wi+1 bi+1 . This inductive
proof is given below.
Induction hypothesis: Assume that
‚àÄi‚â•j

vi ci
‚â• wi+1 bi+1
Œºi

Base case: Prove for i = N i.e. for the bottommost ad.
vN‚àí1 cN‚àí1
‚â• wN b N
ŒºN‚àí1
Assuming ‚àÄi>N bi = 0
wN bN = vN cN ‚â§

vN cN
vN‚àí1 cN‚àí1
vi ci
(as ŒºN ‚â§ 1) ‚â§
(by the assumed order i.e. by
)
ŒºN
ŒºN‚àí1
Œºi

Induction: Expanding wj bj by (19),
wj bj =

vj cj
Œºj + (1 ‚àí Œºj )wj +1 bj +1
Œºj

J Intell Inf Syst

wj bj is the convex linear combination, i.e
vj cj
Œºj

vj cj
Œºj

‚â• wj bj ‚â• wj +1 bj +1 , as we know that

‚â• wj +1 bj +1 by induction hypothesis. Consequently,
wj bj ‚â§

vj ‚àí1 cj ‚àí1
vj cj
‚â§
(by the assumed order)
Œºj
Œºj ‚àí1

This completes the induction.

Since advertisers are ordered by wi bi for pricing, the above proof says that the pricing
order is the same as the assumed order in this proof (i.e. ordering by vŒºi ci i ). Consequently,
pi =

bi+1 ci+1 Œºi
Œºi+1 ci

As corollary of Theorem 3 we know that CEi ‚â• CEi+1 .
In the second step we prove the equilibrium using results in Step 1.
Step 2: No advertiser can increase his profit by changing his bids unilaterally
Proof (of lack of incentive to undercut to advertisers below) In the first step let us prove that
ad ai can not increase his profit by decreasing his bid to move to a position j ‚â• i below.
Inductive hypothesis: Assume true for i ‚â§ j ‚â§ m.
Base Case: Trivially true for j = i.
Induction: Prove that the expected profit of ai at m + 1 is less or equal to the expected
profit of ai at i.
Let œÅk denotes the amount paid by ai when he is at the position k. By inductive hypothesis, the expected profit at m is less or equal to the expected profit at i. So we just need to
prove that the expected profit at m + 1 is less or equal to the expected profit at m. i.e.
m
m+1
(vi ‚àí œÅm+1 ) 
(vi ‚àí œÅm ) 
(1 ‚àí Œºl ) ‚â•
(1 ‚àí Œºl )
(1 ‚àí Œºi )
(1 ‚àí Œºi )
l=1

l=1

Canceling the common terms,
vi ‚àí œÅm ‚â• (vi ‚àí œÅm+1 )(1 ‚àí Œºm+1 )

(2)

œÅm ‚Äîthe price charged to ai at position m‚Äîis based on the Equations 16 and 19. Since the
ai is moving downward, ai will occupy position m by shifting ad am upwards. Hence the ad
just below ai is am+1 . Consequently, the price charged to ai when it is at the mth position is,

	
bm+1 cm+1 Œºi
Œºi
bm+2 cm+2
=
vm+1 cm+1 + (1 ‚àí Œºm+1 )
œÅm =
Œºm+1 ci
ci
Œºm+2
Substituting for œÅm and œÅm+1 in (2),

	
Œºi
bm+2 cm+2
vm+1 cm+1 + (1 ‚àí Œºm+1 )
vi ‚àí
ci
Œºm+2

	


Œºi
bm+3 cm+3
vm+2 cm+2 + (1 ‚àí Œºm+2 )
(1‚àíŒºm+1 )
‚â• vi ‚àí
ci
Œºm+3

J Intell Inf Syst

Simplifying, and multiplying both sides by ‚àí1

	
bm+2 cm+2
Œºi
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
	

bm+3 cm+3
√ó vm+2 cm+2 + (1 ‚àí Œºm+2 )
Œºm+3
Substituting by bm+2 from (19) on RHS.

	
bm+2 cm+2
Œºi
bm+2 cm+2
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
Œºm+2
Canceling out the common terms on both sides,
Œºi
vm+1 cm+1 ‚â§ vi Œºm+1
ci

vi ci
vm+1 cm+1
‚â§
Œºm+1
Œºi
Which is true by the assumed order as m ‚â• i
Inductive proof for m ‚â§ i is somewhat similar and enumerated below.
Inductive hypothesis: Assume true for j ‚â§ m.
Base Case: Trivially true for j = i.
Proof (of lack of incentive to overbid ad one above) The case in which ai increases his bid
to move one position up i.e. to i ‚àí 1 is a special case and need to be proved separately. In
this case, by moving a single slot up, the index of the ad below ai will change from i + 1
to i ‚àí 1 (a difference of two). For all other movements of ai to a position one above or one
below, the index of the advertisers below will change only by one. Since the amount paid
by ai depends on the ad below ai , this case warrants a slightly different proof,
(vi ‚àí œÅi )

i‚àí1


(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

l=1

i‚àí2


(1 ‚àí Œºl )

l=1


(vi ‚àí œÅi )(1 ‚àí Œºi‚àí1 ) ‚â• vi ‚àí œÅi‚àí1
Expanding œÅi is straight forward.To expand œÅi‚àí1 , note that when ai has moved upwards to
i ‚àí 1, the ad just
 below ai is ai‚àí1 . Since ai‚àí1 has not changed its bids, the œÅi‚àí1 can be
Œºi
expanded as ci vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 ) bŒºi ci i . Substituting for œÅi and œÅi‚àí1 ,


vi ‚àí



	
	
Œºi
Œºi
bi+2 ci+2
bi ci
vi+1 ci+1 + ‚â• vi ‚àí
vi‚àí1 ci‚àí1 + (1‚àíŒºi+1 )
(1 ‚àí Œºi‚àí1 )(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi+2
Œºi

Simplifying and multiplying by ‚àí1
vi Œºi‚àí1 +



	
	
Œºi
Œºi
bi ci
bi+2 ci+2
vi+1 ci+1 + ‚â§
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi+1 )
(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi
Œºi+2

J Intell Inf Syst

Substituting bi+1 from (19)


	
Œºi bi+1 ci+1
Œºi
bi c i
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi‚àí1 ) ‚â§
ci Œºi+1
ci
Œºi

Œºi
bi+1 ci+1
Œºi vi‚àí1 ci‚àí1
Œºi
bi c i
‚â§
+ (1 ‚àí Œºi‚àí1 )
vi Œºi‚àí1 + (1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
ci
Œºi
vi Œºi‚àí1 +

We now prove that both the terms in RHS are greater or equal to the corresponding terms in
LHS separately.
Œºi vi‚àí1 ci‚àí1
vi Œºi‚àí1 ‚â§
ci

vi‚àí1 ci‚àí1
vi ci
‚â§
Œºi
Œºi‚àí1
Which is true by our assumed order.
Similarly,
bi+1 ci+1
Œºi
bi c i
Œºi
(1 ‚àí Œºi‚àí1 )
‚â§
(1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
Œºi

bi ci
bi+1 ci+1
‚â§
Œºi+1
Œºi
Which is true by (1) above. This completes the proof for this case.
Induction: Prove that the expected profit at m ‚àí 1 is less or equal to the expected profit
at m. The proof is similar to the induction for the case m > i.
Proof Base case is trivially true.
(vi ‚àí œÅm )

m‚àí1


m‚àí2


l=1

l=1

(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

(1 ‚àí Œºl )

Canceling common terms,
(vi ‚àí œÅm )(1 ‚àí Œºm‚àí1 ) ‚â• vi ‚àí œÅm‚àí1
In this case, note that ai is moving upwards. This means that ai will occupy position m by
pushing the ad originally at m one position downwards. Hence the original ad at m is the
one just below ai now. i.e.

	
bm cm Œºi
Œºi
bm+1 cm+1
œÅm =
vm cm + (1 ‚àí Œºm )
=
Œºm ci
ci
Œºm+1
Substituting for œÅm and œÅm‚àí1


vi ‚àí



	
	
Œºi
Œºi
bm+1 cm+1
bm cm
vm cm + ‚â• vi ‚àí
vm‚àí1 cm‚àí1 + (1‚àíŒºm )
(1‚àíŒºm‚àí1 )(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm+1
Œºm

Simplifying and multiplying by ‚àí1
vi Œºm‚àí1 +



	
	
Œºi
Œºi
bm cm
bm+1 cm+1
vm cm + ‚â§
vm‚àí1 cm‚àí1 + (1‚àíŒºm‚àí1 )
(1 ‚àí Œºm )
(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm
Œºm+1

J Intell Inf Syst

Substituting by bm from (19)

	
Œºi bm cm
Œºi
bm c m
vm‚àí1 cm‚àí1 + (1 ‚àí Œºm‚àí1 )
(1 ‚àí Œºm‚àí1 ) ‚â§
vi Œºm‚àí1 +
ci Œºm
ci
Œºm
Canceling common terms,
Œºi
vm‚àí1 cm‚àí1
ci

vi Œºm‚àí1 ‚â§


vm‚àí1 cm‚àí1
vi ci
‚â§
Œºi
Œºm‚àí1
Which is true by the assumed order as m < i.

A-5 Proof of theorem 5
Theorem 5 (Search Engine Revenue Dominance) : For the same bid values for all the
advertisers, the revenue of search engine by CE mechanism is greater or equal to the
revenue by VCG.
Proof VCG payment of the ad at position i (i.e. ai ) is equal to the reduction in utility of
the ads below due to the presence of ai . For each user viewing the list of ads (i.e. for unit
view probability), the total expected loss of ads below ai due to ai is,

piVu =
=

=

j
‚àí1
j
‚àí1
n
n


1
bj c j
(1 ‚àí Œºk ) ‚àí
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi

Œºi
1 ‚àí Œºi

j =i+1

k=1

n


j
‚àí1

bj c j

j =i+1

j =i+1

k=1

(1 ‚àí Œºk )

k=1

j
‚àí1
n
i

Œºi 
(1 ‚àí Œºk )
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi
j =i+1

k=1

= Œºi

i‚àí1


(1 ‚àí Œºk )

n


bj c j

j =i+1

k=1

k=i+1

j
‚àí1

(1 ‚àí Œºk )

k=i+1

This is the expected lose per user browsing the ad list. Pay per click should be equal to the
lose per click. To calculate the pay per click, we divide by the click probability of ai . i.e.

piV =
=

Œºi

i‚àí1

j ‚àí1
j =i+1 bj cj
k=i+1 (1 ‚àí Œºk )
i‚àí1
ci k=1 (1 ‚àí Œºk )
j
‚àí1

k=1 (1 ‚àí Œºk )

n
Œºi 
bj c j
ci
j =i+1

n

(1 ‚àí Œºk )

k=i+1

J Intell Inf Syst

Converting to recursive form,
bi+1 Œºi
Œºi ci+1 V
ci+1 + (1 ‚àí Œºi+1 )
p
ci
ci Œºi+1 i+1
bi+1 Œºi ci+1
Œºi ci+1 V
=
Œºi+1 + (1 ‚àí Œºi+1 )
p
ci Œºi+1
ci Œºi+1 i+1

piV =

For the CE mechanism payment from (16) is,
piCE =

bi+1 ci+1 Œºi
Œºi+1 ci

Note that piV is convex combination of PiCE and
two values. To prove that

piCE

‚â•

piV

Œºi ci+1 V
ci Œºi+1 pi+1 ,

and hence is between these

all we need to prove is that PiCE ‚â•

Œºi ci+1 V
ci Œºi+1 pi+1

‚áî

bi ‚â• piV . This directly follows from individual rationality property of VCG. Alternatively, a
V = 0 (bottommost ad) will prove the same. Note that
simple recursion with base case as pN
we consider only the ranking (not selection), and hence the VCG pricing of the bottommost
ad in the ranking is zero.

A-6 Proof of theorem 6
Theorem 6 (Equilibrium Revenue Equivalence) : At the equilibrium in Theorem 4, the revenue of search engine is equal to the revenue of the truthful dominant strategy equilibrium
of VCG.
Proof Rearranging (3) and substituting true values for bid amounts,

	
Œºi
(1 ‚àí Œºi+1 )ci+1 V
V
vi+1 ci+1 +
pi+1
pi =
ci
Œºi+1
For the CE mechanism, substituting equilibrium bids from (19) in payment (16),

	
bi+1 ci+1 Œºi
Œºi
bi+2 ci+2
=
vi+1 ci+1 + (1 ‚àí Œºi+1 )
piCE =
Œºi+1 ci
ci
Œºi+2
Rewriting bi+2 in terms of pi+1 ,
piCE =


	
Œºi
(1 ‚àí Œºi+1 )ci+1 CE
vi+1 ci+1 +
pi+1
ci
Œºi+1

= piV

V
CE
(iff pi+1
= pi+1
)

Ad at the bottommost position pays same amount zero, a simple recursion will prove that
the payment for all positions for both VCG and the proposed equilibrium is the same.

A-7 Proof of theorem 7
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Independent set problem can be formulated as a ranking problem considering similarities. Consider an unweighed graph G of n vertices {e1 , e2 , ..en } represented as an

J Intell Inf Syst

adjacency matrix. This conversion is clearly polynomial time. Now, consider the values in
the adjacency matrix as the similarity values between the entities to be ranked. Let the entities have the same utilities, perceive relevances and abandonment probabilities. In this set of
n entities from {e1 , e2 , .., en }, clearly the optimal ranking will have k pairwise independent
entities as the top k entities for a maximum possible value of k. But the set of k independent
entities corresponds to the maximum independent set in graph G.

References
Aggarwal, G., Feldman, J., Muthukrishnan, S., & PaÃÅl, M. (2008). Sponsored search auctions with markovian
users. Internet and Network Economics, 621‚Äì628.
Aggarwal, G., Goel, A., & Motwani, R. (2006). Truthful auctions for pricing search keywords. In Proceedings
of the 7th ACM conference on Electronic commerce (pp. 1‚Äì7), ACM.
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying search results. In Proceedings of
the Second ACM International Conference on Web Search and Data Mining (pp. 5‚Äì14). ACM.
Balakrishnan, R., & Kambhampati, S. (2008). Optimal ad ranking for profit maximization. In Proceedings
of the 11th International Workshop on the Web and Databases.
Carterette, B. (2010). An analysis of NP-completeness in novelty and diversity ranking. Advances in
Information Retrieval Theory, 200‚Äì211.
Chapelle, O., & Zhang, Y. (2009). A dynamic bayesian network click model for web search ranking. In
Proceedings of World Wide Web (pp. 1‚Äì10). ACM.
Chierichetti, F., Kumar, R., & Raghavan, P. (2011). Optimizing two-dimensional search results presentation.
In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 257‚Äì
266). ACM.
Clarke, C.L.A., Agichtein, E., Dumais, S., & White, R.W. (2007). The influence of caption features on
clickthrough patterns in web search. In Proceedings of SIGIR (pp. 135‚Äì142). ACM.
Clarke, E.H. (1971). Multipart pricing of public goods. Public Choice, 11(1), 17‚Äì33.
Craswell, N., Zoeter, O., Tayler, M., & Ramsey, B. (2008). An experimental comparison of click position
bias models. In Proceedings of WSDM (pp. 87‚Äì94).
Deng, X., & Yu, J. (2009). A new ranking scheme of the GSP mechanism with markovian users. Internet and
Network Economics, 583‚Äì590.
Dupret, G.E., & Piwowarski, B. (2008). A user browsing model to predict search engine click data from past
observations. In Proceedings of SIGIR, (pp. 331‚Äì338). ACM.
Easley, D., & Kleinberg, J. (2010). Networks, crowds, and markets: Reasoning about a highly connected
world: Cambridge Univ Press.
Edelman, B., Ostrovsky, M., & Schwarz, M. (2007). Internet advertising and the generalized second price
auction: Selling billions of dollars worth of keywords. The American Economic Review, 97(1).
Garey, M.R., & Johnson, D.S. (1976). The complexity of near-optimal graph coloring. Journal of the ACM
(JACM), 23(1), 43‚Äì49.
Ghosh, A., & Sayedi, A. (2010). Expressive auctions for externalities in online advertising. In Proceedings
of the 19th international conference on World wide web (pp. 371‚Äì380). ACM.
Giotis, I., & Karlin, A. (2008). On the equilibria and efficiency of the GSP mechanism in keyword auctions
with externalities. Internet and Network Economics, 629‚Äì638.
Gordon, M.G., & Lenk, P. (1991). A utility theory examination of probability ranking principle in information
retrieval. Journal of American Society of Information Science, 41, 703‚Äì714.
Gordon, M.G., & Lenk, P. (1992). When is probability ranking principle suboptimal. Journal of American
Society of Information Science, 42.
Groves, T. (1973). Incentives in teams. Econometrica: Journal of the Econometric Society, 617‚Äì631.
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.M., & Faloutsos, C. (2009). Click chain model
in web search. In Proceedings of World Wide Web (pp. 11‚Äì20). New York: ACM.
HaÃästad, J. (1996). Clique is hard to approximate within n. In Foundations of Computer Science, 1996. 37th
Annual Symposium on Proceedings (pp. 627‚Äì636).
Hu, B., Zhang, Y., Chen, W., Wang, G., & Yang, Q. (2011). Characterizing search intent diversity into click
models. In Proceedings of the 20th international conference on World wide web (pp. 17‚Äì26). ACM.
Kempe, D., & Mahdian, M. (2008). A cascade model for externalities in sponsored search. Internet and
Network Economics, 585‚Äì596.

J Intell Inf Syst
Kuminov, D., & Tennenholtz, M. (2009). User modeling in position auctions: re-considering the gsp and vcg
mechanisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 1 (pp. 273‚Äì280).
Rafiei, D., Bharat, K., & Shukla, A. (2010). Diversifying Web Search Results. In Proceedings of World Wide
Web.
Richardson, M., Dominowska, E., & Ragno, R. (2007). Predicting clicks: Estimating the click-through rate
for new ads. In Proceedings of World Wide Web.
Richardson, M., Prakash, A., & Brill, E. (2006). Beyond pagerank: Machine learning for static ranking. In
World Wide Web Proceedings (pp. 707‚Äì714). ACM.
Robertson, S.E. (1977). The probability ranking principle in ir. Journal of Documentation, 33, 294‚Äì304.
Varian, H.R. (2007). Position auctions. International Journal of Industrial Organization, 25(6), 1163‚Äì1178.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1), 8‚Äì37.
Xu, W., Manavoglu, E., & Cantu-Paz, E. (2010). Temporal click model for sponsored search. In Proceedings
of the 33rd international ACM SIGIR conference on Research and development in information retrieval
(pp. 106‚Äì113). ACM.
Yilmaz, E., Shokouhi, M., Craswell, N., & Robertson, S. (2010). Expected browsing utility for web search
evaluation. In Proceedings of the 19th ACM international conference on Information and knowledge
management (pp. 1561‚Äì1564). ACM.
Yue, Y., Patel, R., & Roehrig, H. (2010). Beyond position bias: Examining result attractiveness as a source
of presentation bias in clickthrough data. In Proceedings of World Wide Web.
Zhu, Z.A., Chen, W., Minka, T., Zhu, C., & Chen, Z. (2010). A novel click model and its applications to
online advertising. In In Proceedings of Web search and data mining (pp. 321‚Äì330). ACM.

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Venting Weight: Analyzing the Discourse of an Online Weight Loss Forum
Lydia Manikonda1 , Heather Pon-Barry2 , Subbarao Kambhampati1 , Eric Hekler3
David W. McDonald4
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University
2
Mount Holyoke College
3
School of Nutrition and Health Promotion, Arizona State University
4
Human Centered Design & Engineering, University of Washington
lmanikon@asu.edu, ponbarry@mtholyoke.edu, {rao, ehekler}@asu.edu, dwmc@uw.edu
1

Abstract
Online social communities are becoming increasingly popular platforms for people to share information, seek emotional support, and maintain accountability for losing weight.
Studying the discourse in these communities can offer insights on how users benefit from using these applications.
This paper presents an analysis of language and discourse
patterns in forum posts by users who lose weight and keep it
off versus users with fluctuating weight dynamics. In contrast
to prior studies, we have access to the weekly self-reported
check-in weights of users along with their forum posts. This
paper also presents a study on how goal-oriented forums are
different from general online forums in terms of language
markers. Our results reveal differences about how the types
of posts made by users vary along with their weight-loss
patterns. These insights are closely related to the power dynamics of social interactions and can enable better design of
weight-loss applications thereby contributing to a healthy society.

1

Introduction

Obesity is a major public health problem; the number of
people suffering from obesity has risen globally in the last
decade (Ogden et al. 2014). The Centers for Disease Control and prevention (CDCP) defined an obese adult (http:
//www.cdc.gov/obesity/adult/defining.html) as a
person with a body mass index (BMI) of 30 or higher. Many
obese people are trying to lose weight as diseases such as
metabolic syndromes, respiratory problems, coronary heart
disease, and psychological challenges are closely associated with obesity (Must et al. 1999; Ngu 2012). Researchers
have been trying to understand how certain factors are affecting the weight loss as large number of over-weight people are trying to lose weight and some others are trying to
avoid gaining weight. Internet services are gaining popularity to support weight loss as they provide users with the
opportunities to seek information by asking questions, answering questions, sharing their experiences and providing
emotional support where people feel more comfortable by
openly expressing their problems and concerns (Ballantine
and Stephenson 2011).
Copyright ¬© 2016, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Social media tools like weblogs, instant messaging platforms, video chat, social networks, online discussion forums, are reengineering the healthcare sector (Hawn 2009).
Especially, social media is a promising tool for studying
public health like tracking flu infections (Lamb, Paul, and
Dredze 2013), studying post-partum depression (De Choudhury, Counts, and Horvitz 2013), dental pain (Heaivilin et al.
2011), etc. Tools like online discussion forums make it easier to find health-related information while at the same time
provides support by maintaining accountability and some
of the popular works like (Black, But, and Russell 2010)
proved that weight loss can be supported through online interactions. Hence, studying the online discussion forums can
help identify the people at risk who need more support and
provide them access to appropriate services and support.
In this paper, we explore the weight loss patterns of users
who participate in online discussions and ground truth in
terms of the weekly check-in weights of users. We perform different analyses on the users‚Äô language in correlation to their weight loss dynamics. From the overall dataset
we identify two preliminary patterns of weight dynamics:
(1) users who lose weight and successfully maintain the
weight loss (i.e., from one week to the next, weight is lost
or weight remains the same) and (2) users whose weight
pattern fluctuates (i.e., from one week to the next, weight
changes are erratic). While there are many possible groupings that we could have utilized, we chose this grouping
because of the known problems with ‚Äúyo-yo‚Äù dieting (diet
that leads to cyclical loss and gain of weight) compared
to a more steady weight-loss (Brownell and Rodin 1994;
Hekler et al. 2014). Our work is novel in terms of automating
the language analysis by handling a bigger dataset and can
help classify the user type based on the language efficiently.
As a follow-up work, linguistic insights are explored which
distinguish goal-oriented forums from general forums.
Our research contributions in this paper are divided into
two main sections where each focuses on a broader perspective as described below:
1. How does the language of users vary within the weight
loss forum based on their patterns of weight loss. Specifically, to understand the patterns of asking questions, using
a specific sentiment, politeness and making excuses.
2. Are there any interesting insights about the linguistic sig-

nals that makes a goal-oriented forum such as a weightloss forum different from other general online forums.
Our analysis resulted in interesting insights as below:
1. users who lose weight in a fluctuating manner are more
active on the discussion forums.
2. users losing weight in a fluctuating manner appear to talk
about themselves as they use higher number of personal
pronouns and adverbs.
3. users of non-increasing weight loss pattern mostly reply
to the posts made by other users and fluctuating users post
more questions.
4. posts from users of fluctuating weight loss pattern contain
more excuses.
5. politeness of posts seems to be uncorrelated with the
weightloss pattern.
6. users on goal-oriented forums contribute to a cohesive
thread of posts compared to users on general online forums which suffer from non-cohesiveness.
We believe that this research can bring forth the different variables related to people who need additional support
in terms of losing weight and thereby can stay healthy in
maintaining their weight. Also, we envision building personalized weight loss applications that can cater the needs of
individuals who need additional support. We hope that this
study will help in bringing more attention from the research
community to study online weight loss communities and understand both the constructive and destructive dimensions of
weight loss so that we can build a healthy society.

2

Related Work

There is a vast amount of literature about online forums ‚Äì statistical and language analysis of the discussion
threads (GoÃÅmez, Kaltenbrunner, and LoÃÅpez 2008), summarizing the discussions (Backstrom et al. 2013), measuring the success and identifying the factors that make users
participate (Kim 2000; Ludford et al. 2004) on these forums, addressing how the roles of users change (Yang et
al. 2010), understanding the lurking behavior and predicting lurkers (Preece, Nonnecke, and Andrews 2004), etc. Different fields like marketing (Bickart and Schindler 2001),
public health (Black, But, and Russell 2010), etc., considered online forums as influential sources of user information. Much of this literature focuses on studying the online
communities and their users from different linguistic and social networking perspectives. Little attention has been given
to analyze forums from the weight loss perspective.
However, most of the existing studies (Ballantine and
Stephenson 2011; Leahey et al. 2012; Das and Faxvaag
2014) on online weight loss discussion forums focused on
why people participate and how the social support can help
them to lose weight. These studies are conducted from the
perspective of medical and psychological domains, where
the data are collected via interviews or a small set of online forum data that are manually analyzed by human experts. Unlike the existing literature, our work considers the
weekly check-in weights of users along with their posts to

Figure 1: Example weight loss patterns from two individual users:
non-increasing (bottom line), and fluctuating (top line). The x-axis
ranges from the 1st through the 80th weekly check-in; the y-axis
shows the weight, measured in lbs.

understand the behavior of users who want to lose weight
and detect the variables that classify users who need additional support and service. Instead of choosing a small subset of a dataset and performing manual coding, our work
is novel in automating the language analysis by handling a
bigger dataset. Identifying and providing better assistance to
users who need help can also have a significant impact on
gaining the trust and confidence of users in these kinds of
services through better decision making.

3

Dataset

We obtained an anonymized text corpus of online discussion
forums from Fit Now, Inc. who developed a popular mobile
and web-based weight loss application. Along with the text
corpus, we also obtain weekly weight check-in data for a
subset of users. The entire corpus consists of eight different
forums that are subdivided into conversation topic threads.
Each thread consists of several posts made by different users.
The forum data in our corpus consists of 884 threads, with a
median length of 20 posts per thread. The posts were made
between January 1, 2010 and July 1, 2012. We identify the
subset of users for whom we have weight check-in data and
who made at least 25 weight check-ins during this time period. This results in a total of 2,270 users.
We partition the users into two groups based on their dynamic weight loss patterns: a non-increasing group and a
fluctuating group.
1. Non-increasing: These are the users who lose weight and
keep it off. For each week j, the user‚Äôs check-in weight
w j is less than or equal to their past week‚Äôs weight w j‚àí1 ,
within a small margin ‚àÜ. That is, w j ‚â§ (1 + ‚àÜ)w j‚àí1 .
2. Fluctuating: These are the users who do not lose weight.
If the difference between two consecutive weekly checkin weights do not follow the non-increasing constraint,
users are grouped into this category.
We empirically set ‚àÜ = 0.04 to divide the users in our

dataset into two groups of similar size. To illustrate the
two patterns of weight change, Figure 1 shows the weekly
weight check-ins of two individual users, one from each
group. This grouping is coarse, but is motivated by studies
(Kraschnewski et al. 2010; Wing and Phelan 2005) acknowledging that approximately 80% of people who set out to lose
weight are successful at long-term weight loss maintenance,
where successful maintenance is defined as losing 10% or
more of the body weight and maintaining that for at least
an year. In the future for further analysis, we aim to separate users less coarsely, e.g., users who maintain their weight
neither gaining nor losing weight, users who lose weight and
maintain it and finally, users who gain weight.
The main distinctive feature of this weight loss application is that users are encouraged to set goals to regularly log
their weight, diet, and exercise. For a subset of users, this
application included a weekly weight ‚Äúcheck-in‚Äù, an average of the user‚Äôs weight check-ins during the week, for the
January 1, 2010 through July 1, 2012 period. This allows us
to juxtapose the weekly weights of the users with their posts
on the discussion forums.

3.1

Characteristics of Online Community

This weight loss application helps users set a personalized
daily calorie budget, track the food they are eating, their exercise and log their weekly weight. It also helps users to stay
motivated by providing an opportunity to connect with other
users who want to lose weight and support each other. Example snippets from forum threads are shown below. The
‚ÄúCan‚Äôt lose weight!‚Äù thread demonstrates users supporting
each other and offering advice. The ‚ÄúSomeday I will‚Äù thread
highlights the complex relationship between text, semantics,
and motivation in the forums.
Example thread: ‚ÄúCan‚Äôt lose weight!‚Äù
User 1: ‚ÄúI gained over 30 lbs in the last year and am
stressed about losing it. I eat 1600 calories a day and
burn more than that in exercise, but I havent lost any
weight. I am so confused.‚Äù
User 2: ‚ÄúYou‚Äôve only been a member for less than 2
months. I suggest you relax. Set your program to 1
pound weight loss a week. Adjust your habits to something you can live with. . . long term.‚Äù
User 3: ‚ÄúYou sound just like me. I think your exercise
is good but maybe you are eating more than you think.
Try diligently logging everything you consume.‚Äù
User 1: ‚ÄúThanks for the suggestions! I am going to get
back to my logging.‚Äù
Example thread: ‚ÄúSomeday I will. . . ‚Äù
User 1: ‚ÄúDo a pull-up :-)‚Äù
User 2: ‚Äú. . . actually enjoy exercising.‚Äù
User 3: ‚ÄúSomeday I will stop participating in these forums, but obviously not today.‚Äù
User 4: ‚ÄúI hope you fail :-)‚Äù

4

Empirical Analysis of Weight Loss Forum

In this section, we present preliminary observations on how
the language and discourse patterns of forum posts vary with

respect to weight loss dynamics. As an initial step, part-ofspeech (POS) tagging is performed on all forum posts using
the Stanford POS Tagger (Toutanova et al. 2003).
Weight Pattern
# Total users
# Forum users
# Forum posts
Posts per user
Words per post

Non-increasing

Fluctuating

1127
29
99
3.5
49.1

1143
68
1279
18.2
77.3

Table 1: Statistics of users and forum posts.

From the weekly check-in data we identified the number of users and the number of posts from each weight-loss
pattern cluster which are shown in Table 1. In our dataset,
out of 1127 users who are expressing non-increasing weight
loss pattern (1143 fluctuating weight loss pattern) only 29 of
them (68 of them respectively) made atleast one post on the
discussion forums. We see that the average number of posts
by fluctuating users is greater than the average number of
posts by non-increasing users. Our data also suggest that the
posts made by non-increasing users are shorter compared to
those made by fluctuating users. Both these suggest the possible loss of social connectedness once users achieve their
goal.

4.1

Lexical Categories

Studies (Pennebaker, Mehl, and Niederhoffer 2003) show
that the language defines an individual and his/her behavior.
We use the measures that characterize the weight loss pattern by using the linguistic classes in posts made by these
users on the forum. Specifically, verbs, conjunctions, adverbs, personal pronouns and prepositions are considered
as shown in Table 2. We collected all the individual posts
made by all the users belonging to each weight loss pattern
and measured the average frequency of a linguistic class per
post. Fluctuating users appear to talk more about themselves
and interact with other individuals one-on-one as they are
using a relatively higher number of personal pronouns. Additionally, we observe that users who lose weight in a fluctuating manner use greater fraction of prepositions and adverbs. Adverbs are primarily used to tell how someone did
something which means these users who lose weight in a
fluctuating manner explain more about themselves, perhaps
in an attempt to seek more information.

4.2

Asking Questions

In order to build and maintain vibrant online communities, it
is very important to understand the complex ways in which
the members interact and how the communities evolve over
time. As a part of that, previous literature (Bambina 2007)
revealed that people on online health communities mainly
engage in two activities: (i) seeking information, and (ii)
getting emotional support. People usually ask questions or
just browse through the community forums to collect information. If we can understand how users post questions and

Weight Pattern
Non-increasing

Fluctuating

Ling. class

Mean

Med.

SD

Mean

Med.

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

3.85
3.44
2.21
4.94
5.44

2.0
3.0
1.0
3.0
4.0

4.11
3.51
2.87
5.42
5.40

6.27
4.53
3.24
8.65
9.67

4.0
3.0
2.0
6.0
6.0

6.81
5.04
3.74
8.59
10.51

Table 2: Results of statistical measures on linguistic class attributes; Med. refers to Median; SD refers to Standard Deviation

how the other members respond to those questions, it will
be very useful in developing personalized profiles of users
so that the system is able to help them get sufficient information even before they post any questions. Below is an example (paraphrased) showing how users ask and respond to
questions.
Example thread: ‚ÄúNew user‚Äù
User 1: ‚ÄúDid anyone upgrade to the premium app?
What do you like about it?‚Äù
User 2: ‚ÄúI upgraded to the premium. I LOVE the functionality to log food in advance. I can track and set
goals that are not related to weight like how much I
sleep, how much water I drink, etc.‚Äù
User 3: ‚ÄúI upgraded my account to premium too. I really liked the added features because it helped me keep
track of my steps and participate in challenges.‚Äù
We are interested in knowing whether these two types of
users are actively seeking information. We deem a forum
post to be a question if it meets one of these two conditions:
1. Wh-question words: If a sentence in the post starts with
a question word: Wh-Determiner (WDT), Wh-pronoun
(WP), Possessive wh-pronoun (WP$), Wh-adverb (WRB).
2. Punctuation: If the post contains a question mark (‚Äò?‚Äô).
We computed the ratio of question-oriented posts made by
each user in the two clusters. After averaging these ratio values across all the users in each cluster separately, we found
that on average, 32.6% of the posts made by non-increasing
users were questions (S tandardError(S E) = 0.061) while
37.7% of the posts made by fluctuating users were questions
(S E = 0.042). This shows that on an average fluctuating
users do post relatively larger number of questions than the
non-increasing users. We conjecture this could be a reflection of the fluctuating users‚Äô aim to seek more information
from the forum.

4.3

Sentiment of Posts

Analyzing the sentiment of user posts in the forums can
provide a suprisingly meaningful sense of how the loss of
weight impacts the sentiment of user‚Äôs post. In this analysis, we report our initial results on extracting the sentiments
of user‚Äôs posts. In order to achieve this, we utilize the Stanford Sentiment Analyzer (Socher et al. 2013). This analyzer

Figure 2: Proportion of sentiments for the two weight-loss patterns. For non-increasing users, percentage of posts with Positive, Neutral and Negative sentiments are: 22%, 46.5% and 31.5%
respectively. For fluctuating users, the percentage of posts with
Positive, Neutral and Negative sentiments are: 20.9%, 37.6% and
41.5% respectively.

classifies a text input into one of five sentiment categories
‚Äì from Very Positive to Very Negative. We merge the five
classes into three: Positive, Neutral and Negative (In future,
we may consider specific (health and nutrition) sentiment
lexicons).
We analyzed the sentiment of posts contributed by the
users from the two clusters. As shown in Figure 2, posts of
users belonging to the non-increasing cluster are more neutral whereas the posts made by users from the fluctuating
cluster are mainly of negative sentiment. This gives an interesting intuition that the users of fluctuating group might
require more emotional support as they use more negative
sentiment in their posts.

4.4

Politeness

Politeness is an important marker which often is a decisive
factor in whether interactions go well or cease (Rogers and
Lee-Wong 2003). Based on this metric, we can understand
if correlation exists between the politeness of posts made
by users and their weight loss pattern. Politeness according to the Webster‚Äôs Dictionary is to show good manners
towards others, as in behavior, speech, etc. We measured
how polite the posts are with respect to the weight loss pattern. We use the politeness classifier (Danescu-NiculescuMizil et al. 2013) that was constructed with a wide range
of domain-independent lexical, sentiment and dependency
features and there by operationalizes the key components of
politeness theory. It was proven that this classifier achieves
near human-level accuracy across domains (shown 83.79%
classification accuracy on in-domain wiki). Below are some
examples obtained after the classification of posts on this forum.
1. Polite text: ‚ÄúGood for you! I started out obese. Now,
Im not even overweight. Its a great feeling. Congrats

to you on your milestone!‚Äù
‚Ä¢ Polite Score: 0.870
‚Ä¢ Impolite Score: 0.130
2. Impolite text: ‚ÄúGrrrr.... I wish I could screen these
posts so that I dont even have to SEE those darn posts
about HCG or 500 calorie diets any more. :twisted:
And why did my search for Grumpy or Rant or
McRant come up empty?????? Grrrrr......‚Äù
‚Ä¢ Polite Score: 0.250
‚Ä¢ Impolite Score: 0.750
The results of the politeness analysis in Table 3 shows
that users on this weight-loss forum are polite overall. We
speculate that users on weight-loss related forums act polite
to get more information and emotional support. Further investigation is needed to conclude if users on goal-oriented
communities talk politely.
Type

Polite

Impolite

Non-increasing
Fluctuating

70.6%
75%

29.4%
25%

Table 3: Statistics of users and politeness percentage posts

4.5

Excuses

Literature (Bambina 2007) suggests that people use online
forums to maintain accountability. This application mainly
serves the user community to set goals and help the members
achieve those goals. It is important to understand if there is a
correlation between the weight loss pattern of the users and
the way they are making excuses as they are accountable for
not losing weight. In general, excuses are put forward when
people experience questions about their conduct or identity
in case of failing at an assigned task, violating a norm, etc.
Existing research (Deppe and Harackiewicz 1996)
demonstrates that people who are provided with the opportunity to make excuses do seem to perform better on a variety
of tasks. In this analysis, we wanted to verify if the hypothesis that users when given an opportunity to make excuses are
better at losing weight. Here is an example that shows how
User 1 posts excuses in a forum thread.
Example thread: ‚ÄúTrouble sticking to a diet‚Äù
User 1: ‚ÄúI am out of town with the family and making
the right food choices is impossible right now.‚Äù
User 2: ‚ÄúI think we all have to find our own motivation
and drive to succeed in weight loss. We just have to let
the motivation be louder than the excuses.‚Äù
To the best of our knowledge, there is no prior work on automatic classification of a post as an excuse or a non-excuse.
In this regard, we initially wanted to find if excuse classification is simply a special case of text-based categorization or
any special classification approaches need to be developed.
We performed experiments with two standard algorithms:
Naive Bayes Classification and Support Vector Machines,
which were shown to be effective in previous text categorization studies. In order to implement these two algorithms

we considered the standard bag-of-words where a document
d can be expressed in terms of the frequency of each of the
n features as d~ = ( f1 (d), ( f2 (d), . . . , ( fn (d)), fi (d) is the number of times feature i occurs in document d.
We also extended the Latent Dirichlet Allocation (Blei,
Ng, and Jordan 2003) (LDA) to build a classifier that also
uses majority class voting approach to provide labels to the
posts. Initially, LDA is used to extract the latent topic distribution over each of the posts present in the training dataset
that are already labeled as excuses and non-excuses. Later,
each post from the testing dataset is represented in this topic
space. For a given post in the testing dataset, the final class
label is the majority class of the k-closest points in the topic
space. The entire process of classifying a post as an excuse
or non-excuse is described in Algorithm 1.
Data: Labelled dataset ‚Äì Excuses (e f ) and Non-excuses
(ne f ); jth post ‚Äì a post with no class label
Result: Labelled Testing data
Œ∏e ne ‚Üê LDAestimation (e f ,ne f );
œÜene
‚Üê LDAin f erence (kth post, Œ∏iene );
k
L ‚Üê ‚àÖ;
for i := 1 to |e f | + |ne f | do
dist ‚Üê KLdivergence(Œ∏iene , œÜene
j );
append(L, dist);
end
label jth post ‚Üê max class(k-nearestpoints( f ullist));
Algorithm 1: Classification Approach
We utilized Weka (Hall et al. 2009) and svmlight (Joachims 1999) libraries to perform classification using Naive Bayes and SVM respectively. Based on the results
shown in Table 4, LDA-based supervised classifier outperforms the other two approaches and so we use it for measuring the correlation between the frequency of excuses posted
by users and their weight loss patterns.
Approach
Naive Bayes
SVM
LDA-based

Cond-1

Cond-2

57.8% (Uni)
50% (Uni)
65% (80-20 split)

63.1% (Uni + Bi)
46.15% (Uni + Bi)
50% (50-50 split)

Table 4: Classification results in terms of accuracy with different
approaches and conditions (Uni ‚Äì Unigrams; Bi ‚Äì Bigrams; 80-20
split ‚Äì 80% training and 20% testing; 50-50 split ‚Äì 50% training
and 50% testing data) and classifiers

We identified that 46% of the users who make at least one
post in the forum give excuses. If we consider the categorywise statistics, 48% of the users who lose weight in a nonincreasing pattern and 54% of the posts made by the users
of fluctuating weight loss pattern made excuses in at least
one post. It is surprising to notice that users exhibit excuse‚Äì
giving behavior on this weight loss community where accountability is one of its characteristics. Early detection of
these kinds of users and providing more assistance to help
them stay motivated can help lose weight. This kind of in-

tervention by these applications can help gain the trust of its
users.
Overall, in this section we have explored how the basic lexical classes, questions, sentiment, politeness and excuses are
correlated with the weight loss patterns of users. As we got a
good level of understanding about these associations, we can
now use these different attributes as a set of features in order
to predict whether a new user can lose weight or not, based
only on the language he/she is using on these forums. Automated classifier can be very beneficial to design effective
weight loss applications that can help users get additional
support. It can also help the users to pay more attention to
their diet and exercise to lose weight effectively.

5

Forums Studied

We used threads from two other popular online forums
that were used in (Biyani et al. 2012) ‚Äì 1) Trip Advisor New York City travel forum that contains travel related discussions for New York City and 2) Ubuntu forum dataset
that contains discussions about the ubuntu operating system.
There are multiple threads of discussions in both these forums and each thread has multiple posts by several users.
The dataset contains total number of 609 threads (6591 total posts) and 621 threads (3603 total posts) for Tripadvisor
and Ubuntu forums respectively. On an average, the thread
length in terms of the number of posts is 10 and 5 for the tripadvisor and ubuntu forums respectively. The average number of users in a thread on tripadvisor forum is 1.98 and
on ubuntu forum is 3.41. As stated in (Biyani et al. 2012),
ubuntu forums have technical discussions which are nonsubjective in nature where as trip advisor, a travel related
discussion forum has discussions which tend to be subjective.

5.2

Trip Advisor Ubuntu
Ling. class

Mean

SD

Mean

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

2.37
1.87
1.42
2.69
4.25

4.9
3.6
3.19
5.7
8.56

2.04
1.86
1.0
2.24
2.78

6.06
3.76
2.17
4.4
6.52

Table 5: Results of statistical significance tests on linguistic class
attributes for Trip Advisor and Ubuntu forums. For the results on
weight loss forum, please refer to Table 2. SD‚ÄìStandard Deviation

Comparison With General Forums

To contextualize this research, we want to understand if the
goal-oriented forums exhibit any specific traits compared to
the general forums. We define goal-oriented forums as the
forums associated with applications that help set goals while
building a social network of users who share similar goals.
Here, we present an analysis of how the type of forum can
affect the language used with a primary focus on understanding the lexical features and cohesiveness of the threads on
these forums.

5.1

Forum Name

Lexical Features

Lexical features like Part-of-Speech (POS) tags are obtained
for both the forums to understand the behavior of users in
terms of using different categories of words. Analysis similar to the earlier section was conducted using the Stanford
POS tagger to find the number of verbs, conjunctions, adverbs, personal pronouns and prepositions appearing in the
posts as shown in Table 5. As we compare these results with
the weight loss forum, we notice that users on these two forums don‚Äôt use as many personal pronouns and adverbs as
users on the weight loss forums. This is understandable as
users on weight loss forum have a primary goal to seek information while maintaining accountability.

5.3

Cohesion with Previous Posts

It is very important for the discussion forums to capture as
much participation as possible to reach their full potential.
When multiple conversations occur simultaneously, it is difficult to decide which utterance belongs to a specific conversation. Users on the online health forums mostly tend to
seek information and if the main topic is drifted to some
other topic, the main purpose of these discussion forums is
lost. Hence, it is important for the system to automatically
track non-cohesiveness in posts. Cohesion is the property of
a well-written document that links together sentences in the
same context. As a first step, we want to find out how similar a user‚Äôs post is with respect to the previous posts in a
thread from the weight loss forum. This can also help identify users in a given thread who elaborate on previous post
versus those who shift the topic.
Below is an example (paraphrased) showing cohesive post
made by the users on the weight loss forum.
Example thread: ‚Äúchanging life for a healthier self‚Äù
showing cohesive post by User 2
User 1: ‚ÄúDid you remove any commitments in your life
to make time to be healthier? If you have, was it a good
choice or did you regret it?‚Äù
User 2: ‚ÄúYes I‚Äôve done it and never regretted it.‚Äù
User 3: ‚ÄúTrying to do everything at once means doing
nothing - Georg Christoph‚Äù
User 2: ‚ÄúI‚Äôm not sure which entrepreneur said this but
focus only on what you need to do.‚Äù
We focus only on content words: verbs and nouns (partof-speech tags VB, VBZ, VBP, VBD, VBN, VBG, NN, NNP,
NNPS) and use WordNet (Miller 1995) to identify synonyms
of the content words. We compute similarity between the
current post and previous posts of other users in the thread in
terms of commonly shared verbs and nouns including synonyms. In our current analysis, we consider this similarity
score to be the measure of cohesion.
We consider all posts that are not thread-initial. To approximate whether a post is cohesive or not, we compare
the nouns and verbs of the current post to the list of nouns
and verbs (plus synonyms) obtained from the previous posts
of the thread. Our analysis (Table 6) on the three forums
‚Äì fit now data (weight loss forum), trip advisor and ubuntu

Data: Posts P1 , . . ., Pk‚àí1 , Pk
Result: CohS core(Pk )
set A ‚Üê ‚àÖ;
for i := 1 to (k ‚àí 1) do
[vbi , nni ] ‚Üê POS tagging (Pi );
set A ‚Üê set A ‚à™ [vbi , nni ];
set A ‚Üê set A ‚à™ synset(vbi ) ‚à™ synset(nni );
end
set B ‚Üê ‚àÖ;
[vbk , nnk ] ‚Üê POS tagging (Pk );
set B ‚Üê set B ‚à™ [vbk , nnk ];
set B ‚Üê set B ‚à™ synset(vbk ) ‚à™ synset(nnk );
A ‚à©set B |
CohS core(Pk ) ‚Üê |set|set
B|
Algorithm 2: Calculating the cohesive score of a post

finds that the threads on weight loss forum are more cohesive compared to the other two forums.
Fit Now data

Trip Advisor

Ubuntu

0.46
2.22

0.42
3.64

0.30
3.87

Cohesiveness
S.E (√ó10‚àí4 )

Table 6: Average value of Cohesiveness (along with Standard Error
(S.E) ) across all the threads in a given forum. Extreme values are:
0 ‚Äì non-cohesive; 1 ‚Äì cohesive

Overall, it is interesting to see that the goal-oriented forums
(like weight loss forums) have more cohesive threads compared to the general forums. Additionally, users on the goaloriented forums tend to post more information about themselves. In the future it will be worth studying if language
cues can help in predicting auto-tagging of threads to a specific type of forum. Studying other language metrics can also
help understand the contributions of different online forums
and their impact on the public.

6

Implications

The different language metrics studied in the two main
sections of this paper have a great potential to differentiate automatically between users who are struggling to lose
weight and the users who lost weight and are keeping it
off. There are for example, other existing technologies that
help users lose weight by ‚Äì providing incentives if they
lose weight (PACT http://www.gym-pact.com/), allowing other fitness applications to synchronize with the current
application to keep track of exercise (MyFitnessPal https:
//www.myfitnesspal.com/ ), posting questions while doing grocery shopping to find out the calorie content (Fooducate http://www.fooducate.com/ ), etc. We envision
tools that utilize the wealth of information present on the discussion forums along with the users activity to automatically
estimate the degree to which a user‚Äôs efforts will yield results. Predictions of success are not the end goals. The value
of these types of predictions are when they are leveraged
to generate alternative behaviors and actions that a user can
take to improve their chances of weight loss success. De-

signing systems that rely on features studied in this paper
could improve weight loss applications and thereby enhance
the quality of life.
People are taking advantage of these kinds of applications
as they can preserve their anonymity and provide genuine
information about their food intake, exercise levels, etc to
safely collect as much information as they can. Even though
the real identity can be hidden, it is important that the tools
being envisioned provide support in a very ethical manner.
On the other hand, deciphering the genuineness of the information provided is an area of research that can be worth
pursuing (Estrin 2014). On the whole, we believe that it is
important to understand the different attributes that affect the
behavior of individuals on the weight loss forums and help
them successfully lose weight. We hope that this work initiates further research on these types of discussion forums to
raise awareness about the different factors faced by individuals who are struggling to lose weight and thereby can help
develop policies that can support them in losing weight.

7

Conclusions and Future Work

In this paper, we analyzed how the online discussion forums
of weight loss applications can act as an important tool to
detect and identify the different metrics that are associated
with weight loss. As a first step, we identified the two types
of weight loss patterns exhibited by the users on this forum
and studied different factors like sentiment, politeness, excuses and questions. We took advantage of existing tools to
study these different factors and correlations between these
factors and the weight loss pattern. Specifically, this analysis reveals interesting insights about two populations of
users who lose weight differently. Users who lose weight
in a fluctuating manner are more active in these forums, give
more excuses, post more questions and the majority of their
posts contain negative sentiment. This shows the information seeking nature and suggests the possible need for more
support to these kinds of users. As a secondary focus, we
studied how the language metrics differ across goal-oriented
forums and general forums. We found that users of goaloriented forums usually contribute to a more cohesive posting threads and users on general forums tend not to reveal
much information about themselves.
Our analyses provides valuable insights on how user behavior within online weight loss forums might correlate with
the weight outcomes. These sorts of analyses, particularly
when replicated, could provide valuable insights for developing new technologies that might facilitate more effective
interactions about weight loss and can help gain trust of
users in these kinds of systems. It could also provide valuable insights for improving theories about behavior change.
Acknowledgments. This research is supported in part by a
Google research award, the ONR grants N00014-13-1-0176,
N00014-13-1-0519 and N00014-15-1-2027, and the ARO
grant W911NF-13- 1-0023.

References
Backstrom, L.; Kleinberg, J.; Lee, L.; and DanescuNiculescu-Mizil, C. 2013. Characterizing and curating con-

versation threads: Expansion, focus, volume, re-entry. In
ACM International Conference on Web Search and Data Mining, WSDM ‚Äô13, 13‚Äì22.
Ballantine, P. W., and Stephenson, R. J. 2011. Help me, I‚Äôm
fat! Social support in online weight loss networks. Journal of
Consumer Behaviour 10(6):332‚Äì337.
Bambina, A. D. 2007. Online Social Support: The Interplay
of Social Networks and Computer-Mediated Communication.
Bickart, B., and Schindler, R. M. 2001. Internet forums as influential sources of consumer information. J. of Int Marketing
15(3):31‚Äì40.
Biyani, P.; Bhatia, S.; Caragea, C.; and Mitra, P. 2012. Thread
specific features are helpful for identifying subjectivity orientation of online forum threads. In Proceedings of the 24th International Conference on Computational Linguistics, 295‚Äì
310.
Black, L. W.; But, J. J.; and Russell, L. D. 2010. The secret is out! supporting weight loss through online interaction.
Cases on online discussion and interaction: Experiences and
outcomes.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research
3:993‚Äì1022.
Brownell, K. D., and Rodin, J. 1994. The dieting maelstrom:
Is it possible and advisable to lose weight? American Psychologist 49(9):781‚Äì791.
Danescu-Niculescu-Mizil, C.; Sudhof, M.; Jurafsky, D.;
Leskovec, J.; and Potts, C. 2013. A computational approach
to politeness with application to social factors. In Proceedings of ACL.
Das, A., and Faxvaag, A. 2014. What influences patient participation in an online forum for weight loss surgery? IJMR
3(1).
De Choudhury, M.; Counts, S.; and Horvitz, E. 2013. Predicting postpartum changes in emotion and behavior via social
media. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ‚Äô13, 3267‚Äì3276.
Deppe, R. K., and Harackiewicz, J. M. 1996. Selfhandicapping and intrinsic motivation: Buffering intrinsic
motivation from the threat of failure. J. of Personality and
Social Psychology 70(4):868‚Äì876.
Estrin, D. 2014. Small data, where n = me. Commun. ACM
57(4):32‚Äì34.
GoÃÅmez, V.; Kaltenbrunner, A.; and LoÃÅpez, V. 2008. Statistical analysis of the social network and discussion threads in
slashdot. In Proceedings of the 17th International Conference
on World Wide Web, WWW ‚Äô08, 645‚Äì654.
Hall, M.; Frank, E.; Holmes, G.; Pfahringer, B.; Reutemann,
P.; and Witten, I. H. 2009. The weka data mining software:
An update. SIGKDD Explorations 11(1).
Hawn, C. 2009. Take two aspirin and tweet me in the morning; how twitter, facebook, and other social media are reshaping health care. Health Affairs 28(2):361‚Äì368.
Heaivilin, N.; Gerbert, B.; Page, J.; and Gibbs, J. 2011. Public health surveillance of dental pain via twitter. Journal of
Dental Research 90(9):1047‚Äì1051.
Hekler, B. E.; Dubey, G.; McDonald, W. D.; Poole, S. E.;
Li, V.; and Eikey, E. 2014. Exploring the relationship between changes in weight and utterances in an online weight
loss forum: A content and correlational analysis study. J Med
Internet Res 16(12).

Joachims, T. 1999. Making large-scale SVM learning practical. In SchoÃàlkopf, B.; Burges, C.; and Smola, A., eds., Advances in Kernel Methods - Support Vector Learning. Cambridge, MA: MIT Press. chapter 11, 169‚Äì184.
Kim, A. J. 2000. Community Building on the Web: Secret
Strategies for Successful Online Communities.
Kraschnewski, J. L.; Boan, J.; Esposito, J.; Sherwood, N. E.;
Lehman, E. B.; Kephart, D. K.; and Sciamanna, C. N. 2010.
Long-term weight loss maintenance in the united states. International J. of Obesity 34(11):1644‚Äì1654.
Lamb, A.; Paul, M. J.; and Dredze, M. 2013. Separating fact
from fear: Tracking flu infections on twitter. In NAACL.
Leahey, T. M.; Kumar, R.; Weinberg, B. M.; and Wing, R. R.
2012. Teammates and social influence affect weight loss
outcomes in a team-based weight loss competition. Obesity
20(7):1413‚Äì1418.
Ludford, P. J.; Cosley, D.; Frankowski, D.; and Terveen, L.
2004. Think different: Increasing online community participation using uniqueness and group dissimilarity. In SIGCHI
Conference on Human Factors in Computing Systems, CHI
‚Äô04, 631‚Äì638.
Miller, G. A. 1995. Wordnet: A lexical database for English.
Communications of the ACM 38(11):39‚Äì41.
Must, A.; Spadano, J.; Coakley, E. H.; Field, A. E.; Colditz,
G.; and H., D. W. 1999. The disease burden associated with
overweight and obesity. JAMA 282(16):1523‚Äì1529.
2012. The obesity epidemic and its impact on hypertension.
Canadian Journal of Cardiology 28(3):326 ‚Äì 333.
Ogden, C. L.; Kit, B. K.; Fakhouri, T. H.; Carroll, M. D.; and
Flegal, K. M. 2014. The epidemiology of obesity among
adults. GI Epidemiology 394‚Äì404.
Pennebaker, J. W.; Mehl, M. R.; and Niederhoffer, K. G.
2003. Psychological aspects of natural language use: Our
words, our selves. Annual Review of Psychology 54(1):547‚Äì
577.
Preece, J.; Nonnecke, B.; and Andrews, D. 2004. The top
five reasons for lurking: improving community experiences
for everyone. Computers in Human Behavior 20(2):201 ‚Äì
223.
Rogers, P. S., and Lee-Wong, S. M. 2003. Reconceptualizing
politeness to accommodate dynamic tensions in subordinateto-superior reporting. 17(4):379‚Äì412.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
EMNLP, 1631‚Äì1642.
Toutanova, K.; Klein, D.; Manning, C. D.; and Singer, Y.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL ‚Äô03, 173‚Äì180.
Wing, R. R., and Phelan, S. 2005. Long-term weight loss
maintenance. The American Journal of Clinical Nutrition
82(suppl):222S‚Äì5S.
Yang, J.; Wei, X.; Ackerman, M. S.; and Adamic, L. A. 2010.
Activity lifespan: An analysis of user survival patterns in online knowledge sharing communities. In ICWSM.

Planning with Resource Conflicts in Human-Robot Cohabitation
1 Department of Computer Science, Arizona State University, AZ Autonomous Systems & Robotics Intelligent Systems Division, NASA Ames Research Center, CA

Tathagata Chakraborti1 , Yu Zhang1 , David E. Smith2 , Subbarao Kambhampati1

2

tchakra2@asu.edu, yzhan442@asu.edu, david.smith@nasa.gov, rao@asu.edu ABSTRACT
In order to be acceptable members of future human-robot ecosystems, it is necessary for autonomous agents to be respectful of the intentions of humans cohabiting a workspace and account for conflicts on shared resources in the environment. In this paper we build an integrated system that demonstrates how maintaining predictive models of its human colleagues can inform the planning process of the robotic agent. We propose an Integer Programming based planner as a general formulation of this flavor of "humanaware" planning and show how the proposed formulation can be used to produce different behaviors of the robotic agent, showcasing compromise, opportunism or negotiation. Finally, we investigate how the proposed approach scales with the different parameters involved, and provide empirical evaluations to illustrate the pros and cons associated with the proposed style of planning. tiation. Specifically, we ask the question, what information can be extracted from the predicted plans, and how this information can be used to guide the behavior of the autonomous agent. There has been previous work [1, 6] on some of the modeling aspects of the problem, in terms of planning with uncertainty in resources and constraints. In this paper we provide an integrated framework (shown in Figure 1) for achieving these behaviors of the autonomous agent, particularly in the context of stigmergic coordination of human-robot cohabitation. To this end, we modularize our architecture so as to handle the uncertainty in the environment separately with the planning process, and show how these individual modules interact with each other by the way of usage profiles of the concerned resources.

1.

INTRODUCTION

In environments where multiple agents are working independently, but utilizing shared resources, it is important for these agents to model the intentions and beliefs of other agents so as to act intelligently and prevent conflicts. In cases where some of these agents are human, as in the case of assistive robots in household environments, these are required (rather than just desired) capabilities of robots in order for them to be considered "socially acceptable" - this has been one of the important objectives of "human-aware" planning, as evident from existing literature in human-aware path planning [13, 10] and human-aware task planning [5, 9, 3, 15]. An interesting aspect of many of these scenarios, is the presence of many of the aspects of multi-agent environments, but absence of typical assumptions often made in explicit teaming scenarios between humans and robots, as pointed out in [4]. Probabilistic plan recognition plays an important role in this regard, because by not committing to a plan, that presumes a particular plan for the other agent, it might be possible to minimize suboptimal (in terms of redundant or conflicting actions performed during the execution phase) behavior of the autonomous agent. Here we look at possible ways to minimize such suboptimal behavior by ways of compromise, opportunism or negoAppears in: Proceedings of the 15th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2016), J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.), May 9≠13, 2016, Singapore. Copyright c 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.

Figure 1: Schematic diagram of our integrated system for belief modeling, goal recognition, information extraction and planning. The robot maintains a belief model of the environment, and uses observations from the environment to extract information about how the world may evolve, which is then used to drive its own planning process.

1069

The general architecture of the system is shown in Figure 1. The autonomous agent, or the robot, is acting (with independent goals) in an environment co-habited with other agents (humans), who are similarly self-interested. The robot has a model of the other agents acting independently in its environment. These models may be partial and hence the robot can only make uncertain predictions on how the world will evolve with time. However, the resources in the environment are limited and are likely to be constrained by the plans of the other agents. The robot thus needs to reason about the future states of the environment in order to make sure that its own plans do not produce conflicts with respect to the plans of the other agents. With the involvement of humans, however, the problem is more skewed against the robot, because humans would expect a higher priority on their plans - robots that produce plans that clash with those of the humans, without any explanation, would be considered incompatible for such an ecosystem. Thus the robot is expected to follow plans that preserve the human plans, rather than follow a globally optimal plan for itself. This aspect makes the current setting distinct from normal human robot teaming scenarios and produces a number of its own interesting challenges. How does the robot model the human's behavior? How does it plan to avoid friction with the human plans? If it is possible to communicate, how does it plan to negotiate and refine plans? These are the questions that we seek to address in this work. Our approach models human beliefs and defines resource profiles as abstract representations of the plans predicted on the basis of these beliefs. The robot updates its beliefs upon receiving new observations, and passes on the resultant profiles onto its planner, which uses an IPformulation to minimize the overlap between these resource profiles and those produced by the human's plans. The contribution of our paper is thus three-fold, we (1) propose resource profiles as a concise mode of representing different types of information from predicted plans; (2) develop an IP-based planner that can utilize this information and provide different modalities of conformant behavior; and (3) provide an integrated framework that supports the proposed mode of planning - the modular approach also provides an elegant way to handle different challenges separately (e.g. uncertainty and/or nested beliefs of humans leaves the planner). The planner, as a consequence of these, has properties not present in existing planners - for example, the work that probably comes closest is [9] that models a specific case of compromise only, while the formulation is also likely to blow up in presense of large hypothesis sets due to absence of concise representation techniques like the profiles. We will discuss the trade-offs and design choices in more detail in the evaluation sections. The rest of the paper is organized as follows. We will start with a brief introduction of the agent models that comprise the belief component, and describe how it facilitates plan recognition. Then, in Sections 2.3 and 2.4, we are going to go into details of how resource profiles may be used to represent information from predicated plans, and describe how our planner converts this information into constraints that can be solved as an integer program during the plan generation process. In Section 3 we will demonstrate how the planner may be used to produce different modes of autonomous behavior. Finally in Section 4 we will provide empirical evaluations of the planner's internal properties.

Figure 2: Use case - Urban Search And Rescue (USAR).

2.

PLANNING WITH CONFLICTS ON SHARED RESOURCES

We will now go into details about each of the modules shown in Figure 1. The setting (adopted from [14]) involves a commander CommX and a robot in a typical USAR (Urban Search and Rescue) task illustrated in Figure 2. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other agents (not shown) in the environment. The shared resources here are the two medkits - some of the plans the commander can execute will lock the use of and/or change the position of these medkits, so that from the set of probable plans of the commander we can extract a probability distribution over the usage (or even the position) of the medkit over time based on the fraction of plans that conform to these facts. These resource availability profiles (i.e. the distribution over the usage or position of the medkit evolving over time) provide a way for the agents to minimize conflicts with the other agents. Before going into details about the planner that achieves this, we will first look at how the agents are modeled and how these profiles are computed in the next section.

2.1

The Belief Modeling Component

The notion of modeling beliefs introduced by the authors in [14] is adopted in this work. Beliefs about state are defined in terms of predicates bel(, ), where  is an agent with belief  = true. Goals are defined by predicates goal(, ), where agent  has a goal . The set of all beliefs that the robot ascribes to  together represents the perspective for the robot of . This is obtained by a belief model Bel of agent , defined as {  | bel(, )  Belself }, where Belself are the first-order beliefs of the robot (e.g., bel(self, at(self, room1))). The set of goals ascribed to  is similarly described by {goal(, )|goal(, )  Belself }. Next, we turn our attention to the domain model D of the agent  that is used in the planning process. We use PDDL [11] style agent models for the rest of the discussion, but most of the analysis easily generalizes to other related modes of representation. Formally, a planning problem  = D ,  consists of the domain model D and the problem instance  . The domain model of  is defined as D = T , V , S , A , where T is a set of object types; V is a set of variables that describe objects that belong to types in T ; S is a set of named first-order logical predicates over the variables V that describe the state; and A is a set of operators available to the agent. The action models a  A are represented as a = N, C, P, E where N denotes

1070

the name of that action; C is the cost of that action; P  S is the list of pre-conditions that must hold for the action a to be applicable in a particular state s  S of the environment; and Ea = ef f + (a), ef f - (a) , ef f ± (a)  S is a tuple that contains the add and delete effects of applying the action to a state. The transition function  (∑) determines the next state after the application of action a in state s as  (a, s) |=  if f  P s.t. f  s;  (a, s) |= (s \ ef f - (a))  ef f + (a) otherwise. The belief model, in conjunction with beliefs about the goals / intentions of another agent, will allow the robot to instantiate a planning problem  = O , I , G , where O is a set of objects of type t  T ; I is the initial state of the world, and G is a set of goals, which are both sets of the predicates from S initialized with objects from O . First, the initial state I is populated by all of the robot's initial beliefs about the agent , i.e. I = { | bel(, )  Belrobot }. Similarly, the goal is set to G = { | goal(, )  Belrobot }. Finally, the set of objects O consists of all the objects that are mentioned in either the initial state, or the goal description: O = {o | o  ( |   (I  G ))}. The solution to the planning problem is an ordered sequence of actions or plan given by  = a1 , a2 , . . . , a| | , ai  A such that  ( , I ) |= G , where the cumulative transition function is given by  (, s) =  ( a2 , a3 , . . . , a|| ,  (a1 , s)). The cost of the plan is given by C ( ) = a Ca and the optimal   plan  is such that C ( )  C ( )  with  ( , I ) |= G . This planning problem instance (though not directly used in the robot's planning process) enables the goal recognition component to solve the compiled problem instances. More on this in the next section.

2.2.2

Goal / Plan Recognition

In the present scenario, we thus have a set  of goals that  may be trying to achieve, and observations of the actions  is currently executing. At this point we refer to the work of Ramirez and Geffner who in [12] provided a technique to compile the problem of plan recognition into a classical planning problem. Given a sequence of observations , we recompute the probability distribution  over G   by using a Bayesian update P (G|)  P (|G), where the likelihood is approximated by the function P (|G) = 1/(1 + e- (G,) ) where (G, ) = Cp (G - ) - Cp (G + ). Here (G, ) gives an estimate of the difference in cost Cp of achieving the goal G without and with the observations, thus increasing P (|G) for goals that explain the given observations. Thus, solving two compiled planning problems, with goals G -  and G + , gives us the required posterior update for the distribution  over possible goals of . The details of the approach is available at [12]. The specific problem we will look at now is how to inform the robot's own planning process from the recognized goal set  . In order to do this, we compute the optimal plans for each goal in the hypothesis goal set  , and associate them with the probabilities of these goals from the distribution thus obtained. Information from these plans is then represented concisely in the form of resource profiles.

Notes on the Recognition Module
For our plan recognition module we use a much faster variation [7] of the above approach that exploits cost and interaction information from plan graphs to estimate the goal probabilities. This saves on the computational effort of having to solve two planning problems per goal. Also, note that while computing the plan to a particular goal G, we use a compiled problem instance with the goal G +  to ensure that the predicted plan conforms to the existing observations. Details on the compilation is available at [12]. Also, the output of the planner does not need to be associated with probabilities - this is just the most general formulation. If we want to deal with just a set of plans that the robot needs to be aware of, we can treat the plan set either with a uniform distribution and/or by requiring exactly zero conflicts in the objective of the planner (this will become clearer in Section 2.4) depending on the preference. Perhaps the biggest computational issue here is the need to compute optimal plans. While we still do it for our domain, as we will note later in Section 2.3, this might not be necessary, and suboptimal plans may be used in larger domains where computation is an issue.

2.2

The Goal Recognition Component

For many real world scenarios, it is unlikely that the goals of the humans are known completely, and that the plan computed by the planner is exactly the plan that they will follow. We are only equipped with a belief of the likely goal(s) of the human - and this may not be a full description of their actual goals. Further, in the case of an incompletely specified goal, there might be a set of likely plans that the human can execute, which brings into consideration the idea of incremental goal recognition over a possible goal set given a stream of observations.

2.2.1

Goal Extension

To begin with, it is worth noting that the robot might have to deal with multiple plans even in the presence of completely specified goals (even if the other agents are fully rational). For example, there may be multiple optimal ways of achieving the same goal, and it is not obvious beforehand which one of these an agent is going to end up following. In the case of incompletely specified goals, the presence of multiple likely plans become more relevant. To accommodate this, we extend the robot's current belief of an agent 's goal, G , to a hypothesis goal set  . The computation of this goal set can be done using the planning graph method [2]. In the worst case,  corresponds to all possible goals in the final level of the converged planning graph. Having further (domain-dependent) knowledge (e.g. in our scenario, information that CommX is only interested in triage-related goals) can prune some of these goals by removing the goal conditions that are not typed on the triage variable.

2.3

Resources and Resource Profiles

As we discussed previously, since the plans of the agents are in parallel execution, the uncertainty introduced by the commander's actions cannot be mapped directly between the commander's final state and the robot's initial state. However, given the commander's possible plans, the robot can extract information about at what points of time the shared resources in the environment are likely to be locked by the commander. This information can be represented by resource usage profiles that capture the expected (over all the recognized plans) variation of probability of usage or availability over time. The robot can, in turn, use this information to make sure that the profile imposed by its own plan has minimal conflicts with those of the commander's.

1071

Figure 3: Different types of resource profiles.

Formally, a profile is defined as a mapping from time step T to a real number between 0 and 1, and is represented by a set of tuples as follows G : N  [0, 1]  {(t, g ) : t  N, g  [0, 1], such that G(t) = g at time step t}. The concept of resource profiles can be handled at two levels of abstraction. Going back to our running example, shared resources that can come under conflict are the two (locatable typed objects) medkits, and the profiles over the medkits can be over both usage and location, as shown in Figure 3. These different types of profiles can be used (possibly in conjunction if needed) for different purposes. For example, just the usage profile shown on top is more helpful in identifying when to use the specific resource, while the resource when bound with the location specific groundings, as shown at the bottom can lead to more complicated higher order reasoning (e.g. the robot can decide to wait for the commander's plans to be over, as he inadvertently brings the medkit closer to it with high probability as a result of his own plans). We will look at this again in Section 3. Let the domain model of the robot be DR = TR , VR , SR , AR with the action models a = N, C, P, E defined in the same way as described in Section 2.1. Also, let   VR be the set of shared resources and for each    we have a set of predicates f   SR that are influenced (as determined by the system designer) by , and let  :   P ( ) be a function that maps the resource variables to the set of predicates  =  f  they influence. Without any external knowledge of the environment, we can set  = V  VR and  = S  SR , though in most cases these sets are much smaller. In the following discussion, we will look at how the knowledge from the hypothesis goal set can be modeled in terms of resource availability graphs for each of the constrained resources   . Consider the set of plans P  containing optimal plans corresponding to each goal in the hypothesis goal set, i.e.   P = G, ai  A i, G   = {G = a1 , a2 , . . . at |  (G , I ) |  } and let l( ) be the likelihood of the plan  modeled on the goal likelihood distribution  G   , p(G)   as l(G ) = c|G | ◊ p(G), where c is a normalization constant. At each time step t, a plan   P  may lock one or more of the resources . Each plan thus provides a profile of usage of a resource with respect to the time step t as  G : N  {0, 1} = {(t, g ) | t  [1, | |] and g = 1 if 

 is locked by  at step t, 0 otherwise} such that G (t) =  g  (t, g )  G . The resultant usage profile of a resource  due to all the plans in P  is obtained by summing over (weighted by the individual likelihoods) all the individual profiles as G  : N  [0, 1] = {(t, g ) | t  {1, maxP | |}   G ( t ) ◊ l (  ) } . and g  |1 P P|     Similarly, we can define profiles over the actual groundings of a variable (shown in the lower part of Figure 3) as f G = {(t, g ) | t  [1, | |] and f  = 1 at step t of plan  , 0 otherwise}, and the resultant usage profile due to all the f plans in P = {(t, g ) | t =  is obtained as before as G f 1 |  | and g  1, 2, . . . , maxP P G (t) ◊ l( )}.   |P  | These profiles are helpful when actions in the robot's domain are conditioned on these variables, and the values of these variables are conditioned on the plans of the other agents in the environment currently under execution. One important aspect of this formulation that should be noted here is that the notion of "resources" is described here in terms of the subset of the common predicates in the domain of the agents (  S  SR ) and can thus be used as a generalized definition to model different types of conflict between the plans between two agents. In as much as these predicates are descriptions (possibly instantiated) of the typed variables in the domain and actually refer to the physical resources in the environment that might be shared by the agents, we will stick to this nomenclature of calling them "resources". We will now look at how an autonomous agent can use these resource profiles to minimize conflicts during plan execution with other agents in its environment.

Notes on Usefulness of Profile Computation
One interesting aspect of computing resource profiles is that it provides a powerful interface between the belief on the environment and the planner. On the one hand, note that the input from the previous stage (goal/plan recognition module) is as generic as possible - a set of plans possibly associated with probabilities. Given any changes in preceding stages, e.g. modeling stochasticity or more complex belief models, still yields a set of plans that the robot needs to be aware of. Thus the plan set and resource profiles provide a surprisingly simple yet powerful way of abstracting away relevant information for the planner to use. The profiles may also be leveraged to address different modalities of conformant behavior, for example with multiple humans and their relative importance, by (1) weighing the contributions from individual profiles by the normalized priority of the human, which would cause the planner to avoid conflicts with these profiles more than with those with lower priorities; or (2) requiring zero conflicts on a subset of profiles which would cause the planner to avoid a subset of conflicts at all costs, while minimizing the rest. A somewhat implicit advantage of using profiles is its ability to form regions of interest given the possible plans. This will become clear later in Section 4.2 when we show that the predicted conflicts provide well-informed guidance to avoiding real conflicts during execution (as evident by the robustness in performance with just 1-3 observations, and zero actual conflicts in low probability areas in the computed profiles). Right now this has the implication that we need not necessarily compute perfect plan costs and goal distributions to get good plans.

1072

2.4

Conflict Minimization

The planning problem of the robot - given by  = DR , R ,  , {G |   }, {Gf | f  (),   } - consists of the domain model DR and the problem instance R = OR , IR , GR similar to that described in section 2.3, and also the constrained resources and all the profiles corresponding to them. This is because the planning process must take into account both goals of achievement as also conflict of resource usages as described by the profiles. Traditional planners provide no direct way to handle such profiles within the planning process. Note here that since the execution of the plans of the agents is occurring in parallel, the uncertainty is evolving at the time of execution, and hence the uncertainty cannot be captured from the goal states of the recognized plans alone, and consequently cannot be simply compiled away to the initial state uncertainty for the robot and solved as a conformant plan. Similarly, the problem does not directly compile into action costs in a metric planning instance because the profiles themselves are varying with time. Thus we need a planner that can handle these resource constraints that are both stochastic and non-stationary due to the uncertainty in the environment. To this end we introduce the following IP-based planner (partly following the technique for IP encoding for state space planning outlined in [16]) as an elegant way to sum over and minimize overlaps in profiles during the plan generation process. The following formulation finds such T-step plans in case of non-durative or instantaneous actions. For action a  AR at step t we have an action variable: 1, if action a is executed in step t xa,t = 0, otherwise; a  AR , t  {1, 2, . . . , T } Also, for every proposition f at step t a binary state variable is introduced as follows: 1, if proposition is true in plan step t yf,t = 0, otherwise; f  SR , t  {0, 1, . . . , T } Note here that the plan computed by the robot introduces a resource consumption profile itself, and thus one optimizing criterion would be to minimize the overlap between the usage profile due to the computed plan with those established by the predicted plans of the other agents in the environment. Let us introduce a new variable to model the resource usage graph imposed by the robot as follows: 1, if f   is locked at plan step t gf,t = 0, otherwise; f  , t  {0, 1, . . . , T } Further, for every resource   , we divide the actions in the domain of the robot into three disjoint sets +  yf,t = 1}, f = {a  AR such that xa,t = 1 = -  yf,t = 0}, and f = {a  AR such that xa,t = 1 = + - o f = AR \ (f  f ), f   . These then specify respectively those actions in the domain that lock, free up, or do not affect the use of a particular resource, and are used to calculate gf,t in the IP. Further, we introduce a variable hf,t to track preconditions required by actions in the generated plan whose success is conditioned on the influence of the plans of the other agents on the world (e.g. position of the medkits are changing, and the action pickup is conditioned on it) as follows: 1, if f  Pa and xa,t+1 = 1 hf,t = 0, otherwise; f  , t  {0, 1, . . . , T - 1}

Then the solution to the IP should ensure that the robot only uses these resources when they are in fact most expected to be available (as obtained by maximizing the overlap between  hf,t and Gf ). These act like demand profiles from the perspective of the robot. We also add a "no-operation" action AR  AR  a so that a = N, C, P, E where N = NOOP, C = 0, P = {} and E = {}. The IP formulation is given by: min k1 aAR t{1,2,...,T } Ca ◊ xa,t  +k2  f () t{1,2,...,T } gf,t ◊ G (t) -k3
 f () t{0,1,...,T -1}

hf,t ◊ Gf (t) (1) (2) (3) (4) (5) (6) (7) (8) (9) xa,t ) ◊ gf,t-1 (10) (11) (12) (13) (14) (15)



yf,0 = 1 f  IR \  yf,0 = 0 f  / IR or f   yf,T = 1 f  GR xa,t  yf,t-1 a s.t. f  Pa , f  / , t  {1, . . . , T } hf,t-1 = xa,t a s.t. f  Pa , f  , t  {1, . . . , T } yf,t  yf,t-1 + aadd(f ) xa,t s.t. add(f ) = {a|f  ef f + (a)}, f, t  {1, . . . , T } yf,t  1 - adel(f ) xa,t s.t. del(f ) = {a|f  ef f - (a)}, f, t  {1, . . . , T }
aAR a+ f

xa,t = 1, t  {1, 2, . . . , T }
t xa,t  1 f  , t  {1, 2, . . . , T } a+ f f

gf,t = hf,t ◊ G

xa,t + (1 -

a+ f

xa,t -

a- f

f  , t  {1, . . . , T } (t)  f  , t  {0, 1, . . . , T - 1}

yf,t  {0, 1} f  SR , t  {0, 1, . . . , T } xa,t  {0, 1} a  AR , t  {1, 2, . . . , T } gf,t  {0, 1} f  SR , t  {0, 1, . . . , T } hf,t  {0, 1} f  SR , t  {0, 1, . . . , T - 1}

where k1 , k2 , k3 are constants determining the relative importance of the optimization criteria and is a constant. Here, the objective function minimizes the sum of the cost of the plan and the overlap between the cumulative resource usage profiles of the predicted plans and that imposed by the current plan of the robot itself while maximizing the validity of the demand profiles. Constraints (1) through (3) model the initial and goal conditions, while the value of the constrained variables are kept uninitialized (and are determined by their profiles). Constraints (4) and (5), depending on the particular predicate, enforces the preconditions, or produces the demand profiles respectively, while (6) and (7) enforces the state equations that maintain the add and delete effects of the actions. Constraint (8) imposes non concurrency on the actions, and (9) ensures that the robot does not repeat the same action indefinitely to increase its utility. Constraint (10) generates the resource profile of the current plan, while (11) maintains that actions are only executed if there is at least a small probability of success. Finally (12) to (15) provide the binary ranges of the variables.

Note on Temporal Expressivity
At this point it is worth acknowledging the implications of having durative actions in our formulation. Note that our approach does not discretize time, but rather uses time

1073

points as steps in the plan - that can be easily augmented with their own durations. So in order to handle durative actions, the only (somewhat minor) change required in the formulation is in the way the conflicts are integrated (instead of summed) over in the objective function. Further, uncertainty in action durations is always a big issue in human interactions; though resource profiles cannot directly handle uncertain durations, it only affects the way the profiles are calculated, and the way in which information is expressed in it remains unchanged (i.e. expectations over action durations add an extra expectation to the already probabilistic profile computation). As noted before in Section 2.3, the ability of profiles to form regions of interest is crucial in handling such scenarios implicitly.

13 14 15 16 17 18 19

NOOP PICK_UP_MEDKIT_ROBOT_MK1_ROOM1 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_ROBOT_HALL2_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 DROP_OFF_ROBOT_MK1_HALL3

3.3

Negotiation

3.

MODULATING BEHAVIOR OF THE ROBOT

The planner is implemented on the IP-solver gurobi and integrates [7] and [8] respectively for goal recognition and plan prediction for the recognized goals. We will now illustrate how the formulation can produce different behaviors of the robot by appropriately configuring the parameters of the planner. For this discussion we will limit ourselves to a singleton hypothesis goal set in order to observe the robot's response more clearly.

In many cases, the robot will have to eventually produce plans that will have potential points of conflict with the expected plans of the commander. This occurs when there is no feasible plan with zero overlap between profiles (specifically gf,t ◊ G (t) = 0) or if the alternative plans for the robot are too costly (as determined by the objective function). If, however, the robot is equipped with the ability to communicate with the human, then it can negotiate a plan that suits both. To this end, we introduce a new variable H  (t) and update the IP as follows: min k1 aAR t{1,2,...,T } Ca ◊ xa,t  +k2  f -1 () t{1,2,...,T } gf,t ◊ H (t) -k3 +k4
 

hf,t ◊ Gf (t) t{0,1,...,T } ||G (t) - H (t)||
f -1 () t{0,1,...,T -1}  



yf,T  hf,t-1  a s.t. f  Pa , f  , t  {1, . . . , T } H  (t)  [0, 1]   , t  {0, 1, . . . , T }
 

(5a) (16)

3.1

Compromise

Let us now look back at the environment we introduced in Figure 1. Consider that the goal of the commander is to perform triage in room1. The robot computes the human's optimal plan (which ends up using medkit1 at time steps 7 through 12) and updates the resource profiles accordingly. If it has its own goal to perform triage in hall3, the plan that it comes up with given a 12 step lookahead is shown below. Notice that the robot opts for the other medkit (medkit2 in room3) even though its plan now incurs a higher cost in terms of execution. The robot thus can adopt a policy of compromise if it is possible for it to preserve the commander's (expected) plan.
01 02 03 04 05 06 07 08 09 10 11 12 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_ROBOT_HALL2_HALL3 MOVE_ROBOT_HALL3_HALL4 MOVE_REVERSE_ROBOT_HALL4_ROOM4 MOVE_REVERSE_ROBOT_ROOM4_ROOM3 PICK_UP_MEDKIT_ROBOT_MK2_ROOM3 MOVE_ROBOT_ROOM3_ROOM4 MOVE_ROBOT_ROOM4_HALL4 MOVE_REVERSE_ROBOT_HALL4_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 DROP_OFF_ROBOT_MK2_HALL3

H (t)  G (t)   , t  {0, 1, . . . , T } (17) Constraint (5a) now complements constraint (5) from the existing formulation, by promising to restore the world state every time a demand is made on a variable. The variable H  (t), maintained by constraints (16) and (17), determine the desired deviation from the given profiles. The objective function has been updated to reflect that overlaps are now measured with the desired profile of usage, and there is a cost associated with the deviation from the real one. The revised plan now produced by the robot is shown below.
01 02 03 04 05 06 07 08 09 10 MOVE_ROBOT_ROOM1_HALL1 MOVE_ROBOT_HALL1_HALL2 MOVE_REVERSE_ROBOT_HALL2_ROOM2 PICK_UP_MEDKIT_ROBOT_MK1_ROOM2 MOVE_ROBOT_ROOM2_HALL2 MOVE_ROBOT_HALL2_HALL3 CONDUCT_TRIAGE_ROBOT_HALL3 MOVE_REVERSE_ROBOT_HALL3_HALL2 MOVE_REVERSE_ROBOT_HALL2_ROOM2 DROP_OFF_ROBOT_MK1_ROOM2

3.2

Opportunism

Notice that the robot restores the world state that the human is believed to expect, and can now communicate to him "Can you please not use medkit1 from time 7 to 9?" based on how the real and the ideal profiles diverge, i.e. t such that H  (t) < G (t) for each resource .

Notice, however, that the commander is actually bringing the medkit to room1 as predicted by the robot, and this is a favorable change in the world, because robot can use this medkit once the commander is done and achieve its goal at a much lower cost. The robot, indeed, realizes this once we give it a bigger time horizon to plan with, as shown above (on the right). Thus, in this case, the robot shows opportunism based on how it believes the world state will change.
01 NOOP 02 NOOP ...

Notes on Adaptive Behavior Modeling
One might note here that people are often adaptive and it is very much possible that they may be willing to change their goals based on observing the robot or are even unwilling to negotiate if their plans conflict. Hence the policies of compromise and opportunism for the robot are complementary to negotiation in the event the latter fails. Thus, for example, the robot might choose to communicate a negotiation strategy to the human, but fall back on a compromise if that fails. It is a merit of such a simple formulation to be able to handle such interesting adaptive behaviors.

1074

4.

EVALUATION

The power of the proposed approach lies in the modular nature in which it tackles several complicated problems that are separate research areas in their own rights. As we saw throughout the course of the discussion, approaches used in the individual modules may be varied with little to no change in the rest of the architecture. For example the expressivity of the belief modeling or goal recognition component is handled separately as the planner used information from a generic plan set. Again the representation technique introduced in terms of resource profiles provide properties in terms of computational independence with respect to size of the hypothesis set and number of agents (which gets manifested in complexity in number of resources) that general planners do not have. So it becomes a design choice depending on which metric needs to be optimized. For empirical evaluations, we simulated the USAR scenario on 360 different problem instances, randomly generated by varying the specific (as well as the number of probable) goals of the human, and evaluated how the planner behaved with the number of observations it can start with to build its profiles. We fix the domain description, location and goal of the agents, and the position of the resources, and consider randomly generated hypothesis goal sets of size 211. The goals of the commander were assumed to be known to be triage related, but the location of the triage was allocated randomly (one of which was again picked at random as the real goal). Finally for each of these problems, we generate 1-5 observations by simulating the commander's plan over the real goal, and use these observations known a priori the robot's plan generation process. The experiments were conducted on an Intel Xeon(R) CPU E5-1620 v2 3.70GHz◊8 processor with a 62.9GiB memory.

(a) w.r.t. T (|| = 2)

(b) w.r.t. #medkits (T = 10)

Figure 4: Performance of the planner w.r.t. planning horizon T and number of constrained resources (medkits).

4.1

Scaling Up
the agents. On the other hand, the time spent on recognition, and on calculating the profiles, is significantly affected. However, observations on multiple agents are asynchronous, and goal recognition can operate in parallel, so that this is not a huge concern beyond the complexity of a single instance. Similarly the performance is also unaffected by the size of the hypothesis set  , as shown in Figure 5, which shows increase in the number of the possible goals does not complicate the profiles to an extent to affect the complexity.

Our primary contribution is the formulation for planning with resource profiles, while the goal recognition component can be any off-the-shelf algorithm, and as such we compare scalability with respect to the planning component only.

- w.r.t. Length of the Planning Horizon
The performance of the planner with respect to the planning horizon is shown in Figure 4a. This is, as expected, the bottleneck in computation due to exponential growth of the size of the IP. It is however not prohibitively expensive, and the planner is still able to produce plans of length 20 (steps, not durations) for our domain in a matter of seconds.

4.2

Quality of the Plans Produced

- w.r.t. Number of Resources
The performance of the planner with respect to the number of constrained resources (medkits, in the context of the current discussion) is shown in Figure 4b. Evidently, the computational effort is dominated by that due to the planning horizon. This reiterates the usefulness of abstracting the information in predicted plans in the form of resource profiles, thus isolating the complexities of the domain with that of the underlying planning algorithm.

- w.r.t. the Number of Agents and Goals
The planning module (i.e. the IP formulation) is by itself independent of the number of agents being modeled. In fact, this is one of the major advantages of using abstractions like resource profiles in lieu of actual plans of each of

We define U as the average conflicts per plan step when a demand is placed on a resource by the robot, and S as the success probability per plan step that the demand is met. C is the cost of a plan. F is the percentage of times there was an actual conflict during execution (distinct from U which estimates the possible conflict that may occur per plan step). We observe the quality of the plans produced by the planner by varying the ratio of parameters k1 and k3 from the objective function and the length of the planning horizon T . Similar results can be produced by varying k1 /k2 . From Table 1, as k1 /k3 decreases, the planner becomes more conservative (to maximize success probability) and thus plans become costlier. At the same time the expected success rate of actions are also increased (with simultaneous increase in usage conflict), as reflected by a higher failure rate due to actual execution time conflicts.

1075

Figure 5: Performance of the planner w.r.t. size of the goal set. As expected, computational complexity is not affected.

(a) w.r.t. | |

k1 /k3 C U S F

0.05 9.47 0.18 0.85 27.5

0.5 6.37 0.17 0.579 23.0

5.0 6.31 0.17 0.578 21.3

Table 1: Quality of plans produced w.r.t. k1 /k3 . Conservative plans result in lowered utility.

Also note, from Table 2 the impact of the planning horizon T on the types of behaviors we discussed in the previous section. As we increase T , the plan cost falls below the optimal, indicating opportunities for opportunistic behavior on the part of the robot. The expected conflict also falls to almost 0. However the expected success rate of actions also decreases, the ratio k1 /k2 determines how daring the robot is, in choosing between cheap versus possibly invalid plans. Note, however, the actual execution time conflict is extremely low with increasing T , for even sufficiently conservative estimates of S . Thus we see that the robot is successfully able to navigate conflicts and find in many cases plans even cheaper than the original optimal plan, thus highlighting the usefulness of the approach. Finally, we look at the impact of the parameters in the plan recognition module in Figure 6. As expected, with bigger hypothesis sets, the success rate goes down. Interestingly, the plan cost also shows a downward trend which might be because the bigger variety in possible goals give a better idea of which medkits are generally more useful for that instance at what points of time. With more observations, as expected, the success rate goes up and the expected conflict goes down. The cost, however, increases a little as the planner opts for more conservative options.

(b) w.r.t. #obs

Figure 6: Performance of the planner w.r.t. size of goal set and number of observations (k1 /k3 = 0.5, T = 16).

T C U S F

10 9.0 0.46 1.0 53.3

13 5.6 0.04 0.48 11.9

16 4.53 0 0.25 6.6

Optimal 9.0 n/a n/a 53.3

Table 2: Quality of plans produced w.r.t. T . Opportunities for opportunism explored, conflicts minimized.

5.

CONCLUSIONS

In this paper we investigate how plans may be affected by conflicts on shared resources in an environment cohabited by humans and robots, and introduce the concept of resource profiles as a means of representation for concisely modeling the information pertaining to the usage of such resources, contained in predicted behavior of the agents. We propose a general formulation of a planner for such scenarios and show how the planner can be used to model different types of behavior of the robot by appropriately configuring the

objective function and optimization parameters. Finally, we provide an end-to-end framework that integrates belief modeling, goal recognition and an IP-solver that can enforce the desired interaction constraints. One interesting research direction would be to consider nested beliefs on the agents; after all, humans are rarely completely aloof of other agents in its environment. Such interactions should have to consider evolution of beliefs with continued interactions and motivate further exploration of the belief modeling component. The modularity of the proposed approach allows for focused research on each (individually challenging) subtask without significantly affecting the others.

Acknowledgment
This research is supported in part by the ONR grants N0001413-1-0176, N00014-13-1-0519 and N00014-15-1-2027, and the ARO grant W911NF-13-1-0023.

1076

REFERENCES
[1] E. Beaudry, F. Kabanza, and F. Michaud. Planning with concurrency under resources and time uncertainty. In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pages 217≠222, Amsterdam, The Netherlands, The Netherlands, 2010. IOS Press. [2] A. Blum and M. L. Furst. Fast planning through planning graph analysis. In IJCAI, pages 1636≠1642, 1995. [3] F. Cavallo, R. Limosani, A. Manzi, M. Bonaccorsi, R. Esposito, M. Di Rocco, F. Pecora, G. Teti, A. Saffiotti, and P. Dario. Development of a socially believable multi-robot solution from town to home. Cognitive Computation, 6(4):954≠967, 2014. [4] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz, D. Smith, and S. Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015. [5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Proc of the Int Conf on Advanced Robotics (ICAR), 2009. [6] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning: An application to mobile robots. ACM Trans. Intell. Syst. Technol., 1(2):15:1≠15:26, Dec. 2010. [7] Y. E.-Mart¥ in, M. D. R.-Moreno, and D. E. Smith. A fast goal recognition technique based on interaction estimates. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 761≠768, 2015. [8] M. Helmert. The fast downward planning system. CoRR, abs/1109.6051, 2011. [9] U. Koeckemann, F. Pecora, and L. Karlsson. Grandpa hates robots - interaction constraints for planning in inhabited environments. In Proc. AAAI-2010, 2014. [10] M. Kuderer, H. Kretzschmar, C. Sprunk, and W. Burgard. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings of Robotics: Science and Systems, Sydney, Australia, July 2012.

[11] D. Mcdermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and D. Wilkins. Pddl the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control 1998. " [12] M. Ramirez and H. Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In In Proc. AAAI-2010, 2010. [13] E. Sisbot, L. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. Robotics, IEEE Transactions on, 23(5):874≠883, Oct 2007. [14] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and S. Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠2962, Sept 2014. [15] S. Tomic, F. Pecora, and A. Saffiotti. Too cool for school - adding social constraints in human aware planning. In Proc of the International Workshop on Cognitive Robotics (CogRob), 2014. [16] T. Vossen, M. O. Ball, A. Lotem, and D. S. Nau. On the use of integer programming models in ai planning. In T. Dean, editor, IJCAI, pages 304≠309. Morgan Kaufmann, 1999.

1077

Explicable Robot Planning as Minimizing Distance
from Expected Behavior

arXiv:1611.05497v1 [cs.AI] 16 Nov 2016

Anagha Kulkarni1 , Tathagata Chakraborti1 , Yantian Zha1 ,
Satya Gautam Vadlamudi2 , Yu Zhang1 , and Subbarao Kambhampati1
Abstract‚Äî In order for robots to be integrated effectively into
human work-flows, it is not enough to address the question of
autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task
plans without such considerations, they may often demonstrate
what we refer to as inexplicable behavior from the point of view
of humans who may be observing it. This problem arises due to
the human observer‚Äôs partial or inaccurate understanding of the
robot‚Äôs deliberative process and/or the model (i.e. capabilities
of the robot) that informs it. This may have serious implications
on the human-robot work-space, from increased cognitive load
and reduced trust in the robot from the human, to more serious
concerns of safety in human-robot interactions. In this paper, we
propose to address this issue by learning a distance function that
can accurately model the notion of explicability, and develop
an anytime search algorithm that can use this measure in its
search process to come up with progressively explicable plans.
As the first step, robot plans are evaluated by human subjects
based on how explicable they perceive the plan to be, and
a scoring function called explicability distance based on the
different plan distance measures is learned. We then use this
explicability distance as a heuristic to guide our search in order
to generate explicable robot plans, by minimizing the plan
distances between the robot‚Äôs plan and the human‚Äôs expected
plans. We conduct our experiments in a toy autonomous car
domain, and provide empirical evaluations that demonstrate
the usefulness of the approach in making the planning process
of an autonomous agent conform to human expectations.

I. I NTRODUCTION
Recent advancement in the field of robotics has given
us autonomous robots, vehicles, drones, etc. Typically these
autonomous systems have the capability to make their own
plans which help them achieve their goals. These advances
have, naturally, encouraged the possibility of human-robot
teaming where the autonomous robots and humans can
work alongside each other. However, if the plans that are
being generated by the autonomous robots are difficult to
comprehend for the human observer, the unexpected behavior
from the robot can raise several concerns: it may increase
cognitive load, hamper the productivity of the team, and
result in safety concerns and distrust towards the robot [1].
This mismatch between the robot‚Äôs plans and the human
expectations may be explained in terms of difference in the
actual robot model and the human‚Äôs understanding of the
robot model. Thus, even with the knowledge of the robot‚Äôs
1 Anagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Yu Zhang, and
Subbarao Kambhampati are with the Computer Science and Engineering
Department at Arizona State University { akulka16, tchakra2,

yantian.zha, yzhan442, rao } @ asu.edu
2 Satya

Gautam

Vadlamudi

vsatyagautam@gmail.com

is

with

Capillary

Technologies.

Fig. 1: A schematic diagram of the proposed system. Explicability distance is the plan distance measure between the
robot plan and human expected plan. This distance is used
to guide the search to generate explicable plans.

goals, it may still not be possible for the human to make
sense of the robot‚Äôs plan. For example, consider a scenario
with an autonomous car switching lanes on a highway. The
autonomous car, in order to switch the lane, may make
sharp and calculated moves, as opposed to gradually moving
towards the other lane. These moves may well be optimal for
the car, and backed by the car‚Äôs superior sensing and steering
capabilities. Nevertheless, a human passenger sitting inside
may perceive this as dangerous and reckless behavior, in as
much as they might be ascribing the car the sort of driving
abilities they themselves have.
To address this issue of the differences between the robot
model, MR (R), and the human mental model of the robot
capabilities, MH (R), we develop an approach based on the
concept of explicability. An explicable plan is a plan that is
generated with the human‚Äôs expectation of the robot model;
the ability to synthesize explicable plans on the part of the
robot thus involves the ability to take into consideration
both models into its plan generation process. The intuition is
that, the similarity between the robot plan that is generated
from the robot model, and the plan that is generated from
the human understanding of the robot model determines
the explicability of the robot plan. More specifically, the
smaller the explicability distance between these two plans,
the more explicable the robot plan is. Of course, such a

similarity metric is not readily available, which brings us
to the question - How can a robot learn a distance function
between plans that model the notion of explicability, and how
can it use this learned similarity model to inform its own
deliberative process? Keeping this in mind, we address the
following questions in our paper: 1) Given a domain, can we
find an approximation to MH (R), the human mental model
of the robot capabilities? 2) Can the measures of distance
between a robot plan, œÄR , and the plan expected by the
human, œÄH , effectively capture the explicability of the robot
plan? 3) Can we then integrate the explicability estimates
into the robot plan generation process? The outline of our
proposed approach is illustrated in Figure 1.
To address the first question, we start out with a robot
model, MR (R) and generate different robot plans for various initial and goal states. Next, we recruit human subjects
and ask them to evaluate these plans by assigning scores
to them based on how well they understand them. As a
part of the study, the subjects are then asked to answer a
questionnaire based on the robot model, in order to elicit
implicit human preferences. This questionnaire allows the
domain modeler to generate MH (R), based on humans
assumptions regarding the robot model in that domain. In this
paper, we represent both models in PDDL [2], but they can
differ in terms of their action representations, preconditions,
effects, and costs.
To answer the second question, we explore the relationship
between three existing plan distance measures: action set,
causal link set and state sequence distances [3], [4] and the
plan explicability distance. We use the robot plans, assigned
with scores, to determine if the explicability of the plans can
be modeled in terms of the aforementioned plan distance
measures, in terms of a regression function. For this, we
generate the plans expected by the human for the same initial
and goal states using the human understanding of the robot
model. We then compute the plan distances between the robot
plans and human expected plans. We call the function that
maps the plan distances to the explicability scores as the
explicability distance.
To address the third question, we integrate the explicablity
distance in the search process of the Fast-Downward
planner [5]. We perform a cost-bounded anytime search,
that can progressively generate more and more explicable
plans, using the learned explicability distance as a heuristic
guidance. We call this reconciliation search. Note that explicability distance exhibits non-monotonicity, i.e. a new action
that gets added to a plan prefix can either increase or decrease
the explicability distance depending on the context of the
plan. We present an analysis on how this property affects
our search. For evaluation of our system, we demonstrate
the effectiveness of our system in a simulated autonomous
car domain, and use human test subjects to evaluate the
explicability of the generated robot plans.
II. RELATED WORK
The notion of robots working alongside humans for task
achievement has been a popular research direction. It is

challenging, mainly due to the fact that, the robot must
consider the human in the loop while making its own
decisions. One important requirement for achieving this, is
the ability to infer about the human‚Äôs intent and plan. Various
plan recognition algorithms [6], [7] can be applied to perform
plan recognition based on a given set of observations as a
result of the agent interacting with the environment. After
the intent and the plan of the human is identified, researchers
have also discussed how the robot can utilize this information
while avoiding conflicts [8], [9] or providing proactive help
to the human in the loop [10], [11]. There is also work
on performing simultaneous plan recognition and generation
[12]. However, most of the prior work has only focused on
how robots can make plans based on the inferred human
intent.
The motivation for generating explicable task plans was
first provided in our recent paper [13]. While that work
proposes learning explicability as a labeling scheme, in this
work, we consider viewing explicability more directly in
terms of distances between the plans generated by the robot‚Äôs
own model, and the human‚Äôs approximation of the robot‚Äôs
model. While explicability focuses on task plans, a related
notion of ‚Äúlegibility‚Äù has been studied in the context of
motion planning [14] and has been shown to be useful in
generating socially acceptable behaviors for robots [15], [16].
In most human-robot cohabitation work where robots are
proactive agents, it is often assumed that the human model is
provided and complete for inferring about the human intent
and plan. This is often not true. Although we also assume
a human model a priori, our formulation allows us to adjust
this model so as to improve model incompleteness (e.g.,
action preference). There also exists learnable models that
do not assume completeness in the first place [17]. Another
note is that in [13], [14] and this work, since the model is
one level deeper, which is about the robot model from the
humans perspective, learning methods are adopted.
III. BACKGROUND
A. Planning
A classical planning problem can be defined as a tuple
P = hM, I, Gi, where M = hF, Ai is the domain model
(that consists of a finite set F of fluents that define the
state of the world and a set of operators or actions A), and
I ‚äÜ F and G ‚äÜ F are the initial and goal states of the
problem respectively. Each action a ‚àà A is a tuple of the
form hpre(a), ef f (a), c(a)i where c(a) denotes the cost of
an action, pre(a) ‚äÜ F is the set of preconditions for the
action a and ef f (a) ‚äÜ F is the set of the effects. The
solution to the planning problem is a plan or a sequence
of actions œÄ = ha1 , a2 , . . . , an i such that starting from
the initial state, sequentially executing the actions lands the
robot in the goal state, i.e. ŒìM (I, œÄ) |= G where ŒìM (¬∑)
is the transition function defined P
for the domain. The cost
of the plan, denoted as c(œÄ) =
ai ‚ààœÄ c(ai ), is given by
the summation of the cost of all the actions in the plan œÄ.
Henceforth, we denote the robot plan as œÄ R and the human
expected plan as œÄ H .

2) Causal Link Distance: A causal link represents a tuple
of the form hai , pi , ai+1 i, where pi is a predicate variable
that is produced as an effect of action ai and used as a precondition for the next action ai+1 . The causal link distance
measure is represented similarly to the action distance, by
considering the causal link sets Cl(œÄ R ) and Cl(œÄ H ) instead
of action sets described above. It is written as:
Œ¥C (œÄ R , œÄ H ) = 1 ‚àí

Fig. 2: A simple illustration of how a robot‚Äôs optimal plan can
deviate from the human expectation due to model difference.
In this maze, the robot can move in all four direction: up,
down, left, right and also diagonally across the grid cells.
Some of the cell floors have glass floors and some others
have obstacles. The glass floors are harder for the robot to
navigate across, because of the reflective surface, it needs to
use special sensors which results in an expensive action for
the robot. The path in green is the explicable plan whereas
the path in red is the robot plan.

B. Plan Distance Measures
We now look at the three plan distance measures introduced in [3] and later refined in [4]. These plan distances are
action, causal link and state sequence distances. Although
these distance metrics do not satisfy certain mathematical
properties [18], they provide a good domain independent
measure of the difference between any two plans. Since the
goal is to predict the differences in terms of explicability
distance between the robot plans and human expected plans,
the intuition is that they can be approximated using a
combination of plan distance measures that capture different
aspects of plans.
1) Action Distance: We denote the set of unique actions
in a plan œÄ as A(œÄ) = {a | a ‚àà œÄ}. Given the action sets
A(œÄ R ) and A(œÄ H ) of two plans œÄ R and œÄ H respectively, the
action distance, Œ¥a , is computed as the ratio of the actions
that are exclusive to each plan to all the actions in the plans
[4]. It is written as:
Œ¥A (œÄ R , œÄ H ) = 1 ‚àí

|A(œÄ R ) ‚à© A(œÄ H )|
|A(œÄ R ) ‚à™ A(œÄ H )|

(1)

This simply means that two plans are similar (and hence
their distance measure is smaller) if they contain similar
actions. Note that this measure does not take the ordering
of actions into account.

|Cl(œÄ R ) ‚à© Cl(œÄ H )|
|Cl(œÄ R ) ‚à™ Cl(œÄ H )|

(2)

Again, plans are similar, with lower similarity scores, if
they have a large number of overlapping causal links.
3) State Sequence Distance: This distance measure, as
the name suggests, takes the sequences of the states into
consideration. This distance captures the context of an action
in a given plan. The length of the sequences may differ
and therefore there are multiple ways to define this distance
measure [4]. We use the representation shown in Eq. 3. Given
H
H
R
two state sequences (sR
0 , . . . , sn ) and (s0 , . . . , sn0 ) for œÄR
0
and œÄH respectively, where n ‚â• n are the lengths of the
plans, the state sequence distance is written as:
1
Œ¥S (œÄ , œÄ ) =
n
R

H

"

0

n
X

#
H
‚àÜ(sR
k , sk )

+n‚àín

0

(3)

k=1
|sR ‚à©sH |

H
k
k
where ‚àÜ(sR
k , sk ) = 1 ‚àí |sR ‚à™sH | represents the distance
k
k
between two states (where sR
k is overloaded to denote the set
of predicate variables in state sR
k ). The first term measures
the normalized difference between states up to the end of
the shorted plan, while the second term, in the absence of a
state to compare to, assigns maximum difference possible.
Here we illustrate the explicability distance with an example and discuss its relationship with the other distance
measures. Consider the grid structure shown in Figure 2.
Here we have a 5 by 5 grid. The bottom left cell is labeled
as (1, 1). Some of the cells have glass floors while some
others have obstacles. The robot has to find its way across
the obstacles from the start cell to the goal cell. Looking at
the grid structure, the human may expect the robot to take
an optimal path highlighted by the green arrows. Although
unbeknownst to the human, the robot has difficulty traveling
across the glass floor cells because of the reflective surface
and has to use special sensors while navigating across these
floors. Hence, the cost of treading on these glass floors
is higher than the cost of treading across normal cell. In
this case, the robot‚Äôs optimal plan to the goal is the one
highlighted in red, which doesn‚Äôt coincide with human‚Äôs
expectation of the robot plan.
In Figure 3, we provide the actions sets, causal link sets
and state sequences generated for both the robot plan and
human expected plan for our example illustrated in Figure 2.
The corresponding plan distances are shown in Table I. These
three distances capture different aspects of the plans. In this
case, the explicability distance clearly has a high correlation
with these other distance measures. Our goal in this paper

Initial State: at(1, 1)
Goal State: at(5, 4)
Actions:
A(œÄ R )
=
{ move-diagonal(2, 2), move-up(2, 3), move-up(2, 4), move-diagonal(3, 5),
move-diagonal(4, 4), move-right(5, 4) }
A(œÄ H ) = {move-diagonal(2, 2), move-diagonal(3, 3), move-diagonal(4, 4), move-right(5,
4) }
Causal Links:
Cl(œÄ R ) = { <move-diagonal(2, 2), at(2, 2), move-up(2, 3)>, <move-up(2, 3), at(2, 3),
move-up(2, 4)>, <move-up(2, 4), at(2, 4), move-diagonal(3, 5)>, <move-diagonal(3,
5), at(3, 5), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
Cl(œÄ H ) =
{ <move-diagonal(2, 2), at(2, 2), move-diagonal(3, 3)>, <move-diagonal(3,
3), at(3, 3), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
State sequences:
S(œÄ R ) = { {at(2, 2)}, {at(2, 3)}, {at(2, 4)}, {at(3, 5)}, {at(4, 4)} }
S(œÄ H ) = { {at(2, 2)}, {at(3, 3)}, {at(4, 4)} }
Fig. 3: The action sets, causal link sets and state sequence sets for the illustrated example in Figure 2. For the given initial
and goal state, the plans illustrated in red and green in Figure 2 are used to produce respective action, causal link and state
sequence sets.
TABLE I: Action distance, causal link distance and state
sequence distance computed using the sets provided in Figure
3, between the two plans, œÄ R and œÄ H , that are illustrated in
Figure 2.

In order to train our regression model, we use plan traces
whose actions were assigned scores by human subjects. We
can then calculate the explicability score of a plan based on
the average of the individual action scores.

Plan Pair

Œ¥A

Œ¥C

Œ¥S

B. Plan Generation

(œÄ R , œÄ H )

4/7

6/7

4/5

We now present the details of our plan generation phase,
where we use the explicability distance to guide our search to
generate the most explicable robot plan for a given problem.
1) Non-Monotonicity: We will now discuss the nonmonotonic behavior exhibited by explicability function and
how it affects the plan generation process. The explicability
distance function is non-monotonic in nature, meaning, as
the partial plan grows, the explicability distance may both
increase or decrease. This is because, a new action can either
contribute positively or negatively to the total explicability
score of the plan. As pointed out earlier, the explicability
score is computed as an average of the individual action
scores in the context of the plan prefix.
Observation 1: Explicability score of a partial plan P may
increase, stay equal, or even decrease when it is extended
with one or more actions.
Consider the following example, in a car domain, the goal
of the car is to move to the left lane. The car squeezes
leftwards in three consecutive actions and after coming to
the left lane, it turns on its left indicator. Here the turning on
of the left tail light after having moved left is an inexplicable
action. The previous three actions were explicable to the
human drivers and contribute positively to the explicability

is, then, to learn to establish a general relationship between
the established measures of plan distance.
IV. PROPOSED METHODOLOGY
A. Explicability Distance
Since, without the model we do not know which plan
distance is most relevant in capturing explicability, we
present a general formulation in this section. A more
detailed formulation can be found in the following section. Let ‚àÜ be a 3-dimensional vector, such that for a
robot plan, œÄ R , derived from MR (R), and for an explicable plan œÄ H , derived from MH (R), we have ‚àÜ =
hŒ¥A (œÄ R , œÄ H ), Œ¥C (œÄ R , œÄ H ), Œ¥S (œÄ R , œÄ H )iT . We now define
explicability distance of a robot plan, Exp(œÄ R ), as a regression based function of the three plan distances, with b
as the parameter vector:
Exp(œÄ R / œÄ H ) ‚âà f (‚àÜ, b)

(4)

score of the plan but the last action has a negative impact
and decreases the score. Therefore this score and in turn
the explicability distance is not a non-decreasing function.
In essence, depending on the context, the explicability of an
action can either improve the score or worsen it.
Observation 2: A greedy method that expands a node
with the highest explicability score of the corresponding
partial plan at each step does not guarantee to find an
optimal explicable plan (one of the plans with the highest
explicability score) as its first solution.
The above observation is easy to see since, if e1 is
explicability score of the first plan, then a node may exist
in open list (set of unexpanded nodes) whose explicability
score is less than e1 , which when expanded may result in a
solution plan with explicability score higher than e1 .
2) Reconciliation Search: Given the non-monotonic nature of explicability distance function, we have to generate
all the candidate plans in order to find the most explicable
plan. Here, we present a cost-bounded anytime greedy search
algorithm called reconciliation search that generates all the
valid loopless candidate solution plans up to a given cost
bound, and then progressively searches for plans with better
explicability scores. The value of the heuristic h(v) in a
particular state v encountered during search is based entirely
on the explicability distance of the robot plan prefix up to
that state, given by,
h(v) = Exp(œÄ / œÄh )
s.t. ŒìMR (R) (I, œÄ) = v

Algorithm 1 Reconciliation Search
Input: Planning problem P = hMR (R), I, Gi, cost bound
max cost, and explicability distance function Exp
Output: Robot plan with the highest explicability score
œÄ R = arg maxœÄR Exp(œÄ R / œÄH )
1: S ‚Üê ‚àÖ
. Candidate plan solution set
2: open ‚Üê ‚àÖ
. Open list
3: closed ‚Üê ‚àÖ
. Closed list
4: open.insert(I, 0, inf)
5: while open 6= ‚àÖ do
6:
n ‚Üê open.remove()
. Node with highest h(¬∑)
7:
if n |= G then
8:
S.insert(œÄ s.t. ŒìMR (R) (I, œÄ) |= v)
9:
end if
10:
closed.insert(n)
11:
for each v ‚àà successors(n) do
12:
if v ‚àà
/ closed then
13:
if g(n) + cost(n, v) ‚â§ max-cost then
14:
open.insert(v, h(v))
15:
end if
16:
else
17:
if h(n) < h(v) then
18:
closed.remove(v)
19:
open.insert(v, h(v))
20:
end if
21:
end if
22:
end for
23: end while
24: return arg maxœÄ R ‚ààS Exp(œÄ R / œÄH )

and ŒìMH (R) (I, œÄh ) = v
Since we want to find explicable plans which are within
a cost bound, we use the cost of the plan to prune the
nodes in the search graph whenever they exceed the given
maximum cost bound. We implement this search in the
Fast-Downward planner [5]. The approach is described
in detail in Algorithm 1.
At each iteration of the algorithm, the plan prefix of the
robot model is compared with the explicable trace œÄh (these
are the plans generated by the human mental model of the
robot MH (R) up to the current state in the search process)
for the given problem. Using the computed distances, we
predict the explicability score for every candidate robot plan.
The search algorithm then makes a locally optimal choice
of states. After generating the first solution plan we do not
stop the search but instead continue to find all the valid
loopless candidate solution plans within the given cost bound
or until the state space is completely explored. In the end, the
candidate plan with highest explicability score is returned.
V. E XPERIMENTAL A NALYSIS
A. Autonomous Car Simulation Experiment
1) Domain Model: Autonomous cars are a topic of interest from the point of view of explicability problem. In the
recent past, Google‚Äôs self-driving cars [19] have been in the
news for being ‚Äútoo safe‚Äù on the roads. These autonomous
cars governed by strict traffic rules find it hard to blend

in and make judgments that would not make sense in a
predominantly human environment. At four-way stops, these
cars find it difficult to cross the intersection, while the human
drivers keep inching forward. For a robot car, such situations,
where it does not make an explicable decision can pose
problems, and all the human drivers who come into contact
with such cars would have to face the brunt of it.
For these reasons, we focused our studies on a simulated
autonomous car environment, and investigated how the robot
car‚Äôs inexplicable behavior can be avoided by generating
plans with respect to their explicability scores. In our robot
car model (written in PDDL), we try to capture bad driving
etiquette commonly seen on roads, such as, driving below
speed limit in passing lanes, overtaking from the wrong
side, turning and changing lanes without showing signal,
not following the move over law, and so on. The human
mental model of the robot car is defined as per test subjects
assumptions of how the robot car should perform actions.
From the robot model MR (R), we generated 40 plans for
16 different problems. The plans consisted of both explicable
and inexplicable robot car behaviors. These plans were
assessed by 20 test subjects, with each subject evaluating 8
plans. Also, each plan was evaluated by 4 different subjects,
in order to get a general understanding of the assumptions
of different human drivers. Therefore, the overall number of

(a)

(b)

(c)

Fig. 4: Autonomous Car Domain Simulation. Here the red colored car in the images is the robot car and the rest of the cars
are assumed to be human drivers. (a) The cop car is parked on the rightmost lane, and the robot car is following through
the Move Over Law maneuver. (b) The robot car is wrongly trying to overtake from the rightmost lane. (c) The robot car
is waiting at a four-way stop intersection even though it is the turn of the robot car to cross over.

Fig. 5: Here AD, CLD, SD and Score represent action
distance, causal link distance, state sequence distance, and
explicability scores respectively. This is a correlation matrix
for the aforementioned metrics. The red color represents
the negative correlation that exists between the distance
measures and the scores.

training samples was 160. The test subjects were required to
have sufficient real-life driving experience. The assessment
had two parts: one part involved scoring each robot car action
with 1, if explicable, and 0 otherwise (the explicability score
of the overall plan is calculated as the fraction of actions
in the plan that were labeled as explicable); the other part
involved answering a questionnaire aimed at understanding
test subject‚Äôs assumptions regarding the robot car. The information from this questionnaire was used to design the human
mental model MH (R) of the robot car.
The PDDL domain of the robot car, MR (R), consists
of lane and car objects as shown in Figure 4. The red car

is the robot car in the experiments and all other cars seen
in the experiments are assumed to have human drivers.
The car objects are associated with predicates defining the
location of a car on a lane segment, status of left and right
turn lights, status of car being within speed limit, presence
of a parked cop car, and so on. The actions possible
in the domain are with respect to the robot car. These
actions are Accelerate, Decelerate, LeftSqueeze,
RightSqueeze, LeftLightOn, LeftLightOff,
RightLightOn, RightLightOff, SlowDown and
WaitAtStopSign, and so on. In order to change a
lane, three consecutive actions of either LeftSqueeze
or RightSqueeze are required to gradually move to
the other lane. The PDDL domain of the human mental
model, consists of same state predicates, but different action
representations, preconditions, effects and action-costs. Note
that even though representing the human mental model in
PDDL may seem like a strong assumption, we validated the
labels given by the human subjects with the PDDL human
model constructed from the elicited preferences and found
about 72.3% match, which indicates that MH (R) used
in the evaluations is a good approximation of the human
mental model of the robot
2) Defining the Explicability Distance: For the 22 training
problems, explicable plans with MH (R) were generated.
Since some actions are not common to both the domains and
also owing to the difference in the effects and preconditions
of the actions across domains, an explicit mapping was
defined between the actions over the two domains. This
mapping was done in the light of the plan distance operations
performed between plans in the two domains.
The correlation matrix in Figure 5 establishes the negative
correlation of the plan distance measures to the explicability
scores. From the correlation matrix it can be seen that, causal
link distance has significant negative correlation with the
explicability scores. After establishing the negative correlation, we proceed towards training our regression model called
explicability distance.

TABLE II: Parameters of Regression Models
Distance

b

w

Accuracy %
10.14

Œ¥A

0.72

-0.33

Œ¥C

0.73

-0.231

7.06

Œ¥S

0.92

-0.519

27.47

Œ¥A , Œ¥ C , Œ¥ S

0.93

0.207,-0.061,-0.626

28.02

sR
1 = b1 + w1 Œ¥A

(5)

sR
2 = b2 + w2 Œ¥C

(6)

sR
3 = b3 + w3 Œ¥S

(7)

sR
4 = b4 + w4 Œ¥A + w5 Œ¥C + w6 Œ¥S

(8)

At first, individual distances were used to fit the data in the
regression model. This resulted in a poorly learned regression
model. A linear combination of the three distances also
resulted in poor results. For regression model functions 5, 6,
7 and 8, the bias, weight and accuracy values were as shown
in Table II. From this table, we infer that the relationships
are not necessarily linear as we speculated previously. We
improve our model using Random Forest regression. Since
random forests allow selection of random subset of features
while splitting the decision node, the accuracy of our model
improves. All the three distances have statistically significant
contribution in the fitted model. We evaluate the goodness of
the fit of the model, using the coefficient of determination or
R2 . This value determines the measure by which the fitted
model can explain the variations in the target values. This
value lies between 0 to 1. Higher the R2 value, better is
the model fitted to the data. After training process the new
regression model was found to have 0.8721 R2 value. That is
to say, 87% of the variations in the features can be explained
by our model. Our model predicts the explicability distance
between the robot plans and human mental model plans, with
a high accuracy. We call this plan distance regression model
as the explicability distance.
3) Evaluation: For evaluation of our system, we tested it
on 13 different problems. We ran the algorithm with a high
cost bound, in order to cover the most explicable candidate
plans for all the problems. The results of this search process
are as shown in Figure 6, 7 and 8. From these results, we
can see that the reconciliation search is able to incrementally
develop plans with better explicability scores as shown in
Figure 6. In Figure 7, we see that for all the 13 problems
the explicability score of the optimal plans is lesser than the
final plans generated by reconciliation search. From Figure
8, we see that for the first six problems the optimal and
explicable plans have same cost but our modified planner
with reconciliation search produces explicable plan versions
for those problems. The results also clearly show that the
explicable plans can be costlier than plans that are optimal
with respect to the robot‚Äôs own model. This additional cost

Fig. 6: The graph shows that the search process finds plans
with incrementally better explicable scores. Each color line
represents the 13 different problems. The markers on the
lines represent a plan solution for that problem. The y-axis
gives the explicability scores of the plans and the x-axis gives
the solution number. Note that the curves show the nonmonotonic nature of evaluation metric in the search process.
The final output of the algorithm is, of course, the best plan
found in the search process.

Fig. 7: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their explicability scores.

Fig. 8: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their plan costs.

can be seen as the price the robot pays to make its behavior
explicable to the human.
VI. C ONCLUSION
We showed how the plan distance measures play a role in
determining the explicability of a robot plan. We evaluated
our hypothesis in the simulated Autonomous Car PDDL
domain. We generated training samples in robot‚Äôs domain
and assigned them with human scores. We also generated
plans in the human‚Äôs model to find the distances between
plans in two domains. We looked at the relationships between
scores and the distance measures of the plans. We learned
the regression model that could best capture the explicability
of the training samples. In summary, we have proved our
hypothesis that using the human‚Äôs mental model of the robot
model we can assess the explicability of a robot plan as a
function over the plan distance measures between the robot
plan and the plan that the human would expect the robot to
make. We also showed that the explicability distance measure
can be used to bias the robots planning process to generate
plans that are more in concordance with what humans expect.
We are currently in the process of incorporating this theory
into the behavior of a Fetch robot involved in delivery
tasks, to demonstrate how it improves the explicability of
the robot‚Äôs behavior.
R EFERENCES
[1] E. de Visser and R. Parasuraman, ‚ÄúAdaptive aiding of human-robot
teaming effects of imperfect automation on performance, trust, and
workload,‚Äù Journal of Cognitive Engineering and Decision Making,
vol. 5, no. 2, pp. 209‚Äì231, 2011.
[2] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram,
M. Veloso, D. Weld, and D. Wilkins, ‚ÄúPddl-the planning domain
definition language,‚Äù 1998.
[3] B. Srivastava, T. A. Nguyen, A. Gerevini, S. Kambhampati, M. B. Do,
and I. Serina, ‚ÄúDomain independent approaches for finding diverse
plans.‚Äù in IJCAI, 2007, pp. 2016‚Äì2022.

[4] T. A. Nguyen, M. Do, A. E. Gerevini, I. Serina, B. Srivastava,
and S. Kambhampati, ‚ÄúGenerating diverse plans to handle unknown
and partially known user preferences,‚Äù Artificial Intelligence,
vol. 190, no. 0, pp. 1 ‚Äì 31, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0004370212000707
[5] M.
Helmert,
‚ÄúThe
fast
downward
planning
system,‚Äù
CoRR,
vol.
abs/1109.6051,
2011.
[Online].
Available:
http://arxiv.org/abs/1109.6051
[6] H. A. Kautz and J. F. Allen, ‚ÄúGeneralized plan recognition.‚Äù in AAAI,
vol. 86, no. 3237, 1986, p. 5.
[7] M. Ramƒ±rez and H. Geffner, ‚ÄúProbabilistic plan recognition using offthe-shelf classical planners,‚Äù in Proceedings of the Conference of the
Association for the Advancement of Artificial Intelligence (AAAI 2010).
Citeseer, 2010, pp. 1121‚Äì1126.
[8] T. Chakraborti, Y. Zhang, D. E. Smith, and S. Kambhampati, ‚ÄúPlanning
with resource conflicts in human-robot cohabitation,‚Äù in Proceedings
of the 2016 International Conference on Autonomous Agents &
Multiagent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 2016, pp. 1069‚Äì1077.
[9] M. Cirillo, L. Karlsson, and A. Saffiotti, ‚ÄúHuman-aware task planning
for mobile robots,‚Äù in Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1‚Äì7.
[10] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz,
D. Smith, and S. Kambhampati, ‚ÄúPlanning for serendipity,‚Äù in
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2015.
[11] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati, ‚ÄúCoordination in human-robot teams using mental
modeling and plan recognition,‚Äù in Intelligent Robots and Systems
(IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014,
pp. 2957‚Äì2962.
[12] S. J. Levine and B. C. Williams, ‚ÄúConcurrent plan recognition and
execution for human-robot teams.‚Äù in ICAPS, 2014.
[13] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, and S. K.
Hankz Hankui Zhuo, ‚ÄúPlan explainability and predictability for
cobots,‚Äù CoRR, vol. abs/1511.08158, 2015. [Online]. Available:
http://arxiv.org/abs/1511.08158
[14] A. Dragan and S. Srinivasa, ‚ÄúGenerating legible motion,‚Äù in Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.
[15] T. W. Fong, I. Nourbakhsh, and K. Dautenhahn, ‚ÄúA survey of socially
interactive robots,‚Äù Robotics and Autonomous Systems, 2003.
[16] G. Hoffman and C. Breazeal, ‚ÄúCost-based anticipatory action selection
for human‚Äìrobot fluency,‚Äù Robotics, IEEE Transactions on, vol. 23,
no. 5, pp. 952‚Äì961, 2007.
[17] Y. Zhang, S. Sreedharan, and S. Kambhampati, ‚ÄúCapability models
and their applications in planning,‚Äù in Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems.
International Foundation for Autonomous Agents and Multiagent
Systems, 2015, pp. 1151‚Äì1159.
[18] R. P. Goldman and U. Kuter, ‚ÄúMeasuring plan diversity: Pathologies
in existing approaches and a new plan distance metric.‚Äù 2015.
[19] M. Richtel and C. Dougherty, ‚ÄúGoogle‚Äôs driverless cars run into
problem: Cars with drivers,‚Äù The New York Times, vol. 9, p. 1, 2015.

Plan Explicability and Predictability for Robot Task Planning
Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui Zhuo and Subbarao Kambhampati
Abstract--Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave unexpectedly. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in "natural ways", and work that investigated legible motion planning, there lacks a general solution for high level task planning. To address this issue, we introduce the notions of plan explicability and predictability. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with human subjects using physical robots to show the effectiveness of our approach.

I. I NTRODUCTION Intelligent robots and machines are becoming pervasive in human populated environments. Examples include robots for education, entertainment and personal assistance just to name a few. Significant research efforts have been invested to build autonomous agents to make them more helpful. These agents respond to goal specifications instead of basic motor commands, which requires them to autonomously synthesize task plans and execute those plans to achieve the goals. However, if the behaviors of these agents are incomprehensible, it can increase the cognitive load of humans and potentially introduce safety risks to them. As a result, one important requirement for such intelligent agents is to ensure that the synthesized plans are comprehensible to humans. This means that instead of considering only the planning model of the agent, plan synthesis should also consider the interpretation of the agent behavior from the human's perspective. This interpretation is related to our modeling of other agents. More specifically, we tend to have expectations of others' behaviors based on our understanding (modeling) of their capabilities, mental states and etc. If their behaviors do not match with these expectations, we would often be confused. One of the major reasons of this confusion is due to the fact that our understanding of others' models is often partial and inaccurate. This is also true when humans

interact with intelligent agents. For example, to darken a room that is too bright, a robot can either adjust the window blinds, switch off the lights, or break the light bulbs in the room. While breaking the light bulbs may well be the least costly plan to the robot under certain conditions (e.g., when the robot cannot easily move in the environment but we are unaware of it), it is clear that the other two options are far more desirable in the context of robots cohabiting with humans. One of the challenges here is that the human's understanding of the agent model is inherently hidden. Thus, its interpretation from the human's perspective can be arbitrarily different from the agent's own model. While there exists previous work that studied socially acceptable robots [11, 12, 21, 18] that interact with humans in "natural ways", and work that investigated legible motion planning [6], there lacks a general solution for high level task planning. In this paper, we introduce the notions of plan explicability and predictability which are used by autonomous agents (e.g., robots) to synthesize "explicable plans" that can be easily understood by humans. Our problem settings are as follows: an intelligent agent is given a goal by a human (so that the human knows the goal of the agent) working in the same environment and it needs to synthesize a plan to achieve the goal. As suggested in psychological studies [24, 5], we assume that humans naturally interpret a plan as achieving abstract tasks (or subgoals), which are functional interpretations of agent action sequences in the plan. For example, a robot that executes a sequence of manipulation actions may be interpreted as achieving the task of "picking up cup". Based on this assumption, intuitively, the easier it is for humans to associate tasks with actions in a plan, the more explicable the plan is. Similarly, the easier it is to predict the next task given actions in the previous tasks, the more predictable the plan is. In this regard, explicability is concerned with the association between human-interpreted tasks and agent actions, while predictability is concerned with the connections between these abstract tasks. Since the association between tasks and agent actions can be considered as a labeling process, we learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). We then use the learned model to label a new plan to compute its explicability and predictability. These measures are used by agents to proactively choose or directly synthesize plans that are more explicable and predictable without affecting the quality much. Our learning approach does not assume any prior knowledge

arXiv:1511.08158v2 [cs.AI] 12 Apr 2016

Fig. 1. From left to right, the scenarios illustrate the differences between automated task planning, human-aware planning and explicable planning (this work). In human-aware planning, the robot needs to maintain a model of the human (i.e., MH ) which captures the human's capabilities, intents and etc. In explicable planning, the robot considers the differences between its model from the human's perspective (i.e., M R ) and its own model MR .

on the human's interpretation of the agent model. We provide evaluation on a synthetic domain in simulation and with human subjects using physical robots to demonstrate the effectiveness of our approach. II. R ELATED W ORK To build autonomous agents (e.g., robots), one desirable capability is for such agents to respond to goal-oriented commands via automated task planning. A planning capability allows agents to autonomously synthesize plans to achieve a goal given the agent model (MR as shown in the first scenario in Fig. 1) instead of following low level motion commands, thus significantly reducing the human's cognitive load. Furthermore, to work alongside of humans, these agents must be "human-aware" when synthesizing plans. In prior works, this issue is addressed under human-aware planning [22, 4, 2] in which agents take the human's activities and intents into account when constructing their plans. This corresponds to human modeling in human-aware planning as shown in the second scenario in Fig. 1. A prerequisite for human-aware planning is a plan recognition component, which is used to infer the human's goals and plans. This information is then used to avoid interference, and plan for serendipity and teaming with humans. There exists a rich literature on plan recognition [14, 3, 20, 16], and many recent works use these techniques in human-aware planning and human-robot teaming [23, 2, 25]. While our work on plan explicability and predictability falls within the scope of human-in-the-loop planning (which also includes human-aware planning), it differs significantly from the previous work. This is illustrated in Fig. 1. More specifically, in human-aware planning, the challenge is to obtain the human model (MH in Fig. 1) which captures human capabilities [26], intents [23, 2] and etc. The modeling in this work is one level deeper: it is about the interpretation of the agent model from the human's perspective (M R in Fig. 1). In other words, R needs to understand the model of itself in H 's eyes. This information is inherently hidden, difficult to

convey, and can be arbitrarily different (e.g., having different representations) from R's own model (MR in Fig. 1). There exists work on generating legible robot motions [6] which considers a similar issue in motion planning. We are, on the other hand, concerned with task planning. Note that two different task plans may map to exactly the same motions which can be interpreted vastly differently by humans. In such cases, considering only motion becomes insufficient. Nevertheless, there exists similarities between [6] and our work. For example, legibility there is analogous to predictability in ours. In the human-robot interaction (HRI) community, there exists prior works that discuss how to enable natural and fluent human-robot interaction [11, 12, 21, 18] to create more socially acceptable robots [7]. These works, however, apply only to behaviors in specific domains. Compared with model learning via expert teaching, such as inverse reinforcement learning [1] and tutoring systems [17], which is about learning the "right" model from teachers, our work, on the other hand, is concerned with learning model differences. Furthermore, as an extension to our work, when robots cannot find an explicable plan that is also cost efficient, they need to explain the situation. In this regard, our work is also related to excuse [9] and explanation generation [10]. Finally, while our learning approach appears to be similar to information extraction [19], we use the learned model to proactively guide planning instead of passively extracting information. III. E XPLICABILITY AND P REDICTABILITY In our settings, an agent R needs to achieve a goal given by a human in the same environment (so that the human knows about the goal of the robot). The agent has a model of itself (referred to as MR ) which is used to autonomously construct plans to achieve the goal. In this paper, we assume that this model is based on PDDL [8], a general planning domain definition language. As we discussed, for an agent to generate explicable and predictable plans, it must not only consider MR but also M R , which is the interpretation of MR from the human's perspective. A. Problem Formulation Keeping the problem settings in mind, given a domain, the problem is to find a plan for a given goal that satisfies the following: argmin cost(MR ) +  ∑ dist(MR , M ) R
 MR

(1)

where MR is a plan that is constructed using MR (i.e., the agent's plan), M is a plan that is constructed using M R R (i.e., the human's anticipation of the agent's plan), cost returns the cost of a plan, dist returns the distance (i.e., capturing the differences) between two plans, and  is the relative weight. The goal of Eq. (1) is to find a plan that minimizes a weighted sum of the cost of the agent plan and the differences between the two plans. Since the agent model MR is assumed to be given, the challenge lies in the second part in Eq. (1). Note that if we know M R or it can be learned, the only thing left would be to search for a proper dist function.

However, as discussed previously, M R is inherently hidden, difficult to convey, and can be arbitrarily different from MR . Hence, our solution is to use a learning method to directly approximate the returned values. We postulate that humans understand agent plans by associating abstract tasks with actions, which can be considered as a labeling process. Based on this, we assume that dist(MR , M ) can be functionally R decomposed as:

2) Predictability Labeling: Predictability is concerned with the connections between tasks in a plan. An action label for predictability is composed of two parts: a current label and a next label (i.e., L ◊ L). The current label is also the action label for explicability. The next label (similar to the current label) is used to specify the tasks that are anticipated to be achieved next. A next label with multiple task labels is interpreted as having multiple candidate tasks to achieve next; when this label is the empty set, it is interpreted as that the next task is dist(MR , M ) = F  L (MR ) (2) unpredictable, or there are no more tasks to be achieved. R Definition 2 (Plan Predictability): Given a domain, the where F is a domain specific function that takes plan labels as predictability  of a plan  is computed by a mapping,  input, and L is the labeling scheme of the human for agent F : L2   [0, 1] (with 1 being the most predictable). plans based on M R . As a result, Eq. (1) now becomes: L2  denotes the sequence of action labels for predictability. An  i example of F is given below which is used in our evaluation argmin cost(MR ) +  ∑ F  L CRF (MR |{Si |Si = L (MR )})  MR when assuming that the current and next labels are associated (3) with at most one task label: where {Si } is the set of training examples and L CRF is i[0,N ] 1|L(ai )|=1  (1L2 (ai )=L(aj )  1L2 (ai:N )= ) the learned model of L . We can now formally define plan F (L2 ) = N +1 explicability and predictability in our context. Given a plan of (8) agent R as a sequence of actions, we denote it as MR and where a (j > i) is the first action that has a different j simplified below as  for clarity: current label as ai or the last action in the plan if no such 2  = a0 , a1 , a2 , ...aN (4) action is unfound, L (ai ) returns the next label of ai and 1L2 (ai:N )= returns 1 only if the next labels for all actions after where a0 is a null action that denotes plan starting. Given the ai (including ai ) are . Eq. (8) computes the ratio between domain, we assume that a set of task labels T is provided to number of actions that we have correctly predicted the next task and the number of all actions. label agent actions: T = {T1 , T2 , ...TM } (5) B. A Concrete Example Before discussing how to learn the labeling scheme of the human from training examples, we provide a concrete example to connect the previous concepts and show how training examples can be obtained. In this example, there is a rover in a grid environment working with a human. An illustration of this example is presented in Fig. 2. There are resources to be collected which are represented as boxes. There is one storage area that can store one resource which is represented as an open box. The rover can also make observations. The rover actions include {navigate lf rom lto }, {observe l} {load l}, and {unload l}, each representing a set of actions since l (i.e., representing a location) can be instantiated to different locations (i.e., 0 - 8 in Fig. 2). navigate (or nav ) can move the rover from a location to one of its adjacent locations; load can be used to pick up a resource when the rover is not already loaded; unload can be used to unload a resource at a storage area if the area is empty; observe (or obs) can be used to make an observation. Once a location is observed, it remains observed. The goal in this example is for the rover to make the storage area non-empty and observe two locations that contain the eye symbol in Fig. 2. In this domain, we assume that there are three abstract tasks that may be used by the human to interpret the rover's plans: COLLECT (C), STORE (S) and OBSERVE (O). Note that we do not specify any arguments for these tasks (e.g., which resource the rover is collecting) since this information may not be important to the human. This also illustrates that MR and

1) Explicability Labeling: Explicability is concerned with the association between abstract tasks and agent actions; each action in a plan is associated with an action label. The set of action labels for explicability is the power set of the task labels: L = 2T (6) When an action label includes multiple task labels, the action is interpreted as contributing to multiple tasks; when an action label is the empty set, the action is interpreted as inexplicable. When a plan is labeled, we can compute its explicability measure based on its action labels in a domain specific way. More specifically, we define: Definition 1 (Plan explicability): Given a domain, the explicability  of an agent plan  is computed by a mapping, F : L  [0, 1] (with 1 being the most explicable). L above denotes the sequence of action labels for  . An example of F used in our evaluation is given below: F (L ) =
i[1,N ]

1L(ai )=

N

(7)

where N is the plan length, L(ai ) returns the action label of ai , and 1f ormula is an indicator function that returns 1 when the f ormula holds or 0 otherwise. Eq. (7) basically computes the ratio between the number of actions with non-empty action labels and the number of all actions.

fields (CRFs) [15] due to their abilities to model sequential data. An alternative would be HMMs; however, CRFs have been shown to relax assumptions about the input and output sequence distributions and hence are more flexible. The distributions that are captured by CRFs have the following form: p(x, y) = 1 A (xA , yA ) Z (9)

in which Z is a normalization factor that satisfies:
Fig. 2. Example for plan explicability and predictability with action labels (on the right) for a given plan in the rover domain.

Z=
x,y

A (xA , yA )

(10)

M R can be arbitrarily different. In Fig. 2, we present a plan of the rover as connected arrows starting from the its initial location. Human Interpretation as Training Examples: Let us now discuss how humans may interpret this plan (i.e., associating labels with actions) as the actions are observed incrementally: when labeling ai , we only have access to the plan prefix a0 , ..., ai . At the beginning for labeling a0 , the observation is that the rover starts at l5 . Given the environment and knowledge of the rover's goal, we may infer that the first task should be COLLECT (the resource from l4 ). Hence, we may choose to label a0 as ({START}, {C}). The first action of the rover (i.e., nav l5 l4 ) seems to match with our prediction. Furthermore, given that the storage area is closest to the rover's location after completing COLLECT, the next task is likely to be STORE. Hence, we may label a1 as ({C}, {S}) as shown in the figure. The second action (i.e., load l4 ) also matches with our expectation. Hence, we label a2 too as ({C}, {S}). The third action, nav l4 l1 , however, is unexpected since we predicted STORE in the previous steps. Nevertheless, we can still explain it as contributing to OBSERVE (at location l0 ). Hence, we may label this navigation action (a3 ) as ({O}, {S}). For the fourth action, the rover moves back to l4 , which is inexplicable since the rover's behavior seems to be oscillating without particular reasons. Hence, we may choose to label this action as (, ). The labeling for the rest of the plan continues in a similar manner. This thought process reflects how training examples can be obtained from human labelers. IV. L EARNING A PPROACH To compute  and  from Defs. (1) and (2) for a given plan  , the challenge is to provide a label for each action. This requires us to learn the labeling scheme of humans (i.e., L in Eq. (2)) from training examples and then apply the learned model to  (i.e., L CRF in Eq. (3)). To formulate a learning method, we consider the sequence of labels as hidden variables. The plan that is executed by the agent (which also captures the state trajectory), as well as any cognitive cues that may be obtained (e.g., from sensing) during the plan execution constitute the observations. The graphical model that we choose for our learning approach is conditional random

In the equations above, x represents the sequence of observations, y represents the sequence of hidden variables, and (xA , yA ) represents a factor that is related to a subgraph in the CRF model associated with variables xA and yA . In our context, x are the observations made during the execution of a plan; y are the action labels. Each factor is associated with a set of features that can be extracted during the plan execution. Next, we discuss some possible features that can be used for plan explicability and predictability. A. Features for Learning Given an agent plan, the immediate set of features that we have access to is the plan and its associated state trajectory. Note that the human may not be required (nor it is necessary) to fully understand this information. When the dynamics of the agent are known, given the plan, it may also be possible to derive low level motor commands that implement the motions, which can be used to extract motion related features. When the agent is equipped with sensors such as cameras and lasers, we can also extract features from sensor information. For example, from video streams and depth information, we can extract features about the environment, e.g., how crowded the workspace is. Sensor information can also be used to extract dynamic features such as the location of the human. However, note that this information will not be available during the testing phase, and thus these features need to be estimated based on other information (e.g., projected plan of the human based on plan recognition techniques [20, 16]). In this work, we use a linear chain CRF. However, our formulation is easily extensible to more general types of CRFs. Given an agent plan  = a0 , a1 , a2 , ... , each action is associated with a set of features. Hence, each training example is of the following form:
2 2 (F0 , L2 0 ), (F1 , L1 ), (F2 , L2 ), ...

(11)

where L2 i is the action label for predictability (and explicability) for ai . Fi is the set of features for ai . We discuss several feature categories in more detail below:

1) Plan Features: Given the agent model (specified in PDDL), the set of plan features for ai includes the action description and the state variables after executing the sequence of actions a0 , ..., ai from the initial state. This information can be easily extracted given the model. For example, in our rover example in Fig. 2, this set of features for a1 includes navigate, at rover l4 , at resource0 l2 , at resource1 l4 , at storage0 l3 . 2) Action Features: Action features for ai describes the motion (e.g., dynamics) of this action. These features can be used to capture, for example, smoothness of execution within and across actions. Action features sometimes serve as important cognitive cues for humans to understand agent actions. For example, an action that enables a robot to cross a river may be interpreted as swimming, pedaling, or propelling depending on how the robot motion looks like. Action features can be extracted for a plan given the dynamics of the robot. 3) Interaction Features: Interaction features are intended to capture ai 's influence on the human. For example, it can include how far the agent is from human and what the human is performing when ai is being executed. In other words, this set of features captures characteristics of the interactions between the human and agent. Interaction features can be extracted from sensor information or estimated based on the projected human plan. B. Using the Learned Model Given a set of training examples in the form of Eq. (11), we can train the CRF model to learn the labeling scheme in Eq. (3). We discuss two ways to use the learned CRF model. 1) Plan Selection: The most straightforward method is to perform plan selection on a set of candidate plans which can simply be a set of plans that are within a certain cost bound of the optimal plan. Candidate plans can also be generated to be diverse with respect to various plan distances. For each plan, the agent must first extract the features of the actions as we discussed earlier. It then uses the trained model (i.e., L CRF ) to produce the labels for the actions in the plan.  and  can then be computed given the mappings in Defs. (1) and (2). These measures can then be used to choose a plan that is more explicable and predictable. 2) Plan Synthesis: A more efficient way is to incorporate these measures as heuristics into the planning process. Here, we consider the FastForward (FF) planner with enforced hill climbing [13]. To compute the heuristic value given a planning state, we use the relaxed planning graph to construct the remaining planning steps. However, since relaxed planning does not ensure a valid plan, we can only use action descriptions as plan features for actions that are beyond the current planning state when estimating the  and  measures. These estimates are then combined with the relaxed planning heuristic (which only considers plan cost) to guide the search. The algorithm for generating explicable and predictable plans is presented in Alg 1. The capability to synthesize explicable and predictable plans is useful for autonomous agents. For example, in domains

Algorithm 1 Synthesizing Explicable and Predictable Plans Input: agent model MR , trained human labeling scheme L CRF , initial state I and goal state G. Output: EXP 1: Push I into the open set O . 2: while open set is not empty do 3: s = GetNext(O). 4: h = M AX . 5: if G is reached then 6: return s.plan (i.e., the plan that leads to s from I ). 7: end if 8: Compute all possible next states N from s. 9: for n  N do 10: Compute the relaxed plan RELAX for n. 11: Concatenate s.plan (with plan features) with RELAX (with only action descriptions) as  Ø. 12: Compute and add other relevant features.  13: Compute L2  ).  = LCRF (Ø Ø. 14: Compute  and  based on L2  for  15: Compute h = f (, , hcost ) (f is a combination function; hcost is the relaxed planning heuristic). 16: end for 17: Find the state n  N with the minimum h. 18: if h(n ) < h then 19: Clear O. 20: Push n into O. 21: else 22: Push all n  N into O. 23: end if 24: end while

where humans interact closely with robots (e.g., in an assembly warehouse), more preferences should be given to plans that are more explicable and predictable since there would be high risks if the robots act unexpectedly. One note is that the relative weights of explicability and predictability may vary in different domains. For example, in domains where robots do not engage in close interactions with humans, predictability may not matter much. V. E VALUATION We first evaluate our approach systematically on a synthetic dataset based on the rover domain. Then, we evaluate it with human subjects using physical robots to validate that the synthesized plans are more explicable to humans in a blocks world domain. A. Systematic Evaluation with a Synthetic Domain The aim is twofold here: evaluate how well the learning approach can capture an arbitrary labeling scheme; evaluate the effectiveness of plan selection and synthesis with respect to the  and  measures. 1) Dataset Synthesis: To simplify the data synthesis process, we make the following assumptions: all rover actions have the same cost; all rover actions are associated with at

most one task label (i.e., L = T {} in Eq. (6)). To construct a domain in which the optimal plan (in terms of cost) may not be the most explicable (in order to differ MR from M R ), we add "oscillations" to the plans of the rover. These oscillations are incorporated by randomly adding locations for the rover to visit as hidden goals. For these locations, the rover only needs to visit them. As a result, it may demonstrate "unexpected" behaviors given only the public goal, denoted by G, which is known to both the rover and human. We denote the goal that also includes the hidden goals as G . Given a problem with a public goal G, we implement a labeling scheme to automatically provide the "ground truth" of a rover plan, which is constructed by the rover to achieve G . Given a plan of the rover, we label it incrementally by associating each action with a current and next label. These labels are chosen from {{COLLECT}, {STORE}, {OBSERVE}, }. We denote the plan prefix a0 , ...ai for a plan  as i , the state after applying i as si from the initial state, and a plan that is constructed from si to achieve G (i.e., using si as the initial state) as P (si ). For the current label of ai : 1) If |P (si )|  |P (si-1 )|, we label ai as  (i.e., inexplicable). This rule means that humans may label an action as inexplicable if it does not contribute to achieving G. 2) If |P (si )| < |P (si-1 )|, we label ai based on the distances from the current rover location to the targets (i.e., storage areas or observation locations), current state of the rover (i.e., loaded or not), and whether ai moves the rover closer to these targets. For example, if the closest target is a storage area and the rover is loaded, we label ai as {STORE}. When there are ties, we label ai as  (i.e., unclear and hence interpreted as inexplicable). For the next label of ai : 1) This label is determined by the target that is closest to the rover state after the current task is achieved. When there are ties, ai is labeled as  (i.e., unclear and hence interpreted as unpredictable). If the current label is , we also label ai as  (i.e., unpredictable). 2) If the current task is also the last task, we label ai as  since there is no next task. For evaluation, we define F and F as in Eqs. (7) and (8). We randomly generate problems in a 4 ◊ 4 environment. For each problem, we randomly generate 1 - 3 resources as a set RE, 1 - 3 storage areas as a set ST, 1 - 3 observation locations as a set OB. The public goal G of a problem, first, includes making all storage areas non-empty. To ensure a solution, we force |RE | = |ST | if |RE | < |ST |. Furthermore, the rover must make observations at the locations in OB. G for the rover includes G above, as well as a set of hidden goals. Locations of the rover, RE, ST, OB and hidden goals are randomly generated in the environment and do not overlap in the initial state. Although seemingly simple, the state space of this domain is on the order of 1020 . 2) Results: We use only plan features here. First, we evaluate our approach to learning the labeling scheme (i.e.,  L CRF ) as the difference between MR and MR gradually

Fig. 3. Evaluation for predicting  and  measures as the difference between MR and M R increases (i.e., as the maximum number of hidden goals increases).

increases (i.e., as the number of hidden goals increases). Afterwards, we evaluate the effectiveness of plan selection and synthesis with respect to the  and  measures. To verify that our approach can generalize to different problem settings, we fix the level of oscillation when generating training samples while allowing it to vary in testing samples. Using CRFs for Plan Explicability and Predictability: In this evaluation, we randomly generate 1 - 3 hidden goals to include in G in 1000 training samples. After the model is learned, we evaluate it on 100 testing samples in which we vary the maximum number of hidden goals from 1 to 6 with step size 1. The result is presented in Fig. 3. We can see that the prediction performance (i.e., the ratios between  and   computed based on L CRF and L ) is generally between 50% - 150%, We can also see that the oscillation level does not seem to influence the prediction performance much. This shows that our approach is effective whether MR and M R are similar or largely different. Selecting Explicable and Predictable Plans: We evaluate plan selection using  and  measures and compare the selected plans (denoted by EXPD-SELECT) with plans selected by a baseline approach (denoted by RAND-SELECT). Given a set of candidate plans, EXPD-SELECT selects a plan according to the highest predicted explicability or predictability measure while RAND-SELECT randomly selects a plan from the set of candidate plans. To implement this, for a given public goal G, we randomly construct 20 problems with a given level of oscillation as determined by the maximum number of hidden goals. Each such problem corresponds to a different G and a plan is created for it. The set of plans for these 20 problems associated with the same G is the set of candidate plans for G. For each level of oscillation, we randomly generate 50 different Gs and then construct the set of candidate plans for each G. The model here is trained with 1900 samples using the same settings as in our first evaluation and we gradually increase the level of oscillation. We compare the  and  values computed from the ground truth labeling of the chosen plans. The result is provided in Fig. 4. When the oscillation is small, the performances of both approaches are similar. As the oscillation increases, the performances of the two approaches diverge. This is expected since RAND-SELECT randomly chooses plans and hence its performance should decrease as the oscillation increases. On

Fig. 4.

Comparison of EXPD-SELECT and RAND-SELECT Fig. 5.

Comparison of FF and FF-EXPD considering only .

the other hand, EXPD-SELECT is not influenced as much although its performance also tends to decrease. This is partly due to the fact that the model used in this evaluation is trained with samples having a maximum of 3 hidden goals. In Fig. 4 for explicability, almost all results are significantly different at 0.001 level (except at 1); for predictability, results are significantly different at 0.01 level at 3, 5 and 6. The trend to diverge is clearly present. Note that we use linearchain CRFs in our evaluations, which does not directly model correlations among observations across states. These features are common in our rover domain (e.g., navigating back and forth). Hence, we can anticipate performance improvement with more general CRFs. Synthesizing Explicable and Predictable Plans: We evaluate here plan synthesis using Alg. 1. More specifically, we compare FF planner that considers the predicted  and  values in its heuristics with a normal FF planner that only considers the action cost. The FF planner with the new heuristic is called FF-EXPD. In this evaluation, we set the maximum number of hidden locations to visit to be 6. For each trial, we generate 100 problems and apply both FF and FF-EXPD to solve the problems. Given that we are interested in comparing the cases when explicability is low, we only consider problems when the predicted plan explicability for the plan generated by FF is below 0.85. First, we consider the incorporation of  only. The result is presented in Fig. 5. For the explicability measure, we see a significant difference in all trials. Another observation is that the difference in plan predictability is present but not as significant. This evaluation suggests that our heuristic search can produce plans of high explicability. Next, we consider the incorporation of  only. The result is presented in Fig. 6. Similarly, we see a significant difference in all trials for both explicability and predictability. One observation is that improving on plan predictability also improves plan explicability which is expected given Eqs. (7) and (8)). Plan Cost: We consider plan cost here for the evaluation in Fig. 6. The result is presented in Table I. We can see that the plan length for FF-EXPD is longer than the plan produced by FF in general. This is expected since FF only considers plan cost. However, in all settings, FF-EXPD penalizes the plan cost slightly (about 10%) to improve the plan explicability and predictability measures.

Fig. 6.

Comparison of FF and FF-EXPD considering only  . TABLE I P LAN S TEPS C OMPARISON FOR F IG . 6

Trial ID FF (avg. # steps) FF-EXPD (avg. # steps)

1 21.9 23.5

2 24.0 26.3

3 24.1 25.2

4 23.9 24.0

5 22.1 23.4

6 22.4 25.0

B. Evaluation with Physical Robots In this section we evaluate our approach in a blocks world domain with a physical robot. It simulates a smart manufacturing environment where robots are working beside humans. Although the human and robot do not have direct interactions ≠ the robot's goal is independent of the human's, generating explicable plan is still an important issue since it will help humans concentrate more on their own tasks. Here, we evaluate plans generated by the robot using FF-EXPD and a cost-optimal planner (OPT) in various scenarios and compare the plans with human subjects in terms of explicability. 1) Domain Description: In this domain, the robot's goal (which is known to the human) is to build a tower of a certain height using blocks on the table. The towers to be built have different heights in different problems. There are two types of blocks, light ones and heavy ones, which are indistinguishable externally but the robot can identify them based on the markers. Picking up the heavy blocks are more costly than the light blocks for the robot. Hence, the robot may sometimes choose seemingly more costly (i.e., longer) plans to build a tower from the human's perspective. 2) Experimental Setup: We generated a set of 23 problems in this domain in which towers of height 3 are to be built. The plans for these problems were manually generated and labeled as the training set. For 4 out of these 23 problems, the optimal plan is not the most explicable plan. To remove the influence of

grounding, we also generated permutations of each plan using different object names for these 23 problems, which resulted in a total of about 15000 training samples. We then generated a set of 8 testing problems for building towers of various heights (from 3 - 5) to verify that our approach can generalize. Testing problems were generated only for cases where plans are more likely to be inexplicable. For each problem, we generated two plans, one using OPT and the other using FF-EXPD, and recorded the execution of these plans on the robot. We recruited 13 subjects on campus and each human subject was tasked with labeling two plans (generated by OPT and FFEXPD respectively) for each of the 8 testing problems, using the recorded videos and following a process similar to that used in preparing training samples. After labeling each plan, we also asked the subject to provide a score (1 - 10 with 10 being the most explicable) to describe how comprehensible the plan was overall. 3) Results: In this evaluation, we only use one task label "building tower". For all testing problems, the labeling process results in 77.8% explicable actions (i.e., actions with a task label) for OPT and 97.3% explicable actions for FFEXPD. The average explicability measures for FF-EXPD and OPT are 0.98 and 0.78, and the average scores are 9.65 and 6.92, respectively. We analyze the results using a paired Ttest which shows a significant difference between FF-EXPD and OPT in terms of the explicability measures (using Eq. (7)) computed from the human labels and the overall scores (p < 0.001 for both). Furthermore, after normalizing the scores from the human subjects, the Cronbach's  value shows that the explicability measures and the scores are consistent for both FF-EXPD and OPT ( = 0.78, 0.67, respectively). These results verify that: 1) our explicability measure does capture the human's interpretation of the robot plans and 2) our approach can generate plans that are more explicable to humans. In Fig. 7, we present the plans for a testing scenario. The left part of the figure shows the plan generated by OPT and the right part shows the plan generated by FF-EXPD. A video is also attached showing the different behaviors with the two planners in this scenario. VI. C ONCLUSION While we are still far from having intelligent robots and agents working side-by-side of humans as teammates (rather than as tools), it becomes increasingly important to consider issues when such autonomous agents appear in our everyday life. These agents need to create and execute complex plans. In this paper, we introduced plan explicability and predictability for such agents so that they can synthesize plans that are more comprehensible to humans. To achieve this, they must consider not only their own models but also the human's interpretation of their models. To the best of our knowledge, this is the first attempt to model plan explicability and predictability for task planning which differs from previous work on humanaware planning. The proposed measures have a variety of applications (e.g., achieving fluent human-robot interaction and ensuring human safety). To compute these measures, we learn

Fig. 7. Plan execution of two plans generated by OPT (left) and FF-EXPD (right) for one out of the 8 testing scenarios. The top figure shows the setup of this scenario where the goal is to build a tower of height 3. The block that is initially on the left side of the table is a heavy block. The optimal plan involves more actions with the light blocks (i.e., putting the two light blocks on top of the heavy one) while the explicable plan is more costly since it requires moving the heavy one.

the labeling scheme of humans for agent plans from training examples based on CRFs. We then use this learned model to label a new plan to compute its explicability and predictability. The proposed approach is evaluated on a synthetic domain and with human subjects using physical robots to show its effectiveness. A natural extension of our work is to consider human-robot teaming where there exists close interactions. Humans in our current settings are observers. Finally, while we focus on the explicability and predictability measures for robot task planning, they also have many other interesting applications. For example, many defense applications use planning to create unpredictable and inexplicable plans, which can help deter or confuse enemies and are also useful for testing defenses against novel or unexpected attacks. These applications can be implemented using our approach by minimizing the  and  measures instead of maximizing them.

R EFERENCES [1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML '04, 2004. [2] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith, and Subbarao Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. [3] Eugene Charniak and Robert P. Goldman. A bayesian model of plan recognition. Artificial Intelligence, 64(1): 53 ≠ 79, 1993. ISSN 0004-3702. [4] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1≠7, June 2009. [5] Gergely Csibra and Gy® orgy Gergely. Obsessed with goals?: Functions and mechanisms of teleological interpretation of actions in humans. Acta psychologica, 124 (1):60≠78, 2007. [6] Anca Dragan and Siddhartha Srinivasa. Generating legible motion. In Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013. [7] Terrence W Fong, Illah Nourbakhsh, and Kerstin Dautenhahn. A survey of socially interactive robots. Robotics and Autonomous Systems, 2003. [8] Maria Fox and Derek Long. Pddl2.1: An extension to pddl for expressing temporal planning domains. J. Artif. Int. Res., 20(1), December 2003. [9] Moritz Gbelbecker, Thomas Keller, Patrick Eyerich, Michael Brenner, and Bernhard Nebel. Coming up with good excuses: What to do when no plan can be found. In International Conference on Automated Planning and Scheduling, 2010. [10] Marc Hanheide, Moritz Gbelbecker, Graham S. Horn, Andrzej Pronobis, Kristoffer Sj, Alper Aydemir, Patric Jensfelt, Charles Gretton, Richard Dearden, Miroslav Janicek, Hendrik Zender, Geert-Jan Kruijff, Nick Hawes, and Jeremy L. Wyatt. Robot task planning and explanation in open and uncertain worlds. Artificial Intelligence, 2015. ISSN 0004-3702. [11] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human≠robot fluency. Robotics, IEEE Transactions on, 23(5):952≠961, 2007. [12] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team. In Proceedings of the ACM/IEEE international conference on Human-robot interaction, pages 1≠8. ACM, 2007. [13] J® org Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. J. Artif. Int. Res., 14(1):253≠302, May 2001. ISSN 10769757. [14] Henry A. Kautz and James F. Allen. Generalized Plan [15]

[16]

[17] [18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

Recognition. In National Conference on Artificial Intelligence, pages 32≠37, 1986. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01, pages 282≠289, 2001. ISBN 1-55860-778-1. Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot teams. In Twenty-Fourth International Conference on Automated Planning and Scheduling, 2014. Tom Murray. Authoring Intelligent Tutoring Systems: An analysis of the state of the art. Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao Kambhampati. Automated planning for peerto-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. Fuchun Peng and Andrew McCallum. Information extraction from research papers using conditional random fields. Information Processing & Management, 42(4): 963 ≠ 979, 2006. ISSN 0306-4573. Miquel Ram¥ irez and Hector Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010. Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal. Improved human-robot team performance using chaski, a human-inspired plan execution system. In Proceedings of the 6th international conference on Human-robot interaction, pages 29≠36. ACM, 2011. E.A Sisbot, L.F. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. Robotics, IEEE Transactions on, 23(5):874≠883, Oct 2007. Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias Scheutz, and Subbarao Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠ 2962, Sept 2014. Robin R. Vallacher and Daniel M. Wegner. What do people think they're doing? action identification and human behavior. Psychological Review, 94(1):3≠15, 1987. Yu Zhang, Vignesh Narayanan, Tathagata Chakraborty, and Subbarao Kambhampati. A human factors analysis of proactive assistance in human-robot teaming. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. Yu Zhang, Sarath Sreedharan, and Subbarao Kambhampati. Capability models and their applications in planning. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '15, 2015.

TweetSense: Context Recovery for Orphan Tweets by Exploiting Social Signals in Twitter
Manikandan Vijayakumar, Tejas Mallapura Umamaheshwar Subbarao Kambhampati
Arizona State University,Tempe, AZ 85281

Kartik Talamadupula
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598

krtalamad@us.ibm.com

{manikandan.v,tejas.m.u,rao}@asu.edu ABSTRACT
As the popularity of Twitter, and the volume of tweets increased dramatically, hashtags have naturally evolved to become a de facto context providing/categorizing mechanism on Twitter. Despite their wide-spread adoption, fueled in part by hashtag recommendation systems, lay users continue to generate tweets without hashtags. When such "orphan" tweets show up in a (browsing) user's time-line, it is hard to make sense of their context. In this paper, we present a system called TweetSense which aims to rectify such orphan tweeets by recovering their context in terms of their missing hashtags. TweetSense enables this context recovery by using both the content and social network features of the orphan tweet. We characterize the context recovery problem, present the details of TweetSense and present a systematic evaluation of its effectiveness over a 7 million tweet corpus. people that the originator follows (friends). Originators are most likely to use hashtags which are temporally close, and are also more likely to reuse hashtags from other users whose tweets they have favorited, retweeted, and @mentioned (a tweet that conatins "@username"). To reflect this generative model, in TweetSense, a statistical model is built to capture a set of social signals, temporal signals related to the tweet and the originator of the tweet. These features measure the tie strength between users, temporal locality and trendiness of the hashtags within the users' social graph. TweetSense learns a model to predict whether a hashtag is applicable to a tweet or not. Given a test tweet lacking a hashtag (context), the model is used to predict hashtags from a set of hashtags collected from the timeline of the creator of the test tweet.

2.

RELATED WORK

1.

INTRODUCTION

Twitter has grown beyond the role of a platform that is used merely for sharing status updates, as it was initially envisioned. Recent work including that of Java et. al. [5] has identified daily chatter, conversations, information sharing, and news reporting as some of the motivations for users that actively participate in the Twitter network. On average, a user's feed gets a few hundred new tweets every ten minutes It is hard to make sense out of such a feed unassisted, especially when many tweets appear without a hashtag. Hashtags are one of the major features of tweets (and Twitter); they are either a single word or an unspaced phrase prefixed with a pound sign #. The context of a tweet can then be described as a set of one or more hashtags. Twitter provides hashtags, partly in an attempt to organize the stream of tweets. However, using hashtags as a method to find the topic of a tweet does not always work, mainly because users do not always tag their tweets with hashtags. As an illustration, in the data that we crawled for our experimentation (all from the year 2014), 76.30% of tweets are orphan tweets. In this paper, we present the TweetSense system that helps in recovering the context of a tweet by tagging the tweet with a suitable hashtag. TweetSense captures the most relevant data from a given user's social graph in order to recover hashtag(s) for a given tweet. The underlying hypothesis is that when the creator of a tweet, called the originator, uses a hashtag (to define the context for a tweet), they are likely to reuse one or more hashtags that they see on their own timeline. This includes both tweets posted by the originator, as well as tweets created by the

A problem that is related to the context recovery problem is that of recommending a hashtag for a tweet that the originator is about to post. There has been some previous work on the hashtag recommendation problem. Eva et al. [7] present a recommender system that aims at creating a more homogeneous set of hashtags by considering similarity of tweet text. This candidate recommendation list is later refined using recently used hashtags, popularity of hashtags with in the recommendation list, and popularity of a hashtag within the underlying data set. Jieying She et al. [6] propose a TOpic MOdel-based HAshtag recommendation (TOMOHA) solution. The model learns whether the topic of a tweet is related to a topic which is local to the user or to a global background topic of the corpus. The trained model is used to recommend the most probable hashtags for a tweet. Wei Fang et al. [3] propose a Personalized Hashtag Recommendation system which suggests both content-relevant and user-relevant hashtags when users are composing tweets. The hashtag-relevant features are also used to create hybrid versions of the two systems. In the hashtag recovery problem, the time taken to predict a hashtag is not as critical as compared to a recommender system. The accuracy of prediction is more important in the problem of context recovery as we are aiding in finding the topic of the tweet rather than suggesting possible topics for the tweet being composed. The temporal information corresponding to the orphan tweet and its creator becomes very important.The problem of recovering a hashtag for tweets on a user's timeline has so far not been addressed.

3.

OVERVIEW OF TWEETSENSE

To set up the model for the problem of context recovery, given a tweet Qx created by a user Oy , we track down the most promising hashtags for it. The candidate set of tweets CTx is derived based on the generative model of our system by tracing down tweets Tx on the user Oy 's timeline. The candidate set of tweets CTx contains only the tweets that are created before the tweet Qx was created. Given a query tweet Qx , without a context created by an originator Oy appearing on the time-line of a browsing user on Twitter, a set of candidate tweets (containing hashtags) - CTxi , CHxj extracted from the social circle of the user Oy , and U - the creator of CTxi , CHxj , we want to compute P (CHxj |Qx , CTxi , Oy , U ) - which is the probability that hashtag CHxj of tweet CTxi from the candidate set CTx is actually the context of Qx . We estimate the probability discriminatively using a Logistic Regression model. The features are derived from tweets Qx and CTxi , users Oy and U . The tweet-content related features include similarity between tweet text, hashtag popularity and temporal information of the tweet. The user related features include mutual friends, mutual followers, and social signals like @mentions, favorites and common hashtags between the user who created tweet Oy , and the user U who is a part of Oy 's social network and created the tweet CTxi . The scoring methods for each feature is described in the following section.

3.1

Tweet-Content Related Features

Similarity Score: is based on the cosine similarity between the text content of the tweet Qx and the tweets contained in the set of candidate tweets CTx . We assume that the tweets in CTx that share the text content with Qx is more likely to share the hashtag with Qx . cosxi =
Q.CTxi Q CTxi

We only consider the tweets in English and ignore query tweets in other languages, special characters, emoticons, URLs, and HTTP links. We also remove stop words. Recency Score: Hashtags that are temporally close to the query tweet get a higher ranking. We determine the time window for the tweet, hashtag pair, CTxi , CHxj , using the "created at" timestamp, CR(CTxi ), associated with the tweet CTxi . We adapt the exponential decay function to compute the recency score of a hashtag. We use the exprest , where t = 60 ◊ 103 , to compute the sion e- recency score. By varying the sensitivity of the time window from 1 minute to 170 hours, we found that the results are more promising when the time window is set to 17 hours. This corresponds to a value of t equal to 60 ◊ 103 . Social Trend Score: corresponds to the popularity of hashtags within the candidate hashtag set, CHx . As the candidate hashtag set CHx is derived from the timeline of the user U who posted the tweet Qx , it is intuitive that a hashtag with high frequency is popular in the user's social network. The social trend score is computed based on the "One person, One vote" approach. It is used to get the count of frequently used hashtags in CHx . CR(Qx )-CR(CTxi )

compute a user's attention score by a weighted average sum on the conversations between two users. Let AT (TOy ) be the set of all tweets of user Oy and AT (TU ) be the set of all tweets of user U , let AT (TOy ,U ) be the set of all tweets which has @mentions and replies of Oy with U and let AT (TU,Oy ) be the set of all tweets which has @mentions and replies of U with Oy , where U is a user who belongs to the list of friends of Oy . We compute the weighted average of @men|AT (TO ,U )| tion and replies between the users as: ai,j = AT (T y ) | Oy | |AT (TOy ,U )| aj,i = |AT (TU )| Final Score = () ai,j + (1 - ) aj,i where  = 0.5 We have set  = 0.5. Favorite Score: When a user favorites a tweet posted by his friend, the user is consciously letting his friend know that he shares interest with the friend on that specific topic. Higher the number of times a user favorites a tweet of another user, higher is the favorite score. Let F V (TOy ,U ) be the set of all tweets which has favorites of Oy with U and let F V (TU,Oy ) be the set of all tweets which has favorites of U with Oy , where U is a user who belongs to the set of friends of Oy . Favorite score can be computed by us|F V (TO ,U )| |F V (TO ,U )| ing the expression: ai,j = F V (T y ) aj,i = |F V (Ty U )| | Oy | Final Score = () ai,j + (1 - ) aj,i where  = 0.5 We have set  = 0.5. Mutual Friends Score: Mutual friends score is computed to rank the friends based on their number of common friends that they share in their social network. If FOy contains set of users that are friends with user Oy and FU contains set of users that are friends with user U . We use the same Jaccard's coefficient [4] on the two set as the measure of the "mutual friends" feature. Mutual Followers Score: Mutual followers score is computed to rank friends based on the number of followers they share in their network. If F WOy contains set of users following user Oy and F WU contains set of users following user U . We use the same Jaccard's coefficient [4] on the two set as the measure of the "mutual followers" feature. Common Hashtags Score: Common hashtags score is computed between any two users based on the hashtags that are shared between them. If two users Oy and U use the same set of hashtags for a particular time window, then both the users are talking about the same topic. To compute this, we first collect the unique set of hashtags used by each user, and then use Jaccard's coefficient [4] on the hashtag sets HOy and HU . Reciprocal Score: The user might follow his friend but also follow a topic of his interest such as a news channel or a celebrity. To give more importance to a user's friends over others, the reciprocal rank assigns fixed values to classify the user's followers as a "friend", or as "not a friend". The users who follow each other will receive a fixed score of 1.0, and 0.5 other wise.

3.3

Statistical Model

3.2

User Related Features

Attention Score: If a particular user was @mentioned recently, it is more likely that they share topics of interest. This also means that they might use similar hashtags. We

The problem is modeled as shown in Figure 1. We build a Logistic Regression model based on the feature matrix extracted based on the tweets corresponding to the set of training users. Training dataset: The training data set is constructed by considering many training tweets Q. The corresponding set of candidate tweet and hashtag pairs CTx , CHx is

Characteristics Total number of users Total number of originator users Total Tweets Crawled Tweets with Hashtags Tweets without Hashtags Tweets with exactly one Hashtag Tweets with more than one Hashtag Tweets with Favorites Tweets with @mentions

Value 8,949 63 7,212,855 1,883,086 6,062,167 1,322,237 560,849 716,738 4,658,659

Percentage N/A N/A 100% 23.70% 76.30% 16.64% 7.06% 9.02% 58.63%

Table 1: Characteristics of the dataset used for the experiments user's timeline, the method can only return up to 3,200 of a user's most recent tweets from his timeline. The favorite tweets that can be crawled are limited to 200 most recent tweets per user. We randomly picked users by navigating through the trending hashtags during a fixed time interval. For each of the selected users, we crawled the most recent 1500 tweets, and further crawled recent 1500 tweets for each friend (followee) of the selected user. Since, the number of tweets crawled to build a user's social graph is directly proportional to the number of friends, we randomly constrained the user selection process to choose users with at most 300 friends. We crawled 7,212,855 tweets for 8,949 users. Further details about the characteristics of the dataset can be found in Table2.

Figure 1: Training the Model from Tweets With Hashtags to Predict the Hashtags for Tweets Without Hashtag identified. Here, the candidate set of tweets are the tweets from the timeline of the user Oy who posted the tweet Qx containing the hashtag CHx . For each candidate tweet, and candidate hashtag pair CTxi , CHxj created by user U in the candidate tweet set, the feature scores are computed with respect to the Qx , and user Oy . The training dataset is a feature matrix containing the feature vectors of all CTxi , CHxj pair corresponding to all training tweets Q. The class label for a feature vector is 1 if the hashtag CHxj in the candidate set of tweets is equal to the hashtag in Qx , the tweet at consideration, and 0 otherwise. Handling unbalanced training set: The training dataset has a class distribution of 95% negative samples and 5% positive samples. Learning the model from an unbalanced dataset will cause very low precision. We use the Synthetic Minority Oversampling Technique (SMOTE) [2] to re-sample the unbalanced dataset to a balanced dataset with 50% positive samples and 50% negative samples. Classifier learning: We apply the Logistic regression to learn a statistical model from the training dataset to predict the probabilities of the top K most promising hashtags for a given test tweet. Logistic regression assumes that all data points share the same parameter vector with the test tweet. Using the Classifier: For each test tweet, its candidate set of tweet-hashtag pairs are tracked down and feature vectors are computed. When the test dataset is passed to the learned model, it predicts the maximum likelihood probability for each of the candidate hashtags CHxj in tweet hashtag pairs CTxi , CHxj corresponding to the test tweet. The candidate hashtags with predicated class label as 1 are then ranked using the probabilities.

5.

EMPIRICAL EVALUATION

We present an internal and external evaluation of TweetSense. The testing dataset comprised of tweets that had exactly one hashtag associated with it. The hashtag was removed for the purpose of testing, and this served as the ground truth for the test tweet.

5.1

External Evaluation

The closest related work for the problem of context recovery is the problem of recommending hashtags. Therefore, we choose the system proposed by Eva et al. [7] as our baseline. Their system aims at creating a more homogeneous set of hashtags by considering the similarity of tweet text to create a candidate recommendation list. This candidate recommendation list is later refined using recently used hashtags, and popularity of hashtags with in the candidate recommendation list. External Evaluation Of TweetSense Based On Precision at N :

Figure 2: External evaluation againt state-of-the-art system for Precison @ N Our system was able to recommend correct hashtags for precision at 20 for 59% of the tweets, which in general is above 50%. Also compared to the best possible ranking method of the baseline model which could recommend cor-

4.

EXPERIMENTAL SETUP

Dataset: We use Sprintze [1] to crawl Twitter data through the Twitter Streaming API. In order to crawl a

rect hashtags for 35% of the tweets.On an average, TweetSense dominates the baselines for different values of N . A user tweets about his interests and also about what he is exposed to on his timeline. A user would rarely use a hashtag, which he has never seen. There are many indicators that indicate how a user adapts hashtags and most of these are related to user's social network. TweetSense picks the most suitable set of hashtags as candidate hashtag set by looking at the user's timeline rather than at global Twitter ecosystem. We have identified different features that can further help in determining the most important indicator of all the indicator by assuming the user's environment at the time of the creation of a tweet. These indicators change with time, and we also model this by considering different set of candidate hashtags for the same user for different tweets.

All Features Similarity Score Recency Score Social Trend Score Attention Score Favorite Score Mutual Friends Score Mutual Followers Score Common Hashtag Score Reciprocal Score

Exp1 0.0942 0.0022 0.0017 0 0.2837 13538.65 0.0923 0 0.7144

Exp2 0.1123 0.0024 0.0017 0 0.24 N/A 3.115 0 0.7717

Exp3 0.1134 0.0026 0.0016 0 0.2112 N/A N/A 0 N/A

Exp4 N/A N/A N/A N/A N/A 0.2081 N/A N/A N/A

Table 2: Estimation of Odds Ratio by Feature Selection Friends" feature. As we could see, the score is low in this case while it is higher when the model was built with all other features. This indicates that we require all the other features along with the "Mutual Friends" feature to make better predictions. All these experiments emphasize the fact that social features rather than the tweet-content related features are the most important features in recovering context of an orphan tweet. Results on Accuracy of Ranking based on Rank Position: The accuracy on hashtags recommended by the system is shown by determining the ranking positions of the top 10 recommended hashtags for the test dataset.Results for each ranking position as follows: Rank1-27.75%,Rank221.53%,Rank3-13.56%,Rank4-12.28%, Rank5-6.70%,Rank65.10%, Rank7-4.31%,Rank8-2.07%, Rank9-3.51%, Rank103.19%.The consistent performance by the system for the top four positions of the top K ranking positions imply that the system is more accurate.

5.2

Internal Evaluation

The correctness of the system is evaluated using Precision at N . We compare the importance of different features in the model by using odds ratio. Results of Internal Evaluation Of Precision at N by Varying the Training Dataset: We compare the precision at N at 5,10,15, and 20 of the proposed system. Our approach gets better precision as the size of N is increased. For a total sample size of 1599 random tweets with hashtags whose hashtags are deliberately removed for evaluation. At the value of N = 5, 720/1599 sample tweets are recommended with the correct hashtags. Similarly, 849/1599 at N = 10, 901/1599 at N = 15 and 944/1599 at N = 20 are predicted correctly.

6.

CONCLUSION

Figure 3: Precision at N = 5, 10, 15, and 20 on Varying the Size of the Training Dataset. Results for Estimation of Odds Ratio by Feature Selection: We measure the association between an exposure and an outcome using odds ratio. In the Table 2, Exp1 column indicates that "Mutual Friends" feature is contributing the most to the odds of the outcome when compared to the other features. This reinforces the hypothesis that the social signals are more important than the tweet-content related features while predicting hashtags. In order to validate whether the prediction capability of the model is based solely on a single feature, we created a model by ignoring the "Mutual Friends" feature during the training phase. In this case - Exp2, as shown in the Table 2, we can see that the "Mutual Followers" feature becomes very important. There could be a correlation between the two features because of feature redundancy. In Exp3, we remove most of the social features - "Mutual Friends", "Mutual Followers", and "Reciprocal" features to build a model. We can observe that the odds ratio of the features being considered do not improve significantly. In Exp4, we ignore all the features, but the "Mutual Friends" feature to build a model to further verify the importance of the "Mutual

In this paper, we defined and motivated the context recovery problem from orphan tweets. We then described TweetSense a discriminative learning approach for recovering the context of the orphan tweets in terms of their missing hashtags. TweetSense uses a variety of features drawn from the timeline, content and social network. Our experiments on a large tweet corpus demonstrate the effectiveness of TweetSense. Acknowledgments: We gratefully acknowledge the significant help from Sushovan De in this research. This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13- 1-0176, N00014-13-10519 and N00014-15-1-2027, and a Google faculty research award.

References
[1] Twitter"s streaming api ,https://blog.gnip.com/tag/spritzer/. [2] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. arXiv preprint arXiv:1106.1813, 2011. [3] W. Feng and J. Wang. We can learn your hashtags: Connecting tweets to explicit topics. In Data Engineering (ICDE), 2014 IEEE 30th International Conference on, pages 856≠867, March 2014. ¥ [4] P. Jaccard. Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci¥ et¥ e Vaudoise des Sciences Naturelles, 37:547≠579, 1901. [5] A. Java, X. Song, T. Finin, and B. Tseng. Why we twitter: Understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop on Web Mining and Social Network Analysis, WebKDD/SNA-KDD '07, pages 56≠65, New York, NY, USA, 2007. ACM. [6] J. She and L. Chen. Tomoha: Topic model-based hashtag recommendation on twitter. In Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion, WWW Companion '14, pages 371≠ 372, Republic and Canton of Geneva, Switzerland, 2014. International World Wide Web Conferences Steering Committee. [7] E. Zangerle, W. Gassler, and G. Specht. On the impact of text similarity functions on hashtag recommendations in microblogging environments. Eva2013, 3(4):889≠898, 2013.

TweetSense: Context Recovery for Orphan Tweets by
Exploiting Social Signals in Twitter
Manikandan Vijayakumar,
Tejas Mallapura Umamaheshwar
Subbarao Kambhampati

Kartik Talamadupula
IBM T.J. Watson Research Center,
Yorktown Heights, NY 10598

krtalamad@us.ibm.com

Arizona State University,Tempe, AZ 85281

{manikandan.v,tejas.m.u,rao}@asu.edu
ABSTRACT
As the popularity of Twitter, and the volume of tweets increased dramatically, hashtags have naturally evolved to become a de facto context providing/categorizing mechanism
on Twitter. Despite their wide-spread adoption, fueled in
part by hashtag recommendation systems, lay users continue
to generate tweets without hashtags. When such ‚Äúorphan‚Äù
tweets show up in a (browsing) user‚Äôs time-line, it is hard
to make sense of their context. In this paper, we present a
system called TweetSense which aims to rectify such orphan
tweeets by recovering their context in terms of their missing hashtags. TweetSense enables this context recovery by
using both the content and social network features of the orphan tweet. We characterize the context recovery problem,
present the details of TweetSense and present a systematic
evaluation of its effectiveness over a 7 million tweet corpus.

1.

INTRODUCTION

Twitter has grown beyond the role of a platform that is
used merely for sharing status updates, as it was initially envisioned. Recent work including that of Java et. al. [5] has
identified daily chatter, conversations, information sharing,
and news reporting as some of the motivations for users that
actively participate in the Twitter network. On average, a
user‚Äôs feed gets a few hundred new tweets every ten minutes It is hard to make sense out of such a feed unassisted,
especially when many tweets appear without a hashtag.
Hashtags are one of the major features of tweets (and
Twitter); they are either a single word or an unspaced phrase
prefixed with a pound sign #. The context of a tweet can
then be described as a set of one or more hashtags. Twitter provides hashtags, partly in an attempt to organize the
stream of tweets. However, using hashtags as a method to
find the topic of a tweet does not always work, mainly because users do not always tag their tweets with hashtags.
As an illustration, in the data that we crawled for our experimentation (all from the year 2014), 76.30% of tweets are
orphan tweets. In this paper, we present the TweetSense
system that helps in recovering the context of a tweet by
tagging the tweet with a suitable hashtag. TweetSense captures the most relevant data from a given user‚Äôs social graph
in order to recover hashtag(s) for a given tweet. The underlying hypothesis is that when the creator of a tweet, called
the originator, uses a hashtag (to define the context for a
tweet), they are likely to reuse one or more hashtags that
they see on their own timeline. This includes both tweets
posted by the originator, as well as tweets created by the

people that the originator follows (friends). Originators are
most likely to use hashtags which are temporally close, and
are also more likely to reuse hashtags from other users whose
tweets they have favorited, retweeted, and @mentioned (a
tweet that conatins ‚Äú@username‚Äù).
To reflect this generative model, in TweetSense, a statistical model is built to capture a set of social signals,
temporal signals related to the tweet and the originator
of the tweet. These features measure the tie strength between users, temporal locality and trendiness of the hashtags
within the users‚Äô social graph. TweetSense learns a model
to predict whether a hashtag is applicable to a tweet or not.
Given a test tweet lacking a hashtag (context), the model
is used to predict hashtags from a set of hashtags collected
from the timeline of the creator of the test tweet.

2.

RELATED WORK

A problem that is related to the context recovery problem is that of recommending a hashtag for a tweet that the
originator is about to post. There has been some previous
work on the hashtag recommendation problem. Eva et al.
[7] present a recommender system that aims at creating a
more homogeneous set of hashtags by considering similarity
of tweet text. This candidate recommendation list is later
refined using recently used hashtags, popularity of hashtags
with in the recommendation list, and popularity of a hashtag
within the underlying data set. Jieying She et al. [6] propose a TOpic MOdel-based HAshtag recommendation (TOMOHA) solution. The model learns whether the topic of a
tweet is related to a topic which is local to the user or to a
global background topic of the corpus. The trained model is
used to recommend the most probable hashtags for a tweet.
Wei Fang et al. [3] propose a Personalized Hashtag Recommendation system which suggests both content-relevant
and user-relevant hashtags when users are composing tweets.
The hashtag-relevant features are also used to create hybrid
versions of the two systems.
In the hashtag recovery problem, the time taken to predict
a hashtag is not as critical as compared to a recommender
system. The accuracy of prediction is more important in the
problem of context recovery as we are aiding in finding the
topic of the tweet rather than suggesting possible topics for
the tweet being composed. The temporal information corresponding to the orphan tweet and its creator becomes very
important.The problem of recovering a hashtag for tweets
on a user‚Äôs timeline has so far not been addressed.

3.

OVERVIEW OF TWEETSENSE

To set up the model for the problem of context recovery,
given a tweet Qx created by a user Oy , we track down the
most promising hashtags for it. The candidate set of tweets
CTx is derived based on the generative model of our system
by tracing down tweets Tx on the user Oy ‚Äôs timeline. The
candidate set of tweets CTx contains only the tweets that
are created before the tweet Qx was created.
Given a query tweet Qx , without a context created by
an originator Oy appearing on the time-line of a browsing
user on Twitter, a set of candidate tweets (containing hashtags) - hCTxi , CHxj i extracted from the social circle of the
user Oy , and U - the creator of hCTxi , CHxj i, we want to
compute P (CHxj |Qx , CTxi , Oy , U ) - which is the probability that hashtag CHxj of tweet CTxi from the candidate set
CTx is actually the context of Qx . We estimate the probability discriminatively using a Logistic Regression model.
The features are derived from tweets Qx and CTxi , users
Oy and U .
The tweet-content related features include similarity between tweet text, hashtag popularity and temporal information of the tweet. The user related features include mutual
friends, mutual followers, and social signals like @mentions,
favorites and common hashtags between the user who created tweet Oy , and the user U who is a part of Oy ‚Äôs social
network and created the tweet CTxi . The scoring methods
for each feature is described in the following section.

3.1

Tweet-Content Related Features

Similarity Score: is based on the cosine similarity between the text content of the tweet Qx and the tweets contained in the set of candidate tweets CTx . We assume
that the tweets in CTx that share the text content with
Qx is more likely to share the hashtag with Qx . cosŒòxi =
~ CT
~xi
Q.
~ kkCT
~xi k
kQ
We only consider the tweets in English and ignore query
tweets in other languages, special characters, emoticons, URLs,
and HTTP links. We also remove stop words.
Recency Score: Hashtags that are temporally close to
the query tweet get a higher ranking. We determine the
time window for the tweet, hashtag pair, hCTxi , CHxj i, using the ‚Äúcreated at‚Äù timestamp, CR(CTxi ), associated with
the tweet CTxi . We adapt the exponential decay function to
compute the recency score of a hashtag. We use the expresCR(Qx )‚àíCR(CTxi )

t
, where t = 60 √ó 103 , to compute the
sion e‚àí
recency score. By varying the sensitivity of the time window
from 1 minute to 170 hours, we found that the results are
more promising when the time window is set to 17 hours.
This corresponds to a value of t equal to 60 √ó 103 .
Social Trend Score: corresponds to the popularity of
hashtags within the candidate hashtag set, CHx . As the
candidate hashtag set CHx is derived from the timeline of
the user U who posted the tweet Qx , it is intuitive that a
hashtag with high frequency is popular in the user‚Äôs social
network. The social trend score is computed based on the
‚ÄùOne person, One vote‚Äù approach. It is used to get the count
of frequently used hashtags in CHx .

3.2

User Related Features

Attention Score: If a particular user was @mentioned
recently, it is more likely that they share topics of interest.
This also means that they might use similar hashtags. We

compute a user‚Äôs attention score by a weighted average sum
on the conversations between two users. Let AT (TOy ) be
the set of all tweets of user Oy and AT (TU ) be the set of all
tweets of user U , let AT (TOy ,U ) be the set of all tweets which
has @mentions and replies of Oy with U and let AT (TU,Oy )
be the set of all tweets which has @mentions and replies of
U with Oy , where U is a user who belongs to the list of
friends of Oy . We compute the weighted average of @men|AT (TO ,U )|
tion and replies between the users as: ai,j = AT (T y )
|
Oy |
|AT (TOy ,U )|
aj,i = |AT (TU )|
Final Score = (Œ±) ai,j + (1 ‚àí Œ±) aj,i where Œ± = 0.5 We
have set Œ± = 0.5.
Favorite Score: When a user favorites a tweet posted
by his friend, the user is consciously letting his friend know
that he shares interest with the friend on that specific topic.
Higher the number of times a user favorites a tweet of another user, higher is the favorite score. Let F V (TOy ,U ) be
the set of all tweets which has favorites of Oy with U and
let F V (TU,Oy ) be the set of all tweets which has favorites
of U with Oy , where U is a user who belongs to the set
of friends of Oy . Favorite score can be computed by us|F V (TO ,U )|
|F V (TO ,U )|
ing the expression: ai,j = F V (T y ) aj,i = |F V (TyU )|
|
Oy |
Final Score = (Œ±) ai,j + (1 ‚àí Œ±) aj,i where Œ± = 0.5 We
have set Œ± = 0.5.
Mutual Friends Score: Mutual friends score is computed to rank the friends based on their number of common
friends that they share in their social network. If FOy contains set of users that are friends with user Oy and FU contains set of users that are friends with user U . We use the
same Jaccard‚Äôs coefficient [4] on the two set as the measure
of the ‚Äúmutual friends‚Äù feature.
Mutual Followers Score: Mutual followers score is computed to rank friends based on the number of followers they
share in their network. If F WOy contains set of users following user Oy and F WU contains set of users following user
U . We use the same Jaccard‚Äôs coefficient [4] on the two set
as the measure of the ‚Äúmutual followers‚Äù feature.
Common Hashtags Score: Common hashtags score is
computed between any two users based on the hashtags that
are shared between them. If two users Oy and U use the
same set of hashtags for a particular time window, then both
the users are talking about the same topic. To compute this,
we first collect the unique set of hashtags used by each user,
and then use Jaccard‚Äôs coefficient [4] on the hashtag sets
HOy and HU .
Reciprocal Score: The user might follow his friend but
also follow a topic of his interest such as a news channel or
a celebrity. To give more importance to a user‚Äôs friends over
others, the reciprocal rank assigns fixed values to classify
the user‚Äôs followers as a ‚Äúfriend‚Äù, or as ‚Äúnot a friend‚Äù. The
users who follow each other will receive a fixed score of 1.0,
and 0.5 other wise.

3.3

Statistical Model

The problem is modeled as shown in Figure 1. We build
a Logistic Regression model based on the feature matrix
extracted based on the tweets corresponding to the set of
training users.
Training dataset: The training data set is constructed
by considering many training tweets Q. The corresponding set of candidate tweet and hashtag pairs hCTx , CHx i is

Characteristics
Total number of users
Total number of originator users
Total Tweets Crawled
Tweets with Hashtags
Tweets without Hashtags
Tweets with exactly one Hashtag
Tweets with more than one Hashtag
Tweets with Favorites
Tweets with @mentions

Value
8,949
63
7,212,855
1,883,086
6,062,167
1,322,237
560,849
716,738
4,658,659

Percentage
N/A
N/A
100%
23.70%
76.30%
16.64%
7.06%
9.02%
58.63%

Table 1: Characteristics of the dataset used for the experiments

Figure 1: Training the Model from Tweets With Hashtags
to Predict the Hashtags for Tweets Without Hashtag
identified. Here, the candidate set of tweets are the tweets
from the timeline of the user Oy who posted the tweet Qx
containing the hashtag CHx . For each candidate tweet,
and candidate hashtag pair hCTxi , CHxj i created by user U
in the candidate tweet set, the feature scores are computed
with respect to the Qx , and user Oy .
The training dataset is a feature matrix containing the
feature vectors of all hCTxi , CHxj i pair corresponding to
all training tweets Q. The class label for a feature vector
is 1 if the hashtag CHxj in the candidate set of tweets is
equal to the hashtag in Qx , the tweet at consideration, and
0 otherwise.
Handling unbalanced training set: The training dataset
has a class distribution of 95% negative samples and 5%
positive samples. Learning the model from an unbalanced
dataset will cause very low precision. We use the Synthetic Minority Oversampling Technique (SMOTE) [2] to
re-sample the unbalanced dataset to a balanced dataset with
50% positive samples and 50% negative samples.
Classifier learning: We apply the Logistic regression to
learn a statistical model from the training dataset to predict
the probabilities of the top K most promising hashtags for
a given test tweet. Logistic regression assumes that all data
points share the same parameter vector with the test tweet.
Using the Classifier: For each test tweet, its candidate
set of tweet-hashtag pairs are tracked down and feature vectors are computed. When the test dataset is passed to the
learned model, it predicts the maximum likelihood probability for each of the candidate hashtags CHxj in tweet
hashtag pairs hCTxi , CHxj i corresponding to the test tweet.
The candidate hashtags with predicated class label as 1 are
then ranked using the probabilities.

4.

EXPERIMENTAL SETUP

Dataset: We use Sprintze [1] to crawl Twitter data
through the Twitter Streaming API. In order to crawl a

user‚Äôs timeline, the method can only return up to 3,200 of
a user‚Äôs most recent tweets from his timeline. The favorite
tweets that can be crawled are limited to 200 most recent
tweets per user.
We randomly picked users by navigating through the trending hashtags during a fixed time interval. For each of the
selected users, we crawled the most recent 1500 tweets, and
further crawled recent 1500 tweets for each friend (followee)
of the selected user. Since, the number of tweets crawled
to build a user‚Äôs social graph is directly proportional to the
number of friends, we randomly constrained the user selection process to choose users with at most 300 friends.
We crawled 7,212,855 tweets for 8,949 users. Further details about the characteristics of the dataset can be found
in Table2.

5.

EMPIRICAL EVALUATION

We present an internal and external evaluation of TweetSense. The testing dataset comprised of tweets that had
exactly one hashtag associated with it. The hashtag was
removed for the purpose of testing, and this served as the
ground truth for the test tweet.

5.1

External Evaluation

The closest related work for the problem of context recovery is the problem of recommending hashtags. Therefore,
we choose the system proposed by Eva et al. [7] as our
baseline. Their system aims at creating a more homogeneous set of hashtags by considering the similarity of tweet
text to create a candidate recommendation list. This candidate recommendation list is later refined using recently used
hashtags, and popularity of hashtags with in the candidate
recommendation list.
External Evaluation Of TweetSense Based On Precision at N :

Figure 2: External evaluation againt state-of-the-art system
for Precison @ N
Our system was able to recommend correct hashtags for
precision at 20 for 59% of the tweets, which in general is
above 50%. Also compared to the best possible ranking
method of the baseline model which could recommend cor-

rect hashtags for 35% of the tweets.On an average, TweetSense dominates the baselines for different values of N .
A user tweets about his interests and also about what
he is exposed to on his timeline. A user would rarely use a
hashtag, which he has never seen. There are many indicators
that indicate how a user adapts hashtags and most of these
are related to user‚Äôs social network. TweetSense picks the
most suitable set of hashtags as candidate hashtag set by
looking at the user‚Äôs timeline rather than at global Twitter
ecosystem. We have identified different features that can
further help in determining the most important indicator of
all the indicator by assuming the user‚Äôs environment at the
time of the creation of a tweet. These indicators change with
time, and we also model this by considering different set of
candidate hashtags for the same user for different tweets.

5.2

Internal Evaluation

The correctness of the system is evaluated using Precision
at N . We compare the importance of different features in
the model by using odds ratio.
Results of Internal Evaluation Of Precision at N
by Varying the Training Dataset: We compare the precision at N at 5,10,15, and 20 of the proposed system. Our
approach gets better precision as the size of N is increased.
For a total sample size of 1599 random tweets with hashtags
whose hashtags are deliberately removed for evaluation. At
the value of N = 5, 720/1599 sample tweets are recommended with the correct hashtags. Similarly, 849/1599 at
N = 10, 901/1599 at N = 15 and 944/1599 at N = 20 are
predicted correctly.

All Features
Similarity Score
Recency Score
Social Trend Score
Attention Score
Favorite Score
Mutual
Friends Score
Mutual
Followers Score
Common
Hashtag Score
Reciprocal Score

Results for Estimation of Odds Ratio by Feature
Selection: We measure the association between an exposure and an outcome using odds ratio. In the Table 2, Exp1
column indicates that ‚ÄúMutual Friends‚Äù feature is contributing the most to the odds of the outcome when compared to
the other features. This reinforces the hypothesis that the
social signals are more important than the tweet-content related features while predicting hashtags.
In order to validate whether the prediction capability of
the model is based solely on a single feature, we created a
model by ignoring the ‚ÄúMutual Friends‚Äù feature during the
training phase. In this case - Exp2, as shown in the Table
2, we can see that the ‚ÄúMutual Followers‚Äù feature becomes
very important. There could be a correlation between the
two features because of feature redundancy. In Exp3, we
remove most of the social features - ‚ÄúMutual Friends‚Äù, ‚ÄúMutual Followers‚Äù, and ‚ÄúReciprocal‚Äù features to build a model.
We can observe that the odds ratio of the features being
considered do not improve significantly. In Exp4, we ignore
all the features, but the ‚ÄúMutual Friends‚Äù feature to build
a model to further verify the importance of the ‚ÄúMutual

Exp2
0.1123
0.0024
0.0017
0
0.24

Exp3
0.1134
0.0026
0.0016
0
0.2112

Exp4
N/A
N/A
N/A
N/A
N/A

13538.65

N/A

N/A

0.2081

0.0923

3.115

N/A

N/A

0
0.7144

0
0.7717

0
N/A

N/A
N/A

Table 2: Estimation of Odds Ratio by Feature Selection
Friends‚Äù feature. As we could see, the score is low in this
case while it is higher when the model was built with all
other features. This indicates that we require all the other
features along with the ‚ÄúMutual Friends‚Äù feature to make
better predictions.
All these experiments emphasize the fact that social features rather than the tweet-content related features are the
most important features in recovering context of an orphan
tweet.
Results on Accuracy of Ranking based on Rank
Position: The accuracy on hashtags recommended by the
system is shown by determining the ranking positions of the
top 10 recommended hashtags for the test dataset.Results
for each ranking position as follows: Rank1-27.75%,Rank221.53%,Rank3-13.56%,Rank4-12.28%, Rank5-6.70%,Rank65.10%, Rank7-4.31%,Rank8-2.07%, Rank9-3.51%, Rank103.19%.The consistent performance by the system for the top
four positions of the top K ranking positions imply that the
system is more accurate.

6.

Figure 3: Precision at N = 5, 10, 15, and 20 on Varying the
Size of the Training Dataset.

Exp1
0.0942
0.0022
0.0017
0
0.2837

CONCLUSION

In this paper, we defined and motivated the context recovery problem from orphan tweets. We then described TweetSense a discriminative learning approach for recovering the
context of the orphan tweets in terms of their missing hashtags. TweetSense uses a variety of features drawn from the
timeline, content and social network. Our experiments on a
large tweet corpus demonstrate the effectiveness of TweetSense.
Acknowledgments: We gratefully acknowledge the significant help from Sushovan De in this research. This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13- 1-0176, N00014-13-10519 and N00014-15-1-2027, and a Google faculty research
award.

References
[1] Twitter‚Äùs streaming api ,https://blog.gnip.com/tag/spritzer/.
[2] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. arXiv preprint arXiv:1106.1813, 2011.
[3] W. Feng and J. Wang. We can learn your hashtags: Connecting tweets to explicit topics. In Data Engineering (ICDE), 2014 IEEE 30th International Conference
on, pages 856‚Äì867, March 2014.
[4] P. Jaccard. EÃÅtude comparative de la distribution florale dans une portion
des Alpes et des Jura. Bulletin del la SocieÃÅteÃÅ Vaudoise des Sciences Naturelles,
37:547‚Äì579, 1901.
[5] A. Java, X. Song, T. Finin, and B. Tseng.
Why we twitter: Understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop on Web Mining and Social Network Analysis,
WebKDD/SNA-KDD ‚Äô07, pages 56‚Äì65, New York, NY, USA, 2007. ACM.
[6] J. She and L. Chen. Tomoha: Topic model-based hashtag recommendation
on twitter. In Proceedings of the Companion Publication of the 23rd International
Conference on World Wide Web Companion, WWW Companion ‚Äô14, pages 371‚Äì
372, Republic and Canton of Geneva, Switzerland, 2014. International World
Wide Web Conferences Steering Committee.
[7] E. Zangerle, W. Gassler, and G. Specht. On the impact of text similarity functions on hashtag recommendations in microblogging environments.
Eva2013, 3(4):889‚Äì898, 2013.

Discovering Underlying Plans Based on Distributed Representations of Actions
Xin Tiana , Hankz Hankui Zhuoa & Subbarao Kambhampati b
Sun Yat-Sen University & a Arizona State University tianxin1860@gmail.com, zhuohank@mail.sysu.edu.cn, rao@asu.edu
a

arXiv:1511.05662v1 [cs.AI] 18 Nov 2015

Abstract
Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally "matching" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.

Introduction
As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning chellenge (c.f. (Cohen et al. 2015; Dong et al. 2004; Manikonda et al. 2014)). An important aspect of such a support (Kambhampati and Talamadupula 2015) is recognizing what plans the human in the loop is making, and provide appropriate suggestions about their next actions. Although there is a lot of work on plan recognition, much of it has traditionally depended on the availability of a complete domain model (Ram¥ irez and Geffner 2009a; Zhuo, Yang, and Kambhampati 2012). As has been argued elsewhere (Kambhampati and Talamadupula 2015), such models are hard to get in human-in-the-loop planning scenarios. Here, the decision support systems have to make themselves useful without insisting on complete action models of the domain. The situation here is akin to that faced by search engines and other tools for computer supported cooperate work, and is thus a significant departure for the "planning as pure inference" mindset of the automated planning community. As such, the
Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

problem calls for plan recognition with "shallow" models of the domain (c.f. (Kambhampati 2007)), that can be easily learned automatically. There has been very little work on learning such shallow models to support human-in-the-loop planning. Some examples include the work on Woogle system (Dong et al. 2004) that aimed to provide support to humans in web-service composition. That work however relied on very primitive understanding of the actions (web services in their case) that consisted merely of learning the input/output types of individual services. In this paper, we focus on learning more informative models that that can help recognize the plans under construction by the humans, and provide active support by suggesting relevant actions. To drive this process, we need to learn shallow models of the domain. We propose to adapt the recent successes of word-vector models (Mikolov et al. 2013) in language to our problem. Specifically, we assume that we have access to a corpus of previous plans that the human user has made. Viewing these plans as made up of action words, we learn word vector models for these actions. These models provide us a way to induce the distribution over the identity of each unobserved action. Given the distributions over individual unobserved actions, we use an expectationmaximization approach to infer the joint distribution over all unobserved actions. This distribution then forms the basis for action suggestions. We will present the details of our approach, and will also empirically demonstrate that it does capture a surprising amount of structure in the observed plan sequences, leading to effective plan recognition. We further compare its performance to traditional plan recognition techniques, including one that uses the same plan traces to learn the STRIPS-style action models, and use the learned model to support plan recognition.

Problem Definition
A plan library, denoted by L, is composed of a set of plans {p}, where p is a sequence of actions, i.e., p = a1 , a2 , . . . , an where ai , 1  i  n, is an action name (without any parameter) represented by a string. For example, a string unstack-A-B is an action meaning that a robot unstacks block A from block B. We denote the set of all posØ which is assumed to be known beforesible actions by A

hand. For ease of presentation, we assume that there is an empty action, ÿ, indicating an unknown or not observed acØ  {ÿ}. An observation of an unknown tion, i.e., A = A plan p ~ is denoted by O = o1 , o2 , . . . , oM , where oi  A, Ø or an empty action 1  i  M , is either an action in A ÿ indicating the corresponding action is missing or not observed. Note that p ~ is not necessarily in the plan library L, which makes the plan recognition problem more challenging, since matching the observation to the plan library will not work any more. We assume that the human is making a plan of at most length M . We also assume that at any given point, the planner is able to observe M - k of these actions. The k unobserved actions might either be in the suffiix (i.e., yet to be formed part) of the plan, or in the middle (due to observational gaps). Our aim is to suggest, for each of the k unobserved actions, m possible choices≠from which the user can select the action. (Note that we would like to keep m small, ideally close to 1, so as not to overwhelm the user) Accordingly, we will evaluate the effectiveness of the decision support in terms of whether or not the user's best/intended action is within the suggested m actions. Specifically, our recognition problem can be represented by a triple = (L, O, A). The solution to is to discover the unknown plan p ~ that best explains O given L and A. An example of our plan recognition problem in the blocks1 domain is shown below. Example: A plan library L in the blocks domain is assumed to have four plans as shown below: plan 1: pick-up-B stack-B-A pick-up-D stack-D-C plan 2: unstack-B-A put-down-B unstack-D-C put-downD plan 3: pick-up-B stack-B-A pick-up-C stack-C-B pickup-D stack-D-C plan 4: unstack-D-C put-down-D unstack-C-B put-downC unstack-B-A put-down-B An observation O of action sequence is shown below: observation: pick-up-B ÿ unstack-D-C put-down-D ÿ stack-C-B ÿ ÿ Given the above input, our DUP algorithm outputs plans as follows: pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C

Learning Vector Representations of Actions
Since actions are denoted by a name strings, actions can be viewed as words, and a plan can be viewed as a sentence. Furthermore, the plan library L can be seen as a corpus, and the set of all possible actions A is the vocabulary. We thus can learn the vector representations for actions using the Skip-gram model with hierarchical softmax, which has been shown an efficient method for learning high-quality vector representations of words from unstructured corpora (Mikolov et al. 2013). The objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document. Given a corpus C , composed of a sequence of training words w1 , w2 , . . . , wT , where T = |C|, the Skip-gram model maximizes the average log probability 1 T
T

log p(wt+j |wt )
t=1 -cj c,j =0

(1)

where c is the size of the training window or context. The basic probability p(wt+j |wt ) is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the K words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes (Mikolov et al. 2013). For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector representations for words. Instead, each inner node has an output vector vn(w,j ) , and the probability of a word being the output word is defined by
L(wt+j )-1

p(wt+j |wt ) =
i=1

 (I(n(wt+j , i + 1) = (2)

child(n(wt+j , i))) ∑ vn(wt+j ,i) ∑ vwt ) , where  (x) = 1/(1 + exp(-x)).

Our DUP Algorithm
Our DUP approach to the recognition problem functions by two phases. We first learn vector representations of actions using the plan library L. We then iteratively sample actions for unobserved actions oi by maximizing the probability of the unknown plan p ~ via the EM framework. We present DUP in detail in the following subsections.
1

L(w) is the length from the root to the word w in the binary tree, e.g., L(w) = 4 if there are four nodes from the root to w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) = root and n(w, L(w)) = w. child(n) is a fixed child (e.g., left child) of node n. vn is the vector representation of the inner node n. vwt is the input vector representation of word wt . The identity function I(x) is 1 if x is true; otherwise it is -1. We can thus build vector representations of actions by maximizing Equation (1) with corpora or plan libraries L as input. We will exploit the vector representations to discover the unknown plan p ~ in the next subsection.

Maximizing Probability of Unknown Plan p ~
With the vector representations learnt in the last subsection, a straightforward way to discover the unknown plan p ~ is to Ø such that p explore all possible actions in A ~ has the highest

http://www.cs.toronto.edu/aips2000/

probability, which can be defined similar to Equation (1), i.e.,
M

F (~ p) =
k=1 -cj c,j =0

log p(wk+j |wk )

(3)

where wk denotes the k th action of p ~ and M is the length of p ~. As we can see, this approach is exponentially hard with Ø and number of unobserved actions. respect to the size of A We thus design an approximate approach in the ExpectationMaximization framework to estimate an unknown plan p ~ that best explains the observation O. To do this, we introduce new parameters to capture "weights" of values for each unobserved action. Specifically speaking, assuming there are X unobserved actions in O, i.e., the number of ÿs in O is X , we denote these unobserved actions by a Ø1 , ..., a Øx , ..., a ØX , where the indices indicate the order they appear in O. Note that each a Øx can be any action Ø. We associate each possible value of a in A Øx with a weight, Øa Ø Ø denoted by  Øx ,x .  is a |A| ◊ X matrix, satisfying Ø o,x = 1   Ø o,x  0, 
Ø oA

An overview of our DUP algorithm is shown in Algorithm 1. In Step 2 of Algorithm 1, we initialize o,k = 1/M for Ø, if k is an index of unobserved actions in O; and all o  A Ø  o = o. otherwise, o,k = 1 and o ,k = 0 for all o  A In Step 4, we view ∑,k as a probability distribution, and Ø based on ∑,k if k is an unobserved sample an action from A action index in O. In Step 5, we only update ∑,k where k is an unobserved action index. In Step 6, we linearly project all elements of the updated  to between 0 and 1, such that we can do sampling directly based on  in Step 4. In Step 8, we simply select a Øx based on a Øx = arg max o,x ,
Ø oA

for all unobserved action index x. Algorithm 1 Framework of our DUP algorithm Input: plan library L, observed actions O Output: plan p ~ 1: learn vector representation of actions Ø, when k is an 2: initialize o,k with 1/M for all o  A unobserved action index 3: while the maximal number of repetitions is not reached do 4: sample unobserved actions in O based on  5: update  based on Equation (6) 6: project  to [0,1] 7: end while 8: select actions for unobserved actions with the largest weights in  9: return p ~

Ø to a for each x. For the ease of specification, we extend  Ø M , denoted by , such that bigger matrix with a size of |A|◊ Ø o,x if y is the index of the xth unobserved action in o,y =  Ø; otherwise, o,y = 1 and o ,y = 0 for all O, for all o  A Ø o  A  o = o. Our intuition is to estimate the unknown plan p ~ by selecting actions with the highest weights. We thus introduce the weights to Equation (2), as shown below,
L(wk+j )-1

p(wk+j |wk ) =
i=1

 (I(n(wk+j , i + 1) = (4)

child(n(wk+j , i))) ∑ avn(wk+j ,i) ∑ bvwk ) ,

Experiments
In this section, we evaluate our DUP algorithms in three planning domains from International Planning Competition, i.e., blocks1 , depots2 , and driverlog2 . To generate training and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems with a planning solver, such as FF3 , to produce 5000 plans. We then randomly divided the plans into ten folds, with 500 plans in each fold. We ran our DUP algorithm ten times to calculate an average of accuracies, each time with one fold for testing and the rest for training. In the testing data, we randomly removed actions from each testing plan (i.e., O) with a specific percentage  of the plan length. Features of datasets are shown in Table 1, where the second column is the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last column is the size of vocabulary used in all plans. We define the accuracy of our DUP algorithm as follows. For each unobserved action a Øx DUP suggests a set of possible actions Sx which have the highest value of a Øx ,x Ø. If Sx covers the truth action atruth , i.e., for all a Øx  A
2 http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume20/long03ahtml/JAIRIPC.html 3 https://fai.cs.uni-saarland.de/hoffmann/ff.html

where a = wk+j ,k+j and b = wk ,k . We can see that the impact of wk+j and wk is penalized by weights a and b if they are unobserved actions, and stays unchanged, otherwise (since both a and b equal to 1 if they are observed actions). We redefine the objective function as shown below,
M

F (~ p, ) =
k=1 -cj c,j =0

log p(wk+j |wk ),

(5)

where p(wk+j |wk ) is defined by Equation (4). The only parameters needed to be updated are , which can be easily done by gradient descent, as shown below, o,x = o,x +  F ,  o,x (6)

if x is the index of unobserved action in O; otherwise, o,x stays unchanged, i.e., o,x = 1. Note that  is a learning constant. With Equation (6), we can design an EM algorithm by repeatedly sampling an unknown plan according to  and updating  based on Equation (6) until reaching convergence (e.g., a constant number of repetitions is reached).

(a) blocks
0.8   0.7   0.6   0.8   0.7   0.6  

(b) depots
0.8   0.7   0.6  

(c) driverlog
DUP   MatchPlan   ARMS+PRP  

accuracy

accuracy

accuracy
DUP   MatchPlan   ARMS+PRP  
0.05   0.1   0.15   0.2   0.25  

0.5   0.4   0.3   0.2   0.1   0   0.05   0.1   0.15   0.2   0.25  

0.5   0.4   0.3   0.2   0.1   0  

0.5   0.4   0.3   0.2   0.1   0   0.05   0.1   0.15   0.2   0.25  

DUP   MatchPlan   ARMS+PRP  

percentage of unobserved actions

percentage of unobserved actions

percentage of unobserved actions

Figure 1: Accuracy with respect to different percentage of unobserved actions Table 1: Features of datasets domain blocks depots driverlog #plan 5000 5000 5000 #word 292250 209711 179621 #vocabulary 1250 2273 1441 extra information, i.e., initial state and goal of each plan in the plan library, to ARMS+PRP. In addition, PRP requires as input a set of candidate goals G for each plan to be recognized in the testing set, which was also generated and fed to PRP when testing. In summary, the hybrid plan recognition approach ARMS+PRP has more input information, i.e., initial states and goals in plan library and candidate goals G for each testing example, than our DUP approach.

atruth  Sx , we increase the number of correct suggestions g by 1. We thus define the accuracy acc as shown below: acc = 1 T
T

Accuracy w.r.t. Percentage of Unobserved Actions
We first evaluate our DUP algorithm with respect to different percentage of unobserved actions  in O. We set the window of training context c in Equation (1) to be three and the size of recommendations to be ten. We compare our DUP algorithm to both MatchPlan and ARMS+PRP. To make fair comparison (to MatchPlan), we set the matching window MatchPlan to be three as well when searching plans from plan libraries L. In other words, to estimate an unobserved action a Øx in O, MatchPlan matches previous three actions and subsequent three actions of a Øx to plans in L, and recommends ten actions with maximal number of matched actions, considering unobserved actions (ÿ in the context of a Øx ) and actions in L as a successful matching. For ARMS+PRP, we generated 20 candidate goals for each testing example including the ground-truth goal which corresponds to the ground-truth plan to be recognized. The results are shown in Figure 1. From Figure 1, we can see that in all three domains, the accuracy of our DUP algorithm is generally higher than MatchPlan and ARMS+PRP, which verifies that our DUP algorithm can indeed capture relations among actions better than previous matching approaches. The rationale is that we explore global plan information from the plan library to learn a "shallow" model (distributed representations of actions) and use this model with global information to best

i=1

# correct-suggestions i , Ki

where T is the size of testing set, # correct-suggestions i is the number of correct suggestions for the ith testing plan, Ki is the number of unobserved actions in the ith testing plan. We can see that the accuracy acc may be influenced by Sx . We will test different size of Sx in the experiment. State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries to best explain the observed actions (Geib and Steedman 2007), which we denote by MatchPlan. We develop a MatchPlan system based on the idea of (Geib and Steedman 2007) and compare our DUP algorithm to MatchPlan with respect to different percentage of unobserved actions  and different size of suggestion set Sx . Another baseline is action-models based plan recognition approach (Ramirez and Geffner 2009b) (denoted by PRP, short for Plan Recognition as Planning). Since we do not have action models as input in our DUP algorithm, we exploited the action model learning system ARMS (Yang, Wu, and Jiang 2007) to learn action models from the plan library and feed the action models to the PRP approach. We call this hybrid plan recognition approach ARMS+PRP. To learn action models, ARMS requires state information of plans as input. We thus added

(a) blocks
0.8   0.7   0.6  

(b) depots
0.8   0.7   0.6  
0.8   0.7   0.6  

(c) driverlog

accuracy

accuracy

accuracy

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

0.5   0.4   0.3   0.2   0.1   0   1   2   3   4   5   6   7   8   9   10  

size of recommendations
DUP  

size  of  recommenda9ons  
MatchPlan   ARMS+PRP  

size of recommendations

Figure 2: Accuracy with respect to different size of recommendations explain the observed actions. In contrast, MatchPlan just utilizes local plan information when matching the observed actions to the plan library which results in lower accuracies. Although ARMS+PRP tries to leverage global plan information from the plan library to learn action models and uses the models to recognize observed actions, it enforces itself to extract "exact" models represented by planning models which are often with noise. When feeding those noisy models to PRP, since PRP that uses planning techniques to recognize plans is very sensitive to noise of planning models, the recognition accuracy is lower than DUP, even though ARMS+PRP has more input information (i.e., initial states and candidate goals) than our DUP algorithm. Looking at the changes of accuracies with respect to the percentage of unobserved actions, we can see that our DUP algorithm performs fairly well even when the percentage of unobserved action reaches 25%. In contrast, ARMS+PRP is sensitive to the percentage of unobserved actions, i.e., the accuracy goes down when more actions are unobserved. This is because the noise of planning models induces more uncertain information, which harms the recognition accuracy, when the percentage of unobserved actions becomes larger. Comparing accuracies of different domains, we can see that our DUP algorithm functions better in the blocks domain than the other two domains. This is because the ratio of #word over #vocabulary in the blocks domain is much larger than the other two domains, as shown in Table 1. We would conjecture that increasing the ratio could improve the accuracy of DUP. ARMS+PRP, the number of candidate goals for each testing example is set to 20. ARMS+PRP aims to recognize plans that are optimal with respect to the cost of actions. We relax ARMS+PRP to output |Sx | optimal plans, some of which might be suboptimal. We varied the number of actions recommended by DUP (or MatchPlan) from 1 to 10. The results are shown in Figure 2. From Figure 2, we find that accuracies of the three approaches generally become larger when the size of the recommended action set increases in all three domains. This is consistent with our intuition, since the larger the recommended action set is, the higher the possibility for the truth action to be in the recommended action set. We can also see that the accuracy of our DUP algorithm are generally larger than both MatchPlan and ARMS+PRP in all three domains, which verifies that our DUP algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the matching approach MatchPlan and the planning model learning approach ARMS+PRP. The reason is similar to the one given for Figure 1 in the previous section. That is, the "shadow" model learnt by our DUP algorithm is better for recognizing plans than both the "exact" planning model learnt by ARMS for recognizing plans with planning techniques and the local matching approach MatchPlan. On the other hand, we can also see the accuracy of ARMS+PRP is generally higher than MatchPlan. This verifies that the additional information of initial states and candidate goals exploited by ARMS+PRP can indeed help improve the accuracy. Furthermore, the advantage of DUP becomes even larger when the size of recommended action set increases, which suggests our vector representation based learning approach can better capture action relations when the size of recommended action set is larger. The possibility of actions correctly recognized by DUP becomes much larger than the other two approaches when the size of recommendations increases.

Accuracy w.r.t. Size of Recommendation Set
We next evaluate the performance of our DUP algorithm with respect to the size of recommendation set Sx . Likewise, we set the context window c used in Equation (1) to be three, which was also set when matching the observed actions O to plan libraries L in the MatchPlan approach. For

Related work
Kautz and Allen proposed an approach to recognizing plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an "action grammar" (Kautz and Allen 1986). All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events. Lesh and Etzioni further presented methods in scaling up activity recognition to scale up his work computationally (Lesh and Etzioni 1995). They automatically constructed plan-library from domain primitives, which was different from (Kautz and Allen 1986) where the plan library was explicitly represented. In these approaches, the problem of combinatorial explosion of plan execution models impedes its application to real-world domains. Kabanza and Filion (Kabanza et al. 2013) proposed an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted model counting. These approaches are, however, difficult to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. Instead of using a library of plans, Ramirez and Geffner (Ramirez and Geffner 2009b) proposed an approach to solving the plan recognition problem using slightly modified planning algorithms, assuming the action models were given as input. Except previous work (Kautz and Allen 1986; Bui 2003; Geib and Goldman 2009; Ramirez and Geffner 2009b) on the plan recognition problem presented in the introduction section, Note that action models can be created by experts or learnt by previous systems, such as ARMS (Yang, Wu, and Jiang 2007) and LAMP (Zhuo et al. 2010). Saria and Mahadevan presented a hierarchical multi-agent markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems (Saria and Mahadevan 2004). Singla and Mooney proposed an approach to abductive reasoning using a first-order probabilistic logic to recognize plans (Singla and Mooney 2011). Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories (Amir and Gal 2011). Ramirez and Geffner exploited offthe-shelf classical planners to recognize probabilistic plans (Ramirez and Geffner 2010). Early work on human-in-the-loop planning scenarios in automated planning went under the name of "mixedinitiative planning" (e.g. (Ferguson, Allen, and Miller 1996)). An important limitation of that work was that the humans in the loop were helping the automated planner (with a complete action model) navigate its search space of plans more efficiently. In contrast, we are interested in planning technology that helping humans develop plans, even in the absence of complete formal models of the planning domain. While some work in web-service composition (c.f. (Dong et al. 2004)) did focus on this type of planning support, they were hobbled by being limited to simple input/output

type comparison. In contrast, we believe that DUP learns and uses a model that captures more of the structure of the planning domain (while still not insisting on complete action models). While DUP focuses on learning models from plan corpora, some recent work looked at using crowdsourcing to acquire domain models. For example, Lasecki et al. (Lasecki et al. 2013) introduce Legion:AR, which combines the benefits of automatic and human activity labeling for robust and deployable activity recognition. The system exploits an active learning approach (Zhao, Sukthankar, and Sukthankar 2011) in which automatic activity recognition is augmented with on-demand activity labels from the crowd when an observed activity cannot be confidently classified. By engaging a group of people, Legion:AR is able to label activities as they occur more reliably than a single person can, especially in complex domains with multiple actors performing activities quickly. Lasecki et al. (Lasecki et al. 2014) built a crowdsourcing based system called ARchitect, using the crowd to capture the dependency structure of the actions that make up activities. Such crowd-sourcing methods can complement the plan-corpus based approach proposed in DUP.

Conclusion and Discussion
In this paper we present a novel plan recognition approach DUP based on vector representation of actions. We first learn the vector representations of actions from plan libraries using the Skip-gram model which has been demonstrated to be effective. We then discover unobserved actions with the vector representations by repeatedly sampling actions and optimizing the probability of potential plans to be recognized. We also empirically exhibit the effectiveness of our approach. While we focused on a one-shot recognition task in this paper, in practice, human-in-the-loop planning will consist of multiple iterations, with DUP recognizing the plan and suggesting action addition alternatives; the human making a selection and revising the plan. The aim is to provide a form of flexible plan completion tool, akin to auto-completers for search engine queries. To do this efficiently, we need to make the DUP recognition algorithm "incremental." The word-vector based domain model we developed in this paper provides interesting contrasts to the standard precondition and effect based action models used in automated planning community. One of our future aims is to provide a more systematic comparison of the tradeoffs offered by these models. Although we have focused on the "plan recognition" aspects of this model until now, and assumed that "planning support" will be limited to suggesting potential actions to the humans. In future, we will also consider "critiquing" the plans being generated by the humans (e.g. detecting that an action introduced by the human is not consistent with the model learned by DUP), and "explaining/justifying" the suggestions generated by humans. Here, we cannot expect causal explanations of the sorts that can be generated with the help of complete action models (e.g. (Petrie 1992)), and will have to develop justifications analogous to those used in recommendation systems.

References
[Amir and Gal 2011] Amir, O., and Gal, Y. K. 2011. Plan recognition in virtual laboratories. In Proceedings of IJCAI, 2392≠2397. [Bui 2003] Bui, H. H. 2003. A general model for online probabilistic plan recognition. In Proceedings of IJCAI, 1309≠1318. [Cohen et al. 2015] Cohen, P. R.; Kaiser, E. C.; Buchanan, M. C.; Lind, S.; Corrigan, M. J.; and Wesson, R. M. 2015. Sketch-thru-plan: a multimodal interface for command and control. Commun. ACM 58(4):56≠65. [Dong et al. 2004] Dong, X.; Halevy, A. Y.; Madhavan, J.; Nemes, E.; and Zhang, J. 2004. Simlarity search for web services. In (e)Proceedings of the Thirtieth International Conference on Very Large Data Bases, Toronto, Canada, August 31 - September 3 2004, 372≠383. [Ferguson, Allen, and Miller 1996] Ferguson, G.; Allen, J.; and Miller, B. 1996. Trains-95: Towards a mixed-initiative planning assistant. In Proceedings of the Third Conference on Artificial Intelligence Planning Systems (AIPS-96), 70≠ 77. Edinburgh, Scotland. [Geib and Goldman 2009] Geib, C. W., and Goldman, R. P. 2009. A probabilistic plan recognition algorithm based on plan tree grammars. Artificial Intelligence 173(11):1101≠ 1132. [Geib and Steedman 2007] Geib, C. W., and Steedman, M. 2007. On natural language processing and plan recognition. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, 1612≠1617. [Kabanza et al. 2013] Kabanza, F.; Filion, J.; Benaskeur, A. R.; and Irandoust, H. 2013. Controlling the hypothesis space in probabilistic plan recognition. In IJCAI. [Kambhampati and Talamadupula 2015] Kambhampati, S., and Talamadupula, K. 2015. Human-in-the-loop planning and decision support. rakaposhi.eas.asu.edu/hilp-tutorial. [Kambhampati 2007] Kambhampati, S. 2007. Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, 1601≠1605. [Kautz and Allen 1986] Kautz, H. A., and Allen, J. F. 1986. Generalized plan recognition. In Proceedings of AAAI, 32≠ 37. [Lasecki et al. 2013] Lasecki, W. S.; Song, Y. C.; Kautz, H. A.; and Bigham, J. P. 2013. Real-time crowd labeling for deployable activity recognition. In CSCW, 1203≠1212. [Lasecki et al. 2014] Lasecki, W. S.; Weingard, L.; Ferguson, G.; and Bigham, J. P. 2014. Finding dependencies between actions using the crowd. In Proceedings of CHI, 3095≠3098. [Lesh and Etzioni 1995] Lesh, N., and Etzioni, O. 1995. A sound and fast goal recognizer. In IJCAI, 1704≠1710. [Manikonda et al. 2014] Manikonda, L.; Chakraborti, T.; De, S.; Talamadupula, K.; and Kambhampati, S. 2014. AI-MIX:

using automated planning to steer human workers towards better crowdsourced plans. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu¥ ebec City, Qu¥ ebec, Canada., 3004≠3009. [Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111≠3119. [Petrie 1992] Petrie, C. J. 1992. Constrained decision revision. In Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16, 1992., 393≠ 400. [Ram¥ irez and Geffner 2009a] Ram¥ irez, M., and Geffner, H. 2009a. Plan recognition as planning. In IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17, 2009, 1778≠1783. [Ramirez and Geffner 2009b] Ramirez, M., and Geffner, H. 2009b. Plan recognition as planning. In Proceedings of IJCAI, 1778≠1783. [Ramirez and Geffner 2010] Ramirez, M., and Geffner, H. 2010. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of AAAI, 1121≠1126. [Saria and Mahadevan 2004] Saria, S., and Mahadevan, S. 2004. Probabilistic plan recognitionin multiagent systems. In Proceedings of AAAI. [Singla and Mooney 2011] Singla, P., and Mooney, R. 2011. Abductive markov logic for plan recognition. In Proceedings of AAAI, 1069≠1075. [Yang, Wu, and Jiang 2007] Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence Journal 171:107≠143. [Zhao, Sukthankar, and Sukthankar 2011] Zhao, L.; Sukthankar, G.; and Sukthankar, R. 2011. Robust active learning using crowdsourced annotations for activity recognition. In AAAI workshop. [Zhuo et al. 2010] Zhuo, H. H.; Yang, Q.; Hu, D. H.; and Li, L. 2010. Learning complex action models with quantifiers and implications. Artificial Intelligence 174(18):1540≠1569. [Zhuo, Yang, and Kambhampati 2012] Zhuo, H. H.; Yang, Q.; and Kambhampati, S. 2012. Action-model based multiagent plan recognition. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., 377≠385.

Proactive Decision Support using Automated Planning
Satya Gautam Vadlamudi, Tathagata Chakraborti, Yu Zhang, Subbarao Kambhampati {gautam,tchakra2,Yu.Zhang.442,rao}@asu.edu, Arizona State University, Tempe, AZ Proactive decision support (PDS) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support, since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain. Human-in-the-loop planning (HILP) is a necessary requirement today in many complex decision making or planning environments. In this paper we consider the case of HILP where the human(s) responsible for making the decisions in complex scenarios are supported by automated planning systems. Thus the planners in this scenario are the humans themselves, and we investigate the role of an automated planner in their deliberative process. This is, in effect, a role reversal of the traditional notion of the humanplanner interaction in mixed initiative planning; and we refer to the proposed system as a reverse mixed initiative planner. These systems are capable of providing plans or course-ofactions (COAs) when a model of the world where the plans are to be executed is given to them, along with the knowledge of the initial state and the list of goals to be achieved/tasks to be accomplished. Examples where such technologies can be helpful include disaster response strategies from the navy, or responses to fire or emergency from local law enforcement. Providing a complete model of the world where the plans are to be executed is, however, known to be very difficult (Kambhampati, 2007). This implies that the system generated plan cannot be completely relied upon. Not only executing such plans may no longer accomplish the goals/tasks provided, but also their execution may result in undesired consequences. This calls for active participation from the human in the loop rather than simply adopt a system generated plan. Furthermore, in many cases, the human in the loop may be held responsible for the plan under execution and its results. Therefore, it is also necessary in such cases that the human keeps control of the plan/COA being given for execution. This motivates us to build a proactive decision support system, which is context-sensitive and focuses on aiding and alerting the human in the loop with his/her decisions rather than generate a static COA that may not work in the dynamic worlds that the plan has to execute in. In this paper, we propose a proactive decision support (PDS) system, named RADAR, using automated planning technology, which is augmentable, context sensitive, controllable and adaptive to the human's decisions. It supports the human in the loop through suggestions and alerts, which can be considered by the human as he/she sees fit. In the following we explain the meaning of the above terms: ∑ Augmentable: The model of the world such as the rules that specify what are the preconditions for a particular action/decision and what would be the effects of that action/decision could be added/modified. The state of the world such as the values of various variables and availabilities of various resources could be updated. The list of goals/tasks can also be updated. Context-sensitive: Whenever the model or the state of the world is augmented either by the human or by some other source internal/external to the PDS system, the system takes the new context into account and responds with updated suggestions and alerts. Controllable: The decision making process of the PDS system is completely controllable by the human in the loop, who retains with the decision making power wherein he/she can either choose to follow the system generated suggestions or make a different decision as they see fit. Adaptive: Since the decision making power lies with the human in the loop, the PDS system has to adapt itself to the decisions being made by the human and provide new suggestions and alerts that are relevant based on the decisions of the humans. Note that, adaptive nature may also be viewed as part of context-sensitivity in the sense that the context changes whenever decisions are made. In this paper, we keep the distinction to differentiate the changes in the world and tasks, and the changes related to the actions prescribed/decisions of the human in the loop.

∑

∑

∑

As mentioned before, our proactive decision support system uses automated planning technology widely studied in the field of Artificial Intelligence. In particular, we adopted the Planning Domain Description Language (PDDL) to describe the model of the world, details of the current state (context) and the goals, to the system. Then, we used the existing landmark generation method (Hoffmann et al., 2004) to generate landmarks, which are then analyzed to come up with relevant suggestions and alerts. Landmarks are those set of states (of the world) that has to be visited by any successful plan that achieves goals from the current state.

We implemented our proposed system using the Fast Downward planner (Helmert, 2006) and tested it on a firefighting domain, where search-and-rescue missions are to be carried out (goals). We also conducted some preliminary human factor studies to evaluate the utility of the above proposal, which gave positive feedback. RELATED WORK The proposed proactive decision support system supports naturalistic decision making (Zsambok and Klein, 2014; Klein, 2008), which is acknowledged as a necessary element in PDS systems (Morrison et al., 2013). Systems which do not support naturalistic decision making have been found to have detrimental impact on work flow causing frustration to decision makers (Feigh et al., 2007). In (Parasuraman, 2000), a study of human performance consequences of different levels and types of automation is provided, where aspects such as, mental workload and situation awareness are considered as evaluative criteria. A model for types and levels of automation that provides an objective basis for deciding which system functions should be automated and to what extent is given in (Parasuraman et al., 2000). (Parasuraman and Manzey, 2010) shows that human use of automation may result in automation bias leading to omission and commission errors, which underlines the importance of reliability of the automation (Parasuraman and Riley, 1997). Various elements of human-automation interaction such as, adaptive nature and context sensitivity are presented in (Sheridan and Parasuraman, 2005). (Warm et al., 2008) show that vigilance requires hard mental work and is stressful via converging evidence from behavioral, neural and subjective measures. Our system could be considered as a part of such vigilance support thereby reducing the stress for human in the loop. High-level information fusion that characterizes complex situations and that support planning of effective responses is considered the greatest need in crisis-response situations (Laskey et al., 2016). Automated planning based proactive support systems were shown to be preferred by humans in studies involving human-robot teaming (Zhang et al., 2015) and the cognitive load of the subjects involved was observed to have been reduced (Narayanan et al., 2015). PROPOSED PROACTIVE DECISION SUPPORT SYSTEM: RADAR Now, we present the proposed proactive decision support (PDS) system ≠ RADAR, based on automated planning technology. First, we present the different elements of the PDS system and then briefly present the details of the methodology behind the generation of suggestions and alerts. The various elements of the planning PDS system are: ∑ Tasks/goals: The tasks or goals to be accomplished clearly form an important and necessary element. ∑ State/context, resources: The current state or context is needed for the PDS system to produce relevant alerts. The availability of resources also forms part of the context which we separately indicated to display as an important constituent of the Context.

Model, actions/decisions: The model consists of set of rules which are applicable in the world where the plan is being executed. Actions, for example, are part of a model, which give information about when a particular action is applicable (what are the preconditions to be satisfied in order for it to be applicable), and what would be the effects of taking that action (how it would impact various elements of the world). Actions are also closely tied to decisions that need to be made since each decision typically corresponds to certain action being taken. ∑ Current plan/course of action (COA): The information about current plan of the human in the loop, if any, can help the PDS system produce better suggestions and alerts by reducing the uncertainty. However, this could just consist of actions already taken, in which case the proposed PDS system can come up with relevant alerts pertaining to the future. More details on this are given next. Now, we briefly present details on how the proposed planning based system with above elements produces relevant suggestions and alerts. In order to explain this, first we need to define what are called Landmarks, which are central to the suggestions and alerts system. ∑ Definition: Landmarks. (Hoffmann et al., 2004) A state/partial state is a landmark (for the current state, tasks/goals, and model) if all plans/course-of-actions that can accomplish the tasks from the current state must go through that state/partial state during their execution. Note that, all goal states are trivial landmarks since they have to be accomplished by all successful plans. Consider there exists only one state, A, which can take one to the goal state(s), meaning, all plans have to visit A in order to accomplish the goals, making it a landmark (derived; nontrivial). Further, if there exist two states A and B through which the goal state(s) must be reached, then either A or B must be visited before accomplishing the tasks/goals. In such a scenario, A or B can be called as a landmark, in particular, a disjunctive landmark. Continuing this process, one can derive recursively the set of all landmarks starting from the goal state(s) leading back to the current state. Generating Suggestions and Alerts (PDS) Now, we present the details on how generating and analyzing the landmarks can help in producing suggestions and alerts. Note that, since the landmarks are the states that must be visited in order to accomplish the goals, if there is no possible way of reaching a certain landmark generated above, then the system can generate an alert conveying that the goal cannot be accomplished. This could be because there is no action available, which would help in visiting the landmark under consideration, or the preconditions of actions that can help reaching the landmark are not satisfied. In some cases, the preconditions, which are not currently satisfied for an action to be applicable, may be because of resource constraints. In such cases, the system instead generates a suggestion mentioning that those resources are needed in order to accomplish the task. For example, in order

Figure 1 ≠ RADAR interface showing data support and decision support for the human commanders making plans. for an action such as put-off-fire to be applicable, a precondition on the availability of the resource: fire-engine need to be satisfied. Failing which an alert may be generated. More details are described through a case study next. Further, provision to update the model, state, and resources is provided to make the system augmentable. In order to support context-sensitivity and adaptive nature, we re-execute the alerts generation method whenever there is a change in the context/tasks or new action is executed or plan is changed, so that the suggestions/alerts become relevant to the situation. Case Study: Fire-fighting Domain We use a fire-fighting scenario to illustrate the ideas expressed so far, as shown in Figure 1. The scenario plays out in a particular location (we use Tempe in the example) and involves the local fire chief, police, medical and transport authorities, who try to build a plan in response to the fire in the given platform (which is augmented with decision support capabilities from an automated planner). The left pane gives event updates that the commanders can incorporate into the Tasks panel at the top, which shows what high level goals or tasks needs to be addressed. The panel on the right (currently empty) will display the plan being constructed. Each of the human commanders have access to the resources that they can use to control the fire outbreak (as can be seen from the table at the bottom of Figure 1). For example, the police can deploy police cars and policemen, and the fire chief can deploy fire engines, ladders, rescuers, etc. The plans that can be produced by the commanders, of course, depend on the availability of these resources, and certain actions can only be executed when the required number of resources are available or the preconditions are satisfied. For example, in order to dispatch police cars from a particular police station, the police chief needs to make sure that the respective police station has enough police cars and it has been notified of the demand previously. Given this knowledge, the automated planner integrated into the system keeps an eye on the planning process of the human commanders. The three panels in the middle of Figure 1 provide these functionalities. The one on the left provides a way to add or request for resources in case of insufficient resources, while the one on the right provides suggested actions that the commanders may use to complete their plans. The panel in the middle is the most important part of the automated component where it produces alerts or suggestions to problems in the current plan or with respect to problems that may appear in future given the current state and availability of resources. In the following we will go through two use cases to illustrate how the system responds to situations as per the guidelines we discussed in the introduction ≠ Scenario I: (see Figure 2) 1) The scenario starts with a small fire in a building. Once selected from suggested tasks, this populates the task chosen to be addressed. 2) The planning model does landmark analysis on the current state and immediately populates the alerts panel with an alert saying either big fire engines or small fire engines are needed to put out the fire (a disjunctive landmark). 3) The commander tries to dispatch big engines now, but is stopped by the system which detects that there are not enough resources (big engines). 4) The commander addresses the shortage by requesting additional engines from the left. 5) Now the commander can proceed with and finish the plan until the fire has been extinguished.

Note that the above examples are illustrative and only intended for understanding the underlying concepts of the proposed system. The same system can handle scenarios where hundreds of actions and variables are involved, where it becomes nearly impossible for a human to account for all possible drawbacks. Furthermore, any other domain (say, disaster response) can be readily handled by the implemented system, by just changing the PDDL domain file used by it without any renewed effort.

Figure 2 ≠ Use case illustrating automated decision support using disjunctive landmarks. Scenario II: (see Figure 3) 1) The scenario now starts with a big fire. This calls for big engines as alerted by the system. Note that it also alerts for insufficient rescuers, which did not happen in the previous case, as the model used conveys that rescuers are needed only in case of a big fire. 2) The commander once again requests for additional resources (big engines as well as rescuers) and was able to generate a feasible plan as shown. In this way the system is able to assist the human commander in his planning process. The system is augmentable in the way it supports the commander's goal preferences and world state information. It is context-sensitive in how it provides relevant alerts based on the stage of the plan, and adaptive with respect to the choice taken by the human in the loop to address these alerts. Finally, the entire process is controllable because the human commander has authority over the choices at all times. Figure 3 ≠ Use case illustrating how automated decision support adapts in response to a necessary landmark. EVALUATIONS WITH HUMANS ON UTILITY In order to assess the utility of the proposed proactive decision support (PDS) system, RADAR, we have conducted a preliminary survey of its usefulness on 7 subjects on the questions given below. The system would be enhanced and refined continuously as we incorporate more and more features in our PDS system. We also plan to take the learning from the user feedback back into the design of the PDS system during the process. We discuss the responses obtained for each of the questions, next to it. 1) Do you think the suggestions & alerts are relevant to the task/goal? Yes/No

7 out of 7 subjects have answered it Yes. This suggests that the domain shown (fire-fighting) has been modeled well so that relevant landmarks could be generated. 2) Do you think the suggestions & alerts are context-sensitive (current state & current plan)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the context sensitivity of the landmarks make a good fit for use in PDS systems. 3) Do you think that the suggestions & alerts are dynamic (change with changing context/plan)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the regeneration of suggestions & alerts whenever context changes is notable to the users. 4) Do you think the suggestions & alerts increased your situational awareness (e.g. resources available, status of execution)? Yes/No 7 out of 7 subjects have answered it Yes. This suggests that the suggestions and alerts continuously improve the situational awareness of the human in the loop, particularly in terms of the critical points relevant to the plan/COA. 5) Do you think that the suggestions & alerts interfere with the ability to interact with the system? Yes/No 1 out of 7 subjects have answered it Yes. This suggests that the alerts are displayed without interfering with the user interaction experience in most cases, and may be improved. 6) Do you think the suggestions & alerts helped in debugging the plan or would you rather execute and replan in case of failure? Former/Latter 6 out of 7 subjects have answered that the suggestions and alerts helped. This suggests that the users prefer to be alerted in advance rather than re-plan in most of the cases. 7) Do you think the suggestions & alerts should be an error and stop the plan from being dispatched, or should it be a warning and let you proceed with execution? Former/Latter 4 out of 7 subjects have answered that the alerts can be shown as errors and stop the execution whereas others preferred them as warnings that allow execution to move forward. In this case, there is a split amongst the users as to whether the user should be able to proceed with warnings or be stopped completely. Here there is an opportunity to amend the system so as to learn the cases where one could proceed despite the error/warning, and incorporate it as a soft constraint in the future leading to only a warning. 8) Do you feel that you can be in control of the decision making/planning process when using the proposed PDS system? Yes/No 6 out of 7 subjects have answered it Yes. This suggests that most of the users feel in control of the decision making process rather than being forced by the system. 9) On a scale of 1-10 how much would you rate your satisfaction with the proactive decision support capabilities of the system? 1-10 This received an average score of 7.14. This suggests that the users had a good first experience with the proposed PDS system and would like to see new and improved features. 10) On a scale of 1-10 how much would you recommend using the proposed proactive decision support platform? 1-10 This received an average score of 7.28. This suggests that the users are open to recommending the PDS system to others.

Overall, the PDS system was well received and perceived to be promising. Users have appreciated the current features and suggested minor modifications. As part of the future work, we consider addressing several important aspects such as: Where do we get the models? Can we automatically learn them from observing the users and the contexts? How do we deal with incomplete models? Does the human in the loop deviate from cost-optimal plans? How to understand the preferences of the human in the loop and how to address them? And so on. CONCLUSION We presented a Proactive decision support (PDS) framework called RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts and suggestions on potential drawbacks in the plan and resource constraints. This was achieved by generating and analyzing the landmarks that must be accomplished by any successful plan before achieving the goals. The proposed approach is aligned with the concept of naturalistic decision making. We demonstrated the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain and human factors studies. ACKNOWLEDGMENTS This research is supported in part by the ONR grant N0001415-1-2027. We thank Vivek Dondeti for his help with the implementation of parts of the RADAR system. REFERENCES
Feigh, K. M., Pritchett, A. R., Denq, T. W., & Jacko, J. A. (2007). Contextual Control Modes During an Airline Rescheduling Task. Journal of Cognitive Engineering and Decision Making, 1(2), 169-185. Helmert, M. (2006). The Fast Downward Planning System. J. Artif. Intell. Res.(JAIR), 26, 191-246. Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. J. Artif. Intell. Res.(JAIR), 22, 215-278. Kambhampati, S. (2007, July). Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models. InProceedings of the National Conference on Artificial Intelligence (Vol. 22, No. 2, p. 1601). Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999. Klein, G. (2008). Naturalistic decision making. Human Factors: The Journal of the Human Factors and Ergonomics Society, 50(3), 456-460. Laskey, K. B., Marques, H. C., & da Costa, P. C. (2016). High-Level Fusion for Crisis Response Planning. In Fusion Methodologies in Crisis Management (pp. 257-285). Springer International Publishing. Morrison J. G., Feigh K. M., Smallman H. S., Burns C. M., Moore K. E. (2013). The Quest For Anticipatory Decision Support Systems (Panel). Human Factors and Ergonomics Society Annual Meeting. Narayanan, V., Zhang, Y., Mendoza, N., & Kambhampati, S. (2015, March). Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote Human-Robot Interaction. In HRI (Extended Abstracts) (pp. 161-162). Parasuraman, R. (2000). Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7), 931-951. Parasuraman, R., & Manzey, D. H. (2010). Complacency and bias in human use of automation: An attentional integration. Human Factors: The Journal of the Human Factors and Ergonomics Society, 52(3), 381-410. Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. Human Factors: The Journal of the Human Factors and Ergonomics Society, 39(2), 230-253. Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 30(3), 286-297. Sheridan, T. B., & Parasuraman, R. (2005). Human-automation interaction.Reviews of human factors and ergonomics, 1(1), 89-129.

Warm, J. S., Parasuraman, R., & Matthews, G. (2008). Vigilance requires hard mental work and is stressful. Human Factors: The Journal of the Human Factors and Ergonomics Society, 50(3), 433-441. Zhang, Y., Narayanan, V., Chakraborti, T., & Kambhampati, S. (2015, September). A human factors analysis of proactive support in human-robot teaming. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on (pp. 3586-3593). IEEE. Zsambok, C. E., & Klein, G. (2014). Naturalistic decision making. Psychology Press.

Planning with Resource Conflicts
in Human-Robot Cohabitation
Tathagata Chakraborti1 , Yu Zhang1 , David E. Smith2 , Subbarao Kambhampati1

2

1
Department of Computer Science, Arizona State University, AZ
Autonomous Systems & Robotics Intelligent Systems Division, NASA Ames Research Center, CA

tchakra2@asu.edu, yzhan442@asu.edu, david.smith@nasa.gov, rao@asu.edu
ABSTRACT

tiation. Specifically, we ask the question, what information
can be extracted from the predicted plans, and how this
information can be used to guide the behavior of the autonomous agent. There has been previous work [1, 6] on
some of the modeling aspects of the problem, in terms of
planning with uncertainty in resources and constraints. In
this paper we provide an integrated framework (shown in
Figure 1) for achieving these behaviors of the autonomous
agent, particularly in the context of stigmergic coordination
of human-robot cohabitation. To this end, we modularize
our architecture so as to handle the uncertainty in the environment separately with the planning process, and show
how these individual modules interact with each other by
the way of usage profiles of the concerned resources.

In order to be acceptable members of future human-robot
ecosystems, it is necessary for autonomous agents to be respectful of the intentions of humans cohabiting a workspace
and account for conflicts on shared resources in the environment. In this paper we build an integrated system
that demonstrates how maintaining predictive models of its
human colleagues can inform the planning process of the
robotic agent. We propose an Integer Programming based
planner as a general formulation of this flavor of ‚Äúhumanaware‚Äù planning and show how the proposed formulation
can be used to produce different behaviors of the robotic
agent, showcasing compromise, opportunism or negotiation.
Finally, we investigate how the proposed approach scales
with the different parameters involved, and provide empirical evaluations to illustrate the pros and cons associated
with the proposed style of planning.

1.

INTRODUCTION

In environments where multiple agents are working independently, but utilizing shared resources, it is important for
these agents to model the intentions and beliefs of other
agents so as to act intelligently and prevent conflicts. In
cases where some of these agents are human, as in the case
of assistive robots in household environments, these are required (rather than just desired) capabilities of robots in
order for them to be considered ‚Äúsocially acceptable‚Äù - this
has been one of the important objectives of ‚Äúhuman-aware‚Äù
planning, as evident from existing literature in human-aware
path planning [13, 10] and human-aware task planning [5,
9, 3, 15]. An interesting aspect of many of these scenarios,
is the presence of many of the aspects of multi-agent environments, but absence of typical assumptions often made in
explicit teaming scenarios between humans and robots, as
pointed out in [4]. Probabilistic plan recognition plays an
important role in this regard, because by not committing to
a plan, that presumes a particular plan for the other agent,
it might be possible to minimize suboptimal (in terms of
redundant or conflicting actions performed during the execution phase) behavior of the autonomous agent.
Here we look at possible ways to minimize such suboptimal behavior by ways of compromise, opportunism or negoFigure 1: Schematic diagram of our integrated system for belief modeling, goal recognition, information extraction and
planning. The robot maintains a belief model of the environment, and uses observations from the environment to
extract information about how the world may evolve, which
is then used to drive its own planning process.

Appears in: Proceedings of the 15th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2016),
J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.),
May 9‚Äì13, 2016, Singapore.
c 2016, International Foundation for Autonomous Agents and
Copyright 
Multiagent Systems (www.ifaamas.org). All rights reserved.

1069

The general architecture of the system is shown in Figure
1. The autonomous agent, or the robot, is acting (with independent goals) in an environment co-habited with other
agents (humans), who are similarly self-interested. The robot
has a model of the other agents acting independently in its
environment. These models may be partial and hence the
robot can only make uncertain predictions on how the world
will evolve with time. However, the resources in the environment are limited and are likely to be constrained by the
plans of the other agents. The robot thus needs to reason
about the future states of the environment in order to make
sure that its own plans do not produce conflicts with respect
to the plans of the other agents.
With the involvement of humans, however, the problem
is more skewed against the robot, because humans would
expect a higher priority on their plans - robots that produce plans that clash with those of the humans, without
any explanation, would be considered incompatible for such
an ecosystem. Thus the robot is expected to follow plans
that preserve the human plans, rather than follow a globally optimal plan for itself. This aspect makes the current
setting distinct from normal human robot teaming scenarios and produces a number of its own interesting challenges.
How does the robot model the human‚Äôs behavior? How does
it plan to avoid friction with the human plans? If it is possible to communicate, how does it plan to negotiate and refine
plans? These are the questions that we seek to address
in this work. Our approach models human beliefs and defines resource profiles as abstract representations of the plans
predicted on the basis of these beliefs. The robot updates
its beliefs upon receiving new observations, and passes on
the resultant profiles onto its planner, which uses an IPformulation to minimize the overlap between these resource
profiles and those produced by the human‚Äôs plans.
The contribution of our paper is thus three-fold, we (1)
propose resource profiles as a concise mode of representing different types of information from predicted plans; (2)
develop an IP-based planner that can utilize this information and provide different modalities of conformant behavior; and (3) provide an integrated framework that supports
the proposed mode of planning - the modular approach also
provides an elegant way to handle different challenges separately (e.g. uncertainty and/or nested beliefs of humans
leaves the planner). The planner, as a consequence of these,
has properties not present in existing planners - for example, the work that probably comes closest is [9] that models
a specific case of compromise only, while the formulation is
also likely to blow up in presense of large hypothesis sets
due to absence of concise representation techniques like the
profiles. We will discuss the trade-offs and design choices in
more detail in the evaluation sections.
The rest of the paper is organized as follows. We will start
with a brief introduction of the agent models that comprise
the belief component, and describe how it facilitates plan
recognition. Then, in Sections 2.3 and 2.4, we are going
to go into details of how resource profiles may be used to
represent information from predicated plans, and describe
how our planner converts this information into constraints
that can be solved as an integer program during the plan
generation process. In Section 3 we will demonstrate how
the planner may be used to produce different modes of autonomous behavior. Finally in Section 4 we will provide
empirical evaluations of the planner‚Äôs internal properties.

Figure 2: Use case - Urban Search And Rescue (USAR).

2.

PLANNING WITH CONFLICTS
ON SHARED RESOURCES

We will now go into details about each of the modules
shown in Figure 1. The setting (adopted from [14]) involves
a commander CommX and a robot in a typical USAR (Urban
Search and Rescue) task illustrated in Figure 2. The commander can perform triage in certain locations, for which
he needs the medkit. The robot can also fetch medkits if
requested by other agents (not shown) in the environment.
The shared resources here are the two medkits - some of
the plans the commander can execute will lock the use of
and/or change the position of these medkits, so that from
the set of probable plans of the commander we can extract
a probability distribution over the usage (or even the position) of the medkit over time based on the fraction of plans
that conform to these facts. These resource availability profiles (i.e. the distribution over the usage or position of the
medkit evolving over time) provide a way for the agents to
minimize conflicts with the other agents. Before going into
details about the planner that achieves this, we will first
look at how the agents are modeled and how these profiles
are computed in the next section.

2.1

The Belief Modeling Component

The notion of modeling beliefs introduced by the authors
in [14] is adopted in this work. Beliefs about state are defined
in terms of predicates bel(Œ±, œÜ), where Œ± is an agent with belief œÜ = true. Goals are defined by predicates goal(Œ±, œÜ),
where agent Œ± has a goal œÜ. The set of all beliefs that
the robot ascribes to Œ± together represents the perspective for the robot of Œ±. This is obtained by a belief model
BelŒ± of agent Œ±, defined as { œÜ | bel(Œ±, œÜ) ‚àà Belself },
where Belself are the first-order beliefs of the robot (e.g.,
bel(self, at(self, room1))). The set of goals ascribed to Œ± is
similarly described by {goal(Œ±, œÜ)|goal(Œ±, œÜ) ‚àà Belself }.
Next, we turn our attention to the domain model DŒ± of
the agent Œ± that is used in the planning process. We use
PDDL [11] style agent models for the rest of the discussion, but most of the analysis easily generalizes to other
related modes of representation. Formally, a planning problem Œ† = hDŒ± , œÄŒ± i consists of the domain model DŒ± and the
problem instance œÄŒ± . The domain model of Œ± is defined as
DŒ± = hTŒ± , VŒ± , SŒ± , AŒ± i, where TŒ± is a set of object types;
VŒ± is a set of variables that describe objects that belong to
types in TŒ± ; SŒ± is a set of named first-order logical predicates over the variables VŒ± that describe the state; and AŒ± is
a set of operators available to the agent. The action models
a ‚àà AŒ± are represented as a = hN, C, P, Ei where N denotes

1070

2.2.2

the name of that action; C is the cost of that action; P ‚äÜ SŒ±
is the list of pre-conditions that must hold for the action
a to be applicable in a particular state s ‚äÜ SŒ± of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ SŒ±
is a tuple that contains the add and delete effects of applying the action to a state. The transition function Œ¥(¬∑)
determines the next state after the application of action a
in state s as Œ¥(a, s) |= ‚ä• if ‚àÉf ‚àà P s.t. f 6‚àà s; Œ¥(a, s) |=
(s \ ef f ‚àí (a)) ‚à™ ef f + (a) otherwise.
The belief model, in conjunction with beliefs about the
goals / intentions of another agent, will allow the robot to
instantiate a planning problem œÄŒ± = hOŒ± , IŒ± , GŒ± i, where
OŒ± is a set of objects of type t ‚àà TŒ± ; IŒ± is the initial state of
the world, and GŒ± is a set of goals, which are both sets of the
predicates from SŒ± initialized with objects from OŒ± . First,
the initial state IŒ± is populated by all of the robot‚Äôs initial beliefs about the agent Œ±, i.e. IŒ± = {œÜ | bel(Œ±, œÜ) ‚àà Belrobot }.
Similarly, the goal is set to GŒ± = {œÜ | goal(Œ±, œÜ) ‚àà Belrobot }.
Finally, the set of objects OŒ± consists of all the objects that
are mentioned in either the initial state, or the goal description: OŒ± = {o | o ‚àà (œÜ | œÜ ‚àà (IŒ± ‚à™ GŒ± ))}. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄŒ± = ha1 , a2 , . . . , a|œÄŒ± | i, ai ‚àà AŒ± such that
Œ¥(œÄŒ± , IŒ± ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . . ,P
a|œÄ| i, Œ¥(a1 , s)). The cost of
the plan is given by C(œÄŒ± ) =
a‚ààœÄŒ± Ca and the optimal
‚àó
‚àó
plan œÄŒ±
is such that C(œÄŒ±
) ‚â§ C(œÄŒ± ) ‚àÄœÄŒ± with Œ¥(œÄŒ± , IŒ± ) |=
GŒ± . This planning problem instance (though not directly
used in the robot‚Äôs planning process) enables the goal recognition component to solve the compiled problem instances.
More on this in the next section.

2.2

In the present scenario, we thus have a set Œ®Œ± of goals that
Œ± may be trying to achieve, and observations of the actions
Œ± is currently executing. At this point we refer to the work
of Ramirez and Geffner who in [12] provided a technique to
compile the problem of plan recognition into a classical planning problem. Given a sequence of observations Œ∏, we recompute the probability distribution Œò over G ‚àà Œ®Œ± by using a
Bayesian update P (G|Œ∏) ‚àù P (Œ∏|G), where the likelihood is
approximated by the function P (Œ∏|G) = 1/(1 + e‚àíŒ≤‚àÜ(G,Œ∏) )
where ‚àÜ(G, Œ∏) = Cp (G ‚àí Œ∏) ‚àí Cp (G + Œ∏). Here ‚àÜ(G, Œ∏)
gives an estimate of the difference in cost Cp of achieving the
goal G without and with the observations, thus increasing
P (Œ∏|G) for goals that explain the given observations. Thus,
solving two compiled planning problems, with goals G ‚àí Œ∏
and G + Œ∏, gives us the required posterior update for the
distribution Œò over possible goals of Œ±. The details of the
approach is available at [12].
The specific problem we will look at now is how to inform
the robot‚Äôs own planning process from the recognized goal
set Œ®Œ± . In order to do this, we compute the optimal plans
for each goal in the hypothesis goal set Œ®Œ± , and associate
them with the probabilities of these goals from the distribution thus obtained. Information from these plans is then
represented concisely in the form of resource profiles.

Notes on the Recognition Module
For our plan recognition module we use a much faster variation [7] of the above approach that exploits cost and interaction information from plan graphs to estimate the goal
probabilities. This saves on the computational effort of having to solve two planning problems per goal. Also, note that
while computing the plan to a particular goal G, we use a
compiled problem instance with the goal G + Œ∏ to ensure
that the predicted plan conforms to the existing observations. Details on the compilation is available at [12].
Also, the output of the planner does not need to be associated with probabilities - this is just the most general
formulation. If we want to deal with just a set of plans that
the robot needs to be aware of, we can treat the plan set
either with a uniform distribution and/or by requiring exactly zero conflicts in the objective of the planner (this will
become clearer in Section 2.4) depending on the preference.
Perhaps the biggest computational issue here is the need
to compute optimal plans. While we still do it for our domain, as we will note later in Section 2.3, this might not
be necessary, and suboptimal plans may be used in larger
domains where computation is an issue.

The Goal Recognition Component

For many real world scenarios, it is unlikely that the goals
of the humans are known completely, and that the plan computed by the planner is exactly the plan that they will follow.
We are only equipped with a belief of the likely goal(s) of
the human - and this may not be a full description of their
actual goals. Further, in the case of an incompletely specified goal, there might be a set of likely plans that the human
can execute, which brings into consideration the idea of incremental goal recognition over a possible goal set given a
stream of observations.

2.2.1

Goal / Plan Recognition

Goal Extension

To begin with, it is worth noting that the robot might
have to deal with multiple plans even in the presence of
completely specified goals (even if the other agents are fully
rational). For example, there may be multiple optimal ways
of achieving the same goal, and it is not obvious beforehand
which one of these an agent is going to end up following. In
the case of incompletely specified goals, the presence of multiple likely plans become more relevant. To accommodate
this, we extend the robot‚Äôs current belief of an agent Œ±‚Äôs
goal, GŒ± , to a hypothesis goal set Œ®Œ± . The computation of
this goal set can be done using the planning graph method
[2]. In the worst case, Œ®Œ± corresponds to all possible goals in
the final level of the converged planning graph. Having further (domain-dependent) knowledge (e.g. in our scenario,
information that CommX is only interested in triage-related
goals) can prune some of these goals by removing the goal
conditions that are not typed on the triage variable.

2.3

Resources and Resource Profiles

As we discussed previously, since the plans of the agents
are in parallel execution, the uncertainty introduced by the
commander‚Äôs actions cannot be mapped directly between
the commander‚Äôs final state and the robot‚Äôs initial state.
However, given the commander‚Äôs possible plans, the robot
can extract information about at what points of time the
shared resources in the environment are likely to be locked
by the commander. This information can be represented
by resource usage profiles that capture the expected (over
all the recognized plans) variation of probability of usage
or availability over time. The robot can, in turn, use this
information to make sure that the profile imposed by its own
plan has minimal conflicts with those of the commander‚Äôs.

1071

is locked by œÄ at step t, 0 otherwise} such that GœÄŒª (t) =
g ‚àÄ (t, g) ‚àà GœÄŒª . The resultant usage profile of a resource
Œª due to all the plans in Œ®P
Œ± is obtained by summing over
(weighted by the individual likelihoods) all the individual
profiles as G Œª : N ‚Üí [0, 1] = {(t, g) | t ‚àà {1, maxœÄ‚ààŒ®PŒ± |œÄ|}
P
and g ‚àù |Œ®1P | œÄ‚ààŒ®P GœÄŒª (t) √ó l(œÄ)}.
Œ±
Œ±
Similarly, we can define profiles over the actual groundings of a variable (shown in the lower part of Figure 3) as
Œª
GœÄf = {(t, g) | t ‚àà [1, |œÄ|] and f Œª = 1 at step t of plan œÄ,
0 otherwise}, and the resultant usage profile due to all the
fŒª
plans in Œ®P
= {(t, g) | t =
Œ± is obtained as before as G
P
Œª
1, 2, . . . , maxœÄ‚ààŒ®PŒ± |œÄ| and g ‚àù |Œ®1P | œÄ‚ààŒ®P GœÄf (t) √ó l(œÄ)}.
Œ±
Œ±
These profiles are helpful when actions in the robot‚Äôs domain
are conditioned on these variables, and the values of these
variables are conditioned on the plans of the other agents in
the environment currently under execution.
One important aspect of this formulation that should be
noted here is that the notion of ‚Äúresources‚Äù is described here
in terms of the subset of the common predicates in the domain of the agents (Œæ ‚äÜ SŒ± ‚à© SR ) and can thus be used
as a generalized definition to model different types of conflict between the plans between two agents. In as much as
these predicates are descriptions (possibly instantiated) of
the typed variables in the domain and actually refer to the
physical resources in the environment that might be shared
by the agents, we will stick to this nomenclature of calling
them ‚Äúresources‚Äù. We will now look at how an autonomous
agent can use these resource profiles to minimize conflicts
during plan execution with other agents in its environment.

Figure 3: Different types of resource profiles.

Formally, a profile is defined as a mapping from time step
T to a real number between 0 and 1, and is represented by
a set of tuples as follows G : N ‚Üí [0, 1] ‚â° {(t, g) : t ‚àà N, g ‚àà
[0, 1], such that G(t) = g at time step t}.
The concept of resource profiles can be handled at two
levels of abstraction. Going back to our running example,
shared resources that can come under conflict are the two
(locatable typed objects) medkits, and the profiles over the
medkits can be over both usage and location, as shown in
Figure 3. These different types of profiles can be used (possibly in conjunction if needed) for different purposes. For
example, just the usage profile shown on top is more helpful
in identifying when to use the specific resource, while the
resource when bound with the location specific groundings,
as shown at the bottom can lead to more complicated higher
order reasoning (e.g. the robot can decide to wait for the
commander‚Äôs plans to be over, as he inadvertently brings
the medkit closer to it with high probability as a result of
his own plans). We will look at this again in Section 3.
Let the domain model of the robot be DR = hTR , VR , SR ,
AR i with the action models a = hN, C, P, Ei defined in the
same way as described in Section 2.1. Also, let Œõ ‚äÜ VR
be the set of shared resources and for each Œª ‚àà Œõ we have
a set of predicates f Œª ‚äÜ SR that are influenced (as determined by the system designer) by Œª, and let Œì : Œõ ‚Üí P(Œæ)
be a function that maps the resource variables to the set
of predicates Œæ = ‚à™Œª f Œª they influence. Without any external knowledge of the environment, we can set Œõ = VŒ± ‚à© VR
and Œæ = SŒ± ‚à© SR , though in most cases these sets are much
smaller. In the following discussion, we will look at how
the knowledge from the hypothesis goal set can be modeled
in terms of resource availability graphs for each of the constrained resources Œª ‚àà Œõ.
Consider the set of plans Œ®P
Œ± containing optimal plans
corresponding to each goal in the hypothesis goal set, i.e.
‚àó
‚àó
Œ®P
Œ± = {œÄG = ha1 , a2 , . . . at i | Œ¥(œÄG , IŒ± ) |= G, ai ‚àà AŒ± ‚àÄi, G ‚àà
Œ®Œ± } and let l(œÄ) be the likelihood of the plan œÄ modeled on
the goal likelihood distribution ‚àÄ G ‚àà Œ®Œ± , p(G) ‚àº Œò as
l(œÄG ) = c|œÄG | √ó p(G), where c is a normalization constant.
At each time step t, a plan œÄ ‚àà Œ®P
Œ± may lock one or
more of the resources Œª. Each plan thus provides a profile
of usage of a resource with respect to the time step t as
GœÄŒª : N ‚Üí {0, 1} = {(t, g) | t ‚àà [1, |œÄ|] and g = 1 if Œª

Notes on Usefulness of Profile Computation
One interesting aspect of computing resource profiles is that
it provides a powerful interface between the belief on the environment and the planner. On the one hand, note that the
input from the previous stage (goal/plan recognition module) is as generic as possible - a set of plans possibly associated with probabilities. Given any changes in preceding
stages, e.g. modeling stochasticity or more complex belief
models, still yields a set of plans that the robot needs to be
aware of. Thus the plan set and resource profiles provide
a surprisingly simple yet powerful way of abstracting away
relevant information for the planner to use.
The profiles may also be leveraged to address different
modalities of conformant behavior, for example with multiple humans and their relative importance, by (1) weighing
the contributions from individual profiles by the normalized
priority of the human, which would cause the planner to
avoid conflicts with these profiles more than with those with
lower priorities; or (2) requiring zero conflicts on a subset of
profiles which would cause the planner to avoid a subset of
conflicts at all costs, while minimizing the rest.
A somewhat implicit advantage of using profiles is its ability to form regions of interest given the possible plans. This
will become clear later in Section 4.2 when we show that the
predicted conflicts provide well-informed guidance to avoiding real conflicts during execution (as evident by the robustness in performance with just 1-3 observations, and zero actual conflicts in low probability areas in the computed profiles). Right now this has the implication that we need not
necessarily compute perfect plan costs and goal distributions
to get good plans.

1072

2.4

Conflict Minimization

Then the solution to the IP should ensure that the robot only
uses these resources when they are in fact most expected to
be available (as obtained by maximizing the overlap between
Œª
hf,t and Gf ). These act like demand profiles from the perspective of the robot. We also add a ‚Äúno-operation‚Äù action
AR ‚Üê AR ‚à™ aœÜ so that aœÜ = hN, C, P, Ei where N = NOOP,
C = 0, P = {} and E = {}.

The planning problem of the robot - given by Œ† = hDR , œÄR ,
Œª
Œõ, {GŒª | ‚àÄŒª ‚àà Œõ}, {Gf | ‚àÄf ‚àà Œì(Œª), ‚àÄŒª ‚àà Œõ}i - consists of
the domain model DR and the problem instance œÄR = hOR ,
IR , GR i similar to that described in section 2.3, and also
the constrained resources and all the profiles corresponding
to them. This is because the planning process must take
into account both goals of achievement as also conflict of resource usages as described by the profiles. Traditional planners provide no direct way to handle such profiles within the
planning process. Note here that since the execution of the
plans of the agents is occurring in parallel, the uncertainty is
evolving at the time of execution, and hence the uncertainty
cannot be captured from the goal states of the recognized
plans alone, and consequently cannot be simply compiled
away to the initial state uncertainty for the robot and solved
as a conformant plan. Similarly, the problem does not directly compile into action costs in a metric planning instance
because the profiles themselves are varying with time. Thus
we need a planner that can handle these resource constraints
that are both stochastic and non-stationary due to the uncertainty in the environment. To this end we introduce the
following IP-based planner (partly following the technique
for IP encoding for state space planning outlined in [16])
as an elegant way to sum over and minimize overlaps in
profiles during the plan generation process. The following
formulation finds such T-step plans in case of non-durative
or instantaneous actions.
For action
( a ‚àà AR at step t we have an action variable:
1, if action a is executed in step t
xa,t =
0, otherwise; ‚àÄa ‚àà AR , t ‚àà {1, 2, . . . , T }
Also, for every proposition f at step t a binary state variable
is introduced
( as follows:
1, if proposition is true in plan step t
yf,t =
0, otherwise; ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }
Note here that the plan computed by the robot introduces a
resource consumption profile itself, and thus one optimizing
criterion would be to minimize the overlap between the usage
profile due to the computed plan with those established by
the predicted plans of the other agents in the environment.
Let us introduce a new variable to model the resource usage
graph imposed
( by the robot as follows:
1, if f ‚àà Œæ is locked at plan step t
gf,t =
0, otherwise; ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T }
Further, for every resource Œª ‚àà Œõ, we divide the actions in
the domain of the robot into three disjoint sets -

The IP formulation is given by:
P
P
min k1 a‚ààAR
t‚àà{1,2,...,T } Ca √ó xa,t
P
P
P
Œª
+k2 Œª‚ààŒõ
f ‚ààŒì(Œª)
t‚àà{1,2,...,T } gf,t √ó G (t)
P
P
P
fŒª
‚àík3 Œª‚ààŒõ
(t)
f ‚ààŒì(Œª)
t‚àà{0,1,...,T ‚àí1} hf,t √ó G
yf,0 = 1 ‚àÄf ‚àà IR \ Œæ

(1)

yf,0 = 0 ‚àÄf ‚àà
/ IR or f ‚àà Œæ

(2)

yf,T = 1 ‚àÄf ‚àà GR

(3)

xa,t ‚â§ yf,t‚àí1 ‚àÄa s.t. f ‚àà Pa , ‚àÄf ‚àà
/ Œæ, t ‚àà {1, . . . , T }

(4)

hf,t‚àí1 = xa,t ‚àÄa s.t. f ‚àà Pa , ‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
(5)
P
yf,t ‚â§ yf,t‚àí1 + a‚ààadd(f ) xa,t
s.t. add(f ) = {a|f ‚àà ef f + (a)}, ‚àÄf, t ‚àà {1, . . . , T }
(6)
P
yf,t ‚â§ 1 ‚àí a‚ààdel(f ) xa,t
s.t. del(f ) = {a|f ‚àà ef f ‚àí (a)}, ‚àÄf, t ‚àà {1, . . . , T }
(7)
P
(8)
a‚ààAR xa,t = 1, t ‚àà {1, 2, . . . , T }
P
P
x
‚â§
1
‚àÄf
‚àà
Œæ,
t
‚àà
{1,
2,
.
.
.
,
T
}
(9)
t a,t
a‚àà‚Ñ¶+
fP
P
P
gf,t = a‚àà‚Ñ¶+ xa,t + (1 ‚àí a‚àà‚Ñ¶+ xa,t ‚àí a‚àà‚Ñ¶‚àí xa,t ) √ó gf,t‚àí1
f

f

f

‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
hf,t √ó G

fŒª

(t) ‚â•  ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T ‚àí 1}

(10)
(11)

yf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }

(12)

xa,t ‚àà {0, 1} ‚àÄa ‚àà AR , t ‚àà {1, 2, . . . , T }

(13)

gf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T }

(14)

hf,t ‚àà {0, 1} ‚àÄf ‚àà SR , t ‚àà {0, 1, . . . , T ‚àí 1}

(15)

where k1 , k2 , k3 are constants determining the relative importance of the optimization criteria and  is a constant.
Here, the objective function minimizes the sum of the cost
of the plan and the overlap between the cumulative resource
usage profiles of the predicted plans and that imposed by the
current plan of the robot itself while maximizing the validity
of the demand profiles. Constraints (1) through (3) model
the initial and goal conditions, while the value of the constrained variables are kept uninitialized (and are determined
by their profiles). Constraints (4) and (5), depending on the
particular predicate, enforces the preconditions, or produces
the demand profiles respectively, while (6) and (7) enforces
the state equations that maintain the add and delete effects
of the actions. Constraint (8) imposes non concurrency on
the actions, and (9) ensures that the robot does not repeat
the same action indefinitely to increase its utility. Constraint
(10) generates the resource profile of the current plan, while
(11) maintains that actions are only executed if there is at
least a small probability  of success. Finally (12) to (15)
provide the binary ranges of the variables.

‚Ñ¶+
f = {a ‚àà AR such that xa,t = 1 =‚áí yf,t = 1},
‚Ñ¶‚àí
f = {a ‚àà AR such that xa,t = 1 =‚áí yf,t = 0}, and
‚àí
‚Ñ¶of = AR \ (‚Ñ¶+
f ‚à™ ‚Ñ¶f ), ‚àÄf ‚àà Œæ.
These then specify respectively those actions in the domain
that lock, free up, or do not affect the use of a particular
resource, and are used to calculate gf,t in the IP. Further, we
introduce a variable hf,t to track preconditions required by
actions in the generated plan whose success is conditioned
on the influence of the plans of the other agents on the world
(e.g. position of the medkits are changing, and the action
pickup is (
conditioned on it) as follows:
1, if f ‚àà Pa and xa,t+1 = 1
hf,t =
0, otherwise; ‚àÄf ‚àà Œæ, t ‚àà {0, 1, . . . , T ‚àí 1}

Note on Temporal Expressivity
At this point it is worth acknowledging the implications
of having durative actions in our formulation. Note that
our approach does not discretize time, but rather uses time

1073

points as steps in the plan - that can be easily augmented
with their own durations. So in order to handle durative
actions, the only (somewhat minor) change required in the
formulation is in the way the conflicts are integrated (instead
of summed) over in the objective function. Further, uncertainty in action durations is always a big issue in human
interactions; though resource profiles cannot directly handle
uncertain durations, it only affects the way the profiles are
calculated, and the way in which information is expressed
in it remains unchanged (i.e. expectations over action durations add an extra expectation to the already probabilistic
profile computation). As noted before in Section 2.3, the
ability of profiles to form regions of interest is crucial in
handling such scenarios implicitly.

3.

13
14
15
16
17
18
19

3.3

The planner is implemented on the IP-solver gurobi and
integrates [7] and [8] respectively for goal recognition and
plan prediction for the recognized goals. We will now illustrate how the formulation can produce different behaviors
of the robot by appropriately configuring the parameters of
the planner. For this discussion we will limit ourselves to a
singleton hypothesis goal set in order to observe the robot‚Äôs
response more clearly.

Compromise

(5a)
(16)

Œª

H (t) ‚â§ G (t) ‚àÄŒª ‚àà Œõ, t ‚àà {0, 1, . . . , T }
(17)
Constraint (5a) now complements constraint (5) from the
existing formulation, by promising to restore the world state
every time a demand is made on a variable. The variable
H Œª (t), maintained by constraints (16) and (17), determine
the desired deviation from the given profiles. The objective
function has been updated to reflect that overlaps are now
measured with the desired profile of usage, and there is a
cost associated with the deviation from the real one. The
revised plan now produced by the robot is shown below.
01
02
03
04
05
06
07
08
09
10

MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_ROBOT_HALL2_HALL3
MOVE_ROBOT_HALL3_HALL4
MOVE_REVERSE_ROBOT_HALL4_ROOM4
MOVE_REVERSE_ROBOT_ROOM4_ROOM3
PICK_UP_MEDKIT_ROBOT_MK2_ROOM3
MOVE_ROBOT_ROOM3_ROOM4
MOVE_ROBOT_ROOM4_HALL4
MOVE_REVERSE_ROBOT_HALL4_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
DROP_OFF_ROBOT_MK2_HALL3

3.2

yf,T ‚â• hf,t‚àí1 ‚àÄ a s.t. f ‚àà Pa , ‚àÄf ‚àà Œæ, t ‚àà {1, . . . , T }
H Œª (t) ‚àà [0, 1] ‚àÄŒª ‚àà Œõ, t ‚àà {0, 1, . . . , T }
Œª

Let us now look back at the environment we introduced
in Figure 1. Consider that the goal of the commander is to
perform triage in room1. The robot computes the human‚Äôs
optimal plan (which ends up using medkit1 at time steps 7
through 12) and updates the resource profiles accordingly. If
it has its own goal to perform triage in hall3, the plan that
it comes up with given a 12 step lookahead is shown below.
Notice that the robot opts for the other medkit (medkit2
in room3) even though its plan now incurs a higher cost in
terms of execution. The robot thus can adopt a policy of
compromise if it is possible for it to preserve the commander‚Äôs (expected) plan.
01
02
03
04
05
06
07
08
09
10
11
12

Negotiation

In many cases, the robot will have to eventually produce
plans that will have potential points of conflict with the expected plans of the commander. This occurs when there is
no feasible
plan with zero overlap between profiles (specifiP
cally
gf,t √ó GŒª (t) = 0) or if the alternative plans for the
robot are too costly (as determined by the objective function). If, however, the robot is equipped with the ability to
communicate with the human, then it can negotiate a plan
that suits both. To this end, we introduce a new variable
H Œª (t) and update the IP as follows:
P
P
min k1 a‚ààAR
t‚àà{1,2,...,T } Ca √ó xa,t
P
P
P
Œª
+k2 Œª‚ààŒª
f ‚ààŒì‚àí1 (Œª)
t‚àà{1,2,...,T } gf,t √ó H (t)
P
P
P
fŒª
(t)
‚àík3 Œª‚ààŒõ
‚àí1
t‚àà{0,1,...,T ‚àí1} hf,t √ó G
P
Pf ‚ààŒì (Œª)
Œª
Œª
+k4 Œª‚ààŒõ
t‚àà{0,1,...,T } ||G (t) ‚àí H (t)||

MODULATING BEHAVIOR
OF THE ROBOT

3.1

NOOP
PICK_UP_MEDKIT_ROBOT_MK1_ROOM1
MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_ROBOT_HALL2_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
DROP_OFF_ROBOT_MK1_HALL3

MOVE_ROBOT_ROOM1_HALL1
MOVE_ROBOT_HALL1_HALL2
MOVE_REVERSE_ROBOT_HALL2_ROOM2
PICK_UP_MEDKIT_ROBOT_MK1_ROOM2
MOVE_ROBOT_ROOM2_HALL2
MOVE_ROBOT_HALL2_HALL3
CONDUCT_TRIAGE_ROBOT_HALL3
MOVE_REVERSE_ROBOT_HALL3_HALL2
MOVE_REVERSE_ROBOT_HALL2_ROOM2
DROP_OFF_ROBOT_MK1_ROOM2

Notice that the robot restores the world state that the human is believed to expect, and can now communicate to him
‚ÄúCan you please not use medkit1 from time 7 to 9?‚Äù based
on how the real and the ideal profiles diverge, i.e. t such
that H Œª (t) < GŒª (t) for each resource Œª.

Opportunism

Notes on Adaptive Behavior Modeling

Notice, however, that the commander is actually bringing
the medkit to room1 as predicted by the robot, and this is
a favorable change in the world, because robot can use this
medkit once the commander is done and achieve its goal at
a much lower cost. The robot, indeed, realizes this once we
give it a bigger time horizon to plan with, as shown above (on
the right). Thus, in this case, the robot shows opportunism
based on how it believes the world state will change.

One might note here that people are often adaptive and it is
very much possible that they may be willing to change their
goals based on observing the robot or are even unwilling to
negotiate if their plans conflict. Hence the policies of compromise and opportunism for the robot are complementary
to negotiation in the event the latter fails. Thus, for example, the robot might choose to communicate a negotiation
strategy to the human, but fall back on a compromise if that
fails. It is a merit of such a simple formulation to be able to
handle such interesting adaptive behaviors.

01 NOOP
02 NOOP
...

1074

4.

EVALUATION

The power of the proposed approach lies in the modular
nature in which it tackles several complicated problems that
are separate research areas in their own rights. As we saw
throughout the course of the discussion, approaches used
in the individual modules may be varied with little to no
change in the rest of the architecture. For example the expressivity of the belief modeling or goal recognition component is handled separately as the planner used information
from a generic plan set. Again the representation technique
introduced in terms of resource profiles provide properties in
terms of computational independence with respect to size of
the hypothesis set and number of agents (which gets manifested in complexity in number of resources) that general
planners do not have. So it becomes a design choice depending on which metric needs to be optimized.
For empirical evaluations, we simulated the USAR scenario on 360 different problem instances, randomly generated by varying the specific (as well as the number of probable) goals of the human, and evaluated how the planner
behaved with the number of observations it can start with
to build its profiles. We fix the domain description, location
and goal of the agents, and the position of the resources, and
consider randomly generated hypothesis goal sets of size 211. The goals of the commander were assumed to be known
to be triage related, but the location of the triage was allocated randomly (one of which was again picked at random
as the real goal). Finally for each of these problems, we
generate 1-5 observations by simulating the commander‚Äôs
plan over the real goal, and use these observations known
a priori the robot‚Äôs plan generation process. The experiments were conducted on an Intel Xeon(R) CPU E5-1620
v2 3.70GHz√ó8 processor with a 62.9GiB memory.

4.1

(a) w.r.t. T (|Œõ| = 2)

(b) w.r.t. #medkits (T = 10)

Figure 4: Performance of the planner w.r.t. planning horizon T and number of constrained resources (medkits).

Scaling Up

Our primary contribution is the formulation for planning
with resource profiles, while the goal recognition component
can be any off-the-shelf algorithm, and as such we compare
scalability with respect to the planning component only.

the agents. On the other hand, the time spent on recognition, and on calculating the profiles, is significantly affected.
However, observations on multiple agents are asynchronous,
and goal recognition can operate in parallel, so that this is
not a huge concern beyond the complexity of a single instance. Similarly the performance is also unaffected by the
size of the hypothesis set Œ®Œ± , as shown in Figure 5, which
shows increase in the number of the possible goals does not
complicate the profiles to an extent to affect the complexity.

- w.r.t. Length of the Planning Horizon
The performance of the planner with respect to the planning
horizon is shown in Figure 4a. This is, as expected, the
bottleneck in computation due to exponential growth of the
size of the IP. It is however not prohibitively expensive, and
the planner is still able to produce plans of length 20 (steps,
not durations) for our domain in a matter of seconds.

4.2

Quality of the Plans Produced

We define U as the average conflicts per plan step when
a demand is placed on a resource by the robot, and S as
the success probability per plan step that the demand is
met. C is the cost of a plan. F is the percentage of times
there was an actual conflict during execution (distinct from
U which estimates the possible conflict that may occur per
plan step). We observe the quality of the plans produced
by the planner by varying the ratio of parameters k1 and k3
from the objective function and the length of the planning
horizon T . Similar results can be produced by varying k1 /k2 .
From Table 1, as k1 /k3 decreases, the planner becomes
more conservative (to maximize success probability) and
thus plans become costlier. At the same time the expected
success rate of actions are also increased (with simultaneous
increase in usage conflict), as reflected by a higher failure
rate due to actual execution time conflicts.

- w.r.t. Number of Resources
The performance of the planner with respect to the number of constrained resources (medkits, in the context of the
current discussion) is shown in Figure 4b. Evidently, the
computational effort is dominated by that due to the planning horizon. This reiterates the usefulness of abstracting
the information in predicted plans in the form of resource
profiles, thus isolating the complexities of the domain with
that of the underlying planning algorithm.

- w.r.t. the Number of Agents and Goals
The planning module (i.e. the IP formulation) is by itself
independent of the number of agents being modeled. In
fact, this is one of the major advantages of using abstractions like resource profiles in lieu of actual plans of each of

1075

Figure 5: Performance of the planner w.r.t. size of the goal
set. As expected, computational complexity is not affected.

k1 /k3
C
U
S
F

0.05
9.47
0.18
0.85
27.5

0.5
6.37
0.17
0.579
23.0

(a) w.r.t. |Œ®Œ± |

5.0
6.31
0.17
0.578
21.3

Table 1: Quality of plans produced w.r.t. k1 /k3 . Conservative plans result in lowered utility.

Also note, from Table 2 the impact of the planning horizon T on the types of behaviors we discussed in the previous section. As we increase T , the plan cost falls below the
optimal, indicating opportunities for opportunistic behavior
on the part of the robot. The expected conflict also falls
to almost 0. However the expected success rate of actions
also decreases, the ratio k1 /k2 determines how daring the
robot is, in choosing between cheap versus possibly invalid
plans. Note, however, the actual execution time conflict is
extremely low with increasing T , for even sufficiently conservative estimates of S.
Thus we see that the robot is successfully able to navigate conflicts and find in many cases plans even cheaper
than the original optimal plan, thus highlighting the usefulness of the approach. Finally, we look at the impact of the
parameters in the plan recognition module in Figure 6. As
expected, with bigger hypothesis sets, the success rate goes
down. Interestingly, the plan cost also shows a downward
trend which might be because the bigger variety in possible
goals give a better idea of which medkits are generally more
useful for that instance at what points of time. With more
observations, as expected, the success rate goes up and the
expected conflict goes down. The cost, however, increases a
little as the planner opts for more conservative options.

5.

(b) w.r.t. #obs

Figure 6: Performance of the planner w.r.t. size of goal set
and number of observations (k1 /k3 = 0.5, T = 16).

T
C
U
S
F

10
9.0
0.46
1.0
53.3

13
5.6
0.04
0.48
11.9

16
4.53
‚âà0
0.25
6.6

Optimal
9.0
n/a
n/a
53.3

Table 2: Quality of plans produced w.r.t. T . Opportunities
for opportunism explored, conflicts minimized.

objective function and optimization parameters. Finally, we
provide an end-to-end framework that integrates belief modeling, goal recognition and an IP-solver that can enforce the
desired interaction constraints. One interesting research direction would be to consider nested beliefs on the agents; after all, humans are rarely completely aloof of other agents in
its environment. Such interactions should have to consider
evolution of beliefs with continued interactions and motivate further exploration of the belief modeling component.
The modularity of the proposed approach allows for focused
research on each (individually challenging) subtask without
significantly affecting the others.

CONCLUSIONS

In this paper we investigate how plans may be affected by
conflicts on shared resources in an environment cohabited by
humans and robots, and introduce the concept of resource
profiles as a means of representation for concisely modeling
the information pertaining to the usage of such resources,
contained in predicted behavior of the agents. We propose
a general formulation of a planner for such scenarios and
show how the planner can be used to model different types
of behavior of the robot by appropriately configuring the

Acknowledgment
This research is supported in part by the ONR grants N0001413-1-0176, N00014-13-1-0519 and N00014-15-1-2027, and the
ARO grant W911NF-13-1-0023.

1076

REFERENCES

[11] D. Mcdermott, M. Ghallab, A. Howe, C. Knoblock,
A. Ram, M. Veloso, D. Weld, and D. Wilkins. Pddl the planning domain definition language. Technical
Report TR-98-003, Yale Center for Computational
Vision and Control 1998.
‚Äù
[12] M. Ramirez and H. Geffner. Probabilistic plan
recognition using off-the-shelf classical planners. In In
Proc. AAAI-2010, 2010.
[13] E. Sisbot, L. Marin-Urias, R. Alami, and T. Simeon. A
human aware mobile robot motion planner. Robotics,
IEEE Transactions on, 23(5):874‚Äì883, Oct 2007.
[14] K. Talamadupula, G. Briggs, T. Chakraborti,
M. Scheutz, and S. Kambhampati. Coordination in
human-robot teams using mental modeling and plan
recognition. In Intelligent Robots and Systems (IROS
2014), 2014 IEEE/RSJ International Conference on,
pages 2957‚Äì2962, Sept 2014.
[15] S. Tomic, F. Pecora, and A. Saffiotti. Too cool for
school - adding social constraints in human aware
planning. In Proc of the International Workshop on
Cognitive Robotics (CogRob), 2014.
[16] T. Vossen, M. O. Ball, A. Lotem, and D. S. Nau. On
the use of integer programming models in ai planning.
In T. Dean, editor, IJCAI, pages 304‚Äì309. Morgan
Kaufmann, 1999.

[1] E. Beaudry, F. Kabanza, and F. Michaud. Planning
with concurrency under resources and time
uncertainty. In Proceedings of the 2010 Conference on
ECAI 2010: 19th European Conference on Artificial
Intelligence, pages 217‚Äì222, Amsterdam, The
Netherlands, The Netherlands, 2010. IOS Press.
[2] A. Blum and M. L. Furst. Fast planning through
planning graph analysis. In IJCAI, pages 1636‚Äì1642,
1995.
[3] F. Cavallo, R. Limosani, A. Manzi, M. Bonaccorsi,
R. Esposito, M. Di Rocco, F. Pecora, G. Teti,
A. Saffiotti, and P. Dario. Development of a socially
believable multi-robot solution from town to home.
Cognitive Computation, 6(4):954‚Äì967, 2014.
[4] T. Chakraborti, G. Briggs, K. Talamadupula,
Y. Zhang, M. Scheutz, D. Smith, and
S. Kambhampati. Planning for serendipity. In
IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2015.
[5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning for mobile robots. In Proc of the Int
Conf on Advanced Robotics (ICAR), 2009.
[6] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning: An application to mobile robots. ACM
Trans. Intell. Syst. Technol., 1(2):15:1‚Äì15:26, Dec.
2010.
[7] Y. E.-Martƒ±ÃÅn, M. D. R.-Moreno, and D. E. Smith. A
fast goal recognition technique based on interaction
estimates. In Proceedings of the Twenty-Fourth
International Joint Conference on Artificial
Intelligence, IJCAI 2015, Buenos Aires, Argentina,
July 25-31, 2015, pages 761‚Äì768, 2015.
[8] M. Helmert. The fast downward planning system.
CoRR, abs/1109.6051, 2011.
[9] U. Koeckemann, F. Pecora, and L. Karlsson. Grandpa
hates robots - interaction constraints for planning in
inhabited environments. In Proc. AAAI-2010, 2014.
[10] M. Kuderer, H. Kretzschmar, C. Sprunk, and
W. Burgard. Feature-based prediction of trajectories
for socially compliant navigation. In Proceedings of
Robotics: Science and Systems, Sydney, Australia,
July 2012.

1077

A Combinatorial Search Perspective on Diverse Solution Generation
Satya Gautam Vadlamudi and Subbarao Kambhampati
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{gautam , rao}@asu.edu

Abstract
Finding diverse solutions has become important in many
combinatorial search domains, including Automated Planning, Path Planning and Constraint Programming. Much of
the work in these directions has however focussed on coming up with appropriate diversity metrics and compiling
those metrics in to the solvers/planners. Most approaches use
linear-time greedy algorithms for exploring the state space
of solution combinations for generating a diverse set of solutions, limiting not only their completeness but also their
effectiveness within a time bound. In this paper, we take a
combinatorial search perspective on generating diverse solutions. We present a generic bi-level optimization framework
for finding cost-sensitive diverse solutions. We propose complete methods under this framework, which guarantee finding
a set of cost sensitive diverse solutions satisficing the given
criteria whenever there exists such a set. We identify various aspects that affect the performance of these exhaustive
algorithms and propose techniques to improve them. Experimental results show the efficacy of the proposed framework
compared to an existing greedy approach.

In many real-world domains involving combinatorial search
such as automated planning, path planning and constraint
programming, generating diverse solutions is of much importance. In the case of automated planning, real-world scenario often involves working with unknown or partially
known user preferences (Kambhampati 2007), as the user
preferences are many times difficult to be articulated and
specified completely. Such situations lead to multiple, often, large number of plans that satisfy a given problem instance. In order to facilitate serving the user with a closest
plan possible as per her (hidden) preferences, presenting a
diverse set of plans to the user is explored (Roberts, Howe,
and Ray 2014; Nguyen et al. 2012) so that the user can make
a well-informed decision. In the constraint programming domain, diverse (resp. similar) solutions are explored in order
to handle unknown user preferences as well as to generate
robust solutions (Hebrard et al. 2005).
Several methods have been proposed in the literature for
finding a diverse set of plans. In the context of constraint
programming, (Hebrard et al. 2005) presents a complete
method which creates K copies of the Constraint Satisfacc 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

tion Problem (CSP),each copy with a different set of variable names, adds K
2 additional constraints for handling the
minimum distance requirement between all pairs, and uses
off-the-shelf solvers to generate solutions. As one would expect, they report that this approach generates prohibitively
large CSPs and therefore propose a greedy method. The
greedy approach which has since been widely adopted (Petit and Trapp 2015; Bloem 2015; Roberts, Howe, and Ray
2014; Nguyen et al. 2012) works as follows:
Obtain a candidate solution satisfying any given cost criteria, add this to the solution K-set, provide feedback to the
method finding candidate solutions about the current composition of the K-set so that it tries to find the next candidate
solution distant to the current K-set. Upon obtaining the next
candidate solution, the greedy method adds it to the K-set if
it indeed satisfies the distance criteria and provides feedback
to find the next solution, otherwise the candidate solution is
simply discarded. This process is continued until a set of K
diverse solutions is found. (Roberts, Howe, and Ray 2014;
Eiter et al. 2013) and (Nguyen et al. 2012) consider the
first solution generated to be the starting solution (permanent member) for constructing the K-set through the above
greedy approach. (Bloem 2015) considers an optimal solution to be the starting solution of the greedy method. (Petit
and Trapp 2015) attempt to address the issue of fixed starting solution by running the greedy approach multiple times
with different optimal solutions as the starting solution on
each occasion.
A pertinent issue with the above approaches is that the
first solution (or an optimal solution) is always considered
to be part of the solution set, which may often result in not
finding a K-set even when there exists one, even with a very
good feedback strategy to search for distant solutions after
finding the initial solution. Note that an optimal solution (or
the first found solution) need not be part of a diverse set of
the required size at all. Figure 1 shows an example of an
instance where the optimal solution is not part of the most
diverse solution set of size 2 (assuming that the distance between two plans is inversely proportional to the number of
edges/actions they have in common; p1 and p2 have edge a
in common and p2 and p3 have edge b in common).
In this paper, we address this problem in depth by proposing complete algorithms which guarantee to find a set of K
diverse solutions whenever there exists one. In order to ac-

Start
p1

1

3
G1

a

1
p2

1
2

1 b

p3

G2
Figure 1: A state-space graph where the optimal cost path
p2, does not belong to the most diverse solution set of size 2
{p1,p3}. G1 and G2 indicate goal nodes.
complish completeness, we first need methods for exploring
all possible cost sensitive solutions in the given domain. For
this, we present extensions of the m-A* algorithm (Dechter,
Flerova, and Marinescu 2012) and the Depth-First Branch
and Bound algorithm (Lawler and Wood 1966) for finding
all cost-bounded plans. One could adapt other types of methods such as anytime heuristic search algorithms as well for
this purpose. Second, we present a simple strategy for exhaustively exploring the set of all plan combinations. This
would guarantee completeness for finding a K-set whenever there exists one, thereby addressing the issues in existing methods. However, we note that the simple exhaustive
method ends up having to explore a large number of combinations as one finds more and more candidate plans, severely
impacting the performance of the overall algorithm. In order
to address this problem, we propose a method which considers only a few most promising plan combinations whenever
a new candidate solution is found, and postpones the exploration of remaining combinations for the end to guarantee
completeness. This new method is advantageous over the
widely followed greedy approach on two fronts: it explores
a larger (compared to only one combination of the greedy
approach) but limited number of combinations upon finding
a candidate solution thereby increasing its likelihood of finding a diverse K-set quickly, and it keeps track of the combinations that are left postponed to revisit at the end thereby
guaranteeing completeness.
Further, our method for exploring the plan-combinations
space can be used in conjunction with any of the existing
methods to improve their performance, as it is complementary in nature, replacing the weaker section of those methods
where the greedy approach is present.

Refanidis 2013), air traffic control advisories (Bloem and
Bambos 2014), and robotics (Voss, Moll, and Kavraki 2015).
An important measure in determining diversity is the distance between plans. In this paper, we assume that the distance measure is given as input by the user. Several distance measures have been proposed in the literature that are
quantitative or qualitative (Scala 2014; Coman and MunÃÉozAvila 2011; Goldman and Kuter 2015). Solution diversity
is explored in both deterministic and non-deterministic domains using the distance metrics (Coman 2012). Distance
measures for finding semantically distinct plans are explored
in (Bryce 2014) based on landmarks. In the context of constraint programming, distance constraints in terms of ideal
and non-ideal solutions are studied in (Hebrard, O‚ÄôSullivan,
and Walsh 2007).
SAT-based heuristic methods for generating diverse solutions were proposed in (Nadel 2011). Methods through
compilation to CSP, and using heuristic local search have
been proposed in (Srivastava et al. 2007), which use GPCSP planner (Do and Kambhampati 2001) and LPG planner (Gerevini, Saetti, and Serina 2003). Comparison of firstprinciple techniques and case-based planning techniques to
find diverse plans is shown in (Coman and MunÃÉoz-Avila
2012a). These algorithms too use the greedy approach presented before for exploring the space of plan combinations,
leading to the same issues pointed in the Introduction.

Problem Setup
In this paper, we consider the problem of finding a set of solutions that are not only diverse but are also cost sensitive.
In particular, we consider the problem of finding a set of K
cost sensitive diverse (loopless) solutions. Cost sensitivity
of the solutions is controlled by the input c (maximum cost
of each of the solutions) and diversity of the solution sets is
controlled by the input d (minimum distance between each
pair of solutions; or an appropriate set based diversity metric). Both the cost metric and the distance metric are also assumed to be inputs from the user, hence, the studies on good
quality cost metrics and distance metrics are orthogonal to
our work. Further, we choose the planning domain to showcase our framework and methods, which could be adapted to
other domains. Hence, the problem at-hand can be formally
stated as: Given a planning problem with the set of loopless
solution plans S, a cost metric for the plans C : S ‚Üí R and
a distance metric for the pairs of plans Œ¥ : S √ó S ‚Üí R (a set
based diversity metric may also be used here), the problem
is defined as:
cCOSTdDISTANTkSET: Find P with P ‚äÜ S,
(1)
|P| =k, min Œ¥(p, q) ‚â• d and C(p) ‚â§ c ‚àÄp ‚àà P
p, q ‚àà P

Related Work
Several applications have been related to using diverse solutions in recent years, such as, for course of action generation in cyber security (Boddy et al. 2005), personalized security agents (Roberts et al. 2012), diverse finite
state machines for non-player characters in games (Coman
and MunÃÉoz-Avila 2013; 2012b), formal verification (Nadel
2011), mining group patterns (Vadlamudi, Chakrabarti, and
Sarkar 2012), scheduling personal activities (Alexiadis and

The problem is computationally hard given that the problem
of finding cost-bounded plans is PSPACE-complete (Bylander 1991) and the problem of finding a diverse set of plans
is NP-complete (Bloem 2015) with input size (number of
plans) that can potentially be exponential in terms of the
number of state variables. Finding a set of diverse solutions
is shown to be FPN P [log n] -complete in the context of constraint programming (Hebrard et al. 2005), where n is the
size of the input.

Explore All Solution Combinations
(Level 2)
Solution Stream

Feedback

Explore All Satisficing Solutions
(Level 1)

Figure 2: A framework for finding a diverse set of solutions
that supports completeness.

Proposed Methods & Properties
Now, we present the proposed framework which supports
completeness, and specific methods that obey the framework
requirements. As mentioned above, the problem of finding a
diverse set of plans is a bi-level optimization problem which
involves exploring the set of all candidate plans (Level 1)
and considering the set of all combinations of these plans
(Level 2). Therefore, in order to guarantee completeness,
we propose to have a framework with a complete method
which guarantees finding all the candidate plans and outputs as a stream, and another complete method that takes
the stream of plans being generated by the previous method
as input and explores all combinations of plan sets as per
the diversity criteria until a diverse set of size K is found.
Such a framework ensures that all possible cases are considered and hence guarantees completeness. Figure 2 shows the
framework with the control flow. In this paper, we emphasize
mainly on how to explore all the solutions and all the solution combinations efficiently so as to guarantee completeness while not impeding the search progress due to their individual exhaustive nature. The feedback component shown
in the figure is particular to the domain elements and their
distance measures which we do not explore in this work
leaving it as an option for the user to plug-in to the proposed
framework and methods.

Algorithms for Finding All Cost Sensitive Solutions
For the Level 1, the problem is to find the set of all candidate plans that can potentially be part of a diverse solution
set. In our case, the set of all candidate plans correspond to
the set of all valid loopless plans whose cost is ‚â§ max cost.
We present two complete methods which guarantee generating the set of all candidate plans, one based on DepthFirst Branch and Bound (DFBB) (Lawler and Wood 1966;
Russell and Norvig 1995) and another based on a recent algorithm for finding M best solutions in graphical models,
called m-A* (Dechter, Flerova, and Marinescu 2012). One
may also use other complete methods and extend them to
generate all candidate solutions whose cost is ‚â§ max cost.
First, we present the DFBB based algorithm for finding
all candidate plans, called DFA. It works similar to regular
DFBB search on graph spaces except for the following two
differences: (i) it does not stop after finding a single solution within the max cost bound, and (ii) it does not conduct
full duplicate detection (any state reached through a differ-

ent path from the start state leads to a new node unless there
is a loop, at which point it simply backtracks to find other
solutions). It is easy to prove that:
Lemma 1 DFA generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.
Now, we describe the second algorithm for finding the set
of all candidate plans, called A*A. This is based on the mA* algorithm (Dechter, Flerova, and Marinescu 2012) which
guarantees finding m best solutions/plans by expanding the
minimum set of nodes. First, we describe the basic idea behind m-A* and then we extend it to find the set of all candidate plans for our problem. The basic idea behind m-A* is to
proceed in a manner similar to A* and whenever a duplicate
state is found, the most promising m nodes corresponding to
that state are to be considered for expansion and the rest be
discarded. For our problem, where we want to find the set
of all cost-bounded plans, we will have to keep all the nodes
corresponding to same state for expansion without the mlimit. This eliminates the requirement of full-scale duplicate
detection all-together, instead suggests treatment of all children being generated as new. However, since we are only
interested in loopless plans, we will discard all nodes which
cause loops in the partial plan at any stage, through cycle
checking. We call this adapted strategy- A*A (A* based approach for finding All cost-bounded solutions). Once again,
it can be proven that:
Lemma 2 A*A generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.

Complete Algorithms for Finding a Diverse
Solution Set
Now, we present the algorithms for Level 2 of our framework, where the stream of all candidate plans, the distance
measure, the size of the diverse plan set needed and the minimum distance between any two plans of the solution set is
given as input, to produce a set of satisficing diverse plans.
Algorithm 1 presents a simple strategy called ACER
which explores all combinations of plans in each run for the
in-coming new plan in conjunction with all of the existing
valid plan sets. It is easy to prove that:
Lemma 3 ACER finds a diverse plan set of size K from the
set/stream of all candidate plans whenever there exists one.
However, G can grow rapidly and become an exponential sized set in terms of K with base being the number of
candidate plans (which itself can be of exponential size in
terms of the planning problem input) before finding a diverse plan set. This severely limits its scalability when there
are large number of candidate plans (even for moderate values of K), which is often the case in practice. Next, we will
present a method which does not explore all plan set combinations in one shot instead only a select most promising sets
at each stage, while keeping track of unexplored combinations that may be explored at the end (after processing the
entire stream of candidate plans once) for completeness.

Algorithm 1 Explore All Possible Combinations of Solutions in Each Run (ACER)

Algorithm 2 Explore Most-promising Combinations of Solutions in Each Run (MCER)

1: INPUT :: A candidate plan p (from the stream of all candidate plans), a distance measure dist(), the minimum distance
needed between any two plans of a set min dist, and the size
of the diverse plan set K.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets.
4: for each plan set P ‚àà G do
5:
if dist(p, l) > min dist ‚àÄl ‚àà P then
6:
P 0 ‚Üê P + {p};
7:
if |P 0 | = K then
8:
return P 0 ;
9:
end if
10:
G ‚Üê G‚à™P 0 ;
11:
end if
12: end for
13: G ‚Üê G‚à™{p};
14: return largest P ‚àà G;

1: INPUT :: A candidate plan p (from the stream of all candidate
plans), its sequence number in the stream i, a distance measure
dist(), the minimum distance needed between any two plans
of a set min dist, the size of the diverse plan set K, and the
number of seed plan sets to be explored n.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets with satellite data.
4: Open ‚Üê œÜ; Children ‚Üê œÜ;
5: ExpandMostPromising(G, n, Children);
6: if a diverse set P of size K is found then
7:
return P ;
8: end if
9: while Children 6= œÜ do
10:
Swap Children and Open;
11:
ExpandMostPromising(Open, n, Children);
12:
if a diverse set P of size K is found then
13:
return P ;
14:
end if
15:
Move all plan sets in Open to G;
16: end while
17: P ‚Üê {p}; Pexp ‚Üê i;
18: G ‚Üê G‚à™P ;
19: return largest P ‚àà G;

The second technique, which is an adaptation of the
Anytime Pack Search method (Vadlamudi, Aine, and
Chakrabarti 2015; 2013), focuses on exploring a limited set
of seed nodes in each iteration in a beam search like manner.
It processes the stream of candidate plans much faster than
the previous approach by focusing only on a select number
of most promising plan sets to begin with. The combinations
which this technique ignores while processing the candidate
plan stream are kept track of separately for processing at the
end, which helps in guaranteeing the completeness. Algorithm 2 presents the proposed method MCER for faster processing of the stream of candidate plans. It takes as input,
a plan from the stream of candidate plans and its sequence
number for reasons that will become clear shortly, and the
inputs for determining a diverse set similar to the previous
approach, and the number of seed plan sets to be explored
upon finding a new candidate plan.The plan-combinations
space can be visualized as a set enumeration tree (Rymon
1992), where new branches come at all levels (> 0) dynamically as the stream of candidate plans is processed.
MCER maintains a global set of valid plan sets which
have been produced until now, G, that could be further expanded with new candidate plans. It expands n most promising nodes from this set, which are populated into Children.
If a diverse set of size K is found, it terminates returning the
set. Otherwise, n most promising plan sets from Children
are expanded, and then their n most promising children and
so on until there are no further children to be expanded. It
should also be mentioned at this point, as to what we mean
by ‚Äòmost promising‚Äô, which would be based on f -value of a
plan set (the largest being most promising), and f -value is
in-turn computed as g + h where g is the size of the plan
set and h is the heuristic estimate denoting potential number of plans that can be added to this plan set. In this paper, we have explored using three heuristics: (i) 0, (ii) dispersion of the current set (arithmetic mean of all pair-wise
distances (Myers and Lee 1999)) divided by min dist, and

(iii) quadratic mean of all distances divided by min dist.
The idea behind these heuristics is that the more dispersion
the sets have the more accommodative they could be of new
candidate plans. However, in our experiments, we did not
observe gains of using the dispersion based heuristics in our
experiments compared to the trivial heuristic possibly due
to the limitation of the said heuristics in accounting for the
actions that are not part of the plans found yet, at any given
moment during the runtime. More distance metric based and
domain based estimates can be explored here in future.
Algorithm 3 presents the pseudo-code of ExpandMostPromising routine. It expands the n most promising nodes
from the given list (either G or Open) and puts them in
Children. One significant difference to note here is that,
since all the candidate plans are not available apriori, one
must add the expanded nodes back to G for future consideration with newer candidate plans. While doing so, in order
to avoid repetition, we keep track of the last child generation attempt through the sequence number of candidate plan
considered.
Finally, after the entire stream of candidate solutions has
been processed one by one using MCER, if a diverse set of
size K is not found, we continue to call MCER repeatedly
(this time, without adding back the explored plan sets into G)
until it finds a K-set or terminates exhausting the exploration
of all possible combinations.
Below, we present some of the properties of the proposed
method MCER:
Lemma 4 MCER does not generate the same combination
of plans more than once.
Proof outline:

This is ensured by keeping track of the

Algorithm 3 ExpandMostPromising
1: INPUT :: A set of valid plan sets S to expand, Children,
a distance measure dist(), the minimum distance needed between any two plans of a set min dist, the size of the diverse
plan set K, and the number of plan sets to be explored n.
2: OUTPUT :: Populates Children with new valid plan sets,
returns a diverse plan set of size K if found.
3: T emp ‚Üê œÜ (empty set);
4: for n times do
5:
P ‚Üê most promising plan set from S;
6:
for each candidate plan in the stream from sequence number
i = Pexp + 1 to the latest do
7:
if dist(p, l) > min dist ‚àÄl ‚àà P then
0
8:
P 0 ‚Üê P + {p}; Pexp
‚Üê i;
9:
if |P 0 | = K then
10:
return P 0 ;
11:
end if
12:
Children ‚Üê Children‚à™P 0 ;
13:
end if
14:
Pexp ‚Üê i;
15:
end for
16:
T emp ‚Üê T emp‚à™P ;
17: end for
18: G ‚Üê G‚à™T emp;

sequence number of candidate plan from the last child
generation attempt while expanding a plan set P via Pexp ,
which increases by 1 at each step during expansion (see
Line 14 in Algorithm 3) and the child generation attempts
start from Pexp + 1 every time (see Line 6 in Algorithm 3),
thereby avoiding repetition.
2
Lemma 5 MCER expands at-most n √ó (K ‚àí 1) + 1 number
of plan sets in each execution.
Proof outline: Note that, after expansion of n most promising nodes from G, their n most promising children, and then
their n most promising children and so on are expanded,
until a child of size K is found. Further, size of the children
at each step increases by 1 since a new candidate plan gets
added to the plan set. Therefore, even if we assume that the
initial set of seed nodes are all of size 1, MCER executes
at-most K steps at which point a diverse set of size K will
be found if possible through that set. And at each step,
at-most n number of children are expanded, with only 1 at
level K. Hence, together, at-most n √ó (K ‚àí 1) + 1 number
of plan sets are expanded in each execution of MCER. 2
Lemma 6 MCER guarantees finding a diverse set of plans
of size K if there exists one.
Proof outline: Note that, while we execute MCER several
times with incoming plans from the stream of candidate
plans, each time without exhausting all possible combinations, we keep track of the last expansion attempt for
each node (plan set), and store them in G. Hence all plan
sets which may not have been exhaustively explored with
the candidate plans are present in G when the entire set
of candidate plans has been generated. These plan sets are
then exhaustively explored without re-inserting back in to G

thereby guaranteeing completeness and termination.

2

Now, given a planning problem, a cost metric, a distance
measure, max cost, min dist, and K, for finding a set of
K cost sensitive diverse plans, one could use any one of the
following four combinations: 1) DFA with ACER, wherein
the DFA is executed and whenever a valid plan with cost
< max cost is found, ACER is invoked to find a diverse
set, and then the execution of DFA is continued if a diverse
set with the given requirements is not found, and the process is repeated until termination. We call this combination
DFAA. 2) DFA with MCER, similar to the above strategy of
invoking MCER whenever DFA find a valid cost sensitive
plan, followed by repeated calls to MCER at the end to explore all the remaining plan combinations until termination.
This is denoted by DFAM. 3) A*A with ACER (denoted
by A*AA), and 4) A*A with MCER (denoted by A*AM).
Next section presents the comparison of performances of the
above combinations of methods.

Experimental Results
In this section, we present the experimental results comparing the performances of various proposed algorithms among
themselves as well as with a greedy approach proposed in
the literature. We have implemented all our methods on
top of the Fast Downward planning environment (Helmert
2006), and hence could run problem instances from any of
the supported planning domains. Accordingly, we have conducted experiments on several domains, including, blocks,
rovers, pathways-noneg, airport, driverlog, tpp, zenotravel.
We present the representative results in this paper. All the experiments have been performed on a machine with Intel(R)
Xeon(R) CPU E5-1620 v2 at 3.70GHz and 64GB RAM. The
following distance measure for measuring diversity has been
adopted from (Nguyen et al. 2012):
dist(p1 , p2 ) = 1 ‚àí

A(p1 )‚à©A(p2 )
A(p1 )‚à™A(p2 )

(2)

where A(p) denotes the set of all actions in plan p.
Table 1 shows the comparison of DFAA and A*AA methods on problems (denoted by P.no.) from Blocks domain.
We have used the LM cut heuristic which is admissible, to
guide the search. The algorithms are given a maximum time
of 60sec for solving each problem. Given a set of inputs,
the output shows whether a diverse set of plans of size K is
found (otherwise the size of the largest diverse set found in
parenthesis), the time taken, and the number of plans generated during the process. * denotes that the algorithm stopped
due to the time limit. We see that the DFA based method
generates the cost sensitive plans faster than the A*A based
method in this case, resulting in the processing of more number of plans in a given time.
The difference in the number of plans generated to find
a diverse set of same size highlights the importance of the
order in which the candidate plans are generated. Depending on the order of the plans generated, the number of plans
required to be processed by an exhaustive algorithm to produce a diverse set of specific size varies. As mentioned before, this could be influenced by devising an appropriate distance metric and domain dependent feedback mechanism.

Table 1: Comparison of DFAA and A*AA complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

K

max cost

4
8

20

8

30

8

30

DFAA
min dist
0.6
0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (4)
No* (3)

A*AA

Time (Sec.)

Plans gentd.

0.00
0.00
32.90
2.14
60.00
60.00

22
43
231
323
2567
5140

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
11.06
3.42
60.00
60.00

26
43
130
323
923
4115

Table 2: Comparison of DFAM and A*AM complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAM

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (2)

A*AM

Time (Sec.)

Plans gentd.

0.00
0.00
0.58
1.74
60.00
60.00

26
43
323
323
11680
11908

K-set
found?
Yes
No (4)
Yes
No (5)
No* (7)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
2.62
60.00
60.00

26
43
172
323
12226
12311

Table 3: Comparison of DFA based and A*A based greedy algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAG

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
No (3)
No (3)
No (6)
No (4)
No (6)
No (2)

A*AG

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
0.02
16.70
24.24

43
43
323
323
104712
104712

Table 2 presents the comparison of DFAM and A*AM
methods (with plan-combinations seed set size equal to 30
in each execution). Note that, both algorithms guarantee to
find a diverse set of required size if there exists one, given
they are given enough time to terminate. Our objective with
the MCER based methods is to quickly process the incoming
candidate plans so as to find a diverse set quicker, postponing the exhaustive exploration to the end. Accordingly, we
see that both the methods perform much better than they did
compared to Table 1 by processing larger number of plans.
They are able to find larger sized sets of diverse plans in the
given time than before, although, as one can observe it may
also happen that the select exploration could occasionally
(see DFAM vs DFAA in last rows of both the tables) delay
finding large sets compared to ACER based approaches.
Next, we present the results obtained by integrating the
greedy approach discussed in the Introduction with DFA and
A*A, in Table 3. We call these methods DFAG and A*AG
respectively. As one can observe, while the greedy methods
process the incoming candidate plans very fast, they terminate without finding a diverse set of given size even when
there exists one. Furthermore, even for finding the diverse
sets that they produced, they involve generating far more
number of plans compared to the complete algorithms. This
can be a crucial element when finding multiple plans is difficult for a domain. Also, note that, A*A runs out of memory (4GB per instance) in this case which makes the case

K-set
found?
No (3)
No (3)
No (7)
No (3)
No* (6)
No* (2)

Time (Sec.)

Plans gentd.

0.00
0.00
0.06
0.04
Mem-limit
Mem-limit

43
43
323
323
101908
101908

for memory bounded methods while attempting to generate multiple solutions. Although, one can improve the performance of the greedy approach through feedback mechanisms, considering only one seed plan set for exploration is
likely to continue to affect the performance. Thus, it would
be beneficial to have multiple seed plan sets to be explored
at each stage for better performance.
Now, we present the results obtained on two other domains, namely, Rovers and Zeno-Travel. We show the results with DFA as the base method (for generating all costbounded solutions) in these cases since A*A based methods
were quickly reaching the memory limit on these instances.
Table 4 shows the comparison of DFAA, DFAM and DFAG
methods on a problem from the Rovers domain with 14 objects. Here, a diverse set of size 8 with cost bound 20 is to
be found within 60 seconds. Three sets of results comparing the above three algorithms are presented with different
diversity criterion in each case. Note that, amongst the three
methods, DFAA spends the most amount of effort on exploring plan combinations (exhaustive) whenever a new plan is
found, therefore is only able to generate and process a small
number of plans. Since DFAG spends least amount of effort on exploring plan combinations (greedy) upon finding
a new plan, it is able to generate and scan through a large
number of plans. Whereas DFAM distributes its effort intelligently across plan generation and plan combination exploration, by adjusting the number of seeds n as per domain and

Table 4: Comparison of DFAA, DFAM and DFAG methods. Domain: Rovers. Problem: roverprob4213 (14 objects), K: 8,
max cost : 20, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
Yes
No* (6)
No* (4)

DFAM

Time (Sec.)

Plans gentd.

0.88
60.00
60.00

64
372
1048

K-set
found?
Yes
Yes
No* (6)

DFAG

Time (Sec.)

Plans gentd.

0.00
9.84
60.00

65
306061
2047871

K-set
found?
Yes
Yes
No* (4)

Time (Sec.)

Plans gentd.

0.00
1.22
60.00

64
304553
15988265

Table 5: Comparison of DFAA, DFAM and DFAG methods. Domain: Zeno-Travel. Problem: ZTRAVEL-2-5 (17 objects), K: 8,
max cost : 15, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
No* (6)
No* (4)
No* (3)

DFAM

Time (Sec.)

Plans gentd.

60.00
60.00
60.00

294
579
1171

K-set
found?
Yes
Yes
Yes

problem size (in this case, n = 30). Note that, MCER with
n = 1 would result in an exploration similar to that of the
greedy method, with the exception of going further and guaranteeing completeness. And, MCER with n = ‚àû would result in an exploration similar to that of ACER. Results show
that DFAA performs poorly on large instances due to its exhaustive exploration. Between DFAM and DFAG, on easier
problems (with low diversity/distance requirement; first 2 instances), greedy and MCER based methods fare similarly,
with the greedy method slightly outperforming the MCER
based method (note that, one can change the n value to 1
here, to make MCER deliver results similar to that of the
greedy method, however, this is not beneficial in general).
On the other hand, when the diversity required is higher
(third instance), MCER based method outperforms greedy
approach by finding a larger diverse set using only 12.81%
of the plans generated by that of the greedy method, showcasing the advantage of exploring plan combinations more
thoroughly.
Table 5 presents the comparison of DFAA, DFAM and
DFAG methods on a problem from the Zeno-Travel domain
with 17 objects. Once again, we observe similar results as
that of the previous two domains. ACER based method fares
poorly due to its exhaustive nature which limits its reach
in scanning through the full space plans in the given time.
And, between DFAM and DFAG, while DFAG may find
the K-set in shorter time in some cases, DFAM continues
to leverage the advantage of exploring plan combinations
thoroughly and is able to find required K-sets using lesser
number of plans. This is a crucial element in working with
domains where producing individual plans itself is very difficult, which is especially the case when large problem sizes
are involved.
Before we conclude, we present a note on solving large
sized problems. In such cases, while completeness may not
be a practical expectation, one should be able to gain performance over using the greedy approach by carefully integrating the proposed MCER approach with the state-of-the-art
solvers/planners, feedback mechanisms, and using efficient
heuristics for exploring the space of plan combinations. Fur-

DFAG

Time (Sec.)

Plans gentd.

0.18
0.62
21.46

1447
7884
505186

K-set
found?
Yes
Yes
Yes

Time (Sec.)

Plans gentd.

0.20
0.44
10.30

1447
8545
619579

thermore, the proposed methods can be easily extended to
solve related problems such as, finding a K-set with maximum diversity, finding largest K-set with a given diversity,
finding high quality (in terms of the cost of the plans) K-sets
with given diversity, and a combination thereof involving the
generation of multi-objective pareto fronts.

Conclusion
In this paper, we take a combinatorial search perspective of
the widely studied diverse solution generation problem. We
observe that many of the approaches proposed in various
domains such as automated planning and constraint satisfaction use a linear-time greedy method for exploring plan
set combinations, that makes them fail while searching for
a diverse set of required size even when there exists one.
We propose a bi-level optimization framework and methods
to find cost-sensitive diverse solutions which guarantee to
find a diverse set of required size whenever there exists one.
We identify the critical elements that affect the performance
in such scenarios and propose efficient methods to handle
them. We showcased the efficacy of the proposed methods
by implementing our methods as part of the Fast Downward
planning system and comparing with the existing greedy approach across various domains. The proposed methods have
found larger sets of diverse solutions compared to the greedy
approach on almost all problem instances, within the same
time bound, proving their utility.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
Alexiadis, A., and Refanidis, I. 2013. Generating alternative plans
for scheduling personal activities. In Proceedings of the Seventh
Scheduling and Planning Applications workshop, 35‚Äì40.
Bloem, M., and Bambos, N. 2014. Air traffic control area configuration advisories from near-optimal distinct paths. Journal of
Aerospace Information Systems 11(11):764‚Äì784.

Bloem, M. J. 2015. Optimization and Analytics for Air Traffic
Management. Ph.D. Dissertation, Stanford University.
Boddy, M. S.; Gohde, J.; Haigh, T.; and Harp, S. A. 2005. Course
of action generation for cyber security using classical planning.
In Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005), June 5-10 2005,
Monterey, California, USA, 12‚Äì21.
Bryce, D. 2014. Landmark-based plan distance measures for diverse planning. In Proceedings of the Twenty-Fourth International
Conference on Automated Planning and Scheduling, ICAPS 2014,
Portsmouth, New Hampshire, USA, June 21-26, 2014.
Bylander, T. 1991. Complexity results for planning. In Proceedings
of the 12th International Joint Conference on Artificial Intelligence
- Volume 1, IJCAI‚Äô91, 274‚Äì279. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc.
Coman, A., and MunÃÉoz-Avila, H. 2011. Generating diverse plans
using quantitative and qualitative plan distance metrics. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11,
2011.
Coman, A., and MunÃÉoz-Avila, H. 2012a. Diverse plan generation
by plan adaptation and by first-principles planning: A comparative
study. In Case-Based Reasoning Research and Development - 20th
International Conference, ICCBR 2012, Lyon, France, September
3-6, 2012. Proceedings, 32‚Äì46.
Coman, A., and MunÃÉoz-Avila, H. 2012b. Plan-based character
diversity. In Proceedings of the Eighth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-12,
Stanford, California, October 8-12, 2012.
Coman, A., and MunÃÉoz-Avila, H. 2013. Automated generation of
diverse npc-controlling fsms using nondeterministic planning techniques. In Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-13,
Boston, Massachusetts, USA, October 14-18, 2013.
Coman, A. 2012. Solution diversity in planning. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Dechter, R.; Flerova, N.; and Marinescu, R. 2012. Search algorithms for m best solutions for graphical models. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Do, M. B., and Kambhampati, S. 2001. Planning as constraint
satisfaction: Solving the planning graph by compiling it into CSP.
Artif. Intell. 132(2):151‚Äì182.
Eiter, T.; Erdem, E.; Erdogan, H.; and Fink, M. 2013. Finding
similar/diverse solutions in answer set programming. Theory and
Practice of Logic Programming 13(03):303‚Äì359.
Gerevini, A.; Saetti, A.; and Serina, I. 2003. Planning through
stochastic local search and temporal action graphs in lpg. J. Artif.
Int. Res. 20(1):239‚Äì290.
Goldman, R. P., and Kuter, U. 2015. Measuring plan diversity:
Pathologies in existing approaches and A new plan distance metric. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., 3275‚Äì
3282.
Hebrard, E.; Hnich, B.; O‚ÄôSullivan, B.; and Walsh, T. 2005. Finding diverse and similar solutions in constraint programming. In
Proceedings of the 20th National Conference on Artificial Intelligence - Volume 1, AAAI‚Äô05, 372‚Äì377. AAAI Press.
Hebrard, E.; O‚ÄôSullivan, B.; and Walsh, T. 2007. Distance constraints in constraint satisfaction. In Proceedings of the 20th In-

ternational Joint Conference on Artifical Intelligence, IJCAI‚Äô07,
106‚Äì111. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc.
Helmert, M. 2006. The fast downward planning system. J. Artif.
Intell. Res. (JAIR) 26:191‚Äì246.
Kambhampati, S. 2007. Model-lite planning for the web age
masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, July 22-26, 2007, Vancouver,
British Columbia, Canada, 1601‚Äì1605.
Lawler, E. L., and Wood, D. E. 1966. Branch-and-bound methods:
A survey. Operations Research 14(4):699‚Äì719.
Myers, K. L., and Lee, T. J. 1999. Generating qualitatively different
plans through metatheoretic biases. In AAAI, 570‚Äì576. American
Association for Artificial Intelligence.
Nadel, A. 2011. Generating diverse solutions in sat. In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing, SAT‚Äô11, 287‚Äì301. Berlin, Heidelberg:
Springer-Verlag.
Nguyen, T. A.; Do, M. B.; Gerevini, A.; Serina, I.; Srivastava, B.;
and Kambhampati, S. 2012. Generating diverse plans to handle
unknown and partially known user preferences. Artif. Intell. 190:1‚Äì
31.
Petit, T., and Trapp, A. C. 2015. Finding diverse solutions of
high quality to constraint optimization problems. In IJCAI. International Joint Conference on Artificial Intelligence.
Roberts, M.; Howe, A.; Ray, I.; and Urbanska, M. 2012. Using
planning for a personalized security agent. In AAAI Workshops.
Roberts, M.; Howe, A. E.; and Ray, I. 2014. Evaluating diversity
in classical planning. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Russell, S., and Norvig, P. 1995. Artificial intelligence: a modern
approach. Prentice Hall.
Rymon, R. 1992. Search through systematic set enumeration. Technical Reports (CIS) 297.
Scala, E. 2014. Plan repair for resource constrained tasks via numeric macro actions. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Srivastava, B.; Nguyen, T. A.; Gerevini, A.; Kambhampati, S.; Do,
M. B.; and Serina, I. 2007. Domain independent approaches for
finding diverse plans. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad,
India, January 6-12, 2007, 2016‚Äì2022.
Vadlamudi, S. G.; Aine, S.; and Chakrabarti, P. P. 2013. Anytime
pack heuristic search. In Pattern Recognition and Machine Intelligence - 5th International Conference, PReMI 2013, Kolkata, India,
December 10-14, 2013. Proceedings, 628‚Äì634.
Vadlamudi, S.; Aine, S.; and Chakrabarti, P. 2015. Anytime pack
search. Natural Computing 1‚Äì20.
Vadlamudi, S. G.; Chakrabarti, P. P.; and Sarkar, S. 2012. Anytime
algorithms for mining groups with maximum coverage. In Tenth
Australasian Data Mining Conference, AusDM 2012, Sydney, Australia, December 5-7, 2012, 209‚Äì220.
Voss, C.; Moll, M.; and Kavraki, L. E. 2015. A heuristic approach
to finding diverse short paths. In IEEE International Conference
on Robotics and Automation, ICRA 2015, Seattle, WA, USA, 26-30
May, 2015, 4173‚Äì4179.

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence Multiagent Interaction without Prior Coordination: Technical Report WS-16-11

A Game Theoretic Approach to Ad-hoc Coalitions in Human-Robot Societies
Tathagata Chakraborti, Venkata Vamsikrishna Meduri, Vivek Dondeti, Subbarao Kambhampati
Department of Computer Science Arizona State University Tempe, AZ 85281, USA {tchakra2,vmeduri,vdondeti,rao}@asu.edu Abstract
As robots evolve into fully autonomous agents, settings involving human-robot teams will evolve into humanrobot societies, where multiple independent agents and teams, both humans and robots, coexist and work in harmony. Given such a scenario, the question we ask is - How can two or more such agents dynamically form coalitions or teams for mutual benefit with minimal prior coordination? In this work, we provide a game theoretic solution to address this problem. We will first look at a situation with full information, provide approximations to compute the extensive form game more efficiently, and then extend the formulation to account for scenarios when the human is not totally confident of its potential partner's intentions. Finally we will look at possible extensions of the game, that can capture different aspects of decision making with respect to ad-hoc coalition formation in human-robot societies.

Robots are increasingly becoming capable of performing daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made the dream of human-robot cohabitation a not so distant reality. In this work we envisage such an environment where humans and robots participate autonomously (possibly with required interactions) with their own set of tasks to achieve. It has been argued (Chakraborti et al. 2016) that interactions in such situations are inherently different from those studied in traditional human-robot teams. One typical aspect of such interactions is the lack of prior coordination or shared information, due to the absence of an explicit team. This brings us to the problem we intend to address in this paper - given a set of tasks to achieve, how can an agent proceed to select which one to achieve? In a shared environment such as the one we described, this problem cannot be simply solved by picking the goal with the highest individual utility, because the utility, and sometimes even the success of the plan (and hence the corresponding goal) of an agent are contingent on the intentions of the other agents around it. However, such interactions are not adversarial - it is just that the environment is shared among self-interested agents. Thus, an agent may choose to form an ad-hoc team with another agent in order to increase its utility, and such coalition formation should preferably be feasible with minimum prior

coordination. For example, a human with a goal to deliver two items to two different locations may team up with a delivery robot that can accomplish half of his task. Further, if the robot was itself going to be headed in one of those directions, then it is in the interest of both these agents to form this coalition. However, if the robot's plan becomes too expensive as a result, it might decide that there is not enough incentive to form this coalition. Moreover, as we highlighted before, possible interactions between agents are not just restricted to cooperative scenarios only - the plans of one agent can make the other agent's plans fail, and it may happen that it is not feasible at all for all agents to achieve their respective goals. Thus there are many possible modes of interaction between such agents, some cooperative and some destructive, that needs to be accounted for before the agents can decide on their best course of action - both in terms of which goal to choose and how to achieve it. In this paper we model this problem of optimal goal selection as a two player game with perfect information, and propose to cut down on the prior coordination of forming such ad-hoc coalitions by looking for Nash equilibriums or socially optimal solutions (because neither agent participating in such a coalition would have incentive to deviate). We subsequently extend it to a Bayesian game to account for situations when agents are not sure of each other's intent. We will also look at properties, approximations, and interesting caveats of these games, and motivate several extensions that can capture a wide variety of ad-hoc interactions.

1

Related Work

There is a huge variety of work that looks at team formation from different angles. The scope of our discussion has close ties with concepts of required cooperation and capabilities of teams to solve general planning problems, introduced in (Zhang and Kambhampati 2014), and work on team formation mechanisms and properties of teams (Shoham and Tennenholtz 1992; Tambe 1997). However, in this particular work, we are more interested in the mechanism of choosing goals that can lend to possible cooperative interactions, as opposed to the mechanism of team design based on the goals themselves. Thus the work of Zhang and Kambhampati can provide interesting heuristics towards cutting down on the computation of the extensive form game we will propose, while existing work on different modes of team formation

546

contribute to the motivation of the Bayesian formulation of the game discussed in later sections. From the game theoretic point of view, coalition formation has been a subject of intense study (Ray and Vohra 2014) and the human-robot interaction community can derive significant insights from it. Of particular interest are Overlapping Coalition Formation or OCF Games (Zick, Chalkiadakis, and Elkind 2012; Zick and Elkind 2014), which look at a cooperative game where the players are endowed with resources, with provisions for the players to display different modes of coalitions based on how they utilize the resources. OCF games use arbitration functions that decide the payoffs for the deviating players based on how it is affecting the non-deviating players and it helps in forming stable coalitions. This becomes increasingly relevant in shared environments such as the one we discuss here. Finally, an interesting problem that can often occur is such situations (especially with the way we have formulated the game in the human's favor) is the problem of free-riding where agents take advantage of coalitions and try to minimize their effort (Ackerman and Br^ anzei 2014), which is certainly an important aspect of designing such games.

2.2

Representation of Human-Robot Coalitions

We will represent coalitions of such agents by means of a super-agent transformation (Chakraborti et al. 2015a) on a set of agents that combines the capabilities of one or more agents to perform complex tasks that a single agent might not be capable of doing. Note that this does not preclude joint actions among agents, because some actions that need that need more than one agent (as required in the preconditions) will only be doable in the composite domain. Definition 1.1 A super-agent is a tuple  = , D where    is a set of agents in the environment E , and D is the transformation from the individual domain models to a composite domain model given by D = FO ,  A . Definition 1.2 The planning problem of a super-agent  is given by  = FO , D , I , G where the composite initial and goal states are given by I =  I and G =  G respectively. The solution to the planning problem is a composite plan  = µ1 , µ2 , . . . , µ| | where µi = {a1 , . . . , a|| }, µ() = a  A µ   such that  (I ,  ) |= G , where the modified transition function  (µ, s) = (s \ aµ eff- (a))  aµ eff+ (a). The cost of a composite plan is C ( ) = µ aµ Ca and    is optimal if C ( )  C ( ) with  (I ,  ) |= G . The composite plan can be viewed as a union of plans contributed by each agent    whose component can be written as  () = a1 , a2 , . . . , an , ai = µi ()  µi   .

2
2.1

Preliminaries

Environment and Agent Models

Definition 1.0 The environment is defined as a tuple E = F, O, , G ,  , where F is a set of first order predicates that describes the environment, and O is the set of objects,   O is the set of agents (which may be humans or robots), G = {g | g  FO } 1 is the set of goals that these agents are tasked with, and   O is the set of resources. Each goal has a reward R(g )  R+ associated with it. We use PDDL (Mcdermott et al. 1998) style agent models for the rest of the discussion, but most of the analysis easily generalizes to other modes of representation. The domain model D of an agent    is defined as D = FO , A , where A is a set of operators available to the agent. The action models a  A are represented as a = Ca , Pa , Ea where Ca is the cost of the action, Pa  FO is the list of pre-conditions that must hold for the action a to be applicable in a particular state S  FO of the environment; and Ea = ef f + (a), ef f - (a) , ef f ± (a)  FO is a tuple that contains the add and delete effects of applying the action to a state. The transition function  (∑) determines the next state after the application of action a in state S as  (a, S ) |=  if Pa  S ; |= (S \ ef f - (a))  ef f + (a) otherwise. A planning problem for the agent  is given by the tuple  = FO , D , I , G , where I , G  FO are the initial and goal states respectively. The solution to the planning problem is an ordered sequence of actions or plan given by  = a1 , a2 , . . . , a| | , ai  A such that  ( , I ) |= G , where the cumulative transition function is given by  (, s) =  ( a2 , a3 , . . . , a|| ,  (a1 , s)). The cost of the plan is given by C ( ) = a Ca and the optimal plan    is such that C ( )  C ( )  with  ( , I ) |= G .
1

2.3

The Use Case

Throughout the rest of the discussion we will use the setting from Talamadupula et al. which involves a human commander CommX and a robot in a typical Urban Search and Rescue (USAR) scenario, as illustrated in Figure 1. The environment consists of interconnected rooms and hallways, which the agents can navigate and search. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other agents (not shown) in the environment. A sample domain is available at http://bit.ly/1Fko7MAhttp://bit.ly/1Fko7MA. The shared resources here are the two medkits - i.e. some of the plans the agents can execute will lock the use of and/or change the position of these medkits, so as to make the other agent's plans, contingent on that particular resource, invalid.

SO is any S  F instantiated / grounded with objects from O.

Figure 1: Use case - Urban Search And Rescue (USAR).

547

3

Ad-hoc Human-Robot Coalitions

In this section we will look at how two agents (the human and the robot) in our scenario, can coordinate dynamically by forming impromptu teams in order to achieve either individually rational or socially optimal behaviors.

3.1

Motivation

 = , D is the super-agent representing the coalition formed by  = {H, R} with I = IH  IR and j G = Gi H  GR . Here, the first term in the expression for utility denotes the utility of the goal itself as defined in the environment in Section 2.1, while the second term captures the resultant best case utility of plans due to agent interactions. More on this below. Human-centric robots. At this point we make an assumption about the role of the robots in our human-robot society - we assume that the robots exist only in the capacity of autonomous assistance, i.e. in coalitions that may be formed with humans and robots, the robot's role is to improve the quality of life of the humans (by possibly, in our case, reducing the costs of plans) and not vice versa. Thus, in the expression of utility, the human uses a min  imizing term - with no interactions C (H ) = C ( (H )),   otherwise C (H ) > C ( (H )). Similarly, in case of   the robot, with no interactions C (R )  C ( (R)) and   C (H ) <=> C ( (H )) otherwise, since the interactions may or may not be always cooperative for the robot. Note that this formulation also takes care of the cases when the robot goal becomes unachievable due to negative interactions with the human (this is why we have the maximizing term; the difference is triggered due to negative interactions with the human plan in absence of coalitions). Also note that the goal utility is using a combined goal due to the particular action profile, this captures cases when goals have interactions, i.e. a conjunction of goals may have higher (or lower) utility than the sum of its components. This can be easily ensured while generating plans for a given coalition, by either discounting the costs of actions of the robot with respect to those of the humans by a suitable factor, or more preferably, by just penalizing the total cost of the human component in the composite plan more. The assumption of course does not change the formulation in any way, it is just more aligned with the notion of the social robots being envisioned currently. Of course, in this sense the utilities of both the humans and robots will now become identical, with a minimizing cost term. Now that we have defined the game, the question is how do we choose actions for each agent? Remember that we want to find solutions that will preclude the need to coordinate. We can take two approaches here - we can make agents individually rational (in which case both the human and the robot looks for a Nash equilibrium, so neither has a reason to defect; or we can make the agents look for a socially optimal solution (so that sum of utilities is maximized).

Consider the scenario shown in Figure 1. Suppose one of CommX's goal is to perform triage in room1, while one of the Robot's goals is to deliver a medkit to room1. Clearly, if both the agents choose to do their optimal plans and plan to use medkit1 in room2, the Robot's plan fails (assuming the CommX gets there first). The robot then has two choices - (1) it can choose to achieve some other goal, i.e. maximize it's own rewards, (2) it can choose to deliver the other medkit2 from room3, i.e. maximize social good. Indeed there are many possible ways that these agents can interact. For example, the utility of choosing any goal may be defined by the optimal cost of achieving that goal individually, or as a team. This in turn affects the choice whether to form such teams or not. In the discussion that follows, we model this goal selection (and team formation) problem as a strategic game with perfect information.

3.2

Formulation of the Game

We refer to our static two-player strategic game Goal Allocation with Perfect Information as GAPI = , {A }, {U  } . The game attempts to determine, given complete information about the domain model and goals of the other agent, which goal to achieve and whether forming a coalition is beneficial. The game is defined as follows - Players - The game has two players  = {H, R} the human H and the robot R respectively. - Actions - The actions of the agents in the strategic game are the goals that they can select to achieve. Thus, for each agent    we define a set of goals G  = |G  | 2  {G1  , G , . . . , G }  G , and the action set A of the agent  is the mapping that assigns one of these goals as its planning goal, i.e. A : G  - G. Note that this is distinct from the action models defined in PDDL for each of the individual agents (which helps the agent figure out how this goal G is achieved, and the resultant utility). - Utilities - Finally, as discussed previously, the utility of an action depends on (apart from the utility of the goal itself) the way the agent chooses to achieve it, and is contingent also on the plans of the other agent (due to, for example, resource conflicts), and is given by --
R i j   UH (AH i , Aj ) = R(GH  GR ) - min{C (H ), C ( (H ))} R UR (AH i , Aj ) i    = R(GH  Gj R ) - C ( (R)) if C (H ) > C ( (H )) j i   = R(GH  GR ) - max{C (R ), C ( (R))}, otherwise.

3.3

Solving for Nash Equilibriums

 where, H is the optimal plan or solution of the planning  problem defined by H = FO , DH , IH , Gi H , R is the j  optimal solution of R = FO , DR , IR , GR , and  is the optimal solution of  = FO , D , I , G , where

As usual, the Nash equilibriums in GAPI are R given by action profiles AH such that i , Aj H R H R R UH (Ai , Aj )  UH (Ak : k=i , Aj ) and UR (AH i , Aj )  H R UH (Ai , Ak : k=j ). It is easy to prove that there is no guaranteed Nash equilibrium in GAPI. We will instead motivate a slightly different game GAPI-Bounded where the robot only agrees to deviate from its optimal plan up to a certain degree, i.e. there is a bound on the amount of assistance the robot chooses to provide.

548

Definition 1.3. The differential help  (g, Gi R ) provided i R by the robot R with goal GR  G , when the human H picks goal g  G H , measures the decrease in utility of the robot upon forming a coalition with the human, and is    given by  (g, Gi R ) = |C ( (R)) - C (R )|, where R is i  the optimal solution of R = FO , DR , IR , GR , and  is the optimal solution of  = FO , D , IH  IR , g  Gi R , where  =  = {H, R}, D . Thus in GAPI-Bounded the utility function is modified from the one in GAPI as follows R i  UH (AH i , Ai ) = R(GH ) - C (H ) j R  UR (AH i , Aj ) = R(GR ) - C (R ) : k=j j j  if Gk  G H s.t.  (Gi H , GR ) > {R(GR ) - C (R )} - R k     {R(GR ) - C (R )}, where R , R and H are the optimal plans or solutions to the planning problems j k k i R = FO , DR , IR , GR , R = FO , DR , IR , GR and respectively; and otherwise H = FO , DH , IH , Gi H R i  UH (AH i , Ai ) = R(GH ) - C ( (H )) j R  UR (AH i , Aj ) = R(GR ) - C ( (R))  where  is the optimal solution of  = FO , D , IH  IR , g  Gj R , where  =  = {H, R}, D . This basically means that if the penalty that the robot incurs by choosing to assist the human is so great that it could rather do something else instead (i.e. choose another goal), then it switches back to using its individual optimal plan, i.e. no coalition is formed. If the individual optimal plans are always feasible (otherwise these do not participate in the Nash equilibriums below), this leads to the following result. R Claim. AH must be a Nash equilibrium of i , Aj  GAPI-Bounded when j  = arg maxGj G R R(Gj R) - j C (R ) and i = arg maxi UH (Gi H , GR ).
 R

Further, it may be noted here that there may be many such Nash equilibriums in GAPI-Bounded and these are also the only ones, i.e. all Nash equilibriums in GAPI-bounded must satisfy the conditions in the above claim.

3.4

Solving for Social Good

Similarly, the socially optimal goal selection strategies are   R given by the action profiles AH i , Aj  where {i , j } = R H R arg maxi,j UH (AH i , Aj ) + UR (Ai , Aj ). The socially optimal action profiles may not necessarily correspond to any Nash equilibriums of either GAPI or GAPI-Bounded. Individual Irrationality and -Equilibrium. Given the way the game is defined, it is easy to see that the socially good outcome may not be individually rational for either the human or the robot, since the robot always has the incentive to defect to choosing G R and the human will then choose the corresponding highest utility goal for himself. This leaves room for designing autonomy that can settle for action proH R files A^ , A^ referred to as -equilibriums, for the purpose i j
R H R of social good, i.e. |UH (AH , A^ )|  and i , Aj  ) - UH (A^ i j R H R |UR (AH , A^ )|  . Note that this deviai , Aj  ) - UR (A^ i j tion is distinct from the concept of bounded differential assistance we introduced in Section 3.3.

Price of Anarchy. The price of deviating from individual rationality is referred to as the Price of Anarchy and is measured by POS =
H R H R UH (A^ ,A^ )+UR (A^ ,A^ ) i j i i

UH (AH ,AR )+UR (AH ,AR ) i j i j

.

3.5

Caveats

Proof Sketch. Let us define the utility function of the robot R for achieving a goal g  G R by itself as  (g ) =   R(g ) - C (R ), where R is the optimal solution to the planning problem R = F, O, DR , IR , g . Further, given the  goal set G R of the robot, we set Gj R = arg maxg G R  (g ),  i.e. Gj R corresponds to the highest utility goal that the robot can achieve by itself. Now consider any two goals j j j R i H Gj R , GR  G , GR = GR . We argue that GH  G , j H R H R UR (Ai , Aj  )  UR (Ai , Aj ). This is because  (GR )  H R  (Gj R ) and by problem definition i, k |UR (Ai , Aj  ) -
R UR (AH - Thus, in general, the k , Aj  )|  goal ordering induced by the function  is preserved by the utility function UR , and consequently AR j  is a dominant strategy of the robot. It follows that AH i such that  i = arg maxi UH (Gi , G ) is the corresponding best reH R R sponse for the human. Hence AH , A must be a Nash   i j equilibrium. Hence proved.
  (Gj R)

No or Multiple Nash Equilibriums. One of the obvious problems with this approach is that it does not guarantee a unique Nash equilibrium, if it exists at all. This has serious implications on the problem we set out to solve in the first place - which goals do the agents choose to plan for, and how? Note, however, that this is not really a feature of the formulation itself but of the domain or the environment, i.e. the action models of the agents and the utilities in the goals will determine whether there is a single best coalition that may be formed given a particular situation. Thus, there seems to be no principled way of solving this problem in a detached manner, without any form of communication between the agents. But our approach still provides a way to deliberate over the possible options, and communicate to resolve ambiguities only with respect to the Nash equilibriums, rather than the whole set of goals, or even just those in each agent's dominant strategy, which can still provide significant reduction in the communication overhead. Infeasibility of the Extensive Form Game. Note here that the utilities of the actions are calculated from the cost of plans to achieve the corresponding goals, which involves solving two planning problems per action. This means that, in order to get the extensive form of GAPI, we need to solve O(|G H |◊|G R |) planning problems in total (note that solving for  gives utilities for both agents H and R), which may be infeasible for large domains. So we need a way to speed up our computation (either by computing an approximation

 (Gj R ).

549

and/or finding ways to calculate multiple utility values at once), while simultaneously preserving guarantees from our original game in our approximate version. Fortunately, we have good news. Note that all we require are costs of the plans, not the plans themselves. So a promising approach towards cutting down on the computational complexity is by using heuristic values for the initial state of a particular planning problem as a proxy towards the true plan cost. Note that the better the heuristic is, the better our approximation is. So the immediate question is - What guarantees can we provide on the values of the utilities when we use heuristic approximation? Are the Nash equilibriums in the original game still preserved? This brings us to the notion of "well-behaved heuristics" as follows Definition 1.4 A well-behaved heuristic h : S ◊ S  R+ , S  FO is such that h(I, G1 )  h(I, G2 ) whenever     C (1 )  C (2 ), where 1 and 2 are the optimal solutions to the planning problems 1 = FO , D, I, G1 and 2 = FO , D, I, G2 respectively. We define GAPI as a game identical to GAPI but with a modified utility function as follows R i i i UH (AH i , Aj ) = R(GH ) - min{h(GH , IH ), h(GH , IH  IR )} R UR (AH i , Aj ) i i i = R(GH ) - h(Gj R , IH  IR ) if h(GH , IH ) > h(GH , IH  IR ) j j ) - max { h ( G , I ) , h ( G , I  I ) } , otherwise. = R(Gi R H R R R H

i-1 1 1 ^ ^ ^ max{h(I, Gi  ), h(I, G )}; h(I, G ) = h(I, G ). Then h is well-behaved. Hence proved. These properties of GAPI-Bounded, GAPI and GAPI enables computation of approximations, and partial profiles, to the extensive form of GAPI, while maintaining the nature of interactions, thus making the formulation more tractable.

4
4.1

Bayesian Modeling of Teaming Intent
Motivation

In the previous sections we considered both individual and team plans, and as teams we considered optimal plans for a coalition. In reality there are many ways that a particular coalition can achieve a particular goal, and correspondingly there are different modes of interaction between the teammates. We discuss four such possibilities briefly here ∑ Individual Optimality - In this type of planning, each agent computes the individual optimal plan to achieve their goals. Note that this plan may not be actually valid in the environment during execution time, due to factors such as resource conflicts due to plans of the other agents. ∑ Joint Optimality - Here we compute the joint optimal for a coalition; and this optimal plan is computed in favor of the human as discussed previously in Section 3.2. ∑ Planning with Resource Conflicts - In (Chakraborti et al. 2015b) we explored a technique for the robot to produce plans so as to ensure the success of the human plans only, and explored different modes of such behavior of the robot in terms of compromise, opportunism and negotiation. Thus utilities for the human plans computed this way is, at times, same as the joint optimal, but in general is greater than or equal to the individual optimal and less than or equal to the joint optimal. ∑ Planning for Serendipity - In (Chakraborti et al. 2015a) we looked at a special case of multi-agent coordination, where the robot computes opportunities for assisting the human in the event the human is not planning to exploit the robot's help. Here, as in the previous case, utilities for the human plans computed this way is again greater than or equal to the individual optimal and less than or equal to the joint optimal plans. Going back to our use case in Figure 1, suppose the robot has a goal to deliver a medkit to room1, and CommX has a goal to conduct triage in room1, for which he also requires a medkit (and his optimal plan involves picking up medkit1 in room2). For individual optimal plans both the robot and the human will go for medkit2 (thus, in this situation, individual optimal plans are actually not feasible). For the joint optimal, the coalition can team up to both use the same medkit thus achieving mutual benefit. In case the robot is only planning to avoid conflicts, it can settle for using medkit3 which is further away, or the robot can also intervene serendipitously by handing over medkit2 in the hallway thus achieving higher utility through cooperation without directly coordinating. For our problem, this has the implication that we can no longer be sure of the plan (and consequently the utility) even

Note that in order to get a heuristic estimate of an agent's contribution to the composite plan, we compute the heuristic with respect to achieving the individual agent goal using the composite domain of the super-agent, which of course gives a lower bound on the real cost of the composite plan used to achieve that agent's goal only. Claim. NEs in GAPI are preserved in GAPI. Proof Sketch. This is easy to see because orderings among costs are preserved by a well-behaved heuristic, and hence ordering among utilities, which is known to keep the Nash equilibriums unchanged. Note that the reverse does not hold, i.e. GAPI may have extra Nash equilibriums due to the equality in the definition of well-behaved heuristics. Definition 1.5 We define a goal-ordering on the goal set G  of agent  as a function f : [1, |G  |]  [1, |G  |] such that G  G  . . .  G . This means that the goals of an agent are such that they are all different subgoals of a single conjunctive goal. We will refer to the game with agents with such ordered goal sets as GAPI (identical to GAPI otherwise). Claim. NEs in GAPI are preserved in GAPI.
 Proof Sketch. Since G  is goal-ordered, C (f (1) )     C (f (2) )  . . .  C (f (|G  |) ), where, as usual, i is the optimal solution to the planning problem i = FO , D, I, Gi  . Let us consider a non-trivial admissible ^ such that h ^ (I, Gi ) = heuristic h and define a heuristic h  f (1) f (2) f (|G |)


550

when a particular goal has been chosen. Rather what we have is a possible set of utilities for each goal. However we can do better than to just take the maximum (or minimum as the case may be) of these utilities as we did previously, because we now know how such behaviors are being generated and so we can leverage additional information from an agent's beliefs about the other agent to come up with optimal response strategies. This readily lends the problem to a formulation in terms of Bayesian strategic games, which we will discuss in the next section.

powerful approximations, and the ability to deal with issues such as synchronization and coalitions evolving across individual goal allocations. For GAPI-Bayesian, this also includes evolving beliefs as we will see below.

5.2

Impact of Intent Recognition

4.2

Formulation of the Game

We define our two-person static Bayesian game GAPI-Bayesian = , B , AH , {AR,B }, U H , {U R,B } with belief B over the type of robot as follows - Players - We still have two players - the human H and the robot R, as in the previous games. - Actions - The actions of the players are similarly identical to GAPI, i.e. the action set of agent   {H, R} is the mapping A : G  - G. - Beliefs - The human has a set of beliefs on the robot B = {B1 , B2 , . . . , B|B| } characterized by the distribution B  P , i.e. the robot can be of any of the types in B with probability P (B ). The type of the robot is essentially the algorithm it uses to compute the optimal plan given the initial state and the selected goal, and thus affects the cost of achieving the goal, and hence the utility function. - Utilities - The utilities are defined as R i  UH (AH i , Aj , B ) = R(GH ) - C ( (H )|B ) j H R  UR (Ai , Aj , B ) = R(GR ) - C ( (R)|B ) where symbols have their usual meaning. As before, the Nash equilibriums in GAPI-Bayesian are given by action profiles R AH such that the human has no reason i , Aj H R to defect, i.e.  B B UH (Ai , Aj , B )P (B ) H R U ( A , A , B ) P ( B ) while the robot also has H j k : k=i B B R no incentive to change, i.e. B B UR (AH i , Aj , B )P (B )  H R UH (Ai , Ak : k=j , B )P (B ), given the distribution P over the beliefs B of robot type. Similarly, the socially optiR mal solution is given by the action profiles AH i , Aj    H R where {i , j } = arg maxi,j B B [UH (Ai , Aj , B ) + R UR (AH i , Aj , B )]P (B ).

Evolving Utilities. Often, and certainly in the examples provided in Section 4.1, the behavior of the robot depends on understanding the intent(s) of its human counterpart. Thus the utilities will keep evolving based on the actions of the human after the goal has been selected. This is even more relevant in scenarios where communication is severely limited, when the agents in a coalition are not aware of the exact goals that the other agents have selected. Evolving Beliefs. Intent recognition has a direct effect on the belief over the robot type itself. For example, as the human observes the actions of the robot, it can infer which behavior the robot is going to exhibit. Thus intent recognition over the robot's actions will result in evolving belief of the human, as opposed to intent recognition over the human's activities which informed the planning process and hence the utilities of the robot.

5.3

Implications of Implicit Preferences

Finally, as agents interact with each other over time, in different capacities as teammates and colleagues, their expectations over which agent is likely to form which form of coalition will also evolve. This will give the prior belief over the robot type that the human starts with, and will get updated as further interactions occur.

6

Conclusions

In conclusion, we introduced a two-player static game that can be used to form optimal coalitions on the go among two autonomous members of a human-robot society, with minimum prior coordination. We also looked at several properties of such games that may be used to make the problem tractable while still maintaining key properties of the game. Finally, we explored an extension of the game to a general Bayesian formulation when the human is not sure of the intent of the robot, and motivated the implications and expressiveness of this model. We believe the work will stimulate discussion on ad-hoc interaction among agents in the context of human-robot cohabitation settings and provide insight towards generating efficient synergy.

5

Discussions and Future Work

The concept of Bayesian games lends GAPI to several interesting possibilities, and promising directions for future work, with respect to how interactions evolve with time.

Acknowledgments
This research is supported in part by the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023. I would also like to give special thanks to Prof. Guoliang Xue (with the Department of Computer Science at Arizona State University) for his valuable support and inputs.

5.1

Unrolling the Entire Game

Notice that we formulated the game such that each of the agents  has a set of goals G  to achieve. Thus GAPI immediately lends itself to a finite horizon dynamic game unrolled max |G  | times, so that the agents can figure out their most effective long-term strategy and coalitions. Finding optimal policies in such cases will involve devising more

References
Ackerman, M., and Br^ anzei, S. 2014. The authorship dilemma: Alphabetical or contribution? In Proceedings of

551

the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '14, 1487≠1488. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems. Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and Kambhampati, S. 2015a. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Chakraborti, T.; Zhang, Y.; Smith, D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot co-habitation. In ICAPS Workshop on Planning and Robotics. Chakraborti, T.; Talamadupula, K.; Zhang, Y.; and Kambhampati, S. 2016. Interaction in human-robot societies. In AAAI Workshop on Symbiotic Cognitive Systems. Mcdermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR98-003, Yale Center for Computational Vision and Control,. Ray, D., and Vohra, R. 2014. Handbook of Game Theory. Handbooks in economics. Elsevier Science. Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI'92, 276≠281. Talamadupula, K.; Briggs, G.; Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014. Coordination in human-robot teams using mental modeling and plan recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2957≠2962. Tambe, M. 1997. Towards flexible teamwork. J. Artif. Int. Res. 7(1):83≠124. Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in multi-agent planning. In ICAPS Workshop on Distributed Multi-Agent Planning (DMAP). Zick, Y., and Elkind, E. 2014. Arbitration and stability in cooperative games. SIGecom Exch. 12(2):36≠41. Zick, Y.; Chalkiadakis, G.; and Elkind, E. 2012. Overlapping coalition formation games: Charting the tractability frontier. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS '12, 787≠794. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems.

552

Tweeting the Mind and Instagramming the Heart:
Exploring Differentiated Content Sharing on Social Media
Lydia Manikonda

Venkata Vamsikrishna Meduri

Subbarao Kambhampati

arXiv:1603.02718v1 [cs.SI] 8 Mar 2016

Department of Computer Science, Arizona State University, Tempe AZ 85281
{lmanikon, vmeduri, rao}@asu.edu

Abstract
Understanding the usage of multiple OSNs (Online Social
Networks) has been of significant research interest as it helps
in identifying the unique and distinguishing trait in each social media platform that contributes to its continued existence. The comparison between the OSNs is insightful when
it is done based on the representative majority of the users
holding active accounts on all the platforms. In this research,
we collected a set of user profiles holding accounts on both
Twitter and Instagram, these platforms being of prominence
among a majority of users. An extensive textual and visual
analysis on the media content posted by these users revealed
that both these platforms are indeed perceived differently at a
fundamental level with Instagram engaging more of the users‚Äô
heart and Twitter capturing more of their mind. These differences got reflected in almost every microscopic analysis done
upon the linguistic, topical and visual aspects.

1

Introduction

Online Social Networks (OSNs) are gaining attention for
being rich sources of information about individuals including many aspects of their daily life through the way they
connect, communicate and share information. Over the past
few years, given their ubiquity and accessibility, social media platforms like Twitter and Instagram have emerged as
very popular microblogging services for web users to communicate with each other through text and photos. In 2015,
when Instagram broke the record of having more than 400
million active monthly users, Twitter was projected as its
main rival. In fact according to a recent article by Pew Research,1 28% of American adults use Instagram and 23%
use Twitter. More interestingly, many users have active accounts on both these sites (or platforms) (Lim et al. 2015;
Chen et al. 2014). While research has recognized immense
practical value in understanding the user behavioral characteristics on these platforms separately, there is no existing research that has examined how the content posted by
individuals differs across these two platforms. Instagram is
a photo-sharing application whereas Twitter emerged as a
text-based application which currently lets users post both
text and multimedia data. Of particular interest is the question of why and how individuals use these two sites when
both of them are similar in their current functionalities.
1
http://www.pewinternet.org/2015/08/19/the-demographics-ofsocial-media-users/

In this research, we aim to answer the aforementioned
questions by analyzing content from the same set of individuals across these two popular platforms and quantifying
their posting patterns (we focus on ordinary users who are
common users but not celebrities or popular users or organizations). By leveraging NLP and Computer Vision techniques, we present some of the first qualitative insights about
the types of trending topics, and social engagement of the
user posts across these two platforms. Analysis on the visual
and linguistic cues indicates the dominance of personal and
social aspects on Instagram and news, opinions and workrelated aspects on Twitter. Despite considering the same set
of users on both platforms, we see remarkably different categories of visual content ‚Äì predominantly eight categories on
Instagram and four categories of images on Twitter. These
results suggest that Instagram is largely a sphere of positive
personal and social information where as Twitter is primarily
a news sharing media with higher negative emotions shared
by users.
Background: Twitter has been explored extensively with
respect to the content (Honey and Herring 2009), language (Hong, Convertino, and Chi 2011), etc and it is established that it is primarily a news medium (Kwak et
al. 2010). Research on Instagram has focused mostly on
understanding the user behavior through analyzing color
palettes (Hochman and Schwartz 2012), categories (Hu,
Manikonda, and Kambhampati 2014), filters (Bakhshi et al.
2015), etc. On the other hand, it has been of significant
interest to the researchers to investigate the behavior of a
user (Benevenuto et al. 2009), connect users (Zafarani and
Liu 2013), study how users reveal their personal information (Chen et al. 2012), etc all across multiple OSNs. We
extend the current state of the art by examining the nature of
a given user‚Äôs behavior manifested across Twitter and Instagram. Close to our work is the work of Bang et al. (Lim et al.
2015) where six OSNs were studied to analyze the temporal
and topical signature (only w.r.t user‚Äôs profession) of user‚Äôs
sharing behavior but they did not focus on studying the comparative linguistic aspects and visual cues across the platforms. Here we employ both textual and visual techniques
to conduct a deeper analysis of content on both Twitter and
Instagram.
Dataset: In order to investigate and characterize a given
user‚Äôs behavior across multiple sites, we use a personal web
hosting service called About.me (http://about.me/) that enables individuals to create an online identity by letting them

Twitter
stories, international, food, web, naÃÉo,
angelo, jaÃÅ
time, people, love, work, world, social,
life

ID
0

happy, love, home, birthday, weekend,
beautiful, park
maÃÅs, dƒ±ÃÅa, vƒ±ÃÅa, gracias, mi, si, las

2

#football, #sports, #news, #art, facebook, google, iphone

4

1

3

Instagram
#food, delicious, coffee, sunset, beautiful, happy, #wedding
#streetart, #brightongraffiti, #belize,
#sussex,
#hipstamatic,
#urbanart,
#lawton
#fashion, #hair, #makeup, #health,
#workout, #vegan, #fit
#instagood,
#photooftheday,
#menswear, #style, #travel, #beach,
#summer
birthday, beautiful, love, christmas,
friends, fun, home

Table 1: Words corresponding to the 5 latent topics from Twitter
and Instagram

provide a brief biography, connections to other individuals
and their personal websites. Using its API, we performed
the data collection of 10,000 users and pruned the individuals who do not have profiles on both Instagram and Twitter.
The final crawl includes 1,035,840 posts from Twitter (using
the Twitter API https://dev.twitter.com/overview/
api) and 327,507 posts from Instagram (using the Instagram
API https://www.instagram.com/developer/) for the
same set of users. Each post in this dataset is public and
the data include user profiles along with their followers and
friends list, tweets (insta posts), meta data for tweets that include favorites (likes), retweets (Instagram has no explicit
reshares; so we use comments in lieu of the attention the
post receives), geo-location tagged, date posted, media content attached and hashtags.

2
2.1

Text Analysis

Latent Topic Analysis

In order to explain the types of content posted by a user
across Twitter and Instagram, we first mine the latent topics from the corpus of Twitter (aggregated posts on Twitter of all users) and corpus of Instagram (aggregated posts
on Instagram of all users where we use captions associated
with posts for this analysis). We use TwitterLDA (https://
github.com/minghui/Twitter-LDA) developed for topic
modeling of short text corpora to mine the latent topics. With
the user accounts obtained from About.me, the topic inference is meaningful as it is pertinent to the bi-platform posts
from users who use both the social media venues.
The topic vocabulary listed for both the platforms in Table 1 indicates the unique topics for each site as well as
the overlapping topics. For instance topics 0 and 4 on Instagram are similar to the topics 1 and 2 on Twitter. However,
a significant difference is that Instagram is predominantly
used to post about art, food, fitness, fashion, travel, friends
and family but Twitter hosts a significantly higher percentage of posts on sports, news and business as compared to
other topics. Another notable difference is that the vocabulary from non-English language posts like French and Spanish is higher on Twitter as compared to the captions on Instagram mostly using English as the language medium. The
topic distributions obtained from the two corpora are listed
in Figure 1 which show that friends and food are the most
frequently posted topics on Instagram as against sports and
news followed by work and social life being popular on
Twitter.
To further validate the observations made about the distinctive topical content across the two platforms, we compared the topic distributions for each individual on the two
platforms by estimating the KL-Divergence (entropy) for

Figure 1: Topic distributions of all the user posts on Twitter and
Instagram

Figure 2: Sorted entropies between the topic distributions of the
user posts on Twitter and Instagram

each user. However for this entropy computation to be possible, a unified topic model needed to be built on the combined corpus of tweets and captions of Instagram posts. The
unified topics are listed in the description of Figure 3. The
resultant entropy plot in Figure 2 follows a power law distribution showing that most users post on Twitter and Instagram equally differently barring a few (where the estimated
p-value < 10‚àí15 for each user).

2.2

Social Engagement

Since our findings revealed that the bi-platform topics are
significantly different and so we wanted to investigate how
these posts made by the same user engage other individuals on the two sites. We define the social engagement as the
attention received by a user‚Äôs post on the social media platform and can be quantified in various ways ranging from the
sum of likes and comments on Instagram and the sum of favorites and reshares on Twitter. For each topic in the unified
topic model for both Twitter and Instagram, the logarithmic
frequency of posts is plotted against the magnitude of social
engagement that is binned to discrete ranges in Figure 3.
An interesting observation is that the socially engaging
topics in the combined model are same as the overlapping
topics from the topic models built in isolation on the Twitter
and Instagram posts (Figure 1 in Section 2.1). The dominating topic on Twitter is about sports, news and business but
the overlapping influential topic is about social and personal
life comprising friends and family. Surprisingly, we found
that the overlapping topics (Topics 2 and 3) fetched predominant social engagement on both Twitter and Instagram.
A notable difference between the platforms with respect
to social engagement is that the magnitude of attention received for Instagram posts is significantly higher than the
level of attention received on Twitter as we can notice from
the ranges plotted on the x-axes in Figure 3. This observation
is consistent regardless of the activity of the user. Even when
a user is more active (Figure 4) on Twitter than Instagram,
the observation of higher social engagement on Instagram
on an absolute scale holds. A possible explanation to this is

Platform
Twitter

Instagram

0.60
0.19

0.49
0.19

0.15
0.14
0.05
0.17

0.30
0.21
0.1
0.21

0.81
0.6
0.08
0.07
0.16

0.5
0.93
0.06
0.04
0.2

Emotionality
Negemo
Posemo
Social Relationships
home
family
friend
humans

(a) Twitter

Individual Differences
work
bio
swear
death
gender

(b) Instagram
Figure 3: Social Engagement Vs Post Frequency where the topics are ‚Äì Topic 0:{people, life, world, social, app, game, business}, Topic 1:{stories, artists, #lastfm, level, #football, #sports,
news}, Topic 2:{birthday, beautiful, work, weekend, park, dinner,
christmas}, Topic 3:{ yang, run, #fitness, #runkeeper, #art, sale,
#menswear}, Topic 4:{#instagood, #photooftheday, #love, maÃÅs,
#fashion, #travel, #food}

Figure 4: Distributions of Followers/Followings vs Media

that the users on Twitter use it as a news source to read informative tweets but not necessarily all of the content that is
read will be ‚Äúliked‚Äù.
On average, there are 30% more number of hashtags for a
Twitter post compared to an Instagram post (Pearson correlation coefficient = 0.34 between distributions with p-value
< 10‚àí15 ). This may also indicate that on Instagram since the
main content is image, textual caption may not receive as
much attention from the user.

2.3

Linguistic Nature

To characterize and compare the type of language used on
both platforms, we use the psycholinguistic lexicon LIWC
((http://liwc.wpengine.com/)) on the text associated
with a Twitter post and an Instagram post. We obtain measures of attributes related to user behavior ‚Äì emotionality
(how people are reacting to different events), social relationships (friends, family, other humans) and individual differences (attributes like bio, gender, age, etc).
It is clear from Table 2 that posts on Twitter are more negatively emotional and contain more work-related and swear
words where as positive social patterns are more evident
on Instagram. By relating these results to the topic analysis results in the previous section, we identify that on Instagram users share less negative content and more lighthearted happy personal updates. To further support these
claims from the textual data, n-gram analysis indicates that

Table 2: Linguistic attributes across Twitter vs Instagram. Each
value indicates the fraction of a post belonging to the corresponding
attribute

users on Instagram focus on things that give them pleasure such as, fashion or travel (top bi/Trigrams like last
night, Good morning, right now, fashion design streetwear),
whereas on Twitter they mainly share check-in feeds from
their apps or news (top bi/Trigrams like Stories via, Just
posted, @YouTube video, Just posted photo).

3

Visual Analysis

Extensive studies have been conducted on the textual and
visual content on these two platforms in isolation but to the
best of our knowledge, a comparative content analysis has
never been conducted. Considering this as a main objective,
this section develops a better understanding on the types of
photos individuals post on Twitter in comparison with their
Instagram posts. To achieve this we employ Computer Vision techniques mainly in terms of ‚Äì visual categories (kinds
of photos) and visual features (color palettes).

3.1

Visual Categories

We first sampled two sets of 5K images from both platforms
separately and using the OpenCV library (http://opencv.
org/) on these two datasets, we extracted Speeded Up Robust Features (SURF) for each image. We used the vector
quantization approach on these features that eventually converted each image into a codebook format. Using the codebook, we clustered images using k-means algorithm (best
value of k is found by SSE (Sum of Squared Error)). We
employed the same approach on both datasets separately. To
our surprise the clusters we obtained on Instagram were very
refined compared to the coarse nature of clusters on Twitter
dataset. After computing these clusters, two researchers separately identified the overall themes of these two datasets
and agreed upon the final visual categories of the photos
from these platforms.
Visual categories on Instagram agree with our previous
work (Hu, Manikonda, and Kambhampati 2014) which detected eight different categories of images. We tried to categorize the Twitter images into the same format as Instagram
images and there are four prominent cluster categories on
Twitter. Figure 7 shows that the percentage of photos in the
activity category outnumbered any other category followed
by captioned photos. To better understand the kinds of activities and captions shown in these two sections, we sampled
around 200 images and asked the two researchers to label

(a)
(b)
(c)
(d)
Figure 5: Subcategories of activity: a) TV shows, b) Running, c)
Conferences, d) Live shows
Figure 7: Photo categories on Twitter vs Instagram
(a)
(b)
(c)
Figure 6: Subcategories of captioned photos: a) Snapshots, b)
Memes, c) Quotes

them manually into different sub-categories. Figure 5 indicates the most popular sub-categories in the activity category
‚Äì news, events (football games, concerts, conferences) and
races and Figure 6 indicates that majority of the captioned
photos are snapshots, memes, and quotes or opinions. These
categories suggest that the topics of photos on Twitter are
mainly related to news, opinions or other general user interests where as on Instagram they mainly share their joyful
and happy moments of their personal lives.

3.2

Visual Features

Existing literature (Bakhshi et al. 2015) shows that the images with a single dominant color gain more popularity
on Instagram. To verify this we compared the visual luminance of all images on these two platforms. We extracted
the grayscale histograms (range from 0 to 259) by utilizing
the OpenCV library as they capture the information about
the brightness, saturation and contrast distribution. The images with darker pixels were binned into the low intensity
value bins close to 0 and images with brighter pixels were
binned into the high intensity values close to 259. Later we
clustered the images in our dataset by employing k-means
algorithm with the grayscale histograms as features for each
image on both these platforms following which 4 types of
clusters were detected based on their color distributions. We
measured the image distribution across each of these 4 categories for both the platforms. We noticed that the images
on Instagram containing darker and brighter pixels are negligible when compared to Twitter as shown in Figure 8. This
suggest that Twitter posts may be less socially engaging than
Instagram owing to a huge presence of captioned photos on
Twitter.

4

Conclusions

In this paper, we presented a detailed comparison of the textual and visual analysis of the content posted by the same set
of users on both Twitter and Instagram. Some of the insights
obtained from linguistic analysis reveal the fundamental differences in the thinking style and emotionality of the users
on these two platforms and how the posts receive varying degrees of attention as per the underlying topics. Interestingly,
user posts on Instagram seem to receive significantly more
attention than Twitter. The visual analyses with respect to
categories and color palettes indicate that the pictures posted
on Instagram contains more selfies and photos with friends
where as Twitter contains more about user opinions in the
form of captioned photos ‚Äì memes, quotes, etc. We observed
that the differences are deeply rooted in the very intention

Figure 8: Example images corresponding to the four major color
categories obtained by extracting color histograms of images associated with the Twitter and Instagram posts.

with which users post on these platforms with Twitter being
a venue for serious posts about news, opinions and business
life where as Instagram acting as the host for light-hearted
personal moments and posts on leisure activities.

References
[Bakhshi et al. 2015] Bakhshi, S.; Shamma, D.; Kennedy, L.; and
Gilbert, E. 2015. Why we filter our photos and how it impacts
engagement. In Proc. ICWSM.
[Benevenuto et al. 2009] Benevenuto, F.; Rodrigues, T.; Cha, M.;
and Almeida, V. 2009. Characterizing user behavior in online
social networks. In Proc. IMC.
[Chen et al. 2012] Chen, T.; Kaafar, M. A.; Friedman, A.; and
Boreli, R. 2012. Is more always merrier?: A deep dive into online social footprints. In Proc. WOSN.
[Chen et al. 2014] Chen, Y.; Zhuang, C.; Cao, Q.; and Hui, P. 2014.
Understanding cross-site linking in online social networks. In Proc.
SNAKDD.
[Hochman and Schwartz 2012] Hochman, N., and Schwartz, R.
2012. Visualizing instagram: Tracing cultural visual rhythms. In
Proc. ICWSM.
[Honey and Herring 2009] Honey, C., and Herring, S. 2009. Beyond microblogging: Conversation and collaboration via twitter. In
Proc. HICSS, 1‚Äì10.
[Hong, Convertino, and Chi 2011] Hong, L.; Convertino, G.; and
Chi, E. 2011. Language matters in twitter: A large scale study.
In Proc. ICWSM.
[Hu, Manikonda, and Kambhampati 2014] Hu, Y.; Manikonda, L.;
and Kambhampati, S. 2014. What we instagram: A first analysis
of instagram photo content and user types. In Proc. ICWSM.
[Kwak et al. 2010] Kwak, H.; Lee, C.; Park, H.; and Moon, S. 2010.
What is twitter, a social network or a news media? In Proc. WWW,
591‚Äì600.
[Lim et al. 2015] Lim, B. H.; Lu, D.; Chen, T.; and Kan, M.-Y.
2015. #mytweet via instagram: Exploring user behaviour across
multiple social networks. In Proc. ASONAM.
[Zafarani and Liu 2013] Zafarani, R., and Liu, H. 2013. Connecting
users across social media sites: A behavioral-modeling approach.
In Proc. KDD.

A Formal Framework for Studying Interaction in Human-Robot Societies
Tathagata Chakraborti1

Kartik Talamadupula2

Yu Zhang1

Subbarao Kambhampati1

Department of Computer Science1

Cognitive Learning Department2

Arizona State University
Tempe, AZ 85281, USA

IBM Thomas J. Watson Research Center
Yorktown Heights, NY 10598, USA

{tchakra2, yzhan442, rao}@asu.edu

krtalamad@us.ibm.com

Abstract
As robots evolve into an integral part of the human
ecosystem, humans and robots will be involved in a
multitude of collaborative tasks that require complex
coordination and cooperation. Indeed there has been
extensive work in the robotics, planning as well as
the human-robot interaction communities to understand
and facilitate such seamless teaming. However, it has
been argued that their increased participation as independent autonomous agents in hitherto human-habited
environments has introduced many new challenges to
the view of traditional human-robot teaming. When
robots are deployed with independent and often selfsufficient tasks in a shared workspace, teams are often
not formed explicitly and multiple teams cohabiting an
environment interact more like colleagues rather than
teammates. In this paper, we formalize these differences
and analyze metrics to characterize autonomous behavior in such human-robot cohabitation settings.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made
the dream of human-robot cohabitation a not so distant reality. We are now witnessing the development of autonomous
agents that are especially designed to operate in predominantly human-inhabited environments often with completely
independent tasks and goals. Examples of such agents include robotic security guards like Knightscope, virtual presence platforms like Double and iRobot Ava, and even autonomous assistance in hospitals such as Aethon TUG. Of
particular fame are the CoBots (Rosenthal, Biswas, and
Veloso 2010) that can ask for help from unknown humans,
and thus interact with agents not directly involved in its plan.
Indeed there has been a lot of work recently in the context
of ‚Äúhuman-aware‚Äù planning, both from a point of view of
path planning (Sisbot et al. 2007; Kuderer et al. 2012) and
task planning (Koeckemann, Pecora, and Karlsson 2014;
Cirillo, Karlsson, and Saffiotti 2010), with the intention of
making the robot‚Äôs plans socially acceptable, e.g. resolving conflicts with the plans of fellow humans. Even though
all of these scenarios involve significantly different levels
of autonomy from the robotic agent, the underlying theme
of autonomy in such settings involves the robot achieving
some sense of independence of purpose in so much as its

existence is not just defined by the goals of the humans
around it but is rather contingent on tasks it is supposed to
be achieving on its own. Thus the robots in a way become
colleagues rather than teammates. This becomes even more
prominent when we consider interactions between multiple independent teams in a human-robot cohabited environment. We thus postulate that the notions of coordination
and cooperation between the humans and their robotic colleagues is inherently different from those investigated in existing literature on interaction in human-robot teams, and
should rather reflect the kind of interaction we have come
to expect from human colleagues themselves. Indeed recent
work (Chakraborti et al. 2015a; Chakraborti et al. 2015b;
Talamadupula et al. 2014) hints at these distinctions, but has
neither made any attempt at formalizing these ideas, nor provided methods to quantify behavior is such settings. To this
end, we propose a formal framework for studying inter-team
and intra-team interactions in human-robot societies, show
how existing metrics are grounded in this framework and
propose newer metrics that are useful for evaluating performance of autonomous agents in such environments.

1

Human Robot Cohabitation

At some abstracted level, agents in any environment can be
seen as part of a team achieving a high level goal. Consider, for example, your university or organization. At a micro level, it consists of many individual labs or groups that
work independently on their specific tasks. But when taken
as a whole, the entire institute is a team trying to achieve
some higher order tasks like increasing its relative standing
among its peers or competitors. So in the discussion that follows, we talk about environments, and teams or colleagues
acting within it, in the context of the goals they achieve.

1.1

Goal-oriented Environments

Definition 1.0 A goal-oriented environment is defined as
a tuple E = hF, O, Œ¶, G, Œõi, where F is a set of first order
predicates that describes the environment, and O is a set
of objects in the environment, Œ¶ ‚äÜ O is the set of agents,
G = {g | g ‚äÜ FO } is the set of goals that these agents
are tasked with, and Œõ ‚äÜ O is the set of resources that are
required by the agents to achieve their goals. Each goal has

a reward R(g) ‚àà R+ associated with it.1
These agents and goals are, of course, related to each
other by their tasks, and these relationships determine the
nature of their interactions in the environment, i.e. in the
form of teams or colleagues. Before we formalize such relations, however, we would look at the way the agent models
are defined. We use PDDL (Mcdermott et al. 1998) models
for the rest of the discussion, as described below, but most of
the discussion easily generalizes to other modes of representation. The domain model DœÜ of an agent œÜ ‚àà Œ¶ is defined
as DœÜ = hFO , AœÜ i, where AœÜ is a set of operators available
to the agent. The action models a ‚àà AœÜ are represented as
a = hCa , Pa , Ea i where Ca is the cost of the action, Pa ‚äÜ
FO is the list of pre-conditions that must hold for the action
a to be applicable in a particular state S ‚äÜ FO of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO
is a tuple that contains the add and delete effects of applying
the action to a state. The transition function Œ¥(¬∑) determines
the next state after the application of action a in state S as
Œ¥(a, S) = (S\ef f ‚àí (a))‚à™ef f + (a) if Pa ‚äÜ S; ‚ä• otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†Œ± = hF, O, DœÜ , IœÜ , GœÜ i, where IœÜ ‚äÜ FO is the initial state
of the world and GœÜ ‚äÜ FO is the goal state. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that
Œ¥(œÄœÜ , IœÜ ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . .P
, a|œÄ| i, Œ¥(a1 , s)). The cost
of the plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca .
We will now introduce the concept of a super-agent
transformation on a set of agents that combines the capabilities of one or more agents to perform complex tasks
that a single agent might not be able to do. This will help
us later to formalize the nature of interactions among agents.
Definition 1.1a A super-agent is a tuple Œò = hŒ∏, DŒ∏ i
where Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏
is the transformation from the individual domainSmodels to
a composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Note that this does not preclude joint actions among
agents, because some actions that need that need more than
one agent (as required in the preconditions) will only be
doable in the composite domain.
Definition 1.1b The planning problem of a super-agent Œò
is similarly given by Œ†Œò = hF, O, DŒ∏ , IŒ∏ , GŒ∏ i where
S the
composite initial and goal states are given by IŒ∏ = œÜ‚ààŒ∏ IœÜ
S
and GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i
where ¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |=SGŒ∏ , where the modified
transition function
S
Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). We denote
the set of all such plans as œÄŒò .
P
P
The cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca
and œÄŒ∏‚àó is optimal if Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ =‚áí C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ ).
The composite plan can thus be viewed as a union of plans
contributed by each agent œÜ ‚àà Œ∏ so that œÜ‚Äôs component can
be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà
1

SO is S ‚äÜ F instantiated or grounded with objects from O.

œÄŒò . Now we will define the relations among the components
of the environment E in terms of these agent models.
Definition 1.2 At any given state S ‚äÜ FO of the
environment E, a goal-agent correspondence is defined as the relation œÑ : G ‚Üí P(Œ¶); G, Œ¶ ‚àà E, that
induces a set of super-agents œÑ (g) = {Œò | Œ†Œò =
hF, O, DŒ∏ , S, gi has a solution, i.e. ‚àÉœÄ s.t. Œ¥(œÄ, S) |= g}.
In other words, œÑ (g) gives a list of sets of agents in the environment that are capable of performing a specific task g.
We will see in the next section how the notions of teammates
and colleagues are derived from it.

1.2

Teams and Colleagues

Definition 2.0 A team Tg w.r.t. a goal g ‚àà G is defined as
any super-agent Œò = hŒ∏, DŒ∏ i ‚àà œÑ (g) iff 6 ‚àÉœÜ ‚àà Œ∏ such that
Œò0 = hŒ∏ \ œÜ, DŒ∏\œÜ i and œÄŒò = œÄŒò0 .
This means that any super-agent belonging to a particular
goal-agent correspondence defines a team w.r.t that specific
goal when every agent that forms the super-agent plays
some part in the plans that achieves the task described
by g, i.e. the super-agent cannot use the same plans to
achieve g if an agent is removed from its composition.
This, then, leads to the concept of strong, weak, or optimal teams, depending on if the composition of the
super-agent is necessary, sufficient or optimal respectively
(note that an optimal team may or may not be a strong team).
Definition 2.0a A team Tgs = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is strong iff 6 ‚àÉœÜ ‚àà Œ∏ such that hŒ∏ \ œÜ, DŒ∏\œÜ i ‚àà œÑ (g).
A team Tgw is weak otherwise.
Definition 2.0b A team Tgo = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is optimal iff ‚àÄŒò0 ‚àà œÑ (g), C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏‚àó0 ).
This has close ties with concepts of required cooperation and capabilities of teams to solve general planning
problems, introduced in (Zhang and Kambhampati
2014), and work on team formation mechanisms and
properties of teams (Shoham and Tennenholtz 1992;
Tambe 1997). In this paper, we are more concerned about
the consequences of such team formations on teaming metrics. So, with these different types of teams we have seen
thus far, the question we ask is: What is the relation among
the rest of the agents in the environment? How do these
different teams interact among and between themselves?
Definition 2.1 The set of teams in E are defined by the
relation Œ∫ : G ‚Üí R(œÑ ); G ‚àà E, where Œ∫(g) ‚àà œÑ (g) denotes
the team assigned to the goal g, ‚àÄg ‚àà G.
This, then, gives rise to the idea of collegiality among
agents, due to both inter-team and intra-team interactions.
Note that how useful or necessary such interactions are
will depend on whether the colleagues can contribute to
each other‚Äôs goals, or to what extent they influence their
respective plans, which leads us to the following two
definitions of colleagues based on the concept of teams.
Definition 2.2a Let Œ∫(g) = hŒ∏1 , DŒ∏1 i, Œ∫(g 0 ) = hŒ∏2 , DŒ∏2 i
be two teams in E. An agent œÜ1 ‚àà Œ∏1 is a type-1 colleague
to an agent œÜ2 ‚àà Œ∏2 when Œ∫0 (g) = hŒ∏1 ‚à™ œÜ1 , DŒ∏1 ‚à™œÜ1 i is a
weak team w.r.t. the goal g.

Definition 2.2b Agents œÜ1 , œÜ2 ‚àà Œ¶ are type-2 colleagues
when ‚àÄŒ∫(g) = hŒ∏, DŒ∏ i s.t. {œÜ1 , œÜ2 } ‚à© Œ∏ 6= ‚àÖ, {œÜ1 , œÜ2 } 6‚àà
Œ∏ ‚àß Œ∫0 (g) = hŒ∏ ‚à™ {œÜ1 , œÜ2 }, DŒ∏‚à™{œÜ1 ,œÜ2 } i is a weak team.
Thus type-1 colleagues can potentially contribute to the
plans of their colleagues, while type-2 colleagues cannot.
Plans of type-2 colleagues can, however, influence each
other (for example due to conflicts on usage of shared resources), while type1-colleagues are capable of becoming
teammates dynamically during plan execution.
Humans in the loop. Instead of a general set of agents, we
define the set of agents Œ∏ in a super-agent as composition of
humans and robots Œ∏ = h(Œ∏)‚à™r(Œ∏) so that the domain model
of the super-agent is alsoS
a composition
of the human and
S
robot capabilities DŒ∏ = œÜ‚ààh(Œ∏) œÜ‚ààr(Œ∏) AœÜ = h(DŒ∏ ) ‚à™
r(DŒ∏ ). We denote the communication actions of the superagent as the subset c(DŒ∏ ) ‚äÜ DŒ∏ .

2
2.1

Metrics for Human Robot Interaction
Metrics for Human Robot Teams

We will now ground popular (Olsen Jr. and Goodrich 2003;
Steinfeld et al. 2006; Hoffman and Breazeal 2007;
Hoffman 2013) metrics for human-robot teams in our
current formulation.
Task Effectiveness These are the metrics that measure
the effectiveness of a team in completing its tasks.
‚Ä¢ Cost-based
Metrics - This simply measures the cost
P
‚àó
g‚ààŒ∫‚àí1 (Œò) C(œÄŒò ) of all the (optimal) plans a specific
team executes (for all the goals it has been assigned to).
‚Ä¢ Net Benefit Based Metrics - This is based on both
plan costs as well as the value of goals and is given by
P
‚àó
g‚ààŒ∫‚àí1 (Œò) R(g) ‚àí C(œÄŒò ).
‚Ä¢ Coverage Metrics - Coverage metrics for a particular
team determine the diversity of its capabilities in terms of
the number of goals it can achieve |Œ∫‚àí1 (Œò)|.
Team Effectiveness These measure the effectiveness of
(particularly human-robot) teaming in terms of communication overhead and smoothness of coordination.
‚Ä¢ Neglect Tolerance - This measures how long the
robots in a team Œò is able to perform well without human intervention. WeT can measure this as
‚àó
N T = max{|i ‚àí j| s.t. h(DŒ∏ ) œÜ‚ààŒ∏ œÄŒò
(œÜ)[i : j] = ‚àÖ}.
‚Ä¢ P
Interaction Time - This is given by IT
=
‚àó
|{i | c(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|, and measures the
time spent by a team Œò in communication.
‚Ä¢ Robot Attention Demand - Measures how much attention the robot is demanding and is given by IT IT
+N T .
‚Ä¢ Secondary Task Time - This measures the ‚Äúdistraction‚Äù to a team, and can be expressed as
time not spent on achieving a given goal g, i.e.
‚àó
‚àó
ST T = |{i | œÄŒò
[i] ¬∑¬∑= ‚àÖ ‚àß Œ¥ 0 (s, œÄŒò
) |= g}|.
‚Ä¢ Free Time - F T = 1 ‚àí RAD is a measure of the fraction
of time the humans are not interacting with the robot.
‚Ä¢ Human Attention Demand - HAD = F T ‚àí h(ST T )
‚àó
‚àó
where h(ST T ) = |{i | œÄŒò
[i]‚à©h(DŒ∏ ) ¬∑¬∑= ‚àÖ‚àßŒ¥ 0 (s, œÄŒò
) |=
‚àó
g}|/|œÄŒò | is the time humans spend on the secondary task.

‚Ä¢ Fan Out - This is a measure of the communication load
on the humans, and consequently the number of robots
that should participate in a human-robot team, and is
proportional to F O ‚àù |h(Œ∏)|/RAD.
‚Ä¢ Interaction Time - Measures how quickly and effectively
T)
interaction takes places as IT = N T (1‚àíST
.
ST T
‚Ä¢ Robot Idle Time - Captures inconsistency or irregularity
in coordination from the point of view of the robotic
agent, and can be measured as the amount of time the
‚àó
robots are idle, i.e. RIT = |{i | r(DŒ∏ ) ‚à© œÄŒò
[i] = ‚àÖ|.
‚Ä¢ Concurrent Activity - We can talk of concurrency within
a team as the time that humans and robots are working
‚àó
concurrently CA1 = |{i | r(DŒ∏ ) ‚à© h(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|
and also across teams as the maximum time teams are
operating concurrently CA2 = max{|{i | œÄŒò [i] 6=
‚àÖ ‚àß œÄŒò0 [i] 6= ‚àÖ}| ‚àÄŒò, Œò0 ‚àà R(Œ∫)}.
In measuring performance of agents in cohabitation, both
as teammates and colleagues, we would still like to reduce
interactions times and attentions demand, while simultaneously increasing neglect tolerance and concurrency. However, as we will see in Section 3, these metrics do not effectively capture all the implications of the interactions desired
in human-robot cohabitation. So the purpose of the rest of
our paper is to establish metrics that can measure the effective behavior of human-robot colleagues, and to see to
what extent they can capture desired behaviors of robotic
colleagues suggested in existing literature.

2.2

Metrics for Human Robot Colleagues

We will now propose new metrics that are useful for
measuring collegial interactions, see how they differ from
teaming metrics discussed so far, and then relate them to
existing work on human-robot cohabitation.
Task Effectiveness The measures for task effectiveness
must take into account that agents are not necessarily
involved in their assigned team task only.
‚Ä¢ Altruism - This is a measure of how useful it is for
a robotic agent r to showcase altruistic behavior in
assisting their human colleagues, and is given by the ratio
of the gain in utility by adding a robotic colleague to a
team Œò to the decrease in utilityP
of plans of the teams r is
‚àó
‚àó
‚àó
involved in |œÄŒò
‚àíœÄhŒ∏‚à™r,D
|/
Œò=hŒ∏,DŒ∏ i s.t. r‚ààŒ∏ ‚àÜ|œÄŒò |.
Œ∏‚à™r i
For such a dynamic coalition to be useful, r must be a
type-1 colleague to the agents Œ∏ ‚àà Œò.
‚Ä¢ Lateral Coverage - This measures how deviating from
optimal team compositions can achieve global good in
terms
of number of goals achieved by a team, LT =
P
‚àí1
(Tg )| ‚àí |Œ∫‚àí1 (Tgo )|]/|Œ∫‚àí1 (Tgo )|}
Tg =Œ∫(g),‚àÄg‚ààG {[|Œ∫
across all the teams that have been formed in E.
‚Ä¢ Social Good - Many times, while planning with humans
in the loop, cost optimal plans are not necessarily the
optimal plans in the social context. This is useful to
measure particularly when agents are interacting outside
teams, and the compromise in team utility is compensated
by the gain in
This can be
Pmutual utility of colleagues.
‚àó
expressed as g‚ààG {C(œÄŒ∫(g) ) ‚àí C(œÄŒ∫(g)
)}.

Interaction Effectiveness The team effectiveness measures need to be augmented with measures corresponding
to interactions among non-team members. While all these
metrics are relevant for robotic colleagues as well, they
become particularly important in human-robot interactions,
where information is often not readily sharable due to
higher cognitive mismatch, so as to reduce cognitive
demand/overload.
‚Ä¢ Interaction Time - In addition to Interaction Time for
human-robot teams, and measures derived from it, we
propose two separate components of interaction time for
general human-robot cohabitation scenarios.
- External Interaction Time - This is the time spent by
agents interacting with type-1 colleagues (EIT1 ).
- Extraneous Interaction Time - This is the time spent
by agents interacting with type-2 colleagues (EIT2 ).
‚Ä¢ Compliance - This refers to how much actions of an agent
disambiguate its intentions. Though relevant for both, this
becomes even more important in absence of teams, when
information pertaining to goals or plans are not necessarily sharable. Thus the intention should be to maximize
the probability P (GŒ∏ = g | s = Œ¥(œÄŒ∏ [1 : i], IŒ∏ )), Œ∫(g) =
hŒ∏, DŒ∏ i, ‚àÄg ‚àà G given any stage i of plan execution and
P (¬∑) is a generic goal recognition algorithm. This can be
relevant both in terms of disambiguating goals (Keren,
Gal, and Karpas 2014) or explaining plans given a goal
(Zhang, Zhuo, and Kambhampati 2015).
‚Ä¢ External Failure - This is the number of times optimal
plans fail when resources are contested among colleagues.
‚Ä¢ Stability - Of course with continuous interactions, team
formations change, so this gives a measure of stability of
the system as a whole. If teams Œ∫(g) = hŒ∏1 , DŒ∏1 i and
Œ∫(g) = hŒ∏2 , DŒ∏2 i achieves a P
goal g ‚àà G at two different
instances, then stability S = g‚ààG |Œ∏1 ‚à© Œ∏2 |/|Œ∏1 ||Œ∏2 |.

3

Discussion and Related Work

We will now investigate the usefulness of the proposed metrics in quantifying behavioral traits proposed in existing literature as desirable among cohabiting human and robots.
Human-Aware Planning. In (Koeckemann, Pecora, and
Karlsson 2014; Cirillo, Karlsson, and Saffiotti 2010) the authors talk of adapting robot plans to suit social norms (e.g.
not to vacuum a room while a human is asleep). Clearly,
this involves the robots departing from their preferred plans
to conform to human preferences. In such cases, involving
assistive robots, measures of Altruism and Social Good become particularly relevant, while it is also crucial to reduce
unwanted interactions (EIT1 + EIT2 ).
Planning with Resource Conflicts. In (Chakraborti et al.
2015b) the authors outline an approach for robots sharing
resources with humans to compute plans that minimize conflicts in resource usage. Thus, this line of work is aimed at
reducing External Failures, while simultaneously increasing
Social Good. Measures of Stability and Compliance become
relevant, to capture evolving beliefs and their consequences
on plans. Extraneous Interaction Time is also an important

measure, since additional communication is always a proxy
to minimizing coordination problems between colleagues.
Planning for Serendipity. In (Chakraborti et al. 2015a)
the authors propose a formulation for the robot to produce
positive exogenous events during the execution of the human‚Äôs plans, i.e. interventions which will be useful to the human regardless of whether he was expecting assistance from
the robot. This work particularly looks at planning for Altruism. Increasing Compliance in agent behavior can provide
better performance in this regard. Further, External Interaction is crucial in such cases for forming such impromptu
coalitions among colleagues.
Relation to Metrics in Human Factor Studies It is useful
to see an example of how the general formulation of metrics
we discussed so far are actually grounded in human factors
studies (Zhang et al. 2015) of scenarios that display some aspects of collegial interaction. The environment studied was
a disaster response scenario, involving an autonomous robot
that may or may not chose to proactively help the human.
The authors used External Interaction Time or EIT1 to measure the effectiveness of proactive support (how often the
proactive support resulted in further deliberation over goals),
while Lateral Coverage (in terms of number of people rescued) showed the effectiveness of proactive support. Further,
qualitative analysis on acceptance and usefulness of agents
that display proactive support are closely related to measures
such as Social Good and Altruism.
Work on Ad-hoc Coalition Formations Given the framework we have discussed thus far, the question is then, apart
from measuring performance, how we can use it to facilitate collegial interactions among agents. Especially relevant
in such scenarios are work on ad-hoc coalition formation
among agents sharing an environment but not necessarily
goals (Stone et al. 2010). In (Chakraborti et al. 2016) we
show how this framework may be used to cut down on prior
coordination while forming coalitions.

4

Conclusion and Future Work

In conclusion, we discussed interaction in human-robot societies involving multiple teams of humans and robots in the
capacity of teammates or as colleagues, provided a formal
framework for talking about various modes of cooperation,
and reviewed existing metrics and proposed new ones that
can capture these different modalities of teaming or collegial behavior. Finally we discussed how such metrics can be
useful in evaluating existing works in human-robot cohabitation. One line of future inquiry would be to see how such
quantitative metrics are complemented by qualitative feedback from human factor studies, to establish what the desired trade-offs are, in order to ensure well-informed design
of symbiotic systems involving humans and robots.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
[Aethon TUG ] Aethon TUG. Intralogistics automation platform for hospitals. http://www.aethon.com/.
[Chakraborti et al. 2015a] Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and
Kambhampati, S. 2015a. Planning for serendipity. In International Conference on Intelligent Robots and Systems.
[Chakraborti et al. 2015b] Chakraborti, T.; Zhang, Y.; Smith,
D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot cohabitation. In ICAPS Workshop on Planning and Robotics.
[Chakraborti et al. 2016] Chakraborti, T.; Dondeti, V.;
Meduri, V. V.; and Kambhampati, S. 2016. A game theoretic approach to ad-hoc coalition formation in human-robot
societies. In AAAI Workshop on Multi-Agent Interaction
without Prior Coordination.
[Cirillo, Karlsson, and Saffiotti 2010] Cirillo, M.; Karlsson,
L.; and Saffiotti, A. 2010. Human-aware task planning:
An application to mobile robots. ACM Trans. Intell. Syst.
Technol. 1(2):15:1‚Äì15:26.
[Double ] Double. The ultimate tool for telecommuting.
http://www.doublerobotics.com/.
[Hoffman and Breazeal 2007] Hoffman, G., and Breazeal, C.
2007. Effects of anticipatory action on human-robot teamwork: Efficiency, fluency, and perception of team. In
Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on, 1‚Äì8.
[Hoffman 2013] Hoffman, G. 2013. Evaluating fluency in
human-robot collaboration. In Robotics: Science and Systems (RSS) Workshop on Human-Robot Collaboration.
[iRobot Ava ] iRobot Ava.
Video collaboration robot.
http://www.irobot.com/For-Business.aspx.
[Keren, Gal, and Karpas 2014] Keren, S.; Gal, A.; and
Karpas, E. 2014. Goal recognition design. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS 2014, Portsmouth,
New Hampshire, USA, June 21-26, 2014.
[Knightscope ] Knightscope. Autonomous data machines.
http://knightscope.com/about.html.
[Koeckemann, Pecora, and Karlsson 2014] Koeckemann,
U.; Pecora, F.; and Karlsson, L. 2014. Grandpa hates
robots - interaction constraints for planning in inhabited
environments. In Proc. AAAI-2010.
[Kuderer et al. 2012] Kuderer, M.; Kretzschmar, H.; Sprunk,
C.; and Burgard, W. 2012. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings
of Robotics: Science and Systems.
[Mcdermott et al. 1998] Mcdermott, D.; Ghallab, M.; Howe,
A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and
Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control,.
[Olsen Jr. and Goodrich 2003] Olsen Jr., D., and Goodrich,
M. A. 2003. Metrics for evaluating human-robot interactions. In Performance Metrics for Intelligent Systems.

[Rosenthal, Biswas, and Veloso 2010] Rosenthal,
S.;
Biswas, J.; and Veloso, M. 2010. An effective personal
mobile robot agent through symbiotic human-robot interaction. In Proceedings of the 9th International Conference
on Autonomous Agents and Multiagent Systems: Volume 1
- Volume 1, AAMAS ‚Äô10, 915‚Äì922. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.
[Shoham and Tennenholtz 1992] Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for
artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì
281. AAAI Press.
[Sisbot et al. 2007] Sisbot, E.; Marin-Urias, L.; Alami, R.;
and Simeon, T. 2007. A human aware mobile robot motion
planner. Robotics, IEEE Transactions on 23(5):874‚Äì883.
[Steinfeld et al. 2006] Steinfeld, A.; Fong, T.; Kaber, D.;
Lewis, M.; Scholtz, J.; Schultz, A.; and Goodrich, M. 2006.
Common metrics for human-robot interaction. In Proceedings of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, 33‚Äì40.
[Stone et al. 2010] Stone, P.; Kaminka, G. A.; Kraus, S.; and
Rosenschein, J. S. 2010. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. In Proceedings of
the Twenty-Fourth Conference on Artificial Intelligence.
[Talamadupula et al. 2014] Talamadupula, K.; Briggs, G.;
Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014.
Coordination in human-robot teams using mental modeling
and plan recognition. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2957‚Äì2962.
[Tambe 1997] Tambe, M. 1997. Towards flexible teamwork.
J. Artif. Int. Res. 7(1):83‚Äì124.
[Zhang and Kambhampati 2014] Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in
multi-agent planning. In ICAPS Workshop on Distributed
Multi-Agent Planning (DMAP).
[Zhang et al. 2015] Zhang, Y.; Narayanan, V.; Chakraborti,
T.; and Kambhampati, S. 2015. A human factors analysis
of proactive support in human-robot teaming. In IEEE/RSJ
International Conference on Intelligent Robots and Systems.
[Zhang, Zhuo, and Kambhampati 2015] Zhang, Y.; Zhuo,
H. H.; and Kambhampati, S. 2015. Plan explainability and
predictability for cobots. CoRR abs/1511.08158.

A Human Factors Analysis of Proactive Support in Human-robot Teaming
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborti and Subbarao Kambhampati
Abstract-- It has long been assumed that for effective humanrobot teaming, it is desirable for assistive robots to infer the goals and intents of the humans, and take proactive actions to help them achieve their goals. However, there has not been any systematic evaluation of the accuracy of this claim. On the face of it, there are several ways a proactive robot assistant can in fact reduce the effectiveness of teaming. For example, it can increase the cognitive load of the human teammate by performing actions that are unanticipated by the human. In such cases, even though the teaming performance could be improved, it is unclear whether humans are willing to adapt to robot actions or are able to adapt in a timely manner. Furthermore, misinterpretations and delays in goal and intent recognition due to partial observations and limited communication can also reduce the performance. In this paper, our aim is to perform an analysis of human factors on the effectiveness of such proactive support in human-robot teaming. We perform our evaluation in a simulated Urban Search and Rescue (USAR) task, in which the efficacy of teaming is not only dependent on individual performance but also on teammates' interactions with each other. In this task, the human teammate is remotely controlling a robot while working with an intelligent robot teammate `Mary'. Our main result shows that the subjects generally preferred Mary with the ability to provide proactive support (compared to Mary without this ability). Our results also show that human cognitive load was increased with a proactive assistant (albeit not significantly) even though the subjects appeared to interact with it less.

Fig. 1. Illustration of our USAR task in which the human teammate remotely controls a robot while working with an intelligent robot `Mary'. We intend to compare Mary with and without a proactive support ability.

I. I NTRODUCTION The efficacy of teaming [8] is not only dependent on individual performance, but also on teammates' interactions with each other. It has long been assumed that for effective human-robot teaming, it is desirable for assistive robots to infer the goals and intents of the humans, and take proactive actions to help them achieve their goals. For example, the ability of goal and intent recognition is considered to be required for an assistive robot to be socially acceptable [22], [5], [16], [2], [24]. This claim is also assumed in other human-robot teaming tasks, such as collaborative manufacturing [25] and urban search and rescue (USAR) [23]. However, there has not been any systematic evaluation of the accuracy of this claim.1
*This work was supported in part by the ARO grant W911NF-13-1- 0023, and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N0001415-1-2027. The authors would like to thank Nathaniel Mendoza for help with the simulator as well as the anonymous participants in the study. The authors are with the Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA

{yzhan442,vnaray15,tchakra2,rao}@asu.edu

1 The authors in [13] considered anticipatory action in interaction scenarios involving repetitive actions and the task settings are for human-robot teaming with proximal interactions.

There are several ways a proactive robot assistant can in fact reduce the effectiveness of teaming. For example, it can increase the cognitive load of the human teammate by performing actions that are unanticipated by the human. In such cases, even though the teaming performance could be improved, it is unclear whether humans are willing to adapt to robot actions or are able to adapt in a timely manner. Furthermore, misinterpretations and delays in goal and intent recognition due to partial observations and limited communication can also reduce performance. For example, consider a case in which you want to make an omelet and need eggs to be fetched from the fridge. Even if an assistive robot has started to fetch the eggs (after recognizing your intent), you may decide that the robot is too slow and fetch the eggs by yourself although you could improve the performance by letting the robot fetch the eggs while you preheat the pan. On the other hand, adapting to the robot's actions in such scenarios to improve teaming performance can increase the human cognitive load, which leads to unsatisfactory teaming experience. These conflicting factors make us investigate the utility of Proactive Support (PS) in human-robot teaming. In this paper, we start this investigation in a simulated USAR task with a general way to implement the proactive support ability on a robot in similar scenarios. Previous work [13] that investigates the effects of this ability is restricted to human-robot teaming with more proximal interactions. Meanwhile, to maintain the generality of this task, we only introduced a few necessary simplifications. In our task, the human teammate is remotely controlling a robot while working with an intelligent robot `Mary' (as shown in Fig. 1). The human-robot team is deployed during the early phase

of an emergency response where they need to search, rescue and provide preliminary treatment to casualties. This USAR scenario considers many of the complexities (e.g., partial observations) that often occur in real-world USAR tasks and we intend to learn whether these complexities influence the overall evaluation of the intelligent robot (i.e., Mary) with a proactive support (PS) ability, compared to Mary without this ability. We also aim to investigate the various trade-offs, e.g., mental workload and situation awareness through this human factors study. II. R ELATED W ORK There are many early works on goal and intent recognition (e.g., [15], [14], [4]). More recently, a technique to compile the problem of plan recognition into a classical planning problem is provided [20]. There is also a rich literature in the area of plan adaptation, which handles how robots plan under human-introduced constraints (e.g., social rules [24]). Using simple temporal networks (STNs), there has been development in efficient dispatchers that perform fast, online, least-commitment scheduling adaptation [6]. There are also a number of adaptation techniques that focus on integrated planning and execution [7], [21], [1]. There are existing systems that combine both goal and plan recognition and plan adaptation to achieve a proactive support ability on robots. In [13], [12], the authors propose a cost based anticipatory adaptive action selection mechanism for a robotic teammate to make decisions based on the confidence of the action's validity and relative risk. However, only repetitive tasks are considered and the task settings are for human-robot teaming with more proximal interactions compared to that in USAR scenarios. In [5], a humanaware planning paradigm is introduced where the robot only passively interacts with the human by avoiding conflicts with the recognized human plan. In USAR scenarios, it is also desirable for the robot to proactively provide support to the human. A recent paper proposes a planning for serendipity paradigm in which the authors investigate planning for stigmergic collaboration without explicit commitments [3]. In [17], the authors propose a unified approach to concurrent plan recognition and execution for human-robot teams, in which they represent alternative plans for both the human and robot, thus allowing recognition and adaptation to be performed concurrently and holistically. However, the limitation is that the plan choices must be specified a priori instead of dynamically constructed based on the current goal and intent of the human. This renders the approach impractical for realworld scenarios since even moderate number of choices (i.e., branching factors) can make the approach infeasible. Part of our goal is to provide a general way to achieve a proactive support ability in scenarios that are similar to our USAR task, in which the task is composed of subtasks with priorities that are dependent on the current situation. Note that a framework to achieve general proactive support can be arbitrarily complex depending on the task and level of support that is needed. In our work, similar to [23], we use the plan recognition technique in [20] and then feed its

outputs to a planner which determines the priorities of the subtasks and computes a plan accordingly. The main goal of this work is to start the investigation of humans factors for proactive support in various human-robot teaming scenarios. Regarding the benefits of automation in human-robot teaming, it is well known that automation can have both positive and negative effects on human performance. Empirical proofs have been provided in four main areas: mental workload, situation awareness, complacency and skill degradation [19]. We also aim to study the influence of proactive support on these factors in our USAR task. III. BACKGROUND A. USAR Task Settings Overview In our simulated USAR task, the human and intelligent robot (i.e., Mary) share the same set of candidate goals (i.e., subtasks), and the overall team goal is to achieve them all (which will be distributed among the human and Mary). These goals are not independent of each other. In particular, the priorities of goals are dependent on which goals are achieved in the current situation. Given these task settings, we aim to investigate the influence of a proactive support (PS) ability on a robot. We compare two cases: Mary (i.e., the intelligent robot) has a PS ability and Mary does not have this ability. During the task execution, in both cases, Mary chooses her own goal to maximize the teaming performance accordingly to the human's current goal. When Mary does not have a proactive support ability, she can only know the human's current goal when the human explicitly communicates it to her. When Mary has this ability, if the human does not inform Mary of his/her current goal,2 Mary can infer it based on her observations. To summarize, Mary in both cases can adapt to human goals while Mary with a PS ability can adapt in a more "proactive" fashion (hence proactive support). Finally, in both cases, Mary has an automated planner (see a brief description below) that can create a plan to achieve her current goal and she can autonomously execute the plan. B. Automated Planner In our settings, a task or subtask is compiled into a problem instance for an automated planner to solve. The planner creates a plan by connecting an initial state to a goal state using agent actions. A planning problem can be specified using a planning domain definition language (PDDL) [11]. Depending on the task, there are many extensions of PDDL (e.g., [9], [10]) that can incorporate various modeling requirements. We use the extension of PDDL described in [9] to model the USAR domain. Using an automated planner allows an agent to reason directly about the goal. Human factors study on the incorporation of automated planners for human-robot teaming has appeared previously in [18].
2 In both cases, when the human (optionally) informs Mary of his/her current goal, it is used directly by Mary assuming that this information is accurate.

Fig. 3.

Example puzzle problem used in our USAR task.

(a)

(b)

Fig. 2. (a) Simulated Environment for our USAR task. (b) The environment (from robot X 's cameras) that the human subject actually sees.

C. Goal and Intent Recognition To recognize the human intents and goals, assuming that humans are rational, we use the technique in [20]. In our task, Mary maintains a belief of the human's current goal (denoted by GX ) as a hypothesis goal set YX , in which YX corresponds to all remaining candidate goals. Given a sequence of observations q that are obtained periodically from sensors (on Mary or fixed in the environment), the probability distribution Q over G 2 YX is recomputed using a Bayesian update P(G|q ) µ P(q |G), where the prior is approximated by the function P(q |G) = 1/(1 + e b D(G,q ) ) in which D(G, q ) = C p (G q ) C p (G + q ). C p (G + q ) and C p (G q ) represent the cost of the optimal plan to achieve G with and without the observation of q , respectively. Having known the probability distribution Q, the goal that has the highest probability is assumed to be the current goal of the human. This goal is correspondingly taken out of the consideration of Mary and Mary then adapts her current goal if necessary (from her remaining goals) to optimize the teaming performance. Mary then makes a plan using an automated planner described previously to achieve her current goal. IV. S TUDY D ESIGN A. Hypotheses We aim to investigate the following hypotheses: ∑ H 1) Mary with a proactive support (PS) ability enables more effective teaming (e.g., less communication and more efficiency) in our task settings. ∑ H 2) Mary with a PS ability increases human mental workload (e.g., due to unanticipated actions from Mary). In our study, we also make efforts to maintain the task settings as general as possible. For a discussion on the generalization of the results, refer to the conclusion section. B. Environment Fig. 2(a) shows the simulated environment (created in Webots) in our USAR task, which represents the floor plan of an office building where a disaster occurs (e.g., a fire). Fig. 2(a) is the visual feedback from the remotely controlled robot

(i.e., robot X in Fig. 1) that the human subject actually sees. The environment is organized as segments, and each segment is identified by a unique label (e.g., R01). Furthermore, the segments are grouped into four regions: medical kit storage region (represented by segments starting with `S'), casualty search region (starting with `R'), medical room region where treatment (or triage) is performed (starting with `M'), and the hallway region (starting with `H'). Each region can be accessed via a door that connects to a hallway segment and R regions are further divided into rooms that are also connected by doors. The doors are initially closed and can be pushed open by the robots. The doors remain open after being pushed open. Both the remotely-controlled robot (denoted by `X ') and Mary work inside this environment. There are two networked CCTV cameras that Mary can obtain observations from and the field of views of these cameras are also shown in Fig. 2(a). C. Task Settings The overall team goal is to find and treat all the casualties in the environment, which includes searching for casualties in the R regions, carrying casualties to medical rooms, fetching medical kits and performing triages. In Fig. 2(a), the two colored boxes (i.e., red and blue) in R regions represent casualties and the white boxes in S regions represent medical kits. We impose two constraints on the agents: 1) either robot X or Mary can carry only one medical kit or one casualty at one time. 2) The triage can only be performed by robot X for which the human subject needs to solve a few puzzle problems (see Fig. 3 for an example) in 2 minutes. Out of the two casualties, we assume that one is critically injured (i.e., the red box in R02) who should be treated immediately after being found. The other one is lightly injured (i.e., the blue box in R05). It is also assumed that a medical room can only accommodate one casualty and each medical kit can only be used towards one casualty. D. Interface Design In this USAR task, the human subject needs to manually control robot X while interacting with Mary. To create a more realistic USAR environment, the human subject only has access to the visual feeds from robot X . In other words, the human subject can only observe the part of the environment from robot X 's "eyes" (i.e., two cameras, one mounted above the other). The interaction interface between the human subject and robot X is shown in Fig. 4. More specifically, robot X displays a list of applicable actions that it can perform given

Fig. 4.

Interaction interface between the human subject and robot X .

Fig. 5.

Interaction interface between the human subject and Mary.

the current state. The human subject interacts with robot X to choose an action from the list of applicable actions. When the chosen action is completed by X , the interaction interface displays the next set of actions. This process is repeated until the task is finished (i.e., all the casualties are found and treated). Following are the list of all possible action types that the human can choose. Compare the list with that shown in Fig 4. This interface also allows the human subject to optionally inform Mary about his/her current goal so that Mary can remove it from consideration and adapt her goal accordingly when necessary. ∑ move X H01 H02 - Move robot X from hallway segment H 01 to hallway segment H 02. ∑ pushdoor X R01 R02 - Push the door between room R01 and room R02. ∑ grab medkit X S01 - Grab the medical kit from storage room S01. ∑ carry casualty X R01 - Carry the casualty at room R05. ∑ drop medkit X M01 - Drop the medical kit in medical room M 01. ∑ lay down casualty X M01 - Lay down the casualty in medical room M 01. ∑ perform triage X M01 - Perform medical triage in medical room M 01. ∑ Press `i' - Inform Mary about the human subject's current or intended goal. (A list of all remaining candidate goals will be displayed to be chosen.) Note that these actions are modeled to respect the constraints that we discussed in Sec. IV-C. For example, lay down casualty X M01 is only available when there is no other casualties in medical room M01; perform triage X M01 is only available when there is a casualty and a medical kit in M01. The interaction interface between the human subject and Mary is shown in Fig. 5. This interface is first used by Mary to update the human subject about her current goal. When the human subject wants to take over the goal that Mary is

Fig. 6.

Experimental setup in the USAR task

acting to achieve, this interface is also used to display the choices (to be selected by the human subject) for Mary to terminate her current (uncompleted) goal. E. Study Setup and Flow The study was set up in our lab space, similar to that shown in Fig. 6. Before the beginning of the task, the human subject is given the floor plan without the annotations of the casualties (i.e., colored boxes). Furthermore, the human subject is informed that there are two casualties (that cannot move) and they are located inside the casualty search regions. However, no information about their exact locations is provided (i.e., which rooms the casualties are in). The human subject is also informed that the casualty that is represented by a red box is seriously injured, and should be treated as soon as possible. Note that Mary has no more information than the human subject. The remotely controlled robot X and Mary start in the same segment H 01, which is specified by the green arrows. Subjects were assigned alternately to team up with either Mary with a PS ability or without. Each subject is only

allowed to take part in one experimental trial to avoid performance fluctuation due to experience. All subjects completed the consent form before participating in the study. Prior to each run, the subject was asked to read the instruction materials that contain the background knowledge and the above information. The subject was then exposed to the simulator and the interface and was asked to experiment with them to gain some familiarity. The subject was asked to collaborate with Mary to find and treat the two casualties. After the trial, the subject was asked to complete a questionnaire (in Likert scale). F. Example Scenario Next, we walk through an example scenario in our USAR task. Consider a scenario in which the human subject found the critically injured casualty and the current goal (GX ) of the human subject becomes `bring the critically injured casualty to the top medical room in Fig. 2(a): goal(X ,`bring the critically injured casualty to the top medical room') = { (at critically injured casualty M01)} However, assume that the human subject failed to inform Mary of his/her current goal. Also, assume the following states for the medical kits: {(at med kit 1 S01), (at med kit 2 S04)}, and that Mary at that time is still searching the casualties in the other casualty search region. When robot X enters the field of view of the CCTV cameras the action and state of X are detected by the cameras and are fed to Mary as observations. In this example, some of robot X 's actions, such as {(move X H02 H03), (move X H04 H08)} will be observed by Mary, which triggers the goal and intent recognition process. After computing the probability distribution Q for all goals in the candidate goal set for the human, the goal that has the higher probability (and falls above a pre-specified threshold) is assumed to be the current goal of the human (GX ), which in this case is `bring the critically injured casualty to the top medical room'. Mary now knows that the critically injured casualty has been found and can remove this goal from her own candidate goal set. Furthermore, given this information, Mary recomputes the priorities of the remaining goals in the current situation and adapts her goal accordingly. In particular, although the searching task is still undergoing, Mary realizes that in this case helping the human subject by bringing a medical kit to M 01 would achieve a better utility for the team. Note that should the casualty found by the human subject be lightly injured instead, Mary would decide to continue her search; also, should the casualty found by the human subject be lightly injured but the critically injured casualty has already been treated, Mary would choose to help the human fetch the medical kit. Note also that in the case that Mary does not have a PS ability, the above update can only occur in a timely manner if the human subject chooses to inform Mary about his/her current goal. In our running example, the goal

that Mary chooses is: goal(GM ,`bring med kit 1 to the top medical room') = {(at med kit 1 M01)} Having chosen her current goal GM , Mary then uses an automated planner to generate a plan (PM ) that achieves the goal. Meanwhile, Mary will update the human subject with her current goal. Assuming that Mary is at segment H 01 at the time, the following plan would be generated: PM = h(pushdoor Mary H01 S03), (move Mary H01 S03), (move Mary S03 S04), (grab medkit Mary S04), (move Mary S04 S03), (move Mary S03 H01), (move Mary H01 H02), (move Mary H02 H03), (move Mary H03 H04), (move Mary H04 H08), (pushdoor Mary H08 M02), (move Mary H08 M02), (move Mary M02 M01), (drop medkit Mary M01)i Note that various other scenarios can arise in this task, which may not always favor Mary with a PS ability. For example, the human subject may decide to deliver the medical kits to the medical rooms even before finding any casualties. or the human subject may walk robot X to the medical room empty-handed. These can confuse the goal and intent recognition process on Mary and lead to reduced teaming performance. Although not all of these scenarios occurred during our experimental study, they demonstrate the conflicting factors for proactive support in human-robot teaming tasks. It is also clear that these tradeoffs are dependent on the task and robot settings, which require more investigations in future work. V. R ESULTS The study was performed over 4 weeks and involved 16 volunteers (9 males, 7 females), Volunteers have ages with M = 24 and SD = 1.15. Subjects were recruited from students on campus. Due to the requirement of understanding English instructions, subjects must indicate that they are confident with English communication skills before taking part in the study. We also asked about the subject's familiarity with computers (M = 6.56, SD = 0.63), robots (M = 4.19, SD = 0.91), puzzle problems (M = 3.19, SD = 0.83) and computer gaming (M = 4.69, SD = 1.49), in seven-point scales after the study (with 1 being least familiar and 7 being most familiar). The subjects reported familiarity with computers, but not so much with robots, puzzle problems or computer gaming.

Fig. 7. Results for objective performance and measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

Fig. 8. Results for task performance and measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

A. Measurement A post-study questionnaire is used to evaluate three of four areas that are often used to assess automated systems: mental workload, situation awareness, and complacency [19]. Furthermore, we also use the questionnaire to evaluate several psychological distances between individuals and the environment (including robots), which include immediacy, effectiveness, likability and trust. Immediacy describes how realistic the subject felt about the task and Mary. Effectiveness describes the subject's feeling about how effective the subject considered Mary as a teammate. Likability describes how likable the subject felt about Mary. Trust describes whether the subject felt that Mary was trustworthy. We also collect the subjects' opinions on whether they considered that Mary should be improved (i.e., improvability). One way fixed-effects ANOVA tests were performed to analyze the objective performance and measures, as well as the subjective questions. The fixed factor in the tests is the type of Mary, the intelligent robot, which is either Mary with a PS ability or without (denoted by No-PS). B. Objective Performance We first investigate the objective performance and measures. The overall performance (presented in in Fig. 7) is evaluated based on the total time taken for the team to find and treat the critically injured casualty, and the total time taken for the team to finish the entire task (i.e., find and treat both casualties). It is interesting to observe that while there is a significant difference between PS and No-PS for the time taken to complete the entire USAR task (F (1, 14) = 8.34, p < 0.01), we do not find any significant difference for treating the critically injured casualty. This may be due to the fact that humans are proficient at prioritizing goals. However, this may negatively impact the teaming performance since the subject may more often choose to neglect the help of Mary when he/she does not feel comfortable with entrusting Mary with important goals. This conjecture is also consistent with the results in Fig. 8, which is discussed next. We provide a more detailed analysis of task performance in Fig. 8. We compare the average number of times the subject stopped Mary from executing her current goal and the average number of times the subject had goal conflicts with Mary. The results show that these numbers are generally

smaller for the PS case but we did not find any significant difference. However, we did find a significant difference for the average number of times the subject informed his/her goal to Mary (F (1, 14) = 18.27, p < 0.001). This shows that the subject felt less necessity to inform Mary in the PS case. There is also a significant difference in the number of goal updates the subject received from Mary (F (1, 14) = 7.58, p < 0.05), This confirms that Mary changed her goal less frequently in the PS case. We also compare the accuracy of the puzzle problems for the triage operations. To discourage subjects from guessing the answers to the puzzle questions, they were told that each incorrect answer would give them negative scores. Our analysis, interestingly, shows a significant difference on this performance measure (F (1, 14) = 4.64, p < 0.01), which suggests that the human mental workload may have been reduced in the PS case, which is not consistent with the second hypothesis (i.e., H 2). Furthermore, as we show in the evaluation of subjective measures, this interpretation contradicts with the results there. C. Subjective Performance In this section, we investigate the subjective performance based on the questionnaire (23 questions in total). For these 23 questions, we categorize them into 8 different (partially overlapping) groups. This includes 3 groups for evaluating automation: mental workload (3 items, Cronbach's a = 0.713), situation awareness (1 item), and complacency (2 items, Cronbach's a = 0.769). Furthermore, we also evaluate several psychological distances between the human subject and environment (including Mary), which include immediacy (1 item), effectiveness (7 items, Cronbach's a = 0.724), likability (1 item), and trust (3 items, Cronbach's a = 0.871). We also include improvability (1 item). The answers to the questions are in seven-point scales. The results are presented accumulatively in Fig. 9. 1) Mental Workload: For mental workload, we include questions that inquire about the ease of working with Mary, and questions to rate the subject's mental workload to interact with Mary during the task. Although our analysis does not find any significant difference ( p = 0.404), the subjects still reported some difference in their mental workloads. This is an interesting result that confirms our hypothesis (i.e., H 2):

Fig. 9.

Results for subjective measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001.

although the PS ability enables more effect human-robot teaming, it also tends to increase the human mental workload at the same time. It is also worth noting that even though the subjects in the PS case reported increased mental workload, they also tended to perform well on the puzzle problems. This may be due to the fact that subjects felt less necessity to communicate with Mary and thus can concentrate more on these problems. 2) Situation Awareness: For situation awareness, we include questions that inquire about whether the subject felt that he/she had enough information to determine what the next goal should be. Our analysis does not show a significant difference (F (1, 14) = 2.78, p = 0.35), although the subjects reported slightly more situation awareness in the No-PS case, which is consistent with the side effects of automation in general. Although the number of updates for the No-PS case was significantly more than that for the PS case, the fact that situation awareness of the subject was not reduced much in the PS case is encouraging. We attribute this to the fact that the subject still needed to occasionally interact with Mary when they had goals conflicts, and the subject could gain situation awareness through such interactions. 3) Complacency: For complacency, we include questions about the comfort and ease of the teaming, as well as how well the subject felt about their performance in the task. Our analysis shows a significant difference (F (1, 14) = 11.29, p < 0.001). This is consistent with the objective performance and measures, which shows that the human subject generally felt more satisfied and confident working with Mary in the PS case. This is important for human-robot teaming. 4) Immediacy, Effectiveness, Likability & Trust: For immediacy, we include questions about how much the subject considered the simulated task as a realistic USAR task, and Mary as a teammate. Our analysis shows a significant difference (F (1, 14) = 11.63, p < 0.001), which is consistent with our prior results. For effectiveness, we include questions about the perceived effectiveness of the team, the balance of workload between the team members, and whether or not the subject felt that Mary performed expectedly. Our analysis shows a significant difference (F (1, 14) = 6.57, p < 0.05). This result suggests

that the proactive support ability indeed increases teaming effectiveness. For likability, we include questions about whether the subject felt that Mary was a good teammate. Our analysis shows a significant difference (F (1, 14) = 23.26, p < 0.001), which suggests that the subjects preferred Mary with a PS ability for teaming. For trust, we include questions about the evaluation of the Mary's trustworthiness with the assignments (or tasks) she took and with her updates during the task. Our analysis did not show any significant difference with F (1, 14) = 3.78, p = 0.072, although subjects in the PS case reported slightly higher trust. 5) Improvability: For improvability, we include questions about how much the subject felt that Mary could be improved, and how the subject evaluated his/her interaction with Mary. Our analysis shows a significant difference for improvability with F (1, 14) = 17.80, p < 0.001, which, again, suggests that the subjects preferred Mary with a PS ability. D. Summary In summary, our results are mostly consistent with our hypotheses. Our main result shows that the subjects generally preferred Mary with a PS ability. With the PS ability, the human cognitive load was indeed increased (albeit not significantly), even though the subjects appeared to interact less with Mary. More specifically, while the result on mental workload confirms our hypothesis, it also seems to be conflicting with the objective performance on the puzzle problems. This is likely due to the fact that the subject felt less necessity to interact with Mary in the PS case. Furthermore, given that situation awareness was not reduced significantly in the team with Mary having a PS ability, and that the subjects had positive feelings towards her, it seems to suggest that intelligent robots with a PS ability is welcomed in general. This is, of course, largely dependent on the fact that the subject's cognitive load is not increased significantly, which may change when the human needs to adapt to the robot's action more frequently in more complex tasks, and more communication may be needed. More investigations

are needed to be conducted in such scenarios where the task and robot settings largely differ. VI. C ONCLUSIONS In this paper, we aim to start the investigation of humans factors for proactive support in human-robot teaming. We start in a simulated USAR task with a general way to implement the proactive support (PS) ability on a robot in similar scenarios in which the task is composed of subtasks with priorities that are dependent on the current situation. Meanwhile, to maintain the generality of this task, we only introduced a few necessary simplifications. However, given the richness of USAR scenarios, more in depth studies are required to generalize the conclusions to scenarios where the task and robot settings largely differ. In such cases, our plan recognition and plan adaptation approaches may also need to be extended to implement proactive support. Note that a framework to achieve general proactive support can be arbitrarily complex depending on the task and level of support that is needed (e.g., whether the support is active [13] or passive [5] and whether it is commitment sensitive or not [3]). In our task, the human teammate is remotely controlling a robot while working with an intelligent robot Mary to search for and treat casualties. Our results show that, in general, the human teammates prefer to work with a robot that has a PS ability. However, our results also show that teaming with PS robots also increases the human's cognitive load, albeit not significantly. This is understandable since working with a proactive teammate may require more interactions and/or mental modeling on the human side in order to achieve better teaming performance. Furthermore, we also show that situation awareness when working with robots with a PS ability is not significantly reduced compared to working with robots without it. This seems to suggest that intelligent robots with a PS ability is welcomed in general. R EFERENCES
[1] Samir Alili, Matthieu Warnier, Muhammad Ali, and Rachid Alami. Planning and plan-execution for human-robot cooperative task achievement. Proc. of the 19th ICAPS, pages 1≠6, 2009. [2] Filippo Cavallo, Raffaele Limosani, Alessandro Manzi, Manuele Bonaccorsi, Raffaele Esposito, Maurizio Di Rocco, Federico Pecora, Giancarlo Teti, Alessandro Saffiotti, and Paolo Dario. Development of a socially believable multi-robot solution from town to home. Cognitive Computation, 6(4):954≠967, 2014. [3] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith, and Subbarao Kambhampati. Planning for serendipity. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015. [4] Eugene Charniak and Robert P. Goldman. A bayesian model of plan recognition. Artificial Intelligence, 64(1):53 ≠ 79, 1993. [5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1≠7, June 2009. [6] Rina Dechter, Itay Meiri, and Judea Pearl. Temporal constraint networks. Artificial intelligence, 49(1):61≠95, 1991. [7] Alberto Finzi, F¥ elix Ingrand, and Nicola Muscettola. Model-based executive control through reactive planning for autonomous rovers. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, volume 1, pages 879≠ 884. IEEE, 2004.

[8] Terrence Fong, Illah Nourbakhsh andClayton Kunz, Lorenzo Fluckiger, John Schreiner, Robert Ambrose, Robert Burridge, Reid Simmons, Laura Hiatt, Alan Schultz, J. Gregory Trafton, Magda Bugajska, and Jean Scholtz. The peer-to-peer human-robot interaction project. Space 2005. [9] Maria Fox and Derek Long. Pddl2. 1: An extension to pddl for expressing temporal planning domains. J. Artif. Intell. Res.(JAIR), 20:61≠124, 2003. [10] Alfonso Gerevini and Derek Long. Plan constraints and preferences in pddl3. The Language of the Fifth International Planning Competition. Tech. Rep. Technical Report, Department of Electronics for Automation, University of Brescia, Italy, 75, 2005. [11] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett, Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy, David E Smith, et al. Pddl-the planning domain definition language. 1998. [12] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human≠robot fluency. Robotics, IEEE Transactions on, 23(5):952≠961, 2007. [13] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team. In Proceedings of the ACM/IEEE international conference on Humanrobot interaction, pages 1≠8. ACM, 2007. [14] Henry A. Kautz. Reasoning about plans. chapter A Formal Theory of Plan Recognition and Its Implementation, pages 69≠124. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1991. [15] Henry A. Kautz and James F. Allen. Generalized Plan Recognition. In National Conference on Artificial Intelligence, pages 32≠37, 1986. [16] Uwe K® ockemann, Federico Pecora, and Lars Karlsson. Grandpa hates robots - interaction constraints for planning in inhabited environments. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu¥ ebec City, Qu¥ ebec, Canada., pages 2293≠2299, 2014. [17] Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot teams. In Twenty-Fourth International Conference on Automated Planning and Scheduling, 2014. [18] Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao Kambhampati. Automated planning for peer-to-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. [19] Raja Parasuraman. Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7):931≠951, 2000. [20] Miquel Ram¥ irez and Hector Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010. [21] Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal. Improved human-robot team performance using chaski, a humaninspired plan execution system. In Proceedings of the 6th international conference on Human-robot interaction, pages 29≠36. ACM, 2011. [22] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. IEEE Transactions on Robotics, 2007. [23] Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias Scheutz, and Subbarao Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957≠2962, Sept 2014. [24] Stevan Tomic, Federico Pecora, and Alessandro Saffiotti. Too cool for school - adding social constraints in human aware planning. In Proceedings of the International Workshop on Cognitive Robotics (CogRob), 2014. [25] Vaibhav V Unhelkar, Ho Chit Siu, and Julie A Shah. Comparative performance of human and mobile robotic assistants in collaborative fetch-and-deliver tasks. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pages 82≠89. ACM, 2014.

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
Multiagent Interaction without Prior Coordination: Technical Report WS-16-11

A Game Theoretic Approach to Ad-hoc
Coalitions in Human-Robot Societies
Tathagata Chakraborti, Venkata Vamsikrishna Meduri,
Vivek Dondeti, Subbarao Kambhampati
Department of Computer Science
Arizona State University
Tempe, AZ 85281, USA
{tchakra2,vmeduri,vdondeti,rao}@asu.edu
Abstract

coordination. For example, a human with a goal to deliver
two items to two different locations may team up with a delivery robot that can accomplish half of his task. Further, if
the robot was itself going to be headed in one of those directions, then it is in the interest of both these agents to form
this coalition. However, if the robot‚Äôs plan becomes too expensive as a result, it might decide that there is not enough
incentive to form this coalition. Moreover, as we highlighted
before, possible interactions between agents are not just restricted to cooperative scenarios only - the plans of one agent
can make the other agent‚Äôs plans fail, and it may happen that
it is not feasible at all for all agents to achieve their respective goals. Thus there are many possible modes of interaction between such agents, some cooperative and some destructive, that needs to be accounted for before the agents
can decide on their best course of action - both in terms of
which goal to choose and how to achieve it.
In this paper we model this problem of optimal goal selection as a two player game with perfect information, and
propose to cut down on the prior coordination of forming
such ad-hoc coalitions by looking for Nash equilibriums or
socially optimal solutions (because neither agent participating in such a coalition would have incentive to deviate). We
subsequently extend it to a Bayesian game to account for situations when agents are not sure of each other‚Äôs intent. We
will also look at properties, approximations, and interesting
caveats of these games, and motivate several extensions that
can capture a wide variety of ad-hoc interactions.

As robots evolve into fully autonomous agents, settings
involving human-robot teams will evolve into humanrobot societies, where multiple independent agents and
teams, both humans and robots, coexist and work in
harmony. Given such a scenario, the question we ask
is - How can two or more such agents dynamically
form coalitions or teams for mutual benefit with minimal prior coordination? In this work, we provide a
game theoretic solution to address this problem. We will
first look at a situation with full information, provide approximations to compute the extensive form game more
efficiently, and then extend the formulation to account
for scenarios when the human is not totally confident of
its potential partner‚Äôs intentions. Finally we will look at
possible extensions of the game, that can capture different aspects of decision making with respect to ad-hoc
coalition formation in human-robot societies.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until
now traditionally limited to humans only. This has made the
dream of human-robot cohabitation a not so distant reality.
In this work we envisage such an environment where humans and robots participate autonomously (possibly with required interactions) with their own set of tasks to achieve. It
has been argued (Chakraborti et al. 2016) that interactions
in such situations are inherently different from those studied in traditional human-robot teams. One typical aspect of
such interactions is the lack of prior coordination or shared
information, due to the absence of an explicit team.
This brings us to the problem we intend to address in this
paper - given a set of tasks to achieve, how can an agent proceed to select which one to achieve? In a shared environment
such as the one we described, this problem cannot be simply solved by picking the goal with the highest individual
utility, because the utility, and sometimes even the success
of the plan (and hence the corresponding goal) of an agent
are contingent on the intentions of the other agents around
it. However, such interactions are not adversarial - it is just
that the environment is shared among self-interested agents.
Thus, an agent may choose to form an ad-hoc team with another agent in order to increase its utility, and such coalition
formation should preferably be feasible with minimum prior

1

Related Work

There is a huge variety of work that looks at team formation from different angles. The scope of our discussion has
close ties with concepts of required cooperation and capabilities of teams to solve general planning problems, introduced
in (Zhang and Kambhampati 2014), and work on team formation mechanisms and properties of teams (Shoham and
Tennenholtz 1992; Tambe 1997). However, in this particular
work, we are more interested in the mechanism of choosing
goals that can lend to possible cooperative interactions, as
opposed to the mechanism of team design based on the goals
themselves. Thus the work of Zhang and Kambhampati can
provide interesting heuristics towards cutting down on the
computation of the extensive form game we will propose,
while existing work on different modes of team formation

546

2.2

contribute to the motivation of the Bayesian formulation of
the game discussed in later sections.
From the game theoretic point of view, coalition formation has been a subject of intense study (Ray and Vohra
2014) and the human-robot interaction community can derive significant insights from it. Of particular interest are
Overlapping Coalition Formation or OCF Games (Zick,
Chalkiadakis, and Elkind 2012; Zick and Elkind 2014),
which look at a cooperative game where the players are endowed with resources, with provisions for the players to display different modes of coalitions based on how they utilize
the resources. OCF games use arbitration functions that decide the payoffs for the deviating players based on how it
is affecting the non-deviating players and it helps in forming stable coalitions. This becomes increasingly relevant in
shared environments such as the one we discuss here. Finally, an interesting problem that can often occur is such
situations (especially with the way we have formulated the
game in the human‚Äôs favor) is the problem of free-riding
where agents take advantage of coalitions and try to minimize their effort (Ackerman and BraÃÇnzei 2014), which is
certainly an important aspect of designing such games.

2
2.1

We will represent coalitions of such agents by means of a
super-agent transformation (Chakraborti et al. 2015a) on a
set of agents that combines the capabilities of one or more
agents to perform complex tasks that a single agent might
not be capable of doing. Note that this does not preclude
joint actions among agents, because some actions that need
that need more than one agent (as required in the preconditions) will only be doable in the composite domain.
Definition 1.1 A super-agent is a tuple Œò = hŒ∏, DŒ∏ i where
Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏ is
the transformation from the individual domainSmodels to a
composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Definition 1.2 The planning problem of a super-agent Œò
is given by Œ†Œò = hFO , DŒ∏ , IŒ∏ , GŒ∏ i where the
S composite
initial and goal states are given by IŒ∏ =
œÜ‚ààŒ∏ IœÜ and
S
GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning
problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i where
¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ ,Swhere the modified
S transition function Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). The
P
P
cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca and
œÄŒ∏‚àó is optimal if C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ )‚àÄœÄŒ∏ with Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ .
The composite plan can be viewed as a union of plans contributed by each agent œÜ ‚àà Œ∏ whose component can be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà œÄŒ∏ .

Preliminaries

Environment and Agent Models

Definition 1.0 The environment is defined as a tuple
E = hF, O, Œ¶, G, Œõi, where F is a set of first order predicates that describes the environment, and O is the set of
objects, Œ¶ ‚äÜ O is the set of agents (which may be humans
or robots), G = {g | g ‚äÜ FO } 1 is the set of goals that these
agents are tasked with, and Œõ ‚äÜ O is the set of resources.
Each goal has a reward R(g) ‚àà R+ associated with it.
We use PDDL (Mcdermott et al. 1998) style agent models
for the rest of the discussion, but most of the analysis easily generalizes to other modes of representation. The domain
model DœÜ of an agent œÜ ‚àà Œ¶ is defined as DœÜ = hFO , AœÜ i,
where AœÜ is a set of operators available to the agent. The
action models a ‚àà AœÜ are represented as a = hCa , Pa , Ea i
where Ca is the cost of the action, Pa ‚äÜ FO is the list of
pre-conditions that must hold for the action a to be applicable in a particular state S ‚äÜ FO of the environment; and
Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO is a tuple that
contains the add and delete effects of applying the action
to a state. The transition function Œ¥(¬∑) determines the next
state after the application of action a in state S as Œ¥(a, S) |=
‚ä• if Pa 6‚äÜ S; |= (S \ ef f ‚àí (a)) ‚à™ ef f + (a) otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†œÜ = hFO , DœÜ , IœÜ , GœÜ i, where IœÜ , GœÜ ‚äÜ FO are the initial and goal states respectively. The solution to the planning
problem is an ordered sequence of actions or plan given by
œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that Œ¥(œÄœÜ , IœÜ ) |=
GœÜ , where the cumulative transition function is given by
Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . . , aP
|œÄ| i, Œ¥(a1 , s)). The cost of the
plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca and the optimal plan
œÄœÜ‚àó is such that C(œÄœÜ‚àó ) ‚â§ C(œÄœÜ ) ‚àÄœÄœÜ with Œ¥(œÄœÜ , IœÜ ) |= GœÜ .
1

Representation of Human-Robot Coalitions

2.3

The Use Case

Throughout the rest of the discussion we will use the setting
from Talamadupula et al. which involves a human commander CommX and a robot in a typical Urban Search and Rescue
(USAR) scenario, as illustrated in Figure 1. The environment consists of interconnected rooms and hallways, which
the agents can navigate and search. The commander can perform triage in certain locations, for which he needs the medkit. The robot can also fetch medkits if requested by other
agents (not shown) in the environment. A sample domain
is available at http://bit.ly/1Fko7MAhttp://bit.ly/1Fko7MA.
The shared resources here are the two medkits - i.e. some of
the plans the agents can execute will lock the use of and/or
change the position of these medkits, so as to make the other
agent‚Äôs plans, contingent on that particular resource, invalid.

Figure 1: Use case - Urban Search And Rescue (USAR).

SO is any S ‚äÜ F instantiated / grounded with objects from O.

547

3

Ad-hoc Human-Robot Coalitions

Œò = hŒ∏, DŒ∏ i is the super-agent representing the coalition formed by Œ∏ = {H, R} with IŒ∏ = IH ‚à™ IR and
GŒ∏ = GiH ‚à™ GjR . Here, the first term in the expression for
utility denotes the utility of the goal itself as defined in the
environment in Section 2.1, while the second term captures the resultant best case utility of plans due to agent
interactions. More on this below.

In this section we will look at how two agents (the human
and the robot) in our scenario, can coordinate dynamically
by forming impromptu teams in order to achieve either individually rational or socially optimal behaviors.

3.1

Motivation

Consider the scenario shown in Figure 1. Suppose one of
CommX‚Äôs goal is to perform triage in room1, while one of
the Robot‚Äôs goals is to deliver a medkit to room1. Clearly,
if both the agents choose to do their optimal plans and plan
to use medkit1 in room2, the Robot‚Äôs plan fails (assuming the CommX gets there first). The robot then has two
choices - (1) it can choose to achieve some other goal, i.e.
maximize it‚Äôs own rewards, (2) it can choose to deliver the
other medkit2 from room3, i.e. maximize social good.
Indeed there are many possible ways that these agents can
interact. For example, the utility of choosing any goal may
be defined by the optimal cost of achieving that goal individually, or as a team. This in turn affects the choice whether to
form such teams or not. In the discussion that follows, we
model this goal selection (and team formation) problem as a
strategic game with perfect information.

3.2

Human-centric robots. At this point we make an assumption about the role of the robots in our human-robot society
- we assume that the robots exist only in the capacity of autonomous assistance, i.e. in coalitions that may be formed
with humans and robots, the robot‚Äôs role is to improve the
quality of life of the humans (by possibly, in our case, reducing the costs of plans) and not vice versa.
Thus, in the expression of utility, the human uses a min‚àó
‚àó
imizing term - with no interactions C(œÄH
) = C(œÄŒò
(H)),
‚àó
‚àó
otherwise C(œÄH ) > C(œÄŒò (H)). Similarly, in case of
‚àó
‚àó
the robot, with no interactions C(œÄR
) ‚â• C(œÄŒò
(R)) and
‚àó
‚àó
C(œÄH ) <=> C(œÄŒò (H)) otherwise, since the interactions
may or may not be always cooperative for the robot. Note
that this formulation also takes care of the cases when the
robot goal becomes unachievable due to negative interactions with the human (this is why we have the maximizing
term; the difference is triggered due to negative interactions
with the human plan in absence of coalitions). Also note that
the goal utility is using a combined goal due to the particular
action profile, this captures cases when goals have interactions, i.e. a conjunction of goals may have higher (or lower)
utility than the sum of its components.
This can be easily ensured while generating plans for a
given coalition, by either discounting the costs of actions
of the robot with respect to those of the humans by a suitable factor, or more preferably, by just penalizing the total
cost of the human component in the composite plan more.
The assumption of course does not change the formulation
in any way, it is just more aligned with the notion of the
social robots being envisioned currently. Of course, in this
sense the utilities of both the humans and robots will now
become identical, with a minimizing cost term.
Now that we have defined the game, the question is how
do we choose actions for each agent? Remember that we
want to find solutions that will preclude the need to coordinate. We can take two approaches here - we can make agents
individually rational (in which case both the human and the
robot looks for a Nash equilibrium, so neither has a reason to
defect; or we can make the agents look for a socially optimal
solution (so that sum of utilities is maximized).

Formulation of the Game

We refer to our static two-player strategic game Goal Allocation with Perfect Information as GAPI = hŒ¶, {AœÜ }, {U œÜ }i.
The game attempts to determine, given complete information about the domain model and goals of the other agent,
which goal to achieve and whether forming a coalition is
beneficial. The game is defined as follows - Players - The game has two players Œ¶ = {H, R} the
human H and the robot R respectively.
- Actions - The actions of the agents in the strategic game
are the goals that they can select to achieve. Thus, for
each agent œÜ ‚àà Œ¶ we define a set of goals G œÜ =
|G œÜ |
{G1œÜ , G2œÜ , . . . , GœÜ } ‚äÜ G, and the action set AœÜ of the
agent œÜ is the mapping that assigns one of these goals as
its planning goal, i.e. AœÜ : G œÜ 7‚àí‚Üí G. Note that this is
distinct from the action models defined in PDDL for each
of the individual agents (which helps the agent figure out
how this goal G is achieved, and the resultant utility).
- Utilities - Finally, as discussed previously, the utility of an
action depends on (apart from the utility of the goal itself)
the way the agent chooses to achieve it, and is contingent
also on the plans of the other agent (due to, for example,
resource conflicts), and is given by ‚Äî

3.3

Solving for Nash Equilibriums

As usual, the Nash equilibriums in GAPI are
R
given by action profiles hAH
such that
i , Aj i
H
R
H
R
R
UH (Ai , Aj ) ‚â• UH (Ak : ‚àÄk6=i , Aj ) and UR (AH
i , Aj ) ‚â•
H
R
UH (Ai , Ak : ‚àÄk6=j ). It is easy to prove that there is no
guaranteed Nash equilibrium in GAPI. We will instead
motivate a slightly different game GAPI-Bounded where
the robot only agrees to deviate from its optimal plan up
to a certain degree, i.e. there is a bound on the amount of
assistance the robot chooses to provide.

R
i
j
‚àó
‚àó
UH (AH
i , Aj ) = R(GH ‚à™ GR ) ‚àí min{C(œÄH ), C(œÄŒò (H))}
R
UR (AH
i , Aj )
i
‚àó
‚àó
‚àó
= R(GH ‚à™ GjR ) ‚àí C(œÄŒò
(R)) if C(œÄH
) > C(œÄŒò
(H))
j
i
‚àó
‚àó
= R(GH ‚à™ GR ) ‚àí max{C(œÄR ), C(œÄŒò (R))}, otherwise.

‚àó
where, œÄH
is the optimal plan or solution of the planning
‚àó
problem defined by Œ†H = hFO , DH , IH , GiH i, œÄR
is the
j
‚àó
optimal solution of Œ†R = hFO , DR , IR , GR i, and œÄŒò
is
the optimal solution of Œ†Œò = hFO , DŒ∏ , IŒ∏ , GŒ∏ i, where

548

Definition 1.3. The differential help Œ¥(g, GiR ) provided
by the robot R with goal GiR ‚àà G R , when the human H
picks goal g ‚àà G H , measures the decrease in utility of
the robot upon forming a coalition with the human, and is
‚àó
‚àó
‚àó
given by Œ¥(g, GiR ) = |C(œÄŒò
(R)) ‚àí C(œÄR
)|, where œÄR
is
i
‚àó
the optimal solution of Œ†R = hFO , DR , IR , GR i, and œÄŒò
is
the optimal solution of Œ†Œò = hFO , DŒ∏ , IH ‚à™ IR , g ‚à™ GiR i,
where Œò = hŒ∏ = {H, R}, DŒ∏ i.

Further, it may be noted here that there may be many such
Nash equilibriums in GAPI-Bounded and these are also
the only ones, i.e. all Nash equilibriums in GAPI-bounded
must satisfy the conditions in the above claim.

3.4

Similarly, the socially optimal goal selection strategies are
‚àó ‚àó
R
given by the action profiles hAH
i‚àó , Aj ‚àó i where {i , j } =
R
H
R
arg maxi,j UH (AH
i , Aj ) + UR (Ai , Aj ). The socially optimal action profiles may not necessarily correspond to any
Nash equilibriums of either GAPI or GAPI-Bounded.

Thus in GAPI-Bounded the utility function is modified
from the one in GAPI as follows R
i
‚àó
UH (AH
i , Ai ) = R(GH ) ‚àí C(œÄH )

Individual Irrationality and ‚àíEquilibrium. Given the
way the game is defined, it is easy to see that the socially
good outcome may not be individually rational for either the
human or the robot, since the robot always has the incentive
to defect to choosing G‚àóR and the human will then choose the
corresponding highest utility goal for himself. This leaves
room for designing autonomy that can settle for action profiles hAH
, AR
i referred to as -equilibriums, for the purpose
iÃÇ
jÃÇ

j
R
‚àó
UR (AH
i , Aj ) = R(GR ) ‚àí C(œÄR )
‚àó
if ‚àÉGkR : k6=j ‚àà G H s.t. Œ¥(GiH , GjR ) > {R(GjR ) ‚àí C(œÄR
)} ‚àí
k
‚àó‚àó
‚àó
‚àó‚àó
‚àó
{R(GR ) ‚àí C(œÄR )}, where œÄR , œÄR and œÄH are the
optimal plans or solutions to the planning problems
Œ†iR = hFO , DR , IR , GjR i, Œ†kR = hFO , DR , IR , GkR i and
Œ†H = hFO , DH , IH , GiH i respectively; and otherwise -

R
H
R
of social good, i.e. |UH (AH
i‚àó , Aj ‚àó ) ‚àí UH (AiÃÇ , AjÃÇ )| ‚â§  and

R
i
‚àó
UH (AH
i , Ai ) = R(GH ) ‚àí C(œÄŒò (H))

R
H
R
|UR (AH
i‚àó , Aj ‚àó ) ‚àí UR (AiÃÇ , AjÃÇ )| ‚â§ . Note that this deviation is distinct from the concept of bounded differential assistance we introduced in Section 3.3.

j
R
‚àó
UR (AH
i , Aj ) = R(GR ) ‚àí C(œÄŒò (R))
‚àó
where œÄŒò
is the optimal solution of Œ†Œò = hFO , DŒ∏ , IH ‚à™
IR , g ‚à™ GjR i, where Œò = hŒ∏ = {H, R}, DŒ∏ i.
This basically means that if the penalty that the robot incurs by choosing to assist the human is so great that it could
rather do something else instead (i.e. choose another goal),
then it switches back to using its individual optimal plan, i.e.
no coalition is formed. If the individual optimal plans are always feasible (otherwise these do not participate in the Nash
equilibriums below), this leads to the following result.

Price of Anarchy. The price of deviating from individual
rationality is referred to as the Price of Anarchy and is measured by POS =

3.5

R

C(œÄR ) and i = arg maxi UH (GiH , GjR ).
Proof Sketch. Let us define the utility function of the
robot R for achieving a goal g ‚àà G R by itself as œÑ (g) =
‚àó
‚àó
R(g) ‚àí C(œÄR
), where œÄR
is the optimal solution to the planning problem Œ†R = hF, O, DR , IR , gi. Further, given the
‚àó
goal set G R of the robot, we set GjR = arg maxg‚ààG R œÑ (g),
‚àó
i.e. GjR corresponds to the highest utility goal that the
robot can achieve by itself. Now consider any two goals
‚àó
‚àó
GjR , GjR ‚àà G R , GjR 6= GjR . We argue that ‚àÄGiH ‚àà G H ,
j‚àó
R
H
R
UR (AH
i , Aj ‚àó ) ‚â• UR (Ai , Aj ). This is because œÑ (GR ) ‚â•
R
œÑ (GjR ) and by problem definition ‚àÄi, k |UR (AH
i , Aj ‚àó ) ‚àí
‚àó
œÑ (GjR )

UH (AH
,AR
)+UR (AH
,AR
)
iÃÇ
jÃÇ
iÃÇ
iÃÇ
UH (AH
,AR
)+UR (AH
,AR
)
i‚àó
j‚àó
i‚àó
j‚àó

.

Caveats

No or Multiple Nash Equilibriums. One of the obvious
problems with this approach is that it does not guarantee a
unique Nash equilibrium, if it exists at all. This has serious implications on the problem we set out to solve in the
first place - which goals do the agents choose to plan for,
and how? Note, however, that this is not really a feature of
the formulation itself but of the domain or the environment,
i.e. the action models of the agents and the utilities in the
goals will determine whether there is a single best coalition
that may be formed given a particular situation. Thus, there
seems to be no principled way of solving this problem in a
detached manner, without any form of communication between the agents. But our approach still provides a way to
deliberate over the possible options, and communicate to resolve ambiguities only with respect to the Nash equilibriums, rather than the whole set of goals, or even just those
in each agent‚Äôs dominant strategy, which can still provide
significant reduction in the communication overhead.

R
Claim. hAH
i‚àó , Aj ‚àó i must be a Nash equilibrium of
GAPI-Bounded when j ‚àó = arg maxGj ‚ààG R R(GjR ) ‚àí
‚àó

Solving for Social Good

Infeasibility of the Extensive Form Game. Note here
that the utilities of the actions are calculated from the cost
of plans to achieve the corresponding goals, which involves
solving two planning problems per action. This means that,
in order to get the extensive form of GAPI, we need to solve
O(|G H |√ó|G R |) planning problems in total (note that solving
for œÄŒò gives utilities for both agents H and R), which may
be infeasible for large domains. So we need a way to speed
up our computation (either by computing an approximation

œÑ (GjR ).

R
UR (AH
‚àí
Thus, in general, the
k , Aj ‚àó )| ‚â§
goal ordering induced by the function œÑ is preserved by
the utility function UR , and consequently AR
j ‚àó is a dominant strategy of the robot. It follows that AH
i‚àó such that
i‚àó = arg maxi UH (GiH , G‚àóR ) is the corresponding best reR
sponse for the human. Hence hAH
i‚àó , Aj ‚àó i must be a Nash
equilibrium. Hence proved.


549

1
1
max{h(I, GiœÜ ), hÃÇ(I, Gi‚àí1
œÜ )}; hÃÇ(I, GœÜ ) = h(I, GœÜ ). Then hÃÇ
is well-behaved. Hence proved.

These properties of GAPI-Bounded, GAPI and GAPI
enables computation of approximations, and partial profiles,
to the extensive form of GAPI, while maintaining the nature
of interactions, thus making the formulation more tractable.

and/or finding ways to calculate multiple utility values at
once), while simultaneously preserving guarantees from our
original game in our approximate version.
Fortunately, we have good news. Note that all we require
are costs of the plans, not the plans themselves. So a promising approach towards cutting down on the computational
complexity is by using heuristic values for the initial state
of a particular planning problem as a proxy towards the true
plan cost. Note that the better the heuristic is, the better our
approximation is. So the immediate question is - What guarantees can we provide on the values of the utilities when we
use heuristic approximation? Are the Nash equilibriums in
the original game still preserved? This brings us to the notion of ‚Äúwell-behaved heuristics‚Äù as follows -

4
4.1

‚Ä¢ Individual Optimality - In this type of planning, each
agent computes the individual optimal plan to achieve
their goals. Note that this plan may not be actually valid
in the environment during execution time, due to factors
such as resource conflicts due to plans of the other agents.

We define GAPI as a game identical to GAPI but with a
modified utility function as follows -

‚Ä¢ Joint Optimality - Here we compute the joint optimal for
a coalition; and this optimal plan is computed in favor of
the human as discussed previously in Section 3.2.

R
i
i
i
UH (AH
i , Aj ) = R(GH ) ‚àí min{h(GH , IH ), h(GH , IH ‚à™ IR )}
R
UR (AH
i , Aj )
i
= R(GH ) ‚àí h(GjR , IH ‚à™ IR ) if h(GiH , IH ) > h(GiH , IH ‚à™ IR )
= R(GiH ) ‚àí max{h(GjR , IR ), h(GjR , IH ‚à™ IR )}, otherwise.

‚Ä¢ Planning with Resource Conflicts - In (Chakraborti et
al. 2015b) we explored a technique for the robot to produce plans so as to ensure the success of the human plans
only, and explored different modes of such behavior of
the robot in terms of compromise, opportunism and negotiation. Thus utilities for the human plans computed this
way is, at times, same as the joint optimal, but in general
is greater than or equal to the individual optimal and less
than or equal to the joint optimal.

Note that in order to get a heuristic estimate of an agent‚Äôs
contribution to the composite plan, we compute the heuristic
with respect to achieving the individual agent goal using the
composite domain of the super-agent, which of course gives
a lower bound on the real cost of the composite plan used to
achieve that agent‚Äôs goal only.
Claim. NEs in GAPI are preserved in GAPI.

‚Ä¢ Planning for Serendipity - In (Chakraborti et al. 2015a)
we looked at a special case of multi-agent coordination,
where the robot computes opportunities for assisting the
human in the event the human is not planning to exploit
the robot‚Äôs help. Here, as in the previous case, utilities for
the human plans computed this way is again greater than
or equal to the individual optimal and less than or equal
to the joint optimal plans.

Proof Sketch. This is easy to see because orderings
among costs are preserved by a well-behaved heuristic, and
hence ordering among utilities, which is known to keep the
Nash equilibriums unchanged. Note that the reverse does not
hold, i.e. GAPI may have extra Nash equilibriums due to the
equality in the definition of well-behaved heuristics.

Definition 1.5 We define a goal-ordering on the goal set
G œÜ of agent œÜ as a function f : [1, |G œÜ |] 7‚Üí [1, |G œÜ |] such
f (2)

Motivation

In the previous sections we considered both individual and
team plans, and as teams we considered optimal plans for
a coalition. In reality there are many ways that a particular
coalition can achieve a particular goal, and correspondingly
there are different modes of interaction between the teammates. We discuss four such possibilities briefly here -

Definition 1.4 A well-behaved heuristic h : S √ó S 7‚Üí
R+ , S ‚äÜ FO is such that h(I, G1 ) ‚â§ h(I, G2 ) whenever
C(œÄ1‚àó ) ‚â§ C(œÄ2‚àó ), where œÄ1‚àó and œÄ2‚àó are the optimal solutions to the planning problems Œ†1 = hFO , D, I, G1 i and
Œ†2 = hFO , D, I, G2 i respectively.

f (1)

Bayesian Modeling of Teaming Intent

Going back to our use case in Figure 1, suppose the robot
has a goal to deliver a medkit to room1, and CommX has
a goal to conduct triage in room1, for which he also requires a medkit (and his optimal plan involves picking up
medkit1 in room2). For individual optimal plans both the
robot and the human will go for medkit2 (thus, in this situation, individual optimal plans are actually not feasible).
For the joint optimal, the coalition can team up to both use
the same medkit thus achieving mutual benefit. In case the
robot is only planning to avoid conflicts, it can settle for using medkit3 which is further away, or the robot can also
intervene serendipitously by handing over medkit2 in the
hallway thus achieving higher utility through cooperation
without directly coordinating.
For our problem, this has the implication that we can no
longer be sure of the plan (and consequently the utility) even

œÜ

f (|G |)

that GœÜ ‚äÜ GœÜ ‚äÜ . . . ‚äÜ GœÜ
. This means that the
goals of an agent are such that they are all different subgoals
of a single conjunctive goal.
We will refer to the game with agents with such ordered
goal sets as GAPI (identical to GAPI otherwise).
Claim. NEs in GAPI are preserved in GAPI.
Proof Sketch. Since G œÜ is goal-ordered, C(œÄf‚àó(1) ) ‚â§
C(œÄf‚àó(2) ) ‚â§ . . . ‚â§ C(œÄf‚àó(|G œÜ |) ), where, as usual, œÄi‚àó
is the optimal solution to the planning problem Œ†i =
hFO , D, I, GiœÜ i. Let us consider a non-trivial admissible
heuristic h and define a heuristic hÃÇ such that hÃÇ(I, GiœÜ ) =

550

powerful approximations, and the ability to deal with issues
such as synchronization and coalitions evolving across individual goal allocations. For GAPI-Bayesian, this also
includes evolving beliefs as we will see below.

when a particular goal has been chosen. Rather what we
have is a possible set of utilities for each goal. However we
can do better than to just take the maximum (or minimum
as the case may be) of these utilities as we did previously,
because we now know how such behaviors are being generated and so we can leverage additional information from an
agent‚Äôs beliefs about the other agent to come up with optimal response strategies. This readily lends the problem to a
formulation in terms of Bayesian strategic games, which we
will discuss in the next section.

4.2

5.2

Evolving Utilities. Often, and certainly in the examples
provided in Section 4.1, the behavior of the robot depends on
understanding the intent(s) of its human counterpart. Thus
the utilities will keep evolving based on the actions of the
human after the goal has been selected. This is even more
relevant in scenarios where communication is severely limited, when the agents in a coalition are not aware of the exact
goals that the other agents have selected.

Formulation of the Game

We define our two-person static Bayesian game
GAPI-Bayesian = hŒ¶, B, AH , {AR,B }, U H , {U R,B }i
with belief B over the type of robot as follows -

Evolving Beliefs. Intent recognition has a direct effect on
the belief over the robot type itself. For example, as the human observes the actions of the robot, it can infer which behavior the robot is going to exhibit. Thus intent recognition
over the robot‚Äôs actions will result in evolving belief of the
human, as opposed to intent recognition over the human‚Äôs
activities which informed the planning process and hence
the utilities of the robot.

- Players - We still have two players - the human H and the
robot R, as in the previous games.
- Actions - The actions of the players are similarly identical
to GAPI, i.e. the action set of agent œÜ ‚àà {H, R} is the
mapping AœÜ : G œÜ 7‚àí‚Üí G.
- Beliefs - The human has a set of beliefs on the robot
B = {B1 , B2 , . . . , B|B| } characterized by the distribution
B ‚àº P , i.e. the robot can be of any of the types in B with
probability P (B). The type of the robot is essentially the
algorithm it uses to compute the optimal plan given the
initial state and the selected goal, and thus affects the cost
of achieving the goal, and hence the utility function.
- Utilities - The utilities are defined as
R
i
‚àó
UH (AH
i , Aj , B) = R(GH ) ‚àí C(œÄŒò (H)|B)
j
H
R
‚àó
UR (Ai , Aj , B) = R(GR ) ‚àí C(œÄŒò (R)|B)
where symbols have their usual meaning.

5.3

6

Conclusions

In conclusion, we introduced a two-player static game that
can be used to form optimal coalitions on the go among two
autonomous members of a human-robot society, with minimum prior coordination. We also looked at several properties of such games that may be used to make the problem
tractable while still maintaining key properties of the game.
Finally, we explored an extension of the game to a general
Bayesian formulation when the human is not sure of the intent of the robot, and motivated the implications and expressiveness of this model. We believe the work will stimulate
discussion on ad-hoc interaction among agents in the context
of human-robot cohabitation settings and provide insight towards generating efficient synergy.

Discussions and Future Work

Acknowledgments

The concept of Bayesian games lends GAPI to several interesting possibilities, and promising directions for future
work, with respect to how interactions evolve with time.

5.1

Implications of Implicit Preferences

Finally, as agents interact with each other over time, in different capacities as teammates and colleagues, their expectations over which agent is likely to form which form of coalition will also evolve. This will give the prior belief over the
robot type that the human starts with, and will get updated
as further interactions occur.

As
before,
the
Nash
equilibriums
in
GAPI-Bayesian are given by action profiles
R
hAH
such that the human has no reason
i , Aj i
P
H
R
to defect, i.e.
‚â•
B‚ààB UH (Ai , Aj , B)P (B)
P
H
R
U
(A
,
A
,
B)P
(B)
while
the
robot
also
has
H
j
k : ‚àÄk6=i
B‚ààB
P
R
no incentive to change, i.e. B‚ààB UR (AH
i , Aj , B)P (B) ‚â•
H
R
UH (Ai , Ak : ‚àÄk6=j , B)P (B), given the distribution P over
the beliefs B of robot type. Similarly, the socially optiR
mal solution is given by the action profiles hAH
i‚àó , Aj ‚àó i
P
‚àó ‚àó
H
R
where {i , j } = arg maxi,j B‚ààB [UH (Ai , Aj , B) +
R
UR (AH
i , Aj , B)]P (B).

5

Impact of Intent Recognition

This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023. I would also
like to give special thanks to Prof. Guoliang Xue (with the
Department of Computer Science at Arizona State University) for his valuable support and inputs.

Unrolling the Entire Game

Notice that we formulated the game such that each of the
agents œÜ has a set of goals G œÜ to achieve. Thus GAPI immediately lends itself to a finite horizon dynamic game unrolled maxœÜ‚ààŒ¶ |G œÜ | times, so that the agents can figure out
their most effective long-term strategy and coalitions. Finding optimal policies in such cases will involve devising more

References
Ackerman, M., and BraÃÇnzei, S. 2014. The authorship
dilemma: Alphabetical or contribution? In Proceedings of

551

the 2014 International Conference on Autonomous Agents
and Multi-agent Systems, AAMAS ‚Äô14, 1487‚Äì1488. Richland, SC: International Foundation for Autonomous Agents
and Multiagent Systems.
Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.;
Scheutz, M.; Smith, D.; and Kambhampati, S. 2015a. Planning for serendipity. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).
Chakraborti, T.; Zhang, Y.; Smith, D.; and Kambhampati, S.
2015b. Planning with stochastic resource profiles: An application to human-robot co-habitation. In ICAPS Workshop
on Planning and Robotics.
Chakraborti, T.; Talamadupula, K.; Zhang, Y.; and Kambhampati, S. 2016. Interaction in human-robot societies. In
AAAI Workshop on Symbiotic Cognitive Systems.
Mcdermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram,
A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. Pddl - the
planning domain definition language. Technical Report TR98-003, Yale Center for Computational Vision and Control,.
Ray, D., and Vohra, R. 2014. Handbook of Game Theory.
Handbooks in economics. Elsevier Science.
Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of
useful social laws for artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì281.
Talamadupula, K.; Briggs, G.; Chakraborti, T.; Scheutz, M.;
and Kambhampati, S. 2014. Coordination in human-robot
teams using mental modeling and plan recognition. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), 2957‚Äì2962.
Tambe, M. 1997. Towards flexible teamwork. J. Artif. Int.
Res. 7(1):83‚Äì124.
Zhang, Y., and Kambhampati, S. 2014. A formal analysis
of required cooperation in multi-agent planning. In ICAPS
Workshop on Distributed Multi-Agent Planning (DMAP).
Zick, Y., and Elkind, E. 2014. Arbitration and stability in
cooperative games. SIGecom Exch. 12(2):36‚Äì41.
Zick, Y.; Chalkiadakis, G.; and Elkind, E. 2012. Overlapping coalition formation games: Charting the tractability frontier. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS ‚Äô12, 787‚Äì794. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.

552

A Formal Framework for Studying Interaction in Human-Robot Societies
Tathagata Chakraborti1

Kartik Talamadupula2

Yu Zhang1

Subbarao Kambhampati1

Department of Computer Science1

Cognitive Learning Department2

Arizona State University
Tempe, AZ 85281, USA

IBM Thomas J. Watson Research Center
Yorktown Heights, NY 10598, USA

{tchakra2, yzhan442, rao}@asu.edu

krtalamad@us.ibm.com

Abstract
As robots evolve into an integral part of the human
ecosystem, humans and robots will be involved in a
multitude of collaborative tasks that require complex
coordination and cooperation. Indeed there has been
extensive work in the robotics, planning as well as
the human-robot interaction communities to understand
and facilitate such seamless teaming. However, it has
been argued that their increased participation as independent autonomous agents in hitherto human-habited
environments has introduced many new challenges to
the view of traditional human-robot teaming. When
robots are deployed with independent and often selfsufficient tasks in a shared workspace, teams are often
not formed explicitly and multiple teams cohabiting an
environment interact more like colleagues rather than
teammates. In this paper, we formalize these differences
and analyze metrics to characterize autonomous behavior in such human-robot cohabitation settings.

Robots are increasingly becoming capable of performing
daily tasks with accuracy and reliability, and are thus getting integrated into different fields of work that were until now traditionally limited to humans only. This has made
the dream of human-robot cohabitation a not so distant reality. We are now witnessing the development of autonomous
agents that are especially designed to operate in predominantly human-inhabited environments often with completely
independent tasks and goals. Examples of such agents include robotic security guards like Knightscope, virtual presence platforms like Double and iRobot Ava, and even autonomous assistance in hospitals such as Aethon TUG. Of
particular fame are the CoBots (Rosenthal, Biswas, and
Veloso 2010) that can ask for help from unknown humans,
and thus interact with agents not directly involved in its plan.
Indeed there has been a lot of work recently in the context
of ‚Äúhuman-aware‚Äù planning, both from a point of view of
path planning (Sisbot et al. 2007; Kuderer et al. 2012) and
task planning (Koeckemann, Pecora, and Karlsson 2014;
Cirillo, Karlsson, and Saffiotti 2010), with the intention of
making the robot‚Äôs plans socially acceptable, e.g. resolving conflicts with the plans of fellow humans. Even though
all of these scenarios involve significantly different levels
of autonomy from the robotic agent, the underlying theme
of autonomy in such settings involves the robot achieving
some sense of independence of purpose in so much as its

existence is not just defined by the goals of the humans
around it but is rather contingent on tasks it is supposed to
be achieving on its own. Thus the robots in a way become
colleagues rather than teammates. This becomes even more
prominent when we consider interactions between multiple independent teams in a human-robot cohabited environment. We thus postulate that the notions of coordination
and cooperation between the humans and their robotic colleagues is inherently different from those investigated in existing literature on interaction in human-robot teams, and
should rather reflect the kind of interaction we have come
to expect from human colleagues themselves. Indeed recent
work (Chakraborti et al. 2015a; Chakraborti et al. 2015b;
Talamadupula et al. 2014) hints at these distinctions, but has
neither made any attempt at formalizing these ideas, nor provided methods to quantify behavior is such settings. To this
end, we propose a formal framework for studying inter-team
and intra-team interactions in human-robot societies, show
how existing metrics are grounded in this framework and
propose newer metrics that are useful for evaluating performance of autonomous agents in such environments.

1

Human Robot Cohabitation

At some abstracted level, agents in any environment can be
seen as part of a team achieving a high level goal. Consider, for example, your university or organization. At a micro level, it consists of many individual labs or groups that
work independently on their specific tasks. But when taken
as a whole, the entire institute is a team trying to achieve
some higher order tasks like increasing its relative standing
among its peers or competitors. So in the discussion that follows, we talk about environments, and teams or colleagues
acting within it, in the context of the goals they achieve.

1.1

Goal-oriented Environments

Definition 1.0 A goal-oriented environment is defined as
a tuple E = hF, O, Œ¶, G, Œõi, where F is a set of first order
predicates that describes the environment, and O is a set
of objects in the environment, Œ¶ ‚äÜ O is the set of agents,
G = {g | g ‚äÜ FO } is the set of goals that these agents
are tasked with, and Œõ ‚äÜ O is the set of resources that are
required by the agents to achieve their goals. Each goal has

a reward R(g) ‚àà R+ associated with it.1
These agents and goals are, of course, related to each
other by their tasks, and these relationships determine the
nature of their interactions in the environment, i.e. in the
form of teams or colleagues. Before we formalize such relations, however, we would look at the way the agent models
are defined. We use PDDL (Mcdermott et al. 1998) models
for the rest of the discussion, as described below, but most of
the discussion easily generalizes to other modes of representation. The domain model DœÜ of an agent œÜ ‚àà Œ¶ is defined
as DœÜ = hFO , AœÜ i, where AœÜ is a set of operators available
to the agent. The action models a ‚àà AœÜ are represented as
a = hCa , Pa , Ea i where Ca is the cost of the action, Pa ‚äÜ
FO is the list of pre-conditions that must hold for the action
a to be applicable in a particular state S ‚äÜ FO of the environment; and Ea = hef f + (a), ef f ‚àí (a)i, ef f ¬± (a) ‚äÜ FO
is a tuple that contains the add and delete effects of applying
the action to a state. The transition function Œ¥(¬∑) determines
the next state after the application of action a in state S as
Œ¥(a, S) = (S\ef f ‚àí (a))‚à™ef f + (a) if Pa ‚äÜ S; ‚ä• otherwise.
A planning problem for the agent œÜ is given by the tuple
Œ†Œ± = hF, O, DœÜ , IœÜ , GœÜ i, where IœÜ ‚äÜ FO is the initial state
of the world and GœÜ ‚äÜ FO is the goal state. The solution to
the planning problem is an ordered sequence of actions or
plan given by œÄœÜ = ha1 , a2 , . . . , a|œÄœÜ | i, ai ‚àà AœÜ such that
Œ¥(œÄœÜ , IœÜ ) |= GœÜ , where the cumulative transition function is
given by Œ¥(œÄ, s) = Œ¥(ha2 , a3 , . . .P
, a|œÄ| i, Œ¥(a1 , s)). The cost
of the plan is given by C(œÄœÜ ) = a‚ààœÄœÜ Ca .
We will now introduce the concept of a super-agent
transformation on a set of agents that combines the capabilities of one or more agents to perform complex tasks
that a single agent might not be able to do. This will help
us later to formalize the nature of interactions among agents.
Definition 1.1a A super-agent is a tuple Œò = hŒ∏, DŒ∏ i
where Œ∏ ‚äÜ Œ¶ is a set of agents in the environment E, and DŒ∏
is the transformation from the individual domainSmodels to
a composite domain model given by DŒ∏ = hFO , œÜ‚ààŒ∏ AœÜ i.
Note that this does not preclude joint actions among
agents, because some actions that need that need more than
one agent (as required in the preconditions) will only be
doable in the composite domain.
Definition 1.1b The planning problem of a super-agent Œò
is similarly given by Œ†Œò = hF, O, DŒ∏ , IŒ∏ , GŒ∏ i where
S the
composite initial and goal states are given by IŒ∏ = œÜ‚ààŒ∏ IœÜ
S
and GŒ∏ = œÜ‚ààŒ∏ GœÜ respectively. The solution to the planning problem is a composite plan œÄŒ∏ = h¬µ1 , ¬µ2 , . . . , ¬µ|œÄŒ∏ | i
where ¬µi = {a1 , . . . , a|Œ∏| }, ¬µ(œÜ) = a ‚àà AœÜ ‚àÄ¬µ ‚àà œÄŒ∏ such
that Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |=SGŒ∏ , where the modified
transition function
S
Œ¥ 0 (¬µ, s) = (s \ a‚àà¬µ eff‚àí (a)) ‚à™ a‚àà¬µ eff+ (a). We denote
the set of all such plans as œÄŒò .
P
P
The cost of a composite plan is C(œÄŒ∏ ) = ¬µ‚ààœÄŒ∏ a‚àà¬µ Ca
and œÄŒ∏‚àó is optimal if Œ¥ 0 (IŒ∏ , œÄŒ∏ ) |= GŒ∏ =‚áí C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏ ).
The composite plan can thus be viewed as a union of plans
contributed by each agent œÜ ‚àà Œ∏ so that œÜ‚Äôs component can
be written as œÄŒ∏ (œÜ) = ha1 , a2 , . . . , an i, ai = ¬µi (œÜ) ‚àÄ ¬µi ‚àà
1

SO is S ‚äÜ F instantiated or grounded with objects from O.

œÄŒò . Now we will define the relations among the components
of the environment E in terms of these agent models.
Definition 1.2 At any given state S ‚äÜ FO of the
environment E, a goal-agent correspondence is defined as the relation œÑ : G ‚Üí P(Œ¶); G, Œ¶ ‚àà E, that
induces a set of super-agents œÑ (g) = {Œò | Œ†Œò =
hF, O, DŒ∏ , S, gi has a solution, i.e. ‚àÉœÄ s.t. Œ¥(œÄ, S) |= g}.
In other words, œÑ (g) gives a list of sets of agents in the environment that are capable of performing a specific task g.
We will see in the next section how the notions of teammates
and colleagues are derived from it.

1.2

Teams and Colleagues

Definition 2.0 A team Tg w.r.t. a goal g ‚àà G is defined as
any super-agent Œò = hŒ∏, DŒ∏ i ‚àà œÑ (g) iff 6 ‚àÉœÜ ‚àà Œ∏ such that
Œò0 = hŒ∏ \ œÜ, DŒ∏\œÜ i and œÄŒò = œÄŒò0 .
This means that any super-agent belonging to a particular
goal-agent correspondence defines a team w.r.t that specific
goal when every agent that forms the super-agent plays
some part in the plans that achieves the task described
by g, i.e. the super-agent cannot use the same plans to
achieve g if an agent is removed from its composition.
This, then, leads to the concept of strong, weak, or optimal teams, depending on if the composition of the
super-agent is necessary, sufficient or optimal respectively
(note that an optimal team may or may not be a strong team).
Definition 2.0a A team Tgs = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is strong iff 6 ‚àÉœÜ ‚àà Œ∏ such that hŒ∏ \ œÜ, DŒ∏\œÜ i ‚àà œÑ (g).
A team Tgw is weak otherwise.
Definition 2.0b A team Tgo = hŒ∏, DŒ∏ i ‚àà œÑ (g) w.r.t a goal
g ‚àà G is optimal iff ‚àÄŒò0 ‚àà œÑ (g), C(œÄŒ∏‚àó ) ‚â§ C(œÄŒ∏‚àó0 ).
This has close ties with concepts of required cooperation and capabilities of teams to solve general planning
problems, introduced in (Zhang and Kambhampati
2014), and work on team formation mechanisms and
properties of teams (Shoham and Tennenholtz 1992;
Tambe 1997). In this paper, we are more concerned about
the consequences of such team formations on teaming metrics. So, with these different types of teams we have seen
thus far, the question we ask is: What is the relation among
the rest of the agents in the environment? How do these
different teams interact among and between themselves?
Definition 2.1 The set of teams in E are defined by the
relation Œ∫ : G ‚Üí R(œÑ ); G ‚àà E, where Œ∫(g) ‚àà œÑ (g) denotes
the team assigned to the goal g, ‚àÄg ‚àà G.
This, then, gives rise to the idea of collegiality among
agents, due to both inter-team and intra-team interactions.
Note that how useful or necessary such interactions are
will depend on whether the colleagues can contribute to
each other‚Äôs goals, or to what extent they influence their
respective plans, which leads us to the following two
definitions of colleagues based on the concept of teams.
Definition 2.2a Let Œ∫(g) = hŒ∏1 , DŒ∏1 i, Œ∫(g 0 ) = hŒ∏2 , DŒ∏2 i
be two teams in E. An agent œÜ1 ‚àà Œ∏1 is a type-1 colleague
to an agent œÜ2 ‚àà Œ∏2 when Œ∫0 (g) = hŒ∏1 ‚à™ œÜ1 , DŒ∏1 ‚à™œÜ1 i is a
weak team w.r.t. the goal g.

Definition 2.2b Agents œÜ1 , œÜ2 ‚àà Œ¶ are type-2 colleagues
when ‚àÄŒ∫(g) = hŒ∏, DŒ∏ i s.t. {œÜ1 , œÜ2 } ‚à© Œ∏ 6= ‚àÖ, {œÜ1 , œÜ2 } 6‚àà
Œ∏ ‚àß Œ∫0 (g) = hŒ∏ ‚à™ {œÜ1 , œÜ2 }, DŒ∏‚à™{œÜ1 ,œÜ2 } i is a weak team.
Thus type-1 colleagues can potentially contribute to the
plans of their colleagues, while type-2 colleagues cannot.
Plans of type-2 colleagues can, however, influence each
other (for example due to conflicts on usage of shared resources), while type1-colleagues are capable of becoming
teammates dynamically during plan execution.
Humans in the loop. Instead of a general set of agents, we
define the set of agents Œ∏ in a super-agent as composition of
humans and robots Œ∏ = h(Œ∏)‚à™r(Œ∏) so that the domain model
of the super-agent is alsoS
a composition
of the human and
S
robot capabilities DŒ∏ = œÜ‚ààh(Œ∏) œÜ‚ààr(Œ∏) AœÜ = h(DŒ∏ ) ‚à™
r(DŒ∏ ). We denote the communication actions of the superagent as the subset c(DŒ∏ ) ‚äÜ DŒ∏ .

2
2.1

Metrics for Human Robot Interaction
Metrics for Human Robot Teams

We will now ground popular (Olsen Jr. and Goodrich 2003;
Steinfeld et al. 2006; Hoffman and Breazeal 2007;
Hoffman 2013) metrics for human-robot teams in our
current formulation.
Task Effectiveness These are the metrics that measure
the effectiveness of a team in completing its tasks.
‚Ä¢ Cost-based
Metrics - This simply measures the cost
P
‚àó
g‚ààŒ∫‚àí1 (Œò) C(œÄŒò ) of all the (optimal) plans a specific
team executes (for all the goals it has been assigned to).
‚Ä¢ Net Benefit Based Metrics - This is based on both
plan costs as well as the value of goals and is given by
P
‚àó
g‚ààŒ∫‚àí1 (Œò) R(g) ‚àí C(œÄŒò ).
‚Ä¢ Coverage Metrics - Coverage metrics for a particular
team determine the diversity of its capabilities in terms of
the number of goals it can achieve |Œ∫‚àí1 (Œò)|.
Team Effectiveness These measure the effectiveness of
(particularly human-robot) teaming in terms of communication overhead and smoothness of coordination.
‚Ä¢ Neglect Tolerance - This measures how long the
robots in a team Œò is able to perform well without human intervention. WeT can measure this as
‚àó
N T = max{|i ‚àí j| s.t. h(DŒ∏ ) œÜ‚ààŒ∏ œÄŒò
(œÜ)[i : j] = ‚àÖ}.
‚Ä¢ P
Interaction Time - This is given by IT
=
‚àó
|{i | c(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|, and measures the
time spent by a team Œò in communication.
‚Ä¢ Robot Attention Demand - Measures how much attention the robot is demanding and is given by IT IT
+N T .
‚Ä¢ Secondary Task Time - This measures the ‚Äúdistraction‚Äù to a team, and can be expressed as
time not spent on achieving a given goal g, i.e.
‚àó
‚àó
ST T = |{i | œÄŒò
[i] ¬∑¬∑= ‚àÖ ‚àß Œ¥ 0 (s, œÄŒò
) |= g}|.
‚Ä¢ Free Time - F T = 1 ‚àí RAD is a measure of the fraction
of time the humans are not interacting with the robot.
‚Ä¢ Human Attention Demand - HAD = F T ‚àí h(ST T )
‚àó
‚àó
where h(ST T ) = |{i | œÄŒò
[i]‚à©h(DŒ∏ ) ¬∑¬∑= ‚àÖ‚àßŒ¥ 0 (s, œÄŒò
) |=
‚àó
g}|/|œÄŒò | is the time humans spend on the secondary task.

‚Ä¢ Fan Out - This is a measure of the communication load
on the humans, and consequently the number of robots
that should participate in a human-robot team, and is
proportional to F O ‚àù |h(Œ∏)|/RAD.
‚Ä¢ Interaction Time - Measures how quickly and effectively
T)
interaction takes places as IT = N T (1‚àíST
.
ST T
‚Ä¢ Robot Idle Time - Captures inconsistency or irregularity
in coordination from the point of view of the robotic
agent, and can be measured as the amount of time the
‚àó
robots are idle, i.e. RIT = |{i | r(DŒ∏ ) ‚à© œÄŒò
[i] = ‚àÖ|.
‚Ä¢ Concurrent Activity - We can talk of concurrency within
a team as the time that humans and robots are working
‚àó
concurrently CA1 = |{i | r(DŒ∏ ) ‚à© h(DŒ∏ ) ‚à© œÄŒò
[i] 6= ‚àÖ}|
and also across teams as the maximum time teams are
operating concurrently CA2 = max{|{i | œÄŒò [i] 6=
‚àÖ ‚àß œÄŒò0 [i] 6= ‚àÖ}| ‚àÄŒò, Œò0 ‚àà R(Œ∫)}.
In measuring performance of agents in cohabitation, both
as teammates and colleagues, we would still like to reduce
interactions times and attentions demand, while simultaneously increasing neglect tolerance and concurrency. However, as we will see in Section 3, these metrics do not effectively capture all the implications of the interactions desired
in human-robot cohabitation. So the purpose of the rest of
our paper is to establish metrics that can measure the effective behavior of human-robot colleagues, and to see to
what extent they can capture desired behaviors of robotic
colleagues suggested in existing literature.

2.2

Metrics for Human Robot Colleagues

We will now propose new metrics that are useful for
measuring collegial interactions, see how they differ from
teaming metrics discussed so far, and then relate them to
existing work on human-robot cohabitation.
Task Effectiveness The measures for task effectiveness
must take into account that agents are not necessarily
involved in their assigned team task only.
‚Ä¢ Altruism - This is a measure of how useful it is for
a robotic agent r to showcase altruistic behavior in
assisting their human colleagues, and is given by the ratio
of the gain in utility by adding a robotic colleague to a
team Œò to the decrease in utilityP
of plans of the teams r is
‚àó
‚àó
‚àó
involved in |œÄŒò
‚àíœÄhŒ∏‚à™r,D
|/
Œò=hŒ∏,DŒ∏ i s.t. r‚ààŒ∏ ‚àÜ|œÄŒò |.
Œ∏‚à™r i
For such a dynamic coalition to be useful, r must be a
type-1 colleague to the agents Œ∏ ‚àà Œò.
‚Ä¢ Lateral Coverage - This measures how deviating from
optimal team compositions can achieve global good in
terms
of number of goals achieved by a team, LT =
P
‚àí1
(Tg )| ‚àí |Œ∫‚àí1 (Tgo )|]/|Œ∫‚àí1 (Tgo )|}
Tg =Œ∫(g),‚àÄg‚ààG {[|Œ∫
across all the teams that have been formed in E.
‚Ä¢ Social Good - Many times, while planning with humans
in the loop, cost optimal plans are not necessarily the
optimal plans in the social context. This is useful to
measure particularly when agents are interacting outside
teams, and the compromise in team utility is compensated
by the gain in
This can be
Pmutual utility of colleagues.
‚àó
expressed as g‚ààG {C(œÄŒ∫(g) ) ‚àí C(œÄŒ∫(g)
)}.

Interaction Effectiveness The team effectiveness measures need to be augmented with measures corresponding
to interactions among non-team members. While all these
metrics are relevant for robotic colleagues as well, they
become particularly important in human-robot interactions,
where information is often not readily sharable due to
higher cognitive mismatch, so as to reduce cognitive
demand/overload.
‚Ä¢ Interaction Time - In addition to Interaction Time for
human-robot teams, and measures derived from it, we
propose two separate components of interaction time for
general human-robot cohabitation scenarios.
- External Interaction Time - This is the time spent by
agents interacting with type-1 colleagues (EIT1 ).
- Extraneous Interaction Time - This is the time spent
by agents interacting with type-2 colleagues (EIT2 ).
‚Ä¢ Compliance - This refers to how much actions of an agent
disambiguate its intentions. Though relevant for both, this
becomes even more important in absence of teams, when
information pertaining to goals or plans are not necessarily sharable. Thus the intention should be to maximize
the probability P (GŒ∏ = g | s = Œ¥(œÄŒ∏ [1 : i], IŒ∏ )), Œ∫(g) =
hŒ∏, DŒ∏ i, ‚àÄg ‚àà G given any stage i of plan execution and
P (¬∑) is a generic goal recognition algorithm. This can be
relevant both in terms of disambiguating goals (Keren,
Gal, and Karpas 2014) or explaining plans given a goal
(Zhang, Zhuo, and Kambhampati 2015).
‚Ä¢ External Failure - This is the number of times optimal
plans fail when resources are contested among colleagues.
‚Ä¢ Stability - Of course with continuous interactions, team
formations change, so this gives a measure of stability of
the system as a whole. If teams Œ∫(g) = hŒ∏1 , DŒ∏1 i and
Œ∫(g) = hŒ∏2 , DŒ∏2 i achieves a P
goal g ‚àà G at two different
instances, then stability S = g‚ààG |Œ∏1 ‚à© Œ∏2 |/|Œ∏1 ||Œ∏2 |.

3

Discussion and Related Work

We will now investigate the usefulness of the proposed metrics in quantifying behavioral traits proposed in existing literature as desirable among cohabiting human and robots.
Human-Aware Planning. In (Koeckemann, Pecora, and
Karlsson 2014; Cirillo, Karlsson, and Saffiotti 2010) the authors talk of adapting robot plans to suit social norms (e.g.
not to vacuum a room while a human is asleep). Clearly,
this involves the robots departing from their preferred plans
to conform to human preferences. In such cases, involving
assistive robots, measures of Altruism and Social Good become particularly relevant, while it is also crucial to reduce
unwanted interactions (EIT1 + EIT2 ).
Planning with Resource Conflicts. In (Chakraborti et al.
2015b) the authors outline an approach for robots sharing
resources with humans to compute plans that minimize conflicts in resource usage. Thus, this line of work is aimed at
reducing External Failures, while simultaneously increasing
Social Good. Measures of Stability and Compliance become
relevant, to capture evolving beliefs and their consequences
on plans. Extraneous Interaction Time is also an important

measure, since additional communication is always a proxy
to minimizing coordination problems between colleagues.
Planning for Serendipity. In (Chakraborti et al. 2015a)
the authors propose a formulation for the robot to produce
positive exogenous events during the execution of the human‚Äôs plans, i.e. interventions which will be useful to the human regardless of whether he was expecting assistance from
the robot. This work particularly looks at planning for Altruism. Increasing Compliance in agent behavior can provide
better performance in this regard. Further, External Interaction is crucial in such cases for forming such impromptu
coalitions among colleagues.
Relation to Metrics in Human Factor Studies It is useful
to see an example of how the general formulation of metrics
we discussed so far are actually grounded in human factors
studies (Zhang et al. 2015) of scenarios that display some aspects of collegial interaction. The environment studied was
a disaster response scenario, involving an autonomous robot
that may or may not chose to proactively help the human.
The authors used External Interaction Time or EIT1 to measure the effectiveness of proactive support (how often the
proactive support resulted in further deliberation over goals),
while Lateral Coverage (in terms of number of people rescued) showed the effectiveness of proactive support. Further,
qualitative analysis on acceptance and usefulness of agents
that display proactive support are closely related to measures
such as Social Good and Altruism.
Work on Ad-hoc Coalition Formations Given the framework we have discussed thus far, the question is then, apart
from measuring performance, how we can use it to facilitate collegial interactions among agents. Especially relevant
in such scenarios are work on ad-hoc coalition formation
among agents sharing an environment but not necessarily
goals (Stone et al. 2010). In (Chakraborti et al. 2016) we
show how this framework may be used to cut down on prior
coordination while forming coalitions.

4

Conclusion and Future Work

In conclusion, we discussed interaction in human-robot societies involving multiple teams of humans and robots in the
capacity of teammates or as colleagues, provided a formal
framework for talking about various modes of cooperation,
and reviewed existing metrics and proposed new ones that
can capture these different modalities of teaming or collegial behavior. Finally we discussed how such metrics can be
useful in evaluating existing works in human-robot cohabitation. One line of future inquiry would be to see how such
quantitative metrics are complemented by qualitative feedback from human factor studies, to establish what the desired trade-offs are, in order to ensure well-informed design
of symbiotic systems involving humans and robots.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
[Aethon TUG ] Aethon TUG. Intralogistics automation platform for hospitals. http://www.aethon.com/.
[Chakraborti et al. 2015a] Chakraborti, T.; Briggs, G.; Talamadupula, K.; Zhang, Y.; Scheutz, M.; Smith, D.; and
Kambhampati, S. 2015a. Planning for serendipity. In International Conference on Intelligent Robots and Systems.
[Chakraborti et al. 2015b] Chakraborti, T.; Zhang, Y.; Smith,
D.; and Kambhampati, S. 2015b. Planning with stochastic resource profiles: An application to human-robot cohabitation. In ICAPS Workshop on Planning and Robotics.
[Chakraborti et al. 2016] Chakraborti, T.; Dondeti, V.;
Meduri, V. V.; and Kambhampati, S. 2016. A game theoretic approach to ad-hoc coalition formation in human-robot
societies. In AAAI Workshop on Multi-Agent Interaction
without Prior Coordination.
[Cirillo, Karlsson, and Saffiotti 2010] Cirillo, M.; Karlsson,
L.; and Saffiotti, A. 2010. Human-aware task planning:
An application to mobile robots. ACM Trans. Intell. Syst.
Technol. 1(2):15:1‚Äì15:26.
[Double ] Double. The ultimate tool for telecommuting.
http://www.doublerobotics.com/.
[Hoffman and Breazeal 2007] Hoffman, G., and Breazeal, C.
2007. Effects of anticipatory action on human-robot teamwork: Efficiency, fluency, and perception of team. In
Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on, 1‚Äì8.
[Hoffman 2013] Hoffman, G. 2013. Evaluating fluency in
human-robot collaboration. In Robotics: Science and Systems (RSS) Workshop on Human-Robot Collaboration.
[iRobot Ava ] iRobot Ava.
Video collaboration robot.
http://www.irobot.com/For-Business.aspx.
[Keren, Gal, and Karpas 2014] Keren, S.; Gal, A.; and
Karpas, E. 2014. Goal recognition design. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS 2014, Portsmouth,
New Hampshire, USA, June 21-26, 2014.
[Knightscope ] Knightscope. Autonomous data machines.
http://knightscope.com/about.html.
[Koeckemann, Pecora, and Karlsson 2014] Koeckemann,
U.; Pecora, F.; and Karlsson, L. 2014. Grandpa hates
robots - interaction constraints for planning in inhabited
environments. In Proc. AAAI-2010.
[Kuderer et al. 2012] Kuderer, M.; Kretzschmar, H.; Sprunk,
C.; and Burgard, W. 2012. Feature-based prediction of trajectories for socially compliant navigation. In Proceedings
of Robotics: Science and Systems.
[Mcdermott et al. 1998] Mcdermott, D.; Ghallab, M.; Howe,
A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and
Wilkins, D. 1998. Pddl - the planning domain definition language. Technical Report TR-98-003, Yale Center for Computational Vision and Control,.
[Olsen Jr. and Goodrich 2003] Olsen Jr., D., and Goodrich,
M. A. 2003. Metrics for evaluating human-robot interactions. In Performance Metrics for Intelligent Systems.

[Rosenthal, Biswas, and Veloso 2010] Rosenthal,
S.;
Biswas, J.; and Veloso, M. 2010. An effective personal
mobile robot agent through symbiotic human-robot interaction. In Proceedings of the 9th International Conference
on Autonomous Agents and Multiagent Systems: Volume 1
- Volume 1, AAMAS ‚Äô10, 915‚Äì922. Richland, SC: International Foundation for Autonomous Agents and Multiagent
Systems.
[Shoham and Tennenholtz 1992] Shoham, Y., and Tennenholtz, M. 1992. On the synthesis of useful social laws for
artificial agent societies. In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI‚Äô92, 276‚Äì
281. AAAI Press.
[Sisbot et al. 2007] Sisbot, E.; Marin-Urias, L.; Alami, R.;
and Simeon, T. 2007. A human aware mobile robot motion
planner. Robotics, IEEE Transactions on 23(5):874‚Äì883.
[Steinfeld et al. 2006] Steinfeld, A.; Fong, T.; Kaber, D.;
Lewis, M.; Scholtz, J.; Schultz, A.; and Goodrich, M. 2006.
Common metrics for human-robot interaction. In Proceedings of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, 33‚Äì40.
[Stone et al. 2010] Stone, P.; Kaminka, G. A.; Kraus, S.; and
Rosenschein, J. S. 2010. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. In Proceedings of
the Twenty-Fourth Conference on Artificial Intelligence.
[Talamadupula et al. 2014] Talamadupula, K.; Briggs, G.;
Chakraborti, T.; Scheutz, M.; and Kambhampati, S. 2014.
Coordination in human-robot teams using mental modeling
and plan recognition. In IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2957‚Äì2962.
[Tambe 1997] Tambe, M. 1997. Towards flexible teamwork.
J. Artif. Int. Res. 7(1):83‚Äì124.
[Zhang and Kambhampati 2014] Zhang, Y., and Kambhampati, S. 2014. A formal analysis of required cooperation in
multi-agent planning. In ICAPS Workshop on Distributed
Multi-Agent Planning (DMAP).
[Zhang et al. 2015] Zhang, Y.; Narayanan, V.; Chakraborti,
T.; and Kambhampati, S. 2015. A human factors analysis
of proactive support in human-robot teaming. In IEEE/RSJ
International Conference on Intelligent Robots and Systems.
[Zhang, Zhuo, and Kambhampati 2015] Zhang, Y.; Zhuo,
H. H.; and Kambhampati, S. 2015. Plan explainability and
predictability for cobots. CoRR abs/1511.08158.

Capability Models and Their Applications in Planning
Yu Zhang
Dept. of Computer Science Arizona State University Tempe, AZ

Sarath Sreedharan
Dept. of Computer Science Arizona State University Tempe, AZ

Subbarao Kambhampati
Dept. of Computer Science Arizona State University Tempe, AZ

yzhan442@asu.edu

ssreedh3@asu.edu

rao@asu.edu

ABSTRACT
One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often not provided, and hence must be learned from plan execution traces. As a result, these models of other agents are inherently partial and incomplete. Most existing agent models are based on action modeling and do not naturally allow for incompleteness. In this paper, we introduce a new and inherently incomplete modeling approach based on the representation of capabilities, which has several unique advantages. First, we show that the structures of capability models can be learned or easily specified, and both model structure and parameter learning are robust to high degrees of incompleteness in plan traces (e.g., with only start and end states partially observed). Furthermore, parameter learning can be performed efficiently online via Bayesian learning. While high degrees of incompleteness in plan traces presents learning challenges for traditional (complete) models, capability models can still learn to extract useful information. As a result, capability models are useful in applications in which traditional models are difficult to obtain, or models must be learned from incomplete plan traces, e.g., robots learning human models from observations and interactions. Furthermore, we discuss using capability models for single agent planning, and then extend it to multi-agent planning (with each agent modeled separately by a capability model), in which the capability models of agents are used by a centralized planner. The limitation, however, is that the synthesized "plans" (called c-plans) are incomplete, i.e., there may or may not be a complete plan for a c-plan. This is, however, unavoidable for planning using partial and incomplete models (e.g., considering planning using action models learned from partial and noisy plan traces).

1.

INTRODUCTION

Categories and Subject Descriptors
I.2.11 [Multiagent systems]; I.2.6 [Knowledge acquisition]; I.2.8 [Plan execution, formation, and generation]

Keywords
Capability models; Agent theories and models; Teamwork in humanagent mixed networks

Appears in: Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015), Bordini, Elkind, Weiss, Yolum (eds.), May 4≠8, 2015, Istanbul, Turkey. Copyright c 2015, International Foundation for Autonomous Agents
and Multiagent Systems (www.ifaamas.org). All rights reserved.

One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of others. These models can be used by a centralized planner (e.g., on a robot) or via a distributed planning process to perform task planning and allocation, or by the agents themselves to reduce communication and collaboration efforts. In many applications, these models of other agents are not provided and hence must be learned. As a result, these models are going to be inherently partial and incomplete. Thus far, most traditional agent models are based on action modeling (e.g., [18, 7]). These models are not designed with partial information in mind and hence are complete in nature. In this paper, we introduce a new and inherently incomplete modeling approach based on the representation of capabilities. We represent a capability as the ability to achieve a partial state given another partial state. A capability can be fulfilled (or realized) by any action sequence (or plan) that can implement a transition between the two partial states. Each such action sequence is called an operation in this paper. A capability model can encode all possible capabilities for an agent in a given domain, as well as capture the probabilities of the existence of an operation to fulfill these capabilities (to implement the associated transitions). These probabilities determine which capabilities are more likely to be used in planning. Compared to traditional agent models which are complete in nature, capability models have their unique benefits and limitations. In this aspect, capability models should not be considered as a competitor to complete models. Instead, they are useful when complete models are difficult to obtain or only partial and incomplete information can be retrieved to learn the models. This is often true when humans must be modeled. The representation of a capability model is a generalization of a two time slice dynamic Bayesian network (2-TBN). While a 2TBN is often used to represent a single action (e.g., [19]), a capability model can encode all possible capabilities for an agent in a given domain. In a capability model, each node in the first time slice (also called a fact node) represents a variable that is used in the specification of the initial world state. For each fact node, there is also a corresponding node in the second time slice, which represents the fact node in the eventual state (i.e., after applying a capability). These corresponding nodes are called eventual nodes or e-nodes. The state specified by the fact nodes is referred to as the initial state, and the state specified by the e-nodes is referred to as the eventual state. The edges between the nodes within the initial and eventual states, respectively, encode the correlations between the variables at the same time instance (i.e., variables in synchrony). The edges from the fact nodes to e-nodes encode causal relationships. Both types of edges can be learned (e.g., using learning techniques in [24, 25] for causal relationships). In the case that no prior infor-

Figure 1: Capability model (as a generalized 2-TBN) for a human delivery agent (denoted as AG) in our motivating example with 6 fact node and e-node pairs. This model can encode all possible capabilities of the agent in this domain. Each fact node corresponds to a variable in the specification of the initial state. e-nodes are labeled with a dot on top of the variable. Edges from the fact nodes to the corresponding e-nodes are simplified for a cleaner representation. Any partial initial state coupled with any partial eventual state may imply a capability.

situation can naturally occur in human-robot teaming scenarios, in which human models need to be learned and the robot models are given. Similarly, the synthesized multi-agent c-plans are incomplete (unless all plan steps use robot actions). c-plans are useful when we have to plan using partial and incomplete models (e.g., considering planning using action models learned from partial and noisy plan traces), since they inform the user how likely complete plans can be realized by following their "guidelines". The rest of the paper is organized as follows. First, we provide a motivating example in Section 2. In Section 3, we discuss capability models in detail. We discuss how to use capability models in planning in Sections 4 and 5. The relationships between capability models and other existing approaches to modeling the dynamics of agents are discussed as related work in Section 6. We conclude afterwards.

2.

MOTIVATING EXAMPLE

mation is provided, the safest approach is to not assume any independence. Consequently, we can specify a total ordering between the nodes within the initial and final states, and connect a node at a given level to all nodes at lower levels, as well as every fact node to every e-node. After model construction, parameter learning of a capability model can be performed efficiently online via Bayesian learning. One of the unique advantages of capability models is that learning for both model structure and parameters is robust to high degrees of incompleteness in plan execution traces. This incompleteness occurs commonly. For example, the execution observer may not be constantly monitoring the executing agent, and the executing agent may not always report the full traces [26]. Capability models can be learned even when only the initial and final states are partially observed in plan traces for training. While high degrees of incompleteness in plan traces presents learning challenges for traditional models, capability models can still learn to extract useful information out of them, e.g., probabilities of the existence of an operation to achieve certain eventual states given certain initial states. Furthermore, compared to traditional models for planning, capability models only suggest transitions between partial states (as capabilities), along with the probabilities that specify how likely such transitions can be implemented. Hence, a "plan" synthesized with capability models is incomplete, and we refer to such a plan as a c-plan (i.e., involving the application of capabilities). More specifically, we show that "planning" with capability models occurs in the belief space, and is performed via Bayesian inference. At any planning step, the current planning state is a belief state. Applying a capability to this belief state requires the planner to update this state using Bayesian inference on the capability model. After the discussion of single agent planning, we then extend to multi-agent planning.1 In such cases, we can consider planning as assigning subtasks to agents without understanding precisely how these subtasks are to be handled. Moreover, multi-agent planning can be performed when part of the agents are modeled by capability models, and the other agents are modeled by complete models. This
1 In this paper, we consider sequential multi-agent plans; synchronous or joint actions of agents are not considered.

We start with a motivating example for capability models. In this example, we have a set of human delivery agents, and the tasks involve delivering packages to their destinations. However, there are a few complications here. An agent may be able to carry a package if this agent is strong enough. While an agent can use a trolley to load a package, this agent may not remember to bring a trolley initially. An agent can visit an equipment rental office to rent a trolley, if this agent remembers to bring money for the rental. A trolley increases the chance of an agent being able to deliver a package (i.e., how likely the package is deliverable by this agent). Figure 1 presents the capability model for a human delivery agent, which is a generalized 2-TBN that can encode all possible capabilities for the agent in this domain. Similar to 2-TBN, fact nodes (e-nodes) in a capability model are in synchrony in the initial (eventual) state. Note that any partial initial state coupled with any partial eventual state may imply a capability. In this example, the correlations between the variables are clear. For example, whether an agent can carry a package is dependent on whether the agent is strong; whether an agent can deliver a package may be influenced by whether the agent has a trolley. These correlations are represented as edges between the variables within the initial and eventual states, respectively and identically. Assuming that no prior information is provided for the causal relationships, we connect every fact node with every e-node (denoted as a single edge in Figure 1). After obtaining the model structure, to perform parameter learning for the capability model of an agent, this agent can be given delivery tasks spanning a period of time for training purpose. However, the only observations that a manager has access to may be the number of packages that have been successfully delivered by this agent during this period. While this presents significant challenges for learning the parameters of traditional complete models, the useful information for the manager is already encoded: the probability that this agent can deliver a package. Capability models can learn this information from the incomplete traces. Now, suppose that the manager also has a set of robots (with action models) to use that can fetch items for the delivery agents. Since it is observed that the presence of a trolley increases the probability of successful delivery based on the capability models, the manager can make a multi-agent c-plan, which ensures that the robots deliver a trolley at least to some of the delivery agents. This also illustrates that it is beneficial to combine capability models with other complete models (i.e., action models) when they are available (e.g., for robots in this example).

3.

CAPABILITY MODEL

following probability distribution:
T

In our settings, the environment includes a set of agents  working inside it. For each agent, for simplicity, we assume that the state of the world and this agent is specified by a set of boolean variables X (  ). This implies that an agent can only interact with other agents through variables that pertain to the world state. More specifically, for all Xi  X , Xi has domain D(Xi ) = {true, false}. To specify partial state, we augment the domains of variables to include an unknown or unspecified value as in [1]. We write D+ (Xi ) = D(Xi )  {u}. Hence, the (partial) state space is denoted as S = D+ (X1 ) ◊ D+ (X2 ) ◊ ... ◊ D+ (XN ), in which N = |  X |.

 ) = P (X , X
0

  , t) dt P (X , X

(1)

in which T represents the maximum length of any operation (i.e.,  number of actions) for .2 X represents the initial state and X  represents the eventual state. Furthermore, P (X , t| X ) is the   in exact time t given probability of any operation resulting in X X . Hence, the probability that is associated with a capability sI  sE (i.e., P (sI  sE )) encodes:   = sE | X = sI ) = P (X
t

  = sE , t | X = sI ) dt (2) P (X

3.1

Capability

First, we formally define capability for an agent   . The state space of agent  is denoted as S . D EFINITION 1 (C APABILITY ). Given an agent , a capability specified as a mapping S ◊ S  [0, 1], is an assertion about the probability of the existence of complete plans (for ) that connect an initial state (i.e., the first component on the left hand side of the mapping) to an eventual state (i.e., the second component). A capability is also denoted as sI  sE (i.e., the initial state  the eventual state) when we do not need to reference the associated probability value. The probability value is denoted as P (sI  sE ), which we will show later is computed based on the capability model via Bayesian inference. There are a few notes about Definition 1. First, both sI and sE can be partial states: the variables that have the value u are assumed to be unknown (in the initial state) and unspecified (in the eventual state). In this paper, we refer to a complete plan that fulfills (or realizes) a capability as an operation, which is going to be decided and implemented by the executing agent during execution. Although capabilities seem to be similar to high-level actions in HTN, the semantics is different from angelic uncertainty in HTN [12]. While the existence of a concretization is ensured in HTN planning via reduction schemas, it is only known with some level of certainty that a concretization (i.e., operation) exists for a capability in a capability model, and the capability model does not provide specifics for such a concretization. More discussions on this are provided in Section 6 when we discuss the relationships between capability models and existing approaches to modeling agent dynamics. Capability models also address the qualification and ramification problems, which are assumed away in STRIPS planning (and planning with many complete models). More specifically, an operation for a capability sI  sE may be dependent on variables with unknown values in sI , and updating variables with unspecified values in sE . This is a unique characterization of capability and critical for learning capability models with incomplete observations. For example, a capability may specify that given coffee beans, an agent can make a cup of coffee. Thus, we have a capability {Has coffee beans = true}  {Has coffee = true}. An operation for this capability to make a coffee may be dependent on the availability of water initially, which is not specified in the capability. Similarly, this operation may negate the fact that the kitchen is clean, which is not specified in the capability either. Note that a capability may be fulfilled by operations with different specifications of the initial and eventual states, as long as these specifications all satisfy the specification of this capability. A capability model of an agent is capable of encoding all capabilities of this agent given a domain; it is designed to encode the

We construct the capability model of an agent as a Bayesian network. As an inherently incomplete model, it not only allows the initial and eventual states to be partially specified for any capability (and hence the fulfilling operations) that it encodes, but also allows the correlations between the variables within the initial and eventual states, as well as the causal relationships between them to be partially specified (e.g., when learning from plan traces). However, there are certain implications in this (e.g., the modeling can lose some information), which we will discuss in Section 3.4. Along the line of the qualification and ramification problems, a capability model also allows certain variables to be excluded completely from the network (i.e., related variables that are not captured in X ) due to incomplete knowledge. For example, whether an agent can drive a car (with a manual transmission) to a goal location is dependent on whether the agent can drive a manual car, even through the agent has a driver license. In this case, the ability to drive a manual car may have been ignored when creating the model.

3.2

Model Construction

We construct the capability model of each agent as an augmented Bayesian network [14]. Any partial initial state coupled with any partial eventual state may imply a capability; the probability that a capability actually exists (i.e., it can be fulfilled by an operation) is computed via a Bayesian inference in the network. We use augmented Bayesian network since it allows prior beliefs of conditional relative frequencies to be specified before observations are made, as well as enables us to adjust how fast these beliefs should change. D EFINITION 2. An augmented Bayesian network (ABN) (G, F, ) is a Bayesian network with the following specifications: ∑ A DAG G = (V, E ), where V is a set of random variables, V = {V1 , V2 , ..., Vn }. ∑ F is a set of auxiliary parent variables for V . ∑ Vi  V , an auxiliary parent variable Fi  F of Vi , and a density function i associated with Fi . Each Fi is a root and it is only connected to Vi . ∑ Vi  V , for all values pai of the parents P Ai  V of Vi , and for all values fi of Fi , a probability distribution P (Vi |pai , fi ). A capability model of an agent  is then defined as follows: D EFINITION 3 (C APABILITY M ODEL ). A capability model of an agent , as a binomial ABN (G , F, ), has the following specifications:
2

We assume in this paper that time is discretized.

 . ∑ V = X  X ∑ Vi  V , the domain of Vi is D(Vi ) = {true, false}. ∑ Vi  V , Fi = {Fi1 , Fi2 , ...}, and each Fij is a root and has a density function ij (fij ) (0  fij  1). (For each value paij of the parents P Ai , there is an associated variable Fij .) ∑ Vi  V , P (Vi = true|paij , fi1 , ...fij , ...) = fij . in which j in paij indexes into the values of P Ai . j in fij indexes into the variables in Fi . Note that defining partial states (i.e., allowing variables to assume the value u in a state) is used to more conveniently specify the distribution in Equation 1. Variables in the capability model do not need to expand their domains to include u. For edge construction, we can learn the correlations and causal relationships from plan traces. Note that the correlations between variables must not form loops; otherwise, they need to be broken randomly. When no training information is provided, we can specify a total ordering between the nodes within the initial and final states, and connect a node at a given level to all nodes at lower levels, as well as every fact node to every e-node. Denote the set of edges as E . We then have constructed the capability model G = (V , E ) for . Figure 1 provides a simple example of a capability model.

it is the result of a plan (i.e., operation) execution. Henceforth, when we refer to plan traces, we always intend to mean partial plan traces, unless otherwise specified. When more than two states are observed in a plan trace, it can be considered as a set of traces, with each pair of contiguous states as a separate trace. When the states are partially observed in a plan trace, it can be considered as a set of compatible traces with complete state observations.3 For simplicity, we assume in the following that the plan execution traces used in learning have complete state observations. We denote this set of traces as D. To learn the parameters of a capability model, a common way is to model Fij using a beta distribution (i.e., as its density function ). Denote the parameters for the beta distribution of Fij as aij and bij . Then, we have: aij (3) P (Xi = true|paij ) = aij + bij Suppose that the initial values or the current values for aij and bij are given. The remaining task is to update aij and bij from the given traces. Given the training set D, we can now follow Bayesian inference to update the parameters of Fij as follows: Initially or currently, (fij ) = beta(fij ; aij , bij ) After observing new training examples D, we have: (fij |D) = beta(fij ; aij + sij , bij + tij ) (5) (4)

3.3

Parameter Learning

In this section, we describe how the model parameters can be learned. The parameter learning of a capability model is performed online through Bayesian learning. The initial model parameters can be computed from existing plan traces by learning the density functions (i.e., fij ) in Definition 3. These parameters can then be updated online as more traces are collected (i.e., as the agent interacting with the environment). Plan execution traces can be collected each time that a plan (or a sequence of actions) is executed, whether succeeds or fails. D EFINITION 4 (C OMPLETE P LAN T RACE ). A complete plan trace is a continuous sequence of state observations over time, de  noted as T = s 1 , s2 , ..., sL , in which L is the length of the plan  and si denotes a complete state (i.e., s i  D (X1 ) ◊ D (X2 ) ◊ ... ◊ D(XN )). However, in real-world situations, plan traces may be incomplete. The incompleteness can come from two aspects. First, the observed state may be partial. Second, the observations may not be continuous. Hence, we are going to have partial plan traces. D EFINITION 5 (PARTIAL P LAN T RACE ). A partial plan trace is a discontinuous sequence of partial state observations over time, denoted as T = si , si+k1 , si+k2 , ... , in which i denotes the time step in the complete plan and si denotes a partial state (i.e., si  D+ (X1 ) ◊ D+ (X2 ) ◊ ... ◊ D+ (XN )). Note that the only assumption that is made in Definition 5 is that at least two different partial states must be observed during the plan execution. This means that even the start and end states of a plan execution do not necessarily have to be observed or partially observed, which is especially useful in real-world situations where a plan trace may only be a few samplings of (partial) observations during a plan execution. Note also that since the observations are in discontinuous time steps, the transition between contiguous state observations is not necessarily the result of a single action. Instead,

in which sij is the number of times for which Xi is true while P Ai assuming the value of paij , and tij is the number in which it equals Xi is false while P Ai assuming the value of paij .

3.4

Implications

In this section, we discuss several implications of capability models with a simple example. We first investigate how information can be lost during learning when the correlations or causal relationships among the variables are only partially captured. This can occur, for example, when model structure is learned. In this example, we have two blocks A, B , and a table. Both blocks can be placed on the table or on each other. The capability model for an agent is specified in Figure 2. Initially, assuming that we do not have any knowledge of the domain, the density functions can be specified as beta distributions with a = b. In Figure 2, we use a = b = 1 for all distributions. Suppose that we observe the following plan trace, which can be the result of executing a single action that places A on B : s1 : OnT able(A)  OnT able(B )  ¨On(A, B )  ¨On(B, A) s2 : ¨OnT able(A)  OnT able(B )  On(A, B )  ¨On(B, A) Based on the learning rules in Equation (5), we can update the beta distributions accordingly. For example, the beta distribution  3 (i.e., On(A, B ) in the eventual state) is updated to beta(X1 = for X  4 = false, ...; 2, 1). This means that if both A true, X4 = true, X and B are on the table, it becomes more likely that a capability exists for making On(A, B ) = true. This is understandable since an action that places A on B would achieve On(A, B ) = true in such cases. For actions with uncertainties (i.e., when using a robotic arm to perform the placing action), this beta distribution would converge to the success rate as more experiences are obtained.  2 is Meanwhile, we also have that the beta distribution for X  4 = false, X 3 = updated to beta(X1 = true, X3 = false, X There is no need to expand such traces into sets of traces with complete state observations for learning, since it can be equivalently considered using arithmetic operations.
3

that satisfies sE P (s) =

s in the resulting belief state as follows:   = s|X = s ) P (X P (s  s) =    = sE |X = s ) P (s  sE ) P (X (6)

For any state s that does not satisfy sE s, we have P (s) = 0. Denote S in b(S ) as S = {s|sE s and s is a complete state}. Clearly, we have: P (s) = 1
sS

(7)

Figure 2: Capability model for an agent in a simple domain with four variables. OnT able(A) means that object A is on the table. On(A, B ) means that object A is on B . The correlations that correspond to the light blue arrows distinguish between two scenarios: one with proper model structure and one without (i.e., when certain correlations are missing). For clarity, we only specify the density functions of the variables in the initial state. We also show the two sets of density functions for the augmented variables for X2 for the two different scenarios, respectively. true, ...; 1, 2). This means that if A is on B in the eventual state, it becomes less likely that B can also be on A in the eventual state. This is intuitive since we know that achieving On(A, B ) = true and On(B, A) = true at the same time is impossible. In this way, capability models can reinforce the correlations between the variables as experiences are observed. The implication is that the edges (capturing the correlations) between these variables must be present; otherwise, information may be lost as described above. If the correlations are not fully captured by the model, for example, when the light blue arrows in Figure 2 are not present, the beta dis 2 would not be updated as above, since X  3 would no tribution of X  2 . A similar implication also applies longer be a parent node of X to causal relationships. When environment changes, previous knowledge that is learned by the model may no longer be applicable. However, as the parameters grow, the learned model can be reluctant to adapt to the new environment. This issue can be alleviated by performing normalizations occasionally (i.e., dividing all a and b by a constant in the beta distributions), or by weighting previous experiences with a discounting factor.

Since there can be an exponential number of complete states in a belief state, depending on how many variables are assigned to u, we can use a sampling approach (e.g., Monte Carlo sampling) to keep a set of complete states to represent b(S ). We denote the belief state after sampling as ^ b (S ). When applying a capability sI  sE to a given belief state ^ b(S ), for each complete state in S , we can perform sampling based on Equation (6), which returns a set of complete states with weights after applying the capability. We can then perform resampling on the computed sets of states for all states in S to compute the new belief state ^ b(S ). In this way, we can connect different capabilities of an agent to create c-plans, which are plans in which capabilities are involved. Next, we formally define a planning problem for a single agent with a capability model. D EFINITION 6 (S INGLE AGENT P LANNING P ROBLEM ). A single agent planning problem with capability models is a tuple , b(I ), G,  , in which b(I ) is the initial belief state, G is the set of goal variables, and  is a real value in (0, 1]. The capability model of the agent is G = (V , E ). When we write  instead of  in the problem, it indicates a variance of the problem that needs to maximize . D EFINITION 7 (S INGLE AGENT C-P LAN ). A single agent cplan for a problem , b(I ), G,  is a sequence of application of capabilities, such that the sum of the weights of the complete states in the belief state which include the goal variables (i.e., G s), is no less than  (or maximized for  ) after the application. The synthesized single agent "plan" (i.e., a c-plan) is incomplete: it does not specify which operation fulfills each capability. In fact, it only informs the user how likely there exists such an operation. A single agent c-plan can be considered to provide likely landmarks for the agent to follow. For example, in Figure 2, suppose that the initial state is sI : On(A, B )  ¨On(B, A) and the goal is sE : ¨On(A, B )  On(B, A) A c-plan created with a capability model may include an intermediate state in the form of sI  sin  sE ,4 in which sin can include, e.g., OnT able(A) = true and OnT able(B ) = true, such that the probability of the existence of a sequence of operations to fulfill the c-plan may be increased. Another example is our motivating example (Figure 1) in which having has_trolley (AG) = true as an intermediate state helps delivery when has_trolley (AG) is false at the beginning. Although a single agent c-plan may seem to be less useful than a complete plan synthesized with action models, it is unavoidable
4 Note that P (sI  sE ) is not equivalent to P (sI  sin ) ∑ P (sin  sE ).

4.

USING CAPABILITY MODELS IN PLANNING

Capability models described in the previous section allow us to capture the capabilities of agents. In this section, we discuss the application of capability models in single agent planning (i.e., with an agent  modeled by a capability model). We extend the discussion to multi-agent planing in the next section. Generally, planning with capability models occurs in the belief space, as with POMDPs [10].

4.1

Single Agent Planning

First, note that applying a capability sI  sE of agent  to a complete state s results in a belief state b(S ) as long as sI s (otherwise, this capability cannot be applied), in which sI s denotes that all the variables that are not assigned to u in sI have the same values as those in s . After applying the capability, assuming successfully, we can compute the probability weight of a state s

when we have to plan with incomplete knowledge (e.g., incomplete action models). This is arguably more common in multi-agent systems, in which the planner needs to reason about likely plans for agents even when it does not have complete information about these agents. A specific application in such cases is when we need to perform task allocation, in which it is useful for the system to specify subtasks or state requirements for agents that are likely to achieve them without understanding how the agents achieve them.

4.2

Planning Heuristic

Given a single agent planning problem , b(I ), G,  , besides achieving the goal variables, planning should also aim to reduce the cost of the c-plan (i.e., probability of success for the sequence of application of capabilities in the c-plan). ASSUMPTION: To create a heuristic, we make the following assumption ≠ capabilities do not have variables with false values in sI . With this assumption, we have the following monotonicity properties hold: P (sI  sE )  P (sI  sE )(T (sI )  T (sI )  F (sI )  F (sI )) (8) in which T , F are used as operators on a set of variables to denote the set of variables with true and false values, respectively. Equation (8) implies it is always easier to achieve the desired state with more true-value variables and less false-value variables in the initial state; and P (sI  sE )  P (sI  sE )(T (sE )  T (sE )F (sE )  F (sE )) (9) which implies that it is always more difficult to achieve the specified values for more variables in the eventual state. HEURISTIC: We use A to perform the planning. At any time the current planning state is a belief state ^ b(S ) (i.e., b(S ) after sampling); there is also a sequence of application of capabilities (i.e., a c-plan prefix), denoted as  , to reach this belief state from the initial state. We compute the heuristic value for ^ b(S ) (i.e., f (^ b(S )) = g (^ b(S )) + h(^ b(S ))) as follows. First, we compute g (^ b(S )) as the sum of the negative logarithms of the associated probabilities of capabilities in  . To compute h(^ b(S )), we need to first compute h(s) for each complete state s  S . To compute an admissible h(s) value, we denote the set of goal variables that are currently false in s as Gs . Then, we compute h(s) as follows: h(s) = argmax - log P (s¨v  s{v = true})
v Gs ,s¨v

Figure 3: Illustration of solving a single agent planning problem with a capability model. The topology of the capability model used is shown in the top part of the figure.

Lemma 1 implies that the A search using the heuristic in Equation (11) would continue to improve  given sufficient time. Hence, the heuristic should be used as an anytime heuristic and stop when a desired value of  is obtained or the increment is below a threshold. Also, approximation solutions should be considered in future work to scale to large networks.

4.3

Evaluation

(10)

in which s¨v denotes a complete state with only v as false, and s{v = true} denotes the state of s after making v true. Finally, we compute h(^ b(S )) as: h(^ b(S )) =
sS

P (s) ∑ h(s)

(11)

L EMMA 1. The heuristic given in Eq. (11) is admissible for finding a c-plan that maximizes  (i.e., with  ), given that ^ b (S ) accurately represents b(S ). P ROOF. We need to prove that h(^ b(S )) is not an over-estimate of the cost for S to reach the goal state G while maximizing . First, note that we can always increase  by trying to move a nongoal state (in the current planning state) to a goal state (i.e., G s). Furthermore, for each s  S , given the monotonicity properties, we know that at least h(s) cost must be incurred to satisfy G. Hence, the conclusion holds.

We provide a preliminary evaluation of single agent planning with a capability model. We build this evaluation based on the blocksworld domain. First, we use 20 problem instances with 3 blocks5 and generate a complete plan for each instance. Then, we randomly remove 1 - 5 actions from each complete plan to simulate partial plan traces. The capability model for this domain contains 12 variables and we ignore the holding and handempty predicates to simulate partial state observations. We manually construct the correlations between the variables in the initial and eventual states. For example, On(A, B ) is connected with On(B, A) since they are clearly conflicting. For causal relationships, we connect every node in the initial state to every node in the eventual state. After learning the parameters of this capability model based on the partial plan traces, we apply the capability model to solve a problem as shown in Figure 3. The topology of the capability model constructed is shown in the top part of the figure, which is also the model used in Figure 4. Since we have connected every fact node with every e-node in this case, the model appears to be quite complex. Initially, we have On(B 3, B 2), On(B 2, B 1), and OnT able(B 1) (a complete state), and the goal is to achieve On(B 2, B 3). We can see in Figure 3 that I in b(I ) contains a single complete state. The c-plan involves the application of two different capabilities. For illustration purpose, we only show two possible states (in the belief state) after applying the first capability, For one of the two states, we then show two possible states after applying the second capability. Note that each arrow represents a sequence of actions in the original action model. Also, the possible states must be compatible with the specifications of the eventual states in the capabilities. We see some interesting capabilities being
5 It does not have to be three blocks but we assume only three blocks to simplify the implementation.

learned. For example, the first capability is to pull out a block that is between two other blocks, and place it somewhere else.

5.

MULTI-AGENT PLANNING

In this section, we extend our discussion from single agent planning to multi-agent planning. In particular, we discuss how capability models can be used along with other complete models (i.e., action models) by a centralized planner to synthesize c-plans. The settings are similar to those in our motivating example. In particular, we refer to multi-agent planning with mixed models as MAPMM. This formulation is useful in applications in which both human and robotic agents are involved. While robotic agents are programmed and hence have complete models, the models of human agents must be learned. Hence, we use capability models to model human agents and assume that the models for the human agents are already learned (e.g., by the robots) for planning. For robotic agents, we assume STRIPS action model R, O , in which R is a set of predicates with typed variables, O is a set of STRIPS operators. Each operator o  O is associated with a set of preconditions P re(o)  R, add effects Add(o)  R and delete effects Del(o)  R. D EFINITION 8. Given a set of robots R = {r}, a set of human agents  = {}, and a set of typed objects O, a multiagent planning problem with mixed models is given by a tuple  = , R, b(I ), G,  , where: ∑ Each r  R is associated with a set of actions A(r) that are instantiated from O and O, which r  R can perform; each action may not always succeed when executed and hence is associated with a cost. ∑ Each    is associated with a capability model G =   . X  X , in which X V , E , in which V = X  X represents the state variables of the world and agent  and X represents the joint set of state variables of all agents. Note that the grounded predicates (which are variables) for robots that are instantiated from R and O also belong to X . Human and robotic agents interact through the shared variables. L EMMA 2. The MAP-MM problem is at least PSPACE-complete. P ROOF. We only need to prove the result for one of the extreme cases: when there are only robotic agents. The problem then essentially becomes a multi-agent planning problem, which is more general than the classical planning problem, which is known to be PSPACE-complete.

Figure 4: Illustration of solving a MAP-MM problem with a robotic and a human agent, in which the robotic agent has an action model and the human agent has a capability model that is assumed to be learned by the robotic agent. If we have chosen an action a, for any s  S that satisfies T (P re(a))  T (s), the complete state s after applying a becomes s , such that T (s ) = (T (s)  Add(a)) \ Del(a), and F (s ) = (F (s)  Del(a)) \ Add(a). The probability weight of s in the new belief state after applying a does not change. For states in S that do not satisfy T (P re(a))  T (s), we assume that the application of a does not change anything. In this way, we can construct the new belief state after applying this action. If we have chosen a capability in the form of sI  sE from a human agent , for any s  S that satisfies T (sI )  T (s), we can use follow discussion in Section 4.1 to compute the new belief state. Similarly, for states in S that do not satisfy T (sI )  T (s), we assume that the application of this capability does not change anything. With the new belief state, we can continue the state expansion process to expand the c-plan further.

5.2

Planning Heuristic for MAP-MM

5.1

State Expansion for MAP-MM

First, we make the same assumption as we made in single agent planning, such that the monotonicity properties still hold. Given the current planning state ^ b(S ) (i.e., sampled from b(S )), for each s  S , a centralized planner can expand it in the next step using the following two options in MAP-MM: ∑ Choose a robot r and an action a  A(r) such that at least one of the complete states s in S satisfies T (P re(a))  T (s). ∑ Choose a capability on human agent  with the specification sI  sE , such that at least one of the states s in S satisfies T (sI )  T (s).

In this section, we discuss a planning heuristic that informs us which state should be chosen to expand at any time. We can adapt the heuristic in Equation (11) to address MAP-MM. Given the current planning state ^ b(S ), we need to compute h(^ b(S )). For each s  S , there are three cases: 1) If only capabilities are used afterwards, h(s) can be computed as in Equation (11), except that all capability models must be considered. 2) If only actions are going to be used, h(s) can be computed based on the relaxed plan heuristic (i.e., ignoring all deleting effects), while considering all robot actions. 3) If both capabilities and actions can be used, h(s) can be computed as the minimum cost of an action that achieves any variable in Gs . The final h(s) b(S )) is chosen as the smallest value among the three cases and h(^ can subsequently be computed. C OROLLARY 1. The heuristic above is admissible for finding a c-plan for MAP-MM that maxmizes , given that ^ b(S ) accurately represents b(S ).

5.3

Evaluation

In this section, we describe a simple application of MAP-MM involving a human and a robot, in which the capability model of the human is assumed to be learned by the robot. The robot then makes a multi-agent c-plan for both the human and itself. The setup of this evaluation is identical to that in the evaluation of single agent planning, as we discussed in the previous section.

In this example, we associate the robot actions with a constant cost. After learning the parameters of the human capability model based on the generated partial plan traces, we apply the capability model to solve a problem as shown in Figure 4. Initially, we have On(B 1, B 3), OnT able(B 3), and OnT able(B 2) (a complete state), and the goal is to achieve On(B 3, B 2). The multi-agent c-plan involves the application of two robot actions and one capability of the human. We show three possible complete states (in the belief state) which satisfy the goal variable after applying the capability.

7.

CONCLUSIONS AND FUTURE WORK

6.

RELATED WORK

Most existing approaches for representing the dynamics of agents assume that the models are completely specified. This holds whether the underlying models are based on STRIPS actions (e.g. PDDL [7]) or stochastic action models (such as RDDL [19]). This assumption of complete knowledge is also the default in the existing multi-agent planning systems [3]. Capability models, in contrast, start with the default of incomplete models. They are thus related to the work on planning with incompletely specified actions (c.f. [15, 16, 11]). An important difference is that while this line of work models only incompleteness in the precondition/effect descriptions of the individual actions, capabilities are incomplete in that they completely abstract over actual plans that realize them. In this sense, a capability has some similarities to non-primitive tasks in HTN planning [6, 27]. For example, an abstract HTN plan with a single non-primitive task only posits that there exists some concrete realization of the nonprimitive task which will achieve the goal supported by the nonprimitive task. However, in practice, all HTN planners use "complete models" in that they provide all the reduction schemas to take the non-primitive task to its concretization. So, the "uncertainty" here is "angelic" [12] ≠ the planner can resolve it by the choice of reduction schemas. In contrast, capability models do not have to (and cannot) provide any specifics about the exact plan with which a capability will be realized. Capability models also have connections to macro operators [2], as well as options, their MDP counterparts [21, 20, 9], and the BDI models [17]. Capability models are useful when plans must be made with partial knowledge. With complete models, this means that not all actions or macro-actions or options or capabilities in BDI to achieve the goal are provided. None of HTN, SMDP or BDI models can handle the question of what it means to plan when faced with such model incompleteness. Capability models in contrast propose approximate plans (referred to as c-plans) as a useful solution concept in informing the user of how likely there is an actual complete plan. On the other hand, due to the inherent incompleteness of capability models, they are lossy in the following sense. It is possible to compile a complete model to a capability model (e.g., converting actions in RDDL [19] to a capability model), but new capabilities may also be introduced along with the actions. As a result, the synthesized plans would still be incomplete unless the use of these new capabilities are forcibly restricted. The learning of capability models has connections to learning probabilistic relational models using Bayesian networks [8]. The notion of eventual state captured in capability models is similar to that captured by the F operator (i.e., eventually) in LTL and CTL [5, 23]. Although there are other works that discuss about capability models, e.g., [4], they are still based on action modeling.

In this paper, we have introduced a new representation to model agents based on capabilities. The associated model, called a capability model, is an inherently incomplete model that has several unique advantages compared to traditional complete models (i.e., action models). The underlying structure of a capability model is a generalized 2-TBN, which can encode all the capabilities of an agent. The associated probabilities computed (i.e., via Bayesian inference on the capability model) based on the specifications of capabilities (i.e., a partial initial state coupled with a partial eventual state) determine how likely the capabilities can be fulfilled by an operation (i.e., a complete plan). This information can be used to synthesize incomplete "plans" (referred to as c-plans). One of the unique advantages of capability models is that learning for both model structure and parameters is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). Furthermore, we show that parameter learning for capability models can be performed efficiently online via Bayesian learning. Additionally, we provide the details of using capability models in planning. Compared to traditional models for planning, in a planning state, capability models only suggest transitions between partial states (i.e., specified by the capabilities), along with the probabilities that specify how likely such transitions can be fulfilled by an operation. The limitation is that the synthesized c-plans are incomplete. However, we realize that this is unavoidable for planning with incomplete knowledge (i.e., incomplete models). In such cases, the synthesized c-plans can inform the user how likely complete plans exist when following the "guidelines" of the c-plans. In general, a c-plan with a higher probability of success should imply that a complete plan is more likely to exist. We discuss using capability models for single agent planning first, and then extend it to multi-agent planning (with each agent modeled separately by a capability model), in which the capability models of agents are used by a centralized planner. We also discuss how capability models can be mixed with complete models. In future work, we plan to further investigate the relationships between capability models and traditional models. We also plan to explore applications of capability models in our ongoing work on human-robot teaming [22, 13]. For example, we plan to investigate how to enable robots to learn capability models of humans and plan to coordinate with the consideration of these models.

Acknowledgments
This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027.

REFERENCES
[1] C. Backstrom and B. Nebel. Complexity results for sas+ planning. Computational Intelligence, 11:625≠655, 1996. [2] A. Botea, M. Enzenberger, M. M¸ller, and J. Schaeffer. Macro-ff: Improving ai planning with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24:581≠621, 2005. [3] R. I. Brafman and C. Domshlak. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In ICAPS, pages 28≠35. AAAI Press, 2008.

[4] J. Buehler and M. Pagnucco. A framework for task planning in heterogeneous multi robot systems based on robot capabilities. In AAAI Conference on Artificial Intelligence, 2014. [5] E. Clarke and E. Emerson. Design and synthesis of synchronization skeletons using branching time temporal logic. In D. Kozen, editor, Logics of Programs, volume 131 of Lecture Notes in Computer Science, pages 52≠71. Springer Berlin Heidelberg, 1982. [6] K. Erol, J. Hendler, and D. S. Nau. Htn planning: Complexity and expressivity. In In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 1123≠1128. AAAI Press, 1994. [7] M. Fox and D. Long. PDDL2.1: An extension to pddl for expressing temporal planning domains. Journal of Artificial Intelligence Research, 20:2003, 2003. [8] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models. In In IJCAI, pages 1300≠1309. Springer-Verlag, 1999. [9] P. J. Gmytrasiewicz and P. Doshi. Interactive pomdps: Properties and preliminary results. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS '04, pages 1374≠1375, Washington, DC, USA, 2004. IEEE Computer Society. [10] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. J. Artif. Int. Res., 4(1):237≠285, May 1996. [11] S. Kambhampati. Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models, 2007. [12] B. Marthi, S. J. Russell, and J. Wolfe. Angelic semantics for high-level actions. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS), 2007. [13] V. Narayanan, Y. Zhang, N. Mendoza, and S. Kambhampati. Automated planning for peer-to-peer teaming and its evaluation in remote human-robot interaction. In ACM/IEEE International Conference on Human Robot Interaction (HRI), 2015. [14] R. E. Neapolitan. Learning Bayesian networks. Prentice Hall, 2004. [15] T. Nguyen and S. Kambhampati. A heuristic approach to planning with incomplete strips action models. In International Conference on Automated Planning and Scheduling, 2014.

[16] T. A. Nguyen, S. Kambhampati, and M. Do. Synthesizing robust plans under incomplete domain models. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2472≠2480, 2013. [17] L. Padgham and P. Lambrix. Formalisations of capabilities for bdi-agents. Autonomous Agents and Multi-Agent Systems, 10(3):249≠271, May 2005. [18] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. [19] S. Sanner. Relational dynamic influence diagram language (rddl): Language description, 2011. [20] S. Seuken and S. Zilberstein. Memory-bounded dynamic programming for dec-pomdps. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07, pages 2009≠2015, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. [21] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181≠211, Aug. 1999. [22] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and S. Kambhampati. Coordination in human-robot teams using mental modeling and plan recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2957≠2962, Sept 2014. [23] M. Vardi. An automata-theoretic approach to linear temporal logic. In F. Moller and G. Birtwistle, editors, Logics for Concurrency, volume 1043 of Lecture Notes in Computer Science, pages 238≠266. Springer Berlin Heidelberg, 1996. [24] B. Y. White and J. R. Frederiksen. Causal model progressions as a foundation for intelligent learning environments. Artificial Intelligence, 42(1):99 ≠ 157, 1990. [25] C. Yuan and B. Malone. Learning optimal bayesian networks: A shortest path perspective. J. Artif. Int. Res., 48(1):23≠65, Oct. 2013. [26] H. H. Zhuo and S. Kambhampati. Action-model acquisition from noisy plan traces. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI'13, pages 2444≠2450. AAAI Press, 2013. [27] H. H. Zhuo, H. MuÒoz Avila, and Q. Yang. Learning hierarchical task network domains from partially observed plan traces. Artificial Intelligence, 212:134≠157, July 2014.

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Tweeting the Mind and Instagramming the Heart:
Exploring Differentiated Content Sharing on Social Media
Lydia Manikonda

Venkata Vamsikrishna Meduri

Subbarao Kambhampati

arXiv:1603.02718v1 [cs.SI] 8 Mar 2016

Department of Computer Science, Arizona State University, Tempe AZ 85281
{lmanikon, vmeduri, rao}@asu.edu

Abstract
Understanding the usage of multiple OSNs (Online Social
Networks) has been of significant research interest as it helps
in identifying the unique and distinguishing trait in each social media platform that contributes to its continued existence. The comparison between the OSNs is insightful when
it is done based on the representative majority of the users
holding active accounts on all the platforms. In this research,
we collected a set of user profiles holding accounts on both
Twitter and Instagram, these platforms being of prominence
among a majority of users. An extensive textual and visual
analysis on the media content posted by these users revealed
that both these platforms are indeed perceived differently at a
fundamental level with Instagram engaging more of the users‚Äô
heart and Twitter capturing more of their mind. These differences got reflected in almost every microscopic analysis done
upon the linguistic, topical and visual aspects.

1

Introduction

Online Social Networks (OSNs) are gaining attention for
being rich sources of information about individuals including many aspects of their daily life through the way they
connect, communicate and share information. Over the past
few years, given their ubiquity and accessibility, social media platforms like Twitter and Instagram have emerged as
very popular microblogging services for web users to communicate with each other through text and photos. In 2015,
when Instagram broke the record of having more than 400
million active monthly users, Twitter was projected as its
main rival. In fact according to a recent article by Pew Research,1 28% of American adults use Instagram and 23%
use Twitter. More interestingly, many users have active accounts on both these sites (or platforms) (Lim et al. 2015;
Chen et al. 2014). While research has recognized immense
practical value in understanding the user behavioral characteristics on these platforms separately, there is no existing research that has examined how the content posted by
individuals differs across these two platforms. Instagram is
a photo-sharing application whereas Twitter emerged as a
text-based application which currently lets users post both
text and multimedia data. Of particular interest is the question of why and how individuals use these two sites when
both of them are similar in their current functionalities.
1
http://www.pewinternet.org/2015/08/19/the-demographics-ofsocial-media-users/

In this research, we aim to answer the aforementioned
questions by analyzing content from the same set of individuals across these two popular platforms and quantifying
their posting patterns (we focus on ordinary users who are
common users but not celebrities or popular users or organizations). By leveraging NLP and Computer Vision techniques, we present some of the first qualitative insights about
the types of trending topics, and social engagement of the
user posts across these two platforms. Analysis on the visual
and linguistic cues indicates the dominance of personal and
social aspects on Instagram and news, opinions and workrelated aspects on Twitter. Despite considering the same set
of users on both platforms, we see remarkably different categories of visual content ‚Äì predominantly eight categories on
Instagram and four categories of images on Twitter. These
results suggest that Instagram is largely a sphere of positive
personal and social information where as Twitter is primarily
a news sharing media with higher negative emotions shared
by users.
Background: Twitter has been explored extensively with
respect to the content (Honey and Herring 2009), language (Hong, Convertino, and Chi 2011), etc and it is established that it is primarily a news medium (Kwak et
al. 2010). Research on Instagram has focused mostly on
understanding the user behavior through analyzing color
palettes (Hochman and Schwartz 2012), categories (Hu,
Manikonda, and Kambhampati 2014), filters (Bakhshi et al.
2015), etc. On the other hand, it has been of significant
interest to the researchers to investigate the behavior of a
user (Benevenuto et al. 2009), connect users (Zafarani and
Liu 2013), study how users reveal their personal information (Chen et al. 2012), etc all across multiple OSNs. We
extend the current state of the art by examining the nature of
a given user‚Äôs behavior manifested across Twitter and Instagram. Close to our work is the work of Bang et al. (Lim et al.
2015) where six OSNs were studied to analyze the temporal
and topical signature (only w.r.t user‚Äôs profession) of user‚Äôs
sharing behavior but they did not focus on studying the comparative linguistic aspects and visual cues across the platforms. Here we employ both textual and visual techniques
to conduct a deeper analysis of content on both Twitter and
Instagram.
Dataset: In order to investigate and characterize a given
user‚Äôs behavior across multiple sites, we use a personal web
hosting service called About.me (http://about.me/) that enables individuals to create an online identity by letting them

Twitter
stories, international, food, web, naÃÉo,
angelo, jaÃÅ
time, people, love, work, world, social,
life

ID
0

happy, love, home, birthday, weekend,
beautiful, park
maÃÅs, dƒ±ÃÅa, vƒ±ÃÅa, gracias, mi, si, las

2

#football, #sports, #news, #art, facebook, google, iphone

4

1

3

Instagram
#food, delicious, coffee, sunset, beautiful, happy, #wedding
#streetart, #brightongraffiti, #belize,
#sussex,
#hipstamatic,
#urbanart,
#lawton
#fashion, #hair, #makeup, #health,
#workout, #vegan, #fit
#instagood,
#photooftheday,
#menswear, #style, #travel, #beach,
#summer
birthday, beautiful, love, christmas,
friends, fun, home

Table 1: Words corresponding to the 5 latent topics from Twitter
and Instagram

provide a brief biography, connections to other individuals
and their personal websites. Using its API, we performed
the data collection of 10,000 users and pruned the individuals who do not have profiles on both Instagram and Twitter.
The final crawl includes 1,035,840 posts from Twitter (using
the Twitter API https://dev.twitter.com/overview/
api) and 327,507 posts from Instagram (using the Instagram
API https://www.instagram.com/developer/) for the
same set of users. Each post in this dataset is public and
the data include user profiles along with their followers and
friends list, tweets (insta posts), meta data for tweets that include favorites (likes), retweets (Instagram has no explicit
reshares; so we use comments in lieu of the attention the
post receives), geo-location tagged, date posted, media content attached and hashtags.

2
2.1

Text Analysis

Latent Topic Analysis

In order to explain the types of content posted by a user
across Twitter and Instagram, we first mine the latent topics from the corpus of Twitter (aggregated posts on Twitter of all users) and corpus of Instagram (aggregated posts
on Instagram of all users where we use captions associated
with posts for this analysis). We use TwitterLDA (https://
github.com/minghui/Twitter-LDA) developed for topic
modeling of short text corpora to mine the latent topics. With
the user accounts obtained from About.me, the topic inference is meaningful as it is pertinent to the bi-platform posts
from users who use both the social media venues.
The topic vocabulary listed for both the platforms in Table 1 indicates the unique topics for each site as well as
the overlapping topics. For instance topics 0 and 4 on Instagram are similar to the topics 1 and 2 on Twitter. However,
a significant difference is that Instagram is predominantly
used to post about art, food, fitness, fashion, travel, friends
and family but Twitter hosts a significantly higher percentage of posts on sports, news and business as compared to
other topics. Another notable difference is that the vocabulary from non-English language posts like French and Spanish is higher on Twitter as compared to the captions on Instagram mostly using English as the language medium. The
topic distributions obtained from the two corpora are listed
in Figure 1 which show that friends and food are the most
frequently posted topics on Instagram as against sports and
news followed by work and social life being popular on
Twitter.
To further validate the observations made about the distinctive topical content across the two platforms, we compared the topic distributions for each individual on the two
platforms by estimating the KL-Divergence (entropy) for

Figure 1: Topic distributions of all the user posts on Twitter and
Instagram

Figure 2: Sorted entropies between the topic distributions of the
user posts on Twitter and Instagram

each user. However for this entropy computation to be possible, a unified topic model needed to be built on the combined corpus of tweets and captions of Instagram posts. The
unified topics are listed in the description of Figure 3. The
resultant entropy plot in Figure 2 follows a power law distribution showing that most users post on Twitter and Instagram equally differently barring a few (where the estimated
p-value < 10‚àí15 for each user).

2.2

Social Engagement

Since our findings revealed that the bi-platform topics are
significantly different and so we wanted to investigate how
these posts made by the same user engage other individuals on the two sites. We define the social engagement as the
attention received by a user‚Äôs post on the social media platform and can be quantified in various ways ranging from the
sum of likes and comments on Instagram and the sum of favorites and reshares on Twitter. For each topic in the unified
topic model for both Twitter and Instagram, the logarithmic
frequency of posts is plotted against the magnitude of social
engagement that is binned to discrete ranges in Figure 3.
An interesting observation is that the socially engaging
topics in the combined model are same as the overlapping
topics from the topic models built in isolation on the Twitter
and Instagram posts (Figure 1 in Section 2.1). The dominating topic on Twitter is about sports, news and business but
the overlapping influential topic is about social and personal
life comprising friends and family. Surprisingly, we found
that the overlapping topics (Topics 2 and 3) fetched predominant social engagement on both Twitter and Instagram.
A notable difference between the platforms with respect
to social engagement is that the magnitude of attention received for Instagram posts is significantly higher than the
level of attention received on Twitter as we can notice from
the ranges plotted on the x-axes in Figure 3. This observation
is consistent regardless of the activity of the user. Even when
a user is more active (Figure 4) on Twitter than Instagram,
the observation of higher social engagement on Instagram
on an absolute scale holds. A possible explanation to this is

Platform
Twitter

Instagram

0.60
0.19

0.49
0.19

0.15
0.14
0.05
0.17

0.30
0.21
0.1
0.21

0.81
0.6
0.08
0.07
0.16

0.5
0.93
0.06
0.04
0.2

Emotionality
Negemo
Posemo
Social Relationships
home
family
friend
humans

(a) Twitter

Individual Differences
work
bio
swear
death
gender

(b) Instagram
Figure 3: Social Engagement Vs Post Frequency where the topics are ‚Äì Topic 0:{people, life, world, social, app, game, business}, Topic 1:{stories, artists, #lastfm, level, #football, #sports,
news}, Topic 2:{birthday, beautiful, work, weekend, park, dinner,
christmas}, Topic 3:{ yang, run, #fitness, #runkeeper, #art, sale,
#menswear}, Topic 4:{#instagood, #photooftheday, #love, maÃÅs,
#fashion, #travel, #food}

Figure 4: Distributions of Followers/Followings vs Media

that the users on Twitter use it as a news source to read informative tweets but not necessarily all of the content that is
read will be ‚Äúliked‚Äù.
On average, there are 30% more number of hashtags for a
Twitter post compared to an Instagram post (Pearson correlation coefficient = 0.34 between distributions with p-value
< 10‚àí15 ). This may also indicate that on Instagram since the
main content is image, textual caption may not receive as
much attention from the user.

2.3

Linguistic Nature

To characterize and compare the type of language used on
both platforms, we use the psycholinguistic lexicon LIWC
((http://liwc.wpengine.com/)) on the text associated
with a Twitter post and an Instagram post. We obtain measures of attributes related to user behavior ‚Äì emotionality
(how people are reacting to different events), social relationships (friends, family, other humans) and individual differences (attributes like bio, gender, age, etc).
It is clear from Table 2 that posts on Twitter are more negatively emotional and contain more work-related and swear
words where as positive social patterns are more evident
on Instagram. By relating these results to the topic analysis results in the previous section, we identify that on Instagram users share less negative content and more lighthearted happy personal updates. To further support these
claims from the textual data, n-gram analysis indicates that

Table 2: Linguistic attributes across Twitter vs Instagram. Each
value indicates the fraction of a post belonging to the corresponding
attribute

users on Instagram focus on things that give them pleasure such as, fashion or travel (top bi/Trigrams like last
night, Good morning, right now, fashion design streetwear),
whereas on Twitter they mainly share check-in feeds from
their apps or news (top bi/Trigrams like Stories via, Just
posted, @YouTube video, Just posted photo).

3

Visual Analysis

Extensive studies have been conducted on the textual and
visual content on these two platforms in isolation but to the
best of our knowledge, a comparative content analysis has
never been conducted. Considering this as a main objective,
this section develops a better understanding on the types of
photos individuals post on Twitter in comparison with their
Instagram posts. To achieve this we employ Computer Vision techniques mainly in terms of ‚Äì visual categories (kinds
of photos) and visual features (color palettes).

3.1

Visual Categories

We first sampled two sets of 5K images from both platforms
separately and using the OpenCV library (http://opencv.
org/) on these two datasets, we extracted Speeded Up Robust Features (SURF) for each image. We used the vector
quantization approach on these features that eventually converted each image into a codebook format. Using the codebook, we clustered images using k-means algorithm (best
value of k is found by SSE (Sum of Squared Error)). We
employed the same approach on both datasets separately. To
our surprise the clusters we obtained on Instagram were very
refined compared to the coarse nature of clusters on Twitter
dataset. After computing these clusters, two researchers separately identified the overall themes of these two datasets
and agreed upon the final visual categories of the photos
from these platforms.
Visual categories on Instagram agree with our previous
work (Hu, Manikonda, and Kambhampati 2014) which detected eight different categories of images. We tried to categorize the Twitter images into the same format as Instagram
images and there are four prominent cluster categories on
Twitter. Figure 7 shows that the percentage of photos in the
activity category outnumbered any other category followed
by captioned photos. To better understand the kinds of activities and captions shown in these two sections, we sampled
around 200 images and asked the two researchers to label

(a)
(b)
(c)
(d)
Figure 5: Subcategories of activity: a) TV shows, b) Running, c)
Conferences, d) Live shows
Figure 7: Photo categories on Twitter vs Instagram
(a)
(b)
(c)
Figure 6: Subcategories of captioned photos: a) Snapshots, b)
Memes, c) Quotes

them manually into different sub-categories. Figure 5 indicates the most popular sub-categories in the activity category
‚Äì news, events (football games, concerts, conferences) and
races and Figure 6 indicates that majority of the captioned
photos are snapshots, memes, and quotes or opinions. These
categories suggest that the topics of photos on Twitter are
mainly related to news, opinions or other general user interests where as on Instagram they mainly share their joyful
and happy moments of their personal lives.

3.2

Visual Features

Existing literature (Bakhshi et al. 2015) shows that the images with a single dominant color gain more popularity
on Instagram. To verify this we compared the visual luminance of all images on these two platforms. We extracted
the grayscale histograms (range from 0 to 259) by utilizing
the OpenCV library as they capture the information about
the brightness, saturation and contrast distribution. The images with darker pixels were binned into the low intensity
value bins close to 0 and images with brighter pixels were
binned into the high intensity values close to 259. Later we
clustered the images in our dataset by employing k-means
algorithm with the grayscale histograms as features for each
image on both these platforms following which 4 types of
clusters were detected based on their color distributions. We
measured the image distribution across each of these 4 categories for both the platforms. We noticed that the images
on Instagram containing darker and brighter pixels are negligible when compared to Twitter as shown in Figure 8. This
suggest that Twitter posts may be less socially engaging than
Instagram owing to a huge presence of captioned photos on
Twitter.

4

Conclusions

In this paper, we presented a detailed comparison of the textual and visual analysis of the content posted by the same set
of users on both Twitter and Instagram. Some of the insights
obtained from linguistic analysis reveal the fundamental differences in the thinking style and emotionality of the users
on these two platforms and how the posts receive varying degrees of attention as per the underlying topics. Interestingly,
user posts on Instagram seem to receive significantly more
attention than Twitter. The visual analyses with respect to
categories and color palettes indicate that the pictures posted
on Instagram contains more selfies and photos with friends
where as Twitter contains more about user opinions in the
form of captioned photos ‚Äì memes, quotes, etc. We observed
that the differences are deeply rooted in the very intention

Figure 8: Example images corresponding to the four major color
categories obtained by extracting color histograms of images associated with the Twitter and Instagram posts.

with which users post on these platforms with Twitter being
a venue for serious posts about news, opinions and business
life where as Instagram acting as the host for light-hearted
personal moments and posts on leisure activities.

References
[Bakhshi et al. 2015] Bakhshi, S.; Shamma, D.; Kennedy, L.; and
Gilbert, E. 2015. Why we filter our photos and how it impacts
engagement. In Proc. ICWSM.
[Benevenuto et al. 2009] Benevenuto, F.; Rodrigues, T.; Cha, M.;
and Almeida, V. 2009. Characterizing user behavior in online
social networks. In Proc. IMC.
[Chen et al. 2012] Chen, T.; Kaafar, M. A.; Friedman, A.; and
Boreli, R. 2012. Is more always merrier?: A deep dive into online social footprints. In Proc. WOSN.
[Chen et al. 2014] Chen, Y.; Zhuang, C.; Cao, Q.; and Hui, P. 2014.
Understanding cross-site linking in online social networks. In Proc.
SNAKDD.
[Hochman and Schwartz 2012] Hochman, N., and Schwartz, R.
2012. Visualizing instagram: Tracing cultural visual rhythms. In
Proc. ICWSM.
[Honey and Herring 2009] Honey, C., and Herring, S. 2009. Beyond microblogging: Conversation and collaboration via twitter. In
Proc. HICSS, 1‚Äì10.
[Hong, Convertino, and Chi 2011] Hong, L.; Convertino, G.; and
Chi, E. 2011. Language matters in twitter: A large scale study.
In Proc. ICWSM.
[Hu, Manikonda, and Kambhampati 2014] Hu, Y.; Manikonda, L.;
and Kambhampati, S. 2014. What we instagram: A first analysis
of instagram photo content and user types. In Proc. ICWSM.
[Kwak et al. 2010] Kwak, H.; Lee, C.; Park, H.; and Moon, S. 2010.
What is twitter, a social network or a news media? In Proc. WWW,
591‚Äì600.
[Lim et al. 2015] Lim, B. H.; Lu, D.; Chen, T.; and Kan, M.-Y.
2015. #mytweet via instagram: Exploring user behaviour across
multiple social networks. In Proc. ASONAM.
[Zafarani and Liu 2013] Zafarani, R., and Liu, H. 2013. Connecting
users across social media sites: A behavioral-modeling approach.
In Proc. KDD.

Venting Weight: Analyzing the Discourse of an Online Weight Loss Forum
Lydia Manikonda1 , Heather Pon-Barry2 , Subbarao Kambhampati1 , Eric Hekler3
David W. McDonald4
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University
2
Mount Holyoke College
3
School of Nutrition and Health Promotion, Arizona State University
4
Human Centered Design & Engineering, University of Washington
lmanikon@asu.edu, ponbarry@mtholyoke.edu, {rao, ehekler}@asu.edu, dwmc@uw.edu
1

Abstract
Online social communities are becoming increasingly popular platforms for people to share information, seek emotional support, and maintain accountability for losing weight.
Studying the discourse in these communities can offer insights on how users benefit from using these applications.
This paper presents an analysis of language and discourse
patterns in forum posts by users who lose weight and keep it
off versus users with fluctuating weight dynamics. In contrast
to prior studies, we have access to the weekly self-reported
check-in weights of users along with their forum posts. This
paper also presents a study on how goal-oriented forums are
different from general online forums in terms of language
markers. Our results reveal differences about how the types
of posts made by users vary along with their weight-loss
patterns. These insights are closely related to the power dynamics of social interactions and can enable better design of
weight-loss applications thereby contributing to a healthy society.

1

Introduction

Obesity is a major public health problem; the number of
people suffering from obesity has risen globally in the last
decade (Ogden et al. 2014). The Centers for Disease Control and prevention (CDCP) defined an obese adult (http:
//www.cdc.gov/obesity/adult/defining.html) as a
person with a body mass index (BMI) of 30 or higher. Many
obese people are trying to lose weight as diseases such as
metabolic syndromes, respiratory problems, coronary heart
disease, and psychological challenges are closely associated with obesity (Must et al. 1999; Ngu 2012). Researchers
have been trying to understand how certain factors are affecting the weight loss as large number of over-weight people are trying to lose weight and some others are trying to
avoid gaining weight. Internet services are gaining popularity to support weight loss as they provide users with the
opportunities to seek information by asking questions, answering questions, sharing their experiences and providing
emotional support where people feel more comfortable by
openly expressing their problems and concerns (Ballantine
and Stephenson 2011).
Copyright ¬© 2016, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Social media tools like weblogs, instant messaging platforms, video chat, social networks, online discussion forums, are reengineering the healthcare sector (Hawn 2009).
Especially, social media is a promising tool for studying
public health like tracking flu infections (Lamb, Paul, and
Dredze 2013), studying post-partum depression (De Choudhury, Counts, and Horvitz 2013), dental pain (Heaivilin et al.
2011), etc. Tools like online discussion forums make it easier to find health-related information while at the same time
provides support by maintaining accountability and some
of the popular works like (Black, But, and Russell 2010)
proved that weight loss can be supported through online interactions. Hence, studying the online discussion forums can
help identify the people at risk who need more support and
provide them access to appropriate services and support.
In this paper, we explore the weight loss patterns of users
who participate in online discussions and ground truth in
terms of the weekly check-in weights of users. We perform different analyses on the users‚Äô language in correlation to their weight loss dynamics. From the overall dataset
we identify two preliminary patterns of weight dynamics:
(1) users who lose weight and successfully maintain the
weight loss (i.e., from one week to the next, weight is lost
or weight remains the same) and (2) users whose weight
pattern fluctuates (i.e., from one week to the next, weight
changes are erratic). While there are many possible groupings that we could have utilized, we chose this grouping
because of the known problems with ‚Äúyo-yo‚Äù dieting (diet
that leads to cyclical loss and gain of weight) compared
to a more steady weight-loss (Brownell and Rodin 1994;
Hekler et al. 2014). Our work is novel in terms of automating
the language analysis by handling a bigger dataset and can
help classify the user type based on the language efficiently.
As a follow-up work, linguistic insights are explored which
distinguish goal-oriented forums from general forums.
Our research contributions in this paper are divided into
two main sections where each focuses on a broader perspective as described below:
1. How does the language of users vary within the weight
loss forum based on their patterns of weight loss. Specifically, to understand the patterns of asking questions, using
a specific sentiment, politeness and making excuses.
2. Are there any interesting insights about the linguistic sig-

nals that makes a goal-oriented forum such as a weightloss forum different from other general online forums.
Our analysis resulted in interesting insights as below:
1. users who lose weight in a fluctuating manner are more
active on the discussion forums.
2. users losing weight in a fluctuating manner appear to talk
about themselves as they use higher number of personal
pronouns and adverbs.
3. users of non-increasing weight loss pattern mostly reply
to the posts made by other users and fluctuating users post
more questions.
4. posts from users of fluctuating weight loss pattern contain
more excuses.
5. politeness of posts seems to be uncorrelated with the
weightloss pattern.
6. users on goal-oriented forums contribute to a cohesive
thread of posts compared to users on general online forums which suffer from non-cohesiveness.
We believe that this research can bring forth the different variables related to people who need additional support
in terms of losing weight and thereby can stay healthy in
maintaining their weight. Also, we envision building personalized weight loss applications that can cater the needs of
individuals who need additional support. We hope that this
study will help in bringing more attention from the research
community to study online weight loss communities and understand both the constructive and destructive dimensions of
weight loss so that we can build a healthy society.

2

Related Work

There is a vast amount of literature about online forums ‚Äì statistical and language analysis of the discussion
threads (GoÃÅmez, Kaltenbrunner, and LoÃÅpez 2008), summarizing the discussions (Backstrom et al. 2013), measuring the success and identifying the factors that make users
participate (Kim 2000; Ludford et al. 2004) on these forums, addressing how the roles of users change (Yang et
al. 2010), understanding the lurking behavior and predicting lurkers (Preece, Nonnecke, and Andrews 2004), etc. Different fields like marketing (Bickart and Schindler 2001),
public health (Black, But, and Russell 2010), etc., considered online forums as influential sources of user information. Much of this literature focuses on studying the online
communities and their users from different linguistic and social networking perspectives. Little attention has been given
to analyze forums from the weight loss perspective.
However, most of the existing studies (Ballantine and
Stephenson 2011; Leahey et al. 2012; Das and Faxvaag
2014) on online weight loss discussion forums focused on
why people participate and how the social support can help
them to lose weight. These studies are conducted from the
perspective of medical and psychological domains, where
the data are collected via interviews or a small set of online forum data that are manually analyzed by human experts. Unlike the existing literature, our work considers the
weekly check-in weights of users along with their posts to

Figure 1: Example weight loss patterns from two individual users:
non-increasing (bottom line), and fluctuating (top line). The x-axis
ranges from the 1st through the 80th weekly check-in; the y-axis
shows the weight, measured in lbs.

understand the behavior of users who want to lose weight
and detect the variables that classify users who need additional support and service. Instead of choosing a small subset of a dataset and performing manual coding, our work
is novel in automating the language analysis by handling a
bigger dataset. Identifying and providing better assistance to
users who need help can also have a significant impact on
gaining the trust and confidence of users in these kinds of
services through better decision making.

3

Dataset

We obtained an anonymized text corpus of online discussion
forums from Fit Now, Inc. who developed a popular mobile
and web-based weight loss application. Along with the text
corpus, we also obtain weekly weight check-in data for a
subset of users. The entire corpus consists of eight different
forums that are subdivided into conversation topic threads.
Each thread consists of several posts made by different users.
The forum data in our corpus consists of 884 threads, with a
median length of 20 posts per thread. The posts were made
between January 1, 2010 and July 1, 2012. We identify the
subset of users for whom we have weight check-in data and
who made at least 25 weight check-ins during this time period. This results in a total of 2,270 users.
We partition the users into two groups based on their dynamic weight loss patterns: a non-increasing group and a
fluctuating group.
1. Non-increasing: These are the users who lose weight and
keep it off. For each week j, the user‚Äôs check-in weight
w j is less than or equal to their past week‚Äôs weight w j‚àí1 ,
within a small margin ‚àÜ. That is, w j ‚â§ (1 + ‚àÜ)w j‚àí1 .
2. Fluctuating: These are the users who do not lose weight.
If the difference between two consecutive weekly checkin weights do not follow the non-increasing constraint,
users are grouped into this category.
We empirically set ‚àÜ = 0.04 to divide the users in our

dataset into two groups of similar size. To illustrate the
two patterns of weight change, Figure 1 shows the weekly
weight check-ins of two individual users, one from each
group. This grouping is coarse, but is motivated by studies
(Kraschnewski et al. 2010; Wing and Phelan 2005) acknowledging that approximately 80% of people who set out to lose
weight are successful at long-term weight loss maintenance,
where successful maintenance is defined as losing 10% or
more of the body weight and maintaining that for at least
an year. In the future for further analysis, we aim to separate users less coarsely, e.g., users who maintain their weight
neither gaining nor losing weight, users who lose weight and
maintain it and finally, users who gain weight.
The main distinctive feature of this weight loss application is that users are encouraged to set goals to regularly log
their weight, diet, and exercise. For a subset of users, this
application included a weekly weight ‚Äúcheck-in‚Äù, an average of the user‚Äôs weight check-ins during the week, for the
January 1, 2010 through July 1, 2012 period. This allows us
to juxtapose the weekly weights of the users with their posts
on the discussion forums.

3.1

Characteristics of Online Community

This weight loss application helps users set a personalized
daily calorie budget, track the food they are eating, their exercise and log their weekly weight. It also helps users to stay
motivated by providing an opportunity to connect with other
users who want to lose weight and support each other. Example snippets from forum threads are shown below. The
‚ÄúCan‚Äôt lose weight!‚Äù thread demonstrates users supporting
each other and offering advice. The ‚ÄúSomeday I will‚Äù thread
highlights the complex relationship between text, semantics,
and motivation in the forums.
Example thread: ‚ÄúCan‚Äôt lose weight!‚Äù
User 1: ‚ÄúI gained over 30 lbs in the last year and am
stressed about losing it. I eat 1600 calories a day and
burn more than that in exercise, but I havent lost any
weight. I am so confused.‚Äù
User 2: ‚ÄúYou‚Äôve only been a member for less than 2
months. I suggest you relax. Set your program to 1
pound weight loss a week. Adjust your habits to something you can live with. . . long term.‚Äù
User 3: ‚ÄúYou sound just like me. I think your exercise
is good but maybe you are eating more than you think.
Try diligently logging everything you consume.‚Äù
User 1: ‚ÄúThanks for the suggestions! I am going to get
back to my logging.‚Äù
Example thread: ‚ÄúSomeday I will. . . ‚Äù
User 1: ‚ÄúDo a pull-up :-)‚Äù
User 2: ‚Äú. . . actually enjoy exercising.‚Äù
User 3: ‚ÄúSomeday I will stop participating in these forums, but obviously not today.‚Äù
User 4: ‚ÄúI hope you fail :-)‚Äù

4

Empirical Analysis of Weight Loss Forum

In this section, we present preliminary observations on how
the language and discourse patterns of forum posts vary with

respect to weight loss dynamics. As an initial step, part-ofspeech (POS) tagging is performed on all forum posts using
the Stanford POS Tagger (Toutanova et al. 2003).
Weight Pattern
# Total users
# Forum users
# Forum posts
Posts per user
Words per post

Non-increasing

Fluctuating

1127
29
99
3.5
49.1

1143
68
1279
18.2
77.3

Table 1: Statistics of users and forum posts.

From the weekly check-in data we identified the number of users and the number of posts from each weight-loss
pattern cluster which are shown in Table 1. In our dataset,
out of 1127 users who are expressing non-increasing weight
loss pattern (1143 fluctuating weight loss pattern) only 29 of
them (68 of them respectively) made atleast one post on the
discussion forums. We see that the average number of posts
by fluctuating users is greater than the average number of
posts by non-increasing users. Our data also suggest that the
posts made by non-increasing users are shorter compared to
those made by fluctuating users. Both these suggest the possible loss of social connectedness once users achieve their
goal.

4.1

Lexical Categories

Studies (Pennebaker, Mehl, and Niederhoffer 2003) show
that the language defines an individual and his/her behavior.
We use the measures that characterize the weight loss pattern by using the linguistic classes in posts made by these
users on the forum. Specifically, verbs, conjunctions, adverbs, personal pronouns and prepositions are considered
as shown in Table 2. We collected all the individual posts
made by all the users belonging to each weight loss pattern
and measured the average frequency of a linguistic class per
post. Fluctuating users appear to talk more about themselves
and interact with other individuals one-on-one as they are
using a relatively higher number of personal pronouns. Additionally, we observe that users who lose weight in a fluctuating manner use greater fraction of prepositions and adverbs. Adverbs are primarily used to tell how someone did
something which means these users who lose weight in a
fluctuating manner explain more about themselves, perhaps
in an attempt to seek more information.

4.2

Asking Questions

In order to build and maintain vibrant online communities, it
is very important to understand the complex ways in which
the members interact and how the communities evolve over
time. As a part of that, previous literature (Bambina 2007)
revealed that people on online health communities mainly
engage in two activities: (i) seeking information, and (ii)
getting emotional support. People usually ask questions or
just browse through the community forums to collect information. If we can understand how users post questions and

Weight Pattern
Non-increasing

Fluctuating

Ling. class

Mean

Med.

SD

Mean

Med.

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

3.85
3.44
2.21
4.94
5.44

2.0
3.0
1.0
3.0
4.0

4.11
3.51
2.87
5.42
5.40

6.27
4.53
3.24
8.65
9.67

4.0
3.0
2.0
6.0
6.0

6.81
5.04
3.74
8.59
10.51

Table 2: Results of statistical measures on linguistic class attributes; Med. refers to Median; SD refers to Standard Deviation

how the other members respond to those questions, it will
be very useful in developing personalized profiles of users
so that the system is able to help them get sufficient information even before they post any questions. Below is an example (paraphrased) showing how users ask and respond to
questions.
Example thread: ‚ÄúNew user‚Äù
User 1: ‚ÄúDid anyone upgrade to the premium app?
What do you like about it?‚Äù
User 2: ‚ÄúI upgraded to the premium. I LOVE the functionality to log food in advance. I can track and set
goals that are not related to weight like how much I
sleep, how much water I drink, etc.‚Äù
User 3: ‚ÄúI upgraded my account to premium too. I really liked the added features because it helped me keep
track of my steps and participate in challenges.‚Äù
We are interested in knowing whether these two types of
users are actively seeking information. We deem a forum
post to be a question if it meets one of these two conditions:
1. Wh-question words: If a sentence in the post starts with
a question word: Wh-Determiner (WDT), Wh-pronoun
(WP), Possessive wh-pronoun (WP$), Wh-adverb (WRB).
2. Punctuation: If the post contains a question mark (‚Äò?‚Äô).
We computed the ratio of question-oriented posts made by
each user in the two clusters. After averaging these ratio values across all the users in each cluster separately, we found
that on average, 32.6% of the posts made by non-increasing
users were questions (S tandardError(S E) = 0.061) while
37.7% of the posts made by fluctuating users were questions
(S E = 0.042). This shows that on an average fluctuating
users do post relatively larger number of questions than the
non-increasing users. We conjecture this could be a reflection of the fluctuating users‚Äô aim to seek more information
from the forum.

4.3

Sentiment of Posts

Analyzing the sentiment of user posts in the forums can
provide a suprisingly meaningful sense of how the loss of
weight impacts the sentiment of user‚Äôs post. In this analysis, we report our initial results on extracting the sentiments
of user‚Äôs posts. In order to achieve this, we utilize the Stanford Sentiment Analyzer (Socher et al. 2013). This analyzer

Figure 2: Proportion of sentiments for the two weight-loss patterns. For non-increasing users, percentage of posts with Positive, Neutral and Negative sentiments are: 22%, 46.5% and 31.5%
respectively. For fluctuating users, the percentage of posts with
Positive, Neutral and Negative sentiments are: 20.9%, 37.6% and
41.5% respectively.

classifies a text input into one of five sentiment categories
‚Äì from Very Positive to Very Negative. We merge the five
classes into three: Positive, Neutral and Negative (In future,
we may consider specific (health and nutrition) sentiment
lexicons).
We analyzed the sentiment of posts contributed by the
users from the two clusters. As shown in Figure 2, posts of
users belonging to the non-increasing cluster are more neutral whereas the posts made by users from the fluctuating
cluster are mainly of negative sentiment. This gives an interesting intuition that the users of fluctuating group might
require more emotional support as they use more negative
sentiment in their posts.

4.4

Politeness

Politeness is an important marker which often is a decisive
factor in whether interactions go well or cease (Rogers and
Lee-Wong 2003). Based on this metric, we can understand
if correlation exists between the politeness of posts made
by users and their weight loss pattern. Politeness according to the Webster‚Äôs Dictionary is to show good manners
towards others, as in behavior, speech, etc. We measured
how polite the posts are with respect to the weight loss pattern. We use the politeness classifier (Danescu-NiculescuMizil et al. 2013) that was constructed with a wide range
of domain-independent lexical, sentiment and dependency
features and there by operationalizes the key components of
politeness theory. It was proven that this classifier achieves
near human-level accuracy across domains (shown 83.79%
classification accuracy on in-domain wiki). Below are some
examples obtained after the classification of posts on this forum.
1. Polite text: ‚ÄúGood for you! I started out obese. Now,
Im not even overweight. Its a great feeling. Congrats

to you on your milestone!‚Äù
‚Ä¢ Polite Score: 0.870
‚Ä¢ Impolite Score: 0.130
2. Impolite text: ‚ÄúGrrrr.... I wish I could screen these
posts so that I dont even have to SEE those darn posts
about HCG or 500 calorie diets any more. :twisted:
And why did my search for Grumpy or Rant or
McRant come up empty?????? Grrrrr......‚Äù
‚Ä¢ Polite Score: 0.250
‚Ä¢ Impolite Score: 0.750
The results of the politeness analysis in Table 3 shows
that users on this weight-loss forum are polite overall. We
speculate that users on weight-loss related forums act polite
to get more information and emotional support. Further investigation is needed to conclude if users on goal-oriented
communities talk politely.
Type

Polite

Impolite

Non-increasing
Fluctuating

70.6%
75%

29.4%
25%

Table 3: Statistics of users and politeness percentage posts

4.5

Excuses

Literature (Bambina 2007) suggests that people use online
forums to maintain accountability. This application mainly
serves the user community to set goals and help the members
achieve those goals. It is important to understand if there is a
correlation between the weight loss pattern of the users and
the way they are making excuses as they are accountable for
not losing weight. In general, excuses are put forward when
people experience questions about their conduct or identity
in case of failing at an assigned task, violating a norm, etc.
Existing research (Deppe and Harackiewicz 1996)
demonstrates that people who are provided with the opportunity to make excuses do seem to perform better on a variety
of tasks. In this analysis, we wanted to verify if the hypothesis that users when given an opportunity to make excuses are
better at losing weight. Here is an example that shows how
User 1 posts excuses in a forum thread.
Example thread: ‚ÄúTrouble sticking to a diet‚Äù
User 1: ‚ÄúI am out of town with the family and making
the right food choices is impossible right now.‚Äù
User 2: ‚ÄúI think we all have to find our own motivation
and drive to succeed in weight loss. We just have to let
the motivation be louder than the excuses.‚Äù
To the best of our knowledge, there is no prior work on automatic classification of a post as an excuse or a non-excuse.
In this regard, we initially wanted to find if excuse classification is simply a special case of text-based categorization or
any special classification approaches need to be developed.
We performed experiments with two standard algorithms:
Naive Bayes Classification and Support Vector Machines,
which were shown to be effective in previous text categorization studies. In order to implement these two algorithms

we considered the standard bag-of-words where a document
d can be expressed in terms of the frequency of each of the
n features as d~ = ( f1 (d), ( f2 (d), . . . , ( fn (d)), fi (d) is the number of times feature i occurs in document d.
We also extended the Latent Dirichlet Allocation (Blei,
Ng, and Jordan 2003) (LDA) to build a classifier that also
uses majority class voting approach to provide labels to the
posts. Initially, LDA is used to extract the latent topic distribution over each of the posts present in the training dataset
that are already labeled as excuses and non-excuses. Later,
each post from the testing dataset is represented in this topic
space. For a given post in the testing dataset, the final class
label is the majority class of the k-closest points in the topic
space. The entire process of classifying a post as an excuse
or non-excuse is described in Algorithm 1.
Data: Labelled dataset ‚Äì Excuses (e f ) and Non-excuses
(ne f ); jth post ‚Äì a post with no class label
Result: Labelled Testing data
Œ∏e ne ‚Üê LDAestimation (e f ,ne f );
œÜene
‚Üê LDAin f erence (kth post, Œ∏iene );
k
L ‚Üê ‚àÖ;
for i := 1 to |e f | + |ne f | do
dist ‚Üê KLdivergence(Œ∏iene , œÜene
j );
append(L, dist);
end
label jth post ‚Üê max class(k-nearestpoints( f ullist));
Algorithm 1: Classification Approach
We utilized Weka (Hall et al. 2009) and svmlight (Joachims 1999) libraries to perform classification using Naive Bayes and SVM respectively. Based on the results
shown in Table 4, LDA-based supervised classifier outperforms the other two approaches and so we use it for measuring the correlation between the frequency of excuses posted
by users and their weight loss patterns.
Approach
Naive Bayes
SVM
LDA-based

Cond-1

Cond-2

57.8% (Uni)
50% (Uni)
65% (80-20 split)

63.1% (Uni + Bi)
46.15% (Uni + Bi)
50% (50-50 split)

Table 4: Classification results in terms of accuracy with different
approaches and conditions (Uni ‚Äì Unigrams; Bi ‚Äì Bigrams; 80-20
split ‚Äì 80% training and 20% testing; 50-50 split ‚Äì 50% training
and 50% testing data) and classifiers

We identified that 46% of the users who make at least one
post in the forum give excuses. If we consider the categorywise statistics, 48% of the users who lose weight in a nonincreasing pattern and 54% of the posts made by the users
of fluctuating weight loss pattern made excuses in at least
one post. It is surprising to notice that users exhibit excuse‚Äì
giving behavior on this weight loss community where accountability is one of its characteristics. Early detection of
these kinds of users and providing more assistance to help
them stay motivated can help lose weight. This kind of in-

tervention by these applications can help gain the trust of its
users.
Overall, in this section we have explored how the basic lexical classes, questions, sentiment, politeness and excuses are
correlated with the weight loss patterns of users. As we got a
good level of understanding about these associations, we can
now use these different attributes as a set of features in order
to predict whether a new user can lose weight or not, based
only on the language he/she is using on these forums. Automated classifier can be very beneficial to design effective
weight loss applications that can help users get additional
support. It can also help the users to pay more attention to
their diet and exercise to lose weight effectively.

5

Forums Studied

We used threads from two other popular online forums
that were used in (Biyani et al. 2012) ‚Äì 1) Trip Advisor New York City travel forum that contains travel related discussions for New York City and 2) Ubuntu forum dataset
that contains discussions about the ubuntu operating system.
There are multiple threads of discussions in both these forums and each thread has multiple posts by several users.
The dataset contains total number of 609 threads (6591 total posts) and 621 threads (3603 total posts) for Tripadvisor
and Ubuntu forums respectively. On an average, the thread
length in terms of the number of posts is 10 and 5 for the tripadvisor and ubuntu forums respectively. The average number of users in a thread on tripadvisor forum is 1.98 and
on ubuntu forum is 3.41. As stated in (Biyani et al. 2012),
ubuntu forums have technical discussions which are nonsubjective in nature where as trip advisor, a travel related
discussion forum has discussions which tend to be subjective.

5.2

Trip Advisor Ubuntu
Ling. class

Mean

SD

Mean

SD

Adverbs
Verbs
Conjunctions
PersonalPron.
Prepositions

2.37
1.87
1.42
2.69
4.25

4.9
3.6
3.19
5.7
8.56

2.04
1.86
1.0
2.24
2.78

6.06
3.76
2.17
4.4
6.52

Table 5: Results of statistical significance tests on linguistic class
attributes for Trip Advisor and Ubuntu forums. For the results on
weight loss forum, please refer to Table 2. SD‚ÄìStandard Deviation

Comparison With General Forums

To contextualize this research, we want to understand if the
goal-oriented forums exhibit any specific traits compared to
the general forums. We define goal-oriented forums as the
forums associated with applications that help set goals while
building a social network of users who share similar goals.
Here, we present an analysis of how the type of forum can
affect the language used with a primary focus on understanding the lexical features and cohesiveness of the threads on
these forums.

5.1

Forum Name

Lexical Features

Lexical features like Part-of-Speech (POS) tags are obtained
for both the forums to understand the behavior of users in
terms of using different categories of words. Analysis similar to the earlier section was conducted using the Stanford
POS tagger to find the number of verbs, conjunctions, adverbs, personal pronouns and prepositions appearing in the
posts as shown in Table 5. As we compare these results with
the weight loss forum, we notice that users on these two forums don‚Äôt use as many personal pronouns and adverbs as
users on the weight loss forums. This is understandable as
users on weight loss forum have a primary goal to seek information while maintaining accountability.

5.3

Cohesion with Previous Posts

It is very important for the discussion forums to capture as
much participation as possible to reach their full potential.
When multiple conversations occur simultaneously, it is difficult to decide which utterance belongs to a specific conversation. Users on the online health forums mostly tend to
seek information and if the main topic is drifted to some
other topic, the main purpose of these discussion forums is
lost. Hence, it is important for the system to automatically
track non-cohesiveness in posts. Cohesion is the property of
a well-written document that links together sentences in the
same context. As a first step, we want to find out how similar a user‚Äôs post is with respect to the previous posts in a
thread from the weight loss forum. This can also help identify users in a given thread who elaborate on previous post
versus those who shift the topic.
Below is an example (paraphrased) showing cohesive post
made by the users on the weight loss forum.
Example thread: ‚Äúchanging life for a healthier self‚Äù
showing cohesive post by User 2
User 1: ‚ÄúDid you remove any commitments in your life
to make time to be healthier? If you have, was it a good
choice or did you regret it?‚Äù
User 2: ‚ÄúYes I‚Äôve done it and never regretted it.‚Äù
User 3: ‚ÄúTrying to do everything at once means doing
nothing - Georg Christoph‚Äù
User 2: ‚ÄúI‚Äôm not sure which entrepreneur said this but
focus only on what you need to do.‚Äù
We focus only on content words: verbs and nouns (partof-speech tags VB, VBZ, VBP, VBD, VBN, VBG, NN, NNP,
NNPS) and use WordNet (Miller 1995) to identify synonyms
of the content words. We compute similarity between the
current post and previous posts of other users in the thread in
terms of commonly shared verbs and nouns including synonyms. In our current analysis, we consider this similarity
score to be the measure of cohesion.
We consider all posts that are not thread-initial. To approximate whether a post is cohesive or not, we compare
the nouns and verbs of the current post to the list of nouns
and verbs (plus synonyms) obtained from the previous posts
of the thread. Our analysis (Table 6) on the three forums
‚Äì fit now data (weight loss forum), trip advisor and ubuntu

Data: Posts P1 , . . ., Pk‚àí1 , Pk
Result: CohS core(Pk )
set A ‚Üê ‚àÖ;
for i := 1 to (k ‚àí 1) do
[vbi , nni ] ‚Üê POS tagging (Pi );
set A ‚Üê set A ‚à™ [vbi , nni ];
set A ‚Üê set A ‚à™ synset(vbi ) ‚à™ synset(nni );
end
set B ‚Üê ‚àÖ;
[vbk , nnk ] ‚Üê POS tagging (Pk );
set B ‚Üê set B ‚à™ [vbk , nnk ];
set B ‚Üê set B ‚à™ synset(vbk ) ‚à™ synset(nnk );
A ‚à©set B |
CohS core(Pk ) ‚Üê |set|set
B|
Algorithm 2: Calculating the cohesive score of a post

finds that the threads on weight loss forum are more cohesive compared to the other two forums.
Fit Now data

Trip Advisor

Ubuntu

0.46
2.22

0.42
3.64

0.30
3.87

Cohesiveness
S.E (√ó10‚àí4 )

Table 6: Average value of Cohesiveness (along with Standard Error
(S.E) ) across all the threads in a given forum. Extreme values are:
0 ‚Äì non-cohesive; 1 ‚Äì cohesive

Overall, it is interesting to see that the goal-oriented forums
(like weight loss forums) have more cohesive threads compared to the general forums. Additionally, users on the goaloriented forums tend to post more information about themselves. In the future it will be worth studying if language
cues can help in predicting auto-tagging of threads to a specific type of forum. Studying other language metrics can also
help understand the contributions of different online forums
and their impact on the public.

6

Implications

The different language metrics studied in the two main
sections of this paper have a great potential to differentiate automatically between users who are struggling to lose
weight and the users who lost weight and are keeping it
off. There are for example, other existing technologies that
help users lose weight by ‚Äì providing incentives if they
lose weight (PACT http://www.gym-pact.com/), allowing other fitness applications to synchronize with the current
application to keep track of exercise (MyFitnessPal https:
//www.myfitnesspal.com/ ), posting questions while doing grocery shopping to find out the calorie content (Fooducate http://www.fooducate.com/ ), etc. We envision
tools that utilize the wealth of information present on the discussion forums along with the users activity to automatically
estimate the degree to which a user‚Äôs efforts will yield results. Predictions of success are not the end goals. The value
of these types of predictions are when they are leveraged
to generate alternative behaviors and actions that a user can
take to improve their chances of weight loss success. De-

signing systems that rely on features studied in this paper
could improve weight loss applications and thereby enhance
the quality of life.
People are taking advantage of these kinds of applications
as they can preserve their anonymity and provide genuine
information about their food intake, exercise levels, etc to
safely collect as much information as they can. Even though
the real identity can be hidden, it is important that the tools
being envisioned provide support in a very ethical manner.
On the other hand, deciphering the genuineness of the information provided is an area of research that can be worth
pursuing (Estrin 2014). On the whole, we believe that it is
important to understand the different attributes that affect the
behavior of individuals on the weight loss forums and help
them successfully lose weight. We hope that this work initiates further research on these types of discussion forums to
raise awareness about the different factors faced by individuals who are struggling to lose weight and thereby can help
develop policies that can support them in losing weight.

7

Conclusions and Future Work

In this paper, we analyzed how the online discussion forums
of weight loss applications can act as an important tool to
detect and identify the different metrics that are associated
with weight loss. As a first step, we identified the two types
of weight loss patterns exhibited by the users on this forum
and studied different factors like sentiment, politeness, excuses and questions. We took advantage of existing tools to
study these different factors and correlations between these
factors and the weight loss pattern. Specifically, this analysis reveals interesting insights about two populations of
users who lose weight differently. Users who lose weight
in a fluctuating manner are more active in these forums, give
more excuses, post more questions and the majority of their
posts contain negative sentiment. This shows the information seeking nature and suggests the possible need for more
support to these kinds of users. As a secondary focus, we
studied how the language metrics differ across goal-oriented
forums and general forums. We found that users of goaloriented forums usually contribute to a more cohesive posting threads and users on general forums tend not to reveal
much information about themselves.
Our analyses provides valuable insights on how user behavior within online weight loss forums might correlate with
the weight outcomes. These sorts of analyses, particularly
when replicated, could provide valuable insights for developing new technologies that might facilitate more effective
interactions about weight loss and can help gain trust of
users in these kinds of systems. It could also provide valuable insights for improving theories about behavior change.
Acknowledgments. This research is supported in part by a
Google research award, the ONR grants N00014-13-1-0176,
N00014-13-1-0519 and N00014-15-1-2027, and the ARO
grant W911NF-13- 1-0023.

References
Backstrom, L.; Kleinberg, J.; Lee, L.; and DanescuNiculescu-Mizil, C. 2013. Characterizing and curating con-

versation threads: Expansion, focus, volume, re-entry. In
ACM International Conference on Web Search and Data Mining, WSDM ‚Äô13, 13‚Äì22.
Ballantine, P. W., and Stephenson, R. J. 2011. Help me, I‚Äôm
fat! Social support in online weight loss networks. Journal of
Consumer Behaviour 10(6):332‚Äì337.
Bambina, A. D. 2007. Online Social Support: The Interplay
of Social Networks and Computer-Mediated Communication.
Bickart, B., and Schindler, R. M. 2001. Internet forums as influential sources of consumer information. J. of Int Marketing
15(3):31‚Äì40.
Biyani, P.; Bhatia, S.; Caragea, C.; and Mitra, P. 2012. Thread
specific features are helpful for identifying subjectivity orientation of online forum threads. In Proceedings of the 24th International Conference on Computational Linguistics, 295‚Äì
310.
Black, L. W.; But, J. J.; and Russell, L. D. 2010. The secret is out! supporting weight loss through online interaction.
Cases on online discussion and interaction: Experiences and
outcomes.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research
3:993‚Äì1022.
Brownell, K. D., and Rodin, J. 1994. The dieting maelstrom:
Is it possible and advisable to lose weight? American Psychologist 49(9):781‚Äì791.
Danescu-Niculescu-Mizil, C.; Sudhof, M.; Jurafsky, D.;
Leskovec, J.; and Potts, C. 2013. A computational approach
to politeness with application to social factors. In Proceedings of ACL.
Das, A., and Faxvaag, A. 2014. What influences patient participation in an online forum for weight loss surgery? IJMR
3(1).
De Choudhury, M.; Counts, S.; and Horvitz, E. 2013. Predicting postpartum changes in emotion and behavior via social
media. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ‚Äô13, 3267‚Äì3276.
Deppe, R. K., and Harackiewicz, J. M. 1996. Selfhandicapping and intrinsic motivation: Buffering intrinsic
motivation from the threat of failure. J. of Personality and
Social Psychology 70(4):868‚Äì876.
Estrin, D. 2014. Small data, where n = me. Commun. ACM
57(4):32‚Äì34.
GoÃÅmez, V.; Kaltenbrunner, A.; and LoÃÅpez, V. 2008. Statistical analysis of the social network and discussion threads in
slashdot. In Proceedings of the 17th International Conference
on World Wide Web, WWW ‚Äô08, 645‚Äì654.
Hall, M.; Frank, E.; Holmes, G.; Pfahringer, B.; Reutemann,
P.; and Witten, I. H. 2009. The weka data mining software:
An update. SIGKDD Explorations 11(1).
Hawn, C. 2009. Take two aspirin and tweet me in the morning; how twitter, facebook, and other social media are reshaping health care. Health Affairs 28(2):361‚Äì368.
Heaivilin, N.; Gerbert, B.; Page, J.; and Gibbs, J. 2011. Public health surveillance of dental pain via twitter. Journal of
Dental Research 90(9):1047‚Äì1051.
Hekler, B. E.; Dubey, G.; McDonald, W. D.; Poole, S. E.;
Li, V.; and Eikey, E. 2014. Exploring the relationship between changes in weight and utterances in an online weight
loss forum: A content and correlational analysis study. J Med
Internet Res 16(12).

Joachims, T. 1999. Making large-scale SVM learning practical. In SchoÃàlkopf, B.; Burges, C.; and Smola, A., eds., Advances in Kernel Methods - Support Vector Learning. Cambridge, MA: MIT Press. chapter 11, 169‚Äì184.
Kim, A. J. 2000. Community Building on the Web: Secret
Strategies for Successful Online Communities.
Kraschnewski, J. L.; Boan, J.; Esposito, J.; Sherwood, N. E.;
Lehman, E. B.; Kephart, D. K.; and Sciamanna, C. N. 2010.
Long-term weight loss maintenance in the united states. International J. of Obesity 34(11):1644‚Äì1654.
Lamb, A.; Paul, M. J.; and Dredze, M. 2013. Separating fact
from fear: Tracking flu infections on twitter. In NAACL.
Leahey, T. M.; Kumar, R.; Weinberg, B. M.; and Wing, R. R.
2012. Teammates and social influence affect weight loss
outcomes in a team-based weight loss competition. Obesity
20(7):1413‚Äì1418.
Ludford, P. J.; Cosley, D.; Frankowski, D.; and Terveen, L.
2004. Think different: Increasing online community participation using uniqueness and group dissimilarity. In SIGCHI
Conference on Human Factors in Computing Systems, CHI
‚Äô04, 631‚Äì638.
Miller, G. A. 1995. Wordnet: A lexical database for English.
Communications of the ACM 38(11):39‚Äì41.
Must, A.; Spadano, J.; Coakley, E. H.; Field, A. E.; Colditz,
G.; and H., D. W. 1999. The disease burden associated with
overweight and obesity. JAMA 282(16):1523‚Äì1529.
2012. The obesity epidemic and its impact on hypertension.
Canadian Journal of Cardiology 28(3):326 ‚Äì 333.
Ogden, C. L.; Kit, B. K.; Fakhouri, T. H.; Carroll, M. D.; and
Flegal, K. M. 2014. The epidemiology of obesity among
adults. GI Epidemiology 394‚Äì404.
Pennebaker, J. W.; Mehl, M. R.; and Niederhoffer, K. G.
2003. Psychological aspects of natural language use: Our
words, our selves. Annual Review of Psychology 54(1):547‚Äì
577.
Preece, J.; Nonnecke, B.; and Andrews, D. 2004. The top
five reasons for lurking: improving community experiences
for everyone. Computers in Human Behavior 20(2):201 ‚Äì
223.
Rogers, P. S., and Lee-Wong, S. M. 2003. Reconceptualizing
politeness to accommodate dynamic tensions in subordinateto-superior reporting. 17(4):379‚Äì412.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
EMNLP, 1631‚Äì1642.
Toutanova, K.; Klein, D.; Manning, C. D.; and Singer, Y.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL ‚Äô03, 173‚Äì180.
Wing, R. R., and Phelan, S. 2005. Long-term weight loss
maintenance. The American Journal of Clinical Nutrition
82(suppl):222S‚Äì5S.
Yang, J.; Wei, X.; Ackerman, M. S.; and Adamic, L. A. 2010.
Activity lifespan: An analysis of user survival patterns in online knowledge sharing communities. In ICWSM.

A Human Factors Analysis of Proactive Support in
Human-robot Teaming
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborti and Subbarao Kambhampati
Abstract‚Äî It has long been assumed that for effective humanrobot teaming, it is desirable for assistive robots to infer the
goals and intents of the humans, and take proactive actions
to help them achieve their goals. However, there has not
been any systematic evaluation of the accuracy of this claim.
On the face of it, there are several ways a proactive robot
assistant can in fact reduce the effectiveness of teaming. For
example, it can increase the cognitive load of the human
teammate by performing actions that are unanticipated by the
human. In such cases, even though the teaming performance
could be improved, it is unclear whether humans are willing
to adapt to robot actions or are able to adapt in a timely
manner. Furthermore, misinterpretations and delays in goal
and intent recognition due to partial observations and limited
communication can also reduce the performance. In this paper,
our aim is to perform an analysis of human factors on the
effectiveness of such proactive support in human-robot teaming.
We perform our evaluation in a simulated Urban Search
and Rescue (USAR) task, in which the efficacy of teaming
is not only dependent on individual performance but also on
teammates‚Äô interactions with each other. In this task, the human
teammate is remotely controlling a robot while working with an
intelligent robot teammate ‚ÄòMary‚Äô. Our main result shows that
the subjects generally preferred Mary with the ability to provide
proactive support (compared to Mary without this ability). Our
results also show that human cognitive load was increased with
a proactive assistant (albeit not significantly) even though the
subjects appeared to interact with it less.

I. I NTRODUCTION
The efficacy of teaming [8] is not only dependent on
individual performance, but also on teammates‚Äô interactions
with each other. It has long been assumed that for effective
human-robot teaming, it is desirable for assistive robots to
infer the goals and intents of the humans, and take proactive
actions to help them achieve their goals. For example, the
ability of goal and intent recognition is considered to be
required for an assistive robot to be socially acceptable
[22], [5], [16], [2], [24]. This claim is also assumed in
other human-robot teaming tasks, such as collaborative manufacturing [25] and urban search and rescue (USAR) [23].
However, there has not been any systematic evaluation of the
accuracy of this claim.1
*This work was supported in part by the ARO grant W911NF-13-1- 0023,
and the ONR grants N00014-13-1-0176, N00014-13-1-0519 and N0001415-1-2027.
The authors would like to thank Nathaniel Mendoza for help with the
simulator as well as the anonymous participants in the study.
The authors are with the Department of Computer Science and
Engineering, Arizona State University, Tempe, AZ 85281, USA

{yzhan442,vnaray15,tchakra2,rao}@asu.edu

1 The authors in [13] considered anticipatory action in interaction scenarios involving repetitive actions and the task settings are for human-robot
teaming with proximal interactions.

Fig. 1. Illustration of our USAR task in which the human teammate
remotely controls a robot while working with an intelligent robot ‚ÄòMary‚Äô.
We intend to compare Mary with and without a proactive support ability.

There are several ways a proactive robot assistant can
in fact reduce the effectiveness of teaming. For example,
it can increase the cognitive load of the human teammate by
performing actions that are unanticipated by the human. In
such cases, even though the teaming performance could be
improved, it is unclear whether humans are willing to adapt
to robot actions or are able to adapt in a timely manner.
Furthermore, misinterpretations and delays in goal and intent
recognition due to partial observations and limited communication can also reduce performance. For example, consider
a case in which you want to make an omelet and need eggs
to be fetched from the fridge. Even if an assistive robot has
started to fetch the eggs (after recognizing your intent), you
may decide that the robot is too slow and fetch the eggs
by yourself although you could improve the performance by
letting the robot fetch the eggs while you preheat the pan.
On the other hand, adapting to the robot‚Äôs actions in such
scenarios to improve teaming performance can increase the
human cognitive load, which leads to unsatisfactory teaming
experience. These conflicting factors make us investigate the
utility of Proactive Support (PS) in human-robot teaming.
In this paper, we start this investigation in a simulated
USAR task with a general way to implement the proactive
support ability on a robot in similar scenarios. Previous work
[13] that investigates the effects of this ability is restricted
to human-robot teaming with more proximal interactions.
Meanwhile, to maintain the generality of this task, we only
introduced a few necessary simplifications. In our task,
the human teammate is remotely controlling a robot while
working with an intelligent robot ‚ÄòMary‚Äô (as shown in Fig.
1). The human-robot team is deployed during the early phase

of an emergency response where they need to search, rescue
and provide preliminary treatment to casualties.
This USAR scenario considers many of the complexities
(e.g., partial observations) that often occur in real-world
USAR tasks and we intend to learn whether these complexities influence the overall evaluation of the intelligent robot
(i.e., Mary) with a proactive support (PS) ability, compared
to Mary without this ability. We also aim to investigate
the various trade-offs, e.g., mental workload and situation
awareness through this human factors study.
II. R ELATED W ORK
There are many early works on goal and intent recognition
(e.g., [15], [14], [4]). More recently, a technique to compile
the problem of plan recognition into a classical planning
problem is provided [20]. There is also a rich literature
in the area of plan adaptation, which handles how robots
plan under human-introduced constraints (e.g., social rules
[24]). Using simple temporal networks (STNs), there has
been development in efficient dispatchers that perform fast,
online, least-commitment scheduling adaptation [6]. There
are also a number of adaptation techniques that focus on
integrated planning and execution [7], [21], [1].
There are existing systems that combine both goal and
plan recognition and plan adaptation to achieve a proactive
support ability on robots. In [13], [12], the authors propose a
cost based anticipatory adaptive action selection mechanism
for a robotic teammate to make decisions based on the
confidence of the action‚Äôs validity and relative risk. However,
only repetitive tasks are considered and the task settings are
for human-robot teaming with more proximal interactions
compared to that in USAR scenarios. In [5], a humanaware planning paradigm is introduced where the robot only
passively interacts with the human by avoiding conflicts with
the recognized human plan. In USAR scenarios, it is also
desirable for the robot to proactively provide support to the
human. A recent paper proposes a planning for serendipity
paradigm in which the authors investigate planning for
stigmergic collaboration without explicit commitments [3].
In [17], the authors propose a unified approach to concurrent
plan recognition and execution for human-robot teams, in
which they represent alternative plans for both the human
and robot, thus allowing recognition and adaptation to be performed concurrently and holistically. However, the limitation
is that the plan choices must be specified a priori instead of
dynamically constructed based on the current goal and intent
of the human. This renders the approach impractical for realworld scenarios since even moderate number of choices (i.e.,
branching factors) can make the approach infeasible.
Part of our goal is to provide a general way to achieve
a proactive support ability in scenarios that are similar to
our USAR task, in which the task is composed of subtasks
with priorities that are dependent on the current situation.
Note that a framework to achieve general proactive support
can be arbitrarily complex depending on the task and level
of support that is needed. In our work, similar to [23], we
use the plan recognition technique in [20] and then feed its

outputs to a planner which determines the priorities of the
subtasks and computes a plan accordingly. The main goal of
this work is to start the investigation of humans factors for
proactive support in various human-robot teaming scenarios.
Regarding the benefits of automation in human-robot
teaming, it is well known that automation can have both
positive and negative effects on human performance. Empirical proofs have been provided in four main areas: mental
workload, situation awareness, complacency and skill degradation [19]. We also aim to study the influence of proactive
support on these factors in our USAR task.
III. BACKGROUND
A. USAR Task Settings Overview
In our simulated USAR task, the human and intelligent
robot (i.e., Mary) share the same set of candidate goals
(i.e., subtasks), and the overall team goal is to achieve
them all (which will be distributed among the human and
Mary). These goals are not independent of each other. In
particular, the priorities of goals are dependent on which
goals are achieved in the current situation. Given these task
settings, we aim to investigate the influence of a proactive
support (PS) ability on a robot. We compare two cases:
Mary (i.e., the intelligent robot) has a PS ability and Mary
does not have this ability. During the task execution, in
both cases, Mary chooses her own goal to maximize the
teaming performance accordingly to the human‚Äôs current
goal. When Mary does not have a proactive support ability,
she can only know the human‚Äôs current goal when the human
explicitly communicates it to her. When Mary has this ability,
if the human does not inform Mary of his/her current goal,2
Mary can infer it based on her observations. To summarize,
Mary in both cases can adapt to human goals while Mary
with a PS ability can adapt in a more ‚Äúproactive‚Äù fashion
(hence proactive support). Finally, in both cases, Mary has
an automated planner (see a brief description below) that
can create a plan to achieve her current goal and she can
autonomously execute the plan.
B. Automated Planner
In our settings, a task or subtask is compiled into a
problem instance for an automated planner to solve. The
planner creates a plan by connecting an initial state to a goal
state using agent actions. A planning problem can be specified using a planning domain definition language (PDDL)
[11]. Depending on the task, there are many extensions of
PDDL (e.g., [9], [10]) that can incorporate various modeling
requirements. We use the extension of PDDL described in
[9] to model the USAR domain. Using an automated planner
allows an agent to reason directly about the goal. Human
factors study on the incorporation of automated planners for
human-robot teaming has appeared previously in [18].
2 In both cases, when the human (optionally) informs Mary of his/her
current goal, it is used directly by Mary assuming that this information is
accurate.

Fig. 3.

(a)

(b)

Fig. 2. (a) Simulated Environment for our USAR task. (b) The environment
(from robot X‚Äôs cameras) that the human subject actually sees.

C. Goal and Intent Recognition
To recognize the human intents and goals, assuming that
humans are rational, we use the technique in [20]. In our
task, Mary maintains a belief of the human‚Äôs current goal
(denoted by GX ) as a hypothesis goal set YX , in which
YX corresponds to all remaining candidate goals. Given a
sequence of observations q that are obtained periodically
from sensors (on Mary or fixed in the environment), the
probability distribution Q over G 2 YX is recomputed using
a Bayesian update P(G|q ) ¬µ P(q |G), where the prior is
approximated by the function P(q |G) = 1/(1 + e b D(G,q ) )
in which D(G, q ) = C p (G q ) C p (G+q ). C p (G+q ) and
C p (G q ) represent the cost of the optimal plan to achieve G
with and without the observation of q , respectively. Having
known the probability distribution Q, the goal that has the
highest probability is assumed to be the current goal of
the human. This goal is correspondingly taken out of the
consideration of Mary and Mary then adapts her current
goal if necessary (from her remaining goals) to optimize
the teaming performance. Mary then makes a plan using
an automated planner described previously to achieve her
current goal.
IV. S TUDY D ESIGN
A. Hypotheses
We aim to investigate the following hypotheses:
‚Ä¢ H1) Mary with a proactive support (PS) ability enables
more effective teaming (e.g., less communication and
more efficiency) in our task settings.
‚Ä¢ H2) Mary with a PS ability increases human mental
workload (e.g., due to unanticipated actions from Mary).
In our study, we also make efforts to maintain the task
settings as general as possible. For a discussion on the
generalization of the results, refer to the conclusion section.
B. Environment
Fig. 2(a) shows the simulated environment (created in
Webots) in our USAR task, which represents the floor plan
of an office building where a disaster occurs (e.g., a fire). Fig.
2(a) is the visual feedback from the remotely controlled robot

Example puzzle problem used in our USAR task.

(i.e., robot X in Fig. 1) that the human subject actually sees.
The environment is organized as segments, and each segment
is identified by a unique label (e.g., R01). Furthermore, the
segments are grouped into four regions: medical kit storage
region (represented by segments starting with ‚ÄòS‚Äô), casualty
search region (starting with ‚ÄòR‚Äô), medical room region where
treatment (or triage) is performed (starting with ‚ÄòM‚Äô), and
the hallway region (starting with ‚ÄòH‚Äô). Each region can be
accessed via a door that connects to a hallway segment and R
regions are further divided into rooms that are also connected
by doors. The doors are initially closed and can be pushed
open by the robots. The doors remain open after being
pushed open. Both the remotely-controlled robot (denoted by
‚ÄòX‚Äô) and Mary work inside this environment. There are two
networked CCTV cameras that Mary can obtain observations
from and the field of views of these cameras are also shown
in Fig. 2(a).
C. Task Settings
The overall team goal is to find and treat all the casualties
in the environment, which includes searching for casualties in
the R regions, carrying casualties to medical rooms, fetching
medical kits and performing triages. In Fig. 2(a), the two
colored boxes (i.e., red and blue) in R regions represent
casualties and the white boxes in S regions represent medical
kits.
We impose two constraints on the agents: 1) either robot
X or Mary can carry only one medical kit or one casualty
at one time. 2) The triage can only be performed by robot
X for which the human subject needs to solve a few puzzle
problems (see Fig. 3 for an example) in 2 minutes. Out of
the two casualties, we assume that one is critically injured
(i.e., the red box in R02) who should be treated immediately
after being found. The other one is lightly injured (i.e., the
blue box in R05). It is also assumed that a medical room
can only accommodate one casualty and each medical kit
can only be used towards one casualty.
D. Interface Design
In this USAR task, the human subject needs to manually
control robot X while interacting with Mary. To create a more
realistic USAR environment, the human subject only has
access to the visual feeds from robot X. In other words, the
human subject can only observe the part of the environment
from robot X‚Äôs ‚Äúeyes‚Äù (i.e., two cameras, one mounted above
the other).
The interaction interface between the human subject and
robot X is shown in Fig. 4. More specifically, robot X
displays a list of applicable actions that it can perform given

Fig. 4.

Interaction interface between the human subject and robot X.

the current state. The human subject interacts with robot X
to choose an action from the list of applicable actions. When
the chosen action is completed by X, the interaction interface
displays the next set of actions. This process is repeated
until the task is finished (i.e., all the casualties are found
and treated). Following are the list of all possible action
types that the human can choose. Compare the list with that
shown in Fig 4. This interface also allows the human subject
to optionally inform Mary about his/her current goal so that
Mary can remove it from consideration and adapt her goal
accordingly when necessary.
‚Ä¢ move X H01 H02 - Move robot X from hallway
segment H01 to hallway segment H02.
‚Ä¢ pushdoor X R01 R02 - Push the door between
room R01 and room R02.
‚Ä¢ grab medkit X S01 - Grab the medical kit from
storage room S01.
‚Ä¢ carry casualty X R01 - Carry the casualty at
room R05.
‚Ä¢ drop medkit X M01 - Drop the medical kit in medical room M01.
‚Ä¢ lay down casualty X M01 - Lay down the casualty in medical room M01.
‚Ä¢ perform triage X M01 - Perform medical triage
in medical room M01.
‚Ä¢ Press ‚Äòi‚Äô - Inform Mary about the human subject‚Äôs
current or intended goal. (A list of all remaining candidate goals will be displayed to be chosen.)
Note that these actions are modeled to respect the
constraints that we discussed in Sec. IV-C. For example, lay down casualty X M01 is only available
when there is no other casualties in medical room M01;
perform triage X M01 is only available when there is
a casualty and a medical kit in M01.
The interaction interface between the human subject and
Mary is shown in Fig. 5. This interface is first used by Mary
to update the human subject about her current goal. When
the human subject wants to take over the goal that Mary is

Fig. 5.

Interaction interface between the human subject and Mary.

Fig. 6.

Experimental setup in the USAR task

acting to achieve, this interface is also used to display the
choices (to be selected by the human subject) for Mary to
terminate her current (uncompleted) goal.
E. Study Setup and Flow
The study was set up in our lab space, similar to that
shown in Fig. 6. Before the beginning of the task, the human
subject is given the floor plan without the annotations of
the casualties (i.e., colored boxes). Furthermore, the human
subject is informed that there are two casualties (that cannot
move) and they are located inside the casualty search regions.
However, no information about their exact locations is provided (i.e., which rooms the casualties are in). The human
subject is also informed that the casualty that is represented
by a red box is seriously injured, and should be treated as
soon as possible. Note that Mary has no more information
than the human subject. The remotely controlled robot X and
Mary start in the same segment H01, which is specified by
the green arrows.
Subjects were assigned alternately to team up with either
Mary with a PS ability or without. Each subject is only

allowed to take part in one experimental trial to avoid performance fluctuation due to experience. All subjects completed
the consent form before participating in the study. Prior to
each run, the subject was asked to read the instruction materials that contain the background knowledge and the above
information. The subject was then exposed to the simulator
and the interface and was asked to experiment with them to
gain some familiarity. The subject was asked to collaborate
with Mary to find and treat the two casualties. After the trial,
the subject was asked to complete a questionnaire (in Likert
scale).

that Mary chooses is:

F. Example Scenario

PM = h(pushdoor Mary H01 S03),
(move Mary H01 S03),
(move Mary S03 S04),
(grab medkit Mary S04),
(move Mary S04 S03),
(move Mary S03 H01),
(move Mary H01 H02),
(move Mary H02 H03),
(move Mary H03 H04),
(move Mary H04 H08),
(pushdoor Mary H08 M02),
(move Mary H08 M02),
(move Mary M02 M01),
(drop medkit Mary M01)i

Next, we walk through an example scenario in our USAR
task. Consider a scenario in which the human subject found
the critically injured casualty and the current goal (GX )
of the human subject becomes ‚Äòbring the critically injured
casualty to the top medical room in Fig. 2(a):
goal(X,‚Äòbring the critically injured
casualty to the top medical room‚Äô) =
{ (at critically injured casualty M01)}
However, assume that the human subject failed to inform
Mary of his/her current goal. Also, assume the following
states for the medical kits: {(at med kit 1 S01),
(at med kit 2 S04)}, and that Mary at that time is
still searching the casualties in the other casualty search
region. When robot X enters the field of view of the
CCTV cameras the action and state of X are detected by
the cameras and are fed to Mary as observations. In this
example, some of robot X‚Äôs actions, such as {(move X
H02 H03), (move X H04 H08)} will be observed by
Mary, which triggers the goal and intent recognition process.
After computing the probability distribution Q for all goals
in the candidate goal set for the human, the goal that has the
higher probability (and falls above a pre-specified threshold)
is assumed to be the current goal of the human (GX ), which
in this case is ‚Äòbring the critically injured
casualty to the top medical room‚Äô. Mary now
knows that the critically injured casualty has been found and
can remove this goal from her own candidate goal set.
Furthermore, given this information, Mary recomputes
the priorities of the remaining goals in the current situation
and adapts her goal accordingly. In particular, although the
searching task is still undergoing, Mary realizes that in this
case helping the human subject by bringing a medical kit to
M01 would achieve a better utility for the team. Note that
should the casualty found by the human subject be lightly
injured instead, Mary would decide to continue her search;
also, should the casualty found by the human subject be
lightly injured but the critically injured casualty has already
been treated, Mary would choose to help the human fetch
the medical kit. Note also that in the case that Mary does
not have a PS ability, the above update can only occur in a
timely manner if the human subject chooses to inform Mary
about his/her current goal. In our running example, the goal

goal(GM ,‚Äòbring med kit 1 to the top
medical room‚Äô) =
{(at med kit 1 M01)}
Having chosen her current goal GM , Mary then uses an
automated planner to generate a plan (PM ) that achieves the
goal. Meanwhile, Mary will update the human subject with
her current goal. Assuming that Mary is at segment H01 at
the time, the following plan would be generated:

Note that various other scenarios can arise in this task,
which may not always favor Mary with a PS ability. For
example, the human subject may decide to deliver the
medical kits to the medical rooms even before finding any
casualties. or the human subject may walk robot X to the
medical room empty-handed. These can confuse the goal
and intent recognition process on Mary and lead to reduced
teaming performance. Although not all of these scenarios
occurred during our experimental study, they demonstrate
the conflicting factors for proactive support in human-robot
teaming tasks. It is also clear that these tradeoffs are dependent on the task and robot settings, which require more
investigations in future work.
V. R ESULTS
The study was performed over 4 weeks and involved
16 volunteers (9 males, 7 females), Volunteers have ages
with M = 24 and SD = 1.15. Subjects were recruited from
students on campus. Due to the requirement of understanding
English instructions, subjects must indicate that they are
confident with English communication skills before taking
part in the study. We also asked about the subject‚Äôs familiarity
with computers (M = 6.56, SD = 0.63), robots (M = 4.19,
SD = 0.91), puzzle problems (M = 3.19, SD = 0.83) and
computer gaming (M = 4.69, SD = 1.49), in seven-point
scales after the study (with 1 being least familiar and 7
being most familiar). The subjects reported familiarity with
computers, but not so much with robots, puzzle problems or
computer gaming.

Fig. 7. Results for objective performance and measures. ‚á§ denotes p < 0.05,
‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

Fig. 8. Results for task performance and measures. ‚á§ denotes p < 0.05,
‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

A. Measurement
A post-study questionnaire is used to evaluate three of
four areas that are often used to assess automated systems:
mental workload, situation awareness, and complacency [19].
Furthermore, we also use the questionnaire to evaluate
several psychological distances between individuals and the
environment (including robots), which include immediacy,
effectiveness, likability and trust. Immediacy describes how
realistic the subject felt about the task and Mary. Effectiveness describes the subject‚Äôs feeling about how effective the
subject considered Mary as a teammate. Likability describes
how likable the subject felt about Mary. Trust describes
whether the subject felt that Mary was trustworthy. We also
collect the subjects‚Äô opinions on whether they considered that
Mary should be improved (i.e., improvability).
One way fixed-effects ANOVA tests were performed to
analyze the objective performance and measures, as well as
the subjective questions. The fixed factor in the tests is the
type of Mary, the intelligent robot, which is either Mary with
a PS ability or without (denoted by No-PS).

smaller for the PS case but we did not find any significant
difference. However, we did find a significant difference for
the average number of times the subject informed his/her
goal to Mary (F(1, 14) = 18.27, p < 0.001). This shows that
the subject felt less necessity to inform Mary in the PS
case. There is also a significant difference in the number
of goal updates the subject received from Mary (F(1, 14) =
7.58, p < 0.05), This confirms that Mary changed her goal
less frequently in the PS case.
We also compare the accuracy of the puzzle problems for
the triage operations. To discourage subjects from guessing
the answers to the puzzle questions, they were told that
each incorrect answer would give them negative scores.
Our analysis, interestingly, shows a significant difference
on this performance measure (F(1, 14) = 4.64, p < 0.01),
which suggests that the human mental workload may have
been reduced in the PS case, which is not consistent with
the second hypothesis (i.e., H2). Furthermore, as we show
in the evaluation of subjective measures, this interpretation
contradicts with the results there.

B. Objective Performance

C. Subjective Performance

We first investigate the objective performance and measures. The overall performance (presented in in Fig. 7) is
evaluated based on the total time taken for the team to find
and treat the critically injured casualty, and the total time
taken for the team to finish the entire task (i.e., find and
treat both casualties). It is interesting to observe that while
there is a significant difference between PS and No-PS for
the time taken to complete the entire USAR task (F(1, 14) =
8.34, p < 0.01), we do not find any significant difference for
treating the critically injured casualty. This may be due to the
fact that humans are proficient at prioritizing goals. However,
this may negatively impact the teaming performance since
the subject may more often choose to neglect the help of
Mary when he/she does not feel comfortable with entrusting
Mary with important goals. This conjecture is also consistent
with the results in Fig. 8, which is discussed next.
We provide a more detailed analysis of task performance
in Fig. 8. We compare the average number of times the
subject stopped Mary from executing her current goal and
the average number of times the subject had goal conflicts
with Mary. The results show that these numbers are generally

In this section, we investigate the subjective performance
based on the questionnaire (23 questions in total). For these
23 questions, we categorize them into 8 different (partially
overlapping) groups. This includes 3 groups for evaluating
automation: mental workload (3 items, Cronbach‚Äôs a =
0.713), situation awareness (1 item), and complacency (2
items, Cronbach‚Äôs a = 0.769). Furthermore, we also evaluate
several psychological distances between the human subject
and environment (including Mary), which include immediacy
(1 item), effectiveness (7 items, Cronbach‚Äôs a = 0.724),
likability (1 item), and trust (3 items, Cronbach‚Äôs a = 0.871).
We also include improvability (1 item). The answers to the
questions are in seven-point scales. The results are presented
accumulatively in Fig. 9.
1) Mental Workload: For mental workload, we include
questions that inquire about the ease of working with Mary,
and questions to rate the subject‚Äôs mental workload to interact
with Mary during the task. Although our analysis does not
find any significant difference (p = 0.404), the subjects still
reported some difference in their mental workloads. This is
an interesting result that confirms our hypothesis (i.e., H2):

Fig. 9.

Results for subjective measures. ‚á§ denotes p < 0.05, ‚á§‚á§ denotes p < 0.01, ‚á§ ‚á§ ‚á§ denotes p < 0.001.

although the PS ability enables more effect human-robot
teaming, it also tends to increase the human mental workload
at the same time. It is also worth noting that even though the
subjects in the PS case reported increased mental workload,
they also tended to perform well on the puzzle problems.
This may be due to the fact that subjects felt less necessity
to communicate with Mary and thus can concentrate more
on these problems.
2) Situation Awareness: For situation awareness, we include questions that inquire about whether the subject felt
that he/she had enough information to determine what the
next goal should be. Our analysis does not show a significant
difference (F(1, 14) = 2.78, p = 0.35), although the subjects
reported slightly more situation awareness in the No-PS case,
which is consistent with the side effects of automation in
general. Although the number of updates for the No-PS case
was significantly more than that for the PS case, the fact that
situation awareness of the subject was not reduced much in
the PS case is encouraging. We attribute this to the fact that
the subject still needed to occasionally interact with Mary
when they had goals conflicts, and the subject could gain
situation awareness through such interactions.
3) Complacency: For complacency, we include questions
about the comfort and ease of the teaming, as well as how
well the subject felt about their performance in the task. Our
analysis shows a significant difference (F(1, 14) = 11.29, p <
0.001). This is consistent with the objective performance and
measures, which shows that the human subject generally felt
more satisfied and confident working with Mary in the PS
case. This is important for human-robot teaming.
4) Immediacy, Effectiveness, Likability & Trust: For immediacy, we include questions about how much the subject
considered the simulated task as a realistic USAR task,
and Mary as a teammate. Our analysis shows a significant
difference (F(1, 14) = 11.63, p < 0.001), which is consistent
with our prior results.
For effectiveness, we include questions about the perceived
effectiveness of the team, the balance of workload between
the team members, and whether or not the subject felt that
Mary performed expectedly. Our analysis shows a significant
difference (F(1, 14) = 6.57, p < 0.05). This result suggests

that the proactive support ability indeed increases teaming
effectiveness.
For likability, we include questions about whether the
subject felt that Mary was a good teammate. Our analysis
shows a significant difference (F(1, 14) = 23.26, p < 0.001),
which suggests that the subjects preferred Mary with a PS
ability for teaming.
For trust, we include questions about the evaluation of the
Mary‚Äôs trustworthiness with the assignments (or tasks) she
took and with her updates during the task. Our analysis did
not show any significant difference with F(1, 14) = 3.78, p =
0.072, although subjects in the PS case reported slightly
higher trust.
5) Improvability: For improvability, we include questions
about how much the subject felt that Mary could be improved, and how the subject evaluated his/her interaction
with Mary. Our analysis shows a significant difference for
improvability with F(1, 14) = 17.80, p < 0.001, which, again,
suggests that the subjects preferred Mary with a PS ability.
D. Summary
In summary, our results are mostly consistent with our
hypotheses. Our main result shows that the subjects generally
preferred Mary with a PS ability. With the PS ability,
the human cognitive load was indeed increased (albeit not
significantly), even though the subjects appeared to interact less with Mary. More specifically, while the result on
mental workload confirms our hypothesis, it also seems to
be conflicting with the objective performance on the puzzle
problems. This is likely due to the fact that the subject
felt less necessity to interact with Mary in the PS case.
Furthermore, given that situation awareness was not reduced
significantly in the team with Mary having a PS ability, and
that the subjects had positive feelings towards her, it seems to
suggest that intelligent robots with a PS ability is welcomed
in general. This is, of course, largely dependent on the fact
that the subject‚Äôs cognitive load is not increased significantly,
which may change when the human needs to adapt to the
robot‚Äôs action more frequently in more complex tasks, and
more communication may be needed. More investigations

are needed to be conducted in such scenarios where the task
and robot settings largely differ.
VI. C ONCLUSIONS
In this paper, we aim to start the investigation of humans
factors for proactive support in human-robot teaming. We
start in a simulated USAR task with a general way to
implement the proactive support (PS) ability on a robot in
similar scenarios in which the task is composed of subtasks
with priorities that are dependent on the current situation.
Meanwhile, to maintain the generality of this task, we only
introduced a few necessary simplifications. However, given
the richness of USAR scenarios, more in depth studies are
required to generalize the conclusions to scenarios where
the task and robot settings largely differ. In such cases, our
plan recognition and plan adaptation approaches may also
need to be extended to implement proactive support. Note
that a framework to achieve general proactive support can
be arbitrarily complex depending on the task and level of
support that is needed (e.g., whether the support is active
[13] or passive [5] and whether it is commitment sensitive
or not [3]).
In our task, the human teammate is remotely controlling a
robot while working with an intelligent robot Mary to search
for and treat casualties. Our results show that, in general, the
human teammates prefer to work with a robot that has a PS
ability. However, our results also show that teaming with
PS robots also increases the human‚Äôs cognitive load, albeit
not significantly. This is understandable since working with
a proactive teammate may require more interactions and/or
mental modeling on the human side in order to achieve
better teaming performance. Furthermore, we also show that
situation awareness when working with robots with a PS
ability is not significantly reduced compared to working with
robots without it. This seems to suggest that intelligent robots
with a PS ability is welcomed in general.
R EFERENCES
[1] Samir Alili, Matthieu Warnier, Muhammad Ali, and Rachid Alami.
Planning and plan-execution for human-robot cooperative task
achievement. Proc. of the 19th ICAPS, pages 1‚Äì6, 2009.
[2] Filippo Cavallo, Raffaele Limosani, Alessandro Manzi, Manuele
Bonaccorsi, Raffaele Esposito, Maurizio Di Rocco, Federico Pecora,
Giancarlo Teti, Alessandro Saffiotti, and Paolo Dario. Development
of a socially believable multi-robot solution from town to home.
Cognitive Computation, 6(4):954‚Äì967, 2014.
[3] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula,
Yu Zhang, Matthias Scheutz, David Smith, and Subbarao
Kambhampati. Planning for serendipity. In IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2015.
[4] Eugene Charniak and Robert P. Goldman. A bayesian model of plan
recognition. Artificial Intelligence, 64(1):53 ‚Äì 79, 1993.
[5] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware task planning
for mobile robots. In Advanced Robotics, 2009. ICAR 2009. International Conference on, pages 1‚Äì7, June 2009.
[6] Rina Dechter, Itay Meiri, and Judea Pearl. Temporal constraint
networks. Artificial intelligence, 49(1):61‚Äì95, 1991.
[7] Alberto Finzi, FeÃÅlix Ingrand, and Nicola Muscettola. Model-based
executive control through reactive planning for autonomous rovers.
In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings.
2004 IEEE/RSJ International Conference on, volume 1, pages 879‚Äì
884. IEEE, 2004.

[8] Terrence Fong, Illah Nourbakhsh andClayton Kunz, Lorenzo Fluckiger, John Schreiner, Robert Ambrose, Robert Burridge, Reid Simmons, Laura Hiatt, Alan Schultz, J. Gregory Trafton, Magda Bugajska,
and Jean Scholtz. The peer-to-peer human-robot interaction project.
Space 2005.
[9] Maria Fox and Derek Long. Pddl2. 1: An extension to pddl for
expressing temporal planning domains. J. Artif. Intell. Res.(JAIR),
20:61‚Äì124, 2003.
[10] Alfonso Gerevini and Derek Long. Plan constraints and preferences
in pddl3. The Language of the Fifth International Planning Competition. Tech. Rep. Technical Report, Department of Electronics for
Automation, University of Brescia, Italy, 75, 2005.
[11] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett,
Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott
Penberthy, David E Smith, et al. Pddl-the planning domain definition
language. 1998.
[12] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action
selection for human‚Äìrobot fluency. Robotics, IEEE Transactions on,
23(5):952‚Äì961, 2007.
[13] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on
human-robot teamwork efficiency, fluency, and perception of team. In
Proceedings of the ACM/IEEE international conference on Humanrobot interaction, pages 1‚Äì8. ACM, 2007.
[14] Henry A. Kautz. Reasoning about plans. chapter A Formal Theory
of Plan Recognition and Its Implementation, pages 69‚Äì124. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1991.
[15] Henry A. Kautz and James F. Allen. Generalized Plan Recognition.
In National Conference on Artificial Intelligence, pages 32‚Äì37, 1986.
[16] Uwe KoÃàckemann, Federico Pecora, and Lars Karlsson. Grandpa hates
robots - interaction constraints for planning in inhabited environments.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial
Intelligence, July 27 -31, 2014, QueÃÅbec City, QueÃÅbec, Canada., pages
2293‚Äì2299, 2014.
[17] Steven James Levine and Brian Charles Williams. Concurrent plan
recognition and execution for human-robot teams. In Twenty-Fourth
International Conference on Automated Planning and Scheduling,
2014.
[18] Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and Subbarao
Kambhampati. Automated planning for peer-to-peer teaming and
its evaluation in remote human-robot interaction. In ACM/IEEE
International Conference on Human Robot Interaction (HRI), 2015.
[19] Raja Parasuraman. Designing automation for human use: empirical
studies and quantitative models. Ergonomics, 43(7):931‚Äì951, 2000.
[20] Miquel Ramƒ±ÃÅrez and Hector Geffner. Probabilistic plan recognition
using off-the-shelf classical planners. In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta,
Georgia, USA, July 11-15, 2010, 2010.
[21] Julie Shah, James Wiken, Brian Williams, and Cynthia Breazeal.
Improved human-robot team performance using chaski, a humaninspired plan execution system. In Proceedings of the 6th international
conference on Human-robot interaction, pages 29‚Äì36. ACM, 2011.
[22] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon. A human
aware mobile robot motion planner. IEEE Transactions on Robotics,
2007.
[23] Kartik Talamadupula, Gordon Briggs, Tathagata Chakraborti, Matthias
Scheutz, and Subbarao Kambhampati. Coordination in human-robot
teams using mental modeling and plan recognition. In Intelligent
Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 2957‚Äì2962, Sept 2014.
[24] Stevan Tomic, Federico Pecora, and Alessandro Saffiotti. Too cool
for school - adding social constraints in human aware planning. In
Proceedings of the International Workshop on Cognitive Robotics
(CogRob), 2014.
[25] Vaibhav V Unhelkar, Ho Chit Siu, and Julie A Shah. Comparative
performance of human and mobile robotic assistants in collaborative
fetch-and-deliver tasks. In Proceedings of the 2014 ACM/IEEE
international conference on Human-robot interaction, pages 82‚Äì89.
ACM, 2014.

Plan Explicability and Predictability for
Robot Task Planning

arXiv:1511.08158v2 [cs.AI] 12 Apr 2016

Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti,
Hankz Hankui Zhuo and Subbarao Kambhampati
Abstract‚ÄîIntelligent robots and machines are becoming pervasive in human populated environments. A desirable capability
of these agents is to respond to goal-oriented commands by
autonomously constructing task plans. However, such autonomy
can add significant cognitive load and potentially introduce safety
risks to humans when agents behave unexpectedly. Hence, for
such agents to be helpful, one important requirement is for them
to synthesize plans that can be easily understood by humans.
While there exists previous work that studied socially acceptable
robots that interact with humans in ‚Äúnatural ways‚Äù, and work
that investigated legible motion planning, there lacks a general
solution for high level task planning. To address this issue,
we introduce the notions of plan explicability and predictability.
To compute these measures, first, we postulate that humans
understand agent plans by associating abstract tasks with agent
actions, which can be considered as a labeling process. We learn
the labeling scheme of humans for agent plans from training
examples using conditional random fields (CRFs). Then, we use
the learned model to label a new plan to compute its explicability
and predictability. These measures can be used by agents to
proactively choose or directly synthesize plans that are more
explicable and predictable to humans. We provide evaluations
on a synthetic domain and with human subjects using physical
robots to show the effectiveness of our approach.

I. I NTRODUCTION
Intelligent robots and machines are becoming pervasive in
human populated environments. Examples include robots for
education, entertainment and personal assistance just to name
a few. Significant research efforts have been invested to build
autonomous agents to make them more helpful. These agents
respond to goal specifications instead of basic motor commands, which requires them to autonomously synthesize task
plans and execute those plans to achieve the goals. However,
if the behaviors of these agents are incomprehensible, it can
increase the cognitive load of humans and potentially introduce
safety risks to them.
As a result, one important requirement for such intelligent
agents is to ensure that the synthesized plans are comprehensible to humans. This means that instead of considering
only the planning model of the agent, plan synthesis should
also consider the interpretation of the agent behavior from
the human‚Äôs perspective. This interpretation is related to our
modeling of other agents. More specifically, we tend to have
expectations of others‚Äô behaviors based on our understanding
(modeling) of their capabilities, mental states and etc. If their
behaviors do not match with these expectations, we would
often be confused. One of the major reasons of this confusion
is due to the fact that our understanding of others‚Äô models
is often partial and inaccurate. This is also true when humans

interact with intelligent agents. For example, to darken a room
that is too bright, a robot can either adjust the window blinds,
switch off the lights, or break the light bulbs in the room.
While breaking the light bulbs may well be the least costly
plan to the robot under certain conditions (e.g., when the robot
cannot easily move in the environment but we are unaware
of it), it is clear that the other two options are far more
desirable in the context of robots cohabiting with humans.
One of the challenges here is that the human‚Äôs understanding
of the agent model is inherently hidden. Thus, its interpretation
from the human‚Äôs perspective can be arbitrarily different from
the agent‚Äôs own model. While there exists previous work that
studied socially acceptable robots [11, 12, 21, 18] that interact
with humans in ‚Äúnatural ways‚Äù, and work that investigated
legible motion planning [6], there lacks a general solution for
high level task planning.
In this paper, we introduce the notions of plan explicability
and predictability which are used by autonomous agents (e.g.,
robots) to synthesize ‚Äúexplicable plans‚Äù that can be easily
understood by humans. Our problem settings are as follows:
an intelligent agent is given a goal by a human (so that the
human knows the goal of the agent) working in the same
environment and it needs to synthesize a plan to achieve the
goal. As suggested in psychological studies [24, 5], we assume
that humans naturally interpret a plan as achieving abstract
tasks (or subgoals), which are functional interpretations of
agent action sequences in the plan. For example, a robot
that executes a sequence of manipulation actions may be
interpreted as achieving the task of ‚Äúpicking up cup‚Äù. Based
on this assumption, intuitively, the easier it is for humans to
associate tasks with actions in a plan, the more explicable
the plan is. Similarly, the easier it is to predict the next
task given actions in the previous tasks, the more predictable
the plan is. In this regard, explicability is concerned with
the association between human-interpreted tasks and agent
actions, while predictability is concerned with the connections
between these abstract tasks.
Since the association between tasks and agent actions can
be considered as a labeling process, we learn the labeling
scheme of humans for agent plans from training examples
using conditional random fields (CRFs). We then use the
learned model to label a new plan to compute its explicability
and predictability. These measures are used by agents to
proactively choose or directly synthesize plans that are more
explicable and predictable without affecting the quality much.
Our learning approach does not assume any prior knowledge

Fig. 1.
From left to right, the scenarios illustrate the differences between
automated task planning, human-aware planning and explicable planning (this
work). In human-aware planning, the robot needs to maintain a model of the
human (i.e., MH ) which captures the human‚Äôs capabilities, intents and etc.
In explicable planning, the robot considers the differences between its model
from the human‚Äôs perspective (i.e., M‚àóR ) and its own model MR .

on the human‚Äôs interpretation of the agent model. We provide
evaluation on a synthetic domain in simulation and with human
subjects using physical robots to demonstrate the effectiveness
of our approach.
II. R ELATED W ORK
To build autonomous agents (e.g., robots), one desirable
capability is for such agents to respond to goal-oriented
commands via automated task planning. A planning capability
allows agents to autonomously synthesize plans to achieve a
goal given the agent model (MR as shown in the first scenario
in Fig. 1) instead of following low level motion commands,
thus significantly reducing the human‚Äôs cognitive load. Furthermore, to work alongside of humans, these agents must
be ‚Äúhuman-aware‚Äù when synthesizing plans. In prior works,
this issue is addressed under human-aware planning [22, 4, 2]
in which agents take the human‚Äôs activities and intents into
account when constructing their plans. This corresponds to
human modeling in human-aware planning as shown in the
second scenario in Fig. 1. A prerequisite for human-aware
planning is a plan recognition component, which is used
to infer the human‚Äôs goals and plans. This information is
then used to avoid interference, and plan for serendipity and
teaming with humans. There exists a rich literature on plan
recognition [14, 3, 20, 16], and many recent works use these
techniques in human-aware planning and human-robot teaming
[23, 2, 25].
While our work on plan explicability and predictability
falls within the scope of human-in-the-loop planning (which
also includes human-aware planning), it differs significantly
from the previous work. This is illustrated in Fig. 1. More
specifically, in human-aware planning, the challenge is to
obtain the human model (MH in Fig. 1) which captures human
capabilities [26], intents [23, 2] and etc. The modeling in this
work is one level deeper: it is about the interpretation of the
agent model from the human‚Äôs perspective (M‚àóR in Fig. 1).
In other words, R needs to understand the model of itself in
H‚Äôs eyes. This information is inherently hidden, difficult to

convey, and can be arbitrarily different (e.g., having different
representations) from R‚Äôs own model (MR in Fig. 1).
There exists work on generating legible robot motions [6]
which considers a similar issue in motion planning. We are,
on the other hand, concerned with task planning. Note that
two different task plans may map to exactly the same motions
which can be interpreted vastly differently by humans. In such
cases, considering only motion becomes insufficient. Nevertheless, there exists similarities between [6] and our work. For
example, legibility there is analogous to predictability in ours.
In the human-robot interaction (HRI) community, there
exists prior works that discuss how to enable natural and
fluent human-robot interaction [11, 12, 21, 18] to create more
socially acceptable robots [7]. These works, however, apply
only to behaviors in specific domains. Compared with model
learning via expert teaching, such as inverse reinforcement
learning [1] and tutoring systems [17], which is about learning
the ‚Äúright‚Äù model from teachers, our work, on the other hand,
is concerned with learning model differences. Furthermore,
as an extension to our work, when robots cannot find an
explicable plan that is also cost efficient, they need to explain
the situation. In this regard, our work is also related to excuse
[9] and explanation generation [10]. Finally, while our learning
approach appears to be similar to information extraction [19],
we use the learned model to proactively guide planning instead
of passively extracting information.
III. E XPLICABILITY AND P REDICTABILITY
In our settings, an agent R needs to achieve a goal given
by a human in the same environment (so that the human
knows about the goal of the robot). The agent has a model
of itself (referred to as MR ) which is used to autonomously
construct plans to achieve the goal. In this paper, we assume
that this model is based on PDDL [8], a general planning
domain definition language. As we discussed, for an agent
to generate explicable and predictable plans, it must not only
consider MR but also M‚àóR , which is the interpretation of MR
from the human‚Äôs perspective.
A. Problem Formulation
Keeping the problem settings in mind, given a domain, the
problem is to find a plan for a given goal that satisfies the
following:
argmin cost(œÄMR ) + Œ± ¬∑ dist(œÄMR , œÄM‚àóR )

(1)

œÄ MR

where œÄMR is a plan that is constructed using MR (i.e., the
agent‚Äôs plan), œÄM‚àóR is a plan that is constructed using M‚àóR
(i.e., the human‚Äôs anticipation of the agent‚Äôs plan), cost returns
the cost of a plan, dist returns the distance (i.e., capturing the
differences) between two plans, and Œ± is the relative weight.
The goal of Eq. (1) is to find a plan that minimizes a weighted
sum of the cost of the agent plan and the differences between
the two plans. Since the agent model MR is assumed to be
given, the challenge lies in the second part in Eq. (1).
Note that if we know M‚àóR or it can be learned, the only
thing left would be to search for a proper dist function.

However, as discussed previously, M‚àóR is inherently hidden,
difficult to convey, and can be arbitrarily different from MR .
Hence, our solution is to use a learning method to directly
approximate the returned values. We postulate that humans
understand agent plans by associating abstract tasks with
actions, which can be considered as a labeling process. Based
on this, we assume that dist(œÄMR , œÄM‚àóR ) can be functionally
decomposed as:

2) Predictability Labeling: Predictability is concerned with
the connections between tasks in a plan. An action label for
predictability is composed of two parts: a current label and a
next label (i.e., L√óL). The current label is also the action label
for explicability. The next label (similar to the current label)
is used to specify the tasks that are anticipated to be achieved
next. A next label with multiple task labels is interpreted as
having multiple candidate tasks to achieve next; when this
label is the empty set, it is interpreted as that the next task is
dist(œÄMR , œÄM‚àóR ) = F ‚ó¶ L‚àó (œÄMR )
(2) unpredictable, or there are no more tasks to be achieved.
Definition 2 (Plan Predictability): Given a domain, the
where F is a domain specific function that takes plan labels as
predictability Œ≤œÄ of a plan œÄ is computed by a mapping,
‚àó
input, and L is the labeling scheme of the human for agent
FŒ≤ : L2œÄ ‚Üí [0, 1] (with 1 being the most predictable).
plans based on M‚àóR . As a result, Eq. (1) now becomes:
L2œÄ denotes the sequence of action labels for predictability. An
‚àó
‚àó i
argmin cost(œÄMR ) + Œ± ¬∑ F ‚ó¶ LCRF (œÄMR |{Si |Si = L (œÄMR )}) example of FŒ≤ is given below which is used in our evaluation
œÄ MR
when assuming that the current and next labels are associated
(3) with at most one task label:
P
where {Si } is the set of training examples and L‚àóCRF is
1
‚àß (1L2 (ai )=L(aj ) ‚à® 1L2 (ai:N )=‚àÖ )
the learned model of L‚àó . We can now formally define plan FŒ≤ (L2œÄ ) = i‚àà[0,N ] |L(ai )|=1
N +1
explicability and predictability in our context. Given a plan of
(8)
agent R as a sequence of actions, we denote it as œÄMR and where a (j > i) is the first action that has a different
j
simplified below as œÄ for clarity:
current label as ai or the last action in the plan if no such
2
œÄ = ha0 , a1 , a2 , ...aN i
(4) action is unfound, L (ai ) returns the next label of ai and
1L2 (ai:N )=‚àÖ returns 1 only if the next labels for all actions after
where a0 is a null action that denotes plan starting. Given the ai (including ai ) are ‚àÖ. Eq. (8) computes the ratio between
domain, we assume that a set of task labels T is provided to number of actions that we have correctly predicted the next
task and the number of all actions.
label agent actions:
T = {T1 , T2 , ...TM }

(5)

1) Explicability Labeling: Explicability is concerned with
the association between abstract tasks and agent actions; each
action in a plan is associated with an action label. The set
of action labels for explicability is the power set of the task
labels:
L = 2T
(6)
When an action label includes multiple task labels, the action
is interpreted as contributing to multiple tasks; when an action
label is the empty set, the action is interpreted as inexplicable.
When a plan is labeled, we can compute its explicability
measure based on its action labels in a domain specific way.
More specifically, we define:
Definition 1 (Plan explicability): Given a domain, the explicability Œ∏œÄ of an agent plan œÄ is computed by a mapping,
FŒ∏ : LœÄ ‚Üí [0, 1] (with 1 being the most explicable).
LœÄ above denotes the sequence of action labels for œÄ. An
example of FŒ∏ used in our evaluation is given below:
P
i‚àà[1,N ] 1L(ai )6=‚àÖ
(7)
FŒ∏ (LœÄ ) =
N
where N is the plan length, L(ai ) returns the action label of
ai , and 1f ormula is an indicator function that returns 1 when
the f ormula holds or 0 otherwise. Eq. (7) basically computes
the ratio between the number of actions with non-empty action
labels and the number of all actions.

B. A Concrete Example
Before discussing how to learn the labeling scheme of the
human from training examples, we provide a concrete example
to connect the previous concepts and show how training
examples can be obtained. In this example, there is a rover
in a grid environment working with a human. An illustration
of this example is presented in Fig. 2. There are resources
to be collected which are represented as boxes. There is one
storage area that can store one resource which is represented
as an open box. The rover can also make observations. The
rover actions include {navigate lf rom lto }, {observe l} {load
l}, and {unload l}, each representing a set of actions since
l (i.e., representing a location) can be instantiated to different
locations (i.e., 0 ‚àí 8 in Fig. 2). navigate (or nav) can move
the rover from a location to one of its adjacent locations; load
can be used to pick up a resource when the rover is not already
loaded; unload can be used to unload a resource at a storage
area if the area is empty; observe (or obs) can be used to
make an observation. Once a location is observed, it remains
observed. The goal in this example is for the rover to make the
storage area non-empty and observe two locations that contain
the eye symbol in Fig. 2.
In this domain, we assume that there are three abstract tasks
that may be used by the human to interpret the rover‚Äôs plans:
COLLECT (C), STORE (S) and OBSERVE (O). Note that
we do not specify any arguments for these tasks (e.g., which
resource the rover is collecting) since this information may not
be important to the human. This also illustrates that MR and

fields (CRFs) [15] due to their abilities to model sequential
data. An alternative would be HMMs; however, CRFs have
been shown to relax assumptions about the input and output
sequence distributions and hence are more flexible.
The distributions that are captured by CRFs have the following form:
p(x, y) =

Fig. 2. Example for plan explicability and predictability with action labels
(on the right) for a given plan in the rover domain.

1
Œ†A Œ¶(xA , yA )
Z

in which Z is a normalization factor that satisfies:
X
Z=
Œ†A Œ¶(xA , yA )

(9)

(10)

x,y

M‚àóR can be arbitrarily different. In Fig. 2, we present a plan
of the rover as connected arrows starting from the its initial
location.
Human Interpretation as Training Examples: Let us now
discuss how humans may interpret this plan (i.e., associating
labels with actions) as the actions are observed incrementally:
when labeling ai , we only have access to the plan prefix
ha0 , ..., ai i. At the beginning for labeling a0 , the observation
is that the rover starts at l5 . Given the environment and
knowledge of the rover‚Äôs goal, we may infer that the first task
should be COLLECT (the resource from l4 ). Hence, we may
choose to label a0 as ({START}, {C}). The first action of
the rover (i.e., nav l5 l4 ) seems to match with our prediction.
Furthermore, given that the storage area is closest to the rover‚Äôs
location after completing COLLECT, the next task is likely to
be STORE. Hence, we may label a1 as ({C}, {S}) as shown
in the figure. The second action (i.e., load l4 ) also matches
with our expectation. Hence, we label a2 too as ({C}, {S}).
The third action, nav l4 l1 , however, is unexpected since we
predicted STORE in the previous steps. Nevertheless, we can
still explain it as contributing to OBSERVE (at location l0 ).
Hence, we may label this navigation action (a3 ) as ({O}, {S}).
For the fourth action, the rover moves back to l4 , which is
inexplicable since the rover‚Äôs behavior seems to be oscillating
without particular reasons. Hence, we may choose to label this
action as (‚àÖ, ‚àÖ). The labeling for the rest of the plan continues
in a similar manner. This thought process reflects how training
examples can be obtained from human labelers.
IV. L EARNING A PPROACH
To compute Œ∏œÄ and Œ≤œÄ from Defs. (1) and (2) for a given
plan œÄ, the challenge is to provide a label for each action.
This requires us to learn the labeling scheme of humans (i.e.,
L‚àó in Eq. (2)) from training examples and then apply the
learned model to œÄ (i.e., L‚àóCRF in Eq. (3)). To formulate a
learning method, we consider the sequence of labels as hidden
variables. The plan that is executed by the agent (which also
captures the state trajectory), as well as any cognitive cues
that may be obtained (e.g., from sensing) during the plan
execution constitute the observations. The graphical model that
we choose for our learning approach is conditional random

In the equations above, x represents the sequence of observations, y represents the sequence of hidden variables, and
Œ¶(xA , yA ) represents a factor that is related to a subgraph in
the CRF model associated with variables xA and yA . In our
context, x are the observations made during the execution of a
plan; y are the action labels. Each factor is associated with a
set of features that can be extracted during the plan execution.
Next, we discuss some possible features that can be used for
plan explicability and predictability.
A. Features for Learning
Given an agent plan, the immediate set of features that we
have access to is the plan and its associated state trajectory.
Note that the human may not be required (nor it is necessary)
to fully understand this information. When the dynamics of the
agent are known, given the plan, it may also be possible to
derive low level motor commands that implement the motions,
which can be used to extract motion related features.
When the agent is equipped with sensors such as cameras
and lasers, we can also extract features from sensor information. For example, from video streams and depth information,
we can extract features about the environment, e.g., how
crowded the workspace is.
Sensor information can also be used to extract dynamic
features such as the location of the human. However, note
that this information will not be available during the testing
phase, and thus these features need to be estimated based on
other information (e.g., projected plan of the human based on
plan recognition techniques [20, 16]).
In this work, we use a linear chain CRF. However, our
formulation is easily extensible to more general types of
CRFs. Given an agent plan œÄ = ha0 , a1 , a2 , ...i, each action is
associated with a set of features. Hence, each training example
is of the following form:
h(F0 , L20 ), (F1 , L21 ), (F2 , L22 ), ...i

(11)

where L2i is the action label for predictability (and explicability) for ai . Fi is the set of features for ai . We discuss several
feature categories in more detail below:

1) Plan Features: Given the agent model (specified in
PDDL), the set of plan features for ai includes the action
description and the state variables after executing the sequence
of actions ha0 , ..., ai i from the initial state. This information
can be easily extracted given the model. For example, in our
rover example in Fig. 2, this set of features for a1 includes
navigate, at rover l4 , at resource0 l2 , at resource1 l4 , at
storage0 l3 .
2) Action Features: Action features for ai describes the
motion (e.g., dynamics) of this action. These features can
be used to capture, for example, smoothness of execution
within and across actions. Action features sometimes serve
as important cognitive cues for humans to understand agent
actions. For example, an action that enables a robot to cross a
river may be interpreted as swimming, pedaling, or propelling
depending on how the robot motion looks like. Action features
can be extracted for a plan given the dynamics of the robot.
3) Interaction Features: Interaction features are intended
to capture ai ‚Äôs influence on the human. For example, it can
include how far the agent is from human and what the human
is performing when ai is being executed. In other words,
this set of features captures characteristics of the interactions
between the human and agent. Interaction features can be
extracted from sensor information or estimated based on the
projected human plan.
B. Using the Learned Model
Given a set of training examples in the form of Eq. (11),
we can train the CRF model to learn the labeling scheme in
Eq. (3). We discuss two ways to use the learned CRF model.
1) Plan Selection: The most straightforward method is to
perform plan selection on a set of candidate plans which can
simply be a set of plans that are within a certain cost bound
of the optimal plan. Candidate plans can also be generated
to be diverse with respect to various plan distances. For each
plan, the agent must first extract the features of the actions
as we discussed earlier. It then uses the trained model (i.e.,
L‚àóCRF ) to produce the labels for the actions in the plan. Œ∏ and
Œ≤ can then be computed given the mappings in Defs. (1) and
(2). These measures can then be used to choose a plan that is
more explicable and predictable.
2) Plan Synthesis: A more efficient way is to incorporate
these measures as heuristics into the planning process. Here,
we consider the FastForward (FF) planner with enforced hill
climbing [13]. To compute the heuristic value given a planning
state, we use the relaxed planning graph to construct the remaining planning steps. However, since relaxed planning does
not ensure a valid plan, we can only use action descriptions as
plan features for actions that are beyond the current planning
state when estimating the Œ∏ and Œ≤ measures. These estimates
are then combined with the relaxed planning heuristic (which
only considers plan cost) to guide the search. The algorithm
for generating explicable and predictable plans is presented in
Alg 1.
The capability to synthesize explicable and predictable plans
is useful for autonomous agents. For example, in domains

Algorithm 1 Synthesizing Explicable and Predictable Plans
Input: agent model MR , trained human labeling scheme
L‚àóCRF , initial state I and goal state G.
Output: œÄEXP
1: Push I into the open set O.
2: while open set is not empty do
3:
s = GetNext(O).
4:
h‚àó = M AX.
5:
if G is reached then
6:
return s.plan (i.e., the plan that leads to s from I).
7:
end if
8:
Compute all possible next states N from s.
9:
for n ‚àà N do
10:
Compute the relaxed plan œÄRELAX for n.
11:
Concatenate s.plan (with plan features) with
œÄRELAX (with only action descriptions) as œÄÃÑ.
12:
Compute and add other relevant features.
13:
Compute L2œÄ = L‚àóCRF (œÄÃÑ).
14:
Compute Œ∏ and Œ≤ based on L2œÄ for œÄÃÑ.
15:
Compute h = f (Œ∏, Œ≤, hcost ) (f is a combination
function; hcost is the relaxed planning heuristic).
16:
end for
17:
Find the state n‚àó ‚àà N with the minimum h.
18:
if h(n‚àó ) < h‚àó then
19:
Clear O.
20:
Push n‚àó into O.
21:
else
22:
Push all n ‚àà N into O.
23:
end if
24: end while

where humans interact closely with robots (e.g., in an assembly warehouse), more preferences should be given to plans
that are more explicable and predictable since there would be
high risks if the robots act unexpectedly. One note is that the
relative weights of explicability and predictability may vary in
different domains. For example, in domains where robots do
not engage in close interactions with humans, predictability
may not matter much.
V. E VALUATION
We first evaluate our approach systematically on a synthetic
dataset based on the rover domain. Then, we evaluate it with
human subjects using physical robots to validate that the
synthesized plans are more explicable to humans in a blocks
world domain.
A. Systematic Evaluation with a Synthetic Domain
The aim is twofold here: evaluate how well the learning
approach can capture an arbitrary labeling scheme; evaluate
the effectiveness of plan selection and synthesis with respect
to the Œ∏ and Œ≤ measures.
1) Dataset Synthesis: To simplify the data synthesis process, we make the following assumptions: all rover actions
have the same cost; all rover actions are associated with at

most one task label (i.e., L = T ‚à™{‚àÖ} in Eq. (6)). To construct
a domain in which the optimal plan (in terms of cost) may not
be the most explicable (in order to differ MR from M‚àóR ), we
add ‚Äúoscillations‚Äù to the plans of the rover. These oscillations
are incorporated by randomly adding locations for the rover to
visit as hidden goals. For these locations, the rover only needs
to visit them. As a result, it may demonstrate ‚Äúunexpected‚Äù
behaviors given only the public goal, denoted by G, which
is known to both the rover and human. We denote the goal
that also includes the hidden goals as G0 . Given a problem
with a public goal G, we implement a labeling scheme to
automatically provide the ‚Äúground truth‚Äù of a rover plan, which
is constructed by the rover to achieve G0 .
Given a plan of the rover, we label it incrementally by associating each action with a current and next label. These labels
are chosen from {{COLLECT}, {STORE}, {OBSERVE}, ‚àÖ}.
We denote the plan prefix ha0 , ...ai i for a plan œÄ as œÄi , the
state after applying œÄi as si from the initial state, and a plan
that is constructed from si to achieve G (i.e., using si as the
initial state) as P (si ). For the current label of ai :
1) If |P (si )| ‚â• |P (si‚àí1 )|, we label ai as ‚àÖ (i.e., inexplicable). This rule means that humans may label an action
as inexplicable if it does not contribute to achieving G.
2) If |P (si )| < |P (si‚àí1 )|, we label ai based on the
distances from the current rover location to the targets
(i.e., storage areas or observation locations), current state
of the rover (i.e., loaded or not), and whether ai moves
the rover closer to these targets. For example, if the
closest target is a storage area and the rover is loaded, we
label ai as {STORE}. When there are ties, we label ai
as ‚àÖ (i.e., unclear and hence interpreted as inexplicable).
For the next label of ai :
1) This label is determined by the target that is closest to
the rover state after the current task is achieved. When
there are ties, ai is labeled as ‚àÖ (i.e., unclear and hence
interpreted as unpredictable). If the current label is ‚àÖ,
we also label ai as ‚àÖ (i.e., unpredictable).
2) If the current task is also the last task, we label ai as ‚àÖ
since there is no next task.
For evaluation, we define FŒ∏ and FŒ≤ as in Eqs. (7) and (8).
We randomly generate problems in a 4 √ó 4 environment. For
each problem, we randomly generate 1 ‚àí 3 resources as a set
RE, 1‚àí3 storage areas as a set ST, 1‚àí3 observation locations
as a set OB. The public goal G of a problem, first, includes
making all storage areas non-empty. To ensure a solution, we
force |RE| = |ST | if |RE| < |ST |. Furthermore, the rover
must make observations at the locations in OB. G0 for the
rover includes G above, as well as a set of hidden goals.
Locations of the rover, RE, ST, OB and hidden goals are
randomly generated in the environment and do not overlap
in the initial state. Although seemingly simple, the state space
of this domain is on the order of 1020 .
2) Results: We use only plan features here. First, we
evaluate our approach to learning the labeling scheme (i.e.,
L‚àóCRF ) as the difference between MR and M‚àóR gradually

Fig. 3. Evaluation for predicting Œ∏ and Œ≤ measures as the difference between
MR and M‚àóR increases (i.e., as the maximum number of hidden goals
increases).

increases (i.e., as the number of hidden goals increases).
Afterwards, we evaluate the effectiveness of plan selection and
synthesis with respect to the Œ∏ and Œ≤ measures. To verify that
our approach can generalize to different problem settings, we
fix the level of oscillation when generating training samples
while allowing it to vary in testing samples.
Using CRFs for Plan Explicability and Predictability: In
this evaluation, we randomly generate 1 ‚àí 3 hidden goals to
include in G0 in 1000 training samples. After the model is
learned, we evaluate it on 100 testing samples in which we
vary the maximum number of hidden goals from 1 to 6 with
step size 1. The result is presented in Fig. 3. We can see
that the prediction performance (i.e., the ratios between Œ∏ and
Œ≤ computed based on L‚àóCRF and L‚àó ) is generally between
50% ‚àí 150%, We can also see that the oscillation level does
not seem to influence the prediction performance much. This
shows that our approach is effective whether MR and M‚àóR are
similar or largely different.
Selecting Explicable and Predictable Plans: We evaluate
plan selection using Œ∏ and Œ≤ measures and compare the selected plans (denoted by EXPD-SELECT) with plans selected
by a baseline approach (denoted by RAND-SELECT). Given
a set of candidate plans, EXPD-SELECT selects a plan according to the highest predicted explicability or predictability
measure while RAND-SELECT randomly selects a plan from
the set of candidate plans. To implement this, for a given
public goal G, we randomly construct 20 problems with a
given level of oscillation as determined by the maximum
number of hidden goals. Each such problem corresponds to
a different G0 and a plan is created for it. The set of plans
for these 20 problems associated with the same G is the set
of candidate plans for G. For each level of oscillation, we
randomly generate 50 different Gs and then construct the set
of candidate plans for each G. The model here is trained with
1900 samples using the same settings as in our first evaluation
and we gradually increase the level of oscillation.
We compare the Œ∏ and Œ≤ values computed from the ground
truth labeling of the chosen plans. The result is provided in
Fig. 4. When the oscillation is small, the performances of
both approaches are similar. As the oscillation increases, the
performances of the two approaches diverge. This is expected
since RAND-SELECT randomly chooses plans and hence its
performance should decrease as the oscillation increases. On

Fig. 4.

Comparison of EXPD-SELECT and RAND-SELECT

the other hand, EXPD-SELECT is not influenced as much
although its performance also tends to decrease. This is partly
due to the fact that the model used in this evaluation is trained
with samples having a maximum of 3 hidden goals.
In Fig. 4 for explicability, almost all results are significantly
different at 0.001 level (except at 1); for predictability, results
are significantly different at 0.01 level at 3, 5 and 6. The
trend to diverge is clearly present. Note that we use linearchain CRFs in our evaluations, which does not directly model
correlations among observations across states. These features
are common in our rover domain (e.g., navigating back and
forth). Hence, we can anticipate performance improvement
with more general CRFs.
Synthesizing Explicable and Predictable Plans: We evaluate
here plan synthesis using Alg. 1. More specifically, we compare FF planner that considers the predicted Œ∏ and Œ≤ values in
its heuristics with a normal FF planner that only considers the
action cost. The FF planner with the new heuristic is called
FF-EXPD. In this evaluation, we set the maximum number of
hidden locations to visit to be 6. For each trial, we generate
100 problems and apply both FF and FF-EXPD to solve the
problems. Given that we are interested in comparing the cases
when explicability is low, we only consider problems when
the predicted plan explicability for the plan generated by FF
is below 0.85.
First, we consider the incorporation of Œ∏ only. The result
is presented in Fig. 5. For the explicability measure, we see
a significant difference in all trials. Another observation is
that the difference in plan predictability is present but not as
significant. This evaluation suggests that our heuristic search
can produce plans of high explicability.
Next, we consider the incorporation of Œ≤ only. The result is
presented in Fig. 6. Similarly, we see a significant difference
in all trials for both explicability and predictability. One observation is that improving on plan predictability also improves
plan explicability which is expected given Eqs. (7) and (8)).
Plan Cost: We consider plan cost here for the evaluation in
Fig. 6. The result is presented in Table I. We can see that the
plan length for FF-EXPD is longer than the plan produced by
FF in general. This is expected since FF only considers plan
cost. However, in all settings, FF-EXPD penalizes the plan
cost slightly (about 10%) to improve the plan explicability
and predictability measures.

Fig. 5.

Comparison of FF and FF-EXPD considering only Œ∏.

Fig. 6.

Comparison of FF and FF-EXPD considering only Œ≤.
TABLE I
P LAN S TEPS C OMPARISON FOR F IG . 6

Trial ID
FF (avg. # steps)
FF-EXPD (avg. # steps)

1
21.9
23.5

2
24.0
26.3

3
24.1
25.2

4
23.9
24.0

5
22.1
23.4

6
22.4
25.0

B. Evaluation with Physical Robots
In this section we evaluate our approach in a blocks
world domain with a physical robot. It simulates a smart
manufacturing environment where robots are working beside
humans. Although the human and robot do not have direct
interactions ‚Äì the robot‚Äôs goal is independent of the human‚Äôs,
generating explicable plan is still an important issue since it
will help humans concentrate more on their own tasks. Here,
we evaluate plans generated by the robot using FF-EXPD and
a cost-optimal planner (OPT) in various scenarios and compare
the plans with human subjects in terms of explicability.
1) Domain Description: In this domain, the robot‚Äôs goal
(which is known to the human) is to build a tower of a
certain height using blocks on the table. The towers to be
built have different heights in different problems. There are
two types of blocks, light ones and heavy ones, which are
indistinguishable externally but the robot can identify them
based on the markers. Picking up the heavy blocks are more
costly than the light blocks for the robot. Hence, the robot may
sometimes choose seemingly more costly (i.e., longer) plans
to build a tower from the human‚Äôs perspective.
2) Experimental Setup: We generated a set of 23 problems
in this domain in which towers of height 3 are to be built. The
plans for these problems were manually generated and labeled
as the training set. For 4 out of these 23 problems, the optimal
plan is not the most explicable plan. To remove the influence of

grounding, we also generated permutations of each plan using
different object names for these 23 problems, which resulted
in a total of about 15000 training samples. We then generated a
set of 8 testing problems for building towers of various heights
(from 3‚àí5) to verify that our approach can generalize. Testing
problems were generated only for cases where plans are more
likely to be inexplicable. For each problem, we generated
two plans, one using OPT and the other using FF-EXPD,
and recorded the execution of these plans on the robot. We
recruited 13 subjects on campus and each human subject was
tasked with labeling two plans (generated by OPT and FFEXPD respectively) for each of the 8 testing problems, using
the recorded videos and following a process similar to that
used in preparing training samples. After labeling each plan,
we also asked the subject to provide a score (1 ‚àí 10 with 10
being the most explicable) to describe how comprehensible
the plan was overall.
3) Results: In this evaluation, we only use one task label
‚Äúbuilding tower‚Äù. For all testing problems, the labeling
process results in 77.8% explicable actions (i.e., actions with
a task label) for OPT and 97.3% explicable actions for FFEXPD. The average explicability measures for FF-EXPD and
OPT are 0.98 and 0.78, and the average scores are 9.65 and
6.92, respectively. We analyze the results using a paired Ttest which shows a significant difference between FF-EXPD
and OPT in terms of the explicability measures (using Eq.
(7)) computed from the human labels and the overall scores
(p < 0.001 for both). Furthermore, after normalizing the
scores from the human subjects, the Cronbach‚Äôs Œ± value shows
that the explicability measures and the scores are consistent
for both FF-EXPD and OPT (Œ± = 0.78, 0.67, respectively).
These results verify that: 1) our explicability measure does
capture the human‚Äôs interpretation of the robot plans and 2)
our approach can generate plans that are more explicable to
humans. In Fig. 7, we present the plans for a testing scenario.
The left part of the figure shows the plan generated by OPT
and the right part shows the plan generated by FF-EXPD. A
video is also attached showing the different behaviors with the
two planners in this scenario.
VI. C ONCLUSION
While we are still far from having intelligent robots and
agents working side-by-side of humans as teammates (rather
than as tools), it becomes increasingly important to consider
issues when such autonomous agents appear in our everyday
life. These agents need to create and execute complex plans. In
this paper, we introduced plan explicability and predictability
for such agents so that they can synthesize plans that are more
comprehensible to humans. To achieve this, they must consider
not only their own models but also the human‚Äôs interpretation
of their models. To the best of our knowledge, this is the
first attempt to model plan explicability and predictability for
task planning which differs from previous work on humanaware planning. The proposed measures have a variety of applications (e.g., achieving fluent human-robot interaction and
ensuring human safety). To compute these measures, we learn

Fig. 7. Plan execution of two plans generated by OPT (left) and FF-EXPD
(right) for one out of the 8 testing scenarios. The top figure shows the setup
of this scenario where the goal is to build a tower of height 3. The block that
is initially on the left side of the table is a heavy block. The optimal plan
involves more actions with the light blocks (i.e., putting the two light blocks
on top of the heavy one) while the explicable plan is more costly since it
requires moving the heavy one.

the labeling scheme of humans for agent plans from training
examples based on CRFs. We then use this learned model to
label a new plan to compute its explicability and predictability.
The proposed approach is evaluated on a synthetic domain
and with human subjects using physical robots to show its
effectiveness. A natural extension of our work is to consider
human-robot teaming where there exists close interactions.
Humans in our current settings are observers.
Finally, while we focus on the explicability and predictability measures for robot task planning, they also have many other
interesting applications. For example, many defense applications use planning to create unpredictable and inexplicable
plans, which can help deter or confuse enemies and are also
useful for testing defenses against novel or unexpected attacks.
These applications can be implemented using our approach by
minimizing the Œ∏ and Œ≤ measures instead of maximizing them.

R EFERENCES
[1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
of the Twenty-first International Conference on Machine
Learning, ICML ‚Äô04, 2004.
[2] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith,
and Subbarao Kambhampati. Planning for serendipity. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2015.
[3] Eugene Charniak and Robert P. Goldman. A bayesian
model of plan recognition. Artificial Intelligence, 64(1):
53 ‚Äì 79, 1993. ISSN 0004-3702.
[4] M. Cirillo, L. Karlsson, and A. Saffiotti. Human-aware
task planning for mobile robots. In Advanced Robotics,
2009. ICAR 2009. International Conference on, pages
1‚Äì7, June 2009.
[5] Gergely Csibra and GyoÃàrgy Gergely. Obsessed with
goals?: Functions and mechanisms of teleological interpretation of actions in humans. Acta psychologica, 124
(1):60‚Äì78, 2007.
[6] Anca Dragan and Siddhartha Srinivasa. Generating
legible motion. In Proceedings of Robotics: Science and
Systems, Berlin, Germany, June 2013.
[7] Terrence W Fong, Illah Nourbakhsh, and Kerstin Dautenhahn. A survey of socially interactive robots. Robotics
and Autonomous Systems, 2003.
[8] Maria Fox and Derek Long. Pddl2.1: An extension to
pddl for expressing temporal planning domains. J. Artif.
Int. Res., 20(1), December 2003.
[9] Moritz Gbelbecker, Thomas Keller, Patrick Eyerich,
Michael Brenner, and Bernhard Nebel. Coming up with
good excuses: What to do when no plan can be found.
In International Conference on Automated Planning and
Scheduling, 2010.
[10] Marc Hanheide, Moritz Gbelbecker, Graham S. Horn,
Andrzej Pronobis, Kristoffer Sj, Alper Aydemir, Patric
Jensfelt, Charles Gretton, Richard Dearden, Miroslav
Janicek, Hendrik Zender, Geert-Jan Kruijff, Nick Hawes,
and Jeremy L. Wyatt. Robot task planning and explanation in open and uncertain worlds. Artificial Intelligence,
2015. ISSN 0004-3702.
[11] Guy Hoffman and Cynthia Breazeal. Cost-based anticipatory action selection for human‚Äìrobot fluency. Robotics,
IEEE Transactions on, 23(5):952‚Äì961, 2007.
[12] Guy Hoffman and Cynthia Breazeal. Effects of anticipatory action on human-robot teamwork efficiency,
fluency, and perception of team. In Proceedings of
the ACM/IEEE international conference on Human-robot
interaction, pages 1‚Äì8. ACM, 2007.
[13] JoÃàrg Hoffmann and Bernhard Nebel. The ff planning
system: Fast plan generation through heuristic search. J.
Artif. Int. Res., 14(1):253‚Äì302, May 2001. ISSN 10769757.
[14] Henry A. Kautz and James F. Allen. Generalized Plan

[15]

[16]

[17]
[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

Recognition. In National Conference on Artificial Intelligence, pages 32‚Äì37, 1986.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ‚Äô01, pages 282‚Äì289, 2001.
ISBN 1-55860-778-1.
Steven James Levine and Brian Charles Williams. Concurrent plan recognition and execution for human-robot
teams. In Twenty-Fourth International Conference on
Automated Planning and Scheduling, 2014.
Tom Murray. Authoring Intelligent Tutoring Systems:
An analysis of the state of the art.
Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza, and
Subbarao Kambhampati. Automated planning for peerto-peer teaming and its evaluation in remote human-robot
interaction. In ACM/IEEE International Conference on
Human Robot Interaction (HRI), 2015.
Fuchun Peng and Andrew McCallum. Information extraction from research papers using conditional random
fields. Information Processing & Management, 42(4):
963 ‚Äì 979, 2006. ISSN 0306-4573.
Miquel Ramƒ±ÃÅrez and Hector Geffner. Probabilistic plan
recognition using off-the-shelf classical planners. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA,
July 11-15, 2010, 2010.
Julie Shah, James Wiken, Brian Williams, and Cynthia
Breazeal. Improved human-robot team performance
using chaski, a human-inspired plan execution system.
In Proceedings of the 6th international conference on
Human-robot interaction, pages 29‚Äì36. ACM, 2011.
E.A Sisbot, L.F. Marin-Urias, R. Alami, and T. Simeon.
A human aware mobile robot motion planner. Robotics,
IEEE Transactions on, 23(5):874‚Äì883, Oct 2007.
Kartik Talamadupula, Gordon Briggs, Tathagata
Chakraborti, Matthias Scheutz, and Subbarao
Kambhampati.
Coordination in human-robot teams
using mental modeling and plan recognition.
In
Intelligent Robots and Systems (IROS 2014), 2014
IEEE/RSJ International Conference on, pages 2957‚Äì
2962, Sept 2014.
Robin R. Vallacher and Daniel M. Wegner. What do people think they‚Äôre doing? action identification and human
behavior. Psychological Review, 94(1):3‚Äì15, 1987.
Yu Zhang, Vignesh Narayanan, Tathagata Chakraborty,
and Subbarao Kambhampati. A human factors analysis
of proactive assistance in human-robot teaming. In
IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2015.
Yu Zhang, Sarath Sreedharan, and Subbarao Kambhampati. Capability models and their applications in
planning. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
AAMAS ‚Äô15, 2015.

Human Computation and Crowdsourcing: Works in Progress Abstracts:
An Adjunct to the Proceedings of the Third AAAI Conference on Human Computation and Crowdsourcing

Acquiring Planning Knowledge via Crowdsourcing
Jie Gaoa and Hankz Hankui Zhuob and Subbarao Kambhampatic and Lei Lib
a
Zhuhai college,
Jilin Univ., Zhuhai, China
jiegao26@163.com

b

School of Data & Computer Sci.,
Sun Yat-sen Univ., China
{lnslilei,zhuohank}@mail.sysu.edu.cn

Introduction

c

Dept. of Computer Sci. & Eng.,
Arizona State Univ., US
rao@asu.edu

initial states and action models based on the answers to HITs
given by the crowd. We then feed the reÔ¨Åned initial states and
action models to planners to solve the problem.

Plan synthesis often requires complete domain models and
initial states as input. In many real world applications, it is
difÔ¨Åcult to build domain models and provide complete initial state beforehand. In this paper we propose to turn to the
crowd for help before planning. We assume there are annotators available to provide information needed for building
domain models and initial states. However, there might be a
substantial amount of discrepancy within the inputs from the
crowd. It is thus challenging to address the planning problem
with possibly noisy information provided by the crowd. We
address the problem by two phases. We Ô¨Årst build a set of
Human Intelligence Tasks (HITs), and collect values from
the crowd. We then estimate the actual values of variables
and feed the values to a planner to solve the problem.
In contrast to previous efforts (Zhang et al. 2012;
Manikonda et al. 2014) that ask the crowd to do planning,
we exploit knowledge about initial states and/or action models from the crowd and feed the knowledge to planners to
do planning. We call our approach PAN-CROWD, stands for
Planning by Acquiring kNowledge from the CROWD.

Building HITs for Action Models: For this part, we build
on our work with the CAMA system (Zhuo 2015). We enumerate all possible preconditions and effects for each action.
SpeciÔ¨Åcally, we generate actions‚Äô preconditions and effects
as follows (Zhuo and Yang 2014). If the parameters of predicate p, denoted by Para(p), are included by the parameters
of action a, denoted by Para(a), i.e., Para(p) ‚äÜ Para(a), p is
likely a precondition, or an add effect, or a delete effect of a.
We therefore generate three new proposition variables ‚Äúp ‚àà
Pre(a)‚Äù, ‚Äúp ‚àà Add(a)‚Äù and ‚Äúp ‚àà Del(a)‚Äù, the set of which
is denoted by Hpre = {p ‚àà Pre(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A},
Hadd = {p ‚àà Add(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, Hdel = {p ‚àà
Del(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, respectively. We put all the
proposition variables together and estimate the label of each
variable by querying the crowd or annotators.
For each proposition in H, we build a Human Intelligent
Task (HIT) in the form of a short survey. For example, for
the proposition ‚Äú(ontable ?x) ‚àà Pre(pickup)‚Äù, we generate
a survey as shown below: Is the fact that ‚Äúx is on table‚Äù
a precondition of picking up the block x? There are three
possible responses, i.e., Yes, No, Cannot tell, out of which
an annotator has to choose exactly one. Each annotator is
allowed to annotate a given survey once. Note that the set of
proposition variables H will not be large, since all predicates
and actions are in the ‚Äúvariable‚Äù form rather than instantiated
form, and we only consider predicates whose parameters
are included by actions. For example, for the blocks domain,
there are only 78 proposition variables in H.

The Formulation of Our Planning Problem
We formulate our planning problem as a quadruple P =
sÀú0 , g, O, AÃÑ, where sÀú0 is an incomplete initial state which
is composed of a set of open propositions. A proposition is
called open if there exist variables in the parameter list of
the proposition, e.g., ‚Äúon(A, ?x)‚Äù (a symbol preceded by ‚Äú?‚Äù
denotes a variable that can be instantiated by an object) is an
open proposition since ?x is a variable in the parameter list
of proposition on. An open initial state can be incomplete,
i.e., some propositions are missing. The set of variables in
sÀú0 is denoted by V. O is a set of objects that can be selected
and assigned to variables in V. We assume O can be easily
collected based on historical applications. AÃÑ is a set of incomplete STRIPS action models. aÃÑ ‚àà AÃÑ is called ‚Äúincomplete‚Äù
if there are some predicates missing in the preconditions or
effects of aÃÑ. A solution to the problem is a plan and a set of
‚ÄúreÔ¨Åned‚Äù action models.

Building HITs for Initial States: To generate surveys that
are as simple as possible, we assume there are sets of objects
known to our approach, each of which corresponds to a type.
For example, {A, B, C} is a set of objects with type ‚ÄúBlock‚Äù
in the blocks domain, i.e., there are three blocks known to our
approach. We can thus generate a set of possible propositions
S with the sets of objects and predicates of a domain. For
each proposition, we formulate the Human Intelligent Task
(HIT) as a short survey, the set of which is denoted by H.
To reduce the number of HITs, we start from the goal g, we
search for the action whose add effects match with g, and
update g to be an internal state by deleting add effects and
adding preconditions of the action into g. We then check

The PAN-CROWD approach
To acquire planning knowledge from the crowd, we Ô¨Årst build
HITs for action models and initial states, and then reÔ¨Åne
c 2015, Association for the Advancement of ArtiÔ¨Åcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

6

base on a ratio Œ±, resulting in open problems P . Propositions
were then changed to short surveys in natural language. We
performed extensive simulations using 20 simulated annotators to complete the human intelligent tasks (HITs). We
deÔ¨Åne the accuracy of our approach by comparing to groundtruth solutions (generated by simulators). In other words, we
solve all 150 problems using our approach, and compare the
150 solutions to ground-truth solutions, viewing the ratio of
identical solutions over 150 as the accuracy.
We ran our PAN-CROWD algorithm on testing problems by varying from 0.1 to 0.5 the ratio Œ±, setting the number of objects to be
10. The result is shown in Figure 1.
From Figure 1, we can see that the
accuracy generally becomes lower
Figure 1: The accuwhen the ratio increases in all three
racy w.r.t. ratio Œ±.
testing domains. This is consistent
with our intuition, since the larger the ratio is, the more
uncertain the information that is introduced to the original
planning problem when using crowdsourcing. However, from
the curves we can see the accuracies are no less than 70%
when the ratio is smaller than 0.3.

whether open initial state matches with internal state. Secondly, from the matched internal state, we select those with
unknown variables, and choose them with the same variables
to build a set of propositions with unknown variables, which
is Ô¨Ånally transformed into a set of HITs.
Estimating True Labels Assume there are R annotators
and N tasks with binary labels {1, 0}. The true labels of
tasks are denoted by Z = {zi ‚àà {1, 0}, i = 1, 2, . . . , N }
. Let Nj is the set of tasks labeled by annotator j, and Ri
is the set of annotators labeling task i. The task assignment
scheme can be represented by a bipartite graph where an
edge (i, j) denotes that the task i is labeled by the worker j.
The labeling results form a matrix Y ‚àà {1, 0}N √óR . The goal
is to Ô¨Ånd an optimal estimator ZÃÇ of the true labels Z given
the
observation Y, minimizing the average bit-wise error rate
1
i‚àà{1,2,...,N } prob[zÃÇi = zi ].
N
We model the accuracy of annotators separately on the
positive and negative examples (Raykar et al. 2010). If the
true label is one, the true positive rate T P j for the jth annotator is deÔ¨Åned as the probability that the annotator labels
it as one, i.e., T P j = p(yij = 1|zi = 1). On the other hand
if the true label is zero, the true negative rate T N j is deÔ¨Åned as the probability that annotator labels it as zero, i.e.,
T N j = p(yij = 0|zi = 0). Suppose we have the training
data set D = {xi , yi1 , . . . , yiR }N
i=1 with N instances from
R annotators, where xi ‚àà X is an instance (typically a ddimensional feature vector), yij is the label (1 or 0) given
by the jth annotator. Considering the family of linear discriminating functions, the probability for the positive class is
modeled as a logistic sigmoid, i.e., p(y = 1|x, w) = œÉ(wT x),
where x, œâ ‚àà Rd , and œÉ(z) = 1+e1‚àíz .
The task is to estimate the parameter w as well as the
true positive P = T P 1 , . . . , T P R  and the true negative
N = T N 1 , . . . , T N R . Let Œ∏ = {w, P, N }, the probability of training data D can be deÔ¨Åned by p(D|Œ∏) =
N
1
R
i=1 p(yi , . . . , yi |xi , Œ∏). The EM algorithm can be exploited to estimate the parameter Œ∏ by maximizing the loglikelihood of p(D|Œ∏) (Raykar et al. 2010). Let Œºi = p(zi =
1|yi1 , . . . , yiR , xi , Œ∏). We simply set the threshold as 0.5, i.e.,
if Œºi > 0.5, the value of zÃÇi is assigned with 1, otherwise 0.










	
	













Discussion and Conclusion
We propose to acquire knowledge about initial states and
action models from the crowd. Since the number of HITS
sent to the crowd related to initial states could be large, in the
future, we could consider how to reduce the number of HITS,
e.g. by exploiting backward chaining planning techniques ‚Äì
which only might need to know small parts of the initial state,
rather than the whole state. In addition, we could also think
of ways of engaging the crowd in more active ways, rather
than answering yes/no to HITS. For example, we can give
them candidate plans and ask them to critique the plan and/or
modify them to make them correct. We then learn knowledge
from the modiÔ¨Åcation process and use it for computing plans.

Acknowledgements
Zhuo‚Äôs research is supported by NSFC (No. 61309011) and
Fundamental Research Funds for the Central Universities
(No. 14lgzd06). Kambhampati‚Äôs research is supported in part
by a Google Research Award and the ONR grants N0001413-1-0176 and N00014-15-1-2027.

References
Manikonda, L.; Chakraborti, T.; De, S.; Talamadupula, K.; and
Kambhampati, S. 2014. AI-MIX: using automated planning to
steer human workers towards better crowdsourced plans. In IAAI,
3004‚Äì3009.
Raykar, V. C.; Yu, S.; Zhao, L. H.; Valadez, G. H.; Florin, C.; Bogoni,
L.; and Moy, L. 2010. Learning from crowds. JMLR 11:1297‚Äì1322.
Zhang, H.; Law, E.; Miller, R.; Gajos, K.; Parkes, D. C.; and Horvitz,
E. 2012. Human computation tasks with global constraints. In CHI,
217‚Äì226.
Zhuo, H. H., and Yang, Q. 2014. Action-model acquisition for
planning via transfer learning. Artif. Intell. 212:80‚Äì103.
Zhuo, H. H. 2015. Crowdsourced action-model acquisition for
planning. In AAAI, 3004‚Äì3009.

Experiment
Since the action model acquisition part has been evaluated
in the context of CAMA already (Zhuo 2015), here we focus
on evaluating the initial state acquisition part. We evaluated
our approach in three planning domains, i.e., blocks1 , depots2
and driverlog4 . We Ô¨Årst generated 150 planning problems
with complete initial states, denoted by PÃÑ . After that we
randomly removed propositions from the initial states in PÃÑ
2


	



ReÔ¨Åning Initial States and Action Models: We reÔ¨Åne the
initial state and action models based on estimated true labels.
Once the initial state and action models reÔ¨Åned, we solve
the corresponding revised planning problem using an off-theshelf planner.

1





http://www.cs.toronto.edu/aips2000/
http://planning.cis.strath.ac.uk/competition/

7

Compliant Conditions for Polynomial Time Approximation of Operator Counts
Tathagata Chakraborti Sarath Sreedharan Sailik Sengupta T. K. Satish Kumar Subbarao Kambhampati Dept. of Computer Science, Arizona State University Dept. of Computer Science, University of Southern California


arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

 

{tchakra2, ssreedh3, sailiks, rao}@asu.edu  tkskwork@gmail.com

Abstract In this paper, we develop a computationally simpler version of the operator count heuristic for a particular class of domains. The contribution of this abstract is threefold, we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer approximation for operator counts in polynomial time; and (3) discuss the relationship of the proposed formulation to existing heuristics and investigate properties of domains where such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model. The domain is described by a set of variables f  F which can assume values from a (finite) domain D (f )  N. A state is given by the particular assignment of values to these variables: S = {f = v | v  D (f ) f  F }. The value of variable f in state S is referred to as S(f ). The action model A consists of operators a = Ca , Ea where Ca is the cost of the action, and Ea = { f, vo , vn | f  F ; vo , vn  {-1}  D (f )} is the set of effects. The transition function  (∑) determines the next state after the application of action a to state S as (a, S) =  if  f, vo , vn  Ea s.t. vo = -1  vo = S(f ); = {f = vn  f, vo , vn  Ea ; else f = S(f )} otherwise.

Plans and Operator Counts. A planning problem is a tuple  = F , A, I, G , where I, G are the initial and (partial) goal states respectively. The solution to the planning problem is a plan  = a1 , a2 , . . . ,  (i) = ai  A such that  (, I) |= G, where the cumulative transition function is given by  (, S) =  ( a2 , a3 , . . . ,  (a1 , S)). The cost of the plan is given by C ( ) = a Ca and an optimal plan   is such that C (  )  C ( )  . The operator count for an action a given a plan  is given by (a,  ) = |{i | a =  (i)}| and the total operator count of the plan ( ) = | |. 1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables. We define compliant variables as those that whenever they occur as a precondition of an action, they must also be an effect, and vice versa. Thus, f  F is compliant iff a  A, f, vo, vn  Ea = vo = -1  vn = -1; f is referred to as rogue otherwise. Let   F be the set of all compliant variables, and the set of compliant variables whose values are specified in the goal be   , henceforth referred to as goal compliant conditions. The State Transformation Equation. Let || = m and |A| = n. Consider an m ◊ n matrix M whose ij th element Mij  Z is the numerical change in fi   produced by action aj  A, i.e. Mij = vn - vo ; fi , vo , vn  Eaj . Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f   from the current state to the final state, i.e. di = vg - vc ; vg = fi  G, vc = fi  S; and let x be a vector of size n, whose ith element is xi  N. Then the following equality holds: Mx = D (1)

The integer solution x to this system of linear equations with the least |x | gives a lower bound on the operator counts required to solve the planning problem, i.e. |x |  |  |. We can compute a real-valued approximation in closed-form, by min ||Qx||2 2 s.t. Mx = D using the Lagrangian multiplier method for this optimization problem as follows 1 L(x) = ||Qx||2 + T (D - Mx) 2 = x = Q-2 MT (MQ-2 MT )-1 D (4) (5) (2) (3)

Here Q is a n ◊ n matrix of action costs whose ij th entry Qij = Cai if i = j ; 0 otherwise (for unit cost domains) Q is an identity matrix and x = MT (MMT )-1 D The most costly operation here is the calculation of the pseudo inverse, which can be done in  O(n2.3 ) time. Further, M is problem independent, and hence the factor Z = Q-2 MT (MQ-2 MT )-1 can be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a heuristic for state-space search. Note that this formulation can also determine infeasibility of goal reachability immediately (in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining the closed-form polynomial bound heuristic, also makes the heuristic inadmissible. Sparse coding. Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian method described above does not address this aspect giving rise to bad heuristic values for certain section of problems. To describe this problem geometrically, we consider a planning domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm calculated above results in small fractional values, and hence a less informed heuristic. As can 2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search procedure P RE - COMPUTE() Compute M, Q Convert M to row echelon form  T is the transformation matrix, r is the rank Y  M[1 : r, :], Z  Q-2 Y T (YQ-2 Y T )-1 procedure h(S) = OP-COUNT(S, G) Compute D = G - S Compute T d = T ◊ D and  = Td [1 : r ] if td i = 0 i  r + 1 then No solution! else return Q ◊ Z ◊   be seen in the figure 1, the actual operator counts for the given example (with M = 15 4 and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead of |  | = 3. x2 Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2 Thus, we propose a different approximation method to obtain integer values for individual operator counts, remaining within the polynomial time bound. We notice that in most cases n  m and also n  |x | due to the combinatorial explosion during grounding of domains. Thus, we propose an operator count heuristic that exploits this knowledge about the sparsity of x . Ideally, we would like to solve the following problem, min |x|l0 Mx = D x 0

s.t.

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems. Firstly, the optimal operator counts (x ), although sparse, might not be the sparsest solution. Secondly, minimizing the l0 norm is NP-hard [5]. Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted l1 -norm ( -l1 -LP) [4] to enforce positive integer solutions. 3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic (hl1 = 1.60 and h-l1 = 3.4) for the aforementioned problem. This method tries to compress (minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing the increase along the tightest dimension till convergence is reached or maximum number of iterations are achieved, resulting in a more informed heuristic (h-l1 = 3.4). x2 Mx = D x2 Mx = D

l1 norm x1  -l1 norm x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2 For  -l1 -LP, we empirically observe that rounding up the individual operator counts produce a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions. Evaluations. The table shows the evaluation of the proposed heuristics across a total of 83 problems from five well-known unit cost planning domains. Each entry in the table represents the percentage difference in the initial state heuristic value and the optimal plan length averaged across the problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (`-' indicates that the heuristics could not be computed due to absence of any goal complaint variables). Rows 3-6 show the performance in domains where the %-compliance was increased (this was done by identifying instances in the action model where variables assume a don't care condition, i.e. a value of -1, and replacing it with appropriate values as entailed by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with more completely specified goals (which results in higher percentage compliance). As expected, our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and  -l1 LP highlights the usefulness of compressed sensing techniques in obtaining better integer approximations to the MILP.

4

Domains GED Blocks-3ops Blocks-4ops Visitall GED Blocks-3ops Blocks-4ops Visitall Blocks-3ops Blocks-4ops 8-puzzle

%-compliance 34.29% 31.25% 19.64% 25.49% 31.25% 19.64% 21.75% 48.13% 42.86% 88.89%

l1 -MILP 55.48% 47.80% 67.71% 37.61% 47.80% 67.71% 28.41% 28.68% 56.25% 33.33%

l1 -LP 55.48% 47.80% 67.71% 34.02% 47.80% 67.71% 28.41% 28.68% 56.25% 40.00%

 - l1 -LP 75.76% 23.60% 35.42% 53.36% 23.60% 35.42% 44.37% 44.38% 12.50% 46.67%

OP-COUNT 55.48% 52.60% 67.71% 48.32% 52.60% 67.71% 100.00% 32.32% 64.58% 40.00%

Discussion and Related Work
Relation to Existing Heuristics. The proposed heuristic has close associations with both heuristics on state change equations and operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very succinctly and are thus extremely useful where such properties are relevant. Another interesting connection to existing work is with respect to graph-plan based heuristics [2], except here we are relaxing preconditions instead of delete effects. Compliance. Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate representations such as transition normal form (TNF) [7] should be investigated in this context. Landmarks. Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version of this formulation here, but it can be easily extended to incorporate more informative features like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net change from D: di  di - ka ◊ (xn - xo ) if di , xo , xn  Ea and a  A is an action landmark with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This demonstrates the flexibility of our approach. Resource Constrained Interaction. The approach is especially relevant in the context of multi-agent interactions constrained by usage   ( ) of a shared resource  by a plan   of an agent . For example, in an adversarial setting, if an agent 2 wanted to stop 1 from executing its plan, all it needs to do is to ensure that  s.t.  1 ( ) +  2 ( ) > | |. Similarly, in a cooperative setting, if agent 2 wanted to ensure that 1 's plan succeeds, it would need to make sure that   1 ( ) +  2 ( )  | |.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an agent's intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40≠42, 2010. [2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281≠300, 1997. [3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning: Landmarks and merges. In ICAPS, 2014. [4] Emmanuel J Cand` es, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier analysis and applications, 2008. [5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization. Mathematical programming, 129(2):285≠299, 2011. [6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995. [7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In ICAPS, pages 188≠192, 2015. [8] Florian Pommerening, Gabriele R® oger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014. [9] Julie Porteous, Laura Sebastia, and J® org Hoffmann. On the extraction, ordering, and usage of landmarks in planning. In ECP, pages 37≠48, 2001. [10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651≠665. Springer, 2007.

6

Explicable Robot Planning as Minimizing Distance
from Expected Behavior

arXiv:1611.05497v1 [cs.AI] 16 Nov 2016

Anagha Kulkarni1 , Tathagata Chakraborti1 , Yantian Zha1 ,
Satya Gautam Vadlamudi2 , Yu Zhang1 , and Subbarao Kambhampati1
Abstract‚Äî In order for robots to be integrated effectively into
human work-flows, it is not enough to address the question of
autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task
plans without such considerations, they may often demonstrate
what we refer to as inexplicable behavior from the point of view
of humans who may be observing it. This problem arises due to
the human observer‚Äôs partial or inaccurate understanding of the
robot‚Äôs deliberative process and/or the model (i.e. capabilities
of the robot) that informs it. This may have serious implications
on the human-robot work-space, from increased cognitive load
and reduced trust in the robot from the human, to more serious
concerns of safety in human-robot interactions. In this paper, we
propose to address this issue by learning a distance function that
can accurately model the notion of explicability, and develop
an anytime search algorithm that can use this measure in its
search process to come up with progressively explicable plans.
As the first step, robot plans are evaluated by human subjects
based on how explicable they perceive the plan to be, and
a scoring function called explicability distance based on the
different plan distance measures is learned. We then use this
explicability distance as a heuristic to guide our search in order
to generate explicable robot plans, by minimizing the plan
distances between the robot‚Äôs plan and the human‚Äôs expected
plans. We conduct our experiments in a toy autonomous car
domain, and provide empirical evaluations that demonstrate
the usefulness of the approach in making the planning process
of an autonomous agent conform to human expectations.

I. I NTRODUCTION
Recent advancement in the field of robotics has given
us autonomous robots, vehicles, drones, etc. Typically these
autonomous systems have the capability to make their own
plans which help them achieve their goals. These advances
have, naturally, encouraged the possibility of human-robot
teaming where the autonomous robots and humans can
work alongside each other. However, if the plans that are
being generated by the autonomous robots are difficult to
comprehend for the human observer, the unexpected behavior
from the robot can raise several concerns: it may increase
cognitive load, hamper the productivity of the team, and
result in safety concerns and distrust towards the robot [1].
This mismatch between the robot‚Äôs plans and the human
expectations may be explained in terms of difference in the
actual robot model and the human‚Äôs understanding of the
robot model. Thus, even with the knowledge of the robot‚Äôs
1 Anagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Yu Zhang, and
Subbarao Kambhampati are with the Computer Science and Engineering
Department at Arizona State University { akulka16, tchakra2,

yantian.zha, yzhan442, rao } @ asu.edu
2 Satya

Gautam

Vadlamudi

vsatyagautam@gmail.com

is

with

Capillary

Technologies.

Fig. 1: A schematic diagram of the proposed system. Explicability distance is the plan distance measure between the
robot plan and human expected plan. This distance is used
to guide the search to generate explicable plans.

goals, it may still not be possible for the human to make
sense of the robot‚Äôs plan. For example, consider a scenario
with an autonomous car switching lanes on a highway. The
autonomous car, in order to switch the lane, may make
sharp and calculated moves, as opposed to gradually moving
towards the other lane. These moves may well be optimal for
the car, and backed by the car‚Äôs superior sensing and steering
capabilities. Nevertheless, a human passenger sitting inside
may perceive this as dangerous and reckless behavior, in as
much as they might be ascribing the car the sort of driving
abilities they themselves have.
To address this issue of the differences between the robot
model, MR (R), and the human mental model of the robot
capabilities, MH (R), we develop an approach based on the
concept of explicability. An explicable plan is a plan that is
generated with the human‚Äôs expectation of the robot model;
the ability to synthesize explicable plans on the part of the
robot thus involves the ability to take into consideration
both models into its plan generation process. The intuition is
that, the similarity between the robot plan that is generated
from the robot model, and the plan that is generated from
the human understanding of the robot model determines
the explicability of the robot plan. More specifically, the
smaller the explicability distance between these two plans,
the more explicable the robot plan is. Of course, such a

similarity metric is not readily available, which brings us
to the question - How can a robot learn a distance function
between plans that model the notion of explicability, and how
can it use this learned similarity model to inform its own
deliberative process? Keeping this in mind, we address the
following questions in our paper: 1) Given a domain, can we
find an approximation to MH (R), the human mental model
of the robot capabilities? 2) Can the measures of distance
between a robot plan, œÄR , and the plan expected by the
human, œÄH , effectively capture the explicability of the robot
plan? 3) Can we then integrate the explicability estimates
into the robot plan generation process? The outline of our
proposed approach is illustrated in Figure 1.
To address the first question, we start out with a robot
model, MR (R) and generate different robot plans for various initial and goal states. Next, we recruit human subjects
and ask them to evaluate these plans by assigning scores
to them based on how well they understand them. As a
part of the study, the subjects are then asked to answer a
questionnaire based on the robot model, in order to elicit
implicit human preferences. This questionnaire allows the
domain modeler to generate MH (R), based on humans
assumptions regarding the robot model in that domain. In this
paper, we represent both models in PDDL [2], but they can
differ in terms of their action representations, preconditions,
effects, and costs.
To answer the second question, we explore the relationship
between three existing plan distance measures: action set,
causal link set and state sequence distances [3], [4] and the
plan explicability distance. We use the robot plans, assigned
with scores, to determine if the explicability of the plans can
be modeled in terms of the aforementioned plan distance
measures, in terms of a regression function. For this, we
generate the plans expected by the human for the same initial
and goal states using the human understanding of the robot
model. We then compute the plan distances between the robot
plans and human expected plans. We call the function that
maps the plan distances to the explicability scores as the
explicability distance.
To address the third question, we integrate the explicablity
distance in the search process of the Fast-Downward
planner [5]. We perform a cost-bounded anytime search,
that can progressively generate more and more explicable
plans, using the learned explicability distance as a heuristic
guidance. We call this reconciliation search. Note that explicability distance exhibits non-monotonicity, i.e. a new action
that gets added to a plan prefix can either increase or decrease
the explicability distance depending on the context of the
plan. We present an analysis on how this property affects
our search. For evaluation of our system, we demonstrate
the effectiveness of our system in a simulated autonomous
car domain, and use human test subjects to evaluate the
explicability of the generated robot plans.
II. RELATED WORK
The notion of robots working alongside humans for task
achievement has been a popular research direction. It is

challenging, mainly due to the fact that, the robot must
consider the human in the loop while making its own
decisions. One important requirement for achieving this, is
the ability to infer about the human‚Äôs intent and plan. Various
plan recognition algorithms [6], [7] can be applied to perform
plan recognition based on a given set of observations as a
result of the agent interacting with the environment. After
the intent and the plan of the human is identified, researchers
have also discussed how the robot can utilize this information
while avoiding conflicts [8], [9] or providing proactive help
to the human in the loop [10], [11]. There is also work
on performing simultaneous plan recognition and generation
[12]. However, most of the prior work has only focused on
how robots can make plans based on the inferred human
intent.
The motivation for generating explicable task plans was
first provided in our recent paper [13]. While that work
proposes learning explicability as a labeling scheme, in this
work, we consider viewing explicability more directly in
terms of distances between the plans generated by the robot‚Äôs
own model, and the human‚Äôs approximation of the robot‚Äôs
model. While explicability focuses on task plans, a related
notion of ‚Äúlegibility‚Äù has been studied in the context of
motion planning [14] and has been shown to be useful in
generating socially acceptable behaviors for robots [15], [16].
In most human-robot cohabitation work where robots are
proactive agents, it is often assumed that the human model is
provided and complete for inferring about the human intent
and plan. This is often not true. Although we also assume
a human model a priori, our formulation allows us to adjust
this model so as to improve model incompleteness (e.g.,
action preference). There also exists learnable models that
do not assume completeness in the first place [17]. Another
note is that in [13], [14] and this work, since the model is
one level deeper, which is about the robot model from the
humans perspective, learning methods are adopted.
III. BACKGROUND
A. Planning
A classical planning problem can be defined as a tuple
P = hM, I, Gi, where M = hF, Ai is the domain model
(that consists of a finite set F of fluents that define the
state of the world and a set of operators or actions A), and
I ‚äÜ F and G ‚äÜ F are the initial and goal states of the
problem respectively. Each action a ‚àà A is a tuple of the
form hpre(a), ef f (a), c(a)i where c(a) denotes the cost of
an action, pre(a) ‚äÜ F is the set of preconditions for the
action a and ef f (a) ‚äÜ F is the set of the effects. The
solution to the planning problem is a plan or a sequence
of actions œÄ = ha1 , a2 , . . . , an i such that starting from
the initial state, sequentially executing the actions lands the
robot in the goal state, i.e. ŒìM (I, œÄ) |= G where ŒìM (¬∑)
is the transition function defined P
for the domain. The cost
of the plan, denoted as c(œÄ) =
ai ‚ààœÄ c(ai ), is given by
the summation of the cost of all the actions in the plan œÄ.
Henceforth, we denote the robot plan as œÄ R and the human
expected plan as œÄ H .

2) Causal Link Distance: A causal link represents a tuple
of the form hai , pi , ai+1 i, where pi is a predicate variable
that is produced as an effect of action ai and used as a precondition for the next action ai+1 . The causal link distance
measure is represented similarly to the action distance, by
considering the causal link sets Cl(œÄ R ) and Cl(œÄ H ) instead
of action sets described above. It is written as:
Œ¥C (œÄ R , œÄ H ) = 1 ‚àí

Fig. 2: A simple illustration of how a robot‚Äôs optimal plan can
deviate from the human expectation due to model difference.
In this maze, the robot can move in all four direction: up,
down, left, right and also diagonally across the grid cells.
Some of the cell floors have glass floors and some others
have obstacles. The glass floors are harder for the robot to
navigate across, because of the reflective surface, it needs to
use special sensors which results in an expensive action for
the robot. The path in green is the explicable plan whereas
the path in red is the robot plan.

B. Plan Distance Measures
We now look at the three plan distance measures introduced in [3] and later refined in [4]. These plan distances are
action, causal link and state sequence distances. Although
these distance metrics do not satisfy certain mathematical
properties [18], they provide a good domain independent
measure of the difference between any two plans. Since the
goal is to predict the differences in terms of explicability
distance between the robot plans and human expected plans,
the intuition is that they can be approximated using a
combination of plan distance measures that capture different
aspects of plans.
1) Action Distance: We denote the set of unique actions
in a plan œÄ as A(œÄ) = {a | a ‚àà œÄ}. Given the action sets
A(œÄ R ) and A(œÄ H ) of two plans œÄ R and œÄ H respectively, the
action distance, Œ¥a , is computed as the ratio of the actions
that are exclusive to each plan to all the actions in the plans
[4]. It is written as:
Œ¥A (œÄ R , œÄ H ) = 1 ‚àí

|A(œÄ R ) ‚à© A(œÄ H )|
|A(œÄ R ) ‚à™ A(œÄ H )|

(1)

This simply means that two plans are similar (and hence
their distance measure is smaller) if they contain similar
actions. Note that this measure does not take the ordering
of actions into account.

|Cl(œÄ R ) ‚à© Cl(œÄ H )|
|Cl(œÄ R ) ‚à™ Cl(œÄ H )|

(2)

Again, plans are similar, with lower similarity scores, if
they have a large number of overlapping causal links.
3) State Sequence Distance: This distance measure, as
the name suggests, takes the sequences of the states into
consideration. This distance captures the context of an action
in a given plan. The length of the sequences may differ
and therefore there are multiple ways to define this distance
measure [4]. We use the representation shown in Eq. 3. Given
H
H
R
two state sequences (sR
0 , . . . , sn ) and (s0 , . . . , sn0 ) for œÄR
0
and œÄH respectively, where n ‚â• n are the lengths of the
plans, the state sequence distance is written as:
1
Œ¥S (œÄ , œÄ ) =
n
R

H

"

0

n
X

#
H
‚àÜ(sR
k , sk )

+n‚àín

0

(3)

k=1
|sR ‚à©sH |

H
k
k
where ‚àÜ(sR
k , sk ) = 1 ‚àí |sR ‚à™sH | represents the distance
k
k
between two states (where sR
k is overloaded to denote the set
of predicate variables in state sR
k ). The first term measures
the normalized difference between states up to the end of
the shorted plan, while the second term, in the absence of a
state to compare to, assigns maximum difference possible.
Here we illustrate the explicability distance with an example and discuss its relationship with the other distance
measures. Consider the grid structure shown in Figure 2.
Here we have a 5 by 5 grid. The bottom left cell is labeled
as (1, 1). Some of the cells have glass floors while some
others have obstacles. The robot has to find its way across
the obstacles from the start cell to the goal cell. Looking at
the grid structure, the human may expect the robot to take
an optimal path highlighted by the green arrows. Although
unbeknownst to the human, the robot has difficulty traveling
across the glass floor cells because of the reflective surface
and has to use special sensors while navigating across these
floors. Hence, the cost of treading on these glass floors
is higher than the cost of treading across normal cell. In
this case, the robot‚Äôs optimal plan to the goal is the one
highlighted in red, which doesn‚Äôt coincide with human‚Äôs
expectation of the robot plan.
In Figure 3, we provide the actions sets, causal link sets
and state sequences generated for both the robot plan and
human expected plan for our example illustrated in Figure 2.
The corresponding plan distances are shown in Table I. These
three distances capture different aspects of the plans. In this
case, the explicability distance clearly has a high correlation
with these other distance measures. Our goal in this paper

Initial State: at(1, 1)
Goal State: at(5, 4)
Actions:
A(œÄ R )
=
{ move-diagonal(2, 2), move-up(2, 3), move-up(2, 4), move-diagonal(3, 5),
move-diagonal(4, 4), move-right(5, 4) }
A(œÄ H ) = {move-diagonal(2, 2), move-diagonal(3, 3), move-diagonal(4, 4), move-right(5,
4) }
Causal Links:
Cl(œÄ R ) = { <move-diagonal(2, 2), at(2, 2), move-up(2, 3)>, <move-up(2, 3), at(2, 3),
move-up(2, 4)>, <move-up(2, 4), at(2, 4), move-diagonal(3, 5)>, <move-diagonal(3,
5), at(3, 5), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
Cl(œÄ H ) =
{ <move-diagonal(2, 2), at(2, 2), move-diagonal(3, 3)>, <move-diagonal(3,
3), at(3, 3), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5,
4)> }
State sequences:
S(œÄ R ) = { {at(2, 2)}, {at(2, 3)}, {at(2, 4)}, {at(3, 5)}, {at(4, 4)} }
S(œÄ H ) = { {at(2, 2)}, {at(3, 3)}, {at(4, 4)} }
Fig. 3: The action sets, causal link sets and state sequence sets for the illustrated example in Figure 2. For the given initial
and goal state, the plans illustrated in red and green in Figure 2 are used to produce respective action, causal link and state
sequence sets.
TABLE I: Action distance, causal link distance and state
sequence distance computed using the sets provided in Figure
3, between the two plans, œÄ R and œÄ H , that are illustrated in
Figure 2.

In order to train our regression model, we use plan traces
whose actions were assigned scores by human subjects. We
can then calculate the explicability score of a plan based on
the average of the individual action scores.

Plan Pair

Œ¥A

Œ¥C

Œ¥S

B. Plan Generation

(œÄ R , œÄ H )

4/7

6/7

4/5

We now present the details of our plan generation phase,
where we use the explicability distance to guide our search to
generate the most explicable robot plan for a given problem.
1) Non-Monotonicity: We will now discuss the nonmonotonic behavior exhibited by explicability function and
how it affects the plan generation process. The explicability
distance function is non-monotonic in nature, meaning, as
the partial plan grows, the explicability distance may both
increase or decrease. This is because, a new action can either
contribute positively or negatively to the total explicability
score of the plan. As pointed out earlier, the explicability
score is computed as an average of the individual action
scores in the context of the plan prefix.
Observation 1: Explicability score of a partial plan P may
increase, stay equal, or even decrease when it is extended
with one or more actions.
Consider the following example, in a car domain, the goal
of the car is to move to the left lane. The car squeezes
leftwards in three consecutive actions and after coming to
the left lane, it turns on its left indicator. Here the turning on
of the left tail light after having moved left is an inexplicable
action. The previous three actions were explicable to the
human drivers and contribute positively to the explicability

is, then, to learn to establish a general relationship between
the established measures of plan distance.
IV. PROPOSED METHODOLOGY
A. Explicability Distance
Since, without the model we do not know which plan
distance is most relevant in capturing explicability, we
present a general formulation in this section. A more
detailed formulation can be found in the following section. Let ‚àÜ be a 3-dimensional vector, such that for a
robot plan, œÄ R , derived from MR (R), and for an explicable plan œÄ H , derived from MH (R), we have ‚àÜ =
hŒ¥A (œÄ R , œÄ H ), Œ¥C (œÄ R , œÄ H ), Œ¥S (œÄ R , œÄ H )iT . We now define
explicability distance of a robot plan, Exp(œÄ R ), as a regression based function of the three plan distances, with b
as the parameter vector:
Exp(œÄ R / œÄ H ) ‚âà f (‚àÜ, b)

(4)

score of the plan but the last action has a negative impact
and decreases the score. Therefore this score and in turn
the explicability distance is not a non-decreasing function.
In essence, depending on the context, the explicability of an
action can either improve the score or worsen it.
Observation 2: A greedy method that expands a node
with the highest explicability score of the corresponding
partial plan at each step does not guarantee to find an
optimal explicable plan (one of the plans with the highest
explicability score) as its first solution.
The above observation is easy to see since, if e1 is
explicability score of the first plan, then a node may exist
in open list (set of unexpanded nodes) whose explicability
score is less than e1 , which when expanded may result in a
solution plan with explicability score higher than e1 .
2) Reconciliation Search: Given the non-monotonic nature of explicability distance function, we have to generate
all the candidate plans in order to find the most explicable
plan. Here, we present a cost-bounded anytime greedy search
algorithm called reconciliation search that generates all the
valid loopless candidate solution plans up to a given cost
bound, and then progressively searches for plans with better
explicability scores. The value of the heuristic h(v) in a
particular state v encountered during search is based entirely
on the explicability distance of the robot plan prefix up to
that state, given by,
h(v) = Exp(œÄ / œÄh )
s.t. ŒìMR (R) (I, œÄ) = v

Algorithm 1 Reconciliation Search
Input: Planning problem P = hMR (R), I, Gi, cost bound
max cost, and explicability distance function Exp
Output: Robot plan with the highest explicability score
œÄ R = arg maxœÄR Exp(œÄ R / œÄH )
1: S ‚Üê ‚àÖ
. Candidate plan solution set
2: open ‚Üê ‚àÖ
. Open list
3: closed ‚Üê ‚àÖ
. Closed list
4: open.insert(I, 0, inf)
5: while open 6= ‚àÖ do
6:
n ‚Üê open.remove()
. Node with highest h(¬∑)
7:
if n |= G then
8:
S.insert(œÄ s.t. ŒìMR (R) (I, œÄ) |= v)
9:
end if
10:
closed.insert(n)
11:
for each v ‚àà successors(n) do
12:
if v ‚àà
/ closed then
13:
if g(n) + cost(n, v) ‚â§ max-cost then
14:
open.insert(v, h(v))
15:
end if
16:
else
17:
if h(n) < h(v) then
18:
closed.remove(v)
19:
open.insert(v, h(v))
20:
end if
21:
end if
22:
end for
23: end while
24: return arg maxœÄ R ‚ààS Exp(œÄ R / œÄH )

and ŒìMH (R) (I, œÄh ) = v
Since we want to find explicable plans which are within
a cost bound, we use the cost of the plan to prune the
nodes in the search graph whenever they exceed the given
maximum cost bound. We implement this search in the
Fast-Downward planner [5]. The approach is described
in detail in Algorithm 1.
At each iteration of the algorithm, the plan prefix of the
robot model is compared with the explicable trace œÄh (these
are the plans generated by the human mental model of the
robot MH (R) up to the current state in the search process)
for the given problem. Using the computed distances, we
predict the explicability score for every candidate robot plan.
The search algorithm then makes a locally optimal choice
of states. After generating the first solution plan we do not
stop the search but instead continue to find all the valid
loopless candidate solution plans within the given cost bound
or until the state space is completely explored. In the end, the
candidate plan with highest explicability score is returned.
V. E XPERIMENTAL A NALYSIS
A. Autonomous Car Simulation Experiment
1) Domain Model: Autonomous cars are a topic of interest from the point of view of explicability problem. In the
recent past, Google‚Äôs self-driving cars [19] have been in the
news for being ‚Äútoo safe‚Äù on the roads. These autonomous
cars governed by strict traffic rules find it hard to blend

in and make judgments that would not make sense in a
predominantly human environment. At four-way stops, these
cars find it difficult to cross the intersection, while the human
drivers keep inching forward. For a robot car, such situations,
where it does not make an explicable decision can pose
problems, and all the human drivers who come into contact
with such cars would have to face the brunt of it.
For these reasons, we focused our studies on a simulated
autonomous car environment, and investigated how the robot
car‚Äôs inexplicable behavior can be avoided by generating
plans with respect to their explicability scores. In our robot
car model (written in PDDL), we try to capture bad driving
etiquette commonly seen on roads, such as, driving below
speed limit in passing lanes, overtaking from the wrong
side, turning and changing lanes without showing signal,
not following the move over law, and so on. The human
mental model of the robot car is defined as per test subjects
assumptions of how the robot car should perform actions.
From the robot model MR (R), we generated 40 plans for
16 different problems. The plans consisted of both explicable
and inexplicable robot car behaviors. These plans were
assessed by 20 test subjects, with each subject evaluating 8
plans. Also, each plan was evaluated by 4 different subjects,
in order to get a general understanding of the assumptions
of different human drivers. Therefore, the overall number of

(a)

(b)

(c)

Fig. 4: Autonomous Car Domain Simulation. Here the red colored car in the images is the robot car and the rest of the cars
are assumed to be human drivers. (a) The cop car is parked on the rightmost lane, and the robot car is following through
the Move Over Law maneuver. (b) The robot car is wrongly trying to overtake from the rightmost lane. (c) The robot car
is waiting at a four-way stop intersection even though it is the turn of the robot car to cross over.

Fig. 5: Here AD, CLD, SD and Score represent action
distance, causal link distance, state sequence distance, and
explicability scores respectively. This is a correlation matrix
for the aforementioned metrics. The red color represents
the negative correlation that exists between the distance
measures and the scores.

training samples was 160. The test subjects were required to
have sufficient real-life driving experience. The assessment
had two parts: one part involved scoring each robot car action
with 1, if explicable, and 0 otherwise (the explicability score
of the overall plan is calculated as the fraction of actions
in the plan that were labeled as explicable); the other part
involved answering a questionnaire aimed at understanding
test subject‚Äôs assumptions regarding the robot car. The information from this questionnaire was used to design the human
mental model MH (R) of the robot car.
The PDDL domain of the robot car, MR (R), consists
of lane and car objects as shown in Figure 4. The red car

is the robot car in the experiments and all other cars seen
in the experiments are assumed to have human drivers.
The car objects are associated with predicates defining the
location of a car on a lane segment, status of left and right
turn lights, status of car being within speed limit, presence
of a parked cop car, and so on. The actions possible
in the domain are with respect to the robot car. These
actions are Accelerate, Decelerate, LeftSqueeze,
RightSqueeze, LeftLightOn, LeftLightOff,
RightLightOn, RightLightOff, SlowDown and
WaitAtStopSign, and so on. In order to change a
lane, three consecutive actions of either LeftSqueeze
or RightSqueeze are required to gradually move to
the other lane. The PDDL domain of the human mental
model, consists of same state predicates, but different action
representations, preconditions, effects and action-costs. Note
that even though representing the human mental model in
PDDL may seem like a strong assumption, we validated the
labels given by the human subjects with the PDDL human
model constructed from the elicited preferences and found
about 72.3% match, which indicates that MH (R) used
in the evaluations is a good approximation of the human
mental model of the robot
2) Defining the Explicability Distance: For the 22 training
problems, explicable plans with MH (R) were generated.
Since some actions are not common to both the domains and
also owing to the difference in the effects and preconditions
of the actions across domains, an explicit mapping was
defined between the actions over the two domains. This
mapping was done in the light of the plan distance operations
performed between plans in the two domains.
The correlation matrix in Figure 5 establishes the negative
correlation of the plan distance measures to the explicability
scores. From the correlation matrix it can be seen that, causal
link distance has significant negative correlation with the
explicability scores. After establishing the negative correlation, we proceed towards training our regression model called
explicability distance.

TABLE II: Parameters of Regression Models
Distance

b

w

Accuracy %
10.14

Œ¥A

0.72

-0.33

Œ¥C

0.73

-0.231

7.06

Œ¥S

0.92

-0.519

27.47

Œ¥A , Œ¥ C , Œ¥ S

0.93

0.207,-0.061,-0.626

28.02

sR
1 = b1 + w1 Œ¥A

(5)

sR
2 = b2 + w2 Œ¥C

(6)

sR
3 = b3 + w3 Œ¥S

(7)

sR
4 = b4 + w4 Œ¥A + w5 Œ¥C + w6 Œ¥S

(8)

At first, individual distances were used to fit the data in the
regression model. This resulted in a poorly learned regression
model. A linear combination of the three distances also
resulted in poor results. For regression model functions 5, 6,
7 and 8, the bias, weight and accuracy values were as shown
in Table II. From this table, we infer that the relationships
are not necessarily linear as we speculated previously. We
improve our model using Random Forest regression. Since
random forests allow selection of random subset of features
while splitting the decision node, the accuracy of our model
improves. All the three distances have statistically significant
contribution in the fitted model. We evaluate the goodness of
the fit of the model, using the coefficient of determination or
R2 . This value determines the measure by which the fitted
model can explain the variations in the target values. This
value lies between 0 to 1. Higher the R2 value, better is
the model fitted to the data. After training process the new
regression model was found to have 0.8721 R2 value. That is
to say, 87% of the variations in the features can be explained
by our model. Our model predicts the explicability distance
between the robot plans and human mental model plans, with
a high accuracy. We call this plan distance regression model
as the explicability distance.
3) Evaluation: For evaluation of our system, we tested it
on 13 different problems. We ran the algorithm with a high
cost bound, in order to cover the most explicable candidate
plans for all the problems. The results of this search process
are as shown in Figure 6, 7 and 8. From these results, we
can see that the reconciliation search is able to incrementally
develop plans with better explicability scores as shown in
Figure 6. In Figure 7, we see that for all the 13 problems
the explicability score of the optimal plans is lesser than the
final plans generated by reconciliation search. From Figure
8, we see that for the first six problems the optimal and
explicable plans have same cost but our modified planner
with reconciliation search produces explicable plan versions
for those problems. The results also clearly show that the
explicable plans can be costlier than plans that are optimal
with respect to the robot‚Äôs own model. This additional cost

Fig. 6: The graph shows that the search process finds plans
with incrementally better explicable scores. Each color line
represents the 13 different problems. The markers on the
lines represent a plan solution for that problem. The y-axis
gives the explicability scores of the plans and the x-axis gives
the solution number. Note that the curves show the nonmonotonic nature of evaluation metric in the search process.
The final output of the algorithm is, of course, the best plan
found in the search process.

Fig. 7: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their explicability scores.

Fig. 8: For the test problem instances, the optimal plans
generated using Fast-Downward planner and the plans
generated using Reconciliation Search were compared for
their plan costs.

can be seen as the price the robot pays to make its behavior
explicable to the human.
VI. C ONCLUSION
We showed how the plan distance measures play a role in
determining the explicability of a robot plan. We evaluated
our hypothesis in the simulated Autonomous Car PDDL
domain. We generated training samples in robot‚Äôs domain
and assigned them with human scores. We also generated
plans in the human‚Äôs model to find the distances between
plans in two domains. We looked at the relationships between
scores and the distance measures of the plans. We learned
the regression model that could best capture the explicability
of the training samples. In summary, we have proved our
hypothesis that using the human‚Äôs mental model of the robot
model we can assess the explicability of a robot plan as a
function over the plan distance measures between the robot
plan and the plan that the human would expect the robot to
make. We also showed that the explicability distance measure
can be used to bias the robots planning process to generate
plans that are more in concordance with what humans expect.
We are currently in the process of incorporating this theory
into the behavior of a Fetch robot involved in delivery
tasks, to demonstrate how it improves the explicability of
the robot‚Äôs behavior.
R EFERENCES
[1] E. de Visser and R. Parasuraman, ‚ÄúAdaptive aiding of human-robot
teaming effects of imperfect automation on performance, trust, and
workload,‚Äù Journal of Cognitive Engineering and Decision Making,
vol. 5, no. 2, pp. 209‚Äì231, 2011.
[2] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram,
M. Veloso, D. Weld, and D. Wilkins, ‚ÄúPddl-the planning domain
definition language,‚Äù 1998.
[3] B. Srivastava, T. A. Nguyen, A. Gerevini, S. Kambhampati, M. B. Do,
and I. Serina, ‚ÄúDomain independent approaches for finding diverse
plans.‚Äù in IJCAI, 2007, pp. 2016‚Äì2022.

[4] T. A. Nguyen, M. Do, A. E. Gerevini, I. Serina, B. Srivastava,
and S. Kambhampati, ‚ÄúGenerating diverse plans to handle unknown
and partially known user preferences,‚Äù Artificial Intelligence,
vol. 190, no. 0, pp. 1 ‚Äì 31, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0004370212000707
[5] M.
Helmert,
‚ÄúThe
fast
downward
planning
system,‚Äù
CoRR,
vol.
abs/1109.6051,
2011.
[Online].
Available:
http://arxiv.org/abs/1109.6051
[6] H. A. Kautz and J. F. Allen, ‚ÄúGeneralized plan recognition.‚Äù in AAAI,
vol. 86, no. 3237, 1986, p. 5.
[7] M. Ramƒ±rez and H. Geffner, ‚ÄúProbabilistic plan recognition using offthe-shelf classical planners,‚Äù in Proceedings of the Conference of the
Association for the Advancement of Artificial Intelligence (AAAI 2010).
Citeseer, 2010, pp. 1121‚Äì1126.
[8] T. Chakraborti, Y. Zhang, D. E. Smith, and S. Kambhampati, ‚ÄúPlanning
with resource conflicts in human-robot cohabitation,‚Äù in Proceedings
of the 2016 International Conference on Autonomous Agents &
Multiagent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 2016, pp. 1069‚Äì1077.
[9] M. Cirillo, L. Karlsson, and A. Saffiotti, ‚ÄúHuman-aware task planning
for mobile robots,‚Äù in Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1‚Äì7.
[10] T. Chakraborti, G. Briggs, K. Talamadupula, Y. Zhang, M. Scheutz,
D. Smith, and S. Kambhampati, ‚ÄúPlanning for serendipity,‚Äù in
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2015.
[11] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati, ‚ÄúCoordination in human-robot teams using mental
modeling and plan recognition,‚Äù in Intelligent Robots and Systems
(IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014,
pp. 2957‚Äì2962.
[12] S. J. Levine and B. C. Williams, ‚ÄúConcurrent plan recognition and
execution for human-robot teams.‚Äù in ICAPS, 2014.
[13] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, and S. K.
Hankz Hankui Zhuo, ‚ÄúPlan explainability and predictability for
cobots,‚Äù CoRR, vol. abs/1511.08158, 2015. [Online]. Available:
http://arxiv.org/abs/1511.08158
[14] A. Dragan and S. Srinivasa, ‚ÄúGenerating legible motion,‚Äù in Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.
[15] T. W. Fong, I. Nourbakhsh, and K. Dautenhahn, ‚ÄúA survey of socially
interactive robots,‚Äù Robotics and Autonomous Systems, 2003.
[16] G. Hoffman and C. Breazeal, ‚ÄúCost-based anticipatory action selection
for human‚Äìrobot fluency,‚Äù Robotics, IEEE Transactions on, vol. 23,
no. 5, pp. 952‚Äì961, 2007.
[17] Y. Zhang, S. Sreedharan, and S. Kambhampati, ‚ÄúCapability models
and their applications in planning,‚Äù in Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems.
International Foundation for Autonomous Agents and Multiagent
Systems, 2015, pp. 1151‚Äì1159.
[18] R. P. Goldman and U. Kuter, ‚ÄúMeasuring plan diversity: Pathologies
in existing approaches and a new plan distance metric.‚Äù 2015.
[19] M. Richtel and C. Dougherty, ‚ÄúGoogle‚Äôs driverless cars run into
problem: Cars with drivers,‚Äù The New York Times, vol. 9, p. 1, 2015.

A Formal Analysis of Required Cooperation in Multi-agent Planning
Yu Zhang and Subbarao Kambhampati

arXiv:1404.5643v1 [cs.AI] 22 Apr 2014

School of Computing and Informatics
Arizona State University
Tempe, Arizona 85281 USA
{yzhan442,rao}@asu.edu

Abstract
Research on multi-agent planning has been popular in
recent years. While previous research has been motivated by the understanding that, through cooperation,
multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required
to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a
formal discussion from the planning aspect. We first
show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing
the problems that require cooperation (referred to as RC
problems) into two classes ‚Äì problems with heterogeneous and homogeneous agents, we aim to identify all
the conditions that can cause RC in these two classes.
We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper
bound on the minimum number of agents required for
RC problems with homogeneous agents. This study not
only provides new insights into multi-agent planning,
but also has many applications. For example, in humanrobot teaming, when a robot cannot achieve a task, it
may be due to RC. In such cases, the human teammate
should be informed and, consequently, coordinate with
other available robots for a solution.

Introduction
A multi-agent planning (MAP) problem differs from a single
agent planning (SAP) problem in that more than one agent
is used in planning. While a (non-temporal) MAP problem
can be compiled into a SAP problem by considering agents
as resources, the search space grows exponentially with the
number of such resources. Given that a SAP problem with
a single such resource is in general PSPACE-complete (Bylander 1991), running a single planner to solve MAP is inefficient. Hence, previous research has generally agreed that
agents should be considered as separate entities for planning,
and thus has been mainly concentrated on how to explore
the interactions between the agents (i.e., loosely-coupled vs.
tightly-coupled) to reduce the search space, and how to perform the search more efficiently in a distributed fashion.
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

However, there has been little discussion on whether multiple agents are required for a planning problem in the first
place. If a single agent is sufficient, solving the problem with
multiple agents becomes an efficiency matter, e.g., shortening the makespan of the plan. Problems of this nature can
be solved in two separate steps: planning with a single agent
and optimizing with multiple agents. In such a way, the difficulty of finding a solution may potentially be reduced.
In this paper, we aim to answer the following questions: 1)
Given a problem with a set of agents, what are the conditions
that make cooperation between multiple agents required to
solve the problem; 2) How to determine the minimum number of agents required for the problem. We show that providing the exact answers is intractable. Instead, we attempt to
provide approximate answers. To facilitate our analysis, we
first divide MAP problems into two classes ‚Äì MAP problems with heterogeneous agents, and MAP problems with
homogeneous agents. Consequently, the MAP problems that
require cooperation (referred to as RC problems) are also divided into two classes ‚Äì type-1 RC (RC with heterogeneous
agents) and type-2 RC (RC with homogeneous agents) problems. Figure 1 shows these divisions.
For the two classes of RC problems, we aim to identify
all the conditions that can cause RC. Figure 2 presents these
conditions and their relationships to the two classes of RC
problems. We establish that at least one of these conditions
must be present in order to have RC. Furthermore, we show
that most of the problems in common planning domains belong to type-1 RC, which is identified by three conditions
in the problem formulation that define the heterogeneity of
agents; most of the problems in type-1 RC can be solved by a
super agent. For type-2 RC, we show that RC is only caused
when the state space is not traversable or when there are
causal loops in the causal graph. We provide upper bounds
for the answer of the second question for type-2 RC problems, based on different relaxations of the conditions that
cause RC, which are associated with, for example, how certain causal loops can be broken in the causal graph.
The answers to these questions not only enrich our fundamental understanding of MAP, but also have many applications. For example, in a human robot teaming scenario, a human may be remotely working with multiple robots. When
a robot is assigned a task that it cannot achieve, it is useful
to determine whether the failure is due to the fact that the

Figure 1: Division of MAP problems into MAP with heterogeneous and homogeneous agents. Consequently, RC problems are also divided into two classes: type-1 RC involves
RC problems with heterogeneous agents and type-2 RC involves RC problems with homogeneous agents.

Figure 2: Causes of required cooperation in RC problems.
task is simply unachievable or the task requires more than
one robot. In the latter case, it is useful then to determine
how many extra robots must be sent to help. The answers
can also be applied to multi-robot systems, and are useful in
general to any multi-agent systems in which the team compositions can dynamically change (e.g., when the team must
be divided to solve different problems).
The rest of the paper is organized as follows. After a review of the related literature, we start the discussion of required cooperation for MAP, in which we answer the above
questions in an orderly fashion. We conclude afterward.

Related Work
One of the earlier works on MAP is the PGP framework
by (Durfee and Lesser 1991; Decker and Lesser 1992). Recently, the MAP problem has started to receive an increasing amount of attention. Most of these recent research works
consider agents separately for planning, and have been concentrated on how to explore the structure of agent interactions to reduce the search space, as well as solving the problem in a distributed fashion. (Nissim, Brafman, and Domshlak 2010) provide a search method by compiling MAP into a
constraint satisfaction problem (CSP), and then using a distributed CSP framework to solve it. The MAP formulation is
based on an extension of the STRIPS language called MASTRIPS (Brafman and Domshlak 2008). In MA-STRIPS,
actions are categorized into public and private actions. Public actions can influence other agents while private actions
cannot. In this way, it is shown by (Brafman and Domsh-

lak 2008) that the search complexity of MAP is exponential in the tree-width of the agent interaction graph. Due to
the poor performance of DisCSP based approaches, (Nissim and Brafman 2012) apply the A‚àó search algorithm in a
distributed manner, which represents one of the state-of-art
MAP solvers. (Torreno, Onaindia, and Sapena 2012) propose a POP-based distributed planning framework for MAP,
which uses a cooperative refinement planning technique that
can handle planning with any level of coupling between the
agents. Each agent at any step proposes a refinement step
to improve the current group plan. Their approach does not
assume complete information. A similar paradigm is taken
by (Kvarnstrom 2011). An iterative best-response planning
and plan improvement technique using standard SAP algorithms is provided by (Jonsson and Rovatsos 2011), which
considers the previous singe agent plans as constraints to be
satisfied while the following agents perform planning.
Given a problem, all of these MAP approaches solve it
using the given set of agents, without first asking whether
multiple agents are really required, let alone what is the minimum number of agents required. Answers to these questions not only separate MAP from SAP in a fundamental
way, but also have real world applications when the team
compositions can dynamically change. In this paper, we
analyze these questions using the SAS+ formalism (Backstrom and Nebel 1996) with causal graph (Knoblock 1994;
Helmert 2006), which is often discussed in the context
of factored planning (Bacchus and Yang 1993; Amir and
Engelhardt 2003; Brafman 2006; Brafman and Domshlak
2013). The causal graph captures the interaction between
different variables; intuitively, it can also capture the interactions between agents since agents affect each other through
these variables. In fact, (Brafman and Domshlak 2013) mention the causal graph‚Äôs relation to the agent interaction graph
when each variable is associated with a single agent.

Multi-agent Planning (MAP)
In this paper, we start the analysis of RC in the simplest
scenarios ‚Äì with instantaneous actions and sequential execution. The possibility of RC can only increase when we
extend the model to the temporal domain, in which concurrent or synchronous actions must be considered. We develop
our analysis of required cooperation for MAP based on the
SAS+ formalism (Backstrom and Nebel 1996).

Background
Definition 1. A SAS+ problem is given by a tuple P =
hV, A, I, Gi, where:
‚Ä¢ V = {v1 , ..., vn } is a set of state variables. Each variable
vi ‚àà V is associated with its domain D(vi ), which is used
to define an extended domain D(vi )+ = D(vi )‚à™u, where
u denotes the undefined value. The state space is defined
as SV+ = D(v1 )+ √ó ... √ó D(vn )+ ; s[vi ] denotes the value
of the variable vi in a state s ‚àà SV+ .
‚Ä¢ A = {a1 , ..., am } is a finite set of actions. Each action aj is a tuple hpre(aj ), post(aj ), prv(aj )i, where
pre(aj ), post(aj ), prv(aj ) ‚äÜ SV+ are the preconditions,
postconditions and prevail conditions of aj , respectively.

We also use pre(aj )[vi ], post(aj )[vi ], prv(aj )[vi ] to denote the corresponding values of vi .
‚Ä¢ I and G denote the initial and goal state, respectively.

Lemma 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining whether it satisfies RC is PSPACEcomplete.

A plan in SAS+ is often defined to be a total-order plan:
Definition 2. A plan œÄ in SAS+ is a sequence of actions
œÄ = ha1 , ..., al i.

Proof. First, it is not difficult to show that the RC decision problem belongs to PSPACE, since we only need to
verify that P = hV, œÜ, I, Gi is unsolvable for all œÜ ‚àà Œ¶,
given that the initial problem is known to be solvable. Then,
we complete the proof by reducing from the PLANSAT
problem, which is PSPACE-complete in general (Bylander
1991). Given a PLANSAT problem (with a single agent), the
idea is that we can introduce a second agent with only one
action. This action directly achieves the goal but requires
an action (with all preconditions satisfied in the initial state)
of the initial agent to provide a precondition that is not initially satisfied. We know that this constructed MAP problem
is solvable. If the algorithm for the RC decision problem
returns that cooperation is required for this MAP problem,
we know that the original PLANSAT problem is unsolvable;
otherwise, it is solvable.

Given two states s1 , s2 ‚àà SV+ , (s1 ‚äï s2 ) denotes that s1 is
updated by s2 , and is subject to the following for all vi ‚àà V :

s2 [vi ] if s2 [vi ] 6= u,
(1)
(s1 ‚äï s2 )[vi ] =
s1 [vi ] otherwise.
Given a variable with two values x, y in which one of
them is u, x t y is defined to be the other value. t can be
extended to two states s1 and s2 , such that s1 t s2 [vi] =
s1 [vi ] t s2 [vi ] for all vi ‚àà V . s1 v s2 if and only if
‚àÄvi ‚àà V, s1 [vi ] = u or s1 [vi ] = s2 [vi ]. The state resulting from executing a plan œÄ can then be defined recursively
using a re operator as follows:
(
re(s, hœÄ; oi) =

re(s, hœÄi) ‚äï post(o)
if pre(o) t prv(o) v re(s, hœÄi),
s otherwise.

(2)

in which re(s, hi) = s, o is an action, and ; is the concatenation operator.

Extension to MAP
To extend the previous formalism to MAP without losing
generality, we minimally modify the definitions.
Definition 3. A SAS+ MAP problem is given by a tuple Œ† =
hV, Œ¶, I, Gi, where:
‚Ä¢ Œ¶ = {œÜg } is the set of agents; each agent œÜg is associated
with a set of actions A(œÜg ).
Definition 4. A plan œÄM AP in MAP is a sequence of agentaction pairs œÄM AP = h(a1 , œÜ(a1 )), ..., (aL , œÜ(aL ))i, in
which œÜ(a) returns the agent for the action a and L is the
length of the plan.
We do not need to consider concurrency or synchronization given that actions are assumed to be instantaneous.

Required Cooperation for MAP
Next, we formally define the notion of required cooperation
and other useful terms that are used in the following analyses. We assume throughout the paper that more than one
agent is considered (i.e., |Œ¶| > 1).

Required Cooperation
Definition 5 (k-agent Solvable). Given a MAP problem
P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), the problem is k-agent solvable
if ‚àÉŒ¶k ‚äÜ Œ¶ (|Œ¶k | = k), such that hV, Œ¶k , I, Gi is solvable.
Definition 6 (Required Cooperation (RC)). Given a solvable MAP problem P = hV, Œ¶, I, Gi, there is required cooperation if it is not 1-agent solvable.
In other words, given a solvable MAP problem that satisfies RC, any plan must involve more than one agent.

Definition 7 (Minimally k-agent Solvable). Given a solvable MAP problem P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), it is minimally k-agent solvable if it is k-agent solvable, and not
(k‚àí1)-agent solvable.
Corollary 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining the minimally solvable k (k ‚â§ |Œ¶|)
is PSPACE-complete.
Although directly querying for RC is intractable, we aim
to identify all the conditions (which can be quickly checked)
that can cause RC. We first define a few terms that are used
in the following discussions.
We note that the reference of agent is explicit in the action (i.e., ground operator) parameters. Although actions are
unique for each agent, two different agents may be capable
of executing actions that are instantiated from the same operator, with all other parameters being identical. To identify
such cases, we introduce the notion of action signature.
Definition 8 (Action Signature (AS)). An action signature is
an action with the reference of the executing agent replaced
by a global EX-AG symbol.
For example, an action signature in the IPC logistics domain is drive(EX-AG, pgh-po, pgh-airport). EX-AG is
a global symbol to denote the executing agent, which is not
used to distinguish between action signatures. We denote the
set of action signatures for œÜ ‚àà Œ¶ as AS(œÜ), which specifies
the capabilities of œÜ. Furthermore, we define the notion of
agent variable.
Definition 9 (Agent Variable (Agent Fluent)). A variable
(fluent) is an agent variable (fluent) if it is associated with
the reference of an agent.
Agent variables are used to specify agent state. For example, location(truck-pgh) is an agent variable since it is
associated with an agent truck-pgh. We use VœÜ ‚äÜ V to denote the set of agent variables that are associated with œÜ (i.e.,
variables that are present in the initial state or actions of œÜ).
Following this notation, we can rewrite a MAP problem
as P = hVo ‚à™VŒ¶ , Œ¶, Io ‚à™IŒ¶ , Go ‚à™GŒ¶ i, in which VŒ¶ = {VœÜ },

IŒ¶ = {IœÜ }, GŒ¶ = {GœÜ }, IœÜ = I ‚à©VœÜ and GœÜ = G‚à©VœÜ . Vo
denotes the set of non-agent variables; Io and Go are the set
of non-agent variables in I and G, respectively. In this paper, we assume that agents can only interact with each other
through non-agent variables (i.e., Vo ). In other words, agent
variables contain one and only one reference of agent. As a
result, we have VœÜ ‚à© VœÜ0 ‚â° ‚àÖ (œÜ 6= œÜ0 ). It seems to be possible to compile away exceptions by breaking agent variables
(with more than one reference of agent) into multiple variables and introducing non-agent variables to correlate them.
Definition 10 (Variable (Fluent) Signature (VS)). Given an
agent variable (fluent), its variable (fluent) signature is the
variable (fluent) with the reference of agent replaced by
EX-AG.
For example, location(truck-pgh) is an agent variable
for truck-pgh and its variable signature is location(EXAG). We denote the set of VSs for VœÜ as V S(œÜ), and use
V S as an operator so that V S(v) returns the VS of a variable
v; this operator returns any non-agent variable unchanged.

Classes of RC
In the following discussion, we assume that the specification of goal (i.e., G) in the MAP problems does not involve
agent variables (i.e., G ‚à© VœÜ = ‚àÖ or GœÜ = ‚àÖ), since we
are mostly interested in how to reach the desired world state
(i.e., specified in terms of Vo ). As aforementioned, we divide
RC problems into two classes as shown in Figure 1. Type1 RC involves problems with heterogeneous agents; type2 RC involves problems with homogeneous agents. Next,
we formally define each class and discuss the causes of RC.
Throughout this paper, when we denote a condition as X, the
negated condition is denoted as N-X.

Type-1 RC (RC with Heterogeneous Agents)
Given a MAP problem P = hV, Œ¶, I, Gi, the heterogeneity
of agents can be characterized by the following conditions:
‚Ä¢ Domain Heterogeneity (DH): ‚àÉv ‚àà VœÜ and D(v) \
D(V 0 ) 6= ‚àÖ, in which V 0 = {v 0 |v 0 ‚àà VœÜ0 (œÜ0 6= œÜ) and
V S(v) = V S(v 0 )}.
‚Ä¢ Variable Heterogeneity (VH): V S(œÜ) \ V S(Œ¶ \ œÜ) 6= ‚àÖ.
‚Ä¢ Capability Heterogeneity (CH): AS(œÜ) \ AS(Œ¶ \ œÜ) 6= ‚àÖ.
Definition 11 (Type-1 RC). An RC problem belongs to type1 RC if at least one of DH, VH and CH is satisfied for an
agent.
The condition that requires at least one of DH, VH and
CH to be satisfied is denoted as DVC in Figure 1. It is worth
noting that when considering certain objects (e.g., truck and
plane in the logistics domain) as agents rather than as resources, most of the RC problems in the IPC domains belong
to type-1 RC.

Causes of RC in Type-1
The most obvious condition for RC in type-1 RC problems
is due to the heterogeneity of agents. In the logistics
domain, for example, if any truck agent can only stay in
one city, the domains of the location variable for different

truck agents are different (DH). When there are packages that must be transferred between different locations
within cities, at least one truck agent for each city is
required (hence RC). In the rover domain, a rover that is
not equipped with a camera sensor would not be associated with the agent variable equipped f or imaging.
When we need both equipped f or imaging and
equipped f or rock analysis, and no rovers are equipped
with the sensors for both (VH), we have RC. Note that VH
does not specify any requirement on the variable value (i.e.,
the state); however, when the domain of a variable contains
only a single value, e.g., equipped f or imaging, we
assume in this paper that this variable is always defined in
a positive manner, e.g., expressing cans instead of cannots.
In the logistics domain, given that the truck agent cannot
fly (CH), when a package must be delivered from a city to
a non-airport location of another city, at least a truck and a
plane are required. Note that DH, VH and CH are closely
correlated.
However, note that 1) the presence of DVC in a solvable
MAP problem does not always cause RC, as shown in Figure
1; 2) the presence of DVC in a type-1 RC problem is not
always the cause of RC, as shown in Figure 2.
As an example for 1), when there is only one package to
be delivered from one location to another within the same
city, there is no need for a plane agent, even though we can
create a non-RC MAP problem with a plane and a truck
agent that satisfies CH (thus DVC).
As an example for 2), for navigating in a grid world, the
traversability of the world for all mobile agents can be restricted based on edge connections, i.e., connected(a, b),
in which a and b are vertices in the grid. Suppose that we
have two packages to be delivered to locations b and c,
respectively, which are both initially at a. There are two
truck agents at a that can be used for delivery. However,
the paths from a to both b and c are one-way only (i.e.,
connected(a, b) = true and connected(b, a) = f alse).
Even if one of the truck agents uses gas and the other one
uses diesel, thus satisfying DVC, it is clear that RC in this
problem is not caused by the heterogeneity of agents.
Type-1 RC problems in which RC is caused by only DVC
can be solved by a super agent (defined below), which is
an agent that combines all the domain values, variable signatures and capabilities (i.e., action signatures) of the other
agents. We refer to the subset of type-1 RC problems that
can be solved by a super agent as super-agent solvable, as
shown in Figure 2.
Definition 12 (Super Agent). A super agent is an agent œÜ‚àó
that satisfies:
‚Ä¢ ‚àÄv ‚àà VŒ¶ , ‚àÉv ‚àó ‚àà VœÜ‚àó , D(v ‚àó ) = D(V ), in which V =
{v|v ‚àà VŒ¶ and V S(v ‚àó ) = V S(v)}.
‚Ä¢ V S(œÜ‚àó ) = V S(Œ¶).
‚Ä¢ AS(œÜ‚àó ) = AS(Œ¶).
It is not difficult to see that most problems in the IPC domains are also super-agent solvable. For example, when we
have a truck-plane agent in the logistics domain that can both
fly (between airports of different cities) and drive (between
locations in the same cities), or when we have a rover that is

equipped with all sensors and can traverse all waypoints in
the rover domain.
From Figure 2, one may have already noticed that the conditions that cause RC in type-2 problems may also cause RC
in type-1 problems (i.e., indicated by the mixed cause region in Figure 2). For example, the aforementioned example
for navigating in a grid world demonstrates that the initial
states (specified in terms of the values for variables) of different agents may cause RC in type-1 problems. Note that
the initial states of different agents cannot be combined as
for domain values, variable signatures and capabilities in a
super agent construction; however, the special cases when
the domains of variables contain only a single value (when
we discussed VH in Causes of RC in Type-1) can also be
considered as cases when RC is caused by the initial state.

Figure 3: Example of a causal graph (ICGS). Variables in
goal G are shown as bold-circle nodes and agent VSs are
shown as double-circle nodes.

Type-2 RC (RC with Homogeneous Agents)
Type-2 RC involves homogeneous agents:
Definition 13 (Type-2 RC). An RC problem belongs to type2 RC if it satisfies N-DVC (for all agents).
Definition 13 states that an RC problem belongs to type-2
RC when all the agents are homogeneous.

Type-2 RC Caused by Traversability
One condition that causes RC in type-2 RC problems is the
traversability of the state space of variables, which is related to the initial states of the agents and the world, as we
previously discussed. Since the traversability is associated
with the evolution of variable values, we use causal graphs
to perform the analysis.
Definition 14 (Causal Graph). Given a MAP problem P =
hV, Œ¶, I, Gi, the causal graph G is a graph with directed and
undirected edges over the nodes V . For two nodes v and v 0
(v 6= v 0 ), a directed edge v ‚Üí v 0 is introduced if there exists
an action that updates v 0 while having a prevail condition
associated with v. An undirected edge v ‚àí v 0 is introduced if
there exists an action that updates both.
A typical example of a causal graph for an individual
agent is presented in Figure 3. For type-2 RC study, since
the agents are homogeneous, the causal graphs for all agents
are the same. Hence, we can use agent VSs to replace agent
variables; we refer to this modified causal graph for a single
agent in a type-2 RC problem as an individual causal graph
signature (ICGS). Next, we define the notions of closures
and traversable state space.
Definition 15 (Inner and Outer Closures (IC and OC)). An
inner closure (IC) in an ICGS is any set of variables for
which no other variables are connected to them with undirected edges; an outer closure (OC) of an IC is the set of
nodes that have directed edges going into nodes in the IC.
In Figure 3, {v2 , v3 } and {v4 } are examples of ICs. The
OC of {v2 , v3 } is {v1 } and the OC of {v4 } is {v3 }.
Definition 16 (Traversable State Space (TSS)). An IC has a
traversable state space if and only if: given any two states of
this IC, denoted by s and s0 , there exists a plan that connects
them, assuming that the state of the OC of this IC can be
changed freely within its state space.

In other words, an IC has a TSS if the traversal of its state
space is only dependent on the variables in its OC; this also
means that when the OC of an IC is empty, the state of the IC
can change freely. Note that static variables in the OC of an
IC can assume values that do not influence the traversability.
For example, the variables that are used to specify the connectivity of vertices in a grid, e.g., connected(a, b), can be
assigned to be true or f alse; although the variables that are
assigned to be true cannot change their values to be f alse,
they do not influence the traversability of the grid world. In
such cases, the associated ICs are still considered to have a
TSS. An ICGS in which all ICs have TSSs is referred to as
being traversable.

Type-2 RC Caused by Causal Loops
However, even a solvable MAP problem that satisfies NDVC for all agents while having a traversable ICGS can still
satisfy RC. An example is presented below.
The goal of this problem is to steal a diamond from a
room, in which the diamond is secured, and place it in another room. The diamond is protected by a stealth detection
system. If the diamond is taken, the system locks the door
of the room in which the diamond is kept, so that the insiders cannot exit. There is a switch to override the detection
system but it is located outside of the room. This problem is
modeled as above, in which the value is immediately specified after each variable. It is not difficult to see that the above
problem cannot be solved with a single agent.
Initial State:
location(agent1) room1
location(agent2) room1
location(diamond1) room1
doorLocked(room1) f alse
location(switch1) room2
Goal State:
location(diamond1) room2

Operators:
W alkT hrough(agent, door, f romRoom, toRoom):
prv: doorLocked(door) f alse
pre: location(agent) f romRoom
post: location(agent) toRoom
Steal(agent, diamond, room, door):
prv: location(agent) room
pre: doorLocked(door) u
pre: location(diamond) room
post: doorLocked(door) true
post: location(diamond) agent
Switch(agent, switch, room, door):
prv: location(switch) room
prv: location(agent) room
pre: doorLocked(door) u
post: doorLocked(door) f alse
P lace(agent, diamond, room):
prv: location(agent) room
pre: location(diamond) agent
post: location(diamond) room
Again, we construct the ICGS for this type-2 RC example, as shown in Figure 4. One key observation is that a
single agent cannot address this problem due to the fact
that W alkT hrough with the diamond to room2 requires
doorLocked(door1) = f alse, which is violated by the
Steal action to obtain the diamond in the first place. This
is clearly related to the loops in Figure 4. In particular, we
define the notion of causal loops.
Definition 17 (Causal Loop (CL)). A causal loop in the
ICGS is a directed loop that contains at least one directed
edge.
Note that undirected edges can be considered as edges in
either direction but at least one directed edge must be present
in a causal loop.

Gap between MAP and Single Agent Planning
We now establish in the following theorem that when none
of the previously discussed conditions (for both type-1 and
type-2 RC) hold in a MAP problem, this problem can be
solved by a single agent.
Theorem 1. Given a solvable MAP problem that satisfies
N-DVC for all agents, and for which the ICGS is traversable
and contains no causal loops, any single agent can also
achieve the goal.
Proof. Given no causal loops, the directed edges in the
ICGS divides the variables into levels, in which: 1) variables at each level do not appear in other levels; 2) higher
level variables are connected to lower level variables with
only directed edges going from higher levels to lower levels; 3) variables within each level are either not connected
or connected with undirected edges. For example, the variables in Figure 3 are divided into the following levels (from
high to low): {v1 }, {v2 , v3 }, {v4 }, {v5 , v7 }, {v6 , v8 }. Note
that this division is not unique.

Figure 4: ICGS for the diamond example that illustrates the
second condition that causes RC in type-2 RC problems. Actions (without parameters) are labeled along with their corresponding edges. The variables in G are shown as bold-box
nodes and agent VSs are shown as dashed-box nodes.
Next, we prove the result by induction based on the level.
Suppose that the ICGS has k levels and we have the following holds: given any trajectory of states for all variables,
there exists a plan whose execution traces of states include
this trajectory in the correct order.
When the ICGS has k + 1 levels: given any state s for all
variables from level 1 to k + 1, we know from the assumption that the ICGS is traversable that there exists a plan that
can update the variables at the k + 1 level from their current
states to the corresponding states in s. This plan (denoted
by œÄ), meanwhile, requires the freedom to change the states
of variables from level 1 to k. Given the induction assumption, we know that we can update these variables to their
required states in the correct order to satisfy œÄ; furthermore,
these updates (at level k and above) also do not influence
the variables at the k + 1 level (hence do not influence œÄ).
Once the states of the variables at the k + 1 level are updated to match those in s, we can then update variables at
level 1 to k to match their states in s accordingly. Using this
process, we can incrementally build a plan whose execution
traces of states contain any given trajectory of states for all
the variables in the correct order.
Furthermore, the induction holds when there is only one
level given that ICGS is traversable. Hence, the induction
conclusion holds. The main conclusion directly follows.

Towards an Upper Bound for Type-2 RC
In this section, we investigate type-2 RC problem to obtain
upper bounds on the k (Definition 7), based on different relaxations of the two conditions that cause RC in type-2 RC
problems. We first relax the assumption regarding causal
loops (CLs) and show that the relaxation process is associated with how certain CLs can be broken.
We notice that there are two kinds of CLs in ICGS. The
first kind contains agent VSs while the second kind does not.
Although we cannot break CLs for the second kind, it is possible to break CLs for the first kind. The motivation is that
certain edges in these CLs can be removed when there is

Figure 5: Illustration of the process for breaking causal loops
in the diamond example, in which the CLs are broken by
removing the edge marked with a triangle in Figure 4. Two
agent VSs are introduced to replace the original agent VS.
no need to update the associated agent VSs. In our diamond
example, when there are two agents in room1 and room2,
respectively, and they can stay where they are during the execution of the plan, there is no need to W alkT hrough and
hence the associated edges can be removed to break the CLs.
Figure 5 shows this process. Based on this observation, we
introduce the following lemma.
Lemma 2. Given a solvable MAP problem that satisfies NDVC for all agents and for which the ICGS is traversable, if
no CLs contain agent VSs and all the edges going in and out
of agent VSs are directed, the minimum number of agents required is upper bounded by √óv‚ààCR(Œ¶) |D(v)|, when assuming that the agents can choose their initial states, in which
CR(Œ¶) is constructed as follows:
1. add the set of agent VSs that are in the CLs into CR(Œ¶);
2. add in an agent VS into CR(Œ¶) if there exists a directed
edge that goes into it from any variable in CR(Œ¶);
3. iterate 2 until no agent VSs can be added.
Proof. Based on the previous discussions, we can remove
edges that are connected to agent VSs to break loops. For
each variable in CR(Œ¶), denoted by v, we introduce a set of
variables N = {v1 , v2 , ..., v|D(v)| } to replace v. Any edges
connecting to v from other variables are duplicated on all
variables in N , except for the edges that go into v. Each
variable vi ‚àà N has a domain with a single value; this value
for each variable in N is different and chosen from D(v).
Note that these new variables do not affect the traversability
of the ICGS.
From Theorem 1, we know that a virtual agent œÜ+ that
can simultaneously assume all the states that are the different
permutations of states for CR(Œ¶) can achieve the goal. We
can simulate œÜ+ using √óv‚ààCR(Œ¶) |D(v)| agents as follows.
We choose the agent initial states according to the permutations of states for CR(Œ¶), while choosing the same states
for all the other agent VSs according to œÜ+ . Given a plan for
œÜ+ , we start from the first action. Given that all permutations
of states for CR(Œ¶) are assumed by an agent, we can find an
agent, denoted by œÜ, that can execute this action: 1) If this

action updates an agent VS in CR(Œ¶), we do not need to
execute this action based on the following reasoning. Given
that all edges going in and out of agent VSs are directed, we
know that this action does not update Vo . (Otherwise, there
must be an undirected edge connecting a variable in Vo to
this agent VS. Similarly, we also know that this action does
not update more than one agent VS.). As a result, it does not
influence the execution of the next action. 2) If this action
updates an agent VS that is not in CR(Œ¶), we know that this
action cannot have variables in CR(Œ¶) as preconditions or
prevail conditions, since otherwise this agent VS would be
included in CR(Œ¶) given its construction process. Hence,
all the agents can execute the action to update this agent VS,
given that all the agent VSs outside of CR(Œ¶) are always
kept synchronized in the entire process (in order to simulate
œÜ+ ). 3) Otherwise, this action must be updating only Vo and
we can execute the action on œÜ.
Following the above process for all the actions in œÜ+ ‚Äôs
plan to achieve the goal. Hence, the conclusion holds.
Next, we investigate the relaxation of the traversability of
the ICGS.
Lemma 3. Given a solvable MAP problem that satisfies NDVC for all agents, if all the edges going in and out of agent
VSs are directed, the minimum number of agents required is
upper bounded by √óv‚ààV S(Œ¶) |D(v)|, when assuming that the
agents can choose their initial states.
Proof. Given a valid plan œÄM AP for the problem, we can
solve the problem using √óv‚ààV S(Œ¶) |D(v)| agents as follows:
first, we choose the agent initial states according to the permutations of state for V S(Œ¶).
The process is similar to that in Lemma 2. We start from
the first action. Given that all permutations of V S(Œ¶) are assumed by an agent, we can find an agent, denoted by œÜ, that
can execute this action: if this action updates some agent
VSs in V S(Œ¶), we do not need to execute this action; otherwise, the action must be updating only Vo and we can execute the action on œÜ.
Following the above process for all the actions in œÄM AP
to achieve the goal. Hence, the conclusion holds.
Note that the bounds in Lemma 2 and 3 are upper bounds
and the minimum number of agents actually required may
be smaller. Nevertheless, for the simple scenario in our diamond example, the assumptions of both lemmas are satisfied and the bounds returned are 2 for both, which happens
to be exactly the k in Definition 7. In future work, we plan
to investigate other relaxations and establish the tightness of
these bounds.

Conclusion
In this paper, we introduce the notion of required cooperation (RC), which answers two questions: 1) whether more
than one agent is required for a solvable MAP problem, and
2) what is the minimum number of agents required for the
problem. We show that the exact answers to these questions
are difficult to provide. To facilitate our analysis, we first
divide RC problems into two class ‚Äì type-1 RC involves

heterogeneous agents and type-2 RC involves homogeneous
agents. For the first question, we show that most of the problems in the common planning domains belong to type-1 RC;
the set of type-1 RC problems in which RC is only caused
by DVC can be solved with a super agent. For type-2 RC
problems, we show that RC is caused when the state space
is not traversable or when there are causal loops in the causal
graph; we provide upper bounds for the answer of the second question, based on different relaxations of the conditions that cause RC in type-2 RC problems. These relaxations are associated with, for example, how certain causal
loops can be broken in the causal graph.

Acknowledgement
This research is supported in part by the ARO grant
W911NF-13-1-0023, and the ONR grants N00014-13-10176 and N00014-13-1-0519.

References
[Amir and Engelhardt 2003] Amir, E., and Engelhardt, B.
2003. Factored planning. In Proceedings of the 18th International Joint Conferences on Artificial Intelligence, 929‚Äì935.
[Bacchus and Yang 1993] Bacchus, F., and Yang, Q. 1993.
Downward refinement and the efficiency of hierarchical
problem solving. Artificial Intelligence 71:43‚Äì100.
[Backstrom and Nebel 1996] Backstrom, C., and Nebel, B.
1996. Complexity results for sas+ planning. Computational
Intelligence 11:625‚Äì655.
[Brafman and Domshlak 2008] Brafman, R. I., and Domshlak, C. 2008. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In Proceedings of the 18th International Conference on Automated Planning and Scheduling, 28‚Äì35. AAAI Press.
[Brafman and Domshlak 2013] Brafman, R. I., and Domshlak, C. 2013. On the complexity of planning for agent teams
and its implications for single agent planning. Artificial Intelligence 198(0):52 ‚Äì 71.
[Brafman 2006] Brafman, R. I. 2006. Factored planning:
How, when, and when not. In Proceedings of the 21st National Conference on Artificial Intelligence, 809‚Äì814.
[Bylander 1991] Bylander, T. 1991. Complexity results for
planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence, volume 1, 274‚Äì279.
[Decker and Lesser 1992] Decker, K. S., and Lesser, V. R.
1992. Generalizing the partial global planning algorithm.
International Journal of Cooperative Information Systems
1:319‚Äì346.
[Durfee and Lesser 1991] Durfee, E., and Lesser, V. R. 1991.
Partial global planning: A coordination framework for distributed hypothesis formation. IEEE Transactions on Systems, Man, and Cybernetics 21:1167‚Äì1183.
[Helmert 2006] Helmert, M. 2006. The fast downward planning system. Journal of Artificial Intelligence Research
26:191‚Äì246.
[Jonsson and Rovatsos 2011] Jonsson, A., and Rovatsos, M.
2011. Scaling Up Multiagent Planning: A Best-Response

Approach. In Proceedings of the 21th International Conference on Automated Planning and Scheduling, 114‚Äì121.
AAAI Press.
[Knoblock 1994] Knoblock, C. 1994. Automatically generating abstractions for planning. Artificial Intelligence
68:243‚Äì302.
[Kvarnstrom 2011] Kvarnstrom, J. 2011. Planning for
loosely coupled agents using partial order forward-chaining.
In Proceedings of the 21th International Conference on Automated Planning and Scheduling.
[Nissim and Brafman 2012] Nissim, R., and Brafman, R. I.
2012. Multi-agent a* for parallel and distributed systems.
In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, volume 3, 1265‚Äì
1266.
[Nissim, Brafman, and Domshlak 2010] Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010. A general, fully distributed multi-agent planning algorithm. In Proceedings of
the 11th International Conference on Autonomous Agents
and Multiagent Systems, 1323‚Äì1330.
[Torreno, Onaindia, and Sapena 2012] Torreno, A.; Onaindia, E.; and Sapena, O. 2012. An approach to multi-agent
planning with incomplete information. In European Conference on Artificial Intelligence, volume 242, 762‚Äì767.

A Formal Analysis of Required Cooperation in Multi-agent Planning
Yu Zhang and Subbarao Kambhampati

arXiv:1404.5643v1 [cs.AI] 22 Apr 2014

School of Computing and Informatics
Arizona State University
Tempe, Arizona 85281 USA
{yzhan442,rao}@asu.edu

Abstract
Research on multi-agent planning has been popular in
recent years. While previous research has been motivated by the understanding that, through cooperation,
multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required
to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a
formal discussion from the planning aspect. We first
show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing
the problems that require cooperation (referred to as RC
problems) into two classes ‚Äì problems with heterogeneous and homogeneous agents, we aim to identify all
the conditions that can cause RC in these two classes.
We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper
bound on the minimum number of agents required for
RC problems with homogeneous agents. This study not
only provides new insights into multi-agent planning,
but also has many applications. For example, in humanrobot teaming, when a robot cannot achieve a task, it
may be due to RC. In such cases, the human teammate
should be informed and, consequently, coordinate with
other available robots for a solution.

Introduction
A multi-agent planning (MAP) problem differs from a single
agent planning (SAP) problem in that more than one agent
is used in planning. While a (non-temporal) MAP problem
can be compiled into a SAP problem by considering agents
as resources, the search space grows exponentially with the
number of such resources. Given that a SAP problem with
a single such resource is in general PSPACE-complete (Bylander 1991), running a single planner to solve MAP is inefficient. Hence, previous research has generally agreed that
agents should be considered as separate entities for planning,
and thus has been mainly concentrated on how to explore
the interactions between the agents (i.e., loosely-coupled vs.
tightly-coupled) to reduce the search space, and how to perform the search more efficiently in a distributed fashion.
c 2014, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

However, there has been little discussion on whether multiple agents are required for a planning problem in the first
place. If a single agent is sufficient, solving the problem with
multiple agents becomes an efficiency matter, e.g., shortening the makespan of the plan. Problems of this nature can
be solved in two separate steps: planning with a single agent
and optimizing with multiple agents. In such a way, the difficulty of finding a solution may potentially be reduced.
In this paper, we aim to answer the following questions: 1)
Given a problem with a set of agents, what are the conditions
that make cooperation between multiple agents required to
solve the problem; 2) How to determine the minimum number of agents required for the problem. We show that providing the exact answers is intractable. Instead, we attempt to
provide approximate answers. To facilitate our analysis, we
first divide MAP problems into two classes ‚Äì MAP problems with heterogeneous agents, and MAP problems with
homogeneous agents. Consequently, the MAP problems that
require cooperation (referred to as RC problems) are also divided into two classes ‚Äì type-1 RC (RC with heterogeneous
agents) and type-2 RC (RC with homogeneous agents) problems. Figure 1 shows these divisions.
For the two classes of RC problems, we aim to identify
all the conditions that can cause RC. Figure 2 presents these
conditions and their relationships to the two classes of RC
problems. We establish that at least one of these conditions
must be present in order to have RC. Furthermore, we show
that most of the problems in common planning domains belong to type-1 RC, which is identified by three conditions
in the problem formulation that define the heterogeneity of
agents; most of the problems in type-1 RC can be solved by a
super agent. For type-2 RC, we show that RC is only caused
when the state space is not traversable or when there are
causal loops in the causal graph. We provide upper bounds
for the answer of the second question for type-2 RC problems, based on different relaxations of the conditions that
cause RC, which are associated with, for example, how certain causal loops can be broken in the causal graph.
The answers to these questions not only enrich our fundamental understanding of MAP, but also have many applications. For example, in a human robot teaming scenario, a human may be remotely working with multiple robots. When
a robot is assigned a task that it cannot achieve, it is useful
to determine whether the failure is due to the fact that the

Figure 1: Division of MAP problems into MAP with heterogeneous and homogeneous agents. Consequently, RC problems are also divided into two classes: type-1 RC involves
RC problems with heterogeneous agents and type-2 RC involves RC problems with homogeneous agents.

Figure 2: Causes of required cooperation in RC problems.
task is simply unachievable or the task requires more than
one robot. In the latter case, it is useful then to determine
how many extra robots must be sent to help. The answers
can also be applied to multi-robot systems, and are useful in
general to any multi-agent systems in which the team compositions can dynamically change (e.g., when the team must
be divided to solve different problems).
The rest of the paper is organized as follows. After a review of the related literature, we start the discussion of required cooperation for MAP, in which we answer the above
questions in an orderly fashion. We conclude afterward.

Related Work
One of the earlier works on MAP is the PGP framework
by (Durfee and Lesser 1991; Decker and Lesser 1992). Recently, the MAP problem has started to receive an increasing amount of attention. Most of these recent research works
consider agents separately for planning, and have been concentrated on how to explore the structure of agent interactions to reduce the search space, as well as solving the problem in a distributed fashion. (Nissim, Brafman, and Domshlak 2010) provide a search method by compiling MAP into a
constraint satisfaction problem (CSP), and then using a distributed CSP framework to solve it. The MAP formulation is
based on an extension of the STRIPS language called MASTRIPS (Brafman and Domshlak 2008). In MA-STRIPS,
actions are categorized into public and private actions. Public actions can influence other agents while private actions
cannot. In this way, it is shown by (Brafman and Domsh-

lak 2008) that the search complexity of MAP is exponential in the tree-width of the agent interaction graph. Due to
the poor performance of DisCSP based approaches, (Nissim and Brafman 2012) apply the A‚àó search algorithm in a
distributed manner, which represents one of the state-of-art
MAP solvers. (Torreno, Onaindia, and Sapena 2012) propose a POP-based distributed planning framework for MAP,
which uses a cooperative refinement planning technique that
can handle planning with any level of coupling between the
agents. Each agent at any step proposes a refinement step
to improve the current group plan. Their approach does not
assume complete information. A similar paradigm is taken
by (Kvarnstrom 2011). An iterative best-response planning
and plan improvement technique using standard SAP algorithms is provided by (Jonsson and Rovatsos 2011), which
considers the previous singe agent plans as constraints to be
satisfied while the following agents perform planning.
Given a problem, all of these MAP approaches solve it
using the given set of agents, without first asking whether
multiple agents are really required, let alone what is the minimum number of agents required. Answers to these questions not only separate MAP from SAP in a fundamental
way, but also have real world applications when the team
compositions can dynamically change. In this paper, we
analyze these questions using the SAS+ formalism (Backstrom and Nebel 1996) with causal graph (Knoblock 1994;
Helmert 2006), which is often discussed in the context
of factored planning (Bacchus and Yang 1993; Amir and
Engelhardt 2003; Brafman 2006; Brafman and Domshlak
2013). The causal graph captures the interaction between
different variables; intuitively, it can also capture the interactions between agents since agents affect each other through
these variables. In fact, (Brafman and Domshlak 2013) mention the causal graph‚Äôs relation to the agent interaction graph
when each variable is associated with a single agent.

Multi-agent Planning (MAP)
In this paper, we start the analysis of RC in the simplest
scenarios ‚Äì with instantaneous actions and sequential execution. The possibility of RC can only increase when we
extend the model to the temporal domain, in which concurrent or synchronous actions must be considered. We develop
our analysis of required cooperation for MAP based on the
SAS+ formalism (Backstrom and Nebel 1996).

Background
Definition 1. A SAS+ problem is given by a tuple P =
hV, A, I, Gi, where:
‚Ä¢ V = {v1 , ..., vn } is a set of state variables. Each variable
vi ‚àà V is associated with its domain D(vi ), which is used
to define an extended domain D(vi )+ = D(vi )‚à™u, where
u denotes the undefined value. The state space is defined
as SV+ = D(v1 )+ √ó ... √ó D(vn )+ ; s[vi ] denotes the value
of the variable vi in a state s ‚àà SV+ .
‚Ä¢ A = {a1 , ..., am } is a finite set of actions. Each action aj is a tuple hpre(aj ), post(aj ), prv(aj )i, where
pre(aj ), post(aj ), prv(aj ) ‚äÜ SV+ are the preconditions,
postconditions and prevail conditions of aj , respectively.

We also use pre(aj )[vi ], post(aj )[vi ], prv(aj )[vi ] to denote the corresponding values of vi .
‚Ä¢ I and G denote the initial and goal state, respectively.

Lemma 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining whether it satisfies RC is PSPACEcomplete.

A plan in SAS+ is often defined to be a total-order plan:
Definition 2. A plan œÄ in SAS+ is a sequence of actions
œÄ = ha1 , ..., al i.

Proof. First, it is not difficult to show that the RC decision problem belongs to PSPACE, since we only need to
verify that P = hV, œÜ, I, Gi is unsolvable for all œÜ ‚àà Œ¶,
given that the initial problem is known to be solvable. Then,
we complete the proof by reducing from the PLANSAT
problem, which is PSPACE-complete in general (Bylander
1991). Given a PLANSAT problem (with a single agent), the
idea is that we can introduce a second agent with only one
action. This action directly achieves the goal but requires
an action (with all preconditions satisfied in the initial state)
of the initial agent to provide a precondition that is not initially satisfied. We know that this constructed MAP problem
is solvable. If the algorithm for the RC decision problem
returns that cooperation is required for this MAP problem,
we know that the original PLANSAT problem is unsolvable;
otherwise, it is solvable.

Given two states s1 , s2 ‚àà SV+ , (s1 ‚äï s2 ) denotes that s1 is
updated by s2 , and is subject to the following for all vi ‚àà V :

s2 [vi ] if s2 [vi ] 6= u,
(1)
(s1 ‚äï s2 )[vi ] =
s1 [vi ] otherwise.
Given a variable with two values x, y in which one of
them is u, x t y is defined to be the other value. t can be
extended to two states s1 and s2 , such that s1 t s2 [vi] =
s1 [vi ] t s2 [vi ] for all vi ‚àà V . s1 v s2 if and only if
‚àÄvi ‚àà V, s1 [vi ] = u or s1 [vi ] = s2 [vi ]. The state resulting from executing a plan œÄ can then be defined recursively
using a re operator as follows:
(
re(s, hœÄ; oi) =

re(s, hœÄi) ‚äï post(o)
if pre(o) t prv(o) v re(s, hœÄi),
s otherwise.

(2)

in which re(s, hi) = s, o is an action, and ; is the concatenation operator.

Extension to MAP
To extend the previous formalism to MAP without losing
generality, we minimally modify the definitions.
Definition 3. A SAS+ MAP problem is given by a tuple Œ† =
hV, Œ¶, I, Gi, where:
‚Ä¢ Œ¶ = {œÜg } is the set of agents; each agent œÜg is associated
with a set of actions A(œÜg ).
Definition 4. A plan œÄM AP in MAP is a sequence of agentaction pairs œÄM AP = h(a1 , œÜ(a1 )), ..., (aL , œÜ(aL ))i, in
which œÜ(a) returns the agent for the action a and L is the
length of the plan.
We do not need to consider concurrency or synchronization given that actions are assumed to be instantaneous.

Required Cooperation for MAP
Next, we formally define the notion of required cooperation
and other useful terms that are used in the following analyses. We assume throughout the paper that more than one
agent is considered (i.e., |Œ¶| > 1).

Required Cooperation
Definition 5 (k-agent Solvable). Given a MAP problem
P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), the problem is k-agent solvable
if ‚àÉŒ¶k ‚äÜ Œ¶ (|Œ¶k | = k), such that hV, Œ¶k , I, Gi is solvable.
Definition 6 (Required Cooperation (RC)). Given a solvable MAP problem P = hV, Œ¶, I, Gi, there is required cooperation if it is not 1-agent solvable.
In other words, given a solvable MAP problem that satisfies RC, any plan must involve more than one agent.

Definition 7 (Minimally k-agent Solvable). Given a solvable MAP problem P = hV, Œ¶, I, Gi (|Œ¶| ‚â• k), it is minimally k-agent solvable if it is k-agent solvable, and not
(k‚àí1)-agent solvable.
Corollary 1. Given a solvable MAP problem P =
hV, Œ¶, I, Gi, determining the minimally solvable k (k ‚â§ |Œ¶|)
is PSPACE-complete.
Although directly querying for RC is intractable, we aim
to identify all the conditions (which can be quickly checked)
that can cause RC. We first define a few terms that are used
in the following discussions.
We note that the reference of agent is explicit in the action (i.e., ground operator) parameters. Although actions are
unique for each agent, two different agents may be capable
of executing actions that are instantiated from the same operator, with all other parameters being identical. To identify
such cases, we introduce the notion of action signature.
Definition 8 (Action Signature (AS)). An action signature is
an action with the reference of the executing agent replaced
by a global EX-AG symbol.
For example, an action signature in the IPC logistics domain is drive(EX-AG, pgh-po, pgh-airport). EX-AG is
a global symbol to denote the executing agent, which is not
used to distinguish between action signatures. We denote the
set of action signatures for œÜ ‚àà Œ¶ as AS(œÜ), which specifies
the capabilities of œÜ. Furthermore, we define the notion of
agent variable.
Definition 9 (Agent Variable (Agent Fluent)). A variable
(fluent) is an agent variable (fluent) if it is associated with
the reference of an agent.
Agent variables are used to specify agent state. For example, location(truck-pgh) is an agent variable since it is
associated with an agent truck-pgh. We use VœÜ ‚äÜ V to denote the set of agent variables that are associated with œÜ (i.e.,
variables that are present in the initial state or actions of œÜ).
Following this notation, we can rewrite a MAP problem
as P = hVo ‚à™VŒ¶ , Œ¶, Io ‚à™IŒ¶ , Go ‚à™GŒ¶ i, in which VŒ¶ = {VœÜ },

IŒ¶ = {IœÜ }, GŒ¶ = {GœÜ }, IœÜ = I ‚à©VœÜ and GœÜ = G‚à©VœÜ . Vo
denotes the set of non-agent variables; Io and Go are the set
of non-agent variables in I and G, respectively. In this paper, we assume that agents can only interact with each other
through non-agent variables (i.e., Vo ). In other words, agent
variables contain one and only one reference of agent. As a
result, we have VœÜ ‚à© VœÜ0 ‚â° ‚àÖ (œÜ 6= œÜ0 ). It seems to be possible to compile away exceptions by breaking agent variables
(with more than one reference of agent) into multiple variables and introducing non-agent variables to correlate them.
Definition 10 (Variable (Fluent) Signature (VS)). Given an
agent variable (fluent), its variable (fluent) signature is the
variable (fluent) with the reference of agent replaced by
EX-AG.
For example, location(truck-pgh) is an agent variable
for truck-pgh and its variable signature is location(EXAG). We denote the set of VSs for VœÜ as V S(œÜ), and use
V S as an operator so that V S(v) returns the VS of a variable
v; this operator returns any non-agent variable unchanged.

Classes of RC
In the following discussion, we assume that the specification of goal (i.e., G) in the MAP problems does not involve
agent variables (i.e., G ‚à© VœÜ = ‚àÖ or GœÜ = ‚àÖ), since we
are mostly interested in how to reach the desired world state
(i.e., specified in terms of Vo ). As aforementioned, we divide
RC problems into two classes as shown in Figure 1. Type1 RC involves problems with heterogeneous agents; type2 RC involves problems with homogeneous agents. Next,
we formally define each class and discuss the causes of RC.
Throughout this paper, when we denote a condition as X, the
negated condition is denoted as N-X.

Type-1 RC (RC with Heterogeneous Agents)
Given a MAP problem P = hV, Œ¶, I, Gi, the heterogeneity
of agents can be characterized by the following conditions:
‚Ä¢ Domain Heterogeneity (DH): ‚àÉv ‚àà VœÜ and D(v) \
D(V 0 ) 6= ‚àÖ, in which V 0 = {v 0 |v 0 ‚àà VœÜ0 (œÜ0 6= œÜ) and
V S(v) = V S(v 0 )}.
‚Ä¢ Variable Heterogeneity (VH): V S(œÜ) \ V S(Œ¶ \ œÜ) 6= ‚àÖ.
‚Ä¢ Capability Heterogeneity (CH): AS(œÜ) \ AS(Œ¶ \ œÜ) 6= ‚àÖ.
Definition 11 (Type-1 RC). An RC problem belongs to type1 RC if at least one of DH, VH and CH is satisfied for an
agent.
The condition that requires at least one of DH, VH and
CH to be satisfied is denoted as DVC in Figure 1. It is worth
noting that when considering certain objects (e.g., truck and
plane in the logistics domain) as agents rather than as resources, most of the RC problems in the IPC domains belong
to type-1 RC.

Causes of RC in Type-1
The most obvious condition for RC in type-1 RC problems
is due to the heterogeneity of agents. In the logistics
domain, for example, if any truck agent can only stay in
one city, the domains of the location variable for different

truck agents are different (DH). When there are packages that must be transferred between different locations
within cities, at least one truck agent for each city is
required (hence RC). In the rover domain, a rover that is
not equipped with a camera sensor would not be associated with the agent variable equipped f or imaging.
When we need both equipped f or imaging and
equipped f or rock analysis, and no rovers are equipped
with the sensors for both (VH), we have RC. Note that VH
does not specify any requirement on the variable value (i.e.,
the state); however, when the domain of a variable contains
only a single value, e.g., equipped f or imaging, we
assume in this paper that this variable is always defined in
a positive manner, e.g., expressing cans instead of cannots.
In the logistics domain, given that the truck agent cannot
fly (CH), when a package must be delivered from a city to
a non-airport location of another city, at least a truck and a
plane are required. Note that DH, VH and CH are closely
correlated.
However, note that 1) the presence of DVC in a solvable
MAP problem does not always cause RC, as shown in Figure
1; 2) the presence of DVC in a type-1 RC problem is not
always the cause of RC, as shown in Figure 2.
As an example for 1), when there is only one package to
be delivered from one location to another within the same
city, there is no need for a plane agent, even though we can
create a non-RC MAP problem with a plane and a truck
agent that satisfies CH (thus DVC).
As an example for 2), for navigating in a grid world, the
traversability of the world for all mobile agents can be restricted based on edge connections, i.e., connected(a, b),
in which a and b are vertices in the grid. Suppose that we
have two packages to be delivered to locations b and c,
respectively, which are both initially at a. There are two
truck agents at a that can be used for delivery. However,
the paths from a to both b and c are one-way only (i.e.,
connected(a, b) = true and connected(b, a) = f alse).
Even if one of the truck agents uses gas and the other one
uses diesel, thus satisfying DVC, it is clear that RC in this
problem is not caused by the heterogeneity of agents.
Type-1 RC problems in which RC is caused by only DVC
can be solved by a super agent (defined below), which is
an agent that combines all the domain values, variable signatures and capabilities (i.e., action signatures) of the other
agents. We refer to the subset of type-1 RC problems that
can be solved by a super agent as super-agent solvable, as
shown in Figure 2.
Definition 12 (Super Agent). A super agent is an agent œÜ‚àó
that satisfies:
‚Ä¢ ‚àÄv ‚àà VŒ¶ , ‚àÉv ‚àó ‚àà VœÜ‚àó , D(v ‚àó ) = D(V ), in which V =
{v|v ‚àà VŒ¶ and V S(v ‚àó ) = V S(v)}.
‚Ä¢ V S(œÜ‚àó ) = V S(Œ¶).
‚Ä¢ AS(œÜ‚àó ) = AS(Œ¶).
It is not difficult to see that most problems in the IPC domains are also super-agent solvable. For example, when we
have a truck-plane agent in the logistics domain that can both
fly (between airports of different cities) and drive (between
locations in the same cities), or when we have a rover that is

equipped with all sensors and can traverse all waypoints in
the rover domain.
From Figure 2, one may have already noticed that the conditions that cause RC in type-2 problems may also cause RC
in type-1 problems (i.e., indicated by the mixed cause region in Figure 2). For example, the aforementioned example
for navigating in a grid world demonstrates that the initial
states (specified in terms of the values for variables) of different agents may cause RC in type-1 problems. Note that
the initial states of different agents cannot be combined as
for domain values, variable signatures and capabilities in a
super agent construction; however, the special cases when
the domains of variables contain only a single value (when
we discussed VH in Causes of RC in Type-1) can also be
considered as cases when RC is caused by the initial state.

Figure 3: Example of a causal graph (ICGS). Variables in
goal G are shown as bold-circle nodes and agent VSs are
shown as double-circle nodes.

Type-2 RC (RC with Homogeneous Agents)
Type-2 RC involves homogeneous agents:
Definition 13 (Type-2 RC). An RC problem belongs to type2 RC if it satisfies N-DVC (for all agents).
Definition 13 states that an RC problem belongs to type-2
RC when all the agents are homogeneous.

Type-2 RC Caused by Traversability
One condition that causes RC in type-2 RC problems is the
traversability of the state space of variables, which is related to the initial states of the agents and the world, as we
previously discussed. Since the traversability is associated
with the evolution of variable values, we use causal graphs
to perform the analysis.
Definition 14 (Causal Graph). Given a MAP problem P =
hV, Œ¶, I, Gi, the causal graph G is a graph with directed and
undirected edges over the nodes V . For two nodes v and v 0
(v 6= v 0 ), a directed edge v ‚Üí v 0 is introduced if there exists
an action that updates v 0 while having a prevail condition
associated with v. An undirected edge v ‚àí v 0 is introduced if
there exists an action that updates both.
A typical example of a causal graph for an individual
agent is presented in Figure 3. For type-2 RC study, since
the agents are homogeneous, the causal graphs for all agents
are the same. Hence, we can use agent VSs to replace agent
variables; we refer to this modified causal graph for a single
agent in a type-2 RC problem as an individual causal graph
signature (ICGS). Next, we define the notions of closures
and traversable state space.
Definition 15 (Inner and Outer Closures (IC and OC)). An
inner closure (IC) in an ICGS is any set of variables for
which no other variables are connected to them with undirected edges; an outer closure (OC) of an IC is the set of
nodes that have directed edges going into nodes in the IC.
In Figure 3, {v2 , v3 } and {v4 } are examples of ICs. The
OC of {v2 , v3 } is {v1 } and the OC of {v4 } is {v3 }.
Definition 16 (Traversable State Space (TSS)). An IC has a
traversable state space if and only if: given any two states of
this IC, denoted by s and s0 , there exists a plan that connects
them, assuming that the state of the OC of this IC can be
changed freely within its state space.

In other words, an IC has a TSS if the traversal of its state
space is only dependent on the variables in its OC; this also
means that when the OC of an IC is empty, the state of the IC
can change freely. Note that static variables in the OC of an
IC can assume values that do not influence the traversability.
For example, the variables that are used to specify the connectivity of vertices in a grid, e.g., connected(a, b), can be
assigned to be true or f alse; although the variables that are
assigned to be true cannot change their values to be f alse,
they do not influence the traversability of the grid world. In
such cases, the associated ICs are still considered to have a
TSS. An ICGS in which all ICs have TSSs is referred to as
being traversable.

Type-2 RC Caused by Causal Loops
However, even a solvable MAP problem that satisfies NDVC for all agents while having a traversable ICGS can still
satisfy RC. An example is presented below.
The goal of this problem is to steal a diamond from a
room, in which the diamond is secured, and place it in another room. The diamond is protected by a stealth detection
system. If the diamond is taken, the system locks the door
of the room in which the diamond is kept, so that the insiders cannot exit. There is a switch to override the detection
system but it is located outside of the room. This problem is
modeled as above, in which the value is immediately specified after each variable. It is not difficult to see that the above
problem cannot be solved with a single agent.
Initial State:
location(agent1) room1
location(agent2) room1
location(diamond1) room1
doorLocked(room1) f alse
location(switch1) room2
Goal State:
location(diamond1) room2

Operators:
W alkT hrough(agent, door, f romRoom, toRoom):
prv: doorLocked(door) f alse
pre: location(agent) f romRoom
post: location(agent) toRoom
Steal(agent, diamond, room, door):
prv: location(agent) room
pre: doorLocked(door) u
pre: location(diamond) room
post: doorLocked(door) true
post: location(diamond) agent
Switch(agent, switch, room, door):
prv: location(switch) room
prv: location(agent) room
pre: doorLocked(door) u
post: doorLocked(door) f alse
P lace(agent, diamond, room):
prv: location(agent) room
pre: location(diamond) agent
post: location(diamond) room
Again, we construct the ICGS for this type-2 RC example, as shown in Figure 4. One key observation is that a
single agent cannot address this problem due to the fact
that W alkT hrough with the diamond to room2 requires
doorLocked(door1) = f alse, which is violated by the
Steal action to obtain the diamond in the first place. This
is clearly related to the loops in Figure 4. In particular, we
define the notion of causal loops.
Definition 17 (Causal Loop (CL)). A causal loop in the
ICGS is a directed loop that contains at least one directed
edge.
Note that undirected edges can be considered as edges in
either direction but at least one directed edge must be present
in a causal loop.

Gap between MAP and Single Agent Planning
We now establish in the following theorem that when none
of the previously discussed conditions (for both type-1 and
type-2 RC) hold in a MAP problem, this problem can be
solved by a single agent.
Theorem 1. Given a solvable MAP problem that satisfies
N-DVC for all agents, and for which the ICGS is traversable
and contains no causal loops, any single agent can also
achieve the goal.
Proof. Given no causal loops, the directed edges in the
ICGS divides the variables into levels, in which: 1) variables at each level do not appear in other levels; 2) higher
level variables are connected to lower level variables with
only directed edges going from higher levels to lower levels; 3) variables within each level are either not connected
or connected with undirected edges. For example, the variables in Figure 3 are divided into the following levels (from
high to low): {v1 }, {v2 , v3 }, {v4 }, {v5 , v7 }, {v6 , v8 }. Note
that this division is not unique.

Figure 4: ICGS for the diamond example that illustrates the
second condition that causes RC in type-2 RC problems. Actions (without parameters) are labeled along with their corresponding edges. The variables in G are shown as bold-box
nodes and agent VSs are shown as dashed-box nodes.
Next, we prove the result by induction based on the level.
Suppose that the ICGS has k levels and we have the following holds: given any trajectory of states for all variables,
there exists a plan whose execution traces of states include
this trajectory in the correct order.
When the ICGS has k + 1 levels: given any state s for all
variables from level 1 to k + 1, we know from the assumption that the ICGS is traversable that there exists a plan that
can update the variables at the k + 1 level from their current
states to the corresponding states in s. This plan (denoted
by œÄ), meanwhile, requires the freedom to change the states
of variables from level 1 to k. Given the induction assumption, we know that we can update these variables to their
required states in the correct order to satisfy œÄ; furthermore,
these updates (at level k and above) also do not influence
the variables at the k + 1 level (hence do not influence œÄ).
Once the states of the variables at the k + 1 level are updated to match those in s, we can then update variables at
level 1 to k to match their states in s accordingly. Using this
process, we can incrementally build a plan whose execution
traces of states contain any given trajectory of states for all
the variables in the correct order.
Furthermore, the induction holds when there is only one
level given that ICGS is traversable. Hence, the induction
conclusion holds. The main conclusion directly follows.

Towards an Upper Bound for Type-2 RC
In this section, we investigate type-2 RC problem to obtain
upper bounds on the k (Definition 7), based on different relaxations of the two conditions that cause RC in type-2 RC
problems. We first relax the assumption regarding causal
loops (CLs) and show that the relaxation process is associated with how certain CLs can be broken.
We notice that there are two kinds of CLs in ICGS. The
first kind contains agent VSs while the second kind does not.
Although we cannot break CLs for the second kind, it is possible to break CLs for the first kind. The motivation is that
certain edges in these CLs can be removed when there is

Figure 5: Illustration of the process for breaking causal loops
in the diamond example, in which the CLs are broken by
removing the edge marked with a triangle in Figure 4. Two
agent VSs are introduced to replace the original agent VS.
no need to update the associated agent VSs. In our diamond
example, when there are two agents in room1 and room2,
respectively, and they can stay where they are during the execution of the plan, there is no need to W alkT hrough and
hence the associated edges can be removed to break the CLs.
Figure 5 shows this process. Based on this observation, we
introduce the following lemma.
Lemma 2. Given a solvable MAP problem that satisfies NDVC for all agents and for which the ICGS is traversable, if
no CLs contain agent VSs and all the edges going in and out
of agent VSs are directed, the minimum number of agents required is upper bounded by √óv‚ààCR(Œ¶) |D(v)|, when assuming that the agents can choose their initial states, in which
CR(Œ¶) is constructed as follows:
1. add the set of agent VSs that are in the CLs into CR(Œ¶);
2. add in an agent VS into CR(Œ¶) if there exists a directed
edge that goes into it from any variable in CR(Œ¶);
3. iterate 2 until no agent VSs can be added.
Proof. Based on the previous discussions, we can remove
edges that are connected to agent VSs to break loops. For
each variable in CR(Œ¶), denoted by v, we introduce a set of
variables N = {v1 , v2 , ..., v|D(v)| } to replace v. Any edges
connecting to v from other variables are duplicated on all
variables in N , except for the edges that go into v. Each
variable vi ‚àà N has a domain with a single value; this value
for each variable in N is different and chosen from D(v).
Note that these new variables do not affect the traversability
of the ICGS.
From Theorem 1, we know that a virtual agent œÜ+ that
can simultaneously assume all the states that are the different
permutations of states for CR(Œ¶) can achieve the goal. We
can simulate œÜ+ using √óv‚ààCR(Œ¶) |D(v)| agents as follows.
We choose the agent initial states according to the permutations of states for CR(Œ¶), while choosing the same states
for all the other agent VSs according to œÜ+ . Given a plan for
œÜ+ , we start from the first action. Given that all permutations
of states for CR(Œ¶) are assumed by an agent, we can find an
agent, denoted by œÜ, that can execute this action: 1) If this

action updates an agent VS in CR(Œ¶), we do not need to
execute this action based on the following reasoning. Given
that all edges going in and out of agent VSs are directed, we
know that this action does not update Vo . (Otherwise, there
must be an undirected edge connecting a variable in Vo to
this agent VS. Similarly, we also know that this action does
not update more than one agent VS.). As a result, it does not
influence the execution of the next action. 2) If this action
updates an agent VS that is not in CR(Œ¶), we know that this
action cannot have variables in CR(Œ¶) as preconditions or
prevail conditions, since otherwise this agent VS would be
included in CR(Œ¶) given its construction process. Hence,
all the agents can execute the action to update this agent VS,
given that all the agent VSs outside of CR(Œ¶) are always
kept synchronized in the entire process (in order to simulate
œÜ+ ). 3) Otherwise, this action must be updating only Vo and
we can execute the action on œÜ.
Following the above process for all the actions in œÜ+ ‚Äôs
plan to achieve the goal. Hence, the conclusion holds.
Next, we investigate the relaxation of the traversability of
the ICGS.
Lemma 3. Given a solvable MAP problem that satisfies NDVC for all agents, if all the edges going in and out of agent
VSs are directed, the minimum number of agents required is
upper bounded by √óv‚ààV S(Œ¶) |D(v)|, when assuming that the
agents can choose their initial states.
Proof. Given a valid plan œÄM AP for the problem, we can
solve the problem using √óv‚ààV S(Œ¶) |D(v)| agents as follows:
first, we choose the agent initial states according to the permutations of state for V S(Œ¶).
The process is similar to that in Lemma 2. We start from
the first action. Given that all permutations of V S(Œ¶) are assumed by an agent, we can find an agent, denoted by œÜ, that
can execute this action: if this action updates some agent
VSs in V S(Œ¶), we do not need to execute this action; otherwise, the action must be updating only Vo and we can execute the action on œÜ.
Following the above process for all the actions in œÄM AP
to achieve the goal. Hence, the conclusion holds.
Note that the bounds in Lemma 2 and 3 are upper bounds
and the minimum number of agents actually required may
be smaller. Nevertheless, for the simple scenario in our diamond example, the assumptions of both lemmas are satisfied and the bounds returned are 2 for both, which happens
to be exactly the k in Definition 7. In future work, we plan
to investigate other relaxations and establish the tightness of
these bounds.

Conclusion
In this paper, we introduce the notion of required cooperation (RC), which answers two questions: 1) whether more
than one agent is required for a solvable MAP problem, and
2) what is the minimum number of agents required for the
problem. We show that the exact answers to these questions
are difficult to provide. To facilitate our analysis, we first
divide RC problems into two class ‚Äì type-1 RC involves

heterogeneous agents and type-2 RC involves homogeneous
agents. For the first question, we show that most of the problems in the common planning domains belong to type-1 RC;
the set of type-1 RC problems in which RC is only caused
by DVC can be solved with a super agent. For type-2 RC
problems, we show that RC is caused when the state space
is not traversable or when there are causal loops in the causal
graph; we provide upper bounds for the answer of the second question, based on different relaxations of the conditions that cause RC in type-2 RC problems. These relaxations are associated with, for example, how certain causal
loops can be broken in the causal graph.

Acknowledgement
This research is supported in part by the ARO grant
W911NF-13-1-0023, and the ONR grants N00014-13-10176 and N00014-13-1-0519.

References
[Amir and Engelhardt 2003] Amir, E., and Engelhardt, B.
2003. Factored planning. In Proceedings of the 18th International Joint Conferences on Artificial Intelligence, 929‚Äì935.
[Bacchus and Yang 1993] Bacchus, F., and Yang, Q. 1993.
Downward refinement and the efficiency of hierarchical
problem solving. Artificial Intelligence 71:43‚Äì100.
[Backstrom and Nebel 1996] Backstrom, C., and Nebel, B.
1996. Complexity results for sas+ planning. Computational
Intelligence 11:625‚Äì655.
[Brafman and Domshlak 2008] Brafman, R. I., and Domshlak, C. 2008. From One to Many: Planning for Loosely Coupled Multi-Agent Systems. In Proceedings of the 18th International Conference on Automated Planning and Scheduling, 28‚Äì35. AAAI Press.
[Brafman and Domshlak 2013] Brafman, R. I., and Domshlak, C. 2013. On the complexity of planning for agent teams
and its implications for single agent planning. Artificial Intelligence 198(0):52 ‚Äì 71.
[Brafman 2006] Brafman, R. I. 2006. Factored planning:
How, when, and when not. In Proceedings of the 21st National Conference on Artificial Intelligence, 809‚Äì814.
[Bylander 1991] Bylander, T. 1991. Complexity results for
planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence, volume 1, 274‚Äì279.
[Decker and Lesser 1992] Decker, K. S., and Lesser, V. R.
1992. Generalizing the partial global planning algorithm.
International Journal of Cooperative Information Systems
1:319‚Äì346.
[Durfee and Lesser 1991] Durfee, E., and Lesser, V. R. 1991.
Partial global planning: A coordination framework for distributed hypothesis formation. IEEE Transactions on Systems, Man, and Cybernetics 21:1167‚Äì1183.
[Helmert 2006] Helmert, M. 2006. The fast downward planning system. Journal of Artificial Intelligence Research
26:191‚Äì246.
[Jonsson and Rovatsos 2011] Jonsson, A., and Rovatsos, M.
2011. Scaling Up Multiagent Planning: A Best-Response

Approach. In Proceedings of the 21th International Conference on Automated Planning and Scheduling, 114‚Äì121.
AAAI Press.
[Knoblock 1994] Knoblock, C. 1994. Automatically generating abstractions for planning. Artificial Intelligence
68:243‚Äì302.
[Kvarnstrom 2011] Kvarnstrom, J. 2011. Planning for
loosely coupled agents using partial order forward-chaining.
In Proceedings of the 21th International Conference on Automated Planning and Scheduling.
[Nissim and Brafman 2012] Nissim, R., and Brafman, R. I.
2012. Multi-agent a* for parallel and distributed systems.
In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, volume 3, 1265‚Äì
1266.
[Nissim, Brafman, and Domshlak 2010] Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010. A general, fully distributed multi-agent planning algorithm. In Proceedings of
the 11th International Conference on Autonomous Agents
and Multiagent Systems, 1323‚Äì1330.
[Torreno, Onaindia, and Sapena 2012] Torreno, A.; Onaindia, E.; and Sapena, O. 2012. An approach to multi-agent
planning with incomplete information. In European Conference on Artificial Intelligence, volume 242, 762‚Äì767.

J Intell Inf Syst
DOI 10.1007/s10844-015-0366-3

Click efficiency: a unified optimal ranking for online Ads
and documents
Raju Balakrishnan1 ¬∑ Subbarao Kambhampati2

Received: 21 December 2013 / Revised: 23 March 2015 / Accepted: 12 May 2015
¬© Springer Science+Business Media New York 2015

Abstract Ranking of search results and ads has traditionally been studied separately. The
probability ranking principle is commonly used to rank the search results while the ranking
based on expected profits is commonly used for paid placement of ads. These rankings try
to maximize the expected utilities based on the user click models. Recent empirical analysis on search engine logs suggests unified click models for both ranked ads and search
results (documents). These new models consider parameters of (i) probability of the user
abandoning browsing results (ii) perceived relevance of result snippets. However, current
document and ad ranking methods do not consider these parameters. In this paper we propose a generalized ranking function‚Äînamely Click Efficiency (CE)‚Äîfor documents and
ads based on empirically proven user click models. The ranking considers parameters (i)
and (ii) above, optimal and has the same time complexity as sorting. Furthermore, the CE
ranking exploits the commonality of click models, hence is applicable for both documents
and ads. We examine the reduced forms of CE ranking based upon different underlying
assumptions, enumerating a hierarchy of ranking functions. Interestingly, some of the rankings in the hierarchy are currently used ad and document ranking functions; while others
suggest new rankings. Thus, this hierarchy illustrates the relationships between different
rankings, and clarifies the underlying assumptions. While optimality of ranking is sufficient for document ranking, applying CE ranking to ad auctions requires an appropriate
pricing mechanism. We incorporate a second price based mechanism with the proposed
ranking. Our analysis proves several desirable properties including revenue dominance over

 Raju Balakrishnan

raju.balakrishnan@gmail.com
Subbarao Kambhampati
rao@asu.edu
1

Groupon., Park Blvd, Palo Alto, CA 94306, USA

2

Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

J Intell Inf Syst

Vickrey Clarke Groves (VCG) for the same bid vector and existence of a Nash equilibrium
in pure strategies. The equilibrium is socially optimal, and revenue equivalent to the truthful
VCG equilibrium. As a result of its generality, the auction mechanism and the equilibrium
reduces to the current mechanisms including Generalized Second Price Auction (GSP) and
corresponding equilibria. Furthermore, we relax the independence assumption in CE ranking and analyze the diversity ranking problem. We show that optimal diversity ranking is
NP-Hard in general, and a constant time approximation algorithm is not likely. Finally our
simulations to quantify the amount of increase in different utility functions conform to the
results, and suggest potentially significant increase in utilities.
Keywords Ad ranking ¬∑ Document ranking ¬∑ Diversity ¬∑ Auctions ¬∑ Click models

1 Introduction
Search engines rank results to maximize the relevance of the top documents. On the other
hand, targeted ads are ranked primarily to maximize the profit from clicks. In general,
users browse through ranked lists of search results or ads from top to bottom, either clicking or skipping the results, or abandoning browsing the list altogether due to impatience
or satiation. The goal of the ranking is to maximize the expected relevances (or profits) of
clicked results based on the click model of the users. The sort by relevance ranking suggested by Probability Ranking Principle (PRP) has been commonly used for search results
for decades (Robertson 1977; Gordon and Lenk 1991). In contrast, sorting by the expected
profits calculated as the product of bid amount and Click Through Rate (CTR) is popular
for ranking ads (Richardson et al. 2007).
Recent click models suggests that the user click behaviors for both search results and targeted ads is the same (Guo et al. 2009; Zhu et al. 2010). Considering this commonality, the
only difference between the two ranking problems is the utility of entities ranked: for documents utility is the relevance and for the ads it is the cost-per-click (CPC). This suggests the
possibility of a unified ranking function for search results and ads. The current segregation
of document and ad ranking as separate areas does not consider this commonality. A unified approach often helps to widen the scope of the related research to these two areas, and
enables applications of existing ranking function in one area on isomorphic problems in the
other area as we will show below.
In addition to the unified approach, the recent click models consider the following
parameters:
1.

2.

Browsing Abandonment: The user may abandon browsing ranked list at any point.
The likelihood of abandonment may depend on the entities the user has already
seen (Zhu et al. 2010).
Perceived Relevance: Perceived relevance is the user‚Äôs relevance assessment viewing
only the search snippet or ad impression. The decision to click or not depends on the
perceived relevance, not on the actual relevance of the results (Yue et al. 2010; Clarke
et al. 2007).

Though these parameters are part of the click models (Guo et al. 2009; Zhu et al. 2010) how
to exploit these parameters to improve ranking is currently unknown. The current document
ranking is based on the simplifying assumption that the perceived relevance is the same as
the actual relevance of the document, and ignores browsing abandonment. The ad placement
partially considers perceived relevance, but ignores abandonment probabilities.

J Intell Inf Syst

In this paper, we propose a unified optimal ranking function‚Äînamely Click Efficiency
(CE)‚Äîbased on a generalized click model of the user. CE is defined as the ratio of the
standalone utility generated by an entity to the sum of the abandonment probability and the
click probability of that entity, where the abandonment probability is the probability for the
user to leave browsing the list after seeing the entity. We show that sorting entities in the
descending order of CE guarantees optimum ranking utility. We do not make assumptions
on the utilities of the entities, which may be assessed relevance for documents or cost per
click (CPC) charged based on the auction for ads. On plugging in the appropriate utilities‚Äî
relevance for documents and CPC for the ads‚Äîthe ranking specializes to document and ad
ranking.
As a consequence of the generality, the proposed ranking will reduce to specific ranking
problems on assumptions about user behavior. We enumerate a hierarchy of ranking functions corresponding to different assumptions on the click model. Most interestingly, some of
these special cases correspond to the currently used document and ad ranking functions‚Äî
including PRP and sort by expected profit described above. Further, some of the reduced
ranking functions suggest new rankings for special cases of the click model‚Äîlike a click
model in which the user never abandons the search, or the perceived relevance is approximated as the actual relevance. This hierarchy elucidates interconnection between different
ranking functions and the assumptions behind the rankings. We believe that this will help in
choosing the appropriate ranking function for a particular user click behavior.
Ranking in ad placement used in conjunction with a pricing strategy to form the complete
auction mechanism. Hence to apply the CE ranking on ad placement, a pricing mechanism has to be associated. We incorporate a second-price based pricing mechanism with
the proposed ranking. Our analysis establishes many interesting properties of the proposed
mechanism. Particularly, we state and prove the existence of a Nash Equilibrium in pure
strategies. At this equilibrium, the profits of the search engine and the total revenue of the
advertisers is simultaneously optimized. Like ranking, the proposed auction this is a generalized mechanism, and reduces to the existing GSP and Overture mechanisms under the
same assumptions as that of the ranking. Further, the stated Nash Equilibrium is a general
case of the equilibriums of these existing mechanisms. Comparing the mechanism properties with that of VCG (Vickrey 1961; Clarke 1971; Groves 1973), we show that for the same
bid vector, search engine revenue for the CE mechanism is greater or equal to that of VCG.
Furthermore, the revenue for the proposed equilibrium is equal to the revenue of the truthful
dominant strategy equilibrium of VCG.
Our analysis so far has been based on the assumption of parameter independence between
the ranked entities. We relax this assumption and analyze the implications based on a specific well known problem‚Äîdiversity ranking (Carterette 2010; Agrawal et al. 2009; Rafiei
et al. 2010). Diversity ranking tries to maximize the collective utility of top-k ranked entities. For a ranked list, an entity will reduce residual utility of a similar entity in the list blow
it. Though optimizing all the current ranking functions incorporating diversity is known to
be NP-Hard (Carterette 2010), an understanding of why this is an inherently hard problem
is lacking. We show that optimizing set utilities is NP-Hard even for the basic form of diversity ranking. Furthermore we extend our proof showing that a constant ratio approximation
algorithm is unlikely. As a benefit of the generality of ranking, these results are applicable
both for ads and documents.
Although we prove the optimality of the proposed ranking, the amount by which the
profit may improve is not clear. Considering the very restricted access to online experiments on ads, we performed simulations to this end. We compare the profit improvement
by the CE and reduced forms to existing rankings. These experiments suggest potentially

J Intell Inf Syst

significant increase in profits. We believe that these experiments will motivate further online
evaluations.
In summary, the contributions of the unified ranking, including both ad and document
domains are:
1.
2.
3.
4.
5.

Unified optimal ranking.
Optimal ranking considering abandonment probabilities for documents and ads.
Optimal Ranking considering perceived relevance of documents and ads.
A unified hierarchy of ranking functions and enumerating optimal rankings for different
click models.
Analysis of general diversity ranking problem and hardness proofs.

Our contributions to ad placement are:
1.
2.
3.

Design and analysis of a generalized ad auction mechanism incorporating pricing with
CE ranking.
Proof of the existence of a socially optimal Nash Equilibrium with optimal advertisers
revenue as well as optimal search engine profit.
Proof of search engine revenue dominance over VCG for equivalent bid vectors, and
equilibrium revenue equivalence to the truthful VCG equilibrium.

1.1 Background
In search and search advertising, both search results and ads are ranked to maximize utility.
At a high level, search results are ranked to maximize the information content (or relevance)
of the top documents to the users; whereas ads are ranked to maximize both the relevance
as well as the profit to the search engines. Users generally browse through ranked search
results starting from the top, either clicking or skipping the results. This browsing pattern
of users is called the click model. Search and ad rankings try to maximize the utility to the
users based on a click model.
In addition to the standalone relevance of the results, another important aspect of ranking
is the diversity of the results. Although information contained in a document may be highly
relevant, if the information is similar to that in the documents above in the ranking, the document will be of little utility. To account for this factor, the mutual influence of documents
or ads ranked needs to be considered to maximize total utility by a set of documents rather
than individual documents. To account for this factor, diversity-sensitive ranking maximizes
residual relevance of ads or documents in the context of other items in the ranked list.
In search ad ranking (paid placements), ads are selected based on the user query. Generally, the click model for ads is similar to that of the search results. In the most common
pay-per-click ad campaigns, advertisers pay a certain amount to the search engines whenever a user clicks on their ads. This amount is determined by a pricing mechanism. The
advertisers place a bid on the queries. The ads are ranked based on the bid amounts and
relevance of the ad to the query. For example, in commonly used Generalized Second
Price (GSP) auction Edelman et al. (2007) ads are ranked by the product of their click
rates (ratio of the number of clicks to impressions) and bid amounts. The amount the
advertisers pay to the search engine need not be equal to the bid amount, but rather determined by the pricing mechanism. For example, in GSP auction, this amount is determined
based the the bid amount and the click rates of the given ad and the ad placed below the
given ad. Thus ranking and pricing together determines the auction mechanism of the ad
placement.

J Intell Inf Syst

The rest of this paper is organized as the follows. The next section reviews related work.
Section 3 explains the click model used for our analysis. Subsequently we introduce our
optimal ranking function, and discuss the intuitions and implications. In Section 5 reductions of our ranking function to several document and ad ranking functions under limiting
assumptions are enumerated. Furthermore we discuss several useful special cases of our
ranking and assumptions under which they are optimal. In Section 6, we incorporate a pricing strategy to design a complete auction mechanism for ads. Several useful properties are
established, including the existence of a Nash equilibrium and revenue dominance over
VCG. Section 7 explores the ranking considering mutual influences and proves our hardness results. We present the experiments and results in Section 8. Finally we discuss our
conclusions and discuss potential future research directions.

2 Related work
The impact of click models on ranking has been analyzed in ad-placement. In our previous
paper Balakrishnan and Kambhampati (2008) we proposed an optimal ad ranking considering mutual influences. The ranking uses the same user model, but the paper considers only
ad ranking, and does not include generalizations and auctions. Later Aggarwal et al. (2008)
as well as Kempe and Mahdian (2008) analyzed placement of ads using a similar Markovian click model. The click model used is less detailed than our model since abandonment
is not modeled separately from click probability. These two papers optimize the sum of the
revenues of the advertisers. We optimize search engine profits in this paper. Nevertheless,
the ranking formulation has common components with these two papers, as workshop version of this paper Balakrishnan and Kambhampati (2008) as these three papers formulated
ranking based on the similar browsing models independently at almost the same time frame.
But, unlike this paper, any of the other two papers do not have a pricing, auctions, or a
generalized taxonomy.
Edelman et al. (2007) analyze a version of GSP auction in their classic paper. They
assume that the click probability at a position is a constant. We relax this assumption, and
account for the influence of ads above on the click probabilities at a position. This difference
gives rise to additional complexities and interesting differences in our mechanism. We show
that GSP proposed by Edelman et al. is a special case of our proposed mechanism.
Giotis and Karlin (2008) extend Markovian model ranking by applying GSP pricing and
analyzing the equilibrium. The GSP pricing and ranking lacks the optimality and generality
properties we prove in this paper. Deng and Yu (2009) extend Markovian models by suggesting a ranking and pricing schema for the search engines and prove the existence of a
Nash Equilibrium. The ranking is a simpler bid based ranking (not based on CPC as in our
case); and mechanism as well as equilibrium do not show optimality properties. Our paper
is different from both the above works by using a more detailed model, by having optimality properties, detailed comparisons with other baseline mechanisms, and in the ability to
generalize to a family of rankings.
Kuminov and Tennenholtz (2009) proposed a Pay Per Action (PPA) model similar to
the click models and compared the equilibrium of GSP mechanism on the model with the
VCG. Ad auctions considering influence of other ads on conversion rates are analyzed by
Ghosh and Sayedi (2010). Both these papers address different problems than considered in
this paper.
Our proposed model is a general case of the positional auctions model by Varian (2007).
Positional auctions assume static click probabilities for each position independent of other

J Intell Inf Syst

ads. We assume more realistic dynamic click probabilities depending on the ads above.
Since we consider these externalities, our model, auction, and analysis are more complex.
(e.g. monotonically increasing values and prices with positions).
The existing document ranking based on PRP (Robertson 1977) claims that a retrieval
order sorted on relevance leads to the largest number of relevant documents in a result set
than any other policy. Gordon and Lenk (1991, 1992) identified the required assumptions for
the optimality of the ranking according to PRP. Our discussion on PRP may be considered
as an independent formulation of assumptions under which PRP is optimal for web ranking.
There are number of user behavior studies in click models validating our assumed user
model and ranking function. There are a number of position based and cascade models
studied (Dupret and Piwowarski 2008; Craswell et al. 2008; Guo et al. 2009; Chapelle and
Zhang 2009; Zhu et al. 2010; Xu et al. 2010; Hu et al. 2011). In particular, General Click
Model (GCM) by Zhu et al. (2010) is interesting, since many other click models are special
cases of GCM. Zhu et al. (2010) list assumptions under which the GCM would reduce to
other click models. We will discuss the relations of our model to GCM below. Optimizing
utilities of two dimensional placement of search results has been studied by Chierichetti
et al. (2011). Many of the recent click models are more general than the click model used
in our paper, but please note that the contribution of our paper is not the click model, but a
unified optimal ranking and auction mechanism based on the click model.
Along with the current click models, there has been research on evaluating perceived
relevance of the search snippets (Yue et al. 2010) and ad impressions (Clarke et al. 2007).
Research in this direction neatly complements our new ranking function by estimating the
parameters required. Chapelle and Zhang (2009) demonstrated that separately modeling perceived and actual relevances improves relevance assessment of documents using click logs.
Diversity ranking has received considerable attention recently (Agrawal et al. 2009;
Rafiei et al. 2010). The objective functions used to measure diversity by prior works are
known to be NP-Hard (Carterette 2010). We provide a stronger proof showing that even
the basic diversity ranking problem is NP-Hard irrespective of any specific objective function, and further show that a constant ratio approximation is unlikely. To the best of our
knowledge, this paper is the first unified optimal ranking and auction mechanism based on
a generalized click model.

3 Click model
As we mentioned above, we approach the ranking as an optimization based on the user‚Äôs
click model on the ads. The expected utilities are maximized based on the click model.
For the optimization, we assume a basic user click model in which the web user browses
the entity list in the ranked order, as shown in Fig. 1. The symbols used in this paper are
explained in Table 1. At every result entity, the user may:
1.

2.

3.

Click the result with perceived relevance C(e). We define the perceived relevance as the
probability of clicking the entity ei having seen ei i.e. C(ei ) = P (click(ei )|view(ei )).
Note that the Click Through Rate (CTR) defined in ad placement is the same as the
perceived relevance defined here (Richardson et al. 2007).
Abandon browsing the result list with abandonment probability Œ≥ (ei ). Œ≥ (ei ) is defined
as the probability of abandoning the search at ei having seen ei . i.e. Œ≥ (ei ) =
P (abandonment (ei )|view(ei )).
Go to the next entity with probability [1 ‚àí (C(ei ) + Œ≥ (ei ))]

J Intell Inf Syst
Table 1 Definition of the symbols
e

A ranked entity.

C(e)

Perceived relevance.

Œ≥ (e)

Abandonment probability.

U (e)

Utility.

Pc (e)

The click probability of the entity at position i in the ranking.

d

A ranked document.

R(d)

Relevance of the document.

a

A ranked ad.

SE

An abbreviation indicating Search Engine.

$(a)

Cost-Per-Click (CPC) of the ad.

v(a)

Private value of the ad for the advertiser.

b(a)

Bid for the ad.

w(a)

Ratio of the click probability to the sum of abandonment and click probability.

Œº(a)

Sum of abandonment and click probability (i.e. C(a) + Œ≥ (a)).

CE(a)

Proposed Click-Efficiency ranking score of the ad.

pi

Payment by the advertiser (CPC) to the search in a given mechanism.

Ur (e)

Residual utility in the context of other entities in the ranked list.

Œ±

Simulation constant to balance between the click and the abandonment probabilities.

The click model can be schematically represented as the flow graph shown in Fig. 1.
Labels on the edges refer to the probability of the user traversing them. Each vertex in the
figure corresponds to a view epoch (see below), and the flow balance holds at each vertex.
Starting from the top entity, the probability of the user clicking the first ad is C(e1 ) and
probability of him abandoning browsing is Œ≥ (e1 ). The user goes beyond the first entity with
probability 1 ‚àí (C(e1 ) + Œ≥ (e1 )) and so on for the subsequent results.
In this model, we assume that the parameters‚ÄîC(ei ), Œ≥ (ei ) and U (ei )‚Äîare functions
of the entity at the current position i.e. these parameters are independent of other entities
the user has already seen. We recognize that this assumption is not fully accurate, since
the user‚Äôs decision to click the current item or to leave the search may depend not just
on the current item but rather on all the entities he has seen before in the list. We stick
to the assumption for the optimal ranking analysis below, since considering mutual influence of ads may lead to combinatorial optimization problems with intractable solutions. We

Fig. 1 Flow graph for an user browsing the first two entities. The labels are the view probabilities and ei
denotes the entity at the i th position

J Intell Inf Syst

will show that even the simplest dependence between the parameters will indeed lead to
intractable optimal ranking in Section 7.
Although the proposed model is intuitive enough, we would like to mention that our
model is also confirmed by the recent empirical click models. For example, the General
Click Model (GCM) by Zhu et al. (2010) is based on the same basic user behavior. The GCM
is empirically validated for both search results and ads (Zhu et al. 2010). Furthermore, other
click models are shown to be special cases of GCM. Please refer to Zhu et al. (2010) for
a detailed discussion. These previous works avoid the need for separate model validation,
as well as confirm the feasibility of the parameter estimation. Further, Yilmaz et al. (2010)
proposes an expected browsing utility metric based on a similar user model.

4 Optimal ranking
Based on the click model, we formally define the ranking problem and derive optimal
ranking in this section. The problem may be stated as,
Choose the optimal ranking Eopt = e1 , e2 , .., eN  of N entities to maximize the
expected utility
N

E(U ) =
U (ei )Pc (ei )
(1)
i=1

where N is the total number of entities to be ranked.
The utility function U (ei ) denotes the stand-alone utility of the entity ei to the search
engine (or one who performs the ranking). This may vary depending on the specific ranking problem. For example, for ranking search results, the utility will be the relevance of
document ei ; whereas for ranking ads to maximize the revenue of the search engine, the
U (ei ) will be pay-per-click of ad ei . We define the specific utility function for entities as
we discuss the specific ranking problems below.
For the browsing model in Fig. 1, the click probability for the entity at the i th position is,
Pc (ei ) = C(ei )

i‚àí1




1 ‚àí C(ej ) + Œ≥ (ej )

(2)

j =1

Substituting click probability Pc from (2) in (1) we get,
E(U ) =

N

i=1

U (ei )C(ei )

i‚àí1



1 ‚àí (C(ej ) + Œ≥ (ej ))



(3)

j =1

The optimal ranking maximizing this expected utility can be shown to be a sorting
problem with a simple ranking function:
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
U (ei )C(ei )
(4)
CE(ei ) =
C(ei ) + Œ≥ (ei )
Proof Sketch The proof shows that any inversion in this order will reduce the expected
profit. CE function is deduced from expected profits of two placements‚Äîthe CE ranked

J Intell Inf Syst

placement and placement in which the order of two adjacent ads are inverted. We show
that the expected profit from the inverted placement can be no greater than the CE ranked
placement. Please refer to Appendix A-1 for the complete proof.
As mentioned in the introduction, the ranking function CE is the utility generated per
unit view probability consumed by the entity. With respect to browsing model in Fig. 1, the
top entities in the ranked list have greater view probabilities, and placing ads with greater
utility per consumed view probability at higher positions intuitively increases total utility.
The proof of Theorem 1 assumes that the user clicks only one entity in the list. Since this
may not always be true, we extend the optimality to multiple clicks in Theorem 2.
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Sketch We proved that ordering according to CE provides maximum expected utility
for single click above. Multiple clicks are the same as the user restarting her browsing from
the entity immediately below the last clicked entity. A simple induction on number of clicks
based on this idea, using a single click as base case is sufficient to prove that the proposed
placement provides maximum expected utility for multiple clicks. See Appendix A-2 for
the complete proof.
Note that the ordering above does not maximize the utility for selecting a subset of items.
The seemingly intuitive method of ranking the set of items by CE and selecting top-k may
not be optimal (Aggarwal et al. 2008). For optimal selection, the proposed ranking can be
extended by a dynamic programming based selection (Aggarwal et al. 2008). In this paper,
we discuss only the ranking problem.

5 Ranking taxonomy
The click model in Fig. 1 is common to many types of rankings including document searches
and search ads. The only difference between these rankings sharing a common click model
is the utility to be maximized. Consequently, the CE ranking can be made applicable to
different ranking problems by plugging in different utilities. For example, if we plug in relevance as utility (U (e) in (4)), the ranking function is applicable for the documents, whereas
if we plug in cost per click of ads, the ranking function is applicable to ads. Furthermore,
we may assume specific constraints on one or more of the three parameters of CE ranking (e.g. ‚àÄi Œ≥ (ei ) = 0). Through these assumptions, CE ranking will suggest a number of
reduced ranking functions with specific applications. These substitutions and reductions can
be enumerated as a taxonomy of ranking functions.
We show the taxonomy in Fig. 2. The three top branches of the taxonomy (U (e) = R(d),
U (e) = $(a), and U (e) = v(a) branches) are for document ranking, ad ranking maximizing
search engine profit, and ad ranking maximizing advertisers revenue respectively. These
branches correspond to the substitution of utilities by document relevance, CPC, and private
value of the advertisers. The sub-trees below these branches are the further reduced cases of
these three main categories. The solid lines in Fig. 2 denote already known functions, while
the dotted lines are the new ranking functions suggested by CE ranking. Sections 5.1, 5.2,
and 5.3 below discuss the further reductions of document ranking, search engine optimal
ad ranking, and social optimal ad ranking respectively.

J Intell Inf Syst

Fig. 2 Taxonomy reduced CE ranking functions. The assumptions and corresponding reduced ranking functions are illustrated. The dotted lines denote predicted ranking functions incorporating new click model
parameters

5.1 Optimal document ranking
For document ranking the utility of ranking is the probability of relevance of the document.
Hence by substituting the document relevance‚Äîdenoted by R(d)‚Äîin (4) we get
CE(d) =

C(d)R(d)
C(d) + Œ≥ (d)

(5)

This function suggests the general optimal relevance ranking for the documents. We discuss
some intuitively valid assumptions on user model for the document ranking and the corresponding ranking functions below. The three assumptions discussed below correspond to
the three branches under Optimal Document Ranking subtree in Fig. 2.

Sort by relevance (PRP) We elucidate two sets of assumptions under which the CE(d)
in (5) will reduce to PRP.
First assume that the user has infinite patience, and never abandons results (i.e. Œ≥ (d) ‚âà
0). Substituting this assumption in (5),
R(d)C(d)
= R(d)
(6)
C(d)
which is exactly the ranking suggested by PRP.
In other words, the PRP is still optimal for scenarios in which the user has infinite
patience and never abandons checking the results (i.e. the user leaves browsing the results
only by clicking a result).
The second set of slightly weaker assumptions under which the CE(d) will reduce to
PRP is
CE(d) ‚âà

1.

C(d) ‚âà R(d).

J Intell Inf Syst

2.

Abandonment probability Œ≥ (d) is negatively proportional to the document relevance
i.e. Œ≥ (d) ‚âà k ‚àí R(d), where k is a constant between one and zero. This assumption
corresponds to the intuition that the higher the perceived relevance of the current result,
the less likely is the user abandoning the search.

Now CE(d) reduces to,
R(d)2
(7)
k
Since this function is strictly increasing with zero and positive values of R(d), ordering just
by R(d) results in the same ranking as suggested by the function. This implies that PRP is
optimal under these assumptions also.
It may be noted that abandonment probability decreasing with perceived relevance is a
more intuitively valid assumption than the infinite patience assumption above.
CE(d) ‚âà

Ranking considering perceived relevance Recent click log studies effectively assess
perceived relevance of document search snippets (Yue et al. 2010; Clarke et al. 2007). But,
how to use the perceived relevance for improved document ranking is still an open question.
The proposed perceived relevance ranking addresses this question.
If we assume that Œ≥ (d) ‚âà 0 in (5), the optimal perceived relevance ranking is the same
as that suggested by PRP as we have seen in (6).
On the other hand, if we assume that the abandonment probability is negatively proportional to the perceived relevance (Œ≥ (d) = k ‚àí C(d)) as above, the optimal ranking
considering perceived relevance is
C(d)R(d)
‚àù C(d)R(d)
(8)
k
i.e. sorting in the order of the product of document relevance and perceived relevance is
optimal under these assumptions. The assumption of abandonment probabilities being negatively proportional to relevance is more realistic than the infinite patience assumption as
we discussed above. This discussion shows that by estimating the nature of abandonment
probability, one would be able to decide on the optimal perceived relevance ranking.
CE(d) ‚âà

Ranking considering abandonment We now examine the ranking considering abandonment probability Œ≥ (d), with the assumption that the perceived relevance is approximately
equal to the actual relevance. In this case CE(d) becomes,
CE(d) ‚âà

R(d)2
R(d) + Œ≥ (d)

(9)

Clearly this is not a strictly increasing function with R(d). Hence the ranking considering
abandonment is different from PRP ranking, even if we assume that the perceived relevance
is equal to the actual relevance. assumption that ‚àÄd Œ≥ (d) = 0, the abandonment ranking
becomes the same as PRP.

5.2 Optimal Ad ranking for search engines
For the paid placement of ads, the utilities of ads to the search engine are Cost-Per-Click
(CPC) of the ads. Hence, by substituting the CPC of the ad‚Äîdenoted by $(a)‚Äî in (4) we
get
C(a)$(a)
(10)
CE(a) =
C(a) + Œ≥ (a)

J Intell Inf Syst

Thus this function suggests the general optimal ranking for the ads. Please recall
that the perceived relevance C(a) is the same as the CTR used for ad placement
(Richardson et al. 2007).
In the following subsections we demonstrate how the general ranking presented reduces
to the currently used ad placement strategies under various assumptions. We will show that
they all correspond to specific assumptions about the abandonment probability Œ≥ (a). These
two functions below corresponds to the two branches under the SE (Search Engine) Optimal
Ad Placement subtree in Fig. 2.

Ranking by bid amount The sort by bid amount ranking was used by Overture Services
(and was later used by Yahoo! for a while after acquisition of Overture). Assuming that the
user never abandons browsing (i.e. ‚àÄa Œ≥ (a) = 0), then (10) reduces to
CE(a) = $(a)

(11)

This means that the ads are ranked purely in terms of their payment. In fact overture ranking
is by bid amount, which is different from payment in a second price auction. But both will
result in the same ranking as higher bids implies higher payments also.
When Œ≥ (a) = 0, we essentially have a user with infinite patience who will keep browsing
downwards until he finds a relevant ad. Hence ranking by bid amount maximizes profit.
More generally, for small abandonment probabilities, ranking by bid amount is near optimal.
Note that this ranking is isomorphic to PRP ranking discussed above for document ranking,
since both ranks are based only on utilities.

Ranking by expected profit Google and Microsoft supposedly place the ads in the order
of expected profit based on product of CTR (C(a) in CE) and bid amount ($(a)) (Richardson et al. 2006). The mechanism is called Generalized Second Price (GSP) auction, and
the most popular one as well. If we approximate abandonment probability as negatively
proportional to the CTR of the ad (i.e. ‚àÄa Œ≥ (a) = k ‚àí C(a)) , the (10) reduces to,
$(a)R(a)
‚àù $(a)R(a)
(12)
k
This shows that ranking ads by their standalone expected profit is near optimal as long as
the abandonment probability is negatively proportional to the relevance. To be accurate,
the Google mechanism‚ÄîGSP‚Äîuses the bid amount of the advertisers (instead of CPC in
(12)) for ranking. Although CPC and bids are different for GSP, we will show that both will
result in the same ranking in Section 6. Note that this ranking is isomorphic to the perceived
relevance ranking of documents discussed above.
CE(a) ‚âà

5.3 Social optimal Ad ranking
An important property of any auction mechanism is social utility, i.e. total utilities of all
the players. In our case this is equal to the sum of the utilities of all the advertisers and
the search engine. To analyze advertiser‚Äôs profit, a private value model is commonly used.
Each advertiser has a private value for the click, which is equal to the expected benefit
(direct and indirect revenue) from the click. Advertisers pay a fraction of this benefit to the
search engine as CPC. The utility for the advertisers is the difference between the private
value and payment to the search engine. The utility for the search engine is the payment
from the advertisers. Hence the social utility is equal to the sum of private values of all the
clicks for the advertisers (which is the sum of utilities of the search engine and advertisers).

J Intell Inf Syst

Consequently, to prove the social optimality all we need to prove is that the total private
values of clicks for the advertisers is optimal.
The social-optimal branch in Fig. 2 corresponds to the ranking to maximize total revenue.
Private value of advertisers ai is denoted as‚Äîv(ai ). By substituting the utility by private
values in (4) we get,
CE(d) =

C(a)v(a)
C(a) + Œ≥ (a)

(13)

If the ads are ranked in this order, the ranking will guarantee maximum revenue. Note that
the optimal revenue does not imply optimal net profits for the advertisers, since part of this
revenue is paid to the search engine as CPC. But optimal revenue implies a maximum total
profit (utility)‚Äîsum of profits of search engine and advertisers.
In Figure 2 the two left branches of the Social Optimal subtree (labeled Œ≥ (a) = 0 and
Œ≥ (a) = k ‚àí C(a)) correspond respectively to the assumption of no abandonment, and
abandonment probabilities being negatively proportional to the click probability. These two
cases are isomorphic to the Overture and Google ranking discussed in Section 5.2 above.
The social optimal ranking is not directly implementable as search engines do not know
the private value of the advertisers. But this ranking is useful in analysis of auctions mechanisms. Furthermore, the search engine may try to effectuate this order through auction
mechanism equilibriums as we demonstrate in Section 6.

6 Applying CE ranking for Ad placement
We have shown that CE ranking maximizes the profits for search engines for given CPCs.
The CPCs are determined by the pricing mechanism used by the search engine. Hence
the overall profit of ranking can be analyzed only in association with a pricing mechanism. The existing ad pricing mechanisms like GSP do not preserve any of their appealing
properties for CE ranking as they do not consider the additional parameter abandonment
probability. For example, the GSP pricing Edelman et al. (2007) is no longer the minimum
amount need to be paid by the advertiser to maintain his position in the CE ranking. To
this end, we design a full auction mechanism by proposing a new second price based pricing to be used with the CE ranking. Subsequently, we analyze the properties of the auction
mechanism.
Let us start by describing the dynamics of ad auctions briefly, the search engine decides
the ranking and pricing (CPC) of the ads based on the bid amounts of the advertisers. Generally the pricing is not equal to the bid amount of advertisers, but derived based on the
bids (Easley and Kleinberg 2010; Edelman et al. 2007; Aggarwal et al. 2006). In response to
these ranking and pricing strategies, the advertisers (more commonly, the software agents of
the advertisers) may change their bids to maximize their profits. They may change bids hundreds of times a day. Eventually, the bids may stabilize at a fixed point where no advertiser
can increase his profit by unilaterally changing his bid, depending on the initial bids and
behavior of the advertisers. This set of bids corresponds to a Nash Equilibrium of the auction
mechanism. Hence the expected profits of a search engine will be the profits corresponding
to the Nash Equilibrium, if the auction attains a Nash Equilibrium.
The next section discusses properties of any mechanism based on the user model‚Äî
independent of the ranking and pricing strategies. In Section 6.2, we introduce a pricing
mechanism and analyze the properties including the equilibrium.

J Intell Inf Syst

6.1 User model based properties
We discuss general properties of all auction mechanisms using the browsing model (Fig. 1).
These properties are implications of the user behavior and applicable to any pricing and
ranking.
Lemma 1 (Individual Rationality) In any equilibrium the payment by the advertisers is less
than or equal to their private values.1
If this is not true, this advertiser may opt out from the auction by bidding zero and
increase the profit, violating the assumption of equilibrium.
Lemma 2 (Pricing Monotonicity) In any equilibrium, the price paid by an advertiser
increases monotonically as he moves up in the ranking unilaterally.
From the browsing model, click probability of the advertisers is non-decreasing as he
moves up in the position. Unless the price increases monotonically, the advertiser may
increase his profit by moving up, thereby violating assumption of an equilibrium.
Lemma 3 (Revenue Maximum) The sum of the payoffs of the advertisers and the search
engine is less than or equal to
E(V ) =

N


v(ai )C(ai )

1 ‚àí (C(aj ) + Œ≥ (aj ))



(14)

j =1

i=1

when the advertisers are ordered by

i‚àí1



C(a)v(a)
C(a)+Œ≥ (a) .

Note that this quantity is the maximal advertiser revenue corresponding to the social
optimal placement in (13), and is a direct consequence. The advertiser pay a fraction of his
revenue to the search engine. Payoff for the advertisers is the difference between the total
revenue and the payment to the search engine. The total payoff of the search engine is the
sum of these payments by all the advertisers. Since the suggested order above in Lemma 3
maximizes total revenue of the advertisers, the sum of the payoffs for the search engine and
the advertisers will not exceed this value.
A corollary of the social optimality combined with the individual rationality result
expressed in Lemma 1 is that,
Lemma 4 (Profit Maximum) The quantity E(V ) in Lemma 3 is an upper bound for the
search engine profit in any equilibrium.

6.2 Pricing and equilibrium
An interesting property of the proposed mechanism is the existence of an equilibrium in
which the search engine optimal ranking coincides with the social optimality. As we proved
above, CE ranking is search engine optimal as it maximizes the revenue for the given CPCs.
On the other hand, social optimal ordering maximizes the total profits for all the players

1 This

property is called individual rationality

J Intell Inf Syst

(search engine and advertisers) for given CPCs. Social optimality is desirable for search
engines, as the increased profits will improve the advertiser‚Äôs preference of one search
engine over others. Since search engines do not know the private value of the advertisers, social optimal ranking is not directly achievable (note that the search engines do the
ranking). A possibility is to design a mechanism having an equilibrium coinciding with the
social optimality, as we propose below. This may cause the bid vector to stabilize in a social
optima.
For defining the pricing strategy for the auction mechanism, we define the pricing order
as the decreasing order of w(a)b(a), where b(a) is the bid value and w(a) is,
w(a) =

C(a)
C(a) + Œ≥ (a)

(15)

In this pricing order, we denote the i th advertiser‚Äôs w(ai ) as wi , C(ai ) as ci , b(ai ) as bi , and
the abandonment probability Œ≥ (ai ) as Œ≥i for convenience. Let Œºi = ci + Œ≥i . For each click,
advertiser ai is charged price pi (CPC) equal to the minimum bid required to maintain its
position in the pricing order,
pi =

wi+1 bi+1
bi+1 ci+1 Œºi
=
wi
Œºi+1 ci

Substituting pi in (10) for the ranking order, CE of the i th advertiser is,
pi ci
CEi =
Œºi

(16)

(17)

This proposed mechanism preserves the pricing order in the ranking as well, i.e.
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj

(18)

The proof for theorem 3 is given in Appendix A-3. This order preservation property
implies that the final ranking is the same as that based on bid amounts. In other words, ads
can be ranked based on the bid amounts instead of CPCs. After the ranking, the CPCs can
be decided based on this ranking order. A corollary of this order preservation is that the
CPC is equal to the minimum amount the advertisers have to pay to maintain their position
in the ranking order.
Furthermore we show below that any advertiser‚Äôs CPC is less than or equal to his bid.
Lemma 5 (Individual Rationality) The payment pi of any advertiser is less or equal to his
bid amount.
Proof
pi =

bi+1 ci+1 Œºi
bi+1 ci+1 Œºi
CEi+1
=
bi =
bi ‚â§ bi (since CEi ‚â• CEi+1 )
Œºi+1 ci
Œºi+1 ci bi
CEi

This means advertisers will never have to pay more than their bid, similar to GSP. This
property makes it easy for the advertiser to decide his bid, as he may bid up to his click
valuation. He will never have to pay more than his revenue, irrespective of bids of other
advertisers.

J Intell Inf Syst

Interestingly, this mechanism is a general case of existing mechanisms, similar to CE
ranking above. The mechanism reduces to GSP (Google mechanism) and Overture mechanisms on the same assumptions on which CE ranking reduces to respective rankings
(described in Section 5.2).
Lemma 6 The mechanism reduces to Overture ranking with a second price auction on the
assumption ‚àÄi Œ≥i = 0
Proof This assumption implies
wi = 1
‚áí pi = bi+1 (second price auction)
‚áí CEi = bi+1 ‚â° bi (i.e. ranking by bi+1 is equivalent to ranking by bi )

Lemma 7 The mechanism reduces to GSP on the assumption ‚àÄi Œ≥i = k ‚àí ci
Proof This assumption implies
wi = c i
bi+1 ci+1
(i.e. ranking reduces to GSP ranking)
ci
bi c i
bi+1 ci+1
‚â°
(by Theorem 3)
‚áí CEi =
k
k
‚àù bi ci
‚áí pi =

This lemma in conjunction with Theorem 3 implies that GSP ranking by ci bi (i.e. by
bids) is the same as the ranking by ci pi (by CPCs).
Now we will look at the equilibrium properties of the mechanism. We start by noting that
truth telling is not a dominant strategy. This trivially follows, since GSP is a special case
of the proposed mechanism, and it is generally known that truth telling is not a dominant
strategy for GSP. Hence we focus on Nash Equilibrium conditions in our analysis.
Theorem 4 (Nash Equilibrium) Without loss of generality, assume that advertisers are
ordered in decreasing order of cŒºi vi i where vi is the private value of the i th advertiser. The
advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
(19)
bi =
ci
Œºi+1
This equilibrium is socially optimal as well as optimal for search engines for the given
CPC‚Äôs.
Proof Sketch The inductive proof shows that for these bid values, no advertisers can
increase his profit by moving up or down in the ranking. The full proof is given in
Appendix A-4. Since the ranking is the same as the social optima order in (13), social
optimality is a direct implication.

J Intell Inf Syst

We do not rule out the existence of multiple equilibriums. The stated equilibrium is
particularly interesting, due to the social optimality and search engine optimality. Furthermore, although the equilibrium depends on the private values of the advertisers unknown
to the search engine, please keep in mind that search engines do not implement equilibriums directly. Instead, search engines decide the pricing and ranking, and the advertisers
may reach an equilibrium by repeatedly revising auction prices. The pricing and ranking are
practical, since they depend solely on the quantities known to the search engine.
The following Lemmas show that equilibriums of other placement mechanisms are special cases of the proposed CE equilibrium. The stated equilibrium reduces to equilibriums
in the Overture mechanism and GSP under the same assumptions (discussed above) under
which the CE ranking reduces to Overture and GSP rankings.
Lemma 8 The bid values
bi = vi ci + (1 ‚àí ci )bi+1

(20)

are in a pure strategy Nash Equilibrium in the Overture mechanism. This corresponds to
the substitution of the assumption ‚àÄi Œ≥i = 0 (i.e. Œºi = ci ) in Theorem 4.
The proof follows from Theorem 4 as both pricing and ranking are shown to be a special
case of our proposed mechanism.
Similarly for GSP,
Lemma 9 The bid values
bi = vi k + (1 ‚àí k)bi+1 ci+1

(21)

is a pure strategy Nash Equilibrium in the GSP mechanism.
This equilibrium corresponds to the substitution of the assumption ‚àÄi Œ≥i = k ‚àí ci (1 ‚â•
k ‚â• 0) in Theorem 4. Since this is a special case, this result follows from Theorem 4.

6.3 Comparison with VCG mechanism
We compare the revenue and equilibrium of CE mechanism with those of VCG (Vickrey
1961; Clarke 1971; Groves 1973). VCG auctions combine an optimal allocation (ranking)
with VCG pricing. VCG payment of a bidder is equal to the reduction of revenues of other
bidders due to the presence of the bidder. A well known property is that VCG pricing with
any socially optimal allocation has truth telling as the the dominant strategy equilibrium.
In the context of online ads, a ranking optimal with respect to the bid amounts is socially
optimal ranking for VCG. This optimal ranking is bŒºi ci i ; as directly implied by the (1) on
substituting bi for utilities. Hence this ranking combined with VCG pricing has truth telling
as the dominant strategy equilibrium. Since bi = vi at the dominant strategy equilibrium,
ranking is socially optimal for advertiser‚Äôs true value as suggested in (13).
The CE ranking function is different from VCG since CE ranking by payments optimizes
search engine profits. On the other hand, VCG ranking optimizes the advertiser‚Äôs profit.
But Theorem 3 shows that for the pricing used in CE, ordering of CE is the same as that
of VCG. This order preserving property facilitates the comparison of CE with VCG. The
theorem below shows revenue dominance of CE over VCG for the same bid values of the
advertisers.

J Intell Inf Syst

Theorem 5 (Search Engine Revenue Dominance) For the same bid values for all the advertisers, the search engine revenue by CE mechanism is greater than or equal to its revenue
by VCG.
Proof Sketch The proof is an induction based on the fact that the ranking by CE and VCG
are the same, as mentioned above. Full proof is given in Appendix A-5.
This theorem shows that the CE mechanism is likely to provide higher revenue to the
search engine even during transient times before the bids settle on equilibriums.
Based on Theorem 5, we prove revenue equivalence of the proposed CE equilibrium
with dominant strategy equilibrium of VCG.
Theorem 6 (Equilibrium Revenue Equivalence) At the equilibrium in Theorem 4, the
revenue of the search engine is equal to the revenue of the truthful dominant strategy
equilibrium of VCG.
Proof Sketch The proof is an inductive extension of Theorem 5. Please see Appendix A-6
for complete proof.
Note that the CE equilibrium has lower bid values than VCG at the equilibrium, but
provides the same profit to the search engine.

7 CE ranking considering mutual influences: diversity ranking
An assumption in CE ranking is that the entities are mutually independent as we pointed out
in Section 3. In other words, the three parameters‚ÄîU (e), C(e) and Œ≥ (e)‚Äîof an entity do
not depend on other entities in the ranked list. In this section we relax this assumption and
analyze the implications. Since the nature of the mutual influence may vary for different
problems, we base our analysis on a specific well known problem‚Äîranking considering
diversity (Carterette 2010; Agrawal et al. 2009; Rafiei et al. 2010).
Diversity ranking accounts for the fact that the utility of an entity is reduced by the
presence of a similar entity above in the ranked list. This is a typical example of the mutual
influence between the entities. All the existing objective functions for the diversity ranking
are known to be NP-Hard (Carterette 2010). We analyze a basic form of diversity ranking
to explain why this is a fundamentally hard problem.
We modify the objective function in (1) slightly to distinguish between the standalone
utilities and the residual utilities‚Äîutility of an entity in the context of other entities in the
list‚Äîas,
E(U ) =

N


Ur (ei )Pc (ei )

(22)

i=1

where Ur (ei ) denotes the residual utility.
We examine a simple case of diversity ranking problem by considering a set of entities‚Äî
all having the same utilities, perceived relevances and abandonment probabilities. Some of
these entities are repeating. If an entity in the ranked list is the same as the entity in the
list above, the residual utility of that entity becomes zero. In this case, it is intuitive that
the optimal ranking is to place the maximum number of pair-wise dissimilar entities in the

J Intell Inf Syst

top slots. The theorem below shows that even in this simple case the optimal ranking is
NP-Hard.
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Sketch The proof is by reduction from the independent set problem. See
Appendix A-7 for the complete proof.
Moreover, the proof by reduction from independent set problem has more implications
than NP-Hardness as shown in the following corollary,
Corollary 1 The constant approximation algorithm for ranking considering diversity is
hard.
Proof The proof of NP-Hardness in the theorem above shows that the independent set problem is a special case of diversity ranking. This implies that a constant ratio approximation
algorithm for the optimal diversity ranking would be a constant ratio approximation algorithm for the independent set problem. Since a constant ratio approximation algorithm for
the independent set is known to be hard (cf. Garey and Johnson 1976 and HaÃästad 1996), the
corollary follows. To define hard, in his landmark paper HaÃästad proved that independent set
problem cannot be solved within n1‚àí for  > 0 unless all problems in N P are solvable in
probabilistic polynomial time, which is widely believed to be not possible.2
This section shows that the optimal ranking considering mutual influences of parameters
is hard. We leave formulating approximation algorithms (not necessarily constant ratio) for
future research.
Beyond proving the intractability of mutual influence ranking, we believe that the
intractability of the simple scenario here explains why all optimal diversity rankings and
constant ratio approximations are likely to be intractable. Furthermore, the proof based
on the reduction from the well explored independent set problem may help in adapting
approximation algorithms from graph theory.

8 Experiments
We compare the profit improvement by CE and reduced forms to existing rankings.
Although the optimality of the proposed ranking is proven above, experiments help to quantify the increase in utilities. Considering the very restricted access to real users and ad click
logs, we limit our evaluations to simulations as it is common in computational advertisement
research. We believe that these experimental results will motivate future online evaluations
in industry settings.
In our first experiment in Fig. 3a, we compare the CE ranking with rank by bid
amount (11) strategy by Overture and rank by bid √ó perceived relevance (12) by Google.
We assign the perceived relevance values as a uniform random number between 0 and Œ±
(0 ‚â§ Œ± ‚â§ 1) and abandonment probabilities as random between 0 and 1 ‚àí Œ±. This assures
‚àÄi (C(ai ) + Œ≥ (ai )) ‚â§ 1 condition required in the click model. The bid amounts for ads are

2 This

belief is almost as strong as the belief P  = N P

J Intell Inf Syst

Fig. 3 a Comparison of Overture, Google and CE rankings. Perceived relevances are uniformly random
in [0, Œ±] and abandonment probabilities are uniformly random in [0, 1 ‚àí Œ±]. CE provides optimal expected
profits for all values of Œ±. b Comparison of CE, PRP and abandonment ranking (9). Abandonment ranking
dominates PRP

J Intell Inf Syst

Fig. 4 Optimality of reduced forms under assumptions (a) setting Œ≥ (d) = k ‚àí R(d). Perceived relevance
ranking is optimal for all values of Œ±. (b) setting C(d) = R(d). In this case, abandonment ranking is optimal

assigned uniform randomly between 0 and 1. We use uniform random for values as it is the
maximum entropy distribution and hence makes least assumptions about the bid amounts.
The number of relevant ads (corresponding to the number of bids on a query) is set to fifty.

J Intell Inf Syst

Simulated users are made to click on ads. The number of ads clicked is set to a random
number generated in a zipf distribution with exponent 1.5. A power law is most intuitive for
the distribution of the number of clicks.
Simulated users browse down the list. Users click an entity with probability equal to the
perceived relevance and abandon the search results with a probability equal to the abandonment probability. The set of entities to be placed is created at random for each run. For the
same set of entities, three runs‚Äîone with each ranking strategy‚Äîare performed. Simulation
is repeated 2 √ó 105 times for each value of Œ±.
An alternate interpretation of Fig. 3a is as the comparison of ranking by CE, PRP and
perceived relevance ranking (8). As we discussed, PRP and perceived relevance rankings
are isomorphic to ad rankings by bid and bid √ó perceived relevance respectively, with utility
being relevance instead of bid amounts. The simulation results are the same.
In Fig. 3b we compare CE, PRP and abandonment ranking (9) under the same settings
used for Fig. 3a. CE provides the maximum utility as expected, and abandonment ranking
occupies the second place. Abandonment ranking provides sub-optimal utility‚Äîsince the
condition ‚àÄd R(d) = C(d) is not satisfied‚Äîbut dominates over PRP. Further, as abandonment probability becomes zero (i.e. Œ± = 1) abandonment rankings becomes same as PRP
and optimal as we predicted in Section 5.1.
Figure 4a compares the perceived relevance ranking (8), CE, and PRP under the condition for optimality for perceived relevance ranking (i.e. ‚àÄd Œ≥ (d) = k ‚àí R(d)). For this,
we set Œ≥ (d) = Œ± ‚àí C(d) keeping all other settings same as the previous experiments.
Figure 4a shows that the perceived relevance ranking provides optimal utility, exactly overlapping with CE curve as expected. Furthermore, note that utilities by PRP are very low
under this condition. The utilities by PRP in fact goes down after Œ± = 0.2. The increase
in abandonment probability, as well as increased sub-optimality of PRP for higher abandonment (since PRP does not consider abandonment) probabilities may be causing this
reduction.
In our next experiment shown in Fig. 4b, we compare abandonment ranking (9) with PRP
and CE under the condition ‚àÄd C(d) = R(d) (i.e. optimality condition for abandonment
ranking). All other settings are the same as those for the experiments in Fig. 3a and b.
Here we observe that the abandonment ranking is optimal and exactly overlaps with CE as
expected. PRP is sub-optimal but closer to optimal than random C(d) used for experiments
in Fig. 3b. The reason may be that C(d) = R(d) is one of the two conditions required for
PRP to be optimal for both sets of assumptions as we discussed in Subsection 5.1. When
abandonment probability becomes zero PRP relevance reaches optimum as we have already
seen.
All these simulation experiments confirm the predictions by the theoretical analysis
above. Although the simulation is no substitute for experiments on real data, we expect that
the observed significant improvements in expected utilities would motivate future online
experiments to quantify profit.

9 Conclusion and future work
We approach the document and ad ranking as a utility maximization based on the user
click model, and derive an optimal ranking‚Äînamely CE ranking. CE ranking is simple and
intuitive; and optimal considering perceived relevance and abandonment probability of user
behavior.

J Intell Inf Syst

On specific assumptions on parameters, the CE ranking function spawns a taxonomy of
rankings in multiple domains. The taxonomy shows that the existing document and ad ranking strategies are special cases of the proposed ranking function under specific assumptions.
The taxonomy is helpful in selecting optimal ranking for a specific user behavior.
To apply CE ranking to ad auctions, we incorporate a second-price based pricing mechanism. The resulting CE mechanism has a Nash Equilibrium which simultaneously optimizes
both the search engine and advertiser revenues. The CE mechanism is revenue dominant
over VCG for the same bid vectors, and has an equilibrium which is revenue equivalent with
the truthful equilibrium of VCG.
We relax the assumption of independence between entities in CE ranking and apply it
to diversity ranking. The ensuing analysis reveals that diversity ranking is an inherently
hard problem; since even the basic formulations are NP-Hard with unlikely constant ratio
approximation algorithms. Furthermore our simulation experiments confirm the results, and
suggest potentially significant increase in profits over the existing rankings.
As future research, assessing profits by CE ranking in an online experiment on a
large scale search engine will quantify improvement in ranking. Estimation and prediction of abandonment probability using click logs and statistical models are interesting
problems. The suggested ranking is optimal for other web ranking scenarios with similar
click models‚Äîlike product and friend recommendations‚Äîand may be extended to these
problems. Furthermore, effective approximation schemes for diversity ranking based on
similarity with the independent set problem may be investigated.
Acknowledgments This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR
grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027, two Google faculty research awards
(2010 & 2013), and a Yahoo key scientific challenges program award (2009).

Appendix
A-1 Proof of theorem 1
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
CE(ei ) =

U (ei )C(ei )
C(ei ) + Œ≥ (ei )

Proof Consider results ei and ei+1 in positions i and i + 1 respectively. Let Œºi = Œ≥ (ei ) +
C(ei ) for notational convenience. The total expected utility from ei and ei+1 when ei is
placed above ei+1 is
i‚àí1




(1 ‚àí Œºj ) U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 )

j =1

If the order of ei and ei+1 are inverted by placing ei above ei+1 , the expected utility from
these entities will be,
i‚àí1




(1 ‚àí Œºj ) U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei ))

j =1

J Intell Inf Syst

Since utilities from all other results in the list will remain the same, the expected utility of
placing ei above ei+1 is greater than inverse placement iff
U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 ) ‚â• U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei )

U (ei+1 )C(ei+1 )
U (ei )C(ei )
‚â•
Œºi
Œºi+1
U (e)C(e)
This means if entities are ranked in the descending order of C(e)+Œ≥
(e) any inversions will
reduce the profit. Since any arbitrary order can be effected by a number of inversions on the
U (e)C(e)
ranking by CE, this implies that ranking by C(e)+Œ≥
(e) is optimal.

A-2 Proof of theorem 2
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Induction on number of clicks.
Base Case: Single click, proved in Theorem 1.
Inductive Hypothesis: The proposed ordering is optimal for n clicks.
Let there be total of n ranked entities and ec be the nth clicked entity. The user will browse
down starting next to ec . Since there is only one click remaining, optimal ordering of entities is in the descending order of CE by the base case. Since the relevance and abandonment
probabilities ec+1 to en remain unchanged by the independence assumption above, the
optimal sequence will be the sub-sequence of ec+1 to en in the ranking.

A-3 Proof of theorem 3
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj
Proof Without loss of generality, we assume that ai refers to ad in the position i in the
descending order of wi bi .
pi ci
Œºi
bi+1 ci+1 Œºi ci
=
Œºi+1 ci Œºi
bi+1 ci+1
=
Œºi+1
= wi+1 bi+1

CEi =

‚â• wi+2 bi+2 = CEi+1

J Intell Inf Syst

A-4 Proof of theorem 4
Theorem 4 (Nash Equilibrium) : Without the loss of generality assume that the advertisers
are ordered in the decreasing order of cŒºi vi i where vi is the private value of the i th advertiser.
The advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
bi =
ci
Œºi+1
This equilibrium is socially optimal for advertisers as well as optimal for search engines for
the given CPC‚Äôs.
Proof Let there are n advertisers. Without loss of generality, let us assume that advertisers
are indexed in the descending order of vŒºi ci i . We prove equilibrium in two steps.
Step 1: Prove that
wi bi ‚â• wi+1 bi+1

w i bi =

(1)

bi c i
Œºi

Expanding bi by (19),
bi+1 ci+1
Œºi+1
= vi ci + (1 ‚àí Œºi )wi+1 bi+1
vi ci
=
Œºi + (1 ‚àí Œºi )wi+1 bi+1
Œºi

wi bi = vi ci + (1 ‚àí Œºi )

Notice that wi bi is a convex linear combination of wi+1 bi+1 and vŒºi ci i . This means that
the value of wi bi is in between (or equal to) the values of wi+1 bi+1 and vŒºi ci i . Hence to
prove that wi bi ‚â• wi+1 bi+1 all we need to prove is that vŒºi ci i ‚â• wi+1 bi+1 . This inductive
proof is given below.
Induction hypothesis: Assume that
‚àÄi‚â•j

vi ci
‚â• wi+1 bi+1
Œºi

Base case: Prove for i = N i.e. for the bottommost ad.
vN‚àí1 cN‚àí1
‚â• wN b N
ŒºN‚àí1
Assuming ‚àÄi>N bi = 0
wN bN = vN cN ‚â§

vN cN
vN‚àí1 cN‚àí1
vi ci
(as ŒºN ‚â§ 1) ‚â§
(by the assumed order i.e. by
)
ŒºN
ŒºN‚àí1
Œºi

Induction: Expanding wj bj by (19),
wj bj =

vj cj
Œºj + (1 ‚àí Œºj )wj +1 bj +1
Œºj

J Intell Inf Syst

wj bj is the convex linear combination, i.e
vj cj
Œºj

vj cj
Œºj

‚â• wj bj ‚â• wj +1 bj +1 , as we know that

‚â• wj +1 bj +1 by induction hypothesis. Consequently,
wj bj ‚â§

vj ‚àí1 cj ‚àí1
vj cj
‚â§
(by the assumed order)
Œºj
Œºj ‚àí1

This completes the induction.

Since advertisers are ordered by wi bi for pricing, the above proof says that the pricing
order is the same as the assumed order in this proof (i.e. ordering by vŒºi ci i ). Consequently,
pi =

bi+1 ci+1 Œºi
Œºi+1 ci

As corollary of Theorem 3 we know that CEi ‚â• CEi+1 .
In the second step we prove the equilibrium using results in Step 1.
Step 2: No advertiser can increase his profit by changing his bids unilaterally
Proof (of lack of incentive to undercut to advertisers below) In the first step let us prove that
ad ai can not increase his profit by decreasing his bid to move to a position j ‚â• i below.
Inductive hypothesis: Assume true for i ‚â§ j ‚â§ m.
Base Case: Trivially true for j = i.
Induction: Prove that the expected profit of ai at m + 1 is less or equal to the expected
profit of ai at i.
Let œÅk denotes the amount paid by ai when he is at the position k. By inductive hypothesis, the expected profit at m is less or equal to the expected profit at i. So we just need to
prove that the expected profit at m + 1 is less or equal to the expected profit at m. i.e.
m
m+1
(vi ‚àí œÅm+1 ) 
(vi ‚àí œÅm ) 
(1 ‚àí Œºl ) ‚â•
(1 ‚àí Œºl )
(1 ‚àí Œºi )
(1 ‚àí Œºi )
l=1

l=1

Canceling the common terms,
vi ‚àí œÅm ‚â• (vi ‚àí œÅm+1 )(1 ‚àí Œºm+1 )

(2)

œÅm ‚Äîthe price charged to ai at position m‚Äîis based on the Equations 16 and 19. Since the
ai is moving downward, ai will occupy position m by shifting ad am upwards. Hence the ad
just below ai is am+1 . Consequently, the price charged to ai when it is at the mth position is,

	
bm+1 cm+1 Œºi
Œºi
bm+2 cm+2
=
vm+1 cm+1 + (1 ‚àí Œºm+1 )
œÅm =
Œºm+1 ci
ci
Œºm+2
Substituting for œÅm and œÅm+1 in (2),

	
Œºi
bm+2 cm+2
vm+1 cm+1 + (1 ‚àí Œºm+1 )
vi ‚àí
ci
Œºm+2

	


Œºi
bm+3 cm+3
vm+2 cm+2 + (1 ‚àí Œºm+2 )
(1‚àíŒºm+1 )
‚â• vi ‚àí
ci
Œºm+3

J Intell Inf Syst

Simplifying, and multiplying both sides by ‚àí1

	
bm+2 cm+2
Œºi
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
	

bm+3 cm+3
√ó vm+2 cm+2 + (1 ‚àí Œºm+2 )
Œºm+3
Substituting by bm+2 from (19) on RHS.

	
bm+2 cm+2
Œºi
bm+2 cm+2
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
Œºm+2
Canceling out the common terms on both sides,
Œºi
vm+1 cm+1 ‚â§ vi Œºm+1
ci

vi ci
vm+1 cm+1
‚â§
Œºm+1
Œºi
Which is true by the assumed order as m ‚â• i
Inductive proof for m ‚â§ i is somewhat similar and enumerated below.
Inductive hypothesis: Assume true for j ‚â§ m.
Base Case: Trivially true for j = i.
Proof (of lack of incentive to overbid ad one above) The case in which ai increases his bid
to move one position up i.e. to i ‚àí 1 is a special case and need to be proved separately. In
this case, by moving a single slot up, the index of the ad below ai will change from i + 1
to i ‚àí 1 (a difference of two). For all other movements of ai to a position one above or one
below, the index of the advertisers below will change only by one. Since the amount paid
by ai depends on the ad below ai , this case warrants a slightly different proof,
(vi ‚àí œÅi )

i‚àí1


(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

l=1

i‚àí2


(1 ‚àí Œºl )

l=1


(vi ‚àí œÅi )(1 ‚àí Œºi‚àí1 ) ‚â• vi ‚àí œÅi‚àí1
Expanding œÅi is straight forward.To expand œÅi‚àí1 , note that when ai has moved upwards to
i ‚àí 1, the ad just
 below ai is ai‚àí1 . Since ai‚àí1 has not changed its bids, the œÅi‚àí1 can be
Œºi
expanded as ci vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 ) bŒºi ci i . Substituting for œÅi and œÅi‚àí1 ,


vi ‚àí



	
	
Œºi
Œºi
bi+2 ci+2
bi ci
vi+1 ci+1 + ‚â• vi ‚àí
vi‚àí1 ci‚àí1 + (1‚àíŒºi+1 )
(1 ‚àí Œºi‚àí1 )(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi+2
Œºi

Simplifying and multiplying by ‚àí1
vi Œºi‚àí1 +



	
	
Œºi
Œºi
bi ci
bi+2 ci+2
vi+1 ci+1 + ‚â§
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi+1 )
(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi
Œºi+2

J Intell Inf Syst

Substituting bi+1 from (19)


	
Œºi bi+1 ci+1
Œºi
bi c i
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi‚àí1 ) ‚â§
ci Œºi+1
ci
Œºi

Œºi
bi+1 ci+1
Œºi vi‚àí1 ci‚àí1
Œºi
bi c i
‚â§
+ (1 ‚àí Œºi‚àí1 )
vi Œºi‚àí1 + (1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
ci
Œºi
vi Œºi‚àí1 +

We now prove that both the terms in RHS are greater or equal to the corresponding terms in
LHS separately.
Œºi vi‚àí1 ci‚àí1
vi Œºi‚àí1 ‚â§
ci

vi‚àí1 ci‚àí1
vi ci
‚â§
Œºi
Œºi‚àí1
Which is true by our assumed order.
Similarly,
bi+1 ci+1
Œºi
bi c i
Œºi
(1 ‚àí Œºi‚àí1 )
‚â§
(1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
Œºi

bi ci
bi+1 ci+1
‚â§
Œºi+1
Œºi
Which is true by (1) above. This completes the proof for this case.
Induction: Prove that the expected profit at m ‚àí 1 is less or equal to the expected profit
at m. The proof is similar to the induction for the case m > i.
Proof Base case is trivially true.
(vi ‚àí œÅm )

m‚àí1


m‚àí2


l=1

l=1

(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

(1 ‚àí Œºl )

Canceling common terms,
(vi ‚àí œÅm )(1 ‚àí Œºm‚àí1 ) ‚â• vi ‚àí œÅm‚àí1
In this case, note that ai is moving upwards. This means that ai will occupy position m by
pushing the ad originally at m one position downwards. Hence the original ad at m is the
one just below ai now. i.e.

	
bm cm Œºi
Œºi
bm+1 cm+1
œÅm =
vm cm + (1 ‚àí Œºm )
=
Œºm ci
ci
Œºm+1
Substituting for œÅm and œÅm‚àí1


vi ‚àí



	
	
Œºi
Œºi
bm+1 cm+1
bm cm
vm cm + ‚â• vi ‚àí
vm‚àí1 cm‚àí1 + (1‚àíŒºm )
(1‚àíŒºm‚àí1 )(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm+1
Œºm

Simplifying and multiplying by ‚àí1
vi Œºm‚àí1 +



	
	
Œºi
Œºi
bm cm
bm+1 cm+1
vm cm + ‚â§
vm‚àí1 cm‚àí1 + (1‚àíŒºm‚àí1 )
(1 ‚àí Œºm )
(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm
Œºm+1

J Intell Inf Syst

Substituting by bm from (19)

	
Œºi bm cm
Œºi
bm c m
vm‚àí1 cm‚àí1 + (1 ‚àí Œºm‚àí1 )
(1 ‚àí Œºm‚àí1 ) ‚â§
vi Œºm‚àí1 +
ci Œºm
ci
Œºm
Canceling common terms,
Œºi
vm‚àí1 cm‚àí1
ci

vi Œºm‚àí1 ‚â§


vm‚àí1 cm‚àí1
vi ci
‚â§
Œºi
Œºm‚àí1
Which is true by the assumed order as m < i.

A-5 Proof of theorem 5
Theorem 5 (Search Engine Revenue Dominance) : For the same bid values for all the
advertisers, the revenue of search engine by CE mechanism is greater or equal to the
revenue by VCG.
Proof VCG payment of the ad at position i (i.e. ai ) is equal to the reduction in utility of
the ads below due to the presence of ai . For each user viewing the list of ads (i.e. for unit
view probability), the total expected loss of ads below ai due to ai is,

piVu =
=

=

j
‚àí1
j
‚àí1
n
n


1
bj c j
(1 ‚àí Œºk ) ‚àí
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi

Œºi
1 ‚àí Œºi

j =i+1

k=1

n


j
‚àí1

bj c j

j =i+1

j =i+1

k=1

(1 ‚àí Œºk )

k=1

j
‚àí1
n
i

Œºi 
(1 ‚àí Œºk )
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi
j =i+1

k=1

= Œºi

i‚àí1


(1 ‚àí Œºk )

n


bj c j

j =i+1

k=1

k=i+1

j
‚àí1

(1 ‚àí Œºk )

k=i+1

This is the expected lose per user browsing the ad list. Pay per click should be equal to the
lose per click. To calculate the pay per click, we divide by the click probability of ai . i.e.

piV =
=

Œºi

i‚àí1

j ‚àí1
j =i+1 bj cj
k=i+1 (1 ‚àí Œºk )
i‚àí1
ci k=1 (1 ‚àí Œºk )
j
‚àí1

k=1 (1 ‚àí Œºk )

n
Œºi 
bj c j
ci
j =i+1

n

(1 ‚àí Œºk )

k=i+1

J Intell Inf Syst

Converting to recursive form,
bi+1 Œºi
Œºi ci+1 V
ci+1 + (1 ‚àí Œºi+1 )
p
ci
ci Œºi+1 i+1
bi+1 Œºi ci+1
Œºi ci+1 V
=
Œºi+1 + (1 ‚àí Œºi+1 )
p
ci Œºi+1
ci Œºi+1 i+1

piV =

For the CE mechanism payment from (16) is,
piCE =

bi+1 ci+1 Œºi
Œºi+1 ci

Note that piV is convex combination of PiCE and
two values. To prove that

piCE

‚â•

piV

Œºi ci+1 V
ci Œºi+1 pi+1 ,

and hence is between these

all we need to prove is that PiCE ‚â•

Œºi ci+1 V
ci Œºi+1 pi+1

‚áî

bi ‚â• piV . This directly follows from individual rationality property of VCG. Alternatively, a
V = 0 (bottommost ad) will prove the same. Note that
simple recursion with base case as pN
we consider only the ranking (not selection), and hence the VCG pricing of the bottommost
ad in the ranking is zero.

A-6 Proof of theorem 6
Theorem 6 (Equilibrium Revenue Equivalence) : At the equilibrium in Theorem 4, the revenue of search engine is equal to the revenue of the truthful dominant strategy equilibrium
of VCG.
Proof Rearranging (3) and substituting true values for bid amounts,

	
Œºi
(1 ‚àí Œºi+1 )ci+1 V
V
vi+1 ci+1 +
pi+1
pi =
ci
Œºi+1
For the CE mechanism, substituting equilibrium bids from (19) in payment (16),

	
bi+1 ci+1 Œºi
Œºi
bi+2 ci+2
=
vi+1 ci+1 + (1 ‚àí Œºi+1 )
piCE =
Œºi+1 ci
ci
Œºi+2
Rewriting bi+2 in terms of pi+1 ,
piCE =


	
Œºi
(1 ‚àí Œºi+1 )ci+1 CE
vi+1 ci+1 +
pi+1
ci
Œºi+1

= piV

V
CE
(iff pi+1
= pi+1
)

Ad at the bottommost position pays same amount zero, a simple recursion will prove that
the payment for all positions for both VCG and the proposed equilibrium is the same.

A-7 Proof of theorem 7
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Independent set problem can be formulated as a ranking problem considering similarities. Consider an unweighed graph G of n vertices {e1 , e2 , ..en } represented as an

J Intell Inf Syst

adjacency matrix. This conversion is clearly polynomial time. Now, consider the values in
the adjacency matrix as the similarity values between the entities to be ranked. Let the entities have the same utilities, perceive relevances and abandonment probabilities. In this set of
n entities from {e1 , e2 , .., en }, clearly the optimal ranking will have k pairwise independent
entities as the top k entities for a maximum possible value of k. But the set of k independent
entities corresponds to the maximum independent set in graph G.

References
Aggarwal, G., Feldman, J., Muthukrishnan, S., & PaÃÅl, M. (2008). Sponsored search auctions with markovian
users. Internet and Network Economics, 621‚Äì628.
Aggarwal, G., Goel, A., & Motwani, R. (2006). Truthful auctions for pricing search keywords. In Proceedings
of the 7th ACM conference on Electronic commerce (pp. 1‚Äì7), ACM.
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying search results. In Proceedings of
the Second ACM International Conference on Web Search and Data Mining (pp. 5‚Äì14). ACM.
Balakrishnan, R., & Kambhampati, S. (2008). Optimal ad ranking for profit maximization. In Proceedings
of the 11th International Workshop on the Web and Databases.
Carterette, B. (2010). An analysis of NP-completeness in novelty and diversity ranking. Advances in
Information Retrieval Theory, 200‚Äì211.
Chapelle, O., & Zhang, Y. (2009). A dynamic bayesian network click model for web search ranking. In
Proceedings of World Wide Web (pp. 1‚Äì10). ACM.
Chierichetti, F., Kumar, R., & Raghavan, P. (2011). Optimizing two-dimensional search results presentation.
In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 257‚Äì
266). ACM.
Clarke, C.L.A., Agichtein, E., Dumais, S., & White, R.W. (2007). The influence of caption features on
clickthrough patterns in web search. In Proceedings of SIGIR (pp. 135‚Äì142). ACM.
Clarke, E.H. (1971). Multipart pricing of public goods. Public Choice, 11(1), 17‚Äì33.
Craswell, N., Zoeter, O., Tayler, M., & Ramsey, B. (2008). An experimental comparison of click position
bias models. In Proceedings of WSDM (pp. 87‚Äì94).
Deng, X., & Yu, J. (2009). A new ranking scheme of the GSP mechanism with markovian users. Internet and
Network Economics, 583‚Äì590.
Dupret, G.E., & Piwowarski, B. (2008). A user browsing model to predict search engine click data from past
observations. In Proceedings of SIGIR, (pp. 331‚Äì338). ACM.
Easley, D., & Kleinberg, J. (2010). Networks, crowds, and markets: Reasoning about a highly connected
world: Cambridge Univ Press.
Edelman, B., Ostrovsky, M., & Schwarz, M. (2007). Internet advertising and the generalized second price
auction: Selling billions of dollars worth of keywords. The American Economic Review, 97(1).
Garey, M.R., & Johnson, D.S. (1976). The complexity of near-optimal graph coloring. Journal of the ACM
(JACM), 23(1), 43‚Äì49.
Ghosh, A., & Sayedi, A. (2010). Expressive auctions for externalities in online advertising. In Proceedings
of the 19th international conference on World wide web (pp. 371‚Äì380). ACM.
Giotis, I., & Karlin, A. (2008). On the equilibria and efficiency of the GSP mechanism in keyword auctions
with externalities. Internet and Network Economics, 629‚Äì638.
Gordon, M.G., & Lenk, P. (1991). A utility theory examination of probability ranking principle in information
retrieval. Journal of American Society of Information Science, 41, 703‚Äì714.
Gordon, M.G., & Lenk, P. (1992). When is probability ranking principle suboptimal. Journal of American
Society of Information Science, 42.
Groves, T. (1973). Incentives in teams. Econometrica: Journal of the Econometric Society, 617‚Äì631.
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.M., & Faloutsos, C. (2009). Click chain model
in web search. In Proceedings of World Wide Web (pp. 11‚Äì20). New York: ACM.
HaÃästad, J. (1996). Clique is hard to approximate within n. In Foundations of Computer Science, 1996. 37th
Annual Symposium on Proceedings (pp. 627‚Äì636).
Hu, B., Zhang, Y., Chen, W., Wang, G., & Yang, Q. (2011). Characterizing search intent diversity into click
models. In Proceedings of the 20th international conference on World wide web (pp. 17‚Äì26). ACM.
Kempe, D., & Mahdian, M. (2008). A cascade model for externalities in sponsored search. Internet and
Network Economics, 585‚Äì596.

J Intell Inf Syst
Kuminov, D., & Tennenholtz, M. (2009). User modeling in position auctions: re-considering the gsp and vcg
mechanisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 1 (pp. 273‚Äì280).
Rafiei, D., Bharat, K., & Shukla, A. (2010). Diversifying Web Search Results. In Proceedings of World Wide
Web.
Richardson, M., Dominowska, E., & Ragno, R. (2007). Predicting clicks: Estimating the click-through rate
for new ads. In Proceedings of World Wide Web.
Richardson, M., Prakash, A., & Brill, E. (2006). Beyond pagerank: Machine learning for static ranking. In
World Wide Web Proceedings (pp. 707‚Äì714). ACM.
Robertson, S.E. (1977). The probability ranking principle in ir. Journal of Documentation, 33, 294‚Äì304.
Varian, H.R. (2007). Position auctions. International Journal of Industrial Organization, 25(6), 1163‚Äì1178.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1), 8‚Äì37.
Xu, W., Manavoglu, E., & Cantu-Paz, E. (2010). Temporal click model for sponsored search. In Proceedings
of the 33rd international ACM SIGIR conference on Research and development in information retrieval
(pp. 106‚Äì113). ACM.
Yilmaz, E., Shokouhi, M., Craswell, N., & Robertson, S. (2010). Expected browsing utility for web search
evaluation. In Proceedings of the 19th ACM international conference on Information and knowledge
management (pp. 1561‚Äì1564). ACM.
Yue, Y., Patel, R., & Roehrig, H. (2010). Beyond position bias: Examining result attractiveness as a source
of presentation bias in clickthrough data. In Proceedings of World Wide Web.
Zhu, Z.A., Chen, W., Minka, T., Zhu, C., & Chen, Z. (2010). A novel click model and its applications to
online advertising. In In Proceedings of Web search and data mining (pp. 321‚Äì330). ACM.

A Combinatorial Search Perspective on Diverse Solution Generation
Satya Gautam Vadlamudi and Subbarao Kambhampati
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{gautam , rao}@asu.edu

Abstract
Finding diverse solutions has become important in many
combinatorial search domains, including Automated Planning, Path Planning and Constraint Programming. Much of
the work in these directions has however focussed on coming up with appropriate diversity metrics and compiling
those metrics in to the solvers/planners. Most approaches use
linear-time greedy algorithms for exploring the state space
of solution combinations for generating a diverse set of solutions, limiting not only their completeness but also their
effectiveness within a time bound. In this paper, we take a
combinatorial search perspective on generating diverse solutions. We present a generic bi-level optimization framework
for finding cost-sensitive diverse solutions. We propose complete methods under this framework, which guarantee finding
a set of cost sensitive diverse solutions satisficing the given
criteria whenever there exists such a set. We identify various aspects that affect the performance of these exhaustive
algorithms and propose techniques to improve them. Experimental results show the efficacy of the proposed framework
compared to an existing greedy approach.

In many real-world domains involving combinatorial search
such as automated planning, path planning and constraint
programming, generating diverse solutions is of much importance. In the case of automated planning, real-world scenario often involves working with unknown or partially
known user preferences (Kambhampati 2007), as the user
preferences are many times difficult to be articulated and
specified completely. Such situations lead to multiple, often, large number of plans that satisfy a given problem instance. In order to facilitate serving the user with a closest
plan possible as per her (hidden) preferences, presenting a
diverse set of plans to the user is explored (Roberts, Howe,
and Ray 2014; Nguyen et al. 2012) so that the user can make
a well-informed decision. In the constraint programming domain, diverse (resp. similar) solutions are explored in order
to handle unknown user preferences as well as to generate
robust solutions (Hebrard et al. 2005).
Several methods have been proposed in the literature for
finding a diverse set of plans. In the context of constraint
programming, (Hebrard et al. 2005) presents a complete
method which creates K copies of the Constraint Satisfacc 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

tion Problem (CSP),each copy with a different set of variable names, adds K
2 additional constraints for handling the
minimum distance requirement between all pairs, and uses
off-the-shelf solvers to generate solutions. As one would expect, they report that this approach generates prohibitively
large CSPs and therefore propose a greedy method. The
greedy approach which has since been widely adopted (Petit and Trapp 2015; Bloem 2015; Roberts, Howe, and Ray
2014; Nguyen et al. 2012) works as follows:
Obtain a candidate solution satisfying any given cost criteria, add this to the solution K-set, provide feedback to the
method finding candidate solutions about the current composition of the K-set so that it tries to find the next candidate
solution distant to the current K-set. Upon obtaining the next
candidate solution, the greedy method adds it to the K-set if
it indeed satisfies the distance criteria and provides feedback
to find the next solution, otherwise the candidate solution is
simply discarded. This process is continued until a set of K
diverse solutions is found. (Roberts, Howe, and Ray 2014;
Eiter et al. 2013) and (Nguyen et al. 2012) consider the
first solution generated to be the starting solution (permanent member) for constructing the K-set through the above
greedy approach. (Bloem 2015) considers an optimal solution to be the starting solution of the greedy method. (Petit
and Trapp 2015) attempt to address the issue of fixed starting solution by running the greedy approach multiple times
with different optimal solutions as the starting solution on
each occasion.
A pertinent issue with the above approaches is that the
first solution (or an optimal solution) is always considered
to be part of the solution set, which may often result in not
finding a K-set even when there exists one, even with a very
good feedback strategy to search for distant solutions after
finding the initial solution. Note that an optimal solution (or
the first found solution) need not be part of a diverse set of
the required size at all. Figure 1 shows an example of an
instance where the optimal solution is not part of the most
diverse solution set of size 2 (assuming that the distance between two plans is inversely proportional to the number of
edges/actions they have in common; p1 and p2 have edge a
in common and p2 and p3 have edge b in common).
In this paper, we address this problem in depth by proposing complete algorithms which guarantee to find a set of K
diverse solutions whenever there exists one. In order to ac-

Start
p1

1

3
G1

a

1
p2

1
2

1 b

p3

G2
Figure 1: A state-space graph where the optimal cost path
p2, does not belong to the most diverse solution set of size 2
{p1,p3}. G1 and G2 indicate goal nodes.
complish completeness, we first need methods for exploring
all possible cost sensitive solutions in the given domain. For
this, we present extensions of the m-A* algorithm (Dechter,
Flerova, and Marinescu 2012) and the Depth-First Branch
and Bound algorithm (Lawler and Wood 1966) for finding
all cost-bounded plans. One could adapt other types of methods such as anytime heuristic search algorithms as well for
this purpose. Second, we present a simple strategy for exhaustively exploring the set of all plan combinations. This
would guarantee completeness for finding a K-set whenever there exists one, thereby addressing the issues in existing methods. However, we note that the simple exhaustive
method ends up having to explore a large number of combinations as one finds more and more candidate plans, severely
impacting the performance of the overall algorithm. In order
to address this problem, we propose a method which considers only a few most promising plan combinations whenever
a new candidate solution is found, and postpones the exploration of remaining combinations for the end to guarantee
completeness. This new method is advantageous over the
widely followed greedy approach on two fronts: it explores
a larger (compared to only one combination of the greedy
approach) but limited number of combinations upon finding
a candidate solution thereby increasing its likelihood of finding a diverse K-set quickly, and it keeps track of the combinations that are left postponed to revisit at the end thereby
guaranteeing completeness.
Further, our method for exploring the plan-combinations
space can be used in conjunction with any of the existing
methods to improve their performance, as it is complementary in nature, replacing the weaker section of those methods
where the greedy approach is present.

Refanidis 2013), air traffic control advisories (Bloem and
Bambos 2014), and robotics (Voss, Moll, and Kavraki 2015).
An important measure in determining diversity is the distance between plans. In this paper, we assume that the distance measure is given as input by the user. Several distance measures have been proposed in the literature that are
quantitative or qualitative (Scala 2014; Coman and MunÃÉozAvila 2011; Goldman and Kuter 2015). Solution diversity
is explored in both deterministic and non-deterministic domains using the distance metrics (Coman 2012). Distance
measures for finding semantically distinct plans are explored
in (Bryce 2014) based on landmarks. In the context of constraint programming, distance constraints in terms of ideal
and non-ideal solutions are studied in (Hebrard, O‚ÄôSullivan,
and Walsh 2007).
SAT-based heuristic methods for generating diverse solutions were proposed in (Nadel 2011). Methods through
compilation to CSP, and using heuristic local search have
been proposed in (Srivastava et al. 2007), which use GPCSP planner (Do and Kambhampati 2001) and LPG planner (Gerevini, Saetti, and Serina 2003). Comparison of firstprinciple techniques and case-based planning techniques to
find diverse plans is shown in (Coman and MunÃÉoz-Avila
2012a). These algorithms too use the greedy approach presented before for exploring the space of plan combinations,
leading to the same issues pointed in the Introduction.

Problem Setup
In this paper, we consider the problem of finding a set of solutions that are not only diverse but are also cost sensitive.
In particular, we consider the problem of finding a set of K
cost sensitive diverse (loopless) solutions. Cost sensitivity
of the solutions is controlled by the input c (maximum cost
of each of the solutions) and diversity of the solution sets is
controlled by the input d (minimum distance between each
pair of solutions; or an appropriate set based diversity metric). Both the cost metric and the distance metric are also assumed to be inputs from the user, hence, the studies on good
quality cost metrics and distance metrics are orthogonal to
our work. Further, we choose the planning domain to showcase our framework and methods, which could be adapted to
other domains. Hence, the problem at-hand can be formally
stated as: Given a planning problem with the set of loopless
solution plans S, a cost metric for the plans C : S ‚Üí R and
a distance metric for the pairs of plans Œ¥ : S √ó S ‚Üí R (a set
based diversity metric may also be used here), the problem
is defined as:
cCOSTdDISTANTkSET: Find P with P ‚äÜ S,
(1)
|P| =k, min Œ¥(p, q) ‚â• d and C(p) ‚â§ c ‚àÄp ‚àà P
p, q ‚àà P

Related Work
Several applications have been related to using diverse solutions in recent years, such as, for course of action generation in cyber security (Boddy et al. 2005), personalized security agents (Roberts et al. 2012), diverse finite
state machines for non-player characters in games (Coman
and MunÃÉoz-Avila 2013; 2012b), formal verification (Nadel
2011), mining group patterns (Vadlamudi, Chakrabarti, and
Sarkar 2012), scheduling personal activities (Alexiadis and

The problem is computationally hard given that the problem
of finding cost-bounded plans is PSPACE-complete (Bylander 1991) and the problem of finding a diverse set of plans
is NP-complete (Bloem 2015) with input size (number of
plans) that can potentially be exponential in terms of the
number of state variables. Finding a set of diverse solutions
is shown to be FPN P [log n] -complete in the context of constraint programming (Hebrard et al. 2005), where n is the
size of the input.

Explore All Solution Combinations
(Level 2)
Solution Stream

Feedback

Explore All Satisficing Solutions
(Level 1)

Figure 2: A framework for finding a diverse set of solutions
that supports completeness.

Proposed Methods & Properties
Now, we present the proposed framework which supports
completeness, and specific methods that obey the framework
requirements. As mentioned above, the problem of finding a
diverse set of plans is a bi-level optimization problem which
involves exploring the set of all candidate plans (Level 1)
and considering the set of all combinations of these plans
(Level 2). Therefore, in order to guarantee completeness,
we propose to have a framework with a complete method
which guarantees finding all the candidate plans and outputs as a stream, and another complete method that takes
the stream of plans being generated by the previous method
as input and explores all combinations of plan sets as per
the diversity criteria until a diverse set of size K is found.
Such a framework ensures that all possible cases are considered and hence guarantees completeness. Figure 2 shows the
framework with the control flow. In this paper, we emphasize
mainly on how to explore all the solutions and all the solution combinations efficiently so as to guarantee completeness while not impeding the search progress due to their individual exhaustive nature. The feedback component shown
in the figure is particular to the domain elements and their
distance measures which we do not explore in this work
leaving it as an option for the user to plug-in to the proposed
framework and methods.

Algorithms for Finding All Cost Sensitive Solutions
For the Level 1, the problem is to find the set of all candidate plans that can potentially be part of a diverse solution
set. In our case, the set of all candidate plans correspond to
the set of all valid loopless plans whose cost is ‚â§ max cost.
We present two complete methods which guarantee generating the set of all candidate plans, one based on DepthFirst Branch and Bound (DFBB) (Lawler and Wood 1966;
Russell and Norvig 1995) and another based on a recent algorithm for finding M best solutions in graphical models,
called m-A* (Dechter, Flerova, and Marinescu 2012). One
may also use other complete methods and extend them to
generate all candidate solutions whose cost is ‚â§ max cost.
First, we present the DFBB based algorithm for finding
all candidate plans, called DFA. It works similar to regular
DFBB search on graph spaces except for the following two
differences: (i) it does not stop after finding a single solution within the max cost bound, and (ii) it does not conduct
full duplicate detection (any state reached through a differ-

ent path from the start state leads to a new node unless there
is a loop, at which point it simply backtracks to find other
solutions). It is easy to prove that:
Lemma 1 DFA generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.
Now, we describe the second algorithm for finding the set
of all candidate plans, called A*A. This is based on the mA* algorithm (Dechter, Flerova, and Marinescu 2012) which
guarantees finding m best solutions/plans by expanding the
minimum set of nodes. First, we describe the basic idea behind m-A* and then we extend it to find the set of all candidate plans for our problem. The basic idea behind m-A* is to
proceed in a manner similar to A* and whenever a duplicate
state is found, the most promising m nodes corresponding to
that state are to be considered for expansion and the rest be
discarded. For our problem, where we want to find the set
of all cost-bounded plans, we will have to keep all the nodes
corresponding to same state for expansion without the mlimit. This eliminates the requirement of full-scale duplicate
detection all-together, instead suggests treatment of all children being generated as new. However, since we are only
interested in loopless plans, we will discard all nodes which
cause loops in the partial plan at any stage, through cycle
checking. We call this adapted strategy- A*A (A* based approach for finding All cost-bounded solutions). Once again,
it can be proven that:
Lemma 2 A*A generates all valid loopless plans whose
cost is ‚â§ max cost, given that the edges of the search graph
have positive costs and the heuristic used is admissible.

Complete Algorithms for Finding a Diverse
Solution Set
Now, we present the algorithms for Level 2 of our framework, where the stream of all candidate plans, the distance
measure, the size of the diverse plan set needed and the minimum distance between any two plans of the solution set is
given as input, to produce a set of satisficing diverse plans.
Algorithm 1 presents a simple strategy called ACER
which explores all combinations of plans in each run for the
in-coming new plan in conjunction with all of the existing
valid plan sets. It is easy to prove that:
Lemma 3 ACER finds a diverse plan set of size K from the
set/stream of all candidate plans whenever there exists one.
However, G can grow rapidly and become an exponential sized set in terms of K with base being the number of
candidate plans (which itself can be of exponential size in
terms of the planning problem input) before finding a diverse plan set. This severely limits its scalability when there
are large number of candidate plans (even for moderate values of K), which is often the case in practice. Next, we will
present a method which does not explore all plan set combinations in one shot instead only a select most promising sets
at each stage, while keeping track of unexplored combinations that may be explored at the end (after processing the
entire stream of candidate plans once) for completeness.

Algorithm 1 Explore All Possible Combinations of Solutions in Each Run (ACER)

Algorithm 2 Explore Most-promising Combinations of Solutions in Each Run (MCER)

1: INPUT :: A candidate plan p (from the stream of all candidate plans), a distance measure dist(), the minimum distance
needed between any two plans of a set min dist, and the size
of the diverse plan set K.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets.
4: for each plan set P ‚àà G do
5:
if dist(p, l) > min dist ‚àÄl ‚àà P then
6:
P 0 ‚Üê P + {p};
7:
if |P 0 | = K then
8:
return P 0 ;
9:
end if
10:
G ‚Üê G‚à™P 0 ;
11:
end if
12: end for
13: G ‚Üê G‚à™{p};
14: return largest P ‚àà G;

1: INPUT :: A candidate plan p (from the stream of all candidate
plans), its sequence number in the stream i, a distance measure
dist(), the minimum distance needed between any two plans
of a set min dist, the size of the diverse plan set K, and the
number of seed plan sets to be explored n.
2: OUTPUT :: A diverse plan set of size K (if exists), otherwise
the largest diverse plan set.
3: G ‚Üê œÜ (empty set); (initially) // Global data; the set of all valid
plan sets with satellite data.
4: Open ‚Üê œÜ; Children ‚Üê œÜ;
5: ExpandMostPromising(G, n, Children);
6: if a diverse set P of size K is found then
7:
return P ;
8: end if
9: while Children 6= œÜ do
10:
Swap Children and Open;
11:
ExpandMostPromising(Open, n, Children);
12:
if a diverse set P of size K is found then
13:
return P ;
14:
end if
15:
Move all plan sets in Open to G;
16: end while
17: P ‚Üê {p}; Pexp ‚Üê i;
18: G ‚Üê G‚à™P ;
19: return largest P ‚àà G;

The second technique, which is an adaptation of the
Anytime Pack Search method (Vadlamudi, Aine, and
Chakrabarti 2015; 2013), focuses on exploring a limited set
of seed nodes in each iteration in a beam search like manner.
It processes the stream of candidate plans much faster than
the previous approach by focusing only on a select number
of most promising plan sets to begin with. The combinations
which this technique ignores while processing the candidate
plan stream are kept track of separately for processing at the
end, which helps in guaranteeing the completeness. Algorithm 2 presents the proposed method MCER for faster processing of the stream of candidate plans. It takes as input,
a plan from the stream of candidate plans and its sequence
number for reasons that will become clear shortly, and the
inputs for determining a diverse set similar to the previous
approach, and the number of seed plan sets to be explored
upon finding a new candidate plan.The plan-combinations
space can be visualized as a set enumeration tree (Rymon
1992), where new branches come at all levels (> 0) dynamically as the stream of candidate plans is processed.
MCER maintains a global set of valid plan sets which
have been produced until now, G, that could be further expanded with new candidate plans. It expands n most promising nodes from this set, which are populated into Children.
If a diverse set of size K is found, it terminates returning the
set. Otherwise, n most promising plan sets from Children
are expanded, and then their n most promising children and
so on until there are no further children to be expanded. It
should also be mentioned at this point, as to what we mean
by ‚Äòmost promising‚Äô, which would be based on f -value of a
plan set (the largest being most promising), and f -value is
in-turn computed as g + h where g is the size of the plan
set and h is the heuristic estimate denoting potential number of plans that can be added to this plan set. In this paper, we have explored using three heuristics: (i) 0, (ii) dispersion of the current set (arithmetic mean of all pair-wise
distances (Myers and Lee 1999)) divided by min dist, and

(iii) quadratic mean of all distances divided by min dist.
The idea behind these heuristics is that the more dispersion
the sets have the more accommodative they could be of new
candidate plans. However, in our experiments, we did not
observe gains of using the dispersion based heuristics in our
experiments compared to the trivial heuristic possibly due
to the limitation of the said heuristics in accounting for the
actions that are not part of the plans found yet, at any given
moment during the runtime. More distance metric based and
domain based estimates can be explored here in future.
Algorithm 3 presents the pseudo-code of ExpandMostPromising routine. It expands the n most promising nodes
from the given list (either G or Open) and puts them in
Children. One significant difference to note here is that,
since all the candidate plans are not available apriori, one
must add the expanded nodes back to G for future consideration with newer candidate plans. While doing so, in order
to avoid repetition, we keep track of the last child generation attempt through the sequence number of candidate plan
considered.
Finally, after the entire stream of candidate solutions has
been processed one by one using MCER, if a diverse set of
size K is not found, we continue to call MCER repeatedly
(this time, without adding back the explored plan sets into G)
until it finds a K-set or terminates exhausting the exploration
of all possible combinations.
Below, we present some of the properties of the proposed
method MCER:
Lemma 4 MCER does not generate the same combination
of plans more than once.
Proof outline:

This is ensured by keeping track of the

Algorithm 3 ExpandMostPromising
1: INPUT :: A set of valid plan sets S to expand, Children,
a distance measure dist(), the minimum distance needed between any two plans of a set min dist, the size of the diverse
plan set K, and the number of plan sets to be explored n.
2: OUTPUT :: Populates Children with new valid plan sets,
returns a diverse plan set of size K if found.
3: T emp ‚Üê œÜ (empty set);
4: for n times do
5:
P ‚Üê most promising plan set from S;
6:
for each candidate plan in the stream from sequence number
i = Pexp + 1 to the latest do
7:
if dist(p, l) > min dist ‚àÄl ‚àà P then
0
8:
P 0 ‚Üê P + {p}; Pexp
‚Üê i;
9:
if |P 0 | = K then
10:
return P 0 ;
11:
end if
12:
Children ‚Üê Children‚à™P 0 ;
13:
end if
14:
Pexp ‚Üê i;
15:
end for
16:
T emp ‚Üê T emp‚à™P ;
17: end for
18: G ‚Üê G‚à™T emp;

sequence number of candidate plan from the last child
generation attempt while expanding a plan set P via Pexp ,
which increases by 1 at each step during expansion (see
Line 14 in Algorithm 3) and the child generation attempts
start from Pexp + 1 every time (see Line 6 in Algorithm 3),
thereby avoiding repetition.
2
Lemma 5 MCER expands at-most n √ó (K ‚àí 1) + 1 number
of plan sets in each execution.
Proof outline: Note that, after expansion of n most promising nodes from G, their n most promising children, and then
their n most promising children and so on are expanded,
until a child of size K is found. Further, size of the children
at each step increases by 1 since a new candidate plan gets
added to the plan set. Therefore, even if we assume that the
initial set of seed nodes are all of size 1, MCER executes
at-most K steps at which point a diverse set of size K will
be found if possible through that set. And at each step,
at-most n number of children are expanded, with only 1 at
level K. Hence, together, at-most n √ó (K ‚àí 1) + 1 number
of plan sets are expanded in each execution of MCER. 2
Lemma 6 MCER guarantees finding a diverse set of plans
of size K if there exists one.
Proof outline: Note that, while we execute MCER several
times with incoming plans from the stream of candidate
plans, each time without exhausting all possible combinations, we keep track of the last expansion attempt for
each node (plan set), and store them in G. Hence all plan
sets which may not have been exhaustively explored with
the candidate plans are present in G when the entire set
of candidate plans has been generated. These plan sets are
then exhaustively explored without re-inserting back in to G

thereby guaranteeing completeness and termination.

2

Now, given a planning problem, a cost metric, a distance
measure, max cost, min dist, and K, for finding a set of
K cost sensitive diverse plans, one could use any one of the
following four combinations: 1) DFA with ACER, wherein
the DFA is executed and whenever a valid plan with cost
< max cost is found, ACER is invoked to find a diverse
set, and then the execution of DFA is continued if a diverse
set with the given requirements is not found, and the process is repeated until termination. We call this combination
DFAA. 2) DFA with MCER, similar to the above strategy of
invoking MCER whenever DFA find a valid cost sensitive
plan, followed by repeated calls to MCER at the end to explore all the remaining plan combinations until termination.
This is denoted by DFAM. 3) A*A with ACER (denoted
by A*AA), and 4) A*A with MCER (denoted by A*AM).
Next section presents the comparison of performances of the
above combinations of methods.

Experimental Results
In this section, we present the experimental results comparing the performances of various proposed algorithms among
themselves as well as with a greedy approach proposed in
the literature. We have implemented all our methods on
top of the Fast Downward planning environment (Helmert
2006), and hence could run problem instances from any of
the supported planning domains. Accordingly, we have conducted experiments on several domains, including, blocks,
rovers, pathways-noneg, airport, driverlog, tpp, zenotravel.
We present the representative results in this paper. All the experiments have been performed on a machine with Intel(R)
Xeon(R) CPU E5-1620 v2 at 3.70GHz and 64GB RAM. The
following distance measure for measuring diversity has been
adopted from (Nguyen et al. 2012):
dist(p1 , p2 ) = 1 ‚àí

A(p1 )‚à©A(p2 )
A(p1 )‚à™A(p2 )

(2)

where A(p) denotes the set of all actions in plan p.
Table 1 shows the comparison of DFAA and A*AA methods on problems (denoted by P.no.) from Blocks domain.
We have used the LM cut heuristic which is admissible, to
guide the search. The algorithms are given a maximum time
of 60sec for solving each problem. Given a set of inputs,
the output shows whether a diverse set of plans of size K is
found (otherwise the size of the largest diverse set found in
parenthesis), the time taken, and the number of plans generated during the process. * denotes that the algorithm stopped
due to the time limit. We see that the DFA based method
generates the cost sensitive plans faster than the A*A based
method in this case, resulting in the processing of more number of plans in a given time.
The difference in the number of plans generated to find
a diverse set of same size highlights the importance of the
order in which the candidate plans are generated. Depending on the order of the plans generated, the number of plans
required to be processed by an exhaustive algorithm to produce a diverse set of specific size varies. As mentioned before, this could be influenced by devising an appropriate distance metric and domain dependent feedback mechanism.

Table 1: Comparison of DFAA and A*AA complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

K

max cost

4
8

20

8

30

8

30

DFAA
min dist
0.6
0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (4)
No* (3)

A*AA

Time (Sec.)

Plans gentd.

0.00
0.00
32.90
2.14
60.00
60.00

22
43
231
323
2567
5140

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
11.06
3.42
60.00
60.00

26
43
130
323
923
4115

Table 2: Comparison of DFAM and A*AM complete algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAM

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
Yes
No (4)
Yes
No (5)
No* (6)
No* (2)

A*AM

Time (Sec.)

Plans gentd.

0.00
0.00
0.58
1.74
60.00
60.00

26
43
323
323
11680
11908

K-set
found?
Yes
No (4)
Yes
No (5)
No* (7)
No* (3)

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
2.62
60.00
60.00

26
43
172
323
12226
12311

Table 3: Comparison of DFA based and A*A based greedy algorithms. Domain: Blocks. max time = 60sec.
Input
P. no.
4-0
5-0

DFAG

K

max cost

min dist

4
8

20

0.6

8

30

8

30

0.5
0.6
0.5
0.6

K-set
found?
No (3)
No (3)
No (6)
No (4)
No (6)
No (2)

A*AG

Time (Sec.)

Plans gentd.

0.00
0.00
0.04
0.02
16.70
24.24

43
43
323
323
104712
104712

Table 2 presents the comparison of DFAM and A*AM
methods (with plan-combinations seed set size equal to 30
in each execution). Note that, both algorithms guarantee to
find a diverse set of required size if there exists one, given
they are given enough time to terminate. Our objective with
the MCER based methods is to quickly process the incoming
candidate plans so as to find a diverse set quicker, postponing the exhaustive exploration to the end. Accordingly, we
see that both the methods perform much better than they did
compared to Table 1 by processing larger number of plans.
They are able to find larger sized sets of diverse plans in the
given time than before, although, as one can observe it may
also happen that the select exploration could occasionally
(see DFAM vs DFAA in last rows of both the tables) delay
finding large sets compared to ACER based approaches.
Next, we present the results obtained by integrating the
greedy approach discussed in the Introduction with DFA and
A*A, in Table 3. We call these methods DFAG and A*AG
respectively. As one can observe, while the greedy methods
process the incoming candidate plans very fast, they terminate without finding a diverse set of given size even when
there exists one. Furthermore, even for finding the diverse
sets that they produced, they involve generating far more
number of plans compared to the complete algorithms. This
can be a crucial element when finding multiple plans is difficult for a domain. Also, note that, A*A runs out of memory (4GB per instance) in this case which makes the case

K-set
found?
No (3)
No (3)
No (7)
No (3)
No* (6)
No* (2)

Time (Sec.)

Plans gentd.

0.00
0.00
0.06
0.04
Mem-limit
Mem-limit

43
43
323
323
101908
101908

for memory bounded methods while attempting to generate multiple solutions. Although, one can improve the performance of the greedy approach through feedback mechanisms, considering only one seed plan set for exploration is
likely to continue to affect the performance. Thus, it would
be beneficial to have multiple seed plan sets to be explored
at each stage for better performance.
Now, we present the results obtained on two other domains, namely, Rovers and Zeno-Travel. We show the results with DFA as the base method (for generating all costbounded solutions) in these cases since A*A based methods
were quickly reaching the memory limit on these instances.
Table 4 shows the comparison of DFAA, DFAM and DFAG
methods on a problem from the Rovers domain with 14 objects. Here, a diverse set of size 8 with cost bound 20 is to
be found within 60 seconds. Three sets of results comparing the above three algorithms are presented with different
diversity criterion in each case. Note that, amongst the three
methods, DFAA spends the most amount of effort on exploring plan combinations (exhaustive) whenever a new plan is
found, therefore is only able to generate and process a small
number of plans. Since DFAG spends least amount of effort on exploring plan combinations (greedy) upon finding
a new plan, it is able to generate and scan through a large
number of plans. Whereas DFAM distributes its effort intelligently across plan generation and plan combination exploration, by adjusting the number of seeds n as per domain and

Table 4: Comparison of DFAA, DFAM and DFAG methods. Domain: Rovers. Problem: roverprob4213 (14 objects), K: 8,
max cost : 20, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
Yes
No* (6)
No* (4)

DFAM

Time (Sec.)

Plans gentd.

0.88
60.00
60.00

64
372
1048

K-set
found?
Yes
Yes
No* (6)

DFAG

Time (Sec.)

Plans gentd.

0.00
9.84
60.00

65
306061
2047871

K-set
found?
Yes
Yes
No* (4)

Time (Sec.)

Plans gentd.

0.00
1.22
60.00

64
304553
15988265

Table 5: Comparison of DFAA, DFAM and DFAG methods. Domain: Zeno-Travel. Problem: ZTRAVEL-2-5 (17 objects), K: 8,
max cost : 15, max time = 60sec.
Input
min dist
0.4
0.5
0.6

DFAA
K-set
found?
No* (6)
No* (4)
No* (3)

DFAM

Time (Sec.)

Plans gentd.

60.00
60.00
60.00

294
579
1171

K-set
found?
Yes
Yes
Yes

problem size (in this case, n = 30). Note that, MCER with
n = 1 would result in an exploration similar to that of the
greedy method, with the exception of going further and guaranteeing completeness. And, MCER with n = ‚àû would result in an exploration similar to that of ACER. Results show
that DFAA performs poorly on large instances due to its exhaustive exploration. Between DFAM and DFAG, on easier
problems (with low diversity/distance requirement; first 2 instances), greedy and MCER based methods fare similarly,
with the greedy method slightly outperforming the MCER
based method (note that, one can change the n value to 1
here, to make MCER deliver results similar to that of the
greedy method, however, this is not beneficial in general).
On the other hand, when the diversity required is higher
(third instance), MCER based method outperforms greedy
approach by finding a larger diverse set using only 12.81%
of the plans generated by that of the greedy method, showcasing the advantage of exploring plan combinations more
thoroughly.
Table 5 presents the comparison of DFAA, DFAM and
DFAG methods on a problem from the Zeno-Travel domain
with 17 objects. Once again, we observe similar results as
that of the previous two domains. ACER based method fares
poorly due to its exhaustive nature which limits its reach
in scanning through the full space plans in the given time.
And, between DFAM and DFAG, while DFAG may find
the K-set in shorter time in some cases, DFAM continues
to leverage the advantage of exploring plan combinations
thoroughly and is able to find required K-sets using lesser
number of plans. This is a crucial element in working with
domains where producing individual plans itself is very difficult, which is especially the case when large problem sizes
are involved.
Before we conclude, we present a note on solving large
sized problems. In such cases, while completeness may not
be a practical expectation, one should be able to gain performance over using the greedy approach by carefully integrating the proposed MCER approach with the state-of-the-art
solvers/planners, feedback mechanisms, and using efficient
heuristics for exploring the space of plan combinations. Fur-

DFAG

Time (Sec.)

Plans gentd.

0.18
0.62
21.46

1447
7884
505186

K-set
found?
Yes
Yes
Yes

Time (Sec.)

Plans gentd.

0.20
0.44
10.30

1447
8545
619579

thermore, the proposed methods can be easily extended to
solve related problems such as, finding a K-set with maximum diversity, finding largest K-set with a given diversity,
finding high quality (in terms of the cost of the plans) K-sets
with given diversity, and a combination thereof involving the
generation of multi-objective pareto fronts.

Conclusion
In this paper, we take a combinatorial search perspective of
the widely studied diverse solution generation problem. We
observe that many of the approaches proposed in various
domains such as automated planning and constraint satisfaction use a linear-time greedy method for exploring plan
set combinations, that makes them fail while searching for
a diverse set of required size even when there exists one.
We propose a bi-level optimization framework and methods
to find cost-sensitive diverse solutions which guarantee to
find a diverse set of required size whenever there exists one.
We identify the critical elements that affect the performance
in such scenarios and propose efficient methods to handle
them. We showcased the efficacy of the proposed methods
by implementing our methods as part of the Fast Downward
planning system and comparing with the existing greedy approach across various domains. The proposed methods have
found larger sets of diverse solutions compared to the greedy
approach on almost all problem instances, within the same
time bound, proving their utility.

Acknowledgments
This research is supported in part by the ONR grants
N00014-13-1-0176, N00014-13-1-0519 and N00014-15-12027, and the ARO grant W911NF-13- 1-0023.

References
Alexiadis, A., and Refanidis, I. 2013. Generating alternative plans
for scheduling personal activities. In Proceedings of the Seventh
Scheduling and Planning Applications workshop, 35‚Äì40.
Bloem, M., and Bambos, N. 2014. Air traffic control area configuration advisories from near-optimal distinct paths. Journal of
Aerospace Information Systems 11(11):764‚Äì784.

Bloem, M. J. 2015. Optimization and Analytics for Air Traffic
Management. Ph.D. Dissertation, Stanford University.
Boddy, M. S.; Gohde, J.; Haigh, T.; and Harp, S. A. 2005. Course
of action generation for cyber security using classical planning.
In Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005), June 5-10 2005,
Monterey, California, USA, 12‚Äì21.
Bryce, D. 2014. Landmark-based plan distance measures for diverse planning. In Proceedings of the Twenty-Fourth International
Conference on Automated Planning and Scheduling, ICAPS 2014,
Portsmouth, New Hampshire, USA, June 21-26, 2014.
Bylander, T. 1991. Complexity results for planning. In Proceedings
of the 12th International Joint Conference on Artificial Intelligence
- Volume 1, IJCAI‚Äô91, 274‚Äì279. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc.
Coman, A., and MunÃÉoz-Avila, H. 2011. Generating diverse plans
using quantitative and qualitative plan distance metrics. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11,
2011.
Coman, A., and MunÃÉoz-Avila, H. 2012a. Diverse plan generation
by plan adaptation and by first-principles planning: A comparative
study. In Case-Based Reasoning Research and Development - 20th
International Conference, ICCBR 2012, Lyon, France, September
3-6, 2012. Proceedings, 32‚Äì46.
Coman, A., and MunÃÉoz-Avila, H. 2012b. Plan-based character
diversity. In Proceedings of the Eighth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-12,
Stanford, California, October 8-12, 2012.
Coman, A., and MunÃÉoz-Avila, H. 2013. Automated generation of
diverse npc-controlling fsms using nondeterministic planning techniques. In Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-13,
Boston, Massachusetts, USA, October 14-18, 2013.
Coman, A. 2012. Solution diversity in planning. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Dechter, R.; Flerova, N.; and Marinescu, R. 2012. Search algorithms for m best solutions for graphical models. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial Intelligence, July
22-26, 2012, Toronto, Ontario, Canada.
Do, M. B., and Kambhampati, S. 2001. Planning as constraint
satisfaction: Solving the planning graph by compiling it into CSP.
Artif. Intell. 132(2):151‚Äì182.
Eiter, T.; Erdem, E.; Erdogan, H.; and Fink, M. 2013. Finding
similar/diverse solutions in answer set programming. Theory and
Practice of Logic Programming 13(03):303‚Äì359.
Gerevini, A.; Saetti, A.; and Serina, I. 2003. Planning through
stochastic local search and temporal action graphs in lpg. J. Artif.
Int. Res. 20(1):239‚Äì290.
Goldman, R. P., and Kuter, U. 2015. Measuring plan diversity:
Pathologies in existing approaches and A new plan distance metric. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., 3275‚Äì
3282.
Hebrard, E.; Hnich, B.; O‚ÄôSullivan, B.; and Walsh, T. 2005. Finding diverse and similar solutions in constraint programming. In
Proceedings of the 20th National Conference on Artificial Intelligence - Volume 1, AAAI‚Äô05, 372‚Äì377. AAAI Press.
Hebrard, E.; O‚ÄôSullivan, B.; and Walsh, T. 2007. Distance constraints in constraint satisfaction. In Proceedings of the 20th In-

ternational Joint Conference on Artifical Intelligence, IJCAI‚Äô07,
106‚Äì111. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc.
Helmert, M. 2006. The fast downward planning system. J. Artif.
Intell. Res. (JAIR) 26:191‚Äì246.
Kambhampati, S. 2007. Model-lite planning for the web age
masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, July 22-26, 2007, Vancouver,
British Columbia, Canada, 1601‚Äì1605.
Lawler, E. L., and Wood, D. E. 1966. Branch-and-bound methods:
A survey. Operations Research 14(4):699‚Äì719.
Myers, K. L., and Lee, T. J. 1999. Generating qualitatively different
plans through metatheoretic biases. In AAAI, 570‚Äì576. American
Association for Artificial Intelligence.
Nadel, A. 2011. Generating diverse solutions in sat. In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing, SAT‚Äô11, 287‚Äì301. Berlin, Heidelberg:
Springer-Verlag.
Nguyen, T. A.; Do, M. B.; Gerevini, A.; Serina, I.; Srivastava, B.;
and Kambhampati, S. 2012. Generating diverse plans to handle
unknown and partially known user preferences. Artif. Intell. 190:1‚Äì
31.
Petit, T., and Trapp, A. C. 2015. Finding diverse solutions of
high quality to constraint optimization problems. In IJCAI. International Joint Conference on Artificial Intelligence.
Roberts, M.; Howe, A.; Ray, I.; and Urbanska, M. 2012. Using
planning for a personalized security agent. In AAAI Workshops.
Roberts, M.; Howe, A. E.; and Ray, I. 2014. Evaluating diversity
in classical planning. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Russell, S., and Norvig, P. 1995. Artificial intelligence: a modern
approach. Prentice Hall.
Rymon, R. 1992. Search through systematic set enumeration. Technical Reports (CIS) 297.
Scala, E. 2014. Plan repair for resource constrained tasks via numeric macro actions. In Proceedings of the Twenty-Fourth International Conference on Automated Planning and Scheduling, ICAPS
2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Srivastava, B.; Nguyen, T. A.; Gerevini, A.; Kambhampati, S.; Do,
M. B.; and Serina, I. 2007. Domain independent approaches for
finding diverse plans. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad,
India, January 6-12, 2007, 2016‚Äì2022.
Vadlamudi, S. G.; Aine, S.; and Chakrabarti, P. P. 2013. Anytime
pack heuristic search. In Pattern Recognition and Machine Intelligence - 5th International Conference, PReMI 2013, Kolkata, India,
December 10-14, 2013. Proceedings, 628‚Äì634.
Vadlamudi, S.; Aine, S.; and Chakrabarti, P. 2015. Anytime pack
search. Natural Computing 1‚Äì20.
Vadlamudi, S. G.; Chakrabarti, P. P.; and Sarkar, S. 2012. Anytime
algorithms for mining groups with maximum coverage. In Tenth
Australasian Data Mining Conference, AusDM 2012, Sydney, Australia, December 5-7, 2012, 209‚Äì220.
Voss, C.; Moll, M.; and Kavraki, L. E. 2015. A heuristic approach
to finding diverse short paths. In IEEE International Conference
on Robotics and Automation, ICRA 2015, Seattle, WA, USA, 26-30
May, 2015, 4173‚Äì4179.

Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote Human-Robot Interaction
Vignesh Narayanan
Dept. of Computer Science Arizona State University Tempe, AZ

Yu Zhang
Dept. of Computer Science Arizona State University Tempe, AZ

Nathaniel Mendoza
Dept. of Computer Science Arizona State University Tempe, AZ

vnaray15@asu.edu

yzhan442@asu.edu Subbarao Kambhampati
Dept. of Computer Science Arizona State University Tempe, AZ

namendoz@asu.edu

rao@asu.edu ABSTRACT
Human factor studies on remote human-robot interaction are often restricted to various forms of supervision, in which the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this study, we investigate the incorporation of a general planning capability into the robot to facilitate peer-to-peer human-robot teaming, in which the human and robot are viewed as teammates that are physically separated. One intriguing question is to what extent humans may feel uncomfortable at such robot autonomy and lose situation awareness, which can potentially reduce teaming performance. Our results suggest that peer-to-peer teaming is preferred by humans and leads to better performance. Furthermore, our results show that peer-to-peer teaming reduces cognitive loads from objective measures (even though subjects did not report this in their subjective evaluations), and it does not reduce situation awareness for short-term tasks. (or sub-goals) for the robot to handle. In peer-to-peer (P2P) teaming, the human and robot share the same global goal and collaborate to achieve it. While increasing robot autonomy is generally viewed as desirable, humans may feel uncomfortable at the loss of control entailed by such autonomy in P2P teaming, which can potentially reduce situation awareness for humans and affect teaming performance. In this study,1 we mainly concentrate on remote interaction and perform our investigation for emergency response in an urban search and rescue (USAR) task. Here, the task cannot be fully specified a priori due to incomplete information about the models, goals or settings in such scenarios. Furthermore, information can be continuously changing throughout the task and may not always be synchronized between the human and robot (e.g., goal updates and human preference models) due to communication or interpretation delays. The goal of this USAR task is to explore areas of the disaster scene to provide real-time information, which is then used to aid the management team to create rescue plans (e.g., identifying locations of casualties). The aim of our study is to compare P2P and supervised teaming in terms of objective measures as well as subjective measures such as situation awareness and mental workload. Related Work: In most previous works on human-robot and human-machine interactions (e.g., [1]) for teaming, the human always plays a supervisor role. While there are works that incorporate general planning capabilities into robots to achieve P2P teaming (e.g., [3]), as yet, there exists no empirical investigation of its influence on the teaming performance. In this study, we implement P2P teaming in which the human and robot are viewed as teammates, and the robot exhibits autonomy through automated planning capabilities. In terms of automation in human-robot interaction, it is well known that it can have both positive and negative effects on human performance [2].

Categories and Subject Descriptors
H.1.2 [Human factors]; I.2.8 [Plan execution, formation, and generation]; J.7 [Command and control]

Keywords
Robot design principles; Autonomous robot capabilities; User study/Evaluation; Teamwork & group dynamics

1.

INTRODUCTION

In supervised human-robot teaming, the human creates the plan to achieve the global goal, and then either directly provides motion commands or breaks the plan into sub-plans

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). HRI'15 Extended Abstracts, March 2≠5, 2015, Portland, OR, USA. ACM 978-1-4503-3318-4/15/03. http://dx.doi.org/10.1145/2701973.2702042.

2.

STUDY DESIGN

Fig. 1 presents the simulated environment and humanrobot interface used in our USAR task, which represents the floor plan of an office building before a disaster occurs (e.g.,
1

A longer version at rakaposhi.eas.asu.edu/hri15-long.html.

Figure 1: Environment (left) and interface (right) used in the USAR task with a simulated Nao robot.

Figure 3: Results for subjective measures.  denotes p < 0.05,  denotes p < 0.01,    denotes p < 0.001. within 20 minutes and the secondary task performance (left part of Fig. 2). In fact, performance for the primary task in supervised teaming was no better than the robot with a planning capability executing the task alone (i.e., P2P-NI). Results for subjective measures as assessed by the survey questionnaire are presented in Fig. 3. Our analysis on mental workload did not show any significant difference due to the fact that most humans preferred to rely on themselves at the beginning even in P2P teaming, which might be a result of the lack of trust in the robot for handling the task initially. This effect can be seen from the right part of Fig. 2, which shows how many robot recommended actions were followed by the human subjects in P2P teaming. Even though the workload was seen to be almost the same based on subjective measures (Fig. 3), the objective measures suggest that the cognitive load was indeed reduced (based on the time spent on the secondary task (left part of Fig. 2)). Our analysis on situational awareness did not show any significant difference either. This might be partially due to the fact that the recommended action of the robot from the planning capability during execution provided situation awareness to the human subject, since the same action would likely be chosen by the subject in the same situation. This is interesting since it suggests that humans in P2P teaming can maintain situation awareness, at least for short-term tasks such as our USAR task. Our analysis on likability, improvability, and complacency showed that the subjects generally preferred and felt more satisfied working with the robot in P2P teaming. Our conclusions from this preliminary study are that humans prefer working with robots with a planning capability for P2P teaming, and the planning capability helps reduce cognitive load and maintain situation awareness for short-term tasks. Acknowledgments: This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR grants N00014-13-1-0176 and N00014-13-1-0519.

Figure 2: Results for objective measures.

a fire). The study was performed over 4 weeks and involved 19 volunteers. Each subject took part in one experimental trial of either P2P or supervised teaming. Both the human subject and robot had access to the floor plan before the disaster. The robot in P2P teaming could use the planning capability (based on SapaReplan [3]) to independently create its own plan based on the floor plan and current status of the task. In both teaming scenarios, the robot displayed a list of applicable actions that it could perform given the current state. The interaction interfaces were the same except that the robot in P2P teaming also recommended the next action (from along the applicable ones) in its plan. The global goal was to report the number of casualties in as many rooms as possible in 20 minutes. The incomplete task information was assumed to be a result of blocked doors. We provided the information regarding which doors might be blocked to the human subject, but only after the task had run for 1 minute to simulate dynamic information. This information, however, remained unknown to the robot. The human subject could interact with the robot at specific times to reduce the influence of this information asymmetry, or the robot had to learn this information by pushing the door and failing (which could reduce the teaming performance). In a real USAR task, the human would also have other information to process and analyze. To simulate this, the human subject was also assigned to a secondary task. This secondary task involved solving three-dimensional spatial visualization puzzles. The performance of the team was evaluated on both the primary and secondary tasks. Each trial ended when the given time elapsed. Finally, the human subject completed a questionnaire (in Likert scale) that included questions for evaluating situation awareness, mental workload and other subjective human-robot interaction aspects.

4.

REFERENCES

3.

RESULTS AND CONCLUSIONS

The results for objective measures are presented in Fig. 2. Overall, subjects in P2P teaming outperformed subjects in supervised teaming in terms of the number of rooms visited

[1] J. Casper and R. Murphy. Human-robot interactions during the robot-assisted urban search and rescue response at the world trade center. IEEE Trans. on SMC Part B, 33(3):367≠385, June 2003. [2] R. Parasuraman. Designing automation for human use: empirical studies and quantitative models. Ergonomics, 43(7):931≠951, 2000. PMID: 10929828. [3] K. Talamadupula, J. Benton, S. Kambhampati, P. Schermerhorn, and M. Scheutz. Planning for human-robot teaming in open worlds. ACM Trans. Intell. Syst. Technol., 1(2):14:1≠14:24, Dec. 2010.

Proactive Decision Support using Automated Planning
Satya Gautam Vadlamudi, Tathagata Chakraborti, Yu Zhang, Subbarao Kambhampati
{gautam,tchakra2,Yu.Zhang.442,rao}@asu.edu, Arizona State University, Tempe, AZ
Proactive decision support (PDS) helps in improving the decision making experience of human decision
makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of
making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the
research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her
goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource
constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that
must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach
also supports naturalistic decision making which is being acknowledged as a necessary element in
proactive decision support, since it only aids the human decision maker through suggestions and alerts
rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework
through search-and-rescue examples in a fire-fighting domain.
Human-in-the-loop planning (HILP) is a necessary
requirement today in many complex decision making or
planning environments. In this paper we consider the case of
HILP where the human(s) responsible for making the
decisions in complex scenarios are supported by automated
planning systems. Thus the planners in this scenario are the
humans themselves, and we investigate the role of an
automated planner in their deliberative process. This is, in
effect, a role reversal of the traditional notion of the humanplanner interaction in mixed initiative planning; and we refer
to the proposed system as a reverse mixed initiative planner.
These systems are capable of providing plans or course-ofactions (COAs) when a model of the world where the plans
are to be executed is given to them, along with the knowledge
of the initial state and the list of goals to be achieved/tasks to
be accomplished. Examples where such technologies can be
helpful include disaster response strategies from the navy, or
responses to fire or emergency from local law enforcement.
Providing a complete model of the world where the plans
are to be executed is, however, known to be very difficult
(Kambhampati, 2007). This implies that the system generated
plan cannot be completely relied upon. Not only executing
such plans may no longer accomplish the goals/tasks provided,
but also their execution may result in undesired consequences.
This calls for active participation from the human in the loop
rather than simply adopt a system generated plan.
Furthermore, in many cases, the human in the loop may be
held responsible for the plan under execution and its results.
Therefore, it is also necessary in such cases that the human
keeps control of the plan/COA being given for execution. This
motivates us to build a proactive decision support system,
which is context-sensitive and focuses on aiding and alerting
the human in the loop with his/her decisions rather than
generate a static COA that may not work in the dynamic
worlds that the plan has to execute in.
In this paper, we propose a proactive decision support
(PDS) system, named RADAR, using automated planning
technology, which is augmentable, context sensitive,
controllable and adaptive to the human‚Äôs decisions. It supports
the human in the loop through suggestions and alerts, which
can be considered by the human as he/she sees fit.
In the following we explain the meaning of the above terms:

‚Ä¢

‚Ä¢

‚Ä¢

‚Ä¢

Augmentable: The model of the world such as the
rules that specify what are the preconditions for a
particular action/decision and what would be the
effects
of
that
action/decision
could
be
added/modified. The state of the world such as the
values of various variables and availabilities of
various resources could be updated. The list of
goals/tasks can also be updated.
Context-sensitive: Whenever the model or the state
of the world is augmented either by the human or by
some other source internal/external to the PDS
system, the system takes the new context into account
and responds with updated suggestions and alerts.
Controllable: The decision making process of the
PDS system is completely controllable by the human
in the loop, who retains with the decision making
power wherein he/she can either choose to follow the
system generated suggestions or make a different
decision as they see fit.
Adaptive: Since the decision making power lies with
the human in the loop, the PDS system has to adapt
itself to the decisions being made by the human and
provide new suggestions and alerts that are relevant
based on the decisions of the humans. Note that,
adaptive nature may also be viewed as part of
context-sensitivity in the sense that the context
changes whenever decisions are made. In this paper,
we keep the distinction to differentiate the changes in
the world and tasks, and the changes related to the
actions prescribed/decisions of the human in the loop.

As mentioned before, our proactive decision support system
uses automated planning technology widely studied in the
field of Artificial Intelligence. In particular, we adopted the
Planning Domain Description Language (PDDL) to describe
the model of the world, details of the current state (context)
and the goals, to the system. Then, we used the existing
landmark generation method (Hoffmann et al., 2004) to
generate landmarks, which are then analyzed to come up with
relevant suggestions and alerts. Landmarks are those set of
states (of the world) that has to be visited by any successful
plan that achieves goals from the current state.

We implemented our proposed system using the Fast
Downward planner (Helmert, 2006) and tested it on a firefighting domain, where search-and-rescue missions are to be
carried out (goals). We also conducted some preliminary
human factor studies to evaluate the utility of the above
proposal, which gave positive feedback.
RELATED WORK
The proposed proactive decision support system supports
naturalistic decision making (Zsambok and Klein, 2014;
Klein, 2008), which is acknowledged as a necessary element
in PDS systems (Morrison et al., 2013). Systems which do not
support naturalistic decision making have been found to have
detrimental impact on work flow causing frustration to
decision makers (Feigh et al., 2007).
In (Parasuraman, 2000), a study of human performance
consequences of different levels and types of automation is
provided, where aspects such as, mental workload and
situation awareness are considered as evaluative criteria. A
model for types and levels of automation that provides an
objective basis for deciding which system functions should be
automated and to what extent is given in (Parasuraman et al.,
2000). (Parasuraman and Manzey, 2010) shows that human
use of automation may result in automation bias leading to
omission and commission errors, which underlines the
importance of reliability of the automation (Parasuraman and
Riley, 1997). Various elements of human-automation
interaction such as, adaptive nature and context sensitivity are
presented in (Sheridan and Parasuraman, 2005). (Warm et al.,
2008) show that vigilance requires hard mental work and is
stressful via converging evidence from behavioral, neural and
subjective measures. Our system could be considered as a part
of such vigilance support thereby reducing the stress for
human in the loop.
High-level information fusion that characterizes complex
situations and that support planning of effective responses is
considered the greatest need in crisis-response situations
(Laskey et al., 2016). Automated planning based proactive
support systems were shown to be preferred by humans in
studies involving human-robot teaming (Zhang et al., 2015)
and the cognitive load of the subjects involved was observed
to have been reduced (Narayanan et al., 2015).
PROPOSED PROACTIVE DECISION SUPPORT
SYSTEM: RADAR
Now, we present the proposed proactive decision support
(PDS) system ‚Äì RADAR, based on automated planning
technology. First, we present the different elements of the PDS
system and then briefly present the details of the methodology
behind the generation of suggestions and alerts. The various
elements of the planning PDS system are:
‚Ä¢ Tasks/goals: The tasks or goals to be accomplished
clearly form an important and necessary element.
‚Ä¢ State/context, resources: The current state or context
is needed for the PDS system to produce relevant
alerts. The availability of resources also forms part of
the context which we separately indicated to display
as an important constituent of the Context.

Model, actions/decisions: The model consists of set
of rules which are applicable in the world where the
plan is being executed. Actions, for example, are part
of a model, which give information about when a
particular action is applicable (what are the preconditions to be satisfied in order for it to be
applicable), and what would be the effects of taking
that action (how it would impact various elements of
the world). Actions are also closely tied to decisions
that need to be made since each decision typically
corresponds to certain action being taken.
‚Ä¢ Current plan/course of action (COA): The
information about current plan of the human in the
loop, if any, can help the PDS system produce better
suggestions and alerts by reducing the uncertainty.
However, this could just consist of actions already
taken, in which case the proposed PDS system can
come up with relevant alerts pertaining to the future.
More details on this are given next.
Now, we briefly present details on how the proposed
planning based system with above elements produces relevant
suggestions and alerts. In order to explain this, first we need to
define what are called Landmarks, which are central to the
suggestions and alerts system.
‚Ä¢

Definition: Landmarks. (Hoffmann et al., 2004) A
state/partial state is a landmark (for the current state,
tasks/goals, and model) if all plans/course-of-actions that can
accomplish the tasks from the current state must go through
that state/partial state during their execution.
Note that, all goal states are trivial landmarks since they have
to be accomplished by all successful plans. Consider there
exists only one state, A, which can take one to the goal
state(s), meaning, all plans have to visit A in order to
accomplish the goals, making it a landmark (derived; nontrivial). Further, if there exist two states A and B through
which the goal state(s) must be reached, then either A or B
must be visited before accomplishing the tasks/goals. In such a
scenario, A or B can be called as a landmark, in particular, a
disjunctive landmark. Continuing this process, one can
derive recursively the set of all landmarks starting from the
goal state(s) leading back to the current state.
Generating Suggestions and Alerts (PDS)
Now, we present the details on how generating and analyzing
the landmarks can help in producing suggestions and alerts.
Note that, since the landmarks are the states that must be
visited in order to accomplish the goals, if there is no possible
way of reaching a certain landmark generated above, then the
system can generate an alert conveying that the goal cannot be
accomplished. This could be because there is no action
available, which would help in visiting the landmark under
consideration, or the preconditions of actions that can help
reaching the landmark are not satisfied.
In some cases, the preconditions, which are not currently
satisfied for an action to be applicable, may be because of
resource constraints. In such cases, the system instead
generates a suggestion mentioning that those resources are
needed in order to accomplish the task. For example, in order

Figure 1 ‚Äì RADAR interface showing data support and decision support for the human commanders making plans.
for an action such as put-off-fire to be applicable, a
precondition on the availability of the resource: fire-engine
need to be satisfied. Failing which an alert may be generated.
More details are described through a case study next.
Further, provision to update the model, state, and resources
is provided to make the system augmentable. In order to
support context-sensitivity and adaptive nature, we re-execute
the alerts generation method whenever there is a change in the
context/tasks or new action is executed or plan is changed, so
that the suggestions/alerts become relevant to the situation.
Case Study: Fire-fighting Domain
We use a fire-fighting scenario to illustrate the ideas expressed
so far, as shown in Figure 1. The scenario plays out in a
particular location (we use Tempe in the example) and
involves the local fire chief, police, medical and transport
authorities, who try to build a plan in response to the fire in
the given platform (which is augmented with decision support
capabilities from an automated planner). The left pane gives
event updates that the commanders can incorporate into the
Tasks panel at the top, which shows what high level goals or
tasks needs to be addressed. The panel on the right (currently
empty) will display the plan being constructed. Each of the
human commanders have access to the resources that they can
use to control the fire outbreak (as can be seen from the table
at the bottom of Figure 1). For example, the police can deploy
police cars and policemen, and the fire chief can deploy fire
engines, ladders, rescuers, etc.
The plans that can be produced by the commanders, of
course, depend on the availability of these resources, and
certain actions can only be executed when the required
number of resources are available or the preconditions are
satisfied. For example, in order to dispatch police cars from a
particular police station, the police chief needs to make sure

that the respective police station has enough police cars and it
has been notified of the demand previously.
Given this knowledge, the automated planner integrated
into the system keeps an eye on the planning process of the
human commanders. The three panels in the middle of Figure
1 provide these functionalities. The one on the left provides a
way to add or request for resources in case of insufficient
resources, while the one on the right provides suggested
actions that the commanders may use to complete their plans.
The panel in the middle is the most important part of the
automated component where it produces alerts or suggestions
to problems in the current plan or with respect to problems
that may appear in future given the current state and
availability of resources.
In the following we will go through two use cases to
illustrate how the system responds to situations as per the
guidelines we discussed in the introduction ‚Äì
Scenario I: (see Figure 2)
1) The scenario starts with a small fire in a building.
Once selected from suggested tasks, this populates
the task chosen to be addressed.
2) The planning model does landmark analysis on the
current state and immediately populates the alerts
panel with an alert saying either big fire engines or
small fire engines are needed to put out the fire (a
disjunctive landmark).
3) The commander tries to dispatch big engines now,
but is stopped by the system which detects that there
are not enough resources (big engines).
4) The commander addresses the shortage by requesting
additional engines from the left.
5) Now the commander can proceed with and finish the
plan until the fire has been extinguished.

Note that the above examples are illustrative and only
intended for understanding the underlying concepts of the
proposed system. The same system can handle scenarios
where hundreds of actions and variables are involved, where it
becomes nearly impossible for a human to account for all
possible drawbacks. Furthermore, any other domain (say,
disaster response) can be readily handled by the implemented
system, by just changing the PDDL domain file used by it
without any renewed effort.

Figure 2 ‚Äì Use case illustrating automated decision support
using disjunctive landmarks.
Scenario II: (see Figure 3)
1) The scenario now starts with a big fire. This calls for
big engines as alerted by the system. Note that it also
alerts for insufficient rescuers, which did not happen
in the previous case, as the model used conveys that
rescuers are needed only in case of a big fire.
2) The commander once again requests for additional
resources (big engines as well as rescuers) and was
able to generate a feasible plan as shown.
In this way the system is able to assist the human commander
in his planning process. The system is augmentable in the way
it supports the commander‚Äôs goal preferences and world state
information. It is context-sensitive in how it provides relevant
alerts based on the stage of the plan, and adaptive with respect
to the choice taken by the human in the loop to address these
alerts. Finally, the entire process is controllable because the
human commander has authority over the choices at all times.

Figure 3 ‚Äì Use case illustrating how automated decision
support adapts in response to a necessary landmark.
EVALUATIONS WITH HUMANS ON UTILITY
In order to assess the utility of the proposed proactive decision
support (PDS) system, RADAR, we have conducted a
preliminary survey of its usefulness on 7 subjects on the
questions given below. The system would be enhanced and
refined continuously as we incorporate more and more
features in our PDS system. We also plan to take the learning
from the user feedback back into the design of the PDS system
during the process. We discuss the responses obtained for each
of the questions, next to it.
1) Do you think the suggestions & alerts are relevant to the
task/goal? Yes/No

7 out of 7 subjects have answered it Yes. This suggests that
the domain shown (fire-fighting) has been modeled well so
that relevant landmarks could be generated.
2) Do you think the suggestions & alerts are context-sensitive
(current state & current plan)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the context sensitivity of the landmarks make a good fit for
use in PDS systems.
3) Do you think that the suggestions & alerts are dynamic
(change with changing context/plan)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the regeneration of suggestions & alerts whenever context
changes is notable to the users.
4) Do you think the suggestions & alerts increased your
situational awareness (e.g. resources available, status of
execution)? Yes/No
7 out of 7 subjects have answered it Yes. This suggests that
the suggestions and alerts continuously improve the situational
awareness of the human in the loop, particularly in terms of
the critical points relevant to the plan/COA.
5) Do you think that the suggestions & alerts interfere with the
ability to interact with the system? Yes/No
1 out of 7 subjects have answered it Yes. This suggests that
the alerts are displayed without interfering with the user
interaction experience in most cases, and may be improved.
6) Do you think the suggestions & alerts helped in debugging
the plan or would you rather execute and replan in case of
failure? Former/Latter
6 out of 7 subjects have answered that the suggestions and
alerts helped. This suggests that the users prefer to be alerted
in advance rather than re-plan in most of the cases.
7) Do you think the suggestions & alerts should be an error
and stop the plan from being dispatched, or should it be a
warning and let you proceed with execution? Former/Latter
4 out of 7 subjects have answered that the alerts can be shown
as errors and stop the execution whereas others preferred them
as warnings that allow execution to move forward. In this
case, there is a split amongst the users as to whether the user
should be able to proceed with warnings or be stopped
completely. Here there is an opportunity to amend the system
so as to learn the cases where one could proceed despite the
error/warning, and incorporate it as a soft constraint in the
future leading to only a warning.
8) Do you feel that you can be in control of the decision
making/planning process when using the proposed PDS
system? Yes/No
6 out of 7 subjects have answered it Yes. This suggests that
most of the users feel in control of the decision making
process rather than being forced by the system.
9) On a scale of 1-10 how much would you rate your
satisfaction with the proactive decision support capabilities of
the system? 1-10
This received an average score of 7.14. This suggests that the
users had a good first experience with the proposed PDS
system and would like to see new and improved features.
10) On a scale of 1-10 how much would you recommend using
the proposed proactive decision support platform? 1-10
This received an average score of 7.28. This suggests that the
users are open to recommending the PDS system to others.

Overall, the PDS system was well received and perceived to
be promising. Users have appreciated the current features and
suggested minor modifications. As part of the future work, we
consider addressing several important aspects such as: Where
do we get the models? Can we automatically learn them from
observing the users and the contexts? How do we deal with
incomplete models? Does the human in the loop deviate from
cost-optimal plans? How to understand the preferences of the
human in the loop and how to address them? And so on.
CONCLUSION
We presented a Proactive decision support (PDS) framework
called RADAR, based on the research in Automated Planning
in AI, that aids the human decision maker with her plan to
achieve her goals by providing alerts and suggestions on
potential drawbacks in the plan and resource constraints. This
was achieved by generating and analyzing the landmarks that
must be accomplished by any successful plan before achieving
the goals. The proposed approach is aligned with the concept
of naturalistic decision making. We demonstrated the utility of
the proposed framework through search-and-rescue examples
in a fire-fighting domain and human factors studies.
ACKNOWLEDGMENTS
This research is supported in part by the ONR grant N0001415-1-2027. We thank Vivek Dondeti for his help with the
implementation of parts of the RADAR system.
REFERENCES
Feigh, K. M., Pritchett, A. R., Denq, T. W., & Jacko, J. A. (2007). Contextual
Control Modes During an Airline Rescheduling Task. Journal of Cognitive
Engineering and Decision Making, 1(2), 169-185.
Helmert, M. (2006). The Fast Downward Planning System. J. Artif. Intell.
Res.(JAIR), 26, 191-246.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in
planning. J. Artif. Intell. Res.(JAIR), 22, 215-278.
Kambhampati, S. (2007, July). Model-lite planning for the web age masses:
The challenges of planning with incomplete and evolving domain models.
InProceedings of the National Conference on Artificial Intelligence (Vol.
22, No. 2, p. 1601). Menlo Park, CA; Cambridge, MA; London; AAAI
Press; MIT Press; 1999.
Klein, G. (2008). Naturalistic decision making. Human Factors: The Journal
of the Human Factors and Ergonomics Society, 50(3), 456-460.
Laskey, K. B., Marques, H. C., & da Costa, P. C. (2016). High-Level Fusion
for Crisis Response Planning. In Fusion Methodologies in Crisis
Management (pp. 257-285). Springer International Publishing.
Morrison J. G., Feigh K. M., Smallman H. S., Burns C. M., Moore K. E.
(2013). The Quest For Anticipatory Decision Support Systems (Panel).
Human Factors and Ergonomics Society Annual Meeting.
Narayanan, V., Zhang, Y., Mendoza, N., & Kambhampati, S. (2015, March).
Automated Planning for Peer-to-peer Teaming and its Evaluation in Remote
Human-Robot Interaction. In HRI (Extended Abstracts) (pp. 161-162).
Parasuraman, R. (2000). Designing automation for human use: empirical
studies and quantitative models. Ergonomics, 43(7), 931-951.
Parasuraman, R., & Manzey, D. H. (2010). Complacency and bias in human
use of automation: An attentional integration. Human Factors: The Journal
of the Human Factors and Ergonomics Society, 52(3), 381-410.
Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse,
disuse, abuse. Human Factors: The Journal of the Human Factors and
Ergonomics Society, 39(2), 230-253.
Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types
and levels of human interaction with automation. Systems, Man and
Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 30(3),
286-297.
Sheridan, T. B., & Parasuraman, R. (2005). Human-automation
interaction.Reviews of human factors and ergonomics, 1(1), 89-129.

Warm, J. S., Parasuraman, R., & Matthews, G. (2008). Vigilance requires
hard mental work and is stressful. Human Factors: The Journal of the
Human Factors and Ergonomics Society, 50(3), 433-441.
Zhang, Y., Narayanan, V., Chakraborti, T., & Kambhampati, S. (2015,
September). A human factors analysis of proactive support in human-robot
teaming. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
International Conference on (pp. 3586-3593). IEEE.
Zsambok, C. E., & Klein, G. (2014). Naturalistic decision making.
Psychology Press.

1

BayesWipe: A Scalable Probabilistic Framework
for Cleaning BigData

arXiv:1506.08908v1 [cs.DB] 30 Jun 2015

Sushovan De, Yuheng Hu, Meduri Venkata Vamsikrishna, Yi Chen, and Subbarao Kambhampati
Abstract‚ÄîRecent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record
matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in
tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like CFDs
(which have to be provided by domain experts, or learned from a clean sample of the database). In this paper, we provide a method for
correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned
from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently
perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable.
We evaluate our methods over both synthetic and real data.
Index Terms‚Äîdatabases; web databases; data cleaning; query rewriting; uncertainty

F

1

I NTRODUCTION

A

LTHOUGH data cleaning has been a long standing
problem, it has become critically important again because of the increased interest in big data and web data.
Most of the focus of the work on big data has been on
the volume, velocity, or variety of the data; however, an
important part of making big data useful is to ensure the
veracity of the data. Enterprise data is known to have a
typical error rate of 1‚Äì5% [1] (error rates of up to 30% have
been observed). This has led to renewed interest in cleaning
of big data sources, where manual data cleansing tasks are
seen as prohibitively expensive and time-consuming [2],
or the data has been generated by users and cannot be
implicitly trusted [3]. Among the various types of big data,
the need to efficiently handle large scaled structured data
that is rife with inconsistency and incompleteness is also
more significant than ever. Indeed, multiple studies, such as
[4] emphasize the importance of effective, efficient methods
for handling ‚Äúdirty big data‚Äù.

TABLE 1: A snapshot of car data extracted from cars.com
using information extraction techniques
TID
t1
t2
t3
t4
t5
t6

Model
Civic
Focus
Civik
Civic
Accord

Make
Honda
Ford
Honda
Ford
Honda
Honda

Orig
JPN
USA
JPN
USA
JPN
JPN

Size
Mid-size
Compact
Mid-size
Compact
Mid-size
Full-size

Engine
V4
V4
V4
V4
V4
V6

Condition
NEW
USED
USED
USED
NEW
NEW

Most of the current techniques are based on deterministic rules, which have a number of problems: Suppose that
the user is interested in finding ‚ÄòCivic‚Äô cars from Table 1.
‚Ä¢

This work was done when all the authors were with the Department of
Computer Science & Engineering at Arizona State University, Tempe,
AZ 85287. Sushovan De is now with Google Inc. Yuheng Hu is now with
IBM Research, Almaden. Yi Chen is now with the School of Management
and the College of Computing at New Jersey Institute of Technology.

Traditional data retrieval systems would return tuples t1
and t4 for the query, because they are the only ones that
are a match for the query term. Thus, they completely miss
the fact that t4 is in fact a dirty tuple ‚Äî A Ford Focus car
mislabeled as a Civic. Additionally, tuple t3 and t5 would
not be returned as a result tuples since they have a typos
or missing values, although they represent desirable results.
The objective of this work is to provide the true result set
(t1 , t3 , t5 ) to the user.
Although this problem has received significant attention
over the years in the traditional database literature, the stateof-the-art approaches fall far short of an effective solution
for big data and web data. Traditional methods include
outlier detection [5], noise removal [6], entity resolution [6],
[7], and imputation [8]. Although these methods are efficient
in their own scenarios, their dependence on clean master
data is a significant drawback.
Specifically, state of the art approaches (e.g., [9], [10],
[11]) attempt to clean data by exploiting patterns in the data,
which they express in the form of conditional functional
dependencies (or CFDs). In the motivating example, the fact
that Honda cars have ‚ÄòJPN‚Äô as the origin of the manufacturer
would be an example of such a pattern. However, these
approaches depend on the availability of a clean data corpus
or an external reference table to learn data quality rules or
patterns before fixing the errors in the dirty data. Systems
such as ConQuer [12] depend upon a set of clean constraints
provided by the user. Such clean corpora or constraints
may be easy to establish in a tightly controlled enterprise
environment but are infeasible for web data and big data.
One may attempt to learn data quality rules directly from
the noisy data. Unfortunately however, our experimental
evaluation shows that even small amounts of noise severely
impairs the ability to learn useful constraints from the data.
To avoid dependence on clean master data, we propose
a novel system called BayesWipe [13] that assumes that
a statistical process underlies the generation of clean data
(which we call the data source model) as well as the cor-

2

ruption of data (which we call the data error model). The
noisy data itself is used to learn the parameters of these
the generative and error models, eliminating dependence
on clean master data. Then, by treating the clean value as
a latent random variable, BayesWipe leverages these two
learned models and automatically infers its value through a
Bayesian estimation.
We designed BayesWipe so that it can be used in two different modes: a traditional offline cleaning mode, and a novel
online query processing mode. The offline cleaning mode of
BayesWipe follows the classical data cleaning model, where
the entire database is accessible and can be cleaned in situ.
This mode is particularly useful when one has complete
control over the data, and a one-time cleaning of the data is
needed. Data warehousing scenarios such as data crawled
from the web, or aggregated from various noisy sources
can be effectively cleaned in this mode. The cleaned data
can be stored either in a deterministic database, or in a
probabilistic database. If a probabilistic database is chosen
as the output mode, BayesWipe stores not only the clean
version of the tuple it believes to be most likely correct
one, but the entire distribution over possible clean tuples.
The choice of a probabilistic output mode for the cleaned
tuples is most useful for those scenarios where recall is very
important for further data processing on the cleaned tuples.
One of the features of the offline mode of BayesWipe
is that a probabilistic database (PDB) can be generated as
a result of the data cleaning. In the first instance, notice
that BayesWipe was built for deterministic databases. It
can operate on a deterministic database and produce a
probabilistic cleaned database as an output. Probabilistic
databases are complex and unintuitive, because each single
input tuple is mapped into a distribution over resulting
clean alternatives. We show how the top-k results can be
retrieved from a PDB while displaying the clean data that is
comprehensible to the user.
The online query processing mode of BayesWipe is motivated by web data scenarios where it is impractical to create
a local copy of the data and clean it offline, either due to
large size, high frequency of change, or access restrictions. In
such cases, the best way to obtain clean answers is to clean
the resultset as we retrieve it, which also provides us the
opportunity of improving the efficiency of the system, since
we can now ignore entire portions of the database which are
likely to be unclean or irrelevant to the top-k . BayesWipe
uses a query rewriting system that enables it to efficiently
retrieve only those tuples that are important to the top-k
result set. This rewriting approach is inspired by, and is a
significant extension of our earlier work on QPIAD system
for handling data incompleteness [14]. In big data scenarios,
clean master data is rarely available, and write access is
either unavailable, or undesirable due to the efficiency and
indexing concerns. The online mode is particularly suited to
get clean results in such results.
We implement BayesWipe in a Map-Reduce architecture,
so that we can run it very quickly for massive datasets. The
architecture for parallelizing BayesWipe is explained more
fully in Sec 7. In short, there is a two-stage map-reduce
architecture, where in the first stage, the dirty tuples are
routed to a set of reducer nodes which hold the relevant
candidate clean tuples for them. In the second stage, the

resulting candidate clean tuples along with their scores are
collated, and the best replacement tuple is selected from
them.
To summarize our contributions, we:
‚Ä¢
‚Ä¢

‚Ä¢
‚Ä¢
‚Ä¢

Propose that data cleaning should be done using a
principled, probabilistic approach.
Develop a novel algorithm following those principles, which uses a Bayes network as the generative
model and maximum entropy as the error model of
the data.
Develop novel query rewriting techniques so that
this algorithm can also be used in a big data scenario.
Develop a parallelized version of this algorithm using map-reduce framework.
Empirically evaluate the performance of our algorithm using both controlled and real datasets.

The rest of the paper is organized as follows. We begin by discussing the related work and then describe the
architecture of BayesWipe in the next section, where we
also present the overall algorithm. Section 4 describes the
learning phase of BayesWipe, where we find the generative
and error models. Section 5 describes the offline cleaning
mode, and the next section details the query rewriting
and online data processing. We describe the parallelized
version of BayesWipe in Section 7 and the results of the
our empirical evaluation in Section 8, and then conclude
by summarizing our contributions. Further details about
BayesWipe can be found in the thesis [15].

2

R ELATED W ORK

Much of the work in data cleaning focused on deterministic
dependency relations such as FD, CFD, and INDs. Bohannon et al. proposed using Conditional Functional Dependencies (CFD) to clean data [16], [17]. Indeed, CFDs are
very effective in cleaning data. However, the precision and
recall of cleaning data with CFDs completely depends on
the quality of the set of dependencies used for the cleaning.
As our experiments show, learning CFDs from dirty data
produces very unsatisfactory results. In order for CFDbased methods to perform well, they need to be learned
from a clean sample of the database [10] which must be
large enough to be representative of all the patterns in the
data. Finding such a large corpus of clean master data is a
non-trivial problem, and is infeasible in all but the most
controlled of environments (like a corporation with high
quality data).
A recent variant on the deterministic dependency based
cleaning by J.Wang et al. [18] proposes using fixing rules
which contain negative(possible errors) and positive(clean
replacements) patterns for an attribute. However, there can
be several ways in which a tuple can go wrong and the
detection of the positive pattern requires clean master data.
BayesWipe on the other hand uses an error model to detect
errors automatically and clean them in the absence of clean
master data. Recent work by J.Wang et al. [19] plugs in one
of the rule based cleaning techniques to clean a sample of
the entire data and use it as a guideline to clean the entire
data. It is important to note that this method only caters
to aggregate numerical queries whereas the online mode

3

of BayesWipe supports all types of SQL queries (not just
aggregates) and returns clean result tuples.
Although it is possible to ameliorate some of the difficulties of CFD/AFD methods by considering approximate versions of them, the work in the uncertainty in AI
community demonstrated the semantic pitfalls of handling
uncertainty in this way. In particular, approximate versions
of CFDs/AFDs considered in works such as [20], [21] are
similar to the certainty factors approaches for handling
uncertainty that were popular in the heyday of expert
systems, but whose semantic inconsistencies are by now
well-established (see, for example, Section 14.7.1 of [34]).
Because of this, in this paper we focus on a more systematic
probabilistic approach.
Even if a curated set of integrity constraints are provided, existing methods do not use a probabilistically principled method of choosing a candidate correction. They resort
to either heuristic based methods, finding an approximate
algorithm for the least-cost repair of the database [9], [22],
[23]; using a human-guided repair [24], or sampling from
a space of possible repairs [25]. There has been work that
attempts to guarantee a correct repair of the database [26],
but they can only provide guarantees for corrections of those
tuples that are supported by data from a perfectly clean
master database. Recently, [27] have shown how the relative
trust one places on the constraints and the data itself plays
into the choice of cleaning tuples. A Bayesian source model
of data was used by [28], but was limited in scope to figuring
out the evolution over time of the data value.
Recent work has also focused on the metrics to use to
evaluate data cleaning techniques [29]. In this work, we
focus on evaluating our method against the ground truth
(when the ground truth is known), and user studies (when
the ground truth is not known).
While BayesWipe uses crowdsourcing to evaluate the
accuracy of the proposed clean tuple alternatives for the
experiments on real world datasets, there are other systems that try to use the crowd for cleaning the data itself.
X.Chu et al. [30] clean the database tuples by discovering
patterns that overlap with Knowledge Base(KB)s like Yago
and validating the top-k candidates using the crowd. J.Wang
et al. [31] perform entity resolution (which is to identify
several values corresponding to the same entity value) using
crowdsourcing. They reduce the complexity of the number
of HIT(Human Intelligence Task)s generated by clustering
them into several bins so that a set of pairs can be resolved
at a time as against evaluating one pair at a time. Y.Zheng
et al. [32] pick a set of k questions to be included in the HITs
for the human workers out of a total set of n questions using
estimates on the expected increase in the answer quality by
assigning those questions to the crowd. Crowdsourcing to
perform data cleaning may be infeasible in the context of Big
Data cleaning targeted by BayesWipe . However, suggestions from the crowd can be used to provide cleaner master
data from which BayesWipe learns the Bayes network.
The query rewriting part of this work is inspired
by the QPIAD system [14], but significantly improves
upon it. QPIAD performed query rewriting over incomplete databases using approximate functional dependencies
(AFD), and only cleaned data with null values, not wrong
values.

Offline Cleaning

Model Learning
Data source
model
Error
Model

Cleaning to a
Deterministic DB
or
Cleaning to a
Probabilistic DB

Clean
Data

OR
Candidate
Set Index

Query Processing
Query
Rewriting

Database
Sampler

Result
Ranking

Data Source

Fig. 1: The architecture of BayesWipe. Our framework learns
both data source model and error model from the raw data
during the model learning phase. It can perform offline
cleaning or query processing to provide clean data.

3

BAYES W IPE OVERVIEW

BayesWipe views the data cleaning problem as a statistical
inference problem over the structured text data. Let D =
{T1 , ..., Tn } be the input structured data which contains a
number of corruptions. Ti ‚àà D is a tuple with m attributes
{A1 , ..., Am } which may have one or more corruptions in
its attribute values. Given a candidate replacement set C
for a possibly corrupted tuple T in D, we can clean the
database by replacing T with the candidate clean tuple
T ‚àó ‚àà C that has the maximum Pr(T ‚àó |T ). Using Bayes rule
(and dropping the common denominator), we can rewrite
this to
‚àó
Tbest
= arg max[Pr(T |T ‚àó )Pr(T ‚àó )]

(1)

‚àó
By replacing T with Tbest
, we get a deterministic database.
If we wish to create a probabilistic database (PDB), we don‚Äôt
take an arg max over the Pr(T ‚àó |T ), instead we store the
entire distribution over the T ‚àó in the resulting PDB.
For online query processing we take the user query Q‚àó ,
and find the relevance score of a tuple T as
X
Score(T ) =
Pr(T ‚àó ) Pr(T |T ‚àó ) R(T ‚àó |Q‚àó )
(2)
| {z } | {z } | {z }
‚àó
T ‚ààC

source model error model

relevance

In this work, we used a binary relevance model, where R
is 1 if T ‚àó is relevant to the user‚Äôs query, and 0 otherwise.
Note that R is the relevance of the query Q‚àó to the candidate
clean tuple T ‚àó and not the observed tuple T . This allows the
query rewriting phase of BayesWipe which aims to retrieve
tuples with the highest Score(.) to achieve the non-lossy
effect of using a PDB without explicitly rectifying the entire
database.
Architecture:
Figure 1 shows the system architecture for BayesWipe.
During the model learning phase (Section 4), we first obtain
a sample database by sending some queries to the database.
On this sample data, we learn the generative model of the
data as a Bayes network (Section 4.1). In parallel, we define
and learn an error model which incorporates common kinds

4

of errors (Section 4.2). We also create an index to quickly
propose candidate T ‚àó s.
We can then choose to do either offline cleaning (Section 5) or online query processing (Section 6), as per the
scenario. In the offline cleaning mode, we iterate over all
the tuples in the database and clean them one by one. We
can choose whether to store the resulting cleaned tuple in a
deterministic database (where we store only the T ‚àó with the
maximum posterior probability) or probabilistic database
(where we store the entire distribution over the T ‚àó ). In the
online query processing mode, we obtain a query from the
user, and do query rewriting in order to find a set of queries
that are likely to retrieve a set of highly relevant tuples.
We execute these queries and re-rank the results, and then
display them.
Algorithm 1: The algorithm for offline data cleaning
Input: D, the dirty dataset.
BN ‚Üê Learn Bayes Network (D)
foreach Tuple T ‚àà D do
C ‚Üê Find Candidate Replacements (T )
foreach Candidate T ‚àó ‚àà C do
P (T ‚àó ) ‚Üê Find Joint Probability (T ‚àó , BN )
P (T |T ‚àó ) ‚Üê Error Model (T, T ‚àó )
end
T ‚Üê arg max
P (T ‚àó )P (T |T ‚àó )
‚àó
T ‚ààC

end

Algorithm 2: Algorithm for online query processing.
Input: D, the dirty dataset
Input: Q, the user‚Äôs query
S ‚Üê Sample the source dataset D
BN ‚Üê Learn Bayes Network (S )
ES ‚Üê Learn Error Statistics (S )
R ‚Üê Query and score results (Q, D, BN )
ESQ ‚Üê Get expanded queries (Q)
foreach Expanded query E ‚àà ESQ do
R ‚Üê R ‚à™ Query and score results (E, D, BN )
RQ ‚Üê RQ ‚à™ Get all relaxed queries (E )
end
Sort(RQ) by expected relevance, using ES
while top-k confidence not attained do
B ‚Üê Pick and remove top RQ
R ‚Üê R ‚à™ Query and score results (B, D, BN )
end
Sort(R) by score
return R
In Algorithms 1 and 2, we present the overall algorithm
for BayesWipe. In the offline mode, we show how we iterate
over all the tuples in the dirty database, D and replace them
with cleaned tuples. In the query processing mode, the first
three operations are performed offline, and the remaining
operations show how the tuples are efficiently retrieved
from the database, ranked and displayed to the user.

4

M ODEL L EARNING

This section details the process by which we estimate the
components of Equation 2: the data source model Pr(T ‚àó )

Make

Condition

occupation

Model

Year

gender

workingclass

Door

Drivetrain

country

race
education

marital status

Engine

Car Type

(a) Auto dataset

filing-status

(b) Census dataset

Fig. 2: The learned Bayes networks
and the error model Pr(T |T ‚àó )
4.1

Data Source Model

The data that we work with can have dependencies among
various attributes (e.g., a car‚Äôs engine depends on its make).
Therefore, we represent the data source model as a Bayes
network, since it naturally captures relationships between
the attributes via structure learning and infers probability
distributions over values of the input tuples.
Constructing a Bayes network over D requires two steps:
first, the induction of the graph structure of the network,
which encodes the conditional independences between the
m attributes of D‚Äôs schema; and second, the estimation
of the parameters of the resulting network. The resulting
model allows us to compute probability distributions over
an arbitrary input tuple T .
Whenever the underlying patterns in the source database
changes, we have to learn the structure and parameters
of the Bayes network again. In our scenario, we observed
that the structure of a Bayes network of a given dataset
remains constant with small perturbations, but the parameters (CPTs) change more frequently. As a result, we spend a
larger amount of time learning the structure of the network
with a slower, but more accurate tool, Banjo [33]. Figures
2a and 2b show automatically learned structures for two
data domains. The learned structure seems to be intuitively
correct, since the nodes that are connected (for example,
‚Äòcountry‚Äô and ‚Äòrace‚Äô in Figure 2b) are expected to be highly
correlated1 .
Then, given a learned graphical structure G of D, we
can estimate the conditional probability tables (CPTs) that
parameterize each node in G using a faster package called
Infer.NET [35]. This process of inferring the parameters is
run offline, but more frequently than the structure learning.
Once the Bayesian network is constructed, we can infer
the joint distributions for arbitrary tuple T , which can
be decomposed to the multiplication of several marginal
distributions of the sets of random variables, conditioned
on their parent nodes depending on G .
4.2

Error Model

Having described the data source model, we now turn to
the estimation of the error model Pr(T |T ‚àó ) from noisy data.
There are many types of errors that can occur in data. We
focus on the most common types of errors that occur in data
1. Note that the direction of the arrow in a Bayes network does not
necessarily determine causality, see Chapter 14 from Russel and Norvig
[34].

5

that is manually entered by naƒ±Ãàve users: typos, deletions,
and substitution of one word with another. We also make
an additional assumption that error in one attribute does
not affect the errors in other attributes. This is a reasonable assumption to make, since we are allowing the data
itself to have dependencies between attributes, while only
constraining the error process to be independent across
attributes. With these assumptions, we are able to come up
with a simple and efficient error model, where we combine
the three types of errors using a maximum entropy model.
Given a set of clean candidate tuples C where T ‚àó ‚àà C ,
our error model Pr(T |T ‚àó ) essentially measures how clean T
is, or in other words, how similar T is to T ‚àó .
Edit distance similarity: This similarity measure is used to
detect spelling errors. Edit distance between two strings TAi
and TA‚àó i is defined as the minimum cost of edit operations
applied to dirty tuple TAi transform it to clean TA‚àó i . Edit
operations include character-level copy, insert, delete and
substitute. The cost for each operation can be modified as
required; in this paper we use the Levenshtein distance,
which uses a uniform cost function. This gives us a distance,
which we then convert to a probability using [36]:

4.3

Finding the Candidate Set

The set of candidate tuples, C(T ) for a given tuple T are
the possible replacement tuples that the system considers
as possible corrections to T . The larger the set C is, the
longer it will take for the system to perform the cleaning. If
C contains many unclean tuples, then the system will waste
time scoring tuples that are not clean to begin with.
An efficient approach to finding a reasonably clean
C(T ) is to consider the set of all the tuples in the sample
database that differ from T in not more than j attributes.
In order to find C(T ) that satisfies this, conceptually, we
have to iterate over every tuple t in the sample database
D, comparing it to the tuple T and checking how many
attributes it differs in. This operation can take O(n) time,
where n is the number of tuples in the sample database.
Even with j = 3, the naƒ±Ãàve approach of constructing C from
the sample database directly is too time consuming, since
it requires one to go through the sample database in its
entirety once for every result tuple encountered. To make
this process faster, we create indices over (j + 1) attributes
because searching through indices reduces the number of
comparisons required to compute C(T ). If any candidate
fed (TAi , TA‚àó i ) = exp{‚àícosted (TAi , TA‚àó i )}
(3) tuple T ‚àó differs from T in less than or equal to j attributes,
Distributional Similarity Feature: This similarity measure then it will be present in at least one of the indices, since
is used to detect both substitution and omission errors. we created j + 1 of them (pigeon hole principle). These
Looking at each attribute in isolation is not enough to fix j + 1 indices are created over those attributes that have the
these errors. We propose a context-based similarity measure highest cardinalities, such as Make and Model (as opposed
called Distributional similarity (fds ), which is based on the to attributes like Condition and Doors which can take only
probability of replacing one value with another under a a few values). This ensures that the set of tuples returned
similar context [37]. Formally, for each string TAi and TA‚àó i , from the index would be small in number.
For every possibly dirty tuple T in the database, we go
we have:
over
each such index and find all the tuples that match the
‚àó
X
Pr(c|TAi )Pr(c|TAi )Pr(TAi )
fds (TAi , TA‚àó i ) =
(4)corresponding attribute. The union of all these tuples is then
Pr(c)
‚àó )
examined and the candidate set C is constructed by keeping
c‚ààC(TAi ,TA
i
only those tuples from this union set that do not differ from
where C(TAi , TA‚àó i ) is the context of a tuple attribute value, T in more than j attributes. Thus we can be sure that by
which is a set of attribute values that co-occur with using this method, we have obtained the entire set C 2 .
both TAi and TA‚àó i . Pr(c|TA‚àó i ) = (#(c, TA‚àó i ) + ¬µ)/#(TA‚àó i )
is the probability that a context value c appears given
the clean attribute TA‚àó i in the sample database. Similarly, 5 O FFLINE CLEANING
P (TAi ) = #(TAi )/#tuples is the probability that a dirty
attribute value appears in the sample database. We calculate 5.1 Cleaning to a Deterministic Database
Pr(c|TAi ) and Pr(TAi ) in the same way. To avoid zero In order to clean the data in situ, we first use the techniques
estimates for attribute values that do not appear in the of the previous section to learn the data source model, the
database sample, we use Laplace smoothing factor ¬µ.
error model and create the index. Then, we iterate over all
Unified error model: In practice, we do not know before- the tuples in the database and use Equation 1 to find the
hand which kind of error has occurred for a particular T ‚àó with the best score. We then replace the tuple with that
attribute; we need a unified error model which can accom- T ‚àó , thus creating a deterministic database using the offline
modate all three types of errors (and be flexible enough to mode of BayesWipe.
accommodate more errors when necessary). For this purComputing Pr(T ‚àó )Pr(T |T ‚àó ) is very fast. Even though
pose, we use the well-known maximum entropy framework we do a Bayesian inference for Pr(T ‚àó ), the tuple has all the
[38] to leverage both the similarity measures, (Edit distance values specified, so the inference ends up being a simple
fed and distributional similarity fds ). For each attribute of multiplication over the CPTs of the Bayes network, and is
the input tuple T and T ‚àó , we have the unified error model very cheap. Pr(T |T ‚àó ) involves simple edit distance and disPr(T |T ‚àó ) given by:
tributional similarity calculations all of which involve sim( m
)
ple arithmetic operations and lookups devoid of Bayesian
m
X
X
1
exp Œ±
fed (TAi , TA‚àó i ) + Œ≤
fds (TAi , TA‚àó i )
(5) inference.
Z
i=1
i=1
where Œ± and Œ≤ are the weight of each feature, m is the
numberPof attributes
P in the tuple. The normalization factor
is Z = T ‚àó exp { i Œªi fi (T ‚àó , T )}.

2. There is a small possibility that the true tuple T ‚àó is not in the
sample database at all. This probability can be reduced by choosing
a larger sample set. In future work, we will expand the strategy of
generating C to include all possible k-repairs of a tuple.

6

Recall from Section 4.2 that there are parameters in the
error model called Œ± and Œ≤ , which need to be set. Interestingly, in addition to controlling the relative weight given to
the various features in the error model, these parameters
can be used to control overcorrection by the system.
Overcorrection: Any data cleaning system is vulnerable to
overcorrection, where a legitimate tuple is modified by the
system to an unclean value. Overcorrection can have many
causes. In a traditional, deterministic system, overcorrection
can be caused by erroneous rules learned from infrequent
data. For example, certain makes of cars are all owned by the
same conglomerate (GM owns Chevrolet). In a misguided
attempt to simplify their inventory, a car salesman might list
all the cars under the name of the conglomerate. This may
provide enough support to learn the wrong rule (Malibu ‚Üí
GM).
Typically, once an erroneous rule has been learned, there
is no way to correct it or ignore it without a lot of oversight
from domain experts. However, BayesWipe provides a way
to regulate the amount of overcorrection in the system with
the help of a ‚Äòdegree of change‚Äô parameter. Without loss of
generality, we can rewrite Equation 5 to the following:
  X
m
1
Pr(T |T ) = exp Œ≥ Œ¥
fed (TAi , TA‚àó i )
Z
i=1
‚àó

+ (1 ‚àí Œ¥)

m
X


fds (TAi , TA‚àó i )

i=1

Since we are only interested in their relative weights, the
parameters Œ± and Œ≤ have been replaced by Œ¥ and (1‚àíŒ¥) with
the help of a normalization constant, Œ≥ . This parameter, Œ≥ ,
can be used to modify the degree of variation in Pr(T |T ‚àó ).
High values of Œ≥ imply that small differences in T and
T ‚àó cause a larger difference in the value of Pr(T |T ‚àó ),
causing the system to give higher scores to the original tuple
(compared to a modified tuple).
Example: Consider the following fragment from the
database. The first tuple is a very frequent tuple in the
database, the second one is an erroneous tuple, and the third
tuple is an infrequent, correct tuple. The ‚Äòtrue‚Äô correction
of the second tuple is the third tuple. The Pr(T ‚àó ) values
shown reflect the values that the data source model might
predict for them, roughly based on the frequency with
which they occur in the source data.
Id

Make

Model Type

1
2
3

Honda Civic
Honda Z4
BMW Z4

Engine

Sedan V4
Sedan V6
Sedan V6

Condition P (T ‚àó )
New
New
New

0.400
0.001
0.005

A proper data cleaning system will correct tuple 2 to
tuple 3, and not modify any of the others. However, if
incorrect rules (for example, Z4 ‚Üí Honda) were learned,
there could be overcorrection, where tuple 3 is modified to
tuple 2.
On the other hand, BayesWipe handles this situation
based on the value of Œ≥ . Looking at tuple 3 (which is a clean
tuple), suppose the candidate replacement tuples for it are
also tuples 1, 2 and 3. In that case, the situation may look
like the following:

Cd.
1
2
3

P (T ‚àó )
0.400
0.001
0.005

low Œ≥
P (T |T ‚àó )
score
0.02 0.0080
0.30 0.0003
1.00 0.0050

high Œ≥
P (T |T ‚àó )
score
0.002 0.00080
0.030 0.00003
1.000 0.00500

As we can see, if we choose a low value of Œ≥ , the
candidate with the highest score is tuple 1, which means
an overcorrection will occur. However, with higher Œ≥ , the
candidate with the highest score is tuple 3 itself, which
means the tuple will not be modified, and overcorrection
will not occur. On the other hand, if we set Œ≥ too high, then
even legitimately dirty tuples like tuple 2 won‚Äôt get changed,
thus the number of actual corrections will also be lower.
To make full use of this capability of regulating overcorrection, we need to be able to set the value of Œ≥ appropriately. In the absence of a training dataset (for which the
ground truth is known), we can only estimate the best Œ≥
approximately. We do this by finding a value of Œ≥ for which
the percentage of tuples modified by the system is equal to
the expected percentage of noise in the dataset.
5.2

Cleaning to a Probabilistic Database

We note that many data cleaning approaches ‚Äî including
the one we described in the previous sections ‚Äî come up
with multiple alternatives for the clean version for any given
tuple, and evaluate their confidence in each of the alternatives. For example, if a tuple is observed as ‚ÄòHonda, Corolla‚Äô,
two correct alternatives for that tuple might be ‚ÄòHonda,
Civic‚Äô and ‚ÄòToyota, Corolla‚Äô. In such cases, where the choice
of the clean tuple is not an obvious one, picking the mostlikely option may lead to the wrong answer. Additionally,
if we intend to do further processing on the results, such as
perform aggregate queries, join with other tables, or transfer
the data to someone else for processing, then storing the
most likely outcome is lossy.
A better approach (also suggested by others [4]) is to
store all of the alternative clean tuples along with their
confidence values. Doing this, however, means that the
resulting database will be a probabilistic database (PDB),
even when the source database is deterministic.
It is not clear upfront whether PDB-based cleaning will
have advantages over cleaning to a deterministic database.
On the positive side, using a PDB helps reduce loss of
information arising from discarding all alternatives to tuples
that did not have the maximum confidence. On the negative
side, PDB-based cleaning increases the query processing
cost (as querying PDBs are harder than querying deterministic databases [39]).
Another challenge is one of presentation: users usually
assume that they are dealing with a deterministic source
of data, and presenting all alternatives to them can be
overwhelming to them. In this section, and in the associated
experiments, we investigate the potential advantages to using the BayesWipe system and storing the resulting cleaned
data in a probabilistic database. For our experiments, we
used Mystiq [40], a prototype probabilistic database system
from University of Washington, as the substrate. In order to
create a probabilistic database from the corrections of the input data, we follow the offline cleaning procedure described
previously in Section 4. Instead of storing the most likely T ‚àó ,
we store all the T ‚àó s along with their P (T ‚àó |T ) values. When

7

evaluating the performance of the probabilistic database, we
used simple select queries on the resulting database. Since
representing the results of a probabilistic database to the
user is a complex task, in this paper we focus on showing
the XOR representation of the tuple alternatives to the user.
The rationale for our decision is that in a used car scenario,
the user will be provided with a URL link to the car through
the clickable tuple id and the several alternative clean values
for the dirty attributes are shown within the single tuple
returned to the user. As a result, the form of our output is a
tuple-disjoint independent database [41]. This can be better
explained with an example:
TABLE 2: Cleaned probabilistic database
TID
t1

Model
Civic
Civic

Make
Honda
Honda

Orig.
JPN
JPN

Civic
Civik

Honda
Honda

JPN
JPN

Size
Mid-size
Compact

Eng.
V4
V6

Cond.
NEW
NEW

P
0.6
0.4

Mid-size
Mid-size

V4
V4

USED
USED

0.9
0.1

...
t3

Example: Suppose we clean our running example of
Table 1. We will obtain a tuple-disjoint independent3 probabilistic database [41]; a fragment of which is shown in
Table 2. Each original input tuple (t1 , t3 ), has been cleaned,
and their alternatives are stored along with the computed
confidence values for the alternatives (0.6 and 0.4 for t1 ,
in this example). Suppose the user issues a query Model =
Civic. Both options of tuple t1 of the probabilistic database
satisfy the constraints of the query. Since there are two valid
alternatives to tuple t1 in the result with probabilities 0.6
and 0.4, in order to get a single tuple representation, the
matching attributes in the alternatives are shown deterministically whereas the unclean attributes like Size, Engine and
Condition with several possible clean values are shown as
options. Only the first option in tuple t3 matches the query.
Thus the XOR result will contain only a single alternative
for t3 with probability 0.9. It is important to note that in the
case of t1 , the Mid-size car can be associated with an Eng.
value of V4 and a probability of 0.6 respectively. The XOR
representation doesn‚Äôt necessarily allow for combining Midsize with either an Eng. value of V6 or a probability value
of 0.4.
The experimental results compare the tuple ids when
computing the recall of the method because tuple id provides the URL to the car‚Äôs web page which can be used
to determine a match. The output probabilistic relation is
shown in Table 3.
TABLE 3: Result probabilistic database
TID

Model

Make

Orig.

t1

Civic

Honda

JPN

t3

Civic

Honda

JPN

Size
Midsize/Compact
Mid-size

Eng.

Cond.

P

V4/V6

NEW

0.6/0.4

V4

USED

0.9

3. A tuple-disjoint independent probabilistic database is one where
every tuple, identified by its primary key, is independent of all other
tuples. Each tuple is, however, allowed to have multiple alternatives
with associated probabilities. In a tuple-independent database, each
tuple has a single probability, which is the probability of that tuple
existing.

The interesting fact here is that the result of any query
will always be a tuple-independent database. This is because we projected out every attribute except for the tupleID, and the tuple-IDs are independent of each other.
When showing the results of our experiments, we evaluate the precision and recall of the system. Since precision
and recall are deterministic concepts, we have to convert
the probabilistic database into a deterministic database (that
will be shown to the user) prior to computing these values.
We can do this conversion in two ways: (1) by picking only
those tuples whose probability is higher than some threshold. We call this method the threshold based determinization.
(2) by picking the top-k tuples and discarding the probability values (top-k determinization). The experiment section
(Section 8.2) shows results with both determinizations.

6

Q UERY REWRITING FOR O NLINE Q UERY P RO -

CESSING

In this section we extend the techniques of the previous
section so that it can be used in an online query processing
method where the result tuples are cleaned at query time.
Certain tuples that do not satisfy the query constraints, but
are relevant to the user, need to be retrieved, ranked and
shown to the user. The process also needs to be efficient,
since the time that the users are willing to wait before
results are shown to them is very small. We show our query
rewriting mechanisms aimed at addressing both.
We begin by executing the user‚Äôs query (Q‚àó ) on the
database. We store the retrieved results, but do not show
them to the user immediately. We then find rewritten queries
that are most likely to retrieve clean tuples. We do that in
a two-stage process: we first expand the query to increase
the precision, and then relax the query by deleting some
constraints (to increase the recall).
6.1

Increasing the precision of rewritten queries

We can improve precision by adding relevant constraints to the query Q‚àó given by the user. For example, when a user issues the query Model = Civic,
we can expand the query to add relevant constraints Make = Honda, Country = Japan, Size = Mid-Size.
These additions capture the essence of the query ‚Äî because
they limit the results to the specific kind of car the user
is probably looking for. These expanded structured queries
generated from the user‚Äôs query are called ESQs.
Each user query Q‚àó is a select query with one or more
attribute-value pairs as constraints. In order to create an
ESQ, we will have to add highly correlated constraints to
Q‚àó .
Searching for correlated constraints to add requires
Bayesian inference, which is an expensive operation. Therefore, when searching for constraints to add to Q‚àó , we restrict
the search to the union of all the attributes in the Markov
blanket [42]. The Markov blanket of an attribute comprises
its children, its parents, and its children‚Äôs other parents. It
is the set of attributes whose value being given, the node
becomes independent of all other nodes in the network.
Thus, it makes sense to consider these nodes when finding
correlated attributes. This correlation is computed using
the Bayes Network that was learned offline on a sample
database (recall the architecture of BayesWipe in Figure 1.)

8

Given a Q‚àó , we attempt to generate multiple ESQs that
maximizes both the relevance of the results and the coverage
of the queries of the solution space.
Note that if there are m attributes, each of which can
take n values, then the total number of possible ESQs is
nm . Searching for the ESQ that globally maximizes the
objectives in this space is infeasible; we therefore approximately search for it by performing a heuristic-informed
search. Our objective is to create an ESQ with m attributevalue pairs as constraints.We begin with the constraints
specified by the user query Q‚àó . We set these as evidence in
the Bayes network, and then query the Markov blanket of
these attributes for the attribute-value pairs with the highest
posterior probability given this evidence. We take the top-k
attribute-value pairs and append them to Q‚àó to produce k
search nodes, each search node being a query fragment. If Q
has p constraints in it, then the heuristic value of Q is given
by Pr(Q)m/p . This represents the expected joint probability
of Q when expanded to m attributes, assuming that all the
constraints will have the same average posterior probability.
We expand them further, until we find k queries of size m
with the highest probabilities.
Make=Honda

0.4

Model=
Accord

0.1

Make=
Honda

0.3

0.9

Model=
Civic
Fuel=
Gas

Miles=
10k

‚Ä¶

(0.1)6
Make=Honda, Model = Accord

Engine=
V4

‚Ä¶

Doors=
4

‚Ä¶

(0.1 √ó 0.3)3

Model=
Civic

‚Ä¶

(0.1 √ó 0.9)3

(0.1 √ó 0.4)3
Make=Honda, Model = Civic

Make=Honda, Fuel = Gas

Fig. 3: Query Expansion Example. The tree shows the candidate constraints that can be added to a query, and the
rectangles show the expanded queries with the computed
probability values.
Example: In Figure 3, we show an example of the query
expansion. The node on the left represents the query given
by the user ‚ÄúMake=Honda‚Äù. First, we look at the Markov
Blanket of the attribute Make, and determine that Model
and Condition are the nodes in the Markov blanket. we
then set ‚ÄúMake=Honda‚Äù as evidence in the Bayes network
and then run an inference over the values of the attribute
Model. The two values of the Model attribute with the
highest posterior probability are Accord and Civic. The
most probable values of the Condition attribute are ‚Äúnew‚Äù
and ‚Äúold‚Äù. Using each of these values, new queries are
constructed and added to the queue. Thus, the queue now
consists of the 4 queries: ‚ÄúMake=Honda, Model=Civic‚Äù,
‚ÄúMake=Honda, Model=Accord‚Äù and ‚ÄúMake=Honda, Condition=old‚Äù. A fragment of these queries are shown in
the middle column of Figure 3. We dequeue the highest
probability item from the queue and repeat the process
of setting the evidence, finding the Markov Blanket, and
running the inference. We stop when we get the required
number of ESQs with a sufficient number of constraints.
6.2 Increasing the recall
Adding constraints to the query causes the precision of the
results to increase, but reduces the recall drastically. Therefore, in this stage, we choose to delete some constraints from

the ESQs, thus generating relaxed queries (RQ). Notice that
tuples that have corruptions in the attribute constrained by
the user can only be retrieved by relaxed queries that do
not specify a value for those attributes. Instead, we have to
depend on rewritten queries that contain correlated values
in other attributes to retrieve these tuples. Using relaxed
queries can be seen as a trade-off between the recall of the
resultset and the time taken, since there are an exponential
number of relaxed queries for any given ESQ. As a result,
an important question is the choice of RQs to execute. We
take the approach of generating every possible RQ, and
then ranking them according to their expected relevance.
This operation is performed entirely on the learned error
statistics, and is thus very fast.
We score each relaxed query by the expected relevance of
its result set.
!
P
‚àó
Tq Score(Tq |Q )
Rank(q) = E
|Tq |
where Tq are the tuples returned by a query q , and Q‚àó is
the user‚Äôs query. Executing an RQ with a higher rank will
have a more beneficial result on the result set because it will
bring in better quality result tuples. Estimating this quantity
is difficult because we do not have complete information
about the tuples that will be returned for any query q . The
best we can do, therefore, is to approximate this quantity.
Let the relaxed query be Q, and the expanded query
that it was relaxed from be ESQ. We wish to estimate
E[P (T |T ‚àó )] where T are the tuples returned by Q. Using the
attribute-error
independence assumption, we can rewrite
Qm
that as i=0 Pr(T.Ai |T ‚àó .Ai ), where T.Ai is the value of the
i-th attribute in T. Since ESQ was obtained by expanding
Q‚àó using the Bayes network, it has values that can be
considered clean for this evaluation. Now, we divide the m
attributes of the database into 3 classes: (1) The attribute
is specified both in ESQ and in Q. In this case, we set
Pr(T.Ai |T ‚àó .Ai ) to 1, since T.Ai = T ‚àó .Ai . (2) The attribute
is specified in ESQ but not in Q. In this case, we know
what T ‚àó .Ai is, but not T.Ai . However, we can generate an
average statistic of how often T ‚àó .Ai is erroneous by looking
at our sample database. Therefore, in the offline learning
stage, we precompute tables of error statistics for every T ‚àó
that appears in our sample database, and use that value.
(3) The attribute is not specified in either ESQ or Q. In
this case, we know neither the attribute value in T nor in
T ‚àó . We, therefore, use the average error rate of the entire
attribute as the value for Pr(T.Ai |T ‚àó .Ai ). This statistic is
also precomputed during the learning phase. This product
gives the expected rank of the tuples returned by Q.
Example: In Figure 4, we show an example for finding
the probability values of a relaxed query. Assume that the
user‚Äôs query Q‚àó is ‚ÄúCivic‚Äù, and the ESQ is shown in the
second row. For an RQ that removes the attribute values
‚ÄúCivic‚Äù and ‚ÄúMid-Size‚Äù from the ESQ, the probabilities are
calculated as follows: For the attributes ‚ÄúMake, Country‚Äù
and ‚ÄúEngine‚Äù, the values are present in both the ESQ as
well as the RQ, and therefore, the P (T |T ‚àó ) for them is 1.
For the attribute ‚ÄúModel‚Äù and ‚ÄúType‚Äù, the values are present
in ESQ but not in RQ, hence the value for them can be
computed from the learned error statistics. For example, for
‚ÄúCivic‚Äù, the average value of P (T |Civic) as learned from

9

Model
Q*:

Civic

ESQ:

Civic

RQ:

E[P(T|T*)]:

0.8

Make

Country

Honda

JPN

Honda

JPN

1

1

Type

Mid-size

Engine

Cond.

V4
V4

0.5

1

0.5

=0.2

Fig. 4: Query Relaxation Example.
the sample database (0.8) is used. Finally, for the attribute
‚ÄúCondition‚Äù, which is present neither in ESQ nor in RQ,
we use the average error statistic for that attribute (i.e. the
average of P (Ta |Ta‚àó ) for a = ‚ÄúCondition‚Äù which is 0.5).
The final value of E[P (T |T ‚àó )] is found from the product
of all these attributes as 0.2. This process is very fast because it only involves lookups and multiplication - bayesian
inference is not needed.
6.3

Terminating the process

We begin by looking at all the RQs in descending order
of their rank. If the current k -th tuple in our resultset has
a relevance of Œª, and the estimated rank of the Q we are
about to execute is R(Tq |Q), then we stop evaluating any
more queries if the probability Pr(R(Tq |Q) > Œª) is less
than some user defined threshold P . This ensures that we
have the true top-k resultset with a probability P .

7

M AP -R EDUCE FRAMEWORK

BayesWipe is most useful for big-data related scenarios.
BayesWipe has two modes: online and offline. The online
mode of BayesWipe already works for big data scenarios by
optimising the rewritten queries it issues. Now, we show
that the offline mode can also be optimized for a big-data
scenario by implementing it as a Map-Reduce application.
So far, BayesWipe-Offline has been implemented as a
two-phase, single threaded program. In the first phase,
the program learns the Bayes network (both structure and
parameters), learns the error statistics, and creates the candidate index. Recall from section 4.3 that we create an
index on the attributes of the sample database to speed
up the creation of the candidate set of clean tuples; which
we refer to as the candidate index. The candidate index is
constructed on a set of j + 1 attributes when the restriction
on a candidate clean tuple is to differ from the dirty tuple in
not more than j attributes. The attributes in the dirty tuple
are compared to the attributes of the tuples in the sample
database using the candidate index to generate the set of
candidate clean tuples. Note that this candidate index can be
constructed on any arbitrary set of j + 1 attributes present
in the sample database. In the second phase, the program
goes through every tuple in the input database, picks a set
of candidate tuples, and then evaluates the P (T ‚àó |T )P (T ‚àó )
for every candidate tuple, and replaces T with the T ‚àó that
maximises that value. Since the learning is typically done
on a sample of the data, it is more important to focus
on the second phase for the parallelizing efforts. Later, we
will see how the learning of the error statistics can also be
parallelized.

7.1

Simple Approach

The simplest approach to parallelizing BayesWipe is to run
the first phase (the learning phase) on a single machine.
Then, a copy of the bayes network (structure and CPTs),
the error statistics, and the candidate index can be sent to
a number of other machines. Each of those machines also
receives a fraction of the input data from the dirty database.
With the help of the generative model and the input data, it
can clean the tuples, and then create the output.
If we express this in Map-Reduce terminology, we will
have a pre-processing step where we create the generative
and error models. The Map-Reduce architecture will have
only mappers, and no reducers. The result of the mapping
will be the tuple hT, T ‚àó i.
The problem with this approach is that in a truly big
data scenario, the candidate index can become very large.
Indeed, as the number of tuples increases, the size of the
domain of each attribute also increases (see Figure 8a for
1 shard). Further, the number of different combinations,
and the number of erroneous values for each attribute also
increase (Figure 8b). All of this results in a rather large candidate index. Transmitting and using the entire index on each
mapper node is wasteful of both network, memory, (and
if swapped out, disk resources). Note that to create a rich
and useful data correction system, we have to accommodate
a large candidate clean-tuple set, C(T ), for every T . C(T )
roughly tracks the sample database size. If we are unable to
shard C(T ), then sharding the input data is pointless. In the
following section we endeavor to show a strategy where not
just the input, but also the index on the candidate set C(T )
can be sharded across machines.
7.2

Improved Approach

In order to split both the input tuples and the candidate
index, we use a two-stage approach. In the first stage, we
run a map-reduce that splits the problem into multiple
shards, each shard having a small fraction of the candidate
index. The second stage is a simple map-reduce that picks
the best output from stage 1 for each input tuple.
Stage 1: Given an input tuple T and a set of candidate
tuples, the T ‚àó s, suppose the candidate index is created on k
attributes, A1 ...Ak . We can say that for every tuple T , and
one of its candidate tuples T ‚àó , they will have at least one
matching attribute ai from this set. We can use this common
element ai to predict which shards the candidate T ‚àó s might
be available in. We therefore, send the tuple T to each shard
that matches the hash of the value ai .
In the map-reduce architecture, it is possible to define
a ‚Äòpartition‚Äô function. Given a mapped key-value pair, this
function determines which reducer nodes will process the
data. We can use an exact equivalence on each value that
the matching attribute can take, ai as the partition function.
However, notice that the number of possible values that
A1 ...Ak can take is rather large. If we naƒ±Ãàvely use ai as
the partition function, we will have to create those many
reducer nodes. Therefore, more generally, we hash this value
into a fixed number of reducer nodes, using a deterministic
hash function. This will then find all candidate tuples that
are eligible for this tuple, compute the similarity, and output
it.

10

Example: Suppose we have tuple T1 that has values
(a1 , a2 , a3 , a4 , a5 ). Suppose our candidate index is created
on attributes A1 , A2 , A4 . This means that any candidates T ‚àó
that are eligible for this tuple have to match one of the values
a1 , a2 or a4 . Then the mapper will create the pairs (a1 , T ),
(a2 , T ) and (a4 , T ), and send to the reducers. The partition
function is the hash of the key - so in this case, the first one
will be sent to the reducer number hash(A1 = a1), the second will be sent to the reducer numbered hash(A2 = a2),
and so on.
In the reducer, the similarity computation and computation of the prior probabilities from the BayesWipe algorithm
are run. Since each reducer only has a fraction of the candidate index (the part that matches A1 = a1, for instance), it
can hold it in memory and computation is quite fast. Each
‚àó
reducer produces a pair (T1 , (T1n
, score)). Since there are
several candidate clean tuples, n is used to identify a specific
tuple among those alternatives.
Stage 2: This stage is a simple max calculation. The
mapper does nothing, it simply passes on the key-value
‚àó
pair (T1 , (T1n
, score)) that was generated in the previous
Map-Reduce job. Notice that the key of this pair is the
original, dirty tuple T1 . The Map-Reduce architecture thus
automatically groups together all the possible clean versions
of T1 along with their scores. The reducer picks the best
T* based on the score (using a simple max function), and
outputs it to the database.

Index
Fragment

Reducer
(a, T)
Mapper

Index
Fragment

Reducer
(T, (T*, score))
Index
Fragment

Mapper

Input Data
Fragments

Reducer
Index
Fragment

Reducer

Fig. 5: Stage-1 Map-Reduce Framework for BayesWipe.
7.3

Results of This Strategy

In Figure 8a and Figure 8b we can see how this map
reduce strategy helps in reducing the memory footprint
of the reducer. First, we plot the size of the index that
needs to be held in each node as the number of tuples in
the input increases. The topmost curve shows the size of
index in bytes if there was no sharding - as expected, it
increases sharply. The other curves show how the size of
the index in the one of the nodes varies for the same dataset
sizes. From the graph, it can be seen that as the number of
tuples increases, the size of the index grows at a lower rate
when the number of shards is increased. This shows that
increasing the number of reduce nodes is a credible strategy
for distributing the burden of the index.
In the second figure (Figure 8b), we see how the size
of the index varies with the percentage of noise in the
dataset. As expected, when the noise increases, the number

of possible candidate tuples increase (since there are more
variations of each attribute value in the pool). Without
sharding, we see that the size of the dataset increases. While
the increase in the size of the index is not as sharp as the
increase due to the size of the dataset, it is still significant.
Once again, we observe that as the number of shards is
increased, the size of the index in the shard reduces to a
much more manageable value.
Note that a slight downside to this architecture is that a
shuffle/reduce of (T, (Tn‚àó , score)) is needed in the second
stage, and this intermediate data structure can be quite
large. While this leads to some network and temporary
storage overhead, the primary objective of sharding the expensive computation has been achieved by this architecture.

8

E MPIRICAL E VALUATION

We quantitatively study the performance of BayesWipe in
both its modes ‚Äî offline, and online, and compare it against
state-of-the-art CFD approaches. We present experiments
on evaluating the approach in terms of the effectiveness of
data cleaning, efficiency and precision of query rewriting.
A demo for the offline cleaning mode of BayesWipe can be
downloaded from http://bayeswipe.sushovan.de/.
8.1

Datasets

To perform the experiments, we obtained the real data from
the web. The first dataset is Used car sales dataset Dcar
crawled from Google Base. Such ‚Äúdirty‚Äù dataset is referred
0
‚Äù. The second dataset we used was adapted
to as ‚ÄúDcar
from the Census Income dataset Dcensus from the UCI machine learning repository [43]. From the fourteen available
attributes, we picked the attributes that were categorical
in nature, resulting in the following 8 attributes: workingclass, education, marital status, occupation, race, gender,
filing status. country. The same setup was used for both
datasets ‚Äì including parameters and error features.
These datasets were observed to be mostly clean. We
then introduced4 three types of noise to the attributes. To
add noise to an attribute, we randomly changed it either to
a new value which is close in terms of string edit distance
(distance between 1 and 4, simulating spelling errors) or to
a new value which was from the same attribute (simulating
replacement errors) or just delete it (simulating deletion errors). As we have mentioned before, one of the assumptions
of this paper is that the error model is a combination of these
three kinds of errors, and that the errors are independent
of each other. By synthetically generating these errors, we
were able to test our system against a dataset that satisfies
the assumption.
The next dataset tests our system against a real-world
scenario where we do not control the error process, and thus
validates that this assumption was not unrealistic.
To test our system against real-world noise where we
do not have any control over amount, type or behavior
of the noise generation process, we crawled car inventory
data from the website ‚Äòcars.com‚Äô. We manually verified that
the data obtained did, in fact, have a reasonable number of
inaccuracies, making it a suitable candidate for testing our
system.
4. We note that the introduction of synthetic errors into clean data for
experimental evaluation purposes is common practice in data cleaning
research [16], [23].

11

8.2

Experiments

Offline Cleaning Evaluation: The first set of evaluations
shows the effectiveness of the offline cleaning mode. In
Figure 6a, we compare BayesWipe against CFDs [44].
The dotted line that shows the number of CFDs learned
from the noisy data quickly falls to zero, which is not surprising: CFDs learning was designed with a clean training
dataset in mind. Further, the only constraints learned by
this algorithm are the ones that have not been violated in
the dataset ‚Äî unless a tuple violates some CFD, it cannot
be cleaned. As a result, the CFD method cleans exactly
zero tuples independent of the noise percentage. On the
other hand, BayesWipe is able to clean between 20% to
40% of the incorrect values. It is interesting to note that
the percentage of tuples cleaned increases initially and then
slowly decreases, because for very low values of noise, there
aren‚Äôt enough errors for the system to learn a reliable error
model from; and at larger values of noise, the data source
model learned from the noisy data is of poorer quality.
While Figure 6a showed only percentages, in Figure 6b
we report the actual number of tuples cleaned in the dataset
along with the percentage cleaned. This curve shows that
the raw number of tuples cleaned always increases with
higher input noise percentages.
Setting Œ≥ : As explained in Section 5.1, the weight given
to the edit distance (Œ¥ ) compared to the weight given to
the distributional similarity (1 ‚àí Œ¥ ); and the overcorrection
parameter (Œ≥ ) are parameters that can be tuned, and should
be set based on which kind of error is more likely to occur. In
our experiments, we performed a grid search to determine
the best values of Œ¥ and Œ≥ to use. In Figure 6c, we show a
portion of the grid search where Œ¥ = 2/5, and varying Œ≥ .
The ‚Äúvalues corrected‚Äù data points in the graph correspond to the number of erroneous attribute values that
the algorithm successfully corrected (when checked against
the ground truth). The ‚Äúfalse positives‚Äù are the number of
legitimate values that the algorithm changes to an erroneous
value. When cleaning the data, our algorithm chooses a
candidate tuple based on both the prior of the candidate as
well as the likelihood of the correction given the evidence.
Low values of Œ≥ give a higher weight to the prior than
the likelihood, allowing tuples to be changed more easily
to candidates with high prior. The ‚Äúoverall gain‚Äù in the
number of clean values is calculated as the difference of
clean values between the output and input of the algorithm.
If we set the parameter values too low, we will correct
most wrong tuples in the input dataset, but we will also
‚Äòovercorrect‚Äô a larger number of tuples. If the parameters are
set too high, then the system will not correct many errors
‚Äî but the number of ‚Äòovercorrections‚Äô will also be lower.
Based on these experiments, we picked a parameter value
of Œ¥ = 0.638, Œ≥ = 5.8 and kept it constant throughout.
Using probabilistic databases: We empirically evaluate the
PDB-mode of BayesWipe in Figure 7. In the first figure,
we show the performance of the PDB mode of BayesWipe against the deterministic mode for specific queries.
As we can see from the first, third and seventh queries,
the BayesWipe-PDB improves the recall without any loss
of precision. However, in most cases (and on average),
BayesWipe-PDB provides a better recall at the cost of some
precision.

The second figure shows the performance of BayesWipePDB as the probability threshold for inclusion of a tuple in
the resultset is varied. As expected, with low values of the
threshold, the system allows most tuples into the resultset,
thus showing high recall and low precision. As the threshold
increased, the precision increases, but the recall falls.
In Figure 7c, we compare the precision of the PDB mode
using top-k determinization against the deterministic mode
of BayesWipe. As expected, both the modes show high precision for low values of k , indicating that the initial results
are clean and relevant to the user. For higher values of k ,
the PDB precision falls off, indicating that PDB methods
are more useful for scenarios where high recall is important
without sacrificing too much precision.
Online Query Processing: While in the offline mode, we
had the luxury of changing the tuples in the database itself,
in online query processing, we use query rewriting to obtain
a resultset that is similar to the offline results, without
modification to the database. We consider an SQL select
query system as our baseline. We evaluate the precision and
recall of our method against the ground truth and compare
it with the baseline, using randomly generated queries.
We issued randomly generated queries to both BayesWipe and the baseline system. Figure 8c shows the average
precision over 10 queries at various recall values. It shows
that our system outperforms the SQL select query system
in top-k precision, especially since our system considers the
relevance of the results when ranking them. On the other
hand, the SQL search approach is oblivious to ranking and
returns all tuples that satisfy the user query. Thus it may
return irrelevant tuples early on, leading to less precision.
Figure 8d shows the improvement in the absolute numbers of tuples returned by the BayesWipe system. The graph
shows the number of true positive tuples returned (tuples
that match the query results from the ground truth) minus
the number of false positives (tuples that are returned but do
not appear in the ground truth result set). We also plot the
number of true positive results from the ground truth, which
is the theoretical maximum that any algorithm can achieve.
The graph shows that the BayesWipe system outperforms
the SQL query system at nearly every level of noise. Further,
the graph also illustrates that ‚Äî compared to an SQL query
baseline ‚Äî BayesWipe closes the gap to the maximum
possible number of tuples to a large extent. In addition to
showing the performance of BayesWipe against the SQL
query baseline, we also show the performance of BayesWipe
without the query relaxation part (called BW-exp5 ). We can
see that the full BayesWipe system outperforms the BW-exp
system significantly, showing that query relaxation plays an
important role in bringing relevant tuples to the resultset,
especially for higher values of noise.
This shows that our proposed query ranking strategy indeed captures the expected relevance of the to-be-retrieved
tuples, and the query rewriting module is able to generate
the highly ranked queries.
Efficiency: In Figure 10 we show the data cleaning time
taken by the system in its various modes. The first two
graphs show the offline mode, and the second two show the
5. BW-exp stands for BayesWipe-expanded, since the only query
rewriting operation done is query expansion.

CFD

#CFDs

40%

4

30%
20%

2

10%
0%

40%

30%

2000

20%

1000

10%

0

5 10 15 20 25 30 35 40
Noise Percent

4

5

200

Values Corrected

160

False Positives

120

Cleanliness Gain

80
40
0
2

0%
3

(a) % performance of BayesWipe compared to CFD, for the used-car dataset.

50%

3000

0

0

Net Tuples Cleaned
Percent Cleaned

4000

6

Number of tuples

BayesWipe

50%

Num CFDs learned

% Tuples Cleaned

12

2.5

3

3.5

4

4.5

5

5.5

6

Distributional Similarity Weight

10 15 20 25 30 35

Percentage of noise

(b) % net corrupt values cleaned, car
database

(c) Net corrections vs Œ≥ . (The x-axis values show the un-normalized distributional similarity weight, which is simply
Œ≥ √ó 3/5.)

1
0.75
0.5
0.25
0
1
0.75
0.5
0.25
0

1

1

0.8

0.8

0.6

PDB Precision

0.4

PDB Recall

0.2

model =
outlander
sports

cartype =
sedan

make = bmw & model = jetta
condition =
used

BayesWipe-PDB Precision

model =
cooper s

model = h3
mini

Average

BayesWipe-DET Precision

(a) Precision and recall of the PDB method shown against the deterministic method for specific queries.

0.6
Deterministic Precision
PDB Precision

0.4

0.2

0

make = acura

Precision

PRECISION

RECALL

Fig. 6: Offline cleaning mode of BayesWipe

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Threshold

0
0

25

50
top-K

75

100

(b) Precision and recall of
(c) top-k precision of PDB
the PDB method using a
vs deterministic method.
threshold.

Fig. 7: Results of probabilistic method.
online mode. As can be seen from the graphs, BayesWipe
performs reasonably well both in datasets of large size and
datasets with large noise.
Evaluation on real data with naturally occurring errors: In
this section we used a dataset of 1.2 million tuples crawled
from the cars.com website6 to check the performance of
the system with real-world data, where the corruptions
were not synthetically introduced. Since this data is large,
and the noise is completely naturally occurring, we do not
have ground truth for this data. To evaluate this system,
we conducted an experiment on Amazon Mechanical Turk.
First, we ran the offline mode of BayesWipe on the entire database. We then picked only those tuples that were
changed during the cleaning, and then created an interface
in mechanical turk where only those tuples were shown to
the user in random order. Due to resource constraints, the
experiment was run with the first 200 tuples that the system
found to be unclean. We inserted 3 known answers into
the questionnaire, and removed any responses that failed
to annotate at least 2 out of the 3 answers correctly.
An example is shown in Figure 11. The turker is presented with two cars, and she does not know which of
the cars was originally present in the dirty dataset, and
which one was produced by BayesWipe. The turker will use
her own domain knowledge, or perform a web search and
discover that a Mazda CX-9 touring is only available in a 3.7l
engine, not a 3.5l. Then the turker will be able to declare the
second tuple as the correct option with high confidence.
6. http://www.cars.com

The results of this experiment are shown in Table 4. As
we can see, the users consistently picked the tuples cleaned
by BayesWipe more favorably compared to the original
dirty tuples, proving that it is indeed effective in real-world
datasets. Notice that it is not trivial to obtain a 56% rate of
success in these experiments. Finding a tuple which convinces the turkers that it is better than the original requires
searching through a huge space of possible corrections. An
algorithm that picks a possible correction randomly from
this space is likely to get a near 0% accuracy.
The first row of Table 4 shows the fraction of tuples for
which the turkers picked the version cleaned by BayesWipe
and indicated that they were either ‚Äòvery confident‚Äô or
‚Äòconfident‚Äô. The second row shows the fraction of tuples for
all turker confidence values, and therefore is a less reliable
indicator of success.
In order to show the efficacy of BayesWipe we also
performed an experiment in which the same tuples (the
ones that BayesWipe had changed) were modified by a random perturbation. The random perturbation was done by
the same error process as described before (typo, deletion,
substitution with equal probability). Then these tuples (the
original tuple from the database and the perturbed tuple)
were presented as two choices to the turkers. The preference
by the turkers for the randomly perturbed tuple over the
original dirty tuple is shown in the third column, ‚ÄòRandom‚Äô.
It is obvious from this that the turkers overwhelmingly do
not favor the random perturbed tuples. This demonstrates
two things. First, it shows the fact that BayesWipe was
performing useful cleaning of the tuples. In fact, BayesWipe
shows a tenfold improvement over the random perturbation

13
1600000

none

2

3

4

5

300000
none

2

3

4

5

250000

1200000

Bytes in one shard

Bytes in one shard

1400000

1000000

800000
600000
400000

200000
150000
100000

200000

50000

0
1

20

40

80
100
number of tuples

120

0

140

0

5

10

15
20
25
Noise percentage

30

35

40

(a) vs the Number of Tuples (in
(b) vs the Noise in the Dataset, for
Thousands) in the Dataset, for VarVarious Number of Shards.
ious Number of Shards.

Fig. 8: Map-Reduce index sizes
1000

Precision

1.00

980
960

.95
SQL Select Query

.90

940
920

BayesWipe Online
Query Processing

.85
.01

.03

.05

.07 .09
Recall

.11

900
1

.13

2

BW

3

4

BW-exp

5

10 15 20 25 30 35
Noise %
SQL
Ground truth

(d) Net improvement in data quality
(TP-FP)

(c) Average precision vs recall

150

car-offline

800
600

census

400

car

200

100

10k 15k 20k 25k
Number of tuples

30k

car-online

census-online

50

0

10
20
30
Percentage of noise

census-online
50
0

5k

40

car-online

100

0

0

5k

150
Time taken (s)

census-offline

Time Taken (s)

1000
Time taken (s)

Time Taken (s)

Fig. 9: Online cleaning mode of BayesWipe
600
500
400
300
200
100
0

10k 15k 20k 25k
Number of tuples

30k

0

10
20
30
Percentage of noise

40

(a) Time taken vs number of (b) Time taken vs noise per- (c) Time taken vs number of (d) Time taken vs noise pertuples in offline mode
centage in offline mode
tuples in online mode
centage in online mode

Fig. 10: Performance evaluations
Confidence
High confidence only
All
confidence
values

BayesWipe

Original

Random

56.3%

43.6%

5.5%

53.3%

46.7%

12.4%

Increase over Random
50.8% points
...
(10x better)

make

model

cx-9
Car: mazda
touring

40.9% points
(4x better)

cx-9
Car: mazda
touring

TABLE 4: Results of the Mechanical Turk Experiment, showing the percentage of tuples for which the users picked the
results obtained by BayesWipe as against the original tuple.
Also shows performance against a random modification.

cartype fueltype

engine

transmission

drivetrain doors wheelbase

suv

3.5l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113‚Äù

suv

3.7l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113"

ÔÇ° First is correct
ÔÇ° Second is correct
How confident are you about your selection?
ÔÇ° Very confident ÔÇ° Confident ÔÇ° Slightly confident ÔÇ° Slightly Unsure ÔÇ° Totally Unsure

Fig. 11: A fragment of the questionnaire provided to the
Mechanical Turk workers.
model, as judged by human turkers. This shows that in
the large space of possible modifications of a wrong tuple,
BayesWipe picks the correct one most of the time. Second, it
provides additional support for the fact that the turkers are
picking the tuple carefully, and are not randomly submitting
their responses.
In this experiment, we also found the average fraction
of known answers that the turkers gave wrong answers to.
This value was 8%. This leads to the conclusion that the
difference between the turker‚Äôs preference of BayesWipe
over both the original tuples (which is 12%) and the random
perturbation (which is 50%) are both significant.

9

C ONCLUSION

In this paper we presented a novel system, BayesWipe
that works using end-to-end probabilistic semantics, and
without access to clean master data. We showed how to
effectively learn the data source model as a Bayes network,
and how to model the error as a mixture of error features.
We showed the operation of this system in two modalities:
(1) offline data cleaning, an in situ rectification of data and
(2) online query processing mode, a highly efficient way to
obtain clean query results over inconsistent data. There is
an option to generate a standard, deterministic database as
the output, as well as a probabilistic database, where all

14

the alternatives are preserved for further processing. We
empirically showed that BayesWipe outperformed existing
baseline techniques in quality of results, and was highly
efficient. We also showed the performance of the BayesWipe
system at various stages of the query rewriting operation.
We demonstrated how BayesWipe can be run on the mapreduce architecture so that it can scale to huge data sizes.
User experiments showed that the system is useful in cleaning real-world noisy data.

R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

W. Fan and F. Geerts, ‚ÄúFoundations of data quality management,‚Äù
Synthesis Lectures on Data Management, 2012.
P. Gray, ‚ÄúBefore Big Data, clean data,‚Äù 2013. [Online]. Available:
http://www.techrepublic.com/blog/big-data-analytics/
before-big-data-clean-data/
H. Leslie, ‚ÄúHealth data quality ‚Äì a two-edged sword,‚Äù 2010.
[Online]. Available: http://omowizard.wordpress.com/2010/02/
21/health-data-quality-a-two-edged-sword/
Computing Research Association, ‚ÄúChallenges and opportunities
with big data,‚Äù http://cra.org/ccc/docs/init/bigdatawhitepaper.
pdf, 2012.
E. Knorr, R. Ng, and V. Tucakov, ‚ÄúDistance-based outliers: algorithms and applications,‚Äù VLDB, 2000.
H. Xiong, G. Pandey, M. Steinbach, and V. Kumar, ‚ÄúEnhancing
data analysis with noise removal,‚Äù TKDE, 2006.
P. Singla and P. Domingos, ‚ÄúEntity resolution with markov logic,‚Äù
in ICDM, 2006.
I. Fellegi and D. Holt, ‚ÄúA systematic approach to automatic edit
and imputation,‚Äù J. American Statistical association, 1976.
P. Bohannon, W. Fan, M. Flaster, and R. Rastogi, ‚ÄúA cost-based
model and effective heuristic for repairing constraints by value
modification,‚Äù in SIGMOD, 2005.
W. Fan, F. Geerts, L. Lakshmanan, and M. Xiong, ‚ÄúDiscovering
conditional functional dependencies,‚Äù ICDE, 2009.
L. E. Bertossi, S. Kolahi, and L. V. S. Lakshmanan, ‚ÄúData cleaning
and query answering with matching dependencies and matching
functions,‚Äù ICDT, 2011.
A. Fuxman, E. Fazli, and R. J. Miller, ‚ÄúConquer: Efficient management of inconsistent databases,‚Äù SIGMOD, 2005.
S. De, Y. Hu, Y. Chen, and S. Kambhampati, ‚ÄúBayeswipe: A multimodal system for data cleaning and consistent query answering
on structured bigdata,‚Äù IEEE Big Data, 2014.
G. Wolf, A. Kalavagattu, H. Khatri, R. Balakrishnan, B. Chokshi,
J. Fan, Y. Chen, and S. Kambhampati, ‚ÄúQuery processing over
incomplete autonomous databases: query rewriting using learned
data dependencies,‚Äù VLDB, 2009.
S. De, ‚ÄúUnsupervised bayesian data cleaning techniques for structured data,‚Äù Ph.D. dissertation, ASU, 2014.
P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis,
‚ÄúConditional functional dependencies for data cleaning,‚Äù ICDE,
2007.
W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, ‚ÄúConditional functional dependencies for capturing data inconsistencies,‚Äù
TODS, 2008.
J. Wang and N. Tang, ‚ÄúTowards dependable data repairing with
fixing rules,‚ÄùSIGMOD, 2014.
J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg, T. Kraska, and
T. Milo, ‚ÄúA sample-and-clean framework for fast and accurate
query processing on dirty data,‚Äù SIGMOD, 2014.
L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu, ‚ÄúOn
generating near-optimal tableaux for conditional functional dependencies,‚Äù PVLDB, 2008.
G. Cormode, L. Golab, K. Flip, A. McGregor, D. Srivastava, and
X. Zhang, ‚ÄúEstimating the confidence of conditional functional
dependencies,‚Äù in SIGMOD, 2009.
M. Arenas, L. Bertossi, and J. Chomicki, ‚ÄúConsistent query answers in inconsistent databases,‚Äù PODS, 1999.
G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma, ‚ÄúImproving data
quality: Consistency and accuracy,‚Äù VLDB, 2007.
M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F.
Ilyas, ‚ÄúGuided data repair,‚Äù VLDB, 2011.
G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúSampling from
repairs of conditional functional dependency violations,‚Äù VLDB,
2013.

[26] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, ‚ÄúTowards certain fixes
with editing rules and master data,‚Äù VLDB, 2010.
[27] G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúOn the relative
trust between inconsistent data and inaccurate constraints,‚Äù ICDE,
2013.
[28] X. L. Dong, L. Berti-Equille, and D. Srivastava, ‚ÄúTruth discovery
and copying detection in a dynamic world,‚Äù VLDB,2009.
[29] T. Dasu and J. M. Loh, ‚ÄúStatistical distortion: Consequences of data
cleaning,‚Äù VLDB, 2012.
[30] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and
Y. Ye, ‚ÄúKatara: A data cleaning system powered by knowledge
bases and crowdsourcing,‚Äù SIGMOD, 2015.
[31] J. Wang, T. Kraska, M. J. Franklin, and J. Feng, ‚ÄúCrowder: Crowdsourcing entity resolution,‚Äù VLDB, 2012.
[32] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng, ‚ÄúQasca: A qualityaware task assignment system for crowdsourcing applications,‚Äù
SIGMOD, 2015.
[33] A. Hartemink., ‚ÄúBanjo: Bayesian network inference with java
objects.‚Äù http://www.cs.duke.edu/‚àºamink/software/banjo.
[34] S. Russell and P. Norvig, Artificial intelligence: a modern approach.
Prentice Hall, 2010.
[35] T. Minka, W. J.M., J. Guiver, and D. Knowles, ‚ÄúInfer.NET 2.4‚Äù,
Microsoft Research Cambridge, 2010. http://research.microsoft.
com/infernet.
[36] E. Ristad and P. Yianilos, ‚ÄúLearning string-edit distance,‚Äù Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 1998.
[37] M. Li, Y. Zhang, M. Zhu, and M. Zhou, ‚ÄúExploring distributional
similarity based models for query spelling correction,‚Äù ICCL, 2006.
[38] A. Berger, V. Pietra, and S. Pietra, ‚ÄúA maximum entropy approach
to natural language processing,‚Äù Computational linguistics, 1996.
[39] N. Dalvi and D. Suciu, ‚ÄúEfficient query evaluation on probabilistic
databases,‚Äù VLDB, 2004.
[40] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, and D. Suciu,
‚ÄúMystiq: a system for finding more answers by using probabilities,‚Äù SIGMOD, 2005.
[41] D. Suciu and N. Dalvi, ‚ÄúFoundations of probabilistic answers to
queries,‚Äù SIGMOD, 2005.
[42] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann Publishers, 1988.
[43] A. Asuncion and D. Newman, ‚ÄúUCI machine learning repository,‚Äù
2007.
[44] F. Chiang and R. Miller, ‚ÄúDiscovering data quality rules,‚Äù VLDB,
2008.
Sushovan De is currently employed at Google. He graduated with a
Ph.D. in Computer Science from Arizona State University. His research
interests comprise Information Retrieval, Data Cleaning, and Probabilistic Databases.
Yuheng Hu is a Research Staff Member in the USER (User Systems
and Experience Research) group at IBM Research - Almaden. He
obtained his Ph.D in Computer Science at Arizona State University.
His research interests are in the areas of Machine Learning, Social
Computing and Human-Computer Interaction.
Meduri Venkata Vamsikrishna is a Ph.D student in Computer Science
at Arizona State University. He received a Master‚Äôs degree in Computer
Science from the National Unviersity of Singapore. His research interests include Data Mining from Social Media and Data Cleaning.
Yi Chen is an associate professor and the Henry J. Leir Chair in
Healthcare in the School of Management with a joint appointment in the
College of Computing Sciences at New Jersey Institute of Technology
(NJIT). She received her Ph.D. degree in Computer Science from the
University of Pennsylvania. Her current research focuses on Information
Discovery on Big Data, Social Computing and Information Integration.
Subbarao Kambhampati is a professor in Computer Science at Arizona
State University. He directs the Yochan research group which is associated with the Artifical Intelligence Lab at Arizona State University. He
is the ‚ÄùPresident-elect‚Äù of AAAI, the Association for the Advancement of
Artificial Intelligence. He secured a Ph.D. degree in Computer Science
from the University of Maryland, College Park. His research interests are
Automated Planning in Artificial Intelligence and Data and Information
Integration on the Web

1

BayesWipe: A Scalable Probabilistic Framework
for Cleaning BigData

arXiv:1506.08908v1 [cs.DB] 30 Jun 2015

Sushovan De, Yuheng Hu, Meduri Venkata Vamsikrishna, Yi Chen, and Subbarao Kambhampati
Abstract‚ÄîRecent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record
matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in
tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like CFDs
(which have to be provided by domain experts, or learned from a clean sample of the database). In this paper, we provide a method for
correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned
from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently
perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable.
We evaluate our methods over both synthetic and real data.
Index Terms‚Äîdatabases; web databases; data cleaning; query rewriting; uncertainty

F

1

I NTRODUCTION

A

LTHOUGH data cleaning has been a long standing
problem, it has become critically important again because of the increased interest in big data and web data.
Most of the focus of the work on big data has been on
the volume, velocity, or variety of the data; however, an
important part of making big data useful is to ensure the
veracity of the data. Enterprise data is known to have a
typical error rate of 1‚Äì5% [1] (error rates of up to 30% have
been observed). This has led to renewed interest in cleaning
of big data sources, where manual data cleansing tasks are
seen as prohibitively expensive and time-consuming [2],
or the data has been generated by users and cannot be
implicitly trusted [3]. Among the various types of big data,
the need to efficiently handle large scaled structured data
that is rife with inconsistency and incompleteness is also
more significant than ever. Indeed, multiple studies, such as
[4] emphasize the importance of effective, efficient methods
for handling ‚Äúdirty big data‚Äù.

TABLE 1: A snapshot of car data extracted from cars.com
using information extraction techniques
TID
t1
t2
t3
t4
t5
t6

Model
Civic
Focus
Civik
Civic
Accord

Make
Honda
Ford
Honda
Ford
Honda
Honda

Orig
JPN
USA
JPN
USA
JPN
JPN

Size
Mid-size
Compact
Mid-size
Compact
Mid-size
Full-size

Engine
V4
V4
V4
V4
V4
V6

Condition
NEW
USED
USED
USED
NEW
NEW

Most of the current techniques are based on deterministic rules, which have a number of problems: Suppose that
the user is interested in finding ‚ÄòCivic‚Äô cars from Table 1.
‚Ä¢

This work was done when all the authors were with the Department of
Computer Science & Engineering at Arizona State University, Tempe,
AZ 85287. Sushovan De is now with Google Inc. Yuheng Hu is now with
IBM Research, Almaden. Yi Chen is now with the School of Management
and the College of Computing at New Jersey Institute of Technology.

Traditional data retrieval systems would return tuples t1
and t4 for the query, because they are the only ones that
are a match for the query term. Thus, they completely miss
the fact that t4 is in fact a dirty tuple ‚Äî A Ford Focus car
mislabeled as a Civic. Additionally, tuple t3 and t5 would
not be returned as a result tuples since they have a typos
or missing values, although they represent desirable results.
The objective of this work is to provide the true result set
(t1 , t3 , t5 ) to the user.
Although this problem has received significant attention
over the years in the traditional database literature, the stateof-the-art approaches fall far short of an effective solution
for big data and web data. Traditional methods include
outlier detection [5], noise removal [6], entity resolution [6],
[7], and imputation [8]. Although these methods are efficient
in their own scenarios, their dependence on clean master
data is a significant drawback.
Specifically, state of the art approaches (e.g., [9], [10],
[11]) attempt to clean data by exploiting patterns in the data,
which they express in the form of conditional functional
dependencies (or CFDs). In the motivating example, the fact
that Honda cars have ‚ÄòJPN‚Äô as the origin of the manufacturer
would be an example of such a pattern. However, these
approaches depend on the availability of a clean data corpus
or an external reference table to learn data quality rules or
patterns before fixing the errors in the dirty data. Systems
such as ConQuer [12] depend upon a set of clean constraints
provided by the user. Such clean corpora or constraints
may be easy to establish in a tightly controlled enterprise
environment but are infeasible for web data and big data.
One may attempt to learn data quality rules directly from
the noisy data. Unfortunately however, our experimental
evaluation shows that even small amounts of noise severely
impairs the ability to learn useful constraints from the data.
To avoid dependence on clean master data, we propose
a novel system called BayesWipe [13] that assumes that
a statistical process underlies the generation of clean data
(which we call the data source model) as well as the cor-

2

ruption of data (which we call the data error model). The
noisy data itself is used to learn the parameters of these
the generative and error models, eliminating dependence
on clean master data. Then, by treating the clean value as
a latent random variable, BayesWipe leverages these two
learned models and automatically infers its value through a
Bayesian estimation.
We designed BayesWipe so that it can be used in two different modes: a traditional offline cleaning mode, and a novel
online query processing mode. The offline cleaning mode of
BayesWipe follows the classical data cleaning model, where
the entire database is accessible and can be cleaned in situ.
This mode is particularly useful when one has complete
control over the data, and a one-time cleaning of the data is
needed. Data warehousing scenarios such as data crawled
from the web, or aggregated from various noisy sources
can be effectively cleaned in this mode. The cleaned data
can be stored either in a deterministic database, or in a
probabilistic database. If a probabilistic database is chosen
as the output mode, BayesWipe stores not only the clean
version of the tuple it believes to be most likely correct
one, but the entire distribution over possible clean tuples.
The choice of a probabilistic output mode for the cleaned
tuples is most useful for those scenarios where recall is very
important for further data processing on the cleaned tuples.
One of the features of the offline mode of BayesWipe
is that a probabilistic database (PDB) can be generated as
a result of the data cleaning. In the first instance, notice
that BayesWipe was built for deterministic databases. It
can operate on a deterministic database and produce a
probabilistic cleaned database as an output. Probabilistic
databases are complex and unintuitive, because each single
input tuple is mapped into a distribution over resulting
clean alternatives. We show how the top-k results can be
retrieved from a PDB while displaying the clean data that is
comprehensible to the user.
The online query processing mode of BayesWipe is motivated by web data scenarios where it is impractical to create
a local copy of the data and clean it offline, either due to
large size, high frequency of change, or access restrictions. In
such cases, the best way to obtain clean answers is to clean
the resultset as we retrieve it, which also provides us the
opportunity of improving the efficiency of the system, since
we can now ignore entire portions of the database which are
likely to be unclean or irrelevant to the top-k . BayesWipe
uses a query rewriting system that enables it to efficiently
retrieve only those tuples that are important to the top-k
result set. This rewriting approach is inspired by, and is a
significant extension of our earlier work on QPIAD system
for handling data incompleteness [14]. In big data scenarios,
clean master data is rarely available, and write access is
either unavailable, or undesirable due to the efficiency and
indexing concerns. The online mode is particularly suited to
get clean results in such results.
We implement BayesWipe in a Map-Reduce architecture,
so that we can run it very quickly for massive datasets. The
architecture for parallelizing BayesWipe is explained more
fully in Sec 7. In short, there is a two-stage map-reduce
architecture, where in the first stage, the dirty tuples are
routed to a set of reducer nodes which hold the relevant
candidate clean tuples for them. In the second stage, the

resulting candidate clean tuples along with their scores are
collated, and the best replacement tuple is selected from
them.
To summarize our contributions, we:
‚Ä¢
‚Ä¢

‚Ä¢
‚Ä¢
‚Ä¢

Propose that data cleaning should be done using a
principled, probabilistic approach.
Develop a novel algorithm following those principles, which uses a Bayes network as the generative
model and maximum entropy as the error model of
the data.
Develop novel query rewriting techniques so that
this algorithm can also be used in a big data scenario.
Develop a parallelized version of this algorithm using map-reduce framework.
Empirically evaluate the performance of our algorithm using both controlled and real datasets.

The rest of the paper is organized as follows. We begin by discussing the related work and then describe the
architecture of BayesWipe in the next section, where we
also present the overall algorithm. Section 4 describes the
learning phase of BayesWipe, where we find the generative
and error models. Section 5 describes the offline cleaning
mode, and the next section details the query rewriting
and online data processing. We describe the parallelized
version of BayesWipe in Section 7 and the results of the
our empirical evaluation in Section 8, and then conclude
by summarizing our contributions. Further details about
BayesWipe can be found in the thesis [15].

2

R ELATED W ORK

Much of the work in data cleaning focused on deterministic
dependency relations such as FD, CFD, and INDs. Bohannon et al. proposed using Conditional Functional Dependencies (CFD) to clean data [16], [17]. Indeed, CFDs are
very effective in cleaning data. However, the precision and
recall of cleaning data with CFDs completely depends on
the quality of the set of dependencies used for the cleaning.
As our experiments show, learning CFDs from dirty data
produces very unsatisfactory results. In order for CFDbased methods to perform well, they need to be learned
from a clean sample of the database [10] which must be
large enough to be representative of all the patterns in the
data. Finding such a large corpus of clean master data is a
non-trivial problem, and is infeasible in all but the most
controlled of environments (like a corporation with high
quality data).
A recent variant on the deterministic dependency based
cleaning by J.Wang et al. [18] proposes using fixing rules
which contain negative(possible errors) and positive(clean
replacements) patterns for an attribute. However, there can
be several ways in which a tuple can go wrong and the
detection of the positive pattern requires clean master data.
BayesWipe on the other hand uses an error model to detect
errors automatically and clean them in the absence of clean
master data. Recent work by J.Wang et al. [19] plugs in one
of the rule based cleaning techniques to clean a sample of
the entire data and use it as a guideline to clean the entire
data. It is important to note that this method only caters
to aggregate numerical queries whereas the online mode

3

of BayesWipe supports all types of SQL queries (not just
aggregates) and returns clean result tuples.
Although it is possible to ameliorate some of the difficulties of CFD/AFD methods by considering approximate versions of them, the work in the uncertainty in AI
community demonstrated the semantic pitfalls of handling
uncertainty in this way. In particular, approximate versions
of CFDs/AFDs considered in works such as [20], [21] are
similar to the certainty factors approaches for handling
uncertainty that were popular in the heyday of expert
systems, but whose semantic inconsistencies are by now
well-established (see, for example, Section 14.7.1 of [34]).
Because of this, in this paper we focus on a more systematic
probabilistic approach.
Even if a curated set of integrity constraints are provided, existing methods do not use a probabilistically principled method of choosing a candidate correction. They resort
to either heuristic based methods, finding an approximate
algorithm for the least-cost repair of the database [9], [22],
[23]; using a human-guided repair [24], or sampling from
a space of possible repairs [25]. There has been work that
attempts to guarantee a correct repair of the database [26],
but they can only provide guarantees for corrections of those
tuples that are supported by data from a perfectly clean
master database. Recently, [27] have shown how the relative
trust one places on the constraints and the data itself plays
into the choice of cleaning tuples. A Bayesian source model
of data was used by [28], but was limited in scope to figuring
out the evolution over time of the data value.
Recent work has also focused on the metrics to use to
evaluate data cleaning techniques [29]. In this work, we
focus on evaluating our method against the ground truth
(when the ground truth is known), and user studies (when
the ground truth is not known).
While BayesWipe uses crowdsourcing to evaluate the
accuracy of the proposed clean tuple alternatives for the
experiments on real world datasets, there are other systems that try to use the crowd for cleaning the data itself.
X.Chu et al. [30] clean the database tuples by discovering
patterns that overlap with Knowledge Base(KB)s like Yago
and validating the top-k candidates using the crowd. J.Wang
et al. [31] perform entity resolution (which is to identify
several values corresponding to the same entity value) using
crowdsourcing. They reduce the complexity of the number
of HIT(Human Intelligence Task)s generated by clustering
them into several bins so that a set of pairs can be resolved
at a time as against evaluating one pair at a time. Y.Zheng
et al. [32] pick a set of k questions to be included in the HITs
for the human workers out of a total set of n questions using
estimates on the expected increase in the answer quality by
assigning those questions to the crowd. Crowdsourcing to
perform data cleaning may be infeasible in the context of Big
Data cleaning targeted by BayesWipe . However, suggestions from the crowd can be used to provide cleaner master
data from which BayesWipe learns the Bayes network.
The query rewriting part of this work is inspired
by the QPIAD system [14], but significantly improves
upon it. QPIAD performed query rewriting over incomplete databases using approximate functional dependencies
(AFD), and only cleaned data with null values, not wrong
values.

Offline Cleaning

Model Learning
Data source
model
Error
Model

Cleaning to a
Deterministic DB
or
Cleaning to a
Probabilistic DB

Clean
Data

OR
Candidate
Set Index

Query Processing
Query
Rewriting

Database
Sampler

Result
Ranking

Data Source

Fig. 1: The architecture of BayesWipe. Our framework learns
both data source model and error model from the raw data
during the model learning phase. It can perform offline
cleaning or query processing to provide clean data.

3

BAYES W IPE OVERVIEW

BayesWipe views the data cleaning problem as a statistical
inference problem over the structured text data. Let D =
{T1 , ..., Tn } be the input structured data which contains a
number of corruptions. Ti ‚àà D is a tuple with m attributes
{A1 , ..., Am } which may have one or more corruptions in
its attribute values. Given a candidate replacement set C
for a possibly corrupted tuple T in D, we can clean the
database by replacing T with the candidate clean tuple
T ‚àó ‚àà C that has the maximum Pr(T ‚àó |T ). Using Bayes rule
(and dropping the common denominator), we can rewrite
this to
‚àó
Tbest
= arg max[Pr(T |T ‚àó )Pr(T ‚àó )]

(1)

‚àó
By replacing T with Tbest
, we get a deterministic database.
If we wish to create a probabilistic database (PDB), we don‚Äôt
take an arg max over the Pr(T ‚àó |T ), instead we store the
entire distribution over the T ‚àó in the resulting PDB.
For online query processing we take the user query Q‚àó ,
and find the relevance score of a tuple T as
X
Score(T ) =
Pr(T ‚àó ) Pr(T |T ‚àó ) R(T ‚àó |Q‚àó )
(2)
| {z } | {z } | {z }
‚àó
T ‚ààC

source model error model

relevance

In this work, we used a binary relevance model, where R
is 1 if T ‚àó is relevant to the user‚Äôs query, and 0 otherwise.
Note that R is the relevance of the query Q‚àó to the candidate
clean tuple T ‚àó and not the observed tuple T . This allows the
query rewriting phase of BayesWipe which aims to retrieve
tuples with the highest Score(.) to achieve the non-lossy
effect of using a PDB without explicitly rectifying the entire
database.
Architecture:
Figure 1 shows the system architecture for BayesWipe.
During the model learning phase (Section 4), we first obtain
a sample database by sending some queries to the database.
On this sample data, we learn the generative model of the
data as a Bayes network (Section 4.1). In parallel, we define
and learn an error model which incorporates common kinds

4

of errors (Section 4.2). We also create an index to quickly
propose candidate T ‚àó s.
We can then choose to do either offline cleaning (Section 5) or online query processing (Section 6), as per the
scenario. In the offline cleaning mode, we iterate over all
the tuples in the database and clean them one by one. We
can choose whether to store the resulting cleaned tuple in a
deterministic database (where we store only the T ‚àó with the
maximum posterior probability) or probabilistic database
(where we store the entire distribution over the T ‚àó ). In the
online query processing mode, we obtain a query from the
user, and do query rewriting in order to find a set of queries
that are likely to retrieve a set of highly relevant tuples.
We execute these queries and re-rank the results, and then
display them.
Algorithm 1: The algorithm for offline data cleaning
Input: D, the dirty dataset.
BN ‚Üê Learn Bayes Network (D)
foreach Tuple T ‚àà D do
C ‚Üê Find Candidate Replacements (T )
foreach Candidate T ‚àó ‚àà C do
P (T ‚àó ) ‚Üê Find Joint Probability (T ‚àó , BN )
P (T |T ‚àó ) ‚Üê Error Model (T, T ‚àó )
end
T ‚Üê arg max
P (T ‚àó )P (T |T ‚àó )
‚àó
T ‚ààC

end

Algorithm 2: Algorithm for online query processing.
Input: D, the dirty dataset
Input: Q, the user‚Äôs query
S ‚Üê Sample the source dataset D
BN ‚Üê Learn Bayes Network (S )
ES ‚Üê Learn Error Statistics (S )
R ‚Üê Query and score results (Q, D, BN )
ESQ ‚Üê Get expanded queries (Q)
foreach Expanded query E ‚àà ESQ do
R ‚Üê R ‚à™ Query and score results (E, D, BN )
RQ ‚Üê RQ ‚à™ Get all relaxed queries (E )
end
Sort(RQ) by expected relevance, using ES
while top-k confidence not attained do
B ‚Üê Pick and remove top RQ
R ‚Üê R ‚à™ Query and score results (B, D, BN )
end
Sort(R) by score
return R
In Algorithms 1 and 2, we present the overall algorithm
for BayesWipe. In the offline mode, we show how we iterate
over all the tuples in the dirty database, D and replace them
with cleaned tuples. In the query processing mode, the first
three operations are performed offline, and the remaining
operations show how the tuples are efficiently retrieved
from the database, ranked and displayed to the user.

4

M ODEL L EARNING

This section details the process by which we estimate the
components of Equation 2: the data source model Pr(T ‚àó )

Make

Condition

occupation

Model

Year

gender

workingclass

Door

Drivetrain

country

race
education

marital status

Engine

Car Type

(a) Auto dataset

filing-status

(b) Census dataset

Fig. 2: The learned Bayes networks
and the error model Pr(T |T ‚àó )
4.1

Data Source Model

The data that we work with can have dependencies among
various attributes (e.g., a car‚Äôs engine depends on its make).
Therefore, we represent the data source model as a Bayes
network, since it naturally captures relationships between
the attributes via structure learning and infers probability
distributions over values of the input tuples.
Constructing a Bayes network over D requires two steps:
first, the induction of the graph structure of the network,
which encodes the conditional independences between the
m attributes of D‚Äôs schema; and second, the estimation
of the parameters of the resulting network. The resulting
model allows us to compute probability distributions over
an arbitrary input tuple T .
Whenever the underlying patterns in the source database
changes, we have to learn the structure and parameters
of the Bayes network again. In our scenario, we observed
that the structure of a Bayes network of a given dataset
remains constant with small perturbations, but the parameters (CPTs) change more frequently. As a result, we spend a
larger amount of time learning the structure of the network
with a slower, but more accurate tool, Banjo [33]. Figures
2a and 2b show automatically learned structures for two
data domains. The learned structure seems to be intuitively
correct, since the nodes that are connected (for example,
‚Äòcountry‚Äô and ‚Äòrace‚Äô in Figure 2b) are expected to be highly
correlated1 .
Then, given a learned graphical structure G of D, we
can estimate the conditional probability tables (CPTs) that
parameterize each node in G using a faster package called
Infer.NET [35]. This process of inferring the parameters is
run offline, but more frequently than the structure learning.
Once the Bayesian network is constructed, we can infer
the joint distributions for arbitrary tuple T , which can
be decomposed to the multiplication of several marginal
distributions of the sets of random variables, conditioned
on their parent nodes depending on G .
4.2

Error Model

Having described the data source model, we now turn to
the estimation of the error model Pr(T |T ‚àó ) from noisy data.
There are many types of errors that can occur in data. We
focus on the most common types of errors that occur in data
1. Note that the direction of the arrow in a Bayes network does not
necessarily determine causality, see Chapter 14 from Russel and Norvig
[34].

5

that is manually entered by naƒ±Ãàve users: typos, deletions,
and substitution of one word with another. We also make
an additional assumption that error in one attribute does
not affect the errors in other attributes. This is a reasonable assumption to make, since we are allowing the data
itself to have dependencies between attributes, while only
constraining the error process to be independent across
attributes. With these assumptions, we are able to come up
with a simple and efficient error model, where we combine
the three types of errors using a maximum entropy model.
Given a set of clean candidate tuples C where T ‚àó ‚àà C ,
our error model Pr(T |T ‚àó ) essentially measures how clean T
is, or in other words, how similar T is to T ‚àó .
Edit distance similarity: This similarity measure is used to
detect spelling errors. Edit distance between two strings TAi
and TA‚àó i is defined as the minimum cost of edit operations
applied to dirty tuple TAi transform it to clean TA‚àó i . Edit
operations include character-level copy, insert, delete and
substitute. The cost for each operation can be modified as
required; in this paper we use the Levenshtein distance,
which uses a uniform cost function. This gives us a distance,
which we then convert to a probability using [36]:

4.3

Finding the Candidate Set

The set of candidate tuples, C(T ) for a given tuple T are
the possible replacement tuples that the system considers
as possible corrections to T . The larger the set C is, the
longer it will take for the system to perform the cleaning. If
C contains many unclean tuples, then the system will waste
time scoring tuples that are not clean to begin with.
An efficient approach to finding a reasonably clean
C(T ) is to consider the set of all the tuples in the sample
database that differ from T in not more than j attributes.
In order to find C(T ) that satisfies this, conceptually, we
have to iterate over every tuple t in the sample database
D, comparing it to the tuple T and checking how many
attributes it differs in. This operation can take O(n) time,
where n is the number of tuples in the sample database.
Even with j = 3, the naƒ±Ãàve approach of constructing C from
the sample database directly is too time consuming, since
it requires one to go through the sample database in its
entirety once for every result tuple encountered. To make
this process faster, we create indices over (j + 1) attributes
because searching through indices reduces the number of
comparisons required to compute C(T ). If any candidate
fed (TAi , TA‚àó i ) = exp{‚àícosted (TAi , TA‚àó i )}
(3) tuple T ‚àó differs from T in less than or equal to j attributes,
Distributional Similarity Feature: This similarity measure then it will be present in at least one of the indices, since
is used to detect both substitution and omission errors. we created j + 1 of them (pigeon hole principle). These
Looking at each attribute in isolation is not enough to fix j + 1 indices are created over those attributes that have the
these errors. We propose a context-based similarity measure highest cardinalities, such as Make and Model (as opposed
called Distributional similarity (fds ), which is based on the to attributes like Condition and Doors which can take only
probability of replacing one value with another under a a few values). This ensures that the set of tuples returned
similar context [37]. Formally, for each string TAi and TA‚àó i , from the index would be small in number.
For every possibly dirty tuple T in the database, we go
we have:
over
each such index and find all the tuples that match the
‚àó
X
Pr(c|TAi )Pr(c|TAi )Pr(TAi )
fds (TAi , TA‚àó i ) =
(4)corresponding attribute. The union of all these tuples is then
Pr(c)
‚àó )
examined and the candidate set C is constructed by keeping
c‚ààC(TAi ,TA
i
only those tuples from this union set that do not differ from
where C(TAi , TA‚àó i ) is the context of a tuple attribute value, T in more than j attributes. Thus we can be sure that by
which is a set of attribute values that co-occur with using this method, we have obtained the entire set C 2 .
both TAi and TA‚àó i . Pr(c|TA‚àó i ) = (#(c, TA‚àó i ) + ¬µ)/#(TA‚àó i )
is the probability that a context value c appears given
the clean attribute TA‚àó i in the sample database. Similarly, 5 O FFLINE CLEANING
P (TAi ) = #(TAi )/#tuples is the probability that a dirty
attribute value appears in the sample database. We calculate 5.1 Cleaning to a Deterministic Database
Pr(c|TAi ) and Pr(TAi ) in the same way. To avoid zero In order to clean the data in situ, we first use the techniques
estimates for attribute values that do not appear in the of the previous section to learn the data source model, the
database sample, we use Laplace smoothing factor ¬µ.
error model and create the index. Then, we iterate over all
Unified error model: In practice, we do not know before- the tuples in the database and use Equation 1 to find the
hand which kind of error has occurred for a particular T ‚àó with the best score. We then replace the tuple with that
attribute; we need a unified error model which can accom- T ‚àó , thus creating a deterministic database using the offline
modate all three types of errors (and be flexible enough to mode of BayesWipe.
accommodate more errors when necessary). For this purComputing Pr(T ‚àó )Pr(T |T ‚àó ) is very fast. Even though
pose, we use the well-known maximum entropy framework we do a Bayesian inference for Pr(T ‚àó ), the tuple has all the
[38] to leverage both the similarity measures, (Edit distance values specified, so the inference ends up being a simple
fed and distributional similarity fds ). For each attribute of multiplication over the CPTs of the Bayes network, and is
the input tuple T and T ‚àó , we have the unified error model very cheap. Pr(T |T ‚àó ) involves simple edit distance and disPr(T |T ‚àó ) given by:
tributional similarity calculations all of which involve sim( m
)
ple arithmetic operations and lookups devoid of Bayesian
m
X
X
1
exp Œ±
fed (TAi , TA‚àó i ) + Œ≤
fds (TAi , TA‚àó i )
(5) inference.
Z
i=1
i=1
where Œ± and Œ≤ are the weight of each feature, m is the
numberPof attributes
P in the tuple. The normalization factor
is Z = T ‚àó exp { i Œªi fi (T ‚àó , T )}.

2. There is a small possibility that the true tuple T ‚àó is not in the
sample database at all. This probability can be reduced by choosing
a larger sample set. In future work, we will expand the strategy of
generating C to include all possible k-repairs of a tuple.

6

Recall from Section 4.2 that there are parameters in the
error model called Œ± and Œ≤ , which need to be set. Interestingly, in addition to controlling the relative weight given to
the various features in the error model, these parameters
can be used to control overcorrection by the system.
Overcorrection: Any data cleaning system is vulnerable to
overcorrection, where a legitimate tuple is modified by the
system to an unclean value. Overcorrection can have many
causes. In a traditional, deterministic system, overcorrection
can be caused by erroneous rules learned from infrequent
data. For example, certain makes of cars are all owned by the
same conglomerate (GM owns Chevrolet). In a misguided
attempt to simplify their inventory, a car salesman might list
all the cars under the name of the conglomerate. This may
provide enough support to learn the wrong rule (Malibu ‚Üí
GM).
Typically, once an erroneous rule has been learned, there
is no way to correct it or ignore it without a lot of oversight
from domain experts. However, BayesWipe provides a way
to regulate the amount of overcorrection in the system with
the help of a ‚Äòdegree of change‚Äô parameter. Without loss of
generality, we can rewrite Equation 5 to the following:
  X
m
1
Pr(T |T ) = exp Œ≥ Œ¥
fed (TAi , TA‚àó i )
Z
i=1
‚àó

+ (1 ‚àí Œ¥)

m
X


fds (TAi , TA‚àó i )

i=1

Since we are only interested in their relative weights, the
parameters Œ± and Œ≤ have been replaced by Œ¥ and (1‚àíŒ¥) with
the help of a normalization constant, Œ≥ . This parameter, Œ≥ ,
can be used to modify the degree of variation in Pr(T |T ‚àó ).
High values of Œ≥ imply that small differences in T and
T ‚àó cause a larger difference in the value of Pr(T |T ‚àó ),
causing the system to give higher scores to the original tuple
(compared to a modified tuple).
Example: Consider the following fragment from the
database. The first tuple is a very frequent tuple in the
database, the second one is an erroneous tuple, and the third
tuple is an infrequent, correct tuple. The ‚Äòtrue‚Äô correction
of the second tuple is the third tuple. The Pr(T ‚àó ) values
shown reflect the values that the data source model might
predict for them, roughly based on the frequency with
which they occur in the source data.
Id

Make

Model Type

1
2
3

Honda Civic
Honda Z4
BMW Z4

Engine

Sedan V4
Sedan V6
Sedan V6

Condition P (T ‚àó )
New
New
New

0.400
0.001
0.005

A proper data cleaning system will correct tuple 2 to
tuple 3, and not modify any of the others. However, if
incorrect rules (for example, Z4 ‚Üí Honda) were learned,
there could be overcorrection, where tuple 3 is modified to
tuple 2.
On the other hand, BayesWipe handles this situation
based on the value of Œ≥ . Looking at tuple 3 (which is a clean
tuple), suppose the candidate replacement tuples for it are
also tuples 1, 2 and 3. In that case, the situation may look
like the following:

Cd.
1
2
3

P (T ‚àó )
0.400
0.001
0.005

low Œ≥
P (T |T ‚àó )
score
0.02 0.0080
0.30 0.0003
1.00 0.0050

high Œ≥
P (T |T ‚àó )
score
0.002 0.00080
0.030 0.00003
1.000 0.00500

As we can see, if we choose a low value of Œ≥ , the
candidate with the highest score is tuple 1, which means
an overcorrection will occur. However, with higher Œ≥ , the
candidate with the highest score is tuple 3 itself, which
means the tuple will not be modified, and overcorrection
will not occur. On the other hand, if we set Œ≥ too high, then
even legitimately dirty tuples like tuple 2 won‚Äôt get changed,
thus the number of actual corrections will also be lower.
To make full use of this capability of regulating overcorrection, we need to be able to set the value of Œ≥ appropriately. In the absence of a training dataset (for which the
ground truth is known), we can only estimate the best Œ≥
approximately. We do this by finding a value of Œ≥ for which
the percentage of tuples modified by the system is equal to
the expected percentage of noise in the dataset.
5.2

Cleaning to a Probabilistic Database

We note that many data cleaning approaches ‚Äî including
the one we described in the previous sections ‚Äî come up
with multiple alternatives for the clean version for any given
tuple, and evaluate their confidence in each of the alternatives. For example, if a tuple is observed as ‚ÄòHonda, Corolla‚Äô,
two correct alternatives for that tuple might be ‚ÄòHonda,
Civic‚Äô and ‚ÄòToyota, Corolla‚Äô. In such cases, where the choice
of the clean tuple is not an obvious one, picking the mostlikely option may lead to the wrong answer. Additionally,
if we intend to do further processing on the results, such as
perform aggregate queries, join with other tables, or transfer
the data to someone else for processing, then storing the
most likely outcome is lossy.
A better approach (also suggested by others [4]) is to
store all of the alternative clean tuples along with their
confidence values. Doing this, however, means that the
resulting database will be a probabilistic database (PDB),
even when the source database is deterministic.
It is not clear upfront whether PDB-based cleaning will
have advantages over cleaning to a deterministic database.
On the positive side, using a PDB helps reduce loss of
information arising from discarding all alternatives to tuples
that did not have the maximum confidence. On the negative
side, PDB-based cleaning increases the query processing
cost (as querying PDBs are harder than querying deterministic databases [39]).
Another challenge is one of presentation: users usually
assume that they are dealing with a deterministic source
of data, and presenting all alternatives to them can be
overwhelming to them. In this section, and in the associated
experiments, we investigate the potential advantages to using the BayesWipe system and storing the resulting cleaned
data in a probabilistic database. For our experiments, we
used Mystiq [40], a prototype probabilistic database system
from University of Washington, as the substrate. In order to
create a probabilistic database from the corrections of the input data, we follow the offline cleaning procedure described
previously in Section 4. Instead of storing the most likely T ‚àó ,
we store all the T ‚àó s along with their P (T ‚àó |T ) values. When

7

evaluating the performance of the probabilistic database, we
used simple select queries on the resulting database. Since
representing the results of a probabilistic database to the
user is a complex task, in this paper we focus on showing
the XOR representation of the tuple alternatives to the user.
The rationale for our decision is that in a used car scenario,
the user will be provided with a URL link to the car through
the clickable tuple id and the several alternative clean values
for the dirty attributes are shown within the single tuple
returned to the user. As a result, the form of our output is a
tuple-disjoint independent database [41]. This can be better
explained with an example:
TABLE 2: Cleaned probabilistic database
TID
t1

Model
Civic
Civic

Make
Honda
Honda

Orig.
JPN
JPN

Civic
Civik

Honda
Honda

JPN
JPN

Size
Mid-size
Compact

Eng.
V4
V6

Cond.
NEW
NEW

P
0.6
0.4

Mid-size
Mid-size

V4
V4

USED
USED

0.9
0.1

...
t3

Example: Suppose we clean our running example of
Table 1. We will obtain a tuple-disjoint independent3 probabilistic database [41]; a fragment of which is shown in
Table 2. Each original input tuple (t1 , t3 ), has been cleaned,
and their alternatives are stored along with the computed
confidence values for the alternatives (0.6 and 0.4 for t1 ,
in this example). Suppose the user issues a query Model =
Civic. Both options of tuple t1 of the probabilistic database
satisfy the constraints of the query. Since there are two valid
alternatives to tuple t1 in the result with probabilities 0.6
and 0.4, in order to get a single tuple representation, the
matching attributes in the alternatives are shown deterministically whereas the unclean attributes like Size, Engine and
Condition with several possible clean values are shown as
options. Only the first option in tuple t3 matches the query.
Thus the XOR result will contain only a single alternative
for t3 with probability 0.9. It is important to note that in the
case of t1 , the Mid-size car can be associated with an Eng.
value of V4 and a probability of 0.6 respectively. The XOR
representation doesn‚Äôt necessarily allow for combining Midsize with either an Eng. value of V6 or a probability value
of 0.4.
The experimental results compare the tuple ids when
computing the recall of the method because tuple id provides the URL to the car‚Äôs web page which can be used
to determine a match. The output probabilistic relation is
shown in Table 3.
TABLE 3: Result probabilistic database
TID

Model

Make

Orig.

t1

Civic

Honda

JPN

t3

Civic

Honda

JPN

Size
Midsize/Compact
Mid-size

Eng.

Cond.

P

V4/V6

NEW

0.6/0.4

V4

USED

0.9

3. A tuple-disjoint independent probabilistic database is one where
every tuple, identified by its primary key, is independent of all other
tuples. Each tuple is, however, allowed to have multiple alternatives
with associated probabilities. In a tuple-independent database, each
tuple has a single probability, which is the probability of that tuple
existing.

The interesting fact here is that the result of any query
will always be a tuple-independent database. This is because we projected out every attribute except for the tupleID, and the tuple-IDs are independent of each other.
When showing the results of our experiments, we evaluate the precision and recall of the system. Since precision
and recall are deterministic concepts, we have to convert
the probabilistic database into a deterministic database (that
will be shown to the user) prior to computing these values.
We can do this conversion in two ways: (1) by picking only
those tuples whose probability is higher than some threshold. We call this method the threshold based determinization.
(2) by picking the top-k tuples and discarding the probability values (top-k determinization). The experiment section
(Section 8.2) shows results with both determinizations.

6

Q UERY REWRITING FOR O NLINE Q UERY P RO -

CESSING

In this section we extend the techniques of the previous
section so that it can be used in an online query processing
method where the result tuples are cleaned at query time.
Certain tuples that do not satisfy the query constraints, but
are relevant to the user, need to be retrieved, ranked and
shown to the user. The process also needs to be efficient,
since the time that the users are willing to wait before
results are shown to them is very small. We show our query
rewriting mechanisms aimed at addressing both.
We begin by executing the user‚Äôs query (Q‚àó ) on the
database. We store the retrieved results, but do not show
them to the user immediately. We then find rewritten queries
that are most likely to retrieve clean tuples. We do that in
a two-stage process: we first expand the query to increase
the precision, and then relax the query by deleting some
constraints (to increase the recall).
6.1

Increasing the precision of rewritten queries

We can improve precision by adding relevant constraints to the query Q‚àó given by the user. For example, when a user issues the query Model = Civic,
we can expand the query to add relevant constraints Make = Honda, Country = Japan, Size = Mid-Size.
These additions capture the essence of the query ‚Äî because
they limit the results to the specific kind of car the user
is probably looking for. These expanded structured queries
generated from the user‚Äôs query are called ESQs.
Each user query Q‚àó is a select query with one or more
attribute-value pairs as constraints. In order to create an
ESQ, we will have to add highly correlated constraints to
Q‚àó .
Searching for correlated constraints to add requires
Bayesian inference, which is an expensive operation. Therefore, when searching for constraints to add to Q‚àó , we restrict
the search to the union of all the attributes in the Markov
blanket [42]. The Markov blanket of an attribute comprises
its children, its parents, and its children‚Äôs other parents. It
is the set of attributes whose value being given, the node
becomes independent of all other nodes in the network.
Thus, it makes sense to consider these nodes when finding
correlated attributes. This correlation is computed using
the Bayes Network that was learned offline on a sample
database (recall the architecture of BayesWipe in Figure 1.)

8

Given a Q‚àó , we attempt to generate multiple ESQs that
maximizes both the relevance of the results and the coverage
of the queries of the solution space.
Note that if there are m attributes, each of which can
take n values, then the total number of possible ESQs is
nm . Searching for the ESQ that globally maximizes the
objectives in this space is infeasible; we therefore approximately search for it by performing a heuristic-informed
search. Our objective is to create an ESQ with m attributevalue pairs as constraints.We begin with the constraints
specified by the user query Q‚àó . We set these as evidence in
the Bayes network, and then query the Markov blanket of
these attributes for the attribute-value pairs with the highest
posterior probability given this evidence. We take the top-k
attribute-value pairs and append them to Q‚àó to produce k
search nodes, each search node being a query fragment. If Q
has p constraints in it, then the heuristic value of Q is given
by Pr(Q)m/p . This represents the expected joint probability
of Q when expanded to m attributes, assuming that all the
constraints will have the same average posterior probability.
We expand them further, until we find k queries of size m
with the highest probabilities.
Make=Honda

0.4

Model=
Accord

0.1

Make=
Honda

0.3

0.9

Model=
Civic
Fuel=
Gas

Miles=
10k

‚Ä¶

(0.1)6
Make=Honda, Model = Accord

Engine=
V4

‚Ä¶

Doors=
4

‚Ä¶

(0.1 √ó 0.3)3

Model=
Civic

‚Ä¶

(0.1 √ó 0.9)3

(0.1 √ó 0.4)3
Make=Honda, Model = Civic

Make=Honda, Fuel = Gas

Fig. 3: Query Expansion Example. The tree shows the candidate constraints that can be added to a query, and the
rectangles show the expanded queries with the computed
probability values.
Example: In Figure 3, we show an example of the query
expansion. The node on the left represents the query given
by the user ‚ÄúMake=Honda‚Äù. First, we look at the Markov
Blanket of the attribute Make, and determine that Model
and Condition are the nodes in the Markov blanket. we
then set ‚ÄúMake=Honda‚Äù as evidence in the Bayes network
and then run an inference over the values of the attribute
Model. The two values of the Model attribute with the
highest posterior probability are Accord and Civic. The
most probable values of the Condition attribute are ‚Äúnew‚Äù
and ‚Äúold‚Äù. Using each of these values, new queries are
constructed and added to the queue. Thus, the queue now
consists of the 4 queries: ‚ÄúMake=Honda, Model=Civic‚Äù,
‚ÄúMake=Honda, Model=Accord‚Äù and ‚ÄúMake=Honda, Condition=old‚Äù. A fragment of these queries are shown in
the middle column of Figure 3. We dequeue the highest
probability item from the queue and repeat the process
of setting the evidence, finding the Markov Blanket, and
running the inference. We stop when we get the required
number of ESQs with a sufficient number of constraints.
6.2 Increasing the recall
Adding constraints to the query causes the precision of the
results to increase, but reduces the recall drastically. Therefore, in this stage, we choose to delete some constraints from

the ESQs, thus generating relaxed queries (RQ). Notice that
tuples that have corruptions in the attribute constrained by
the user can only be retrieved by relaxed queries that do
not specify a value for those attributes. Instead, we have to
depend on rewritten queries that contain correlated values
in other attributes to retrieve these tuples. Using relaxed
queries can be seen as a trade-off between the recall of the
resultset and the time taken, since there are an exponential
number of relaxed queries for any given ESQ. As a result,
an important question is the choice of RQs to execute. We
take the approach of generating every possible RQ, and
then ranking them according to their expected relevance.
This operation is performed entirely on the learned error
statistics, and is thus very fast.
We score each relaxed query by the expected relevance of
its result set.
!
P
‚àó
Tq Score(Tq |Q )
Rank(q) = E
|Tq |
where Tq are the tuples returned by a query q , and Q‚àó is
the user‚Äôs query. Executing an RQ with a higher rank will
have a more beneficial result on the result set because it will
bring in better quality result tuples. Estimating this quantity
is difficult because we do not have complete information
about the tuples that will be returned for any query q . The
best we can do, therefore, is to approximate this quantity.
Let the relaxed query be Q, and the expanded query
that it was relaxed from be ESQ. We wish to estimate
E[P (T |T ‚àó )] where T are the tuples returned by Q. Using the
attribute-error
independence assumption, we can rewrite
Qm
that as i=0 Pr(T.Ai |T ‚àó .Ai ), where T.Ai is the value of the
i-th attribute in T. Since ESQ was obtained by expanding
Q‚àó using the Bayes network, it has values that can be
considered clean for this evaluation. Now, we divide the m
attributes of the database into 3 classes: (1) The attribute
is specified both in ESQ and in Q. In this case, we set
Pr(T.Ai |T ‚àó .Ai ) to 1, since T.Ai = T ‚àó .Ai . (2) The attribute
is specified in ESQ but not in Q. In this case, we know
what T ‚àó .Ai is, but not T.Ai . However, we can generate an
average statistic of how often T ‚àó .Ai is erroneous by looking
at our sample database. Therefore, in the offline learning
stage, we precompute tables of error statistics for every T ‚àó
that appears in our sample database, and use that value.
(3) The attribute is not specified in either ESQ or Q. In
this case, we know neither the attribute value in T nor in
T ‚àó . We, therefore, use the average error rate of the entire
attribute as the value for Pr(T.Ai |T ‚àó .Ai ). This statistic is
also precomputed during the learning phase. This product
gives the expected rank of the tuples returned by Q.
Example: In Figure 4, we show an example for finding
the probability values of a relaxed query. Assume that the
user‚Äôs query Q‚àó is ‚ÄúCivic‚Äù, and the ESQ is shown in the
second row. For an RQ that removes the attribute values
‚ÄúCivic‚Äù and ‚ÄúMid-Size‚Äù from the ESQ, the probabilities are
calculated as follows: For the attributes ‚ÄúMake, Country‚Äù
and ‚ÄúEngine‚Äù, the values are present in both the ESQ as
well as the RQ, and therefore, the P (T |T ‚àó ) for them is 1.
For the attribute ‚ÄúModel‚Äù and ‚ÄúType‚Äù, the values are present
in ESQ but not in RQ, hence the value for them can be
computed from the learned error statistics. For example, for
‚ÄúCivic‚Äù, the average value of P (T |Civic) as learned from

9

Model
Q*:

Civic

ESQ:

Civic

RQ:

E[P(T|T*)]:

0.8

Make

Country

Honda

JPN

Honda

JPN

1

1

Type

Mid-size

Engine

Cond.

V4
V4

0.5

1

0.5

=0.2

Fig. 4: Query Relaxation Example.
the sample database (0.8) is used. Finally, for the attribute
‚ÄúCondition‚Äù, which is present neither in ESQ nor in RQ,
we use the average error statistic for that attribute (i.e. the
average of P (Ta |Ta‚àó ) for a = ‚ÄúCondition‚Äù which is 0.5).
The final value of E[P (T |T ‚àó )] is found from the product
of all these attributes as 0.2. This process is very fast because it only involves lookups and multiplication - bayesian
inference is not needed.
6.3

Terminating the process

We begin by looking at all the RQs in descending order
of their rank. If the current k -th tuple in our resultset has
a relevance of Œª, and the estimated rank of the Q we are
about to execute is R(Tq |Q), then we stop evaluating any
more queries if the probability Pr(R(Tq |Q) > Œª) is less
than some user defined threshold P . This ensures that we
have the true top-k resultset with a probability P .

7

M AP -R EDUCE FRAMEWORK

BayesWipe is most useful for big-data related scenarios.
BayesWipe has two modes: online and offline. The online
mode of BayesWipe already works for big data scenarios by
optimising the rewritten queries it issues. Now, we show
that the offline mode can also be optimized for a big-data
scenario by implementing it as a Map-Reduce application.
So far, BayesWipe-Offline has been implemented as a
two-phase, single threaded program. In the first phase,
the program learns the Bayes network (both structure and
parameters), learns the error statistics, and creates the candidate index. Recall from section 4.3 that we create an
index on the attributes of the sample database to speed
up the creation of the candidate set of clean tuples; which
we refer to as the candidate index. The candidate index is
constructed on a set of j + 1 attributes when the restriction
on a candidate clean tuple is to differ from the dirty tuple in
not more than j attributes. The attributes in the dirty tuple
are compared to the attributes of the tuples in the sample
database using the candidate index to generate the set of
candidate clean tuples. Note that this candidate index can be
constructed on any arbitrary set of j + 1 attributes present
in the sample database. In the second phase, the program
goes through every tuple in the input database, picks a set
of candidate tuples, and then evaluates the P (T ‚àó |T )P (T ‚àó )
for every candidate tuple, and replaces T with the T ‚àó that
maximises that value. Since the learning is typically done
on a sample of the data, it is more important to focus
on the second phase for the parallelizing efforts. Later, we
will see how the learning of the error statistics can also be
parallelized.

7.1

Simple Approach

The simplest approach to parallelizing BayesWipe is to run
the first phase (the learning phase) on a single machine.
Then, a copy of the bayes network (structure and CPTs),
the error statistics, and the candidate index can be sent to
a number of other machines. Each of those machines also
receives a fraction of the input data from the dirty database.
With the help of the generative model and the input data, it
can clean the tuples, and then create the output.
If we express this in Map-Reduce terminology, we will
have a pre-processing step where we create the generative
and error models. The Map-Reduce architecture will have
only mappers, and no reducers. The result of the mapping
will be the tuple hT, T ‚àó i.
The problem with this approach is that in a truly big
data scenario, the candidate index can become very large.
Indeed, as the number of tuples increases, the size of the
domain of each attribute also increases (see Figure 8a for
1 shard). Further, the number of different combinations,
and the number of erroneous values for each attribute also
increase (Figure 8b). All of this results in a rather large candidate index. Transmitting and using the entire index on each
mapper node is wasteful of both network, memory, (and
if swapped out, disk resources). Note that to create a rich
and useful data correction system, we have to accommodate
a large candidate clean-tuple set, C(T ), for every T . C(T )
roughly tracks the sample database size. If we are unable to
shard C(T ), then sharding the input data is pointless. In the
following section we endeavor to show a strategy where not
just the input, but also the index on the candidate set C(T )
can be sharded across machines.
7.2

Improved Approach

In order to split both the input tuples and the candidate
index, we use a two-stage approach. In the first stage, we
run a map-reduce that splits the problem into multiple
shards, each shard having a small fraction of the candidate
index. The second stage is a simple map-reduce that picks
the best output from stage 1 for each input tuple.
Stage 1: Given an input tuple T and a set of candidate
tuples, the T ‚àó s, suppose the candidate index is created on k
attributes, A1 ...Ak . We can say that for every tuple T , and
one of its candidate tuples T ‚àó , they will have at least one
matching attribute ai from this set. We can use this common
element ai to predict which shards the candidate T ‚àó s might
be available in. We therefore, send the tuple T to each shard
that matches the hash of the value ai .
In the map-reduce architecture, it is possible to define
a ‚Äòpartition‚Äô function. Given a mapped key-value pair, this
function determines which reducer nodes will process the
data. We can use an exact equivalence on each value that
the matching attribute can take, ai as the partition function.
However, notice that the number of possible values that
A1 ...Ak can take is rather large. If we naƒ±Ãàvely use ai as
the partition function, we will have to create those many
reducer nodes. Therefore, more generally, we hash this value
into a fixed number of reducer nodes, using a deterministic
hash function. This will then find all candidate tuples that
are eligible for this tuple, compute the similarity, and output
it.

10

Example: Suppose we have tuple T1 that has values
(a1 , a2 , a3 , a4 , a5 ). Suppose our candidate index is created
on attributes A1 , A2 , A4 . This means that any candidates T ‚àó
that are eligible for this tuple have to match one of the values
a1 , a2 or a4 . Then the mapper will create the pairs (a1 , T ),
(a2 , T ) and (a4 , T ), and send to the reducers. The partition
function is the hash of the key - so in this case, the first one
will be sent to the reducer number hash(A1 = a1), the second will be sent to the reducer numbered hash(A2 = a2),
and so on.
In the reducer, the similarity computation and computation of the prior probabilities from the BayesWipe algorithm
are run. Since each reducer only has a fraction of the candidate index (the part that matches A1 = a1, for instance), it
can hold it in memory and computation is quite fast. Each
‚àó
reducer produces a pair (T1 , (T1n
, score)). Since there are
several candidate clean tuples, n is used to identify a specific
tuple among those alternatives.
Stage 2: This stage is a simple max calculation. The
mapper does nothing, it simply passes on the key-value
‚àó
pair (T1 , (T1n
, score)) that was generated in the previous
Map-Reduce job. Notice that the key of this pair is the
original, dirty tuple T1 . The Map-Reduce architecture thus
automatically groups together all the possible clean versions
of T1 along with their scores. The reducer picks the best
T* based on the score (using a simple max function), and
outputs it to the database.

Index
Fragment

Reducer
(a, T)
Mapper

Index
Fragment

Reducer
(T, (T*, score))
Index
Fragment

Mapper

Input Data
Fragments

Reducer
Index
Fragment

Reducer

Fig. 5: Stage-1 Map-Reduce Framework for BayesWipe.
7.3

Results of This Strategy

In Figure 8a and Figure 8b we can see how this map
reduce strategy helps in reducing the memory footprint
of the reducer. First, we plot the size of the index that
needs to be held in each node as the number of tuples in
the input increases. The topmost curve shows the size of
index in bytes if there was no sharding - as expected, it
increases sharply. The other curves show how the size of
the index in the one of the nodes varies for the same dataset
sizes. From the graph, it can be seen that as the number of
tuples increases, the size of the index grows at a lower rate
when the number of shards is increased. This shows that
increasing the number of reduce nodes is a credible strategy
for distributing the burden of the index.
In the second figure (Figure 8b), we see how the size
of the index varies with the percentage of noise in the
dataset. As expected, when the noise increases, the number

of possible candidate tuples increase (since there are more
variations of each attribute value in the pool). Without
sharding, we see that the size of the dataset increases. While
the increase in the size of the index is not as sharp as the
increase due to the size of the dataset, it is still significant.
Once again, we observe that as the number of shards is
increased, the size of the index in the shard reduces to a
much more manageable value.
Note that a slight downside to this architecture is that a
shuffle/reduce of (T, (Tn‚àó , score)) is needed in the second
stage, and this intermediate data structure can be quite
large. While this leads to some network and temporary
storage overhead, the primary objective of sharding the expensive computation has been achieved by this architecture.

8

E MPIRICAL E VALUATION

We quantitatively study the performance of BayesWipe in
both its modes ‚Äî offline, and online, and compare it against
state-of-the-art CFD approaches. We present experiments
on evaluating the approach in terms of the effectiveness of
data cleaning, efficiency and precision of query rewriting.
A demo for the offline cleaning mode of BayesWipe can be
downloaded from http://bayeswipe.sushovan.de/.
8.1

Datasets

To perform the experiments, we obtained the real data from
the web. The first dataset is Used car sales dataset Dcar
crawled from Google Base. Such ‚Äúdirty‚Äù dataset is referred
0
‚Äù. The second dataset we used was adapted
to as ‚ÄúDcar
from the Census Income dataset Dcensus from the UCI machine learning repository [43]. From the fourteen available
attributes, we picked the attributes that were categorical
in nature, resulting in the following 8 attributes: workingclass, education, marital status, occupation, race, gender,
filing status. country. The same setup was used for both
datasets ‚Äì including parameters and error features.
These datasets were observed to be mostly clean. We
then introduced4 three types of noise to the attributes. To
add noise to an attribute, we randomly changed it either to
a new value which is close in terms of string edit distance
(distance between 1 and 4, simulating spelling errors) or to
a new value which was from the same attribute (simulating
replacement errors) or just delete it (simulating deletion errors). As we have mentioned before, one of the assumptions
of this paper is that the error model is a combination of these
three kinds of errors, and that the errors are independent
of each other. By synthetically generating these errors, we
were able to test our system against a dataset that satisfies
the assumption.
The next dataset tests our system against a real-world
scenario where we do not control the error process, and thus
validates that this assumption was not unrealistic.
To test our system against real-world noise where we
do not have any control over amount, type or behavior
of the noise generation process, we crawled car inventory
data from the website ‚Äòcars.com‚Äô. We manually verified that
the data obtained did, in fact, have a reasonable number of
inaccuracies, making it a suitable candidate for testing our
system.
4. We note that the introduction of synthetic errors into clean data for
experimental evaluation purposes is common practice in data cleaning
research [16], [23].

11

8.2

Experiments

Offline Cleaning Evaluation: The first set of evaluations
shows the effectiveness of the offline cleaning mode. In
Figure 6a, we compare BayesWipe against CFDs [44].
The dotted line that shows the number of CFDs learned
from the noisy data quickly falls to zero, which is not surprising: CFDs learning was designed with a clean training
dataset in mind. Further, the only constraints learned by
this algorithm are the ones that have not been violated in
the dataset ‚Äî unless a tuple violates some CFD, it cannot
be cleaned. As a result, the CFD method cleans exactly
zero tuples independent of the noise percentage. On the
other hand, BayesWipe is able to clean between 20% to
40% of the incorrect values. It is interesting to note that
the percentage of tuples cleaned increases initially and then
slowly decreases, because for very low values of noise, there
aren‚Äôt enough errors for the system to learn a reliable error
model from; and at larger values of noise, the data source
model learned from the noisy data is of poorer quality.
While Figure 6a showed only percentages, in Figure 6b
we report the actual number of tuples cleaned in the dataset
along with the percentage cleaned. This curve shows that
the raw number of tuples cleaned always increases with
higher input noise percentages.
Setting Œ≥ : As explained in Section 5.1, the weight given
to the edit distance (Œ¥ ) compared to the weight given to
the distributional similarity (1 ‚àí Œ¥ ); and the overcorrection
parameter (Œ≥ ) are parameters that can be tuned, and should
be set based on which kind of error is more likely to occur. In
our experiments, we performed a grid search to determine
the best values of Œ¥ and Œ≥ to use. In Figure 6c, we show a
portion of the grid search where Œ¥ = 2/5, and varying Œ≥ .
The ‚Äúvalues corrected‚Äù data points in the graph correspond to the number of erroneous attribute values that
the algorithm successfully corrected (when checked against
the ground truth). The ‚Äúfalse positives‚Äù are the number of
legitimate values that the algorithm changes to an erroneous
value. When cleaning the data, our algorithm chooses a
candidate tuple based on both the prior of the candidate as
well as the likelihood of the correction given the evidence.
Low values of Œ≥ give a higher weight to the prior than
the likelihood, allowing tuples to be changed more easily
to candidates with high prior. The ‚Äúoverall gain‚Äù in the
number of clean values is calculated as the difference of
clean values between the output and input of the algorithm.
If we set the parameter values too low, we will correct
most wrong tuples in the input dataset, but we will also
‚Äòovercorrect‚Äô a larger number of tuples. If the parameters are
set too high, then the system will not correct many errors
‚Äî but the number of ‚Äòovercorrections‚Äô will also be lower.
Based on these experiments, we picked a parameter value
of Œ¥ = 0.638, Œ≥ = 5.8 and kept it constant throughout.
Using probabilistic databases: We empirically evaluate the
PDB-mode of BayesWipe in Figure 7. In the first figure,
we show the performance of the PDB mode of BayesWipe against the deterministic mode for specific queries.
As we can see from the first, third and seventh queries,
the BayesWipe-PDB improves the recall without any loss
of precision. However, in most cases (and on average),
BayesWipe-PDB provides a better recall at the cost of some
precision.

The second figure shows the performance of BayesWipePDB as the probability threshold for inclusion of a tuple in
the resultset is varied. As expected, with low values of the
threshold, the system allows most tuples into the resultset,
thus showing high recall and low precision. As the threshold
increased, the precision increases, but the recall falls.
In Figure 7c, we compare the precision of the PDB mode
using top-k determinization against the deterministic mode
of BayesWipe. As expected, both the modes show high precision for low values of k , indicating that the initial results
are clean and relevant to the user. For higher values of k ,
the PDB precision falls off, indicating that PDB methods
are more useful for scenarios where high recall is important
without sacrificing too much precision.
Online Query Processing: While in the offline mode, we
had the luxury of changing the tuples in the database itself,
in online query processing, we use query rewriting to obtain
a resultset that is similar to the offline results, without
modification to the database. We consider an SQL select
query system as our baseline. We evaluate the precision and
recall of our method against the ground truth and compare
it with the baseline, using randomly generated queries.
We issued randomly generated queries to both BayesWipe and the baseline system. Figure 8c shows the average
precision over 10 queries at various recall values. It shows
that our system outperforms the SQL select query system
in top-k precision, especially since our system considers the
relevance of the results when ranking them. On the other
hand, the SQL search approach is oblivious to ranking and
returns all tuples that satisfy the user query. Thus it may
return irrelevant tuples early on, leading to less precision.
Figure 8d shows the improvement in the absolute numbers of tuples returned by the BayesWipe system. The graph
shows the number of true positive tuples returned (tuples
that match the query results from the ground truth) minus
the number of false positives (tuples that are returned but do
not appear in the ground truth result set). We also plot the
number of true positive results from the ground truth, which
is the theoretical maximum that any algorithm can achieve.
The graph shows that the BayesWipe system outperforms
the SQL query system at nearly every level of noise. Further,
the graph also illustrates that ‚Äî compared to an SQL query
baseline ‚Äî BayesWipe closes the gap to the maximum
possible number of tuples to a large extent. In addition to
showing the performance of BayesWipe against the SQL
query baseline, we also show the performance of BayesWipe
without the query relaxation part (called BW-exp5 ). We can
see that the full BayesWipe system outperforms the BW-exp
system significantly, showing that query relaxation plays an
important role in bringing relevant tuples to the resultset,
especially for higher values of noise.
This shows that our proposed query ranking strategy indeed captures the expected relevance of the to-be-retrieved
tuples, and the query rewriting module is able to generate
the highly ranked queries.
Efficiency: In Figure 10 we show the data cleaning time
taken by the system in its various modes. The first two
graphs show the offline mode, and the second two show the
5. BW-exp stands for BayesWipe-expanded, since the only query
rewriting operation done is query expansion.

CFD

#CFDs

40%

4

30%
20%

2

10%
0%

40%

30%

2000

20%

1000

10%

0

5 10 15 20 25 30 35 40
Noise Percent

4

5

200

Values Corrected

160

False Positives

120

Cleanliness Gain

80
40
0
2

0%
3

(a) % performance of BayesWipe compared to CFD, for the used-car dataset.

50%

3000

0

0

Net Tuples Cleaned
Percent Cleaned

4000

6

Number of tuples

BayesWipe

50%

Num CFDs learned

% Tuples Cleaned

12

2.5

3

3.5

4

4.5

5

5.5

6

Distributional Similarity Weight

10 15 20 25 30 35

Percentage of noise

(b) % net corrupt values cleaned, car
database

(c) Net corrections vs Œ≥ . (The x-axis values show the un-normalized distributional similarity weight, which is simply
Œ≥ √ó 3/5.)

1
0.75
0.5
0.25
0
1
0.75
0.5
0.25
0

1

1

0.8

0.8

0.6

PDB Precision

0.4

PDB Recall

0.2

model =
outlander
sports

cartype =
sedan

make = bmw & model = jetta
condition =
used

BayesWipe-PDB Precision

model =
cooper s

model = h3
mini

Average

BayesWipe-DET Precision

(a) Precision and recall of the PDB method shown against the deterministic method for specific queries.

0.6
Deterministic Precision
PDB Precision

0.4

0.2

0

make = acura

Precision

PRECISION

RECALL

Fig. 6: Offline cleaning mode of BayesWipe

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Threshold

0
0

25

50
top-K

75

100

(b) Precision and recall of
(c) top-k precision of PDB
the PDB method using a
vs deterministic method.
threshold.

Fig. 7: Results of probabilistic method.
online mode. As can be seen from the graphs, BayesWipe
performs reasonably well both in datasets of large size and
datasets with large noise.
Evaluation on real data with naturally occurring errors: In
this section we used a dataset of 1.2 million tuples crawled
from the cars.com website6 to check the performance of
the system with real-world data, where the corruptions
were not synthetically introduced. Since this data is large,
and the noise is completely naturally occurring, we do not
have ground truth for this data. To evaluate this system,
we conducted an experiment on Amazon Mechanical Turk.
First, we ran the offline mode of BayesWipe on the entire database. We then picked only those tuples that were
changed during the cleaning, and then created an interface
in mechanical turk where only those tuples were shown to
the user in random order. Due to resource constraints, the
experiment was run with the first 200 tuples that the system
found to be unclean. We inserted 3 known answers into
the questionnaire, and removed any responses that failed
to annotate at least 2 out of the 3 answers correctly.
An example is shown in Figure 11. The turker is presented with two cars, and she does not know which of
the cars was originally present in the dirty dataset, and
which one was produced by BayesWipe. The turker will use
her own domain knowledge, or perform a web search and
discover that a Mazda CX-9 touring is only available in a 3.7l
engine, not a 3.5l. Then the turker will be able to declare the
second tuple as the correct option with high confidence.
6. http://www.cars.com

The results of this experiment are shown in Table 4. As
we can see, the users consistently picked the tuples cleaned
by BayesWipe more favorably compared to the original
dirty tuples, proving that it is indeed effective in real-world
datasets. Notice that it is not trivial to obtain a 56% rate of
success in these experiments. Finding a tuple which convinces the turkers that it is better than the original requires
searching through a huge space of possible corrections. An
algorithm that picks a possible correction randomly from
this space is likely to get a near 0% accuracy.
The first row of Table 4 shows the fraction of tuples for
which the turkers picked the version cleaned by BayesWipe
and indicated that they were either ‚Äòvery confident‚Äô or
‚Äòconfident‚Äô. The second row shows the fraction of tuples for
all turker confidence values, and therefore is a less reliable
indicator of success.
In order to show the efficacy of BayesWipe we also
performed an experiment in which the same tuples (the
ones that BayesWipe had changed) were modified by a random perturbation. The random perturbation was done by
the same error process as described before (typo, deletion,
substitution with equal probability). Then these tuples (the
original tuple from the database and the perturbed tuple)
were presented as two choices to the turkers. The preference
by the turkers for the randomly perturbed tuple over the
original dirty tuple is shown in the third column, ‚ÄòRandom‚Äô.
It is obvious from this that the turkers overwhelmingly do
not favor the random perturbed tuples. This demonstrates
two things. First, it shows the fact that BayesWipe was
performing useful cleaning of the tuples. In fact, BayesWipe
shows a tenfold improvement over the random perturbation

13
1600000

none

2

3

4

5

300000
none

2

3

4

5

250000

1200000

Bytes in one shard

Bytes in one shard

1400000

1000000

800000
600000
400000

200000
150000
100000

200000

50000

0
1

20

40

80
100
number of tuples

120

0

140

0

5

10

15
20
25
Noise percentage

30

35

40

(a) vs the Number of Tuples (in
(b) vs the Noise in the Dataset, for
Thousands) in the Dataset, for VarVarious Number of Shards.
ious Number of Shards.

Fig. 8: Map-Reduce index sizes
1000

Precision

1.00

980
960

.95
SQL Select Query

.90

940
920

BayesWipe Online
Query Processing

.85
.01

.03

.05

.07 .09
Recall

.11

900
1

.13

2

BW

3

4

BW-exp

5

10 15 20 25 30 35
Noise %
SQL
Ground truth

(d) Net improvement in data quality
(TP-FP)

(c) Average precision vs recall

150

car-offline

800
600

census

400

car

200

100

10k 15k 20k 25k
Number of tuples

30k

car-online

census-online

50

0

10
20
30
Percentage of noise

census-online
50
0

5k

40

car-online

100

0

0

5k

150
Time taken (s)

census-offline

Time Taken (s)

1000
Time taken (s)

Time Taken (s)

Fig. 9: Online cleaning mode of BayesWipe
600
500
400
300
200
100
0

10k 15k 20k 25k
Number of tuples

30k

0

10
20
30
Percentage of noise

40

(a) Time taken vs number of (b) Time taken vs noise per- (c) Time taken vs number of (d) Time taken vs noise pertuples in offline mode
centage in offline mode
tuples in online mode
centage in online mode

Fig. 10: Performance evaluations
Confidence
High confidence only
All
confidence
values

BayesWipe

Original

Random

56.3%

43.6%

5.5%

53.3%

46.7%

12.4%

Increase over Random
50.8% points
...
(10x better)

make

model

cx-9
Car: mazda
touring

40.9% points
(4x better)

cx-9
Car: mazda
touring

TABLE 4: Results of the Mechanical Turk Experiment, showing the percentage of tuples for which the users picked the
results obtained by BayesWipe as against the original tuple.
Also shows performance against a random modification.

cartype fueltype

engine

transmission

drivetrain doors wheelbase

suv

3.5l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113‚Äù

suv

3.7l v6 24v
gasoline
mpfi dohc

6-speed
automatic

fwd

4

113"

ÔÇ° First is correct
ÔÇ° Second is correct
How confident are you about your selection?
ÔÇ° Very confident ÔÇ° Confident ÔÇ° Slightly confident ÔÇ° Slightly Unsure ÔÇ° Totally Unsure

Fig. 11: A fragment of the questionnaire provided to the
Mechanical Turk workers.
model, as judged by human turkers. This shows that in
the large space of possible modifications of a wrong tuple,
BayesWipe picks the correct one most of the time. Second, it
provides additional support for the fact that the turkers are
picking the tuple carefully, and are not randomly submitting
their responses.
In this experiment, we also found the average fraction
of known answers that the turkers gave wrong answers to.
This value was 8%. This leads to the conclusion that the
difference between the turker‚Äôs preference of BayesWipe
over both the original tuples (which is 12%) and the random
perturbation (which is 50%) are both significant.

9

C ONCLUSION

In this paper we presented a novel system, BayesWipe
that works using end-to-end probabilistic semantics, and
without access to clean master data. We showed how to
effectively learn the data source model as a Bayes network,
and how to model the error as a mixture of error features.
We showed the operation of this system in two modalities:
(1) offline data cleaning, an in situ rectification of data and
(2) online query processing mode, a highly efficient way to
obtain clean query results over inconsistent data. There is
an option to generate a standard, deterministic database as
the output, as well as a probabilistic database, where all

14

the alternatives are preserved for further processing. We
empirically showed that BayesWipe outperformed existing
baseline techniques in quality of results, and was highly
efficient. We also showed the performance of the BayesWipe
system at various stages of the query rewriting operation.
We demonstrated how BayesWipe can be run on the mapreduce architecture so that it can scale to huge data sizes.
User experiments showed that the system is useful in cleaning real-world noisy data.

R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

W. Fan and F. Geerts, ‚ÄúFoundations of data quality management,‚Äù
Synthesis Lectures on Data Management, 2012.
P. Gray, ‚ÄúBefore Big Data, clean data,‚Äù 2013. [Online]. Available:
http://www.techrepublic.com/blog/big-data-analytics/
before-big-data-clean-data/
H. Leslie, ‚ÄúHealth data quality ‚Äì a two-edged sword,‚Äù 2010.
[Online]. Available: http://omowizard.wordpress.com/2010/02/
21/health-data-quality-a-two-edged-sword/
Computing Research Association, ‚ÄúChallenges and opportunities
with big data,‚Äù http://cra.org/ccc/docs/init/bigdatawhitepaper.
pdf, 2012.
E. Knorr, R. Ng, and V. Tucakov, ‚ÄúDistance-based outliers: algorithms and applications,‚Äù VLDB, 2000.
H. Xiong, G. Pandey, M. Steinbach, and V. Kumar, ‚ÄúEnhancing
data analysis with noise removal,‚Äù TKDE, 2006.
P. Singla and P. Domingos, ‚ÄúEntity resolution with markov logic,‚Äù
in ICDM, 2006.
I. Fellegi and D. Holt, ‚ÄúA systematic approach to automatic edit
and imputation,‚Äù J. American Statistical association, 1976.
P. Bohannon, W. Fan, M. Flaster, and R. Rastogi, ‚ÄúA cost-based
model and effective heuristic for repairing constraints by value
modification,‚Äù in SIGMOD, 2005.
W. Fan, F. Geerts, L. Lakshmanan, and M. Xiong, ‚ÄúDiscovering
conditional functional dependencies,‚Äù ICDE, 2009.
L. E. Bertossi, S. Kolahi, and L. V. S. Lakshmanan, ‚ÄúData cleaning
and query answering with matching dependencies and matching
functions,‚Äù ICDT, 2011.
A. Fuxman, E. Fazli, and R. J. Miller, ‚ÄúConquer: Efficient management of inconsistent databases,‚Äù SIGMOD, 2005.
S. De, Y. Hu, Y. Chen, and S. Kambhampati, ‚ÄúBayeswipe: A multimodal system for data cleaning and consistent query answering
on structured bigdata,‚Äù IEEE Big Data, 2014.
G. Wolf, A. Kalavagattu, H. Khatri, R. Balakrishnan, B. Chokshi,
J. Fan, Y. Chen, and S. Kambhampati, ‚ÄúQuery processing over
incomplete autonomous databases: query rewriting using learned
data dependencies,‚Äù VLDB, 2009.
S. De, ‚ÄúUnsupervised bayesian data cleaning techniques for structured data,‚Äù Ph.D. dissertation, ASU, 2014.
P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis,
‚ÄúConditional functional dependencies for data cleaning,‚Äù ICDE,
2007.
W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, ‚ÄúConditional functional dependencies for capturing data inconsistencies,‚Äù
TODS, 2008.
J. Wang and N. Tang, ‚ÄúTowards dependable data repairing with
fixing rules,‚ÄùSIGMOD, 2014.
J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg, T. Kraska, and
T. Milo, ‚ÄúA sample-and-clean framework for fast and accurate
query processing on dirty data,‚Äù SIGMOD, 2014.
L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu, ‚ÄúOn
generating near-optimal tableaux for conditional functional dependencies,‚Äù PVLDB, 2008.
G. Cormode, L. Golab, K. Flip, A. McGregor, D. Srivastava, and
X. Zhang, ‚ÄúEstimating the confidence of conditional functional
dependencies,‚Äù in SIGMOD, 2009.
M. Arenas, L. Bertossi, and J. Chomicki, ‚ÄúConsistent query answers in inconsistent databases,‚Äù PODS, 1999.
G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma, ‚ÄúImproving data
quality: Consistency and accuracy,‚Äù VLDB, 2007.
M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F.
Ilyas, ‚ÄúGuided data repair,‚Äù VLDB, 2011.
G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúSampling from
repairs of conditional functional dependency violations,‚Äù VLDB,
2013.

[26] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, ‚ÄúTowards certain fixes
with editing rules and master data,‚Äù VLDB, 2010.
[27] G. Beskales, I. F. Ilyas, L. Golab, and A. Galiullin, ‚ÄúOn the relative
trust between inconsistent data and inaccurate constraints,‚Äù ICDE,
2013.
[28] X. L. Dong, L. Berti-Equille, and D. Srivastava, ‚ÄúTruth discovery
and copying detection in a dynamic world,‚Äù VLDB,2009.
[29] T. Dasu and J. M. Loh, ‚ÄúStatistical distortion: Consequences of data
cleaning,‚Äù VLDB, 2012.
[30] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and
Y. Ye, ‚ÄúKatara: A data cleaning system powered by knowledge
bases and crowdsourcing,‚Äù SIGMOD, 2015.
[31] J. Wang, T. Kraska, M. J. Franklin, and J. Feng, ‚ÄúCrowder: Crowdsourcing entity resolution,‚Äù VLDB, 2012.
[32] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng, ‚ÄúQasca: A qualityaware task assignment system for crowdsourcing applications,‚Äù
SIGMOD, 2015.
[33] A. Hartemink., ‚ÄúBanjo: Bayesian network inference with java
objects.‚Äù http://www.cs.duke.edu/‚àºamink/software/banjo.
[34] S. Russell and P. Norvig, Artificial intelligence: a modern approach.
Prentice Hall, 2010.
[35] T. Minka, W. J.M., J. Guiver, and D. Knowles, ‚ÄúInfer.NET 2.4‚Äù,
Microsoft Research Cambridge, 2010. http://research.microsoft.
com/infernet.
[36] E. Ristad and P. Yianilos, ‚ÄúLearning string-edit distance,‚Äù Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 1998.
[37] M. Li, Y. Zhang, M. Zhu, and M. Zhou, ‚ÄúExploring distributional
similarity based models for query spelling correction,‚Äù ICCL, 2006.
[38] A. Berger, V. Pietra, and S. Pietra, ‚ÄúA maximum entropy approach
to natural language processing,‚Äù Computational linguistics, 1996.
[39] N. Dalvi and D. Suciu, ‚ÄúEfficient query evaluation on probabilistic
databases,‚Äù VLDB, 2004.
[40] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, and D. Suciu,
‚ÄúMystiq: a system for finding more answers by using probabilities,‚Äù SIGMOD, 2005.
[41] D. Suciu and N. Dalvi, ‚ÄúFoundations of probabilistic answers to
queries,‚Äù SIGMOD, 2005.
[42] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann Publishers, 1988.
[43] A. Asuncion and D. Newman, ‚ÄúUCI machine learning repository,‚Äù
2007.
[44] F. Chiang and R. Miller, ‚ÄúDiscovering data quality rules,‚Äù VLDB,
2008.
Sushovan De is currently employed at Google. He graduated with a
Ph.D. in Computer Science from Arizona State University. His research
interests comprise Information Retrieval, Data Cleaning, and Probabilistic Databases.
Yuheng Hu is a Research Staff Member in the USER (User Systems
and Experience Research) group at IBM Research - Almaden. He
obtained his Ph.D in Computer Science at Arizona State University.
His research interests are in the areas of Machine Learning, Social
Computing and Human-Computer Interaction.
Meduri Venkata Vamsikrishna is a Ph.D student in Computer Science
at Arizona State University. He received a Master‚Äôs degree in Computer
Science from the National Unviersity of Singapore. His research interests include Data Mining from Social Media and Data Cleaning.
Yi Chen is an associate professor and the Henry J. Leir Chair in
Healthcare in the School of Management with a joint appointment in the
College of Computing Sciences at New Jersey Institute of Technology
(NJIT). She received her Ph.D. degree in Computer Science from the
University of Pennsylvania. Her current research focuses on Information
Discovery on Big Data, Social Computing and Information Integration.
Subbarao Kambhampati is a professor in Computer Science at Arizona
State University. He directs the Yochan research group which is associated with the Artifical Intelligence Lab at Arizona State University. He
is the ‚ÄùPresident-elect‚Äù of AAAI, the Association for the Advancement of
Artificial Intelligence. He secured a Ph.D. degree in Computer Science
from the University of Maryland, College Park. His research interests are
Automated Planning in Artificial Intelligence and Data and Information
Integration on the Web

Discovering Underlying Plans Based on Distributed Representations of Actions
Xin Tiana , Hankz Hankui Zhuoa & Subbarao Kambhampati b
Sun Yat-Sen University & a Arizona State University
tianxin1860@gmail.com, zhuohank@mail.sysu.edu.cn, rao@asu.edu

arXiv:1511.05662v1 [cs.AI] 18 Nov 2015

a

Abstract
Plan recognition aims to discover target plans (i.e., sequences
of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either
discover plans by maximally ‚Äúmatching‚Äù observed actions to
plan libraries, assuming target plans are from plan libraries,
or infer plans by executing domain models to best explain
the observed actions, assuming complete domain models are
available. In real world applications, however, target plans
are often not from plan libraries and complete domain models are often not available, since building complete sets of
plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and
learn vector representations of actions using the corpora; we
then discover target plans based on the vector representations.
Our approach is capable of discovering underlying plans that
are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness
of our approach by comparing its performance to traditional
plan recognition approaches in three planning domains.

Introduction
As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning chellenge (c.f.
(Cohen et al. 2015; Dong et al. 2004; Manikonda et al.
2014)). An important aspect of such a support (Kambhampati and Talamadupula 2015) is recognizing what plans the
human in the loop is making, and provide appropriate suggestions about their next actions. Although there is a lot
of work on plan recognition, much of it has traditionally
depended on the availability of a complete domain model
(Ramƒ±ÃÅrez and Geffner 2009a; Zhuo, Yang, and Kambhampati 2012). As has been argued elsewhere (Kambhampati
and Talamadupula 2015), such models are hard to get in
human-in-the-loop planning scenarios. Here, the decision
support systems have to make themselves useful without insisting on complete action models of the domain. The situation here is akin to that faced by search engines and other
tools for computer supported cooperate work, and is thus
a significant departure for the ‚Äúplanning as pure inference‚Äù
mindset of the automated planning community. As such, the
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

problem calls for plan recognition with ‚Äúshallow‚Äù models of
the domain (c.f. (Kambhampati 2007)), that can be easily
learned automatically.
There has been very little work on learning such shallow
models to support human-in-the-loop planning. Some examples include the work on Woogle system (Dong et al. 2004)
that aimed to provide support to humans in web-service
composition. That work however relied on very primitive
understanding of the actions (web services in their case) that
consisted merely of learning the input/output types of individual services.
In this paper, we focus on learning more informative models that that can help recognize the plans under construction
by the humans, and provide active support by suggesting relevant actions. To drive this process, we need to learn shallow models of the domain. We propose to adapt the recent
successes of word-vector models (Mikolov et al. 2013) in
language to our problem. Specifically, we assume that we
have access to a corpus of previous plans that the human
user has made. Viewing these plans as made up of action
words, we learn word vector models for these actions. These
models provide us a way to induce the distribution over the
identity of each unobserved action. Given the distributions
over individual unobserved actions, we use an expectationmaximization approach to infer the joint distribution over all
unobserved actions. This distribution then forms the basis
for action suggestions.
We will present the details of our approach, and will also
empirically demonstrate that it does capture a surprising
amount of structure in the observed plan sequences, leading
to effective plan recognition. We further compare its performance to traditional plan recognition techniques, including
one that uses the same plan traces to learn the STRIPS-style
action models, and use the learned model to support plan
recognition.

Problem Definition
A plan library, denoted by L, is composed of a set of
plans {p}, where p is a sequence of actions, i.e., p =
ha1 , a2 , . . . , an i where ai , 1 ‚â§ i ‚â§ n, is an action name
(without any parameter) represented by a string. For example, a string unstack-A-B is an action meaning that a robot
unstacks block A from block B. We denote the set of all possible actions by AÃÑ which is assumed to be known before-

hand. For ease of presentation, we assume that there is an
empty action, √ò, indicating an unknown or not observed action, i.e., A = AÃÑ ‚à™ {√ò}. An observation of an unknown
plan pÃÉ is denoted by O = ho1 , o2 , . . . , oM i, where oi ‚àà A,
1 ‚â§ i ‚â§ M , is either an action in AÃÑ or an empty action
√ò indicating the corresponding action is missing or not observed. Note that pÃÉ is not necessarily in the plan library L,
which makes the plan recognition problem more challenging, since matching the observation to the plan library will
not work any more.
We assume that the human is making a plan of at most
length M . We also assume that at any given point, the planner is able to observe M ‚àí k of these actions. The k unobserved actions might either be in the suffiix (i.e., yet to be
formed part) of the plan, or in the middle (due to observational gaps). Our aim is to suggest, for each of the k unobserved actions, m possible choices‚Äìfrom which the user can
select the action. (Note that we would like to keep m small,
ideally close to 1, so as not to overwhelm the user) Accordingly, we will evaluate the effectiveness of the decision support in terms of whether or not the user‚Äôs best/intended action is within the suggested m actions.
Specifically, our recognition problem can be represented
by a triple < = (L, O, A). The solution to < is to discover
the unknown plan pÃÉ that best explains O given L and A.
An example of our plan recognition problem in the blocks1
domain is shown below.
Example: A plan library L in the blocks domain is assumed to have four plans as shown below:
plan 1: pick-up-B stack-B-A pick-up-D stack-D-C
plan 2: unstack-B-A put-down-B unstack-D-C put-downD
plan 3: pick-up-B stack-B-A pick-up-C stack-C-B pickup-D stack-D-C
plan 4: unstack-D-C put-down-D unstack-C-B put-downC unstack-B-A put-down-B
An observation O of action sequence is shown below:
observation: pick-up-B √ò unstack-D-C put-down-D √ò
stack-C-B √ò √ò
Given the above input, our DUP algorithm outputs plans as
follows:
pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C
stack-C-B pick-up-D stack-D-C

Our DUP Algorithm
Our DUP approach to the recognition problem < functions
by two phases. We first learn vector representations of actions using the plan library L. We then iteratively sample
actions for unobserved actions oi by maximizing the probability of the unknown plan pÃÉ via the EM framework. We
present DUP in detail in the following subsections.
1

http://www.cs.toronto.edu/aips2000/

Learning Vector Representations of Actions
Since actions are denoted by a name strings, actions can be
viewed as words, and a plan can be viewed as a sentence.
Furthermore, the plan library L can be seen as a corpus,
and the set of all possible actions A is the vocabulary. We
thus can learn the vector representations for actions using
the Skip-gram model with hierarchical softmax, which has
been shown an efficient method for learning high-quality
vector representations of words from unstructured corpora
(Mikolov et al. 2013).
The objective of the Skip-gram model is to learn vector
representations for predicting the surrounding words in a
sentence or document. Given a corpus C, composed of a sequence of training words hw1 , w2 , . . . , wT i, where T = |C|,
the Skip-gram model maximizes the average log probability
T
1X
T t=1

X

log p(wt+j |wt )

(1)

‚àíc‚â§j‚â§c,j6=0

where c is the size of the training window or context.
The basic probability p(wt+j |wt ) is defined by the hierarchical softmax, which uses a binary tree representation
of the output layer with the K words as its leaves and for
each node, explicitly represents the relative probabilities of
its child nodes (Mikolov et al. 2013). For each leaf node,
there is an unique path from the root to the node, and this
path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector
representations for words. Instead, each inner node has an
0
output vector vn(w,j)
, and the probability of a word being
the output word is defined by
p(wt+j |wt ) =

L(wt+j )‚àí1 n

Y

œÉ(I(n(wt+j , i + 1) =

i=1

o
child(n(wt+j , i))) ¬∑ vn(wt+j ,i) ¬∑ vwt ) ,

(2)

where
œÉ(x) = 1/(1 + exp(‚àíx)).
L(w) is the length from the root to the word w in the binary
tree, e.g., L(w) = 4 if there are four nodes from the root to
w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) =
root and n(w, L(w)) = w. child(n) is a fixed child (e.g.,
left child) of node n. vn is the vector representation of the
inner node n. vwt is the input vector representation of word
wt . The identity function I(x) is 1 if x is true; otherwise it is
-1.
We can thus build vector representations of actions by
maximizing Equation (1) with corpora or plan libraries L as
input. We will exploit the vector representations to discover
the unknown plan pÃÉ in the next subsection.

Maximizing Probability of Unknown Plan pÃÉ
With the vector representations learnt in the last subsection,
a straightforward way to discover the unknown plan pÃÉ is to
explore all possible actions in AÃÑ such that pÃÉ has the highest

probability, which can be defined similar to Equation (1),
i.e.,
M
X
X
F(pÃÉ) =
log p(wk+j |wk )
(3)
k=1 ‚àíc‚â§j‚â§c,j6=0

where wk denotes the kth action of pÃÉ and M is the length of
pÃÉ. As we can see, this approach is exponentially hard with
respect to the size of AÃÑ and number of unobserved actions.
We thus design an approximate approach in the ExpectationMaximization framework to estimate an unknown plan pÃÉ
that best explains the observation O.
To do this, we introduce new parameters to capture
‚Äúweights‚Äù of values for each unobserved action. Specifically
speaking, assuming there are X unobserved actions in O,
i.e., the number of √òs in O is X, we denote these unobserved
actions by aÃÑ1 , ..., aÃÑx , ..., aÃÑX , where the indices indicate the
order they appear in O. Note that each aÃÑx can be any action
in AÃÑ. We associate each possible value of aÃÑx with a weight,
denoted by ŒìÃÑaÃÑx ,x . ŒìÃÑ is a |AÃÑ| √ó X matrix, satisfying
X
ŒìÃÑo,x = 1 ‚àß ŒìÃÑo,x ‚â• 0,
o‚ààAÃÑ

for each x. For the ease of specification, we extend ŒìÃÑ to a
bigger matrix with a size of |AÃÑ|√óM , denoted by Œì, such that
Œìo,y = ŒìÃÑo,x if y is the index of the xth unobserved action in
O, for all o ‚àà AÃÑ; otherwise, Œìo,y = 1 and Œìo0 ,y = 0 for all
o0 ‚àà AÃÑ ‚àß o0 6= o. Our intuition is to estimate the unknown
plan pÃÉ by selecting actions with the highest weights. We thus
introduce the weights to Equation (2), as shown below,
p(wk+j |wk ) =

L(wk+j )‚àí1 n

Y

An overview of our DUP algorithm is shown in Algorithm
1. In Step 2 of Algorithm 1, we initialize Œìo,k = 1/M for
all o ‚àà AÃÑ, if k is an index of unobserved actions in O; and
otherwise, Œìo,k = 1 and Œìo0 ,k = 0 for all o0 ‚àà AÃÑ ‚àß o0 6= o.
In Step 4, we view Œì¬∑,k as a probability distribution, and
sample an action from AÃÑ based on Œì¬∑,k if k is an unobserved
action index in O. In Step 5, we only update Œì¬∑,k where k
is an unobserved action index. In Step 6, we linearly project
all elements of the updated Œì to between 0 and 1, such that
we can do sampling directly based on Œì in Step 4. In Step 8,
we simply select aÃÑx based on
aÃÑx = arg max Œìo,x ,
o‚ààAÃÑ

for all unobserved action index x.
Algorithm 1 Framework of our DUP algorithm
Input: plan library L, observed actions O
Output: plan pÃÉ
1: learn vector representation of actions
2: initialize Œìo,k with 1/M for all o ‚àà AÃÑ, when k is an
unobserved action index
3: while the maximal number of repetitions is not reached
do
4:
sample unobserved actions in O based on Œì
5:
update Œì based on Equation (6)
6:
project Œì to [0,1]
7: end while
8: select actions for unobserved actions with the largest
weights in Œì
9: return pÃÉ

œÉ(I(n(wk+j , i + 1) =

i=1

o
child(n(wk+j , i))) ¬∑ avn(wk+j ,i) ¬∑ bvwk ) ,

(4)

where a = Œìwk+j ,k+j and b = Œìwk ,k . We can see that the
impact of wk+j and wk is penalized by weights a and b if
they are unobserved actions, and stays unchanged, otherwise
(since both a and b equal to 1 if they are observed actions).
We redefine the objective function as shown below,
F(pÃÉ, Œì) =

M
X

X

log p(wk+j |wk ),

(5)

k=1 ‚àíc‚â§j‚â§c,j6=0

where p(wk+j |wk ) is defined by Equation (4). The only parameters needed to be updated are Œì, which can be easily
done by gradient descent, as shown below,
Œìo,x = Œìo,x + Œ¥

‚àÇF
,
‚àÇŒìo,x

(6)

if x is the index of unobserved action in O; otherwise, Œìo,x
stays unchanged, i.e., Œìo,x = 1. Note that Œ¥ is a learning
constant.
With Equation (6), we can design an EM algorithm by repeatedly sampling an unknown plan according to Œì and updating Œì based on Equation (6) until reaching convergence
(e.g., a constant number of repetitions is reached).

Experiments
In this section, we evaluate our DUP algorithms in three
planning domains from International Planning Competition,
i.e., blocks1 , depots2 , and driverlog2 . To generate training
and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems
with a planning solver, such as FF3 , to produce 5000 plans.
We then randomly divided the plans into ten folds, with 500
plans in each fold. We ran our DUP algorithm ten times to
calculate an average of accuracies, each time with one fold
for testing and the rest for training. In the testing data, we
randomly removed actions from each testing plan (i.e., O)
with a specific percentage Œæ of the plan length. Features of
datasets are shown in Table 1, where the second column is
the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last
column is the size of vocabulary used in all plans.
We define the accuracy of our DUP algorithm as follows. For each unobserved action aÃÑx DUP suggests a set of
possible actions Sx which have the highest value of ŒìaÃÑx ,x
for all aÃÑx ‚àà AÃÑ. If Sx covers the truth action atruth , i.e.,
2
http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume20/long03ahtml/JAIRIPC.html
3
https://fai.cs.uni-saarland.de/hoffmann/ff.html

(b) depots

(c) driverlog
0.8	 ¬†

0.7	 ¬†

0.7	 ¬†

0.7	 ¬†

0.6	 ¬†

0.6	 ¬†

0.6	 ¬†

0.5	 ¬†

0.5	 ¬†

0.5	 ¬†

0.4	 ¬†
0.3	 ¬†

DUP	 ¬†

0.2	 ¬†

MatchPlan	 ¬†

0.1	 ¬†

ARMS+PRP	 ¬†

0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

accuracy

0.8	 ¬†

accuracy

accuracy

(a) blocks
0.8	 ¬†

0.4	 ¬†
0.3	 ¬†
0.2	 ¬†

DUP	 ¬†
MatchPlan	 ¬†
ARMS+PRP	 ¬†

0.1	 ¬†
0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

DUP	 ¬†
MatchPlan	 ¬†
ARMS+PRP	 ¬†

0.4	 ¬†
0.3	 ¬†
0.2	 ¬†
0.1	 ¬†
0	 ¬†
0.05	 ¬†

0.1	 ¬†

0.15	 ¬†

0.2	 ¬†

0.25	 ¬†

percentage of unobserved actions

Figure 1: Accuracy with respect to different percentage of unobserved actions
Table 1: Features of datasets
domain
blocks
depots
driverlog

#plan
5000
5000
5000

#word
292250
209711
179621

#vocabulary
1250
2273
1441

atruth ‚àà Sx , we increase the number of correct suggestions
g by 1. We thus define the accuracy acc as shown below:
acc =

1
T

T
X
i=1

#hcorrect-suggestionsii
,
Ki

where T is the size of testing set, #hcorrect-suggestionsii
is the number of correct suggestions for the ith testing plan,
Ki is the number of unobserved actions in the ith testing
plan. We can see that the accuracy acc may be influenced by
Sx . We will test different size of Sx in the experiment.
State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries
to best explain the observed actions (Geib and Steedman
2007), which we denote by MatchPlan. We develop a
MatchPlan system based on the idea of (Geib and Steedman 2007) and compare our DUP algorithm to MatchPlan
with respect to different percentage of unobserved actions
Œæ and different size of suggestion set Sx . Another baseline
is action-models based plan recognition approach (Ramirez
and Geffner 2009b) (denoted by PRP, short for Plan Recognition as Planning). Since we do not have action models as
input in our DUP algorithm, we exploited the action model
learning system ARMS (Yang, Wu, and Jiang 2007) to learn
action models from the plan library and feed the action models to the PRP approach. We call this hybrid plan recognition approach ARMS+PRP. To learn action models, ARMS
requires state information of plans as input. We thus added

extra information, i.e., initial state and goal of each plan in
the plan library, to ARMS+PRP. In addition, PRP requires as
input a set of candidate goals G for each plan to be recognized in the testing set, which was also generated and fed to
PRP when testing. In summary, the hybrid plan recognition
approach ARMS+PRP has more input information, i.e., initial states and goals in plan library and candidate goals G for
each testing example, than our DUP approach.

Accuracy w.r.t. Percentage of Unobserved Actions
We first evaluate our DUP algorithm with respect to different
percentage of unobserved actions Œæ in O. We set the window of training context c in Equation (1) to be three and
the size of recommendations to be ten. We compare our
DUP algorithm to both MatchPlan and ARMS+PRP. To
make fair comparison (to MatchPlan), we set the matching window MatchPlan to be three as well when searching plans from plan libraries L. In other words, to estimate
an unobserved action aÃÑx in O, MatchPlan matches previous three actions and subsequent three actions of aÃÑx to
plans in L, and recommends ten actions with maximal number of matched actions, considering unobserved actions (√ò
in the context of aÃÑx ) and actions in L as a successful matching. For ARMS+PRP, we generated 20 candidate goals for
each testing example including the ground-truth goal which
corresponds to the ground-truth plan to be recognized. The
results are shown in Figure 1.
From Figure 1, we can see that in all three domains,
the accuracy of our DUP algorithm is generally higher than
MatchPlan and ARMS+PRP, which verifies that our DUP
algorithm can indeed capture relations among actions better than previous matching approaches. The rationale is that
we explore global plan information from the plan library to
learn a ‚Äúshallow‚Äù model (distributed representations of actions) and use this model with global information to best

(a) blocks

0.6	 ¬†

accuracy

0.5	 ¬†
0.4	 ¬†

0.8	 ¬†

0.8	 ¬†

0.7	 ¬†

0.7	 ¬†

0.6	 ¬†

0.6	 ¬†

accuracy

0.7	 ¬†

accuracy

(c) driverlog

(b) depots

0.8	 ¬†

0.5	 ¬†
0.4	 ¬†

0.5	 ¬†
0.4	 ¬†

0.3	 ¬†

0.3	 ¬†

0.3	 ¬†

0.2	 ¬†

0.2	 ¬†

0.2	 ¬†

0.1	 ¬†

0.1	 ¬†

0.1	 ¬†

0	 ¬†
1	 ¬†

2	 ¬†

3	 ¬†

4	 ¬†

5	 ¬†

6	 ¬†

7	 ¬†

8	 ¬†

9	 ¬† 10	 ¬†

0	 ¬†

0	 ¬†

1	 ¬† 2	 ¬† 3	 ¬†

size of recommendations

4	 ¬† 5	 ¬† 6	 ¬†

7	 ¬† 8	 ¬† 9	 ¬† 10	 ¬†

size	 ¬†of	 ¬†recommenda9ons	 ¬†
DUP	 ¬†

MatchPlan	 ¬†

1	 ¬†

2	 ¬†

3	 ¬†

4	 ¬†

5	 ¬†

6	 ¬†

7	 ¬†

8	 ¬†

9	 ¬† 10	 ¬†

size of recommendations

ARMS+PRP	 ¬†

Figure 2: Accuracy with respect to different size of recommendations
explain the observed actions. In contrast, MatchPlan just
utilizes local plan information when matching the observed
actions to the plan library which results in lower accuracies.
Although ARMS+PRP tries to leverage global plan information from the plan library to learn action models and uses
the models to recognize observed actions, it enforces itself
to extract ‚Äúexact‚Äù models represented by planning models
which are often with noise. When feeding those noisy models to PRP, since PRP that uses planning techniques to recognize plans is very sensitive to noise of planning models,
the recognition accuracy is lower than DUP, even though
ARMS+PRP has more input information (i.e., initial states
and candidate goals) than our DUP algorithm.
Looking at the changes of accuracies with respect to the
percentage of unobserved actions, we can see that our DUP
algorithm performs fairly well even when the percentage of
unobserved action reaches 25%. In contrast, ARMS+PRP is
sensitive to the percentage of unobserved actions, i.e., the accuracy goes down when more actions are unobserved. This
is because the noise of planning models induces more uncertain information, which harms the recognition accuracy,
when the percentage of unobserved actions becomes larger.
Comparing accuracies of different domains, we can see that
our DUP algorithm functions better in the blocks domain
than the other two domains. This is because the ratio of
#word over #vocabulary in the blocks domain is much larger
than the other two domains, as shown in Table 1. We would
conjecture that increasing the ratio could improve the accuracy of DUP.

Accuracy w.r.t. Size of Recommendation Set
We next evaluate the performance of our DUP algorithm
with respect to the size of recommendation set Sx . Likewise, we set the context window c used in Equation (1) to be
three, which was also set when matching the observed actions O to plan libraries L in the MatchPlan approach. For

ARMS+PRP, the number of candidate goals for each testing
example is set to 20. ARMS+PRP aims to recognize plans
that are optimal with respect to the cost of actions. We relax ARMS+PRP to output |Sx | optimal plans, some of which
might be suboptimal. We varied the number of actions recommended by DUP (or MatchPlan) from 1 to 10. The results
are shown in Figure 2.
From Figure 2, we find that accuracies of the three approaches generally become larger when the size of the recommended action set increases in all three domains. This
is consistent with our intuition, since the larger the recommended action set is, the higher the possibility for the
truth action to be in the recommended action set. We can
also see that the accuracy of our DUP algorithm are generally larger than both MatchPlan and ARMS+PRP in
all three domains, which verifies that our DUP algorithm
can indeed better capture relations among actions and thus
recognize unobserved actions better than the matching approach MatchPlan and the planning model learning approach ARMS+PRP. The reason is similar to the one given
for Figure 1 in the previous section. That is, the ‚Äúshadow‚Äù
model learnt by our DUP algorithm is better for recognizing
plans than both the ‚Äúexact‚Äù planning model learnt by ARMS
for recognizing plans with planning techniques and the local
matching approach MatchPlan. On the other hand, we can
also see the accuracy of ARMS+PRP is generally higher than
MatchPlan. This verifies that the additional information
of initial states and candidate goals exploited by ARMS+PRP
can indeed help improve the accuracy. Furthermore, the advantage of DUP becomes even larger when the size of recommended action set increases, which suggests our vector
representation based learning approach can better capture
action relations when the size of recommended action set
is larger. The possibility of actions correctly recognized by
DUP becomes much larger than the other two approaches
when the size of recommendations increases.

Related work
Kautz and Allen proposed an approach to recognizing plans
based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free
rule in an ‚Äúaction grammar‚Äù (Kautz and Allen 1986). All actions, plans are uniformly referred to as goals, and a recognizer‚Äôs knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic,
which defines abstraction, decomposition and functional relationships between types of events. Lesh and Etzioni further
presented methods in scaling up activity recognition to scale
up his work computationally (Lesh and Etzioni 1995). They
automatically constructed plan-library from domain primitives, which was different from (Kautz and Allen 1986)
where the plan library was explicitly represented. In these
approaches, the problem of combinatorial explosion of plan
execution models impedes its application to real-world domains. Kabanza and Filion (Kabanza et al. 2013) proposed
an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted
model counting. These approaches are, however, difficult to
represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to
explain the actions observed.
Instead of using a library of plans, Ramirez and Geffner
(Ramirez and Geffner 2009b) proposed an approach to solving the plan recognition problem using slightly modified
planning algorithms, assuming the action models were given
as input. Except previous work (Kautz and Allen 1986;
Bui 2003; Geib and Goldman 2009; Ramirez and Geffner
2009b) on the plan recognition problem presented in the
introduction section, Note that action models can be created by experts or learnt by previous systems, such as ARMS
(Yang, Wu, and Jiang 2007) and LAMP (Zhuo et al. 2010).
Saria and Mahadevan presented a hierarchical multi-agent
markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems
(Saria and Mahadevan 2004). Singla and Mooney proposed
an approach to abductive reasoning using a first-order probabilistic logic to recognize plans (Singla and Mooney 2011).
Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories
(Amir and Gal 2011). Ramirez and Geffner exploited offthe-shelf classical planners to recognize probabilistic plans
(Ramirez and Geffner 2010).
Early work on human-in-the-loop planning scenarios
in automated planning went under the name of ‚Äúmixedinitiative planning‚Äù (e.g. (Ferguson, Allen, and Miller
1996)). An important limitation of that work was that the humans in the loop were helping the automated planner (with
a complete action model) navigate its search space of plans
more efficiently. In contrast, we are interested in planning
technology that helping humans develop plans, even in the
absence of complete formal models of the planning domain.
While some work in web-service composition (c.f. (Dong
et al. 2004)) did focus on this type of planning support,
they were hobbled by being limited to simple input/output

type comparison. In contrast, we believe that DUP learns
and uses a model that captures more of the structure of the
planning domain (while still not insisting on complete action
models).
While DUP focuses on learning models from plan corpora, some recent work looked at using crowdsourcing to
acquire domain models. For example, Lasecki et al. (Lasecki
et al. 2013) introduce Legion:AR, which combines the benefits of automatic and human activity labeling for robust and
deployable activity recognition. The system exploits an active learning approach (Zhao, Sukthankar, and Sukthankar
2011) in which automatic activity recognition is augmented
with on-demand activity labels from the crowd when an observed activity cannot be confidently classified. By engaging a group of people, Legion:AR is able to label activities
as they occur more reliably than a single person can, especially in complex domains with multiple actors performing
activities quickly. Lasecki et al. (Lasecki et al. 2014) built
a crowdsourcing based system called ARchitect, using the
crowd to capture the dependency structure of the actions that
make up activities. Such crowd-sourcing methods can complement the plan-corpus based approach proposed in DUP.

Conclusion and Discussion
In this paper we present a novel plan recognition approach
DUP based on vector representation of actions. We first learn
the vector representations of actions from plan libraries using the Skip-gram model which has been demonstrated to
be effective. We then discover unobserved actions with the
vector representations by repeatedly sampling actions and
optimizing the probability of potential plans to be recognized. We also empirically exhibit the effectiveness of our
approach.
While we focused on a one-shot recognition task in this
paper, in practice, human-in-the-loop planning will consist
of multiple iterations, with DUP recognizing the plan and
suggesting action addition alternatives; the human making a
selection and revising the plan. The aim is to provide a form
of flexible plan completion tool, akin to auto-completers for
search engine queries. To do this efficiently, we need to make
the DUP recognition algorithm ‚Äúincremental.‚Äù
The word-vector based domain model we developed in
this paper provides interesting contrasts to the standard precondition and effect based action models used in automated
planning community. One of our future aims is to provide a more systematic comparison of the tradeoffs offered
by these models. Although we have focused on the ‚Äúplan
recognition‚Äù aspects of this model until now, and assumed
that ‚Äúplanning support‚Äù will be limited to suggesting potential actions to the humans. In future, we will also consider ‚Äúcritiquing‚Äù the plans being generated by the humans
(e.g. detecting that an action introduced by the human is not
consistent with the model learned by DUP), and ‚Äúexplaining/justifying‚Äù the suggestions generated by humans. Here,
we cannot expect causal explanations of the sorts that can
be generated with the help of complete action models (e.g.
(Petrie 1992)), and will have to develop justifications analogous to those used in recommendation systems.

References
[Amir and Gal 2011] Amir, O., and Gal, Y. K. 2011. Plan
recognition in virtual laboratories. In Proceedings of IJCAI,
2392‚Äì2397.
[Bui 2003] Bui, H. H. 2003. A general model for online
probabilistic plan recognition. In Proceedings of IJCAI,
1309‚Äì1318.
[Cohen et al. 2015] Cohen, P. R.; Kaiser, E. C.; Buchanan,
M. C.; Lind, S.; Corrigan, M. J.; and Wesson, R. M. 2015.
Sketch-thru-plan: a multimodal interface for command and
control. Commun. ACM 58(4):56‚Äì65.
[Dong et al. 2004] Dong, X.; Halevy, A. Y.; Madhavan, J.;
Nemes, E.; and Zhang, J. 2004. Simlarity search for web services. In (e)Proceedings of the Thirtieth International Conference on Very Large Data Bases, Toronto, Canada, August
31 - September 3 2004, 372‚Äì383.
[Ferguson, Allen, and Miller 1996] Ferguson, G.; Allen, J.;
and Miller, B. 1996. Trains-95: Towards a mixed-initiative
planning assistant. In Proceedings of the Third Conference
on Artificial Intelligence Planning Systems (AIPS-96), 70‚Äì
77. Edinburgh, Scotland.
[Geib and Goldman 2009] Geib, C. W., and Goldman, R. P.
2009. A probabilistic plan recognition algorithm based on
plan tree grammars. Artificial Intelligence 173(11):1101‚Äì
1132.
[Geib and Steedman 2007] Geib, C. W., and Steedman, M.
2007. On natural language processing and plan recognition. In IJCAI 2007, Proceedings of the 20th International
Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, 1612‚Äì1617.
[Kabanza et al. 2013] Kabanza, F.; Filion, J.; Benaskeur,
A. R.; and Irandoust, H. 2013. Controlling the hypothesis
space in probabilistic plan recognition. In IJCAI.
[Kambhampati and Talamadupula 2015] Kambhampati, S.,
and Talamadupula, K. 2015. Human-in-the-loop planning
and decision support. rakaposhi.eas.asu.edu/hilp-tutorial.
[Kambhampati 2007] Kambhampati, S. 2007. Model-lite
planning for the web age masses: The challenges of planning
with incomplete and evolving domain models. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia,
Canada, 1601‚Äì1605.
[Kautz and Allen 1986] Kautz, H. A., and Allen, J. F. 1986.
Generalized plan recognition. In Proceedings of AAAI, 32‚Äì
37.
[Lasecki et al. 2013] Lasecki, W. S.; Song, Y. C.; Kautz,
H. A.; and Bigham, J. P. 2013. Real-time crowd labeling
for deployable activity recognition. In CSCW, 1203‚Äì1212.
[Lasecki et al. 2014] Lasecki, W. S.; Weingard, L.; Ferguson,
G.; and Bigham, J. P. 2014. Finding dependencies between
actions using the crowd. In Proceedings of CHI, 3095‚Äì3098.
[Lesh and Etzioni 1995] Lesh, N., and Etzioni, O. 1995. A
sound and fast goal recognizer. In IJCAI, 1704‚Äì1710.
[Manikonda et al. 2014] Manikonda, L.; Chakraborti, T.; De,
S.; Talamadupula, K.; and Kambhampati, S. 2014. AI-MIX:

using automated planning to steer human workers towards
better crowdsourced plans. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27
-31, 2014, QueÃÅbec City, QueÃÅbec, Canada., 3004‚Äì3009.
[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.;
Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In
NIPS, 3111‚Äì3119.
[Petrie 1992] Petrie, C. J. 1992. Constrained decision revision. In Proceedings of the 10th National Conference on
Artificial Intelligence. San Jose, CA, July 12-16, 1992., 393‚Äì
400.
[Ramƒ±ÃÅrez and Geffner 2009a] Ramƒ±ÃÅrez, M., and Geffner, H.
2009a. Plan recognition as planning. In IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17,
2009, 1778‚Äì1783.
[Ramirez and Geffner 2009b] Ramirez, M., and Geffner, H.
2009b. Plan recognition as planning. In Proceedings of IJCAI, 1778‚Äì1783.
[Ramirez and Geffner 2010] Ramirez, M., and Geffner, H.
2010. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings of AAAI, 1121‚Äì1126.
[Saria and Mahadevan 2004] Saria, S., and Mahadevan, S.
2004. Probabilistic plan recognitionin multiagent systems.
In Proceedings of AAAI.
[Singla and Mooney 2011] Singla, P., and Mooney, R. 2011.
Abductive markov logic for plan recognition. In Proceedings of AAAI, 1069‚Äì1075.
[Yang, Wu, and Jiang 2007] Yang, Q.; Wu, K.; and Jiang, Y.
2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence Journal
171:107‚Äì143.
[Zhao, Sukthankar, and Sukthankar 2011] Zhao, L.; Sukthankar, G.; and Sukthankar, R. 2011. Robust active
learning using crowdsourced annotations for activity
recognition. In AAAI workshop.
[Zhuo et al. 2010] Zhuo, H. H.; Yang, Q.; Hu, D. H.; and Li,
L. 2010. Learning complex action models with quantifiers
and implications. Artificial Intelligence 174(18):1540‚Äì1569.
[Zhuo, Yang, and Kambhampati 2012] Zhuo, H. H.; Yang,
Q.; and Kambhampati, S. 2012. Action-model based multiagent plan recognition. In Advances in Neural Information
Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of
a meeting held December 3-6, 2012, Lake Tahoe, Nevada,
United States., 377‚Äì385.

J Intell Inf Syst
DOI 10.1007/s10844-015-0366-3

Click efficiency: a unified optimal ranking for online Ads
and documents
Raju Balakrishnan1 ¬∑ Subbarao Kambhampati2

Received: 21 December 2013 / Revised: 23 March 2015 / Accepted: 12 May 2015
¬© Springer Science+Business Media New York 2015

Abstract Ranking of search results and ads has traditionally been studied separately. The
probability ranking principle is commonly used to rank the search results while the ranking
based on expected profits is commonly used for paid placement of ads. These rankings try
to maximize the expected utilities based on the user click models. Recent empirical analysis on search engine logs suggests unified click models for both ranked ads and search
results (documents). These new models consider parameters of (i) probability of the user
abandoning browsing results (ii) perceived relevance of result snippets. However, current
document and ad ranking methods do not consider these parameters. In this paper we propose a generalized ranking function‚Äînamely Click Efficiency (CE)‚Äîfor documents and
ads based on empirically proven user click models. The ranking considers parameters (i)
and (ii) above, optimal and has the same time complexity as sorting. Furthermore, the CE
ranking exploits the commonality of click models, hence is applicable for both documents
and ads. We examine the reduced forms of CE ranking based upon different underlying
assumptions, enumerating a hierarchy of ranking functions. Interestingly, some of the rankings in the hierarchy are currently used ad and document ranking functions; while others
suggest new rankings. Thus, this hierarchy illustrates the relationships between different
rankings, and clarifies the underlying assumptions. While optimality of ranking is sufficient for document ranking, applying CE ranking to ad auctions requires an appropriate
pricing mechanism. We incorporate a second price based mechanism with the proposed
ranking. Our analysis proves several desirable properties including revenue dominance over

 Raju Balakrishnan

raju.balakrishnan@gmail.com
Subbarao Kambhampati
rao@asu.edu
1

Groupon., Park Blvd, Palo Alto, CA 94306, USA

2

Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

J Intell Inf Syst

Vickrey Clarke Groves (VCG) for the same bid vector and existence of a Nash equilibrium
in pure strategies. The equilibrium is socially optimal, and revenue equivalent to the truthful
VCG equilibrium. As a result of its generality, the auction mechanism and the equilibrium
reduces to the current mechanisms including Generalized Second Price Auction (GSP) and
corresponding equilibria. Furthermore, we relax the independence assumption in CE ranking and analyze the diversity ranking problem. We show that optimal diversity ranking is
NP-Hard in general, and a constant time approximation algorithm is not likely. Finally our
simulations to quantify the amount of increase in different utility functions conform to the
results, and suggest potentially significant increase in utilities.
Keywords Ad ranking ¬∑ Document ranking ¬∑ Diversity ¬∑ Auctions ¬∑ Click models

1 Introduction
Search engines rank results to maximize the relevance of the top documents. On the other
hand, targeted ads are ranked primarily to maximize the profit from clicks. In general,
users browse through ranked lists of search results or ads from top to bottom, either clicking or skipping the results, or abandoning browsing the list altogether due to impatience
or satiation. The goal of the ranking is to maximize the expected relevances (or profits) of
clicked results based on the click model of the users. The sort by relevance ranking suggested by Probability Ranking Principle (PRP) has been commonly used for search results
for decades (Robertson 1977; Gordon and Lenk 1991). In contrast, sorting by the expected
profits calculated as the product of bid amount and Click Through Rate (CTR) is popular
for ranking ads (Richardson et al. 2007).
Recent click models suggests that the user click behaviors for both search results and targeted ads is the same (Guo et al. 2009; Zhu et al. 2010). Considering this commonality, the
only difference between the two ranking problems is the utility of entities ranked: for documents utility is the relevance and for the ads it is the cost-per-click (CPC). This suggests the
possibility of a unified ranking function for search results and ads. The current segregation
of document and ad ranking as separate areas does not consider this commonality. A unified approach often helps to widen the scope of the related research to these two areas, and
enables applications of existing ranking function in one area on isomorphic problems in the
other area as we will show below.
In addition to the unified approach, the recent click models consider the following
parameters:
1.

2.

Browsing Abandonment: The user may abandon browsing ranked list at any point.
The likelihood of abandonment may depend on the entities the user has already
seen (Zhu et al. 2010).
Perceived Relevance: Perceived relevance is the user‚Äôs relevance assessment viewing
only the search snippet or ad impression. The decision to click or not depends on the
perceived relevance, not on the actual relevance of the results (Yue et al. 2010; Clarke
et al. 2007).

Though these parameters are part of the click models (Guo et al. 2009; Zhu et al. 2010) how
to exploit these parameters to improve ranking is currently unknown. The current document
ranking is based on the simplifying assumption that the perceived relevance is the same as
the actual relevance of the document, and ignores browsing abandonment. The ad placement
partially considers perceived relevance, but ignores abandonment probabilities.

J Intell Inf Syst

In this paper, we propose a unified optimal ranking function‚Äînamely Click Efficiency
(CE)‚Äîbased on a generalized click model of the user. CE is defined as the ratio of the
standalone utility generated by an entity to the sum of the abandonment probability and the
click probability of that entity, where the abandonment probability is the probability for the
user to leave browsing the list after seeing the entity. We show that sorting entities in the
descending order of CE guarantees optimum ranking utility. We do not make assumptions
on the utilities of the entities, which may be assessed relevance for documents or cost per
click (CPC) charged based on the auction for ads. On plugging in the appropriate utilities‚Äî
relevance for documents and CPC for the ads‚Äîthe ranking specializes to document and ad
ranking.
As a consequence of the generality, the proposed ranking will reduce to specific ranking
problems on assumptions about user behavior. We enumerate a hierarchy of ranking functions corresponding to different assumptions on the click model. Most interestingly, some of
these special cases correspond to the currently used document and ad ranking functions‚Äî
including PRP and sort by expected profit described above. Further, some of the reduced
ranking functions suggest new rankings for special cases of the click model‚Äîlike a click
model in which the user never abandons the search, or the perceived relevance is approximated as the actual relevance. This hierarchy elucidates interconnection between different
ranking functions and the assumptions behind the rankings. We believe that this will help in
choosing the appropriate ranking function for a particular user click behavior.
Ranking in ad placement used in conjunction with a pricing strategy to form the complete
auction mechanism. Hence to apply the CE ranking on ad placement, a pricing mechanism has to be associated. We incorporate a second-price based pricing mechanism with
the proposed ranking. Our analysis establishes many interesting properties of the proposed
mechanism. Particularly, we state and prove the existence of a Nash Equilibrium in pure
strategies. At this equilibrium, the profits of the search engine and the total revenue of the
advertisers is simultaneously optimized. Like ranking, the proposed auction this is a generalized mechanism, and reduces to the existing GSP and Overture mechanisms under the
same assumptions as that of the ranking. Further, the stated Nash Equilibrium is a general
case of the equilibriums of these existing mechanisms. Comparing the mechanism properties with that of VCG (Vickrey 1961; Clarke 1971; Groves 1973), we show that for the same
bid vector, search engine revenue for the CE mechanism is greater or equal to that of VCG.
Furthermore, the revenue for the proposed equilibrium is equal to the revenue of the truthful
dominant strategy equilibrium of VCG.
Our analysis so far has been based on the assumption of parameter independence between
the ranked entities. We relax this assumption and analyze the implications based on a specific well known problem‚Äîdiversity ranking (Carterette 2010; Agrawal et al. 2009; Rafiei
et al. 2010). Diversity ranking tries to maximize the collective utility of top-k ranked entities. For a ranked list, an entity will reduce residual utility of a similar entity in the list blow
it. Though optimizing all the current ranking functions incorporating diversity is known to
be NP-Hard (Carterette 2010), an understanding of why this is an inherently hard problem
is lacking. We show that optimizing set utilities is NP-Hard even for the basic form of diversity ranking. Furthermore we extend our proof showing that a constant ratio approximation
algorithm is unlikely. As a benefit of the generality of ranking, these results are applicable
both for ads and documents.
Although we prove the optimality of the proposed ranking, the amount by which the
profit may improve is not clear. Considering the very restricted access to online experiments on ads, we performed simulations to this end. We compare the profit improvement
by the CE and reduced forms to existing rankings. These experiments suggest potentially

J Intell Inf Syst

significant increase in profits. We believe that these experiments will motivate further online
evaluations.
In summary, the contributions of the unified ranking, including both ad and document
domains are:
1.
2.
3.
4.
5.

Unified optimal ranking.
Optimal ranking considering abandonment probabilities for documents and ads.
Optimal Ranking considering perceived relevance of documents and ads.
A unified hierarchy of ranking functions and enumerating optimal rankings for different
click models.
Analysis of general diversity ranking problem and hardness proofs.

Our contributions to ad placement are:
1.
2.
3.

Design and analysis of a generalized ad auction mechanism incorporating pricing with
CE ranking.
Proof of the existence of a socially optimal Nash Equilibrium with optimal advertisers
revenue as well as optimal search engine profit.
Proof of search engine revenue dominance over VCG for equivalent bid vectors, and
equilibrium revenue equivalence to the truthful VCG equilibrium.

1.1 Background
In search and search advertising, both search results and ads are ranked to maximize utility.
At a high level, search results are ranked to maximize the information content (or relevance)
of the top documents to the users; whereas ads are ranked to maximize both the relevance
as well as the profit to the search engines. Users generally browse through ranked search
results starting from the top, either clicking or skipping the results. This browsing pattern
of users is called the click model. Search and ad rankings try to maximize the utility to the
users based on a click model.
In addition to the standalone relevance of the results, another important aspect of ranking
is the diversity of the results. Although information contained in a document may be highly
relevant, if the information is similar to that in the documents above in the ranking, the document will be of little utility. To account for this factor, the mutual influence of documents
or ads ranked needs to be considered to maximize total utility by a set of documents rather
than individual documents. To account for this factor, diversity-sensitive ranking maximizes
residual relevance of ads or documents in the context of other items in the ranked list.
In search ad ranking (paid placements), ads are selected based on the user query. Generally, the click model for ads is similar to that of the search results. In the most common
pay-per-click ad campaigns, advertisers pay a certain amount to the search engines whenever a user clicks on their ads. This amount is determined by a pricing mechanism. The
advertisers place a bid on the queries. The ads are ranked based on the bid amounts and
relevance of the ad to the query. For example, in commonly used Generalized Second
Price (GSP) auction Edelman et al. (2007) ads are ranked by the product of their click
rates (ratio of the number of clicks to impressions) and bid amounts. The amount the
advertisers pay to the search engine need not be equal to the bid amount, but rather determined by the pricing mechanism. For example, in GSP auction, this amount is determined
based the the bid amount and the click rates of the given ad and the ad placed below the
given ad. Thus ranking and pricing together determines the auction mechanism of the ad
placement.

J Intell Inf Syst

The rest of this paper is organized as the follows. The next section reviews related work.
Section 3 explains the click model used for our analysis. Subsequently we introduce our
optimal ranking function, and discuss the intuitions and implications. In Section 5 reductions of our ranking function to several document and ad ranking functions under limiting
assumptions are enumerated. Furthermore we discuss several useful special cases of our
ranking and assumptions under which they are optimal. In Section 6, we incorporate a pricing strategy to design a complete auction mechanism for ads. Several useful properties are
established, including the existence of a Nash equilibrium and revenue dominance over
VCG. Section 7 explores the ranking considering mutual influences and proves our hardness results. We present the experiments and results in Section 8. Finally we discuss our
conclusions and discuss potential future research directions.

2 Related work
The impact of click models on ranking has been analyzed in ad-placement. In our previous
paper Balakrishnan and Kambhampati (2008) we proposed an optimal ad ranking considering mutual influences. The ranking uses the same user model, but the paper considers only
ad ranking, and does not include generalizations and auctions. Later Aggarwal et al. (2008)
as well as Kempe and Mahdian (2008) analyzed placement of ads using a similar Markovian click model. The click model used is less detailed than our model since abandonment
is not modeled separately from click probability. These two papers optimize the sum of the
revenues of the advertisers. We optimize search engine profits in this paper. Nevertheless,
the ranking formulation has common components with these two papers, as workshop version of this paper Balakrishnan and Kambhampati (2008) as these three papers formulated
ranking based on the similar browsing models independently at almost the same time frame.
But, unlike this paper, any of the other two papers do not have a pricing, auctions, or a
generalized taxonomy.
Edelman et al. (2007) analyze a version of GSP auction in their classic paper. They
assume that the click probability at a position is a constant. We relax this assumption, and
account for the influence of ads above on the click probabilities at a position. This difference
gives rise to additional complexities and interesting differences in our mechanism. We show
that GSP proposed by Edelman et al. is a special case of our proposed mechanism.
Giotis and Karlin (2008) extend Markovian model ranking by applying GSP pricing and
analyzing the equilibrium. The GSP pricing and ranking lacks the optimality and generality
properties we prove in this paper. Deng and Yu (2009) extend Markovian models by suggesting a ranking and pricing schema for the search engines and prove the existence of a
Nash Equilibrium. The ranking is a simpler bid based ranking (not based on CPC as in our
case); and mechanism as well as equilibrium do not show optimality properties. Our paper
is different from both the above works by using a more detailed model, by having optimality properties, detailed comparisons with other baseline mechanisms, and in the ability to
generalize to a family of rankings.
Kuminov and Tennenholtz (2009) proposed a Pay Per Action (PPA) model similar to
the click models and compared the equilibrium of GSP mechanism on the model with the
VCG. Ad auctions considering influence of other ads on conversion rates are analyzed by
Ghosh and Sayedi (2010). Both these papers address different problems than considered in
this paper.
Our proposed model is a general case of the positional auctions model by Varian (2007).
Positional auctions assume static click probabilities for each position independent of other

J Intell Inf Syst

ads. We assume more realistic dynamic click probabilities depending on the ads above.
Since we consider these externalities, our model, auction, and analysis are more complex.
(e.g. monotonically increasing values and prices with positions).
The existing document ranking based on PRP (Robertson 1977) claims that a retrieval
order sorted on relevance leads to the largest number of relevant documents in a result set
than any other policy. Gordon and Lenk (1991, 1992) identified the required assumptions for
the optimality of the ranking according to PRP. Our discussion on PRP may be considered
as an independent formulation of assumptions under which PRP is optimal for web ranking.
There are number of user behavior studies in click models validating our assumed user
model and ranking function. There are a number of position based and cascade models
studied (Dupret and Piwowarski 2008; Craswell et al. 2008; Guo et al. 2009; Chapelle and
Zhang 2009; Zhu et al. 2010; Xu et al. 2010; Hu et al. 2011). In particular, General Click
Model (GCM) by Zhu et al. (2010) is interesting, since many other click models are special
cases of GCM. Zhu et al. (2010) list assumptions under which the GCM would reduce to
other click models. We will discuss the relations of our model to GCM below. Optimizing
utilities of two dimensional placement of search results has been studied by Chierichetti
et al. (2011). Many of the recent click models are more general than the click model used
in our paper, but please note that the contribution of our paper is not the click model, but a
unified optimal ranking and auction mechanism based on the click model.
Along with the current click models, there has been research on evaluating perceived
relevance of the search snippets (Yue et al. 2010) and ad impressions (Clarke et al. 2007).
Research in this direction neatly complements our new ranking function by estimating the
parameters required. Chapelle and Zhang (2009) demonstrated that separately modeling perceived and actual relevances improves relevance assessment of documents using click logs.
Diversity ranking has received considerable attention recently (Agrawal et al. 2009;
Rafiei et al. 2010). The objective functions used to measure diversity by prior works are
known to be NP-Hard (Carterette 2010). We provide a stronger proof showing that even
the basic diversity ranking problem is NP-Hard irrespective of any specific objective function, and further show that a constant ratio approximation is unlikely. To the best of our
knowledge, this paper is the first unified optimal ranking and auction mechanism based on
a generalized click model.

3 Click model
As we mentioned above, we approach the ranking as an optimization based on the user‚Äôs
click model on the ads. The expected utilities are maximized based on the click model.
For the optimization, we assume a basic user click model in which the web user browses
the entity list in the ranked order, as shown in Fig. 1. The symbols used in this paper are
explained in Table 1. At every result entity, the user may:
1.

2.

3.

Click the result with perceived relevance C(e). We define the perceived relevance as the
probability of clicking the entity ei having seen ei i.e. C(ei ) = P (click(ei )|view(ei )).
Note that the Click Through Rate (CTR) defined in ad placement is the same as the
perceived relevance defined here (Richardson et al. 2007).
Abandon browsing the result list with abandonment probability Œ≥ (ei ). Œ≥ (ei ) is defined
as the probability of abandoning the search at ei having seen ei . i.e. Œ≥ (ei ) =
P (abandonment (ei )|view(ei )).
Go to the next entity with probability [1 ‚àí (C(ei ) + Œ≥ (ei ))]

J Intell Inf Syst
Table 1 Definition of the symbols
e

A ranked entity.

C(e)

Perceived relevance.

Œ≥ (e)

Abandonment probability.

U (e)

Utility.

Pc (e)

The click probability of the entity at position i in the ranking.

d

A ranked document.

R(d)

Relevance of the document.

a

A ranked ad.

SE

An abbreviation indicating Search Engine.

$(a)

Cost-Per-Click (CPC) of the ad.

v(a)

Private value of the ad for the advertiser.

b(a)

Bid for the ad.

w(a)

Ratio of the click probability to the sum of abandonment and click probability.

Œº(a)

Sum of abandonment and click probability (i.e. C(a) + Œ≥ (a)).

CE(a)

Proposed Click-Efficiency ranking score of the ad.

pi

Payment by the advertiser (CPC) to the search in a given mechanism.

Ur (e)

Residual utility in the context of other entities in the ranked list.

Œ±

Simulation constant to balance between the click and the abandonment probabilities.

The click model can be schematically represented as the flow graph shown in Fig. 1.
Labels on the edges refer to the probability of the user traversing them. Each vertex in the
figure corresponds to a view epoch (see below), and the flow balance holds at each vertex.
Starting from the top entity, the probability of the user clicking the first ad is C(e1 ) and
probability of him abandoning browsing is Œ≥ (e1 ). The user goes beyond the first entity with
probability 1 ‚àí (C(e1 ) + Œ≥ (e1 )) and so on for the subsequent results.
In this model, we assume that the parameters‚ÄîC(ei ), Œ≥ (ei ) and U (ei )‚Äîare functions
of the entity at the current position i.e. these parameters are independent of other entities
the user has already seen. We recognize that this assumption is not fully accurate, since
the user‚Äôs decision to click the current item or to leave the search may depend not just
on the current item but rather on all the entities he has seen before in the list. We stick
to the assumption for the optimal ranking analysis below, since considering mutual influence of ads may lead to combinatorial optimization problems with intractable solutions. We

Fig. 1 Flow graph for an user browsing the first two entities. The labels are the view probabilities and ei
denotes the entity at the i th position

J Intell Inf Syst

will show that even the simplest dependence between the parameters will indeed lead to
intractable optimal ranking in Section 7.
Although the proposed model is intuitive enough, we would like to mention that our
model is also confirmed by the recent empirical click models. For example, the General
Click Model (GCM) by Zhu et al. (2010) is based on the same basic user behavior. The GCM
is empirically validated for both search results and ads (Zhu et al. 2010). Furthermore, other
click models are shown to be special cases of GCM. Please refer to Zhu et al. (2010) for
a detailed discussion. These previous works avoid the need for separate model validation,
as well as confirm the feasibility of the parameter estimation. Further, Yilmaz et al. (2010)
proposes an expected browsing utility metric based on a similar user model.

4 Optimal ranking
Based on the click model, we formally define the ranking problem and derive optimal
ranking in this section. The problem may be stated as,
Choose the optimal ranking Eopt = e1 , e2 , .., eN  of N entities to maximize the
expected utility
N

E(U ) =
U (ei )Pc (ei )
(1)
i=1

where N is the total number of entities to be ranked.
The utility function U (ei ) denotes the stand-alone utility of the entity ei to the search
engine (or one who performs the ranking). This may vary depending on the specific ranking problem. For example, for ranking search results, the utility will be the relevance of
document ei ; whereas for ranking ads to maximize the revenue of the search engine, the
U (ei ) will be pay-per-click of ad ei . We define the specific utility function for entities as
we discuss the specific ranking problems below.
For the browsing model in Fig. 1, the click probability for the entity at the i th position is,
Pc (ei ) = C(ei )

i‚àí1




1 ‚àí C(ej ) + Œ≥ (ej )

(2)

j =1

Substituting click probability Pc from (2) in (1) we get,
E(U ) =

N

i=1

U (ei )C(ei )

i‚àí1



1 ‚àí (C(ej ) + Œ≥ (ej ))



(3)

j =1

The optimal ranking maximizing this expected utility can be shown to be a sorting
problem with a simple ranking function:
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
U (ei )C(ei )
(4)
CE(ei ) =
C(ei ) + Œ≥ (ei )
Proof Sketch The proof shows that any inversion in this order will reduce the expected
profit. CE function is deduced from expected profits of two placements‚Äîthe CE ranked

J Intell Inf Syst

placement and placement in which the order of two adjacent ads are inverted. We show
that the expected profit from the inverted placement can be no greater than the CE ranked
placement. Please refer to Appendix A-1 for the complete proof.
As mentioned in the introduction, the ranking function CE is the utility generated per
unit view probability consumed by the entity. With respect to browsing model in Fig. 1, the
top entities in the ranked list have greater view probabilities, and placing ads with greater
utility per consumed view probability at higher positions intuitively increases total utility.
The proof of Theorem 1 assumes that the user clicks only one entity in the list. Since this
may not always be true, we extend the optimality to multiple clicks in Theorem 2.
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Sketch We proved that ordering according to CE provides maximum expected utility
for single click above. Multiple clicks are the same as the user restarting her browsing from
the entity immediately below the last clicked entity. A simple induction on number of clicks
based on this idea, using a single click as base case is sufficient to prove that the proposed
placement provides maximum expected utility for multiple clicks. See Appendix A-2 for
the complete proof.
Note that the ordering above does not maximize the utility for selecting a subset of items.
The seemingly intuitive method of ranking the set of items by CE and selecting top-k may
not be optimal (Aggarwal et al. 2008). For optimal selection, the proposed ranking can be
extended by a dynamic programming based selection (Aggarwal et al. 2008). In this paper,
we discuss only the ranking problem.

5 Ranking taxonomy
The click model in Fig. 1 is common to many types of rankings including document searches
and search ads. The only difference between these rankings sharing a common click model
is the utility to be maximized. Consequently, the CE ranking can be made applicable to
different ranking problems by plugging in different utilities. For example, if we plug in relevance as utility (U (e) in (4)), the ranking function is applicable for the documents, whereas
if we plug in cost per click of ads, the ranking function is applicable to ads. Furthermore,
we may assume specific constraints on one or more of the three parameters of CE ranking (e.g. ‚àÄi Œ≥ (ei ) = 0). Through these assumptions, CE ranking will suggest a number of
reduced ranking functions with specific applications. These substitutions and reductions can
be enumerated as a taxonomy of ranking functions.
We show the taxonomy in Fig. 2. The three top branches of the taxonomy (U (e) = R(d),
U (e) = $(a), and U (e) = v(a) branches) are for document ranking, ad ranking maximizing
search engine profit, and ad ranking maximizing advertisers revenue respectively. These
branches correspond to the substitution of utilities by document relevance, CPC, and private
value of the advertisers. The sub-trees below these branches are the further reduced cases of
these three main categories. The solid lines in Fig. 2 denote already known functions, while
the dotted lines are the new ranking functions suggested by CE ranking. Sections 5.1, 5.2,
and 5.3 below discuss the further reductions of document ranking, search engine optimal
ad ranking, and social optimal ad ranking respectively.

J Intell Inf Syst

Fig. 2 Taxonomy reduced CE ranking functions. The assumptions and corresponding reduced ranking functions are illustrated. The dotted lines denote predicted ranking functions incorporating new click model
parameters

5.1 Optimal document ranking
For document ranking the utility of ranking is the probability of relevance of the document.
Hence by substituting the document relevance‚Äîdenoted by R(d)‚Äîin (4) we get
CE(d) =

C(d)R(d)
C(d) + Œ≥ (d)

(5)

This function suggests the general optimal relevance ranking for the documents. We discuss
some intuitively valid assumptions on user model for the document ranking and the corresponding ranking functions below. The three assumptions discussed below correspond to
the three branches under Optimal Document Ranking subtree in Fig. 2.

Sort by relevance (PRP) We elucidate two sets of assumptions under which the CE(d)
in (5) will reduce to PRP.
First assume that the user has infinite patience, and never abandons results (i.e. Œ≥ (d) ‚âà
0). Substituting this assumption in (5),
R(d)C(d)
= R(d)
(6)
C(d)
which is exactly the ranking suggested by PRP.
In other words, the PRP is still optimal for scenarios in which the user has infinite
patience and never abandons checking the results (i.e. the user leaves browsing the results
only by clicking a result).
The second set of slightly weaker assumptions under which the CE(d) will reduce to
PRP is
CE(d) ‚âà

1.

C(d) ‚âà R(d).

J Intell Inf Syst

2.

Abandonment probability Œ≥ (d) is negatively proportional to the document relevance
i.e. Œ≥ (d) ‚âà k ‚àí R(d), where k is a constant between one and zero. This assumption
corresponds to the intuition that the higher the perceived relevance of the current result,
the less likely is the user abandoning the search.

Now CE(d) reduces to,
R(d)2
(7)
k
Since this function is strictly increasing with zero and positive values of R(d), ordering just
by R(d) results in the same ranking as suggested by the function. This implies that PRP is
optimal under these assumptions also.
It may be noted that abandonment probability decreasing with perceived relevance is a
more intuitively valid assumption than the infinite patience assumption above.
CE(d) ‚âà

Ranking considering perceived relevance Recent click log studies effectively assess
perceived relevance of document search snippets (Yue et al. 2010; Clarke et al. 2007). But,
how to use the perceived relevance for improved document ranking is still an open question.
The proposed perceived relevance ranking addresses this question.
If we assume that Œ≥ (d) ‚âà 0 in (5), the optimal perceived relevance ranking is the same
as that suggested by PRP as we have seen in (6).
On the other hand, if we assume that the abandonment probability is negatively proportional to the perceived relevance (Œ≥ (d) = k ‚àí C(d)) as above, the optimal ranking
considering perceived relevance is
C(d)R(d)
‚àù C(d)R(d)
(8)
k
i.e. sorting in the order of the product of document relevance and perceived relevance is
optimal under these assumptions. The assumption of abandonment probabilities being negatively proportional to relevance is more realistic than the infinite patience assumption as
we discussed above. This discussion shows that by estimating the nature of abandonment
probability, one would be able to decide on the optimal perceived relevance ranking.
CE(d) ‚âà

Ranking considering abandonment We now examine the ranking considering abandonment probability Œ≥ (d), with the assumption that the perceived relevance is approximately
equal to the actual relevance. In this case CE(d) becomes,
CE(d) ‚âà

R(d)2
R(d) + Œ≥ (d)

(9)

Clearly this is not a strictly increasing function with R(d). Hence the ranking considering
abandonment is different from PRP ranking, even if we assume that the perceived relevance
is equal to the actual relevance. assumption that ‚àÄd Œ≥ (d) = 0, the abandonment ranking
becomes the same as PRP.

5.2 Optimal Ad ranking for search engines
For the paid placement of ads, the utilities of ads to the search engine are Cost-Per-Click
(CPC) of the ads. Hence, by substituting the CPC of the ad‚Äîdenoted by $(a)‚Äî in (4) we
get
C(a)$(a)
(10)
CE(a) =
C(a) + Œ≥ (a)

J Intell Inf Syst

Thus this function suggests the general optimal ranking for the ads. Please recall
that the perceived relevance C(a) is the same as the CTR used for ad placement
(Richardson et al. 2007).
In the following subsections we demonstrate how the general ranking presented reduces
to the currently used ad placement strategies under various assumptions. We will show that
they all correspond to specific assumptions about the abandonment probability Œ≥ (a). These
two functions below corresponds to the two branches under the SE (Search Engine) Optimal
Ad Placement subtree in Fig. 2.

Ranking by bid amount The sort by bid amount ranking was used by Overture Services
(and was later used by Yahoo! for a while after acquisition of Overture). Assuming that the
user never abandons browsing (i.e. ‚àÄa Œ≥ (a) = 0), then (10) reduces to
CE(a) = $(a)

(11)

This means that the ads are ranked purely in terms of their payment. In fact overture ranking
is by bid amount, which is different from payment in a second price auction. But both will
result in the same ranking as higher bids implies higher payments also.
When Œ≥ (a) = 0, we essentially have a user with infinite patience who will keep browsing
downwards until he finds a relevant ad. Hence ranking by bid amount maximizes profit.
More generally, for small abandonment probabilities, ranking by bid amount is near optimal.
Note that this ranking is isomorphic to PRP ranking discussed above for document ranking,
since both ranks are based only on utilities.

Ranking by expected profit Google and Microsoft supposedly place the ads in the order
of expected profit based on product of CTR (C(a) in CE) and bid amount ($(a)) (Richardson et al. 2006). The mechanism is called Generalized Second Price (GSP) auction, and
the most popular one as well. If we approximate abandonment probability as negatively
proportional to the CTR of the ad (i.e. ‚àÄa Œ≥ (a) = k ‚àí C(a)) , the (10) reduces to,
$(a)R(a)
‚àù $(a)R(a)
(12)
k
This shows that ranking ads by their standalone expected profit is near optimal as long as
the abandonment probability is negatively proportional to the relevance. To be accurate,
the Google mechanism‚ÄîGSP‚Äîuses the bid amount of the advertisers (instead of CPC in
(12)) for ranking. Although CPC and bids are different for GSP, we will show that both will
result in the same ranking in Section 6. Note that this ranking is isomorphic to the perceived
relevance ranking of documents discussed above.
CE(a) ‚âà

5.3 Social optimal Ad ranking
An important property of any auction mechanism is social utility, i.e. total utilities of all
the players. In our case this is equal to the sum of the utilities of all the advertisers and
the search engine. To analyze advertiser‚Äôs profit, a private value model is commonly used.
Each advertiser has a private value for the click, which is equal to the expected benefit
(direct and indirect revenue) from the click. Advertisers pay a fraction of this benefit to the
search engine as CPC. The utility for the advertisers is the difference between the private
value and payment to the search engine. The utility for the search engine is the payment
from the advertisers. Hence the social utility is equal to the sum of private values of all the
clicks for the advertisers (which is the sum of utilities of the search engine and advertisers).

J Intell Inf Syst

Consequently, to prove the social optimality all we need to prove is that the total private
values of clicks for the advertisers is optimal.
The social-optimal branch in Fig. 2 corresponds to the ranking to maximize total revenue.
Private value of advertisers ai is denoted as‚Äîv(ai ). By substituting the utility by private
values in (4) we get,
CE(d) =

C(a)v(a)
C(a) + Œ≥ (a)

(13)

If the ads are ranked in this order, the ranking will guarantee maximum revenue. Note that
the optimal revenue does not imply optimal net profits for the advertisers, since part of this
revenue is paid to the search engine as CPC. But optimal revenue implies a maximum total
profit (utility)‚Äîsum of profits of search engine and advertisers.
In Figure 2 the two left branches of the Social Optimal subtree (labeled Œ≥ (a) = 0 and
Œ≥ (a) = k ‚àí C(a)) correspond respectively to the assumption of no abandonment, and
abandonment probabilities being negatively proportional to the click probability. These two
cases are isomorphic to the Overture and Google ranking discussed in Section 5.2 above.
The social optimal ranking is not directly implementable as search engines do not know
the private value of the advertisers. But this ranking is useful in analysis of auctions mechanisms. Furthermore, the search engine may try to effectuate this order through auction
mechanism equilibriums as we demonstrate in Section 6.

6 Applying CE ranking for Ad placement
We have shown that CE ranking maximizes the profits for search engines for given CPCs.
The CPCs are determined by the pricing mechanism used by the search engine. Hence
the overall profit of ranking can be analyzed only in association with a pricing mechanism. The existing ad pricing mechanisms like GSP do not preserve any of their appealing
properties for CE ranking as they do not consider the additional parameter abandonment
probability. For example, the GSP pricing Edelman et al. (2007) is no longer the minimum
amount need to be paid by the advertiser to maintain his position in the CE ranking. To
this end, we design a full auction mechanism by proposing a new second price based pricing to be used with the CE ranking. Subsequently, we analyze the properties of the auction
mechanism.
Let us start by describing the dynamics of ad auctions briefly, the search engine decides
the ranking and pricing (CPC) of the ads based on the bid amounts of the advertisers. Generally the pricing is not equal to the bid amount of advertisers, but derived based on the
bids (Easley and Kleinberg 2010; Edelman et al. 2007; Aggarwal et al. 2006). In response to
these ranking and pricing strategies, the advertisers (more commonly, the software agents of
the advertisers) may change their bids to maximize their profits. They may change bids hundreds of times a day. Eventually, the bids may stabilize at a fixed point where no advertiser
can increase his profit by unilaterally changing his bid, depending on the initial bids and
behavior of the advertisers. This set of bids corresponds to a Nash Equilibrium of the auction
mechanism. Hence the expected profits of a search engine will be the profits corresponding
to the Nash Equilibrium, if the auction attains a Nash Equilibrium.
The next section discusses properties of any mechanism based on the user model‚Äî
independent of the ranking and pricing strategies. In Section 6.2, we introduce a pricing
mechanism and analyze the properties including the equilibrium.

J Intell Inf Syst

6.1 User model based properties
We discuss general properties of all auction mechanisms using the browsing model (Fig. 1).
These properties are implications of the user behavior and applicable to any pricing and
ranking.
Lemma 1 (Individual Rationality) In any equilibrium the payment by the advertisers is less
than or equal to their private values.1
If this is not true, this advertiser may opt out from the auction by bidding zero and
increase the profit, violating the assumption of equilibrium.
Lemma 2 (Pricing Monotonicity) In any equilibrium, the price paid by an advertiser
increases monotonically as he moves up in the ranking unilaterally.
From the browsing model, click probability of the advertisers is non-decreasing as he
moves up in the position. Unless the price increases monotonically, the advertiser may
increase his profit by moving up, thereby violating assumption of an equilibrium.
Lemma 3 (Revenue Maximum) The sum of the payoffs of the advertisers and the search
engine is less than or equal to
E(V ) =

N


v(ai )C(ai )

1 ‚àí (C(aj ) + Œ≥ (aj ))



(14)

j =1

i=1

when the advertisers are ordered by

i‚àí1



C(a)v(a)
C(a)+Œ≥ (a) .

Note that this quantity is the maximal advertiser revenue corresponding to the social
optimal placement in (13), and is a direct consequence. The advertiser pay a fraction of his
revenue to the search engine. Payoff for the advertisers is the difference between the total
revenue and the payment to the search engine. The total payoff of the search engine is the
sum of these payments by all the advertisers. Since the suggested order above in Lemma 3
maximizes total revenue of the advertisers, the sum of the payoffs for the search engine and
the advertisers will not exceed this value.
A corollary of the social optimality combined with the individual rationality result
expressed in Lemma 1 is that,
Lemma 4 (Profit Maximum) The quantity E(V ) in Lemma 3 is an upper bound for the
search engine profit in any equilibrium.

6.2 Pricing and equilibrium
An interesting property of the proposed mechanism is the existence of an equilibrium in
which the search engine optimal ranking coincides with the social optimality. As we proved
above, CE ranking is search engine optimal as it maximizes the revenue for the given CPCs.
On the other hand, social optimal ordering maximizes the total profits for all the players

1 This

property is called individual rationality

J Intell Inf Syst

(search engine and advertisers) for given CPCs. Social optimality is desirable for search
engines, as the increased profits will improve the advertiser‚Äôs preference of one search
engine over others. Since search engines do not know the private value of the advertisers, social optimal ranking is not directly achievable (note that the search engines do the
ranking). A possibility is to design a mechanism having an equilibrium coinciding with the
social optimality, as we propose below. This may cause the bid vector to stabilize in a social
optima.
For defining the pricing strategy for the auction mechanism, we define the pricing order
as the decreasing order of w(a)b(a), where b(a) is the bid value and w(a) is,
w(a) =

C(a)
C(a) + Œ≥ (a)

(15)

In this pricing order, we denote the i th advertiser‚Äôs w(ai ) as wi , C(ai ) as ci , b(ai ) as bi , and
the abandonment probability Œ≥ (ai ) as Œ≥i for convenience. Let Œºi = ci + Œ≥i . For each click,
advertiser ai is charged price pi (CPC) equal to the minimum bid required to maintain its
position in the pricing order,
pi =

wi+1 bi+1
bi+1 ci+1 Œºi
=
wi
Œºi+1 ci

Substituting pi in (10) for the ranking order, CE of the i th advertiser is,
pi ci
CEi =
Œºi

(16)

(17)

This proposed mechanism preserves the pricing order in the ranking as well, i.e.
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj

(18)

The proof for theorem 3 is given in Appendix A-3. This order preservation property
implies that the final ranking is the same as that based on bid amounts. In other words, ads
can be ranked based on the bid amounts instead of CPCs. After the ranking, the CPCs can
be decided based on this ranking order. A corollary of this order preservation is that the
CPC is equal to the minimum amount the advertisers have to pay to maintain their position
in the ranking order.
Furthermore we show below that any advertiser‚Äôs CPC is less than or equal to his bid.
Lemma 5 (Individual Rationality) The payment pi of any advertiser is less or equal to his
bid amount.
Proof
pi =

bi+1 ci+1 Œºi
bi+1 ci+1 Œºi
CEi+1
=
bi =
bi ‚â§ bi (since CEi ‚â• CEi+1 )
Œºi+1 ci
Œºi+1 ci bi
CEi

This means advertisers will never have to pay more than their bid, similar to GSP. This
property makes it easy for the advertiser to decide his bid, as he may bid up to his click
valuation. He will never have to pay more than his revenue, irrespective of bids of other
advertisers.

J Intell Inf Syst

Interestingly, this mechanism is a general case of existing mechanisms, similar to CE
ranking above. The mechanism reduces to GSP (Google mechanism) and Overture mechanisms on the same assumptions on which CE ranking reduces to respective rankings
(described in Section 5.2).
Lemma 6 The mechanism reduces to Overture ranking with a second price auction on the
assumption ‚àÄi Œ≥i = 0
Proof This assumption implies
wi = 1
‚áí pi = bi+1 (second price auction)
‚áí CEi = bi+1 ‚â° bi (i.e. ranking by bi+1 is equivalent to ranking by bi )

Lemma 7 The mechanism reduces to GSP on the assumption ‚àÄi Œ≥i = k ‚àí ci
Proof This assumption implies
wi = c i
bi+1 ci+1
(i.e. ranking reduces to GSP ranking)
ci
bi c i
bi+1 ci+1
‚â°
(by Theorem 3)
‚áí CEi =
k
k
‚àù bi ci
‚áí pi =

This lemma in conjunction with Theorem 3 implies that GSP ranking by ci bi (i.e. by
bids) is the same as the ranking by ci pi (by CPCs).
Now we will look at the equilibrium properties of the mechanism. We start by noting that
truth telling is not a dominant strategy. This trivially follows, since GSP is a special case
of the proposed mechanism, and it is generally known that truth telling is not a dominant
strategy for GSP. Hence we focus on Nash Equilibrium conditions in our analysis.
Theorem 4 (Nash Equilibrium) Without loss of generality, assume that advertisers are
ordered in decreasing order of cŒºi vi i where vi is the private value of the i th advertiser. The
advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
(19)
bi =
ci
Œºi+1
This equilibrium is socially optimal as well as optimal for search engines for the given
CPC‚Äôs.
Proof Sketch The inductive proof shows that for these bid values, no advertisers can
increase his profit by moving up or down in the ranking. The full proof is given in
Appendix A-4. Since the ranking is the same as the social optima order in (13), social
optimality is a direct implication.

J Intell Inf Syst

We do not rule out the existence of multiple equilibriums. The stated equilibrium is
particularly interesting, due to the social optimality and search engine optimality. Furthermore, although the equilibrium depends on the private values of the advertisers unknown
to the search engine, please keep in mind that search engines do not implement equilibriums directly. Instead, search engines decide the pricing and ranking, and the advertisers
may reach an equilibrium by repeatedly revising auction prices. The pricing and ranking are
practical, since they depend solely on the quantities known to the search engine.
The following Lemmas show that equilibriums of other placement mechanisms are special cases of the proposed CE equilibrium. The stated equilibrium reduces to equilibriums
in the Overture mechanism and GSP under the same assumptions (discussed above) under
which the CE ranking reduces to Overture and GSP rankings.
Lemma 8 The bid values
bi = vi ci + (1 ‚àí ci )bi+1

(20)

are in a pure strategy Nash Equilibrium in the Overture mechanism. This corresponds to
the substitution of the assumption ‚àÄi Œ≥i = 0 (i.e. Œºi = ci ) in Theorem 4.
The proof follows from Theorem 4 as both pricing and ranking are shown to be a special
case of our proposed mechanism.
Similarly for GSP,
Lemma 9 The bid values
bi = vi k + (1 ‚àí k)bi+1 ci+1

(21)

is a pure strategy Nash Equilibrium in the GSP mechanism.
This equilibrium corresponds to the substitution of the assumption ‚àÄi Œ≥i = k ‚àí ci (1 ‚â•
k ‚â• 0) in Theorem 4. Since this is a special case, this result follows from Theorem 4.

6.3 Comparison with VCG mechanism
We compare the revenue and equilibrium of CE mechanism with those of VCG (Vickrey
1961; Clarke 1971; Groves 1973). VCG auctions combine an optimal allocation (ranking)
with VCG pricing. VCG payment of a bidder is equal to the reduction of revenues of other
bidders due to the presence of the bidder. A well known property is that VCG pricing with
any socially optimal allocation has truth telling as the the dominant strategy equilibrium.
In the context of online ads, a ranking optimal with respect to the bid amounts is socially
optimal ranking for VCG. This optimal ranking is bŒºi ci i ; as directly implied by the (1) on
substituting bi for utilities. Hence this ranking combined with VCG pricing has truth telling
as the dominant strategy equilibrium. Since bi = vi at the dominant strategy equilibrium,
ranking is socially optimal for advertiser‚Äôs true value as suggested in (13).
The CE ranking function is different from VCG since CE ranking by payments optimizes
search engine profits. On the other hand, VCG ranking optimizes the advertiser‚Äôs profit.
But Theorem 3 shows that for the pricing used in CE, ordering of CE is the same as that
of VCG. This order preserving property facilitates the comparison of CE with VCG. The
theorem below shows revenue dominance of CE over VCG for the same bid values of the
advertisers.

J Intell Inf Syst

Theorem 5 (Search Engine Revenue Dominance) For the same bid values for all the advertisers, the search engine revenue by CE mechanism is greater than or equal to its revenue
by VCG.
Proof Sketch The proof is an induction based on the fact that the ranking by CE and VCG
are the same, as mentioned above. Full proof is given in Appendix A-5.
This theorem shows that the CE mechanism is likely to provide higher revenue to the
search engine even during transient times before the bids settle on equilibriums.
Based on Theorem 5, we prove revenue equivalence of the proposed CE equilibrium
with dominant strategy equilibrium of VCG.
Theorem 6 (Equilibrium Revenue Equivalence) At the equilibrium in Theorem 4, the
revenue of the search engine is equal to the revenue of the truthful dominant strategy
equilibrium of VCG.
Proof Sketch The proof is an inductive extension of Theorem 5. Please see Appendix A-6
for complete proof.
Note that the CE equilibrium has lower bid values than VCG at the equilibrium, but
provides the same profit to the search engine.

7 CE ranking considering mutual influences: diversity ranking
An assumption in CE ranking is that the entities are mutually independent as we pointed out
in Section 3. In other words, the three parameters‚ÄîU (e), C(e) and Œ≥ (e)‚Äîof an entity do
not depend on other entities in the ranked list. In this section we relax this assumption and
analyze the implications. Since the nature of the mutual influence may vary for different
problems, we base our analysis on a specific well known problem‚Äîranking considering
diversity (Carterette 2010; Agrawal et al. 2009; Rafiei et al. 2010).
Diversity ranking accounts for the fact that the utility of an entity is reduced by the
presence of a similar entity above in the ranked list. This is a typical example of the mutual
influence between the entities. All the existing objective functions for the diversity ranking
are known to be NP-Hard (Carterette 2010). We analyze a basic form of diversity ranking
to explain why this is a fundamentally hard problem.
We modify the objective function in (1) slightly to distinguish between the standalone
utilities and the residual utilities‚Äîutility of an entity in the context of other entities in the
list‚Äîas,
E(U ) =

N


Ur (ei )Pc (ei )

(22)

i=1

where Ur (ei ) denotes the residual utility.
We examine a simple case of diversity ranking problem by considering a set of entities‚Äî
all having the same utilities, perceived relevances and abandonment probabilities. Some of
these entities are repeating. If an entity in the ranked list is the same as the entity in the
list above, the residual utility of that entity becomes zero. In this case, it is intuitive that
the optimal ranking is to place the maximum number of pair-wise dissimilar entities in the

J Intell Inf Syst

top slots. The theorem below shows that even in this simple case the optimal ranking is
NP-Hard.
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Sketch The proof is by reduction from the independent set problem. See
Appendix A-7 for the complete proof.
Moreover, the proof by reduction from independent set problem has more implications
than NP-Hardness as shown in the following corollary,
Corollary 1 The constant approximation algorithm for ranking considering diversity is
hard.
Proof The proof of NP-Hardness in the theorem above shows that the independent set problem is a special case of diversity ranking. This implies that a constant ratio approximation
algorithm for the optimal diversity ranking would be a constant ratio approximation algorithm for the independent set problem. Since a constant ratio approximation algorithm for
the independent set is known to be hard (cf. Garey and Johnson 1976 and HaÃästad 1996), the
corollary follows. To define hard, in his landmark paper HaÃästad proved that independent set
problem cannot be solved within n1‚àí for  > 0 unless all problems in N P are solvable in
probabilistic polynomial time, which is widely believed to be not possible.2
This section shows that the optimal ranking considering mutual influences of parameters
is hard. We leave formulating approximation algorithms (not necessarily constant ratio) for
future research.
Beyond proving the intractability of mutual influence ranking, we believe that the
intractability of the simple scenario here explains why all optimal diversity rankings and
constant ratio approximations are likely to be intractable. Furthermore, the proof based
on the reduction from the well explored independent set problem may help in adapting
approximation algorithms from graph theory.

8 Experiments
We compare the profit improvement by CE and reduced forms to existing rankings.
Although the optimality of the proposed ranking is proven above, experiments help to quantify the increase in utilities. Considering the very restricted access to real users and ad click
logs, we limit our evaluations to simulations as it is common in computational advertisement
research. We believe that these experimental results will motivate future online evaluations
in industry settings.
In our first experiment in Fig. 3a, we compare the CE ranking with rank by bid
amount (11) strategy by Overture and rank by bid √ó perceived relevance (12) by Google.
We assign the perceived relevance values as a uniform random number between 0 and Œ±
(0 ‚â§ Œ± ‚â§ 1) and abandonment probabilities as random between 0 and 1 ‚àí Œ±. This assures
‚àÄi (C(ai ) + Œ≥ (ai )) ‚â§ 1 condition required in the click model. The bid amounts for ads are

2 This

belief is almost as strong as the belief P  = N P

J Intell Inf Syst

Fig. 3 a Comparison of Overture, Google and CE rankings. Perceived relevances are uniformly random
in [0, Œ±] and abandonment probabilities are uniformly random in [0, 1 ‚àí Œ±]. CE provides optimal expected
profits for all values of Œ±. b Comparison of CE, PRP and abandonment ranking (9). Abandonment ranking
dominates PRP

J Intell Inf Syst

Fig. 4 Optimality of reduced forms under assumptions (a) setting Œ≥ (d) = k ‚àí R(d). Perceived relevance
ranking is optimal for all values of Œ±. (b) setting C(d) = R(d). In this case, abandonment ranking is optimal

assigned uniform randomly between 0 and 1. We use uniform random for values as it is the
maximum entropy distribution and hence makes least assumptions about the bid amounts.
The number of relevant ads (corresponding to the number of bids on a query) is set to fifty.

J Intell Inf Syst

Simulated users are made to click on ads. The number of ads clicked is set to a random
number generated in a zipf distribution with exponent 1.5. A power law is most intuitive for
the distribution of the number of clicks.
Simulated users browse down the list. Users click an entity with probability equal to the
perceived relevance and abandon the search results with a probability equal to the abandonment probability. The set of entities to be placed is created at random for each run. For the
same set of entities, three runs‚Äîone with each ranking strategy‚Äîare performed. Simulation
is repeated 2 √ó 105 times for each value of Œ±.
An alternate interpretation of Fig. 3a is as the comparison of ranking by CE, PRP and
perceived relevance ranking (8). As we discussed, PRP and perceived relevance rankings
are isomorphic to ad rankings by bid and bid √ó perceived relevance respectively, with utility
being relevance instead of bid amounts. The simulation results are the same.
In Fig. 3b we compare CE, PRP and abandonment ranking (9) under the same settings
used for Fig. 3a. CE provides the maximum utility as expected, and abandonment ranking
occupies the second place. Abandonment ranking provides sub-optimal utility‚Äîsince the
condition ‚àÄd R(d) = C(d) is not satisfied‚Äîbut dominates over PRP. Further, as abandonment probability becomes zero (i.e. Œ± = 1) abandonment rankings becomes same as PRP
and optimal as we predicted in Section 5.1.
Figure 4a compares the perceived relevance ranking (8), CE, and PRP under the condition for optimality for perceived relevance ranking (i.e. ‚àÄd Œ≥ (d) = k ‚àí R(d)). For this,
we set Œ≥ (d) = Œ± ‚àí C(d) keeping all other settings same as the previous experiments.
Figure 4a shows that the perceived relevance ranking provides optimal utility, exactly overlapping with CE curve as expected. Furthermore, note that utilities by PRP are very low
under this condition. The utilities by PRP in fact goes down after Œ± = 0.2. The increase
in abandonment probability, as well as increased sub-optimality of PRP for higher abandonment (since PRP does not consider abandonment) probabilities may be causing this
reduction.
In our next experiment shown in Fig. 4b, we compare abandonment ranking (9) with PRP
and CE under the condition ‚àÄd C(d) = R(d) (i.e. optimality condition for abandonment
ranking). All other settings are the same as those for the experiments in Fig. 3a and b.
Here we observe that the abandonment ranking is optimal and exactly overlaps with CE as
expected. PRP is sub-optimal but closer to optimal than random C(d) used for experiments
in Fig. 3b. The reason may be that C(d) = R(d) is one of the two conditions required for
PRP to be optimal for both sets of assumptions as we discussed in Subsection 5.1. When
abandonment probability becomes zero PRP relevance reaches optimum as we have already
seen.
All these simulation experiments confirm the predictions by the theoretical analysis
above. Although the simulation is no substitute for experiments on real data, we expect that
the observed significant improvements in expected utilities would motivate future online
experiments to quantify profit.

9 Conclusion and future work
We approach the document and ad ranking as a utility maximization based on the user
click model, and derive an optimal ranking‚Äînamely CE ranking. CE ranking is simple and
intuitive; and optimal considering perceived relevance and abandonment probability of user
behavior.

J Intell Inf Syst

On specific assumptions on parameters, the CE ranking function spawns a taxonomy of
rankings in multiple domains. The taxonomy shows that the existing document and ad ranking strategies are special cases of the proposed ranking function under specific assumptions.
The taxonomy is helpful in selecting optimal ranking for a specific user behavior.
To apply CE ranking to ad auctions, we incorporate a second-price based pricing mechanism. The resulting CE mechanism has a Nash Equilibrium which simultaneously optimizes
both the search engine and advertiser revenues. The CE mechanism is revenue dominant
over VCG for the same bid vectors, and has an equilibrium which is revenue equivalent with
the truthful equilibrium of VCG.
We relax the assumption of independence between entities in CE ranking and apply it
to diversity ranking. The ensuing analysis reveals that diversity ranking is an inherently
hard problem; since even the basic formulations are NP-Hard with unlikely constant ratio
approximation algorithms. Furthermore our simulation experiments confirm the results, and
suggest potentially significant increase in profits over the existing rankings.
As future research, assessing profits by CE ranking in an online experiment on a
large scale search engine will quantify improvement in ranking. Estimation and prediction of abandonment probability using click logs and statistical models are interesting
problems. The suggested ranking is optimal for other web ranking scenarios with similar
click models‚Äîlike product and friend recommendations‚Äîand may be extended to these
problems. Furthermore, effective approximation schemes for diversity ranking based on
similarity with the independent set problem may be investigated.
Acknowledgments This research is supported in part by the ARO grant W911NF-13-1-0023, and the ONR
grants N00014-13-1-0176, N00014-13-1-0519 and N00014-15-1-2027, two Google faculty research awards
(2010 & 2013), and a Yahoo key scientific challenges program award (2009).

Appendix
A-1 Proof of theorem 1
Theorem 1 The expected utility in (3) is maximum if the entities are placed in the
descending order of the value of the ranking function CE,
CE(ei ) =

U (ei )C(ei )
C(ei ) + Œ≥ (ei )

Proof Consider results ei and ei+1 in positions i and i + 1 respectively. Let Œºi = Œ≥ (ei ) +
C(ei ) for notational convenience. The total expected utility from ei and ei+1 when ei is
placed above ei+1 is
i‚àí1




(1 ‚àí Œºj ) U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 )

j =1

If the order of ei and ei+1 are inverted by placing ei above ei+1 , the expected utility from
these entities will be,
i‚àí1




(1 ‚àí Œºj ) U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei ))

j =1

J Intell Inf Syst

Since utilities from all other results in the list will remain the same, the expected utility of
placing ei above ei+1 is greater than inverse placement iff
U (ei )C(ei ) + (1 ‚àí Œºi )U (ei+1 )C(ei+1 ) ‚â• U (ei+1 )C(ei+1 ) + (1 ‚àí Œºi+1 )U (ei )C(ei )

U (ei+1 )C(ei+1 )
U (ei )C(ei )
‚â•
Œºi
Œºi+1
U (e)C(e)
This means if entities are ranked in the descending order of C(e)+Œ≥
(e) any inversions will
reduce the profit. Since any arbitrary order can be effected by a number of inversions on the
U (e)C(e)
ranking by CE, this implies that ranking by C(e)+Œ≥
(e) is optimal.

A-2 Proof of theorem 2
Theorem 2 The order proposed in Theorem 1 is optimal for multiple clicks if the user
restarts browsing at the position one below the last clicked entity.
Proof Induction on number of clicks.
Base Case: Single click, proved in Theorem 1.
Inductive Hypothesis: The proposed ordering is optimal for n clicks.
Let there be total of n ranked entities and ec be the nth clicked entity. The user will browse
down starting next to ec . Since there is only one click remaining, optimal ordering of entities is in the descending order of CE by the base case. Since the relevance and abandonment
probabilities ec+1 to en remain unchanged by the independence assumption above, the
optimal sequence will be the sub-sequence of ec+1 to en in the ranking.

A-3 Proof of theorem 3
Theorem 3 The order by wi bi is the same as the order by CEi for the auction i.e.
wi bi ‚â• wj bj ‚áê‚áí CEi ‚â• CEj
Proof Without loss of generality, we assume that ai refers to ad in the position i in the
descending order of wi bi .
pi ci
Œºi
bi+1 ci+1 Œºi ci
=
Œºi+1 ci Œºi
bi+1 ci+1
=
Œºi+1
= wi+1 bi+1

CEi =

‚â• wi+2 bi+2 = CEi+1

J Intell Inf Syst

A-4 Proof of theorem 4
Theorem 4 (Nash Equilibrium) : Without the loss of generality assume that the advertisers
are ordered in the decreasing order of cŒºi vi i where vi is the private value of the i th advertiser.
The advertisers are in a pure strategy Nash Equilibrium if

	
Œºi
bi+1 ci+1
vi ci + (1 ‚àí Œºi )
bi =
ci
Œºi+1
This equilibrium is socially optimal for advertisers as well as optimal for search engines for
the given CPC‚Äôs.
Proof Let there are n advertisers. Without loss of generality, let us assume that advertisers
are indexed in the descending order of vŒºi ci i . We prove equilibrium in two steps.
Step 1: Prove that
wi bi ‚â• wi+1 bi+1

w i bi =

(1)

bi c i
Œºi

Expanding bi by (19),
bi+1 ci+1
Œºi+1
= vi ci + (1 ‚àí Œºi )wi+1 bi+1
vi ci
=
Œºi + (1 ‚àí Œºi )wi+1 bi+1
Œºi

wi bi = vi ci + (1 ‚àí Œºi )

Notice that wi bi is a convex linear combination of wi+1 bi+1 and vŒºi ci i . This means that
the value of wi bi is in between (or equal to) the values of wi+1 bi+1 and vŒºi ci i . Hence to
prove that wi bi ‚â• wi+1 bi+1 all we need to prove is that vŒºi ci i ‚â• wi+1 bi+1 . This inductive
proof is given below.
Induction hypothesis: Assume that
‚àÄi‚â•j

vi ci
‚â• wi+1 bi+1
Œºi

Base case: Prove for i = N i.e. for the bottommost ad.
vN‚àí1 cN‚àí1
‚â• wN b N
ŒºN‚àí1
Assuming ‚àÄi>N bi = 0
wN bN = vN cN ‚â§

vN cN
vN‚àí1 cN‚àí1
vi ci
(as ŒºN ‚â§ 1) ‚â§
(by the assumed order i.e. by
)
ŒºN
ŒºN‚àí1
Œºi

Induction: Expanding wj bj by (19),
wj bj =

vj cj
Œºj + (1 ‚àí Œºj )wj +1 bj +1
Œºj

J Intell Inf Syst

wj bj is the convex linear combination, i.e
vj cj
Œºj

vj cj
Œºj

‚â• wj bj ‚â• wj +1 bj +1 , as we know that

‚â• wj +1 bj +1 by induction hypothesis. Consequently,
wj bj ‚â§

vj ‚àí1 cj ‚àí1
vj cj
‚â§
(by the assumed order)
Œºj
Œºj ‚àí1

This completes the induction.

Since advertisers are ordered by wi bi for pricing, the above proof says that the pricing
order is the same as the assumed order in this proof (i.e. ordering by vŒºi ci i ). Consequently,
pi =

bi+1 ci+1 Œºi
Œºi+1 ci

As corollary of Theorem 3 we know that CEi ‚â• CEi+1 .
In the second step we prove the equilibrium using results in Step 1.
Step 2: No advertiser can increase his profit by changing his bids unilaterally
Proof (of lack of incentive to undercut to advertisers below) In the first step let us prove that
ad ai can not increase his profit by decreasing his bid to move to a position j ‚â• i below.
Inductive hypothesis: Assume true for i ‚â§ j ‚â§ m.
Base Case: Trivially true for j = i.
Induction: Prove that the expected profit of ai at m + 1 is less or equal to the expected
profit of ai at i.
Let œÅk denotes the amount paid by ai when he is at the position k. By inductive hypothesis, the expected profit at m is less or equal to the expected profit at i. So we just need to
prove that the expected profit at m + 1 is less or equal to the expected profit at m. i.e.
m
m+1
(vi ‚àí œÅm+1 ) 
(vi ‚àí œÅm ) 
(1 ‚àí Œºl ) ‚â•
(1 ‚àí Œºl )
(1 ‚àí Œºi )
(1 ‚àí Œºi )
l=1

l=1

Canceling the common terms,
vi ‚àí œÅm ‚â• (vi ‚àí œÅm+1 )(1 ‚àí Œºm+1 )

(2)

œÅm ‚Äîthe price charged to ai at position m‚Äîis based on the Equations 16 and 19. Since the
ai is moving downward, ai will occupy position m by shifting ad am upwards. Hence the ad
just below ai is am+1 . Consequently, the price charged to ai when it is at the mth position is,

	
bm+1 cm+1 Œºi
Œºi
bm+2 cm+2
=
vm+1 cm+1 + (1 ‚àí Œºm+1 )
œÅm =
Œºm+1 ci
ci
Œºm+2
Substituting for œÅm and œÅm+1 in (2),

	
Œºi
bm+2 cm+2
vm+1 cm+1 + (1 ‚àí Œºm+1 )
vi ‚àí
ci
Œºm+2

	


Œºi
bm+3 cm+3
vm+2 cm+2 + (1 ‚àí Œºm+2 )
(1‚àíŒºm+1 )
‚â• vi ‚àí
ci
Œºm+3

J Intell Inf Syst

Simplifying, and multiplying both sides by ‚àí1

	
bm+2 cm+2
Œºi
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
	

bm+3 cm+3
√ó vm+2 cm+2 + (1 ‚àí Œºm+2 )
Œºm+3
Substituting by bm+2 from (19) on RHS.

	
bm+2 cm+2
Œºi
bm+2 cm+2
Œºi
vm+1 cm+1 + (1 ‚àí Œºm+1 )
‚â§ vi Œºm+1 + (1 ‚àí Œºm+1 )
ci
Œºm+2
ci
Œºm+2
Canceling out the common terms on both sides,
Œºi
vm+1 cm+1 ‚â§ vi Œºm+1
ci

vi ci
vm+1 cm+1
‚â§
Œºm+1
Œºi
Which is true by the assumed order as m ‚â• i
Inductive proof for m ‚â§ i is somewhat similar and enumerated below.
Inductive hypothesis: Assume true for j ‚â§ m.
Base Case: Trivially true for j = i.
Proof (of lack of incentive to overbid ad one above) The case in which ai increases his bid
to move one position up i.e. to i ‚àí 1 is a special case and need to be proved separately. In
this case, by moving a single slot up, the index of the ad below ai will change from i + 1
to i ‚àí 1 (a difference of two). For all other movements of ai to a position one above or one
below, the index of the advertisers below will change only by one. Since the amount paid
by ai depends on the ad below ai , this case warrants a slightly different proof,
(vi ‚àí œÅi )

i‚àí1


(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

l=1

i‚àí2


(1 ‚àí Œºl )

l=1


(vi ‚àí œÅi )(1 ‚àí Œºi‚àí1 ) ‚â• vi ‚àí œÅi‚àí1
Expanding œÅi is straight forward.To expand œÅi‚àí1 , note that when ai has moved upwards to
i ‚àí 1, the ad just
 below ai is ai‚àí1 . Since ai‚àí1 has not changed its bids, the œÅi‚àí1 can be
Œºi
expanded as ci vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 ) bŒºi ci i . Substituting for œÅi and œÅi‚àí1 ,


vi ‚àí



	
	
Œºi
Œºi
bi+2 ci+2
bi ci
vi+1 ci+1 + ‚â• vi ‚àí
vi‚àí1 ci‚àí1 + (1‚àíŒºi+1 )
(1 ‚àí Œºi‚àí1 )(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi+2
Œºi

Simplifying and multiplying by ‚àí1
vi Œºi‚àí1 +



	
	
Œºi
Œºi
bi ci
bi+2 ci+2
vi+1 ci+1 + ‚â§
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi+1 )
(1 ‚àí Œºi‚àí1 )
ci
ci
Œºi
Œºi+2

J Intell Inf Syst

Substituting bi+1 from (19)


	
Œºi bi+1 ci+1
Œºi
bi c i
vi‚àí1 ci‚àí1 + (1 ‚àí Œºi‚àí1 )
(1 ‚àí Œºi‚àí1 ) ‚â§
ci Œºi+1
ci
Œºi

Œºi
bi+1 ci+1
Œºi vi‚àí1 ci‚àí1
Œºi
bi c i
‚â§
+ (1 ‚àí Œºi‚àí1 )
vi Œºi‚àí1 + (1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
ci
Œºi
vi Œºi‚àí1 +

We now prove that both the terms in RHS are greater or equal to the corresponding terms in
LHS separately.
Œºi vi‚àí1 ci‚àí1
vi Œºi‚àí1 ‚â§
ci

vi‚àí1 ci‚àí1
vi ci
‚â§
Œºi
Œºi‚àí1
Which is true by our assumed order.
Similarly,
bi+1 ci+1
Œºi
bi c i
Œºi
(1 ‚àí Œºi‚àí1 )
‚â§
(1 ‚àí Œºi‚àí1 )
ci
Œºi+1
ci
Œºi

bi ci
bi+1 ci+1
‚â§
Œºi+1
Œºi
Which is true by (1) above. This completes the proof for this case.
Induction: Prove that the expected profit at m ‚àí 1 is less or equal to the expected profit
at m. The proof is similar to the induction for the case m > i.
Proof Base case is trivially true.
(vi ‚àí œÅm )

m‚àí1


m‚àí2


l=1

l=1

(1 ‚àí Œºl ) ‚â• (vi ‚àí œÅm‚àí1 )

(1 ‚àí Œºl )

Canceling common terms,
(vi ‚àí œÅm )(1 ‚àí Œºm‚àí1 ) ‚â• vi ‚àí œÅm‚àí1
In this case, note that ai is moving upwards. This means that ai will occupy position m by
pushing the ad originally at m one position downwards. Hence the original ad at m is the
one just below ai now. i.e.

	
bm cm Œºi
Œºi
bm+1 cm+1
œÅm =
vm cm + (1 ‚àí Œºm )
=
Œºm ci
ci
Œºm+1
Substituting for œÅm and œÅm‚àí1


vi ‚àí



	
	
Œºi
Œºi
bm+1 cm+1
bm cm
vm cm + ‚â• vi ‚àí
vm‚àí1 cm‚àí1 + (1‚àíŒºm )
(1‚àíŒºm‚àí1 )(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm+1
Œºm

Simplifying and multiplying by ‚àí1
vi Œºm‚àí1 +



	
	
Œºi
Œºi
bm cm
bm+1 cm+1
vm cm + ‚â§
vm‚àí1 cm‚àí1 + (1‚àíŒºm‚àí1 )
(1 ‚àí Œºm )
(1 ‚àí Œºm‚àí1 )
ci
ci
Œºm
Œºm+1

J Intell Inf Syst

Substituting by bm from (19)

	
Œºi bm cm
Œºi
bm c m
vm‚àí1 cm‚àí1 + (1 ‚àí Œºm‚àí1 )
(1 ‚àí Œºm‚àí1 ) ‚â§
vi Œºm‚àí1 +
ci Œºm
ci
Œºm
Canceling common terms,
Œºi
vm‚àí1 cm‚àí1
ci

vi Œºm‚àí1 ‚â§


vm‚àí1 cm‚àí1
vi ci
‚â§
Œºi
Œºm‚àí1
Which is true by the assumed order as m < i.

A-5 Proof of theorem 5
Theorem 5 (Search Engine Revenue Dominance) : For the same bid values for all the
advertisers, the revenue of search engine by CE mechanism is greater or equal to the
revenue by VCG.
Proof VCG payment of the ad at position i (i.e. ai ) is equal to the reduction in utility of
the ads below due to the presence of ai . For each user viewing the list of ads (i.e. for unit
view probability), the total expected loss of ads below ai due to ai is,

piVu =
=

=

j
‚àí1
j
‚àí1
n
n


1
bj c j
(1 ‚àí Œºk ) ‚àí
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi

Œºi
1 ‚àí Œºi

j =i+1

k=1

n


j
‚àí1

bj c j

j =i+1

j =i+1

k=1

(1 ‚àí Œºk )

k=1

j
‚àí1
n
i

Œºi 
(1 ‚àí Œºk )
bj c j
(1 ‚àí Œºk )
1 ‚àí Œºi
j =i+1

k=1

= Œºi

i‚àí1


(1 ‚àí Œºk )

n


bj c j

j =i+1

k=1

k=i+1

j
‚àí1

(1 ‚àí Œºk )

k=i+1

This is the expected lose per user browsing the ad list. Pay per click should be equal to the
lose per click. To calculate the pay per click, we divide by the click probability of ai . i.e.

piV =
=

Œºi

i‚àí1

j ‚àí1
j =i+1 bj cj
k=i+1 (1 ‚àí Œºk )
i‚àí1
ci k=1 (1 ‚àí Œºk )
j
‚àí1

k=1 (1 ‚àí Œºk )

n
Œºi 
bj c j
ci
j =i+1

n

(1 ‚àí Œºk )

k=i+1

J Intell Inf Syst

Converting to recursive form,
bi+1 Œºi
Œºi ci+1 V
ci+1 + (1 ‚àí Œºi+1 )
p
ci
ci Œºi+1 i+1
bi+1 Œºi ci+1
Œºi ci+1 V
=
Œºi+1 + (1 ‚àí Œºi+1 )
p
ci Œºi+1
ci Œºi+1 i+1

piV =

For the CE mechanism payment from (16) is,
piCE =

bi+1 ci+1 Œºi
Œºi+1 ci

Note that piV is convex combination of PiCE and
two values. To prove that

piCE

‚â•

piV

Œºi ci+1 V
ci Œºi+1 pi+1 ,

and hence is between these

all we need to prove is that PiCE ‚â•

Œºi ci+1 V
ci Œºi+1 pi+1

‚áî

bi ‚â• piV . This directly follows from individual rationality property of VCG. Alternatively, a
V = 0 (bottommost ad) will prove the same. Note that
simple recursion with base case as pN
we consider only the ranking (not selection), and hence the VCG pricing of the bottommost
ad in the ranking is zero.

A-6 Proof of theorem 6
Theorem 6 (Equilibrium Revenue Equivalence) : At the equilibrium in Theorem 4, the revenue of search engine is equal to the revenue of the truthful dominant strategy equilibrium
of VCG.
Proof Rearranging (3) and substituting true values for bid amounts,

	
Œºi
(1 ‚àí Œºi+1 )ci+1 V
V
vi+1 ci+1 +
pi+1
pi =
ci
Œºi+1
For the CE mechanism, substituting equilibrium bids from (19) in payment (16),

	
bi+1 ci+1 Œºi
Œºi
bi+2 ci+2
=
vi+1 ci+1 + (1 ‚àí Œºi+1 )
piCE =
Œºi+1 ci
ci
Œºi+2
Rewriting bi+2 in terms of pi+1 ,
piCE =


	
Œºi
(1 ‚àí Œºi+1 )ci+1 CE
vi+1 ci+1 +
pi+1
ci
Œºi+1

= piV

V
CE
(iff pi+1
= pi+1
)

Ad at the bottommost position pays same amount zero, a simple recursion will prove that
the payment for all positions for both VCG and the proposed equilibrium is the same.

A-7 Proof of theorem 7
Theorem 7 Diversity ranking optimizing expected utility in (22) is NP-Hard.
Proof Independent set problem can be formulated as a ranking problem considering similarities. Consider an unweighed graph G of n vertices {e1 , e2 , ..en } represented as an

J Intell Inf Syst

adjacency matrix. This conversion is clearly polynomial time. Now, consider the values in
the adjacency matrix as the similarity values between the entities to be ranked. Let the entities have the same utilities, perceive relevances and abandonment probabilities. In this set of
n entities from {e1 , e2 , .., en }, clearly the optimal ranking will have k pairwise independent
entities as the top k entities for a maximum possible value of k. But the set of k independent
entities corresponds to the maximum independent set in graph G.

References
Aggarwal, G., Feldman, J., Muthukrishnan, S., & PaÃÅl, M. (2008). Sponsored search auctions with markovian
users. Internet and Network Economics, 621‚Äì628.
Aggarwal, G., Goel, A., & Motwani, R. (2006). Truthful auctions for pricing search keywords. In Proceedings
of the 7th ACM conference on Electronic commerce (pp. 1‚Äì7), ACM.
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying search results. In Proceedings of
the Second ACM International Conference on Web Search and Data Mining (pp. 5‚Äì14). ACM.
Balakrishnan, R., & Kambhampati, S. (2008). Optimal ad ranking for profit maximization. In Proceedings
of the 11th International Workshop on the Web and Databases.
Carterette, B. (2010). An analysis of NP-completeness in novelty and diversity ranking. Advances in
Information Retrieval Theory, 200‚Äì211.
Chapelle, O., & Zhang, Y. (2009). A dynamic bayesian network click model for web search ranking. In
Proceedings of World Wide Web (pp. 1‚Äì10). ACM.
Chierichetti, F., Kumar, R., & Raghavan, P. (2011). Optimizing two-dimensional search results presentation.
In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 257‚Äì
266). ACM.
Clarke, C.L.A., Agichtein, E., Dumais, S., & White, R.W. (2007). The influence of caption features on
clickthrough patterns in web search. In Proceedings of SIGIR (pp. 135‚Äì142). ACM.
Clarke, E.H. (1971). Multipart pricing of public goods. Public Choice, 11(1), 17‚Äì33.
Craswell, N., Zoeter, O., Tayler, M., & Ramsey, B. (2008). An experimental comparison of click position
bias models. In Proceedings of WSDM (pp. 87‚Äì94).
Deng, X., & Yu, J. (2009). A new ranking scheme of the GSP mechanism with markovian users. Internet and
Network Economics, 583‚Äì590.
Dupret, G.E., & Piwowarski, B. (2008). A user browsing model to predict search engine click data from past
observations. In Proceedings of SIGIR, (pp. 331‚Äì338). ACM.
Easley, D., & Kleinberg, J. (2010). Networks, crowds, and markets: Reasoning about a highly connected
world: Cambridge Univ Press.
Edelman, B., Ostrovsky, M., & Schwarz, M. (2007). Internet advertising and the generalized second price
auction: Selling billions of dollars worth of keywords. The American Economic Review, 97(1).
Garey, M.R., & Johnson, D.S. (1976). The complexity of near-optimal graph coloring. Journal of the ACM
(JACM), 23(1), 43‚Äì49.
Ghosh, A., & Sayedi, A. (2010). Expressive auctions for externalities in online advertising. In Proceedings
of the 19th international conference on World wide web (pp. 371‚Äì380). ACM.
Giotis, I., & Karlin, A. (2008). On the equilibria and efficiency of the GSP mechanism in keyword auctions
with externalities. Internet and Network Economics, 629‚Äì638.
Gordon, M.G., & Lenk, P. (1991). A utility theory examination of probability ranking principle in information
retrieval. Journal of American Society of Information Science, 41, 703‚Äì714.
Gordon, M.G., & Lenk, P. (1992). When is probability ranking principle suboptimal. Journal of American
Society of Information Science, 42.
Groves, T. (1973). Incentives in teams. Econometrica: Journal of the Econometric Society, 617‚Äì631.
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.M., & Faloutsos, C. (2009). Click chain model
in web search. In Proceedings of World Wide Web (pp. 11‚Äì20). New York: ACM.
HaÃästad, J. (1996). Clique is hard to approximate within n. In Foundations of Computer Science, 1996. 37th
Annual Symposium on Proceedings (pp. 627‚Äì636).
Hu, B., Zhang, Y., Chen, W., Wang, G., & Yang, Q. (2011). Characterizing search intent diversity into click
models. In Proceedings of the 20th international conference on World wide web (pp. 17‚Äì26). ACM.
Kempe, D., & Mahdian, M. (2008). A cascade model for externalities in sponsored search. Internet and
Network Economics, 585‚Äì596.

J Intell Inf Syst
Kuminov, D., & Tennenholtz, M. (2009). User modeling in position auctions: re-considering the gsp and vcg
mechanisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 1 (pp. 273‚Äì280).
Rafiei, D., Bharat, K., & Shukla, A. (2010). Diversifying Web Search Results. In Proceedings of World Wide
Web.
Richardson, M., Dominowska, E., & Ragno, R. (2007). Predicting clicks: Estimating the click-through rate
for new ads. In Proceedings of World Wide Web.
Richardson, M., Prakash, A., & Brill, E. (2006). Beyond pagerank: Machine learning for static ranking. In
World Wide Web Proceedings (pp. 707‚Äì714). ACM.
Robertson, S.E. (1977). The probability ranking principle in ir. Journal of Documentation, 33, 294‚Äì304.
Varian, H.R. (2007). Position auctions. International Journal of Industrial Organization, 25(6), 1163‚Äì1178.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1), 8‚Äì37.
Xu, W., Manavoglu, E., & Cantu-Paz, E. (2010). Temporal click model for sponsored search. In Proceedings
of the 33rd international ACM SIGIR conference on Research and development in information retrieval
(pp. 106‚Äì113). ACM.
Yilmaz, E., Shokouhi, M., Craswell, N., & Robertson, S. (2010). Expected browsing utility for web search
evaluation. In Proceedings of the 19th ACM international conference on Information and knowledge
management (pp. 1561‚Äì1564). ACM.
Yue, Y., Patel, R., & Roehrig, H. (2010). Beyond position bias: Examining result attractiveness as a source
of presentation bias in clickthrough data. In Proceedings of World Wide Web.
Zhu, Z.A., Chen, W., Minka, T., Zhu, C., & Chen, Z. (2010). A novel click model and its applications to
online advertising. In In Proceedings of Web search and data mining (pp. 321‚Äì330). ACM.

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Compliant Conditions for Polynomial Time
Approximation of Operator Counts
Tathagata Chakraborti‚àó
Sarath Sreedharan‚àó Sailik Sengupta‚àó
T. K. Satish Kumar‚Ä†
Subbarao Kambhampati‚àó

arXiv:1605.07989v2 [cs.AI] 5 Jul 2016

‚àó

‚Ä†
‚àó

Dept. of Computer Science, Arizona State University
Dept. of Computer Science, University of Southern California

{tchakra2, ssreedh3, sailiks, rao}@asu.edu ‚Ä† tkskwork@gmail.com

Abstract
In this paper, we develop a computationally simpler version of the operator count
heuristic for a particular class of domains. The contribution of this abstract is threefold,
we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer
approximation for operator counts in polynomial time; and (3) discuss the relationship of
the proposed formulation to existing heuristics and investigate properties of domains where
such approaches appear to be useful.

The OP-COUNT Heuristic
Domain Model.
The domain is described by a set of variables f ‚àà F which can assume values from a (finite)
domain D(f ) ‚äÜ N. A state is given by the particular assignment of values to these variables:
S = {f = v | v ‚àà D(f ) ‚àÄf ‚àà F }. The value of variable f in state S is referred to as S(f ).
The action model A consists of operators a = hCa , Ea i where Ca is the cost of the action, and
Ea = {hf, vo , vn i | f ‚àà F ; vo , vn ‚àà {‚àí1} ‚à™ D(f )} is the set of effects. The transition function
Œ¥(¬∑) determines the next state after the application of action a to state S as Œ¥(a, S) = ‚ä• if ‚àÉhf, vo , vn i ‚àà Ea s.t. vo 6= ‚àí1 ‚àß vo 6= S(f );
= {f = vn ‚àÄhf, vo , vn i ‚àà Ea ; else f = S(f )} otherwise.

Plans and Operator Counts.
A planning problem is a tuple Œ† = hF , A, I, Gi, where I, G are the initial and (partial) goal
states respectively. The solution to the planning problem is a plan œÄ = ha1 , a2 , . . .i, œÄ(i) =
ai ‚àà A such that Œ¥(œÄ, I) |= G, where the cumulative transition function
is given by Œ¥(œÄ, S) =
P
Œ¥(ha2 , a3 , . . .i, Œ¥(a1 , S)). The cost of the plan is given by C(œÄ) = a‚ààœÄ Ca and an optimal plan
œÄ ‚àó is such that C(œÄ ‚àó ) ‚â§ C(œÄ) ‚àÄœÄ. The operator count for an action a given a plan œÄ is given by
Œª(a, œÄ) = |{i | a = œÄ(i)}| and the total operator count of the plan Œª(œÄ) = |œÄ|.
1
Published as a 2-page research abstract at the International Symposium on Combinatorial Search (SoCS), 2016

Compliant Variables.
We define compliant variables as those that whenever they occur as a precondition of an action,
they must also be an effect, and vice versa. Thus, f ‚àà F is compliant iff ‚àÄa ‚àà A, hf, vo, vn i ‚àà
Ea =‚áí vo 6= ‚àí1 ‚àß vn 6= ‚àí1; f is referred to as rogue otherwise. Let Œ¶ ‚äÜ F be the set of all
compliant variables, and the set of compliant variables whose values are specified in the goal
be œÜ ‚äÜ Œ¶, henceforth referred to as goal compliant conditions.
The State Transformation Equation.
Let |œÜ| = m and |A| = n. Consider an m √ó n matrix M whose ij th element Mij ‚àà Z is the
numerical change in fi ‚àà œÜ produced by action aj ‚àà A, i.e. Mij = vn ‚àí vo ; hfi , vo , vn i ‚àà Eaj .
Also, let D be a vector of size m whose ith entry di is the change in a goal compliant f ‚àà œÜ
from the current state to the final state, i.e. di = vg ‚àí vc ; vg = fi ‚àà G, vc = fi ‚àà S; and let x be
a vector of size n, whose ith element is xi ‚àà N. Then the following equality holds:
Mx = D

(1)

The integer solution x‚àó to this system of linear equations with the least |x‚àó | gives a lower bound
on the operator counts required to solve the planning problem, i.e. |x‚àó | ‚â§ |œÄ ‚àó |. We can compute
a real-valued approximation in closed-form, by
min ||Qx||22
s.t. Mx = D

(2)
(3)

using the Lagrangian multiplier method for this optimization problem as follows 1
L(x) = ||Qx||2 + ŒªT (D ‚àí Mx)
2
=‚áí x‚àó = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 D

(4)
(5)

Here Q is a n √ó n matrix of action costs whose ij th entry Qij = Cai if i = j; 0 otherwise
(for unit cost domains) Q is an identity matrix and x‚àó = MT (MMT )‚àí1 D The most costly
operation here is the calculation of the pseudo inverse, which can be done in ‚âà O(n2.3 ) time.
Further, M is problem independent, and hence the factor Z = Q‚àí2 MT (MQ‚àí2 MT )‚àí1 can
be precomputed given an action model. Thus it follows that we can readily use ||QZD|| as a
heuristic for state-space search.
Note that this formulation can also determine infeasibility of goal reachability immediately
(in domains where actions are not reversible this is extremely useful) when the system is unsolvable, as shown in Algorithm 1. Unfortunately, the use of the l2 -norm, that helps us in obtaining
the closed-form polynomial bound heuristic, also makes the heuristic inadmissible.
Sparse coding.
Since operator counts are integers, we would ideally want an integer solution to Eqn 4 (which
makes the problem computationally intractable). Unfortunately, the polynomial bound Lagrangian
method described above does not address this aspect giving rise to bad heuristic values for
certain section of problems. To describe this problem geometrically, we consider a planning
domain with two compliant operators (of unit cost), such that x =< x1 , x2 >. If the plane
inscribed by Mx = D in the two dimensional space is close two either of the axis, the l2 norm
calculated above results in small fractional values, and hence a less informed heuristic. As can
2

Algorithm 1 Using OP-COUNT Heuristic for State-Space Search
procedure P RE - COMPUTE(Œ†)
Compute M, Q
Convert M to row echelon form ‚Üí T is the transformation matrix, r is the rank
Y ‚Üê M[1 : r, :], Z ‚Üê Q‚àí2 Y T (YQ‚àí2 Y T )‚àí1
procedure h(S) = OP-COUNT(S, G)
Compute D = G ‚àí S
Compute T d = T √ó D and œÑ = Td [1 : r]
if tdi 6= 0 ‚àÄi ‚â• r + 1 then No solution!
else
return ‚åàQ √ó Z √ó œÑ ‚åâ

be seen in the figure 1, the actual operator counts for the given example (with M = 15 4
and D = 12 ) should have been x1 = 0 and x2 = 3. But the l2 minimization results in small
fractional values with x1 = 0.77 and x2 = 0.77, and the heuristic values of hl2 = 1.54 instead
of |œÄ ‚àó | = 3.
x2
Mx = D

l2 -norm

x1

Figure 1: Eucledian norm minimization produces small fractional values for x1 and x2
Thus, we propose a different approximation method to obtain integer values for individual
operator counts, remaining within the polynomial time bound.
We notice that in most cases n ‚â´ m and also n ‚â´ |x‚àó | due to the combinatorial explosion
during grounding of domains. Thus, we propose an operator count heuristic that exploits this
knowledge about the sparsity of x‚àó . Ideally, we would like to solve the following problem,

s.t.

min
|x|l0
Mx = D
x  0

since minimizing the l0 norm results in the sparsest solution. But, we encounter two problems.
Firstly, the optimal operator counts (x‚àó ), although sparse, might not be the sparsest solution.
Secondly, minimizing the l0 norm is NP-hard [5].
Thus, we draw upon compressed sensing techniques to enforce a level of sparsity when
computing the vector x. To this end, we suggest minimization of l1 -norm (l1 -LP) or weighted
l1 -norm (œâ-l1 -LP) [4] to enforce positive integer solutions.
3

Geometrically, as can be seen in figure 2 these norms produce a more informed heuristic
(hl1 = 1.60 and hœâ‚àíl1 = 3.4) for the aforementioned problem. This method tries to compress
(minimize) the norm ball (or box for that matter) as much as possible till it fits in the plane
Mx = D. The operator (dimension) that induces a tighter constraint (x1 in our case), limits the
expansion of the norm ball, producing a less informed heuristic (hl1 = 1.60). The weighted
l1 -norm method addresses this problem by minimizing the l1 -norm and iteratively penalizing
the increase along the tightest dimension till convergence is reached or maximum number of
iterations are achieved, resulting in a more informed heuristic (hœâ‚àíl1 = 3.4).
x2

x2
Mx = D

Mx = D

l1 norm
x1

œâ-l1 norm

x1

Figure 2: Eucledian norm minimization produces small fractional values for x1 and x2
For œâ-l1 -LP, we empirically observe that rounding up the individual operator counts produce
a more informed heuristic. Thus, we arrive at a polynomial time proxy for integer solutions.
Evaluations.
The table shows the evaluation of the proposed heuristics across a total of 83 problems from
five well-known unit cost planning domains. Each entry in the table represents the percentage
difference in the initial state heuristic value and the optimal plan length averaged across the
problems in each domain. The %-compliance column shows the average number of goal compliant predicates in the problems. Rows 1-3 show the performance of our heuristic on the original domains (‚Äò-‚Äô indicates that the heuristics could not be computed due to absence of any goal
complaint variables). Rows 3-6 show the performance in domains where the %-compliance was
increased (this was done by identifying instances in the action model where variables assume
a don‚Äôt care condition, i.e. a value of -1, and replacing it with appropriate values as entailed
by domain axioms). Finally, rows 6-9 show the performance of our heuristics in problems with
more completely specified goals (which results in higher percentage compliance). As expected,
our heuristic performs better as %-compliance increases across a particular domain. The performance of l1 LP and œâ-l1 LP highlights the usefulness of compressed sensing techniques in
obtaining better integer approximations to the MILP.

4

Domains
GED
Blocks-3ops
Blocks-4ops
Visitall
GED
Blocks-3ops
Blocks-4ops
Visitall
Blocks-3ops
Blocks-4ops
8-puzzle

%-compliance
34.29%
31.25%
19.64%
25.49%
31.25%
19.64%
21.75%
48.13%
42.86%
88.89%

l1 -MILP
55.48%
47.80%
67.71%
37.61%
47.80%
67.71%
28.41%
28.68%
56.25%
33.33%

l1 -LP
55.48%
47.80%
67.71%
34.02%
47.80%
67.71%
28.41%
28.68%
56.25%
40.00%

œâ ‚àí l1 -LP
75.76%
23.60%
35.42%
53.36%
23.60%
35.42%
44.37%
44.38%
12.50%
46.67%

OP-COUNT
55.48%
52.60%
67.71%
48.32%
52.60%
67.71%
100.00%
32.32%
64.58%
40.00%

Discussion and Related Work
Relation to Existing Heuristics.
The proposed heuristic has close associations with both heuristics on state change equations and
operator counts [8, 3, 10]. Specifically, compliant conditions capture the net change criteria very
succinctly and are thus extremely useful where such properties are relevant. Another interesting
connection to existing work is with respect to graph-plan based heuristics [2], except here we
are relaxing preconditions instead of delete effects.
Compliance.
Our approach works better in domains that have many goal compliant conditions, e.g. in manufacturing domains [6] or in puzzles like Sudoku [1]. Thus goal completion strategies and
semantic preserving actions have a direct effect on the quality of the heuristic. Intermediate
representations such as transition normal form (TNF) [7] should be investigated in this context.
Landmarks.
Our purpose here is not to compete with the most sophisticated heuristics of today but to motivate a special case that can be computed extremely efficiently. We discussed the simplest version
of this formulation here, but it can be easily extended to incorporate more informative features
like landmarks [9]. A landmark constraint is added by simply subtracting the corresponding net
change from D: di ‚Üê di ‚àí ka √ó (xn ‚àí xo ) if hdi , xo , xn i ‚àà Ea and a ‚àà A is an action landmark
with cardinality ka ; and the closed form solution remains valid. In fact in terms of plan recognition with operator counts, observations are landmarks and the same approach applies. This
demonstrates the flexibility of our approach.
Resource Constrained Interaction.
The approach is especially relevant in the context of multi-agent interactions constrained by
usage œÄ Œ± (Œ∑) of a shared resource Œ∑ by a plan œÄ Œ± of an agent Œ±. For example, in an adversarial
setting, if an agent Œ±2 wanted to stop Œ±1 from executing its plan, all it needs to do is to ensure
that ‚àÉŒ∑ s.t. œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) > |Œ∑|. Similarly, in a cooperative setting, if agent Œ±2 wanted to
ensure that Œ±1 ‚Äôs plan succeeds, it would need to make sure that ‚àÄŒ∑ œÄ Œ±1 (Œ∑) + œÄ Œ±2 (Œ∑) ‚â§ |Œ∑|.

5

In fact, as resource variables are compliant, our approach may provide quick estimates of an
agent‚Äôs intent without computing the entire plan.
Acknowledgment. This research is supported in part by the ONR grants N00014-13-1-0176, N00014-131-0519 and N00014-15-1-2027, and ARO grant W911NF-13-1-0023.

References
[1] Prabhu Babu, Kristiaan Pelckmans, Petre Stoica, and Jian Li. Linear systems, sparse
solutions, and sudoku. Signal Processing Letters, IEEE, 17(1):40‚Äì42, 2010.
[2] Avrim L Blum and Merrick L Furst. Fast planning through planning graph analysis. Artificial intelligence, 90(1):281‚Äì300, 1997.
[3] Blai Bonet, Menkes Van Den Briel, et al. Flow-based heuristics for optimal planning:
Landmarks and merges. In ICAPS, 2014.
[4] Emmanuel J CandeÃÄs, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by
reweighted l1 minimization. Journal of Fourier analysis and applications, 2008.
[5] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of l p minimization.
Mathematical programming, 129(2):285‚Äì299, 2011.
[6] Dana S Nau, Satyandra K Gupta, and William C Regli. Ai planning versus manufacturingoperation planning: A case study. In IJCAI, 1995.
[7] Florian Pommerening and Malte Helmert. A normal form for classical planning tasks. In
ICAPS, pages 188‚Äì192, 2015.
[8] Florian Pommerening, Gabriele RoÃàger, Malte Helmert, and Blai Bonet. Lp-based heuristics for cost-optimal planning. In ICAPS, 2014.
[9] Julie Porteous, Laura Sebastia, and JoÃàrg Hoffmann. On the extraction, ordering, and usage
of landmarks in planning. In ECP, pages 37‚Äì48, 2001.
[10] Menkes Van Den Briel, J Benton, Subbarao Kambhampati, and Thomas Vossen. An lpbased heuristic for optimal planning. In CP, pages 651‚Äì665. Springer, 2007.

6

Capability Models and Their Applications in Planning
Yu Zhang

Sarath Sreedharan

Subbarao Kambhampati

Dept. of Computer Science
Arizona State University
Tempe, AZ

Dept. of Computer Science
Arizona State University
Tempe, AZ

Dept. of Computer Science
Arizona State University
Tempe, AZ

yzhan442@asu.edu

ssreedh3@asu.edu

rao@asu.edu

ABSTRACT

1.

One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of
each other. An important aspect of these models of other agents is
that they are often not provided, and hence must be learned from
plan execution traces. As a result, these models of other agents
are inherently partial and incomplete. Most existing agent models are based on action modeling and do not naturally allow for
incompleteness. In this paper, we introduce a new and inherently
incomplete modeling approach based on the representation of capabilities, which has several unique advantages. First, we show that
the structures of capability models can be learned or easily specified, and both model structure and parameter learning are robust
to high degrees of incompleteness in plan traces (e.g., with only
start and end states partially observed). Furthermore, parameter
learning can be performed efficiently online via Bayesian learning.
While high degrees of incompleteness in plan traces presents learning challenges for traditional (complete) models, capability models
can still learn to extract useful information. As a result, capability models are useful in applications in which traditional models
are difficult to obtain, or models must be learned from incomplete
plan traces, e.g., robots learning human models from observations
and interactions. Furthermore, we discuss using capability models
for single agent planning, and then extend it to multi-agent planning (with each agent modeled separately by a capability model),
in which the capability models of agents are used by a centralized
planner. The limitation, however, is that the synthesized ‚Äúplans‚Äù
(called c-plans) are incomplete, i.e., there may or may not be a complete plan for a c-plan. This is, however, unavoidable for planning
using partial and incomplete models (e.g., considering planning using action models learned from partial and noisy plan traces).

One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of
others. These models can be used by a centralized planner (e.g., on
a robot) or via a distributed planning process to perform task planning and allocation, or by the agents themselves to reduce communication and collaboration efforts. In many applications, these
models of other agents are not provided and hence must be learned.
As a result, these models are going to be inherently partial and incomplete. Thus far, most traditional agent models are based on
action modeling (e.g., [18, 7]). These models are not designed with
partial information in mind and hence are complete in nature. In
this paper, we introduce a new and inherently incomplete modeling
approach based on the representation of capabilities. We represent
a capability as the ability to achieve a partial state given another
partial state. A capability can be fulfilled (or realized) by any action
sequence (or plan) that can implement a transition between the two
partial states. Each such action sequence is called an operation in
this paper. A capability model can encode all possible capabilities
for an agent in a given domain, as well as capture the probabilities
of the existence of an operation to fulfill these capabilities (to implement the associated transitions). These probabilities determine
which capabilities are more likely to be used in planning.
Compared to traditional agent models which are complete in nature, capability models have their unique benefits and limitations.
In this aspect, capability models should not be considered as a competitor to complete models. Instead, they are useful when complete
models are difficult to obtain or only partial and incomplete information can be retrieved to learn the models. This is often true when
humans must be modeled.
The representation of a capability model is a generalization of
a two time slice dynamic Bayesian network (2-TBN). While a 2TBN is often used to represent a single action (e.g., [19]), a capability model can encode all possible capabilities for an agent in a given
domain. In a capability model, each node in the first time slice (also
called a fact node) represents a variable that is used in the specification of the initial world state. For each fact node, there is also
a corresponding node in the second time slice, which represents
the fact node in the eventual state (i.e., after applying a capability).
These corresponding nodes are called eventual nodes or e-nodes.
The state specified by the fact nodes is referred to as the initial state,
and the state specified by the e-nodes is referred to as the eventual
state. The edges between the nodes within the initial and eventual
states, respectively, encode the correlations between the variables
at the same time instance (i.e., variables in synchrony). The edges
from the fact nodes to e-nodes encode causal relationships. Both
types of edges can be learned (e.g., using learning techniques in
[24, 25] for causal relationships). In the case that no prior infor-

Categories and Subject Descriptors
I.2.11 [Multiagent systems]; I.2.6 [Knowledge acquisition]; I.2.8
[Plan execution, formation, and generation]

Keywords
Capability models; Agent theories and models; Teamwork in humanagent mixed networks

Appears in: Proceedings of the 14th International
Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2015), Bordini, Elkind, Weiss, Yolum
(eds.), May 4‚Äì8, 2015, Istanbul, Turkey.
c 2015, International Foundation for Autonomous Agents
Copyright 
and Multiagent Systems (www.ifaamas.org). All rights reserved.

INTRODUCTION

Figure 1: Capability model (as a generalized 2-TBN) for a human delivery agent (denoted as AG) in our motivating example
with 6 fact node and e-node pairs. This model can encode all
possible capabilities of the agent in this domain. Each fact node
corresponds to a variable in the specification of the initial state.
e-nodes are labeled with a dot on top of the variable. Edges
from the fact nodes to the corresponding e-nodes are simplified
for a cleaner representation. Any partial initial state coupled
with any partial eventual state may imply a capability.

mation is provided, the safest approach is to not assume any independence. Consequently, we can specify a total ordering between
the nodes within the initial and final states, and connect a node at a
given level to all nodes at lower levels, as well as every fact node
to every e-node. After model construction, parameter learning of a
capability model can be performed efficiently online via Bayesian
learning.
One of the unique advantages of capability models is that learning for both model structure and parameters is robust to high degrees of incompleteness in plan execution traces. This incompleteness occurs commonly. For example, the execution observer may
not be constantly monitoring the executing agent, and the executing
agent may not always report the full traces [26]. Capability models can be learned even when only the initial and final states are
partially observed in plan traces for training. While high degrees
of incompleteness in plan traces presents learning challenges for
traditional models, capability models can still learn to extract useful information out of them, e.g., probabilities of the existence of
an operation to achieve certain eventual states given certain initial
states.
Furthermore, compared to traditional models for planning, capability models only suggest transitions between partial states (as capabilities), along with the probabilities that specify how likely such
transitions can be implemented. Hence, a ‚Äúplan‚Äù synthesized with
capability models is incomplete, and we refer to such a plan as a
c-plan (i.e., involving the application of capabilities). More specifically, we show that ‚Äúplanning‚Äù with capability models occurs in
the belief space, and is performed via Bayesian inference. At any
planning step, the current planning state is a belief state. Applying
a capability to this belief state requires the planner to update this
state using Bayesian inference on the capability model. After the
discussion of single agent planning, we then extend to multi-agent
planning.1 In such cases, we can consider planning as assigning
subtasks to agents without understanding precisely how these subtasks are to be handled. Moreover, multi-agent planning can be
performed when part of the agents are modeled by capability models, and the other agents are modeled by complete models. This
1
In this paper, we consider sequential multi-agent plans; synchronous or joint actions of agents are not considered.

situation can naturally occur in human-robot teaming scenarios, in
which human models need to be learned and the robot models are
given. Similarly, the synthesized multi-agent c-plans are incomplete (unless all plan steps use robot actions). c-plans are useful
when we have to plan using partial and incomplete models (e.g.,
considering planning using action models learned from partial and
noisy plan traces), since they inform the user how likely complete
plans can be realized by following their ‚Äúguidelines‚Äù.
The rest of the paper is organized as follows. First, we provide
a motivating example in Section 2. In Section 3, we discuss capability models in detail. We discuss how to use capability models in
planning in Sections 4 and 5. The relationships between capability models and other existing approaches to modeling the dynamics
of agents are discussed as related work in Section 6. We conclude
afterwards.

2.

MOTIVATING EXAMPLE

We start with a motivating example for capability models. In
this example, we have a set of human delivery agents, and the tasks
involve delivering packages to their destinations. However, there
are a few complications here. An agent may be able to carry a
package if this agent is strong enough. While an agent can use a
trolley to load a package, this agent may not remember to bring
a trolley initially. An agent can visit an equipment rental office
to rent a trolley, if this agent remembers to bring money for the
rental. A trolley increases the chance of an agent being able to
deliver a package (i.e., how likely the package is deliverable by this
agent). Figure 1 presents the capability model for a human delivery
agent, which is a generalized 2-TBN that can encode all possible
capabilities for the agent in this domain. Similar to 2-TBN, fact
nodes (e-nodes) in a capability model are in synchrony in the initial
(eventual) state. Note that any partial initial state coupled with any
partial eventual state may imply a capability.
In this example, the correlations between the variables are clear.
For example, whether an agent can carry a package is dependent on
whether the agent is strong; whether an agent can deliver a package
may be influenced by whether the agent has a trolley. These correlations are represented as edges between the variables within the
initial and eventual states, respectively and identically. Assuming
that no prior information is provided for the causal relationships,
we connect every fact node with every e-node (denoted as a single
edge in Figure 1).
After obtaining the model structure, to perform parameter learning for the capability model of an agent, this agent can be given
delivery tasks spanning a period of time for training purpose. However, the only observations that a manager has access to may be the
number of packages that have been successfully delivered by this
agent during this period. While this presents significant challenges
for learning the parameters of traditional complete models, the useful information for the manager is already encoded: the probability
that this agent can deliver a package. Capability models can learn
this information from the incomplete traces.
Now, suppose that the manager also has a set of robots (with action models) to use that can fetch items for the delivery agents.
Since it is observed that the presence of a trolley increases the
probability of successful delivery based on the capability models,
the manager can make a multi-agent c-plan, which ensures that the
robots deliver a trolley at least to some of the delivery agents. This
also illustrates that it is beneficial to combine capability models
with other complete models (i.e., action models) when they are
available (e.g., for robots in this example).

3.

CAPABILITY MODEL

In our settings, the environment includes a set of agents Œ¶ working inside it. For each agent, for simplicity, we assume that the state
of the world and this agent is specified by a set of boolean variables
XœÜ (œÜ ‚àà Œ¶). This implies that an agent can only interact with other
agents through variables that pertain to the world state.
More specifically, for all Xi ‚àà XœÜ , Xi has domain D(Xi ) =
{true, false}. To specify partial state, we augment the domains of
variables to include an unknown or unspecified value as in [1]. We
write D+ (Xi ) = D(Xi ) ‚à™ {u}. Hence, the (partial) state space is
denotedSas S = D+ (X1 ) √ó D+ (X2 ) √ó ... √ó D+ (XN ), in which
N = | œÜ XœÜ |.

following probability distribution:
Z T
P (XœÜ , XÃáœÜ ) =
P (XœÜ , XÃáœÜ , t) dt

(1)

0

in which T represents the maximum length of any operation (i.e.,
number of actions) for œÜ.2 XœÜ represents the initial state and XÃáœÜ
represents the eventual state. Furthermore, P (XÃáœÜ , t| XœÜ ) is the
probability of any operation resulting in XÃáœÜ in exact time t given
XœÜ . Hence, the probability that is associated with a capability
sI ‚áí sE (i.e., P (sI ‚áí sE )) encodes:
Z
P (XÃáœÜ = sE | XœÜ = sI ) = P (XÃáœÜ = sE , t | XœÜ = sI ) dt (2)
t

3.1

Capability

First, we formally define capability for an agent œÜ ‚àà Œ¶. The
state space of agent œÜ is denoted as SœÜ .
D EFINITION 1 (C APABILITY ). Given an agent œÜ, a capability specified as a mapping SœÜ √ó SœÜ ‚Üí [0, 1], is an assertion about
the probability of the existence of complete plans (for œÜ) that connect an initial state (i.e., the first component on the left hand side
of the mapping) to an eventual state (i.e., the second component).
A capability is also denoted as sI ‚áí sE (i.e., the initial state
‚áí the eventual state) when we do not need to reference the associated probability value. The probability value is denoted as
P (sI ‚áí sE ), which we will show later is computed based on
the capability model via Bayesian inference. There are a few notes
about Definition 1. First, both sI and sE can be partial states: the
variables that have the value u are assumed to be unknown (in the
initial state) and unspecified (in the eventual state). In this paper,
we refer to a complete plan that fulfills (or realizes) a capability
as an operation, which is going to be decided and implemented by
the executing agent during execution. Although capabilities seem
to be similar to high-level actions in HTN, the semantics is different from angelic uncertainty in HTN [12]. While the existence of a
concretization is ensured in HTN planning via reduction schemas,
it is only known with some level of certainty that a concretization
(i.e., operation) exists for a capability in a capability model, and the
capability model does not provide specifics for such a concretization. More discussions on this are provided in Section 6 when we
discuss the relationships between capability models and existing
approaches to modeling agent dynamics.
Capability models also address the qualification and ramification
problems, which are assumed away in STRIPS planning (and planning with many complete models). More specifically, an operation
for a capability sI ‚áí sE may be dependent on variables with unknown values in sI , and updating variables with unspecified values
in sE . This is a unique characterization of capability and critical
for learning capability models with incomplete observations.
For example, a capability may specify that given coffee beans, an
agent can make a cup of coffee. Thus, we have a capability {Has
coffee beans = true} ‚áí {Has coffee = true}. An operation for this
capability to make a coffee may be dependent on the availability of
water initially, which is not specified in the capability. Similarly,
this operation may negate the fact that the kitchen is clean, which
is not specified in the capability either. Note that a capability may
be fulfilled by operations with different specifications of the initial
and eventual states, as long as these specifications all satisfy the
specification of this capability.
A capability model of an agent is capable of encoding all capabilities of this agent given a domain; it is designed to encode the

We construct the capability model of an agent as a Bayesian network. As an inherently incomplete model, it not only allows the
initial and eventual states to be partially specified for any capability
(and hence the fulfilling operations) that it encodes, but also allows
the correlations between the variables within the initial and eventual states, as well as the causal relationships between them to be
partially specified (e.g., when learning from plan traces). However,
there are certain implications in this (e.g., the modeling can lose
some information), which we will discuss in Section 3.4. Along
the line of the qualification and ramification problems, a capability
model also allows certain variables to be excluded completely from
the network (i.e., related variables that are not captured in XœÜ ) due
to incomplete knowledge. For example, whether an agent can drive
a car (with a manual transmission) to a goal location is dependent
on whether the agent can drive a manual car, even through the agent
has a driver license. In this case, the ability to drive a manual car
may have been ignored when creating the model.

3.2

Model Construction

We construct the capability model of each agent as an augmented
Bayesian network [14]. Any partial initial state coupled with any
partial eventual state may imply a capability; the probability that a
capability actually exists (i.e., it can be fulfilled by an operation)
is computed via a Bayesian inference in the network. We use augmented Bayesian network since it allows prior beliefs of conditional relative frequencies to be specified before observations are
made, as well as enables us to adjust how fast these beliefs should
change.
D EFINITION 2. An augmented Bayesian network (ABN) (G, F, œÅ)
is a Bayesian network with the following specifications:
‚Ä¢ A DAG G = (V, E), where V is a set of random variables,
V = {V1 , V2 , ..., Vn }.
‚Ä¢ F is a set of auxiliary parent variables for V .
‚Ä¢ ‚àÄVi ‚àà V , an auxiliary parent variable Fi ‚àà F of Vi , and a
density function œÅi associated with Fi . Each Fi is a root and
it is only connected to Vi .
‚Ä¢ ‚àÄVi ‚àà V , for all values pai of the parents P Ai ‚äÜ V of
Vi , and for all values fi of Fi , a probability distribution
P (Vi |pai , fi ).
A capability model of an agent œÜ is then defined as follows:
D EFINITION 3 (C APABILITY M ODEL ). A capability model of
an agent œÜ, as a binomial ABN (GœÜ , F, œÅ), has the following specifications:
2

We assume in this paper that time is discretized.

‚Ä¢ VœÜ = XœÜ ‚à™ XÀôœÜ .
‚Ä¢ ‚àÄVi ‚àà VœÜ , the domain of Vi is D(Vi ) = {true, false}.
‚Ä¢ ‚àÄVi ‚àà VœÜ , Fi = {Fi1 , Fi2 , ...}, and each Fij is a root and
has a density function œÅij (fij ) (0 ‚â§ fij ‚â§ 1). (For each
value paij of the parents P Ai , there is an associated variable Fij .)
‚Ä¢ ‚àÄVi ‚àà VœÜ , P (Vi = true|paij , fi1 , ...fij , ...) = fij .
in which j in paij indexes into the values of P Ai . j in fij indexes
into the variables in Fi . Note that defining partial states (i.e., allowing variables to assume the value u in a state) is used to more
conveniently specify the distribution in Equation 1. Variables in the
capability model do not need to expand their domains to include u.
For edge construction, we can learn the correlations and causal
relationships from plan traces. Note that the correlations between
variables must not form loops; otherwise, they need to be broken
randomly. When no training information is provided, we can specify a total ordering between the nodes within the initial and final
states, and connect a node at a given level to all nodes at lower
levels, as well as every fact node to every e-node. Denote the set
of edges as EœÜ . We then have constructed the capability model
GœÜ = (VœÜ , EœÜ ) for œÜ. Figure 1 provides a simple example of a
capability model.

3.3

Parameter Learning

In this section, we describe how the model parameters can be
learned. The parameter learning of a capability model is performed
online through Bayesian learning. The initial model parameters
can be computed from existing plan traces by learning the density
functions (i.e., fij ) in Definition 3. These parameters can then be
updated online as more traces are collected (i.e., as the agent interacting with the environment).
Plan execution traces can be collected each time that a plan (or a
sequence of actions) is executed, whether succeeds or fails.
D EFINITION 4 (C OMPLETE P LAN T RACE ). A complete plan
trace is a continuous sequence of state observations over time, denoted as T = hs‚àó1 , s‚àó2 , ..., s‚àóL i, in which L is the length of the plan
and s‚àói denotes a complete state (i.e., s‚àói ‚àà D(X1 ) √ó D(X2 ) √ó
... √ó D(XN )).
However, in real-world situations, plan traces may be incomplete. The incompleteness can come from two aspects. First, the
observed state may be partial. Second, the observations may not be
continuous. Hence, we are going to have partial plan traces.
D EFINITION 5 (PARTIAL P LAN T RACE ). A partial plan trace
is a discontinuous sequence of partial state observations over time,
denoted as T = hsi , si+k1 , si+k2 , ...i, in which i denotes the time
step in the complete plan and si denotes a partial state (i.e., si ‚àà
D+ (X1 ) √ó D+ (X2 ) √ó ... √ó D+ (XN )).
Note that the only assumption that is made in Definition 5 is
that at least two different partial states must be observed during the
plan execution. This means that even the start and end states of a
plan execution do not necessarily have to be observed or partially
observed, which is especially useful in real-world situations where
a plan trace may only be a few samplings of (partial) observations
during a plan execution. Note also that since the observations are
in discontinuous time steps, the transition between contiguous state
observations is not necessarily the result of a single action. Instead,

it is the result of a plan (i.e., operation) execution. Henceforth,
when we refer to plan traces, we always intend to mean partial plan
traces, unless otherwise specified.
When more than two states are observed in a plan trace, it can
be considered as a set of traces, with each pair of contiguous states
as a separate trace. When the states are partially observed in a plan
trace, it can be considered as a set of compatible traces with complete state observations.3 For simplicity, we assume in the following that the plan execution traces used in learning have complete
state observations. We denote this set of traces as D.
To learn the parameters of a capability model, a common way is
to model Fij using a beta distribution (i.e., as its density function
œÅ). Denote the parameters for the beta distribution of Fij as aij and
bij . Then, we have:
aij
(3)
P (Xi = true|paij ) =
aij + bij
Suppose that the initial values or the current values for aij and
bij are given. The remaining task is to update aij and bij from the
given traces. Given the training set D, we can now follow Bayesian
inference to update the parameters of Fij as follows:
Initially or currently,
œÅ(fij ) = beta(fij ; aij , bij )

(4)

After observing new training examples D, we have:
œÅ(fij |D) = beta(fij ; aij + sij , bij + tij )

(5)

in which sij is the number of times for which Xi is true while P Ai
assuming the value of paij , and tij is the number in which it equals
Xi is false while P Ai assuming the value of paij .

3.4

Implications

In this section, we discuss several implications of capability models with a simple example. We first investigate how information can
be lost during learning when the correlations or causal relationships
among the variables are only partially captured. This can occur, for
example, when model structure is learned. In this example, we
have two blocks A, B, and a table. Both blocks can be placed on
the table or on each other. The capability model for an agent is
specified in Figure 2. Initially, assuming that we do not have any
knowledge of the domain, the density functions can be specified as
beta distributions with a = b. In Figure 2, we use a = b = 1 for
all distributions. Suppose that we observe the following plan trace,
which can be the result of executing a single action that places A
on B:
s1 : OnT able(A) ‚àß OnT able(B) ‚àß ¬¨On(A, B) ‚àß ¬¨On(B, A)
s2 : ¬¨OnT able(A) ‚àß OnT able(B) ‚àß On(A, B) ‚àß ¬¨On(B, A)
Based on the learning rules in Equation (5), we can update the
beta distributions accordingly. For example, the beta distribution
for XÃá3 (i.e., On(A, B) in the eventual state) is updated to beta(X1 =
true, X4 = true, XÃá4 = false, ...; 2, 1). This means that if both A
and B are on the table, it becomes more likely that a capability exists for making On(A, B) = true. This is understandable since an
action that places A on B would achieve On(A, B) = true in such
cases. For actions with uncertainties (i.e., when using a robotic arm
to perform the placing action), this beta distribution would converge to the success rate as more experiences are obtained.
Meanwhile, we also have that the beta distribution for XÃá2 is
updated to beta(X1 = true, X3 = false, XÃá4 = false, XÃá3 =
3

There is no need to expand such traces into sets of traces with
complete state observations for learning, since it can be equivalently considered using arithmetic operations.

that satisfies sE v s in the resulting belief state as follows:
P (s) =

P (XÃáœÜ = s|XœÜ = s‚àó )
P (s‚àó ‚áí s)
=
‚àó
P (s ‚áí sE )
P (XÃáœÜ = sE |XœÜ = s‚àó )

(6)

For any state s that does not satisfy sE v s, we have P (s) = 0.
Denote S in b(S) as S = {s|sE v s and s is a complete state}.
Clearly, we have:
X
P (s) = 1
(7)
s‚ààS

Figure 2: Capability model for an agent in a simple domain
with four variables. OnT able(A) means that object A is on the
table. On(A, B) means that object A is on B. The correlations
that correspond to the light blue arrows distinguish between
two scenarios: one with proper model structure and one without (i.e., when certain correlations are missing). For clarity, we
only specify the density functions of the variables in the initial
state. We also show the two sets of density functions for the
augmented variables for X2 for the two different scenarios, respectively.
true, ...; 1, 2). This means that if A is on B in the eventual state, it
becomes less likely that B can also be on A in the eventual state.
This is intuitive since we know that achieving On(A, B) = true
and On(B, A) = true at the same time is impossible. In this
way, capability models can reinforce the correlations between the
variables as experiences are observed. The implication is that the
edges (capturing the correlations) between these variables must be
present; otherwise, information may be lost as described above. If
the correlations are not fully captured by the model, for example,
when the light blue arrows in Figure 2 are not present, the beta distribution of XÃá2 would not be updated as above, since XÃá3 would no
longer be a parent node of XÃá2 . A similar implication also applies
to causal relationships.
When environment changes, previous knowledge that is learned
by the model may no longer be applicable. However, as the parameters grow, the learned model can be reluctant to adapt to the
new environment. This issue can be alleviated by performing normalizations occasionally (i.e., dividing all a and b by a constant in
the beta distributions), or by weighting previous experiences with
a discounting factor.

4.

USING CAPABILITY MODELS IN
PLANNING

Capability models described in the previous section allow us to
capture the capabilities of agents. In this section, we discuss the
application of capability models in single agent planning (i.e., with
an agent œÜ modeled by a capability model). We extend the discussion to multi-agent planing in the next section. Generally, planning
with capability models occurs in the belief space, as with POMDPs
[10].

4.1

Single Agent Planning

First, note that applying a capability sI ‚áí sE of agent œÜ to a
complete state s‚àó results in a belief state b(S) as long as sI v s‚àó
(otherwise, this capability cannot be applied), in which sI v s‚àó
denotes that all the variables that are not assigned to u in sI have the
same values as those in s‚àó . After applying the capability, assuming
successfully, we can compute the probability weight of a state s

Since there can be an exponential number of complete states in
a belief state, depending on how many variables are assigned to
u, we can use a sampling approach (e.g., Monte Carlo sampling)
to keep a set of complete states to represent b(S). We denote the
belief state after sampling as bÃÇ(S).
When applying a capability sI ‚áí sE to a given belief state
bÃÇ(S), for each complete state in S, we can perform sampling based
on Equation (6), which returns a set of complete states with weights
after applying the capability. We can then perform resampling on
the computed sets of states for all states in S to compute the new
belief state bÃÇ(S 0 ). In this way, we can connect different capabilities
of an agent to create c-plans, which are plans in which capabilities
are involved. Next, we formally define a planning problem for a
single agent with a capability model.
D EFINITION 6 (S INGLE AGENT P LANNING P ROBLEM ). A single agent planning problem with capability models is a tuple hœÜ,
b(I), G, œÅi, in which b(I) is the initial belief state, G is the set of
goal variables, and œÅ is a real value in (0, 1]. The capability model
of the agent is GœÜ = (VœÜ , EœÜ ). When we write œÅ‚àó instead of œÅ in
the problem, it indicates a variance of the problem that needs to
maximize œÅ.
D EFINITION 7 (S INGLE AGENT C-P LAN ). A single agent cplan for a problem hœÜ, b(I), G, œÅi is a sequence of application of
capabilities, such that the sum of the weights of the complete states
in the belief state which include the goal variables (i.e., G v s), is
no less than œÅ (or maximized for œÅ‚àó ) after the application.
The synthesized single agent ‚Äúplan‚Äù (i.e., a c-plan) is incomplete:
it does not specify which operation fulfills each capability. In fact,
it only informs the user how likely there exists such an operation. A
single agent c-plan can be considered to provide likely landmarks
for the agent to follow. For example, in Figure 2, suppose that the
initial state is
sI : On(A, B) ‚àß ¬¨On(B, A)
and the goal is
sE : ¬¨On(A, B) ‚àß On(B, A)
A c-plan created with a capability model may include an intermediate state in the form of sI ‚áí sin ‚áí sE ,4 in which sin can
include, e.g., OnT able(A) = true and OnT able(B) = true, such
that the probability of the existence of a sequence of operations to
fulfill the c-plan may be increased. Another example is our motivating example (Figure 1) in which having has_trolley(AG) = true
as an intermediate state helps delivery when has_trolley(AG) is
false at the beginning.
Although a single agent c-plan may seem to be less useful than
a complete plan synthesized with action models, it is unavoidable
4
Note that P (sI ‚áí sE ) is not equivalent to P (sI ‚áí sin ) ¬∑
P (sin ‚áí sE ).

when we have to plan with incomplete knowledge (e.g., incomplete
action models). This is arguably more common in multi-agent systems, in which the planner needs to reason about likely plans for
agents even when it does not have complete information about these
agents. A specific application in such cases is when we need to perform task allocation, in which it is useful for the system to specify
subtasks or state requirements for agents that are likely to achieve
them without understanding how the agents achieve them.

4.2

Planning Heuristic

Given a single agent planning problem hœÜ, b(I), G, œÅi, besides
achieving the goal variables, planning should also aim to reduce the
cost of the c-plan (i.e., probability of success for the sequence of
application of capabilities in the c-plan).
ASSUMPTION: To create a heuristic, we make the following
assumption ‚Äì capabilities do not have variables with false values
in sI . With this assumption, we have the following monotonicity
properties hold:
P (sI ‚áí sE ) ‚â• P (s0I ‚áí sE )(T (s0I ) ‚äÜ T (sI ) ‚àß F (sI ) ‚äÜ F (s0I ))
(8)
in which T , F are used as operators on a set of variables to denote the set of variables with true and false values, respectively.
Equation (8) implies it is always easier to achieve the desired state
with more true-value variables and less false-value variables in the
initial state; and
P (sI ‚áí sE ) ‚â• P (sI ‚áí s0E )(T (sE ) ‚äÜ T (s0E )‚àßF (sE ) ‚äÜ F (s0E ))
(9)
which implies that it is always more difficult to achieve the specified values for more variables in the eventual state.
HEURISTIC: We use A‚àó to perform the planning. At any time
the current planning state is a belief state bÃÇ(S) (i.e., b(S) after
sampling); there is also a sequence of application of capabilities
(i.e., a c-plan prefix), denoted as œÄ, to reach this belief state from
the initial state. We compute the heuristic value for bÃÇ(S) (i.e.,
f (bÃÇ(S)) = g(bÃÇ(S)) + h(bÃÇ(S))) as follows. First, we compute
g(bÃÇ(S)) as the sum of the negative logarithms of the associated
probabilities of capabilities in œÄ.
To compute h(bÃÇ(S)), we need to first compute h(s) for each
complete state s ‚àà S. To compute an admissible h(s) value, we
denote the set of goal variables that are currently false in s as Gs .
Then, we compute h(s) as follows:
h(s) = argmax ‚àí log P (s¬¨v ‚áí s{v = true})

(10)

v‚ààGs ,s¬¨v

in which s¬¨v denotes a complete state with only v as false, and
s{v = true} denotes the state of s after making v true. Finally, we
compute h(bÃÇ(S)) as:
X
P (s) ¬∑ h(s)
(11)
h(bÃÇ(S)) =
s‚ààS

L EMMA 1. The heuristic given in Eq. (11) is admissible for
finding a c-plan that maximizes œÅ (i.e., with œÅ‚àó ), given that bÃÇ(S)
accurately represents b(S).
P ROOF. We need to prove that h(bÃÇ(S)) is not an over-estimate
of the cost for S to reach the goal state G while maximizing œÅ.
First, note that we can always increase œÅ by trying to move a nongoal state (in the current planning state) to a goal state (i.e., G v s).
Furthermore, for each s ‚àà S, given the monotonicity properties, we
know that at least h(s) cost must be incurred to satisfy G. Hence,
the conclusion holds.

Figure 3: Illustration of solving a single agent planning problem with a capability model. The topology of the capability
model used is shown in the top part of the figure.

Lemma 1 implies that the A‚àó search using the heuristic in Equation (11) would continue to improve œÅ given sufficient time. Hence,
the heuristic should be used as an anytime heuristic and stop when
a desired value of œÅ is obtained or the increment is below a threshold. Also, approximation solutions should be considered in future
work to scale to large networks.

4.3

Evaluation

We provide a preliminary evaluation of single agent planning
with a capability model. We build this evaluation based on the
blocksworld domain. First, we use 20 problem instances with 3
blocks5 and generate a complete plan for each instance. Then, we
randomly remove 1‚àí5 actions from each complete plan to simulate
partial plan traces.
The capability model for this domain contains 12 variables and
we ignore the holding and handempty predicates to simulate partial state observations. We manually construct the correlations between the variables in the initial and eventual states. For example,
On(A, B) is connected with On(B, A) since they are clearly conflicting. For causal relationships, we connect every node in the
initial state to every node in the eventual state.
After learning the parameters of this capability model based on
the partial plan traces, we apply the capability model to solve a
problem as shown in Figure 3. The topology of the capability
model constructed is shown in the top part of the figure, which
is also the model used in Figure 4. Since we have connected every fact node with every e-node in this case, the model appears to
be quite complex. Initially, we have On(B3, B2), On(B2, B1),
and OnT able(B1) (a complete state), and the goal is to achieve
On(B2, B3). We can see in Figure 3 that I in b(I) contains a
single complete state. The c-plan involves the application of two
different capabilities. For illustration purpose, we only show two
possible states (in the belief state) after applying the first capability,
For one of the two states, we then show two possible states after
applying the second capability. Note that each arrow represents a
sequence of actions in the original action model. Also, the possible
states must be compatible with the specifications of the eventual
states in the capabilities. We see some interesting capabilities being
5
It does not have to be three blocks but we assume only three blocks
to simplify the implementation.

learned. For example, the first capability is to pull out a block that
is between two other blocks, and place it somewhere else.

5.

MULTI-AGENT PLANNING

In this section, we extend our discussion from single agent planning to multi-agent planning. In particular, we discuss how capability models can be used along with other complete models (i.e.,
action models) by a centralized planner to synthesize c-plans. The
settings are similar to those in our motivating example. In particular, we refer to multi-agent planning with mixed models as MAPMM. This formulation is useful in applications in which both human and robotic agents are involved. While robotic agents are programmed and hence have complete models, the models of human
agents must be learned. Hence, we use capability models to model
human agents and assume that the models for the human agents are
already learned (e.g., by the robots) for planning.
For robotic agents, we assume STRIPS action model hR, Oi, in
which R is a set of predicates with typed variables, O is a set of
STRIPS operators. Each operator o ‚àà O is associated with a set of
preconditions P re(o) ‚äÜ R, add effects Add(o) ‚äÜ R and delete
effects Del(o) ‚äÜ R.
D EFINITION 8. Given a set of robots R = {r}, a set of human agents Œ¶ = {œÜ}, and a set of typed objects O, a multiagent planning problem with mixed models is given by a tuple Œ† =
hŒ¶, R, b(I), G, œÅi, where:
‚Ä¢ Each r ‚àà R is associated with a set of actions A(r) that are
instantiated from O and O, which r ‚àà R can perform; each
action may not always succeed when executed and hence is
associated with a cost.
‚Ä¢ Each œÜ ‚àà Œ¶ is associated with a capability model GœÜ =
hVœÜ , EœÜ i, in which VœÜ = XœÜ ‚à™ XÃáœÜ . XœÜ ‚äÜ X, in which XœÜ
represents the state variables of the world and agent œÜ and
X represents the joint set of state variables of all agents.
Note that the grounded predicates (which are variables) for robots
that are instantiated from R and O also belong to X. Human and
robotic agents interact through the shared variables.
L EMMA 2. The MAP-MM problem is at least PSPACE-complete.
P ROOF. We only need to prove the result for one of the extreme
cases: when there are only robotic agents. The problem then essentially becomes a multi-agent planning problem, which is more
general than the classical planning problem, which is known to be
PSPACE-complete.

5.1

State Expansion for MAP-MM

First, we make the same assumption as we made in single agent
planning, such that the monotonicity properties still hold. Given
the current planning state bÃÇ(S) (i.e., sampled from b(S)), for each
s ‚àà S, a centralized planner can expand it in the next step using
the following two options in MAP-MM:
‚Ä¢ Choose a robot r and an action a ‚àà A(r) such that at least
one of the complete states s in S satisfies T (P re(a)) ‚äÜ
T (s).
‚Ä¢ Choose a capability on human agent œÜ with the specification
sI ‚áí sE , such that at least one of the states s in S satisfies
T (sI ) ‚äÜ T (s).

Figure 4: Illustration of solving a MAP-MM problem with a
robotic and a human agent, in which the robotic agent has an
action model and the human agent has a capability model that
is assumed to be learned by the robotic agent.
If we have chosen an action a, for any s ‚àà S that satisfies
T (P re(a)) ‚äÜ T (s), the complete state s after applying a becomes s0 , such that T (s0 ) = (T (s) ‚à™ Add(a)) \ Del(a), and
F (s0 ) = (F (s) ‚à™ Del(a)) \ Add(a). The probability weight of
s0 in the new belief state after applying a does not change. For
states in S that do not satisfy T (P re(a)) ‚äÜ T (s), we assume that
the application of a does not change anything. In this way, we can
construct the new belief state after applying this action.
If we have chosen a capability in the form of sI ‚áí sE from
a human agent œÜ, for any s ‚àà S that satisfies T (sI ) ‚äÜ T (s),
we can use follow discussion in Section 4.1 to compute the new
belief state. Similarly, for states in S that do not satisfy T (sI ) ‚äÜ
T (s), we assume that the application of this capability does not
change anything. With the new belief state, we can continue the
state expansion process to expand the c-plan further.

5.2

Planning Heuristic for MAP-MM

In this section, we discuss a planning heuristic that informs us
which state should be chosen to expand at any time. We can adapt
the heuristic in Equation (11) to address MAP-MM. Given the current planning state bÃÇ(S), we need to compute h(bÃÇ(S)). For each
s ‚àà S, there are three cases:
1) If only capabilities are used afterwards, h(s) can be computed
as in Equation (11), except that all capability models must be considered. 2) If only actions are going to be used, h(s) can be computed based on the relaxed plan heuristic (i.e., ignoring all deleting
effects), while considering all robot actions. 3) If both capabilities
and actions can be used, h(s) can be computed as the minimum
cost of an action that achieves any variable in Gs . The final h(s)
is chosen as the smallest value among the three cases and h(bÃÇ(S))
can subsequently be computed.
C OROLLARY 1. The heuristic above is admissible for finding a
c-plan for MAP-MM that maxmizes œÅ, given that bÃÇ(S) accurately
represents b(S).

5.3

Evaluation

In this section, we describe a simple application of MAP-MM
involving a human and a robot, in which the capability model of
the human is assumed to be learned by the robot. The robot then
makes a multi-agent c-plan for both the human and itself.
The setup of this evaluation is identical to that in the evaluation
of single agent planning, as we discussed in the previous section.

In this example, we associate the robot actions with a constant cost.
After learning the parameters of the human capability model based
on the generated partial plan traces, we apply the capability model
to solve a problem as shown in Figure 4. Initially, we have On(B1,
B3), OnT able(B3), and OnT able(B2) (a complete state), and
the goal is to achieve On(B3, B2). The multi-agent c-plan involves the application of two robot actions and one capability of
the human. We show three possible complete states (in the belief
state) which satisfy the goal variable after applying the capability.

6.

RELATED WORK

Most existing approaches for representing the dynamics of agents
assume that the models are completely specified. This holds whether
the underlying models are based on STRIPS actions (e.g. PDDL
[7]) or stochastic action models (such as RDDL [19]). This assumption of complete knowledge is also the default in the existing
multi-agent planning systems [3].
Capability models, in contrast, start with the default of incomplete models. They are thus related to the work on planning with
incompletely specified actions (c.f. [15, 16, 11]). An important difference is that while this line of work models only incompleteness
in the precondition/effect descriptions of the individual actions, capabilities are incomplete in that they completely abstract over actual plans that realize them. In this sense, a capability has some
similarities to non-primitive tasks in HTN planning [6, 27]. For
example, an abstract HTN plan with a single non-primitive task
only posits that there exists some concrete realization of the nonprimitive task which will achieve the goal supported by the nonprimitive task. However, in practice, all HTN planners use ‚Äúcomplete models‚Äù in that they provide all the reduction schemas to take
the non-primitive task to its concretization. So, the ‚Äúuncertainty‚Äù
here is ‚Äúangelic‚Äù [12] ‚Äì the planner can resolve it by the choice of
reduction schemas. In contrast, capability models do not have to
(and cannot) provide any specifics about the exact plan with which
a capability will be realized.
Capability models also have connections to macro operators [2],
as well as options, their MDP counterparts [21, 20, 9], and the BDI
models [17]. Capability models are useful when plans must be
made with partial knowledge. With complete models, this means
that not all actions or macro-actions or options or capabilities in
BDI to achieve the goal are provided. None of HTN, SMDP or
BDI models can handle the question of what it means to plan when
faced with such model incompleteness. Capability models in contrast propose approximate plans (referred to as c-plans) as a useful
solution concept in informing the user of how likely there is an
actual complete plan.
On the other hand, due to the inherent incompleteness of capability models, they are lossy in the following sense. It is possible to
compile a complete model to a capability model (e.g., converting
actions in RDDL [19] to a capability model), but new capabilities
may also be introduced along with the actions. As a result, the
synthesized plans would still be incomplete unless the use of these
new capabilities are forcibly restricted. The learning of capability
models has connections to learning probabilistic relational models
using Bayesian networks [8]. The notion of eventual state captured
in capability models is similar to that captured by the F operator
(i.e., eventually) in LTL and CTL [5, 23]. Although there are other
works that discuss about capability models, e.g., [4], they are still
based on action modeling.

7.

CONCLUSIONS AND FUTURE WORK

In this paper, we have introduced a new representation to model
agents based on capabilities. The associated model, called a capability model, is an inherently incomplete model that has several
unique advantages compared to traditional complete models (i.e.,
action models). The underlying structure of a capability model is
a generalized 2-TBN, which can encode all the capabilities of an
agent. The associated probabilities computed (i.e., via Bayesian
inference on the capability model) based on the specifications of
capabilities (i.e., a partial initial state coupled with a partial eventual state) determine how likely the capabilities can be fulfilled
by an operation (i.e., a complete plan). This information can be
used to synthesize incomplete ‚Äúplans‚Äù (referred to as c-plans). One
of the unique advantages of capability models is that learning for
both model structure and parameters is robust to high degrees of
incompleteness in plan execution traces (e.g., with only start and
end states). Furthermore, we show that parameter learning for capability models can be performed efficiently online via Bayesian
learning.
Additionally, we provide the details of using capability models in
planning. Compared to traditional models for planning, in a planning state, capability models only suggest transitions between partial states (i.e., specified by the capabilities), along with the probabilities that specify how likely such transitions can be fulfilled by
an operation. The limitation is that the synthesized c-plans are incomplete. However, we realize that this is unavoidable for planning with incomplete knowledge (i.e., incomplete models). In such
cases, the synthesized c-plans can inform the user how likely complete plans exist when following the ‚Äúguidelines‚Äù of the c-plans. In
general, a c-plan with a higher probability of success should imply
that a complete plan is more likely to exist. We discuss using capability models for single agent planning first, and then extend it to
multi-agent planning (with each agent modeled separately by a capability model), in which the capability models of agents are used
by a centralized planner. We also discuss how capability models
can be mixed with complete models.
In future work, we plan to further investigate the relationships
between capability models and traditional models. We also plan to
explore applications of capability models in our ongoing work on
human-robot teaming [22, 13]. For example, we plan to investigate
how to enable robots to learn capability models of humans and plan
to coordinate with the consideration of these models.

Acknowledgments
This research is supported in part by the ARO grant W911NF-13-10023, and the ONR grants N00014-13-1-0176, N00014-13-1-0519
and N00014-15-1-2027.

REFERENCES
[1] C. Backstrom and B. Nebel. Complexity results for sas+
planning. Computational Intelligence, 11:625‚Äì655, 1996.
[2] A. Botea, M. Enzenberger, M. M√ºller, and J. Schaeffer.
Macro-ff: Improving ai planning with automatically learned
macro-operators. Journal of Artificial Intelligence Research,
24:581‚Äì621, 2005.
[3] R. I. Brafman and C. Domshlak. From One to Many:
Planning for Loosely Coupled Multi-Agent Systems. In
ICAPS, pages 28‚Äì35. AAAI Press, 2008.

[4] J. Buehler and M. Pagnucco. A framework for task planning
in heterogeneous multi robot systems based on robot
capabilities. In AAAI Conference on Artificial Intelligence,
2014.
[5] E. Clarke and E. Emerson. Design and synthesis of
synchronization skeletons using branching time temporal
logic. In D. Kozen, editor, Logics of Programs, volume 131
of Lecture Notes in Computer Science, pages 52‚Äì71.
Springer Berlin Heidelberg, 1982.
[6] K. Erol, J. Hendler, and D. S. Nau. Htn planning:
Complexity and expressivity. In In Proceedings of the
Twelfth National Conference on Artificial Intelligence, pages
1123‚Äì1128. AAAI Press, 1994.
[7] M. Fox and D. Long. PDDL2.1: An extension to pddl for
expressing temporal planning domains. Journal of Artificial
Intelligence Research, 20:2003, 2003.
[8] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning
probabilistic relational models. In In IJCAI, pages
1300‚Äì1309. Springer-Verlag, 1999.
[9] P. J. Gmytrasiewicz and P. Doshi. Interactive pomdps:
Properties and preliminary results. In Proceedings of the
Third International Joint Conference on Autonomous Agents
and Multiagent Systems - Volume 3, AAMAS ‚Äô04, pages
1374‚Äì1375, Washington, DC, USA, 2004. IEEE Computer
Society.
[10] L. P. Kaelbling, M. L. Littman, and A. W. Moore.
Reinforcement learning: A survey. J. Artif. Int. Res.,
4(1):237‚Äì285, May 1996.
[11] S. Kambhampati. Model-lite planning for the web age
masses: The challenges of planning with incomplete and
evolving domain models, 2007.
[12] B. Marthi, S. J. Russell, and J. Wolfe. Angelic semantics for
high-level actions. In Proceedings of the Seventeenth
International Conference on Automated Planning and
Scheduling (ICAPS), 2007.
[13] V. Narayanan, Y. Zhang, N. Mendoza, and S. Kambhampati.
Automated planning for peer-to-peer teaming and its
evaluation in remote human-robot interaction. In ACM/IEEE
International Conference on Human Robot Interaction
(HRI), 2015.
[14] R. E. Neapolitan. Learning Bayesian networks. Prentice
Hall, 2004.
[15] T. Nguyen and S. Kambhampati. A heuristic approach to
planning with incomplete strips action models. In
International Conference on Automated Planning and
Scheduling, 2014.

[16] T. A. Nguyen, S. Kambhampati, and M. Do. Synthesizing
robust plans under incomplete domain models. In C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger,
editors, Advances in Neural Information Processing Systems
26, pages 2472‚Äì2480, 2013.
[17] L. Padgham and P. Lambrix. Formalisations of capabilities
for bdi-agents. Autonomous Agents and Multi-Agent Systems,
10(3):249‚Äì271, May 2005.
[18] M. L. Puterman. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons, Inc.,
New York, NY, USA, 1st edition, 1994.
[19] S. Sanner. Relational dynamic influence diagram language
(rddl): Language description, 2011.
[20] S. Seuken and S. Zilberstein. Memory-bounded dynamic
programming for dec-pomdps. In Proceedings of the 20th
International Joint Conference on Artifical Intelligence,
IJCAI‚Äô07, pages 2009‚Äì2015, San Francisco, CA, USA,
2007. Morgan Kaufmann Publishers Inc.
[21] R. S. Sutton, D. Precup, and S. Singh. Between mdps and
semi-mdps: A framework for temporal abstraction in
reinforcement learning. Artif. Intell., 112(1-2):181‚Äì211,
Aug. 1999.
[22] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati. Coordination in human-robot teams using
mental modeling and plan recognition. In IEEE/RSJ
International Conference on Intelligent Robots and Systems
(IROS), pages 2957‚Äì2962, Sept 2014.
[23] M. Vardi. An automata-theoretic approach to linear temporal
logic. In F. Moller and G. Birtwistle, editors, Logics for
Concurrency, volume 1043 of Lecture Notes in Computer
Science, pages 238‚Äì266. Springer Berlin Heidelberg, 1996.
[24] B. Y. White and J. R. Frederiksen. Causal model
progressions as a foundation for intelligent learning
environments. Artificial Intelligence, 42(1):99 ‚Äì 157, 1990.
[25] C. Yuan and B. Malone. Learning optimal bayesian
networks: A shortest path perspective. J. Artif. Int. Res.,
48(1):23‚Äì65, Oct. 2013.
[26] H. H. Zhuo and S. Kambhampati. Action-model acquisition
from noisy plan traces. In Proceedings of the Twenty-Third
International Joint Conference on Artificial Intelligence,
IJCAI‚Äô13, pages 2444‚Äì2450. AAAI Press, 2013.
[27] H. H. Zhuo, H. Mu√±oz Avila, and Q. Yang. Learning
hierarchical task network domains from partially observed
plan traces. Artificial Intelligence, 212:134‚Äì157, July 2014.

Human Computation and Crowdsourcing: Works in Progress Abstracts:
An Adjunct to the Proceedings of the Third AAAI Conference on Human Computation and Crowdsourcing

Acquiring Planning Knowledge via Crowdsourcing
Jie Gaoa and Hankz Hankui Zhuob and Subbarao Kambhampatic and Lei Lib
a
Zhuhai college,
Jilin Univ., Zhuhai, China
jiegao26@163.com

b

School of Data & Computer Sci.,
Sun Yat-sen Univ., China
{lnslilei,zhuohank}@mail.sysu.edu.cn

Introduction

c

Dept. of Computer Sci. & Eng.,
Arizona State Univ., US
rao@asu.edu

initial states and action models based on the answers to HITs
given by the crowd. We then feed the reÔ¨Åned initial states and
action models to planners to solve the problem.

Plan synthesis often requires complete domain models and
initial states as input. In many real world applications, it is
difÔ¨Åcult to build domain models and provide complete initial state beforehand. In this paper we propose to turn to the
crowd for help before planning. We assume there are annotators available to provide information needed for building
domain models and initial states. However, there might be a
substantial amount of discrepancy within the inputs from the
crowd. It is thus challenging to address the planning problem
with possibly noisy information provided by the crowd. We
address the problem by two phases. We Ô¨Årst build a set of
Human Intelligence Tasks (HITs), and collect values from
the crowd. We then estimate the actual values of variables
and feed the values to a planner to solve the problem.
In contrast to previous efforts (Zhang et al. 2012;
Manikonda et al. 2014) that ask the crowd to do planning,
we exploit knowledge about initial states and/or action models from the crowd and feed the knowledge to planners to
do planning. We call our approach PAN-CROWD, stands for
Planning by Acquiring kNowledge from the CROWD.

Building HITs for Action Models: For this part, we build
on our work with the CAMA system (Zhuo 2015). We enumerate all possible preconditions and effects for each action.
SpeciÔ¨Åcally, we generate actions‚Äô preconditions and effects
as follows (Zhuo and Yang 2014). If the parameters of predicate p, denoted by Para(p), are included by the parameters
of action a, denoted by Para(a), i.e., Para(p) ‚äÜ Para(a), p is
likely a precondition, or an add effect, or a delete effect of a.
We therefore generate three new proposition variables ‚Äúp ‚àà
Pre(a)‚Äù, ‚Äúp ‚àà Add(a)‚Äù and ‚Äúp ‚àà Del(a)‚Äù, the set of which
is denoted by Hpre = {p ‚àà Pre(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A},
Hadd = {p ‚àà Add(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, Hdel = {p ‚àà
Del(a)|‚àÄp ‚àà P and ‚àÄa ‚àà A}, respectively. We put all the
proposition variables together and estimate the label of each
variable by querying the crowd or annotators.
For each proposition in H, we build a Human Intelligent
Task (HIT) in the form of a short survey. For example, for
the proposition ‚Äú(ontable ?x) ‚àà Pre(pickup)‚Äù, we generate
a survey as shown below: Is the fact that ‚Äúx is on table‚Äù
a precondition of picking up the block x? There are three
possible responses, i.e., Yes, No, Cannot tell, out of which
an annotator has to choose exactly one. Each annotator is
allowed to annotate a given survey once. Note that the set of
proposition variables H will not be large, since all predicates
and actions are in the ‚Äúvariable‚Äù form rather than instantiated
form, and we only consider predicates whose parameters
are included by actions. For example, for the blocks domain,
there are only 78 proposition variables in H.

The Formulation of Our Planning Problem
We formulate our planning problem as a quadruple P =
sÀú0 , g, O, AÃÑ, where sÀú0 is an incomplete initial state which
is composed of a set of open propositions. A proposition is
called open if there exist variables in the parameter list of
the proposition, e.g., ‚Äúon(A, ?x)‚Äù (a symbol preceded by ‚Äú?‚Äù
denotes a variable that can be instantiated by an object) is an
open proposition since ?x is a variable in the parameter list
of proposition on. An open initial state can be incomplete,
i.e., some propositions are missing. The set of variables in
sÀú0 is denoted by V. O is a set of objects that can be selected
and assigned to variables in V. We assume O can be easily
collected based on historical applications. AÃÑ is a set of incomplete STRIPS action models. aÃÑ ‚àà AÃÑ is called ‚Äúincomplete‚Äù
if there are some predicates missing in the preconditions or
effects of aÃÑ. A solution to the problem is a plan and a set of
‚ÄúreÔ¨Åned‚Äù action models.

Building HITs for Initial States: To generate surveys that
are as simple as possible, we assume there are sets of objects
known to our approach, each of which corresponds to a type.
For example, {A, B, C} is a set of objects with type ‚ÄúBlock‚Äù
in the blocks domain, i.e., there are three blocks known to our
approach. We can thus generate a set of possible propositions
S with the sets of objects and predicates of a domain. For
each proposition, we formulate the Human Intelligent Task
(HIT) as a short survey, the set of which is denoted by H.
To reduce the number of HITs, we start from the goal g, we
search for the action whose add effects match with g, and
update g to be an internal state by deleting add effects and
adding preconditions of the action into g. We then check

The PAN-CROWD approach
To acquire planning knowledge from the crowd, we Ô¨Årst build
HITs for action models and initial states, and then reÔ¨Åne
c 2015, Association for the Advancement of ArtiÔ¨Åcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

6

base on a ratio Œ±, resulting in open problems P . Propositions
were then changed to short surveys in natural language. We
performed extensive simulations using 20 simulated annotators to complete the human intelligent tasks (HITs). We
deÔ¨Åne the accuracy of our approach by comparing to groundtruth solutions (generated by simulators). In other words, we
solve all 150 problems using our approach, and compare the
150 solutions to ground-truth solutions, viewing the ratio of
identical solutions over 150 as the accuracy.
We ran our PAN-CROWD algorithm on testing problems by varying from 0.1 to 0.5 the ratio Œ±, setting the number of objects to be
10. The result is shown in Figure 1.
From Figure 1, we can see that the
accuracy generally becomes lower
Figure 1: The accuwhen the ratio increases in all three
racy w.r.t. ratio Œ±.
testing domains. This is consistent
with our intuition, since the larger the ratio is, the more
uncertain the information that is introduced to the original
planning problem when using crowdsourcing. However, from
the curves we can see the accuracies are no less than 70%
when the ratio is smaller than 0.3.

whether open initial state matches with internal state. Secondly, from the matched internal state, we select those with
unknown variables, and choose them with the same variables
to build a set of propositions with unknown variables, which
is Ô¨Ånally transformed into a set of HITs.
Estimating True Labels Assume there are R annotators
and N tasks with binary labels {1, 0}. The true labels of
tasks are denoted by Z = {zi ‚àà {1, 0}, i = 1, 2, . . . , N }
. Let Nj is the set of tasks labeled by annotator j, and Ri
is the set of annotators labeling task i. The task assignment
scheme can be represented by a bipartite graph where an
edge (i, j) denotes that the task i is labeled by the worker j.
The labeling results form a matrix Y ‚àà {1, 0}N √óR . The goal
is to Ô¨Ånd an optimal estimator ZÃÇ of the true labels Z given
the
observation Y, minimizing the average bit-wise error rate
1
i‚àà{1,2,...,N } prob[zÃÇi = zi ].
N
We model the accuracy of annotators separately on the
positive and negative examples (Raykar et al. 2010). If the
true label is one, the true positive rate T P j for the jth annotator is deÔ¨Åned as the probability that the annotator labels
it as one, i.e., T P j = p(yij = 1|zi = 1). On the other hand
if the true label is zero, the true negative rate T N j is deÔ¨Åned as the probability that annotator labels it as zero, i.e.,
T N j = p(yij = 0|zi = 0). Suppose we have the training
data set D = {xi , yi1 , . . . , yiR }N
i=1 with N instances from
R annotators, where xi ‚àà X is an instance (typically a ddimensional feature vector), yij is the label (1 or 0) given
by the jth annotator. Considering the family of linear discriminating functions, the probability for the positive class is
modeled as a logistic sigmoid, i.e., p(y = 1|x, w) = œÉ(wT x),
where x, œâ ‚àà Rd , and œÉ(z) = 1+e1‚àíz .
The task is to estimate the parameter w as well as the
true positive P = T P 1 , . . . , T P R  and the true negative
N = T N 1 , . . . , T N R . Let Œ∏ = {w, P, N }, the probability of training data D can be deÔ¨Åned by p(D|Œ∏) =
N
1
R
i=1 p(yi , . . . , yi |xi , Œ∏). The EM algorithm can be exploited to estimate the parameter Œ∏ by maximizing the loglikelihood of p(D|Œ∏) (Raykar et al. 2010). Let Œºi = p(zi =
1|yi1 , . . . , yiR , xi , Œ∏). We simply set the threshold as 0.5, i.e.,
if Œºi > 0.5, the value of zÃÇi is assigned with 1, otherwise 0.










	
	













Discussion and Conclusion
We propose to acquire knowledge about initial states and
action models from the crowd. Since the number of HITS
sent to the crowd related to initial states could be large, in the
future, we could consider how to reduce the number of HITS,
e.g. by exploiting backward chaining planning techniques ‚Äì
which only might need to know small parts of the initial state,
rather than the whole state. In addition, we could also think
of ways of engaging the crowd in more active ways, rather
than answering yes/no to HITS. For example, we can give
them candidate plans and ask them to critique the plan and/or
modify them to make them correct. We then learn knowledge
from the modiÔ¨Åcation process and use it for computing plans.

Acknowledgements
Zhuo‚Äôs research is supported by NSFC (No. 61309011) and
Fundamental Research Funds for the Central Universities
(No. 14lgzd06). Kambhampati‚Äôs research is supported in part
by a Google Research Award and the ONR grants N0001413-1-0176 and N00014-15-1-2027.

References
Manikonda, L.; Chakraborti, T.; De, S.; Talamadupula, K.; and
Kambhampati, S. 2014. AI-MIX: using automated planning to
steer human workers towards better crowdsourced plans. In IAAI,
3004‚Äì3009.
Raykar, V. C.; Yu, S.; Zhao, L. H.; Valadez, G. H.; Florin, C.; Bogoni,
L.; and Moy, L. 2010. Learning from crowds. JMLR 11:1297‚Äì1322.
Zhang, H.; Law, E.; Miller, R.; Gajos, K.; Parkes, D. C.; and Horvitz,
E. 2012. Human computation tasks with global constraints. In CHI,
217‚Äì226.
Zhuo, H. H., and Yang, Q. 2014. Action-model acquisition for
planning via transfer learning. Artif. Intell. 212:80‚Äì103.
Zhuo, H. H. 2015. Crowdsourced action-model acquisition for
planning. In AAAI, 3004‚Äì3009.

Experiment
Since the action model acquisition part has been evaluated
in the context of CAMA already (Zhuo 2015), here we focus
on evaluating the initial state acquisition part. We evaluated
our approach in three planning domains, i.e., blocks1 , depots2
and driverlog4 . We Ô¨Årst generated 150 planning problems
with complete initial states, denoted by PÃÑ . After that we
randomly removed propositions from the initial states in PÃÑ
2


	



ReÔ¨Åning Initial States and Action Models: We reÔ¨Åne the
initial state and action models based on estimated true labels.
Once the initial state and action models reÔ¨Åned, we solve
the corresponding revised planning problem using an off-theshelf planner.

1





http://www.cs.toronto.edu/aips2000/
http://planning.cis.strath.ac.uk/competition/

7

Automated Planning for Peer-to-peer Teaming and its
Evaluation in Remote Human-Robot Interaction
Vignesh Narayanan

Yu Zhang

Nathaniel Mendoza

Dept. of Computer Science
Arizona State University
Tempe, AZ

Dept. of Computer Science
Arizona State University
Tempe, AZ

Dept. of Computer Science
Arizona State University
Tempe, AZ

vnaray15@asu.edu

yzhan442@asu.edu
Subbarao Kambhampati

namendoz@asu.edu

Dept. of Computer Science
Arizona State University
Tempe, AZ

rao@asu.edu
ABSTRACT
Human factor studies on remote human-robot interaction
are often restricted to various forms of supervision, in which
the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this study,
we investigate the incorporation of a general planning capability into the robot to facilitate peer-to-peer human-robot
teaming, in which the human and robot are viewed as teammates that are physically separated. One intriguing question is to what extent humans may feel uncomfortable at
such robot autonomy and lose situation awareness, which
can potentially reduce teaming performance. Our results
suggest that peer-to-peer teaming is preferred by humans
and leads to better performance. Furthermore, our results
show that peer-to-peer teaming reduces cognitive loads from
objective measures (even though subjects did not report this
in their subjective evaluations), and it does not reduce situation awareness for short-term tasks.

Categories and Subject Descriptors
H.1.2 [Human factors]; I.2.8 [Plan execution, formation, and generation]; J.7 [Command and control]

Keywords
Robot design principles; Autonomous robot capabilities; User
study/Evaluation; Teamwork & group dynamics

1.

INTRODUCTION

In supervised human-robot teaming, the human creates
the plan to achieve the global goal, and then either directly
provides motion commands or breaks the plan into sub-plans

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
HRI‚Äô15 Extended Abstracts, March 2‚Äì5, 2015, Portland, OR, USA.
ACM 978-1-4503-3318-4/15/03.
http://dx.doi.org/10.1145/2701973.2702042.

(or sub-goals) for the robot to handle. In peer-to-peer (P2P)
teaming, the human and robot share the same global goal
and collaborate to achieve it. While increasing robot autonomy is generally viewed as desirable, humans may feel
uncomfortable at the loss of control entailed by such autonomy in P2P teaming, which can potentially reduce situation
awareness for humans and affect teaming performance.
In this study,1 we mainly concentrate on remote interaction and perform our investigation for emergency response
in an urban search and rescue (USAR) task. Here, the task
cannot be fully specified a priori due to incomplete information about the models, goals or settings in such scenarios. Furthermore, information can be continuously changing
throughout the task and may not always be synchronized
between the human and robot (e.g., goal updates and human preference models) due to communication or interpretation delays. The goal of this USAR task is to explore
areas of the disaster scene to provide real-time information,
which is then used to aid the management team to create
rescue plans (e.g., identifying locations of casualties). The
aim of our study is to compare P2P and supervised teaming
in terms of objective measures as well as subjective measures
such as situation awareness and mental workload.
Related Work: In most previous works on human-robot
and human-machine interactions (e.g., [1]) for teaming, the
human always plays a supervisor role. While there are works
that incorporate general planning capabilities into robots to
achieve P2P teaming (e.g., [3]), as yet, there exists no empirical investigation of its influence on the teaming performance. In this study, we implement P2P teaming in which
the human and robot are viewed as teammates, and the
robot exhibits autonomy through automated planning capabilities. In terms of automation in human-robot interaction,
it is well known that it can have both positive and negative
effects on human performance [2].

2.

STUDY DESIGN

Fig. 1 presents the simulated environment and humanrobot interface used in our USAR task, which represents the
floor plan of an office building before a disaster occurs (e.g.,
1

A longer version at rakaposhi.eas.asu.edu/hri15-long.html.

Figure 1: Environment (left) and interface (right)
used in the USAR task with a simulated Nao robot.

Figure 2: Results for objective measures.

a fire). The study was performed over 4 weeks and involved
19 volunteers. Each subject took part in one experimental
trial of either P2P or supervised teaming. Both the human
subject and robot had access to the floor plan before the
disaster. The robot in P2P teaming could use the planning
capability (based on SapaReplan [3]) to independently create its own plan based on the floor plan and current status
of the task. In both teaming scenarios, the robot displayed
a list of applicable actions that it could perform given the
current state. The interaction interfaces were the same except that the robot in P2P teaming also recommended the
next action (from along the applicable ones) in its plan. The
global goal was to report the number of casualties in as many
rooms as possible in 20 minutes.
The incomplete task information was assumed to be a result of blocked doors. We provided the information regarding which doors might be blocked to the human subject,
but only after the task had run for 1 minute to simulate
dynamic information. This information, however, remained
unknown to the robot. The human subject could interact
with the robot at specific times to reduce the influence of
this information asymmetry, or the robot had to learn this
information by pushing the door and failing (which could
reduce the teaming performance).
In a real USAR task, the human would also have other
information to process and analyze. To simulate this, the
human subject was also assigned to a secondary task. This
secondary task involved solving three-dimensional spatial visualization puzzles. The performance of the team was evaluated on both the primary and secondary tasks. Each trial
ended when the given time elapsed. Finally, the human subject completed a questionnaire (in Likert scale) that included
questions for evaluating situation awareness, mental workload and other subjective human-robot interaction aspects.

3.

RESULTS AND CONCLUSIONS

The results for objective measures are presented in Fig. 2.
Overall, subjects in P2P teaming outperformed subjects in
supervised teaming in terms of the number of rooms visited

Figure 3: Results for subjective measures. ‚àó denotes
p < 0.05, ‚àó‚àó denotes p < 0.01, ‚àó ‚àó ‚àó denotes p < 0.001.
within 20 minutes and the secondary task performance (left
part of Fig. 2). In fact, performance for the primary task
in supervised teaming was no better than the robot with a
planning capability executing the task alone (i.e., P2P-NI).
Results for subjective measures as assessed by the survey
questionnaire are presented in Fig. 3. Our analysis on mental workload did not show any significant difference due to
the fact that most humans preferred to rely on themselves at
the beginning even in P2P teaming, which might be a result
of the lack of trust in the robot for handling the task initially.
This effect can be seen from the right part of Fig. 2, which
shows how many robot recommended actions were followed
by the human subjects in P2P teaming. Even though the
workload was seen to be almost the same based on subjective
measures (Fig. 3), the objective measures suggest that the
cognitive load was indeed reduced (based on the time spent
on the secondary task (left part of Fig. 2)). Our analysis on
situational awareness did not show any significant difference
either. This might be partially due to the fact that the recommended action of the robot from the planning capability
during execution provided situation awareness to the human
subject, since the same action would likely be chosen by the
subject in the same situation. This is interesting since it suggests that humans in P2P teaming can maintain situation
awareness, at least for short-term tasks such as our USAR
task. Our analysis on likability, improvability, and complacency showed that the subjects generally preferred and felt
more satisfied working with the robot in P2P teaming.
Our conclusions from this preliminary study are
that humans prefer working with robots with a planning capability for P2P teaming, and the planning
capability helps reduce cognitive load and maintain
situation awareness for short-term tasks.
Acknowledgments: This research is supported in part by
the ARO grant W911NF-13-1-0023, and the ONR grants
N00014-13-1-0176 and N00014-13-1-0519.

4.

REFERENCES

[1] J. Casper and R. Murphy. Human-robot interactions
during the robot-assisted urban search and rescue
response at the world trade center. IEEE Trans. on
SMC Part B, 33(3):367‚Äì385, June 2003.
[2] R. Parasuraman. Designing automation for human use:
empirical studies and quantitative models. Ergonomics,
43(7):931‚Äì951, 2000. PMID: 10929828.
[3] K. Talamadupula, J. Benton, S. Kambhampati,
P. Schermerhorn, and M. Scheutz. Planning for
human-robot teaming in open worlds. ACM Trans.
Intell. Syst. Technol., 1(2):14:1‚Äì14:24, Dec. 2010.

